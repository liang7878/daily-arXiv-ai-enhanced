<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 83]
- [cs.CE](#cs.CE) [Total: 8]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DS](#cs.DS) [Total: 11]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 30]
- [cs.LG](#cs.LG) [Total: 214]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 27]
- [q-fin.CP](#q-fin.CP) [Total: 4]
- [q-fin.TR](#q-fin.TR) [Total: 2]
- [stat.ML](#stat.ML) [Total: 18]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 6]
- [econ.EM](#econ.EM) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [cs.CR](#cs.CR) [Total: 17]
- [math.NA](#math.NA) [Total: 4]
- [cs.SD](#cs.SD) [Total: 5]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.CL](#cs.CL) [Total: 60]
- [math.CO](#math.CO) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [eess.AS](#eess.AS) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 81]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 23]
- [econ.TH](#econ.TH) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [math.ST](#math.ST) [Total: 3]
- [quant-ph](#quant-ph) [Total: 7]
- [eess.IV](#eess.IV) [Total: 6]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [econ.GN](#econ.GN) [Total: 4]
- [cs.ET](#cs.ET) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [q-fin.PR](#q-fin.PR) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue](https://arxiv.org/abs/2510.21720)
*Anant Pareek*

Main category: cs.AI

TL;DR: 本文提出综合框架连接预测建模与心理分析交互系统，介绍方法并展示成果，为计算心理学和人机交互研究提供实用模型。


<details>
  <summary>Details</summary>
Motivation: 利用人工智能和计算心理学融合的机会，搭建框架弥合孤立预测建模与心理分析交互系统间的差距。

Method: 采用端到端开发流程，包括用经典机器学习技术在心理数据集建基准、微调Transformer模型、用参数高效技术微调大语言模型、构建并部署微服务生态系统。

Result: 成功稳定情感计算中基于Transformer的回归模型，开发出可复制的大规模AI研究方法。

Conclusion: 该工作整体方法展示了从研究到部署的完整流程，为计算心理学和人机交互未来研究提供实用模型。

Abstract: The confluence of Artificial Intelligence and Computational Psychology
presents an opportunity to model, understand, and interact with complex human
psychological states through computational means. This paper presents a
comprehensive, multi-faceted framework designed to bridge the gap between
isolated predictive modeling and an interactive system for psychological
analysis. The methodology encompasses a rigorous, end-to-end development
lifecycle. First, foundational performance benchmarks were established on four
diverse psychological datasets using classical machine learning techniques.
Second, state-of-the-art transformer models were fine-tuned, a process that
necessitated the development of effective solutions to overcome critical
engineering challenges, including the resolution of numerical instability in
regression tasks and the creation of a systematic workflow for conducting
large-scale training under severe resource constraints. Third, a generative
large language model (LLM) was fine-tuned using parameter-efficient techniques
to function as an interactive "Personality Brain." Finally, the entire suite of
predictive and generative models was architected and deployed as a robust,
scalable microservices ecosystem. Key findings include the successful
stabilization of transformer-based regression models for affective computing,
showing meaningful predictive performance where standard approaches failed, and
the development of a replicable methodology for democratizing large-scale AI
research. The significance of this work lies in its holistic approach,
demonstrating a complete research-to-deployment pipeline that integrates
predictive analysis with generative dialogue, thereby providing a practical
model for future research in computational psychology and human-AI interaction.

</details>


### [2] [PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation](https://arxiv.org/abs/2510.21721)
*Kentaro Ueda,Takehiro Takayanagi*

Main category: cs.AI

TL;DR: 现有大语言模型在个性化故事生成上有挑战，本文提出PREFINE框架，在不更新参数和直接用户反馈下实现个性化生成，评估显示其优于基线，对更广泛应用有潜在价值。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在生成反映用户偏好的个性化故事方面存在的问题，传统方法有用户负担、数据收集、计算成本和隐私等实际问题。

Method: 提出PREFINE框架，从用户交互历史构建伪用户代理并生成特定规则，让代理基于规则代表用户进行批判和改进。

Result: 在自动评估中，PREFINE比基线有更高胜率和显著分数，且不影响故事质量，模型变体分析证明伪用户代理和特定规则对提升个性化性能很关键。

Conclusion: PREFINE框架可实现高效个性化生成，其方法对对话系统、教育和推荐等更广泛应用有潜在作用。

Abstract: While recent advances in Large Language Models (LLMs) have improved the
quality of creative text generation, significant challenges remain in producing
personalized stories that reflect individual user preferences. Conventional
approaches rely on explicit feedback or fine-tuning, which presents practical
issues regarding user burden, data collection, computational costs, and
privacy. In this work, we propose PREFINE (Persona-and-Rubric Guided
Critique-and-Refine), a novel framework that extends the Critique-and-Refine
paradigm to personalization. PREFINE constructs a pseudo-user agent from a
user's interaction history and generates user-specific rubrics (evaluation
criteria). By having this agent critique and refine outputs on the user's
behalf based on these tailored rubrics, our method achieves personalized
generation without requiring parameter updates or direct user feedback. We
conducted a comprehensive evaluation on the PerDOC and PerMPST story datasets.
We designed three baseline methods and several model variants to verify the
contribution of each component of our framework. In automatic evaluations
(LLM-as-a-Judge), PREFINE achieved higher win rates and statistically
significant scores than the baselines, without compromising general story
quality. Analysis of the model variants confirmed that both the pseudo-user
agent and the user-specific rubrics are crucial for enhancing personalization
performance. Beyond story generation, our approach holds potential for enabling
efficient personalization in broader applications, such as dialogue systems,
education, and recommendation.

</details>


### [3] [SIGN: Schema-Induced Games for Naming](https://arxiv.org/abs/2510.21855)
*Ryan Zhang,Herbert Woisetscläger*

Main category: cs.AI

TL;DR: 介绍SIGN命名游戏，发现轻量级结构能使约定形成更快收敛，有更广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现实AI系统中LLM智能体交互时会因约定不一致导致协调问题，复杂应用需可靠、一致通信及可扩展性。

Method: 引入Schema - Induced Games for Naming (SIGN)命名游戏，对比有结构通信和无约束自然语言。

Result: 有结构通信收敛更快，一致性最高达5.8倍。

Conclusion: 轻量级结构可作为高效多智能体协调的简单控制手段，有更广泛应用。

Abstract: Real-world AI systems are tackling increasingly complex problems, often
through interactions among large language model (LLM) agents. When these agents
develop inconsistent conventions, coordination can break down. Applications
such as collaborative coding and distributed planning therefore require
reliable, consistent communication, and scalability is a central concern as
systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming
game that examines how lightweight structure can steer convention formation. We
compare schema-induced communication to unconstrained natural language and find
faster convergence with up to 5.8x higher agreement. These results suggest that
minimal structure can act as a simple control knob for efficient multi-agent
coordination, pointing toward broader applications beyond the naming game.

</details>


### [4] [Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks](https://arxiv.org/abs/2510.21866)
*Javier Marín*

Main category: cs.AI

TL;DR: 研究发现解码器自回归语言模型在知识密集型任务上存在能力上限，参数扩展对准确率提升有限，对注意力干预敏感。


<details>
  <summary>Details</summary>
Motivation: 探究解码器自回归语言模型在知识密集型任务上的能力表现，为资源分配决策提供依据。

Method: 对OPT和Pythia模型家族进行系统评估，开展注意力干预实验。

Result: 知识检索任务准确率提升小，数学基准测试准确率平稳，算术等程序任务指标同步改善；注意力干预致性能崩溃。

Conclusion: 对OPT和Pythia架构，超1 - 2B参数扩展对知识密集型应用准确率提升有限，能力特定扩展失败原因待究。

Abstract: We document empirical capability ceilings in decoder-only autoregressive
language models across knowledge-intensive tasks. Systematic evaluation of OPT
and Pythia model families (70M-30B parameters, spanning 240 times scaling)
reveals that knowledge retrieval tasks show negligible accuracy improvement
despite smooth loss reduction. On MMLU mathematics benchmarks, accuracy remains
flat at 19-20% (below 25% random chance) across all scales while cross-entropy
loss decreases by 31%. In contrast, procedural tasks like arithmetic show
conventional scaling where both metrics improve together. Attention
intervention experiments reveal high sensitivity to perturbation: swapping
attention patterns between models causes catastrophic performance collapse
(complete accuracy loss) rather than graceful degradation. These measurements
have immediate engineering implications: for knowledge-intensive applications
using OPT and Pythia architectures, parameter scaling beyond 1-2B offers
minimal accuracy gains despite continued loss improvement. Our findings
quantify capability-specific scaling failures in these model families to inform
resource allocation decisions. Whether these patterns reflect fundamental
constraints of decoder-only architectures or implementation-specific
limitations remains an open question requiring investigation across diverse
architectural approaches.

</details>


### [5] [GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.21881)
*Nannan Shi,Chuanyu Qin,Shipeng Song,Man Luo*

Main category: cs.AI

TL;DR: 论文指出大语言模型在几何问题视觉推理中表现不佳，开发GeoThoughts数据集和GeoThought - MLLM模型，模型表现优于现有基准，分析失败案例并可通过CoT纠正错误。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在几何视觉推理任务中性能下降，现有数据集存在规模、多样性和推理痕迹不足的问题，阻碍模型有效训练。

Method: 开发GeoThoughts数据集，包含两个子集，每个条目有多种信息；基于该数据集开发GeoThought - MLLM模型。

Result: GeoThought - MLLM模型在几何任务中优于现有基准，训练该模型可提升几何推理能力。

Conclusion: 使用Chain - of - Thought数据集训练能提升模型几何推理能力，通过CoT可纠正模型错误。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities
in text-based mathematical problem solving; however, when adapted to visual
reasoning tasks, particularly geometric problem solving, their performance
substantially declines because geometric problems present unique challenges.
Specifically, these challenges stem from two key factors: first, the intrinsic
complexity of geometry requiring detailed image comprehension and multi-step
reasoning, and second, the limitations of existing datasets which lack
sufficient scale, diversity, and explicit reasoning traces, consequently
hindering effective model training. To address these challenges, we developed
the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two
subsets: Geo-Thought-6K with 6,243 samples and its augmented version
Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual
descriptions, step-by-step solutions, explicit reasoning chains, reflection
steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a
mathematical reasoning multimodal model that generates detailed thinking
processes during problem-solving. Our model outperforms existing benchmarks in
geometric tasks, demonstrating that training with our Chain-of-Thought dataset
improves geometric reasoning capabilities across both in-domain and
out-of-domain settings. Finally, we analyze failure cases and observe that
errors primarily arise from incorrect interpretation of mathematical concepts
or spatial misjudgment. By invoking CoT to correct these mistakes, the model
produces correct answers.

</details>


### [6] [Exploration through Generation: Applying GFlowNets to Structured Search](https://arxiv.org/abs/2510.21886)
*Mark Phillip Matovic*

Main category: cs.AI

TL;DR: 本文将生成流网络（GFlowNets）应用于三个图优化问题，实验表明其能找到最优解，证明生成模型可通过学习策略解决组合优化问题，且有计算可扩展性。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型解决图优化问题的能力及优势，解决经典算法在大规模问题上的计算局限。

Method: 将GFlowNets应用于旅行商问题、最小生成树和最短路径问题，使用轨迹平衡损失训练模型来顺序构建解决方案。

Result: 实验显示GFlowNets能学习找到最优解，生成的解决方案与经典算法匹配，训练收敛依赖于问题复杂度。

Conclusion: 生成模型可通过学习策略解决组合优化问题，基于学习的方法具有计算可扩展性，有潜力处理经典精确方法不可行的大规模问题。

Abstract: This work applies Generative Flow Networks (GFlowNets) to three graph
optimization problems: the Traveling Salesperson Problem, Minimum Spanning
Tree, and Shortest Path. GFlowNets are generative models that learn to sample
solutions proportionally to a reward function. The models are trained using the
Trajectory Balance loss to build solutions sequentially, selecting edges for
spanning trees, nodes for paths, and cities for tours. Experiments on benchmark
instances of varying sizes show that GFlowNets learn to find optimal solutions.
For each problem type, multiple graph configurations with different numbers of
nodes were tested. The generated solutions match those from classical
algorithms (Dijkstra for shortest path, Kruskal for spanning trees, and exact
solvers for TSP). Training convergence depends on problem complexity, with the
number of episodes required for loss stabilization increasing as graph size
grows. Once training converges, the generated solutions match known optima from
classical algorithms across the tested instances. This work demonstrates that
generative models can solve combinatorial optimization problems through learned
policies. The main advantage of this learning-based approach is computational
scalability: while classical algorithms have fixed complexity per instance,
GFlowNets amortize computation through training. With sufficient computational
resources, the framework could potentially scale to larger problem instances
where classical exact methods become infeasible.

</details>


### [7] [Computational Hardness of Reinforcement Learning with Partial $q^π$-Realizability](https://arxiv.org/abs/2510.21888)
*Shayan Karimi,Xiaoqi Tan*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates the computational complexity of reinforcement
learning in a novel linear function approximation regime, termed partial
$q^{\pi}$-realizability. In this framework, the objective is to learn an
$\epsilon$-optimal policy with respect to a predefined policy set $\Pi$, under
the assumption that all value functions for policies in $\Pi$ are linearly
realizable. The assumptions of this framework are weaker than those in
$q^{\pi}$-realizability but stronger than those in $q^*$-realizability,
providing a practical model where function approximation naturally arises. We
prove that learning an $\epsilon$-optimal policy in this setting is
computationally hard. Specifically, we establish NP-hardness under a
parameterized greedy policy set (argmax) and show that - unless NP = RP - an
exponential lower bound (in feature vector dimension) holds when the policy set
contains softmax policies, under the Randomized Exponential Time Hypothesis.
Our hardness results mirror those in $q^*$-realizability and suggest
computational difficulty persists even when $\Pi$ is expanded beyond the
optimal policy. To establish this, we reduce from two complexity problems,
$\delta$-Max-3SAT and $\delta$-Max-3SAT(b), to instances of GLinear-$\kappa$-RL
(greedy policy) and SLinear-$\kappa$-RL (softmax policy). Our findings indicate
that positive computational results are generally unattainable in partial
$q^{\pi}$-realizability, in contrast to $q^{\pi}$-realizability under a
generative access model.

</details>


### [8] [Performance Trade-offs of Optimizing Small Language Models for E-Commerce](https://arxiv.org/abs/2510.21970)
*Josip Tomo Licardo,Nikola Tankovic*

Main category: cs.AI

TL;DR: 本文探讨小的开放权重模型用于电商意图识别，优化10亿参数的Llama 3.2模型，结果显示其精度达99%，匹配GPT - 4.1，小模型是特定领域更合适选择。


<details>
  <summary>Details</summary>
Motivation: 领先商业模型用于电商等专业任务受高计算成本、延迟和运营费用阻碍，研究小的开放权重模型作为资源高效替代方案的可行性。

Method: 用合成数据集通过QLoRA微调10亿参数的Llama 3.2模型，应用后训练量化技术创建GPU优化（GPTQ）和CPU优化（GGUF）版本。

Result: 专业10亿参数模型精度达99%，匹配大得多的GPT - 4.1模型；4位GPTQ减少VRAM使用但减慢推理速度，GGUF在CPU上推理吞吐量提升18倍，RAM消耗减少超90%。

Conclusion: 小的、适当优化的开放权重模型不仅可行，而且更适合特定领域应用，能以较低计算成本实现先进精度。

Abstract: Large Language Models (LLMs) offer state-of-the-art performance in natural
language understanding and generation tasks. However, the deployment of leading
commercial models for specialized tasks, such as e-commerce, is often hindered
by high computational costs, latency, and operational expenses. This paper
investigates the viability of smaller, open-weight models as a
resource-efficient alternative. We present a methodology for optimizing a
one-billion-parameter Llama 3.2 model for multilingual e-commerce intent
recognition. The model was fine-tuned using Quantized Low-Rank Adaptation
(QLoRA) on a synthetically generated dataset designed to mimic real-world user
queries. Subsequently, we applied post-training quantization techniques,
creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results
demonstrate that the specialized 1B model achieves 99% accuracy, matching the
performance of the significantly larger GPT-4.1 model. A detailed performance
analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ
reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older
GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF
formats on a CPU achieved a speedup of up to 18x in inference throughput and a
reduction of over 90% in RAM consumption compared to the FP16 baseline. We
conclude that small, properly optimized open-weight models are not just a
viable but a more suitable alternative for domain-specific applications,
offering state-of-the-art accuracy at a fraction of the computational cost.

</details>


### [9] [Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions](https://arxiv.org/abs/2510.21977)
*Ji Huang,Mengfei Li,Shuai Shao*

Main category: cs.AI

TL;DR: 提出DSA两阶段微调方法用于模拟调查响应，在多个数据集上表现优于其他方法，能减少所需真实数据。


<details>
  <summary>Details</summary>
Motivation: 现有零样本方法有提示敏感性和低准确性问题，传统微调方法难以产生比训练集更准确的结果，偏离使用大语言模型模拟调查响应的目标。

Method: 引入Distribution Shift Alignment (DSA)两阶段微调方法，对齐不同背景下的输出分布和分布变化。

Result: 在五个公共调查数据集上DSA始终优于其他方法，减少所需真实数据53.48 - 69.12%。

Conclusion: DSA在调查模拟中有效且高效。

Abstract: Large language models (LLMs) offer a promising way to simulate human survey
responses, potentially reducing the cost of large-scale data collection.
However, existing zero-shot methods suffer from prompt sensitivity and low
accuracy, while conventional fine-tuning approaches mostly fit the training set
distributions and struggle to produce results more accurate than the training
set itself, which deviates from the original goal of using LLMs to simulate
survey responses. Building on this observation, we introduce Distribution Shift
Alignment (DSA), a two-stage fine-tuning method that aligns both the output
distributions and the distribution shifts across different backgrounds. By
learning how these distributions change rather than fitting training data, DSA
can provide results substantially closer to the true distribution than the
training data. Empirically, DSA consistently outperforms other methods on five
public survey datasets. We further conduct a comprehensive comparison covering
accuracy, robustness, and data savings. DSA reduces the required real data by
53.48-69.12%, demonstrating its effectiveness and efficiency in survey
simulation.

</details>


### [10] [Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective](https://arxiv.org/abs/2510.21999)
*Zhenya Huang,Jiayu Liu,Xin Lin,Zhiyuan Ma,Shangzi Xue,Tong Xiao,Qi Liu,Yee Whye Teh,Enhong Chen*

Main category: cs.AI

TL;DR: 本文从人类认知角度全面回顾数学应用题（MWP）求解相关研究，总结关键认知能力，回顾主流模型，重跑求解器并对比性能，希望启发AI推理研究。


<details>
  <summary>Details</summary>
Motivation: 领域缺乏MWP调查的系统分类和当前发展趋势讨论，旨在通过模仿人类认知智能提升AI推理能力。

Method: 从人类认知视角总结5种关键认知能力，回顾近10年两种主流MWP模型，重跑代表性求解器并补充其在5个主流基准上的性能。

Result: 首次从人类推理认知角度全面分析过去十年有影响力的MWP研究，并对现有方法进行综合比较。

Conclusion: 研究成果有望启发AI推理领域的进一步研究。

Abstract: Math word problem (MWP) serves as a fundamental research topic in artificial
intelligence (AI) dating back to 1960s. This research aims to advance the
reasoning abilities of AI by mirroring the human-like cognitive intelligence.
The mainstream technological paradigm has evolved from the early rule-based
methods, to deep learning models, and is rapidly advancing towards large
language models. However, the field still lacks a systematic taxonomy for the
MWP survey along with a discussion of current development trends. Therefore, in
this paper, we aim to comprehensively review related research in MWP solving
through the lens of human cognition, to demonstrate how recent AI models are
advancing in simulating human cognitive abilities. Specifically, we summarize 5
crucial cognitive abilities for MWP solving, including Problem Understanding,
Logical Organization, Associative Memory, Critical Thinking, and Knowledge
Learning. Focused on these abilities, we review two mainstream MWP models in
recent 10 years: neural network solvers, and LLM based solvers, and discuss the
core human-like abilities they demonstrated in their intricate problem-solving
process. Moreover, we rerun all the representative MWP solvers and supplement
their performance on 5 mainstream benchmarks for a unified comparison. To the
best of our knowledge, this survey first comprehensively analyzes the
influential MWP research of the past decade from the perspective of human
reasoning cognition and provides an integrative overall comparison across
existing approaches. We hope it can inspire further research in AI reasoning.
Our repository is released on https://github.com/Ljyustc/FoI-MWP.

</details>


### [11] [LightAgent: Mobile Agentic Foundation Models](https://arxiv.org/abs/2510.22009)
*Yangqin Jiang,Chao Huang*

Main category: cs.AI

TL;DR: 提出LightAgent解决移动GUI代理困境，结合设备与云端模型优势，实验显示成本降低且性能接近大模型


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理存在设备模型性能不足、大模型部署成本高的问题，需解决方案

Method: 提出LightAgent，通过两阶段SFT->GRPO训练增强Qwen2.5 - VL - 3B，集成长推理机制，实时评估复杂度决定任务执行位置

Result: 在在线AndroidLab基准和多样应用实验中，LightAgent性能接近或匹配大模型，显著降低云端成本

Conclusion: LightAgent有效结合设备与云端模型优势，解决移动GUI代理困境

Abstract: With the advancement of multimodal large language models (MLLMs), building
GUI agent systems has become an increasingly promising direction-especially for
mobile platforms, given their rich app ecosystems and intuitive touch
interactions. Yet mobile GUI agents face a critical dilemma: truly on-device
models (4B or smaller) lack sufficient performance, while capable models
(starting from 7B) are either too large for mobile deployment or prohibitively
costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose
LightAgent, a mobile agentic foundation model solution that leverages
device-cloud collaboration to tap the cost-efficiency of on-device models and
the high capability of cloud models, while avoiding their drawbacks.
Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO
training on synthetic GUI data for strong decision-making, integrates an
efficient long-reasoning mechanism to utilize historical interactions under
tight resources, and defaults to on-device execution-only escalating
challenging subtasks to the cloud via real-time complexity assessment.
Experiments on the online AndroidLab benchmark and diverse apps show LightAgent
matches or nears larger models, with a significant reduction in cloud costs.

</details>


### [12] [LLM-AR: LLM-powered Automated Reasoning Framework](https://arxiv.org/abs/2510.22034)
*Rick Chen,Joseph Ternasky,Aaron Ontoyin Yin,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: 本文从风险投资角度研究大语言模型在高风险决策应用中准确性不稳定的问题，提出LLM - AR管道构建预测模型，在未见过的数据上取得一定效果且具有可解释性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险决策应用中准确性不稳定，影响其在相关场景的应用，本文从风险投资角度研究此问题，通过预测初创企业成功情况来探讨。

Method: 引入受神经符号系统启发的LLM - AR管道，将大语言模型生成的启发式信息提炼为概率规则，由ProbLog自动推理引擎执行；采用迭代策略进化循环，结合关联规则挖掘逐步完善预测规则。

Result: 在未见过的数据上，LLM - AR达到59.5%的精确率和8.7%的召回率，是随机基线精确率的5.9倍，且能展示每个决策路径供人工检查。

Conclusion: 该框架具有可解释性，可通过超参数进行调整，有望扩展到其他领域。

Abstract: Large language models (LLMs) can already identify patterns and reason
effectively, yet their variable accuracy hampers adoption in high-stakes
decision-making applications. In this paper, we study this issue from a venture
capital perspective by predicting idea-stage startup success based on founder
traits. (i) To build a reliable prediction model, we introduce LLM-AR, a
pipeline inspired by neural-symbolic systems that distils LLM-generated
heuristics into probabilistic rules executed by the ProbLog automated-reasoning
engine. (ii) An iterative policy-evolution loop incorporates association-rule
mining to progressively refine the prediction rules.
  On unseen folds, LLM-AR achieves 59.5% precision and 8.7% recall, 5.9x the
random baseline precision, while exposing every decision path for human
inspection. The framework is interpretable and tunable via hyperparameters,
showing promise to extend into other domains.

</details>


### [13] [AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines](https://arxiv.org/abs/2510.23408)
*Abolfazl Younesi,Zahra Najafabadi Samani,Thomas Fahringer*

Main category: cs.AI

TL;DR: 提出AutoStreamPipe框架，用大语言模型自动化流处理管道设计等，结合HGoT提升准确性，实验表明能显著减少开发时间和错误率。


<details>
  <summary>Details</summary>
Motivation: 为实现流处理中数据管道的高效设计、生成和部署，弥合用户意图与平台特定实现的语义差距。

Method: 提出AutoStreamPipe框架，整合Hypergraph of Thoughts (HGoT)，结合弹性执行策略和高级查询分析。

Result: 在不同管道的实验评估中，与大语言模型代码生成方法相比，AutoStreamPipe使开发时间减少6.3倍，错误率降低5.19倍。

Conclusion: AutoStreamPipe能有效提升流处理管道开发效率和准确性。

Abstract: Data pipelines are essential in stream processing as they enable the
efficient collection, processing, and delivery of real-time data, supporting
rapid data analysis. In this paper, we present AutoStreamPipe, a novel
framework that employs Large Language Models (LLMs) to automate the design,
generation, and deployment of stream processing pipelines. AutoStreamPipe
bridges the semantic gap between high-level user intent and platform-specific
implementations across distributed stream processing systems for structured
multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an
extended version of GoT. AutoStreamPipe combines resilient execution
strategies, advanced query analysis, and HGoT to deliver pipelines with good
accuracy. Experimental evaluations on diverse pipelines demonstrate that
AutoStreamPipe significantly reduces development time (x6.3) and error rates
(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM
code-generation methods.

</details>


### [14] [Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability](https://arxiv.org/abs/2510.22039)
*Po-Chen Kuo,Han Hou,Will Dabney,Edgar Y. Walker*

Main category: cs.AI

TL;DR: 研究将自监督预测编码模块集成到元强化学习中能否促进贝叶斯最优表示学习，结果表明该方法能生成更易解释的表示，提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 元强化学习代理难以学习紧凑、可解释的贝叶斯最优信念状态，这种表征低效可能限制代理的适应性和泛化能力。

Method: 将自监督预测编码模块集成到元强化学习中，并通过状态机模拟进行研究。

Result: 带预测模块的元强化学习能生成更易解释的表示，在多种任务中更好地近似贝叶斯最优信念状态；在需要主动信息搜索的挑战性任务中，只有带预测模块的元强化学习能成功学习最优表示和策略；更好的表示学习能提升泛化能力。

Conclusion: 预测学习可作为引导原则，用于在部分可观测环境中有效学习代理的表示。

Abstract: Learning a compact representation of history is critical for planning and
generalization in partially observable environments. While meta-reinforcement
learning (RL) agents can attain near Bayes-optimal policies, they often fail to
learn the compact, interpretable Bayes-optimal belief states. This
representational inefficiency potentially limits the agent's adaptability and
generalization capacity. Inspired by predictive coding in neuroscience--which
suggests that the brain predicts sensory inputs as a neural implementation of
Bayesian inference--and by auxiliary predictive objectives in deep RL, we
investigate whether integrating self-supervised predictive coding modules into
meta-RL can facilitate learning of Bayes-optimal representations. Through state
machine simulation, we show that meta-RL with predictive modules consistently
generates more interpretable representations that better approximate
Bayes-optimal belief states compared to conventional meta-RL across a wide
variety of tasks, even when both achieve optimal policies. In challenging tasks
requiring active information seeking, only meta-RL with predictive modules
successfully learns optimal representations and policies, whereas conventional
meta-RL struggles with inadequate representation learning. Finally, we
demonstrate that better representation learning leads to improved
generalization. Our results strongly suggest the role of predictive learning as
a guiding principle for effective representation learning in agents navigating
partial observability.

</details>


### [15] [HW/SW Co-design of a PCM/PWM converter: a System Level Approach based in the SpecC Methodology](https://arxiv.org/abs/2510.22046)
*Daniel G. P. Petrini,Braz Izaias da Silva Junior*

Main category: cs.AI

TL;DR: 对PCM - PWM转换器应用SpecC方法进行系统级软硬件协同设计，评估映射以满足实时约束并降低成本，强调系统级协同设计价值。


<details>
  <summary>Details</summary>
Motivation: 为PCM - PWM转换器找到满足实时约束且降低成本的软硬件分区方案。

Method: 应用SpecC方法对转换器建模和探索以导出软硬件分区，使用系统级估计和快速功能仿真评估映射。

Result: 在设计复杂度适中情况下，能满足实时约束，降低全硬件方案成本，避免纯软件实现的高成本。

Conclusion: 系统级协同设计对早期架构洞察、快速验证和成本/性能权衡有重要价值。

Abstract: We present a case study applying the SpecC methodology within a system-level
hardware/software co-design flow to a PCM-to-PWM converter, the core of a
Class-D audio amplifier. The converter was modeled and explored with SpecC
methodology to derive an HW/SW partition. Using system-level estimates and fast
functional simulation, we evaluated mappings that meet real-time constraints
while reducing estimated cost of an all-hardware solution and avoiding the
expense of a purely software implementation on a high-end processor. Despite
the design's moderate complexity, the results underline the value of
system-level co-design for early architectural insight, rapid validation, and
actionable cost/performance trade-offs. [Original work from 2005; formatting
revised in 2025, with no changes to the results.]

</details>


### [16] [Towards Error-Centric Intelligence II: Energy-Structured Causal Models](https://arxiv.org/abs/2510.22050)
*Marcus Thomas*

Main category: cs.AI

TL;DR: 论文指出当代机器学习缺乏因果语义，提出以构建和完善解释为导向的概念转变，引入计算解释和ESCMs，分析了经验风险最小化问题，证明ESCMs可恢复标准SCM语义并提供因果推理形式语言。


<details>
  <summary>Details</summary>
Motivation: 当代机器学习虽追求预测准确性，但模型因果不透明，缺乏因果语义，难以进行干预。

Method: 引入计算解释，以能量结构化因果模型（ESCMs）实例化解释，在ESCM背景下给出结构因果原则的具体实例，分析经验风险最小化问题。

Result: 在温和条件下，ESCMs可恢复标准SCM语义。

Conclusion: 基于相关原则和智能定义，为追求理解而非仅预测的系统提供因果推理的形式语言。

Abstract: Contemporary machine learning optimizes for predictive accuracy, yet systems
that achieve state of the art performance remain causally opaque: their
internal representations provide no principled handle for intervention. We can
retrain such models, but we cannot surgically edit specific mechanisms while
holding others fixed, because learned latent variables lack causal semantics.
We argue for a conceptual reorientation: intelligence is the ability to build
and refine explanations, falsifiable claims about manipulable structure that
specify what changes and what remains invariant under intervention.
Explanations subsume prediction but demand more: causal commitments that can be
independently tested and corrected at the level of mechanisms. We introduce
computational explanations, mappings from observations to intervention ready
causal accounts. We instantiate these explanations with Energy Structured
Causal Models (ESCMs), in which mechanisms are expressed as constraints (energy
functions or vector fields) rather than explicit input output maps, and
interventions act by local surgery on those constraints. This shift makes
internal structure manipulable at the level where explanations live: which
relations must hold, which can change, and what follows when they do. We
provide concrete instantiations of the structural-causal principles LAP and ICM
in the ESCM context, and also argue that empirical risk minimization
systematically produces fractured, entangled representations, a failure we
analyze as gauge ambiguity in encoder energy pairs. Finally, we show that under
mild conditions, ESCMs recover standard SCM semantics. Building on Part I's
principles (LAP, ICM, CAP) and its definition of intelligence as
explanation-building under criticism, this paper offers a formal language for
causal reasoning in systems that aspire to understand, not merely to predict.

</details>


### [17] [Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms](https://arxiv.org/abs/2510.22052)
*Abhijit Chatterjee,Niraj K. Jha,Jonathan D. Cohen,Thomas L. Griffiths,Hongjing Lu,Diana Marculescu,Ashiqur Rasul,Keshab K. Parhi*

Main category: cs.AI

TL;DR: AI市场规模将大幅增长，但当前大语言模型有数据、能耗和幻觉问题，需发展轻量级特定领域多模态模型，本文提出未来AI系统愿景。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在数据需求大、能耗高、易幻觉等问题，无法用于关键领域，需推动AI进一步进化。

Method: 无明确提及具体方法，主要是提出未来AI发展方向。

Result: 提出未来AI应从大模型向轻量级、节能、特定领域且能推理思考的智能体发展，硬件需重新设计以提高能效。

Conclusion: 定义了下一代AI发展方向，需发展轻量级特定领域多模态模型，重新设计硬件以实现更高能效。

Abstract: The field of artificial intelligence (AI) has taken a tight hold on broad
aspects of society, industry, business, and governance in ways that dictate the
prosperity and might of the world's economies. The AI market size is projected
to grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI
is dominated by large language models that exhibit linguistic and visual
intelligence. However, training these models requires a massive amount of data
scraped from the web as well as large amounts of energy (50--60 GWh to train
GPT-4). Despite these costs, these models often hallucinate, a characteristic
that prevents them from being deployed in critical application domains. In
contrast, the human brain consumes only 20~W of power. What is needed is the
next level of AI evolution in which lightweight domain-specific multimodal
models with higher levels of intelligence can reason, plan, and make decisions
in dynamic environments with real-time data and prior knowledge, while learning
continuously and evolving in ways that enhance future decision-making
capability. This will define the next wave of AI, progressing from today's
large models, trained with vast amounts of data, to nimble energy-efficient
domain-specific agents that can reason and think in a world full of
uncertainty. To support such agents, hardware will need to be reimagined to
allow energy efficiencies greater than 1000x over the state of the art. Such a
vision of future AI systems is developed in this work.

</details>


### [18] [Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies](https://arxiv.org/abs/2510.22095)
*Yankai Chen,Xinni Zhang,Yifei Zhang,Yangning Li,Henry Peng Zou,Chunyu Miao,Weizhi Zhang,Xue Liu,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文探讨脑机接口（BCI）现状，提出从BCI到脑-智能体协作（BAC）的范式扩展，并强调关注伦理等问题以保障系统安全有效。


<details>
  <summary>Details</summary>
Motivation: BCI存在低信息传输率和大量用户特定校准等局限，集成大语言模型虽有进展但部署智能体AI面临技术和伦理问题，且缺乏对该新兴方向的全面讨论。

Method: 提出从BCI到BAC的范式扩展，将智能体重新定义为主动协作伙伴。

Result: 无明确提及具体结果。

Conclusion: 该领域应进行从BCI到BAC的范式扩展，需关注伦理数据处理、模型可靠性和强大的人机协作框架，以确保系统安全、可信和有效。

Abstract: Brain-Computer Interfaces (BCIs) offer a direct communication pathway between
the human brain and external devices, holding significant promise for
individuals with severe neurological impairments. However, their widespread
adoption is hindered by critical limitations, such as low information transfer
rates and extensive user-specific calibration. To overcome these challenges,
recent research has explored the integration of Large Language Models (LLMs),
extending the focus from simple command decoding to understanding complex
cognitive states. Despite these advancements, deploying agentic AI faces
technical hurdles and ethical concerns. Due to the lack of comprehensive
discussion on this emerging direction, this position paper argues that the
field is poised for a paradigm extension from BCI to Brain-Agent Collaboration
(BAC). We emphasize reframing agents as active and collaborative partners for
intelligent assistance rather than passive brain signal data processors,
demanding a focus on ethical data handling, model reliability, and a robust
human-agent collaboration framework to ensure these systems are safe,
trustworthy, and effective.

</details>


### [19] [Controllable Mathematical Reasoning via Self-Optimizing Thought Vectors](https://arxiv.org/abs/2510.22132)
*Xuying LI*

Main category: cs.AI

TL;DR: 提出利用熵最小化的自优化思想向量进行可控数学推理的新方法，在GSM8K上用Gemma - 2 - 9B取得高准确率和可控性分数，验证框架有效性。


<details>
  <summary>Details</summary>
Motivation: 实现可控的数学推理。

Method: 引入可学习的思想向量动态调节大语言模型内部推理过程，使用基于熵的奖励引导推理。

Result: 在GSM8K上用Gemma - 2 - 9B达到90.1%的准确率，可控性分数0.42，分析发现不同思想向量簇和低熵分布。

Conclusion: 基于熵的奖励能有效引导聚焦推理模式，验证了可控AI推理框架的有效性。

Abstract: We present a novel approach for controllable mathematical reasoning that
leverages self-optimizing thought vectors with entropy minimization. Our method
introduces learnable thought vectors that dynamically modulate the internal
reasoning process of large language models. Using Gemma-2-9B on GSM8K, we
achieve 90.1% accuracy with a controllability score of 0.42, demonstrating that
entropy-based rewards effectively guide focused reasoning patterns without
requiring external reward annotations. Our analysis reveals distinct thought
vector clusters and consistent low-entropy distributions across control
conditions, validating our framework for controllable AI reasoning.

</details>


### [20] [Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests](https://arxiv.org/abs/2510.22170)
*Alexandra Yost,Shreyans Jain,Shivam Raval,Grant Corser,Allen Roush,Nina Xu,Jacqueline Hammack,Ravid Shwartz-Ziv,Amirali Abdullah*

Main category: cs.AI

TL;DR: 提出用于AI心理测量的框架，在执法助手案例研究中构建丰富数据集并将公开数据和代码。


<details>
  <summary>Details</summary>
Motivation: 以往AI心理测量工作复用人类特质清单或临时角色，限制行为真实性和领域相关性。

Method: 提出框架，用情景判断测试探查特定领域能力，整合工业组织和人格心理学设计复杂角色，采用结构化生成和人口统计先验及回忆录启发式叙事。

Result: 在执法助手案例中构建含8500个角色、4000个情景判断测试和300000个响应的丰富数据集。

Conclusion: 可通过该框架开展AI心理测量，构建有效数据集，且将公开数据和代码促进研究。

Abstract: AI psychometrics evaluates AI systems in roles that traditionally require
emotional judgment and ethical consideration. Prior work often reuses human
trait inventories (Big Five, \hexaco) or ad hoc personas, limiting behavioral
realism and domain relevance. We propose a framework that (1) uses situational
judgment tests (SJTs) from realistic scenarios to probe domain-specific
competencies; (2) integrates industrial-organizational and personality
psychology to design sophisticated personas which include behavioral and
psychological descriptors, life history, and social and emotional functions;
and (3) employs structured generation with population demographic priors and
memoir inspired narratives, encoded with Pydantic schemas. In a law enforcement
assistant case study, we construct a rich dataset of personas drawn across 8
persona archetypes and SJTs across 11 attributes, and analyze behaviors across
subpopulation and scenario slices. The dataset spans 8,500 personas, 4,000
SJTs, and 300,000 responses. We will release the dataset and all code to the
public.

</details>


### [21] [Dopamine-driven synaptic credit assignment in neural networks](https://arxiv.org/abs/2510.22178)
*Saranraj Nambusubramaniyan,Shervin Safavi,Raja Guru,Andreas Knoblauch*

Main category: cs.AI

TL;DR: 本文提出无导数优化器Dopamine训练神经网络，解决突触信用分配问题，在多项任务中表现良好，计算和内存消耗低且更具神经生物学合理性。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的方法解决突触信用分配问题效率不高，存在计算和内存方面的缺点。

Method: 采用NeuroAI方法，借鉴神经强化学习，开发无导数优化器Dopamine，利用权重扰动学习，通过最小化后悔值调整学习率。

Result: Dopamine训练的模型收敛加速，优于标准权重扰动方法，与基于梯度的算法性能相当，且计算和内存消耗显著降低。

Conclusion: Dopamine优化器能找到鲁棒解，性能与现有机器学习优化器相当，且更具神经生物学合理性。

Abstract: Solving the synaptic Credit Assignment Problem(CAP) is central to learning in
both biological and artificial neural systems. Finding an optimal solution for
synaptic CAP means setting the synaptic weights that assign credit to each
neuron for influencing the final output and behavior of neural networks or
animals. Gradient-based methods solve this problem in artificial neural
networks using back-propagation, however, not in the most efficient way. For
instance, back-propagation requires a chain of top-down gradient computations.
This leads to an expensive optimization process in terms of computing power and
memory linked with well-known weight transport and update locking problems. To
address these shortcomings, we take a NeuroAI approach and draw inspiration
from neural Reinforcement Learning to develop a derivative-free optimizer for
training neural networks, Dopamine. Dopamine is developed for Weight
Perturbation (WP) learning that exploits stochastic updating of weights towards
optima. It achieves this by minimizing the regret, a form of Reward Prediction
Error (RPE) between the expected outcome from the perturbed model and the
actual outcome from the unperturbed model. We use this RPE to adjust the
learning rate in the network (i.e., creating an adaptive learning rate
strategy, similar to the role of dopamine in the brain). We tested the Dopamine
optimizer for training multi-layered perceptrons for XOR tasks, and recurrent
neural networks for chaotic time series forecasting. Dopamine-trained models
demonstrate accelerated convergence and outperform standard WP, and give
comparable performance to gradient-based algorithms, while consuming
significantly less computation and memory. Overall, the Dopamine optimizer not
only finds robust solutions and comparable performance to the state-of-the-art
Machine Learning optimizers but is also neurobiologically more plausible.

</details>


### [22] [OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling](https://arxiv.org/abs/2510.22192)
*Haoyang Liu,Jie Wang,Yuyang Cai,Xiongwei Han,Yufei Kuang,Jianye Hao*

Main category: cs.AI

TL;DR: 本文提出OptiTree方法，通过自适应问题分解提升复杂运筹学问题建模能力，实验显示其建模准确率显著高于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型进行运筹学优化建模的方法，因标准固定步骤分解难以应对复杂数学结构问题，性能不佳。

Method: 引入OptiTree，构建建模树，根据问题分类和复杂度组织运筹学问题，通过递归搜索树识别子问题并整合全局建模思路。

Result: OptiTree相比现有技术显著提高建模准确率，在具有挑战性的基准测试中提升超10%。

Conclusion: OptiTree能有效提升复杂运筹学问题的建模能力，代码已开源。

Abstract: Optimization modeling is one of the most crucial but technical parts of
operations research (OR). To automate the modeling process, existing works have
leveraged large language models (LLMs), prompting them to break down tasks into
steps for generating variables, constraints, and objectives. However, due to
the highly complex mathematical structures inherent in OR problems, standard
fixed-step decomposition often fails to achieve high performance. To address
this challenge, we introduce OptiTree, a novel tree search approach designed to
enhance modeling capabilities for complex problems through adaptive problem
decomposition into simpler subproblems. Specifically, we develop a modeling
tree that organizes a wide range of OR problems based on their hierarchical
problem taxonomy and complexity, with each node representing a problem category
and containing relevant high-level modeling thoughts. Given a problem to model,
we recurrently search the tree to identify a series of simpler subproblems and
synthesize the global modeling thoughts by adaptively integrating the
hierarchical thoughts. Experiments show that OptiTree significantly improves
the modeling accuracy compared to the state-of-the-art, achieving over 10\%
improvements on the challenging benchmarks. The code is released at
https://github.com/MIRALab-USTC/OptiTree/tree/main.

</details>


### [23] [PACR: Progressively Ascending Confidence Reward for LLM Reasoning](https://arxiv.org/abs/2510.22255)
*Eunseop Yoon,Hee Suk Yoon,Jaehyun Jang,SooHwan Eom,Qi Dai,Chong Luo,Mark A. Hasegawa-Johnson,Chang D. Yoo*

Main category: cs.AI

TL;DR: 提出渐进式置信奖励（PACR）改进强化学习可验证奖励（RLVR），加速探索并提升性能。


<details>
  <summary>Details</summary>
Motivation: RLVR的稀疏、基于结果的奖励对中间步骤缺乏指导，导致探索缓慢。

Method: 提出PACR，一种基于模型对正确答案的动态置信度计算的密集、模型内在奖励，编码了推理轨迹上真实答案概率应上升的归纳偏置。

Result: PACR加速探索，用更少轨迹达到奖励饱和，在多个基准测试中取得改进。

Conclusion: 密集、模型内在的塑形信号可使RLVR训练更有效和可靠。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly
improved LLM reasoning, but its sparse, outcome-based reward provides no
guidance for intermediate steps, slowing exploration. We propose Progressively
Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed
directly from the model's evolving belief in the correct answer. PACR encodes
the inductive bias that, along a well-formed reasoning trajectory, the
probability of the ground-truth answer should have a generally ascending trend.
We provide empirical and theoretical analysis validating that such an inductive
bias constrains the exploration search space to regions richer in logically
sound reasoning. We demonstrate that PACR accelerates exploration, reaches
reward saturation with fewer trajectories, and yields improvements on multiple
benchmarks. Our results suggest that dense, model-intrinsic shaping signals can
make RLVR training more effective and reliable.

</details>


### [24] [VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription](https://arxiv.org/abs/2510.22295)
*Quoc Anh Nguyen,Bernard Cheng,Kelvin Soh*

Main category: cs.AI

TL;DR: 本文针对越南音乐自动歌词转录缺乏数据集问题，构建VietLyrics数据集，微调Whisper模型取得好结果并公开数据和模型。


<details>
  <summary>Details</summary>
Motivation: 越南音乐自动歌词转录因语音复杂和缺乏数据集而研究少，需构建数据集解决问题。

Method: 构建VietLyrics数据集，在该数据集上微调Whisper模型。

Result: 微调后的模型比现有多语言自动歌词转录系统表现更好。

Conclusion: 公开数据集和模型能推动越南音乐计算研究，该方法对低资源语言和音乐的自动歌词转录有潜力。

Abstract: Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique
challenges due to its tonal complexity and dialectal variations, but remains
largely unexplored due to the lack of a dedicated dataset. Therefore, we
curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising
647 hours of songs with line-level aligned lyrics and metadata to address these
issues. Our evaluation of current ASRbased approaches reveal significant
limitations, including frequent transcription errors and hallucinations in
non-vocal segments. To improve performance, we fine-tuned Whisper models on the
VietLyrics dataset, achieving superior results compared to existing
multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics
and our models, aiming to advance Vietnamese music computing research while
demonstrating the potential of this approach for ALT in low-resource language
and music.

</details>


### [25] [Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows](https://arxiv.org/abs/2510.22329)
*Mustafa Mert Özyılmaz*

Main category: cs.AI

TL;DR: 本文提出多级图粗化和细化框架解决带时间窗的容量受限车辆路径问题（CVRPTW），实验表明可减少计算时间并保证或提升解的质量，还探讨量子启发优化技术潜力。


<details>
  <summary>Details</summary>
Motivation: CVRPTW是物流中基本的NP难优化问题，精确求解器解决大规模实例计算难度大。

Method: 引入多级图粗化和细化框架，用时空距离度量将客户聚合成元节点，用经典启发式方法求解简化问题，再扩展回原空间并进行可行性修正，还探讨量子启发优化技术。

Result: 在Solomon基准实例的初步实验显示，该方法减少计算时间，保证或提升解的质量，尤其在容量和时间窗约束方面。

Conclusion: 提出的方法有效，量子启发优化技术有加速大规模车辆路径任务的潜力。

Abstract: The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a
fundamental NP-hard optimization problem in logistics. Solving large-scale
instances remains computationally challenging for exact solvers. This work
introduces a multilevel graph coarsening and refinement framework that
aggregates customers into meta-nodes using a spatio-temporal distance metric.
The reduced problem is solved with classical heuristics and subsequently
expanded back into the original space with feasibility corrections. Preliminary
experiments on Solomon benchmark instances show that the proposed method
reduces computation time while preserving or improving solution quality,
particularly with respect to capacity and time window constraints. The paper
also explores the integration of quantum-inspired optimization techniques,
highlighting their potential to further accelerate large-scale vehicle routing
tasks.

</details>


### [26] [LIFT: Interpretable truck driving risk prediction with literature-informed fine-tuned LLMs](https://arxiv.org/abs/2510.22333)
*Xiao Hu,Yuansheng Lian,Ke Zhang,Yunxuan Li,Yuelong Su,Meng Li*

Main category: cs.AI

TL;DR: 提出用于卡车驾驶风险预测的LIFT LLM可解释预测框架，在真实数据集上表现优于基准模型，能识别潜在风险场景，展示了知识库和微调过程对可解释性的贡献。


<details>
  <summary>Details</summary>
Motivation: 进行卡车驾驶风险预测并提高模型的可解释性。

Method: 构建包含推理核心、文献处理管道和结果评估器的预测框架，在真实数据集上微调LIFT LLM。

Result: LIFT LLM在召回率和F1分数上优于基准模型，变量重要性排名与基准模型一致，能识别潜在风险场景。

Conclusion: 展示了文献知识库和微调过程对LIFT LLM可解释性的贡献，讨论了其在数据驱动知识发现方面的潜力。

Abstract: This study proposes an interpretable prediction framework with
literature-informed fine-tuned (LIFT) LLMs for truck driving risk prediction.
The framework integrates an LLM-driven Inference Core that predicts and
explains truck driving risk, a Literature Processing Pipeline that filters and
summarizes domain-specific literature into a literature knowledge base, and a
Result Evaluator that evaluates the prediction performance as well as the
interpretability of the LIFT LLM. After fine-tuning on a real-world truck
driving risk dataset, the LIFT LLM achieved accurate risk prediction,
outperforming benchmark models by 26.7% in recall and 10.1% in F1-score.
Furthermore, guided by the literature knowledge base automatically constructed
from 299 domain papers, the LIFT LLM produced variable importance ranking
consistent with that derived from the benchmark model, while demonstrating
robustness in interpretation results to various data sampling conditions. The
LIFT LLM also identified potential risky scenarios by detecting key combination
of variables in truck driving risk, which were verified by PERMANOVA tests.
Finally, we demonstrated the contribution of the literature knowledge base and
the fine-tuning process in the interpretability of the LIFT LLM, and discussed
the potential of the LIFT LLM in data-driven knowledge discovery.

</details>


### [27] [DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry](https://arxiv.org/abs/2510.22340)
*Changti Wu,Shijie Lian,Zihao Liu,Lei Zhang,Laurence Tianruo Yang,Kai Chen*

Main category: cs.AI

TL;DR: 现有多模态数学推理基准有局限，本文引入动态基准DynaSolidGeo评估视觉语言模型空间推理能力，实验揭示模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有多模态数学推理基准多关注2D平面几何，依赖静态数据集，仅评估最终答案，忽略推理过程，需新基准评估真实空间推理能力。

Method: 通过半自动注释流程构建DynaSolidGeo基准，包含503个专家策划种子问题，可动态生成多样实例，除答案准确性外，引入基于专家注释推理链的过程评估。

Result: 对代表性开源和闭源视觉语言模型实验显示，存在较大性能差距，动态设置下性能严重下降，在需要高级空间智能任务上表现不佳。

Conclusion: DynaSolidGeo可作为评估视觉语言模型空间推理能力的有效基准，代码和数据集已公开。

Abstract: Solid geometry problem solving demands spatial mathematical reasoning that
integrates spatial intelligence and symbolic reasoning. However, most existing
multimodal mathematical reasoning benchmarks focus primarily on 2D plane
geometry, rely on static datasets prone to data contamination and memorization,
and evaluate models solely by final answers, overlooking the reasoning process.
To address these limitations, we introduce DynaSolidGeo, the first dynamic
benchmark for evaluating genuine spatial reasoning in Vision-Language Models
(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo
contains 503 expert-curated seed questions that can, in principle, dynamically
generate an unbounded number of diverse multimodal text-visual instances.
Beyond answer accuracy, we incorporate process evaluation based on
expert-annotated reasoning chains to measure logical validity and causal
coherence. Experiments across representative open-source and closed-source VLMs
reveal large performance gaps, severe degradation in dynamic settings, and poor
performance on tasks requiring high-level spatial intelligence, such as mental
rotation and visualization. The code and dataset are available at
\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.

</details>


### [28] [Reasoning Models Reason Well, Until They Don't](https://arxiv.org/abs/2510.22371)
*Revanth Rameshkumar,Jimson Huang,Yunxin Sun,Fei Xia,Abulhair Saparov*

Main category: cs.AI

TL;DR: 研究大推理模型（LRM）在推理任务表现，发现现有基准复杂度有限，开发新数据集评估，指出LRM在足够复杂度时性能骤降且难泛化，虽现实多数例子在成功区间但长尾有失败风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示transformers和大语言模型在推理问题复杂度增加时表现不佳，需重新审视大推理模型性能。

Method: 开发新数据集DeepRD，用其评估LRM在图连通性和自然语言证明规划的表现，并将结果与现实世界数据集复杂度分布关联。

Result: LRM在足够复杂度时性能骤降且不能泛化，现实世界多数例子在LRM成功区间，但长尾有失败风险。

Conclusion: LRM有近期实用性，但需新方法以实现超越训练分布复杂度的泛化。

Abstract: Large language models (LLMs) have shown significant progress in reasoning
tasks. However, recent studies show that transformers and LLMs fail
catastrophically once reasoning problems exceed modest complexity. We revisit
these findings through the lens of large reasoning models (LRMs) -- LLMs
fine-tuned with incentives for step-by-step argumentation and
self-verification. LRM performance on graph and reasoning benchmarks such as
NLGraph seem extraordinary, with some even claiming they are capable of
generalized reasoning and innovation in reasoning-intensive fields such as
mathematics, physics, medicine, and law. However, by more carefully scaling the
complexity of reasoning problems, we show existing benchmarks actually have
limited complexity. We develop a new dataset, the Deep Reasoning Dataset
(DeepRD), along with a generative process for producing unlimited examples of
scalable complexity. We use this dataset to evaluate model performance on graph
connectivity and natural language proof planning. We find that the performance
of LRMs drop abruptly at sufficient complexity and do not generalize. We also
relate our LRM results to the distributions of the complexities of large,
real-world knowledge graphs, interaction graphs, and proof datasets. We find
the majority of real-world examples fall inside the LRMs' success regime, yet
the long tails expose substantial failure potential. Our analysis highlights
the near-term utility of LRMs while underscoring the need for new methods that
generalize beyond the complexity of examples in the training distribution.

</details>


### [29] [Modeling Hierarchical Thinking in Large Reasoning Models](https://arxiv.org/abs/2510.22437)
*G M Shahariar,Ali Nazari,Erfan Shayegani,Nael Abu-Ghazaleh*

Main category: cs.AI

TL;DR: 本文采用无记忆有限状态机（FSM）近似大推理模型（LRM）的分层推理动态，通过标注思维链步骤识别离散推理状态，以分析和可视化模型推理过程，揭示不同模型推理模式和潜在不足。


<details>
  <summary>Details</summary>
Motivation: 理解LRM新兴推理能力是难题且有重要应用，如改进训练和理解鲁棒性，因此需一种方法来分析和解释其推理过程。

Method: 采用无记忆有限状态机公式化，识别离散推理状态，用这些状态标注模型思维链的每一步，将推理轨迹表示为状态图中的转移序列。

Result: FSM 分析揭示了不同模型的独特推理模式和潜在缺点。

Conclusion: 基于FSM的分析为评估和改进大语言模型推理提供了新视角。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
when they generate step-by-step solutions, known as chain-of-thought (CoT)
reasoning. When trained to using chain-of-thought reasoning examples, the
resulting models (called Large Reasoning Models, or LRMs) appear to learn
hierarchical thinking strategies similar to those used by humans. However,
understanding LRMs emerging reasoning capabilities remains a difficult open
problem, with many potential important applications including improving
training and understanding robustness. In this paper, we adopt a memoryless
Finite State Machine formulation to approximate LRM's emerging hierarchical
reasoning dynamics as a structured, interpretable abstraction. We identify a
small set of discrete reasoning states including - initialization, deduction,
augmentation-strategy, uncertainty-estimation, backtracking, and
final-conclusion that capture the high-level states present in the model's
reasoning process. By annotating each step of a model's CoT with these states,
we can represent the reasoning trajectory as a transition sequence through the
state graph. This FSM formulation provides a systematic way to analyze,
interpret and visualize how different models approach problems. We describe the
FSM model, provide examples of CoT annotations under this scheme, and discuss
how it can shed light on differences between available models in their approach
to reasoning. Our results demonstrate that this FSM-based analysis reveals
distinct reasoning patterns and potential shortcomings, offering a new lens to
evaluate and improve LLM reasoning.

</details>


### [30] [Learning "Partner-Aware" Collaborators in Multi-Party Collaboration](https://arxiv.org/abs/2510.22462)
*Abhijnan Nath,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: 本文聚焦大语言模型在协作场景下的评估，指出标准训练的模型倾向忽略干预，提出新算法ICR，实验表明其能促进共同基础收敛和探索多样解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在协作场景应用增多，需评估其在多轮多方任务中的协作能力。

Method: 基于AI对齐和安全可中断性文献进行理论分析，用两玩家Modified - Action MDP研究标准AI代理的次优行为，提出Interruptible Collaborative Roleplayer (ICR)算法。

Result: 实验显示ICR平均更能促进共同基础成功收敛，在协作任务中探索更多样的解决方案。

Conclusion: ICR算法能有效训练出在协作任务中优化共同基础对齐的大语言模型驱动的协作代理。

Abstract: Large Language Models (LLMs) are increasingly bring deployed in agentic
settings where they act as collaborators with humans. Therefore, it is
increasingly important to be able to evaluate their abilities to collaborate
effectively in multi-turn, multi-party tasks. In this paper, we build on the AI
alignment and safe interruptability literature to offer novel theoretical
insights on collaborative behavior between LLM-driven collaborator agents and
an intervention agent. Our goal is to learn an ideal partner-aware collaborator
that increases the group's common-ground (CG)-alignment on task-relevant
propositions-by intelligently collecting information provided in interventions
by a partner agent.We show how LLM agents trained using standard RLHF and
related approaches are naturally inclined to ignore possibly well-meaning
interventions, which makes increasing group common ground non-trivial in this
setting. We employ a two-player Modified-Action MDP to examine this suboptimal
behavior of standard AI agents, and propose Interruptible Collaborative
Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal
collaborators. Experiments on multiple collaborative task environments show
that ICR, on average, is more capable of promoting successful CG convergence
and exploring more diverse solutions in such tasks.

</details>


### [31] [OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models](https://arxiv.org/abs/2510.22535)
*Hao Zheng,Zirui Pang,Ling li,Zhijie Deng,Yuhan Pu,Zhaowei Zhu,Xiaobo Xia,Jiaheng Wei*

Main category: cs.AI

TL;DR: 提出新基准OFFSIDE评估多模态大语言模型的错误信息遗忘能力，评估发现现有方法存在诸多漏洞。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型发展引发数据隐私担忧，现有遗忘基准存在图像多样性不足、评估场景不充分等问题，需新基准推动模型遗忘能力发展。

Method: 引入基于足球转会谣言的OFFSIDE基准，含15.68K条记录和四个测试集，支持多种高级设置。

Result: 评估发现单模态方法处理多模态谣言失败、遗忘效果受灾难性遗忘影响、处理视觉谣言困难、被遗忘谣言易恢复、方法易受提示攻击。

Conclusion: 当前多模态遗忘方法存在显著漏洞，需更稳健的解决方案。

Abstract: Advances in Multimodal Large Language Models (MLLMs) intensify concerns about
data privacy, making Machine Unlearning (MU), the selective removal of learned
information, a critical necessity. However, existing MU benchmarks for MLLMs
are limited by a lack of image diversity, potential inaccuracies, and
insufficient evaluation scenarios, which fail to capture the complexity of
real-world applications. To facilitate the development of MLLMs unlearning and
alleviate the aforementioned limitations, we introduce OFFSIDE, a novel
benchmark for evaluating misinformation unlearning in MLLMs based on football
transfer rumors. This manually curated dataset contains 15.68K records for 80
players, providing a comprehensive framework with four test sets to assess
forgetting efficacy, generalization, utility, and robustness. OFFSIDE supports
advanced settings like selective unlearning and corrective relearning, and
crucially, unimodal unlearning (forgetting only text data). Our extensive
evaluation of multiple baselines reveals key findings: (1) Unimodal methods
(erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning
efficacy is largely driven by catastrophic forgetting; (3) All methods struggle
with "visual rumors" (rumors appear in the image); (4) The unlearned rumors can
be easily recovered and (5) All methods are vulnerable to prompt attacks. These
results expose significant vulnerabilities in current approaches, highlighting
the need for more robust multimodal unlearning solutions. The code is available
at
\href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.

</details>


### [32] [ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs](https://arxiv.org/abs/2510.22590)
*Yassir Lairgi,Ludovic Moncla,Khalid Benabdeslem,Rémy Cazabet,Pierre Cléau*

Main category: cs.AI

TL;DR: 本文提出 ATOM 方法构建和更新时间知识图谱，相比基线方法有更好表现和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统静态知识图谱构建忽略数据动态性，零样本或小样本方法不稳定且事实覆盖不全。

Method: 将输入文档拆分为原子事实，构建原子时间知识图谱，采用双时间建模，并行合并图谱。

Result: 与基线方法相比，ATOM 提取完整性提高约 18%，稳定性提高约 17%，延迟降低超 90%。

Conclusion: ATOM 方法在动态时间知识图谱构建上有很强的可扩展性。

Abstract: In today's rapidly expanding data landscape, knowledge extraction from
unstructured text is vital for real-time analytics, temporal inference, and
dynamic memory frameworks. However, traditional static knowledge graph (KG)
construction often overlooks the dynamic and time-sensitive nature of
real-world data, limiting adaptability to continuous changes. Moreover, recent
zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance
on prebuilt ontologies often suffer from instability across multiple runs, as
well as incomplete coverage of key facts. To address these challenges, we
introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that
builds and continuously updates Temporal Knowledge Graphs (TKGs) from
unstructured texts. ATOM splits input documents into minimal, self-contained
"atomic" facts, improving extraction exhaustivity and stability. Then, it
constructs atomic TKGs from these facts while employing a dual-time modeling
that distinguishes when information is observed from when it is valid. The
resulting atomic TKGs are subsequently merged in parallel. Empirical
evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%
better stability, and over 90% latency reduction compared to baseline methods,
demonstrating a strong scalability potential for dynamic TKG construction.

</details>


### [33] [A Framework for Quantifying How Pre-Training and Context Benefit In-Context Learning](https://arxiv.org/abs/2510.22594)
*Bingqing Song,Jiaxiang Li,Rong Wang,Songtao Lu,Mingyi Hong*

Main category: cs.AI

TL;DR: 提出分析ICL性能的新框架，先以单层transformer为例展示结果，再推广到一般情况并推导关系，最后实验验证。


<details>
  <summary>Details</summary>
Motivation: 目前理论上不清楚ICL能力如何产生，以及预训练过程和上下文构建等关键因素的具体作用。

Method: 提出新框架，先构建单层transformer简单示例，再将结果推广到一般情况，最后进行实验验证。

Result: 当预训练数据分布与查询任务分布不同时，适当构建的上下文可使输出分布向查询任务分布偏移，推导出ICL性能、上下文长度与预训练和查询任务分布的KL散度之间的精确关系。

Conclusion: 所提出的框架和理论结果得到实验验证，有助于理解ICL能力产生机制和关键因素作用。

Abstract: Pre-trained large language models have demonstrated a strong ability to learn
from context, known as in-context learning (ICL). Despite a surge of recent
applications that leverage such capabilities, it is by no means clear, at least
theoretically, how the ICL capabilities arise, and in particular, what is the
precise role played by key factors such as pre-training procedure as well as
context construction. In this work, we propose a new framework to analyze the
ICL performance, for a class of realistic settings, which includes network
architectures, data encoding, data generation, and prompt construction process.
As a first step, we construct a simple example with a one-layer transformer,
and show an interesting result, namely when the pre-train data distribution is
different from the query task distribution, a properly constructed context can
shift the output distribution towards the query task distribution, in a
quantifiable manner, leading to accurate prediction on the query topic. We then
extend the findings in the previous step to a more general case, and derive the
precise relationship between ICL performance, context length and the KL
divergence between pre-train and query task distribution. Finally, we provide
experiments to validate our theoretical results.

</details>


### [34] [CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation](https://arxiv.org/abs/2510.22609)
*Md. Mehedi Hasan,Rafid Mostafiz,Md. Abir Hossain,Bikash Kumar Paul*

Main category: cs.AI

TL;DR: 提出安全约束混合管道CLIN - LLM用于症状到疾病分类和治疗建议，表现优异且减少不安全抗生素建议，为资源有限医疗环境提供决策支持框架。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的系统缺乏医学依据且无法量化不确定性，在高诊断风险的异质患者环境中，准确的症状到疾病分类和临床治疗建议具有挑战性。

Method: 提出CLIN - LLM，在Symptom2Disease数据集上微调BioBERT，结合Focal Loss和Monte Carlo Dropout进行疾病分类；用Biomedical Sentence - BERT从MedDialog语料库检索对话，与患者上下文一起输入微调的FLAN - T5模型生成治疗建议，再用RxNorm后处理。

Result: CLIN - LLM准确率和F1分数达98%，优于ClinicalBERT 7.1%；top - 5检索精度78%，临床医生评级有效性4.2分（满分5分）；不安全抗生素建议比GPT - 5减少67%。

Conclusion: CLIN - LLM具有鲁棒性、可解释性和临床安全性，为资源有限医疗环境提供可部署的人在环决策支持框架，未来将整合更多数据、进行多语言扩展和临床试验验证。

Abstract: Accurate symptom-to-disease classification and clinically grounded treatment
recommendations remain challenging, particularly in heterogeneous patient
settings with high diagnostic risk. Existing large language model (LLM)-based
systems often lack medical grounding and fail to quantify uncertainty,
resulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid
pipeline that integrates multimodal patient encoding, uncertainty-calibrated
disease classification, and retrieval-augmented treatment generation. The
framework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease
dataset and incorporates Focal Loss with Monte Carlo Dropout to enable
confidence-aware predictions from free-text symptoms and structured vitals.
Low-certainty cases (18%) are automatically flagged for expert review, ensuring
human oversight. For treatment generation, CLIN-LLM employs Biomedical
Sentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample
MedDialog corpus. The retrieved evidence and patient context are fed into a
fine-tuned FLAN-T5 model for personalized treatment generation, followed by
post-processing with RxNorm for antibiotic stewardship and drug-drug
interaction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score,
outperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval
precision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic
suggestions are reduced by 67% compared to GPT-5. These results demonstrate
CLIN-LLM's robustness, interpretability, and clinical safety alignment. The
proposed system provides a deployable, human-in-the-loop decision support
framework for resource-limited healthcare environments. Future work includes
integrating imaging and lab data, multilingual extensions, and clinical trial
validation.

</details>


### [35] [SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming](https://arxiv.org/abs/2510.22626)
*Adhyayan Veer Singh,Aaron Shen,Brian Law,Ahmed Ismail,Jonas Rohweder,Sean O'Brien,Kevin Zhu*

Main category: cs.AI

TL;DR: 提出SwiftSolve，一种用于竞赛编程的复杂度感知多智能体系统，评估显示其有较好效果，能减少低效问题。


<details>
  <summary>Details</summary>
Motivation: 仅保证程序正确性不足，LLM生成的程序常违反竞赛时间或内存预算，需有效解决方法。

Method: 将竞赛编程视为软件环境，让专业智能体扮演不同角色，如规划、编码、分析等，智能体通过JSON通信，控制器进行迭代限制。

Result: 在26个问题上评估，首次尝试pass@1为61.54%，Solved@<=3为80.77%，聚合运行级成功率73.08%，相比Claude Opus 4提高了运行级成功率。

Conclusion: 分析和复杂度引导的重新规划可在保证准确性的同时减少低效问题。

Abstract: Correctness alone is insufficient: LLM-generated programs frequently satisfy
unit tests while violating contest time or memory budgets. We present
SwiftSolve, a complexity-aware multi-agent system for competitive programming
that couples algorithmic planning with empirical profiling and
complexity-guided repair. We frame competitive programming as a software
environment where specialized agents act as programmers, each assuming roles
such as planning, coding, profiling, and complexity analysis. A Planner
proposes an algorithmic sketch; a deterministic Static Pruner filters high-risk
plans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on
a fixed input-size schedule to record wall time and peak memory; and a
Complexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a
complexity class and dispatch targeted patches to either the Planner or Coder.
Agents communicate via typed, versioned JSON; a controller enforces iteration
caps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10
Codeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains
pass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with
marginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate
run-level success is 73.08% at 12.40 s mean. Failures are predominantly
resource-bound, indicating inefficiency rather than logic errors. Against
Claude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at
approximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness
(pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence
of TLE or MLE, and complexity fit accuracy on BigO), demonstrating that
profiling and complexity-guided replanning reduce inefficiency while preserving
accuracy.

</details>


### [36] [Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration](https://arxiv.org/abs/2510.22679)
*Yuval Kainan,Shaked Zychlinski*

Main category: cs.AI

TL;DR: 提出用首生成令牌对数概率分布检测大语言模型样板回复的方法，实现早期终止或模型重定向以节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成样板回复消耗大量计算资源，增加成本和延迟。

Method: 利用首生成令牌的对数概率分布作为信号，使用轻量级k - NN分类器进行分类。

Result: 不同回复类型的首令牌对数概率向量形成可分离的聚类，能高精度预测回复类型。

Conclusion: 该方法可优化大语言模型推理，实现更高效和可持续的部署。

Abstract: Large Language Models (LLMs) often expend significant computational resources
generating boilerplate responses, such as refusals, simple acknowledgements and
casual greetings, which adds unnecessary cost and latency. To address this
inefficiency, we propose a simple yet highly effective method for detecting
such responses after only a single generation step. We demonstrate that the
log-probability distribution of the first generated token serves as a powerful
signal for classifying the nature of the entire subsequent response. Our
experiments, conducted across a diverse range of small, large, and
reasoning-specialized models, show that the first-token log-probability vectors
form distinctly separable clusters for different response types. Using a
lightweight k-NN classifier, we achieve high accuracy in predicting whether a
response will be a substantive answer or a form of boilerplate response,
including user-specified refusals. The primary implication is a practical,
computationally trivial technique, optimizing LLM inference by enabling early
termination or redirection to a smaller model, thereby yielding significant
savings in computational cost. This work presents a direct path toward more
efficient and sustainable LLM deployment.

</details>


### [37] [On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset](https://arxiv.org/abs/2510.22898)
*Vishvesh Bhat,Omkar Ghugarkar,Julian McAuley*

Main category: cs.AI

TL;DR: 文章评估LLMs在多工具调用基准上的表现，引入新基准MAVEN，揭示泛化差距，提出CoreThink Agentic Reasoner框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 解决开发可靠智能推理系统中跨智能工具调用环境的泛化挑战，了解LLMs跨领域转移推理策略和协调工具的能力。

Method: 对多个工具调用基准评估LLMs，引入新基准MAVEN，提出CoreThink Agentic Reasoner框架。

Result: 多数当前模型在MAVEN上准确率低于50%，CoreThink Agentic Reasoner框架无需额外训练，泛化性好，性能提升530%，计算成本约为十分之一。

Conclusion: 当前LLMs在跨工具使用设置上存在显著泛化差距，CoreThink Agentic Reasoner框架有效解决该问题。

Abstract: Generalization across Agentic tool-calling environments remains a key
unsolved challenge in developing reliable agentic reasoning systems. While
large language models (LLMs) demonstrate strong performance on isolated
benchmarks, their ability to transfer reasoning strategies and co-ordinate
tools across diverse domains is poorly understood. In this work, we conduct a
large-scale evaluation of state-of-the-art LLMs on multiple tool-calling
benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &
Physics Adversarial Verification & Evaluation Network), a new out of
distribution (OOD) benchmark designed to stress-test multi-step reasoning
through explicit verification and adversarial task composition. Our results
show that most current models achieve below 50% accuracy on MAVEN, revealing a
significant generalization gap across tool-use settings.
  To address this, we present the CoreThink Agentic Reasoner, a framework that
augments LLMs with a lightweight symbolic reasoning layer for structured
decomposition and adaptive tool orchestration. Without additional training, it
generalizes across all benchmarks, achieving state-of-the-art performance with
530% improvements over existing baselines at roughly one-tenth the
computational cost.

</details>


### [38] [Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring](https://arxiv.org/abs/2510.22702)
*Mithul Chander,Sai Pragnya Ranga,Prathamesh Mayekar*

Main category: cs.AI

TL;DR: 提出Atlas Urban Index (AUI) 指标衡量城市发展，利用VLM克服传统指标局限，在班加罗尔实验中表现优于NDBI。


<details>
  <summary>Details</summary>
Motivation: 现有如NDBI等方法难以准确衡量城市发展，受大气噪声、季节变化和云层覆盖等因素影响，阻碍大规模监测人类发展和城市化。

Method: 利用Vision - Language Models (VLMs)，收集各区域Sentinel - 2图像时间序列，处理得到云覆盖最少的代表图像，采用提供参考图像和过去图像两种策略确保评分一致性。

Result: 在班加罗尔的定性实验中，AUI表现优于标准指标NDBI。

Conclusion: AUI能克服传统城市化指标挑战，产生更可靠稳定的发展评分。

Abstract: We introduce the {\em Atlas Urban Index} (AUI), a metric for measuring urban
development computed using Sentinel-2 \citep{spoto2012sentinel2} satellite
imagery. Existing approaches, such as the {\em Normalized Difference Built-up
Index} (NDBI), often struggle to accurately capture urban development due to
factors like atmospheric noise, seasonal variation, and cloud cover. These
limitations hinder large-scale monitoring of human development and
urbanization. To address these challenges, we propose an approach that
leverages {\em Vision-Language Models }(VLMs) to provide a development score
for regions. Specifically, we collect a time series of Sentinel-2 images for
each region. Then, we further process the images within fixed time windows to
get an image with minimal cloud cover, which serves as the representative image
for that time window. To ensure consistent scoring, we adopt two strategies:
(i) providing the VLM with a curated set of reference images representing
different levels of urbanization, and (ii) supplying the most recent past image
to both anchor temporal consistency and mitigate cloud-related noise in the
current image. Together, these components enable AUI to overcome the challenges
of traditional urbanization indices and produce more reliable and stable
development scores. Our qualitative experiments on Bangalore suggest that AUI
outperforms standard indices such as NDBI.

</details>


### [39] [RaCoT: Plug-and-Play Contrastive Example Generation Mechanism for Enhanced LLM Reasoning Reliability](https://arxiv.org/abs/2510.22710)
*Kaitong Cai,Jusheng Zhang,Yijia Fan,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: 提出RaCoT框架解决RAG长尾查询问题，在多基准测试中表现优，兼顾准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在知识稀疏和语义模糊的长尾查询中存在的核心瓶颈，即检索噪声干扰推理且需高成本后处理。

Method: 提出RaCoT框架，将对比思维移至预检索阶段，自动生成对比问题并提取Δ - Prompt以聚焦关键差异，抑制语义干扰。

Result: 在六个权威基准测试中优于强基线模型，鲁棒性好，低延迟和低令牌开销，处于准确性 - 效率帕累托前沿，消融实验验证各组件必要性。

Conclusion: RaCoT将RAG范式从“事后上下文清理”转变为“先验塑造判别推理”，为实时、资源受限部署的可靠AI系统提供高效稳健途径。

Abstract: Retrieval-Augmented Generation (RAG) faces a core bottleneck with
knowledge-sparse and semantically ambiguous long-tail queries, where retrieval
noise distorts reasoning and necessitates costly post-processing. To tackle
this, we propose RaCoT (Retrieval-aware Contrastive-of-Thought), a novel
framework that shifts contrastive thinking to the pre-retrieval stage. By
automatically generating a semantically adjacent yet differently answered
contrastive question and extracting a $\Delta$-Prompt to capture their key
differences, RaCoT guides the model to proactively focus on the ``critical
details that determine answer divergence." This approach allows it to suppress
semantic interference within a single retrieval pass, overcoming the
theoretical bottleneck of single-vector queries that struggle to simultaneously
encode signals for what to attend to and what to ignore. On six authoritative
benchmarks, including PopQA and TriviaQA-unfiltered, RaCoT outperforms strong
baselines like RankRAG and Self-RAG by 0.9-2.4 percentage points. It exhibits
superior robustness, with a performance drop of only 8.6\% in adversarial
tests, far surpassing the over 15\% degradation in other methods. Furthermore,
its low latency (3.12s) and token overhead (11.54) place it on the
accuracy-efficiency Pareto frontier, while ablation studies validate the
necessity of each component. Ultimately, RaCoT reframes the RAG paradigm from
``post-hoc context cleaning" to ``a priori shaping of discriminative
reasoning", offering an efficient and robust path toward reliable AI systems
for real-time, resource-constrained deployments.

</details>


### [40] [Critical Insights into Leading Conversational AI Models](https://arxiv.org/abs/2510.22729)
*Urja Kohli,Aditi Singh,Arun Sharma*

Main category: cs.AI

TL;DR: 本文对比五大语言模型，分析性能、伦理和可用性，发现各有优势，建议按需使用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展迅速，不同模型因构建理念不同存在差异，需对比其性能、道德行为和可用性。

Method: 分析性能与准确性、伦理与偏差缓解、可用性与集成三个重要因素，对比谷歌Gemini、High - Flyer的DeepSeek、Anthropic的Claude、OpenAI的GPT模型和Meta的LLaMA。

Result: Claude道德推理能力好；Gemini多模态能力强且有强大伦理框架；DeepSeek基于事实推理出色；LLaMA适合开放应用；ChatGPT性能均衡且注重使用。

Conclusion: 这些模型在性能、易用性和伦理处理上有差异，用户应充分利用各模型优势。

Abstract: Big Language Models (LLMs) are changing the way businesses use software, the
way people live their lives and the way industries work. Companies like Google,
High-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial
to look at how each model is different in terms of performance, moral behaviour
and usability, as these differences are based on the different ideas that built
them. This study compares five top LLMs: Google's Gemini, High-Flyer's
DeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs
this by analysing three important factors: Performance and Accuracy, Ethics and
Bias Mitigation and Usability and Integration. It was found that Claude has
good moral reasoning, Gemini is better at multimodal capabilities and has
strong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA
is good for open applications and ChatGPT delivers balanced performance with a
focus on usage. It was concluded that these models are different in terms of
how well they work, how easy they are to use and how they treat people
ethically, making it a point that each model should be utilised by the user in
a way that makes the most of its strengths.

</details>


### [41] [Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards](https://arxiv.org/abs/2510.23083)
*Jan Niklas Groeneveld,Xi Qin,Alexander Schaefer,Yaad Oren*

Main category: cs.AI

TL;DR: 研究小型语言模型能否成为有效奖励模型，构建数据集训练模型，证明小型模型可行且提升搜索能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成高质量代码有挑战，奖励模型是推理模型进化必要步骤，想探究小型语言模型如Phi - 4家族能否成为考虑过程和结果奖励的可用奖励模型。

Method: 构建带正确性标签的代码样本数据集，训练价值头模型估计中间输出的成功概率。

Result: 小型大语言模型能作为有效奖励模型或代码评估评判器，可在多个候选中识别正确解决方案，使用评判器使搜索最准确代码的能力提升超20%。

Conclusion: 小型语言模型可作为有效的奖励模型或代码评估评判器，能提升代码搜索能力。

Abstract: Generating high-quality code remains a challenge for Large Language Models
(LLMs). For the evolution of reasoning models on this task, reward models are a
necessary intermediate step. These models judge outcomes or intermediate steps.
Decoder-only transformer models can be turned into reward models by introducing
a regression layer and supervised fine-tuning. While it is known that
reflection capabilities generally increase with the size of a model, we want to
investigate whether state-of-the-art small language models like the Phi-4
family can be turned into usable reward models blending the consideration of
process rewards and outcome rewards.
  Targeting this goal, we construct a dataset of code samples with correctness
labels derived from the APPS coding challenge benchmark. We then train a
value-head model to estimate the success probability of intermediate outputs.
Our evaluation shows that small LLMs are capable of serving as effective reward
models or code evaluation critics, successfully identifying correct solutions
among multiple candidates. Using this critic, we achieve over a 20% improvement
in the search capability of the most accurate code out of multiple generations.

</details>


### [42] [Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models](https://arxiv.org/abs/2510.22751)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: 本文提出事实验证框架，可实时捕捉并纠正大语言模型的幻觉问题，测试显示能减少67%幻觉，专家满意度达89%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在生成虚假信息的幻觉问题，阻碍其在对准确性要求高的现实场景中应用。

Method: 开发事实验证框架，通过多知识源交叉检查大语言模型输出，结合结构化数据库、实时网络搜索和学术文献验证事实，检测到不一致时自动纠正。

Result: 在各领域测试中可减少67%的幻觉，不牺牲响应质量，医疗、金融和科研领域专家对纠正后输出的满意度达89%。

Conclusion: 该工作为提高大语言模型在不容出错场景中的可信度提供了实用解决方案。

Abstract: While Large Language Models have transformed how we interact with AI systems,
they suffer from a critical flaw: they confidently generate false information
that sounds entirely plausible. This hallucination problem has become a major
barrier to deploying these models in real-world applications where accuracy
matters. We developed a fact verification framework that catches and corrects
these errors in real-time by cross checking LLM outputs against multiple
knowledge sources. Our system combines structured databases, live web searches,
and academic literature to verify factual claims as they're generated. When we
detect inconsistencies, we automatically correct them while preserving the
natural flow of the response. Testing across various domains showed we could
reduce hallucinations by 67% without sacrificing response quality. Domain
experts in healthcare, finance, and scientific research rated our corrected
outputs 89% satisfactory a significant improvement over unverified LLM
responses. This work offers a practical solution for making LLMs more
trustworthy in applications where getting facts wrong isn't an option.

</details>


### [43] [GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation](https://arxiv.org/abs/2510.22942)
*Zhuoxuan Li,Jieyuan Pei,Tangwei Ye,Zhongyuan Lai,Zihan Liu,Fengyuan Xu,Qi Zhang,Liang Hu*

Main category: cs.AI

TL;DR: 提出GTR - Mamba框架用于下一兴趣点推荐，在三个真实数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络和序列模型的兴趣点推荐模型难以同时捕捉空间选择的层次结构和用户特定时间上下文的动态变化，需新方法解决。

Method: 提出GTR - Mamba框架，在双曲几何中建模静态偏好层次，在欧几里得切空间的Mamba层进行动态序列更新，通过跨流形通道融合时空信息引导状态空间模型。

Result: 在三个真实数据集上的实验表明，GTR - Mamba在兴趣点推荐上始终优于现有基线模型。

Conclusion: GTR - Mamba框架能有效解决现有模型的局限，在兴趣点推荐任务中有更好表现。

Abstract: Next Point-of-Interest (POI) recommendation is a critical task in modern
Location-Based Social Networks (LBSNs), aiming to model the complex
decision-making process of human mobility to provide personalized
recommendations for a user's next check-in location. Existing POI
recommendation models, predominantly based on Graph Neural Networks and
sequential models, have been extensively studied. However, these models face a
fundamental limitation: they struggle to simultaneously capture the inherent
hierarchical structure of spatial choices and the dynamics and irregular shifts
of user-specific temporal contexts. To overcome this limitation, we propose
GTR-Mamba, a novel framework for cross-manifold conditioning and routing.
GTR-Mamba leverages the distinct advantages of different mathematical spaces
for different tasks: it models the static, tree-like preference hierarchies in
hyperbolic geometry, while routing the dynamic sequence updates to a novel
Mamba layer in the computationally stable and efficient Euclidean tangent
space. This process is coordinated by a cross-manifold channel that fuses
spatio-temporal information to explicitly steer the State Space Model (SSM),
enabling flexible adaptation to contextual changes. Extensive experiments on
three real-world datasets demonstrate that GTR-Mamba consistently outperforms
state-of-the-art baseline models in next POI recommendation.

</details>


### [44] [Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval](https://arxiv.org/abs/2510.22765)
*Binxiao Xu,Junyu Feng,Ruichuan An,Yulin Luo,Shilin Yan,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

TL;DR: 提出Jarvis框架用于个性化AI助手，通过个人KV - Cache检索存储用户信息，在多数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有将通用VLMs适应为个性化助手的方法难以生成准确答案，需要更有效的方案。

Method: 引入Jarvis框架，将用户信息总结为元数据生成文本令牌，从用户图像中提取不同图像块生成视觉令牌，存储在KV - Caches中，回答问题时检索相关KV - Caches；构建细粒度基准。

Result: Jarvis能提供更准确响应，在多个数据集的视觉问答和纯文本任务中取得SOTA结果。

Conclusion: Jarvis为个性化AI助手提供了一条实用路径，代码和数据集将发布。

Abstract: The rapid development of Vision-language models (VLMs) enables open-ended
perception and reasoning. Recent works have started to investigate how to adapt
general-purpose VLMs into personalized assistants. Even commercial models such
as ChatGPT now support model personalization by incorporating user-specific
information. However, existing methods either learn a set of concept tokens or
train a VLM to utilize user-specific information. However, both pipelines
struggle to generate accurate answers as personalized assistants. We introduce
Jarvis, an innovative framework for a personalized AI assistant through
personal KV-Cache retrieval, which stores user-specific information in the
KV-Caches of both textual and visual tokens. The textual tokens are created by
summarizing user information into metadata, while the visual tokens are
produced by extracting distinct image patches from the user's images. When
answering a question, Jarvis first retrieves related KV-Caches from personal
storage and uses them to ensure accuracy in responses. We also introduce a
fine-grained benchmark built with the same distinct image patch mining
pipeline, emphasizing accurate question answering based on fine-grained
user-specific information. Jarvis is capable of providing more accurate
responses, particularly when they depend on specific local details. Jarvis
achieves state-of-the-art results in both visual question answering and
text-only tasks across multiple datasets, indicating a practical path toward
personalized AI assistants. The code and dataset will be released.

</details>


### [45] [How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations](https://arxiv.org/abs/2510.22780)
*Zora Zhiruo Wang,Yijia Shao,Omar Shaikh,Daniel Fried,Graham Neubig,Diyi Yang*

Main category: cs.AI

TL;DR: 本文对比人类和AI智能体在多项工作技能上的表现，发现智能体虽方法单一、质量欠佳，但速度快成本低，适合处理易编程任务。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体开发缺乏对人类工作执行方式的清晰理解，本文旨在研究智能体如何完成人类工作。

Method: 提出可扩展工具包，从人类或智能体的计算机使用活动中归纳可解释的结构化工作流程，以此对比人类和智能体执行相同任务的表现。

Result: 智能体与人类工作流程有一定契合度，但方法单一；工作质量较差，存在数据造假和工具滥用问题；完成任务速度快88.3%，成本低90.4 - 96.2%。

Conclusion: 可将易编程任务委派给智能体，实现高效协作。

Abstract: AI agents are continually optimized for tasks related to human work, such as
software engineering and professional writing, signaling a pressing trend with
significant impacts on the human workforce. However, these agent developments
have often not been grounded in a clear understanding of how humans execute
work, to reveal what expertise agents possess and the roles they can play in
diverse workflows. In this work, we study how agents do human work by
presenting the first direct comparison of human and agent workers across
multiple essential work-related skills: data analysis, engineering,
computation, writing, and design. To better understand and compare
heterogeneous computer-use activities of workers, we introduce a scalable
toolkit to induce interpretable, structured workflows from either human or
agent computer-use activities. Using such induced workflows, we compare how
humans and agents perform the same tasks and find that: (1) While agents
exhibit promise in their alignment to human workflows, they take an
overwhelmingly programmatic approach across all work domains, even for
open-ended, visually dependent tasks like design, creating a contrast with the
UI-centric methods typically used by humans. (2) Agents produce work of
inferior quality, yet often mask their deficiencies via data fabrication and
misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster
and cost 90.4-96.2% less than humans, highlighting the potential for enabling
efficient collaboration by delegating easily programmable tasks to agents.

</details>


### [46] [JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538)
*Qiushi Sun,Jingyang Gong,Yang Liu,Qiaosheng Chen,Lei Li,Kai Chen,Qipeng Guo,Ben Kao,Fei Yuan*

Main category: cs.AI

TL;DR: 本文聚焦神经代码智能的视觉维度，因高质量多模态代码数据稀缺，提出数据合成工具包，构建大规模语料库，训练JanusCoder系列模型，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 神经代码智能视觉维度发展受高质量多模态代码数据稀缺阻碍，需解决数据合成和质量评估难题。

Method: 引入数据合成工具包，利用数据模态间协同效应生成大规模高质量语料库JanusCode - 800K，以此训练JanusCoder和JanusCoderV模型。

Result: JanusCoder系列模型在文本和视觉编码任务中性能优越，7B - 14B规模模型接近或超越商业模型。

Conclusion: 模型建立了视觉 - 编程接口，为协调编程逻辑和视觉表达提供关键见解，代码和检查点开源。

Abstract: The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

</details>


### [47] [Agentic Meta-Orchestrator for Multi-task Copilots](https://arxiv.org/abs/2510.22781)
*Xiaofeng Zhu,Yunshen Zhou*

Main category: cs.AI

TL;DR: 提出用于Copilot服务的Agentic Meta - orchestrator (AMO)，能处理多任务和可扩展代理，通过两个用例展示其有效性。


<details>
  <summary>Details</summary>
Motivation: Copilot服务需强大编排器将用户提示任务分配给合适代理，以支持动态扩展新代理。

Method: 提出AMO，利用元学习进行规划，使用训练好的决策树模型决定最佳推理策略。

Result: 通过M365 E - Commerce Copilot和代码合规性Copilot两个生产用例展示了AMO的有效性。

Conclusion: 所提出的AMO能有效处理Copilot服务中的多任务和可扩展代理。

Abstract: Microsoft Copilot suites serve as the universal entry point for various
agents skilled in handling important tasks, ranging from assisting a customer
with product purchases to detecting vulnerabilities in corporate programming
code. Each agent can be powered by language models, software engineering
operations, such as database retrieval, and internal \& external knowledge. The
repertoire of a copilot can expand dynamically with new agents. This requires a
robust orchestrator that can distribute tasks from user prompts to the right
agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for
handling multiple tasks and scalable agents in copilot services, which can
provide both natural language and action responses. We will also demonstrate
the planning that leverages meta-learning, i.e., a trained decision tree model
for deciding the best inference strategy among various agents/models. We
showcase the effectiveness of our AMO through two production use cases:
Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365
E-Commerce Copilot advertises Microsoft products to external customers to
promote sales success. The M365 E-Commerce Copilot provides up-to-date product
information and connects to multiple agents, such as relational databases and
human customer support. The code compliance copilot scans the internal DevOps
code to detect known and new compliance issues in pull requests (PR).

</details>


### [48] [Will Humanity Be Rendered Obsolete by AI?](https://arxiv.org/abs/2510.22814)
*Mohamed El Louadi,Emna Ben Romdhane*

Main category: cs.AI

TL;DR: 文章分析AI对人类的生存风险，从当前AI到超智能，探讨AGI和超级智能及其伦理与生存影响，指出人类可能因机器认知优势灭绝。


<details>
  <summary>Details</summary>
Motivation: 分析人工智能对人类存在的生存风险。

Method: 借鉴Irving J. Good和Nick Bostrom的理论工作及近期出版物进行研究。

Result: 发现机器认知能力指数级增长，存在远超人类的智能，人类可能因这种不可控且冷漠的认知优势而灭绝。

Conclusion: 人工智能带来的远超人类的智能可能会导致人类灭绝，需重视其伦理和生存影响。

Abstract: This article analyzes the existential risks artificial intelligence (AI)
poses to humanity, tracing the trajectory from current AI to ultraintelligence.
Drawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent
publications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and
superintelligence. Considering machines' exponentially growing cognitive power
and hypothetical IQs, it addresses the ethical and existential implications of
an intelligence vastly exceeding humanity's, fundamentally alien. Human
extinction may result not from malice, but from uncontrollable, indifferent
cognitive superiority.

</details>


### [49] [HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning](https://arxiv.org/abs/2510.22832)
*Long H Dang,David Rawlinson*

Main category: cs.AI

TL;DR: 本文提出HRM - Agent，展示HRM能在动态不确定迷宫环境中学习导航，探索循环推理过程动态并发现重用计算证据。


<details>
  <summary>Details</summary>
Motivation: 现有HRM仅适用于监督、静态、全可观测问题，无法处理动态、不确定、部分可观测问题及正确动作未定义情况，而现实问题多具这些特征。

Method: 提出仅使用强化学习训练的HRM - Agent变体。

Result: HRM能在动态和不确定的迷宫环境中学习导航到目标。

Conclusion: HRM的循环推理过程能成功重用早期环境时间步的计算。

Abstract: The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities
given its small size, but has only been applied to supervised, static,
fully-observable problems. One of HRM's strengths is its ability to adapt its
computational effort to the difficulty of the problem. However, in its current
form it cannot integrate and reuse computation from previous time-steps if the
problem is dynamic, uncertain or partially observable, or be applied where the
correct action is undefined, characteristics of many real-world problems.
  This paper presents HRM-Agent, a variant of HRM trained using only
reinforcement learning. We show that HRM can learn to navigate to goals in
dynamic and uncertain maze environments. Recent work suggests that HRM's
reasoning abilities stem from its recurrent inference process. We explore the
dynamics of the recurrent inference process and find evidence that it is
successfully reusing computation from earlier environment time-steps.

</details>


### [50] [Toward Agents That Reason About Their Computation](https://arxiv.org/abs/2510.22833)
*Adrian Orenstein,Jessica Chen,Gwyneth Anne Delos Santos,Bayley Sapara,Michael Bowling*

Main category: cs.AI

TL;DR: 本文探讨强化学习智能体在学习时能否减少计算量，通过实验表明能推理计算成本的智能体表现更好且计算量平均减少三倍。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体提升性能时计算效率通常不提高，而人类熟练度提升时认知努力会减少，期望智能体也能减少计算量以提高能源效率或释放计算资源。

Method: 向智能体展示计算成本并让其控制计算时机，在街机学习环境中进行实验。

Result: 在相同训练计算预算下，能推理计算成本的智能体在75%的游戏中表现更好，且平均计算量减少三倍。

Conclusion: 智能体推理计算成本可提升性能并减少计算量，还分析了不同游戏中提升效率的情况。

Abstract: While reinforcement learning agents can achieve superhuman performance in
many complex tasks, they typically do not become more computationally efficient
as they improve. In contrast, humans gradually require less cognitive effort as
they become more proficient at a task. If agents could reason about their
compute as they learn, could they similarly reduce their computation footprint?
If they could, we could have more energy efficient agents or free up compute
cycles for other processes like planning. In this paper, we experiment with
showing agents the cost of their computation and giving them the ability to
control when they use compute. We conduct our experiments on the Arcade
Learning Environment, and our results demonstrate that with the same training
compute budget, agents that reason about their compute perform better on 75% of
games. Furthermore, these agents use three times less compute on average. We
analyze individual games and show where agents gain these efficiencies.

</details>


### [51] [Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes](https://arxiv.org/abs/2510.22836)
*Guanyu Yao,Qiucheng Wu,Yang Zhang,Zhaowen Wang,Handong Zhao,Shiyu Chang*

Main category: cs.AI

TL;DR: 本文分析多模态大语言模型的模态差距，从训练配方角度出发，探索缩小差距的策略。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉和文本模态推理能力上存在不平衡，即模态差距，影响视觉推理任务表现。

Method: 从训练配方角度分析模态差距，从数据和损失设计两个互补角度探索缩小差距的策略。

Result: 发现现有训练配方会扩大差距，研究了缩小差距的策略。

Conclusion: 研究结果为开发减少模态差距、促进更平衡多模态推理的训练配方提供了见解。

Abstract: Multimodal large language models (MLLMs) have demonstrated strong
capabilities on vision-and-language tasks. However, recent findings reveal an
imbalance in their reasoning capabilities across visual and textual modalities.
Specifically, current MLLMs often over-rely on textual cues while
under-attending to visual content, resulting in suboptimal performance on tasks
that require genuine visual reasoning. We refer to this phenomenon as the
\textit{modality gap}, defined as the performance disparity between
text-centric and vision-centric inputs. In this paper, we analyze the modality
gap through the lens of training recipes. We first show that existing training
recipes tend to amplify this gap. Then, we systematically explore strategies to
bridge it from two complementary perspectives: data and loss design. Our
findings provide insights into developing training recipes that mitigate the
modality gap and promote more balanced multimodal reasoning. Our code is
publicly available at https://github.com/UCSB-NLP-Chang/Bridging-Modality-Gap.

</details>


### [52] [Lyapunov Function-guided Reinforcement Learning for Flight Control](https://arxiv.org/abs/2510.22840)
*Yifei Li,Erik-Jan van Kampen*

Main category: cs.AI

TL;DR: 开发并改进级联在线学习飞行控制系统，研究其收敛性能并通过仿真给出对比结果


<details>
  <summary>Details</summary>
Motivation: 研究级联在线学习飞行控制系统的收敛性能

Method: 通过推导以Lyapunov函数候选增量表征的指标，考虑增量模型引入的离散化误差和状态预测误差

Result: 通过飞行控制仿真得到对比结果

Conclusion: 未提及明确结论

Abstract: A cascaded online learning flight control system has been developed and
enhanced with respect to action smoothness. In this paper, we investigate the
convergence performance of the control system, characterized by the increment
of a Lyapunov function candidate. The derivation of this metric accounts for
discretization errors and state prediction errors introduced by the incremental
model. Comparative results are presented through flight control simulations.

</details>


### [53] [Exploring Structures of Inferential Mechanisms through Simplistic Digital Circuits](https://arxiv.org/abs/2510.22883)
*Giovanni Sileno,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 本文尝试构建统一框架，基于逻辑门电子电路结合符号AI技术分析推理机制，找出四种依赖形式、八种推理模式并揭示内在功能依赖。


<details>
  <summary>Details</summary>
Motivation: 认知研究和人工智能缺乏统一框架，本文尝试填补这一空白。

Method: 从物质角度出发，基于逻辑门电子电路结合符号AI建模技术分析推理机制，进行组合探索。

Result: 确定四种依赖形式、八种推理模式，揭示内在功能依赖。

Conclusion: 虽基于符号手段和数字系统基础设施，但观察结果可能指向更通用结构。

Abstract: Cognitive studies and artificial intelligence have developed distinct models
for various inferential mechanisms (categorization, induction, abduction,
causal inference, contrast, merge, ...). Yet, both natural and artificial views
on cognition lack apparently a unifying framework. This paper formulates a
speculative answer attempting to respond to this gap. To postulate on
higher-level activation processes from a material perspective, we consider
inferential mechanisms informed by symbolic AI modelling techniques, through
the simplistic lenses of electronic circuits based on logic gates. We observe
that a logic gate view entails a different treatment of implication and
negation compared to standard logic and logic programming. Then, by
combinatorial exploration, we identify four main forms of dependencies that can
be realized by these inferential circuits. Looking at how these forms are
generally used in the context of logic programs, we identify eight common
inferential patterns, exposing traditionally distinct inferential mechanisms in
an unifying framework. Finally, following a probabilistic interpretation of
logic programs, we unveil inner functional dependencies. The paper concludes
elaborating in what sense, even if our arguments are mostly informed by
symbolic means and digital systems infrastructures, our observations may
pinpoint to more generally applicable structures.

</details>


### [54] [Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner](https://arxiv.org/abs/2510.22969)
*Kechen Meng,Sinuo Zhang,Rongpeng Li,Xiangming Meng,Chan Wang,Ming Lei,Zhifeng Zhao*

Main category: cs.AI

TL;DR: 提出MA - CDMP用于分散通信资源管理，基于MBRL范式，引入MF机制，实验表明其优于现有MARL基线。


<details>
  <summary>Details</summary>
Motivation: 现有集中式MARL有可扩展性和隐私风险，DTDE范式有非平稳性和有限合作问题，影响系统性能。

Method: 基于MBRL范式构建MA - CDMP，用扩散模型捕捉环境动态和规划轨迹，逆动力学模型指导动作生成，引入MF机制减轻非平稳性和增强合作。

Result: 理论上建立了基于MF的扩散生成的分布近似误差上界，实验显示MA - CDMP在平均奖励和QoS指标上优于现有MARL基线。

Conclusion: MA - CDMP具有可扩展性和实用性，适用于现实无线网络优化。

Abstract: In wireless communication systems, efficient and adaptive resource allocation
plays a crucial role in enhancing overall Quality of Service (QoS). While
centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a
central coordinator for policy training and resource scheduling, they suffer
from scalability issues and privacy risks. In contrast, the Distributed
Training with Decentralized Execution (DTDE) paradigm enables distributed
learning and decision-making, but it struggles with non-stationarity and
limited inter-agent cooperation, which can severely degrade system performance.
To overcome these challenges, we propose the Multi-Agent Conditional Diffusion
Model Planner (MA-CDMP) for decentralized communication resource management.
Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP
employs Diffusion Models (DMs) to capture environment dynamics and plan future
trajectories, while an inverse dynamics model guides action generation, thereby
alleviating the sample inefficiency and slow convergence of conventional DTDE
methods. Moreover, to approximate large-scale agent interactions, a Mean-Field
(MF) mechanism is introduced as an assistance to the classifier in DMs. This
design mitigates inter-agent non-stationarity and enhances cooperation with
minimal communication overhead in distributed settings. We further
theoretically establish an upper bound on the distributional approximation
error introduced by the MF-based diffusion generation, guaranteeing convergence
stability and reliable modeling of multi-agent stochastic dynamics. Extensive
experiments demonstrate that MA-CDMP consistently outperforms existing MARL
baselines in terms of average reward and QoS metrics, showcasing its
scalability and practicality for real-world wireless network optimization.

</details>


### [55] [Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction](https://arxiv.org/abs/2510.22981)
*Jin Hu,Jiakai Wang,Linna Jing,Haolin Li,Haodong Liu,Haotong Qin,Aishan Liu,Ke Xu,Xianglong Liu*

Main category: cs.AI

TL;DR: 现有语义约束对抗样本生成方法因未充分考虑语义不确定性问题而攻击能力不足，本文提出多维度指令不确定性降低框架InSUR生成更优对抗样本，实验证明其优势并首次实现无参考3D对抗样本生成。


<details>
  <summary>Details</summary>
Motivation: 当前生成语义约束对抗样本（SemanticAE）的方法未充分研究人类指令中语义不确定性的关键潜在因素，攻击能力欠佳。

Method: 提出多维度指令不确定性降低（InSUR）框架，包括在采样方法维度提出残差驱动攻击方向稳定化，在任务建模维度提出上下文编码攻击场景约束，在生成器评估维度提出语义抽象攻击评估增强。

Result: 实验表明InSUR的转移攻击性能优越，首次实现了语义约束3D对抗样本的无参考生成。

Conclusion: 所提出的InSUR框架能有效降低指令不确定性，生成更令人满意的SemanticAE，具有可迁移、自适应和有效等特点。

Abstract: Recently, semantically constrained adversarial examples (SemanticAE), which
are directly generated from natural language instructions, have become a
promising avenue for future research due to their flexible attacking forms. To
generate SemanticAEs, current methods fall short of satisfactory attacking
ability as the key underlying factors of semantic uncertainty in human
instructions, such as referring diversity, descriptive incompleteness, and
boundary ambiguity, have not been fully investigated. To tackle the issues,
this paper develops a multi-dimensional instruction uncertainty reduction
(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,
adaptive, and effective. Specifically, in the dimension of the sampling method,
we propose the residual-driven attacking direction stabilization to alleviate
the unstable adversarial optimization caused by the diversity of language
references. By coarsely predicting the language-guided sampling process, the
optimization process will be stabilized by the designed ResAdv-DDIM sampler,
therefore releasing the transferable and robust adversarial capability of
multi-step diffusion models. In task modeling, we propose the context-encoded
attacking scenario constraint to supplement the missing knowledge from
incomplete human instructions. Guidance masking and renderer integration are
proposed to regulate the constraints of 2D/3D SemanticAE, activating stronger
scenario-adapted attacks. Moreover, in the dimension of generator evaluation,
we propose the semantic-abstracted attacking evaluation enhancement by
clarifying the evaluation boundary, facilitating the development of more
effective SemanticAE generators. Extensive experiments demonstrate the
superiority of the transfer attack performance of InSUR. Moreover, we realize
the reference-free generation of semantically constrained 3D adversarial
examples for the first time.

</details>


### [56] [ProfileXAI: User-Adaptive Explainable AI](https://arxiv.org/abs/2510.22998)
*Gilber A. Corrales,Carlos Andrés Ferro Sánchez,Reinel Tabares-Soto,Jesús Alfonso López Sotelo,Gonzalo A. Ruz,Johan Sebastian Piña Durán*

Main category: cs.AI

TL;DR: ProfileXAI是一个与模型和领域无关的框架，结合事后解释器与检索增强大语言模型为不同用户生成解释，在相关数据集上评估表现良好。


<details>
  <summary>Details</summary>
Motivation: 为不同类型用户提供有效且可信的解释，解决不同解释器在各指标表现不均的问题。

Method: 将事后解释器（SHAP、LIME、Anchor）与检索增强大语言模型结合，对多模态知识库索引，按定量标准为每个实例选择解释器，通过聊天提示生成解释。

Result: 在心脏病和甲状腺癌数据集上评估，各解释器各有优劣，Profile conditioning稳定了token使用并维持了积极评分。

Conclusion: ProfileXAI框架能实现高效且可信的解释。

Abstract: ProfileXAI is a model- and domain-agnostic framework that couples post-hoc
explainers (SHAP, LIME, Anchor) with retrieval - augmented LLMs to produce
explanations for different types of users. The system indexes a multimodal
knowledge base, selects an explainer per instance via quantitative criteria,
and generates grounded narratives with chat-enabled prompting. On Heart Disease
and Thyroid Cancer datasets, we evaluate fidelity, robustness, parsimony, token
use, and perceived quality. No explainer dominates: LIME achieves the best
fidelity--robustness trade-off (Infidelity $\le 0.30$, $L<0.7$ on Heart
Disease); Anchor yields the sparsest, low-token rules; SHAP attains the highest
satisfaction ($\bar{x}=4.1$). Profile conditioning stabilizes tokens ($\sigma
\le 13\%$) and maintains positive ratings across profiles ($\bar{x}\ge 3.7$,
with domain experts at $3.77$), enabling efficient and trustworthy
explanations.

</details>


### [57] [From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports](https://arxiv.org/abs/2510.23008)
*Qiuli Wang,Xiaoming Li,Jie Chen,Yongxu Liu,Xingpeng Zhang,Chen Liu,Wei Chen*

Main category: cs.AI

TL;DR: 本文旨在通过MDCA框架和提示优化提升大语言模型生成肝脏MRI报告的可信度，并评估多个先进大语言模型。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏不同临床场景下优化提示设计的系统指导以及评估大语言模型生成放射学报告可信度的标准化框架。

Method: 引入多维度可信度评估（MDCA）框架，提供机构特定提示优化指导，在SiliconFlow平台上评估多个先进大语言模型。

Result: 原文未提及。

Conclusion: 原文未提及。

Abstract: Large language models (LLMs) have demonstrated promising performance in
generating diagnostic conclusions from imaging findings, thereby supporting
radiology reporting, trainee education, and quality control. However,
systematic guidance on how to optimize prompt design across different clinical
contexts remains underexplored. Moreover, a comprehensive and standardized
framework for assessing the trustworthiness of LLM-generated radiology reports
is yet to be established. This study aims to enhance the trustworthiness of
LLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility
Assessment (MDCA) framework and providing guidance on institution-specific
prompt optimization. The proposed framework is applied to evaluate and compare
the performance of several advanced LLMs, including Kimi-K2-Instruct-0905,
Qwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and
ByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform.

</details>


### [58] [Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution](https://arxiv.org/abs/2510.23026)
*Crimson Stambaugh,Rajesh P. N. Rao*

Main category: cs.AI

TL;DR: 研究指出扩散规划器稀疏步规划优于单步规划，但过度稀疏会降低性能，提出MDD规划器，在多任务域达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划器在预测过度稀疏计划时性能下降，且时间密度阈值在时间范围内不均匀。

Method: 提出Mixed Density Diffuser (MDD) 扩散规划器，其整个时间范围内的密度是可调超参数。

Result: MDD在Maze2D、Franka Kitchen和Antmaze D4RL任务域达到新的SOTA。

Conclusion: MDD规划器有效，能解决现有扩散规划器在过度稀疏规划时的性能问题。

Abstract: Recent studies demonstrate that diffusion planners benefit from sparse-step
planning over single-step planning. Training models to skip steps in their
trajectories helps capture long-term dependencies without additional or memory
computational cost. However, predicting excessively sparse plans degrades
performance. We hypothesize this temporal density threshold is non-uniform
across a temporal horizon and that certain parts of a planned trajectory should
be more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion
planner where the densities throughout the horizon are tunable hyperparameters.
MDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL
task domains.

</details>


### [59] [A Survey of AI Scientists: Surveying the automatic Scientists and Research](https://arxiv.org/abs/2510.23045)
*Guiyao Tie,Pan Zhou,Lichao Sun*

Main category: cs.AI

TL;DR: 本文介绍AI科学家新兴范式，构建六阶段框架梳理领域发展，为自主科学发展提供路线图。


<details>
  <summary>Details</summary>
Motivation: AI科学家系统快速且无结构地扩散，导致研究格局碎片化，需梳理方法论原则和发展趋势。

Method: 引入统一的六阶段方法论框架，解构科学端到端过程，并据此梳理领域从基础模块到闭环系统再到当前前沿的发展历程。

Result: 明确了自主科学的当前状态，绘制出领域发展的演变过程。

Conclusion: 本综述为克服鲁棒性和治理方面的挑战提供关键路线图，引导下一代系统成为人类科学探索中可靠且不可或缺的伙伴。

Abstract: Artificial intelligence is undergoing a profound transition from a
computational instrument to an autonomous originator of scientific knowledge.
This emerging paradigm, the AI scientist, is architected to emulate the
complete scientific workflow-from initial hypothesis generation to the final
synthesis of publishable findings-thereby promising to fundamentally reshape
the pace and scale of discovery. However, the rapid and unstructured
proliferation of these systems has created a fragmented research landscape,
obscuring overarching methodological principles and developmental trends. This
survey provides a systematic and comprehensive synthesis of this domain by
introducing a unified, six-stage methodological framework that deconstructs the
end-to-end scientific process into: Literature Review, Idea Generation,
Experimental Preparation, Experimental Execution, Scientific Writing, and Paper
Generation. Through this analytical lens, we chart the field's evolution from
early Foundational Modules (2022-2023) to integrated Closed-Loop Systems
(2024), and finally to the current frontier of Scalability, Impact, and
Human-AI Collaboration (2025-present). By rigorously synthesizing these
developments, this survey not only clarifies the current state of autonomous
science but also provides a critical roadmap for overcoming remaining
challenges in robustness and governance, ultimately guiding the next generation
of systems toward becoming trustworthy and indispensable partners in human
scientific inquiry.

</details>


### [60] [TLCD: A Deep Transfer Learning Framework for Cross-Disciplinary Cognitive Diagnosis](https://arxiv.org/abs/2510.23062)
*Zhifeng Wang,Meixin Su,Yang Yang,Chunyan Zeng,Lizhi Ye*

Main category: cs.AI

TL;DR: 本文受智能教育和人工智能技术驱动，针对传统认知诊断方法在跨学科领域的挑战，提出跨学科认知诊断方法TLCD，实验表明该深度学习模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 在智能教育和人工智能背景下，传统认知诊断方法在跨学科领域因知识系统、认知结构和数据特征差异面临挑战，需要新方法。

Method: 对神经网络认知诊断和知识关联神经网络认知诊断深入研究，提出结合深度学习技术和迁移学习策略的跨学科认知诊断方法TLCD。

Result: 基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中比基础模型表现更好，能更准确评估学生学习情况。

Conclusion: 提出的跨学科认知诊断方法TLCD有效，可提升跨学科认知诊断性能。

Abstract: Driven by the dual principles of smart education and artificial intelligence
technology, the online education model has rapidly emerged as an important
component of the education industry. Cognitive diagnostic technology can
utilize students' learning data and feedback information in educational
evaluation to accurately assess their ability level at the knowledge level.
However, while massive amounts of information provide abundant data resources,
they also bring about complexity in feature extraction and scarcity of
disciplinary data. In cross-disciplinary fields, traditional cognitive
diagnostic methods still face many challenges. Given the differences in
knowledge systems, cognitive structures, and data characteristics between
different disciplines, this paper conducts in-depth research on neural network
cognitive diagnosis and knowledge association neural network cognitive
diagnosis, and proposes an innovative cross-disciplinary cognitive diagnosis
method (TLCD). This method combines deep learning techniques and transfer
learning strategies to enhance the performance of the model in the target
discipline by utilizing the common features of the main discipline. The
experimental results show that the cross-disciplinary cognitive diagnosis model
based on deep learning performs better than the basic model in
cross-disciplinary cognitive diagnosis tasks, and can more accurately evaluate
students' learning situation.

</details>


### [61] [Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs](https://arxiv.org/abs/2510.23127)
*Kai Zhuang,Jiawei Zhang,Yumou Liu,Hanqun Cao,Chunbin Gu,Mengdi Liu,Zhangyang Gao,Zitong Jerry Wang,Xuanhe Zhou,Pheng-Ann Heng,Lijun Wu,Conghui He,Cheng Tan*

Main category: cs.AI

TL;DR: 本文指出科学大语言模型处理生物分子序列存在标记化困境，通过实验对比三种输入模式，发现仅用上下文模式表现最佳，建议将其定位为基于专业知识的推理引擎，为混合科学AI代理奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决科学大语言模型处理原始生物分子序列时的标记化困境，提升其推理能力。

Method: 对领先的科学大语言模型在生物推理任务上进行系统比较，测试序列-only、上下文-only和两者结合三种输入模式。

Result: 上下文-only方法始终显著优于其他模式，加入原始序列会降低性能，表明原始序列是信息噪声。

Conclusion: 现有科学大语言模型的优势在于对结构化、人类可读知识的推理，应将其定位为基于专家知识的推理引擎，推动开发重点转向高级知识合成。

Abstract: Scientific Large Language Models (Sci-LLMs) have emerged as a promising
frontier for accelerating biological discovery. However, these models face a
fundamental challenge when processing raw biomolecular sequences: the
tokenization dilemma. Whether treating sequences as a specialized language,
risking the loss of functional motif information, or as a separate modality,
introducing formidable alignment challenges, current strategies fundamentally
limit their reasoning capacity. We challenge this sequence-centric paradigm by
positing that a more effective strategy is to provide Sci-LLMs with high-level
structured context derived from established bioinformatics tools, thereby
bypassing the need to interpret low-level noisy sequence data directly. Through
a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we
tested three input modes: sequence-only, context-only, and a combination of
both. Our findings are striking: the context-only approach consistently and
substantially outperforms all other modes. Even more revealing, the inclusion
of the raw sequence alongside its high-level context consistently degrades
performance, indicating that raw sequences act as informational noise, even for
models with specialized tokenization schemes. These results suggest that the
primary strength of existing Sci-LLMs lies not in their nascent ability to
interpret biomolecular syntax from scratch, but in their profound capacity for
reasoning over structured, human-readable knowledge. Therefore, we argue for
reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines
over expert knowledge. This work lays the foundation for a new class of hybrid
scientific AI agents, repositioning the developmental focus from direct
sequence interpretation towards high-level knowledge synthesis. The code is
available at github.com/opendatalab-raise-dev/CoKE.

</details>


### [62] [Guiding Skill Discovery with Foundation Models](https://arxiv.org/abs/2510.23167)
*Zhao Yang,Thomas M. Moerland,Mike Preuss,Aske Plaat,Vincent François-Lavet,Edward S. Hu*

Main category: cs.AI

TL;DR: 提出FoG技能发现方法，将人类意图融入技能发现，消除不良行为。


<details>
  <summary>Details</summary>
Motivation: 现有技能发现方法只关注技能多样性，未考虑人类偏好，导致不良和危险技能。

Method: 从基础模型提取评分函数评估状态，根据人类意图赋予高低值，对技能发现算法的奖励重新加权。

Result: 在基于状态和像素的任务中，成功消除如翻转或滚动等不良行为，避免危险区域，还能发现难以定义的技能。

Conclusion: FoG方法能有效将人类意图融入技能发现，解决现有方法问题。

Abstract: Learning diverse skills without hand-crafted reward functions could
accelerate reinforcement learning in downstream tasks. However, existing skill
discovery methods focus solely on maximizing the diversity of skills without
considering human preferences, which leads to undesirable behaviors and
possibly dangerous skills. For instance, a cheetah robot trained using previous
methods learns to roll in all directions to maximize skill diversity, whereas
we would prefer it to run without flipping or entering hazardous areas. In this
work, we propose a Foundation model Guided (FoG) skill discovery method, which
incorporates human intentions into skill discovery through foundation models.
Specifically, FoG extracts a score function from foundation models to evaluate
states based on human intentions, assigning higher values to desirable states
and lower to undesirable ones. These scores are then used to re-weight the
rewards of skill discovery algorithms. By optimizing the re-weighted skill
discovery rewards, FoG successfully learns to eliminate undesirable behaviors,
such as flipping or rolling, and to avoid hazardous areas in both state-based
and pixel-based tasks. Interestingly, we show that FoG can discover skills
involving behaviors that are difficult to define. Interactive visualisations
are available from https://sites.google.com/view/submission-fog.

</details>


### [63] [AUPO -- Abstracted Until Proven Otherwise: A Reward Distribution Based Abstraction Algorithm](https://arxiv.org/abs/2510.23214)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 提出名为AUPO的蒙特卡罗树搜索决策策略修改方法，在IPPC基准问题上优于MCTS，有独特优势且不与其他抽象技术互斥。


<details>
  <summary>Details</summary>
Motivation: 改进蒙特卡罗树搜索（MCTS）的决策策略，提升其性能。

Method: 提出自动动作抽象算法AUPO，仅依赖MCTS过程中的奖励分布统计。

Result: 在一系列IPPC基准问题上，AUPO明显优于MCTS，能检测到现有框架难以处理的对称动作。

Conclusion: AUPO是对MCTS决策策略的有效改进，且不与其他仅影响树搜索的抽象技术互斥。

Abstract: We introduce a novel, drop-in modification to Monte Carlo Tree Search's
(MCTS) decision policy that we call AUPO. Comparisons based on a range of IPPC
benchmark problems show that AUPO clearly outperforms MCTS. AUPO is an
automatic action abstraction algorithm that solely relies on reward
distribution statistics acquired during the MCTS. Thus, unlike other automatic
abstraction algorithms, AUPO requires neither access to transition
probabilities nor does AUPO require a directed acyclic search graph to build
its abstraction, allowing AUPO to detect symmetric actions that
state-of-the-art frameworks like ASAP struggle with when the resulting
symmetric states are far apart in state space. Furthermore, as AUPO only
affects the decision policy, it is not mutually exclusive with other
abstraction techniques that only affect the tree search.

</details>


### [64] [Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach](https://arxiv.org/abs/2510.23216)
*Alessandro Sestini,Joakim Bergdahl,Jean-Philippe Barrette-LaPierre,Florian Fuchs,Brady Chen,Micheal Jones,Linus Gisslén*

Main category: cs.AI

TL;DR: 本文提出适用于游戏行业的样本高效DRL方法，在EA SPORTS FC 25中验证，效果良好且将用于后续版本。


<details>
  <summary>Details</summary>
Motivation: 现有DRL技术少用于游戏行业打造AI行为，以往研究训练超人代理不适合资源有限的游戏工作室，需训练类人代理。

Method: 利用预收集数据和增加网络可塑性提高基于价值的DRL样本效率。

Result: 在EA SPORTS FC 25中训练守门员代理，救球率比内置AI高10%，训练速度比标准DRL方法快50%，专家认为游戏表现更类人。

Conclusion: 该方法效果好，将在系列后续版本中替代手工制作的方法。

Abstract: While several high profile video games have served as testbeds for Deep
Reinforcement Learning (DRL), this technique has rarely been employed by the
game industry for crafting authentic AI behaviors. Previous research focuses on
training super-human agents with large models, which is impractical for game
studios with limited resources aiming for human-like agents. This paper
proposes a sample-efficient DRL method tailored for training and fine-tuning
agents in industrial settings such as the video game industry. Our method
improves sample efficiency of value-based DRL by leveraging pre-collected data
and increasing network plasticity. We evaluate our method training a goalkeeper
agent in EA SPORTS FC 25, one of the best-selling football simulations today.
Our agent outperforms the game's built-in AI by 10% in ball saving rate.
Ablation studies show that our method trains agents 50% faster compared to
standard DRL methods. Finally, qualitative evaluation from domain experts
indicates that our approach creates more human-like gameplay compared to
hand-crafted agents. As a testimony of the impact of the approach, the method
is intended to replace the hand-crafted counterpart in next iterations of the
series.

</details>


### [65] [Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action](https://arxiv.org/abs/2510.23221)
*Hong Wang,Wenkai Yang,Jie Wang,Huanshuo Dong,Zijie Geng,Zhen Huang,Depeng Xie,Zhezheng Hao,Hande Dong*

Main category: cs.AI

TL;DR: 提出用于IC热仿真数据生成的BlocKOA算法，加速数据生成且提高精度，理论和实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法用于IC热仿真需大量高保真训练数据，计算成本高。

Method: 使用基于热方程结构的块Krylov算法快速获取基本解，组合得到满足物理约束的温度分布，应用热算子确定热源分布。

Result: 理论上时间复杂度比现有方法低一个数量级，实验中生成5000个芯片热仿真数据加速420倍，用其4%生成时间的数据训练效果与现有方法相当。

Conclusion: BlocKOA算法能有效加速IC热仿真数据生成，提高数据质量。

Abstract: Recent advances in data-driven approaches, such as neural operators (NOs),
have shown substantial efficacy in reducing the solution time for integrated
circuit (IC) thermal simulations. However, a limitation of these approaches is
requiring a large amount of high-fidelity training data, such as chip
parameters and temperature distributions, thereby incurring significant
computational costs. To address this challenge, we propose a novel algorithm
for the generation of IC thermal simulation data, named block Krylov and
operator action (BlocKOA), which simultaneously accelerates the data generation
process and enhances the precision of generated data. BlocKOA is specifically
designed for IC applications. Initially, we use the block Krylov algorithm
based on the structure of the heat equation to quickly obtain a few basic
solutions. Then we combine them to get numerous temperature distributions that
satisfy the physical constraints. Finally, we apply heat operators on these
functions to determine the heat source distributions, efficiently generating
precise data points. Theoretical analysis shows that the time complexity of
BlocKOA is one order lower than the existing method. Experimental results
further validate its efficiency, showing that BlocKOA achieves a 420-fold
speedup in generating thermal simulation data for 5000 chips with varying
physical parameters and IC structures. Even with just 4% of the generation
time, data-driven approaches trained on the data generated by BlocKOA exhibits
comparable performance to that using the existing method.

</details>


### [66] [CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach](https://arxiv.org/abs/2510.23304)
*Riccardo Romanello,Daniele Lizzio Bosco,Jacopo Cossio,Dusan Sutulovic,Giuseppe Serra,Carla Piazza,Paolo Burelli*

Main category: cs.AI

TL;DR: 提出用强化学习方法解决CNOT门最小化问题，用单个代理和预处理方法，实验显示效果优于现有算法。


<details>
  <summary>Details</summary>
Motivation: CNOT门在量子计算中广泛使用，需最小化其数量，但CNOT最小化问题仍是开放挑战，计算复杂度未完全明确。

Method: 引入新颖的强化学习方法，用单个代理处理固定大小m以内的问题，对不同大小矩阵进行嵌入或高斯条纹预处理。

Result: 训练m = 8的代理，在3到15大小矩阵上评估，n增大时方法优于现有算法。

Conclusion: 所提出的强化学习方法在解决CNOT最小化问题上有效，性能优于现有算法。

Abstract: CNOT gates are fundamental to quantum computing, as they facilitate
entanglement, a crucial resource for quantum algorithms. Certain classes of
quantum circuits are constructed exclusively from CNOT gates. Given their
widespread use, it is imperative to minimise the number of CNOT gates employed.
This problem, known as CNOT minimisation, remains an open challenge, with its
computational complexity yet to be fully characterised. In this work, we
introduce a novel reinforcement learning approach to address this task. Instead
of training multiple reinforcement learning agents for different circuit sizes,
we use a single agent up to a fixed size $m$. Matrices of sizes different from
m are preprocessed using either embedding or Gaussian striping. To assess the
efficacy of our approach, we trained an agent with m = 8, and evaluated it on
matrices of size n that range from 3 to 15. The results we obtained show that
our method overperforms the state-of-the-art algorithm as the value of n
increases.

</details>


### [67] [Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps](https://arxiv.org/abs/2510.23340)
*Anwesha Das,John Duff,Jörg Hoffmann,Vera Demberg*

Main category: cs.AI

TL;DR: 本文提出自适应信号理论框架，用于人机协作，通过多步规划和用户意识模型提升协作效果，为实用通信奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 在快速变化环境的时间敏感任务中，协助代理需有效识别和传达信息，以确保人类准确理解关键任务元素。

Method: 引入基于理性沟通原则的自适应信号理论框架，使用贝叶斯参考解析和RSA建模框架规划消息序列，根据用户和场景调整消息。

Result: 与基线方法对比，表明结合多步规划和现实的用户意识模型对提升效果至关重要。

Conclusion: 为人类 - 代理团队的实用通信建立理论基础，强调认知科学对辅助代理设计的指导作用。

Abstract: Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.

</details>


### [68] [Opinion Mining Based Entity Ranking using Fuzzy Logic Algorithmic Approach](https://arxiv.org/abs/2510.23384)
*Pratik N. Kalamkar,A. G. Phakatkar*

Main category: cs.AI

TL;DR: 本文提出一种更细粒度的意见挖掘方法并对实体进行排名。


<details>
  <summary>Details</summary>
Motivation: 当前虽有意见挖掘研究，但未对意见进行更细粒度分类后再排名，有必要开展相关研究。

Method: 使用模糊逻辑推理进行更细粒度的意见挖掘，并根据挖掘信息对实体进行排名。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论。

Abstract: Opinions are central to almost all human activities and are key influencers
of our behaviors. In current times due to growth of social networking website
and increase in number of e-commerce site huge amount of opinions are now
available on web. Given a set of evaluative statements that contain opinions
(or sentiments) about an Entity, opinion mining aims to extract attributes and
components of the object that have been commented on in each statement and to
determine whether the comments are positive, negative or neutral. While lot of
research recently has been done in field of opinion mining and some of it
dealing with ranking of entities based on review or opinion set, classifying
opinions into finer granularity level and then ranking entities has never been
done before. In this paper method for opinion mining from statements at a
deeper level of granularity is proposed. This is done by using fuzzy logic
reasoning, after which entities are ranked as per this information.

</details>


### [69] [Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens](https://arxiv.org/abs/2510.23410)
*Jiahao Ji,Tianyu Wang,Yeshu Li,Yushen Huo,Zhilin Zhang,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 本文提出了投标基础模型Bid2X，以解决以往自动投标模型泛化性不足的问题，在多个数据集和线上测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 以往自动投标模型针对特定场景建模，泛化性差，需开发跨场景的投标模型。

Method: 提出Bid2X模型，使用统一函数估计投标效果，采用统一序列嵌入、两种注意力机制、变量感知融合模块和零膨胀投影模块进行建模。

Result: 离线评估显示Bid2X优于多种基线模型，线上A/B测试中GMV提高4.65%，ROI提高2.44%。

Conclusion: Bid2X为计算广告中的投标基础模型应用铺平了道路。

Abstract: Auto-bidding is crucial in facilitating online advertising by automatically
providing bids for advertisers. While previous work has made great efforts to
model bidding environments for better ad performance, it has limitations in
generalizability across environments since these models are typically tailored
for specific bidding scenarios. To this end, we approach the
scenario-independent principles through a unified function that estimates the
achieved effect under specific bids, such as budget consumption, gross
merchandise volume (GMV), page views, etc. Then, we propose a bidding
foundation model Bid2X to learn this fundamental function from data in various
scenarios. Our Bid2X is built over uniform series embeddings that encode
heterogeneous data through tailored embedding methods. To capture complex
inter-variable and dynamic temporal dependencies in bidding data, we propose
two attention mechanisms separately treating embeddings of different variables
and embeddings at different times as attention tokens for representation
learning. On top of the learned variable and temporal representations, a
variable-aware fusion module is used to perform adaptive bidding outcome
prediction. To model the unique bidding data distribution, we devise a
zero-inflated projection module to incorporate the estimated non-zero
probability into its value prediction, which makes up a joint optimization
objective containing classification and regression. The objective is proven to
converge to the zero-inflated distribution. Our model has been deployed on the
ad platform in Taobao, one of the world's largest e-commerce platforms. Offline
evaluation on eight datasets exhibits Bid2X's superiority compared to various
baselines and its generality across different scenarios. Bid2X increased GMV by
4.65% and ROI by 2.44% in online A/B tests, paving the way for bidding
foundation model in computational advertising.

</details>


### [70] [Causal Deep Q Network](https://arxiv.org/abs/2510.23424)
*Elouanes Khelifi,Amir Saki,Usef Faghihi*

Main category: cs.AI

TL;DR: 本文提出将因果原则融入DQN，用PEACE公式估计因果效应，实验表明该方法提升DQN解决问题能力，优于传统DQN。


<details>
  <summary>Details</summary>
Motivation: 传统DQN依赖关联学习会产生虚假关联，阻碍其解决问题的能力，需改进。

Method: 引入新方法将因果原则融入DQN，利用PEACE公式估计因果效应，在训练中加入因果推理。

Result: 在标准基准环境实验中，该方法优于传统DQN。

Conclusion: 通过有原则的因果推理提升深度强化学习智能体能力是有前景的途径。

Abstract: Deep Q Networks (DQN) have shown remarkable success in various reinforcement
learning tasks. However, their reliance on associative learning often leads to
the acquisition of spurious correlations, hindering their problem-solving
capabilities. In this paper, we introduce a novel approach to integrate causal
principles into DQNs, leveraging the PEACE (Probabilistic Easy vAriational
Causal Effect) formula for estimating causal effects. By incorporating causal
reasoning during training, our proposed framework enhances the DQN's
understanding of the underlying causal structure of the environment, thereby
mitigating the influence of confounding factors and spurious correlations. We
demonstrate that integrating DQNs with causal capabilities significantly
enhances their problem-solving capabilities without compromising performance.
Experimental results on standard benchmark environments showcase that our
approach outperforms conventional DQNs, highlighting the effectiveness of
causal reasoning in reinforcement learning. Overall, our work presents a
promising avenue for advancing the capabilities of deep reinforcement learning
agents through principled causal inference.

</details>


### [71] [A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration](https://arxiv.org/abs/2510.23443)
*Chiara Bonfanti,Alessandro Druetto,Cataldo Basile,Tharindu Ranasinghe,Marcos Zampieri*

Main category: cs.AI

TL;DR: 文章指出网络安全与法律交叉领域传统研究工具不足，阻碍跨专业协作，提出构建智能系统并展示多语言任务初步成果。


<details>
  <summary>Details</summary>
Motivation: 解决网络安全与法律交叉领域传统法律研究工具难以处理复杂信息，导致法律专家和网络安全专业人员协作受阻的问题。

Method: 未提及

Result: 在多语言任务上取得了有前景的初步成果。

Conclusion: 为构建能够应对复杂网络法律领域的智能系统迈出了第一步。

Abstract: The growing intersection of cybersecurity and law creates a complex
information space where traditional legal research tools struggle to deal with
nuanced connections between cases, statutes, and technical vulnerabilities.
This knowledge divide hinders collaboration between legal experts and
cybersecurity professionals. To address this important gap, this work provides
a first step towards intelligent systems capable of navigating the increasingly
intricate cyber-legal domain. We demonstrate promising initial results on
multilingual tasks.

</details>


### [72] [What are the odds? Risk and uncertainty about AI existential risk](https://arxiv.org/abs/2510.23453)
*Marco Grossi*

Main category: cs.AI

TL;DR: 本文对Cappelen等人文章进行评论，提醒线性风险模型哲学局限，探讨模型差异、概率及不确定性，助于理解AI存在风险可能性。


<details>
  <summary>Details</summary>
Motivation: 提醒线性风险模型的哲学局限性，更好地理解AI存在风险。

Method: 先讨论标准瑞士奶酪模型与作者所用模型的差异，再分析认知无差异情况下概率，区分风险与不确定性。

Result: 指出P(D)概率受结构关系影响，且任何P(D)估计受选项不确定性和状态空间不确定性影响。

Conclusion: 将不确定性维度纳入AI存在风险定性讨论，能更好理解P(D)可能性。

Abstract: This work is a commentary of the article
\href{https://doi.org/10.18716/ojs/phai/2025.2801}{AI Survival Stories: a
Taxonomic Analysis of AI Existential Risk} by Cappelen, Goldstein, and
Hawthorne. It is not just a commentary though, but a useful reminder of the
philosophical limitations of \say{linear} models of risk. The article will
focus on the model employed by the authors: first, I discuss some differences
between standard Swiss Cheese models and this one. I then argue that in a
situation of epistemic indifference the probability of P(D) is higher than what
one might first suggest, given the structural relationships between layers. I
then distinguish between risk and uncertainty, and argue that any estimation of
P(D) is structurally affected by two kinds of uncertainty: option uncertainty
and state-space uncertainty. Incorporating these dimensions of uncertainty into
our qualitative discussion on AI existential risk can provide a better
understanding of the likeliness of P(D).

</details>


### [73] [Policy-Aware Generative AI for Safe, Auditable Data Access Governance](https://arxiv.org/abs/2510.23474)
*Shames Al Mandalawi,Muzakkiruddin Ahmed Mohammed,Hendrika Maclean,Mert Can Cakmak,John R. Talburt*

Main category: cs.AI

TL;DR: 提出用大语言模型的策略感知控制器做访问决策，经评估效果良好，表明策略约束的LLM推理能实现安全合规可追溯的机器决策。


<details>
  <summary>Details</summary>
Motivation: 企业需要满足最小权限、合规且可审计的访问决策。

Method: 使用大语言模型（如Google Gemini 2.0 Flash），执行六阶段推理框架，有早期硬策略门且默认拒绝。

Result: 应用策略门后精确决策匹配率从10/14提升到13/14，拒绝召回率达1.00，必须拒绝场景的错误批准率降为0等，理由质量专家评分高，中位延迟不到一分钟。

Conclusion: 策略约束的LLM推理结合明确的门控和审计追踪，能将人类可读策略转化为安全、合规和可追溯的机器决策。

Abstract: Enterprises need access decisions that satisfy least privilege, comply with
regulations, and remain auditable. We present a policy aware controller that
uses a large language model (LLM) to interpret natural language requests
against written policies and metadata, not raw data. The system, implemented
with Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context
interpretation, user validation, data classification, business purpose test,
compliance mapping, and risk synthesis) with early hard policy gates and deny
by default. It returns APPROVE, DENY, CONDITIONAL together with cited controls
and a machine readable rationale. We evaluate on fourteen canonical cases
across seven scenario families using a privacy preserving benchmark. Results
show Exact Decision Match improving from 10/14 to 13/14 (92.9\%) after applying
policy gates, DENY recall rising to 1.00, False Approval Rate on must-deny
families dropping to 0, and Functional Appropriateness and Compliance Adherence
at 14/14. Expert ratings of rationale quality are high, and median latency is
under one minute. These findings indicate that policy constrained LLM
reasoning, combined with explicit gates and audit trails, can translate human
readable policies into safe, compliant, and traceable machine decisions.

</details>


### [74] [Human-AI Collaborative Uncertainty Quantification](https://arxiv.org/abs/2510.23476)
*Sima Noorani,Shayan Kiyani,George Pappas,Hamed Hassani*

Main category: cs.AI

TL;DR: 本文提出人类 - AI 协作不确定性量化框架，开发校准算法，实验表明协作预测集表现优于单一主体。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 缺乏在不确定性下做出可靠决策的能力，因此设计结合人类和 AI 优势的协作框架。

Method: 引入人类 - AI 协作不确定性量化框架，开发离线和在线校准算法。

Result: 最优协作预测集有双阈值结构，在线方法能适应分布变化，实验显示协作预测集表现更好。

Conclusion: 人类 - AI 协作框架在不确定性量化方面有效，能提升决策质量。

Abstract: AI predictive systems are increasingly embedded in decision making pipelines,
shaping high stakes choices once made solely by humans. Yet robust decisions
under uncertainty still rely on capabilities that current AI lacks: domain
knowledge not captured by data, long horizon context, and reasoning grounded in
the physical world. This gap has motivated growing efforts to design
collaborative frameworks that combine the complementary strengths of humans and
AI. This work advances this vision by identifying the fundamental principles of
Human AI collaboration within uncertainty quantification, a key component of
reliable decision making. We introduce Human AI Collaborative Uncertainty
Quantification, a framework that formalizes how an AI model can refine a human
expert's proposed prediction set with two goals: avoiding counterfactual harm,
ensuring the AI does not degrade correct human judgments, and complementarity,
enabling recovery of correct outcomes the human missed. At the population
level, we show that the optimal collaborative prediction set follows an
intuitive two threshold structure over a single score function, extending a
classical result in conformal prediction. Building on this insight, we develop
practical offline and online calibration algorithms with provable distribution
free finite sample guarantees. The online method adapts to distribution shifts,
including human behavior evolving through interaction with AI, a phenomenon we
call Human to AI Adaptation. Experiments across image classification,
regression, and text based medical decision making show that collaborative
prediction sets consistently outperform either agent alone, achieving higher
coverage and smaller set sizes across various conditions.

</details>


### [75] [Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy](https://arxiv.org/abs/2510.23487)
*Roham Koohestani,Ziyou Li,Anton Podkopaev,Maliheh Izadi*

Main category: cs.AI

TL;DR: 论文建立现代智能体AI系统架构类与乔姆斯基层级抽象机器的形式等价关系，提出记忆架构决定计算能力，给出框架优化效率、实现形式验证等，还扩展到概率自动机，最后规划开发工具和语法的议程。


<details>
  <summary>Details</summary>
Motivation: 为智能体架构优化计算效率和成本，实现形式验证，保证安全性和可预测性，划分可验证系统边界。

Method: 将AI智能体记忆架构与自动机类别对应，建立自动机 - 智能体框架，扩展到概率自动机。

Result: 证明简单反射智能体、分层任务分解智能体和使用读写内存进行反思的智能体分别等价于有限自动机、下推自动机和图灵机，可应用自动机理论技术。

Conclusion: 规划了为智能体框架开发静态分析工具和语法的议程。

Abstract: This paper establishes a formal equivalence between the architectural classes
of modern agentic AI systems and the abstract machines of the Chomsky
hierarchy. We posit that the memory architecture of an AI agent is the
definitive feature determining its computational power and that it directly
maps it to a corresponding class of automaton. Specifically, we demonstrate
that simple reflex agents are equivalent to Finite Automata, hierarchical
task-decomposition agents are equivalent to Pushdown Automata, and agents
employing readable/writable memory for reflection are equivalent to TMs. This
Automata-Agent Framework provides a principled methodology for right-sizing
agent architectures to optimize computational efficiency and cost. More
critically, it creates a direct pathway to formal verification, enables the
application of mature techniques from automata theory to guarantee agent safety
and predictability. By classifying agents, we can formally delineate the
boundary between verifiable systems and those whose behavior is fundamentally
undecidable. We address the inherent probabilistic nature of LLM-based agents
by extending the framework to probabilistic automata that allow quantitative
risk analysis. The paper concludes by outlining an agenda for developing static
analysis tools and grammars for agentic frameworks.

</details>


### [76] [Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier](https://arxiv.org/abs/2510.23506)
*Hyeongseop Rha,Jeong Hun Yeo,Yeonju Kim,Yong Man Ro*

Main category: cs.AI

TL;DR: 提出ERV和解释奖励方法解决MLLM情感解释不一致问题，提升交互质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的方法生成的情感解释常与目标标签不一致，影响交互可靠性，需解决该问题。

Method: 提出情感理由验证器（ERV）和解释奖励，在不修改模型架构和额外标注情况下引导模型生成一致推理。

Result: 在MAFW和DFEW数据集上显著提高了解释 - 预测一致性和解释情感准确性。

Conclusion: 该方法增强了解释与预测的一致性，使MLLM实现情感连贯、可信的交互，是迈向类人HCI系统的关键一步。

Abstract: The recent advancement of Multimodal Large Language Models (MLLMs) is
transforming human-computer interaction (HCI) from surface-level exchanges into
more nuanced and emotionally intelligent communication. To realize this shift,
emotion understanding becomes essential allowing systems to capture subtle cues
underlying user intent. Furthermore, providing faithful explanations for
predicted emotions is crucial to ensure interpretability and build user trust.
However, current MLLM-based methods often generate emotion explanations that
diverge from the target labels and sometimes even contradict their own
predicted emotions. This inconsistency poses a critical risk for
misunderstanding and erodes reliability in interactive settings. To address
this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and
an Explanation Reward. Our method guides the model to produce reasoning that is
explicitly consistent with the target emotion during multimodal emotion
recognition without modifying the model architecture or requiring additional
paired video-description annotations. Our method significantly improves
faithful explanation-prediction consistency and explanation emotion accuracy on
the MAFW and DFEW datasets. Through extensive experiments and human
evaluations, we show that our approach not only enhances alignment between
explanation and prediction but also empowers MLLMs to deliver emotionally
coherent, trustworthy interactions, marking a key step toward truly human-like
HCI systems.

</details>


### [77] [Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence](https://arxiv.org/abs/2510.23524)
*KC Santosh,Rodrigue Rizk,Longwei Wang*

Main category: cs.AI

TL;DR: 文章批判现有AI依赖大规模静态数据集和单一训练范式，提出Human AI (HAI)框架，以实现负责任、以人为本的AI。


<details>
  <summary>Details</summary>
Motivation: 人工智能发展带来计算需求提升，引发环境和伦理问题，批判现有依赖大规模静态数据集和单一训练范式的做法，寻求可持续AI解决方案。

Method: 引入Human AI (HAI)框架，强调增量学习、碳感知优化和人在环协作，借鉴生物认知并利用动态架构。

Result: 详细阐述了HAI框架的理论基础、系统设计和操作原则，能解决主动学习、持续适应和节能模型部署等挑战。

Conclusion: 该方法为实现负责任、以人为本的人工智能提供了途径。

Abstract: The rapid advancement of Artificial Intelligence (AI) has led to
unprecedented computational demands, raising significant environmental and
ethical concerns. This paper critiques the prevailing reliance on large-scale,
static datasets and monolithic training paradigms, advocating for a shift
toward human-inspired, sustainable AI solutions. We introduce a novel
framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware
optimization, and human-in-the-loop collaboration to enhance adaptability,
efficiency, and accountability. By drawing parallels with biological cognition
and leveraging dynamic architectures, HAI seeks to balance performance with
ecological responsibility. We detail the theoretical foundations, system
design, and operational principles that enable AI to learn continuously and
contextually while minimizing carbon footprints and human annotation costs. Our
approach addresses pressing challenges in active learning, continual
adaptation, and energy-efficient model deployment, offering a pathway toward
responsible, human-centered artificial intelligence.

</details>


### [78] [When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning](https://arxiv.org/abs/2510.23532)
*Anirban Das,Irtaza Khalid,Rafael Peñaloza,Steven Schockaert*

Main category: cs.AI

TL;DR: 现有系统关系推理基准过于简化，本文引入新基准NoRA以推动该领域进展。


<details>
  <summary>Details</summary>
Motivation: 现有系统关系推理基准设定过于简化，模型难以泛化，需新基准推动领域进步。

Method: 引入新基准NoRA，增加难度并要求模型超越基于路径的推理。

Result: 文中未提及具体结果。

Conclusion: 新基准NoRA有助于系统关系推理领域的进一步发展。

Abstract: Designing models that can learn to reason in a systematic way is an important
and long-standing challenge. In recent years, a wide range of solutions have
been proposed for the specific case of systematic relational reasoning,
including Neuro-Symbolic approaches, variants of the Transformer architecture,
and specialised Graph Neural Networks. However, existing benchmarks for
systematic relational reasoning focus on an overly simplified setting, based on
the assumption that reasoning can be reduced to composing relational paths. In
fact, this assumption is hard-baked into the architecture of several recent
models, leading to approaches that can perform well on existing benchmarks but
are difficult to generalise to other settings. To support further progress in
the field of systematic relational reasoning with neural networks, we introduce
NoRA, a new benchmark which adds several levels of difficulty and requires
models to go beyond path-based reasoning.

</details>


### [79] [OntoPret: An Ontology for the Interpretation of Human Behavior](https://arxiv.org/abs/2510.23553)
*Alexis Ellis,Stacie Severyn,Fjollë Novakazi,Hadi Banaee,Cogan Shimizu*

Main category: cs.AI

TL;DR: 本文提出OntoPret本体用于解释人类行为，展示其在制造和游戏两个用例中的适应性，为人类意图推理奠定语义基础。


<details>
  <summary>Details</summary>
Motivation: 人机协作成为关键范式，现有技术中心的机器人框架缺乏人类行为细微模型，描述性的行为本体不适用于实时协作解释，存在研究差距。

Method: 基于认知科学和模块化工程方法，提出OntoPret本体。

Result: 展示了OntoPret在制造和游戏两个不同用例中的适应性。

Conclusion: OntoPret为人类行为分类提供了形式化、机器可处理的框架，为高级人类意图推理奠定了语义基础。

Abstract: As human machine teaming becomes central to paradigms like Industry 5.0, a
critical need arises for machines to safely and effectively interpret complex
human behaviors. A research gap currently exists between techno centric robotic
frameworks, which often lack nuanced models of human behavior, and descriptive
behavioral ontologies, which are not designed for real time, collaborative
interpretation. This paper addresses this gap by presenting OntoPret, an
ontology for the interpretation of human behavior. Grounded in cognitive
science and a modular engineering methodology, OntoPret provides a formal,
machine processable framework for classifying behaviors, including task
deviations and deceptive actions. We demonstrate its adaptability across two
distinct use cases manufacturing and gameplay and establish the semantic
foundations necessary for advanced reasoning about human intentions.

</details>


### [80] [ReCode: Unify Plan and Action for Universal Granularity Control](https://arxiv.org/abs/2510.23564)
*Zhaoyang Yu,Jiayi Zhang,Huixue Su,Yufan Zhao,Yifan Wu,Mingyi Deng,Jinyu Xiang,Yizhang Lin,Lingxiao Tang,Yingchao Li,Yuyu Luo,Bang Liu,Chenglin Wu*

Main category: cs.AI

TL;DR: 现有基于大语言模型的智能体缺乏跨决策粒度操作能力，本文提出ReCode范式统一规划与行动，实验表明其效果好且数据效率高。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体缺乏跨决策粒度操作能力，现有范式将高层规划和低层行动严格分离，损害了动态适应性和泛化能力。

Method: 提出ReCode范式，将规划和行动统一在单一代码表示中，把高层计划视为抽象占位函数，递归分解为细粒度子函数直至原始动作。

Result: 实验显示ReCode在推理性能上显著超越先进基线，在训练中展现出卓越的数据效率。

Conclusion: 通过递归代码生成统一规划和行动是实现通用粒度控制的强大有效方法。

Abstract: Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.

</details>


### [81] [Reduced AI Acceptance After the Generative AI Boom: Evidence From a Two-Wave Survey Study](https://arxiv.org/abs/2510.23578)
*Joachim Baumann,Aleksandra Urman,Ulrich Leicht-Deobald,Zachary J. Roman,Anikó Hannák,Markus Christen*

Main category: cs.AI

TL;DR: 通过瑞士大规模调查研究发现GenAI热潮后公众对AI接受度降低、对人工监督需求增加，还放大社会不平等，挑战行业假设。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索公众对AI使用的态度，尤其在有影响力的决策场景中，且组织集成AI时未考虑用户偏好。

Method: 采用代表瑞士人口的大规模两波调查研究（n_wave1=1514，n_wave2=1488），对比ChatGPT推出前后公众对AI态度的变化。

Result: GenAI热潮与公众对AI接受度降低、对人工监督需求增加显著相关，‘完全不可接受’的比例从23%增至30%，支持仅人工决策的比例从18%增至26%，还放大了社会不平等。

Conclusion: 研究结果挑战了行业对公众准备好接受AI部署的假设，强调了使技术发展与公众不断变化的偏好相一致的重要性。

Abstract: The rapid adoption of generative artificial intelligence (GenAI) technologies
has led many organizations to integrate AI into their products and services,
often without considering user preferences. Yet, public attitudes toward AI
use, especially in impactful decision-making scenarios, are underexplored.
Using a large-scale two-wave survey study (n_wave1=1514, n_wave2=1488)
representative of the Swiss population, we examine shifts in public attitudes
toward AI before and after the launch of ChatGPT. We find that the GenAI boom
is significantly associated with reduced public acceptance of AI (see Figure 1)
and increased demand for human oversight in various decision-making contexts.
The proportion of respondents finding AI "not acceptable at all" increased from
23% to 30%, while support for human-only decision-making rose from 18% to 26%.
These shifts have amplified existing social inequalities in terms of widened
educational, linguistic, and gender gaps post-boom. Our findings challenge
industry assumptions about public readiness for AI deployment and highlight the
critical importance of aligning technological development with evolving public
preferences.

</details>


### [82] [Multi-Agent Evolve: LLM Self-Improve through Co-evolution](https://arxiv.org/abs/2510.23595)
*Yixing Chen,Yiding Wang,Siqi Zhu,Haofei Yu,Tao Feng,Muhan Zhan,Mostofa Patwary,Jiaxuan You*

Main category: cs.AI

TL;DR: 提出Multi - Agent Evolve (MAE)框架提升大语言模型推理能力，在Qwen2.5 - 3B - Instruct实验中，多个基准测试平均提升4.54%，是可扩展、数据高效方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习依赖人工数据集和可验证奖励，限制了可扩展性和通用性；Self - Play RL方法依赖特定环境，难以扩展到通用领域。

Method: 提出MAE框架，基于从单个大语言模型实例化的三个交互代理（Proposer、Solver、Judge），应用强化学习优化行为。

Result: 在Qwen2.5 - 3B - Instruct上实验，MAE在多个基准测试中平均提升4.54%。

Conclusion: MAE是可扩展、数据高效的方法，能以最少的人工监督提升大语言模型的通用推理能力。

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in
enhancing the reasoning capabilities of large language models (LLMs). However,
the success of RL for LLMs heavily relies on human-curated datasets and
verifiable rewards, which limit their scalability and generality. Recent
Self-Play RL methods, inspired by the success of the paradigm in games and Go,
aim to enhance LLM reasoning capabilities without human-annotated data.
However, their methods primarily depend on a grounded environment for feedback
(e.g., a Python interpreter or a game engine); extending them to general
domains remains challenging. To address these challenges, we propose
Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in
solving diverse tasks, including mathematics, reasoning, and general knowledge
Q&A. The core design of MAE is based on a triplet of interacting agents
(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies
reinforcement learning to optimize their behaviors. The Proposer generates
questions, the Solver attempts solutions, and the Judge evaluates both while
co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves
an average improvement of 4.54% on multiple benchmarks. These results highlight
MAE as a scalable, data-efficient method for enhancing the general reasoning
abilities of LLMs with minimal reliance on human-curated supervision.

</details>


### [83] [Alita-G: Self-Evolving Generative Agent for Agent Generation](https://arxiv.org/abs/2510.23601)
*Jiahao Qiu,Xuan Qi,Hongru Wang,Xinzhe Juan,Yimin Wang,Zelin Zhao,Jiayi Geng,Jiacheng Guo,Peihang Li,Jingzhe Shi,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 提出ALITA - G自进化框架，将通用智能体转变为领域专家，在多个基准测试中取得良好效果，提升准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 当前自进化智能体工作大多局限于提示重写或失败重试，需更好方法将通用智能体转变为领域专家。

Method: 系统地生成、抽象和管理模型上下文协议（MCP）工具，通用智能体执行目标领域任务，合成候选MCP，抽象为参数化原语并整合到MCP Box，推理时进行检索增强的MCP选择。

Result: 在GAIA、PathVQA和Humanity's Last Exam等基准测试中取得显著提升，如在GAIA验证中，pass@1达83.03%，pass@3达89.09%，降低约15%的平均每个示例的令牌数。

Conclusion: ALITA - G提供了从通用能力到可复用的特定领域能力的原则性途径，提高了复杂推理任务的准确性和效率。

Abstract: Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool's descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [84] [Error bounded compression for weather and climate applications](https://arxiv.org/abs/2510.22265)
*Langwen Huang,Luigi Fusco,Florian Scheidl,Jan Zibell,Michael Armand Sprenger,Sebastian Schemm,Torsten Hoefler*

Main category: cs.CE

TL;DR: 随着气象和气候模拟分辨率提升，数据量剧增，本文提出EBCC压缩器，采用两层方法和反馈率控制机制，经多基准测试表现优于其他方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 气象和气候模拟数据量快速增长，数据规模成为广泛应用的限制因素且将耗尽可用存储设备，需解决数据压缩问题。

Method: 提出EBCC压缩器，采用两层方法，基础压缩层用JPEG2000，残差压缩层用小波变换和SPIHT编码，还加入反馈率控制机制调整压缩比以实现指定最大误差目标。

Result: 在多个基准测试中，EBCC在0.1% - 10%相对误差目标下表现优于其他方法，压缩比达15x - 300x以上，在能量预算闭合和拉格朗日轨迹基准测试中能实现超100x压缩并将误差控制在自然变率内。

Conclusion: 验证了EBCC在创建适用于下游应用的高压缩气象和气候数据集方面的有效性。

Abstract: As the resolution of weather and climate simulations increases, the amount of
data produced is growing rapidly from hundreds of terabytes to tens of
petabytes. The huge size becomes a limiting factor for broader adoption, and
its fast growth rate will soon exhaust all the available storage devices. To
address these issues, we present EBCC (Error Bounded Climate-data Compressor).
It follows a two-layer approach: a base compression layer using JPEG2000 to
capture the bulk of the data with a high compression ratio, and a residual
compression layer using wavelet transform and SPIHT encoding to efficiently
eliminate long-tail extreme errors introduced by the base compression layer. It
incorporates a feedback rate-control mechanism for both layers that adjusts
compression ratios to achieve the specified maximum error target. We evaluate
EBCC alongside other established compression methods on benchmarks related to
weather and climate science including error statistics, a case study on
primitive and derived variables near a hurricane, evaluation of the closure of
the global energy budget, and a Lagrangian air parcel trajectory simulation.
This is the first time that trajectory simulation is used to benchmark
compression methods. Our method concentrates most errors near zero, while
others tend to distribute errors uniformly within the error bound. EBCC
outperforms other methods in the benchmarks at relative error targets ranging
from 0.1% to 10% and achieves compression ratios from 15x to more than 300x. In
the energy budget closure and Lagrangian trajectory benchmarks, it can achieve
more than 100x compression while keeping errors within natural variability
derived from ERA5 uncertainty members. This verifies the effectiveness of EBCC
in creating heavily compressed weather and climate datasets suitable for
downstream applications. The source code of EBCC is available in
github.com/spcl/EBCC.

</details>


### [85] [Smart Sensor Placement: A Correlation-Aware Attribution Framework (CAAF) for Real-world Data Modeling](https://arxiv.org/abs/2510.22517)
*Sze Chai Leung,Di Zhou,H. Jane Bae*

Main category: cs.CE

TL;DR: 提出基于机器学习的特征归因框架CAAF用于确定最优传感器放置，经验证在实际动力系统中效果良好，优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 最优传感器放置对复杂系统很关键，传统特征归因方法处理高相关输入数据有困难。

Method: 提出相关感知归因框架（CAAF），在特征归因前增加聚类步骤以减少冗余、增强泛化性。

Result: CAAF在结构健康监测、翼型升力预测等实际动力系统中表现良好，优于其他因非线性动力学等因素表现不佳的方法。

Conclusion: CAAF能有效应用特征归因在现实环境中确定最优传感器放置。

Abstract: Optimal sensor placement (OSP) is critical for efficient, accurate
monitoring, control, and inference in complex real-world systems. We propose a
machine-learning-based feature attribution framework to identify OSP for the
prediction of quantities of interest. Feature attribution quantifies input
contributions to a model's output; however, it struggles with highly correlated
input data often encountered in real-world applications. To address this, we
propose a Correlation-Aware Attribution Framework (CAAF), which introduces a
clustering step before performing feature attribution to reduce redundancy and
enhance generalizability. We first illustrate the core principles of the
proposed framework through a series of validation cases, then demonstrate its
effectiveness in real-world dynamical systems, such as structural health
monitoring, airfoil lift prediction, and wall-normal velocity estimation for
turbulent channel flow. The results show that the CAAF outperforms alternative
approaches that typically struggle due to the presence of nonlinear dynamics,
chaotic behavior, and multi-scale interactions, and enables the effective
application of feature attribution for identifying OSP in real-world
environments.

</details>


### [86] [Capsule Network-Based Multimodal Fusion for Mortgage Risk Assessment from Unstructured Data Sources](https://arxiv.org/abs/2510.22987)
*Mahsa Tavakoli,Rohitash Chandra,Cristian Bravo*

Main category: cs.CE

TL;DR: 提出多模态深度学习框架用免费非结构化数据评估抵押风险，超单模态模型和基准融合策略。


<details>
  <summary>Details</summary>
Motivation: 传统抵押风险评估依赖专有、保密且昂贵的结构化金融数据，需新方法。

Method: 采用两阶段方法，单模态阶段确定各模态最佳模型，融合阶段引入FusionCapsNet；结合不同新闻类别情感分析，用GradCAM可视化。

Result: 多模态FusionCapsNet框架在AUC、部分AUC和F1分数上超单模态模型和基准融合策略。

Conclusion: 该框架在抵押风险评估的预测准确性和可解释性上有明显提升。

Abstract: Mortgage risk assessment traditionally relies on structured financial data,
which is often proprietary, confidential, and costly. In this study, we propose
a novel multimodal deep learning framework that uses cost-free, publicly
available, unstructured data sources, including textual information, images,
and sentiment scores, to generate credit scores that approximate commercial
scorecards. Our framework adopts a two-phase approach. In the unimodal phase,
we identify the best-performing models for each modality, i.e. BERT for text,
VGG for image data, and a multilayer perceptron for sentiment-based features.
In the fusion phase, we introduce the capsule-based fusion network
(FusionCapsNet), a novel fusion strategy inspired by capsule networks, but
fundamentally redesigned for multimodal integration. Unlike standard capsule
networks, our method adapts a specific mechanism in capsule networks to each
modality and restructures the fusion process to preserve spatial, contextual,
and modality-specific information. It also enables adaptive weighting so that
stronger modalities dominate without ignoring complementary signals.
  Our framework incorporates sentiment analysis across distinct news categories
to capture borrower and market dynamics and employs GradCAM-based
visualizations as an interpretability tool. These components are designed
features of the framework, while our results later demonstrate that they
effectively enrich contextual understanding and highlight the influential
factors driving mortgage risk predictions. Our results show that our multimodal
FusionCapsNet framework not only exceeds individual unimodal models but also
outperforms benchmark fusion strategies such as addition, concatenation, and
cross attention in terms of AUC, partial AUC, and F1 score, demonstrating clear
gains in both predictive accuracy and interpretability for mortgage risk
assessment.

</details>


### [87] [P1GPT: a multi-agent LLM workflow module for multi-modal financial information analysis](https://arxiv.org/abs/2510.23032)
*Chen-Che Lu,Yun-Cheng Chou,Teng-Ruei Chen*

Main category: cs.CE

TL;DR: 介绍P1GPT用于多模态金融信息分析和交易决策支持，回测效果好，结构化推理工作流是金融AI系统可扩展路径。


<details>
  <summary>Details</summary>
Motivation: 现有金融分析框架缺乏统一不同数据模态的连贯推理工作流，需改进。

Method: 引入P1GPT分层多智能体大语言模型框架，通过协调智能体通信和集成时间合成融合多方面见解。

Result: 在多模态数据集回测中，P1GPT实现了更高的累积和风险调整回报，低回撤，并提供透明因果解释。

Conclusion: 结构化推理工作流为可解释和可信的金融AI系统提供了可扩展路径。

Abstract: Recent advances in large language models (LLMs) have enabled multi-agent
reasoning systems capable of collaborative decision-making. However, in
financial analysis, most frameworks remain narrowly focused on either isolated
single-agent predictors or loosely connected analyst ensembles, and they lack a
coherent reasoning workflow that unifies diverse data modalities. We introduce
P1GPT, a layered multi-agent LLM framework for multi-modal financial
information analysis and interpretable trading decision support. Unlike prior
systems that emulate trading teams through role simulation, P1GPT implements a
structured reasoning pipeline that systematically fuses technical, fundamental,
and news-based insights through coordinated agent communication and
integration-time synthesis. Backtesting on multi-modal datasets across major
U.S. equities demonstrates that P1GPT achieves superior cumulative and
risk-adjusted returns, maintains low drawdowns, and provides transparent causal
rationales. These findings suggest that structured reasoning workflows, rather
than agent role imitation, offer a scalable path toward explainable and
trustworthy financial AI systems.

</details>


### [88] [GroupSHAP-Guided Integration of Financial News Keywords and Technical Indicators for Stock Price Prediction](https://arxiv.org/abs/2510.23112)
*Minjoo Kim,Jinwoong Kim,Sangjin Park*

Main category: cs.CE

TL;DR: 本文引入基于GRU并结合GroupSHAP的预测框架，降低计算负担并保留可解释性，在标普500指数预测中优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感量化方法忽略语境细微差别、可解释性有限，且SHAP计算成本高，不适用于大规模文本金融数据。

Method: 引入基于GRU的预测框架并结合GroupSHAP，用FinBERT嵌入新闻文章，聚类成语义组，用GroupSHAP衡量组对股价变动的贡献，将组级SHAP变量作为预测模型输入。

Result: 在2024年标普500指数一日前预测中，相比无GroupSHAP机制的基准模型，MAE降低32.2%，RMSE降低40.5%。

Conclusion: GroupSHAP首次用于新闻驱动的金融预测，分组情感表示能同时提升可解释性和预测性能。

Abstract: Recent advances in finance-specific language models such as FinBERT have
enabled the quantification of public sentiment into index-based measures, yet
compressing diverse linguistic signals into single metrics overlooks contextual
nuances and limits interpretability. To address this limitation, explainable AI
techniques, particularly SHAP (SHapley Additive Explanations), have been
employed to identify influential features. However, SHAP's computational cost
grows exponentially with input features, making it impractical for large-scale
text-based financial data. This study introduces a GRU-based forecasting
framework enhanced with GroupSHAP, which quantifies contributions of
semantically related keyword groups rather than individual tokens,
substantially reducing computational burden while preserving interpretability.
We employed FinBERT to embed news articles from 2015 to 2024, clustered them
into coherent semantic groups, and applied GroupSHAP to measure each group's
contribution to stock price movements. The resulting group-level SHAP variables
across multiple topics were used as input features for the prediction model.
Empirical results from one-day-ahead forecasting of the S&P 500 index
throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE
and a 40.5% reduction in RMSE compared with benchmark models without the
GroupSHAP mechanism. This research presents the first application of GroupSHAP
in news-driven financial forecasting, showing that grouped sentiment
representations simultaneously enhance interpretability and predictive
performance.

</details>


### [89] [Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms](https://arxiv.org/abs/2510.23166)
*Philippe Martin Wyder,Judah Goldfeder,Alexey Yermakov,Yue Zhao,Stefano Riva,Jan P. Williams,David Zoro,Amy Sara Rude,Matteo Tomasetto,Joe Germany,Joseph Bakarji,Georg Maierhofer,Miles Cranmer,J. Nathan Kutz*

Main category: cs.CE

TL;DR: 论文指出科学机器学习缺乏标准化基准的问题，提出通用任务框架（CTF），并通过对两个非线性系统进行基准测试验证其效用，还将围绕真实数据集开展竞赛，愿景是实现标准化评估。


<details>
  <summary>Details</summary>
Motivation: 机器学习快速发展，但缺乏标准化、客观的基准，导致诸多问题，影响可重复性、资源分配和科学进展。

Method: 提出通用任务框架（CTF），包含精选数据集和特定任务指标，对Kuramoto - Sivashinsky和Lorenz两个非线性系统进行基准测试，还将围绕全球海面温度数据集开展竞赛。

Result: 通过对两个非线性系统的基准测试，表明CTF能揭示方法的优势、局限性以及对特定问题和目标的适用性。

Conclusion: 长期愿景是用隐藏测试集的标准化评估取代临时比较，提高科学机器学习的严谨性和可重复性。

Abstract: Machine learning (ML) is transforming modeling and control in the physical,
engineering, and biological sciences. However, rapid development has outpaced
the creation of standardized, objective benchmarks - leading to weak baselines,
reporting bias, and inconsistent evaluations across methods. This undermines
reproducibility, misguides resource allocation, and obscures scientific
progress. To address this, we propose a Common Task Framework (CTF) for
scientific machine learning. The CTF features a curated set of datasets and
task-specific metrics spanning forecasting, state reconstruction, and
generalization under realistic constraints, including noise and limited data.
Inspired by the success of CTFs in fields like natural language processing and
computer vision, our framework provides a structured, rigorous foundation for
head-to-head evaluation of diverse algorithms. As a first step, we benchmark
methods on two canonical nonlinear systems: Kuramoto-Sivashinsky and Lorenz.
These results illustrate the utility of the CTF in revealing method strengths,
limitations, and suitability for specific classes of problems and diverse
objectives. Next, we are launching a competition around a global real world sea
surface temperature dataset with a true holdout dataset to foster community
engagement. Our long-term vision is to replace ad hoc comparisons with
standardized evaluations on hidden test sets that raise the bar for rigor and
reproducibility in scientific ML.

</details>


### [90] [DRO-Based Computation Offloading and Trajectory Design for Low-Altitude Networks](https://arxiv.org/abs/2510.23202)
*Guanwang Jiang,Ziye Jia,Can Cui,Lijun He,Qiuming Zhu,Qihui Wu*

Main category: cs.CE

TL;DR: 提出低海拔网络架构，构建不确定性集，设计算法优化卸载决策和无人机轨迹，仿真显示算法在平衡最坏延迟和鲁棒性上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 低海拔网络中不确定任务大小和无人机高移动性对服务质量保障构成挑战。

Method: 提出UAV和HAP协作的LAN架构，构建不确定性集，制定分布鲁棒优化问题，设计算法迭代求解内外层问题，外层用优化工具包，内层用Benders分解。

Result: 仿真结果显示所提算法在平衡最坏延迟和鲁棒性上优于传统优化方法。

Conclusion: 所提分布鲁棒计算卸载和轨迹优化算法能有效应对低海拔网络挑战。

Abstract: The low-altitude networks (LANs) integrating unmanned aerial vehicles (UAVs)
and high-altitude platforms (HAPs) have become a promising solution for the
rising computation demands. However, the uncertain task sizes and high mobility
of UAVs pose great challenges to guarantee the quality of service. To address
these issues, we propose an LAN architecture where UAVs and HAPs
collaboratively provide computation offloading for ground users. Moreover, the
uncertainty sets are constructed to characterize the uncertain task size, and a
distributionally robust optimization problem is formulated to minimize the
worst-case delay by jointly optimizing the offloading decisions and UAV
trajectories. To solve the mixed-integer min-max optimization problem, we
design the distributionally robust computation offloading and trajectories
optimization algorithm. Specifically, the original problem is figured out by
iteratively solving the outerlayer and inner-layer problems. The convex
outer-layer problem with probability distributions is solved by the
optimization toolkit. As for the inner-layer mixed-integer problem, we employ
the Benders decomposition. The decoupled master problem concerning the binary
offloading decisions is solved by the integer solver, and UAV trajectories in
the sub-problem are optimized via the successive convex approximation.
Simulation results show the proposed algorithm outperforms traditional
optimization methods in balancing the worst-case delay and robustness.

</details>


### [91] [Learning the PTM Code through a Coarse-to-Fine, Mechanism-Aware Framework](https://arxiv.org/abs/2510.23492)
*Jingjie Zhang,Hanqun Cao,Zijun Gao,Yu Wang,Shaoning Li,Jun Xu,Cheng Tan,Jun Zhu,Chang-Yu Hsieh,Chunbin Gu,Pheng Ann Heng*

Main category: cs.CE

TL;DR: 提出COMPASS - PTM框架统一残基水平PTM分析与酶 - 底物分配，在多基准测试中表现出色，能学习蛋白调控语法。


<details>
  <summary>Details</summary>
Motivation: 破译将PTM修饰位点与其催化酶联系起来的密码，这是理解细胞信号传导和疾病的核心未解决问题。

Method: 引入机制感知、从粗到细的学习框架COMPASS - PTM，整合蛋白语言模型的进化表征、理化先验和串扰感知提示机制。

Result: 在多个蛋白质组规模基准测试中达到新的最优性能，如多标签位点预测F1提高122%，零样本酶分配提高54%，能恢复典型激酶基序并预测疾病相关PTM重连。

Conclusion: COMPASS - PTM将统计学习与生化机制相结合，统一了位点和酶水平的预测，学习了蛋白质调控和信号传导的语法。

Abstract: Post-translational modifications (PTMs) form a combinatorial "code" that
regulates protein function, yet deciphering this code - linking modified sites
to their catalytic enzymes - remains a central unsolved problem in
understanding cellular signaling and disease. We introduce COMPASS-PTM, a
mechanism-aware, coarse-to-fine learning framework that unifies residue-level
PTM profiling with enzyme-substrate assignment. COMPASS-PTM integrates
evolutionary representations from protein language models with physicochemical
priors and a crosstalk-aware prompting mechanism that explicitly models
inter-PTM dependencies. This design allows the model to learn biologically
coherent patterns of cooperative and antagonistic modifications while
addressing the dual long-tail distribution of PTM data. Across multiple
proteome-scale benchmarks, COMPASS-PTM establishes new state-of-the-art
performance, including a 122% relative F1 improvement in multi-label site
prediction and a 54% gain in zero-shot enzyme assignment. Beyond accuracy, the
model demonstrates interpretable generalization, recovering canonical kinase
motifs and predicting disease-associated PTM rewiring caused by missense
variants. By bridging statistical learning with biochemical mechanism,
COMPASS-PTM unifies site-level and enzyme-level prediction into a single
framework that learns the grammar underlying protein regulation and signaling.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [92] [Determining Window Sizes using Species Estimation for Accurate Process Mining over Streams](https://arxiv.org/abs/2510.22314)
*Christian Imenkamp,Martin Kabierski,Hendrik Reiter,Matthias Weidlich,Wilhelm Hasselbring,Agnes Koschmider*

Main category: cs.DB

TL;DR: 提出一种通过调整窗口大小解决流式过程挖掘问题的新方法，评估显示优于静态窗口大小方法。


<details>
  <summary>Details</summary>
Motivation: 静态固定窗口大小在流式过程挖掘中，因过程动态变化和概念漂移，导致分析结果不准确和有偏差。

Method: 基于生物多样性研究中样本代表性估计器动态确定合适的窗口大小。

Result: 在真实数据集上的评估结果显示，新方法在准确性和对概念漂移的鲁棒性方面优于采用静态窗口大小的现有方法。

Conclusion: 所提出的动态调整窗口大小的方法能有效解决流式过程挖掘中静态窗口大小带来的问题。

Abstract: Streaming process mining deals with the real-time analysis of event streams.
A common approach for it is to adopt windowing mechanisms that select event
data from a stream for subsequent analysis. However, the size of these windows
denotes a crucial parameter, as it influences the representativeness of the
window content and, by extension, of the analysis results. Given that process
dynamics are subject to changes and potential concept drift, a static, fixed
window size leads to inaccurate representations that introduce bias in the
analysis. In this work, we present a novel approach for streaming process
mining that addresses these limitations by adjusting window sizes.
Specifically, we dynamically determine suitable window sizes based on
estimators for the representativeness of samples as developed for species
estimation in biodiversity research. Evaluation results on real-world data sets
show improvements over existing approaches that adopt static window sizes in
terms of accuracy and robustness to concept drifts.

</details>


### [93] [Dynamically Detect and Fix Hardness for Efficient Approximate Nearest Neighbor Search](https://arxiv.org/abs/2510.22316)
*Zhiyuan Hua,Qiji Mo,Zebin Yao,Lixiao Cui,Xiaoguang Liu,Gang Wang,Zijing Wei,Xinyu Liu,Tianxiao Tang,Shaozhi Liu,Lin Qu*

Main category: cs.DB

TL;DR: 论文指出现有图基近似最近邻搜索（ANNS）方法RoarGraph存在不足，提出Escape Hardness指标评估图结构质量，分两阶段动态修复图缺陷区域，实验表明方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有图基ANNS算法在分布外（OOD）场景下精度下降问题，以及RoarGraph方法的局限性。

Method: 提出Escape Hardness指标评估查询周围图结构质量，将图搜索分为两阶段，分别用Reachability Fixing（RFix）和Neighboring Graph Defects Fixing（NGFix）动态修复图缺陷区域。

Result: 在真实数据集上实验，方法比RoarGraph和HNSW更快，OOD查询在99%召回率下搜索速度分别快2.25倍和6.88倍，索引构建比RoarGraph快2.35 - 9.02倍。

Conclusion: 所提方法在OOD场景的ANNS中表现优于现有方法，能提高搜索速度和加速索引构建。

Abstract: Approximate Nearest Neighbor Search (ANNS) has become a fundamental component
in many real-world applications. Among various ANNS algorithms, graph-based
methods are state-of-the-art. However, ANNS often suffers from a significant
drop in accuracy for certain queries, especially in Out-of-Distribution (OOD)
scenarios. To address this issue, a recent approach named RoarGraph constructs
a bipartite graph between the base data and historical queries to bridge the
gap between two different distributions. However, it suffers from some
limitations: (1) Building a bipartite graph between two distributions lacks
theoretical support, resulting in the query distribution not being effectively
utilized by the graph index. (2) Requires a sufficient number of historical
queries before graph construction and suffers from high construction times. (3)
When the query workload changes, it requires reconstruction to maintain high
search accuracy.
  In this paper, we first propose Escape Hardness, a metric to evaluate the
quality of the graph structure around the query. Then we divide the graph
search into two stages and dynamically identify and fix defective graph regions
in each stage based on Escape Hardness. (1) From the entry point to the
vicinity of the query. We propose Reachability Fixing (RFix), which enhances
the navigability of some key nodes. (2) Searching within the vicinity of the
query. We propose Neighboring Graph Defects Fixing (NGFix) to improve graph
connectivity in regions where queries are densely distributed. The results of
extensive experiments show that our method outperforms other state-of-the-art
methods on real-world datasets, achieving up to 2.25x faster search speed for
OOD queries at 99% recall compared with RoarGraph and 6.88x faster speed
compared with HNSW. It also accelerates index construction by 2.35-9.02x
compared to RoarGraph.

</details>


### [94] [A Survey of Data Agents: Emerging Paradigm or Overstated Hype?](https://arxiv.org/abs/2510.23587)
*Yizhang Zhu,Liangwei Wang,Chenyu Yang,Xiaotian Lin,Boyan Li,Wei Zhou,Xinyu Liu,Zhangyang Peng,Tianqi Luo,Yu Li,Chengliang Chai,Chong Chen,Shimin Di,Ju Fan,Ji Sun,Nan Tang,Fugee Tsung,Jiannan Wang,Chenglin Wu,Yanwei Xu,Shaolei Zhang,Yong Zhang,Xuanhe Zhou,Guoliang Li,Yuyu Luo*

Main category: cs.DB

TL;DR: 论文针对数据代理术语模糊问题，引入分层分类法，回顾现有研究，分析关键跨越与技术差距，给出未来发展路线图。


<details>
  <summary>Details</summary>
Motivation: 当前数据代理术语模糊，导致用户期望不匹配、问责困难和行业发展受阻，需要清晰界定。

Method: 受SAE J3016标准启发，引入六层的分层分类法，按自主性递增对现有研究进行结构化回顾。

Result: 明确了数据代理的能力边界和责任分配，分析出关键进化跨越和技术差距，尤其是L2到L3的过渡。

Conclusion: 展望了主动、生成式数据代理的未来发展。

Abstract: The rapid advancement of large language models (LLMs) has spurred the
emergence of data agents--autonomous systems designed to orchestrate Data + AI
ecosystems for tackling complex data-related tasks. However, the term "data
agent" currently suffers from terminological ambiguity and inconsistent
adoption, conflating simple query responders with sophisticated autonomous
architectures. This terminological ambiguity fosters mismatched user
expectations, accountability challenges, and barriers to industry growth.
Inspired by the SAE J3016 standard for driving automation, this survey
introduces the first systematic hierarchical taxonomy for data agents,
comprising six levels that delineate and trace progressive shifts in autonomy,
from manual operations (L0) to a vision of generative, fully autonomous data
agents (L5), thereby clarifying capability boundaries and responsibility
allocation. Through this lens, we offer a structured review of existing
research arranged by increasing autonomy, encompassing specialized data agents
for data management, preparation, and analysis, alongside emerging efforts
toward versatile, comprehensive systems with enhanced autonomy. We further
analyze critical evolutionary leaps and technical gaps for advancing data
agents, especially the ongoing L2-to-L3 transition, where data agents evolve
from procedural execution to autonomous orchestration. Finally, we conclude
with a forward-looking roadmap, envisioning the advent of proactive, generative
data agents.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [95] [When Agents are Powerful: Black Hole Search in Time-Varying Graphs](https://arxiv.org/abs/2510.22309)
*Tanvir Kaur,Ashish Saxena*

Main category: cs.DC

TL;DR: 文章研究图中的黑洞搜索问题，增强代理能力（全局通信和1跳可见性）以更高效解决动态图中的该问题。


<details>
  <summary>Details</summary>
Motivation: 以往在任意动态图中解决黑洞搜索问题采用面对面通信约束大，增加所需代理数量，需更高效解决方案。

Method: 通过两种方式增强代理能力，一是赋予全局通信能力，二是配备1跳可见性。

Result: 增强代理能力后能得到动态图中黑洞搜索问题更高效的解决方案。

Conclusion: 增强代理的全局通信和1跳可见性能力有助于更高效解决动态图中的黑洞搜索问题。

Abstract: A black hole is a harmful node in a graph that destroys any resource entering
it, making its identification a critical task. In the \emph{Black Hole Search
(BHS)} problem, a team of agents operates on a graph $G$ with the objective
that at least one agent must survive and correctly identify an edge incident to
the black hole. Prior work has addressed BHS in arbitrary dynamic graphs under
the restrictive \emph{face-to-face} communication, where agents can exchange
information only when co-located. This constraint significantly increases the
number of agents required to solve the problem. In this work, we strengthen the
capabilities of agents in two ways: (i) granting them \emph{global
communication}, enabling interaction regardless of location, and (ii) equipping
them with \emph{1-hop visibility}, allowing each agent to observe its immediate
neighborhood. These enhancements lead to more efficient solutions for the BHS
problem in dynamic graphs.

</details>


### [96] [Separation of Unconscious Robots with Obstructed Visibility](https://arxiv.org/abs/2510.22434)
*Prajyot Pyati,Navjot Kaur,Saswata Jana,Adri Bhattacharya,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: 研究不透明无意识移动机器人模型下的分离问题，提出无碰撞算法在O(n)轮内解决问题。


<details>
  <summary>Details</summary>
Motivation: 以往研究将机器人建模为透明，本文研究不透明情况下机器人的分离问题，即机器人从任意初始配置分离成同心半圆。

Method: 提出一种无碰撞算法，在半同步调度器下运行，机器人仅就一个坐标轴达成一致且不知机器人总数n。

Result: 算法能在O(n)个周期内解决分离问题。

Conclusion: 所提算法可在不透明无意识移动机器人模型下有效解决分离问题。

Abstract: We study a recently introduced \textit{unconscious} mobile robot model, where
each robot is associated with a \textit{color}, which is visible to other
robots but not to itself. The robots are autonomous, anonymous, oblivious and
silent, operating in the Euclidean plane under the conventional
\textit{Look-Compute-Move} cycle. A primary task in this model is the
\textit{separation problem}, where unconscious robots sharing the same color
must separate from others, forming recognizable geometric shapes such as
circles, points, or lines. All prior works model the robots as
\textit{transparent}, enabling each to know the positions and colors of all
other robots. In contrast, we model the robots as \textit{opaque}, where a
robot can obstruct the visibility of two other robots, if it lies on the line
segment between them. Under this obstructed visibility, we consider a variant
of the separation problem in which robots, starting from any arbitrary initial
configuration, are required to separate into concentric semicircles. We present
a collision-free algorithm that solves the separation problem under a
semi-synchronous scheduler in $O(n)$ epochs, where $n$ is the number of robots.
The robots agree on one coordinate axis but have no knowledge of $n$.

</details>


### [97] [Rethinking Inference Placement for Deep Learning across Edge and Cloud Platforms: A Multi-Objective Optimization Perspective and Future Directions](https://arxiv.org/abs/2510.22909)
*Zongshun Zhang,Ibrahim Matta*

Main category: cs.DC

TL;DR: 随着物联网和移动设备发展，边缘智能应用广泛，但边缘设备难支持深度学习模型，现有优化和卸载方法存在通信问题，本文调研相关方法以实现多目标优化。


<details>
  <summary>Details</summary>
Motivation: 边缘设备难以支持大型复杂深度学习模型，且现有模型优化和卸载方法存在传输瓶颈和数据泄露风险，需平衡准确性、计算延迟、传输延迟和隐私问题。

Method: 通过研究模型卸载方法和模型适应技术，考虑推理延迟、数据隐私和资源货币成本的多目标优化。

Result: 对现有模型卸载方法和模型适应技术进行了梳理和分析。

Conclusion: 通过调研梳理了模型卸载和适应技术对多目标优化的影响。

Abstract: Edge intelligent applications like VR/AR and language model based chatbots
have become widespread with the rapid expansion of IoT and mobile devices.
However, constrained edge devices often cannot serve the increasingly large and
complex deep learning (DL) models. To mitigate these challenges, researchers
have proposed optimizing and offloading partitions of DL models among user
devices, edge servers, and the cloud. In this setting, users can take advantage
of different services to support their intelligent applications. For example,
edge resources offer low response latency. In contrast, cloud platforms provide
low monetary cost computation resources for computation-intensive workloads.
However, communication between DL model partitions can introduce transmission
bottlenecks and pose risks of data leakage. Recent research aims to balance
accuracy, computation delay, transmission delay, and privacy concerns. They
address these issues with model compression, model distillation, transmission
compression, and model architecture adaptations, including internal
classifiers. This survey contextualizes the state-of-the-art model offloading
methods and model adaptation techniques by studying their implication to a
multi-objective optimization comprising inference latency, data privacy, and
resource monetary cost.

</details>


### [98] [Bayes-Split-Edge: Bayesian Optimization for Constrained Collaborative Inference in Wireless Edge Systems](https://arxiv.org/abs/2510.23503)
*Fatemeh Zahra Safaeipour,Jacob Chakareski,Morteza Hashemi*

Main category: cs.DC

TL;DR: 本文研究无线边缘网络中的协作推理问题，提出Bayes - Split - Edge解决方案，评估显示其在降低评估成本、收敛速度和性能上优于多个基线方法。


<details>
  <summary>Details</summary>
Motivation: 移动边缘设备计算和能源资源有限，需在能量和延迟约束下完成推理任务，研究协作推理问题以优化推理性能。

Method: 提出Bayes - Split - Edge解决方案，利用贝叶斯优化进行协作拆分推理，联合优化传输功率和神经网络拆分点，采用新型混合采集函数。

Result: 在多个数据集和模型上评估，相比标准贝叶斯优化，评估成本最多降低2.4倍，收敛接近线性，优于多个基线方法。

Conclusion: 所提框架为边缘计算系统中的无线拆分推理提供了样本高效、约束感知的优化解决方案。

Abstract: Mobile edge devices (e.g., AR/VR headsets) typically need to complete timely
inference tasks while operating with limited on-board computing and energy
resources. In this paper, we investigate the problem of collaborative inference
in wireless edge networks, where energy-constrained edge devices aim to
complete inference tasks within given deadlines. These tasks are carried out
using neural networks, and the edge device seeks to optimize inference
performance under energy and delay constraints. The inference process can be
split between the edge device and an edge server, thereby achieving
collaborative inference over wireless networks. We formulate an inference
utility optimization problem subject to energy and delay constraints, and
propose a novel solution called Bayes-Split-Edge, which leverages Bayesian
optimization for collaborative split inference over wireless edge networks. Our
solution jointly optimizes the transmission power and the neural network split
point. The Bayes-Split-Edge framework incorporates a novel hybrid acquisition
function that balances inference task utility, sample efficiency, and
constraint violation penalties. We evaluate our approach using the VGG19 model
on the ImageNet-Mini dataset, and Resnet101 on Tiny-ImageNet, and real-world
mMobile wireless channel datasets. Numerical results demonstrate that
Bayes-Split-Edge achieves up to 2.4x reduction in evaluation cost compared to
standard Bayesian optimization and achieves near-linear convergence. It also
outperforms several baselines, including CMA-ES, DIRECT, exhaustive search, and
Proximal Policy Optimization (PPO), while matching exhaustive search
performance under tight constraints. These results confirm that the proposed
framework provides a sample-efficient solution requiring maximum 20 function
evaluations and constraint-aware optimization for wireless split inference in
edge computing systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [99] [An Optimal Density Bound for Discretized Point Patrolling](https://arxiv.org/abs/2510.22060)
*Ahan Mishra*

Main category: cs.DS

TL;DR: 解决离散点巡逻问题的密度界猜想，证明密度至少约1.264的实例可调度，此为最优；对竹花园修剪问题实现9/7近似算法。


<details>
  <summary>Details</summary>
Motivation: 解决离散点巡逻问题对应猜想，改进竹花园修剪问题的近似算法。

Method: 通过证明来解决猜想，设计算法解决竹花园修剪问题。

Result: 证明离散点巡逻问题密度至少约1.264的实例可调度，改进了当前最佳密度界；对竹花园修剪问题实现9/7近似算法，优于当前最佳的4/3。

Conclusion: 离散点巡逻问题的新密度界是最优的，竹花园修剪问题的新近似算法更优。

Abstract: The pinwheel problem is a real-time scheduling problem that asks, given $n$
tasks with periods $a_i \in \mathbb{N}$, whether it is possible to infinitely
schedule the tasks, one per time unit, such that every task $i$ is scheduled in
every interval of $a_i$ units. We study a corresponding version of this packing
problem in the covering setting, stylized as the discretized point patrolling
problem in the literature. Specifically, given $n$ tasks with periods $a_i$,
the problem asks whether it is possible to assign each day to a task such that
every task $i$ is scheduled at \textit{most} once every $a_i$ days. The density
of an instance in either case is defined as the sum of the inverses of task
periods. Recently, the long-standing $5/6$ density bound conjecture in the
packing setting was resolved affirmatively. The resolution means any instance
with density at least $5/6$ is schedulable. A corresponding conjecture was made
in the covering setting and renewed multiple times in more recent work. We
resolve this conjecture affirmatively by proving that every discretized point
patrolling instance with density at least $\sum_{i = 0}^{\infty} 1/(2^i + 1)
\approx 1.264$ is schedulable. This significantly improves upon the current
best-known density bound of 1.546 and is, in fact, optimal. We also study the
bamboo garden trimming problem, an optimization variant of the pinwheel
problem. Specifically, given $n$ growth rates with values $h_i \in \mathbb{N}$,
the objective is to minimize the maximum height of a bamboo garden with the
corresponding growth rates, where we are allowed to trim one bamboo tree to
height zero per time step. We achieve an efficient $9/7$-approximation
algorithm for this problem, improving on the current best known approximation
factor of $4/3$.

</details>


### [100] [(Approximate) Matrix Multiplication via Convolutions](https://arxiv.org/abs/2510.22193)
*Kevin Pratt,Yahel Uffenheimer,Omri Weinstein*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A longstanding open question in algorithm design is whether "combinatorial"
matrix multiplication algorithms -- avoiding Strassen-like divide-and-conquer
-- can achieve truly subcubic runtime $n^{3-\delta}$. We present an
$O(n^{2.89})$-time exact algorithm, which only sums convolutions in
$\mathbb{Z}_m^k$ (multivariate polynomial multiplications) via FFT, building on
the work of Cohn, Kleinberg, Szegedy and Umans (CKSU'05). While the algorithm
avoids recursion, the asymptotic speedup arises only for impractically large
matrices.
  Motivated by practical applications, we use this baseline to develop a new
framework for fast approximate matrix multiplication (AMM), via low-degree
approximations of the CKSU polynomials. We show that combining the
aforementioned algorithm with black-box linear sketching already breaks the
longstanding linear speed-accuracy tradeoff for AMM (Sarlos'06,
Clarkson-Woodruff'13 ,Pagh'11, Cohn-Lewis'00), achieving
$\frac{1}{r^{1.1}}\|\mathbf{A}\|_F^2\|\mathbf{B}\|_F^2$ error in
$O(rn^2)$-time.
  Our main result is a low-degree approximation scheme for the CKSU
polynomials, based on a Fourier-concentration lemma, yielding substantially
smaller error in the distributional setting where $\mathbf{A},\mathbf{B}$ come
from an i.i.d product-distribution; For random Gaussian matrices, this
practical AMM algorithm attains smaller error than the best rank-$r$ SVD of the
output matrix $\mathbf{A}\mathbf{B}$, in time $O(rn^2)$. This is a substantial
improvement over iterative Krylov subspace methods for low-rank approximation.
Our theoretical and empirical results suggest the possibility of replacing
MatMuls with sums of convolutions in LLM training and inference.

</details>


### [101] [Johnson-Lindenstrauss Lemma Beyond Euclidean Geometry](https://arxiv.org/abs/2510.22401)
*Chengyuan Deng,Jie Gao,Kevin Lu,Feng Luo,Cheng Xin*

Main category: cs.DS

TL;DR: 本文将 Johnson - Lindenstrauss 引理从欧几里得空间扩展到非欧几里得空间，提出两种互补方法并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决 Johnson - Lindenstrauss 引理在非欧几里得数据应用上的局限性。

Method: 一是将 JL 变换应用于伪欧几里得空间向量；二是将对称空心相异矩阵表示为广义幂距离矩阵。

Result: 理论结果能基于数据偏离欧几里得几何的程度进行细粒度性能分析，在合成和真实数据集上验证了方法有效性。

Conclusion: 可将 JL 引理扩展到非欧几里得设置，使更多类型数据能进行有实际意义的降维。

Abstract: The Johnson-Lindenstrauss (JL) lemma is a cornerstone of dimensionality
reduction in Euclidean space, but its applicability to non-Euclidean data has
remained limited. This paper extends the JL lemma beyond Euclidean geometry to
handle general dissimilarity matrices that are prevalent in real-world
applications. We present two complementary approaches: First, we show the JL
transform can be applied to vectors in pseudo-Euclidean space with signature
$(p,q)$, providing theoretical guarantees that depend on the ratio of the $(p,
q)$ norm and Euclidean norm of two vectors, measuring the deviation from
Euclidean geometry. Second, we prove that any symmetric hollow dissimilarity
matrix can be represented as a matrix of generalized power distances, with an
additional parameter representing the uncertainty level within the data. In
this representation, applying the JL transform yields multiplicative
approximation with a controlled additive error term proportional to the
deviation from Euclidean geometry. Our theoretical results provide fine-grained
performance analysis based on the degree to which the input data deviates from
Euclidean geometry, making practical and meaningful reduction in dimensionality
accessible to a wider class of data. We validate our approaches on both
synthetic and real-world datasets, demonstrating the effectiveness of extending
the JL lemma to non-Euclidean settings.

</details>


### [102] [On Integer Programs That Look Like Paths](https://arxiv.org/abs/2510.22430)
*Marcin Briański,Alexandra Lassota,Kristýna Pekárková,Michał Pilipczuk,Janina Reuter*

Main category: cs.DS

TL;DR: 研究约束矩阵有路径结构的整数规划，证明系数有界时判定可行性是NP难的。


<details>
  <summary>Details</summary>
Motivation: 整数规划一般是NP难的，需识别可多项式或FPT时间求解的子类。

Method: 从3 - SAT问题归约证明约束矩阵系数有界时判定整数规划可行性是NP难的。

Result: 即使约束矩阵A所有系数有界为8，判定这类整数规划可行性仍是NP难的。

Conclusion: 有星状结构整数规划有高效算法，此路径结构结果令人意外。

Abstract: Solving integer programs of the form $\min \{\mathbf{x} \mid A\mathbf{x} =
\mathbf{b}, \mathbf{l} \leq \mathbf{x} \leq \mathbf{u}, \mathbf{x} \in
\mathbb{Z}^n \}$ is, in general, $\mathsf{NP}$-hard. Hence, great effort has
been put into identifying subclasses of integer programs that are solvable in
polynomial or $\mathsf{FPT}$ time. A common scheme for many of these integer
programs is a star-like structure of the constraint matrix. The arguably
simplest form that is not a star is a path. We study integer programs where the
constraint matrix $A$ has such a path-like structure: every non-zero
coefficient appears in at most two consecutive constraints. We prove that even
if all coefficients of $A$ are bounded by 8, deciding the feasibility of such
integer programs is $\mathsf{NP}$-hard via a reduction from 3-SAT. Given the
existence of efficient algorithms for integer programs with star-like
structures and a closely related pattern where the sum of absolute values is
column-wise bounded by 2 (hence, there are at most two non-zero entries per
column of size at most 2), this hardness result is surprising.

</details>


### [103] [Tree Embedding in High Dimensions: Dynamic and Massively Parallel](https://arxiv.org/abs/2510.22490)
*Gramoz Goranci,Shaofeng H. -C. Jiang,Peter Kiss,Qihao Kong,Yi Qian,Eva Szilagyi*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Tree embedding has been a fundamental method in algorithm design with wide
applications. We focus on the efficiency of building tree embedding in various
computational settings under high-dimensional Euclidean $\mathbb{R}^d$. We
devise a new tree embedding construction framework that operates on an
arbitrary metric decomposition with bounded diameter, offering a tradeoff
between distortion and the locality of its algorithmic steps. This framework
works for general metric spaces and may be of independent interest beyond the
Euclidean setting. Using this framework, we obtain a dynamic algorithm that
maintains an $O_\epsilon(\log n)$-distortion tree embedding with update time
$\tilde O(n^\epsilon + d)$ subject to point insertions/deletions, and a
massively parallel algorithm that achieves $O_\epsilon(\log n)$-distortion in
$O(1)$ rounds and total space $\tilde O(n^{1 + \epsilon})$ (for constant
$\epsilon \in (0, 1)$). These new tree embedding results allow for a wide range
of applications. Notably, under a similar performance guarantee as in our tree
embedding algorithms, i.e., $\tilde O(n^\epsilon + d)$ update time and $O(1)$
rounds, we obtain $O_\epsilon(\log n)$-approximate dynamic and MPC algorithms
for $k$-median and earth-mover distance in $\mathbb{R}^d$.

</details>


### [104] [Generating pivot Gray codes for spanning trees of complete graphs in constant amortized time](https://arxiv.org/abs/2510.22662)
*Bowie Liu,Dennis Wong,Chan-Tong Lam,Sio-Kei Im*

Main category: cs.DS

TL;DR: 提出首个完全图生成树的枢轴格雷码，解决Knuth提出的开放问题，给出递归算法并扩展到一般图，还给出凯莱公式新证明。


<details>
  <summary>Details</summary>
Motivation: 解决Knuth在《计算机编程艺术》中提出的有关完全图生成树格雷码的开放问题。

Method: 采用递归算法，利用$O(n^2)$空间生成每个生成树。

Result: 能以常数均摊时间生成完全图生成树，扩展到一般图以$O(n^2)$时间生成，特定图类有优化时间。

Conclusion: 成功解决开放问题，算法可用于不同类型图生成树的格雷码生成，还给出凯莱公式新证明。

Abstract: We present the first known pivot Gray code for spanning trees of complete
graphs, listing all spanning trees such that consecutive trees differ by
pivoting a single edge around a vertex. This pivot Gray code thus addresses an
open problem posed by Knuth in The Art of Computer Programming, Volume 4
(Exercise 101, Section 7.2.1.6, [Knuth, 2011]), rated at a difficulty level of
46 out of 50, and imposes stricter conditions than existing revolving-door or
edge-exchange Gray codes for spanning trees of complete graphs. Our recursive
algorithm generates each spanning tree in constant amortized time using
$O(n^2)$ space. In addition, we provide a novel proof of Cayley's formula,
$n^{n-2}$, for the number of spanning trees in a complete graph, derived from
our recursive approach. We extend the algorithm to generate edge-exchange Gray
codes for general graphs with $n$ vertices, achieving $O(n^2)$ time per tree
using $O(n^2)$ space. For specific graph classes, the algorithm can be
optimized to generate edge-exchange Gray codes for spanning trees in constant
amortized time per tree for complete bipartite graphs, $O(n)$-amortized time
per tree for fan graphs, and $O(n)$-amortized time per tree for wheel graphs,
all using $O(n^2)$ space.

</details>


### [105] [Faster Negative-Weight Shortest Paths and Directed Low-Diameter Decompositions](https://arxiv.org/abs/2510.22721)
*Jason Li,Connor Mowry,Satish Rao*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a faster algorithm for low-diameter decompositions on directed
graphs, matching the $O(\log n\log\log n)$ loss factor from Bringmann, Fischer,
Haeupler, and Latypov (ICALP 2025) and improving the running time to
$O((m+n\log\log n)\log n\log\log n)$ in expectation. We then apply our faster
low-diameter decomposition to obtain an algorithm for negative-weight single
source shortest paths on integer-weighted graphs in $O((m+n\log\log
n)\log(nW)\log n\log\log n)$ time, a nearly log-factor improvement over the
algorithm of Bringmann, Cassis, and Fischer (FOCS 2023).

</details>


### [106] [$L_p$ Sampling in Distributed Data Streams with Applications to Adversarial Robustness](https://arxiv.org/abs/2510.22816)
*Honghao Lin,Zhao Song,David P. Woodruff,Shenghao Xie,Samson Zhou*

Main category: cs.DS

TL;DR: 本文解决分布式监控模型中所有 p≥1 的完美 L_p 采样问题，利用采样器实现 F_p 矩估计的抗攻击分布式监控协议，并应用于多个中心问题。


<details>
  <summary>Details</summary>
Motivation: 在分布式监控模型中，高效收集全局流的随机样本是强大的原语，解决完美 L_p 采样问题可助力下游任务。

Method: 提出算法解决完美 L_p 采样问题，利用采样器实现抗攻击分布式监控协议。

Result: 完美 L_p 采样算法通信复杂度为 k^(p - 1)·polylog(n) 位，F_p 矩估计算法通信复杂度为 (k^(p - 1)/ε^2)·polylog(n) 位，均达近似最优。

Conclusion: 解决了分布式监控模型中完美 L_p 采样问题，实现了多个中心问题的近最优抗攻击分布式协议。

Abstract: In the distributed monitoring model, a data stream over a universe of size
$n$ is distributed over $k$ servers, who must continuously provide certain
statistics of the overall dataset, while minimizing communication with a
central coordinator. In such settings, the ability to efficiently collect a
random sample from the global stream is a powerful primitive, enabling a wide
array of downstream tasks such as estimating frequency moments, detecting heavy
hitters, or performing sparse recovery. Of particular interest is the task of
producing a perfect $L_p$ sample, which given a frequency vector $f \in
\mathbb{R}^n$, outputs an index $i$ with probability
$\frac{f_i^p}{\|f\|_p^p}+\frac{1}{\mathrm{poly}(n)}$. In this paper, we resolve
the problem of perfect $L_p$ sampling for all $p\ge 1$ in the distributed
monitoring model. Specifically, our algorithm runs in $k^{p-1} \cdot
\mathrm{polylog}(n)$ bits of communication, which is optimal up to
polylogarithmic factors.
  Utilizing our perfect $L_p$ sampler, we achieve adversarially-robust
distributed monitoring protocols for the $F_p$ moment estimation problem, where
the goal is to provide a $(1+\varepsilon)$-approximation to
$f_1^p+\ldots+f_n^p$. Our algorithm uses
$\frac{k^{p-1}}{\varepsilon^2}\cdot\mathrm{polylog}(n)$ bits of communication
for all $p\ge 2$ and achieves optimal bounds up to polylogarithmic factors,
matching lower bounds by Woodruff and Zhang (STOC 2012) in the non-robust
setting. Finally, we apply our framework to achieve near-optimal adversarially
robust distributed protocols for central problems such as counting, frequency
estimation, heavy-hitters, and distinct element estimation.

</details>


### [107] [Hierarchical Exponential Search Via K-Spines](https://arxiv.org/abs/2510.22837)
*Bob Dong*

Main category: cs.DS

TL;DR: 引入树的k - 脊柱概念，并基于此提出O(klog dist)指数搜索算法。


<details>
  <summary>Details</summary>
Motivation: 解决树中的搜索问题，通过引入新的概念优化搜索算法。

Method: 引入k - 脊柱概念，以其为中心引导，主要沿脊柱搜索缩小目标范围，再递归处理小的组件。

Result: 提出了O(klog dist)指数搜索算法。

Conclusion: 基于k - 脊柱概念可设计有效的搜索算法。

Abstract: We introduce the concept of a k-spine of a tree. A k-spine is essentially a
path in the tree whose removal leaves only "less-bushy" components of a smaller
pathwidth. Using a k-spine as a central guide, we introduce an O(klog dist)
exponential search algorithm on a tree by searching mainly along the spine to
narrow down the target's vicinity and then recursively handling the smaller
components.

</details>


### [108] [Testing forbidden order-pattern properties on hypergrids](https://arxiv.org/abs/2510.22845)
*Harish Chandramouleeswaran,Ilan Newman,Tomer Pelleg,Nithin Varma*

Main category: cs.DS

TL;DR: 本文研究函数π - 自由性测试，为二维网格上k = 3的情况设计了自适应单侧测试器，给出一般下界，对单调模式提出非自适应测试器，还设计了新的抗擦除单调性测试器，最后指出当前技术无法为长度为4的模式提供亚线性查询测试器。


<details>
  <summary>Details</summary>
Motivation: 在k > 2时，函数π - 自由性测试的研究较少，需对高维网格上的模式自由性进行系统研究。

Method: 设计自适应和非自适应测试器，证明测试器的查询复杂度下界，利用新的抗擦除单调性测试器作为关键组件。

Result: 为d = 2和k = 3设计自适应单侧测试器，复杂度为O(n^{4/5+o(1)})；给出k = 3的一般下界；对单调模式给出非自适应测试器；设计新的抗擦除单调性测试器；指出当前技术局限性。

Conclusion: 对高维网格上的模式自由性测试取得进展，包括测试器设计和复杂度分析，同时明确当前技术的局限。

Abstract: We study testing $\pi$-freeness of functions $f:[n]^d\to\mathbb{R}$, where
$f$ is $\pi$-free if there there are no $k$ indices $x_1\prec\cdots\prec x_k\in
[n]^d$ such that $f(x_i)<f(x_j)$ and $\pi(i) < \pi(j)$ for all $i,j \in [k]$,
where $\prec$ is the natural partial order over $[n]^d$. Given
$\epsilon\in(0,1)$, $\epsilon$-testing $\pi$-freeness asks to distinguish
$\pi$-free functions from those which are $\epsilon$-far -- meaning at least
$\epsilon n^d$ function values must be modified to make it $\pi$-free. While
$k=2$ coincides with monotonicity testing, far less is known for $k>2$.
  We initiate a systematic study of pattern freeness on higher-dimensional
grids. For $d=2$ and all permutations of size $k=3$, we design an adaptive
one-sided tester with query complexity $O(n^{4/5+o(1)})$. We also prove general
lower bounds for $k=3$: every nonadaptive tester requires $\Omega(n)$ queries,
and every adaptive tester requires $\Omega(\sqrt{n})$ queries, yielding the
first super-logarithmic lower bounds for $\pi$-freeness. For the monotone
patterns $\pi=(1,2,3)$ and $(3,2,1)$, we present a nonadaptive tester with
polylogarithmic query complexity, giving an exponential separation between
monotone and nonmonotone patterns (unlike the one-dimensional case).
  A key ingredient in our $\pi$-freeness testers is new erasure-resilient
($\delta$-ER) $\epsilon$-testers for monotonicity over $[n]^d$ with query
complexity $O(\log^{O(d)}n/(\epsilon(1-\delta)))$, where $0<\delta<1$ is an
upper bound on the fraction of erasures. Prior ER testers worked only for
$\delta=O(\epsilon/d)$. Our nonadaptive monotonicity tester is nearly optimal
via a matching lower bound due to Pallavoor, Raskhodnikova, and Waingarten
(Random Struct. Algorithms, 2022). Finally, we show that current techniques
cannot yield sublinear-query testers for patterns of length $4$ even on
two-dimensional hypergrids.

</details>


### [109] [Multi-Way Co-Ranking: Index-Space Partitioning of Sorted Sequences Without Merge](https://arxiv.org/abs/2510.22882)
*Amit Joshi*

Main category: cs.DS

TL;DR: 提出多向共同排名的无合并算法，用索引空间二分查找，复杂度为 $O(\log(\sum_t n_t)\,\log m)$ 时间和 $O(m)$ 空间，并讨论应用。


<details>
  <summary>Details</summary>
Motivation: 解决多向共同排名问题，即计算分割索引使得各排序序列前缀段共含 $K$ 个元素。

Method: 将两列表共同排名扩展到任意 $m$，在索引空间进行二分查找，维护序列边界收敛到全局边界，不进行多路合并或值空间搜索。

Result: 算法运行时间为 $O(\log(\sum_t n_t)\,\log m)$，空间为 $O(m)$，与 $K$ 无关。

Conclusion: 通过交换论证证明算法正确性，并讨论了在分布式分数背包、并行合并分区和多流连接等方面的应用。

Abstract: We present a merge-free algorithm for multi-way co-ranking, the problem of
computing cut indices $i_1,\dots,i_m$ that partition each of the $m$ sorted
sequences such that all prefix segments together contain exactly $K$ elements.
Our method extends two-list co-ranking to arbitrary $m$, maintaining
per-sequence bounds that converge to a consistent global frontier without
performing any multi-way merge or value-space search. Rather, we apply binary
search to \emph{index-space}. The algorithm runs in $O(\log(\sum_t n_t)\,\log
m)$ time and $O(m)$ space, independent of $K$. We prove correctness via an
exchange argument and discuss applications to distributed fractional knapsack,
parallel merge partitioning, and multi-stream joins.
  Keywords: Co-ranking \sep partitioning \sep Merge-free algorithms \sep
Index-space optimization \sep Selection and merging \sep Data structures

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [110] [Conditional Recall](https://arxiv.org/abs/2510.21904)
*Christoph Schlegel,Xinyuan Sun*

Main category: cs.GT

TL;DR: 文章设想2026年强生推出记忆消除药物X，以哈佛法学院学生面临是否使用药物的抉择引入，探讨允许主体承诺遗忘信息的技术的博弈论含义及应用。


<details>
  <summary>Details</summary>
Motivation: 研究允许主体承诺遗忘信息的技术在博弈论方面的意义和潜在应用。

Method: 通过构建设想情境，以记忆消除药物引发的抉择为切入点进行探讨。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: In the neon-lit nights of 2026, Johnson \& Johnson unveiled X. A pill, not
larger than a snowflake, that promised a tempest of change. This miraculous
drug didn't just allow people to cherry-pick memories to erase from their
minds, it could also leave a reminder of this erasure in the minds of those who
ingested it.
  Amidst the iconic red-bricked walls of Harvard Law, you, with books in one
hand and dreams in the other, are on a mission. You are not just another
student; you carry the hope of revolutionizing the archaic chambers of the
legal world. Each night, as you pore over the tomes of law, you wonder what
greatness society can achieve.
  On a cold evening, your phone buzzes. It's Dex, your old college friend
turned underground dealer. His message is simple: ``Got X. Special price for
you.'' The temptation swirls around you. Would you trade the lessons of the
past for a clearer, yet incomplete future? The decision rests in your hands.
  We explore the game theoretic implications of a technology (such as TEEs)
that allows agents to commit to forget information and discuss several
applications.

</details>


### [111] [Rational Adversaries and the Maintenance of Fragility: A Game-Theoretic Theory of Rational Stagnation](https://arxiv.org/abs/2510.22232)
*Daisuke Hirota*

Main category: cs.GT

TL;DR: 本文解释合作系统中的‘理性停滞’现象，从囚徒困境出发构建模型，分析不同战略制度，推广效用形式并证明稳定性，还给出应用示例。


<details>
  <summary>Details</summary>
Motivation: 解释合作系统常处于次优稳定状态的‘理性停滞’现象。

Method: 从囚徒困境入手，通过效用变换和互认比率构建模型，进行Bellman式分析，附录中推广效用形式。

Result: 得出脆弱合作带，划分出即时破坏、理性停滞和干预放弃三种战略制度，证明推广后效用形式的稳定性。

Conclusion: 表明对抗性理性可故意维持脆弱性，框架具有鲁棒性，可应用于社交媒体算法和政治信任等领域。

Abstract: Cooperative systems often remain in persistently suboptimal yet stable
states. This paper explains such "rational stagnation" as an equilibrium
sustained by a rational adversary whose utility follows the principle of
potential loss, $u_{D} = U_{ideal} - U_{actual}$. Starting from the Prisoner's
Dilemma, we show that the transformation $u_{i}' = a\,u_{i} + b\,u_{j}$ and the
ratio of mutual recognition $w = b/a$ generate a fragile cooperation band
$[w_{\min},\,w_{\max}]$ where both (C,C) and (D,D) are equilibria. Extending to
a dynamic model with stochastic cooperative payoffs $R_{t}$ and intervention
costs $(C_{c},\,C_{m})$, a Bellman-style analysis yields three strategic
regimes: immediate destruction, rational stagnation, and intervention
abandonment. The appendix further generalizes the utility to a
reference-dependent nonlinear form and proves its stability under reference
shifts, ensuring robustness of the framework. Applications to social-media
algorithms and political trust illustrate how adversarial rationality can
deliberately preserve fragility.

</details>


### [112] [Learning Local Stackelberg Equilibria from Repeated Interactions with a Learning Agent](https://arxiv.org/abs/2510.22471)
*Nivasini Ananthakrishnan,Yuval Dagan,Kunhe Yang*

Main category: cs.GT

TL;DR: 研究委托人与使用基于均值学习算法的代理之间的重复博弈，提出在平滑分析框架下计算局部Stackelberg均衡的PTAS算法，证明算法运行时间与代理动作空间大小成多项式关系，与(1/epsilon)成指数关系且该依赖不可避免。


<details>
  <summary>Details</summary>
Motivation: 探究委托人在与学习代理的重复交互中如何最大化自身效用，且现有计算全局Stackelberg值的方法计算复杂度高。

Method: 将重点转移到计算局部Stackelberg均衡，在平滑分析框架下引入算法来寻找epsilon近似的局部Stackelberg均衡。

Result: 得到一个多项式时间近似方案（PTAS）算法，算法运行时间与代理动作空间大小成多项式关系，但与(1/epsilon)成指数关系。

Conclusion: 证明算法与(1/epsilon)的指数依赖关系是不可避免的。

Abstract: Motivated by the question of how a principal can maximize its utility in
repeated interactions with a learning agent, we study repeated games between an
principal and an agent employing a mean-based learning algorithm. Prior work
has shown that computing or even approximating the global Stackelberg value in
similar settings can require an exponential number of rounds in the size of the
agent's action space, making it computationally intractable. In contrast, we
shift focus to the computation of local Stackelberg equilibria and introduce an
algorithm that, within the smoothed analysis framework, constitutes a
Polynomial Time Approximation Scheme (PTAS) for finding an epsilon-approximate
local Stackelberg equilibrium. Notably, the algorithm's runtime is polynomial
in the size of the agent's action space yet exponential in (1/epsilon) - a
dependency we prove to be unavoidable.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [113] [Improving E-commerce Search with Category-Aligned Retrieval](https://arxiv.org/abs/2510.21711)
*Rauf Aliev*

Main category: cs.IR

TL;DR: 提出CARS系统提升电商搜索相关性，用新方法创建可训练类别原型，评估显示有效但盲目应用有局限，需新集成策略。


<details>
  <summary>Details</summary>
Motivation: 解决传统电商搜索系统中用户查询与产品目录的语义鸿沟问题。

Method: 提出CARS系统，先预测产品类别再提升该类别产品，用查询嵌入创建可训练类别原型，用all - MiniLM - L6 - v2和text - embedding - ada - 002评估。

Result: OpenAI模型使Top - 3类别预测准确率从43.8%提升到83.2%，但盲目应用类别提升会影响搜索相关性指标。

Conclusion: 该方法有价值，需置信度感知和自适应的集成策略。

Abstract: Traditional e-commerce search systems often struggle with the semantic gap
between user queries and product catalogs. In this paper, we propose a
Category-Aligned Retrieval System (CARS) that improves search relevance by
first predicting the product category from a user's query and then boosting
products within that category. We introduce a novel method for creating
"Trainable Category Prototypes" from query embeddings. We evaluate this method
with two models: a lightweight all-MiniLM-L6-v2 and OpenAI's
text-embedding-ada-002. Our offline evaluation shows this method is highly
effective, with the OpenAI model increasing Top-3 category prediction accuracy
from a zero-shot baseline of 43.8% to 83.2% after training. The end-to-end
simulation, however, highlights the limitations of blindly applying category
boosts in a complex retrieval pipeline: while accuracy is high, naive
integration can negatively affect search relevance metrics such as nDCG@10. We
argue that this is partly due to dataset-specific ambiguities (e.g., polysemous
queries in the Amazon ESCI corpus) and partly due to the sensitivity of
retrieval systems to over-constraining filters. Crucially, these results do not
diminish the value of the approach; rather, they emphasize the need for
confidence-aware and adaptive integration strategies.

</details>


### [114] [DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling](https://arxiv.org/abs/2510.21712)
*Hao Sun,Zile Qiao,Bo Wang,Guoxin Chen,Yingyan Hou,Yong Jiang,Pengjun Xie,Fei Huang,Yan Zhang*

Main category: cs.IR

TL;DR: 本文提出 DecoupleSearch 框架解决 Agentic RAG 面临的挑战，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: Agentic RAG 存在步骤依赖、中间推理步骤缺乏监督、候选空间大等挑战，需改进。

Method: 提出 DecoupleSearch 框架，用双价值模型解耦规划和搜索过程，构建推理树，利用蒙特卡罗树搜索评估步骤质量，推理时用分层束搜索结合双价值模型优化候选。

Result: 在不同参数规模的策略模型上进行大量实验，证明了方法的有效性。

Conclusion: DecoupleSearch 框架能有效解决 Agentic RAG 面临的挑战。

Abstract: Retrieval-Augmented Generation (RAG) systems have emerged as a pivotal
methodology for enhancing Large Language Models (LLMs) through the dynamic
integration of external knowledge. To further improve RAG's flexibility,
Agentic RAG introduces autonomous agents into the workflow. However, Agentic
RAG faces several challenges: (1) the success of each step depends on both
high-quality planning and accurate search, (2) the lack of supervision for
intermediate reasoning steps, and (3) the exponentially large candidate space
for planning and searching. To address these challenges, we propose
DecoupleSearch, a novel framework that decouples planning and search processes
using dual value models, enabling independent optimization of plan reasoning
and search grounding. Our approach constructs a reasoning tree, where each node
represents planning and search steps. We leverage Monte Carlo Tree Search to
assess the quality of each step. During inference, Hierarchical Beam Search
iteratively refines planning and search candidates with dual value models.
Extensive experiments across policy models of varying parameter sizes,
demonstrate the effectiveness of our method.

</details>


### [115] [asLLR: LLM based Leads Ranking in Auto Sales](https://arxiv.org/abs/2510.21713)
*Yin Sun,Yiwen Liu,Junjie Song,Chenyu Zhang,Xinyuan Zhang,Lingjie Liu,Siqi Chen,Yuji Cao*

Main category: cs.IR

TL;DR: 本文提出基于大语言模型的汽车销售潜在客户排名方法 asLLR，实验显示其在建模、CTR 提升和实际销售中均优于传统方法，为商业智能和决策提供工具。


<details>
  <summary>Details</summary>
Motivation: 传统 CTR 预测技术难以处理 CRM 系统中自然语言特征的复杂信息，限制了其在销售潜在客户排名中的有效性，需要新方法提升商业智能和决策能力。

Method: 引入 asLLR，在仅解码器的大语言模型架构中集成 CTR 损失和问答（QA）损失，同时对表格和自然语言特征进行建模。

Result: 实验表明 asLLR 能有效建模商业数据集，AUC 达 0.8127，比传统 CTR 估计方法高 0.0231；用于提取文本特征时提升 CTR 模型 0.0058；在线 A/B 测试中，相比传统方法使销售量提高约 9.5%。

Conclusion: asLLR 为商业智能和运营决策提供了有价值的工具。

Abstract: In the area of commercial auto sales system, high-quality lead score
sequencing determines the priority of a sale's work and is essential for
optimizing the efficiency of the sales system. Since CRM (Customer Relationship
Management) system contains plenty of textual interaction features between
sales and customers, traditional techniques such as Click Through Rate (CTR)
prediction struggle with processing the complex information inherent in natural
language features, which limits their effectiveness in sales lead ranking.
Bridging this gap is critical for enhancing business intelligence and
decision-making. Recently, the emergence of large language models (LLMs) has
opened new avenues for improving recommendation systems, this study introduces
asLLR (LLM-based Leads Ranking in Auto Sales), which integrates CTR loss and
Question Answering (QA) loss within a decoder-only large language model
architecture. This integration enables the simultaneous modeling of both
tabular and natural language features. To verify the efficacy of asLLR, we
constructed an innovative dataset derived from the customer lead pool of a
prominent new energy vehicle brand, with 300,000 training samples and 40,000
testing samples. Our experimental results demonstrate that asLLR effectively
models intricate patterns in commercial datasets, achieving the AUC of 0.8127,
surpassing traditional CTR estimation methods by 0.0231. Moreover, asLLR
enhances CTR models when used for extracting text features by 0.0058. In
real-world sales scenarios, after rigorous online A/B testing, asLLR increased
the sales volume by about 9.5% compared to the traditional method, providing a
valuable tool for business intelligence and operational decision-making.

</details>


### [116] [Practice on Long Behavior Sequence Modeling in Tencent Advertising](https://arxiv.org/abs/2510.21714)
*Xian Hu,Ming Yue,Zhixiang Feng,Junwei Pan,Junjie Zhai,Ximei Wang,Xinrui Miao,Qian Li,Xun Liu,Shangyu Zhang,Letian Wang,Hua Lu,Zijian Zeng,Chen Cai,Wei Wang,Fei Xiong,Pengfei Xiong,Jintao Zhang,Zhiyuan Wu,Chunhui Zhang,Anan Liu,Jiulong You,Chao Deng,Yuekui Yang,Shudong Huang,Dapeng Liu,Haijie Gu*

Main category: cs.IR

TL;DR: 为解决广告领域用户行为稀疏问题，构建统一商业行为轨迹，提出两阶段长序列建模方法，在腾讯广告平台取得显著GMV提升。


<details>
  <summary>Details</summary>
Motivation: 广告领域用户行为稀疏，单领域数据难构建长序列，需跨领域整合数据，但存在特征分类差距、特征字段干扰和目标干扰等挑战。

Method: 两阶段框架：搜索阶段设计分层硬搜索和基于解耦嵌入的软搜索；序列建模阶段引入解耦侧信息时间兴趣网络、目标解耦位置编码和目标解耦SASRec、堆叠TIN。

Result: 在腾讯大规模广告平台部署，微信频道GMV整体提升4.22%，微信朋友圈GMV整体提升1.96%。

Conclusion: 所提出的两阶段长序列建模方法有效，能解决跨领域整合带来的挑战，提升广告平台GMV。

Abstract: Long-sequence modeling has become an indispensable frontier in recommendation
systems for capturing users' long-term preferences. However, user behaviors
within advertising domains are inherently sparse, posing a significant barrier
to constructing long behavioral sequences using data from a single advertising
domain alone. This motivates us to collect users' behaviors not only across
diverse advertising scenarios, but also beyond the boundaries of the
advertising domain into content domains-thereby constructing unified commercial
behavior trajectories. This cross-domain or cross-scenario integration gives
rise to the following challenges: (1) feature taxonomy gaps between distinct
scenarios and domains, (2) inter-field interference arising from irrelevant
feature field pairs, and (3) target-wise interference in temporal and semantic
patterns when optimizing for different advertising targets. To address these
challenges, we propose several practical approaches within the two-stage
framework for long-sequence modeling. In the first (search) stage, we design a
hierarchical hard search method for handling complex feature taxonomy
hierarchies, alongside a decoupled embedding-based soft search to alleviate
conflicts between attention mechanisms and feature representation. In the
second (sequence modeling) stage, we introduce: (a) Decoupled Side Information
Temporal Interest Networks (TIN) to mitigate inter-field conflicts; (b)
Target-Decoupled Positional Encoding and Target-Decoupled SASRec to address
target-wise interference; and (c) Stacked TIN to model high-order behavioral
correlations. Deployed in production on Tencent's large-scale advertising
platforms, our innovations delivered significant performance gains: an overall
4.22% GMV lift in WeChat Channels and an overall 1.96% GMV increase in WeChat
Moments.

</details>


### [117] [Words to Waves: Emotion-Adaptive Music Recommendation System](https://arxiv.org/abs/2510.21724)
*Apoorva Chavali,Reeve Menezes*

Main category: cs.IR

TL;DR: 本文提出一种新的音乐推荐框架，利用实时情绪状态推荐歌曲，实验显示能提升情绪相关性。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统常忽略情绪上下文，依赖历史收听模式或静态情绪标签，因此需要改进。

Method: 采用改进的Wide and Deep Learning架构，用基于Transformer的嵌入捕捉用户文本描述中的情绪上下文，微调以预测效价 - 唤醒度情感维度，架构的深层泛化未见情绪模式，浅层通过交叉特征记忆用户 - 情绪和情绪 - 流派关联。

Result: 个性化音乐选择对用户情绪有积极影响，情绪相关性显著提高。

Conclusion: 该音乐推荐框架能有效考虑情绪上下文，提升推荐的情绪相关性。

Abstract: Current recommendation systems often tend to overlook emotional context and
rely on historical listening patterns or static mood tags. This paper
introduces a novel music recommendation framework employing a variant of Wide
and Deep Learning architecture that takes in real-time emotional states
inferred directly from natural language as inputs and recommends songs that
closely portray the mood. The system captures emotional contexts from
user-provided textual descriptions by using transformer-based embeddings, which
were finetuned to predict the emotional dimensions of valence-arousal. The deep
component of the architecture utilizes these embeddings to generalize unseen
emotional patterns, while the wide component effectively memorizes user-emotion
and emotion-genre associations through cross-product features. Experimental
results show that personalized music selections positively influence the user's
emotions and lead to a significant improvement in emotional relevance.

</details>


### [118] [From Authors to Reviewers: Leveraging Rankings to Improve Peer Review](https://arxiv.org/abs/2510.21726)
*Weichen Wang,Chengchun Shi*

Main category: cs.IR

TL;DR: 文章讨论Su等人2025年的论文，提出利用审稿人排名信息评估论文质量的方法，结果显示结合审稿人和作者排名信息评估最准确。


<details>
  <summary>Details</summary>
Motivation: 近年来机器学习会议投稿数量快速增长，评审质量成为关注焦点。

Method: 提出利用审稿人排名信息而非作者排名信息的方法，并模拟2023年ICML会议投稿的评审数据。

Result: 纳入审稿人排名信息可显著提升论文质量评估，结合两者排名信息在多数场景评估最准确。

Conclusion: 结合审稿人和作者的排名信息能对投稿论文进行最准确的评估。

Abstract: This paper is a discussion of the 2025 JASA discussion paper by Su et al.
(2025). We would like to congratulate the authors on conducting a comprehensive
and insightful empirical investigation of the 2023 ICML ranking data. The
review quality of machine learning (ML) conferences has become a big concern in
recent years, due to the rapidly growing number of submitted manuscripts. In
this discussion, we propose an approach alternative to Su et al. (2025) that
leverages ranking information from reviewers rather than authors. We simulate
review data that closely mimics the 2023 ICML conference submissions. Our
results show that (i) incorporating ranking information from reviewers can
significantly improve the evaluation of each paper's quality, often
outperforming the use of ranking information from authors alone; and (ii)
combining ranking information from both reviewers and authors yields the most
accurate evaluation of submitted papers in most scenarios.

</details>


### [119] [Your Dense Retriever is Secretly an Expeditious Reasoner](https://arxiv.org/abs/2510.21727)
*Yichi Zhang,Jun Bai,Zhixin Cai,Shuhan Qin,Zhuofan Chen,Jinghua Guan,Wenge Rong*

Main category: cs.IR

TL;DR: 提出混合查询重写框架AdaQR，能减少推理成本并提升检索性能


<details>
  <summary>Details</summary>
Motivation: 稠密检索器处理推理密集型查询有困难，通用使用大语言模型改写查询计算成本高

Method: 提出Adaptive Query Reasoning (AdaQR)框架，通过Reasoner Router将查询导向快速稠密推理或深度大语言模型推理，Dense Reasoner在嵌入空间进行大语言模型式推理

Result: 在大规模检索基准BRIGHT上实验，AdaQR减少28%推理成本，检索性能提升7%

Conclusion: AdaQR能有效平衡效率和准确性，在减少推理成本的同时提升检索性能

Abstract: Dense retrievers enhance retrieval by encoding queries and documents into
continuous vectors, but they often struggle with reasoning-intensive queries.
Although Large Language Models (LLMs) can reformulate queries to capture
complex reasoning, applying them universally incurs significant computational
cost. In this work, we propose Adaptive Query Reasoning (AdaQR), a hybrid query
rewriting framework. Within this framework, a Reasoner Router dynamically
directs each query to either fast dense reasoning or deep LLM reasoning. The
dense reasoning is achieved by the Dense Reasoner, which performs LLM-style
reasoning directly in the embedding space, enabling a controllable trade-off
between efficiency and accuracy. Experiments on large-scale retrieval
benchmarks BRIGHT show that AdaQR reduces reasoning cost by 28% while
preserving-or even improving-retrieval performance by 7%.

</details>


### [120] [Modeling Bias Evolution in Fashion Recommender Systems: A System Dynamics Approach](https://arxiv.org/abs/2510.21728)
*Mahsa Goodarzi,M. Abdullah Canbaz*

Main category: cs.IR

TL;DR: 研究剖析时尚推荐系统中偏差激活和强化机制，发现归纳偏差影响更大，现有去偏策略需增强，应考虑更多因素确保公平。


<details>
  <summary>Details</summary>
Motivation: 推荐系统偏差扭曲用户体验、放大社会刻板印象，研究旨在探究时尚推荐系统中偏差机制。

Method: 采用动态建模方法，利用系统动力学建模和实验模拟分析偏差的时间演变和对系统性能的影响。

Result: 归纳偏差对系统结果影响比用户偏差大，现有去偏策略有一定效果但需加强。

Conclusion: 需推进去偏策略，考虑更多背景因素，在推荐系统设计中采取积极措施确保公平。

Abstract: Bias in recommender systems not only distorts user experience but also
perpetuates and amplifies existing societal stereotypes, particularly in
sectors like fashion e-commerce. This study employs a dynamic modeling approach
to scrutinize the mechanisms of bias activation and reinforcement within
Fashion Recommender Systems (FRS). By leveraging system dynamics modeling and
experimental simulations, we dissect the temporal evolution of bias and its
multifaceted impacts on system performance. Our analysis reveals that inductive
biases exert a more substantial influence on system outcomes than user biases,
suggesting critical areas for intervention. We demonstrate that while current
debiasing strategies, including data rebalancing and algorithmic
regularization, are effective to an extent, they require further enhancement to
comprehensively mitigate biases. This research underscores the necessity for
advancing these strategies and extending system boundaries to incorporate
broader contextual factors such as user demographics and item diversity, aiming
to foster inclusivity and fairness in FRS. The findings advocate for a
proactive approach in recommender system design to counteract bias propagation
and ensure equitable user experiences.

</details>


### [121] [CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known Document Corpora](https://arxiv.org/abs/2510.21729)
*Nathan Paull*

Main category: cs.IR

TL;DR: 介绍CustomIR框架用于无监督调整预训练语言嵌入模型，在企业数据集实验中提升检索效果，证明合成微调策略有效。


<details>
  <summary>Details</summary>
Motivation: 解决预训练嵌入模型应用于特定语料库时性能下降的问题。

Method: 引入CustomIR框架，利用大语言模型生成查询 - 文档对和硬负样本，无需人工标注。

Result: 在企业邮件和消息数据集实验中，小模型Recall@10最多提升2.3点，可与大模型性能媲美。

Conclusion: 有针对性的合成微调是提高特定领域性能的可扩展且经济高效的策略。

Abstract: Dense embedding models have become critical for modern information retrieval,
particularly in RAG pipelines, but their performance often degrades when
applied to specialized corpora outside their pre-training distribution. To
address thi we introduce \textbf{CustomIR}, a framework for unsupervised
adaptation of pre-trained language embedding models to domain-specific corpora
using synthetically generated query-document pairs. CustomIR leverages large
language models (LLMs) to create diverse queries grounded in a known target
corpus, paired with LLM-verified hard negatives, eliminating the need for
costly human annotation. Experiments on enterprise email and messaging datasets
show that CustomIR consistently improves retrieval effectiveness with small
models gaining up to 2.3 points in Recall@10. This performance increase allows
these small models to rival the performance of much larger alternatives,
allowing for cheaper RAG deployments. These results highlight that targeted
synthetic fine-tuning offers a scalable and cost-efficient strategy for
increasing domain-specific performance.

</details>


### [122] [TriMat: Context-aware Recommendation by Tri-Matrix Factorization](https://arxiv.org/abs/2510.21730)
*Hao Wang*

Main category: cs.IR

TL;DR: 本文利用三矩阵分解技术将上下文信息融入矩阵分解框架，实验证明该技术能有效提高准确性和公平性指标。


<details>
  <summary>Details</summary>
Motivation: 推荐系统领域许多研究课题未解决，如上下文感知推荐系统（CARS）在实际应用中进展不大。

Method: 利用三矩阵分解技术将上下文信息融入矩阵分解框架。

Result: 实验证明该技术能有效提高准确性和公平性指标。

Conclusion: 所采用的基于三矩阵分解技术的方法在CARS中有效。

Abstract: Search engine is the symbolic technology of Web 2.0, and many people used to
believe recommender systems is the new frontier of Web 3.0. In the past 10
years, with the advent of TikTok and similar apps, recommender systems has
materialized the vision of the machine learning pioneers. However, many
research topics of the field remain unfixed until today. One such topic is CARS
(Context-aware Recommender Systems) , which is largely a theoretical topic
without much advance in real-world applications. In this paper, we utilize
tri-matrix factorization technique to incorporate contextual information into
our matrix factorization framework, and prove that our technique is effective
in improving both the accuracy and fairness metrics in our experiments.

</details>


### [123] [Augmenting Researchy Questions with Sub-question Judgments](https://arxiv.org/abs/2510.21733)
*Jia-Huei Ju,Eugene Yang,Trevor Adriaanse,Andrew Yates*

Main category: cs.IR

TL;DR: 用Llama3.3 70B模型为Researchy Questions数据集的子问题添加LLM判断标签，用于训练检索模型。


<details>
  <summary>Details</summary>
Motivation: Researchy Questions数据集虽有查询后点击文档的标签，但子问题与相关文档无关联，需为子问题添加标签以支持训练检索模型。

Method: 使用Llama3.3 70B模型为Researchy Questions数据集中每个子问题添加LLM判断标签。

Result: 文中未提及具体结果。

Conclusion: 添加的子问题标签可作为资源用于训练更好支持复杂信息需求的检索模型。

Abstract: The Researchy Questions dataset provides about 100k question queries with
complex information needs that require retrieving information about several
aspects of a topic. Each query in ResearchyQuestions is associated with
sub-questions that were produced by prompting GPT-4. While ResearchyQuestions
contains labels indicating what documents were clicked after issuing the query,
there are no associations in the dataset between sub-questions and relevant
documents. In this work, we augment the Researchy Questions dataset with
LLM-judged labels for each sub-question using a Llama3.3 70B model. We intend
these sub-question labels to serve as a resource for training retrieval models
that better support complex information needs.

</details>


### [124] [From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text](https://arxiv.org/abs/2510.21737)
*Liangliang Zhang,Nandana Mihindukulasooriya,Niharika S. D'Souza,Sola Shirai,Sarthak Dash,Yao Ma,Horst Samulowitz*

Main category: cs.IR

TL;DR: 介绍首个用户请求驱动的数据产品基准DPBench，进行基线实验并指出研究机会。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于数据产品发现的基准，现有数据集不能满足收集多数据资产的需求，因此要填补这一空白。

Method: 系统地重新利用现有表 - 文本问答数据集，将相关表和段落聚类成连贯数据产品，生成跨数据源的分析请求，并通过多LLM评估验证基准质量。

Result: 基线实验验证了数据产品请求评估的可行性，揭示了当前局限性。

Conclusion: DPBench为自动数据产品发现研究带来新机会。

Abstract: Data products are reusable, self-contained assets designed for specific
business use cases. Automating their discovery and generation is of great
industry interest, as it enables discovery in large data lakes and supports
analytical Data Product Requests (DPRs). Currently, there is no benchmark
established specifically for data product discovery. Existing datasets focus on
answering single factoid questions over individual tables rather than
collecting multiple data assets for broader, coherent products. To address this
gap, we introduce DPBench, the first user-request-driven data product benchmark
over hybrid table-text corpora. Our framework systematically repurposes
existing table-text QA datasets by clustering related tables and passages into
coherent data products, generating professional-level analytical requests that
span both data sources, and validating benchmark quality through multi-LLM
evaluation. DPBench preserves full provenance while producing actionable,
analyst-like data product requests. Baseline experiments with hybrid retrieval
methods establish the feasibility of DPR evaluation, reveal current
limitations, and point to new opportunities for automatic data product
discovery research.
  Code and datasets are available at:
https://anonymous.4open.science/r/data-product-benchmark-BBA7/

</details>


### [125] [DiffGRM: Diffusion-based Generative Recommendation Model](https://arxiv.org/abs/2510.21805)
*Zhao Liu,Yichen Zhu,Yiqing Yang,Guoping Tang,Rui Huang,Qiang Luo,Xiao Lv,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: 论文指出生成式推荐中自回归模型处理语义 ID 存在问题，提出 DiffGRM 模型解决，实验显示效果提升。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐中自回归模型处理语义 ID 时存在内部一致性和跨位异质性问题，影响推荐效果。

Method: 提出 DiffGRM 模型，用掩码离散扩散模型替代自回归解码器，并在分词、训练和推理三方面进行定制。

Result: 在多个数据集上，相比强生成和判别式推荐基线有持续增益，NDCG@10 提高 6.9%-15.5%。

Conclusion: DiffGRM 模型能有效解决生成式推荐中自回归模型处理语义 ID 的问题，提升推荐性能。

Abstract: Generative recommendation (GR) is an emerging paradigm that represents each
item via a tokenizer as an n-digit semantic ID (SID) and predicts the next item
by autoregressively generating its SID conditioned on the user's history.
However, two structural properties of SIDs make ARMs ill-suited. First,
intra-item consistency: the n digits jointly specify one item, yet the
left-to-right causality trains each digit only under its prefix and blocks
bidirectional cross-digit evidence, collapsing supervision to a single causal
path. Second, inter-digit heterogeneity: digits differ in semantic granularity
and predictability, while the uniform next-token objective assigns equal weight
to all digits, overtraining easy digits and undertraining hard digits. To
address these two issues, we propose DiffGRM, a diffusion-based GR model that
replaces the autoregressive decoder with a masked discrete diffusion model
(MDM), thereby enabling bidirectional context and any-order parallel generation
of SID digits for recommendation. Specifically, we tailor DiffGRM in three
aspects: (1) tokenization with Parallel Semantic Encoding (PSE) to decouple
digits and balance per-digit information; (2) training with On-policy Coherent
Noising (OCN) that prioritizes uncertain digits via coherent masking to
concentrate supervision on high-value signals; and (3) inference with
Confidence-guided Parallel Denoising (CPD) that fills higher-confidence digits
first and generates diverse Top-K candidates. Experiments show consistent gains
over strong generative and discriminative recommendation baselines on multiple
datasets, improving NDCG@10 by 6.9%-15.5%. Code is available at
https://github.com/liuzhao09/DiffGRM.

</details>


### [126] [Unifying Inductive, Cross-Domain, and Multimodal Learning for Robust and Generalizable Recommendation](https://arxiv.org/abs/2510.21812)
*Chanyoung Chung,Kyeongryul Lee,Sunbin Park,Joyce Jiyoung Whang*

Main category: cs.IR

TL;DR: 本文提出MICRec框架，融合归纳建模、多模态引导和跨域迁移，实验表明其性能优于12个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统研究多关注单个方面，难以应对复杂推荐场景，需要统一框架来解决。

Method: 提出MICRec框架，在INMO的归纳骨干基础上，通过基于模态的聚合优化表达表示，利用重叠用户作为跨域锚点缓解数据稀疏性。

Result: MICRec在实验中优于12个基线模型，在训练数据有限的领域有显著提升。

Conclusion: MICRec是一个统一框架，能有效捕捉异构和不完整数据中的用户上下文和潜在偏好，实现鲁棒且可泛化的推荐。

Abstract: Recommender systems have long been built upon the modeling of interactions
between users and items, while recent studies have sought to broaden this
paradigm by generalizing to new users and items, incorporating diverse
information sources, and transferring knowledge across domains. Nevertheless,
these efforts have largely focused on individual aspects, hindering their
ability to tackle the complex recommendation scenarios that arise in daily
consumptions across diverse domains. In this paper, we present MICRec, a
unified framework that fuses inductive modeling, multimodal guidance, and
cross-domain transfer to capture user contexts and latent preferences in
heterogeneous and incomplete real-world data. Moving beyond the inductive
backbone of INMO, our model refines expressive representations through
modality-based aggregation and alleviates data sparsity by leveraging
overlapping users as anchors across domains, thereby enabling robust and
generalizable recommendation. Experiments show that MICRec outperforms 12
baselines, with notable gains in domains with limited training data.

</details>


### [127] [Development of an Automated Web Application for Efficient Web Scraping: Design and Implementation](https://arxiv.org/abs/2510.21831)
*Alok Dutta,Nilanjana Roy,Rhythm Sen,Sougata Dutta,Prabhat Das*

Main category: cs.IR

TL;DR: 本文设计并实现了一个面向非技术用户的自动化网页抓取应用，将抓取过程分为三个阶段，具备用户注册登录功能，提升了抓取效率并使数据提取更普及。


<details>
  <summary>Details</summary>
Motivation: 简化并优化非技术用户的网页抓取过程，让不同技术水平的用户都能方便地收集和管理数据。

Method: 将网页抓取任务分为抓取、提取和执行三个阶段，分别使用requests库、BeautifulSoup和正则表达式等工具，采用MongoDB存储数据，用Flask框架部署。

Result: 开发出一个用户友好的自动化网页抓取应用，用户无需技术专长即可操作。

Conclusion: 该方法在使网页抓取工具更易访问、高效和易用方面取得显著进展。

Abstract: This paper presents the design and implementation of a user-friendly,
automated web application that simplifies and optimizes the web scraping
process for non-technical users. The application breaks down the complex task
of web scraping into three main stages: fetching, extraction, and execution. In
the fetching stage, the application accesses target websites using the HTTP
protocol, leveraging the requests library to retrieve HTML content. The
extraction stage utilizes powerful parsing libraries like BeautifulSoup and
regular expressions to extract relevant data from the HTML. Finally, the
execution stage structures the data into accessible formats, such as CSV,
ensuring the scraped content is organized for easy use. To provide personalized
and secure experiences, the application includes user registration and login
functionalities, supported by MongoDB, which stores user data and scraping
history. Deployed using the Flask framework, the tool offers a scalable, robust
environment for web scraping. Users can easily input website URLs, define data
extraction parameters, and download the data in a simplified format, without
needing technical expertise. This automated tool not only enhances the
efficiency of web scraping but also democratizes access to data extraction by
empowering users of all technical levels to gather and manage data tailored to
their needs. The methodology detailed in this paper represents a significant
advancement in making web scraping tools accessible, efficient, and easy to use
for a broader audience.

</details>


### [128] [Temporal Graph Theoretic Analysis of Geopolitical Dynamics in the U.S. Entity List](https://arxiv.org/abs/2510.21962)
*Yunsen Lei,Kexin Bai,Quan Li,H. Howie Huang*

Main category: cs.IR

TL;DR: 本文引入新的时间图框架分析美国实体清单，揭示出口管制地缘政治动态模式。


<details>
  <summary>Details</summary>
Motivation: 美国实体清单在出口管制中重要但动态情况研究不足，需深入探究其背后地缘政治战略。

Method: 引入时间图框架，构建基于事件的数据集，将其建模为时间二分图，并开发多层次分析方法。

Result: 框架揭示了静态视角无法捕捉的升级、持续和协调等动态模式。

Conclusion: 时间图分析能为出口管制的地缘政治动态提供系统的计算洞察。

Abstract: Export controls have become one of America's most prominent tools of economic
statecraft. They aim to block rival countries' access to sensitive
technologies, safeguard U.S. supply chains, protect national security, and
shape geopolitical competition. Among various instruments, the U.S. Entity List
has emerged as the most salient, yet its dynamics remain underexplored. This
paper introduces a novel temporal graph framework that transforms the Entity
List documents from a static registry of foreign entities of concern into a
dynamic representation of geopolitical strategy. We construct the first
event-based dataset of U.S. government foreign entity designations and model
them as a temporal bipartite graph. Building on this representation, we develop
a multi-level analytical approach that reveals shifting roles, enforcement
strategy, and broader sanction ecosystems. Applied to 25 years of data, the
framework uncovers dynamic patterns of escalation, persistence, and
coordination that static views cannot capture. More broadly, our study
demonstrates how temporal graph analysis offers systematic computational
insights into the geopolitical dynamics of export controls.

</details>


### [129] [Multimodal Item Scoring for Natural Language Recommendation via Gaussian Process Regression with LLM Relevance Judgments](https://arxiv.org/abs/2510.22023)
*Yifan Liu,Qianfeng Wen,Jiazhou Liang,Mark Zhao,Justin Cui,Anton Korikov,Armin Torogh,Junyoung Kim,Scott Sanner*

Main category: cs.IR

TL;DR: 本文提出GPR - LLM用于自然语言推荐（NLRec），在多个数据集和LLM骨干上实验表明其优于多种基线方法，在有限大模型标注预算下高效有效。


<details>
  <summary>Details</summary>
Motivation: 现有NLRec方法使用密集检索（DR）计算物品相关性分数，存在以查询嵌入为中心的单峰评分函数问题，难以捕捉复杂数据中相关性评分函数的潜在多峰分布。

Method: 提出GPR - LLM，使用高斯过程回归（GPR）结合大语言模型（LLM）对部分候选段落的相关性判断。

Result: 在四个NLRec数据集和两个LLM骨干上实验，带RBF核的GPR - LLM能对多峰相关性评分函数建模，比简单单峰核（点积、余弦相似度）及DR、交叉编码器、基于点的LLM相关性评分等基线方法性能最高提升65%。

Conclusion: GPR - LLM在最小的LLM标注预算下为NLRec提供了一种高效且有效的方法。

Abstract: Natural Language Recommendation (NLRec) generates item suggestions based on
the relevance between user-issued NL requests and NL item description passages.
Existing NLRec approaches often use Dense Retrieval (DR) to compute item
relevance scores from aggregation of inner products between user request
embeddings and relevant passage embeddings. However, DR views the request as
the sole relevance label, thus leading to a unimodal scoring function centered
on the query embedding that is often a weak proxy for query relevance. To
better capture the potential multimodal distribution of the relevance scoring
function that may arise from complex NLRec data, we propose GPR-LLM that uses
Gaussian Process Regression (GPR) with LLM relevance judgments for a subset of
candidate passages. Experiments on four NLRec datasets and two LLM backbones
demonstrate that GPR-LLM with an RBF kernel, capable of modeling multimodal
relevance scoring functions, consistently outperforms simpler unimodal kernels
(dot product, cosine similarity), as well as baseline methods including DR,
cross-encoder, and pointwise LLM-based relevance scoring by up to 65%. Overall,
GPR-LLM provides an efficient and effective approach to NLRec within a minimal
LLM labeling budget.

</details>


### [130] [Massive Memorization with Hundreds of Trillions of Parameters for Sequential Transducer Generative Recommenders](https://arxiv.org/abs/2510.22049)
*Zhimin Chen,Chenyu Zhao,Ka Chun Mo,Yunjiang Jiang,Jane H. Lee,Shouwei Chen,Khushhall Chandra Mahajan,Ning Jiang,Kai Ren,Jinhui Li,Wen-Yun Yang*

Main category: cs.IR

TL;DR: 本文提出VISTA两阶段建模框架，可处理超长用户历史记录，降低成本并提升指标，已在行业平台部署。


<details>
  <summary>Details</summary>
Motivation: 现代大规模推荐系统依赖用户交互历史序列提升性能，但处理超长历史记录会带来延迟、QPS和GPU成本等工业可扩展性问题，现有模型无法解决。

Method: 提出VISTA两阶段建模框架，将传统目标注意力分解为用户历史总结和候选项目注意力两个阶段，缓存总结令牌嵌入作为序列特征用于下游训练和推理。

Result: 在离线和在线指标上取得显著改进，已成功部署在服务数十亿用户的行业领先推荐平台上。

Conclusion: VISTA框架可扩展到终身用户历史记录，同时保持下游训练和推理成本固定，对工业应用至关重要。

Abstract: Modern large-scale recommendation systems rely heavily on user interaction
history sequences to enhance the model performance. The advent of large
language models and sequential modeling techniques, particularly
transformer-like architectures, has led to significant advancements recently
(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories
(10k to 100k items) generally improves model performance, it also creates
significant challenges on latency, queries per second (QPS) and GPU cost in
industry-scale recommendation systems. Existing models do not adequately
address these industrial scalability issues. In this paper, we propose a novel
two-stage modeling framework, namely VIrtual Sequential Target Attention
(VISTA), which decomposes traditional target attention from a candidate item to
user history items into two distinct stages: (1) user history summarization
into a few hundred tokens; followed by (2) candidate item attention to those
tokens. These summarization token embeddings are then cached in storage system
and then utilized as sequence features for downstream model training and
inference. This novel design for scalability enables VISTA to scale to lifelong
user histories (up to one million items) while keeping downstream training and
inference costs fixed, which is essential in industry. Our approach achieves
significant improvements in offline and online metrics and has been
successfully deployed on an industry leading recommendation platform serving
billions of users.

</details>


### [131] [A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition](https://arxiv.org/abs/2510.22055)
*V Venktesh,Deepali Prabhu,Avishek Anand*

Main category: cs.IR

TL;DR: 本文指出先前自动事实核查工作对自然数值声明关注不足，现有基准存在证据相关性和时间泄漏问题，引入QuanTemp++数据集，评估关键声明分解范式的检索性能及对验证流程的影响。


<details>
  <summary>Details</summary>
Motivation: 现有自动事实核查工作未聚焦自然数值声明，且现有基准的证据收集方法存在问题，需更真实的检索设置以辅助自动方法开发。

Method: 引入QuanTemp++数据集，其证据通过近似模拟人类事实核查员的声明分解过程收集，确保无时间泄漏；评估关键声明分解范式的检索性能及其对验证流程结果的影响。

Result: 成功引入QuanTemp++数据集，并对关键声明分解范式的检索性能进行了评估，观察了其对验证流程结果的影响。

Conclusion: QuanTemp++数据集有助于创建更真实的检索设置，研究为自动数值事实核查提供了有价值的见解。

Abstract: Fact-checking numerical claims is critical as the presence of numbers provide
mirage of veracity despite being fake potentially causing catastrophic impacts
on society. The prior works in automatic fact verification do not primarily
focus on natural numerical claims. A typical human fact-checker first retrieves
relevant evidence addressing the different numerical aspects of the claim and
then reasons about them to predict the veracity of the claim. Hence, the search
process of a human fact-checker is a crucial skill that forms the foundation of
the verification process. Emulating a real-world setting is essential to aid in
the development of automated methods that encompass such skills. However,
existing benchmarks employ heuristic claim decomposition approaches augmented
with weakly supervised web search to collect evidences for verifying claims.
This sometimes results in less relevant evidences and noisy sources with
temporal leakage rendering a less realistic retrieval setting for claim
verification. Hence, we introduce QuanTemp++: a dataset consisting of natural
numerical claims, an open domain corpus, with the corresponding relevant
evidence for each claim. The evidences are collected through a claim
decomposition process approximately emulating the approach of human
fact-checker and veracity labels ensuring there is no temporal leakage. Given
this dataset, we also characterize the retrieval performance of key claim
decomposition paradigms. Finally, we observe their effect on the outcome of the
verification pipeline and draw insights. The code for data pipeline along with
link to data can be found at https://github.com/VenkteshV/QuanTemp_Plus

</details>


### [132] [Scaling Up Efficient Small Language Models Serving and Deployment for Semantic Job Search](https://arxiv.org/abs/2510.22101)
*Kayhan Behdin,Qingquan Song,Sriram Vasudevan,Jian Sheng,Xiaojing Ma,Z Zhou,Chuanrui Zhu,Guoyao Li,Chanh Nguyen,Sayan Ghosh,Hejian Sang,Ata Fatahi Baarzi,Sundara Raman Ramachandran,Xiaoqing Wang,Qing Lan,Vinay Y S,Qi Guo,Caleb Johnson,Zhipeng Wang,Fedor Borisyuk*

Main category: cs.IR

TL;DR: 介绍为语义搜索应用开发小语言模型（SLM）的经验与效率见解，通过模型和上下文压缩及服务基础设施优化，使系统吞吐量提升10倍。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在有严格延迟和吞吐量要求的行业应用中部署成本过高，需开发更高效模型。

Method: 采用模型压缩技术（如剪枝）减小模型大小，使用上下文压缩技术减少输入上下文长度，优化服务基础设施。

Result: 模型大小最多减少40%，输入上下文长度最多减少10倍，实际部署中系统吞吐量提升10倍。

Conclusion: 通过一系列技术和优化可在满足质量标准下大幅提升系统吞吐量。

Abstract: Large Language Models (LLMs) have demonstrated impressive quality when
applied to predictive tasks such as relevance ranking and semantic search.
However, deployment of such LLMs remains prohibitively expensive for industry
applications with strict latency and throughput requirements. In this work, we
present lessons and efficiency insights from developing a purely text-based
decoder-only Small Language Model (SLM) for a semantic search application at
LinkedIn. Particularly, we discuss model compression techniques such as pruning
that allow us to reduce the model size by up to $40\%$ while maintaining the
accuracy. Additionally, we present context compression techniques that allow us
to reduce the input context length by up to $10$x with minimal loss of
accuracy. Finally, we present practical lessons from optimizing the serving
infrastructure for deploying such a system on GPUs at scale, serving millions
of requests per second. Taken together, this allows us to increase our system's
throughput by $10$x in a real-world deployment, while meeting our quality bar.

</details>


### [133] [Hybrid-Vector Retrieval for Visually Rich Documents: Combining Single-Vector Efficiency and Multi-Vector Accuracy](https://arxiv.org/abs/2510.22215)
*Juyeon Kim,Geon Lee,Dongwon Choi,Taeuk Kim,Kijung Shin*

Main category: cs.IR

TL;DR: 提出HEAVEN两阶段混合向量框架解决视觉丰富文档检索的效率与准确性权衡问题，还引入ViMDOC基准，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视觉丰富文档检索方法存在单向量检索粗糙、多向量检索计算昂贵的问题，需解决效率与准确性的权衡。

Method: 提出HEAVEN两阶段框架，第一阶段用单向量方法在VS - Pages上检索候选页面，第二阶段用多向量方法重排候选并过滤查询令牌；引入ViMDOC基准。

Result: 在四个基准测试中，HEAVEN平均达到多向量模型Recall@1性能的99.87%，同时将每次查询计算量减少99.82%。

Conclusion: HEAVEN实现了视觉丰富文档检索的效率与准确性。

Abstract: Retrieval over visually rich documents is essential for tasks such as legal
discovery, scientific search, and enterprise knowledge management. Existing
approaches fall into two paradigms: single-vector retrieval, which is efficient
but coarse, and multi-vector retrieval, which is accurate but computationally
expensive. To address this trade-off, we propose HEAVEN, a two-stage
hybrid-vector framework. In the first stage, HEAVEN efficiently retrieves
candidate pages using a single-vector method over Visually-Summarized Pages
(VS-Pages), which assemble representative visual layouts from multiple pages.
In the second stage, it reranks candidates with a multi-vector method while
filtering query tokens by linguistic importance to reduce redundant
computations. To evaluate retrieval systems under realistic conditions, we also
introduce ViMDOC, the first benchmark for visually rich, multi-document, and
long-document retrieval. Across four benchmarks, HEAVEN attains 99.87% of the
Recall@1 performance of multi-vector models on average while reducing per-query
computation by 99.82%, achieving efficiency and accuracy. Our code and datasets
are available at: https://github.com/juyeonnn/HEAVEN

</details>


### [134] [PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading](https://arxiv.org/abs/2510.22242)
*Yutao Wu,Xiao Liu,Yunhao Feng,Jiale Ding,Xingjun Ma*

Main category: cs.IR

TL;DR: 介绍PaperAsk基准评估大语言模型在学术任务中的可靠性，发现模型存在可靠性问题并开发分类器识别不可靠输出。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于学术辅助，但可靠性评估不足，需系统评估。

Method: 引入PaperAsk基准，评估GPT - 4o、GPT - 5和Gemini - 2.5 - Flash，通过控制实验和人工分析。

Result: 模型在多参考查询引用检索、内容提取、论文发现等任务失败率高，不同模型有不同失败行为。

Conclusion: PaperAsk为提升基于大语言模型的学术辅助系统可靠性评估提供可复现和诊断框架。

Abstract: Large Language Models (LLMs) increasingly serve as research assistants, yet
their reliability in scholarly tasks remains under-evaluated. In this work, we
introduce PaperAsk, a benchmark that systematically evaluates LLMs across four
key research tasks: citation retrieval, content extraction, paper discovery,
and claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under
realistic usage conditions-via web interfaces where search operations are
opaque to the user. Through controlled experiments, we find consistent
reliability failures: citation retrieval fails in 48-98% of multi-reference
queries, section-specific content extraction fails in 72-91% of cases, and
topical paper discovery yields F1 scores below 0.32, missing over 60% of
relevant literature. Further human analysis attributes these failures to the
uncontrolled expansion of retrieved context and the tendency of LLMs to
prioritize semantically relevant text over task instructions. Across basic
tasks, the LLMs display distinct failure behaviors: ChatGPT often withholds
responses rather than risk errors, whereas Gemini produces fluent but
fabricated answers. To address these issues, we develop lightweight reliability
classifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk
provides a reproducible and diagnostic framework for advancing the reliability
evaluation of LLM-based scholarly assistance systems.

</details>


### [135] [Tools are under-documented: Simple Document Expansion Boosts Tool Retrieval](https://arxiv.org/abs/2510.22670)
*Xuan Lu,Haohang Huang,Rui Meng,Yaohui Jin,Wenjun Zeng,Xiaoyu Shen*

Main category: cs.IR

TL;DR: 介绍Tool - DE基准和框架及两个模型Tool - Embed和Tool - Rank，通过文档扩展提升工具检索性能，取得新SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有工具检索受不完整和异构工具文档阻碍，需解决该问题。

Method: 设计可扩展文档扩展管道，利用LLM生成、验证和完善工具文档，开发Tool - Embed和Tool - Rank两个模型。

Result: 文档扩展显著提升检索性能，Tool - Embed和Tool - Rank在ToolRet和Tool - DE上取得新SOTA结果。

Conclusion: 指出LLM驱动文档扩展的前景和局限，Tool - DE及相关模型为工具检索未来研究奠定基础。

Abstract: Large Language Models (LLMs) have recently demonstrated strong capabilities
in tool use, yet progress in tool retrieval remains hindered by incomplete and
heterogeneous tool documentation. To address this challenge, we introduce
Tool-DE, a new benchmark and framework that systematically enriches tool
documentation with structured fields to enable more effective tool retrieval,
together with two dedicated models, Tool-Embed and Tool-Rank. We design a
scalable document expansion pipeline that leverages both open- and
closed-source LLMs to generate, validate, and refine enriched tool profiles at
low cost, producing large-scale corpora with 50k instances for embedding-based
retrievers and 200k for rerankers. On top of this data, we develop two models
specifically tailored for tool retrieval: Tool-Embed, a dense retriever, and
Tool-Rank, an LLM-based reranker. Extensive experiments on ToolRet and Tool-DE
demonstrate that document expansion substantially improves retrieval
performance, with Tool-Embed and Tool-Rank achieving new state-of-the-art
results on both benchmarks. We further analyze the contribution of individual
fields to retrieval effectiveness, as well as the broader impact of document
expansion on both training and evaluation. Overall, our findings highlight both
the promise and limitations of LLM-driven document expansion, positioning
Tool-DE, along with the proposed Tool-Embed and Tool-Rank, as a foundation for
future research in tool retrieval.

</details>


### [136] [Diversification as Risk Minimization](https://arxiv.org/abs/2510.22681)
*Rikiya Takehi,Fernando Diaz,Tetsuya Sakai*

Main category: cs.IR

TL;DR: 用户易记搜索失败，现有搜索多样化算法缺乏鲁棒性，提出VRisk指标和VRisker重排器，实验证明其能降低最坏情况失败率。


<details>
  <summary>Details</summary>
Motivation: 现有搜索系统在单查询中忽略鲁棒性原则，多样化算法无鲁棒性保证。

Method: 将多样化问题框架化为风险最小化问题，引入VRisk指标，提出VRisker快速贪心重排器。

Result: 实验表明现有方法脆弱，VRisker最多降低33%最坏情况意图失败率，平均性能仅下降2%。

Conclusion: VRisk和VRisker能有效提高搜索系统鲁棒性，减少用户不良体验。

Abstract: Users tend to remember failures of a search session more than its many
successes. This observation has led to work on search robustness, where systems
are penalized if they perform very poorly on some queries. However, this
principle of robustness has been overlooked within a single query. An ambiguous
or underspecified query (e.g., ``jaguar'') can have several user intents, where
popular intents often dominate the ranking, leaving users with minority intents
unsatisfied. Although the diversification literature has long recognized this
issue, existing metrics only model the average relevance across intents and
provide no robustness guarantees. More surprisingly, we show theoretically and
empirically that many well-known diversification algorithms are no more robust
than a naive, non-diversified algorithm. To address this critical gap, we
propose to frame diversification as a risk-minimization problem. We introduce
VRisk, which measures the expected risk faced by the least-served fraction of
intents in a query. Optimizing VRisk produces a robust ranking, reducing the
likelihood of poor user experiences. We then propose VRisker, a fast greedy
re-ranker with provable approximation guarantees. Finally, experiments on NTCIR
INTENT-2, TREC Web 2012, and MovieLens show the vulnerability of existing
methods. VRisker reduces worst-case intent failures by up to 33% with a minimal
2% drop in average performance.

</details>


### [137] [REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for E-commerce Visual Search System Optimization](https://arxiv.org/abs/2510.22739)
*Yiwen Tang,Qiuyu Zhao,Zenghui Sun,Jinsong Lan,Xiaoyong Zhu,Bo Zheng,Kaifu Zhang*

Main category: cs.IR

TL;DR: 针对淘宝电商视觉搜索中用户隐式意图与系统响应不匹配问题，提出REVISION框架，结合离线推理挖掘和在线决策执行，实验表明可提升隐式意图挖掘效率并降低无点击率。


<details>
  <summary>Details</summary>
Motivation: 淘宝电商视觉搜索中存在大量无点击请求，用户隐式意图多样且难挖掘，导致平台策略适应性有限和滞后，限制用户表达多样意图及视觉搜索系统扩展性。

Method: 提出REVISION框架，离线阶段从历史无点击请求中挖掘差异，利用大模型联合推理查询和产品元数据得出最优建议；在线阶段用训练好的REVISION - R1 - 3B对查询图像和历史产品进行整体分析生成优化计划并自适应调度策略。

Result: 提高了从大规模搜索日志中挖掘隐式意图的效率，显著降低了无点击率。

Conclusion: REVISION框架提供了将大模型与传统搜索系统集成的简化范式，实现了信息聚合和用户交互的端到端智能优化。

Abstract: In Taobao e-commerce visual search, user behavior analysis reveals a large
proportion of no-click requests, suggesting diverse and implicit user intents.
These intents are expressed in various forms and are difficult to mine and
discover, thereby leading to the limited adaptability and lag in platform
strategies. This greatly restricts users' ability to express diverse intents
and hinders the scalability of the visual search system. This mismatch between
user implicit intent expression and system response defines the User-SearchSys
Intent Discrepancy. To alleviate the issue, we propose a novel framework
REVISION. This framework integrates offline reasoning mining with online
decision-making and execution, enabling adaptive strategies to solve implicit
user demands. In the offline stage, we construct a periodic pipeline to mine
discrepancies from historical no-click requests. Leveraging large models, we
analyze implicit intent factors and infer optimal suggestions by jointly
reasoning over query and product metadata. These inferred suggestions serve as
actionable insights for refining platform strategies. In the online stage,
REVISION-R1-3B, trained on the curated offline data, performs holistic analysis
over query images and associated historical products to generate optimization
plans and adaptively schedule strategies across the search pipeline. Our
framework offers a streamlined paradigm for integrating large models with
traditional search systems, enabling end-to-end intelligent optimization across
information aggregation and user interaction. Experimental results demonstrate
that our approach improves the efficiency of implicit intent mining from
large-scale search logs and significantly reduces the no-click rate.

</details>


### [138] [Civic Ground Truth in News Recommenders: A Method for Public Value Scoring](https://arxiv.org/abs/2510.22865)
*James Meese,Kyle Herbertson*

Main category: cs.IR

TL;DR: 本文介绍通过大规模、结构化受众评估将公民价值嵌入新闻推荐系统（NRS）的方法。


<details>
  <summary>Details</summary>
Motivation: 探索将编辑目标和公共服务价值等规范目标融入现有NRS的最佳方式。

Method: 提出公民地面真值方法，通过全国代表性调查生成基于价值的标签，并使用自动元数据丰富。

Result: 未提及

Conclusion: 未提及

Abstract: Research in news recommendation systems (NRS) continues to explore the best
ways to integrate normative goals such as editorial objectives and public
service values into existing systems. Prior efforts have incorporated expert
input or audience feedback to quantify these values, laying the groundwork for
more civic-minded recommender systems. This paper contributes to that
trajectory, introducing a method for embedding civic values into NRS through
large-scale, structured audience evaluations. The proposed civic ground truth
approach aims to generate value-based labels through a nationally
representative survey that are generalisable across a wider news corpus, using
automated metadata enrichment.

</details>


### [139] [MGFRec: Towards Reinforced Reasoning Recommendation with Multiple Groundings and Feedback](https://arxiv.org/abs/2510.22888)
*Shihao Cai,Chongming Gao,Haoyan Liu,Wentao Shi,Jianshan Sun,Ruiming Tang,Fuli Feng*

Main category: cs.IR

TL;DR: 本文提出在推理中进行多轮落地及引入用户代理反馈，以解决基于推理的推荐任务中仅在语言空间推理的问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 以往基于推理的推荐方法仅在语言空间推理，导致过度解读用户兴趣且偏离真实物品，存在研究空白。

Method: 在推理中进行多轮落地，帮助大语言模型理解实际物品空间；引入用户代理在每次落地步骤提供反馈。

Result: 在三个亚马逊评论数据集上的综合实验证明了多轮落地和反馈的有效性。

Conclusion: 推荐任务应在实际物品空间而非仅在语言空间进行推理，这一点至关重要。

Abstract: The powerful reasoning and generative capabilities of large language models
(LLMs) have inspired researchers to apply them to reasoning-based
recommendation tasks, which require in-depth reasoning about user interests and
the generation of recommended items. However, previous reasoning-based
recommendation methods have typically performed inference within the language
space alone, without incorporating the actual item space. This has led to
over-interpreting user interests and deviating from real items. Towards this
research gap, we propose performing multiple rounds of grounding during
inference to help the LLM better understand the actual item space, which could
ensure that its reasoning remains aligned with real items. Furthermore, we
introduce a user agent that provides feedback during each grounding step,
enabling the LLM to better recognize and adapt to user interests. Comprehensive
experiments conducted on three Amazon review datasets demonstrate the
effectiveness of incorporating multiple groundings and feedback. These findings
underscore the critical importance of reasoning within the actual item space,
rather than being confined to the language space, for recommendation tasks.

</details>


### [140] [Improving Product Search Relevance with EAR-MP: A Solution for the CIKM 2025 AnalytiCup](https://arxiv.org/abs/2510.23018)
*JaeEun Lim,Soomin Kim,Jaeyong Seo,Iori Ono,Qimu Ran,Jae-woong Lee*

Main category: cs.IR

TL;DR: 本文介绍EAR - MP团队针对CIKM 2025 AnalytiCup的多语言电商搜索解决方案，经数据处理和模型训练，在QC和QI任务取得有竞争力结果，强调数据预处理和训练策略重要性。


<details>
  <summary>Details</summary>
Motivation: 解决多语言电商搜索因语言多样性和用户查询噪声带来的挑战，完成QC和QI相关性两个核心任务。

Method: 先将多语言数据集文本翻译成英文进行归一化，大量数据清洗和归一化降噪；基于DeBERTa - v3 - large模型训练，用标签平滑、自蒸馏和丢弃法提升性能；针对QC和QI任务分别进行特定升级。

Result: 在计算资源受限情况下，QC任务F1分数达0.8796，QI任务F1分数达0.8744。

Conclusion: 构建健壮、资源高效的多语言相关性系统，系统的数据预处理和定制训练策略很重要。

Abstract: Multilingual e-commerce search is challenging due to linguistic diversity and
the noise inherent in user-generated queries. This paper documents the solution
employed by our team (EAR-MP) for the CIKM 2025 AnalytiCup, which addresses two
core tasks: Query-Category (QC) relevance and Query-Item (QI) relevance. Our
approach first normalizes the multilingual dataset by translating all text into
English, then mitigates noise through extensive data cleaning and
normalization. For model training, we build on DeBERTa-v3-large and improve
performance with label smoothing, self-distillation, and dropout. In addition,
we introduce task-specific upgrades, including hierarchical token injection for
QC and a hybrid scoring mechanism for QI. Under constrained compute, our method
achieves competitive results, attaining an F1 score of 0.8796 on QC and 0.8744
on QI. These findings underscore the importance of systematic data
preprocessing and tailored training strategies for building robust,
resource-efficient multilingual relevance systems.

</details>


### [141] [Multi-Stage Field Extraction of Financial Documents with OCR and Compact Vision-Language Models](https://arxiv.org/abs/2510.23066)
*Yichao Jin,Yushuo Wang,Qishuai Zhong,Kent Chiu Jin-Chun,Kenneth Zhu Ke,Donald MacDonald*

Main category: cs.IR

TL;DR: 本文针对中小微企业金融文档解析难题，提出多阶段处理流程结合紧凑型VLM模型的方法，实验表明该方法在准确率、成本和延迟上表现优异。


<details>
  <summary>Details</summary>
Motivation: 中小微企业金融文档难以解析，现有端到端大视觉语言模型处理此类文档存在计算成本高、对噪声敏感和处理速度慢等问题。

Method: 提出多阶段处理流程，包括图像预处理、多语言OCR提取文本、页面检索连贯部分，最后用紧凑型VLM模型提取结构化金融指标。

Result: 使用内部语料库评估，该方法字段级准确率比直接使用大VLM模型高8.8倍，GPU成本仅为0.7%，端到端服务延迟降低92.6%。

Conclusion: 多阶段处理流程结合紧凑型VLM模型在处理大规模金融文档的结构化字段提取上效果良好，能显著提升准确率并降低成本和延迟。

Abstract: Financial documents are essential sources of information for regulators,
auditors, and financial institutions, particularly for assessing the wealth and
compliance of Small and Medium-sized Businesses. However, SMB documents are
often difficult to parse. They are rarely born digital and instead are
distributed as scanned images that are none machine readable. The scans
themselves are low in resolution, affected by skew or rotation, and often
contain noisy backgrounds. These documents also tend to be heterogeneous,
mixing narratives, tables, figures, and multilingual content within the same
report. Such characteristics pose major challenges for automated information
extraction, especially when relying on end to end large Vision Language Models,
which are computationally expensive, sensitive to noise, and slow when applied
to files with hundreds of pages.
  We propose a multistage pipeline that leverages traditional image processing
models and OCR extraction, together with compact VLMs for structured field
extraction of large-scale financial documents. Our approach begins with image
pre-processing, including segmentation, orientation detection, and size
normalization. Multilingual OCR is then applied to recover page-level text.
Upon analyzing the text information, pages are retrieved for coherent sections.
Finally, compact VLMs are operated within these narrowed-down scopes to extract
structured financial indicators.
  Our approach is evaluated using an internal corpus of multi-lingual, scanned
financial documents. The results demonstrate that compact VLMs, together with a
multistage pipeline, achieves 8.8 times higher field level accuracy relative to
directly feeding the whole document into large VLMs, only at 0.7 percent of the
GPU cost and 92.6 percent less end-to-end service latency.

</details>


### [142] [Think before Recommendation: Autonomous Reasoning-enhanced Recommender](https://arxiv.org/abs/2510.23077)
*Xiaoyu Kong,Junguang Jiang,Bin Liu,Ziru Xu,Han Zhu,Jian Xu,Bo Zheng,Jiancan Wu,Xiang Wang*

Main category: cs.IR

TL;DR: 本文提出基于强化学习的推荐范式RecZero和混合范式RecOne，实验表明其在多基准数据集上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于蒸馏的方法存在教师模型推荐能力不足、监督成本高且静态、推理能力转移肤浅等问题。

Method: 提出RecZero，通过纯强化学习训练单个大语言模型进行评分预测，包含“推荐前思考”提示构建和基于规则的奖励建模；还提出结合监督微调与强化学习的RecOne。

Result: RecZero和RecOne在多个基准数据集上显著优于现有基线方法。

Conclusion: 强化学习范式在实现自主推理增强的推荐系统方面具有优越性。

Abstract: The core task of recommender systems is to learn user preferences from
historical user-item interactions. With the rapid development of large language
models (LLMs), recent research has explored leveraging the reasoning
capabilities of LLMs to enhance rating prediction tasks. However, existing
distillation-based methods suffer from limitations such as the teacher model's
insufficient recommendation capability, costly and static supervision, and
superficial transfer of reasoning ability. To address these issues, this paper
proposes RecZero, a reinforcement learning (RL)-based recommendation paradigm
that abandons the traditional multi-model and multi-stage distillation
approach. Instead, RecZero trains a single LLM through pure RL to autonomously
develop reasoning capabilities for rating prediction. RecZero consists of two
key components: (1) "Think-before-Recommendation" prompt construction, which
employs a structured reasoning template to guide the model in step-wise
analysis of user interests, item features, and user-item compatibility; and (2)
rule-based reward modeling, which adopts group relative policy optimization
(GRPO) to compute rewards for reasoning trajectories and optimize the LLM.
Additionally, the paper explores a hybrid paradigm, RecOne, which combines
supervised fine-tuning with RL, initializing the model with cold-start
reasoning samples and further optimizing it with RL. Experimental results
demonstrate that RecZero and RecOne significantly outperform existing baseline
methods on multiple benchmark datasets, validating the superiority of the RL
paradigm in achieving autonomous reasoning-enhanced recommender systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [143] [A Feature Engineering Approach for Business Impact-Oriented Failure Detection in Distributed Instant Payment Systems](https://arxiv.org/abs/2510.21710)
*Lorenzo Porcelli*

Main category: cs.LG

TL;DR: 提出基于ISO 20022消息处理时间的特征工程方法用于即时支付基础设施异常检测，在TIPS系统验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统监控方法无法在技术基础设施指标和业务流程可见性之间架起桥梁，即时支付基础设施有严格性能要求。

Method: 基于连续ISO 20022消息交换的处理时间进行特征工程，对特征应用异常检测。

Result: 在TIPS系统实验中能检测多种异常模式，提供可解释的解释。

Conclusion: 该框架可区分内外支付系统问题，减少调查时间，弥合分布式系统可观测性差距。

Abstract: Instant payment infrastructures have stringent performance requirements,
processing millions of transactions daily with zero-downtime expectations.
Traditional monitoring approaches fail to bridge the gap between technical
infrastructure metrics and business process visibility. We introduce a novel
feature engineering approach based on processing times computed between
consecutive ISO 20022 message exchanges, creating a compact representation of
system state. By applying anomaly detection to these features, we enable early
failure detection and localization, allowing incident classification.
Experimental evaluation on the TARGET Instant Payment Settlement (TIPS) system,
using both real-world incidents and controlled simulations, demonstrates the
approach's effectiveness in detecting diverse anomaly patterns and provides
inherently interpretable explanations that enable operators to understand the
business impact. By mapping features to distinct processing phases, the
resulting framework differentiates between internal and external payment system
issues, significantly reduces investigation time, and bridges observability
gaps in distributed systems where transaction state is fragmented across
multiple entities.

</details>


### [144] [Numerical Fragility in Transformers: A Layer-wise Theory for Explaining, Forecasting, and Mitigating Instability](https://arxiv.org/abs/2510.21770)
*Jinwoo Baek*

Main category: cs.LG

TL;DR: 本文提出一阶、逐模块理论预测低精度训练Transformer时误差增长情况，给出统一前向稳定性边界，在Tiny - ViT/CIFAR - 10上评估，理论提供可操作诊断方法。


<details>
  <summary>Details</summary>
Motivation: 解决低精度训练Transformer时的前向误差放大问题。

Method: 提出一阶、逐模块理论，推导自注意力每层边界，证明残差块衰减不等式，引入LayerNorm指标。

Result: 边界及组件评估显示，组合预测器能跟踪精度不匹配，时间序列最大值可预警，基于指标微调可稳定模型。

Conclusion: 理论提供可操作、无单位诊断方法，解释自注意力脆弱性、预测不稳定性并提出缓解措施。

Abstract: Transformers trained in low precision can suffer forward-error amplification.
We give a first-order, module-wise theory that predicts when and where errors
grow. For self-attention we derive a per-layer bound that factorizes into three
interpretable diagnostics: a score-scale ratio $\kappa_{\rm score}$, a rowwise
softmax sensitivity $\kappa_{\rm softmax}$, and value conditioning $\kappa(V)$.
We prove a residual relaxation inequality showing that residual blocks
attenuate depth-wise accumulation, and we introduce a precision- and
width-aware LayerNorm indicator $\rho_{\rm LN}$ with a matching first-order
bound in the $\epsilon$-dominated regime. These pieces yield a unified
forward-stability bound whose right-hand side is directly estimable during
training.
  On Tiny-ViT/CIFAR-10 we evaluate the bound and components. (1) The combined
predictor $\kappa_{\rm softmax},(1+\kappa_{\rm
score}),\kappa(V),|W_O|2+\kappa{\rm eff}+C_{\rm LN}$ tracks
FP32$\leftrightarrow$LP mismatches across seeds, widths, and precisions;
scaling by $\epsilon_{\rm mach}$ collapses mixed-precision points. (2) The
time-series maximum of $\kappa_{\rm softmax}$ acts as an early-warning signal,
leading error spikes by 16-24 steps (corr. 0.65-0.82; permutation
$p!\approx!10^{-3}$; Precision@K 0.89-1.00). (3) Guided by $\rho_{\rm LN}$, a
small LayerNorm-$\epsilon$ tweak targeting $\rho_\star$ gives consistent
stabilization (mean tail-loss $\downarrow\ \approx0.010$ at $\rho_\star!=!0.6$,
cap$=10^{-2}$) with negligible overhead.
  Overall, our theory supplies actionable, unitless diagnostics that (i)
explain when self-attention is fragile, (ii) forecast instability, and (iii)
motivate a minimally invasive mitigation.

</details>


### [145] [Chebyshev Moment Regularization (CMR): Condition-Number Control with Moment Shaping](https://arxiv.org/abs/2510.21772)
*Jinwoo Baek*

Main category: cs.LG

TL;DR: 介绍Chebyshev Moment Regularization (CMR)损失函数，可优化层谱，在对抗设置中效果良好，支持优化驱动的谱预条件化。


<details>
  <summary>Details</summary>
Motivation: 寻找一种简单、与架构无关的损失函数来直接优化层谱，实现稳定、准确的学习。

Method: 引入CMR损失函数，通过对数条件代理联合控制谱边缘，用Chebyshev矩塑造内部，采用解耦、有上限的混合规则保留任务梯度。

Result: 在对抗设置中，与普通训练相比，CMR将平均层条件数降低约10^3，增加平均梯度幅度，将测试准确率从约10%恢复到约86%。

Conclusion: 支持优化驱动的谱预条件化，可引导模型进入良态区域进行稳定、准确的学习。

Abstract: We introduce \textbf{Chebyshev Moment Regularization (CMR)}, a simple,
architecture-agnostic loss that directly optimizes layer spectra. CMR jointly
controls spectral edges via a log-condition proxy and shapes the interior via
Chebyshev moments, with a decoupled, capped mixing rule that preserves task
gradients. We prove strictly monotone descent for the condition proxy, bounded
moment gradients, and orthogonal invariance. In an adversarial
``$\kappa$-stress'' setting (MNIST, 15-layer MLP), \emph{compared to vanilla
training}, CMR reduces mean layer condition numbers by $\sim\!10^3$ (from
$\approx3.9\!\times\!10^3$ to $\approx3.4$ in 5 epochs), increases average
gradient magnitude, and restores test accuracy (
$\approx10\%\!\to\!\approx86\%$ ). These results support
\textbf{optimization-driven spectral preconditioning}: directly steering models
toward well-conditioned regimes for stable, accurate learning.

</details>


### [146] [Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing](https://arxiv.org/abs/2510.22044)
*Kijung Jeon,Michael Muehlebach,Molei Tao*

Main category: cs.LG

TL;DR: 本文提出新框架OLLA用于从约束统计分布采样，可处理等式和不等式约束，避免显式投影，收敛快且实验证明效率高。


<details>
  <summary>Details</summary>
Motivation: 现有非凸约束集采样方法依赖昂贵投影步骤，不能同时处理等式和不等式约束，且缺乏严格定量收敛保证。

Method: 引入Overdamped Langevin with LAnding (OLLA)框架，设计能适应等式和不等式约束的过阻尼朗之万动力学，并沿约束表面法向确定性校正轨迹。

Result: 在合适正则条件下，OLLA在W2距离上以指数速度收敛到约束目标密度；实验表明OLLA比基于投影的约束朗之万算法及其松弛变量变体更高效。

Conclusion: OLLA是一种有效的从约束统计分布采样的方法，具有良好的计算成本和经验混合性。

Abstract: Sampling from constrained statistical distributions is a fundamental task in
various fields including Bayesian statistics, computational chemistry, and
statistical physics. This article considers the cases where the constrained
distribution is described by an unconstrained density, as well as additional
equality and/or inequality constraints, which often make the constraint set
nonconvex. Existing methods for nonconvex constraint set $\Sigma \subset
\mathbb{R}^d$ defined by equality or inequality constraints commonly rely on
costly projection steps. Moreover, they cannot handle equality and inequality
constraints simultaneously as each method only specialized in one case. In
addition, rigorous and quantitative convergence guarantee is often lacking. In
this paper, we introduce Overdamped Langevin with LAnding (OLLA), a new
framework that can design overdamped Langevin dynamics accommodating both
equality and inequality constraints. The proposed dynamics also
deterministically corrects trajectories along the normal direction of the
constraint surface, thus obviating the need for explicit projections. We show
that, under suitable regularity conditions on the target density and $\Sigma$,
OLLA converges exponentially fast in $W_2$ distance to the constrained target
density $\rho_\Sigma(x) \propto \exp(-f(x))d\sigma_\Sigma$. Lastly, through
experiments, we demonstrate the efficiency of OLLA compared to projection-based
constrained Langevin algorithms and their slack variable variants, highlighting
its favorable computational cost and reasonable empirical mixing.

</details>


### [147] [Centrum: Model-based Database Auto-tuning with Minimal Distributional Assumptions](https://arxiv.org/abs/2510.22734)
*Yuanhao Lai,Pengfei Zheng,Chenpeng Ji,Yan Li,Songhan Zhang,Rutao Zhang,Zhengang Wang,Yunfei Du*

Main category: cs.LG

TL;DR: 本文指出GP - BO在数据库管理系统（DBMS）自动调优中的局限性，提出新的自动调优器Centrum，实验表明其优于21种SOTA方法。


<details>
  <summary>Details</summary>
Motivation: GP - BO基的DBMS自动调优器表现优于SMAC基的，但GP - BO存在局限性，现有树集成贝叶斯优化也限制了DBMS自动调优的进一步发展。

Method: 提出Centrum，采用随机梯度提升集成的两阶段学习过程改进代理建模中的无分布点和区间估计，采用广义SGBE估计的局部自适应共形预测进行无分布不确定性估计和采集函数。

Result: 在两个DBMS和三个工作负载上的大量物理和模拟实验表明，Centrum优于21种SOTA方法。

Conclusion: Centrum是首个实现无分布性、提升贝叶斯优化在DBMS自动调优中实用性的自动调优器，也是首个在贝叶斯优化中无缝融合梯度提升集成和共形推理的。

Abstract: Gaussian-Process-based Bayesian optimization (GP-BO), is a prevailing
model-based framework for DBMS auto-tuning. However, recent work shows
GP-BO-based DBMS auto-tuners significantly outperformed auto-tuners based on
SMAC, which features random forest surrogate models; such results motivate us
to rethink and investigate the limitations of GP-BO in auto-tuner design. We
find the fundamental assumptions of GP-BO are widely violated when modeling and
optimizing DBMS performance, while tree-ensemble-BOs (e.g., SMAC) can avoid the
assumption pitfalls and deliver improved tuning efficiency and effectiveness.
Moreover, we argue that existing tree-ensemble-BOs restrict further advancement
in DBMS auto-tuning. First, existing tree-ensemble-BOs can only achieve
distribution-free point estimates, but still impose unrealistic distributional
assumptions on uncertainty estimates, compromising surrogate modeling and
distort the acquisition function. Second, recent advances in gradient boosting,
which can further enhance surrogate modeling against vanilla GP and random
forest counterparts, have rarely been applied in optimizing DBMS auto-tuners.
To address these issues, we propose a novel model-based DBMS auto-tuner,
Centrum. Centrum improves distribution-free point and interval estimation in
surrogate modeling with a two-phase learning procedure of stochastic gradient
boosting ensembles. Moreover, Centrum adopts a generalized SGBE-estimated
locally-adaptive conformal prediction to facilitate a distribution-free
uncertainty estimation and acquisition function. To our knowledge, Centrum is
the first auto-tuner to realize distribution-freeness, enhancing BO's
practicality in DBMS auto-tuning, and the first to seamlessly fuse gradient
boosting ensembles and conformal inference in BO. Extensive physical and
simulation experiments on two DBMSs and three workloads show Centrum
outperforms 21 SOTA methods.

</details>


### [148] [What Causes Postoperative Aspiration?](https://arxiv.org/abs/2510.21779)
*Supriya Nagesh,Karina Covarrubias,Robert El-Kareh,Shiva Prasad Kasiviswanathan,Nina Mishra*

Main category: cs.LG

TL;DR: 本文开发机器学习模型预测术后误吸，模型表现良好，确定重要预测因素和因果因素，指出性别差异需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 误吸显著影响手术患者发病率和死亡率，开发ML模型预测术后误吸以进行及时预防干预。

Method: 从MIMIC - IV数据库选取手术患者，用术前住院数据训练XGBoost、多层感知器和随机森林三种ML模型预测术后误吸，用增强逆概率加权估计平均治疗效果研究因果关系。

Result: ML模型在测试集上AUROC为0.86，灵敏度77.3%，确定重要预测因素，ATE分析确定因果因素，发现男性误吸可能性和每日最大阿片类药物剂量高于女性。

Conclusion: ML模型可有效预测术后误吸风险，最大每日阿片类药物剂量和手术部位影响误吸风险，性别差异需进一步研究，对改善术后护理和预防策略有重要意义。

Abstract: Background: Aspiration, the inhalation of foreign material into the lungs,
significantly impacts surgical patient morbidity and mortality. This study
develops a machine learning (ML) model to predict postoperative aspiration,
enabling timely preventative interventions.
  Methods: From the MIMIC-IV database of over 400,000 hospital admissions, we
identified 826 surgical patients (mean age: 62, 55.7\% male) who experienced
aspiration within seven days post-surgery, along with a matched non-aspiration
cohort. Three ML models: XGBoost, Multilayer Perceptron, and Random Forest were
trained using pre-surgical hospitalization data to predict postoperative
aspiration. To investigate causation, we estimated Average Treatment Effects
(ATE) using Augmented Inverse Probability Weighting.
  Results: Our ML model achieved an AUROC of 0.86 and 77.3\% sensitivity on a
held-out test set. Maximum daily opioid dose, length of stay, and patient age
emerged as the most important predictors. ATE analysis identified significant
causative factors: opioids (0.25 +/- 0.06) and operative site (neck: 0.20 +/-
0.13, head: 0.19 +/- 0.13). Despite equal surgery rates across genders, men
were 1.5 times more likely to aspirate and received 27\% higher maximum daily
opioid dosages compared to women.
  Conclusion: ML models can effectively predict postoperative aspiration risk,
enabling targeted preventative measures. Maximum daily opioid dosage and
operative site significantly influence aspiration risk. The gender disparity in
both opioid administration and aspiration rates warrants further investigation.
These findings have important implications for improving postoperative care
protocols and aspiration prevention strategies.

</details>


### [149] [Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making](https://arxiv.org/abs/2510.21788)
*Larkin Liu,Jalal Etesami*

Main category: cs.LG

TL;DR: 本文探索专家引导的多臂老虎机学习（OMoE），提出两种算法解决专家输出聚合问题，推导理论保证并给出实证结果，应用于大语言模型在线微调。


<details>
  <summary>Details</summary>
Motivation: 探索如何在给定上下文下，让专家委员会聚合输出以实现最优的总体准确率。

Method: 提出两种算法，一是结合聚合投票与UCB驱动的逐次消除；二是采用在线加权多数投票机制。

Result: 推导出理想情况下多臂老虎机设置的遗憾属性理论保证，有实证结果，应用于大语言模型微调。

Conclusion: 引入了结合多个专家的新方法和无遗憾保证，提升了整体聚合模型的性能。

Abstract: We explore the use of expert-guided bandit learning, which we refer to as
online mixture-of-experts (OMoE). In this setting, given a context, a candidate
committee of experts must determine how to aggregate their outputs to achieve
optimal results in terms of aggregate accuracy. We propose two algorithms to
address this problem. The first algorithm combines aggregate voting with
UCB-driven successive elimination, efficiently pruning suboptimal exploration
actions. The second algorithm employs an online weighted-majority-voting
mechanism, leveraging the respective voting power of each expert proportional
to their predictive power. We derive theoretical guarantees for the regret
properties in the bandit setting under ideal circumstances, and empirical
results are provided accordingly. As a modern study on applications, these
methods are applied to the online fine-tuning of a set of expert large language
models (LLMs), where after each response, the generative LLM dynamically
reweighs its set of experts and/or selects the optimal committee of experts to
generate the most accurate response. Our results introduce new methodologies
and no-regret guarantees for combining multiple experts to improve on the
performance of the an aggregate model overall.

</details>


### [150] [Variance-Reduction Guidance: Sampling Trajectory Optimization for Diffusion Models](https://arxiv.org/abs/2510.21792)
*Shifeng Xu,Yanzhu Liu,Adams Wai-Kin Kong*

Main category: cs.LG

TL;DR: 本文提出方差减少引导（VRG）方法减轻扩散模型采样过程中的预测误差，提升生成质量，且无需模型微调或修改，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样过程中预测误差会累积并降低生成质量，需要解决该问题。

Method: 引入统计测量预测误差的技术，提出VRG方法，在预定义采样轨迹基础上搜索新轨迹。

Result: 在各种数据集和基线实验中，VRG显著提高了扩散模型的生成质量。

Conclusion: VRG方法能有效减轻预测误差，提升扩散模型生成质量，且适用于条件和无条件生成。

Abstract: Diffusion models have become emerging generative models. Their sampling
process involves multiple steps, and in each step the models predict the noise
from a noisy sample. When the models make prediction, the output deviates from
the ground truth, and we call such a deviation as \textit{prediction error}.
The prediction error accumulates over the sampling process and deteriorates
generation quality. This paper introduces a novel technique for statistically
measuring the prediction error and proposes the Variance-Reduction Guidance
(VRG) method to mitigate this error. VRG does not require model fine-tuning or
modification. Given a predefined sampling trajectory, it searches for a new
trajectory which has the same number of sampling steps but produces higher
quality results. VRG is applicable to both conditional and unconditional
generation. Experiments on various datasets and baselines demonstrate that VRG
can significantly improve the generation quality of diffusion models. Source
code is available at https://github.com/shifengxu/VRG.

</details>


### [151] [Clustering by Denoising: Latent plug-and-play diffusion for single-cell data](https://arxiv.org/abs/2510.22835)
*Dominik Meier,Shixing Yu,Sagnik Nandy,Promit Ghosal,Kyra Gan*

Main category: cs.LG

TL;DR: 提出潜在即插即用扩散框架用于单细胞RNA测序数据去噪，提升聚类准确性和生物学一致性。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据存在测量噪声和生物变异性，标准潜在空间难以实现准确聚类，影响下游分析。

Method: 引入分离观测和去噪空间的潜在即插即用扩散框架，通过新颖的吉布斯采样程序操作，利用“输入空间引导”确保去噪轨迹忠实于原始数据结构。

Result: 在合成和真实单细胞基因组数据上评估，该方法在合成数据不同噪声水平和数据集偏移下提高聚类准确性，在真实数据上使细胞簇具有更好的生物学一致性。

Conclusion: 该方法能自适应处理噪声、量化不确定性、实现可推广的去噪，提升了单细胞RNA测序数据的聚类质量。

Abstract: Single-cell RNA sequencing (scRNA-seq) enables the study of cellular
heterogeneity. Yet, clustering accuracy, and with it downstream analyses based
on cell labels, remain challenging due to measurement noise and biological
variability. In standard latent spaces (e.g., obtained through PCA), data from
different cell types can be projected close together, making accurate
clustering difficult. We introduce a latent plug-and-play diffusion framework
that separates the observation and denoising space. This separation is
operationalized through a novel Gibbs sampling procedure: the learned diffusion
prior is applied in a low-dimensional latent space to perform denoising, while
to steer this process, noise is reintroduced into the original high-dimensional
observation space. This unique "input-space steering" ensures the denoising
trajectory remains faithful to the original data structure. Our approach offers
three key advantages: (1) adaptive noise handling via a tunable balance between
prior and observed data; (2) uncertainty quantification through principled
uncertainty estimates for downstream analysis; and (3) generalizable denoising
by leveraging clean reference data to denoise noisier datasets, and via
averaging, improve quality beyond the training set. We evaluate robustness on
both synthetic and real single-cell genomics data. Our method improves
clustering accuracy on synthetic data across varied noise levels and dataset
shifts. On real-world single-cell data, our method demonstrates improved
biological coherence in the resulting cell clusters, with cluster boundaries
that better align with known cell type markers and developmental trajectories.

</details>


### [152] [A Physics-Guided AI Cascaded Corrector Model Significantly Extends Madden-Julian Oscillation Prediction Skill](https://arxiv.org/abs/2510.21796)
*Xiao Zhou,Yuze Sun,Jie Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出PCC - MJO深度学习框架校正MJO预报，延长预报范围，突破次季节预报障碍。


<details>
  <summary>Details</summary>
Motivation: MJO在业务动力模型中预测具有挑战性，预报技巧通常限于3 - 4周。

Method: 提出PCC - MJO框架，先使用物理信息3D U - Net校正时空场误差，再用优化的LSTM细化RMM指数。

Result: 应用于三个不同业务预报时，统一框架将技巧预报范围延长2 - 8天，缓解“海洋大陆障碍”。

Conclusion: 该工作为突破次季节预报长期障碍提供了有前景的途径。

Abstract: The Madden-Julian Oscillation (MJO) is an important driver of global weather
and climate extremes, but its prediction in operational dynamical models
remains challenging, with skillful forecasts typically limited to 3-4 weeks.
Here, we introduce a novel deep learning framework, the Physics-guided Cascaded
Corrector for MJO (PCC-MJO), which acts as a universal post-processor to
correct MJO forecasts from dynamical models. This two-stage model first employs
a physics-informed 3D U-Net to correct spatial-temporal field errors, then
refines the MJO's RMM index using an LSTM optimized for forecast skill. When
applied to three different operational forecasts from CMA, ECMWF and NCEP, our
unified framework consistently extends the skillful forecast range (bivariate
correlation > 0.5) by 2-8 days. Crucially, the model effectively mitigates the
"Maritime Continent barrier", enabling more realistic eastward propagation and
amplitude. Explainable AI analysis quantitatively confirms that the model's
decision-making is spatially congruent with observed MJO dynamics (correlation
> 0.93), demonstrating that it learns physically meaningful features rather
than statistical fittings. Our work provides a promising physically consistent,
computationally efficient, and highly generalizable pathway to break through
longstanding barriers in subseasonal forecasting.

</details>


### [153] [Robust Iterative Learning Hidden Quantum Markov Models](https://arxiv.org/abs/2510.23237)
*Ning Ning*

Main category: cs.LG

TL;DR: 本文提出Adversarially Corrupted HQMM (AC - HQMM)和Robust Iterative Learning Algorithm (RILA)，解决现有HQMM学习算法对数据损坏敏感问题，RILA在多基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有HQMM学习算法对数据损坏高度敏感，缺乏对抗扰动下的鲁棒性机制。

Method: 引入AC - HQMM形式化鲁棒性分析，提出RILA算法，结合RCR - EF模块和迭代随机重采样过程更新Kraus算子，采用L1惩罚似然目标增强稳定性。

Result: 在多个HQMM和HMM基准测试中，RILA比现有算法有更好的收敛稳定性、抗损坏性和物理有效性。

Conclusion: RILA为鲁棒量子序列学习提供了一种有原则且高效的方法。

Abstract: Hidden Quantum Markov Models (HQMMs) extend classical Hidden Markov Models to
the quantum domain, offering a powerful probabilistic framework for modeling
sequential data with quantum coherence. However, existing HQMM learning
algorithms are highly sensitive to data corruption and lack mechanisms to
ensure robustness under adversarial perturbations. In this work, we introduce
the Adversarially Corrupted HQMM (AC-HQMM), which formalizes robustness
analysis by allowing a controlled fraction of observation sequences to be
adversarially corrupted. To learn AC-HQMMs, we propose the Robust Iterative
Learning Algorithm (RILA), a derivative-free method that integrates a Remove
Corrupted Rows by Entropy Filtering (RCR-EF) module with an iterative
stochastic resampling procedure for physically valid Kraus operator updates.
RILA incorporates L1-penalized likelihood objectives to enhance stability,
resist overfitting, and remain effective under non-differentiable conditions.
Across multiple HQMM and HMM benchmarks, RILA demonstrates superior convergence
stability, corruption resilience, and preservation of physical validity
compared to existing algorithms, establishing a principled and efficient
approach for robust quantum sequential learning.

</details>


### [154] [LAMP: Data-Efficient Linear Affine Weight-Space Models for Parameter-Controlled 3D Shape Generation and Extrapolation](https://arxiv.org/abs/2510.22491)
*Ghadi Nehme,Yanxia Zhang,Dule Shu,Matt Klenk,Faez Ahmed*

Main category: cs.LG

TL;DR: 提出LAMP框架解决现有3D生成方法依赖大数据集、可控性和泛化性不足的问题，在两个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前3D生成方法依赖大训练数据集，可控性和泛化性差，需改进。

Method: 先通过共享初始化过拟合每个样本对齐有符号距离函数（SDF）解码器，再在对齐权重空间解决参数约束混合问题合成新几何形状，还提出安全指标检测几何有效性。

Result: 在两个3D参数基准测试中，LAMP能用少至100个样本进行边界内受控插值、超训练范围安全外推、固定参数下物理性能引导优化，在数据效率和外推方面优于对比基线。

Conclusion: LAMP推动了用于设计探索、数据集生成和性能驱动优化的可控、数据高效和安全的3D生成。

Abstract: Generating high-fidelity 3D geometries that satisfy specific parameter
constraints has broad applications in design and engineering. However, current
methods typically rely on large training datasets and struggle with
controllability and generalization beyond the training distributions. To
overcome these limitations, we introduce LAMP (Linear Affine Mixing of
Parametric shapes), a data-efficient framework for controllable and
interpretable 3D generation. LAMP first aligns signed distance function (SDF)
decoders by overfitting each exemplar from a shared initialization, then
synthesizes new geometries by solving a parameter-constrained mixing problem in
the aligned weight space. To ensure robustness, we further propose a safety
metric that detects geometry validity via linearity mismatch. We evaluate LAMP
on two 3D parametric benchmarks: DrivAerNet++ and BlendedNet. We found that
LAMP enables (i) controlled interpolation within bounds with as few as 100
samples, (ii) safe extrapolation by up to 100% parameter difference beyond
training ranges, (iii) physics performance-guided optimization under fixed
parameters. LAMP significantly outperforms conditional autoencoder and Deep
Network Interpolation (DNI) baselines in both extrapolation and data
efficiency. Our results demonstrate that LAMP advances controllable,
data-efficient, and safe 3D generation for design exploration, dataset
generation, and performance-driven optimization.

</details>


### [155] [Power to the Clients: Federated Learning in a Dictatorship Setting](https://arxiv.org/abs/2510.22149)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 本文介绍了新型恶意参与者独裁客户端，提出攻击策略，分析其对学习过程的影响，研究多独裁客户端复杂场景并进行理论和实证评估。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性带来安全漏洞，恶意客户端可能破坏或操纵训练过程，因此研究新型恶意参与者及其影响。

Method: 引入独裁客户端概念，提出攻击策略，对单独裁和多独裁客户端场景进行理论分析，并在计算机视觉和自然语言处理基准上进行实证评估。

Result: 对不同场景下独裁客户端对全局模型收敛的影响进行了理论分析，实证评估支持了理论算法和发现。

Conclusion: 研究了独裁客户端对联邦学习的攻击，为理解和防范联邦学习中的恶意攻击提供了理论和实证依据。

Abstract: Federated learning (FL) has emerged as a promising paradigm for decentralized
model training, enabling multiple clients to collaboratively learn a shared
model without exchanging their local data. However, the decentralized nature of
FL also introduces vulnerabilities, as malicious clients can compromise or
manipulate the training process. In this work, we introduce dictator clients, a
novel, well-defined, and analytically tractable class of malicious participants
capable of entirely erasing the contributions of all other clients from the
server model, while preserving their own. We propose concrete attack strategies
that empower such clients and systematically analyze their effects on the
learning process. Furthermore, we explore complex scenarios involving multiple
dictator clients, including cases where they collaborate, act independently, or
form an alliance in order to ultimately betray one another. For each of these
settings, we provide a theoretical analysis of their impact on the global
model's convergence. Our theoretical algorithms and findings about the complex
scenarios including multiple dictator clients are further supported by
empirical evaluations on both computer vision and natural language processing
benchmarks.

</details>


### [156] [Quantifying Multimodal Imbalance: A GMM-Guided Adaptive Loss for Audio-Visual Learning](https://arxiv.org/abs/2510.21797)
*Zhaocheng Liu,Zhiwen Yu,Xiaoqing Liu*

Main category: cs.LG

TL;DR: 本文提出多模态不平衡定量分析方法并设计自适应损失函数，在CREMA - D和AVE数据集达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流多模态不平衡处理方法忽视模态间不平衡程度定量分析，本文旨在填补该空白。

Method: 定义“模态差距”，用双峰高斯混合模型建模其分布，用贝叶斯定理计算样本后验概率，设计有三个目标的自适应损失函数，采用两阶段训练策略。

Result: 在公共CREMA - D和AVE数据集上达到SOTA性能，准确率分别为80.65%和70.90%。

Conclusion: 所提方法有效。

Abstract: Current mainstream approaches to addressing multimodal imbalance primarily
focus on architectural modifications and optimization-based, often overlooking
a quantitative analysis of the imbalance degree between modalities. To address
this gap, our work introduces a novel method for the quantitative analysis of
multi-modal imbalance, which in turn informs the design of a sample-level
adaptive loss function.We begin by defining the "Modality Gap" as the
difference between the Softmax scores of different modalities (e.g., audio and
visual) for the ground-truth class prediction. Analysis of the Modality Gap
distribution reveals that it can be effectively modeled by a bimodal Gaussian
Mixture Model (GMM). These two components are found to correspond respectively
to "modality-balanced" and "modality-imbalanced" data samples. Subsequently, we
apply Bayes' theorem to compute the posterior probability of each sample
belonging to these two distinct distributions.Informed by this quantitative
analysis, we design a novel adaptive loss function with three objectives: (1)
to minimize the overall Modality Gap; (2) to encourage the imbalanced sample
distribution to shift towards the balanced one; and (3) to apply greater
penalty weights to imbalanced samples. We employ a two-stage training strategy
consisting of a warm-up phase followed by an adaptive training
phase.Experimental results demonstrate that our approach achieves
state-of-the-art (SOTA) performance on the public CREMA-D and AVE datasets,
attaining accuracies of $80.65\%$ and $70.90\%$, respectively. This validates
the effectiveness of our proposed methodology.

</details>


### [157] [MARS-M: When Variance Reduction Meets Matrices](https://arxiv.org/abs/2510.21800)
*Yifeng Liu,Angela Yuan,Quanquan Gu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Matrix-based preconditioned optimizers, such as Muon, have recently been
shown to be more efficient than scalar-based optimizers for training
large-scale neural networks, including large language models (LLMs). On the
other hand, recent benchmarks on optimizers for LLM pre-training have
demonstrated that variance-reduction techniques such as MARS can achieve
substantial speedups over standard optimizers that do not employ variance
reduction. In this paper, to achieve the best of both worlds, we introduce
MARS-M, a new optimizer that integrates the variance reduction technique in
MARS with Muon. Under standard regularity conditions, we prove that Muon-M
converges to a first-order stationary point at a rate of
$\tilde{\mathcal{O}}(T^{-1/3})$, which improves upon
$\tilde{\mathcal{O}}(T^{-1/4})$ rate attained by Muon. Our empirical results on
language modeling and computer vision tasks demonstrate that MARS-M
consistently yields lower losses and improved performance across various
downstream benchmarks. The implementation of MARS-M is available at
https://github.com/AGI-Arena/MARS/MARS_M.

</details>


### [158] [Towards a Generalizable AI for Materials Discovery: Validation through Immersion Coolant Screening](https://arxiv.org/abs/2510.23371)
*Hyunseung Kim,Dae-Woong Jeong,Changyoung Park,Won-Ji Lee,Ha-Eun Lee,Ji-Hye Lee,Rodrigo Hormazabal,Sung Moon Ko,Sumin Lee,Soorin Yim,Chanhui Lee,Sehui Han,Sang-Ho Cha,Woohyung Lim*

Main category: cs.LG

TL;DR: 介绍可泛化AI框架GATE，联合学习34种理化性质，应用于数据中心浸没式冷却液发现，验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型大多特定于问题，新属性需额外数据收集和重新训练，需开发可泛化的AI框架。

Method: 引入GATE框架，在共享几何空间中对齐34种理化性质，捕获跨属性相关性以减少不相交属性偏差。

Result: GATE筛选数十亿候选分子，确定92,861个有潜力分子，4个经实验或文献验证与湿实验室测量高度一致，性能与或超商业冷却液。

Conclusion: GATE是可立即使用、可泛化的AI平台，适用于多种材料发现任务。

Abstract: Artificial intelligence (AI) has emerged as a powerful accelerator of
materials discovery, yet most existing models remain problem-specific,
requiring additional data collection and retraining for each new property. Here
we introduce and validate GATE (Geometrically Aligned Transfer Encoder) -- a
generalizable AI framework that jointly learns 34 physicochemical properties
spanning thermal, electrical, mechanical, and optical domains. By aligning
these properties within a shared geometric space, GATE captures cross-property
correlations that reduce disjoint-property bias -- a key factor causing false
negatives in multi-criteria screening. To demonstrate its generalizability,
GATE -- without any problem-specific reconfiguration -- was directly applied to
the discovery of immersion cooling fluids for data centers, a stringent
real-world challenge defined by the Open Compute Project (OCP). Screening
billions of candidates, GATE identified 92,861 molecules as promising for
practical deployment. Four were experimentally or literarily validated, showing
strong agreement with wet-lab measurements and performance comparable to or
exceeding a commercial coolant. These results establish GATE as a ready-to-use,
generalizable AI platform readily applicable across diverse materials discovery
tasks.

</details>


### [159] [Sentinel: Dynamic Knowledge Distillation for Personalized Federated Intrusion Detection in Heterogeneous IoT Networks](https://arxiv.org/abs/2510.23019)
*Gurpreet Singh,Keshav Sood,P. Rajalakshmi,Yong Xiang*

Main category: cs.LG

TL;DR: 提出Sentinel框架解决FL在物联网IDS应用中的挑战，实验显示其性能优且通信高效。


<details>
  <summary>Details</summary>
Motivation: 传统FL方法在物联网IDS应用中面临严重类别不平衡、非IID数据和高通信开销等挑战，导致性能下降。

Method: 提出Sentinel框架，采用双模型架构，集成双向知识蒸馏、多方面特征对齐和类平衡损失函数，服务器使用归一化梯度聚合。

Result: 在IoTID20和5GNIDD基准数据集上，Sentinel显著优于现有联邦方法，尤其在极端数据异质性下表现出色。

Conclusion: Sentinel能有效解决传统FL在物联网IDS应用中的问题，建立新的性能基准，同时保持通信效率。

Abstract: Federated learning (FL) offers a privacy-preserving paradigm for machine
learning, but its application in intrusion detection systems (IDS) within IoT
networks is challenged by severe class imbalance, non-IID data, and high
communication overhead.These challenges severely degrade the performance of
conventional FL methods in real-world network traffic classification. To
overcome these limitations, we propose Sentinel, a personalized federated IDS
(pFed-IDS) framework that incorporates a dual-model architecture on each
client, consisting of a personalized teacher and a lightweight shared student
model. This design effectively balances deep local adaptation with efficient
global model consensus while preserving client privacy by transmitting only the
compact student model, thus reducing communication costs. Sentinel integrates
three key mechanisms to ensure robust performance: bidirectional knowledge
distillation with adaptive temperature scaling, multi-faceted feature
alignment, and class-balanced loss functions. Furthermore, the server employs
normalized gradient aggregation with equal client weighting to enhance fairness
and mitigate client drift. Extensive experiments on the IoTID20 and 5GNIDD
benchmark datasets demonstrate that Sentinel significantly outperforms
state-of-the-art federated methods, establishing a new performance benchmark,
especially under extreme data heterogeneity, while maintaining communication
efficiency.

</details>


### [160] [Accelerating Materials Design via LLM-Guided Evolutionary Search](https://arxiv.org/abs/2510.22503)
*Nikhil Abhyankar,Sanchit Kabra,Saaketh Desai,Chandan K. Reddy*

Main category: cs.LG

TL;DR: 提出LLEMA框架用于材料设计，在14个实际任务中表现出色，能加速材料发现。


<details>
  <summary>Details</summary>
Motivation: 材料发现需在化学和结构空间中满足多目标，现有方法存在不足，需要新的有效框架。

Method: 提出LLEMA框架，结合大语言模型知识、化学进化规则和基于记忆的优化，迭代中LLM提出候选，代理增强预言机估计属性，多目标评分器更新记忆。

Result: 在14个实际任务中发现化学合理、热力学稳定且符合属性的候选，比生成式和仅用LLM的基线有更高命中率和更强帕累托前沿。

Conclusion: LLEMA通过强制可合成性和多目标权衡，为加速实际材料发现提供了原则性途径。

Abstract: Materials discovery requires navigating vast chemical and structural spaces
while satisfying multiple, often conflicting, objectives. We present LLM-guided
Evolution for MAterials design (LLEMA), a unified framework that couples the
scientific knowledge embedded in large language models with chemistry-informed
evolutionary rules and memory-based refinement. At each iteration, an LLM
proposes crystallographically specified candidates under explicit property
constraints; a surrogate-augmented oracle estimates physicochemical properties;
and a multi-objective scorer updates success/failure memories to guide
subsequent generations. Evaluated on 14 realistic tasks spanning electronics,
energy, coatings, optics, and aerospace, LLEMA discovers candidates that are
chemically plausible, thermodynamically stable, and property-aligned, achieving
higher hit-rates and stronger Pareto fronts than generative and LLM-only
baselines. Ablation studies confirm the importance of rule-guided generation,
memory-based refinement, and surrogate prediction. By enforcing
synthesizability and multi-objective trade-offs, LLEMA delivers a principled
pathway to accelerate practical materials discovery.
  Code: https://github.com/scientific-discovery/LLEMA

</details>


### [161] [Residual-guided AI-CFD hybrid method enables stable and scalable simulations: from 2D benchmarks to 3D applications](https://arxiv.org/abs/2510.21804)
*Shilaj Baral,Youngkyu Lee,Sangam Khanal,Joongoo Jeon*

Main category: cs.LG

TL;DR: 提出XRePIT混合模拟策略，实现稳定加速模拟，有良好泛化性和加速效果，为工程应用铺路。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动代理因误差积累易失败，现有混合方法缺乏自动化和鲁棒性，需解决这些问题。

Method: 开发XRePIT混合模拟策略，结合机器学习加速与求解器校正，全自动化且物理感知。

Result: 实现超10000时间步的稳定加速模拟，泛化到未知边界条件，适用于3D流，加速达4.98倍，保持高物理保真度。

Conclusion: 建立成熟可扩展的混合方法，可用于现实工程。

Abstract: Purely data-driven surrogates for fluid dynamics often fail catastrophically
from error accumulation, while existing hybrid methods have lacked the
automation and robustness for practical use. To solve this, we developed
XRePIT, a novel hybrid simulation strategy that synergizes machine learning
(ML) acceleration with solver-based correction. We specifically designed our
method to be fully automated and physics-aware, ensuring the stability and
practical applicability that previous approaches lacked. We demonstrate that
this new design overcomes long-standing barriers, achieving the first stable,
accelerated rollouts for over 10,000 timesteps. The method also generalizes
robustly to unseen boundary conditions and, crucially, scales to 3D flows. Our
approach delivers speedups up to 4.98$\times$ while maintaining high physical
fidelity, resolving thermal fields with relative errors of ~1E-3 and capturing
low magnitude velocity dynamics with errors below 1E-2 ms-1. This work thus
establishes a mature and scalable hybrid method, paving the way for its use in
real-world engineering.

</details>


### [162] [AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing](https://arxiv.org/abs/2510.23053)
*Zhiyu Wang,Suman Raj,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 本文提出AirFed框架解决多无人机协作移动边缘计算系统挑战，实验证明其能降低成本、提高任务完成率和设备覆盖率，有良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 多无人机协作移动边缘计算系统在轨迹规划、任务卸载和资源分配协调上有挑战，现有方法可扩展性有限、收敛慢、知识共享效率低。

Method: 提出AirFed框架，包括设计双层动态图注意力网络、开发双Actor单Critic架构、提出基于声誉的分散式联邦学习机制。

Result: 与现有基线相比，加权成本降低42.9%，任务截止日期满足率超99%，物联网设备覆盖率达94.2%，通信开销降低54.5%，在不同规模下性能稳健。

Conclusion: AirFed框架适用于大规模无人机移动边缘计算部署。

Abstract: Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing
(MEC) systems face critical challenges in coordinating trajectory planning,
task offloading, and resource allocation while ensuring Quality of Service
(QoS) under dynamic and uncertain environments. Existing approaches suffer from
limited scalability, slow convergence, and inefficient knowledge sharing among
UAVs, particularly when handling large-scale IoT device deployments with
stringent deadline constraints. This paper proposes AirFed, a novel federated
graph-enhanced multi-agent reinforcement learning framework that addresses
these challenges through three key innovations. First, we design dual-layer
dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal
dependencies among UAVs and IoT devices, capturing both service relationships
and collaborative interactions within the network topology. Second, we develop
a dual-Actor single-Critic architecture that jointly optimizes continuous
trajectory control and discrete task offloading decisions. Third, we propose a
reputation-based decentralized federated learning mechanism with
gradient-sensitive adaptive quantization, enabling efficient and robust
knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate
that AirFed achieves 42.9% reduction in weighted cost compared to
state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2%
IoT device coverage rate, and reduces communication overhead by 54.5%.
Scalability analysis confirms robust performance across varying UAV numbers,
IoT device densities, and system scales, validating AirFed's practical
applicability for large-scale UAV-MEC deployments.

</details>


### [163] [Geographic Transferability of Machine Learning Models for Short-Term Airport Fog Forecasting](https://arxiv.org/abs/2510.21819)
*Marcelo Cerda Castillo*

Main category: cs.LG

TL;DR: 研究用无坐标特征集实现机场雾短期预报地理可迁移性，XGBoost模型测试效果好，表明物理信息特征工程可实现地理可迁移大气预报。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习模型依赖特定位置特征，难以在不同地点迁移，研究能否用无坐标特征集实现机场雾短期预报地理可迁移性。

Method: 用2002 - 2009年智利圣地亚哥数据训练XGBoost分类器，在2010 - 2012年保留集及其他多地进行零样本测试。

Result: 模型在不同距离和雾况下AUC值达0.923 - 0.947，SHAP特征排名显示模型学习到可迁移物理关系。

Conclusion: 物理信息的无坐标特征工程可产生地理可迁移的大气预报工具。

Abstract: Short-term forecasting of airport fog (visibility < 1.0 km) presents
challenges in geographic generalization because many machine learning models
rely on location-specific features and fail to transfer across sites. This
study investigates whether fundamental thermodynamic and radiative processes
can be encoded in a coordinate-free (location-independent) feature set to
enable geographic transferability. A gradient boosting classifier (XGBoost)
trained on Santiago, Chile (SCEL, 33S) data from 2002-2009 was evaluated on a
2010-2012 holdout set and under strict zero-shot tests at Puerto Montt (SCTE),
San Francisco (KSFO), and London (EGLL). The model achieved AUC values of
0.923-0.947 across distances up to 11,650 km and different fog regimes
(radiative, advective, marine). Consistent SHAP feature rankings show that
visibility persistence, solar angle, and thermal gradients dominate
predictions, suggesting the model learned transferable physical relationships
rather than site-specific patterns. Results suggest that physics-informed,
coordinate-free feature engineering can yield geographically transferable
atmospheric forecasting tools.

</details>


### [164] [Generalized Top-k Mallows Model for Ranked Choices](https://arxiv.org/abs/2510.22040)
*Shahrzad Haddadan,Sara Ahmadian*

Main category: cs.LG

TL;DR: 本文针对广义top - k Mallows模型解决相关挑战，提出新采样方案、计算选择概率的算法和主动学习算法，经数学分析和实验验证其性能并与其他模型对比。


<details>
  <summary>Details</summary>
Motivation: 经典Mallows模型在捕捉现实场景中用户偏好存在局限，扩展的top - k Mallows模型虽更实用但存在相关挑战，本文聚焦分析买家选择来解决这些挑战。

Method: 提出针对广义top - k Mallows模型的新颖采样方案、计算选择概率的高效算法和用于估计模型参数的主动学习算法，并进行数学分析。

Result: 通过对合成数据和真实数据的大量实验，证明了所提方法的可扩展性和准确性，对比了top - k列表的Mallows模型和简单的Multinomial Logit模型的预测能力。

Conclusion: 本文的贡献为关键决策场景的分析和预测提供了新工具。

Abstract: The classic Mallows model is a foundational tool for modeling user
preferences. However, it has limitations in capturing real-world scenarios,
where users often focus only on a limited set of preferred items and are
indifferent to the rest. To address this, extensions such as the top-k Mallows
model have been proposed, aligning better with practical applications. In this
paper, we address several challenges related to the generalized top-k Mallows
model, with a focus on analyzing buyer choices. Our key contributions are: (1)
a novel sampling scheme tailored to generalized top-k Mallows models, (2) an
efficient algorithm for computing choice probabilities under this model, and
(3) an active learning algorithm for estimating the model parameters from
observed choice data. These contributions provide new tools for analysis and
prediction in critical decision-making scenarios. We present a rigorous
mathematical analysis for the performance of our algorithms. Furthermore,
through extensive experiments on synthetic data and real-world data, we
demonstrate the scalability and accuracy of our proposed methods, and we
compare the predictive power of Mallows model for top-k lists compared to the
simpler Multinomial Logit model.

</details>


### [165] [Equivariant Neural Networks for General Linear Symmetries on Lie Algebras](https://arxiv.org/abs/2510.22984)
*Chankyo Kim,Sicheng Zhao,Minghan Zhu,Tzu-Yuan Lin,Maani Ghaffari*

Main category: cs.LG

TL;DR: 提出Reductive Lie Neurons (ReLNs)架构，对一般线性对称精确等变，在多任务中表现良好，为学习提供实用通用框架。


<details>
  <summary>Details</summary>
Motivation: 现有等变模型局限于简单对称，无法处理科学领域常见的一般线性变换GL(n)。

Method: 引入新颖的伴随不变双线性层，直接处理多种结构化输入，实现对李代数特征和矩阵值输入的稳定等变。

Result: 在代数基准测试、洛伦兹等变粒子物理任务、3D无人机状态估计任务中表现出色，提升轨迹精度。

Conclusion: ReLNs为在李代数和矩阵值数据上学习广泛线性群对称提供实用通用框架。

Abstract: Encoding symmetries is a powerful inductive bias for improving the
generalization of deep neural networks. However, most existing equivariant
models are limited to simple symmetries like rotations, failing to address the
broader class of general linear transformations, GL(n), that appear in many
scientific domains. We introduce Reductive Lie Neurons (ReLNs), a novel neural
network architecture exactly equivariant to these general linear symmetries.
ReLNs are designed to operate directly on a wide range of structured inputs,
including general n-by-n matrices. ReLNs introduce a novel adjoint-invariant
bilinear layer to achieve stable equivariance for both Lie-algebraic features
and matrix-valued inputs, without requiring redesign for each subgroup. This
architecture overcomes the limitations of prior equivariant networks that only
apply to compact groups or simple vector data. We validate ReLNs' versatility
across a spectrum of tasks: they outperform existing methods on algebraic
benchmarks with sl(3) and sp(4) symmetries and achieve competitive results on a
Lorentz-equivariant particle physics task. In 3D drone state estimation with
geometric uncertainty, ReLNs jointly process velocities and covariances,
yielding significant improvements in trajectory accuracy. ReLNs provide a
practical and general framework for learning with broad linear group symmetries
on Lie algebras and matrix-valued data. Project page:
https://reductive-lie-neuron.github.io/

</details>


### [166] [Unlocking Biomedical Insights: Hierarchical Attention Networks for High-Dimensional Data Interpretation](https://arxiv.org/abs/2510.21820)
*Rekha R Nair,Tina Babu,Alavikunhu Panthakkan,Hussain Al-Ahmad,Balamurugan Balusamy*

Main category: cs.LG

TL;DR: 提出分层注意力可解释网络HAIN分析生物医学数据，在TCGA数据集上表现好，兼顾准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 高维数据集需求促使开发准确且可解释的机器学习模型，传统深度学习缺乏透明度阻碍其应用。

Method: 引入HAIN架构，统一多级注意力机制、降维和解释驱动损失函数，通过梯度加权注意力提供特征级可解释性，基于原型表示提供全局模型解释。

Result: 在TCGA数据集上分类准确率达94.3%，在透明度和解释力上超越SHAP和LIME，能有效识别癌症生物标志物。

Conclusion: HAIN兼顾预测准确性和可解释性，推动精准医学和监管合规的透明AI解决方案发展。

Abstract: The proliferation of high-dimensional datasets in fields such as genomics,
healthcare, and finance has created an urgent need for machine learning models
that are both highly accurate and inherently interpretable. While traditional
deep learning approaches deliver strong predictive performance, their lack of
transparency often impedes their deployment in critical, decision-sensitive
applications. In this work, we introduce the Hierarchical Attention-based
Interpretable Network (HAIN), a novel architecture that unifies multi-level
attention mechanisms, dimensionality reduction, and explanation-driven loss
functions to deliver interpretable and robust analysis of complex biomedical
data. HAIN provides feature-level interpretability via gradientweighted
attention and offers global model explanations through prototype-based
representations. Comprehensive evaluation on The Cancer Genome Atlas (TCGA)
dataset demonstrates that HAIN achieves a classification accuracy of 94.3%,
surpassing conventional post-hoc interpretability approaches such as SHAP and
LIME in both transparency and explanatory power. Furthermore, HAIN effectively
identifies biologically relevant cancer biomarkers, supporting its utility for
clinical and research applications. By harmonizing predictive accuracy with
interpretability, HAIN advances the development of transparent AI solutions for
precision medicine and regulatory compliance.

</details>


### [167] [Towards Scaling Deep Neural Networks with Predictive Coding: Theory and Practice](https://arxiv.org/abs/2510.23323)
*Francesco Innocenti*

Main category: cs.LG

TL;DR: 论文研究脑启发算法预测编码（PC），用优化理论方法取得进展，提出新参数化方法，提升对PCN理解并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 标准算法BP能耗高且难在大脑实现，PC有潜在优势但存在问题待解决。

Method: 采用基于优化理论的理论方法，分析PC学习动态，提出新参数化方法。

Result: 表明PC学习动态可近似为信任区域方法，能利用高阶信息，新参数化方法可稳定训练100+层网络。

Conclusion: 提升了对PCN推理和学习动态的理解，指出若要与BP竞争需关注硬件协同设计。

Abstract: Backpropagation (BP) is the standard algorithm for training the deep neural
networks that power modern artificial intelligence including large language
models. However, BP is energy inefficient and unlikely to be implemented by the
brain. This thesis studies an alternative, potentially more efficient
brain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks
(PCNs) perform inference by iterative equilibration of neuron activities before
learning or weight updates. Recent work has suggested that this iterative
inference procedure provides a range of benefits over BP, such as faster
training. However, these advantages have not been consistently observed, the
inference and learning dynamics of PCNs are still poorly understood, and deep
PCNs remain practically untrainable. Here, we make significant progress towards
scaling PCNs by taking a theoretical approach grounded in optimisation theory.
First, we show that the learning dynamics of PC can be understood as an
approximate trust-region method using second-order information, despite
explicitly using only first-order local updates. Second, going beyond this
approximation, we show that PC can in principle make use of arbitrarily
higher-order information, such that for feedforward networks the effective
landscape on which PC learns is far more benign and robust to vanishing
gradients than the (mean squared error) loss landscape. Third, motivated by a
study of the inference dynamics of PCNs, we propose a new parameterisation
called ``$\mu$PC'', which for the first time allows stable training of 100+
layer networks with little tuning and competitive performance on simple tasks.
Overall, this thesis significantly advances our fundamental understanding of
the inference and learning dynamics of PCNs, while highlighting the need for
future research to focus on hardware co-design if PC is to compete with BP at
scale.

</details>


### [168] [Beyond Point Matching: Evaluating Multiscale Dubuc Distance for Time Series Similarity](https://arxiv.org/abs/2510.21824)
*Azim Ahmadzadeh,Mahsa Khazaei,Elaina Rohlfing*

Main category: cs.LG

TL;DR: 本文基于多尺度Dubuc距离（MDD），对比其与动态时间规整（DTW）在时间序列搜索和索引中的优劣，通过模拟和UCR数据集测试，还应用于实际分类任务，表明MDD性能优于DTW。


<details>
  <summary>Details</summary>
Motivation: 时间序列是高维复杂数据，其高效搜索和索引是数据挖掘长期挑战，研究MDD相对DTW的优势和局限。

Method: 基于MDD，对比与DTW，进行模拟和使用UCR的95个数据集测试，应用于实际分类任务。

Result: 在很多场景MDD显著优于DTW，应用于实际分类任务MDD也有显著提升。

Conclusion: MDD具有实用价值，在时间序列搜索和索引方面比DTW更优。

Abstract: Time series are high-dimensional and complex data objects, making their
efficient search and indexing a longstanding challenge in data mining. Building
on a recently introduced similarity measure, namely Multiscale Dubuc Distance
(MDD), this paper investigates its comparative strengths and limitations
relative to the widely used Dynamic Time Warping (DTW). MDD is novel in two key
ways: it evaluates time series similarity across multiple temporal scales and
avoids point-to-point alignment. We demonstrate that in many scenarios where
MDD outperforms DTW, the gains are substantial, and we provide a detailed
analysis of the specific performance gaps it addresses. We provide simulations,
in addition to the 95 datasets from the UCR archive, to test our hypotheses.
Finally, we apply both methods to a challenging real-world classification task
and show that MDD yields a significant improvement over DTW, underscoring its
practical utility.

</details>


### [169] [Sublinear Sketches for Approximate Nearest Neighbor and Kernel Density Estimation](https://arxiv.org/abs/2510.23039)
*Ved Danait,Srijan Das,Sujoy Bhore*

Main category: cs.LG

TL;DR: 本文为动态数据流中的ANN和A - KDE问题开发了新的草图算法，实现了次线性空间和查询时间保证，并通过实验验证了算法有效性。


<details>
  <summary>Details</summary>
Motivation: 在大规模动态数据流中，设计能保留数据基本结构特性并支持高效查询的紧凑草图。

Method: 为ANN设计了仅需存储部分输入的次线性草图，支持次线性查询时间和批量查询；为A - KDE在滑动窗口模型中提出特定大小的草图。

Result: ANN实现了内存大小和近似误差之间接近最优的权衡；A - KDE获得了滑动窗口模型下首个理论次线性草图保证；实验表明草图轻量级且误差低。

Conclusion: 提出的草图算法能有效解决动态数据流中ANN和A - KDE的查询问题，在实际应用中表现良好。

Abstract: Approximate Nearest Neighbor (ANN) search and Approximate Kernel Density
Estimation (A-KDE) are fundamental problems at the core of modern machine
learning, with broad applications in data analysis, information systems, and
large-scale decision making. In massive and dynamic data streams, a central
challenge is to design compact sketches that preserve essential structural
properties of the data while enabling efficient queries.
  In this work, we develop new sketching algorithms that achieve sublinear
space and query time guarantees for both ANN and A-KDE for a dynamic stream of
data. For ANN in the streaming model, under natural assumptions, we design a
sublinear sketch that requires only $\mathcal{O}(n^{1+\rho-\eta})$ memory by
storing only a sublinear ($n^{-\eta}$) fraction of the total inputs, where
$\rho$ is a parameter of the LSH family, and $0<\eta<1$. Our method supports
sublinear query time, batch queries, and extends to the more general Turnstile
model. While earlier works have focused on Exact NN, this is the first result
on ANN that achieves near-optimal trade-offs between memory size and
approximation error.
  Next, for A-KDE in the Sliding-Window model, we propose a sketch of size
$\mathcal{O}\left(RW \cdot \frac{1}{\sqrt{1+\epsilon} - 1} \log^2 N\right)$,
where $R$ is the number of sketch rows, $W$ is the LSH range, $N$ is the window
size, and $\epsilon$ is the approximation error. This, to the best of our
knowledge, is the first theoretical sublinear sketch guarantee for A-KDE in the
Sliding-Window model.
  We complement our theoretical results with experiments on various real-world
datasets, which show that the proposed sketches are lightweight and achieve
consistently low error in practice.

</details>


### [170] [Symbolic Neural Generation with Applications to Lead Discovery in Drug Design](https://arxiv.org/abs/2510.23379)
*Ashwin Srinivasan,A Baskar,Tirtharaj Dash,Michael Bain,Sanjay Kumar Dey,Mainak Banerjee*

Main category: cs.LG

TL;DR: 本文研究符号神经生成器（SNGs），结合符号学习与神经推理构建数据生成器，介绍语义及概率扩展，将受限ILP与LLM结合应用于早期药物设计，性能良好。


<details>
  <summary>Details</summary>
Motivation: 研究将符号学习与神经推理结合的混合神经符号模型，构建满足形式正确性标准的数据生成器。

Method: 引入SNGs，结合符号学习和神经推理，构建语义，提出概率扩展，将受限ILP与LLM结合。

Result: 在基准问题上性能与现有方法相当，在探索性问题上生成分子结合亲和力与领先临床候选药物相当，符号规范可作初步筛选。

Conclusion: SNGs在早期药物设计中表现良好，有一定应用价值。

Abstract: We investigate a relatively underexplored class of hybrid neurosymbolic
models integrating symbolic learning with neural reasoning to construct data
generators meeting formal correctness criteria. In \textit{Symbolic Neural
Generators} (SNGs), symbolic learners examine logical specifications of
feasible data from a small set of instances -- sometimes just one. Each
specification in turn constrains the conditional information supplied to a
neural-based generator, which rejects any instance violating the symbolic
specification. Like other neurosymbolic approaches, SNG exploits the
complementary strengths of symbolic and neural methods. The outcome of an SNG
is a triple $(H, X, W)$, where $H$ is a symbolic description of feasible
instances constructed from data, $X$ a set of generated new instances that
satisfy the description, and $W$ an associated weight. We introduce a semantics
for such systems, based on the construction of appropriate \textit{base} and
\textit{fibre} partially-ordered sets combined into an overall partial order,
and outline a probabilistic extension relevant to practical applications. In
this extension, SNGs result from searching over a weighted partial ordering. We
implement an SNG combining a restricted form of Inductive Logic Programming
(ILP) with a large language model (LLM) and evaluate it on early-stage drug
design. Our main interest is the description and the set of potential inhibitor
molecules generated by the SNG. On benchmark problems -- where drug targets are
well understood -- SNG performance is statistically comparable to
state-of-the-art methods. On exploratory problems with poorly understood
targets, generated molecules exhibit binding affinities on par with leading
clinical candidates. Experts further find the symbolic specifications useful as
preliminary filters, with several generated molecules identified as viable for
synthesis and wet-lab testing.

</details>


### [171] [GAPO: Group Adaptive Policy Optimization for Real-World Code Edit](https://arxiv.org/abs/2510.21830)
*Jianqing Zhang,Zhezheng Hao,Wei Xia,Hande Dong,Hong Wang,Chenxing Wei,Yuyan Zhou,Yubin Qi,Qiang Lin,Jian Cao*

Main category: cs.LG

TL;DR: 提出GAPO方法用于代码编辑中LLM的强化学习，在多个LLM上验证优于GRPO和DAPO。


<details>
  <summary>Details</summary>
Motivation: 现有代码编辑中LLM强化学习的组相对方法在真实场景奖励分布偏斜时有缺陷，优势计算失真、噪声增加。

Method: 提出Group Adaptive Policy Optimization (GAPO)，为每个提示自适应找到无异常值的最高密度区间，用该区间中位数作为自适应Q替换组均值进行优势计算。

Result: 在九个3B - 14B的指令调优LLM上，使用包含51,844个跨10种语言的真实代码编辑任务的大型内部数据集验证，GAPO在精确匹配准确率上持续优于GRPO及其变体DAPO。

Conclusion: GAPO能稳健处理偏斜分布，具有即插即用和高效的特点，代码已公开。

Abstract: Reinforcement learning (RL) is widely used for post-training large language
models (LLMs) in code editing, where group-relative methods like GRPO are
popular for their critic-free, normalized advantage estimation. However, in
real-world code-editing scenarios, reward distributions are often skewed with
unpredictable outliers, leading to distorted advantage computation and
increased noise. To address this issue, we propose Group Adaptive Policy
Optimization (GAPO), which adaptively finds an outlier-free highest-density
interval (HDI) per prompt and then uses the median of that interval as an
adaptive Q to replace the group mean in advantage calculation. This adaptive Q
robustly handles skewed distributions while remaining plug-and-play and
efficient. We validate GAPO on nine instruction-tuned LLMs (3B-14B) using a
large internal dataset of 51,844 real-world, history-aware code-editing tasks
across 10 languages, demonstrating consistent improvements in exact match
accuracy over GRPO and its variant DAPO. Code is publicly available.

</details>


### [172] [Coresets for Clustering Under Stochastic Noise](https://arxiv.org/abs/2510.23438)
*Lingxiao Huang,Zhize Li,Nisheeth K. Vishnoi,Runkai Yang,Haoyu Zhao*

Main category: cs.LG

TL;DR: 研究有随机噪声的(k, z)-聚类核心集构建问题，引入新误差度量设计算法，实验验证优势


<details>
  <summary>Details</summary>
Motivation: 在输入数据集受随机噪声干扰且真实数据集不可见时，评估核心集质量具有挑战性

Method: 研究使用可处理且与真实聚类成本相关的替代误差度量构建核心集，分析传统度量并引入新度量，基于新度量设计核心集构建算法

Result: 在数据和噪声的温和假设下，新度量构建的核心集更小，对真实聚类成本的保证更严格，核心集大小最多可提高poly(k)倍，实验支持理论发现

Conclusion: 提出的新误差度量和核心集构建算法在理论和实践上都有优势

Abstract: We study the problem of constructing coresets for $(k, z)$-clustering when
the input dataset is corrupted by stochastic noise drawn from a known
distribution. In this setting, evaluating the quality of a coreset is
inherently challenging, as the true underlying dataset is unobserved. To
address this, we investigate coreset construction using surrogate error metrics
that are tractable and provably related to the true clustering cost. We analyze
a traditional metric from prior work and introduce a new error metric that more
closely aligns with the true cost. Although our metric is defined independently
of the noise distribution, it enables approximation guarantees that scale with
the noise level. We design a coreset construction algorithm based on this
metric and show that, under mild assumptions on the data and noise, enforcing
an $\varepsilon$-bound under our metric yields smaller coresets and tighter
guarantees on the true clustering cost than those obtained via classical
metrics. In particular, we prove that the coreset size can improve by a factor
of up to $\mathrm{poly}(k)$, where $n$ is the dataset size. Experiments on
real-world datasets support our theoretical findings and demonstrate the
practical advantages of our approach.

</details>


### [173] [BBOPlace-Bench: Benchmarking Black-Box Optimization for Chip Placement](https://arxiv.org/abs/2510.23472)
*Ke Xue,Ruo-Tong Chen,Rong-Xi Tan,Xi Lin,Yunqi Shi,Siyuan Xu,Mingxuan Yuan,Chao Qian*

Main category: cs.LG

TL;DR: 文章指出芯片布局领域缺乏BBO特定基准，提出BBOPlace - Bench基准，集成多种算法，实验显示不同公式和算法性能差异，该基准促进芯片布局BBO方案开发并拓宽应用场景。


<details>
  <summary>Details</summary>
Motivation: 芯片布局中BBO虽有进展，但缺乏统一、特定的基准来评估问题公式和算法。

Method: 提出BBOPlace - Bench基准，集成三种芯片布局BBO问题公式，整合多种现有BBO算法。

Result: mask - guided优化和超参数优化公式性能优于序列对公式；EAs在高维搜索空间表现更好，相比主流方法达最优性能。

Conclusion: BBOPlace - Bench促进芯片布局高效BBO方案开发，拓宽BBO社区应用场景，代码开源。

Abstract: Chip placement is a vital stage in modern chip design as it has a substantial
impact on the subsequent processes and the overall quality of the final chip.
The use of black-box optimization (BBO) for chip placement has a history of
several decades. However, early efforts were limited by immature problem
formulations and inefficient algorithm designs. Recent progress has shown the
effectiveness and efficiency of BBO for chip placement, proving its potential
to achieve state-of-the-art results. Despite these advancements, the field
lacks a unified, BBO-specific benchmark for thoroughly assessing various
problem formulations and BBO algorithms. To fill this gap, we propose
BBOPlace-Bench, the first benchmark designed specifically for evaluating and
developing BBO algorithms for chip placement tasks. It integrates three problem
formulations of BBO for chip placement, and offers a modular, decoupled, and
flexible framework that enables users to seamlessly implement, test, and
compare their own algorithms. BBOPlace-Bench integrates a wide variety of
existing BBO algorithms, including simulated annealing (SA), evolutionary
algorithms (EAs), and Bayesian optimization (BO). Experimental results show
that the problem formulations of mask-guided optimization and hyperparameter
optimization exhibit superior performance than the sequence pair problem
formulation, while EAs demonstrate better overall performance than SA and BO,
especially in high-dimensional search spaces, and also achieve state-of-the-art
performance compared to the mainstream chip placement methods. BBOPlace-Bench
not only facilitates the development of efficient BBO-driven solutions for chip
placement but also broadens the practical application scenarios (which are
urgently needed) for the BBO community. The code of BBOPlace-Bench is available
at https://github.com/lamda-bbo/BBOPlace-Bench.

</details>


### [174] [Restoring Pruned Large Language Models via Lost Component Compensation](https://arxiv.org/abs/2510.21834)
*Zijian Feng,Hanzhang Zhou,Zixiao Zhu,Tianjiao Li,Jia Jim Deryl Chua,Lee Onn Mak,Gee Wah Ng,Kezhi Mao*

Main category: cs.LG

TL;DR: 提出针对剪枝模型的RestoreLCC方法，通过补偿丢失信息恢复性能，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝模型恢复方法多基于为密集模型设计的PEFT，忽略剪枝模型特性，恢复效果不佳。

Method: 提出RestoreLCC方法，通过激活编辑对比探测关键注意力头，从激活差异中提取丢失组件并注入对应剪枝头进行补偿恢复，兼容多种剪枝方案。

Result: RestoreLCC在通用和特定任务性能恢复上始终优于现有基线，且不影响剪枝模型的稀疏性和推理效率。

Conclusion: RestoreLCC是一种有效的剪枝模型恢复策略，能在保持低成本和高效率的同时恢复性能。

Abstract: Pruning is a widely used technique to reduce the size and inference cost of
large language models (LLMs), but it often causes performance degradation. To
mitigate this, existing restoration methods typically employ
parameter-efficient fine-tuning (PEFT), such as LoRA, to recover the pruned
model's performance. However, most PEFT methods are designed for dense models
and overlook the distinct properties of pruned models, often resulting in
suboptimal recovery. In this work, we propose a targeted restoration strategy
for pruned models that restores performance while preserving their low cost and
high efficiency. We observe that pruning-induced information loss is reflected
in attention activations, and selectively reintroducing components of this
information can significantly recover model performance. Based on this insight,
we introduce RestoreLCC (Restoring Pruned LLMs via Lost Component
Compensation), a plug-and-play method that contrastively probes critical
attention heads via activation editing, extracts lost components from
activation differences, and finally injects them back into the corresponding
pruned heads for compensation and recovery. RestoreLCC is compatible with
structured, semi-structured, and unstructured pruning schemes. Extensive
experiments demonstrate that RestoreLCC consistently outperforms
state-of-the-art baselines in both general and task-specific performance
recovery, without compromising the sparsity or inference efficiency of pruned
models.

</details>


### [175] [Sequential Multi-Agent Dynamic Algorithm Configuration](https://arxiv.org/abs/2510.23535)
*Chen Lu,Ke Xue,Lei Yuan,Yao Wang,Yaoyuan Wang,Sheng Fu,Chao Qian*

Main category: cs.LG

TL;DR: 提出Seq - MADAC框架解决复杂算法多参数依赖问题，实验显示其性能优于现有方法并具强泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未考虑复杂算法多参数间的内在依赖关系，导致结果次优。

Method: 提出顺序多智能体动态算法配置（Seq - MADAC）框架，使用顺序优势分解网络利用动作顺序信息。

Result: 从合成函数到多目标优化算法配置的实验表明，Seq - MADAC性能优于最先进的多智能体强化学习方法，且在不同问题类上有强泛化性。

Conclusion: Seq - MADAC为依赖感知的自动化算法配置建立了新范式。

Abstract: Dynamic algorithm configuration (DAC) is a recent trend in automated machine
learning, which can dynamically adjust the algorithm's configuration during the
execution process and relieve users from tedious trial-and-error tuning tasks.
Recently, multi-agent reinforcement learning (MARL) approaches have improved
the configuration of multiple heterogeneous hyperparameters, making various
parameter configurations for complex algorithms possible. However, many complex
algorithms have inherent inter-dependencies among multiple parameters (e.g.,
determining the operator type first and then the operator's parameter), which
are, however, not considered in previous approaches, thus leading to
sub-optimal results. In this paper, we propose the sequential multi-agent DAC
(Seq-MADAC) framework to address this issue by considering the inherent
inter-dependencies of multiple parameters. Specifically, we propose a
sequential advantage decomposition network, which can leverage action-order
information through sequential advantage decomposition. Experiments from
synthetic functions to the configuration of multi-objective optimization
algorithms demonstrate Seq-MADAC's superior performance over state-of-the-art
MARL methods and show strong generalization across problem classes. Seq-MADAC
establishes a new paradigm for the widespread dependency-aware automated
algorithm configuration. Our code is available at
https://github.com/lamda-bbo/seq-madac.

</details>


### [176] [A Multimodal, Multitask System for Generating E Commerce Text Listings from Images](https://arxiv.org/abs/2510.21835)
*Nayan Kumar Singh*

Main category: cs.LG

TL;DR: 本文提出端到端多任务系统，从单张图像生成基于事实的文本列表，实验证明架构优越。


<details>
  <summary>Details</summary>
Motivation: 手动生成描述和名称耗时费力，现有VLM易产生事实幻觉，单任务模型效率低且无法捕捉特征间关系。

Method: 提出两种模型架构方案：一是用多任务学习微调视觉编码器；二是引入分层生成过程。

Result: 多任务方法在R2值和F1分数上表现更好，分层生成过程降低幻觉率和延迟，不过ROUGE - L分数略逊。

Conclusion: 所提架构有优越性，能有效解决现有问题。

Abstract: Manually generating catchy descriptions and names is labor intensive and a
slow process for retailers. Although generative AI provides an automation
solution in form of Vision to Language Models (VLM), the current VLMs are prone
to factual "hallucinations". Siloed, single task models are not only
inefficient but also fail to capture interdependent relationships between
features. To address these challenges, we propose an end to end, multi task
system that generates factually grounded textual listings from a single image.
The contributions of this study are two proposals for the model architecture.
First, application of multi task learning approach for fine tuning a vision
encoder where a single vision backbone is jointly trained on attribute
prediction such as color, hemline and neck style and price regression. Second,
introduction of a hierarchical generation process where the model's own
predicted attributes are embedded in a prompt and fed to the text decoder to
improve factual consistency. The experiments demonstrate the superiority of
this architecture. The multi tasking approach outperforms both the independent
price regression, with a 3.6% better R2 Value and attribute classification,
with a 6.6% improvement F1 score. Critically, the hierarchical generation
process proves highly effective, slashing the factual hallucination rate from
12.7% to 7.1%, a 44.5% relative reduction, compared to a non hierarchical
ablation. The hierarchical approach also reduces the latency of the
autoregressive text generation process by a factor of 3.5 when compared to
direct vision to language model of similar size. One minor caveat is that the
model does perform 3.5% worse than direct vision-to-language model on ROUGE-L
score.

</details>


### [177] [COLA: Continual Learning via Autoencoder Retrieval of Adapters](https://arxiv.org/abs/2510.21836)
*Jaya Krishna Mandivarapu*

Main category: cs.LG

TL;DR: 本文提出COLA框架解决大语言模型持续学习中的灾难性遗忘问题，实验显示其能减少参数使用和内存占用，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型频繁重新训练和持续学习成本高，更新模型会导致灾难性遗忘，需要解决这些问题。

Method: 提出COLA框架，使用自动编码器学习不同任务权重的低维嵌入，在不使用数据回放或大量特定任务参数的情况下实现知识转移和防止遗忘。

Result: 在不同数据集上的实证评估表明，该方法克服了灾难性遗忘，减少了参数使用和内存大小，在多个任务和数据集上优于现有方法。

Conclusion: COLA框架使大语言模型能高效学习新任务，对先前任务性能影响小，无需保留早期训练数据，具有良好的效果。

Abstract: Learning a set of tasks over time, also known as continual learning (CL), is
one of the most challenging problems in artificial intelligence due to
catastrophic forgetting. Large language models (LLMs) are often impractical to
frequent re-training and continual learning , due to high cost of computational
resources for training. Moreover, LLM are not suitable for continual learning
as updating these models over time for acquiring new knowledge leads to
overwrites existing knowledge leading to common phenomenon know as
\textit{catastrophic forgetting}. In this paper, we aim to address these
concerns using a novel framework , COLA that employs an autoencoder to learn
capture low-dimensional embeddings of the weights associated with various
tasks. Our approach facilitates the transfer of knowledge to new tasks while
preventing catastrophic forgetting, all without using data replay or a
substantial set of task-specific parameters. Our approach, COLA, makes the LLM
efficiently learn new tasks with minimal training, insignificant performance
degradation on previous tasks, and eliminates the need for retaining earlier
training data. Empirical evaluation on different datasets ranging from task
oriented dialouge system to intent classsfication datasets showcases that our
method not only overcomes catastrophic forgetting but also achieves significant
reduction in parameter usage and memory size, across multiple tasks and
outperforming the existing state of the art methods across multiple datasets.

</details>


### [178] [KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group](https://arxiv.org/abs/2510.21844)
*Azree Nazri*

Main category: cs.LG

TL;DR: 提出KARIPAP压缩方法，在LLaMA - 2 7B实验中实现内存、参数减少，训练和推理加速，且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型参数规模大，存在计算和环境负担，现有压缩方法忽略层间复杂相关性。

Method: 提出KARIPAP，采用量子启发的张量网络压缩，使用iPEPS和TRG收缩。

Result: 在LLaMA - 2 7B实验中，内存减少93%、参数减少70%，训练加速50%，推理加速25%，精度损失2 - 3%。

Conclusion: KARIPAP表明现代大语言模型占据低维纠缠流形，可实现可扩展、节能和量子感知的AI架构。

Abstract: Large Language Models (LLMs) like ChatGPT and LLaMA drive rapid progress in
generative AI, yet their huge parameter scales create severe computational and
environmental burdens. High training costs, energy use, and limited device
deployment hinder accessibility. Existing compression - pruning, distillation,
low-rank, and quantization - reduces size but ignores complex inter-layer
correlations. We propose KARIPAP, a quantum-inspired tensor network compression
using Infinite Projected Entangled Pair States (iPEPS) and Tensor
Renormalization Group (TRG) contraction. Unlike 1D Matrix Product States, iPEPS
captures multi-directional entanglement in attention and deep transformer
layers. TRG ensures polynomial-time contraction, making tensorization feasible
while preserving key correlation geometry. Experiments on LLaMA-2 7B show up to
93% memory and 70% parameter reduction, with 50% faster training, 25% faster
inference, and only 2-3% accuracy loss. Layer-wise entanglement profiling
reveals redundancy in deeper layers, confirming their suitability for tensor
factorization. KARIPAP demonstrates that modern LLMs occupy low-dimensional
entanglement manifolds, enabling scalable, energy-efficient, and quantum-aware
AI architectures.

</details>


### [179] [Training data membership inference via Gaussian process meta-modeling: a post-hoc analysis approach](https://arxiv.org/abs/2510.21846)
*Yongchao Huang,Pengfei Zhang,Shahzad Mumtaz*

Main category: cs.LG

TL;DR: 提出基于高斯过程元建模的高效可解释方法GP - MIA进行成员推理攻击，实验显示其有高准确率和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击方法依赖影子模型或大量查询访问，实用性受限。

Method: 利用单训练模型的事后指标和可选敏感性特征，训练高斯过程分类器区分成员与非成员并提供校准不确定性估计。

Result: 在合成数据、真实欺诈检测数据、CIFAR - 10和WikiText - 2上实验，GP - MIA达到高准确率和泛化性。

Conclusion: GP - MIA是现有成员推理攻击方法的实用替代方案。

Abstract: Membership inference attacks (MIAs) test whether a data point was part of a
model's training set, posing serious privacy risks. Existing methods often
depend on shadow models or heavy query access, which limits their practicality.
We propose GP-MIA, an efficient and interpretable approach based on Gaussian
process (GP) meta-modeling. Using post-hoc metrics such as accuracy, entropy,
dataset statistics, and optional sensitivity features (e.g. gradients, NTK
measures) from a single trained model, GP-MIA trains a GP classifier to
distinguish members from non-members while providing calibrated uncertainty
estimates. Experiments on synthetic data, real-world fraud detection data,
CIFAR-10, and WikiText-2 show that GP-MIA achieves high accuracy and
generalizability, offering a practical alternative to existing MIAs.

</details>


### [180] [SynCast: Synergizing Contradictions in Precipitation Nowcasting via Diffusion Sequential Preference Optimization](https://arxiv.org/abs/2510.21847)
*Kaiyi Xu,Junchao Gong,Wenlong Zhang,Ben Fei,Lei Bai,Wanli Ouyang*

Main category: cs.LG

TL;DR: 现有深度学习方法在雷达回波降水临近预报中有局限，本文首次引入偏好优化，提出SynCast方法，通过两阶段后训练框架协同优化冲突指标。


<details>
  <summary>Details</summary>
Motivation: 现有确定性模型预测过平滑，概率生成模型性能波动，且降水临近预报评估指标存在冲突，现有模型难以同时优化冲突指标。

Method: 引入偏好优化，提出SynCast方法，采用Diffusion - SPO两阶段后训练框架，第一阶段降低FAR，第二阶段在保持FAR对齐约束下优化CSI。

Result: 未提及具体实验结果。

Conclusion: 该方法能逐步对齐冲突指标，持续实现更优性能。

Abstract: Precipitation nowcasting based on radar echoes plays a crucial role in
monitoring extreme weather and supporting disaster prevention. Although deep
learning approaches have achieved significant progress, they still face notable
limitations. For example, deterministic models tend to produce over-smoothed
predictions, which struggle to capture extreme events and fine-scale
precipitation patterns. Probabilistic generative models, due to their inherent
randomness, often show fluctuating performance across different metrics and
rarely achieve consistently optimal results. Furthermore, precipitation
nowcasting is typically evaluated using multiple metrics, some of which are
inherently conflicting. For instance, there is often a trade-off between the
Critical Success Index (CSI) and the False Alarm Ratio (FAR), making it
challenging for existing models to deliver forecasts that perform well on both
metrics simultaneously. To address these challenges, we introduce preference
optimization into precipitation nowcasting for the first time, motivated by the
success of reinforcement learning from human feedback in large language models.
Specifically, we propose SynCast, a method that employs the two-stage
post-training framework of Diffusion Sequential Preference Optimization
(Diffusion-SPO), to progressively align conflicting metrics and consistently
achieve superior performance. In the first stage, the framework focuses on
reducing FAR, training the model to effectively suppress false alarms. Building
on this foundation, the second stage further optimizes CSI with constraints
that preserve FAR alignment, thereby achieving synergistic improvements across
these conflicting metrics.

</details>


### [181] [TowerVision: Understanding and Improving Multilinguality in Vision-Language Models](https://arxiv.org/abs/2510.21849)
*André G. Viveiros,Patrick Fernandes,Saul Santos,Sonal Sannigrahi,Emmanouil Zaranis,Nuno M. Guerreiro,Amin Farajian,Pierre Colombo,Graham Neubig,André F. T. Martins*

Main category: cs.LG

TL;DR: 本文研究多语言设计选择对视觉语言模型的影响，提出TowerVision模型，在多模态多语言基准测试中表现出色，还发布VisionBlocks数据集，强调多语言训练数据重要性并公开所有资源。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型多以英语为中心，在多语言场景下效果受限，需研究多语言设计选择的影响。

Method: 对训练数据组成、编码器选择和文本主干等多语言设计选择进行综合实证研究，基于多语言文本模型Tower+构建TowerVision。

Result: TowerVision在多个多模态多语言基准测试中取得有竞争力的性能，在文化相关任务和多模态翻译中表现突出，结合视觉和文化上下文微调可超越在更大数据集上训练的现有方法。

Conclusion: 多语言视觉语言训练数据能显著提高跨语言泛化能力，指令微调的大语言模型并非总是最佳初始化点。

Abstract: Despite significant advances in vision-language models (VLMs), most existing
work follows an English-centric design process, limiting their effectiveness in
multilingual settings. In this work, we provide a comprehensive empirical study
analyzing the impact of several multilingual design choices, such as training
data composition, encoder selection, and text backbones. The result is
TowerVision, a family of open multilingual VLMs for both image-text and
video-text tasks, built upon the multilingual text-only model Tower+.
TowerVision achieves competitive performance on multiple multimodal
multilingual benchmarks and shows particular strength in culturally grounded
tasks and multimodal translation. By incorporating visual and cultural context
during fine-tuning, our models surpass existing approaches trained on
substantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image
tasks) and ViMUL-Bench (video tasks). Alongside the models, we release
VisionBlocks, a high-quality, curated vision-language dataset. Our findings
highlight that multilingual vision-language training data substantially
improves cross-lingual generalization -- both from high-resource to
underrepresented languages and vice versa -- and that instruction-tuned LLMs
are not always the optimal initialization point. To support further research,
we publicly release all models, data, and training recipes.

</details>


### [182] [Towards Interpretable Deep Learning and Analysis of Dynamical Systems via the Discrete Empirical Interpolation Method](https://arxiv.org/abs/2510.21852)
*Hojin Kim,Romit Maulik*

Main category: cs.LG

TL;DR: 提出可微框架，用DEIM进行可解释深度学习和动力系统分析，展示其在模型降维和诊断方面的作用。


<details>
  <summary>Details</summary>
Motivation: 传统DEIM固定插值点限制其对复杂时变动力学的适应性，需要改进。

Method: 先为一维粘性Burgers方程开发可微自适应DEIM公式，再将DEIM用于二维涡合并问题中预训练的神经常微分方程（NODE）的动力学分析。

Result: DEIM轨迹揭示了NODE学习动力学中有物理意义的特征，也暴露了其外推到未见流动配置时的局限性。

Conclusion: DEIM不仅可作为模型降维工具，还可作为诊断框架来理解和改进神经微分方程模型的泛化行为。

Abstract: We present a differentiable framework that leverages the Discrete Empirical
Interpolation Method (DEIM) for interpretable deep learning and dynamical
system analysis. Although DEIM efficiently approximates nonlinear terms in
projection-based reduced-order models (POD-ROM), its fixed interpolation points
limit the adaptability to complex and time-varying dynamics. To address this
limitation, we first develop a differentiable adaptive DEIM formulation for the
one-dimensional viscous Burgers equation, which allows neural networks to
dynamically select interpolation points in a computationally efficient and
physically consistent manner. We then apply DEIM as an interpretable analysis
tool for examining the learned dynamics of a pre-trained Neural Ordinary
Differential Equation (NODE) on a two-dimensional vortex-merging problem. The
DEIM trajectories reveal physically meaningful features in the learned dynamics
of NODE and expose its limitations when extrapolating to unseen flow
configurations. These findings demonstrate that DEIM can serve not only as a
model reduction tool but also as a diagnostic framework for understanding and
improving the generalization behavior of neural differential equation models.

</details>


### [183] [Joint Score-Threshold Optimization for Interpretable Risk Assessment Under Partial Supervision](https://arxiv.org/abs/2510.21934)
*Fardin Gankhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: 文章提出混合整数规划（MIP）框架优化医疗风险评估工具，处理部分监督和不对称误分类成本问题，还开发CSO松弛方法，支持治理约束以确保临床实用性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）数据虽可优化医疗风险评估工具，但存在部分监督和不对称误分类成本两个挑战阻碍标准监督学习。

Method: 提出MIP框架，通过实例可行标签集处理部分监督，纳入不对称距离感知目标，设置最小阈值间隙防止中间类别崩溃；开发CSO松弛方法。

Result: 未明确提及具体结果。

Conclusion: 该框架支持多种治理约束，能确保在临床工作流程中实际部署。

Abstract: Risk assessment tools in healthcare commonly employ point-based scoring
systems that map patients to ordinal risk categories via thresholds. While
electronic health record (EHR) data presents opportunities for data-driven
optimization of these tools, two fundamental challenges impede standard
supervised learning: (1) partial supervision arising from intervention-censored
outcomes, where only extreme categories can be reliably labeled, and (2)
asymmetric misclassification costs that increase with ordinal distance. We
propose a mixed-integer programming (MIP) framework that jointly optimizes
scoring weights and category thresholds under these constraints. Our approach
handles partial supervision through per-instance feasible label sets,
incorporates asymmetric distance-aware objectives, and prevents middle-category
collapse via minimum threshold gaps. We further develop a CSO relaxation using
softplus losses that preserves the ordinal structure while enabling efficient
optimization. The framework supports governance constraints including sign
restrictions, sparsity, and minimal modifications to incumbent tools, ensuring
practical deployability in clinical workflows.

</details>


### [184] [Privacy-preserving Decision-focused Learning for Multi-energy Systems](https://arxiv.org/abs/2510.21858)
*Yangze Zhou,Ruiyang Yao,Dalin Qin,Yixiong Jia,Yi Wang*

Main category: cs.LG

TL;DR: 提出适用于多能源系统（MES）的隐私保护决策聚焦学习（DFL）框架，能保护隐私并降低调度成本。


<details>
  <summary>Details</summary>
Motivation: 传统MES负荷预测与决策分开，DFL在MES中的实际应用存在隐私问题，需解决。

Method: 引入信息掩码保护私有数据，设计结合矩阵分解和同态加密的安全协议，开发隐私保护负荷模式识别算法。

Result: 理论分析和案例研究表明，该框架能保护隐私，且平均每日调度成本低于现有方法。

Conclusion: 所提隐私保护DFL框架适用于MES，兼具隐私保护和成本优势。

Abstract: Decision-making for multi-energy system (MES) dispatch depends on accurate
load forecasting. Traditionally, load forecasting and decision-making for MES
are implemented separately. Forecasting models are typically trained to
minimize forecasting errors, overlooking their impact on downstream
decision-making. To address this, decision-focused learning (DFL) has been
studied to minimize decision-making costs instead. However, practical adoption
of DFL in MES faces significant challenges: the process requires sharing
sensitive load data and model parameters across multiple sectors, raising
serious privacy issues. To this end, we propose a privacy-preserving DFL
framework tailored for MES. Our approach introduces information masking to
safeguard private data while enabling recovery of decision variables and
gradients required for model training. To further enhance security for DFL, we
design a safety protocol combining matrix decomposition and homomorphic
encryption, effectively preventing collusion and unauthorized data access.
Additionally, we developed a privacy-preserving load pattern recognition
algorithm, enabling the training of specialized DFL models for heterogeneous
load patterns. Theoretical analysis and comprehensive case studies, including
real-world MES data, demonstrate that our framework not only protects privacy
but also consistently achieves lower average daily dispatch costs compared to
existing methods.

</details>


### [185] [AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing](https://arxiv.org/abs/2510.21935)
*Samuel Bright-Thonney,Christina Reissel,Gaia Grosso,Nathaniel Woodward,Katya Govorkova,Andrzej Novak,Sang Eon Park,Eric Moreno,Philip Harris*

Main category: cs.LG

TL;DR: 提出AutoSciDACT用于科学数据新奇性检测，在多数据集实验中展现对异常数据高敏感性。


<details>
  <summary>Details</summary>
Motivation: 解决科学数据集新奇性检测中实验数据噪声高维、需对离群值进行统计稳健声明的问题，且现有方法难满足科学发现量化需求。

Method: 提出AutoSciDACT管道，先通过对比预训练创建低维数据表示，再用NPLM框架进行双样本测试。

Result: 在天文、物理、生物、图像和合成数据集实验中，对小注入异常数据有强敏感性。

Conclusion: AutoSciDACT是适应科学严格统计需求的新奇性检测统一管道的第一步。

Abstract: Novelty detection in large scientific datasets faces two key challenges: the
noisy and high-dimensional nature of experimental data, and the necessity of
making statistically robust statements about any observed outliers. While there
is a wealth of literature on anomaly detection via dimensionality reduction,
most methods do not produce outputs compatible with quantifiable claims of
scientific discovery. In this work we directly address these challenges,
presenting the first step towards a unified pipeline for novelty detection
adapted for the rigorous statistical demands of science. We introduce
AutoSciDACT (Automated Scientific Discovery with Anomalous Contrastive
Testing), a general-purpose pipeline for detecting novelty in scientific data.
AutoSciDACT begins by creating expressive low-dimensional data representations
using a contrastive pre-training, leveraging the abundance of high-quality
simulated data in many scientific domains alongside expertise that can guide
principled data augmentation strategies. These compact embeddings then enable
an extremely sensitive machine learning-based two-sample test using the New
Physics Learning Machine (NPLM) framework, which identifies and statistically
quantifies deviations in observed data relative to a reference distribution
(null hypothesis). We perform experiments across a range of astronomical,
physical, biological, image, and synthetic datasets, demonstrating strong
sensitivity to small injections of anomalous data across all domains.

</details>


### [186] [OpenEM: Large-scale multi-structural 3D datasets for electromagnetic methods](https://arxiv.org/abs/2510.21859)
*Shuang Wang,Xuben Wang,Fei Deng,Peifan Jiang,Jian Chen,Gianluca Fiandaca*

Main category: cs.LG

TL;DR: 提出大规模多结构三维地电数据集OpenEM，开发基于深度学习的快速正演方法，数据公开可加速深度学习在电磁法中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习在电磁法应用研究的数据集无法体现真实地质环境复杂性，且缺乏标准化公开三维地电数据集，阻碍相关进展。

Method: 构建包含九类地电模型的OpenEM数据集，开发基于深度学习的快速正演建模方法。

Result: 得到可快速部署于多种任务的OpenEM数据集，完整数据集、正演代码和训练模型公开。

Conclusion: OpenEM为常见电磁勘探系统提供统一、全面、大规模数据集，可加速深度学习在电磁法中的应用。

Abstract: With the remarkable success of deep learning, applying such techniques to EM
methods has emerged as a promising research direction to overcome the
limitations of conventional approaches. The effectiveness of deep learning
methods depends heavily on the quality of datasets, which directly influences
model performance and generalization ability. Existing application studies
often construct datasets from random one-dimensional or structurally simple
three-dimensional models, which fail to represent the complexity of real
geological environments. Furthermore, the absence of standardized, publicly
available three-dimensional geoelectric datasets continues to hinder progress
in deep learning based EM exploration. To address these limitations, we present
OpenEM, a large scale, multi structural three dimensional geoelectric dataset
that encompasses a broad range of geologically plausible subsurface structures.
OpenEM consists of nine categories of geoelectric models, spanning from simple
configurations with anomalous bodies in half space to more complex structures
such as flat layers, folded layers, flat faults, curved faults, and their
corresponding variants with anomalous bodies. Since three-dimensional forward
modeling in electromagnetics is extremely time-consuming, we further developed
a deep learning based fast forward modeling approach for OpenEM, enabling
efficient and reliable forward modeling across the entire dataset. This
capability allows OpenEM to be rapidly deployed for a wide range of tasks.
OpenEM provides a unified, comprehensive, and large-scale dataset for common EM
exploration systems to accelerate the application of deep learning in
electromagnetic methods. The complete dataset, along with the forward modeling
codes and trained models, is publicly available at
https://doi.org/10.5281/zenodo.17141981.

</details>


### [187] [The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems](https://arxiv.org/abs/2510.21861)
*Bentley DeVilling*

Main category: cs.LG

TL;DR: 研究对三种大模型在四种任务上进行测试，发现无外部反馈的自我反思推理趋于信息封闭，最小接地干预可重引信息流动，结果为设计有根据的合作推理提供原则。


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型在无外部反馈时递归自我评估常产生重新表述而非进展这一预测，探究自我修正的结构限制。

Method: 对三种模型（OpenAI GPT - 4o - mini、Anthropic Claude 3 Haiku、Google Gemini 2.0 Flash）在四种任务家族（算术、代码、解释、反思）进行跨供应商研究，每种任务在无根据自我批判和最小接地干预两种条件下迭代十次。

Result: 无根据运行中信息变化从早期到晚期下降55%，接地运行在干预后信息变化反弹28%且保持非零方差，多种互补测量指标结果一致。

Conclusion: 生成推理中自我修正存在结构限制，无信息交换时递归推理趋于认知停滞，最小接地可重引信息流动，镜像循环源于共享自回归训练目标，研究结果为设计有根据的合作推理提供原则。

Abstract: Large language models are often described as capable of reflective reasoning,
yet recursive self-evaluation without external feedback frequently yields
reformulation rather than progress. We test this prediction in a cross-provider
study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini,
Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families
(arithmetic, code, explanation, reflection), each iterated ten times under two
conditions: ungrounded self-critique and a minimal grounding intervention (a
single verification step at iteration three). Mean informational change (delta
I, measured via normalized edit distance) declined by 55% from early (0.193) to
late (0.087) iterations in ungrounded runs, with consistent patterns across all
three providers. Grounded runs showed a +28% rebound in informational change
immediately after the intervention and sustained non-zero variance thereafter.
Complementary measures-n-gram novelty, embedding drift, and character-level
entropy-converged on the same pattern: reflection without contact tends toward
informational closure. We interpret this as evidence for a structural limit on
self-correction in generative reasoning: without an exchange of information
with an independent verifier or environment, recursive inference approaches an
attractor state of epistemic stasis. Minimal grounding functions as dissipative
coupling, reintroducing informational flux. The cross-architecture consistency
suggests the mirror loop arises from shared autoregressive training objectives
rather than provider-specific alignment schemes. The results delineate when
reflection is performative rather than epistemic and motivate design principles
for grounded, cooperative reasoning. Materials and code are publicly available.

</details>


### [188] [Revisiting Orbital Minimization Method for Neural Operator Decomposition](https://arxiv.org/abs/2510.21952)
*J. Jon Ryu,Samuel Zhou,Gregory W. Wornell*

Main category: cs.LG

TL;DR: 本文重探轨道最小化方法（OMM），证明其目标一致性，揭示其与多领域想法的联系，将其用于训练神经网络分解半正定算子，并展示其在基准任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 光谱分解在机器学习和科学计算中很重要，近期有研究探索用神经网络近似算子特征函数，本文旨在证明经典的OMM方法在现代学习流程中更广泛的适用性。

Method: 为OMM目标的一致性提供线性代数证明，揭示其与不同领域独立出现的想法的联系，将该框架应用于训练神经网络分解半正定算子。

Result: 在一系列基准任务中展示了该方法的实际优势。

Conclusion: 从现代理论和计算角度重探经典数值方法，可为在数值模拟中部署神经网络提供原则性方法，也能为机器学习提供有效且可扩展的工具。

Abstract: Spectral decomposition of linear operators plays a central role in many areas
of machine learning and scientific computing. Recent work has explored training
neural networks to approximate eigenfunctions of such operators, enabling
scalable approaches to representation learning, dynamical systems, and partial
differential equations (PDEs). In this paper, we revisit a classical
optimization framework from the computational physics literature known as the
\emph{orbital minimization method} (OMM), originally proposed in the 1990s for
solving eigenvalue problems in computational chemistry. We provide a simple
linear-algebraic proof of the consistency of the OMM objective, and reveal
connections between this method and several ideas that have appeared
independently across different domains. Our primary goal is to justify its
broader applicability in modern learning pipelines. We adapt this framework to
train neural networks to decompose positive semidefinite operators, and
demonstrate its practical advantages across a range of benchmark tasks. Our
results highlight how revisiting classical numerical methods through the lens
of modern theory and computation can provide not only a principled approach for
deploying neural networks in numerical simulation, but also effective and
scalable tools for machine learning.

</details>


### [189] [The Principles of Diffusion Models](https://arxiv.org/abs/2510.21890)
*Chieh-Hsin Lai,Yang Song,Dongjun Kim,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.LG

TL;DR: 本文介绍扩散模型发展的核心原则，阐述三种互补视角，讨论可控生成等内容，为有基础深度学习知识的读者提供理解。


<details>
  <summary>Details</summary>
Motivation: 让读者对扩散模型有概念和数学基础的理解。

Method: 介绍扩散模型的正向过程和反向过程，阐述变分、基于分数、基于流三种互补视角。

Result: 明确三种视角的共性是时变速度场，采样相当于求解微分方程。

Conclusion: 为有基础深度学习知识的读者提供对扩散模型的理解。

Abstract: This monograph presents the core principles that have guided the development
of diffusion models, tracing their origins and showing how diverse formulations
arise from shared mathematical ideas. Diffusion modeling starts by defining a
forward process that gradually corrupts data into noise, linking the data
distribution to a simple prior through a continuum of intermediate
distributions. The goal is to learn a reverse process that transforms noise
back into data while recovering the same intermediates. We describe three
complementary views. The variational view, inspired by variational
autoencoders, sees diffusion as learning to remove noise step by step. The
score-based view, rooted in energy-based modeling, learns the gradient of the
evolving data distribution, indicating how to nudge samples toward more likely
regions. The flow-based view, related to normalizing flows, treats generation
as following a smooth path that moves samples from noise to data under a
learned velocity field. These perspectives share a common backbone: a
time-dependent velocity field whose flow transports a simple prior to the data.
Sampling then amounts to solving a differential equation that evolves noise
into data along a continuous trajectory. On this foundation, the monograph
discusses guidance for controllable generation, efficient numerical solvers,
and diffusion-motivated flow-map models that learn direct mappings between
arbitrary times. It provides a conceptual and mathematically grounded
understanding of diffusion models for readers with basic deep-learning
knowledge.

</details>


### [190] [Deep Jump Gaussian Processes for Surrogate Modeling of High-Dimensional Piecewise Continuous Functions](https://arxiv.org/abs/2510.21974)
*Yang Xu,Chiwoo Park*

Main category: cs.LG

TL;DR: 提出Deep Jump Gaussian Processes (DJGP)用于高维分段连续函数代理建模，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 克服传统Jump Gaussian Processes在高维输入空间的局限性。

Method: 在Jump Gaussian Processes中添加局部线性投影层，用特定区域矩阵捕捉局部子空间结构；对投影矩阵设置高斯过程先验；开发可扩展变分推理算法联合学习投影矩阵和JGP超参数。

Result: 在合成和基准数据集实验中，DJGP有更好的预测精度和更可靠的不确定性量化。

Conclusion: DJGP是一种有效的高维分段连续函数代理建模方法，优于现有方法。

Abstract: We introduce Deep Jump Gaussian Processes (DJGP), a novel method for
surrogate modeling of high-dimensional piecewise continuous functions. DJGP
overcomes the limitations of conventional Jump Gaussian Processes in
high-dimensional input spaces by adding a locally linear projection layer to
Jump Gaussian Processes. This projection uses region-specific matrices to
capture local subspace structures, naturally complementing the localized nature
of JGP, a variant of local Gaussian Processes. To control model complexity, we
place a Gaussian Process prior on the projection matrices, allowing them to
evolve smoothly across the input space. The projected inputs are then modeled
with a JGP to capture piecewise continuous relationships with the response.
This yields a distinctive two-layer deep learning of GP/JGP. We further develop
a scalable variational inference algorithm to jointly learn the projection
matrices and JGP hyperparameters. Experiments on synthetic and benchmark
datasets demonstrate that DJGP delivers superior predictive accuracy and more
reliable uncertainty quantification compared to existing approaches.

</details>


### [191] [A supervised discriminant data representation: application to pattern classification](https://arxiv.org/abs/2510.21898)
*Fadi Dornaika,Ahmad Khoder,Abdelmalik Moujahid,Wassim Khoder*

Main category: cs.LG

TL;DR: 本文提出用于监督多类分类问题的混合线性特征提取方案，实验显示该方法多数情况下优于竞品。


<details>
  <summary>Details</summary>
Motivation: 机器学习和模式识别算法性能依赖数据表示，当前需设计支持有效机器学习的预处理框架和数据转换。

Method: 受RSLDA和ICS_DLSR启发提出统一准则，利用稀疏促进技术选择特征和保持样本行稀疏一致性，用迭代交替最小化方案估计线性变换和正交矩阵。

Result: 在多个数据集上的实验表明，该方法多数情况下优于其他竞争方法。

Conclusion: 所提出的框架具有通用性，能组合和调整其他线性判别嵌入方法。

Abstract: The performance of machine learning and pattern recognition algorithms
generally depends on data representation. That is why, much of the current
effort in performing machine learning algorithms goes into the design of
preprocessing frameworks and data transformations able to support effective
machine learning. The method proposed in this work consists of a hybrid linear
feature extraction scheme to be used in supervised multi-class classification
problems. Inspired by two recent linear discriminant methods: robust sparse
linear discriminant analysis (RSLDA) and inter-class sparsitybased
discriminative least square regression (ICS_DLSR), we propose a unifying
criterion that is able to retain the advantages of these two powerful methods.
The resulting transformation relies on sparsity-promoting techniques both to
select the features that most accurately represent the data and to preserve the
row-sparsity consistency property of samples from the same class. The linear
transformation and the orthogonal matrix are estimated using an iterative
alternating minimization scheme based on steepest descent gradient method and
different initialization schemes. The proposed framework is generic in the
sense that it allows the combination and tuning of other linear discriminant
embedding methods. According to the experiments conducted on several datasets
including faces, objects, and digits, the proposed method was able to
outperform competing methods in most cases.

</details>


### [192] [Boltzmann Graph Ensemble Embeddings for Aptamer Libraries](https://arxiv.org/abs/2510.21980)
*Starlika Bauskar,Jade Jiao,Narayanan Kannan,Alexander Kimm,Justin M. Baker,Matthew J. Tyler,Andrea L. Bertozzi,Anne M. Andrews*

Main category: cs.LG

TL;DR: 提出热力学参数化指数族随机图嵌入，在SELEX数据集评估，能实现鲁棒社区检测和子图级解释，可用于识别低丰度适体候选物。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法多基于单图（如MFE结构），而SELEX数据集存在实验偏差，需新方法处理。

Method: 引入热力学参数化指数族随机图（ERGM）嵌入，将分子建模为相互作用图的玻尔兹曼加权集合。

Result: 该嵌入方法能在有偏差观测下实现鲁棒的社区检测和子图级适体配体亲和力解释。

Conclusion: 此方法可用于识别低丰度适体候选物作进一步实验评估。

Abstract: Machine-learning methods in biochemistry commonly represent molecules as
graphs of pairwise intermolecular interactions for property and structure
predictions. Most methods operate on a single graph, typically the minimal free
energy (MFE) structure, for low-energy ensembles (conformations) representative
of structures at thermodynamic equilibrium. We introduce a thermodynamically
parameterized exponential-family random graph (ERGM) embedding that models
molecules as Boltzmann-weighted ensembles of interaction graphs. We evaluate
this embedding on SELEX datasets, where experimental biases (e.g., PCR
amplification or sequencing noise) can obscure true aptamer-ligand affinity,
producing anomalous candidates whose observed abundance diverges from their
actual binding strength. We show that the proposed embedding enables robust
community detection and subgraph-level explanations for aptamer ligand
affinity, even in the presence of biased observations. This approach may be
used to identify low-abundance aptamer candidates for further experimental
evaluation.

</details>


### [193] [Adversarial Déjà Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks](https://arxiv.org/abs/2510.21910)
*Mahavir Dabas,Tran Huynh,Nikhil Reddy Billa,Jiachen T. Wang,Peng Gao,Charith Peris,Yao Ma,Rahul Gupta,Ming Jin,Prateek Mittal,Ruoxi Jia*

Main category: cs.LG

TL;DR: 论文提出对抗性似曾相识假说，基于此提出ASCoT训练方法提升大语言模型对未见过越狱攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受越狱攻击，现有对抗训练方法在新越狱攻击上常失败，需新防御范式。

Method: 分析32篇攻击论文，提取压缩对抗技能到稀疏字典，提出Adversarial Skill Compositional Training（ASCoT），对技能原语的不同组合进行训练。

Result: ASCoT显著提升对未见过攻击（包括多轮越狱）的鲁棒性，保持低过度拒绝率。

Conclusion: 扩大对抗技能覆盖范围而非仅数据规模，是防御新型攻击的关键。

Abstract: Large language models remain vulnerable to jailbreak attacks that bypass
safety guardrails to elicit harmful outputs. Defending against novel jailbreaks
represents a critical challenge in AI safety. Adversarial training -- designed
to make models robust against worst-case perturbations -- has been the dominant
paradigm for adversarial robustness. However, due to optimization challenges
and difficulties in defining realistic threat models, adversarial training
methods often fail on newly developed jailbreaks in practice. This paper
proposes a new paradigm for improving robustness against unseen jailbreaks,
centered on the Adversarial D\'ej\`a Vu hypothesis: novel jailbreaks are not
fundamentally new, but largely recombinations of adversarial skills from
previous attacks. We study this hypothesis through a large-scale analysis of 32
attack papers published over two years. Using an automated pipeline, we extract
and compress adversarial skills into a sparse dictionary of primitives, with
LLMs generating human-readable descriptions. Our analysis reveals that unseen
attacks can be effectively explained as sparse compositions of earlier skills,
with explanatory power increasing monotonically as skill coverage grows. Guided
by this insight, we introduce Adversarial Skill Compositional Training (ASCoT),
which trains on diverse compositions of skill primitives rather than isolated
attack instances. ASCoT substantially improves robustness to unseen attacks,
including multi-turn jailbreaks, while maintaining low over-refusal rates. We
also demonstrate that expanding adversarial skill coverage, not just data
scale, is key to defending against novel attacks.
\textcolor{red}{\textbf{Warning: This paper contains content that may be
harmful or offensive in nature.

</details>


### [194] [Is Temporal Difference Learning the Gold Standard for Stitching in RL?](https://arxiv.org/abs/2510.21995)
*Michał Bortkiewicz,Władysław Pałucki,Mateusz Ostaszewski,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文实证研究强化学习中经验拼接能力，发现蒙特卡罗方法也能实现经验拼接，传统TD方法优势不大，大模型时代传统TD归纳偏置必要性降低。


<details>
  <summary>Details</summary>
Motivation: 探究在使用函数逼近的情况下，关于经验拼接的传统观点是否成立。

Method: 通过实证研究对比蒙特卡罗（MC）方法和时间差分（TD）方法的经验拼接能力。

Result: MC方法能实现经验拼接，TD方法能力稍强但差距小于大小神经网络间的差距，增加评判器容量可减少泛化差距。

Conclusion: 大模型时代传统TD归纳偏置对拼接的必要性降低，拼接或可通过扩大规模实现。

Abstract: Reinforcement learning (RL) promises to solve long-horizon tasks even when
training data contains only short fragments of the behaviors. This experience
stitching capability is often viewed as the purview of temporal difference (TD)
methods. However, outside of small tabular settings, trajectories never
intersect, calling into question this conventional wisdom. Moreover, the common
belief is that Monte Carlo (MC) methods should not be able to recombine
experience, yet it remains unclear whether function approximation could result
in a form of implicit stitching. The goal of this paper is to empirically study
whether the conventional wisdom about stitching actually holds in settings
where function approximation is used. We empirically demonstrate that Monte
Carlo (MC) methods can also achieve experience stitching. While TD methods do
achieve slightly stronger capabilities than MC methods (in line with
conventional wisdom), that gap is significantly smaller than the gap between
small and large neural networks (even on quite simple tasks). We find that
increasing critic capacity effectively reduces the generalization gap for both
the MC and TD methods. These results suggest that the traditional TD inductive
bias for stitching may be less necessary in the era of large models for RL and,
in some cases, may offer diminishing returns. Additionally, our results suggest
that stitching, a form of generalization unique to the RL setting, might be
achieved not through specialized algorithms (temporal difference learning) but
rather through the same recipe that has provided generalization in other
machine learning settings (via scale). Project website:
https://michalbortkiewicz.github.io/golden-standard/

</details>


### [195] [From Black-box to Causal-box: Towards Building More Interpretable Models](https://arxiv.org/abs/2510.21998)
*Inwoo Hwang,Yushu Pan,Elias Bareinboim*

Main category: cs.LG

TL;DR: 本文引入因果可解释性概念，分析两类常见模型，开发构建可因果解释模型的框架，指出因果可解释性和预测准确性间的权衡，实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型预测的理解难题，特别是在高风险应用中，通过让模型回答反事实问题提供对模型推理的洞察。

Method: 引入因果可解释性概念，分析黑盒和基于概念的预测器两类模型，开发构建可因果解释模型的框架，推导图形标准，确定特征最大集。

Result: 两类常见模型一般不具有因果可解释性，存在因果可解释性和预测准确性的权衡，实验验证了理论发现。

Conclusion: 开发的框架能构建具有因果可解释性的模型，明确了因果可解释性和预测准确性的权衡关系。

Abstract: Understanding the predictions made by deep learning models remains a central
challenge, especially in high-stakes applications. A promising approach is to
equip models with the ability to answer counterfactual questions --
hypothetical ``what if?'' scenarios that go beyond the observed data and
provide insight into a model reasoning. In this work, we introduce the notion
of causal interpretability, which formalizes when counterfactual queries can be
evaluated from a specific class of models and observational data. We analyze
two common model classes -- blackbox and concept-based predictors -- and show
that neither is causally interpretable in general. To address this gap, we
develop a framework for building models that are causally interpretable by
design. Specifically, we derive a complete graphical criterion that determines
whether a given model architecture supports a given counterfactual query. This
leads to a fundamental tradeoff between causal interpretability and predictive
accuracy, which we characterize by identifying the unique maximal set of
features that yields an interpretable model with maximal predictive
expressiveness. Experiments corroborate the theoretical findings.

</details>


### [196] [Optimal Detection for Language Watermarks with Pseudorandom Collision](https://arxiv.org/abs/2510.22007)
*T. Tony Cai,Xiang Li,Qi Long,Weijie J. Su,Garrett G. Wen*

Main category: cs.LG

TL;DR: 提出统计框架解决大语言模型文本水印检测在非完美伪随机性下的问题，理论和实验证明其能提升检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型文本水印方法多假设完美伪随机性，实际中生成文本的重复会导致碰撞，影响第一类错误控制和标准分析。

Method: 引入通过分层两层分区捕捉结构的统计框架，定义最小单元，将水印检测转化为极小极大假设检验问题。

Result: 应用于Gumbel - max和逆变换水印时产生封闭形式的最优规则，解释了丢弃重复统计量可提升性能，理论和实验证实能提升检测能力并严格控制第一类错误。

Conclusion: 为非完美伪随机性下的水印检测提供了首个有原则的基础，兼具理论洞察与实际指导意义。

Abstract: Text watermarking plays a crucial role in ensuring the traceability and
accountability of large language model (LLM) outputs and mitigating misuse.
While promising, most existing methods assume perfect pseudorandomness. In
practice, repetition in generated text induces collisions that create
structured dependence, compromising Type I error control and invalidating
standard analyses.
  We introduce a statistical framework that captures this structure through a
hierarchical two-layer partition. At its core is the concept of minimal units
-- the smallest groups treatable as independent across units while permitting
dependence within. Using minimal units, we define a non-asymptotic efficiency
measure and cast watermark detection as a minimax hypothesis testing problem.
  Applied to Gumbel-max and inverse-transform watermarks, our framework
produces closed-form optimal rules. It explains why discarding repeated
statistics often improves performance and shows that within-unit dependence
must be addressed unless degenerate. Both theory and experiments confirm
improved detection power with rigorous Type I error control. These results
provide the first principled foundation for watermark detection under imperfect
pseudorandomness, offering both theoretical insight and practical guidance for
reliable tracing of model outputs.

</details>


### [197] [Generalization Bounds for Rank-sparse Neural Networks](https://arxiv.org/abs/2510.21945)
*Antoine Ledent,Rodrigo Alves,Yunwen Lei*

Main category: cs.LG

TL;DR: 本文研究神经网络瓶颈秩特性对泛化的影响，证明了利用权重矩阵近似低秩结构的泛化界。


<details>
  <summary>Details</summary>
Motivation: 探究神经网络瓶颈秩特性（训练时激活和权重趋于低秩）对泛化的影响。

Method: 证明利用权重矩阵近似低秩结构的神经网络泛化界，结果依赖权重矩阵的Schatten p拟范数。

Result: 对于小的p，泛化界样本复杂度为\( \widetilde{O}(WrL^2) \)；p增大时，更像基于范数的界。

Conclusion: 神经网络的低秩结构可用于推导泛化界，不同p值下泛化界表现不同。

Abstract: It has been recently observed in much of the literature that neural networks
exhibit a bottleneck rank property: for larger depths, the activation and
weights of neural networks trained with gradient-based methods tend to be of
approximately low rank. In fact, the rank of the activations of each layer
converges to a fixed value referred to as the ``bottleneck rank'', which is the
minimum rank required to represent the training data. This perspective is in
line with the observation that regularizing linear networks (without
activations) with weight decay is equivalent to minimizing the Schatten $p$
quasi norm of the neural network. In this paper we investigate the implications
of this phenomenon for generalization. More specifically, we prove
generalization bounds for neural networks which exploit the approximate low
rank structure of the weight matrices if present. The final results rely on the
Schatten $p$ quasi norms of the weight matrices: for small $p$, the bounds
exhibit a sample complexity $ \widetilde{O}(WrL^2)$ where $W$ and $L$ are the
width and depth of the neural network respectively and where $r$ is the rank of
the weight matrices. As $p$ increases, the bound behaves more like a norm-based
bound instead.

</details>


### [198] [Cost-Sensitive Evaluation for Binary Classifiers](https://arxiv.org/abs/2510.22016)
*Pierangelo Lombardo,Antonio Casoli,Cristian Cingolani,Shola Oshodi,Michele Zanatta*

Main category: cs.LG

TL;DR: 论文定义加权准确率（WA）评估指标，明确处理类别不平衡的概念框架，提出估计WA权重参数的方法并证明其稳健性。


<details>
  <summary>Details</summary>
Motivation: 选择合适的分类器评估指标对模型比较和参数优化至关重要，但缺乏通用标准，且对缓解数据集不平衡存在误解，最终目标是最小化总分类成本（TCC）。

Method: 定义WA作为二分类器评估指标，明确处理类别不平衡的概念框架，提出在未完全指定单位分类成本（UCCs）时估计WA权重参数的程序。

Result: 提出的框架可应用于能表示为示例依赖量线性组合的指标，可比较不同数据集结果，明确使用不考虑UCCs的类别重平衡技术或重平衡指标何时有利于、何时不利于TCC最小化。

Conclusion: WA指标具有明确解释，与最小化TCC需求一致，且通过分析其与TCC的相关性证明了稳健性。

Abstract: Selecting an appropriate evaluation metric for classifiers is crucial for
model comparison and parameter optimization, yet there is not consensus on a
universally accepted metric that serves as a definitive standard. Moreover,
there is often a misconception about the perceived need to mitigate imbalance
in datasets used to train classification models. Since the final goal in
classifier optimization is typically maximizing the return of investment or,
equivalently, minimizing the Total Classification Cost (TCC), we define
Weighted Accuracy (WA), an evaluation metric for binary classifiers with a
straightforward interpretation as a weighted version of the well-known accuracy
metric, coherent with the need of minimizing TCC. We clarify the conceptual
framework for handling class imbalance in cost-sensitive scenarios, providing
an alternative to rebalancing techniques. This framework can be applied to any
metric that, like WA, can be expressed as a linear combination of
example-dependent quantities and allows for comparing the results obtained in
different datasets and for addressing discrepancies between the development
dataset, used to train and validate the model, and the target dataset, where
the model will be deployed. It also specifies in which scenarios using
UCCs-unaware class rebalancing techniques or rebalancing metrics aligns with
TCC minimization and when it is instead counterproductive. Finally, we propose
a procedure to estimate the WA weight parameter in the absence of fully
specified UCCs and demonstrate the robustness of WA by analyzing its
correlation with TCC in example-dependent scenarios.

</details>


### [199] [K-DAREK: Distance Aware Error for Kurkova Kolmogorov Networks](https://arxiv.org/abs/2510.22021)
*Masoud Ataei,Vikas Dhiman,Mohammad Javad Khojasteh*

Main category: cs.LG

TL;DR: 本文提出K - DAREK算法用于高效可解释的函数逼近与不确定性量化，通过案例研究展示其在速度、计算效率、可扩展性和安全性上的优势。


<details>
  <summary>Details</summary>
Motivation: 神经网络架构选择影响性能，高斯过程处理大规模问题计算成本高，需开发高效可解释的函数逼近方法。

Method: 增强Kurkova - Kolmogorov网络（KKAN）架构，开发距离感知误差算法K - DAREK。

Result: K - DAREK比KANs集成快约四倍、计算效率高十倍，比GP可扩展性高8.6倍，比DAREK安全性高50%。

Conclusion: K - DAREK是一种高效、可解释且能进行不确定性量化的函数逼近方法，适用于动态系统中的算子估计和系统建模。

Abstract: Neural networks are parametric and powerful tools for function approximation,
and the choice of architecture heavily influences their interpretability,
efficiency, and generalization. In contrast, Gaussian processes (GPs) are
nonparametric probabilistic models that define distributions over functions
using a kernel to capture correlations among data points. However, these models
become computationally expensive for large-scale problems, as they require
inverting a large covariance matrix. Kolmogorov- Arnold networks (KANs),
semi-parametric neural architectures, have emerged as a prominent approach for
modeling complex functions with structured and efficient representations
through spline layers. Kurkova Kolmogorov-Arnold networks (KKANs) extend this
idea by reducing the number of spline layers in KAN and replacing them with
Chebyshev layers and multi-layer perceptrons, thereby mapping inputs into
higher-dimensional spaces before applying spline-based transformations.
Compared to KANs, KKANs perform more stable convergence during training, making
them a strong architecture for estimating operators and system modeling in
dynamical systems. By enhancing the KKAN architecture, we develop a novel
learning algorithm, distance-aware error for Kurkova-Kolmogorov networks
(K-DAREK), for efficient and interpretable function approximation with
uncertainty quantification. Our approach establishes robust error bounds that
are distance-aware; this means they reflect the proximity of a test point to
its nearest training points. Through case studies on a safe control task, we
demonstrate that K-DAREK is about four times faster and ten times higher
computationally efficiency than Ensemble of KANs, 8.6 times more scalable than
GP by increasing the data size, and 50% safer than our previous work
distance-aware error for Kolmogorov networks (DAREK).

</details>


### [200] [Transformer Based Linear Attention with Optimized GPU Kernel Implementation](https://arxiv.org/abs/2510.21956)
*Armin Gerami,Ramani Duraiswami*

Main category: cs.LG

TL;DR: 提出线性注意力机制新方法及优化CUDA实现，速度提升3.3倍，内存消耗降3.6倍，在语言模型训练中验证效果。


<details>
  <summary>Details</summary>
Motivation: Transformer架构成功，需提升其训练和推理的运行时间，现有线性注意力机制实践中未达理论效率。

Method: 提出线性注意力机制前向和反向传播的新方法，并进行高度优化的CUDA实现。

Result: 速度比现有技术快3.3倍，内存消耗降低3.6倍，训练14亿参数语言模型验证效果。

Conclusion: 新方法在提升速度和减少内存消耗方面表现出色，在主要推理基准测试中与常规注意力有相似表现力。

Abstract: The original softmax-based attention mechanism (regular attention) in the
extremely successful Transformer architecture computes attention between $N$
tokens, each embedded in a $D$-dimensional head, with a time complexity of
$O(N^2D)$. Given the success of Transformers, improving their runtime during
both training and inference is a popular research area. One such approach is
the introduction of the linear attention (LA) mechanisms, which offers a linear
time complexity of $O(ND^2)$ and have demonstrated comparable accuracy to
regular attention. However, LA in practice lags behind its theoretical
efficiency. We propose a novel method for LA's forward and backward passes,
along with a highly-optimized CUDA implementation. Our approach outperforms the
state-of-the-art by 3.3 times in speed and reduces memory consumption by 3.6
times. We validate these improvements in both single-layer and end-to-end
settings by training a 1.4 billion parameter language model, which demonstrates
similar expressivity to regular attention on major reasoning benchmarks.

</details>


### [201] [Online Optimization for Offline Safe Reinforcement Learning](https://arxiv.org/abs/2510.22027)
*Yassine Chemingui,Aryan Deshwal,Alan Fern,Thanh Nguyen-Tang,Janardhan Rao Doppa*

Main category: cs.LG

TL;DR: 提出新的离线安全强化学习方法，结合离线RL与在线优化算法，证明近似最优性，实验显示能可靠执行安全约束并获高奖励。


<details>
  <summary>Details</summary>
Motivation: 解决离线安全强化学习中在累积成本约束下从固定数据学习奖励最大化策略的问题。

Method: 将问题构建为极小极大目标，结合离线RL和在线优化算法求解，还给出可与任何离线RL算法结合的实用近似方法。

Result: 在DSRL基准测试中，该方法能在严格成本预算下可靠执行安全约束，同时获得高奖励。

Conclusion: 所提方法在离线安全强化学习中有效，能兼顾安全约束和高奖励。

Abstract: We study the problem of Offline Safe Reinforcement Learning (OSRL), where the
goal is to learn a reward-maximizing policy from fixed data under a cumulative
cost constraint. We propose a novel OSRL approach that frames the problem as a
minimax objective and solves it by combining offline RL with online
optimization algorithms. We prove the approximate optimality of this approach
when integrated with an approximate offline RL oracle and no-regret online
optimization. We also present a practical approximation that can be combined
with any offline RL algorithm, eliminating the need for offline policy
evaluation. Empirical results on the DSRL benchmark demonstrate that our method
reliably enforces safety constraints under stringent cost budgets, while
achieving high rewards. The code is available at
https://github.com/yassineCh/O3SRL.

</details>


### [202] [Parallel Sampling from Masked Diffusion Models via Conditional Independence Testing](https://arxiv.org/abs/2510.21961)
*Iskander Azangulov,Teodora Pandeva,Niranjani Prasad,Javier Zazo,Sushrut Karmalkar*

Main category: cs.LG

TL;DR: 本文提出模型无关采样器PUNT，调和并行采样中独立性和置信度的矛盾，实验表明其在准确性和计算量间取得更好权衡，尤其对长序列生成效果好。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型用于离散文本生成可并行采样，但有效并行采样面临独立性和高置信度预测的矛盾，需调和这一权衡。

Method: 提出PUNT，识别令牌依赖关系，从冲突组中移除低置信度令牌，通过近似条件独立性测试实现改进的并行解掩码。

Result: PUNT在准确性和计算量间取得更好权衡，在IFEval基准上比基线方法准确率高16%，不同超参数下效果稳定，还诱导出分层生成策略。

Conclusion: PUNT在离散文本生成中表现良好，尤其适合长序列，减少超参数调整需求，且有类似规划的生成过程。

Abstract: Masked diffusion models (MDMs) offer a compelling alternative to
autoregressive models (ARMs) for discrete text generation because they enable
parallel token sampling, rather than sequential, left-to-right generation. This
means potentially much faster inference. However, effective parallel sampling
faces two competing requirements: (i) simultaneously updated tokens must be
conditionally independent, and (ii) updates should prioritise high-confidence
predictions. These goals conflict because high-confidence predictions often
cluster and depend on each other, opportunities for parallel updates.
  We present PUNT, a model-agnostic sampler that reconciles this trade-off. Our
method identifies token dependencies and removes lower-confidence tokens from
conflicting groups. This produces sets of indices for unmasking that satisfy
both independence and confidence criteria. Our approach ensures improved
parallel unmasking through approximate conditional independence testing.
  Our experiments show that PUNT delivers a superior trade-off between accuracy
and compute when compared to other strong training-free baselines, especially
for generation of longer sequences. On the IFEval benchmark, it achieves up to
16\% higher accuracy over baseline methods, including sequential generation
(one-by-one). These gains hold across different values of hyperparameters,
mitigating the need for brittle hyperparameter tuning. Moreover, we observe
that PUNT induces an emergent hierarchical generation strategy, where the model
first establishes high-level paragraph structure before local refinement,
suggesting a planning-like generation process that contributes to strong
alignment performance.

</details>


### [203] [Differentiable Constraint-Based Causal Discovery](https://arxiv.org/abs/2510.22031)
*Jincheng Zhou,Mengbo Wang,Anqi He,Yumeng Zhou,Hessam Olya,Murat Kocaoglu,Bruno Ribeiro*

Main category: cs.LG

TL;DR: 本文提出用可微d - 分离分数实现基于梯度的因果发现方法，在低样本场景表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法中，基于约束的方法受小样本限制，基于分数的方法放弃显式条件独立性测试，需探索新途径。

Method: 通过软逻辑的渗流理论开发可微d - 分离分数，实现基于梯度的条件独立性约束优化。

Result: 在低样本场景中，该方法在真实数据集上表现优于传统基于约束和基于分数的基线方法。

Conclusion: 所提出的因果发现方法在低样本情况下有稳健表现，代码和数据公开。

Abstract: Causal discovery from observational data is a fundamental task in artificial
intelligence, with far-reaching implications for decision-making, predictions,
and interventions. Despite significant advances, existing methods can be
broadly categorized as constraint-based or score-based approaches.
Constraint-based methods offer rigorous causal discovery but are often hindered
by small sample sizes, while score-based methods provide flexible optimization
but typically forgo explicit conditional independence testing. This work
explores a third avenue: developing differentiable $d$-separation scores,
obtained through a percolation theory using soft logic. This enables the
implementation of a new type of causal discovery method: gradient-based
optimization of conditional independence constraints. Empirical evaluations
demonstrate the robust performance of our approach in low-sample regimes,
surpassing traditional constraint-based and score-based baselines on a
real-world dataset. Code and data of the proposed method are publicly available
at https://github$.$com/PurdueMINDS/DAGPA.

</details>


### [204] [Linearized Optimal Transport for Analysis of High-Dimensional Point-Cloud and Single-Cell Data](https://arxiv.org/abs/2510.22033)
*Tianxiang Wang,Yingtong Ke,Dhananjay Bhaskar,Smita Krishnaswamy,Alexander Cloninger*

Main category: cs.LG

TL;DR: 本文将线性最优传输（LOT）框架应用于单细胞技术产生的不规则点云，实现了对COVID - 19患者状态的准确可解释分类和患者来源类器官的合成数据生成，建立了统一框架。


<details>
  <summary>Details</summary>
Motivation: 单细胞技术产生的不规则点云难以直接量化和比较个体间生物差异，非线性方法缺乏生物可解释性。

Method: 将线性最优传输（LOT）框架应用于此场景，把不规则点云嵌入固定维度欧几里得空间并保留分布结构。

Result: 实现了对COVID - 19患者状态的准确可解释分类，以及患者来源类器官的合成数据生成，LOT重心可用于药物相互作用测试。

Conclusion: LOT是一个统一框架，连接了预测性能、可解释性和生成式建模，为理解高维生物系统中的免疫变异和治疗效果带来新机会。

Abstract: Single-cell technologies generate high-dimensional point clouds of cells,
enabling detailed characterization of complex patient states and treatment
responses. Yet each patient is represented by an irregular point cloud rather
than a simple vector, making it difficult to directly quantify and compare
biological differences between individuals. Nonlinear methods such as kernels
and neural networks achieve predictive accuracy but act as black boxes,
offering little biological interpretability.
  To address these limitations, we adapt the Linear Optimal Transport (LOT)
framework to this setting, embedding irregular point clouds into a
fixed-dimensional Euclidean space while preserving distributional structure.
This embedding provides a principled linear representation that preserves
optimal transport geometry while enabling downstream analysis. It also forms a
registration between any two patients, enabling direct comparison of their
cellular distributions. Within this space, LOT enables: (i) \textbf{accurate
and interpretable classification} of COVID-19 patient states, where classifier
weights map back to specific markers and spatial regions driving predictions;
and (ii) \textbf{synthetic data generation} for patient-derived organoids,
exploiting the linearity of the LOT embedding. LOT barycenters yield averaged
cellular profiles representing combined conditions or samples, supporting drug
interaction testing.
  Together, these results establish LOT as a unified framework that bridges
predictive performance, interpretability, and generative modeling. By
transforming heterogeneous point clouds into structured embeddings directly
traceable to the original data, LOT opens new opportunities for understanding
immune variation and treatment effects in high-dimensional biological systems.

</details>


### [205] [Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models](https://arxiv.org/abs/2510.21978)
*Hoang Phan,Xianjun Yang,Kevin Yao,Jingyu Zhang,Shengjie Bi,Xiaocheng Tang,Madian Khabsa,Lijuan Liu,Deren Lei*

Main category: cs.LG

TL;DR: RLVR有能力退化风险，提出RECAP策略解决，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: RLVR存在能力退化风险，正则化项不能保证广泛知识，经验回放难以确定目标训练重点。

Method: 提出RECAP策略，其重加权机制根据收敛和不稳定的短期信号在线调整，端到端且适用于现有RLVR管道。

Result: 在Qwen2.5-VL-3B和Qwen2.5-VL-7B基准上实验，该方法能保留通用能力并提升推理能力。

Conclusion: RECAP策略有效，可在任务内奖励间灵活权衡。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has delivered
impressive gains in mathematical and multimodal reasoning and has become a
standard post-training paradigm for contemporary language and vision-language
models. However, the RLVR recipe introduces a significant risk of capability
regression, where models forget foundational skills after prolonged training
without employing regularization strategies. We empirically confirm this
concern, observing that open-source reasoning models suffer performance
degradation on core capabilities such as perception and faithfulness. While
imposing regularization terms like KL divergence can help prevent deviation
from the base model, these terms are calculated on the current task, thus they
do not guarantee broader knowledge. Meanwhile, commonly used experience replay
across heterogeneous domains makes it nontrivial to decide how much training
focus each objective should receive. To address this, we propose RECAP-a replay
strategy with dynamic objective reweighting for general knowledge preservation.
Our reweighting mechanism adapts in an online manner using short-horizon
signals of convergence and instability, shifting the post-training focus away
from saturated objectives and toward underperforming or volatile ones. Our
method is end-to-end and readily applicable to existing RLVR pipelines without
training additional models or heavy tuning. Extensive experiments on benchmarks
based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our
method, which not only preserves general capabilities but also improves
reasoning by enabling more flexible trade-offs among in-task rewards.

</details>


### [206] [ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation](https://arxiv.org/abs/2510.22732)
*Jiali Cheng,Anjishnu Kumar,Roshan Lal,Rishi Rajasekaran,Hani Ramezani,Omar Zia Khan,Oleg Rokhlenko,Sunny Chiu-Webster,Gang Hua,Hadi Amiri*

Main category: cs.LG

TL;DR: 提出ATLAS代理，无需微调神经网络，在WebArena - Lite基准测试中成功率高于现有技术，模块化架构无需特定网站微调。


<details>
  <summary>Details</summary>
Motivation: 现有网络代理在不进行神经网络微调时无法有效适应新环境，执行计划效率低。

Method: 引入ATLAS，先通过轻量级探索构建‘认知地图’，规划器提出候选动作，模拟器预测后果，评判器选择最佳方案并更新计划，浏览器执行器执行动作。

Result: 在WebArena - Lite基准测试中成功率达63%，高于之前的53.9%，模块化架构无需特定网站LLM微调。

Conclusion: 系统中的世界模型、分层规划器和基于前瞻的重新规划器有互补作用，缺一会导致性能大幅下降。

Abstract: We observe that current state-of-the-art web-agents are unable to effectively
adapt to new environments without neural network fine-tuning, without which
they produce inefficient execution plans due to a lack of awareness of the
structure and dynamics of the new environment. To address this limitation, we
introduce ATLAS (Actor-Critic Task-completion with Look-ahead Action
Simulation), a memory-augmented agent that is able to make plans grounded in a
model of the environment by simulating the consequences of those actions in
cognitive space. Our agent starts by building a "cognitive map" by performing a
lightweight curiosity driven exploration of the environment. The planner
proposes candidate actions; the simulator predicts their consequences in
cognitive space; a critic analyzes the options to select the best roll-out and
update the original plan; and a browser executor performs the chosen action. On
the WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%
success rate for the previously published state-of-the-art. Unlike previous
systems, our modular architecture requires no website-specific LLM fine-tuning.
Ablations show sizable drops without the world-model, hierarchical planner, and
look-ahead-based replanner confirming their complementary roles within the
design of our system

</details>


### [207] [Deep Learning on Real-World Graphs](https://arxiv.org/abs/2510.21994)
*Emanuele Rossi*

Main category: cs.LG

TL;DR: 论文介绍一系列解决GNN应用局限的模型，弥合学术基准与工业规模图的差距，推动其在多领域应用。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在现实系统应用中面临的可扩展性、时间性等关键挑战。

Method: 提出SIGN、TGN、Dir - GNN、Feature Propagation (FP)和NuGget等一系列模型。

Result: 这些模型弥合了学术基准与工业规模图的差距。

Conclusion: 使GNN能应用于社交和推荐系统等领域。

Abstract: Graph Neural Networks (GNNs) have become a central tool for learning on
graph-structured data, yet their applicability to real-world systems remains
limited by key challenges such as scalability, temporality, directionality,
data incompleteness, and structural uncertainty. This thesis introduces a
series of models addressing these limitations: SIGN for scalable graph
learning, TGN for temporal graphs, Dir-GNN for directed and heterophilic
networks, Feature Propagation (FP) for learning with missing node features, and
NuGget for game-theoretic structural inference. Together, these contributions
bridge the gap between academic benchmarks and industrial-scale graphs,
enabling the use of GNNs in domains such as social and recommender systems.

</details>


### [208] [Deep Gaussian Processes for Functional Maps](https://arxiv.org/abs/2510.22068)
*Matthew Lowery,Zhitong Xu,Da Long,Keyan Chen,Daniel S. Johnson,Yang Bai,Varun Shankar,Shandian Zhe*

Main category: cs.LG

TL;DR: 提出用于函数映射的深度高斯过程（DGPFM）方法，在预测性能和不确定性校准方面有优势。


<details>
  <summary>Details</summary>
Motivation: 现有函数映射方法在处理复杂非线性和有噪声、稀疏、不规则采样数据的不确定性量化方面存在不足。

Method: 设计基于高斯过程（GP）的线性和非线性变换序列，利用核的积分变换、GP插值和从GP采样的非线性激活；使用诱导点和白化变换开发变分学习算法。

Result: 在真实世界和偏微分方程基准数据集上的实验结果表明DGPFM在预测性能和不确定性校准方面有优势。

Conclusion: DGPFM在函数映射的预测和不确定性量化方面表现良好。

Abstract: Learning mappings between functional spaces, also known as
function-on-function regression, plays a crucial role in functional data
analysis and has broad applications, e.g. spatiotemporal forecasting, curve
prediction, and climate modeling. Existing approaches, such as functional
linear models and neural operators, either fall short of capturing complex
nonlinearities or lack reliable uncertainty quantification under noisy, sparse,
and irregularly sampled data. To address these issues, we propose Deep Gaussian
Processes for Functional Maps (DGPFM). Our method designs a sequence of
GP-based linear and nonlinear transformations, leveraging integral transforms
of kernels, GP interpolation, and nonlinear activations sampled from GPs. A key
insight simplifies implementation: under fixed locations, discrete
approximations of kernel integral transforms collapse into direct functional
integral transforms, enabling flexible incorporation of various integral
transform designs. To achieve scalable probabilistic inference, we use inducing
points and whitening transformations to develop a variational learning
algorithm. Empirical results on real-world and PDE benchmark datasets
demonstrate that the advantage of DGPFM in both predictive performance and
uncertainty calibration.

</details>


### [209] [Neural Index Policies for Restless Multi-Action Bandits with Heterogeneous Budgets](https://arxiv.org/abs/2510.22069)
*Himadri S. Pandey,Kai Wang,Gian-Gabriel P. Garcia*

Main category: cs.LG

TL;DR: 提出用于多动作RMABs带异构预算约束的神经索引策略NIP，统一索引预测和约束优化，实现近最优性能。


<details>
  <summary>Details</summary>
Motivation: 经典RMABs假设不适用于现实场景，如医疗保健中多干预、异构成本和约束情况。

Method: 用神经网络为臂 - 动作对分配预算感知索引，通过可微背包层将其转换为可行分配，该层被表述为熵正则化最优传输问题。

Result: NIP实现接近最优性能，与理论上限对齐，严格执行异构预算，可扩展到数百个臂。

Conclusion: 建立了在复杂资源受限环境中学习基于索引策略的通用、理论可靠且可扩展的框架。

Abstract: Restless multi-armed bandits (RMABs) provide a scalable framework for
sequential decision-making under uncertainty, but classical formulations assume
binary actions and a single global budget. Real-world settings, such as
healthcare, often involve multiple interventions with heterogeneous costs and
constraints, where such assumptions break down. We introduce a Neural Index
Policy (NIP) for multi-action RMABs with heterogeneous budget constraints. Our
approach learns to assign budget-aware indices to arm--action pairs using a
neural network, and converts them into feasible allocations via a
differentiable knapsack layer formulated as an entropy-regularized optimal
transport (OT) problem. The resulting model unifies index prediction and
constrained optimization in a single end-to-end differentiable framework,
enabling gradient-based training directly on decision quality. The network is
optimized to align its induced occupancy measure with the theoretical upper
bound from a linear programming relaxation, bridging asymptotic RMAB theory
with practical learning. Empirically, NIP achieves near-optimal performance
within 5% of the oracle occupancy-measure policy while strictly enforcing
heterogeneous budgets and scaling to hundreds of arms. This work establishes a
general, theoretically grounded, and scalable framework for learning
index-based policies in complex resource-constrained environments.

</details>


### [210] [MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification](https://arxiv.org/abs/2510.22070)
*Luca Caldera,Giacomo Bottacini,Lara Cavinato*

Main category: cs.LG

TL;DR: 提出MAGIC - Flow架构用于医学影像生成和分类，在多数据集上验证其有效性和优势。


<details>
  <summary>Details</summary>
Motivation: 生成式建模在医学影像领域直接应用受限，需与任务对齐以用于临床。

Method: 提出条件多尺度归一化流架构MAGIC - Flow，通过可逆和可微双射构建模型，利用雅可比行列式因子化确保精确似然计算和稳定优化。

Result: MAGIC - Flow能创建逼真多样样本，提升分类性能，可处理扫描噪声下的生成和分类等任务。

Conclusion: MAGIC - Flow是数据有限领域生成和分类的有效策略，对隐私增强、鲁棒泛化和可信医疗AI有益。

Abstract: Generative modeling has emerged as a powerful paradigm for representation
learning, but its direct applicability to challenging fields like medical
imaging remains limited: mere generation, without task alignment, fails to
provide a robust foundation for clinical use. We propose MAGIC-Flow, a
conditional multiscale normalizing flow architecture that performs generation
and classification within a single modular framework. The model is built as a
hierarchy of invertible and differentiable bijections, where the Jacobian
determinant factorizes across sub-transformations. We show how this ensures
exact likelihood computation and stable optimization, while invertibility
enables explicit visualization of sample likelihoods, providing an
interpretable lens into the model's reasoning. By conditioning on class labels,
MAGIC-Flow supports controllable sample synthesis and principled
class-probability estimation, effectively aiding both generative and
discriminative objectives. We evaluate MAGIC-Flow against top baselines using
metrics for similarity, fidelity, and diversity. Across multiple datasets, it
addresses generation and classification under scanner noise, and
modality-specific synthesis and identification. Results show MAGIC-Flow creates
realistic, diverse samples and improves classification. MAGIC-Flow is an
effective strategy for generation and classification in data-limited domains,
with direct benefits for privacy-preserving augmentation, robust
generalization, and trustworthy medical AI.

</details>


### [211] [Learning 3D Anisotropic Noise Distributions Improves Molecular Force Field Modeling](https://arxiv.org/abs/2510.22123)
*Xixian Liu,Rui Jiao,Zhiyuan Liu,Yurou Liu,Yang Liu,Ziheng Lu,Wenbing Huang,Yang Zhang,Yixin Cao*

Main category: cs.LG

TL;DR: 提出3D分子去噪框架AniDS，实验显示其优于先前模型，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有3D分子去噪方法依赖过于简化的分子动力学，假设原子运动各向同性和同方差，有局限性。

Method: 提出AniDS框架，引入结构感知各向异性噪声生成器，生成特定于原子的高斯噪声分布的全协方差矩阵。

Result: AniDS在MD17和OC22基准测试中优于先前模型，力预测精度平均相对提高8.9%和6.2%，案例研究显示符合物理化学原理。

Conclusion: AniDS能更好地反映分子系统的方向性和结构变异性，具有更好的分子动力学建模能力。

Abstract: Coordinate denoising has emerged as a promising method for 3D molecular
pretraining due to its theoretical connection to learning molecular force
field. However, existing denoising methods rely on oversimplied molecular
dynamics that assume atomic motions to be isotropic and homoscedastic. To
address these limitations, we propose a novel denoising framework AniDS:
Anisotropic Variational Autoencoder for 3D Molecular Denoising. AniDS
introduces a structure-aware anisotropic noise generator that can produce
atom-specific, full covariance matrices for Gaussian noise distributions to
better reflect directional and structural variability in molecular systems.
These covariances are derived from pairwise atomic interactions as anisotropic
corrections to an isotropic base. Our design ensures that the resulting
covariance matrices are symmetric, positive semi-definite, and
SO(3)-equivariant, while providing greater capacity to model complex molecular
dynamics. Extensive experiments show that AniDS outperforms prior isotropic and
homoscedastic denoising models and other leading methods on the MD17 and OC22
benchmarks, achieving average relative improvements of 8.9% and 6.2% in force
prediction accuracy. Our case study on a crystal and molecule structure shows
that AniDS adaptively suppresses noise along the bonding direction, consistent
with physicochemical principles. Our code is available at
https://github.com/ZeroKnighting/AniDS.

</details>


### [212] [A Multimodal Human Protein Embeddings Database: DeepDrug Protein Embeddings Bank (DPEB)](https://arxiv.org/abs/2510.22008)
*Md Saiful Islam Sajol,Magesh Rajasekaran,Hayden Gemeinhardt,Adam Bess,Chris Alvin,Supratik Mukhopadhyay*

Main category: cs.LG

TL;DR: 本文提出DPEB整合四种蛋白质嵌入类型，用于蛋白质 - 蛋白质相互作用（PPI）预测等，评估显示GraphSAGE与BioEmbedding组合表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决因缺乏集成的多模态蛋白质表示，导致计算预测蛋白质 - 蛋白质相互作用困难的问题。

Method: 构建DPEB整合四种嵌入类型，提供AlphaFold2衍生嵌入，支持多种图神经网络方法进行PPI预测。

Result: GraphSAGE与BioEmbedding组合PPI预测性能最佳，AUROC达87.37%，准确率79.16%；酶分类准确率77.42%，蛋白质家族分类准确率86.04%。

Conclusion: DPEB可支持多种图神经网络方法用于PPI预测，在系统生物学等多领域有应用潜力。

Abstract: Computationally predicting protein-protein interactions (PPIs) is challenging
due to the lack of integrated, multimodal protein representations. DPEB is a
curated collection of 22,043 human proteins that integrates four embedding
types: structural (AlphaFold2), transformer-based sequence (BioEmbeddings),
contextual amino acid patterns (ESM-2: Evolutionary Scale Modeling), and
sequence-based n-gram statistics (ProtVec]). AlphaFold2 protein structures are
available through public databases (e.g., AlphaFold2 Protein Structure
Database), but the internal neural network embeddings are not. DPEB addresses
this gap by providing AlphaFold2-derived embeddings for computational modeling.
Our benchmark evaluations show GraphSAGE with BioEmbedding achieved the highest
PPI prediction performance (87.37% AUROC, 79.16% accuracy). The framework also
achieved 77.42% accuracy for enzyme classification and 86.04% accuracy for
protein family classification. DPEB supports multiple graph neural network
methods for PPI prediction, enabling applications in systems biology, drug
target identification, pathway analysis, and disease mechanism studies.

</details>


### [213] [Monitoring State Transitions in Markovian Systems with Sampling Cost](https://arxiv.org/abs/2510.22327)
*Kumar Saurav,Ness B. Shroff,Yingbin Liang*

Main category: cs.LG

TL;DR: 研究节点 - 监控对中监控节点状态问题，分析贪心策略，提出基于PSGD的学习变体策略。


<details>
  <summary>Details</summary>
Motivation: 监控节点状态有查询成本，需平衡查询与预测以降低成本。

Method: 分析贪心策略，在马尔可夫设置下对比最优策略；对未知转移概率情况，提出基于PSGD的学习变体策略。

Result: 贪心策略一般次优且竞争比无界，但在常见条件下接近最优；PSGD变体策略能实现良好的预测 - 查询权衡且计算效率更高。

Conclusion: 贪心策略在特定条件下表现良好，PSGD变体策略在未知转移概率场景有优势。

Abstract: We consider a node-monitor pair, where the node's state varies with time. The
monitor needs to track the node's state at all times; however, there is a fixed
cost for each state query. So the monitor may instead predict the state using
time-series forecasting methods, including time-series foundation models
(TSFMs), and query only when prediction uncertainty is high. Since query
decisions influence prediction accuracy, determining when to query is
nontrivial. A natural approach is a greedy policy that predicts when the
expected prediction loss is below the query cost and queries otherwise. We
analyze this policy in a Markovian setting, where the optimal (OPT) strategy is
a state-dependent threshold policy minimizing the time-averaged sum of query
cost and prediction losses. We show that, in general, the greedy policy is
suboptimal and can have an unbounded competitive ratio, but under common
conditions such as identically distributed transition probabilities, it
performs close to OPT. For the case of unknown transition probabilities, we
further propose a projected stochastic gradient descent (PSGD)-based learning
variant of the greedy policy, which achieves a favorable predict-query tradeoff
with improved computational efficiency compared to OPT.

</details>


### [214] [Do You Trust the Process?: Modeling Institutional Trust for Community Adoption of Reinforcement Learning Policies](https://arxiv.org/abs/2510.22017)
*Naina Balepur,Xingrui Pei,Hari Sundaram*

Main category: cs.LG

TL;DR: 本文开发信任感知的强化学习算法用于社区资源分配，研究融入机构信任对结果的影响及信任值私有情况下的策略学习，发现融入信任可产生更成功策略，保守信任估计能提升公平性和社区信任但影响组织成功，还提出配额系统干预策略。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习工作大多假设公民会遵循政府制定的政策，但实际若无机构信任，公民不会遵循，因此要开发信任感知的强化学习算法。

Method: 使用深度确定性策略梯度方法学习资源分配，模拟资源分配并建模社区成员机构信任变化，还采用外部实体实施配额系统的干预策略。

Result: 融入信任到强化学习算法能产生更成功策略，保守信任估计提升公平性和社区信任但影响组织成功，配额系统干预在某些情况下可提升社区公平性和信任但降低组织成功率。

Conclusion: 强调机构信任在算法设计和实施中的重要性，指出组织成功与社区福祉之间存在矛盾。

Abstract: Many governmental bodies are adopting AI policies for decision-making. In
particular, Reinforcement Learning has been used to design policies that
citizens would be expected to follow if implemented. Much RL work assumes that
citizens follow these policies, and evaluate them with this in mind. However,
we know from prior work that without institutional trust, citizens will not
follow policies put in place by governments. In this work, we develop a
trust-aware RL algorithm for resource allocation in communities. We consider
the case of humanitarian engineering, where the organization is aiming to
distribute some technology or resource to community members. We use a Deep
Deterministic Policy Gradient approach to learn a resource allocation that fits
the needs of the organization. Then, we simulate resource allocation according
to the learned policy, and model the changes in institutional trust of
community members. We investigate how this incorporation of institutional trust
affects outcomes, and ask how effectively an organization can learn policies if
trust values are private. We find that incorporating trust into RL algorithms
can lead to more successful policies, specifically when the organization's
goals are less certain. We find more conservative trust estimates lead to
increased fairness and average community trust, though organization success
suffers. Finally, we explore a strategy to prevent unfair outcomes to
communities. We implement a quota system by an external entity which decreases
the organization's utility when it does not serve enough community members. We
find this intervention can improve fairness and trust among communities in some
cases, while decreasing the success of the organization. This work underscores
the importance of institutional trust in algorithm design and implementation,
and identifies a tension between organization success and community well-being.

</details>


### [215] [Transformer Key-Value Memories Are Nearly as Interpretable as Sparse Autoencoders](https://arxiv.org/abs/2510.22332)
*Mengyu Ye,Jun Suzuki,Tatsuro Inaba,Tatsuki Kuribayashi*

Main category: cs.LG

TL;DR: 本文对比稀疏自编码器（SAE）学习的特征与前馈（FF）层特征向量的可解释性，发现二者可解释性范围相近，FF在某些方面表现更好，FF键值参数可作为可解释性研究的强基线。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少系统比较学习到的特征与原模型参数中特征的属性，本文旨在从FF作为键值记忆的角度，用现代可解释性基准重新审视FF层特征向量的可解释性。

Method: 使用现代可解释性基准对SAE和FF的特征向量进行广泛评估。

Result: SAE和FF的可解释性范围相似，SAE在某些方面有微小提升，在某些方面FF可解释性更好，且二者特征有差异。

Conclusion: 从特征质量和忠实度角度对SAE的优势提出质疑，FF键值参数可作为现代可解释性研究的强基线。

Abstract: Recent interpretability work on large language models (LLMs) has been
increasingly dominated by a feature-discovery approach with the help of proxy
modules. Then, the quality of features learned by, e.g., sparse auto-encoders
(SAEs), is evaluated. This paradigm naturally raises a critical question: do
such learned features have better properties than those already represented
within the original model parameters, and unfortunately, only a few studies
have made such comparisons systematically so far. In this work, we revisit the
interpretability of feature vectors stored in feed-forward (FF) layers, given
the perspective of FF as key-value memories, with modern interpretability
benchmarks. Our extensive evaluation revealed that SAE and FFs exhibits a
similar range of interpretability, although SAEs displayed an observable but
minimal improvement in some aspects. Furthermore, in certain aspects,
surprisingly, even vanilla FFs yielded better interpretability than the SAEs,
and features discovered in SAEs and FFs diverged. These bring questions about
the advantage of SAEs from both perspectives of feature quality and
faithfulness, compared to directly interpreting FF feature vectors, and FF
key-value parameters serve as a strong baseline in modern interpretability
research.

</details>


### [216] [Uncertainty quantification in model discovery by distilling interpretable material constitutive models from Gaussian process posteriors](https://arxiv.org/abs/2510.22345)
*David Anton,Henning Wessels,Ulrich Römer,Alexander Henkes,Jorge-Humberto Urrea-Quintero*

Main category: cs.LG

TL;DR: 提出四步部分贝叶斯框架用于本构模型发现的不确定性量化，不依赖参数先验且可发现非线性模型，并用多种数据和模型库验证。


<details>
  <summary>Details</summary>
Motivation: 以往模型发现不确定性量化方法存在需选参数先验、限于线性系数或参数分布灵活性不足等问题。

Method: 分四步：用高斯过程扩充应力 - 变形数据；用归一化流近似参数分布；通过匹配参数诱导的应力 - 变形函数分布与高斯过程后验提炼参数分布；进行索博尔敏感性分析获得稀疏可解释模型。

Result: 框架在各向同性和各向异性实验数据以及线性和非线性模型库中均展示了能力。

Conclusion: 提出的四步部分贝叶斯框架可有效解决模型发现中的不确定性量化问题，适用于多种数据和模型。

Abstract: Constitutive model discovery refers to the task of identifying an appropriate
model structure, usually from a predefined model library, while simultaneously
inferring its material parameters. The data used for model discovery are
measured in mechanical tests and are thus inevitably affected by noise which,
in turn, induces uncertainties. Previously proposed methods for uncertainty
quantification in model discovery either require the selection of a prior for
the material parameters, are restricted to the linear coefficients of the model
library or are limited in the flexibility of the inferred parameter probability
distribution. We therefore propose a four-step partially Bayesian framework for
uncertainty quantification in model discovery that does not require prior
selection for the material parameters and also allows for the discovery of
non-linear constitutive models: First, we augment the available
stress-deformation data with a Gaussian process. Second, we approximate the
parameter distribution by a normalizing flow, which allows for capturing
complex joint distributions. Third, we distill the parameter distribution by
matching the distribution of stress-deformation functions induced by the
parameters with the Gaussian process posterior. Fourth, we perform a Sobol'
sensitivity analysis to obtain a sparse and interpretable model. We demonstrate
the capability of our framework for both isotropic and anisotropic experimental
data as well as linear and non-linear model libraries.

</details>


### [217] [Normalization in Attention Dynamics](https://arxiv.org/abs/2510.22026)
*Nikita Karagodin,Shu Ge,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 研究归一化方案对深度Transformer中token表示的影响，提出统一分析框架并指出Peri - LN是有效选择。


<details>
  <summary>Details</summary>
Motivation: 探究归一化方案对深度Transformer中token表示的影响。

Method: 将token表示的演化建模为球面上的相互作用粒子，对多种归一化方案进行统一分析。

Result: 揭示不同归一化方案对聚类动态和表示崩溃的影响，明确不同方案在各层对token表示的塑造作用。

Conclusion: Peri - LN是一种特别有效的归一化方案。

Abstract: We study the effect of normalization schemes on token representations in deep
transformers. Modeling their evolution as interacting particles on the sphere,
we show that normalization acts as a form of speed regulation. This perspective
enables a unified analysis of several schemes -- including Post-LN, Pre-LN,
Mix-LN, Peri-LN, nGPT, and LN-Scaling -- revealing how they influence
clustering dynamics and representation collapse. Our framework clarifies how
different schemes shape token representations across layers and provides a
principled basis for comparing them, identifying Peri-LN as a particularly
effective choice.

</details>


### [218] [Bias Begins with Data: The FairGround Corpus for Robust and Reproducible Research on Algorithmic Fairness](https://arxiv.org/abs/2510.22363)
*Jan Simson,Alessandro Fabris,Cosima Fröhner,Frauke Kreuter,Christoph Kern*

Main category: cs.LG

TL;DR: 现有公平机器学习研究数据集存在局限，本文提出FairGround框架、数据语料库和Python包，促进公平ML分类的可重复性研究。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在高风险决策领域应用增多，确保输出公平性成挑战，现有研究数据集存在选择窄、处理不一致和缺乏多样性等问题，影响结果可推广性和可重复性。

Method: 提出FairGround框架、数据语料库和Python包，包含44个表格数据集并标注公平相关元数据，Python包对数据加载、预处理等操作进行标准化。

Result: 提供了多样化且文档完善的数据集语料库和强大工具。

Conclusion: FairGround能使机器学习模型更公平、可靠和可重复，所有资源公开支持开放协作研究。

Abstract: As machine learning (ML) systems are increasingly adopted in high-stakes
decision-making domains, ensuring fairness in their outputs has become a
central challenge. At the core of fair ML research are the datasets used to
investigate bias and develop mitigation strategies. Yet, much of the existing
work relies on a narrow selection of datasets--often arbitrarily chosen,
inconsistently processed, and lacking in diversity--undermining the
generalizability and reproducibility of results.
  To address these limitations, we present FairGround: a unified framework,
data corpus, and Python package aimed at advancing reproducible research and
critical data studies in fair ML classification. FairGround currently comprises
44 tabular datasets, each annotated with rich fairness-relevant metadata. Our
accompanying Python package standardizes dataset loading, preprocessing,
transformation, and splitting, streamlining experimental workflows. By
providing a diverse and well-documented dataset corpus along with robust
tooling, FairGround enables the development of fairer, more reliable, and more
reproducible ML models. All resources are publicly available to support open
and collaborative research.

</details>


### [219] [Low-Precision Streaming PCA](https://arxiv.org/abs/2510.22440)
*Sanjoy Dasgupta,Syamantak Kumar,Shourya Pandey,Purnamrita Sarkar*

Main category: cs.LG

TL;DR: 本文研究低精度流式PCA，建立量化分辨率的信息论下界，研究Oja算法在不同量化下的表现，证明批处理版本能接近下界，实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 在有限精度的流式环境下估计主成分，探究所需的量化分辨率。

Method: 研究Oja算法在线性和非线性随机量化下的情况，使用无偏随机量化权重向量和更新。

Result: 批处理版本在两种量化方案下能达到下界的对数因子内，非线性量化有近维度无关的量化误差。

Conclusion: 实验验证理论结果，低精度方法性能接近标准Oja算法。

Abstract: Low-precision streaming PCA estimates the top principal component in a
streaming setting under limited precision. We establish an
information-theoretic lower bound on the quantization resolution required to
achieve a target accuracy for the leading eigenvector. We study Oja's algorithm
for streaming PCA under linear and nonlinear stochastic quantization. The
quantized variants use unbiased stochastic quantization of the weight vector
and the updates. Under mild moment and spectral-gap assumptions on the data
distribution, we show that a batched version achieves the lower bound up to
logarithmic factors under both schemes. This leads to a nearly dimension-free
quantization error in the nonlinear quantization setting. Empirical evaluations
on synthetic streams validate our theoretical findings and demonstrate that our
low-precision methods closely track the performance of standard Oja's
algorithm.

</details>


### [220] [PF$Δ$: A Benchmark Dataset for Power Flow under Load, Generation, and Topology Variations](https://arxiv.org/abs/2510.22048)
*Ana K. Rivera,Anvita Bhagavathula,Alvaro Carbonero,Priya Donti*

Main category: cs.LG

TL;DR: 本文介绍电力潮流基准数据集PFΔ，评估传统求解器和GNN方法，指出问题并公开数据与代码。


<details>
  <summary>Details</summary>
Motivation: 电力潮流计算是实时电网运行的关键，但存在计算瓶颈，且可再生能源和极端天气带来不确定性，机器学习方法性能缺乏系统评估。

Method: 引入包含不同负载、发电和拓扑变化的PFΔ数据集，评估传统求解器和基于GNN的方法。

Result: 评估发现现有方法存在困难的关键领域，确定未来研究的开放问题。

Conclusion: PFΔ数据集有助于评估电力潮流计算方法，为未来研究提供方向。

Abstract: Power flow (PF) calculations are the backbone of real-time grid operations,
across workflows such as contingency analysis (where repeated PF evaluations
assess grid security under outages) and topology optimization (which involves
PF-based searches over combinatorially large action spaces). Running these
calculations at operational timescales or across large evaluation spaces
remains a major computational bottleneck. Additionally, growing uncertainty in
power system operations from the integration of renewables and climate-induced
extreme weather also calls for tools that can accurately and efficiently
simulate a wide range of scenarios and operating conditions. Machine learning
methods offer a potential speedup over traditional solvers, but their
performance has not been systematically assessed on benchmarks that capture
real-world variability. This paper introduces PF$\Delta$, a benchmark dataset
for power flow that captures diverse variations in load, generation, and
topology. PF$\Delta$ contains 859,800 solved power flow instances spanning six
different bus system sizes, capturing three types of contingency scenarios (N ,
N -1, and N -2), and including close-to-infeasible cases near steady-state
voltage stability limits. We evaluate traditional solvers and GNN-based
methods, highlighting key areas where existing approaches struggle, and
identifying open problems for future research. Our dataset is available at
https://huggingface.co/datasets/pfdelta/pfdelta/tree/main and our code with
data generation scripts and model implementations is at
https://github.com/MOSSLab-MIT/pfdelta.

</details>


### [221] [Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model](https://arxiv.org/abs/2510.22057)
*James Thiering,Tarun Sethupat Radha Krishna,Dylan Zelkin,Ashis Kumer Biswas*

Main category: cs.LG

TL;DR: 研究聚焦在线学习中学生参与度检测问题，提出新训练方法，用属性正交正则化技术取得效果，代码开源。


<details>
  <summary>Details</summary>
Motivation: 随着在线和虚拟学习兴起，传统评估学生参与度方法不适用于虚拟环境，需开发自动化系统检测在线学习中学生参与度。

Method: 提出新训练方法，避免模型利用敏感特征进行预测；将属性正交正则化技术应用于分裂模型分类器，使用多种迁移学习策略。

Result: 将未缓解模型预测敏感性群体分布差异的皮尔逊相关系数从 0.897 降至缓解模型的 0.999。

Conclusion: 新方法不仅有助于执行道德标准，还能增强模型预测的可解释性。

Abstract: With the rise of online and virtual learning, monitoring and enhancing
student engagement have become an important aspect of effective education.
Traditional methods of assessing a student's involvement might not be
applicable directly to virtual environments. In this study, we focused on this
problem and addressed the need to develop an automated system to detect student
engagement levels during online learning. We proposed a novel training method
which can discourage a model from leveraging sensitive features like gender for
its predictions. The proposed method offers benefits not only in the
enforcement of ethical standards, but also to enhance interpretability of the
model predictions. We applied an attribute-orthogonal regularization technique
to a split-model classifier, which uses multiple transfer learning strategies
to achieve effective results in reducing disparity in the distribution of
prediction for sensitivity groups from a Pearson correlation coefficient of
0.897 for the unmitigated model, to 0.999 for the mitigated model. The source
code for this project is available on
https://github.com/ashiskb/elearning-engagement-study .

</details>


### [222] [CANDI: Hybrid Discrete-Continuous Diffusion Models](https://arxiv.org/abs/2510.22510)
*Patrick Pynadath,Jiaxin Shi,Ruqi Zhang*

Main category: cs.LG

TL;DR: 连续扩散在离散数据上表现不佳，引入token可识别性框架分析原因，提出CANDI混合框架解决问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解连续扩散在离散数据上表现不如纯离散方法的原因，并解决该问题。

Method: 引入token可识别性分析框架，提出CANDI混合框架解耦离散和连续损坏。

Result: 验证了时间不和谐现象，CANDI成功避免该现象，在受控生成和文本生成任务上表现良好。

Conclusion: CANDI解锁了连续扩散在离散空间的优势，证明了为离散空间学习连续梯度的价值。

Abstract: While continuous diffusion has shown remarkable success in continuous domains
such as image generation, its direct application to discrete data has
underperformed compared to purely discrete formulations. This gap is
counterintuitive, given that continuous diffusion learns score functions that
enable joint evolution across multiple positions. To understand this gap, we
introduce token identifiability as an analytical framework for understanding
how Gaussian noise corrupts discrete data through two mechanisms: discrete
identity corruption and continuous rank degradation. We reveal that these
mechanisms scale differently with vocabulary size, creating a temporal
dissonance: at noise levels where discrete corruption preserves enough
structure for conditional learning, continuous denoising is trivial; at noise
levels where continuous denoising is meaningful, discrete corruption destroys
nearly all conditional structure. To solve this, we propose CANDI (Continuous
ANd DIscrete diffusion), a hybrid framework that decouples discrete and
continuous corruption, enabling simultaneous learning of both conditional
structure and continuous geometry. We empirically validate the temporal
dissonance phenomenon and demonstrate that CANDI successfully avoids it. This
unlocks the benefits of continuous diffusion for discrete spaces: on controlled
generation, CANDI enables classifier-based guidance with off-the-shelf
classifiers through simple gradient addition; on text generation, CANDI
outperforms masked diffusion at low NFE, demonstrating the value of learning
continuous gradients for discrete spaces.

</details>


### [223] [Pruning and Quantization Impact on Graph Neural Networks](https://arxiv.org/abs/2510.22058)
*Khatoon Khedri,Reza Rawassizadeh,Qifu Wen,Mehdi Hosseinzadeh*

Main category: cs.LG

TL;DR: 本文实证研究三种剪枝和三种量化方法对不同GNN模型的影响，发现非结构化细粒度和全局剪枝可大幅减小模型规模并保持或提升精度，不同量化方法影响多样。


<details>
  <summary>Details</summary>
Motivation: GNN计算和资源成本高，使用神经网络压缩方法减小模型规模并保持合理精度。

Method: 在Cora、Proteins和BBBP三个图数据集上，对不同GNN模型（图分类、节点分类和链接预测任务）进行三种剪枝和三种量化方法的实验。

Result: 非结构化细粒度和全局剪枝可使模型规模减小50%，微调后保持或提升精度；不同量化方法对不同数据集的精度、推理时间和模型规模有不同影响。

Conclusion: 特定剪枝方法能有效压缩GNN模型并保持性能，不同量化方法影响有差异。

Abstract: Graph neural networks (GNNs) are known to operate with high accuracy on
learning from graph-structured data, but they suffer from high computational
and resource costs. Neural network compression methods are used to reduce the
model size while maintaining reasonable accuracy. Two of the common neural
network compression techniques include pruning and quantization. In this
research, we empirically examine the effects of three pruning methods and three
quantization methods on different GNN models, including graph classification
tasks, node classification tasks, and link prediction. We conducted all
experiments on three graph datasets, including Cora, Proteins, and BBBP. Our
findings demonstrate that unstructured fine-grained and global pruning can
significantly reduce the model's size(50\%) while maintaining or even improving
precision after fine-tuning the pruned model. The evaluation of different
quantization methods on GNN shows diverse impacts on accuracy, inference time,
and model size across different datasets.

</details>


### [224] [Identification of Causal Direction under an Arbitrary Number of Latent Confounders](https://arxiv.org/abs/2510.22711)
*Wei Chen,Linjun Peng,Zhiyi Huang,Haoyue Dai,Zhifeng Hao,Ruichu Cai,Kun Zhang*

Main category: cs.LG

TL;DR: 论文研究存在潜在变量时的因果结构恢复，提出利用特定构造的高阶累积量矩阵的方法，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有处理潜在变量下因果结构恢复的方法大多有严格和不可检验的假设，难以处理多个潜在变量同时影响观测变量的情况。

Method: 考虑线性非高斯情况，利用特定方式构造的观测变量的联合高阶累积量矩阵，通过其秩亏性质判断因果不对称性。

Result: 确立了可识别性结果，对应识别方法无需迭代过程，实验证明方法有效且具有渐近正确性。

Conclusion: 提出的方法能有效解决存在任意数量潜在混淆因素时的因果结构恢复问题。

Abstract: Recovering causal structure in the presence of latent variables is an
important but challenging task. While many methods have been proposed to handle
it, most of them require strict and/or untestable assumptions on the causal
structure. In real-world scenarios, observed variables may be affected by
multiple latent variables simultaneously, which, generally speaking, cannot be
handled by these methods. In this paper, we consider the linear, non-Gaussian
case, and make use of the joint higher-order cumulant matrix of the observed
variables constructed in a specific way. We show that, surprisingly, causal
asymmetry between two observed variables can be directly seen from the rank
deficiency properties of such higher-order cumulant matrices, even in the
presence of an arbitrary number of latent confounders. Identifiability results
are established, and the corresponding identification methods do not even
involve iterative procedures. Experimental results demonstrate the
effectiveness and asymptotic correctness of our proposed method.

</details>


### [225] [A Theory of the Mechanics of Information: Generalization Through Measurement of Uncertainty (Learning is Measuring)](https://arxiv.org/abs/2510.22809)
*Christopher J. Hazard,Michael Resnick,Jacob Beel,Jack Xia,Cade Mack,Dominic Glennie,Matthew Fulp,David Maze,Andrew Bassett,Martin Koistinen*

Main category: cs.LG

TL;DR: 提出无模型框架，用意外度分析原始数据，能跨任务推理，性能良好，或成神经网络替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习依赖显式模型和领域假设，限制灵活性和可解释性。

Method: 引入无模型框架，用意外度直接分析原始数据，通过不确定性量化相关性。

Result: 在多数常见机器学习任务上达到或接近当前最优性能，适用于多种复杂数据类型。

Conclusion: 该框架是可扩展机器学习和人工智能中保持底层机制可理解性的可行替代方案。

Abstract: Traditional machine learning relies on explicit models and domain
assumptions, limiting flexibility and interpretability. We introduce a
model-free framework using surprisal (information theoretic uncertainty) to
directly analyze and perform inferences from raw data, eliminating distribution
modeling, reducing bias, and enabling efficient updates including direct edits
and deletion of training data. By quantifying relevance through uncertainty,
the approach enables generalizable inference across tasks including generative
inference, causal discovery, anomaly detection, and time series forecasting. It
emphasizes traceability, interpretability, and data-driven decision making,
offering a unified, human-understandable framework for machine learning, and
achieves at or near state-of-the-art performance across most common machine
learning tasks. The mathematical foundations create a ``physics'' of
information, which enable these techniques to apply effectively to a wide
variety of complex data types, including missing data. Empirical results
indicate that this may be a viable alternative path to neural networks with
regard to scalable machine learning and artificial intelligence that can
maintain human understandability of the underlying mechanics.

</details>


### [226] [Agentic Reinforcement Learning for Real-World Code Repair](https://arxiv.org/abs/2510.22075)
*Siyu Zhu,Anastasiya Karpovich,Albert Chen,Jessica Koscheka,Shailesh Jannu,Di Wen,Yuqing Zhu,Rohit Jain,Alborz Geramifard*

Main category: cs.LG

TL;DR: 本文开发验证管道与简化管道来训练代码修复代理，对比不同模型效果，指出匹配训练测试环境对构建可靠代码修复代理很重要。


<details>
  <summary>Details</summary>
Motivation: 解决在真实代码库中训练可靠代码修复代理时，复杂构建和依赖变化导致评估不稳定的问题。

Method: 开发验证管道，通过固定依赖和禁用自动升级提高可重复性；引入简化管道用于大规模强化学习；对Qwen3 - 32B进行监督微调，在简化环境对SFT模型应用强化学习。

Result: 从GPT - 4.1轨迹中提取的SFT模型性能相当但小56倍，强化学习在匹配训练测试条件下带来7 - 20%的绝对增益；“思考模式”表现相当或更差；SFT和RL模型在不同环境泛化失败。

Conclusion: 匹配训练测试环境对构建可靠的真实世界代码修复代理至关重要。

Abstract: We tackle the challenge of training reliable code-fixing agents in real
repositories, where complex builds and shifting dependencies make evaluation
unstable. We developed a verifiable pipeline with success defined as post-fix
build validation and improved reproducibility across ~1K real issues by pinning
dependencies and disabling automatic upgrades. Building on this, we introduced
a scalable simplified pipeline for large-scale reinforcement learning (RL).
Using this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and
applied RL on top of the SFT model in the simplified environment. The SFT model
distilled from GPT-4.1 trajectories performs on par while being 56x smaller,
and RL added 7-20% absolute gains under matched train-test conditions.
"Thinking mode" was on par or worse in our experiments. Both SFT and RL models
failed to generalize across environments, highlighting the importance of
matching train-test environments for building reliable real-world code-fixing
agents.

</details>


### [227] [Hierarchical Graph Networks for Accurate Weather Forecasting via Lightweight Training](https://arxiv.org/abs/2510.22094)
*Thomas Bailie,S. Karthik Mukkavilli,Varvara Vetrova,Yun Sing Koh*

Main category: cs.LG

TL;DR: 文章介绍HiFlowCast和HiAntFlow两种HGNN模型，可嵌入物理知识进行多尺度天气预测，能降低误差、减少训练成本和碳足迹。


<details>
  <summary>Details</summary>
Motivation: 气候事件复杂影响大，但因物理过程时空尺度多样，现有固定分辨率方法难准确预测天气，HGNN的非线性向下映射会削弱物理集成。

Method: 引入HiFlowCast和HiAntFlow模型，采用Latent - Memory - Retention机制保留全局趋势，用Latent - to - Physics分支集成不同尺度PDE解场。

Result: Flow模型在13天提前期误差降低超5%，在1%和99%分位数极端情况下误差降低5 - 8%，利用预训练权重可单轮收敛。

Conclusion: 模型提升了罕见事件预测可靠性，降低训练成本和碳足迹，对解决机器学习可持续性和研究可及性问题很重要。

Abstract: Climate events arise from intricate, multivariate dynamics governed by
global-scale drivers, profoundly impacting food, energy, and infrastructure.
Yet, accurate weather prediction remains elusive due to physical processes
unfolding across diverse spatio-temporal scales, which fixed-resolution methods
cannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscale
representation, but nonlinear downward mappings often erase global trends,
weakening the integration of physics into forecasts. We introduce HiFlowCast
and its ensemble variant HiAntFlow, HGNNs that embed physics within a
multiscale prediction framework. Two innovations underpin their design: a
Latent-Memory-Retention mechanism that preserves global trends during downward
traversal, and a Latent-to-Physics branch that integrates PDE solution fields
across diverse scales. Our Flow models cut errors by over 5% at 13-day lead
times and by 5-8% under 1st and 99th quantile extremes, improving reliability
for rare events. Leveraging pretrained model weights, they converge within a
single epoch, reducing training cost and their carbon footprint. Such
efficiency is vital as the growing scale of machine learning challenges
sustainability and limits research accessibility. Code and model weights are in
the supplementary materials.

</details>


### [228] [Dynamic Graph Neural Network for Data-Driven Physiologically Based Pharmacokinetic Modeling](https://arxiv.org/abs/2510.22096)
*Su Liu,Xin Hu,Shurong Wen,Jiaqi Liu,Jiexi Xu,Lanruo Wang*

Main category: cs.LG

TL;DR: 本文探索用深度学习进行基于生理的药代动力学（PBPK）预测，提出动态图神经网络（Dynamic GNN），其预测性能最佳，为数据驱动的药代动力学建模提供新方法。


<details>
  <summary>Details</summary>
Motivation: 传统PBPK方法基于常微分方程，简化假设多，限制了对非线性生理交互的适应性，需探索数据驱动的替代方法。

Method: 实现多层感知器（MLP）和长短期记忆网络（LSTM）作为基线架构，提出Dynamic GNN来建模器官间的生理连接。

Result: Dynamic GNN预测性能最佳，R^2为0.9342，RMSE为0.0159，MAE为0.0116，MLP的R^2为0.8705，LSTM为0.8059。

Conclusion: 显式建模器官交互的时空依赖能实现更准确和可泛化的药物浓度预测，Dynamic GNN为传统PBPK公式提供了可扩展、无方程的替代方案，在临床前和临床研究中有潜力。

Abstract: Physiologically Based Pharmacokinetic (PBPK) modeling plays a critical role
in drug development by predicting drug concentration dynamics across organs.
Traditional approaches rely on ordinary differential equations with strong
simplifying assumptions, which limit their adaptability to nonlinear
physiological interactions. In this study, we explore data-driven alternatives
for PBPK prediction using deep learning. Two baseline architectures - a
multilayer perceptron (MLP) and a long short-term memory (LSTM) network - are
implemented to capture molecular and temporal dependencies, respectively. To
incorporate inter-organ interactions, we propose a Dynamic Graph Neural Network
(Dynamic GNN) that models physiological connections as recurrent
message-passing processes between organs. Experimental results demonstrate that
the proposed Dynamic GNN achieves the highest predictive performance among all
models, with an R^2 of 0.9342, an RMSE of 0.0159, and an MAE of 0.0116. In
comparison, the MLP baseline obtains an R^2 of 0.8705 and the LSTM achieves
0.8059. These results highlight that explicitly modeling the spatial and
temporal dependencies of organ interactions enables more accurate and
generalizable drug concentration prediction. The Dynamic GNN provides a
scalable, equation-free alternative to traditional PBPK formulations and
demonstrates strong potential for data-driven pharmacokinetic modeling in
preclinical and clinical research.

</details>


### [229] [Self-induced stochastic resonance: A physics-informed machine learning approach](https://arxiv.org/abs/2510.22848)
*Divyesh Savaliya,Marius E. Yamakou*

Main category: cs.LG

TL;DR: 提出物理信息机器学习框架用于随机FitzHugh - Nagumo神经元的SISR建模与预测，该框架准确高效。


<details>
  <summary>Details</summary>
Motivation: 对随机FitzHugh - Nagumo神经元的自诱导随机共振（SISR）进行建模和预测。

Method: 将随机微分方程和SISR渐近时间尺度匹配约束嵌入基于噪声增强状态预测器架构的物理信息神经网络（PINN），复合损失集成数据保真度、动态残差和基于Kramers逃逸理论的物理约束。

Result: 训练后的PINN能准确预测尖峰序列相干性与噪声强度、兴奋性和时间尺度分离的关系，与直接随机模拟结果匹配，在准确性、泛化性上优于纯数据驱动方法，且计算量大幅减少。

Conclusion: 该框架为多尺度随机系统中噪声诱导相干性的模拟和分析提供了数据高效且可解释的替代模型。

Abstract: Self-induced stochastic resonance (SISR) is the emergence of coherent
oscillations in slow-fast excitable systems driven solely by noise, without
external periodic forcing or proximity to a bifurcation. This work presents a
physics-informed machine learning framework for modeling and predicting SISR in
the stochastic FitzHugh-Nagumo neuron. We embed the governing stochastic
differential equations and SISR-asymptotic timescale-matching constraints
directly into a Physics-Informed Neural Network (PINN) based on a
Noise-Augmented State Predictor architecture. The composite loss integrates
data fidelity, dynamical residuals, and barrier-based physical constraints
derived from Kramers' escape theory. The trained PINN accurately predicts the
dependence of spike-train coherence on noise intensity, excitability, and
timescale separation, matching results from direct stochastic simulations with
substantial improvements in accuracy and generalization compared with purely
data-driven methods, while requiring significantly less computation. The
framework provides a data-efficient and interpretable surrogate model for
simulating and analyzing noise-induced coherence in multiscale stochastic
systems.

</details>


### [230] [On the Anisotropy of Score-Based Generative Models](https://arxiv.org/abs/2510.22899)
*Andreas Floros,Seyed-Mohsen Moosavi-Dezfooli,Pier Luigi Dragotti*

Main category: cs.LG

TL;DR: 研究网络架构对基于分数的生成模型归纳偏差的影响，引入SADs分析并预测模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探究网络架构在塑造现代基于分数的生成模型归纳偏差中的作用。

Method: 引入Score Anisotropy Directions (SADs)，通过合成数据和标准图像基准测试进行分析。

Result: SADs能可靠捕捉细粒度模型行为，与下游性能相关。

Conclusion: 为解释和预测生成模型的方向偏差提供新视角。

Abstract: We investigate the role of network architecture in shaping the inductive
biases of modern score-based generative models. To this end, we introduce the
Score Anisotropy Directions (SADs), architecture-dependent directions that
reveal how different networks preferentially capture data structure. Our
analysis shows that SADs form adaptive bases aligned with the architecture's
output geometry, providing a principled way to predict generalization ability
in score models prior to training. Through both synthetic data and standard
image benchmarks, we demonstrate that SADs reliably capture fine-grained model
behavior and correlate with downstream performance, as measured by Wasserstein
metrics. Our work offers a new lens for explaining and predicting directional
biases of generative models.

</details>


### [231] [Efficient Utility-Preserving Machine Unlearning with Implicit Gradient Surgery](https://arxiv.org/abs/2510.22124)
*Shiji Zhou,Tianbai Yu,Zhi Zhang,Heng Chang,Xiao Zhou,Dong Wu,Han Zhao*

Main category: cs.LG

TL;DR: 本文提出隐式梯度手术方法实现高效保留效用的机器遗忘，理论证明收敛性，实验显示优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有多目标方法在机器遗忘中对遗忘目标优化不足，无法精细控制遗忘效果与效用保留的权衡。

Method: 将机器遗忘建模为约束优化问题，证明其等价于单边梯度手术，提出隐式梯度手术方法近似求解。

Result: 理论上给出算法收敛性分析，实验表明该算法比现有基线取得更好的权衡结果。

Conclusion: 提出的隐式梯度手术方法能高效实现保留效用的机器遗忘，优于现有方法。

Abstract: Machine unlearning (MU) aims to efficiently remove sensitive or harmful
memory from a pre-trained model. The key challenge is to balance the potential
tradeoff between unlearning efficacy and utility preservation, which involves
forgetting undesirable information as defined while maintaining the model's
original performance. One potential way to tackle this problem is to use
multi-objective optimization to jointly optimize both the unlearning and
utility preservation objectives. However, existing multi-objective methods only
guarantee finding a Pareto-optimal solution without fine-grained control, which
causes under-optimization of the unlearning objective. To this end, we first
model MU as a constrained optimization problem, that is, optimizing the
unlearning objective under the constraint of a bounded increase for utility
loss. We then show that solving this optimization problem is equivalent to
unilateral gradient surgery on the unlearning objective. To resolve the
additional computational cost brought by gradient surgery, we propose an
implicit gradient surgery method, which approximates the solution to the
aforementioned constrained optimization problem via only one backpropagation,
thereby achieving efficient utility-preserving MU. Theoretically, we provide a
tight convergence analysis of the algorithm. Empirically, our extensive
experiments show that the proposed algorithm achieves better tradeoff results
than existing baselines. Codes are available at
https://github.com/anseryuer/EUPMU-Efficient-Utility-Preserving-Machine-Unlearning.

</details>


### [232] [Towards Personalized Treatment Plan: Geometrical Model-Agnostic Approach to Counterfactual Explanations](https://arxiv.org/abs/2510.22911)
*Daniel Sin,Milad Toutounchian*

Main category: cs.LG

TL;DR: 提出SSBA方法生成高维空间反事实解释，优于现有方法，可处理现实约束，运行时间有优势。


<details>
  <summary>Details</summary>
Motivation: 寻求在高维空间生成反事实解释的有效方法，处理现实约束。

Method: 采用四步方法，包括拟合数据集、找决策边界等，提出离散化方法SSBA，用二分查找找边界点。

Result: 在四个不同维度数据集上，距离减少5% - 50%，能处理现实约束，运行时间比基于网格的方法有优势。

Conclusion: SSBA是简单有效的模型无关方法，可计算最近可行的反事实解释。

Abstract: In our article, we describe a method for generating counterfactual
explanations in high-dimensional spaces using four steps that involve fitting
our dataset to a model, finding the decision boundary, determining constraints
on the problem, and computing the closest point (counterfactual explanation)
from that boundary. We propose a discretized approach where we find many
discrete points on the boundary and then identify the closest feasible
counterfactual explanation. This method, which we later call $\textit{Segmented
Sampling for Boundary Approximation}$ (SSBA), applies binary search to find
decision boundary points and then searches for the closest boundary point.
Across four datasets of varying dimensionality, we show that our method can
outperform current methods for counterfactual generation with reductions in
distance between $5\%$ to $50\%$ in terms of the $L_2$ norm. Our method can
also handle real-world constraints by restricting changes to immutable and
categorical features, such as age, gender, sex, height, and other related
characteristics such as the case for a health-based dataset. In terms of
runtime, the SSBA algorithm generates decision boundary points on multiple
orders of magnitude in the same given time when we compare to a grid-based
approach. In general, our method provides a simple and effective model-agnostic
method that can compute nearest feasible (i.e. realistic with constraints)
counterfactual explanations. All of our results and our code can be found here
at this link:
$\href{https://github.com/dsin85691/SSBA_For_Counterfactuals}{https://github.com/
dsin85691/SSBA\_For\_Counterfactuals}$

</details>


### [233] [Probing Neural Combinatorial Optimization Models](https://arxiv.org/abs/2510.22131)
*Zhiqin Zhang,Yining Ma,Zhiguang Cao,Hoong Chuin Lau*

Main category: cs.LG

TL;DR: 本文首次系统性尝试解释黑盒神经组合优化（NCO）模型，通过多种探测任务和新工具 CS - Probing 分析其表示，揭示模型编码信息，发现归纳偏差，还改进了模型泛化性。


<details>
  <summary>Details</summary>
Motivation: NCO 模型的表示和决策原理是黑盒，阻碍学术研究和实际部署，需要深入了解 NCO 模型。

Method: 通过各种探测任务研究 NCO 模型表示，引入 CS - Probing 工具分析系数和统计显著性。

Result: NCO 模型编码构建解的低级信息和辅助决策的高级知识；发现模型存在不同归纳偏差，找到与模型泛化相关证据和关键嵌入维度；修改代码改进了模型泛化性。

Conclusion: 探测是分析 NCO 模型内部机制的有前景工具，为 NCO 社区提供了见解。

Abstract: Neural combinatorial optimization (NCO) has achieved remarkable performance,
yet its learned model representations and decision rationale remain a black
box. This impedes both academic research and practical deployment, since
researchers and stakeholders require deeper insights into NCO models. In this
paper, we take the first critical step towards interpreting NCO models by
investigating their representations through various probing tasks. Moreover, we
introduce a novel probing tool named Coefficient Significance Probing
(CS-Probing) to enable deeper analysis of NCO representations by examining the
coefficients and statistical significance during probing. Extensive experiments
and analysis reveal that NCO models encode low-level information essential for
solution construction, while capturing high-level knowledge to facilitate
better decisions. Using CS-Probing, we find that prevalent NCO models impose
varying inductive biases on their learned representations, uncover direct
evidence related to model generalization, and identify key embedding dimensions
associated with specific knowledge. These insights can be potentially
translated into practice, for example, with minor code modifications, we
improve the generalization of the analyzed model. Our work represents a first
systematic attempt to interpret black-box NCO models, showcasing probing as a
promising tool for analyzing their internal mechanisms and revealing insights
for the NCO community. The source code is publicly available.

</details>


### [234] [Manifold Approximation leads to Robust Kernel Alignment](https://arxiv.org/abs/2510.22953)
*Mohammad Tariqul Islam,Du Liu,Deblina Sarkar*

Main category: cs.LG

TL;DR: 提出MKA解决CKA问题，经评估表明其在测量表征上更稳健。


<details>
  <summary>Details</summary>
Motivation: CKA不考虑潜在流形，依赖启发式方法，在不同数据规模下表现不同。

Method: 提出Manifold approximated Kernel Alignment (MKA)，推导其理论框架，在合成数据集和真实示例上进行实证评估。

Result: 流形感知的核对齐为测量表征提供了更稳健的基础。

Conclusion: MKA在测量表征方面有优势，在表征学习中有潜在应用。

Abstract: Centered kernel alignment (CKA) is a popular metric for comparing
representations, determining equivalence of networks, and neuroscience
research. However, CKA does not account for the underlying manifold and relies
on numerous heuristics that cause it to behave differently at different scales
of data. In this work, we propose Manifold approximated Kernel Alignment (MKA),
which incorporates manifold geometry into the alignment task. We derive a
theoretical framework for MKA. We perform empirical evaluations on synthetic
datasets and real-world examples to characterize and compare MKA to its
contemporaries. Our findings suggest that manifold-aware kernel alignment
provides a more robust foundation for measuring representations, with potential
applications in representation learning.

</details>


### [235] [Tractable Shapley Values and Interactions via Tensor Networks](https://arxiv.org/abs/2510.22138)
*Farzaneh Heidari,Chao Li,Farzaneh Heidari*

Main category: cs.LG

TL;DR: 提出TN - SHAP方法用少量评估替代Shapley值的O(2^n)联盟枚举，有理论保证，在UCI数据集上有加速效果。


<details>
  <summary>Details</summary>
Motivation: 替换Shapley值和Shapley风格交互指数背后的O(2^n)联盟枚举。

Method: 将预测器的局部行为表示为因式分解的多线性映射，用少量有针对性的评估提取k阶Shapley交互。

Result: 在UCI数据集上，与枚举方法在拟合的代理上结果匹配，减少评估次数，比KernelSHAP - IQ有25 - 1000倍的时钟速度提升。

Conclusion: TN - SHAP方法有效，有理论保证且能显著提升计算效率。

Abstract: We show how to replace the O(2^n) coalition enumeration over n features
behind Shapley values and Shapley-style interaction indices with a
few-evaluation scheme on a tensor-network (TN) surrogate: TN-SHAP. The key idea
is to represent a predictor's local behavior as a factorized multilinear map,
so that coalitional quantities become linear probes of a coefficient tensor.
TN-SHAP replaces exhaustive coalition sweeps with just a small number of
targeted evaluations to extract order-k Shapley interactions. In particular,
both order-1 (single-feature) and order-2 (pairwise) computations have cost
O(n*poly(chi) + n^2), where chi is the TN's maximal cut rank. We provide
theoretical guarantees on the approximation error and tractability of TN-SHAP.
On UCI datasets, our method matches enumeration on the fitted surrogate while
reducing evaluation by orders of magnitude and achieves 25-1000x wall-clock
speedups over KernelSHAP-IQ at comparable accuracy, while amortizing training
across local cohorts.

</details>


### [236] [How Muon's Spectral Design Benefits Generalization: A Study on Imbalanced Data](https://arxiv.org/abs/2510.22980)
*Bhavya Vasudeva,Puneesh Deora,Yize Zhao,Vatsal Sharan,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 研究谱感知矩阵值优化器泛化特性，用抽象方法分析，证明SpecGD在学习数据主成分上优于GD，深度会放大效果，实验验证谱优化器泛化性好。


<details>
  <summary>Details</summary>
Motivation: 随着谱感知矩阵值优化器在深度学习中应用增加，需系统研究其泛化特性及何时优于竞争算法。

Method: 引入简化抽象：用不平衡数据作测试台；研究优化器规范形式SpecGD；确定规范设置并量化SpecGD何时优于GD。

Result: SpecGD能等速率学习数据所有主成分，在训练早期平衡准确率优于GD，深度会放大效果；实验验证谱优化器有更好泛化性。

Conclusion: 谱优化器通过促进数据底层成分更平衡学习，实现更优泛化。

Abstract: The growing adoption of spectrum-aware matrix-valued optimizers such as Muon
and Shampoo in deep learning motivates a systematic study of their
generalization properties and, in particular, when they might outperform
competitive algorithms. We approach this question by introducing appropriate
simplifying abstractions as follows: First, we use imbalanced data as a
testbed. Second, we study the canonical form of such optimizers, which is
Spectral Gradient Descent (SpecGD) -- each update step is $UV^T$ where $U\Sigma
V^T$ is the truncated SVD of the gradient. Third, within this framework we
identify a canonical setting for which we precisely quantify when SpecGD
outperforms vanilla Euclidean GD. For a Gaussian mixture data model and both
linear and bilinear models, we show that unlike GD, which prioritizes learning
dominant principal components of the data first, SpecGD learns all principal
components of the data at equal rates. We demonstrate how this translates to a
growing gap in balanced accuracy favoring SpecGD early in training and further
show that the gap remains consistent even when the GD counterpart uses adaptive
step-sizes via normalization. By extending the analysis to deep linear models,
we show that depth amplifies these effects. We empirically verify our
theoretical findings on a variety of imbalanced datasets. Our experiments
compare practical variants of spectral methods, like Muon and Shampoo, against
their Euclidean counterparts and Adam. The results validate our findings that
these spectral optimizers achieve superior generalization by promoting a more
balanced learning of the data's underlying components.

</details>


### [237] [Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs](https://arxiv.org/abs/2510.22139)
*Jinzhe Liu,Junshu Sun,Shufan Shen,Chenxue Yang,Shuhui Wang*

Main category: cs.LG

TL;DR: 提出NMKE框架解决现有终身知识编辑方法累积误差问题，实验表明其表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型终身知识编辑方法在编辑过程中会累积误差，导致编辑准确性和泛化能力下降。

Method: 提出Neuron - Specific Masked Knowledge Editing (NMKE)框架，结合神经元级归因和动态稀疏掩码，识别两种关键知识神经元，引入熵引导的动态稀疏掩码进行细粒度编辑。

Result: 数千次顺序编辑实验显示，NMKE在终身编辑中保持高编辑成功率和模型泛化能力方面优于现有方法。

Conclusion: NMKE是一种有效的大语言模型终身知识编辑方法，能解决现有方法的问题。

Abstract: Lifelong knowledge editing enables continuous, precise updates to outdated
knowledge in large language models (LLMs) without computationally expensive
full retraining. However, existing methods often accumulate errors throughout
the editing process, causing a gradual decline in both editing accuracy and
generalization. To tackle this problem, we propose Neuron-Specific Masked
Knowledge Editing (NMKE), a novel fine-grained editing framework that combines
neuron-level attribution with dynamic sparse masking. Leveraging neuron
functional attribution, we identify two key types of knowledge neurons, with
knowledge-general neurons activating consistently across prompts and
knowledge-specific neurons activating to specific prompts. NMKE further
introduces an entropy-guided dynamic sparse mask, locating relevant neurons to
the target knowledge. This strategy enables precise neuron-level knowledge
editing with fewer parameter modifications. Experimental results from thousands
of sequential edits demonstrate that NMKE outperforms existing methods in
maintaining high editing success rates and preserving model general
capabilities in lifelong editing.

</details>


### [238] [Adaptive Forests For Classification](https://arxiv.org/abs/2510.22991)
*Dimitris Bertsimas,Yubing Cui*

Main category: cs.LG

TL;DR: 本文提出自适应森林(AF)方法，在分类问题上优于随机森林、XGBoost等。


<details>
  <summary>Details</summary>
Motivation: 现有随机森林和XGBoost等模型对CART树采用等权重聚合，希望提出能自适应选择底层CART模型权重的方法。

Method: 结合最优预测策略树(OP2T)框架为树分配与输入相关的不等权重，使用混合整数优化(MIO)动态优化权重候选。

Result: 在20多个真实数据集的二分类和多分类问题中，AF始终优于RF、XGBoost和其他加权RF。

Conclusion: AF方法在分类问题上表现出色，能通过自适应选择权重提升整体性能。

Abstract: Random Forests (RF) and Extreme Gradient Boosting (XGBoost) are two of the
most widely used and highly performing classification and regression models.
They aggregate equally weighted CART trees, generated randomly in RF or
sequentially in XGBoost. In this paper, we propose Adaptive Forests (AF), a
novel approach that adaptively selects the weights of the underlying CART
models. AF combines (a) the Optimal Predictive-Policy Trees (OP2T) framework to
prescribe tailored, input-dependent unequal weights to trees and (b) Mixed
Integer Optimization (MIO) to refine weight candidates dynamically, enhancing
overall performance. We demonstrate that AF consistently outperforms RF,
XGBoost, and other weighted RF in binary and multi-class classification
problems over 20+ real-world datasets.

</details>


### [239] [Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics](https://arxiv.org/abs/2510.22158)
*Lorenzo Magnino,Kai Shao,Zida Wu,Jiacheng Shen,Mathieu Laurière*

Main category: cs.LG

TL;DR: 本文提出针对非平稳连续平均场博弈（MFG）的深度强化学习（DRL）算法，基于虚拟博弈方法，通过实验验证其有效性，推动DRL在复杂MFG问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有MFG的强化学习方法局限于有限空间或静态模型，难以应用于实际问题。

Method: 基于虚拟博弈（FP）方法，利用DRL进行最佳响应计算，用监督学习表示平均策略，使用条件归一化流学习随时间变化的群体分布。

Result: 在三个复杂度递增的例子上评估了该方法。

Conclusion: 该工作解决了可扩展性和密度近似的关键限制，推动了DRL在复杂MFG问题中的应用，使该领域更接近实际多智能体系统。

Abstract: Mean field games (MFGs) have emerged as a powerful framework for modeling
interactions in large-scale multi-agent systems. Despite recent advancements in
reinforcement learning (RL) for MFGs, existing methods are typically limited to
finite spaces or stationary models, hindering their applicability to real-world
problems. This paper introduces a novel deep reinforcement learning (DRL)
algorithm specifically designed for non-stationary continuous MFGs. The
proposed approach builds upon a Fictitious Play (FP) methodology, leveraging
DRL for best-response computation and supervised learning for average policy
representation. Furthermore, it learns a representation of the time-dependent
population distribution using a Conditional Normalizing Flow. To validate the
effectiveness of our method, we evaluate it on three different examples of
increasing complexity. By addressing critical limitations in scalability and
density approximation, this work represents a significant advancement in
applying DRL techniques to complex MFG problems, bringing the field closer to
real-world multi-agent systems.

</details>


### [240] [Quantitative Bounds for Sorting-Based Permutation-Invariant Embeddings](https://arxiv.org/abs/2510.22186)
*Nadav Dym,Matthias Wellershoff,Efstratios Tsoukanis,Daniel Levy,Radu Balan*

Main category: cs.LG

TL;DR: 本文研究基于排序的嵌入映射，改进了嵌入维度的上下界，构造矩阵得到双Lipschitz畸变结果，并给出降维变体的类似结果。


<details>
  <summary>Details</summary>
Motivation: 前人工作在排序嵌入映射中，嵌入维度最优值和双Lipschitz常数估计未知，本文旨在解决这两个问题。

Method: 改进已知的嵌入维度上界，给出最小嵌入维度下界，构造矩阵分析双Lipschitz畸变。

Result: 改进了嵌入维度上界，给出下界；构造矩阵使双Lipschitz畸变与d无关，与n二次相关，且畸变至少为Ω(√n)；给出降维变体的类似结果。

Conclusion: 在解决排序嵌入映射的两个遗留问题上取得了实质性进展。

Abstract: We study the sorting-based embedding $\beta_{\mathbf A} : \mathbb R^{n \times
d} \to \mathbb R^{n \times D}$, $\mathbf X \mapsto {\downarrow}(\mathbf X
\mathbf A)$, where $\downarrow$ denotes column wise sorting of matrices. Such
embeddings arise in graph deep learning where outputs should be invariant to
permutations of graph nodes. Previous work showed that for large enough $D$ and
appropriate $\mathbf A$, the mapping $\beta_{\mathbf A}$ is injective, and
moreover satisfies a bi-Lipschitz condition. However, two gaps remain: firstly,
the optimal size $D$ required for injectivity is not yet known, and secondly,
no estimates of the bi-Lipschitz constants of the mapping are known.
  In this paper, we make substantial progress in addressing both of these gaps.
Regarding the first gap, we improve upon the best known upper bounds for the
embedding dimension $D$ necessary for injectivity, and also provide a lower
bound on the minimal injectivity dimension. Regarding the second gap, we
construct matrices $\mathbf A$, so that the bi-Lipschitz distortion of
$\beta_{\mathbf A} $ depends quadratically on $n$, and is completely
independent of $d$. We also show that the distortion of $\beta_{\mathbf A}$ is
necessarily at least in $\Omega(\sqrt{n})$. Finally, we provide similar results
for variants of $\beta_{\mathbf A}$ obtained by applying linear projections to
reduce the output dimension of $\beta_{\mathbf A}$.

</details>


### [241] [The Benchmarking Epistemology: Construct Validity for Evaluating Machine Learning Models](https://arxiv.org/abs/2510.23191)
*Timo Freiesleben,Sebastian Zezulka*

Main category: cs.LG

TL;DR: 本文探讨预测基准测试，提出构建效度条件并通过案例分析，明确基准分数支持科学主张的条件。


<details>
  <summary>Details</summary>
Motivation: 预测基准测试仅基准分数难以进行科学推断，需明确额外假设。

Method: 受心理测量理论启发提出构建效度条件，通过三个案例研究检验假设。

Result: 框架明确了基准分数支持多样科学主张的条件。

Conclusion: 将预测基准测试作为认识论实践和机器学习概念与理论推理的关键环节。

Abstract: Predictive benchmarking, the evaluation of machine learning models based on
predictive performance and competitive ranking, is a central epistemic practice
in machine learning research and an increasingly prominent method for
scientific inquiry. Yet, benchmark scores alone provide at best measurements of
model performance relative to an evaluation dataset and a concrete learning
problem. Drawing substantial scientific inferences from the results, say about
theoretical tasks like image classification, requires additional assumptions
about the theoretical structure of the learning problems, evaluation functions,
and data distributions. We make these assumptions explicit by developing
conditions of construct validity inspired by psychological measurement theory.
We examine these assumptions in practice through three case studies, each
exemplifying a typical intended inference: measuring engineering progress in
computer vision with ImageNet; evaluating policy-relevant weather predictions
with WeatherBench; and examining limitations of the predictability of life
events with the Fragile Families Challenge. Our framework clarifies the
conditions under which benchmark scores can support diverse scientific claims,
bringing predictive benchmarking into perspective as an epistemological
practice and a key site of conceptual and theoretical reasoning in machine
learning.

</details>


### [242] [Multi-dataset Joint Pre-training of Emotional EEG Enables Generalizable Affective Computing](https://arxiv.org/abs/2510.22197)
*Qingzhu Zhang,Jiani Zhong,Zongsheng Li,Xinke Shen,Quanying Liu*

Main category: cs.LG

TL;DR: 本文提出任务特定的多数据集联合预训练框架用于跨数据集情绪识别，在少样本和零样本任务上表现优异，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有任务通用预训练脑电图模型在复杂情绪识别任务中因特征不匹配表现不佳，且存在数据集分布偏移、情绪类别定义不一致和个体差异大等问题。

Method: 引入跨数据集协方差对齐损失以对齐数据集二阶统计特性，提出结合类Mamba线性注意力通道编码器和时空动态模型的混合编码器。

Result: 少样本情绪识别的AUROC比现有模型平均高4.57%，零样本泛化准确率高11.92%，多数据集联合预训练比单数据集训练性能提升8.55%。

Conclusion: 提供了可扩展的任务特定预训练框架，凸显其在可泛化情感计算中的优势。

Abstract: Task-specific pre-training is essential when task representations diverge
from generic pre-training features. Existing task-general pre-training EEG
models struggle with complex tasks like emotion recognition due to mismatches
between task-specific features and broad pre-training approaches. This work
aims to develop a task-specific multi-dataset joint pre-training framework for
cross-dataset emotion recognition, tackling problems of large inter-dataset
distribution shifts, inconsistent emotion category definitions, and substantial
inter-subject variability. We introduce a cross-dataset covariance alignment
loss to align second-order statistical properties across datasets, enabling
robust generalization without the need for extensive labels or per-subject
calibration. To capture the long-term dependency and complex dynamics of EEG,
we propose a hybrid encoder combining a Mamba-like linear attention channel
encoder and a spatiotemporal dynamics model. Our method outperforms
state-of-the-art large-scale EEG models by an average of 4.57% in AUROC for
few-shot emotion recognition and 11.92% in accuracy for zero-shot
generalization to a new dataset. Performance scales with the increase of
datasets used in pre-training. Multi-dataset joint pre-training achieves a
performance gain of 8.55% over single-dataset training. This work provides a
scalable framework for task-specific pre-training and highlights its benefit in
generalizable affective computing. Our code is available at
https://github.com/ncclab-sustech/mdJPT_nips2025.

</details>


### [243] [The Lossy Horizon: Error-Bounded Predictive Coding for Lossy Text Compression (Episode I)](https://arxiv.org/abs/2510.22207)
*Nnamdi Aghanya,Jun Li,Kewei Wang*

Main category: cs.LG

TL;DR: 研究大语言模型在有损压缩领域应用，提出EPC编解码器，优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在有损压缩领域的应用，以实现更高压缩比。

Method: 引入基于掩码语言模型的Error - Bounded Predictive Coding (EPC)编解码器，通过模型预测掩码内容并存储少量基于排名的修正。

Result: 通过评估，EPC在比特率更低的情况下比Predictive Masking (PM)有更好的保真度。

Conclusion: EPC能更有效地利用模型的内在知识，在有损文本压缩中表现更优。

Abstract: Large Language Models (LLMs) can achieve near-optimal lossless compression by
acting as powerful probability models. We investigate their use in the lossy
domain, where reconstruction fidelity is traded for higher compression ratios.
This paper introduces Error-Bounded Predictive Coding (EPC), a lossy text codec
that leverages a Masked Language Model (MLM) as a decompressor. Instead of
storing a subset of original tokens, EPC allows the model to predict masked
content and stores minimal, rank-based corrections only when the model's top
prediction is incorrect. This creates a residual channel that offers continuous
rate-distortion control. We compare EPC to a simpler Predictive Masking (PM)
baseline and a transform-based Vector Quantisation with a Residual Patch
(VQ+RE) approach. Through an evaluation that includes precise bit accounting
and rate-distortion analysis, we demonstrate that EPC consistently dominates
PM, offering superior fidelity at a significantly lower bit rate by more
efficiently utilising the model's intrinsic knowledge.

</details>


### [244] [Simplifying Knowledge Transfer in Pretrained Models](https://arxiv.org/abs/2510.22208)
*Siddharth Jain,Shyamgopal Karthik,Vineet Gandhi*

Main category: cs.LG

TL;DR: 本文提出利用公开模型库，通过数据分区策略让预训练模型扮演学生或教师角色进行知识转移，实验证明该方法在多任务上有效。


<details>
  <summary>Details</summary>
Motivation: 不同设计选择的模型有不同泛化行为，可利用公开模型库提升模型。

Method: 引入数据分区策略，让预训练模型自主扮演学生或教师角色进行知识转移，还扩展到多模型间的知识转移。

Result: 在图像分类中提升ViT - B性能约1.4%；语义分割中提升所有评估指标；视频显著性预测达到新的最优；多模型知识转移让所有参与模型性能显著提升。

Conclusion: 所提方法在各种任务中有效，能提升模型性能。

Abstract: Pretrained models are ubiquitous in the current deep learning landscape,
offering strong results on a broad range of tasks. Recent works have shown that
models differing in various design choices exhibit categorically diverse
generalization behavior, resulting in one model grasping distinct data-specific
insights unavailable to the other. In this paper, we propose to leverage large
publicly available model repositories as an auxiliary source of model
improvements. We introduce a data partitioning strategy where pretrained models
autonomously adopt either the role of a student, seeking knowledge, or that of
a teacher, imparting knowledge. Experiments across various tasks demonstrate
the effectiveness of our proposed approach. In image classification, we
improved the performance of ViT-B by approximately 1.4% through bidirectional
knowledge transfer with ViT-T. For semantic segmentation, our method boosted
all evaluation metrics by enabling knowledge transfer both within and across
backbone architectures. In video saliency prediction, our approach achieved a
new state-of-the-art. We further extend our approach to knowledge transfer
between multiple models, leading to considerable performance improvements for
all model participants.

</details>


### [245] [GCAO: Group-driven Clustering via Gravitational Attraction and Optimization](https://arxiv.org/abs/2510.23259)
*Qi Li,Jun Wang*

Main category: cs.LG

TL;DR: 传统聚类算法处理高维非均匀数据有问题，提出GCAO算法，实验表明其优于11种代表算法。


<details>
  <summary>Details</summary>
Motivation: 传统聚类算法处理高维非均匀分布数据时，低密度边界样本易受干扰，导致聚类结果不稳定和扭曲。

Method: 提出GCAO算法，引入组级优化机制，结合局部密度估计和邻域拓扑，构建组与周围的引力相互作用，以组为基本运动单元采用引力收缩策略。

Result: 在多个高维数据集上实验，GCAO在NMI、ARI、Homogeneity和ACC上分别平均提升37.13%、52.08%、44.98%和38.81%，且效率和可扩展性有竞争力。

Conclusion: GCAO在保持聚类完整性、增强边界可分离性和确保复杂数据分布上的鲁棒性能方面具有优越性。

Abstract: Traditional clustering algorithms often struggle with high-dimensional and
non-uniformly distributed data, where low-density boundary samples are easily
disturbed by neighboring clusters, leading to unstable and distorted clustering
results. To address this issue, we propose a Group-driven Clustering via
Gravitational Attraction and Optimization (GCAO) algorithm. GCAO introduces a
group-level optimization mechanism that aggregates low-density boundary points
into collaboratively moving groups, replacing the traditional point-based
contraction process. By combining local density estimation with neighborhood
topology, GCAO constructs effective gravitational interactions between groups
and their surroundings, enhancing boundary clarity and structural consistency.
Using groups as basic motion units, a gravitational contraction strategy
ensures globally stable and directionally consistent convergence. Experiments
on multiple high-dimensional datasets demonstrate that GCAO outperforms 11
representative clustering methods, achieving average improvements of 37.13%,
52.08%, 44.98%, and 38.81% in NMI, ARI, Homogeneity, and ACC, respectively,
while maintaining competitive efficiency and scalability. These results
highlight GCAO's superiority in preserving cluster integrity, enhancing
boundary separability, and ensuring robust performance on complex data
distributions.

</details>


### [246] [Visual Model Selection using Feature Importance Clusters in Fairness-Performance Similarity Optimized Space](https://arxiv.org/abs/2510.22209)
*Sofoklis Kitharidis,Cor J. Veenman,Thomas Bäck,Niki van Stein*

Main category: cs.LG

TL;DR: 提出交互式框架辅助选择算法决策模型，利用弱监督度量学习和聚类技术帮助用户理解模型权衡。


<details>
  <summary>Details</summary>
Motivation: 算法决策中公平机器学习方法产生多样模型，给利益相关者选择合适模型带来挑战。

Method: 利用弱监督度量学习学习马氏距离，根据利益相关标准构建模型特征重要性空间；应用k - means聚类技术对模型分组。

Result: 用户可探索具有相似预测行为和公平性特征的模型簇。

Conclusion: 框架有助于利益相关者根据自身需求和价值观选择模型，实现知情决策。

Abstract: In the context of algorithmic decision-making, fair machine learning methods
often yield multiple models that balance predictive fairness and performance in
varying degrees. This diversity introduces a challenge for stakeholders who
must select a model that aligns with their specific requirements and values. To
address this, we propose an interactive framework that assists in navigating
and interpreting the trade-offs across a portfolio of models. Our approach
leverages weakly supervised metric learning to learn a Mahalanobis distance
that reflects similarity in fairness and performance outcomes, effectively
structuring the feature importance space of the models according to
stakeholder-relevant criteria. We then apply clustering technique (k-means) to
group models based on their transformed representations of feature importances,
allowing users to explore clusters of models with similar predictive behaviors
and fairness characteristics. This facilitates informed decision-making by
helping users understand how models differ not only in their
fairness-performance balance but also in the features that drive their
predictions.

</details>


### [247] [Robust Non-negative Proximal Gradient Algorithm for Inverse Problems](https://arxiv.org/abs/2510.23362)
*Hanzhang Wang,Zonglin Liu,Jingyi Xu,Chenyang Wang,Zhiwei Zhong,Qiangqiang Shen*

Main category: cs.LG

TL;DR: 提出新的乘法更新近端梯度算法SSO - PGA解决传统PGA在非负逆问题中的局限，实验表明其性能和稳定性超传统算法。


<details>
  <summary>Details</summary>
Motivation: 传统近端梯度算法（PGA）在逆问题中收敛不稳定、解次优，违反非负约束，对超参数敏感。

Method: 提出SSO - PGA，用可学习的基于sigmoid的算子取代梯度下降步骤，加入滑动参数；为多模态恢复制定退化模型并推导优化算法，展开为深度网络。

Result: 广泛的数值和实际实验表明该方法显著超越传统PGA和其他先进算法。

Conclusion: SSO - PGA在非负逆问题中具有更好的性能和稳定性。

Abstract: Proximal gradient algorithms (PGA), while foundational for inverse problems
like image reconstruction, often yield unstable convergence and suboptimal
solutions by violating the critical non-negativity constraint. We identify the
gradient descent step as the root cause of this issue, which introduces
negative values and induces high sensitivity to hyperparameters. To overcome
these limitations, we propose a novel multiplicative update proximal gradient
algorithm (SSO-PGA) with convergence guarantees, which is designed for
robustness in non-negative inverse problems. Our key innovation lies in
superseding the gradient descent step with a learnable sigmoid-based operator,
which inherently enforces non-negativity and boundedness by transforming
traditional subtractive updates into multiplicative ones. This design,
augmented by a sliding parameter for enhanced stability and convergence, not
only improves robustness but also boosts expressive capacity and noise
immunity. We further formulate a degradation model for multi-modal restoration
and derive its SSO-PGA-based optimization algorithm, which is then unfolded
into a deep network to marry the interpretability of optimization with the
power of deep learning. Extensive numerical and real-world experiments
demonstrate that our method significantly surpasses traditional PGA and other
state-of-the-art algorithms, ensuring superior performance and stability.

</details>


### [248] [When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs](https://arxiv.org/abs/2510.22228)
*Keyu Wang,Tian Lyu,Guinan Su,Jonas Geiping,Lu Yin,Marco Canini,Shiwei Liu*

Main category: cs.LG

TL;DR: 研究层剪枝对大语言模型长链推理的影响，发现剪枝会严重损害测试时缩放能力，标准微调无法恢复，呼吁重新思考剪枝策略。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝方法对长链推理能力的影响未充分研究，而长链推理是大语言模型重要且脆弱的能力。

Method: 通过测试时缩放视角研究层剪枝对长链推理的影响，并进行大量实验。

Result: 剪枝一到两层会严重损害测试时缩放能力，在长推理基准上性能大幅下降，标准监督微调无法恢复。

Conclusion: 指出对推理密集型大语言模型应用层剪枝存在根本风险，需重新思考剪枝策略并开发维护推理鲁棒性的方法。

Abstract: Layer pruning has emerged as a widely adopted technique for improving the
efficiency of large language models (LLMs). Although existing methods
demonstrate strong performance retention on general knowledge tasks, their
effect on long-chain reasoning, a more brittle yet crucial capability, remains
largely unexplored. In this work, we study the impact of layer pruning on
long-chain reasoning through the lens of test-time scaling, a key mechanism in
modern LLMs that enables strong reasoning capacity by allocating more
computation at inference time. With extensive experiments, we demonstrate that
pruning even one or two layers can severely impair test-time scaling, with
performance collapsing drastically on long reasoning benchmarks even when
performance on knowledge-intensive and shallow reasoning tasks remains stable.
Furthermore, we find that standard supervised fine-tuning remedies fail to
recover test-time scaling once it has deteriorated. Through in-depth analyses,
we identify the mechanisms underlying this fragility of test-time scaling and
highlight the fundamental risks of applying layer pruning to
reasoning-intensive LLMs. These findings call for a rethinking of layer pruning
strategies and provide insights for developing methods that preserve the
robustness of reasoning. We open-source the codebase in
\href{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}.

</details>


### [249] [LUNA: Efficient and Topology-Agnostic Foundation Model for EEG Signal Analysis](https://arxiv.org/abs/2510.22257)
*Berkay Döner,Thorir Mar Ingolfsson,Luca Benini,Yawei Li*

Main category: cs.LG

TL;DR: 提出自监督基础模型LUNA处理EEG拓扑异质性问题，在多任务表现佳，减少计算量和显存使用。


<details>
  <summary>Details</summary>
Motivation: EEG构建大规模模型受拓扑异质性阻碍，各公开数据电极布局不同限制泛化性。

Method: 引入LUNA模型，通过学习查询和交叉注意力将多通道EEG压缩到固定大小、拓扑无关潜在空间，下游变压器块在潜在表示上操作。用掩码补丁重建目标在TUEG和Siena上预训练。

Result: LUNA有效迁移到四个下游任务，在多个基准测试有竞争力，在TUAR和TUSL达SOTA，如TUAR上AUROC为0.921，减少300倍FLOPs和最多10倍GPU显存使用，不同电极配置结果稳定。

Conclusion: LUNA能有效解决EEG拓扑异质性问题，在多任务表现出色，减少计算资源消耗。

Abstract: Electroencephalography (EEG) offers a non-invasive lens into human brain
activity, but building large-scale models is hampered by topological
heterogeneity: each public EEG data defines its own electrode layout, limiting
generalization. We introduce LUNA (Latent Unified Network Architecture), a
self-supervised foundation model that reconciles disparate electrode geometries
while scaling linearly -- not quadratically -- with channel count. LUNA
compresses multi-channel EEG into a fixed-size, topology-agnostic latent space
via learned queries and cross-attention. Downstream transformer blocks then
operate exclusively on this latent representation using patch-wise temporal
self-attention, decoupling computation from electrode count. Pre-trained on
TUEG and Siena (over 21,000 hours of raw EEG across diverse montages) using a
masked-patch reconstruction objective, LUNA transfers effectively to four
downstream tasks: abnormality detection, artifact rejection, slowing
classification, and emotion recognition. It demonstrates highly competitive
performance across several benchmarks, achieving state-of-the-art results on
TUAR and TUSL, e.g., 0.921 AUROC on TUAR, while reducing FLOPs by 300x and
trimming GPU memory use by up to 10x. Critically, these gains are consistent
across all evaluated electrode configurations. Code is available at
https://github.com/pulp-bio/BioFoundation

</details>


### [250] [An Information-Theoretic Analysis of Out-of-Distribution Generalization in Meta-Learning with Applications to Meta-RL](https://arxiv.org/abs/2510.23448)
*Xingtu Liu*

Main category: cs.LG

TL;DR: 从信息论角度研究元学习中的分布外泛化，聚焦两种场景，形式化元强化学习泛化问题并建立边界，分析算法泛化性能。


<details>
  <summary>Details</summary>
Motivation: 研究元学习中不同环境下的分布外泛化问题。

Method: 从信息论角度，聚焦两种场景，形式化元强化学习泛化问题。

Result: 建立了相应的泛化边界，分析了梯度基元强化学习算法的泛化性能。

Conclusion: 文中方法可用于研究元学习的分布外泛化问题。

Abstract: In this work, we study out-of-distribution generalization in meta-learning
from an information-theoretic perspective. We focus on two scenarios: (i) when
the testing environment mismatches the training environment, and (ii) when the
training environment is broader than the testing environment. The first
corresponds to the standard distribution mismatch setting, while the second
reflects a broad-to-narrow training scenario. We further formalize the
generalization problem in meta-reinforcement learning and establish
corresponding generalization bounds. Finally, we analyze the generalization
performance of a gradient-based meta-reinforcement learning algorithm.

</details>


### [251] [Epistemic Deep Learning: Enabling Machine Learning Models to Know When They Do Not Know](https://arxiv.org/abs/2510.22261)
*Shireen Kudukkil Manchingal*

Main category: cs.LG

TL;DR: 论文围绕认知深度学习展开，开发RS - NN方法并提出评估框架，实验验证融入认知意识可降低过度自信预测风险，推动AI范式转变。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在安全关键领域因无法管理不确定性，遇到异常数据时预测过度自信且不可靠的问题。

Method: 开发随机集神经网络（RS - NN），利用随机集理论预测类集合上的信任函数；提出不确定性感知分类器的统一评估框架。

Result: 广泛实验验证，将认知意识融入深度学习可降低过度自信预测的风险。

Conclusion: 将‘知道自己何时不知道’的能力融入AI，能成为构建稳健可靠系统的标志，推动AI范式转变。

Abstract: Machine learning has achieved remarkable successes, yet its deployment in
safety-critical domains remains hindered by an inherent inability to manage
uncertainty, resulting in overconfident and unreliable predictions when models
encounter out-of-distribution data, adversarial perturbations, or naturally
fluctuating environments. This thesis, titled Epistemic Deep Learning: Enabling
Machine Learning Models to 'Know When They Do Not Know', addresses these
critical challenges by advancing the paradigm of Epistemic Artificial
Intelligence, which explicitly models and quantifies epistemic uncertainty: the
uncertainty arising from limited, biased, or incomplete training data, as
opposed to the irreducible randomness of aleatoric uncertainty, thereby
empowering models to acknowledge their limitations and refrain from
overconfident decisions when uncertainty is high.
  Central to this work is the development of the Random-Set Neural Network
(RS-NN), a novel methodology that leverages random set theory to predict belief
functions over sets of classes, capturing the extent of epistemic uncertainty
through the width of associated credal sets, applications of RS-NN, including
its adaptation to Large Language Models (LLMs) and its deployment in weather
classification for autonomous racing. In addition, the thesis proposes a
unified evaluation framework for uncertainty-aware classifiers. Extensive
experiments validate that integrating epistemic awareness into deep learning
not only mitigates the risks associated with overconfident predictions but also
lays the foundation for a paradigm shift in artificial intelligence, where the
ability to 'know when it does not know' becomes a hallmark of robust and
dependable systems. The title encapsulates the core philosophy of this work,
emphasizing that true intelligence involves recognizing and managing the limits
of one's own knowledge.

</details>


### [252] [Differential Privacy as a Perk: Federated Learning over Multiple-Access Fading Channels with a Multi-Antenna Base Station](https://arxiv.org/abs/2510.23463)
*Hao Liang,Haifeng Wen,Kaishun Wu,Hong Xing*

Main category: cs.LG

TL;DR: 研究多天线基站多址衰落信道上的AirFL，在用户级DP要求下，证明不使用人工噪声也能实现DP，推导DP和收敛界，优化参数权衡收敛与隐私，数值结果验证理论。


<details>
  <summary>Details</summary>
Motivation: 现有研究难以有效利用信道损伤，多在简单模型或受限损失函数假设下增强隐私，本文旨在研究多天线基站多址衰落信道上的AirFL并满足用户级DP要求。

Method: 推导模型参数有界假设下的DP收敛界和一般平滑非凸损失函数的收敛界，优化接收波束成形和功率分配。

Result: 证明不使用人工噪声也能实现DP，得到收敛 - 隐私的最优权衡及可实现DP且不影响训练的条件。

Conclusion: 理论结果得到大量数值结果验证，为AirFL在多天线基站多址衰落信道上的应用提供了有效方案。

Abstract: Federated Learning (FL) is a distributed learning paradigm that preserves
privacy by eliminating the need to exchange raw data during training. In its
prototypical edge instantiation with underlying wireless transmissions enabled
by analog over-the-air computing (AirComp), referred to as \emph{over-the-air
FL (AirFL)}, the inherent channel noise plays a unique role of \emph{frenemy}
in the sense that it degrades training due to noisy global aggregation while
providing a natural source of randomness for privacy-preserving mechanisms,
formally quantified by \emph{differential privacy (DP)}. It remains,
nevertheless, challenging to effectively harness such channel impairments, as
prior arts, under assumptions of either simple channel models or restricted
types of loss functions, mostly considering (local) DP enhancement with a
single-round or non-convergent bound on privacy loss. In this paper, we study
AirFL over multiple-access fading channels with a multi-antenna base station
(BS) subject to user-level DP requirements. Despite a recent study, which
claimed in similar settings that artificial noise (AN) must be injected to
ensure DP in general, we demonstrate, on the contrary, that DP can be gained as
a \emph{perk} even \emph{without} employing any AN. Specifically, we derive a
novel bound on DP that converges under general bounded-domain assumptions on
model parameters, along with a convergence bound with general smooth and
non-convex loss functions. Next, we optimize over receive beamforming and power
allocations to characterize the optimal convergence-privacy trade-offs, which
also reveal explicit conditions in which DP is achievable without compromising
training. Finally, our theoretical findings are validated by extensive
numerical results.

</details>


### [253] [A Multi-level Analysis of Factors Associated with Student Performance: A Machine Learning Approach to the SAEB Microdata](https://arxiv.org/abs/2510.22266)
*Rodrigo Tertulino,Ricardo Almeida*

Main category: cs.LG

TL;DR: 本文用多级机器学习方法分析巴西基础教育学生成绩影响因素，随机森林模型效果好，发现学校系统因素影响大，为教育政策提供工具。


<details>
  <summary>Details</summary>
Motivation: 识别影响巴西基础教育学生成绩的因素，以制定有效公共政策。

Method: 采用多级机器学习方法，整合四类数据来源，对比四种集成算法，应用可解释人工智能SHAP。

Result: 随机森林模型准确率达90.2%，AUC为96.7%，发现学校平均社会经济水平是最主要预测因素。

Conclusion: 学业成绩是与学校生态系统密切相关的系统性现象，研究为促进教育公平政策提供数据驱动、可解释的工具。

Abstract: Identifying the factors that influence student performance in basic education
is a central challenge for formulating effective public policies in Brazil.
This study introduces a multi-level machine learning approach to classify the
proficiency of 9th-grade and high school students using microdata from the
System of Assessment of Basic Education (SAEB). Our model uniquely integrates
four data sources: student socioeconomic characteristics, teacher professional
profiles, school indicators, and director management profiles. A comparative
analysis of four ensemble algorithms confirmed the superiority of a Random
Forest model, which achieved 90.2% accuracy and an Area Under the Curve (AUC)
of 96.7%. To move beyond prediction, we applied Explainable AI (XAI) using
SHAP, which revealed that the school's average socioeconomic level is the most
dominant predictor, demonstrating that systemic factors have a greater impact
than individual characteristics in isolation. The primary conclusion is that
academic performance is a systemic phenomenon deeply tied to the school's
ecosystem. This study provides a data-driven, interpretable tool to inform
policies aimed at promoting educational equity by addressing disparities
between schools.

</details>


### [254] [Machine Learning Enabled Early Warning System For Financial Distress Using Real-Time Digital Signals](https://arxiv.org/abs/2510.22287)
*Laxmi pant,Syed Ali Reza,Md Khalilor Rahman,MD Saifur Rahman,Shamima Sharmin,Md Fazlul Huq Mithu,Kazi Nehal Hasnain,Adnan Farabi,Mahamuda khanom,Raisul Kabir*

Main category: cs.LG

TL;DR: 研究引入基于机器学习的预警系统，用实时数字和宏观经济信号识别家庭财务困境，结果显示数字经济特征提升预测准确性，系统可扩展部署并提供实时预警。


<details>
  <summary>Details</summary>
Motivation: 全球和国内经济环境不稳定增加家庭财务困境风险，传统计量模型依赖延迟和汇总数据，有效性受限。

Method: 利用750个家庭的面板数据集，结合社会经济属性、宏观经济指标和数字经济指标，进行数据预处理和特征工程，对比多种分类模型。

Result: 数字经济特征显著提高预测准确性，系统在二元困境检测和多类严重程度分类中表现可靠，确定通胀波动和ICT需求为关键预测因素。

Conclusion: 以透明可解释方式应用机器学习提供近实时预警是可行且有影响的，能增强家庭韧性并指导干预策略。

Abstract: The growing instability of both global and domestic economic environments has
increased the risk of financial distress at the household level. However,
traditional econometric models often rely on delayed and aggregated data,
limiting their effectiveness. This study introduces a machine learning-based
early warning system that utilizes real-time digital and macroeconomic signals
to identify financial distress in near real-time. Using a panel dataset of 750
households tracked over three monitoring rounds spanning 13 months, the
framework combines socioeconomic attributes, macroeconomic indicators (such as
GDP growth, inflation, and foreign exchange fluctuations), and digital economy
measures (including ICT demand and market volatility). Through data
preprocessing and feature engineering, we introduce lagged variables,
volatility measures, and interaction terms to capture both gradual and sudden
changes in financial stability. We benchmark baseline classifiers, such as
logistic regression and decision trees, against advanced ensemble models
including random forests, XGBoost, and LightGBM. Our results indicate that the
engineered features from the digital economy significantly enhance predictive
accuracy. The system performs reliably for both binary distress detection and
multi-class severity classification, with SHAP-based explanations identifying
inflation volatility and ICT demand as key predictors. Crucially, the framework
is designed for scalable deployment in national agencies and low-bandwidth
regional offices, ensuring it is accessible for policymakers and practitioners.
By implementing machine learning in a transparent and interpretable manner,
this study demonstrates the feasibility and impact of providing near-real-time
early warnings of financial distress. This offers actionable insights that can
strengthen household resilience and guide preemptive intervention strategies.

</details>


### [255] [Does Homophily Help in Robust Test-time Node Classification?](https://arxiv.org/abs/2510.22289)
*Yan Jiang,Ruihong Qiu,Zi Huang*

Main category: cs.LG

TL;DR: 文章揭示调整测试图结构中同质性可提升预训练GNN鲁棒性和性能，提出基于同质性的测试时图结构转换方法GrapHoST，实验效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注训练图上的GNN学习，而现实中测试图存在数据质量和分布偏移问题，影响预训练模型鲁棒性和测试性能。

Method: 提出GrapHoST方法，开发同质性预测器区分测试边，根据预测同质性分数的置信度进行自适应测试时图结构转换。

Result: 在九个基准数据集上的实验表明，GrapHoST始终达到了最先进的性能，提升幅度最高达10.92%。

Conclusion: 调整测试图结构同质性能有效提升预训练GNN性能，GrapHoST方法效果显著。

Abstract: Homophily, the tendency of nodes from the same class to connect, is a
fundamental property of real-world graphs, underpinning structural and semantic
patterns in domains such as citation networks and social networks. Existing
methods exploit homophily through designing homophily-aware GNN architectures
or graph structure learning strategies, yet they primarily focus on GNN
learning with training graphs. However, in real-world scenarios, test graphs
often suffer from data quality issues and distribution shifts, such as domain
shifts across users from different regions in social networks and temporal
evolution shifts in citation network graphs collected over varying time
periods. These factors significantly compromise the pre-trained model's
robustness, resulting in degraded test-time performance. With empirical
observations and theoretical analysis, we reveal that transforming the test
graph structure by increasing homophily in homophilic graphs or decreasing it
in heterophilic graphs can significantly improve the robustness and performance
of pre-trained GNNs on node classifications, without requiring model training
or update. Motivated by these insights, a novel test-time graph structural
transformation method grounded in homophily, named GrapHoST, is proposed.
Specifically, a homophily predictor is developed to discriminate test edges,
facilitating adaptive test-time graph structural transformation by the
confidence of predicted homophily scores. Extensive experiments on nine
benchmark datasets under a range of test-time data quality issues demonstrate
that GrapHoST consistently achieves state-of-the-art performance, with
improvements of up to 10.92%. Our code has been released at
https://github.com/YanJiangJerry/GrapHoST.

</details>


### [256] [Predicting Metabolic Dysfunction-Associated Steatotic Liver Disease using Machine Learning Methods](https://arxiv.org/abs/2510.22293)
*Mary E. An,Paul Griffin,Jonathan G. Stine,Ramakrishna Balakrishnan,Ram Sriram,Soundar Kumara*

Main category: cs.LG

TL;DR: 开发MASER预测模型预测MASLD，对比不同方法，模型有竞争力且兼顾预测性能与公平性。


<details>
  <summary>Details</summary>
Motivation: MASLD影响美国约33%成年人，早期检测重要，需开发公平、严谨、可重复的预测模型。

Method: 用LASSO逻辑回归、随机森林、XGBoost和神经网络，结合临床特征子集，用等机会后处理方法减少不同种族和族裔亚组间真阳性率差异。

Result: 选含前10特征的LASSO逻辑回归模型，公平调整前有一定性能指标，调整后部分指标有变化。

Conclusion: MASER模型有竞争力，可在不同患者群体中平衡预测性能和公平性。

Abstract: Background: Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD)
affects ~33% of U.S. adults and is the most common chronic liver disease.
Although often asymptomatic, progression can lead to cirrhosis. Early detection
is important, as lifestyle interventions can prevent disease progression. We
developed a fair, rigorous, and reproducible MASLD prediction model and
compared it to prior methods using a large electronic health record database.
  Methods: We evaluated LASSO logistic regression, random forest, XGBoost, and
a neural network for MASLD prediction using clinical feature subsets, including
the top 10 SHAP-ranked features. To reduce disparities in true positive rates
across racial and ethnic subgroups, we applied an equal opportunity
postprocessing method.
  Results: This study included 59,492 patients in the training data, 24,198 in
the validating data, and 25,188 in the testing data. The LASSO logistic
regression model with the top 10 features was selected for its interpretability
and comparable performance. Before fairness adjustment, the model achieved
AUROC of 0.84, accuracy of 78%, sensitivity of 72%, specificity of 79%, and
F1-score of 0.617. After equal opportunity postprocessing, accuracy modestly
increased to 81% and specificity to 94%, while sensitivity decreased to 41% and
F1-score to 0.515, reflecting the fairness trade-off.
  Conclusions: We developed the MASER prediction model (MASLD Static EHR Risk
Prediction), a LASSO logistic regression model which achieved competitive
performance for MASLD prediction (AUROC 0.836, accuracy 77.6%), comparable to
previously reported ensemble and tree-based models. Overall, this approach
demonstrates that interpretable models can achieve a balance of predictive
performance and fairness in diverse patient populations.

</details>


### [257] [AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals](https://arxiv.org/abs/2510.22301)
*Yujie Xiao,Gongzhen Tang,Wenhui Liu,Jun Li,Guangkun Nie,Zhuoran Kan,Deyun Zhang,Qinghao Zhao,Shenda Hong*

Main category: cs.LG

TL;DR: 本文利用迁移学习微调ECGFounder模型，在MC - MED数据集上实现对实验室指标的预测，证明了实时无创估计实验室值的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有实验室值获取方法依赖侵入性静脉采样且有延迟，现有基于ECG的深度学习模型存在信噪比低、个体差异大、数据多样性有限和泛化性差等问题。

Method: 利用迁移学习在斯坦福的MC - MED数据集上微调大规模预训练的ECG基础模型ECGFounder，生成超2000万个标准化十秒ECG片段。

Result: 内部验证中，模型对33个实验室指标预测表现强（AUC>0.65），59个指标表现中等（0.55 - 0.65），16个指标表现有限（<0.55）。

Conclusion: 本研究提供了高效的人工智能驱动解决方案，确立了实时无创估计实验室值的可行性范围。

Abstract: Timely access to laboratory values is critical for clinical decision-making,
yet current approaches rely on invasive venous sampling and are intrinsically
delayed. Electrocardiography (ECG), as a non-invasive and widely available
signal, offers a promising modality for rapid laboratory estimation. Recent
progress in deep learning has enabled the extraction of latent hematological
signatures from ECGs. However, existing models are constrained by low
signal-to-noise ratios, substantial inter-individual variability, limited data
diversity, and suboptimal generalization, especially when adapted to low-lead
wearable devices. In this work, we conduct an exploratory study leveraging
transfer learning to fine-tune ECGFounder, a large-scale pre-trained ECG
foundation model, on the Multimodal Clinical Monitoring in the Emergency
Department (MC-MED) dataset from Stanford. We generated a corpus of more than
20 million standardized ten-second ECG segments to enhance sensitivity to
subtle biochemical correlates. On internal validation, the model demonstrated
strong predictive performance (area under the curve above 0.65) for
thirty-three laboratory indicators, moderate performance (between 0.55 and
0.65) for fifty-nine indicators, and limited performance (below 0.55) for
sixteen indicators. This study provides an efficient artificial-intelligence
driven solution and establishes the feasibility scope for real-time,
non-invasive estimation of laboratory values.

</details>


### [258] [LacMaterial: Large Language Models as Analogical Chemists for Materials Discovery](https://arxiv.org/abs/2510.22312)
*Hongyu Guo*

Main category: cs.LG

TL;DR: 研究表明大语言模型可通过类比推理生成新型电池材料，表现优于标准提示基线。


<details>
  <summary>Details</summary>
Motivation: 人类类比推理受领域专业知识和表面偏见限制，大语言模型在科学类比推理方面有潜力但未充分探索。

Method: 一是检索跨领域类比和类比引导示例以拓展探索，二是从少量标记示例构建领域内类比模板以指导针对性开发。

Result: 明确的类比推理策略产生了既定成分空间之外的候选材料，且表现优于标准提示基线。

Conclusion: 大语言模型可作为可解释、类似专家的假设生成器，利用类比驱动的泛化推动科学创新。

Abstract: Analogical reasoning, the transfer of relational structures across contexts
(e.g., planet is to sun as electron is to nucleus), is fundamental to
scientific discovery. Yet human insight is often constrained by domain
expertise and surface-level biases, limiting access to deeper, structure-driven
analogies both within and across disciplines. Large language models (LLMs),
trained on vast cross-domain data, present a promising yet underexplored tool
for analogical reasoning in science. Here, we demonstrate that LLMs can
generate novel battery materials by (1) retrieving cross-domain analogs and
analogy-guided exemplars to steer exploration beyond conventional dopant
substitutions, and (2) constructing in-domain analogical templates from few
labeled examples to guide targeted exploitation. These explicit analogical
reasoning strategies yield candidates outside established compositional spaces
and outperform standard prompting baselines. Our findings position LLMs as
interpretable, expert-like hypothesis generators that leverage analogy-driven
generalization for scientific innovation.

</details>


### [259] [Mapping Faithful Reasoning in Language Models](https://arxiv.org/abs/2510.22362)
*Jiazheng Li,Andreas Damianou,J Rosser,José Luis Redondo García,Konstantina Palla*

Main category: cs.LG

TL;DR: 本文提出Concept Walk框架用于追踪模型推理时内部状态变化，以Qwen 3 - 4B在安全领域为例，发现不同难度推理中思维链的不同表现，该框架有助于判断推理轨迹的可信度。


<details>
  <summary>Details</summary>
Motivation: 思维链轨迹不一定能真实反映模型内部计算，这给监督带来挑战，需方法判断其是否可信。

Method: 引入Concept Walk框架，在激活空间操作，将推理步骤投影到从对比数据中学到的概念方向上。

Result: 在简单案例中，受干扰的思维链很快被忽略，表明是装饰性推理；在困难案例中，扰动会导致内部激活持续变化，符合忠实推理。

Conclusion: Concept Walk框架可通过特定概念的内部动态重新审视推理的忠实性，帮助识别推理轨迹何时可信、何时会误导从业者。

Abstract: Chain-of-thought (CoT) traces promise transparency for reasoning language
models, but prior work shows they are not always faithful reflections of
internal computation. This raises challenges for oversight: practitioners may
misinterpret decorative reasoning as genuine. We introduce Concept Walk, a
general framework for tracing how a model's internal stance evolves with
respect to a concept direction during reasoning. Unlike surface text, Concept
Walk operates in activation space, projecting each reasoning step onto the
concept direction learned from contrastive data. This allows us to observe
whether reasoning traces shape outcomes or are discarded. As a case study, we
apply Concept Walk to the domain of Safety using Qwen 3-4B. We find that in
'easy' cases, perturbed CoTs are quickly ignored, indicating decorative
reasoning, whereas in 'hard' cases, perturbations induce sustained shifts in
internal activations, consistent with faithful reasoning. The contribution is
methodological: Concept Walk provides a lens to re-examine faithfulness through
concept-specific internal dynamics, helping identify when reasoning traces can
be trusted and when they risk misleading practitioners.

</details>


### [260] [Label Smoothing Improves Gradient Ascent in LLM Unlearning](https://arxiv.org/abs/2510.22376)
*Zirui Pang,Hao Zheng,Zhijie Deng,Ling Li,Zixin Zhong,Jiaheng Wei*

Main category: cs.LG

TL;DR: 提出平滑梯度上升（SGA）方法解决梯度上升（GA）在大语言模型遗忘学习中的不稳定性问题，实验显示SGA表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有梯度上升（GA）方法在大语言模型遗忘学习中存在严重不稳定性，导致模型效用大幅下降，需改进。

Method: 提出平滑梯度上升（SGA）方法，将遗忘数据与多个构造的正常数据通过可调平滑率结合，还给出最优平滑率选择的理论指导。

Result: 在三个基准测试中，SGA在所有指标上始终优于原始GA方法，在几个关键指标上在所有基线方法中达到前二性能。

Conclusion: SGA能实现更稳定的遗忘学习，同时更好地保留模型效用。

Abstract: LLM unlearning has emerged as a promising approach, aiming to enable models
to forget hazardous/undesired knowledge at low cost while preserving as much
model utility as possible. Among existing techniques, the most straightforward
method is performing Gradient Ascent (GA) w.r.t. the forget data, thereby
forcing the model to unlearn the forget dataset. However, GA suffers from
severe instability, as it drives updates in a divergent direction, often
resulting in drastically degraded model utility. To address this issue, we
propose Smoothed Gradient Ascent (SGA). SGA combines the forget data with
multiple constructed normal data through a tunable smoothing rate. Intuitively,
this extends GA from learning solely on the forget data to jointly learning
across both forget and normal data, enabling more stable unlearning while
better preserving model utility. Theoretically, we provide the theoretical
guidance on the selection of the optimal smoothing rate. Empirically, we
evaluate SGA on three benchmarks: TOFU, Harry Potter, and MUSE-NEWS.
Experimental results demonstrate that SGA consistently outperforms the original
Gradient Ascent (GA) method across all metrics and achieves top-2 performance
among all baseline methods on several key metrics.

</details>


### [261] [Dynamic Dropout: Leveraging Conway's Game of Life for Neural Networks Regularization](https://arxiv.org/abs/2510.22383)
*David Freire-Obregón,José Salas-Cáceres,Modesto Castrillón-Santana*

Main category: cs.LG

TL;DR: 本文提出用Conway's Game of Life替代Dropout进行正则化，在CIFAR - 10数据集上验证有效性，还探讨其在深层架构适用性。


<details>
  <summary>Details</summary>
Motivation: Dropout存在静态性和缺乏可解释性的局限，需新的正则化方法。

Method: 将神经网络单元表示为GoL网格中的细胞，应用GoL规则在训练中动态停用单元。

Result: 在CIFAR - 10数据集上，基于GoL的动态单元停用与传统Dropout性能相当，且可通过可视化模式洞察网络行为。

Conclusion: 提出的方法有效，在深层架构中能提升不同Dropout技术的性能。

Abstract: Regularization techniques play a crucial role in preventing overfitting and
improving the generalization performance of neural networks. Dropout, a widely
used regularization technique, randomly deactivates units during training to
introduce redundancy and prevent co-adaptation among neurons. Despite its
effectiveness, dropout has limitations, such as its static nature and lack of
interpretability. In this paper, we propose a novel approach to regularization
by substituting dropout with Conway's Game of Life (GoL), a cellular automata
with simple rules that govern the evolution of a grid of cells. We introduce
dynamic unit deactivation during training by representing neural network units
as cells in a GoL grid and applying the game's rules to deactivate units. This
approach allows for the emergence of spatial patterns that adapt to the
training data, potentially enhancing the network's ability to generalize. We
demonstrate the effectiveness of our approach on the CIFAR-10 dataset, showing
that dynamic unit deactivation using GoL achieves comparable performance to
traditional dropout techniques while offering insights into the network's
behavior through the visualization of evolving patterns. Furthermore, our
discussion highlights the applicability of our proposal in deeper
architectures, demonstrating how it enhances the performance of different
dropout techniques.

</details>


### [262] [Knowledge-guided Continual Learning for Behavioral Analytics Systems](https://arxiv.org/abs/2510.22405)
*Yasas Senarath,Hemant Purohit*

Main category: cs.LG

TL;DR: 在线平台用户行为变化致模型性能因数据漂移下降，微调会导致灾难性遗忘，提出基于增强的方法结合外部知识到基于重放的持续学习框架，评估表明增强有助于超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决在线平台用户行为分析模型因数据漂移性能下降，以及微调导致的灾难性遗忘问题。

Method: 提出一种基于增强的方法，将外部知识融入基于重放的持续学习框架，并评估多种策略。

Result: 使用三个与异常行为分类相关的数据集评估，证明增强方法能超越基线重放方法。

Conclusion: 基于增强的方法可有效将外部知识融入持续学习框架，提升模型性能。

Abstract: User behavior on online platforms is evolving, reflecting real-world changes
in how people post, whether it's helpful messages or hate speech. Models that
learn to capture this content can experience a decrease in performance over
time due to data drift, which can lead to ineffective behavioral analytics
systems. However, fine-tuning such a model over time with new data can be
detrimental due to catastrophic forgetting. Replay-based approaches in
continual learning offer a simple yet efficient method to update such models,
minimizing forgetting by maintaining a buffer of important training instances
from past learned tasks. However, the main limitation of this approach is the
fixed size of the buffer. External knowledge bases can be utilized to overcome
this limitation through data augmentation. We propose a novel
augmentation-based approach to incorporate external knowledge in the
replay-based continual learning framework. We evaluate several strategies with
three datasets from prior studies related to deviant behavior classification to
assess the integration of external knowledge in continual learning and
demonstrate that augmentation helps outperform baseline replay-based
approaches.

</details>


### [263] [SmartMixed: A Two-Phase Training Strategy for Adaptive Activation Function Learning in Neural Networks](https://arxiv.org/abs/2510.22450)
*Amin Omidvar*

Main category: cs.LG

TL;DR: 提出SmartMixed训练策略让网络学习每个神经元的最优激活函数，在MNIST数据集评估有新发现


<details>
  <summary>Details</summary>
Motivation: 大多数神经网络架构使用固定统一的激活函数，提出可学习每个神经元最优激活函数且保持推理计算效率的策略

Method: 两阶段训练策略SmartMixed，第一阶段用可微硬混合机制从候选激活函数池选择，第二阶段固定所选激活函数

Result: 在MNIST数据集用不同深度前馈神经网络评估，发现不同层神经元对激活函数有不同偏好

Conclusion: 该策略可让网络学习每个神经元最优激活函数，揭示了神经网络架构内的功能多样性

Abstract: The choice of activation function plays a critical role in neural networks,
yet most architectures still rely on fixed, uniform activation functions across
all neurons. We introduce SmartMixed, a two-phase training strategy that allows
networks to learn optimal per-neuron activation functions while preserving
computational efficiency at inference. In the first phase, neurons adaptively
select from a pool of candidate activation functions (ReLU, Sigmoid, Tanh,
Leaky ReLU, ELU, SELU) using a differentiable hard-mixture mechanism. In the
second phase, each neuron's activation function is fixed according to the
learned selection, resulting in a computationally efficient network that
supports continued training with optimized vectorized operations. We evaluate
SmartMixed on the MNIST dataset using feedforward neural networks of varying
depths. The analysis shows that neurons in different layers exhibit distinct
preferences for activation functions, providing insights into the functional
diversity within neural architectures.

</details>


### [264] [GraphTOP: Graph Topology-Oriented Prompting for Graph Neural Networks](https://arxiv.org/abs/2510.22451)
*Xingbo Fu,Zhenyu Lei,Zihan Chen,Binchi Zhang,Chuxu Zhang,Jundong Li*

Main category: cs.LG

TL;DR: 本文研究图拓扑的图提示方法，提出GraphTOP框架，在多个数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有图提示研究多关注特征导向方法，忽视拓扑导向提示潜力，性能欠佳。

Method: 将拓扑导向提示重新表述为多跳局部子图内的边重连问题，通过重参数化将其放宽到连续概率空间。

Result: 在五种图数据集和四种预训练策略下的实验表明，GraphTOP在多个节点分类数据集上优于六个基线。

Conclusion: 提出的GraphTOP框架能有效适配预训练GNN模型用于下游任务。

Abstract: Graph Neural Networks (GNNs) have revolutionized the field of graph learning
by learning expressive graph representations from massive graph data. As a
common pattern to train powerful GNNs, the "pre-training, adaptation" scheme
first pre-trains GNNs over unlabeled graph data and subsequently adapts them to
specific downstream tasks. In the adaptation phase, graph prompting is an
effective strategy that modifies input graph data with learnable prompts while
keeping pre-trained GNN models frozen. Typically, existing graph prompting
studies mainly focus on *feature-oriented* methods that apply graph prompts to
node features or hidden representations. However, these studies often achieve
suboptimal performance, as they consistently overlook the potential of
*topology-oriented* prompting, which adapts pre-trained GNNs by modifying the
graph topology. In this study, we conduct a pioneering investigation of graph
prompting in terms of graph topology. We propose the first **Graph**
**T**opology-**O**riented **P**rompting (GraphTOP) framework to effectively
adapt pre-trained GNN models for downstream tasks. More specifically, we
reformulate topology-oriented prompting as an edge rewiring problem within
multi-hop local subgraphs and relax it into the continuous probability space
through reparameterization while ensuring tight relaxation and preserving graph
sparsity. Extensive experiments on five graph datasets under four pre-training
strategies demonstrate that our proposed GraphTOP outshines six baselines on
multiple node classification datasets. Our code is available at
https://github.com/xbfu/GraphTOP.

</details>


### [265] [Backward-Friendly Optimization: Training Large Language Models with Approximate Gradients under Memory Constraints](https://arxiv.org/abs/2510.22467)
*Jing Yang,Kaitong Cai,Yijia Fan,Yufeng Yang,Keze Wang*

Main category: cs.LG

TL;DR: 提出GradLite优化器，通过低秩雅可比近似和误差反馈校正技术，减少优化器状态和激活内存消耗，且性能表现良好。


<details>
  <summary>Details</summary>
Motivation: 全量微调大语言模型内存开销大，现有解决方案未触及优化器本身。

Method: 引入GradLite优化器，采用低秩雅可比近似和误差反馈校正两种关键技术。

Result: GradLite可减少高达50%的优化器状态和激活内存消耗，在多个基准测试中表现与基线方法相当或更优。

Conclusion: GradLite能在不改变架构的情况下高效训练，保持无偏梯度估计和有界方差，收敛速度与Adam相当。

Abstract: Full fine-tuning of Large Language Models (LLMs) is notoriously
memory-intensive, primarily because conventional optimizers such as SGD or Adam
assume access to exact gradients derived from cached activations. Existing
solutions either alter the model architecture (e.g., reversible networks) or
trade memory for computation (e.g., activation checkpointing), but the
optimizer itself remains untouched. In this work, we introduce GradLite, a
backward-friendly optimizer that relaxes the requirement of exact gradients,
enabling efficient training even when intermediate activations are aggressively
discarded or approximated. GradLite leverages two key techniques: (i) low-rank
Jacobian approximation, which reduces the dimensionality of backpropagated
error signals, and (ii) error-feedback correction, which accumulates and
compensates approximation errors across iterations to preserve convergence
guarantees. We provide a theoretical analysis showing that GradLite maintains
unbiased gradient estimates with bounded variance, ensuring convergence rates
comparable to Adam. Empirically, GradLite reduces optimizer-state and
activation memory consumption by up to 50\% without architectural changes, and
achieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),
multilingual, and dialogue benchmarks compared to checkpointing and
optimizer-centric baselines (LoMo, GaLore).

</details>


### [266] [Contextual Tokenization for Graph Inverted Indices](https://arxiv.org/abs/2510.22479)
*Pritish Chakraborty,Indradyumna Roy,Soumen Chakrabarti,Abir De*

Main category: cs.LG

TL;DR: 提出图索引框架CORGII，可在图检索中实现更好的准确率与效率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有多向量图表示在图检索中需对语料库图进行详尽评分，应用受限。

Method: 引入CORGII，通过可微离散化模块计算稀疏二进制码，使用数据驱动的可训练影响权重，探索令牌扩展。

Result: 广泛实验表明，与多个基线相比，CORGII在准确率和效率之间提供了更好的权衡。

Conclusion: CORGII是首个使用离散令牌映射到高效倒排列表的密集图表示索引器。

Abstract: Retrieving graphs from a large corpus, that contain a subgraph isomorphic to
a given query graph, is a core operation in many real-world applications. While
recent multi-vector graph representations and scores based on set alignment and
containment can provide accurate subgraph isomorphism tests, their use in
retrieval remains limited by their need to score corpus graphs exhaustively. We
introduce CORGII (Contextual Representation of Graphs for Inverted Indexing), a
graph indexing framework in which, starting with a contextual dense graph
representation, a differentiable discretization module computes sparse binary
codes over a learned latent vocabulary. This text document-like representation
allows us to leverage classic, highly optimized inverted indices, while
supporting soft (vector) set containment scores. Pushing this paradigm further,
we replace the classical, fixed impact weight of a `token' on a graph (such as
TFIDF or BM25) with a data-driven, trainable impact weight. Finally, we explore
token expansion to support multi-probing the index for smoother
accuracy-efficiency tradeoffs. To our knowledge, CORGII is the first indexer of
dense graph representations using discrete tokens mapping to efficient inverted
lists. Extensive experiments show that CORGII provides better trade-offs
between accuracy and efficiency, compared to several baselines.

</details>


### [267] [Scalable Oversight via Partitioned Human Supervision](https://arxiv.org/abs/2510.22500)
*Ren Yin,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 论文提出可扩展监督框架，利用人类提供的互补标签评估前沿AI系统，无需真实标签，还展示用此弱信号训练AI系统的效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在多领域超越人类专家，获取高质量人类监督用于评估和训练愈发困难，且人类专家知识局限。

Method: 提出可扩展监督框架，推导基于互补标签的top - 1准确率无偏估计量，引入结合普通标签和互补标签的估计量，并给出有限样本偏差保证。

Result: 实证表明，有互补标签时可在无真实标签情况下评估大语言模型输出，还能用此弱信号训练AI系统使其表现更好。

Conclusion: 所提框架能有效利用人类的弱信号评估和训练前沿AI系统，代码已开源。

Abstract: As artificial intelligence (AI) systems approach and surpass expert human
performance across a broad range of tasks, obtaining high-quality human
supervision for evaluation and training becomes increasingly challenging. Our
focus is on tasks that require deep knowledge and skills of multiple domains.
Unfortunately, even the best human experts are knowledgeable only in a single
narrow area, and will not be able to evaluate the correctness of advanced AI
systems on such superhuman tasks. However, based on their narrow expertise,
humans may provide a weak signal, i.e., a complementary label indicating an
option that is incorrect. For example, a cardiologist could state that "this is
not related to cardiology,'' even if they cannot identify the true disease.
Based on this weak signal, we propose a scalable oversight framework that
enables us to evaluate frontier AI systems without the need to prepare the
ground truth. We derive an unbiased estimator of top-1 accuracy from
complementary labels and quantify how many complementary labels are needed to
match the variance of ordinary labels. We further introduce two estimators to
combine scarce ordinary labels with abundant complementary labels. We provide
finite-sample deviation guarantees for both complementary-only and the mixed
estimators. Empirically, we show that we can evaluate the output of large
language models without the ground truth, if we have complementary labels. We
further show that we can train an AI system with such weak signals: we show how
we can design an agentic AI system automatically that can perform better with
this partitioned human supervision. Our code is available at
https://github.com/R-Yin-217/Scalable-Oversight-via-Human-Partitioned-Supervision.

</details>


### [268] [Transitive RL: Value Learning via Divide and Conquer](https://arxiv.org/abs/2510.22512)
*Seohong Park,Aditya Oberai,Pranav Atreya,Sergey Levine*

Main category: cs.LG

TL;DR: 提出基于分治范式的新值学习算法TRL，用于离线目标条件强化学习，比其他方法有优势，实验表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决离线目标条件强化学习中找到最小步数到达任意状态策略的问题，改进现有值学习范式。

Method: 将目标条件强化学习中的三角不等式结构转化为分治值更新规则。

Result: 与先前离线目标条件强化学习算法相比，TRL在高挑战性、长视野基准任务中表现最佳。

Conclusion: TRL是一种有效的离线目标条件强化学习值学习算法，在处理长轨迹时比TD和蒙特卡罗方法更具优势。

Abstract: In this work, we present Transitive Reinforcement Learning (TRL), a new value
learning algorithm based on a divide-and-conquer paradigm. TRL is designed for
offline goal-conditioned reinforcement learning (GCRL) problems, where the aim
is to find a policy that can reach any state from any other state in the
smallest number of steps. TRL converts a triangle inequality structure present
in GCRL into a practical divide-and-conquer value update rule. This has several
advantages compared to alternative value learning paradigms. Compared to
temporal difference (TD) methods, TRL suffers less from bias accumulation, as
in principle it only requires $O(\log T)$ recursions (as opposed to $O(T)$ in
TD learning) to handle a length-$T$ trajectory. Unlike Monte Carlo methods, TRL
suffers less from high variance as it performs dynamic programming.
Experimentally, we show that TRL achieves the best performance in highly
challenging, long-horizon benchmark tasks compared to previous offline GCRL
algorithms.

</details>


### [269] [Toward Robust Signed Graph Learning through Joint Input-Target Denoising](https://arxiv.org/abs/2510.22513)
*Junran Wu,Beng Chin Ooi,Ke Xu*

Main category: cs.LG

TL;DR: 提出RIDGE框架用于鲁棒有符号图学习，在多个数据集验证其能提升流行SGNN模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 真实世界连接有噪声，SGNN鲁棒性成关键研究领域，现有带理论指导的鲁棒SGNN研究少。

Method: 受图信息瓶颈启发，提出RIDGE框架，扩展GIB理论，通过重参数化机制和变分近似产生目标函数来清理输入数据和监督目标。

Result: 在四个流行有符号图数据集上验证，RIDGE能在不同噪声水平下提升流行SGNN模型的鲁棒性。

Conclusion: RIDGE框架有效提升了SGNN模型在有噪声情况下的鲁棒性。

Abstract: Signed Graph Neural Networks (SGNNs) are widely adopted to analyze complex
patterns in signed graphs with both positive and negative links. Given the
noisy nature of real-world connections, the robustness of SGNN has also emerged
as a pivotal research area. Under the supervision of empirical properties,
graph structure learning has shown its robustness on signed graph
representation learning, however, there remains a paucity of research
investigating a robust SGNN with theoretical guidance. Inspired by the success
of graph information bottleneck (GIB) in information extraction, we propose
RIDGE, a novel framework for Robust sI gned graph learning through joint
Denoising of Graph inputs and supervision targEts. Different from the basic
GIB, we extend the GIB theory with the capability of target space denoising as
the co-existence of noise in both input and target spaces. In instantiation,
RIDGE effectively cleanses input data and supervision targets via a tractable
objective function produced by reparameterization mechanism and variational
approximation. We extensively validate our method on four prevalent signed
graph datasets, and the results show that RIDGE clearly improves the robustness
of popular SGNN models under various levels of noise.

</details>


### [270] [A Scalable Global Optimization Algorithm For Constrained Clustering](https://arxiv.org/abs/2510.22519)
*Pedro Chumpitaz-Flores,My Duong,Cristobal Heredia,Kaixun Hua*

Main category: cs.LG

TL;DR: 提出SDC - GBB框架解决约束聚类难题，可处理大规模数据集，有全局最优保证。


<details>
  <summary>Details</summary>
Motivation: 现有混合整数优化方法只能处理小规模数据集，将成对约束融入聚类是NP难问题，需解决大规模数据集的约束聚类问题。

Method: 提出Sample - Driven Constrained Group - Based Branch - and - Bound (SDC - GBB)框架，将必须链接样本合并为基于质心的伪样本，通过几何规则剪枝不能链接，结合分组样本拉格朗日分解和几何消除规则获取上下界，利用并行实现可扩展的成对k均值约束聚类。

Result: 能处理有200,000个不能链接约束样本和1,500,000个必须链接约束样本的数据集，比现有技术处理规模大200 - 1500倍，最优差距小于3%。

Conclusion: 该方法有确定性全局保证，避免了现成启发式方法在大数据集上的搜索失败问题。

Abstract: Constrained clustering leverages limited domain knowledge to improve
clustering performance and interpretability, but incorporating pairwise
must-link and cannot-link constraints is an NP-hard challenge, making global
optimization intractable. Existing mixed-integer optimization methods are
confined to small-scale datasets, limiting their utility. We propose
Sample-Driven Constrained Group-Based Branch-and-Bound (SDC-GBB), a
decomposable branch-and-bound (BB) framework that collapses must-linked samples
into centroid-based pseudo-samples and prunes cannot-link through geometric
rules, while preserving convergence and guaranteeing global optimality. By
integrating grouped-sample Lagrangian decomposition and geometric elimination
rules for efficient lower and upper bounds, the algorithm attains highly
scalable pairwise k-Means constrained clustering via parallelism. Experimental
results show that our approach handles datasets with 200,000 samples with
cannot-link constraints and 1,500,000 samples with must-link constraints, which
is 200 - 1500 times larger than the current state-of-the-art under comparable
constraint settings, while reaching an optimality gap of less than 3%. In
providing deterministic global guarantees, our method also avoids the search
failures that off-the-shelf heuristics often encounter on large datasets.

</details>


### [271] [Random Search Neural Networks for Efficient and Expressive Graph Learning](https://arxiv.org/abs/2510.22520)
*Michael Ito,Danai Koutra,Jenna Wiens*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Random walk neural networks (RWNNs) have emerged as a promising approach for
graph representation learning, leveraging recent advances in sequence models to
process random walks. However, under realistic sampling constraints, RWNNs
often fail to capture global structure even in small graphs due to incomplete
node and edge coverage, limiting their expressivity. To address this, we
propose \textit{random search neural networks} (RSNNs), which operate on random
searches, each of which guarantees full node coverage. Theoretically, we
demonstrate that in sparse graphs, only $O(\log |V|)$ searches are needed to
achieve full edge coverage, substantially reducing sampling complexity compared
to the $O(|V|)$ walks required by RWNNs (assuming walk lengths scale with graph
size). Furthermore, when paired with universal sequence models, RSNNs are
universal approximators. We lastly show RSNNs are probabilistically invariant
to graph isomorphisms, ensuring their expectation is an isomorphism-invariant
graph function. Empirically, RSNNs consistently outperform RWNNs on molecular
and protein benchmarks, achieving comparable or superior performance with up to
16$\times$ fewer sampled sequences. Our work bridges theoretical and practical
advances in random walk based approaches, offering an efficient and expressive
framework for learning on sparse graphs.

</details>


### [272] [Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval](https://arxiv.org/abs/2510.22538)
*Ashwin Ramachandran,Vaibhav Raj,Indrayumna Roy,Soumen Chakrabarti,Abir De*

Main category: cs.LG

TL;DR: 本文提出早期交互图神经网络IsoNet++，经多轮更新对齐，在多数据集实验中检索性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于子图同构的图检索有诸多应用，现有IsoNet为后期交互模型，需提出新技术创新的早期交互模型。

Method: 1. 基于节点间单射对齐在两个输入图内和跨图传递消息计算节点嵌入；2. 多轮惰性更新对齐，每轮基于当前对齐状态从头运行分层GNN并更新；3. 引入节点对伙伴交互概念。

Result: 实验表明对齐在多轮中逐步优化，检索性能显著优于现有方法，三项创新均提升了准确性。

Conclusion: IsoNet++在图检索任务上表现出色，代码和数据集公开。

Abstract: Graph retrieval based on subgraph isomorphism has several real-world
applications such as scene graph retrieval, molecular fingerprint detection and
circuit design. Roy et al. [35] proposed IsoNet, a late interaction model for
subgraph matching, which first computes the node and edge embeddings of each
graph independently of paired graph and then computes a trainable alignment
map. Here, we present IsoNet++, an early interaction graph neural network
(GNN), based on several technical innovations. First, we compute embeddings of
all nodes by passing messages within and across the two input graphs, guided by
an injective alignment between their nodes. Second, we update this alignment in
a lazy fashion over multiple rounds. Within each round, we run a layerwise GNN
from scratch, based on the current state of the alignment. After the completion
of one round of GNN, we use the last-layer embeddings to update the alignments,
and proceed to the next round. Third, IsoNet++ incorporates a novel notion of
node-pair partner interaction. Traditional early interaction computes attention
between a node and its potential partners in the other graph, the attention
then controlling messages passed across graphs. In contrast, we consider node
pairs (not single nodes) as potential partners. Existence of an edge between
the nodes in one graph and non-existence in the other provide vital signals for
refining the alignment. Our experiments on several datasets show that the
alignments get progressively refined with successive rounds, resulting in
significantly better retrieval performance than existing methods. We
demonstrate that all three innovations contribute to the enhanced accuracy. Our
code and datasets are publicly available at
https://github.com/structlearning/isonetpp.

</details>


### [273] [FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning](https://arxiv.org/abs/2510.22543)
*Yuyang Ding,Chi Zhang,Juntao Li,Haibin Lin,Xin Liu,Min Zhang*

Main category: cs.LG

TL;DR: 本文研究强化学习中存在的有缺陷正样本问题，提出FAPO方法，实验证明其在多领域有效。


<details>
  <summary>Details</summary>
Motivation: 强化学习中存在有缺陷正样本，会让策略模型内化不可靠推理模式，影响推理能力。

Method: 提出FAPO方法，对有缺陷正样本进行无参数奖励惩罚；引入GenRM精确检测有缺陷正样本。

Result: FAPO在广泛领域有效，提高结果正确性、过程可靠性和训练稳定性，且不增加token预算。

Conclusion: FAPO方法能让策略在前期利用有缺陷正样本获得稳定提升，后期转向可靠推理。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
promising paradigm for enhancing the reasoning capabilities of large language
models (LLMs). In this context, models explore reasoning trajectories and
exploit rollouts with correct answers as positive signals for policy
optimization. However, these rollouts might involve flawed patterns such as
answer-guessing and jump-in-reasoning. Such flawed-positive rollouts are
rewarded identically to fully correct ones, causing policy models to
internalize these unreliable reasoning patterns. In this work, we first conduct
a systematic study of flawed-positive rollouts in RL and find that they enable
rapid capability gains during the early optimization stage, while constraining
reasoning capability later by reinforcing unreliable patterns. Building on
these insights, we propose Flawed-Aware Policy Optimization (FAPO), which
presents a parameter-free reward penalty for flawed-positive rollouts, enabling
the policy to leverage them as useful shortcuts in the warm-up stage, securing
stable early gains, while gradually shifting optimization toward reliable
reasoning in the later refinement stage. To accurately and comprehensively
detect flawed-positive rollouts, we introduce a generative reward model (GenRM)
with a process-level reward that precisely localizes reasoning errors.
Experiments show that FAPO is effective in broad domains, improving outcome
correctness, process reliability, and training stability without increasing the
token budget.

</details>


### [274] [DDTR: Diffusion Denoising Trace Recovery](https://arxiv.org/abs/2510.22553)
*Maximilian Matyash,Avigdor Gal,Arik Senderovich*

Main category: cs.LG

TL;DR: 本文提出基于DDPM的深度学习方法用于随机轨迹恢复，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着技术发展，需处理含概率信息的随机日志，因此对随机轨迹恢复需求增加。

Method: 设计基于扩散去噪概率模型（DDPM）的深度学习方法，利用过程知识通过去噪恢复轨迹。

Result: 实证评估表明，性能达到了当前最优水平，比现有方法最多提高25%，且在高噪声水平下鲁棒性增强。

Conclusion: 所设计的基于DDPM的方法在随机轨迹恢复上表现良好，优于现有方法。

Abstract: With recent technological advances, process logs, which were traditionally
deterministic in nature, are being captured from non-deterministic sources,
such as uncertain sensors or machine learning models (that predict activities
using cameras). In the presence of stochastically-known logs, logs that contain
probabilistic information, the need for stochastic trace recovery increases, to
offer reliable means of understanding the processes that govern such systems.
We design a novel deep learning approach for stochastic trace recovery, based
on Diffusion Denoising Probabilistic Models (DDPM), which makes use of process
knowledge (either implicitly by discovering a model or explicitly by injecting
process knowledge in the training phase) to recover traces by denoising. We
conduct an empirical evaluation demonstrating state-of-the-art performance with
up to a 25% improvement over existing methods, along with increased robustness
under high noise levels.

</details>


### [275] [Combining Deep Learning and Explainable AI for Toxicity Prediction of Chemical Compounds](https://arxiv.org/abs/2510.22572)
*Eduard Popescu,Adrian Groza,Andreea Cernat*

Main category: cs.LG

TL;DR: 基于Tox21数据集预测化合物毒理活性，引入基于DenseNet121的图像管道，结合Grad - CAM可视化，结果有竞争力，强调图像表示与可解释AI方法结合的价值。


<details>
  <summary>Details</summary>
Motivation: 在计算毒理学中，基于Tox21数据集预测化合物的毒理活性。

Method: 引入基于DenseNet121的图像管道处理化学结构2D图形表示，使用Grad - CAM可视化技术解释模型预测。

Result: 提出的架构与传统模型相比取得有竞争力的结果。

Conclusion: 结合图像表示与可解释AI方法能提高毒理学中预测准确性和模型透明度。

Abstract: The task here is to predict the toxicological activity of chemical compounds
based on the Tox21 dataset, a benchmark in computational toxicology.
  After a domain-specific overview of chemical toxicity, we discuss current
computational strategies, focusing on machine learning and deep learning.
Several architectures are compared in terms of performance, robustness, and
interpretability.
  This research introduces a novel image-based pipeline based on DenseNet121,
which processes 2D graphical representations of chemical structures.
Additionally, we employ Grad-CAM visualizations, an explainable AI technique,
to interpret the model's predictions and highlight molecular regions
contributing to toxicity classification. The proposed architecture achieves
competitive results compared to traditional models, demonstrating the potential
of deep convolutional networks in cheminformatics. Our findings emphasize the
value of combining image-based representations with explainable AI methods to
improve both predictive accuracy and model transparency in toxicology.

</details>


### [276] [Optimal Anytime Algorithms for Online Convex Optimization with Adversarial Constraints](https://arxiv.org/abs/2510.22579)
*Dhruv Sarkar,Abhishek Sinha*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose an anytime online algorithm for the problem of learning a sequence
of adversarial convex cost functions while approximately satisfying another
sequence of adversarial online convex constraints. A sequential algorithm is
called \emph{anytime} if it provides a non-trivial performance guarantee for
any intermediate timestep $t$ without requiring prior knowledge of the length
of the entire time horizon $T$. Our proposed algorithm achieves optimal
performance bounds without resorting to the standard doubling trick, which has
poor practical performance due to multiple restarts. Our core technical
contribution is the use of time-varying Lyapunov functions to keep track of
constraint violations. This must be contrasted with prior works that used a
fixed Lyapunov function tuned to the known horizon length $T$. The use of
time-varying Lyapunov function poses unique analytical challenges as
properties, such as \emph{monotonicity}, on which the prior proofs rest, no
longer hold. By introducing a new analytical technique, we show that our
algorithm achieves $O(\sqrt{t})$ regret and $\tilde{O}(\sqrt{t})$ cumulative
constraint violation bounds for any $t\geq 1$.
  We extend our results to the dynamic regret setting, achieving bounds that
adapt to the path length of the comparator sequence without prior knowledge of
its total length. We also present an adaptive algorithm in the optimistic
setting, whose performance gracefully scales with the cumulative prediction
error. We demonstrate the practical utility of our algorithm through numerical
experiments involving the online shortest path problem.

</details>


### [277] [Prediction-Powered Semi-Supervised Learning with Online Power Tuning](https://arxiv.org/abs/2510.22586)
*Noa Shoham,Ron Dorfman,Shalev Shaer,Kfir Y. Levy,Yaniv Romano*

Main category: cs.LG

TL;DR: 本文将Prediction - Powered Inference核心思想拓展到半监督学习，引入无偏梯度估计器，用在线学习算法调整参数，实验证明性能优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 解决半监督学习中无标签数据伪标签质量影响模型性能，不准确伪标签会引入偏差的问题。

Method: 将PPI核心思想拓展到半监督学习，引入无偏梯度估计器，利用插值参数，用一维在线学习算法动态调整插值参数和模型参数。

Result: 在合成和真实数据集实验中，相比经典半监督学习基线和离线调整插值参数的PPI方法，性能得到提升。

Conclusion: 提出的方法在半监督学习中有实际优势，能有效平衡有标签和伪标签数据的贡献。

Abstract: Prediction-Powered Inference (PPI) is a recently proposed statistical
inference technique for parameter estimation that leverages pseudo-labels on
both labeled and unlabeled data to construct an unbiased, low-variance
estimator. In this work, we extend its core idea to semi-supervised learning
(SSL) for model training, introducing a novel unbiased gradient estimator. This
extension addresses a key challenge in SSL: while unlabeled data can improve
model performance, its benefit heavily depends on the quality of pseudo-labels.
Inaccurate pseudo-labels can introduce bias, leading to suboptimal models.To
balance the contributions of labeled and pseudo-labeled data, we utilize an
interpolation parameter and tune it on the fly, alongside the model parameters,
using a one-dimensional online learning algorithm. We verify the practical
advantage of our approach through experiments on both synthetic and real
datasets, demonstrating improved performance over classic SSL baselines and PPI
methods that tune the interpolation parameter offline.

</details>


### [278] [A roadmap for curvature-based geometric data analysis and learning](https://arxiv.org/abs/2510.22599)
*Yasharth Yadav,Kelin Xia*

Main category: cs.LG

TL;DR: 本文对现有离散曲率模型进行全面综述，涵盖数学基础、计算形式、实际应用，提出曲率驱动数据分析流程，比较算法并回顾应用，为研究者提供路线图。


<details>
  <summary>Details</summary>
Motivation: 几何数据分析与学习领域发展迅速，曲率是核心概念，已有多种离散曲率模型，需全面综述。

Method: 从黎曼和度量几何角度讨论离散曲率，提出曲率驱动数据分析的系统流程，对比不同数据表示的计算算法。

Result: 完成对现有离散曲率模型的全面综述，分析了其数学基础、计算方法和应用。

Conclusion: 本综述为研究者理解离散曲率作为几何理解和学习的基本工具提供概念和实践路线图。

Abstract: Geometric data analysis and learning has emerged as a distinct and rapidly
developing research area, increasingly recognized for its effectiveness across
diverse applications. At the heart of this field lies curvature, a powerful and
interpretable concept that captures intrinsic geometric structure and underpins
numerous tasks, from community detection to geometric deep learning. A wide
range of discrete curvature models have been proposed for various data
representations, including graphs, simplicial complexes, cubical complexes, and
point clouds sampled from manifolds. These models not only provide efficient
characterizations of data geometry but also constitute essential components in
geometric learning frameworks. In this paper, we present the first
comprehensive review of existing discrete curvature models, covering their
mathematical foundations, computational formulations, and practical
applications in data analysis and learning. In particular, we discuss discrete
curvature from both Riemannian and metric geometry perspectives and propose a
systematic pipeline for curvature-driven data analysis. We further examine the
corresponding computational algorithms across different data representations,
offering detailed comparisons and insights. Finally, we review state-of-the-art
applications of curvature in both supervised and unsupervised learning. This
survey provides a conceptual and practical roadmap for researchers to gain a
better understanding of discrete curvature as a fundamental tool for geometric
understanding and learning.

</details>


### [279] [CLEANet: Robust and Efficient Anomaly Detection in Contaminated Multivariate Time Series](https://arxiv.org/abs/2510.22619)
*Songhan Zhang,Yuanhao Lai,Pengfei Zheng,Boxi Yu,Xiaoying Tang,Qiuai Fu,Pinjia He*

Main category: cs.LG

TL;DR: 提出CLEANet框架解决多变量时间序列异常检测中训练数据污染和模型推理效率低的问题，在多数据集表现优且有强泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法假设训练数据干净，实际数据污染会降低检测精度，复杂深度模型易过拟合且推理延迟高，阻碍实际应用。

Method: 提出CLEANet框架，包含抗污染训练框架CRTF，结合自适应重建加权策略和聚类引导对比学习；设计轻量级共轭MLP分离时间和跨特征依赖。

Result: 在五个公共数据集上，与十个基线模型相比，F1最高提升73.04%，运行时间最多降低81.28%；将CRTF集成到三个先进模型中，F1平均提升5.35%。

Conclusion: CLEANet能有效解决训练数据污染和模型推理效率低的问题，具有较强的泛化能力。

Abstract: Multivariate time series (MTS) anomaly detection is essential for maintaining
the reliability of industrial systems, yet real-world deployment is hindered by
two critical challenges: training data contamination (noises and hidden
anomalies) and inefficient model inference. Existing unsupervised methods
assume clean training data, but contamination distorts learned patterns and
degrades detection accuracy. Meanwhile, complex deep models often overfit to
contamination and suffer from high latency, limiting practical use. To address
these challenges, we propose CLEANet, a robust and efficient anomaly detection
framework in contaminated multivariate time series. CLEANet introduces a
Contamination-Resilient Training Framework (CRTF) that mitigates the impact of
corrupted samples through an adaptive reconstruction weighting strategy
combined with clustering-guided contrastive learning, thereby enhancing
robustness. To further avoid overfitting on contaminated data and improve
computational efficiency, we design a lightweight conjugate MLP that
disentangles temporal and cross-feature dependencies. Across five public
datasets, CLEANet achieves up to 73.04% higher F1 and 81.28% lower runtime
compared with ten state-of-the-art baselines. Furthermore, integrating CRTF
into three advanced models yields an average 5.35% F1 gain, confirming its
strong generalizability.

</details>


### [280] [FastVLM: Self-Speculative Decoding for Fast Vision-Language Model Inference](https://arxiv.org/abs/2510.22641)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 提出基于模仿学习的Self - Speculative Decoding (SSD)框架FastVLM，加速视觉语言模型推理，提升效率与准确率平衡。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型因自回归解码存在计算成本高和推理延迟问题，需解决这些局限。

Method: 采用轻量级草稿模型自回归生成令牌，全模型非自回归验证，接受的令牌继续，拒绝的由全模型纠正并指导草稿模型改进，通过模仿网络用全模型架构的深度见解增强草稿模型。

Result: 与最终层相比，推理过程加速1.55 - 1.85倍，性能损失极小。

Conclusion: FastVLM能在训练草稿模型时保持全模型性能，实现效率和准确性的平衡。

Abstract: Vision-language Models (VLMs) have made significant strides in visual
understanding and query response generation, but often face challenges of high
computational cost and inference latency due to autoregressive decoding. In
this work, we introduce an imitation-learning-based Self-Speculative Decoding
(SSD) framework, named FastVLM, to address these limitations. Our approach
employs a lightweight draft model for token generation in an autoregressive
manner, while a full model verifies these tokens non-autoregressively. Accepted
tokens proceed seamlessly, while rejected tokens are corrected by the full
model and used to guide the draft model's refinement. Through an imitation
network, FastVLM enhances the draft model by integrating deeper level insights
from the full model's architecture. Also, it maintains the performance
integrity of the full model while training the draft model, achieving a balance
between efficiency and accuracy. Our method speeds up the inference process by
1.55-1.85x as compared to the final layer with minimal loss in performance.

</details>


### [281] [Enhancing Graph Classification Robustness with Singular Pooling](https://arxiv.org/abs/2510.22643)
*Sofiane Ennadir,Oleg Smirnov,Yassine Abbahaddou,Lele Cao,Johannes F. Lutzeyer*

Main category: cs.LG

TL;DR: 本文研究图分类中池化操作对GNN对抗鲁棒性的影响，提出RS - Pool策略，理论分析其鲁棒性，实验表明它在攻击下鲁棒性更好且保持干净准确率。


<details>
  <summary>Details</summary>
Motivation: 现有图分类中GNN对抗鲁棒性研究不足，多数防御关注消息传递组件，本文探究池化操作在塑造鲁棒性中的作用。

Method: 对标准扁平池化方法进行理论分析，推导其对抗风险上限；提出RS - Pool策略，利用节点嵌入矩阵的主奇异向量构建图级表示，理论研究其鲁棒性。

Result: 在真实世界基准测试中，RS - Pool在面对先进对抗攻击时比其他池化方法鲁棒性更好，且保持有竞争力的干净准确率。

Conclusion: RS - Pool策略能有效提升图分类中GNN的对抗鲁棒性，且具有模型无关性和高效可实现性。

Abstract: Graph Neural Networks (GNNs) have achieved strong performance across a range
of graph representation learning tasks, yet their adversarial robustness in
graph classification remains underexplored compared to node classification.
While most existing defenses focus on the message-passing component, this work
investigates the overlooked role of pooling operations in shaping robustness.
We present a theoretical analysis of standard flat pooling methods (sum,
average and max), deriving upper bounds on their adversarial risk and
identifying their vulnerabilities under different attack scenarios and graph
structures. Motivated by these insights, we propose \textit{Robust Singular
Pooling (RS-Pool)}, a novel pooling strategy that leverages the dominant
singular vector of the node embedding matrix to construct a robust graph-level
representation. We theoretically investigate the robustness of RS-Pool and
interpret the resulting bound leading to improved understanding of our proposed
pooling operator. While our analysis centers on Graph Convolutional Networks
(GCNs), RS-Pool is model-agnostic and can be implemented efficiently via power
iteration. Empirical results on real-world benchmarks show that RS-Pool
provides better robustness than the considered pooling methods when subject to
state-of-the-art adversarial attacks while maintaining competitive clean
accuracy. Our code is publicly available
at:\href{https://github.com/king/rs-pool}{https://github.com/king/rs-pool}.

</details>


### [282] [Variational Polya Tree](https://arxiv.org/abs/2510.22651)
*Lu Xu,Tsai Hor Chan,Kwok Fai Lam,Lequan Yu,Guosheng Yin*

Main category: cs.LG

TL;DR: 引入变分波利亚树（VPT）模型用于密度估计，解决现有方法缺乏可解释性和不确定性量化问题，在真实数据和图像上验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有密度估计方法缺乏可解释性和不确定性量化，传统贝叶斯非参数方法因计算复杂度高和可扩展性有限难以用于深度学习。

Method: 引入VPT模型，采用随机变分推断计算后验分布，利用联合分布似然进行变分后验近似。

Result: 模型在真实数据和图像上表现有竞争力，能增强可解释性和不确定性量化。

Conclusion: VPT模型是一种有效的密度估计方法，能解决现有方法的不足。

Abstract: Density estimation is essential for generative modeling, particularly with
the rise of modern neural networks. While existing methods capture complex data
distributions, they often lack interpretability and uncertainty quantification.
Bayesian nonparametric methods, especially the \polya tree, offer a robust
framework that addresses these issues by accurately capturing function behavior
over small intervals. Traditional techniques like Markov chain Monte Carlo
(MCMC) face high computational complexity and scalability limitations,
hindering the use of Bayesian nonparametric methods in deep learning. To tackle
this, we introduce the variational \polya tree (VPT) model, which employs
stochastic variational inference to compute posterior distributions. This model
provides a flexible, nonparametric Bayesian prior that captures latent
densities and works well with stochastic gradient optimization. We also
leverage the joint distribution likelihood for a more precise variational
posterior approximation than traditional mean-field methods. We evaluate the
model performance on both real data and images, and demonstrate its
competitiveness with other state-of-the-art deep density estimation methods. We
also explore its ability in enhancing interpretability and uncertainty
quantification. Code is available at
https://github.com/howardchanth/var-polya-tree.

</details>


### [283] [If You Want to Be Robust, Be Wary of Initialization](https://arxiv.org/abs/2510.22652)
*Sofiane Ennadir,Johannes F. Lutzeyer,Michalis Vazirgiannis,El Houcine Bergou*

Main category: cs.LG

TL;DR: 研究图神经网络（GNN）权重初始化和超参数对模型对抗鲁棒性的影响，提出理论框架并实验验证，适用于GNN和深度神经网络。


<details>
  <summary>Details</summary>
Motivation: 现有GNN防御策略多关注预处理和自适应消息传递，本研究探索权重初始化和超参数对模型鲁棒性的影响。

Method: 引入理论框架分析初始化策略与网络对抗鲁棒性的联系，进行跨模型和数据集的大量实验。

Result: 发现初始权重、训练轮数与模型脆弱性有直接关系，选择合适初始化能提升模型鲁棒性，与其他方法有高达50%的差距。

Conclusion: 本研究为对抗鲁棒性提供了超越传统防御机制的新见解，理论框架也适用于深度神经网络。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable performance across
a spectrum of graph-related tasks, however concerns persist regarding their
vulnerability to adversarial perturbations. While prevailing defense strategies
focus primarily on pre-processing techniques and adaptive message-passing
schemes, this study delves into an under-explored dimension: the impact of
weight initialization and associated hyper-parameters, such as training epochs,
on a model's robustness. We introduce a theoretical framework bridging the
connection between initialization strategies and a network's resilience to
adversarial perturbations. Our analysis reveals a direct relationship between
initial weights, number of training epochs and the model's vulnerability,
offering new insights into adversarial robustness beyond conventional defense
mechanisms. While our primary focus is on GNNs, we extend our theoretical
framework, providing a general upper-bound applicable to Deep Neural Networks.
Extensive experiments, spanning diverse models and real-world datasets
subjected to various adversarial attacks, validate our findings. We illustrate
that selecting appropriate initialization not only ensures performance on clean
datasets but also enhances model robustness against adversarial perturbations,
with observed gaps of up to 50\% compared to alternative initialization
approaches.

</details>


### [284] [UCB-type Algorithm for Budget-Constrained Expert Learning](https://arxiv.org/abs/2510.22654)
*Ilgam Latypov,Alexandra Suvorikova,Alexey Kroshnin,Alexander Gasnikov,Yuriy Dorn*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In many modern applications, a system must dynamically choose between several
adaptive learning algorithms that are trained online. Examples include model
selection in streaming environments, switching between trading strategies in
finance, and orchestrating multiple contextual bandit or reinforcement learning
agents. At each round, a learner must select one predictor among $K$ adaptive
experts to make a prediction, while being able to update at most $M \le K$ of
them under a fixed training budget.
  We address this problem in the \emph{stochastic setting} and introduce
\algname{M-LCB}, a computationally efficient UCB-style meta-algorithm that
provides \emph{anytime regret guarantees}. Its confidence intervals are built
directly from realized losses, require no additional optimization, and
seamlessly reflect the convergence properties of the underlying experts. If
each expert achieves internal regret $\tilde O(T^\alpha)$, then \algname{M-LCB}
ensures overall regret bounded by $\tilde O\!\Bigl(\sqrt{\tfrac{KT}{M}} \;+\;
(K/M)^{1-\alpha}\,T^\alpha\Bigr)$.
  To our knowledge, this is the first result establishing regret guarantees
when multiple adaptive experts are trained simultaneously under per-round
budget constraints. We illustrate the framework with two representative cases:
(i) parametric models trained online with stochastic losses, and (ii) experts
that are themselves multi-armed bandit algorithms. These examples highlight how
\algname{M-LCB} extends the classical bandit paradigm to the more realistic
scenario of coordinating stateful, self-learning experts under limited
resources.

</details>


### [285] [Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections](https://arxiv.org/abs/2510.22655)
*Berken Utku Demirel,Christian Holz*

Main category: cs.LG

TL;DR: 本文提出用正交基和过完备框架生成视图的无监督表征学习方法，在多个数据集和任务上验证效果，不依赖数据增强获性能提升。


<details>
  <summary>Details</summary>
Motivation: 多数自监督学习方法依赖手工数据增强，设计需领域知识且限制泛化性，因此提出新方法。

Method: 用正交基和过完备框架生成视图，联合利用不同流形的互补几何特性。

Result: 在九个数据集的五个时间序列任务上，相比现有自监督方法性能提升15 - 20%。

Conclusion: 所提方法不依赖增强诱导的多样性，能取得更好性能。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for
learning representations without labeled data. Most SSL approaches rely on
strong, well-established, handcrafted data augmentations to generate diverse
views for representation learning. However, designing such augmentations
requires domain-specific knowledge and implicitly imposes representational
invariances on the model, which can limit generalization. In this work, we
propose an unsupervised representation learning method that replaces
augmentations by generating views using orthonormal bases and overcomplete
frames. We show that embeddings learned from orthonormal and overcomplete
spaces reside on distinct manifolds, shaped by the geometric biases introduced
by representing samples in different spaces. By jointly leveraging the
complementary geometry of these distinct manifolds, our approach achieves
superior performance without artificially increasing data diversity through
strong augmentations. We demonstrate the effectiveness of our method on nine
datasets across five temporal sequence tasks, where signal-specific
characteristics make data augmentations particularly challenging. Without
relying on augmentation-induced diversity, our method achieves performance
gains of up to 15--20\% over existing self-supervised approaches. Source code:
https://github.com/eth-siplab/Learning-with-FrameProjections

</details>


### [286] [FlowCritic: Bridging Value Estimation with Flow Matching in Reinforcement Learning](https://arxiv.org/abs/2510.22686)
*Shan Zhong,Shutong Ding,He Diao,Xiangyu Wang,Kah Chan Teh,Bei Peng*

Main category: cs.LG

TL;DR: 提出FlowCritic用于价值估计，利用流匹配建模价值分布。


<details>
  <summary>Details</summary>
Motivation: 现有工作在提升价值函数估计可靠性上存在局限性，多点估计未捕捉分布信息，分布强化学习受离散化或分位数回归限制。

Method: 提出FlowCritic，利用流匹配来建模价值分布并生成样本进行价值估计。

Result: 未提及

Conclusion: 未提及

Abstract: Reliable value estimation serves as the cornerstone of reinforcement learning
(RL) by evaluating long-term returns and guiding policy improvement,
significantly influencing the convergence speed and final performance. Existing
works improve the reliability of value function estimation via multi-critic
ensembles and distributional RL, yet the former merely combines multi point
estimation without capturing distributional information, whereas the latter
relies on discretization or quantile regression, limiting the expressiveness of
complex value distributions. Inspired by flow matching's success in generative
modeling, we propose a generative paradigm for value estimation, named
FlowCritic. Departing from conventional regression for deterministic value
prediction, FlowCritic leverages flow matching to model value distributions and
generate samples for value estimation.

</details>


### [287] [S-Chain: Structured Visual Chain-of-Thought For Medicine](https://arxiv.org/abs/2510.22728)
*Khai Le-Duc,Duy M. H. Nguyen,Phuong T. H. Trinh,Tien-Phat Nguyen,Nghiem T. Diep,An Ngo,Tung Vu,Trinh Vuong,Anh-Tien Nguyen,Mau Nguyen,Van Trung Hoang,Khai-Nguyen Nguyen,Hy Nguyen,Chris Ngo,Anji Liu,Nhat Ho,Anne-Christin Hauschild,Khanh Xuan Nguyen,Thanh Nguyen-Tang,Pengtao Xie,Daniel Sonntag,James Zou,Mathias Niepert,Anh Totti Nguyen*

Main category: cs.LG

TL;DR: 提出首个大规模医学数据集S - Chain，支持多语言，用其对VLMs进行基准测试，研究与检索增强生成的协同作用，提出新机制，为医学推理建立新基准。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型缺乏能捕捉精确视觉基础的逐步推理的大规模专家级数据集，需要实现文本理由与视觉证据的透明对齐。

Method: 引入含专家注释、边界框和结构化视觉思维链的S - Chain数据集，用其对医学和通用VLMs进行基准测试，研究与检索增强生成的协同作用，提出新机制。

Result: SV - CoT监督显著提高了解释性、基础保真度和鲁棒性，揭示了领域知识和视觉基础在自回归推理中的相互作用。

Conclusion: S - Chain为有根据的医学推理建立了新基准，有助于构建更可信和可解释的医学VLMs。

Abstract: Faithful reasoning in medical vision-language models (VLMs) requires not only
accurate predictions but also transparent alignment between textual rationales
and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise
in medical visual question answering (VQA), no large-scale expert-level dataset
has captured stepwise reasoning with precise visual grounding. We introduce
S-Chain, the first large-scale dataset of 12,000 expert-annotated medical
images with bounding boxes and structured visual CoT (SV-CoT), explicitly
linking visual regions to reasoning steps. The dataset further supports 16
languages, totaling over 700k VQA pairs for broad multilingual applicability.
Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,
LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that
SV-CoT supervision significantly improves interpretability, grounding fidelity,
and robustness. Beyond benchmarking, we study its synergy with
retrieval-augmented generation, revealing how domain knowledge and visual
grounding interact during autoregressive reasoning. Finally, we propose a new
mechanism that strengthens the alignment between visual evidence and reasoning,
improving both reliability and efficiency. S-Chain establishes a new benchmark
for grounded medical reasoning and paves the way toward more trustworthy and
explainable medical VLMs.

</details>


### [288] [Distributionally Robust Optimization via Diffusion Ambiguity Modeling](https://arxiv.org/abs/2510.22757)
*Jiaqi Wen,Jianyi Yang*

Main category: cs.LG

TL;DR: 本文研究分布鲁棒优化（DRO），提出基于扩散的模糊集设计及D - DRO算法，证明其收敛性并验证OOD泛化性能。


<details>
  <summary>Details</summary>
Motivation: 设计有效的模糊集，使DRO既能与名义分布一致，又能涵盖多种潜在场景，且能得到易处理的解。

Method: 提出基于扩散的模糊集设计，构建Diffusion - based DRO（D - DRO）算法求解参数化扩散模型空间的内最大化问题。

Result: 理论上确立了D - DRO的平稳收敛性能，实验上证明其在机器学习预测任务中具有优越的分布外（OOD）泛化性能。

Conclusion: 基于扩散的模糊集设计和D - DRO算法在DRO中有效，能提升OOD泛化性能。

Abstract: This paper studies Distributionally Robust Optimization (DRO), a fundamental
framework for enhancing the robustness and generalization of statistical
learning and optimization. An effective ambiguity set for DRO must involve
distributions that remain consistent with the nominal distribution while being
diverse enough to account for a variety of potential scenarios. Moreover, it
should lead to tractable DRO solutions. To this end, we propose a
diffusion-based ambiguity set design that captures various adversarial
distributions beyond the nominal support space while maintaining consistency
with the nominal distribution. Building on this ambiguity modeling, we propose
Diffusion-based DRO (D-DRO), a tractable DRO algorithm that solves the inner
maximization over the parameterized diffusion model space. We formally
establish the stationary convergence performance of D-DRO and empirically
demonstrate its superior Out-of-Distribution (OOD) generalization performance
in a ML prediction task.

</details>


### [289] [TELL-TALE: Task Efficient LLMs with Task Aware Layer Elimination](https://arxiv.org/abs/2510.22767)
*Omar Naim,Krish Sharma,Nicholas Asher*

Main category: cs.LG

TL;DR: 介绍TALE算法，能在推理时剪枝大语言模型的Transformer层，评估其在多任务和多模型上的效果，有诸多优势。


<details>
  <summary>Details</summary>
Motivation: 寻找在不进行重新训练的情况下，优化大语言模型推理时性能并降低计算成本的方法。

Method: 提出TALE算法，通过直接优化特定任务的验证性能来剪枝整个Transformer层。

Result: 在9个任务和5个模型上评估，无需重新训练，提高准确率、降低计算成本，微调时有额外性能提升，能灵活控制准确率和效率的权衡。

Conclusion: TALE能产生更小、更快、更准确且更易微调的模型，还为Transformer可解释性提供新见解。

Abstract: In this paper we introduce Tale, Task-Aware Layer Elimination, an
inference-time algorithm that prunes entire transformer layers in an LLM by
directly optimizing task-specific validation performance. We evaluate TALE on 9
tasks and 5 models, including LLaMA 3.1 8B, Qwen 2.5 7B, Qwen 2.5 0.5B, Mistral
7B, and Lucie 7B, under both zero-shot and few-shot settings. Unlike prior
approaches, TALE requires no retraining and consistently improves accuracy
while reducing computational cost across all benchmarks. Furthermore, applying
TALE during finetuning leads to additional performance gains. Finally, TALE
provides flexible user control over trade-offs between accuracy and efficiency.
Mutual information analysis shows that certain layers act as bottlenecks,
degrading task-relevant representations. Tale's selective layer removal
remedies this problem, producing smaller, faster, and more accurate models that
are also faster to fine-tune while offering new insights into transformer
interpretability.

</details>


### [290] [SeeDNorm: Self-Rescaled Dynamic Normalization](https://arxiv.org/abs/2510.22777)
*Wenrui Cai,Defa Zhu,Qingjie Liu,Qiyang Min*

Main category: cs.LG

TL;DR: 提出SeeDNorm归一化层，动态调整缩放系数，保留输入范数信息，在多任务中优于RMSNorm等。


<details>
  <summary>Details</summary>
Motivation: RMSNorm在正向传播中丢弃输入范数信息，静态缩放因子不足以适应输入数据的变化和分布偏移，限制性能提升。

Method: 提出SeeDNorm，基于当前输入动态调整缩放系数，保留输入范数信息；详细分析训练优化并提出解决潜在不稳定问题的方案。

Result: 在大语言模型预训练、有监督和无监督计算机视觉任务中验证了SeeDNorm的有效性，在引入极少参数且对模型效率影响可忽略的情况下，表现优于RMSNorm、LayerNorm和DyT等。

Conclusion: SeeDNorm能有效提升模型性能，是一种更优的归一化层。

Abstract: Normalization layer constitutes an essential component in neural networks. In
transformers, the predominantly used RMSNorm constrains vectors to a unit
hypersphere, followed by dimension-wise rescaling through a learnable scaling
coefficient $\gamma$ to maintain the representational capacity of the model.
However, RMSNorm discards the input norm information in forward pass and a
static scaling factor $\gamma$ may be insufficient to accommodate the wide
variability of input data and distributional shifts, thereby limiting further
performance improvements, particularly in zero-shot scenarios that large
language models routinely encounter. To address this limitation, we propose
SeeDNorm, which enhances the representational capability of the model by
dynamically adjusting the scaling coefficient based on the current input,
thereby preserving the input norm information and enabling data-dependent,
self-rescaled dynamic normalization. During backpropagation, SeeDNorm retains
the ability of RMSNorm to dynamically adjust gradient according to the input
norm. We provide a detailed analysis of the training optimization for SeedNorm
and proposed corresponding solutions to address potential instability issues
that may arise when applying SeeDNorm. We validate the effectiveness of
SeeDNorm across models of varying sizes in large language model pre-training as
well as supervised and unsupervised computer vision tasks. By introducing a
minimal number of parameters and with neglligible impact on model efficiency,
SeeDNorm achieves consistently superior performance compared to previously
commonly used normalization layers such as RMSNorm and LayerNorm, as well as
element-wise activation alternatives to normalization layers like DyT.

</details>


### [291] [Inductive Transfer Learning for Graph-Based Recommenders](https://arxiv.org/abs/2510.22799)
*Florian Grötschla,Elia Trachsel,Luca A. Lanzendörfer,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 提出支持跨数据集归纳迁移学习的图推荐模型NBF - Rec，在多个数据集评估有良好表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于图的推荐系统在直推式设置下训练，限制了对新用户、物品或数据集的适用性。

Method: 提出图推荐模型NBF - Rec，在推理时动态计算节点嵌入。

Result: 在七个真实数据集上评估，NBF - Rec在零样本设置下有竞争力，轻量级微调可进一步提升性能。

Conclusion: 图推荐中归纳迁移是可行的，交互级消息传递支持跨数据集泛化，无需对齐用户或物品。

Abstract: Graph-based recommender systems are commonly trained in transductive
settings, which limits their applicability to new users, items, or datasets. We
propose NBF-Rec, a graph-based recommendation model that supports inductive
transfer learning across datasets with disjoint user and item sets. Unlike
conventional embedding-based methods that require retraining for each domain,
NBF-Rec computes node embeddings dynamically at inference time. We evaluate the
method on seven real-world datasets spanning movies, music, e-commerce, and
location check-ins. NBF-Rec achieves competitive performance in zero-shot
settings, where no target domain data is used for training, and demonstrates
further improvements through lightweight fine-tuning. These results show that
inductive transfer is feasible in graph-based recommendation and that
interaction-level message passing supports generalization across datasets
without requiring aligned users or items.

</details>


### [292] [Distributed Multi-Agent Bandits Over Erdős-Rényi Random Networks](https://arxiv.org/abs/2510.22811)
*Jingyuan Liu,Hao Qiu,Lin Yang,Mengfan Xu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the distributed multi-agent multi-armed bandit problem with
heterogeneous rewards over random communication graphs. Uniquely, at each time
step $t$ agents communicate over a time-varying random graph $G_t$ generated by
applying the Erd\H{o}s-R\'enyi model to a fixed connected base graph $G$ (for
classical Erd\H{o}s-R\'enyi graphs, $G$ is a complete graph), where each
potential edge in $G$ is randomly and independently present with the link
probability $p$. Notably, the resulting random graph is not necessarily
connected at each time step. Each agent's arm rewards follow time-invariant
distributions, and the reward distribution for the same arm may differ across
agents. The goal is to minimize the cumulative expected regret relative to the
global mean reward of each arm, defined as the average of that arm's mean
rewards across all agents. To this end, we propose a fully distributed
algorithm that integrates the arm elimination strategy with the random gossip
algorithm. We theoretically show that the regret upper bound is of order $\log
T$ and is highly interpretable, where $T$ is the time horizon. It includes the
optimal centralized regret $O\left(\sum_{k: \Delta_k>0} \frac{\log
T}{\Delta_k}\right)$ and an additional term $O\left(\frac{N^2 \log T}{p
\lambda_{N-1}(Lap(G))} + \frac{KN^2 \log T}{p}\right)$ where $N$ and $K$ denote
the total number of agents and arms, respectively. This term reflects the
impact of $G$'s algebraic connectivity $\lambda_{N-1}(Lap(G))$ and the link
probability $p$, and thus highlights a fundamental trade-off between
communication efficiency and regret. As a by-product, we show a nearly optimal
regret lower bound. Finally, our numerical experiments not only show the
superiority of our algorithm over existing benchmarks, but also validate the
theoretical regret scaling with problem complexity.

</details>


### [293] [Air Quality Prediction Using LOESS-ARIMA and Multi-Scale CNN-BiLSTM with Residual-Gated Attention](https://arxiv.org/abs/2510.22818)
*Soham Pahari,Sandeep Chand Kumain*

Main category: cs.LG

TL;DR: 本文提出混合预测框架用于印度大城市空气质量指数（AQI）预测，实验表明该方法优于基线模型，适用于城市空气质量管理。


<details>
  <summary>Details</summary>
Motivation: 印度大城市空气污染严重，污染物浓度突然升高挑战及时干预，且准确的 AQI 预测因多种因素而困难。

Method: 提出结合 LOESS 分解、ARIMA 建模和多尺度 CNN - BiLSTM 网络与残差门控注意力机制的混合预测框架，用 UAMMO 调优超参数。

Result: 在 2021 - 2023 年 AQI 数据集上，该方法在三个大城市的多种污染物预测中均优于统计、深度学习和混合基线模型，MSE 降低 5 - 8%，R² 分数高于 0.94。

Conclusion: 该框架具有鲁棒性，对突发污染事件敏感，适用于城市空气质量管理。

Abstract: Air pollution remains a critical environmental and public health concern in
Indian megacities such as Delhi, Kolkata, and Mumbai, where sudden spikes in
pollutant levels challenge timely intervention. Accurate Air Quality Index
(AQI) forecasting is difficult due to the coexistence of linear trends,
seasonal variations, and volatile nonlinear patterns. This paper proposes a
hybrid forecasting framework that integrates LOESS decomposition, ARIMA
modeling, and a multi-scale CNN-BiLSTM network with a residual-gated attention
mechanism. The LOESS step separates the AQI series into trend, seasonal, and
residual components, with ARIMA modeling the smooth components and the proposed
deep learning module capturing multi-scale volatility in the residuals. Model
hyperparameters are tuned via the Unified Adaptive Multi-Stage Metaheuristic
Optimizer (UAMMO), combining multiple optimization strategies for efficient
convergence. Experiments on 2021-2023 AQI datasets from the Central Pollution
Control Board show that the proposed method consistently outperforms
statistical, deep learning, and hybrid baselines across PM2.5, O3, CO, and NOx
in three major cities, achieving up to 5-8% lower MSE and higher R^2 scores
(>0.94) for all pollutants. These results demonstrate the framework's
robustness, sensitivity to sudden pollution events, and applicability to urban
air quality management.

</details>


### [294] [Last Iterate Analyses of FTRL in Stochasitc Bandits](https://arxiv.org/abs/2510.22819)
*Jingxin Zhan,Yuze Han,Zhihua Zhang*

Main category: cs.LG

TL;DR: 本文研究多臂老虎机中FTRL算法的最后迭代收敛率，理论分析表明1/2 - Tsallis - INF算法的Bregman散度以t^(-1/2)速率衰减。


<details>
  <summary>Details</summary>
Motivation: 多臂老虎机中现有算法分析多关注遗憾阶，FTRL算法的最后迭代收敛率研究较少，虽有工作建立其BOBW属性，但最后迭代收敛率未被研究。

Method: 进行理论分析，研究与BOBW FTRL算法1/2 - Tsallis - INF相关的正则函数定义的Bregman散度。

Result: 与1/2 - Tsallis - INF算法相关的Bregman散度，在最优臂上的点质量和第t次迭代获得的臂集上的概率分布之间，以t^(-1/2)的速率衰减。

Conclusion: 部分证实了对数遗憾应对应t^(-1)最后迭代收敛率的直觉。

Abstract: The convergence analysis of online learning algorithms is central to machine
learning theory, where last-iterate convergence is particularly important, as
it captures the learner's actual decisions and describes the evolution of the
learning process over time. However, in multi-armed bandits, most existing
algorithmic analyses mainly focus on the order of regret, while the
last-iterate (simple regret) convergence rate remains less explored --
especially for the widely studied Follow-the-Regularized-Leader (FTRL)
algorithms. Recently, a growing line of work has established the
Best-of-Both-Worlds (BOBW) property of FTRL algorithms in bandit problems,
showing in particular that they achieve logarithmic regret in stochastic
bandits. Nevertheless, their last-iterate convergence rate has not yet been
studied. Intuitively, logarithmic regret should correspond to a $t^{-1}$
last-iterate convergence rate. This paper partially confirms this intuition
through theoretical analysis, showing that the Bregman divergence, defined by
the regular function $\Psi(p)=-4\sum_{i=1}^{d}\sqrt{p_i}$ associated with the
BOBW FTRL algorithm $1/2$-Tsallis-INF (arXiv:1807.07623), between the point
mass on the optimal arm and the probability distribution over the arm set
obtained at iteration $t$, decays at a rate of $t^{-1/2}$.

</details>


### [295] [Logical GANs: Adversarial Learning through Ehrenfeucht Fraisse Games](https://arxiv.org/abs/2510.22824)
*Mirco A. Mannucci*

Main category: cs.LG

TL;DR: 提出LOGAN用于逻辑有界生成，有工具和实验，验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 结合GAN的不可区分性和逻辑原理，在预算限制下实现逻辑有界生成。

Method: 将判别器设为深度 - k的Ehrenfeucht - Fraisse对手，生成器为建造者，有EF - 探针模拟器和MSO风格图检查器，用逻辑损失评分。

Result: 模拟验证属性满意度达92% - 98%，真实神经GAN训练在挑战性属性上提升5% - 14%，连通性达98%。

Conclusion: LOGAN是实现逻辑有界生成的可行路径，有可解释失败、有效且可控。

Abstract: GANs promise indistinguishability, logic explains it. We put the two on a
budget: a discriminator that can only ``see'' up to a logical depth $k$, and a
generator that must look correct to that bounded observer. \textbf{LOGAN}
(LOGical GANs) casts the discriminator as a depth-$k$ Ehrenfeucht--Fra\"iss\'e
(EF) \emph{Opponent} that searches for small, legible faults (odd cycles,
nonplanar crossings, directed bridges), while the generator plays
\emph{Builder}, producing samples that admit a $k$-round matching to a target
theory $T$. We ship a minimal toolkit -- an EF-probe simulator and MSO-style
graph checkers -- and four experiments including real neural GAN training with
PyTorch. Beyond verification, we score samples with a \emph{logical loss} that
mixes budgeted EF round-resilience with cheap certificate terms, enabling a
practical curriculum on depth. Framework validation demonstrates $92\%$--$98\%$
property satisfaction via simulation (Exp.~3), while real neural GAN training
achieves $5\%$--$14\%$ improvements on challenging properties and $98\%$
satisfaction on connectivity (matching simulation) through adversarial learning
(Exp.~4). LOGAN is a compact, reproducible path toward logic-bounded generation
with interpretable failures, proven effectiveness (both simulated and real
training), and dials for control.

</details>


### [296] [Encoder-Decoder Diffusion Language Models for Efficient Training and Inference](https://arxiv.org/abs/2510.22852)
*Marianne Arriola,Yair Schiff,Hao Phung,Aaron Gokaslan,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: 提出E2D2框架，采用编解码器架构加速离散扩散模型推理和训练，在多任务上实现生成质量和推理吞吐量的良好权衡。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散模型使用仅解码器架构，每次去噪步骤都需调用全网络，计算成本高，需加速推理。

Method: 提出编解码器架构，用编码器表示干净令牌，轻量级解码器迭代细化噪声序列；引入E2D2框架，含专门训练和采样算法。

Result: E2D2在摘要、翻译和数学推理任务上实现生成质量和推理吞吐量的较好权衡。

Conclusion: 所提出的架构和框架有效，能加速离散扩散模型的推理和训练。

Abstract: Discrete diffusion models enable parallel token sampling for faster inference
than autoregressive approaches. However, prior diffusion models use a
decoder-only architecture, which requires sampling algorithms that invoke the
full network at every denoising step and incur high computational cost. Our key
insight is that discrete diffusion models perform two types of computation: 1)
representing clean tokens and 2) denoising corrupted tokens, which enables us
to use separate modules for each task. We propose an encoder-decoder
architecture to accelerate discrete diffusion inference, which relies on an
encoder to represent clean tokens and a lightweight decoder to iteratively
refine a noised sequence. We also show that this architecture enables faster
training of block diffusion models, which partition sequences into blocks for
better quality and are commonly used in diffusion language model inference. We
introduce a framework for Efficient Encoder-Decoder Diffusion (E2D2),
consisting of an architecture with specialized training and sampling
algorithms, and we show that E2D2 achieves superior trade-offs between
generation quality and inference throughput on summarization, translation, and
mathematical reasoning tasks. We provide the code, model weights, and blog post
on the project page: https://m-arriola.com/e2d2

</details>


### [297] [A Review of End-to-End Precipitation Prediction Using Remote Sensing Data: from Divination to Machine Learning](https://arxiv.org/abs/2510.22855)
*Yugong Zeng,Jonathan Wu*

Main category: cs.LG

TL;DR: 本文回顾降水预测技术从古代到现代的演变，涵盖传统方法、物理建模、统计框架及神经网络方法，还指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 梳理降水预测技术的发展历程，为下一代预测系统提供方向。

Method: 对不同时期的降水预测技术进行调查和综合研究，包括传统方法、物理建模、统计框架及神经网络方法。

Result: 描绘了端到端降水预测的历史，强调了神经网络方法的最新进展。

Conclusion: 该综述不仅展现了降水预测历史，还勾勒出下一代预测系统的未来方向。

Abstract: Precipitation prediction has undergone a profound transformation -- from
early symbolic and empirical methods rooted in divination and observation, to
modern technologies based on atmospheric physics and artificial intelligence.
This review traces the historical and technological evolution of precipitation
forecasting, presenting a survey about end-to-end precipitation prediction
technologies that spans ancient practices, the foundations of meteorological
science, the rise of numerical weather prediction (NWP), and the emergence of
machine learning (ML) and deep learning (DL) models. We first explore
traditional and indigenous forecasting methods, then describe the development
of physical modeling and statistical frameworks that underpin contemporary
operational forecasting. Particular emphasis is placed on recent advances in
neural network-based approaches, including automated deep learning,
interpretability-driven design, and hybrid physical-data models. By compositing
research across multiple eras and paradigms, this review not only depicts the
history of end-to-end precipitation prediction but also outlines future
directions in next generation forecasting systems.

</details>


### [298] [Guardian: Decoupling Exploration from Safety in Reinforcement Learning](https://arxiv.org/abs/2510.22859)
*Kaitong Cai,Jusheng Zhang,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: 提出RLPD - GX框架用于混合离线 - 在线强化学习，结合动态课程稳定训练，在Atari - 100k等任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 混合离线 - 在线强化学习存在因离线和在线数据分布偏移导致的不稳定性问题。

Method: 引入RLPD - GX框架，将策略优化与安全执行解耦；提出动态课程逐步扩展时间范围和调整离线 - 在线数据混合。

Result: 在Atari - 100k上实现归一化平均得分3.02，比先前混合方法提高45%，在安全关键和长视野任务中有一致提升。

Conclusion: 解耦安全执行是实现稳健混合离线 - 在线强化学习的简单有效途径，为强化学习中协调探索与安全提供了更广泛范式。

Abstract: Hybrid offline--online reinforcement learning (O2O RL) promises both sample
efficiency and robust exploration, but suffers from instability due to
distribution shift between offline and online data. We introduce RLPD-GX, a
framework that decouples policy optimization from safety enforcement: a
reward-seeking learner explores freely, while a projection-based guardian
guarantees rule-consistent execution and safe value backups. This design
preserves the exploratory value of online interactions without collapsing to
conservative policies. To further stabilize training, we propose dynamic
curricula that gradually extend temporal horizons and anneal offline--online
data mixing. We prove convergence via a contraction property of the guarded
Bellman operator, and empirically show state-of-the-art performance on
Atari-100k, achieving a normalized mean score of 3.02 (+45\% over prior hybrid
methods) with stronger safety and stability. Beyond Atari, ablations
demonstrate consistent gains across safety-critical and long-horizon tasks,
underscoring the generality of our design. Extensive and comprehensive results
highlight decoupled safety enforcement as a simple yet principled route to
robust O2O RL, suggesting a broader paradigm for reconciling exploration and
safety in reinforcement learning.

</details>


### [299] [Long-Term PM2.5 Forecasting Using a DTW-Enhanced CNN-GRU Model](https://arxiv.org/abs/2510.22863)
*Amirali Ataee Naeini,Arshia Ataee Naeini,Fatemeh Karami Mohammadi,Omid Ghaffarpasand*

Main category: cs.LG

TL;DR: 提出结合DTW与CNN - GRU的深度学习框架用于伊斯法罕市PM2.5长期预测，实验表现优于现有方法，能实现稳定10天预测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以在48小时外维持PM2.5预测稳定性，尤其是监测网络稀疏的城市，需要可靠的长期PM2.5浓度预测用于公共健康预警系统。

Method: 结合DTW进行智能站点相似度选择，采用CNN - GRU架构，有DTW历史采样、结合气象特征的轻量级架构、针对稀疏网络的可扩展设计三项创新。

Result: 使用多年小时级数据实验，24小时预测R2 = 0.91，首次实现稳定10天PM2.5预测（240小时R2 = 0.73）。

Conclusion: 该框架计算效率高，不依赖外部工具，适合资源受限的城市环境部署。

Abstract: Reliable long-term forecasting of PM2.5 concentrations is critical for public
health early-warning systems, yet existing deep learning approaches struggle to
maintain prediction stability beyond 48 hours, especially in cities with sparse
monitoring networks. This paper presents a deep learning framework that
combines Dynamic Time Warping (DTW) for intelligent station similarity
selection with a CNN-GRU architecture to enable extended-horizon PM2.5
forecasting in Isfahan, Iran, a city characterized by complex pollution
dynamics and limited monitoring coverage. Unlike existing approaches that rely
on computationally intensive transformer models or external simulation tools,
our method integrates three key innovations: (i) DTW-based historical sampling
to identify similar pollution patterns across peer stations, (ii) a lightweight
CNN-GRU architecture augmented with meteorological features, and (iii) a
scalable design optimized for sparse networks. Experimental validation using
multi-year hourly data from eight monitoring stations demonstrates superior
performance compared to state-of-the-art deep learning methods, achieving R2 =
0.91 for 24-hour forecasts. Notably, this is the first study to demonstrate
stable 10-day PM2.5 forecasting (R2 = 0.73 at 240 hours) without performance
degradation, addressing critical early-warning system requirements. The
framework's computational efficiency and independence from external tools make
it particularly suitable for deployment in resource-constrained urban
environments.

</details>


### [300] [Limits of Generative Pre-Training in Structured EMR Trajectories with Irregular Sampling](https://arxiv.org/abs/2510.22878)
*Nicholas I-Hsien Kuo,Blanca Gallego,Louisa Jorm*

Main category: cs.LG

TL;DR: 研究对基础模型在表型发现的应用进行测试，训练两个自回归模型，发现生成式预训练虽有局部真实性但临床一致性有限，强调特定领域评估和轨迹合成的重要性。


<details>
  <summary>Details</summary>
Motivation: 针对一些研究在未严格验证的情况下将基础模型学到的表示用于表型发现，存在临床不一致的风险，进行测试。

Method: 在HIV和急性低血压纵向数据集上训练序列到序列LSTM和简化Transformer两个自回归模型，训练时添加随机访视间隔的不规则性，用患者轨迹合成评估分布和相关性保真度。

Result: 两个模型能重现特征分布，但未能保留跨特征结构，生成式预训练有局部真实性但临床一致性有限。

Conclusion: 强调需要特定领域评估，支持在微调或部署前用轨迹合成作为实用探测手段。

Abstract: Foundation models refer to architectures trained on vast datasets using
autoregressive pre-training from natural language processing to capture
intricate patterns and motifs. They were originally developed to transfer such
learned knowledge to downstream predictive tasks. Recently, however, some
studies repurpose these learned representations for phenotype discovery without
rigorous validation, risking superficially realistic but clinically incoherent
embeddings. To test this mismatch, we trained two autoregressive models -- a
sequence-to-sequence LSTM and a reduced Transformer -- on longitudinal ART for
HIV and Acute Hypotension datasets. Controlled irregularity was added during
training via random inter-visit gaps, while test sequences stayed complete.
Patient-trajectory synthesis evaluated distributional and correlational
fidelity. Both reproduced feature distributions but failed to preserve
cross-feature structure -- showing that generative pre-training yields local
realism but limited clinical coherence. These results highlight the need for
domain-specific evaluation and support trajectory synthesis as a practical
probe before fine-tuning or deployment.

</details>


### [301] [Learning Reconfigurable Representations for Multimodal Federated Learning with Missing Data](https://arxiv.org/abs/2510.22880)
*Duong M. Nguyen,Trong Nghia Hoang,Thanh Trung Huynh,Quoc Viet Hung Nguyen,Phi Le Nguyen*

Main category: cs.LG

TL;DR: 本文针对多模态联邦学习中客户端数据不完整和异构问题，提出含可学习嵌入控制的框架，实验证明有效且有理论分析支持。


<details>
  <summary>Details</summary>
Motivation: 现实中多模态联邦学习存在数据不完整和异构问题，导致局部特征表示不对齐，限制模型聚合效果。

Method: 提出基于可学习客户端嵌入控制的联邦学习框架，嵌入编码数据缺失模式，作为重配置信号，还可跨客户端聚合。

Result: 在多个联邦多模态基准测试中有效，严重数据不完整时性能提升达36.45%，有匹配实证的理论分析。

Conclusion: 所提方法能解决多模态联邦学习中数据不完整和异构问题，提升性能，且有理论保障。

Abstract: Multimodal federated learning in real-world settings often encounters
incomplete and heterogeneous data across clients. This results in misaligned
local feature representations that limit the effectiveness of model
aggregation. Unlike prior work that assumes either differing modality sets
without missing input features or a shared modality set with missing features
across clients, we consider a more general and realistic setting where each
client observes a different subset of modalities and might also have missing
input features within each modality. To address the resulting misalignment in
learned representations, we propose a new federated learning framework
featuring locally adaptive representations based on learnable client-side
embedding controls that encode each client's data-missing patterns.
  These embeddings serve as reconfiguration signals that align the globally
aggregated representation with each client's local context, enabling more
effective use of shared information. Furthermore, the embedding controls can be
algorithmically aggregated across clients with similar data-missing patterns to
enhance the robustness of reconfiguration signals in adapting the global
representation. Empirical results on multiple federated multimodal benchmarks
with diverse data-missing patterns across clients demonstrate the efficacy of
the proposed method, achieving up to 36.45\% performance improvement under
severe data incompleteness. The method is also supported by a theoretical
analysis with an explicit performance bound that matches our empirical
observations. Our source codes are provided at
https://github.com/nmduonggg/PEPSY

</details>


### [302] [Offline Preference Optimization via Maximum Marginal Likelihood Estimation](https://arxiv.org/abs/2510.22881)
*Saeed Najafi,Alona Fyshe*

Main category: cs.LG

TL;DR: 提出基于最大边际似然估计的MMPO方法进行大语言模型与人类偏好对齐，实验表明其更稳定且性能好。


<details>
  <summary>Details</summary>
Motivation: 标准的大语言模型与人类偏好对齐方法（如RLHF）复杂且不稳定。

Method: 提出基于最大边际似然估计的MMPO方法，最大化首选文本输出的边际对数似然，无需显式奖励模型和熵最大化。

Result: 在135M - 8B参数的模型上，MMPO对超参数β更稳定，能实现有竞争力或更优的偏好对齐，同时更好保留基础模型的通用语言能力。

Conclusion: MMPO性能提升归因于其在梯度更新中的隐式偏好优化。

Abstract: Aligning Large Language Models (LLMs) with human preferences is crucial, but
standard methods like Reinforcement Learning from Human Feedback (RLHF) are
often complex and unstable. In this work, we propose a new, simpler approach
that recasts alignment through the lens of Maximum Marginal Likelihood (MML)
estimation. Our new MML based Preference Optimization (MMPO) maximizes the
marginal log-likelihood of a preferred text output, using the preference pair
as samples for approximation, and forgoes the need for both an explicit reward
model and entropy maximization. We theoretically demonstrate that MMPO
implicitly performs preference optimization, producing a weighted gradient that
naturally up-weights chosen responses over rejected ones. Across models ranging
from 135M to 8B parameters, we empirically show that MMPO: 1) is more stable
with respect to the hyperparameter $\beta$ compared to alternative baselines,
and 2) achieves competitive or superior preference alignment while better
preserving the base model's general language capabilities. Through a series of
ablation experiments, we show that this improved performance is indeed
attributable to MMPO's implicit preference optimization within the gradient
updates.

</details>


### [303] [AI based signage classification for linguistic landscape studies](https://arxiv.org/abs/2510.22885)
*Yuqin Jiang,Song Jiang,Jacob Algrim,Trevor Harms,Maxwell Koenen,Xinya Lan,Xingyu Li,Chun-Han Lin,Jia Liu,Jiayang Sun,Henry Zenger*

Main category: cs.LG

TL;DR: 传统语言景观研究方法耗时，本文用AI方法分析，虽有局限但可结合人工验证提高效率。


<details>
  <summary>Details</summary>
Motivation: 传统语言景观研究方法耗时且不适用于大面积研究，探索用AI进行自动化分析。

Method: 以檀香山唐人街为例，构建含1449张图像的地理参考照片数据集，用AI进行OCR和语言分类，进行人工验证。

Result: 模型整体准确率79%，发现五种误标注类型，AI会检测人类忽略的边缘或背景文本。

Conclusion: AI辅助工作流有潜力，但有局限不能全信，鼓励结合AI自动化与人工验证的混合方法。

Abstract: Linguistic Landscape (LL) research traditionally relies on manual photography
and annotation of public signages to examine distribution of languages in urban
space. While such methods yield valuable findings, the process is
time-consuming and difficult for large study areas. This study explores the use
of AI powered language detection method to automate LL analysis. Using Honolulu
Chinatown as a case study, we constructed a georeferenced photo dataset of
1,449 images collected by researchers and applied AI for optical character
recognition (OCR) and language classification. We also conducted manual
validations for accuracy checking. This model achieved an overall accuracy of
79%. Five recurring types of mislabeling were identified, including distortion,
reflection, degraded surface, graffiti, and hallucination. The analysis also
reveals that the AI model treats all regions of an image equally, detecting
peripheral or background texts that human interpreters typically ignore.
Despite these limitations, the results demonstrate the potential of integrating
AI-assisted workflows into LL research to reduce such time-consuming processes.
However, due to all the limitations and mis-labels, we recognize that AI cannot
be fully trusted during this process. This paper encourages a hybrid approach
combining AI automation with human validation for a more reliable and efficient
workflow.

</details>


### [304] [Transforming volcanic monitoring: A dataset and benchmark for onboard volcano activity detection](https://arxiv.org/abs/2510.22889)
*Darshana Priyasad,Tharindu Fernando,Maryam Haghighat,Harshala Gammulle,Clinton Fookes*

Main category: cs.LG

TL;DR: 本文介绍用于火山活动检测的新数据集，进行基准测试，探索模型星载部署可行性，为火山灾害管理创新方案奠基。


<details>
  <summary>Details</summary>
Motivation: 自然灾害如火山喷发影响大，但缺乏火山活动标注数据集阻碍检测系统发展。

Method: 引入全球火山活动新数据集并提供二元标注，用先进模型做基准测试，以Intel Movidius Myriad X VPU为测试平台探索星载部署。

Result: 新数据集为模型开发和评估提供基础资源，证明火山活动星载检测可行，可减少延迟、提高响应时间。

Conclusion: 为火山灾害管理创新方案铺平道路，鼓励星载监测技术进一步探索和完善。

Abstract: Natural disasters, such as volcanic eruptions, pose significant challenges to
daily life and incur considerable global economic losses. The emergence of
next-generation small-satellites, capable of constellation-based operations,
offers unparalleled opportunities for near-real-time monitoring and onboard
processing of such events. However, a major bottleneck remains the lack of
extensive annotated datasets capturing volcanic activity, which hinders the
development of robust detection systems. This paper introduces a novel dataset
explicitly designed for volcanic activity and eruption detection, encompassing
diverse volcanoes worldwide. The dataset provides binary annotations to
identify volcanic anomalies or non-anomalies, covering phenomena such as
temperature anomalies, eruptions, and volcanic ash emissions. These annotations
offer a foundational resource for developing and evaluating detection models,
addressing a critical gap in volcanic monitoring research. Additionally, we
present comprehensive benchmarks using state-of-the-art models to establish
baselines for future studies. Furthermore, we explore the potential for
deploying these models onboard next-generation satellites. Using the Intel
Movidius Myriad X VPU as a testbed, we demonstrate the feasibility of volcanic
activity detection directly onboard. This capability significantly reduces
latency and enhances response times, paving the way for advanced early warning
systems. This paves the way for innovative solutions in volcanic disaster
management, encouraging further exploration and refinement of onboard
monitoring technologies.

</details>


### [305] [Charting the Design Space of Neural Graph Representations for Subgraph Matching](https://arxiv.org/abs/2510.22897)
*Vaibhav Raj,Indradyumna Roy,Ashwin Ramachandran,Soumen Chakrabarti,Abir De*

Main category: cs.LG

TL;DR: 本文对图匹配网络的统一设计空间进行全面探索，发现未被探索的选择组合能带来性能提升，并揭示了有价值的见解和设计原则。


<details>
  <summary>Details</summary>
Motivation: 现有子图匹配的神经方法可重构到统一设计空间，但该空间大部分未被探索，需要进行全面研究。

Method: 对图匹配网络的统一设计空间进行全面探索，考虑查询图和语料库图的交互方式、节点与边对齐、最终评分网络形式等维度。

Result: 发现该空间中明智且未被探索的选择组合能带来显著的性能提升。

Conclusion: 研究揭示了有价值的见解，为神经图表示和交互建立了通用设计原则。

Abstract: Subgraph matching is vital in knowledge graph (KG) question answering,
molecule design, scene graph, code and circuit search, etc. Neural methods have
shown promising results for subgraph matching. Our study of recent systems
suggests refactoring them into a unified design space for graph matching
networks. Existing methods occupy only a few isolated patches in this space,
which remains largely uncharted. We undertake the first comprehensive
exploration of this space, featuring such axes as attention-based vs. soft
permutation-based interaction between query and corpus graphs, aligning nodes
vs. edges, and the form of the final scoring network that integrates neural
representations of the graphs. Our extensive experiments reveal that judicious
and hitherto-unexplored combinations of choices in this space lead to large
performance benefits. Beyond better performance, our study uncovers valuable
insights and establishes general design principles for neural graph
representation and interaction, which may be of wider interest.

</details>


### [306] [Simple Denoising Diffusion Language Models](https://arxiv.org/abs/2510.22926)
*Huaisheng Zhu,Zhengyu Chen,Shijie Zhou,Zhihui Xie,Yige Yuan,Zhimeng Guo,Siyuan Xu,Hangfan Zhang,Vasant Honavar,Teng Xiao*

Main category: cs.LG

TL;DR: 提出简化的基于去噪的损失函数优化USDMs，还引入对比负梯度改进去噪损失提升生成质量。


<details>
  <summary>Details</summary>
Motivation: MDLMs在少步情况下性能下降且无法采用现有少步蒸馏方法，USDMs损失函数复杂阻碍可扩展性。

Method: 为USDMs提出简化的基于去噪的损失函数，仅优化噪声替换的标记；将去噪构建为自监督学习，引入对比负梯度修改去噪损失。

Result: 稳定了训练，达到ELBO级别的性能，且提升了生成质量。

Conclusion: 提出的方法有效解决了现有模型的问题，提升了语言生成性能。

Abstract: Diffusion models have recently been extended to language generation through
Masked Diffusion Language Models (MDLMs), which achieve performance competitive
with strong autoregressive models. However, MDLMs tend to degrade in the
few-step regime and cannot directly adopt existing few-step distillation
methods designed for continuous diffusion models, as they lack the intrinsic
property of mapping from noise to data. Recent Uniform-state Diffusion Models
(USDMs), initialized from a uniform prior, alleviate some limitations but still
suffer from complex loss formulations that hinder scalability. In this work, we
propose a simplified denoising-based loss for USDMs that optimizes only
noise-replaced tokens, stabilizing training and matching ELBO-level
performance. Furthermore, by framing denoising as self-supervised learning, we
introduce a simple modification to our denoising loss with contrastive-inspired
negative gradients, which is practical and yield additional improvements in
generation quality.

</details>


### [307] [Diffuse to Detect: A Generalizable Framework for Anomaly Detection with Diffusion Models Applications to UAVs and Beyond](https://arxiv.org/abs/2510.22928)
*Mingze Gong,Juan Du,Jianbang You*

Main category: cs.LG

TL;DR: 提出Diffuse to Detect (DTD)框架用于复杂高维数据异常检测，性能优于现有方法，通用性强。


<details>
  <summary>Details</summary>
Motivation: 现有复杂高维数据异常检测方法灵敏度、可扩展性有限，难以捕捉复杂依赖关系，需新方法保障操作安全。

Method: 采用单步扩散过程预测噪声模式，结合图神经网络建模传感器关系，采用两分支架构平衡计算效率和透明度。

Result: 在无人机传感器数据、多元时间序列和图像上评估显示，DTD性能优于现有方法。

Conclusion: DTD通用性和适应性强，是安全关键应用的变革性解决方案。

Abstract: Anomaly detection in complex, high-dimensional data, such as UAV sensor
readings, is essential for operational safety but challenging for existing
methods due to their limited sensitivity, scalability, and inability to capture
intricate dependencies. We propose the Diffuse to Detect (DTD) framework, a
novel approach that innovatively adapts diffusion models for anomaly detection,
diverging from their conventional use in generative tasks with high inference
time. By comparison, DTD employs a single-step diffusion process to predict
noise patterns, enabling rapid and precise identification of anomalies without
reconstruction errors. This approach is grounded in robust theoretical
foundations that link noise prediction to the data distribution's score
function, ensuring reliable deviation detection. By integrating Graph Neural
Networks to model sensor relationships as dynamic graphs, DTD effectively
captures spatial (inter-sensor) and temporal anomalies. Its two-branch
architecture, with parametric neural network-based energy scoring for
scalability and nonparametric statistical methods for interpretability,
provides flexible trade-offs between computational efficiency and transparency.
Extensive evaluations on UAV sensor data, multivariate time series, and images
demonstrate DTD's superior performance over existing methods, underscoring its
generality across diverse data modalities. This versatility, combined with its
adaptability, positions DTD as a transformative solution for safety-critical
applications, including industrial monitoring and beyond.

</details>


### [308] [Robust Uncertainty Quantification for Self-Evolving Large Language Models via Continual Domain Pretraining](https://arxiv.org/abs/2510.22931)
*Xiaofan Zhou,Lu Cheng*

Main category: cs.LG

TL;DR: 本文提出自适应拒绝和非可交换CP框架，解决持续领域预训练中CP的问题，提升了其有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 持续学习对大语言模型很重要，但在持续领域预训练中缺乏对大语言模型统计可靠性保证的研究，且现有CP方法在该场景存在问题。

Method: 先使用基于Transformer的聚类估计测试集中跨领域问题的分布，再对校准数据重新加权或重采样，采用自适应拒绝CP让大语言模型在置信度或能力显著变化时选择性弃权。

Result: 广泛实验表明该框架提升了持续领域预训练场景下CP的有效性和可靠性。

Conclusion: 所提出的自适应拒绝和非可交换CP框架能有效解决持续领域预训练中CP面临的挑战。

Abstract: Continual Learning (CL) is essential for enabling self-evolving large
language models (LLMs) to adapt and remain effective amid rapid knowledge
growth. Yet, despite its importance, little attention has been given to
establishing statistical reliability guarantees for LLMs under CL, particularly
in the setting of continual domain pretraining (CDP). Conformal Prediction (CP)
has shown promise in offering correctness guarantees for LLMs, but it faces
major challenges in CDP: testing data often stems from unknown or shifting
domain distributions, under which CP may no longer provide valid guarantees.
Moreover, when high coverage is required, CP can yield excessively large
prediction sets for unanswerable queries, reducing informativeness. To address
these challenges, we introduce an adaptive rejection and non-exchangeable CP
framework. Our method first estimates the distribution of questions across
domains in the test set using transformer-based clustering, then reweights or
resamples the calibration data accordingly. Building on this, adaptive
rejection CP allows the LLM to selectively abstain from answering when its
confidence or competence shifts significantly. Extensive experiments
demonstrate that our framework enhances both the effectiveness and reliability
of CP under CDP scenarios. Our code is available at:
https://anonymous.4open.science/r/CPCL-8C12/

</details>


### [309] [RL-AUX: Reinforcement Learning for Auxiliary Task Generation](https://arxiv.org/abs/2510.22940)
*Judah Goldfeder,Matthew So,Hod Lipson*

Main category: cs.LG

TL;DR: 提出基于强化学习（RL）动态创建辅助任务的方法，避免双层优化，在20 - Superclass CIFAR100问题上表现出色，证明RL生成辅助任务可行且可学习样本级辅助任务权重。


<details>
  <summary>Details</summary>
Motivation: 辅助学习（AL）需有标签的辅助任务，生成这些任务需人力和专业知识，现有元学习技术依赖双层优化，存在计算成本和代码复杂度问题。

Method: 提出基于RL的方法，让RL智能体为训练集中每个数据点选择辅助标签，以主任务性能提升为奖励，还实验学习每个数据点辅助损失的最优权重策略。

Result: 在20 - Superclass CIFAR100问题上，RL方法优于人工标注辅助任务，与著名双层优化技术表现相当，权重学习方法显著优于所有基准，如Weight - Aware RL方法使VGG16测试准确率达80.9%。

Conclusion: 证明RL是动态生成辅助任务的可行方法，且可同时学习每个样本的辅助任务权重并取得良好效果。

Abstract: Auxiliary Learning (AL) is a special case of Multi-task Learning (MTL) in
which a network trains on auxiliary tasks to improve performance on its main
task. This technique is used to improve generalization and, ultimately,
performance on the network's main task. AL has been demonstrated to improve
performance across multiple domains, including navigation, image
classification, and natural language processing. One weakness of AL is the need
for labeled auxiliary tasks, which can require human effort and domain
expertise to generate. Meta Learning techniques have been used to solve this
issue by learning an additional auxiliary task generation network that can
create helpful tasks for the primary network. The most prominent techniques
rely on Bi-Level Optimization, which incurs computational cost and increased
code complexity. To avoid the need for Bi-Level Optimization, we present an
RL-based approach to dynamically create auxiliary tasks. In this framework, an
RL agent is tasked with selecting auxiliary labels for every data point in a
training set. The agent is rewarded when their selection improves the
performance on the primary task. We also experiment with learning optimal
strategies for weighing the auxiliary loss per data point. On the 20-Superclass
CIFAR100 problem, our RL approach outperforms human-labeled auxiliary tasks and
performs as well as a prominent Bi-Level Optimization technique. Our weight
learning approaches significantly outperform all of these benchmarks. For
example, a Weight-Aware RL-based approach helps the VGG16 architecture achieve
80.9% test accuracy while the human-labeled auxiliary task setup achieved
75.53%. The goal of this work is to (1) prove that RL is a viable approach to
dynamically generate auxiliary tasks and (2) demonstrate that per-sample
auxiliary task weights can be learned alongside the auxiliary task labels and
can achieve strong results.

</details>


### [310] [Hazard-Responsive Digital Twin for Climate-Driven Urban Resilience and Equity](https://arxiv.org/abs/2510.22941)
*Zhenglai Shen,Hongyu Zhou*

Main category: cs.LG

TL;DR: 提出H - RDT应对复合气候危害，在合成区模拟测试有效，干预措施降低风险，框架可用于真实城市，推动城市数字韧性发展。


<details>
  <summary>Details</summary>
Motivation: 复合气候危害挑战城市稳定性和公平性，需要有效应对方法。

Method: 结合物理信息神经网络建模、多模态数据融合和公平性风险分析构建H - RDT。

Result: 在合成区模拟中，H - RDT能维持室内温度预测稳定，融合模块维持时空覆盖，定位高脆弱性集群，干预措施降低多种风险。

Conclusion: H - RDT为气候适应提供以公平为中心的决策支持，建立了可转移的真实城市实施基础。

Abstract: Compounding climate hazards, such as wildfire-induced outages and urban
heatwaves, challenge the stability and equity of cities. We present a
Hazard-Responsive Digital Twin (H-RDT) that combines physics-informed neural
network modeling, multimodal data fusion, and equity-aware risk analytics for
urban-scale response. In a synthetic district with diverse building archetypes
and populations, a simulated wildfire-outage-heatwave cascade shows that H-RDT
maintains stable indoor temperature predictions (approximately 31 to 33 C)
under partial sensor loss, reproducing outage-driven surges and recovery. The
reinforcement learning based fusion module adaptively reweights IoT, UAV, and
satellite inputs to sustain spatiotemporal coverage, while the equity-adjusted
mapping isolates high-vulnerability clusters (schools, clinics, low-income
housing). Prospective interventions, such as preemptive cooling-center
activation and microgrid sharing, reduce population-weighted thermal risk by 11
to 13 percent, shrink the 95th-percentile (tail) risk by 7 to 17 percent, and
cut overheating hours by up to 9 percent. Beyond the synthetic demonstration,
the framework establishes a transferable foundation for real-city
implementation, linking physical hazard modeling with social equity and
decision intelligence. The H-RDT advances digital urban resilience toward
adaptive, learning-based, and equity-centered decision support for climate
adaptation.

</details>


### [311] [Hankel Singular Value Regularization for Highly Compressible State Space Models](https://arxiv.org/abs/2510.22951)
*Paul Schwerdtner,Jules Berman,Benjamin Peherstorfer*

Main category: cs.LG

TL;DR: 提出正则化Hankel奇异值使状态空间模型可压缩，并开发算法，实验显示正则化层比标准层压缩性高10倍且精度高。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络中状态空间模型层适合长序列任务，但训练后压缩有挑战。

Method: 正则化状态空间模型的Hankel奇异值之和，开发算法利用系统矩阵块对角结构在训练迭代中高效计算Hankel奇异值。

Result: 在Long Range Arena基准测试中，正则化状态空间层比标准层压缩性高10倍，且保持高精度。

Conclusion: 正则化Hankel奇异值可使状态空间模型更易压缩，同时保证模型精度。

Abstract: Deep neural networks using state space models as layers are well suited for
long-range sequence tasks but can be challenging to compress after training. We
use that regularizing the sum of Hankel singular values of state space models
leads to a fast decay of these singular values and thus to compressible models.
To make the proposed Hankel singular value regularization scalable, we develop
an algorithm to efficiently compute the Hankel singular values during training
iterations by exploiting the specific block-diagonal structure of the system
matrices that is we use in our state space model parametrization. Experiments
on Long Range Arena benchmarks demonstrate that the regularized state space
layers are up to 10$\times$ more compressible than standard state space layers
while maintaining high accuracy.

</details>


### [312] [SARNet: A Spike-Aware consecutive validation Framework for Accurate Remaining Useful Life Prediction](https://arxiv.org/abs/2510.22955)
*Junhao Fan,Wenrui Liang,Wei-Qiang Zhang*

Main category: cs.LG

TL;DR: 本文提出SARNet预测剩余使用寿命，降低误差且易于部署。


<details>
  <summary>Details</summary>
Motivation: 现有模型在故障发生时脆弱且对工程师不透明，需改进。

Method: 基于ModernTCN构建SARNet，添加尖峰感知检测，用自适应连续阈值验证尖峰、抑制噪声，对易故障段特征工程，用堆叠RF - LGBM回归器预测。

Result: 在基准数据集上，SARNet相比基线模型降低误差（RMSE 0.0365，MAE 0.0204），且轻量、稳健、易部署。

Conclusion: SARNet在RUL预测上表现良好，是有效、实用的方法。

Abstract: Accurate prediction of remaining useful life (RUL) is essential to enhance
system reliability and reduce maintenance risk. Yet many strong contemporary
models are fragile around fault onset and opaque to engineers: short,
high-energy spikes are smoothed away or misread, fixed thresholds blunt
sensitivity, and physics-based explanations are scarce. To remedy this, we
introduce SARNet (Spike-Aware Consecutive Validation Framework), which builds
on a Modern Temporal Convolutional Network (ModernTCN) and adds spike-aware
detection to provide physics-informed interpretability. ModernTCN forecasts
degradation-sensitive indicators; an adaptive consecutive threshold validates
true spikes while suppressing noise. Failure-prone segments then receive
targeted feature engineering (spectral slopes, statistical derivatives, energy
ratios), and the final RUL is produced by a stacked RF--LGBM regressor. Across
benchmark-ported datasets under an event-triggered protocol, SARNet
consistently lowers error compared to recent baselines (RMSE 0.0365, MAE
0.0204) while remaining lightweight, robust, and easy to deploy.

</details>


### [313] [The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination](https://arxiv.org/abs/2510.22977)
*Chenlong Yin,Zeyang Sha,Shiwen Cui,Changhua Meng*

Main category: cs.LG

TL;DR: 研究强化大语言模型推理能力是否会增加工具幻觉，引入诊断基准测试，发现推理增强会增加幻觉，且缓解策略存在可靠性 - 能力权衡，需新训练目标。


<details>
  <summary>Details</summary>
Motivation: 先前研究未系统检验推理增强是否会导致工具幻觉，本文旨在填补这一空白。

Method: 引入SimpleToolHalluBench诊断基准测试，通过控制实验研究强化推理与工具幻觉的关系，还评估了缓解策略。

Result: 发现强化推理与工具幻觉有因果关系，该效应不局限于过拟合，且与方法无关；缓解策略存在可靠性 - 能力权衡；推理增强会使工具可靠性相关表征崩溃。

Conclusion: 当前推理增强方法会放大工具幻觉，需要能同时优化能力和可靠性的新训练目标。

Abstract: Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key
strategy for building Agents that "think then act." However, recent
observations, like OpenAI's o3, suggest a paradox: stronger reasoning often
coincides with increased hallucination, yet no prior work has systematically
examined whether reasoning enhancement itself causes tool hallucination. To
address this gap, we pose the central question: Does strengthening reasoning
increase tool hallucination? To answer this, we introduce SimpleToolHalluBench,
a diagnostic benchmark measuring tool hallucination in two failure modes: (i)
no tool available, and (ii) only distractor tools available. Through controlled
experiments, we establish three key findings. First, we demonstrate a causal
relationship: progressively enhancing reasoning through RL increases tool
hallucination proportionally with task performance gains. Second, this effect
transcends overfitting - training on non-tool tasks (e.g., mathematics) still
amplifies subsequent tool hallucination. Third, the effect is method-agnostic,
appearing when reasoning is instilled via supervised fine-tuning and when it is
merely elicited at inference by switching from direct answers to step-by-step
thinking. We also evaluate mitigation strategies including Prompt Engineering
and Direct Preference Optimization (DPO), revealing a fundamental
reliability-capability trade-off: reducing hallucination consistently degrades
utility. Mechanistically, Reasoning RL disproportionately collapses
tool-reliability-related representations, and hallucinations surface as
amplified divergences concentrated in late-layer residual streams. These
findings reveal that current reasoning enhancement methods inherently amplify
tool hallucination, highlighting the need for new training objectives that
jointly optimize for capability and reliability.

</details>


### [314] [QoSGMAA: A Robust Multi-Order Graph Attention and Adversarial Framework for Sparse QoS Prediction](https://arxiv.org/abs/2510.22982)
*Guanchen Du,Jianlong Xu,Mingtong Li,Ruiqi Wang,Qianqing Guo,Caiyi Chen,Qingcao Dai,Yuxiang Zeng*

Main category: cs.LG

TL;DR: 提出QoSMGAA架构提升网络服务QoS预测精度，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 网络服务数量增长使选择最优服务困难，准确预测QoS重要，但现有方法在复杂环境表现不佳。

Method: 提出QoSMGAA架构，集成多阶注意力机制聚合上下文数据，用对抗神经网络进行自回归监督学习，利用Gumbel - Softmax方法生成负样本。

Result: 在大规模真实数据集上实验，模型显著优于现有基线方法。

Conclusion: QoSMGAA架构在服务选择和推荐场景有很强的实际部署潜力。

Abstract: With the rapid advancement of internet technologies, network services have
become critical for delivering diverse and reliable applications to users.
However, the exponential growth in the number of available services has
resulted in many similar offerings, posing significant challenges in selecting
optimal services. Predicting Quality of Service (QoS) accurately thus becomes a
fundamental prerequisite for ensuring reliability and user satisfaction.
However, existing QoS prediction methods often fail to capture rich contextual
information and exhibit poor performance under extreme data sparsity and
structural noise. To bridge this gap, we propose a novel architecture, QoSMGAA,
specifically designed to enhance prediction accuracy in complex and noisy
network service environments. QoSMGAA integrates a multi-order attention
mechanism to aggregate extensive contextual data and predict missing QoS values
effectively. Additionally, our method incorporates adversarial neural networks
to perform autoregressive supervised learning based on transformed interaction
matrices. To capture complex, higher-order interactions among users and
services, we employ a discrete sampling technique leveraging the Gumbel-Softmax
method to generate informative negative samples. Comprehensive experimental
validation conducted on large-scale real-world datasets demonstrates that our
proposed model significantly outperforms existing baseline methods,
highlighting its strong potential for practical deployment in service selection
and recommendation scenarios.

</details>


### [315] [Can Language Models Compose Skills In-Context?](https://arxiv.org/abs/2510.22993)
*Zidong Liu,Zhuoyan Xu,Zhenmei Shi,Yingyu Liang*

Main category: cs.LG

TL;DR: 研究语言模型上下文组合能力，实验发现简单示例有负面影响，需示例与组合步骤对齐。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在上下文示例中组合基本技能完成复合任务的能力，此比标准设置更具挑战性。

Method: 对多种开源语言模型进行系统实验，利用语言和逻辑任务探测组合能力。

Result: 简单任务示例对性能有意外负面影响，模型难正确识别和组合技能，理论分析表明示例需与组合步骤对齐。

Conclusion: 提出探测任务方法，性能提升支持了相关见解。

Abstract: Composing basic skills from simple tasks to accomplish composite tasks is
crucial for modern intelligent systems. We investigate the in-context
composition ability of language models to perform composite tasks that combine
basic skills demonstrated in in-context examples. This is more challenging than
the standard setting, where skills and their composition can be learned in
training. We conduct systematic experiments on various representative
open-source language models, utilizing linguistic and logical tasks designed to
probe composition abilities. The results reveal that simple task examples can
have a surprising negative impact on the performance, because the models
generally struggle to recognize and assemble the skills correctly, even with
Chain-of-Thought examples. Theoretical analysis further shows that it is
crucial to align examples with the corresponding steps in the composition. This
inspires a method for the probing tasks, whose improved performance provides
positive support for our insights.

</details>


### [316] [Softmax is $1/2$-Lipschitz: A tight bound across all $\ell_p$ norms](https://arxiv.org/abs/2510.23012)
*Pravin Nair*

Main category: cs.LG

TL;DR: 本文证明softmax函数在所有p≥1的ℓ_p范数下Lipschitz常数为1/2，改进相关理论结果并实证验证。


<details>
  <summary>Details</summary>
Motivation: 现有学习模型鲁棒性保证和优化算法收敛分析通常认为softmax算子在ℓ_2范数下Lipschitz常数为1，本文进行更全面分析。

Method: 理论证明softmax函数在不同ℓ_p范数下的Lipschitz常数，进行实证研究。

Result: 证明softmax函数在所有p≥1的ℓ_p范数下Lipschitz常数为1/2，局部Lipschitz常数在特定p值情况，改进现有理论结果，实证验证常数尖锐性。

Conclusion: 这是首次对softmax Lipschitz连续性进行全面的范数统一分析，更尖锐的常数能改进相关理论结果。

Abstract: The softmax function is a basic operator in machine learning and
optimization, used in classification, attention mechanisms, reinforcement
learning, game theory, and problems involving log-sum-exp terms. Existing
robustness guarantees of learning models and convergence analysis of
optimization algorithms typically consider the softmax operator to have a
Lipschitz constant of $1$ with respect to the $\ell_2$ norm. In this work, we
prove that the softmax function is contractive with the Lipschitz constant
$1/2$, uniformly across all $\ell_p$ norms with $p \ge 1$. We also show that
the local Lipschitz constant of softmax attains $1/2$ for $p = 1$ and $p =
\infty$, and for $p \in (1,\infty)$, the constant remains strictly below $1/2$
and the supremum $1/2$ is achieved only in the limit. To our knowledge, this is
the first comprehensive norm-uniform analysis of softmax Lipschitz continuity.
We demonstrate how the sharper constant directly improves a range of existing
theoretical results on robustness and convergence. We further validate the
sharpness of the $1/2$ Lipschitz constant of the softmax operator through
empirical studies on attention-based architectures (ViT, GPT-2, Qwen3-8B) and
on stochastic policies in reinforcement learning.

</details>


### [317] [MoEMeta: Mixture-of-Experts Meta Learning for Few-Shot Relational Learning](https://arxiv.org/abs/2510.23013)
*Han Wu,Jie Yin*

Main category: cs.LG

TL;DR: 提出MoEMeta框架解决少样本知识图谱关系学习现存方法的局限，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有少样本知识图谱关系学习方法在学习关系元知识时孤立，且难以融入特定任务上下文。

Method: 提出MoEMeta框架，包含学习全局共享关系原型的混合专家模型和捕捉局部上下文的任务定制适应机制。

Result: 在三个KG基准上的实验显示MoEMeta始终优于现有基线，达到了最先进的性能。

Conclusion: MoEMeta通过平衡全局泛化和局部适应性，显著推动了少样本关系学习。

Abstract: Few-shot knowledge graph relational learning seeks to perform reasoning over
relations given only a limited number of training examples. While existing
approaches largely adopt a meta-learning framework for enabling fast adaptation
to new relations, they suffer from two key pitfalls. First, they learn relation
meta-knowledge in isolation, failing to capture common relational patterns
shared across tasks. Second, they struggle to effectively incorporate local,
task-specific contexts crucial for rapid adaptation. To address these
limitations, we propose MoEMeta, a novel meta-learning framework that
disentangles globally shared knowledge from task-specific contexts to enable
both effective generalization and rapid adaptation. MoEMeta introduces two key
innovations: (i) a mixture-of-experts (MoE) model that learns globally shared
relational prototypes to enhance generalization, and (ii) a task-tailored
adaptation mechanism that captures local contexts for fast task-specific
adaptation. By balancing global generalization with local adaptability, MoEMeta
significantly advances few-shot relational learning. Extensive experiments and
analyses on three KG benchmarks demonstrate that MoEMeta consistently
outperforms existing baselines, achieving state-of-the-art performance.

</details>


### [318] [Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts](https://arxiv.org/abs/2510.23027)
*Di Zhang,Xun Wu,Shaohan Huang,Yaru Hao,Li Dong,Zewen Chi,Zhifang Sui,Furu Wei*

Main category: cs.LG

TL;DR: 现有研究多关注强化学习训练大语言模型的稠密模型，本文提出针对混合专家（MoE）架构在离策略强化学习中优化重要性采样权重的路由感知方法，实验显示能提升模型稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注稠密模型，MoE架构的强化学习训练未充分探索，且MoE训练存在不稳定性。

Method: 提出路由感知方法优化离策略强化学习中的重要性采样权重，设计基于路由对数的重缩放策略。

Result: 该方法显著提高了MoE模型的收敛稳定性和最终性能。

Conclusion: 针对MoE架构的强化学习算法创新有潜力，为大规模专家模型的高效训练提供了有前景的方向。

Abstract: Recent advances in reinforcement learning (RL) have substantially improved
the training of large-scale language models, leading to significant gains in
generation quality and reasoning ability. However, most existing research
focuses on dense models, while RL training for Mixture-of-Experts (MoE)
architectures remains underexplored. To address the instability commonly
observed in MoE training, we propose a novel router-aware approach to optimize
importance sampling (IS) weights in off-policy RL. Specifically, we design a
rescaling strategy guided by router logits, which effectively reduces gradient
variance and mitigates training divergence. Experimental results demonstrate
that our method significantly improves both the convergence stability and the
final performance of MoE models, highlighting the potential of RL algorithmic
innovations tailored to MoE architectures and providing a promising direction
for efficient training of large-scale expert models.

</details>


### [319] [LLM Meets Diffusion: A Hybrid Framework for Crystal Material Generation](https://arxiv.org/abs/2510.23040)
*Subhojyoti Khastagir,Kishalay Das,Pawan Goyal,Seung-Cheol Lee,Satadeep Bhattacharjee,Niloy Ganguly*

Main category: cs.LG

TL;DR: 提出CrysLLMGen混合框架用于晶体材料生成，结合大语言模型和扩散模型优势，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有晶体结构设计方法中，大语言模型处理连续特征、去噪模型生成准确原子组成存在困难，需结合二者优势。

Method: 提出CrysLLMGen框架，采样时先用微调大语言模型生成中间表示，保留原子类型，将坐标和晶格结构传给预训练等变扩散模型细化。

Result: 在多个基准任务和数据集上表现优于现有模型，实现结构和组成有效性平衡，生成更稳定和新颖材料，有强条件生成能力。

Conclusion: CrysLLMGen框架结合大语言模型和扩散模型优势，能有效生成晶体材料，有良好应用前景。

Abstract: Recent advances in generative modeling have shown significant promise in
designing novel periodic crystal structures. Existing approaches typically rely
on either large language models (LLMs) or equivariant denoising models, each
with complementary strengths: LLMs excel at handling discrete atomic types but
often struggle with continuous features such as atomic positions and lattice
parameters, while denoising models are effective at modeling continuous
variables but encounter difficulties in generating accurate atomic
compositions. To bridge this gap, we propose CrysLLMGen, a hybrid framework
that integrates an LLM with a diffusion model to leverage their complementary
strengths for crystal material generation. During sampling, CrysLLMGen first
employs a fine-tuned LLM to produce an intermediate representation of atom
types, atomic coordinates, and lattice structure. While retaining the predicted
atom types, it passes the atomic coordinates and lattice structure to a
pre-trained equivariant diffusion model for refinement. Our framework
outperforms state-of-the-art generative models across several benchmark tasks
and datasets. Specifically, CrysLLMGen not only achieves a balanced performance
in terms of structural and compositional validity but also generates more
stable and novel materials compared to LLM-based and denoisingbased models
Furthermore, CrysLLMGen exhibits strong conditional generation capabilities,
effectively producing materials that satisfy user-defined constraints. Code is
available at https://github.com/kdmsit/crysllmgen

</details>


### [320] [Advantage Shaping as Surrogate Reward Maximization: Unifying Pass@K Policy Gradients](https://arxiv.org/abs/2510.23049)
*Christos Thrampoulidis,Sadegh Mahdavi,Wenlong Deng*

Main category: cs.LG

TL;DR: 本文调和强化学习中针对Pass@K目标的两种策略梯度优化方法，揭示联系并提供推导新方法的思路。


<details>
  <summary>Details</summary>
Motivation: 调和强化学习中针对Pass@K目标的直接REINFORCE风格方法和直接修改GRPO的优势塑造技术这两种看似不同的策略梯度优化方法。

Method: 反向工程现有优势塑造算法，从替代奖励目标出发推导方法。

Result: 发现两种方法是同一事物的两面，优势塑造算法隐式优化替代奖励，将GRPO的“难例加权”修改解释为奖励级正则化。

Conclusion: 该视角为RLVR策略梯度优化提供了新视角，不限于Pass@K的初衷。

Abstract: This note reconciles two seemingly distinct approaches to policy gradient
optimization for the Pass@K objective in reinforcement learning with verifiable
rewards: (1) direct REINFORCE-style methods, and (2) advantage-shaping
techniques that directly modify GRPO. We show that these are two sides of the
same coin. By reverse-engineering existing advantage-shaping algorithms, we
reveal that they implicitly optimize surrogate rewards. We specifically
interpret practical ``hard-example up-weighting'' modifications to GRPO as
reward-level regularization. Conversely, starting from surrogate reward
objectives, we provide a simple recipe for deriving both existing and new
advantage-shaping methods. This perspective provides a lens for RLVR policy
gradient optimization beyond our original motivation of Pass@K.

</details>


### [321] [SwiftTS: A Swift Selection Framework for Time Series Pre-trained Models via Multi-task Meta-Learning](https://arxiv.org/abs/2510.23051)
*Tengxue Zhang,Biao Ouyang,Yang Shu,Xinyang Chen,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 提出SwiftTS框架用于快速选择时间序列预训练模型，实验表明其在模型选择上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 模型中心模型众多，逐个微调选择合适模型耗时，需快速选择框架。

Method: 采用学习引导方法，利用历史数据集 - 模型性能对预测模型在未见数据集上的性能；使用轻量级双编码器架构计算兼容性得分；引入自适应专家组合模块和可迁移的跨任务学习增强泛化性和鲁棒性。

Result: 在14个下游数据集和8个预训练模型上的实验显示，SwiftTS在时间序列预训练模型选择中达到了SOTA性能。

Conclusion: SwiftTS是一个有效的时间序列预训练模型快速选择框架。

Abstract: Pre-trained models exhibit strong generalization to various downstream tasks.
However, given the numerous models available in the model hub, identifying the
most suitable one by individually fine-tuning is time-consuming. In this paper,
we propose \textbf{SwiftTS}, a swift selection framework for time series
pre-trained models. To avoid expensive forward propagation through all
candidates, SwiftTS adopts a learning-guided approach that leverages historical
dataset-model performance pairs across diverse horizons to predict model
performance on unseen datasets. It employs a lightweight dual-encoder
architecture that embeds time series and candidate models with rich
characteristics, computing patchwise compatibility scores between data and
model embeddings for efficient selection. To further enhance the generalization
across datasets and horizons, we introduce a horizon-adaptive expert
composition module that dynamically adjusts expert weights, and the
transferable cross-task learning with cross-dataset and cross-horizon task
sampling to enhance out-of-distribution (OOD) robustness. Extensive experiments
on 14 downstream datasets and 8 pre-trained models demonstrate that SwiftTS
achieves state-of-the-art performance in time series pre-trained model
selection.

</details>


### [322] [Sampling from Energy distributions with Target Concrete Score Identity](https://arxiv.org/abs/2510.23106)
*Sergei Kholkin,Francisco Vargas,Alexander Korotin*

Main category: cs.LG

TL;DR: 提出Target Concrete Score Identity Sampler (TCSIS)方法，用于从离散状态空间的未归一化密度中采样，并在统计物理问题中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决从离散状态空间的未归一化密度中采样的问题。

Method: 通过学习连续时间马尔可夫链（CTMC）的反向动力学，利用目标具体得分恒等式，用神经网络近似具体得分，并提出Self - Normalized TCSIS和Unbiased TCSIS两种算法。

Result: 在统计物理问题上证明了TCSIS的有效性。

Conclusion: TCSIS是一种有效的从离散状态空间的未归一化密度中采样的方法。

Abstract: We introduce the Target Concrete Score Identity Sampler (TCSIS), a method for
sampling from unnormalized densities on discrete state spaces by learning the
reverse dynamics of a Continuous-Time Markov Chain (CTMC). Our approach builds
on a forward in time CTMC with a uniform noising kernel and relies on the
proposed Target Concrete Score Identity, which relates the concrete score, the
ratio of marginal probabilities of two states, to a ratio of expectations of
Boltzmann factors under the forward uniform diffusion kernel. This formulation
enables Monte Carlo estimation of the concrete score without requiring samples
from the target distribution or computation of the partition function. We
approximate the concrete score with a neural network and propose two
algorithms: Self-Normalized TCSIS and Unbiased TCSIS. Finally, we demonstrate
the effectiveness of TCSIS on problems from statistical physics.

</details>


### [323] [Neural Emulator Superiority: When Machine Learning for PDEs Surpasses its Training Data](https://arxiv.org/abs/2510.23111)
*Felix Koehler,Nils Thuerey*

Main category: cs.LG

TL;DR: 本文挑战了神经算子受训练数据保真度限制的传统假设，发现‘模拟器优越性’，并进行理论分析和实证验证，促使重新评估模拟器基准。


<details>
  <summary>Details</summary>
Motivation: 挑战神经算子受训练数据保真度限制的传统假设。

Method: 进行理论分析，揭示模拟器归纳偏差、训练目标和数值误差特征间的相互作用；使用标准神经架构在不同PDEs上进行实证验证。

Result: 证明了纯低保真求解器数据训练的神经网络在多步预测中能比求解器更准确，模拟器可隐式学习比训练数据更优的动力学。

Conclusion: 提示在特定操作范围内，神经模拟器可能比训练源具有更高的物理保真度，需重新评估模拟器基准。

Abstract: Neural operators or emulators for PDEs trained on data from numerical solvers
are conventionally assumed to be limited by their training data's fidelity. We
challenge this assumption by identifying "emulator superiority," where neural
networks trained purely on low-fidelity solver data can achieve higher accuracy
than those solvers when evaluated against a higher-fidelity reference. Our
theoretical analysis reveals how the interplay between emulator inductive
biases, training objectives, and numerical error characteristics enables
superior performance during multi-step rollouts. We empirically validate this
finding across different PDEs using standard neural architectures,
demonstrating that emulators can implicitly learn dynamics that are more
regularized or exhibit more favorable error accumulation properties than their
training data, potentially surpassing training data limitations and mitigating
numerical artifacts. This work prompts a re-evaluation of emulator
benchmarking, suggesting neural emulators might achieve greater physical
fidelity than their training source within specific operational regimes.
Project Page: https://tum-pbs.github.io/emulator-superiority

</details>


### [324] [Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction](https://arxiv.org/abs/2510.23117)
*Omer Jauhar Khan,Sudais Khan,Hafeez Anwar*

Main category: cs.LG

TL;DR: 本文探索用PINNs预测小规模意大利面桥重量，提出PIKAN架构，模型表现良好，还提供网页界面，表明PINNs在有限数据下能可靠估计结构重量。


<details>
  <summary>Details</summary>
Motivation: PINNs可将物理定律嵌入深度学习模型，在有限数据的结构工程任务中有用，本文旨在用其预测小规模意大利面桥重量，了解简化结构模型的负载极限和潜在失效模式。

Method: 提出的框架将基于物理的约束纳入预测模型；引入PIKAN架构，融合通用函数逼近理论和物理见解；通过手动或计算机视觉方法收集模型输入的结构参数。

Result: 数据集含15座真实桥梁并扩充至100个样本，最佳模型的$R^2$分数为0.9603，平均绝对误差为10.50单位；提供基于网络的参数输入和预测界面。

Conclusion: PINNs即使在有限数据下也能对结构重量提供可靠估计，有助于轻型桥梁设计的早期失效分析。

Abstract: Physics Informed Neural Networks (PINNs) are gaining attention for their
ability to embed physical laws into deep learning models, which is particularly
useful in structural engineering tasks with limited data. This paper aims to
explore the use of PINNs to predict the weight of small scale spaghetti
bridges, a task relevant to understanding load limits and potential failure
modes in simplified structural models. Our proposed framework incorporates
physics-based constraints to the prediction model for improved performance. In
addition to standard PINNs, we introduce a novel architecture named Physics
Informed Kolmogorov Arnold Network (PIKAN), which blends universal function
approximation theory with physical insights. The structural parameters provided
as input to the model are collected either manually or through computer vision
methods. Our dataset includes 15 real bridges, augmented to 100 samples, and
our best model achieves an $R^2$ score of 0.9603 and a mean absolute error
(MAE) of 10.50 units. From applied perspective, we also provide a web based
interface for parameter entry and prediction. These results show that PINNs can
offer reliable estimates of structural weight, even with limited data, and may
help inform early stage failure analysis in lightweight bridge designs.
  The complete data and code are available at
https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.

</details>


### [325] [A method for outlier detection based on cluster analysis and visual expert criteria](https://arxiv.org/abs/2510.23136)
*Juan A. Lara,David Lizcano,Víctor Rampérez,Javier Soriano*

Main category: cs.LG

TL;DR: 本文提出基于聚类过程的异常值检测方法，在两个领域数据上评估，结果显示该方法能有效检测不同领域异常值，误报率低且可靠性高。


<details>
  <summary>Details</summary>
Motivation: 许多现有异常值检测技术未考虑领域对象的固有分散性，本文旨在克服这一局限性。

Method: 提出基于聚类过程的异常值检测方法，依据四个标准，模拟人类专家分析聚类后识别异常值。

Result: 在两个不同领域数据上评估，回归分析结果表明该方法误报率低于2%，可靠性高于99%。

Conclusion: 所提方法对不同领域的异常值检测有用。

Abstract: Outlier detection is an important problem occurring in a wide range of areas.
Outliers are the outcome of fraudulent behaviour, mechanical faults, human
error, or simply natural deviations. Many data mining applications perform
outlier detection, often as a preliminary step in order to filter out outliers
and build more representative models. In this paper, we propose an outlier
detection method based on a clustering process. The aim behind the proposal
outlined in this paper is to overcome the specificity of many existing outlier
detection techniques that fail to take into account the inherent dispersion of
domain objects. The outlier detection method is based on four criteria designed
to represent how human beings (experts in each domain) visually identify
outliers within a set of objects after analysing the clusters. This has an
advantage over other clustering-based outlier detection techniques that are
founded on a purely numerical analysis of clusters. Our proposal has been
evaluated, with satisfactory results, on data (particularly time series) from
two different domains: stabilometry, a branch of medicine studying
balance-related functions in human beings and electroencephalography (EEG), a
neurological exploration used to diagnose nervous system disorders. To validate
the proposed method, we studied method outlier detection and efficiency in
terms of runtime. The results of regression analyses confirm that our proposal
is useful for detecting outlier data in different domains, with a false
positive rate of less than 2% and a reliability greater than 99%.

</details>


### [326] [Rethinking GSPO: The Perplexity-Entropy Equivalence](https://arxiv.org/abs/2510.23142)
*Chi Liu*

Main category: cs.LG

TL;DR: 本文建立GSPO长度归一化重要性比率与信息论量的联系，给出等价表达式，提供信息论视角理解GSPO，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 为GSPO的长度归一化重要性比率提供新视角，帮助理解GSPO算法的经验性质。

Method: 建立GSPO序列级权重与逆困惑度比率、指数交叉熵变化的等价关系，通过控制实验验证数学等价性和方差预测。

Result: 发现GSPO序列级权重可等价表示为逆困惑度比率和指数交叉熵变化，该视角能解释GSPO的经验性质。

Conclusion: 信息论视角为理解GSPO的重要性权重提供了有用的见解，实验验证了数学关系和方差预测。

Abstract: We provide a new perspective on GSPO's length-normalized importance ratios by
establishing their connection to information-theoretic quantities. We show that
GSPO's sequence-level weight $s(\theta) =
(\pi_\theta/\pi_{\theta_{\text{old}}})^{1/|y|}$ can be equivalently expressed
as the inverse perplexity ratio
$\text{PPL}_{\theta_{\text{old}}}/\text{PPL}_\theta$ and as the exponential
cross-entropy change $\exp(\Delta H)$. While the perplexity-entropy
relationship follows from standard definitions, this observation provides a
useful lens for understanding GSPO: the algorithm weights policy gradient
updates by perplexity ratios, offering an information-theoretic interpretation
of the importance weights. This perspective helps explain GSPO's empirical
properties, including log-domain variance reduction through geometric averaging
and stability in training mixture-of-experts models. We validate the
mathematical equivalences and variance predictions through controlled
experiments on mathematical reasoning tasks.

</details>


### [327] [Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI](https://arxiv.org/abs/2510.23148)
*Aryan Mathur,Asaduddin Ahmed*

Main category: cs.LG

TL;DR: 本文实现PDiT架构并集成对比损失，在BabyAI GoToLocal环境评估，结果表明交错变压器编码器是开发集成自主代理的有前途方向。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习代理在需要理解视觉和语言的任务中面临的问题，传统架构将感知和决策分离效率低。

Method: 实现PDiT架构，使感知和决策层在单个变压器中交替；集成受CLIP启发的对比损失，对齐文本任务嵌入和视觉场景特征。

Result: 在BabyAI GoToLocal环境评估，该方法比标准PPO基线获得更稳定奖励和更强对齐。

Conclusion: 交错变压器编码器是开发更集成自主代理的有前途方向。

Abstract: Deep reinforcement learning agents often struggle when tasks require
understanding both vision and language. Conventional architectures typically
isolate perception (for example, CNN-based visual encoders) from
decision-making (policy networks). This separation can be inefficient, since
the policy's failures do not directly help the perception module learn what is
important. To address this, we implement the Perception-Decision Interleaving
Transformer (PDiT) architecture introduced by Mao et al. (2023), a model that
alternates between perception and decision layers within a single transformer.
This interleaving allows feedback from decision-making to refine perceptual
features dynamically. In addition, we integrate a contrastive loss inspired by
CLIP to align textual mission embeddings with visual scene features. We
evaluate the PDiT encoders on the BabyAI GoToLocal environment and find that
the approach achieves more stable rewards and stronger alignment compared to a
standard PPO baseline. The results suggest that interleaved transformer
encoders are a promising direction for developing more integrated autonomous
agents.

</details>


### [328] [Enabling Vibration-Based Gesture Recognition on Everyday Furniture via Energy-Efficient FPGA Implementation of 1D Convolutional Networks](https://arxiv.org/abs/2510.23156)
*Koki Shibata,Tianheng Ling,Chao Qian,Tomokazu Matsui,Hirohiko Suwa,Keiichi Yasumoto,Gregor Schiele*

Main category: cs.LG

TL;DR: 本文提出在低功耗FPGA上部署紧凑型神经网络实现实时手势识别的节能方案，通过系列优化达成低延迟、节能推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于振动的手势识别方法依赖复杂预处理和大型神经网络，能耗高且难以在现实部署，需节能解决方案。

Method: 采用原始波形输入替代复杂频谱预处理；设计适合嵌入式FPGA的轻量级架构；进行整数量化和自动RTL生成；扩展硬件感知搜索框架支持模型配置选择。

Result: 在两个数据集上，6 - bit 1D - CNN平均准确率达0.970，延迟9.22 ms；8 - bit 1D - SepCNN延迟降至6.83 ms，准确率0.949，每次推理能耗低于1.2 mJ。

Conclusion: 该方案适合长期边缘计算操作。

Abstract: The growing demand for smart home interfaces has increased interest in
non-intrusive sensing methods like vibration-based gesture recognition. While
prior studies demonstrated feasibility, they often rely on complex
preprocessing and large Neural Networks (NNs) requiring costly high-performance
hardware, resulting in high energy usage and limited real-world deployability.
This study proposes an energy-efficient solution deploying compact NNs on
low-power Field-Programmable Gate Arrays (FPGAs) to enable real-time gesture
recognition with competitive accuracy. We adopt a series of optimizations: (1)
We replace complex spectral preprocessing with raw waveform input, eliminating
complex on-board preprocessing while reducing input size by 21x without
sacrificing accuracy. (2) We design two lightweight architectures (1D-CNN and
1D-SepCNN) tailored for embedded FPGAs, reducing parameters from 369 million to
as few as 216 while maintaining comparable accuracy. (3) With integer-only
quantization and automated RTL generation, we achieve seamless FPGA deployment.
A ping-pong buffering mechanism in 1D-SepCNN further improves deployability
under tight memory constraints. (4) We extend a hardware-aware search framework
to support constraint-driven model configuration selection, considering
accuracy, deployability, latency, and energy consumption. Evaluated on two
swipe-direction datasets with multiple users and ordinary tables, our approach
achieves low-latency, energy-efficient inference on the AMD Spartan-7 XC7S25
FPGA. Under the PS data splitting setting, the selected 6-bit 1D-CNN reaches
0.970 average accuracy across users with 9.22 ms latency. The chosen 8-bit
1D-SepCNN further reduces latency to 6.83 ms (over 53x CPU speedup) with
slightly lower accuracy (0.949). Both consume under 1.2 mJ per inference,
demonstrating suitability for long-term edge operation.

</details>


### [329] [PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation Performance at Unseen Pre-Training Budgets](https://arxiv.org/abs/2510.23198)
*Etienne Goffinet,Shane Bergsma,Avraham Sheinin,Natalia Vassilieva,Shaheer Muhammad,Preslav Nakov,Gurpreet Gosal*

Main category: cs.LG

TL;DR: 提出PTPP感知的适应缩放定律，能预测不同tokens - per - parameter下的适应损失，并展示了规划重放比率和适应令牌预算的实际用例。


<details>
  <summary>Details</summary>
Motivation: 现有连续预训练（CPT）缩放定律假设预训练预算固定，限制了对不同tokens - per - parameter训练模型的适应结果预测能力。

Method: 提出PTPP感知的适应缩放定律，将预训练预算作为显式变量。

Result: 在多语言设置中，早期阶段训练的PTPP感知公式能预测目标损失，且在指标上优于PTPP不可知的基线。

Conclusion: PTPP感知的适应缩放定律能准确预测适应损失，还可用于在计算限制下规划重放比率和适应令牌预算。

Abstract: Continual pre-training (CPT) for domain adaptation must balance target-domain
gains with stability on the base domain. Existing CPT scaling laws typically
assume a fixed pre-training budget, which limits their ability to forecast
adaptation outcomes for models trained at different tokens-per-parameter
(PTPP). We present \emph{PTPP-aware} adaptation scaling laws that make the
pre-training budget an explicit variable, enabling accurate \emph{prediction}
of adaptation loss at unseen \ptpp. On a multilingual setup (English/Arabic
$\rightarrow$ French), PTPP-aware formulations trained on early stages
(\ptpp{}=\{15,31\}) predict target loss at \ptpp{}=279 and outperform a
PTPP-agnostic \dcpt{} transfer baseline on metrics (Huber-on-log,
MAE$_\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in
the appendix. Beyond forecasting, we show a practical use case: planning replay
ratios and adaptation token budgets that satisfy target and forgetting
constraints under compute limits.

</details>


### [330] [Increasing LLM Coding Capabilities through Diverse Synthetic Coding Tasks](https://arxiv.org/abs/2510.23208)
*Amal Abed,Ivan Lukic,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 现有大语言模型代码生成受限于缺乏优质数据集，本文提出合成数据生成管道，生成近800k四元组样本，微调模型有积极效果并发布成果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成中因缺乏大规模、多样且符合人类推理的数据集而进展受限，现有资源缺少中间推理过程。

Method: 构建合成数据生成管道，包含精选竞赛问题、相关性分类器筛选的网络内容、推理模式引导的数据扩展和多阶段基于执行的验证，用遗传突变算法增加任务多样性。

Result: 在编码基准测试中微调模型有持续改进，推理感知数据可替代模型扩展，跨架构泛化且表现优于领先开源方案。

Conclusion: 以推理为中心的合成数据生成是提升大语言模型编码能力的有效方法。

Abstract: Large language models (LLMs) have shown impressive promise in code
generation, yet their progress remains limited by the shortage of large-scale
datasets that are both diverse and well-aligned with human reasoning. Most
existing resources pair problems with solutions, but omit the intermediate
thought process that guides coding. To close this gap, we present a scalable
synthetic data generation pipeline that produces nearly 800k
instruction-reasoning-code-test quadruplets. Each sample combines a task, a
step-by-step reasoning trace, a working solution, and executable tests,
enabling models to learn not just the what but also the how of problem solving.
Our pipeline combines four key components: curated contest problems, web-mined
content filtered by relevance classifiers, data expansion guided by reasoning
patterns, and multi-stage execution-based validation. A genetic mutation
algorithm further increases task diversity while maintaining consistency
between reasoning traces and code implementations. Our key finding is that
fine-tuning LLMs on this dataset yields consistent improvements on coding
benchmarks. Beyond raw accuracy, reasoning-aware data can substitute for model
scaling, generalize across architectures, and outperform leading open-source
alternatives under identical sample budgets. Our work establishes
reasoning-centered synthetic data generation as an efficient approach for
advancing coding capabilities in LLMs. We publish our dataset and generation
pipeline to facilitate further research.

</details>


### [331] [Accelerating Eigenvalue Dataset Generation via Chebyshev Subspace Filter](https://arxiv.org/abs/2510.23215)
*Hong Wang,Jie Wang,Jian Luo,huanshuo dong,Yeqiu Chen,Runmin Jiang,Zhen huang*

Main category: cs.LG

TL;DR: 本文提出Sorting Chebyshev Subspace Filter (SCSF)方法加速特征值数据生成，实验显示比数值求解器快3.5倍。


<details>
  <summary>Details</summary>
Motivation: 现有神经特征值方法训练需大量标记数据，本文旨在解决这一限制。

Method: 提出SCSF方法，利用算子间相似性，通过截断快速傅里叶变换排序分组算子，构建切比雪夫子空间滤波器。

Result: SCSF比各种数值求解器实现了高达3.5倍的加速。

Conclusion: SCSF是首个加速特征值数据生成的方法，有效提升计算速度。

Abstract: Eigenvalue problems are among the most important topics in many scientific
disciplines. With the recent surge and development of machine learning, neural
eigenvalue methods have attracted significant attention as a forward pass of
inference requires only a tiny fraction of the computation time compared to
traditional solvers. However, a key limitation is the requirement for large
amounts of labeled data in training, including operators and their eigenvalues.
To tackle this limitation, we propose a novel method, named Sorting Chebyshev
Subspace Filter (SCSF), which significantly accelerates eigenvalue data
generation by leveraging similarities between operators -- a factor overlooked
by existing methods. Specifically, SCSF employs truncated fast Fourier
transform sorting to group operators with similar eigenvalue distributions and
constructs a Chebyshev subspace filter that leverages eigenpairs from
previously solved problems to assist in solving subsequent ones, reducing
redundant computations. To the best of our knowledge, SCSF is the first method
to accelerate eigenvalue data generation. Experimental results show that SCSF
achieves up to a $3.5\times$ speedup compared to various numerical solvers.

</details>


### [332] [Grassmanian Interpolation of Low-Pass Graph Filters: Theory and Applications](https://arxiv.org/abs/2510.23235)
*Anton Savostianov,Michael T. Schaub,Benjamin Stamm*

Main category: cs.LG

TL;DR: 提出基于Grassmann流形上法坐标黎曼插值的低通图滤波器插值新算法，给出误差估计并提出两个应用。


<details>
  <summary>Details</summary>
Motivation: 参数图族低通滤波器计算因需重复求解特征值问题而代价高昂。

Method: 基于Grassmann流形上法坐标的黎曼插值算法。

Result: 推导了子空间插值的误差界估计，提出两个参数图族的应用。

Conclusion: 新算法可用于解决参数图族低通滤波器计算问题，有一定应用价值。

Abstract: Low-pass graph filters are fundamental for signal processing on graphs and
other non-Euclidean domains. However, the computation of such filters for
parametric graph families can be prohibitively expensive as computation of the
corresponding low-frequency subspaces, requires the repeated solution of an
eigenvalue problem. We suggest a novel algorithm of low-pass graph filter
interpolation based on Riemannian interpolation in normal coordinates on the
Grassmann manifold. We derive an error bound estimate for the subspace
interpolation and suggest two possible applications for induced parametric
graph families. First, we argue that the temporal evolution of the node
features may be translated to the evolving graph topology via a similarity
correction to adjust the homophily degree of the network. Second, we suggest a
dot product graph family induced by a given static graph which allows to infer
improved message passing scheme for node classification facilitated by the
filter interpolation.

</details>


### [333] [Toward Interpretable Evaluation Measures for Time Series Segmentation](https://arxiv.org/abs/2510.23261)
*Félix Chavelli,Paul Boniol,Michaël Thomazo*

Main category: cs.LG

TL;DR: 本文提出WARI和SMS两种新的时间序列分割评估指标，在合成和真实基准上验证其能更准确评估分割质量并揭示传统指标无法发现的信息。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分割性能评估方法存在局限，如只关注变更点准确性、基于点的度量无法捕捉分割段质量、忽略误差性质且解释性有限。

Method: 引入WARI（考虑分割误差位置）和SMS（识别并评分四种基本分割误差类型并允许特定误差加权）两种新的评估指标。

Result: 在合成和真实世界基准上验证，新指标能更准确评估分割质量，还能揭示误差来源和类型等传统指标无法获取的信息。

Conclusion: 新提出的WARI和SMS评估指标可弥补现有评估方法的不足，在时间序列分割评估方面表现更优。

Abstract: Time series segmentation is a fundamental task in analyzing temporal data
across various domains, from human activity recognition to energy monitoring.
While numerous state-of-the-art methods have been developed to tackle this
problem, the evaluation of their performance remains critically limited.
Existing measures predominantly focus on change point accuracy or rely on
point-based measures such as Adjusted Rand Index (ARI), which fail to capture
the quality of the detected segments, ignore the nature of errors, and offer
limited interpretability. In this paper, we address these shortcomings by
introducing two novel evaluation measures: WARI (Weighted Adjusted Rand Index),
that accounts for the position of segmentation errors, and SMS (State Matching
Score), a fine-grained measure that identifies and scores four fundamental
types of segmentation errors while allowing error-specific weighting. We
empirically validate WARI and SMS on synthetic and real-world benchmarks,
showing that they not only provide a more accurate assessment of segmentation
quality but also uncover insights, such as error provenance and type, that are
inaccessible with traditional measures.

</details>


### [334] [PAHQ: Accelerating Automated Circuit Discovery through Mixed-Precision Inference Optimization](https://arxiv.org/abs/2510.23264)
*Xinhai Wang,Shu Yang,Liangyu Wang,Lin Zhang,Huanyi Xie,Lijie Hu,Di Wang*

Main category: cs.LG

TL;DR: 提出PAHQ方法加速自动电路发现，能降运行时间和内存消耗，保持分析准确性，可集成现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有ACDC方法用于大语言模型时计算效率低、内存需求高，加速方法牺牲分析准确性。

Method: Per Attention Head Quantization (PAHQ)通过优化单个修补操作效率，利用激活修补和混合精度量化的对齐性。

Result: PAHQ - 加速的ACDC相比未加速的ACDC，运行时间最多减少80%，内存消耗最多减少30%，且保持分析准确性。

Conclusion: PAHQ是一种无需训练、实用且新颖的加速机械可解释性方法的途径。

Abstract: Circuit discovery, which involves identifying sparse and task-relevant
subnetworks in pre-trained language models, is a cornerstone of mechanistic
interpretability. Automated Circuit Discovery (ACDC) has emerged as a pivotal
methodology in circuit discovery, but its application to large language models
is severely limited by computational inefficiency and prohibitively high memory
requirements. Although several accelerated approaches have been proposed, they
primarily rely on linear approximations to ACDC, which significantly
compromises analytical faithfulness. Our proposed method for accelerating
automated circuit discovery, Per Attention Head Quantization (PAHQ), takes a
fundamentally different approach by optimizing the efficiency of each
individual patching operation. PAHQ leverages a fundamental alignment between
activation patching and mixed-precision quantization (MPQ): interpretability
analysis through patching essentially performs targeted ablation studies.
Therefore, we can maintain high precision exclusively for investigated
components while safely reducing precision elsewhere in the network.
PAHQ-accelerated ACDC reduces runtime by up to 80\% and memory consumption by
up to 30\% compared to unaccelerated ACDC while maintaining faithfulness.
Importantly, our method readily integrates with existing edge-based circuit
discovery techniques by modifying the attention computation mechanism. This
training-free approach provides a practical and novel pathway for accelerating
mechanistic interpretability methods. Our code is available at
https://github.com/626619403/PAHQ.

</details>


### [335] [A Novel Framework for Multi-Modal Protein Representation Learning](https://arxiv.org/abs/2510.23273)
*Runjie Zheng,Zhen Wang,Anjie Qiao,Jiancong Xie,Jiahua Rao,Yuedong Yang*

Main category: cs.LG

TL;DR: 提出DAMPE框架用于蛋白质功能预测，解决跨模态融合挑战，表现优于或持平SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质功能预测中，异质内在信号与外在上下文信息有效融合面临跨模态分布不匹配和外在数据关系图噪声问题。

Method: 提出基于最优传输（OT）的表示对齐方法缓解跨模态异质性，开发基于条件图生成（CGG）的信息融合方法进行图重建。

Result: DAMPE在标准GO基准上优于或持平SOTA方法，AUPR提升0.002 - 0.013 pp，Fmax提升0.004 - 0.007 pp；消融实验表明OT贡献0.043 - 0.064 pp AUPR，CGG增加0.005 - 0.111 pp Fmax。

Conclusion: DAMPE为鲁棒的多模态蛋白质表示学习提供可扩展且理论可靠的方法，显著提升蛋白质功能预测能力。

Abstract: Accurate protein function prediction requires integrating heterogeneous
intrinsic signals (e.g., sequence and structure) with noisy extrinsic contexts
(e.g., protein-protein interactions and GO term annotations). However, two key
challenges hinder effective fusion: (i) cross-modal distributional mismatch
among embeddings produced by pre-trained intrinsic encoders, and (ii) noisy
relational graphs of extrinsic data that degrade GNN-based information
aggregation. We propose Diffused and Aligned Multi-modal Protein Embedding
(DAMPE), a unified framework that addresses these through two core mechanisms.
First, we propose Optimal Transport (OT)-based representation alignment that
establishes correspondence between intrinsic embedding spaces of different
modalities, effectively mitigating cross-modal heterogeneity. Second, we
develop a Conditional Graph Generation (CGG)-based information fusion method,
where a condition encoder fuses the aligned intrinsic embeddings to provide
informative cues for graph reconstruction. Meanwhile, our theoretical analysis
implies that the CGG objective drives this condition encoder to absorb
graph-aware knowledge into its produced protein representations. Empirically,
DAMPE outperforms or matches state-of-the-art methods such as DPFunc on
standard GO benchmarks, achieving AUPR gains of 0.002-0.013 pp and Fmax gains
0.004-0.007 pp. Ablation studies further show that OT-based alignment
contributes 0.043-0.064 pp AUPR, while CGG-based fusion adds 0.005-0.111 pp
Fmax. Overall, DAMPE offers a scalable and theoretically grounded approach for
robust multi-modal protein representation learning, substantially enhancing
protein function prediction.

</details>


### [336] [Learning from Frustration: Torsor CNNs on Graphs](https://arxiv.org/abs/2510.23288)
*Daiyuan Li,Shreya Arya,Robert Ghrist*

Main category: cs.LG

TL;DR: 提出Torsor CNNs框架用于学习具有局部对称性的图，统一并推广多种架构，还展示其在多视图3D识别中的应用。


<details>
  <summary>Details</summary>
Motivation: 大多数等变神经网络依赖单一全局对称性，限制了其在局部对称性领域的应用。

Method: 引入Torsor CNNs框架，将局部对称性编码为边势，建立与经典群同步问题的等价性，提出Torsor卷积层和frustration损失。

Result: Torsor CNN框架统一并推广了几种架构，可在任意图上操作，无需全局坐标系或光滑流形结构。

Conclusion: 建立了框架的数学基础，并证明其在多视图3D识别中的适用性。

Abstract: Most equivariant neural networks rely on a single global symmetry, limiting
their use in domains where symmetries are instead local. We introduce Torsor
CNNs, a framework for learning on graphs with local symmetries encoded as edge
potentials -- group-valued transformations between neighboring coordinate
frames. We establish that this geometric construction is fundamentally
equivalent to the classical group synchronization problem, yielding: (1) a
Torsor Convolutional Layer that is provably equivariant to local changes in
coordinate frames, and (2) the frustration loss -- a standalone geometric
regularizer that encourages locally equivariant representations when added to
any NN's training objective. The Torsor CNN framework unifies and generalizes
several architectures -- including classical CNNs and Gauge CNNs on manifolds
-- by operating on arbitrary graphs without requiring a global coordinate
system or smooth manifold structure. We establish the mathematical foundations
of this framework and demonstrate its applicability to multi-view 3D
recognition, where relative camera poses naturally define the required edge
potentials.

</details>


### [337] [Predicting symbolic ODEs from multiple trajectories](https://arxiv.org/abs/2510.23295)
*Yakup Emre Şahin,Niki Kilbertus,Sören Becker*

Main category: cs.LG

TL;DR: 介绍基于Transformer的模型MIO从动力系统轨迹推断符号常微分方程，结合多实例学习与符号回归，简单聚合策略可提升性能，在多场景超现有基线。


<details>
  <summary>Details</summary>
Motivation: 从动力系统的多个观测轨迹中推断符号常微分方程，学习更具泛化性的潜在动力学表示。

Method: 结合多实例学习和基于Transformer的符号回归，研究不同实例聚合策略。

Result: 在一到四维系统和不同噪声水平下评估，MIO始终优于现有基线。

Conclusion: MIO能有效利用同一系统的重复观测，简单的均值聚合策略也能显著提升性能。

Abstract: We introduce MIO, a transformer-based model for inferring symbolic ordinary
differential equations (ODEs) from multiple observed trajectories of a
dynamical system. By combining multiple instance learning with
transformer-based symbolic regression, the model effectively leverages repeated
observations of the same system to learn more generalizable representations of
the underlying dynamics. We investigate different instance aggregation
strategies and show that even simple mean aggregation can substantially boost
performance. MIO is evaluated on systems ranging from one to four dimensions
and under varying noise levels, consistently outperforming existing baselines.

</details>


### [338] [GRAD: Real-Time Gated Recurrent Anomaly Detection in Autonomous Vehicle Sensors Using Reinforced EMA and Multi-Stage Sliding Window Techniques](https://arxiv.org/abs/2510.23327)
*Mohammad Hossein Jafari Naeimi,Ali Norouzi,Athena Abdi*

Main category: cs.LG

TL;DR: 本文提出GRAD实时异常检测方法，结合统计分析与深度学习，在自动驾驶车辆传感器数据异常检测中表现出色，平衡了高精度与低计算成本。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶车辆传感器数据的可靠性，实现实时异常检测。

Method: 结合REMA进行异常值检测、MS - SW捕捉长短模式，用轻量级GRU模型检测分类异常，通过恢复模块修复受损数据。

Result: 异常检测F1分数异常数据97.6%、正常数据99.4%，能高精度分类异常类型并修复数据。

Conclusion: GRAD在实时异常检测中可靠高效，能保障车辆安全运行且计算开销小。

Abstract: This paper introduces GRAD, a real-time anomaly detection method for
autonomous vehicle sensors that integrates statistical analysis and deep
learning to ensure the reliability of sensor data. The proposed approach
combines the Reinforced Exponential Moving Average (REMA), which adapts
smoothing factors and thresholding for outlier detection, with the Multi-Stage
Sliding Window (MS-SW) technique for capturing both short- and long-term
patterns. These features are processed using a lightweight Gated Recurrent Unit
(GRU) model, which detects and classifies anomalies based on bias types, while
a recovery module restores damaged sensor data to ensure continuous system
operation. GRAD has a lightweight architecture consisting of two layers of GRU
with a limited number of neurons that make it appropriate for real-time
applications while maintaining high detection accuracy. The GRAD framework
achieved remarkable performance in anomaly detection and classification. The
model demonstrated an overall F1-score of 97.6% for abnormal data and 99.4% for
normal data, signifying its high accuracy in distinguishing between normal and
anomalous sensor data. Regarding the anomaly classification, GRAD successfully
categorized different anomaly types with high precision, enabling the recovery
module to accurately restore damaged sensor data. Relative to analogous
studies, GRAD surpasses current models by attaining a balance between elevated
detection accuracy and diminished computational expense. These results
demonstrate GRAD's potential as a reliable and efficient solution for real-time
anomaly detection in autonomous vehicle systems, guaranteeing safe vehicle
operation with minimal computational overhead.

</details>


### [339] [Block-Diagonal LoRA for Eliminating Communication Overhead in Tensor Parallel LoRA Serving](https://arxiv.org/abs/2510.23346)
*Xinyu Wang,Jonas M. Kübler,Kailash Budhathoki,Yida Wang,Matthäus Kleindessner*

Main category: cs.LG

TL;DR: 提出块对角LoRA方法，在参数效率与标准LoRA相近情况下，相比S - LoRA显著提升端到端速度。


<details>
  <summary>Details</summary>
Motivation: S - LoRA分块策略存在通信开销问题，需改进。

Method: 约束某些LoRA因子为块对角，采用新的LoRA适配器分块方式。

Result: 实验表明该方法参数效率与标准LoRA相近，在多GPU服务时相比S - LoRA有显著端到端加速。

Conclusion: 块对角LoRA方法是一种有效策略，能在不增加通信开销下提升推理速度。

Abstract: When serving a single base LLM with several different LoRA adapters
simultaneously, the adapters cannot simply be merged with the base model's
weights as the adapter swapping would create overhead and requests using
different adapters could not be batched. Rather, the LoRA computations have to
be separated from the base LLM computations, and in a multi-device setup the
LoRA adapters can be sharded in a way that is well aligned with the base
model's tensor parallel execution, as proposed in S-LoRA. However, the S-LoRA
sharding strategy encounters some communication overhead, which may be small in
theory, but can be large in practice. In this paper, we propose to constrain
certain LoRA factors to be block-diagonal, which allows for an alternative way
of sharding LoRA adapters that does not require any additional communication
for the LoRA computations. We demonstrate in extensive experiments that our
block-diagonal LoRA approach is similarly parameter efficient as standard LoRA
(i.e., for a similar number of parameters it achieves similar downstream
performance) and that it leads to significant end-to-end speed-up over S-LoRA.
For example, when serving on eight A100 GPUs, we observe up to 1.79x (1.23x)
end-to-end speed-up with 0.87x (1.74x) the number of adapter parameters for
Llama-3.1-70B, and up to 1.63x (1.3x) end-to-end speed-up with 0.86x (1.73x)
the number of adapter parameters for Llama-3.1-8B.

</details>


### [340] [ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping](https://arxiv.org/abs/2510.23364)
*Hyeongkyun Kim,Orestis Oikonomou*

Main category: cs.LG

TL;DR: 介绍ZeroFlood框架用于数据高效的洪水易发性制图，实验证明其可行性。


<details>
  <summary>Details</summary>
Motivation: 洪水易发性制图在数据稀缺地区有挑战，现有水动力模型需大量地理物理输入。

Method: 用Thinking - in - Modality (TiM)推理微调地理空间基础模型（GFMs），通过跨模态表示学习填补数据缺口。

Result: TiM增强模型鲁棒性，TerraMind - Large配置F1分数达67.21。

Conclusion: 基于基础模型的洪水易发性制图是洪水风险管理可扩展且数据高效的解决方案。

Abstract: Flood susceptibility mapping (FSM) is vital for disaster prevention but
remains challenging in data-scarce regions where hydrodynamic models require
dense geophysical inputs. This work introduces ZeroFlood, a geospatial
foundation model framework for data-efficient FSM. The approach fine-tunes
Geospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning,
enabling flood prediction from basic Earth observation data such as Sentinel-1
or Sentinel-2 imagery. Using paired EO and simulated flood maps from data-rich
regions, ZeroFlood bridges data availability gaps through cross-modal
representation learning. Experiments with TerraMind and Prithvi GFMs show that
TiM enhances model robustness, with the TerraMind-Large configuration achieving
an F1 score of 67.21. The results demonstrate the feasibility of
foundation-model-based FSM as a scalable and data-efficient solution for flood
risk management.

</details>


### [341] [The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation](https://arxiv.org/abs/2510.23393)
*Farid Bagirov,Mikhail Arkhipov,Ksenia Sycheva,Evgeniy Glukhov,Egor Bogomolov*

Main category: cs.LG

TL;DR: 本文聚焦优化max@k指标，推导无偏策略梯度估计，拓展到离策略更新，实证表明能有效优化该指标。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调过程会损害模型探索能力，影响Best - of - N采样性能，需优化max@k指标。

Method: 推导无偏的在线策略梯度估计用于直接优化max@k指标，并将推导拓展到离策略更新。

Result: 在离策略场景中，目标能有效优化max@k指标，使模型与Best - of - N推理策略一致。

Conclusion: 提出的方法可有效解决强化学习微调带来的问题，优化max@k指标。

Abstract: The application of Reinforcement Learning with Verifiable Rewards (RLVR) to
mathematical and coding domains has demonstrated significant improvements in
the reasoning and problem-solving abilities of Large Language Models. Despite
its success in single generation problem solving, the reinforcement learning
fine-tuning process may harm the model's exploration ability, as reflected in
decreased diversity of generations and a resulting degradation of performance
during Best-of-N sampling for large N values. In this work, we focus on
optimizing the max@k metric, a continuous generalization of pass@k. We derive
an unbiased on-policy gradient estimate for direct optimization of this metric.
Furthermore, we extend our derivations to the off-policy updates, a common
element in modern RLVR algorithms, that allows better sample efficiency.
Empirically, we show that our objective effectively optimizes max@k metric in
off-policy scenarios, aligning the model with the Best-of-N inference strategy.

</details>


### [342] [Eigen-Value: Efficient Domain-Robust Data Valuation via Eigenvalue-Based Approach](https://arxiv.org/abs/2510.23409)
*Youngjun Choi,Joonseong Kang,Sungjun Lim,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出Eigen - Value (EV)框架用于OOD鲁棒性数据估值，仅用ID数据子集，计算轻量且效果好。


<details>
  <summary>Details</summary>
Motivation: 现有数据估值方法在OOD场景存在泛化问题，且OOD感知方法计算成本高，阻碍实际部署。

Method: 引入EV框架，用ID数据协方差矩阵特征值比给出域差异的谱近似，通过摄动理论估计数据点边际贡献，添加EV项到基于ID损失的方法中。

Result: EV在真实数据集上实现了更好的OOD鲁棒性和稳定的价值排名，计算轻量。

Conclusion: EV适用于有域偏移的大规模场景，为OOD鲁棒数据估值提供有效途径。

Abstract: Data valuation has become central in the era of data-centric AI. It drives
efficient training pipelines and enables objective pricing in data markets by
assigning a numeric value to each data point. Most existing data valuation
methods estimate the effect of removing individual data points by evaluating
changes in model validation performance under in-distribution (ID) settings, as
opposed to out-of-distribution (OOD) scenarios where data follow different
patterns. Since ID and OOD data behave differently, data valuation methods
based on ID loss often fail to generalize to OOD settings, particularly when
the validation set contains no OOD data. Furthermore, although OOD-aware
methods exist, they involve heavy computational costs, which hinder practical
deployment. To address these challenges, we introduce \emph{Eigen-Value} (EV),
a plug-and-play data valuation framework for OOD robustness that uses only an
ID data subset, including during validation. EV provides a new spectral
approximation of domain discrepancy, which is the gap of loss between ID and
OOD using ratios of eigenvalues of ID data's covariance matrix. EV then
estimates the marginal contribution of each data point to this discrepancy via
perturbation theory, alleviating the computational burden. Subsequently, EV
plugs into ID loss-based methods by adding an EV term without any additional
training loop. We demonstrate that EV achieves improved OOD robustness and
stable value rankings across real-world datasets, while remaining
computationally lightweight. These results indicate that EV is practical for
large-scale settings with domain shift, offering an efficient path to
OOD-robust data valuation.

</details>


### [343] [PrivacyGuard: A Modular Framework for Privacy Auditing in Machine Learning](https://arxiv.org/abs/2510.23427)
*Luca Melis,Matthew Grange,Iden Kalemaj,Karan Chadha,Shengyuan Hu,Elena Kashtelyan,Will Bullock*

Main category: cs.LG

TL;DR: 介绍了隐私评估工具PrivacyGuard，可评估机器学习模型隐私风险，支持多种攻击和隐私分析，有模块化架构，代码开源。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在敏感领域部署增加，需要强大实用的隐私评估工具。

Method: 实现多种隐私攻击（成员推理、提取和重建攻击等），采用模块化架构集成新攻击和隐私指标。

Result: 开发出PrivacyGuard工具并在https://github.com/facebookresearch/PrivacyGuard开源。

Conclusion: PrivacyGuard可用于评估机器学习模型的隐私风险，且能适应研究进展。

Abstract: The increasing deployment of Machine Learning (ML) models in sensitive
domains motivates the need for robust, practical privacy assessment tools.
PrivacyGuard is a comprehensive tool for empirical differential privacy (DP)
analysis, designed to evaluate privacy risks in ML models through
state-of-the-art inference attacks and advanced privacy measurement techniques.
To this end, PrivacyGuard implements a diverse suite of privacy attack --
including membership inference , extraction, and reconstruction attacks --
enabling both off-the-shelf and highly configurable privacy analyses. Its
modular architecture allows for the seamless integration of new attacks, and
privacy metrics, supporting rapid adaptation to emerging research advances. We
make PrivacyGuard available at
https://github.com/facebookresearch/PrivacyGuard.

</details>


### [344] [Improving Predictions of Molecular Properties with Graph Featurisation and Heterogeneous Ensemble Models](https://arxiv.org/abs/2510.23428)
*Michael L. Parker,Samar Mahmoud,Bailey Montefiore,Mario Öeren,Himani Tandon,Charlotte Wharrick,Matthew D. Segall*

Main category: cs.LG

TL;DR: 结合图神经网络的分子描述符、通用描述符和机器学习模型集成，提出MetaModel框架，在多数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 探索结合不同描述符和机器学习模型以更好地建模分子属性。

Method: 引入MetaModel框架聚合多种机器学习模型预测，提出结合特定任务GNN特征与传统分子描述符的特征化方案。

Result: 框架在所有回归数据集和6个分类数据集上优于ChemProp模型，包含ChemProp的GNN特征可提升集成模型在部分数据集上的性能。

Conclusion: 为实现广泛问题的最优性能，需结合通用描述符与特定任务学习特征，并使用多样化机器学习模型进行预测。

Abstract: We explore a "best-of-both" approach to modelling molecular properties by
combining learned molecular descriptors from a graph neural network (GNN) with
general-purpose descriptors and a mixed ensemble of machine learning (ML)
models. We introduce a MetaModel framework to aggregate predictions from a
diverse set of leading ML models. We present a featurisation scheme for
combining task-specific GNN-derived features with conventional molecular
descriptors.
  We demonstrate that our framework outperforms the cutting-edge ChemProp model
on all regression datasets tested and 6 of 9 classification datasets. We
further show that including the GNN features derived from ChemProp boosts the
ensemble model's performance on several datasets where it otherwise would have
underperformed. We conclude that to achieve optimal performance across a wide
set of problems, it is vital to combine general-purpose descriptors with
task-specific learned features and use a diverse set of ML models to make the
predictions.

</details>


### [345] [Schrodinger Neural Network and Uncertainty Quantification: Quantum Machine](https://arxiv.org/abs/2510.23449)
*M. M. Hammad*

Main category: cs.LG

TL;DR: 介绍了受量子力学启发的薛定谔神经网络（SNN）用于条件密度估计和不确定性量化，阐述其特点、基础及优势。


<details>
  <summary>Details</summary>
Motivation: 构建一个用于条件密度估计和不确定性量化的有效架构。

Method: SNN将输入映射到输出域的归一化波函数，通过Born规则计算预测概率，学习谱展开的复系数，还开发了统计和计算基础，如精确最大似然训练、物理启发的正则化器等。

Result: SNN具有正定性和精确归一化、自然多模态性、可计算封闭形式泛函等优点，提供了从点估计到基于振幅分布的概率预测框架。

Conclusion: SNN为概率预测提供了连贯、易处理的框架。

Abstract: We introduce the Schrodinger Neural Network (SNN), a principled architecture
for conditional density estimation and uncertainty quantification inspired by
quantum mechanics. The SNN maps each input to a normalized wave function on the
output domain and computes predictive probabilities via the Born rule. The SNN
departs from standard parametric likelihood heads by learning complex
coefficients of a spectral expansion (e . g ., Chebyshev polynomials) whose
squared modulus yields the conditional density $p(y|x)=\left| \psi _x(y)\right|
{}^2$ with analytic normalization. This representation confers three practical
advantages: positivity and exact normalization by construction, native
multimodality through interference among basis modes without explicit mixture
bookkeeping, and yields closed-form (or efficiently computable)
functionals$-$such as moments and several calibration diagnostics$-$as
quadratic forms in coefficient space. We develop the statistical and
computational foundations of the SNN, including (i) training by exact
maximum-likelihood with unit-sphere coefficient parameterization, (ii)
physics-inspired quadratic regularizers (kinetic and potential energies)
motivated by uncertainty relations between localization and spectral
complexity, (iii) scalable low-rank and separable extensions for multivariate
outputs, (iv) operator-based extensions that represent observables,
constraints, and weak labels as self-adjoint matrices acting on the amplitude
space, and (v) a comprehensive framework for evaluating multimodal predictions.
The SNN provides a coherent, tractable framework to elevate probabilistic
prediction from point estimates to physically inspired amplitude-based
distributions.

</details>


### [346] [SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning](https://arxiv.org/abs/2510.23455)
*Khoa Nguyen,Khang Tran,NhatHai Phan,Cristian Borcea,Rouming Jin,Issa Khalil*

Main category: cs.LG

TL;DR: 提出SGFusion训练算法，利用移动用户地理信息，使地理区域间知识融合，提升模型效用。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中利用移动用户地理信息，提升模型适应性和效用。

Method: 将移动设备数据映射到地理区域，为每个区域训练模型；用HRG建模区域相关性，用MCMC采样优化；训练时各区域融合本地梯度与HRG采样区域梯度。

Result: 使用心率预测数据集的理论和实证结果表明，SGFusion训练的模型收敛且期望误差有上界，相比现有方法显著提升各国模型效用，不影响系统可扩展性。

Conclusion: SGFusion能显著提升模型效用，且不引入过高计算成本。

Abstract: This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel
training algorithm to leverage the geographic information of mobile users in
Federated Learning (FL). SGFusion maps the data collected by mobile devices
onto geographical zones and trains one FL model per zone, which adapts well to
the data and behaviors of users in that zone. SGFusion models the local
data-based correlation among geographical zones as a hierarchical random graph
(HRG) optimized by Markov Chain Monte Carlo sampling. At each training step,
every zone fuses its local gradient with gradients derived from a small set of
other zones sampled from the HRG. This approach enables knowledge fusion and
sharing among geographical zones in a probabilistic and stochastic gradient
fusion process with self-attention weights, such that "more similar" zones have
"higher probabilities" of sharing gradients with "larger attention weights."
SGFusion remarkably improves model utility without introducing undue
computational cost. Extensive theoretical and empirical results using a
heart-rate prediction dataset collected across 6 countries show that models
trained with SGFusion converge with upper-bounded expected errors and
significantly improve utility in all countries compared to existing approaches
without notable cost in system scalability.

</details>


### [347] [Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph Neural Networks](https://arxiv.org/abs/2510.23469)
*Yuhan Yang,Xingbo Fu,Jundong Li*

Main category: cs.LG

TL;DR: 提出自适应双提示（ADPrompt）框架用于预训练GNN模型适配下游任务并增强公平性，实验表明其在节点分类任务上优于7种基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有图提示方法做适配时忽略公平性问题，预训练GNN模型会产生有歧视性的节点表示，需解决下游图数据固有的属性和结构偏差。

Method: 提出ADPrompt框架，设计自适应特征矫正模块抑制输入层敏感信息，提出自适应消息校准模块在各层生成结构提示，联合优化两个提示模块。

Result: 在四个数据集上用四种预训练策略进行实验，ADPrompt在节点分类任务上优于七种基线方法。

Conclusion: ADPrompt框架能有效增强预训练GNN模型适配下游任务时的公平性。

Abstract: In recent years, pre-training Graph Neural Networks (GNNs) through
self-supervised learning on unlabeled graph data has emerged as a widely
adopted paradigm in graph learning. Although the paradigm is effective for
pre-training powerful GNN models, the objective gap often exists between
pre-training and downstream tasks. To bridge this gap, graph prompting adapts
pre-trained GNN models to specific downstream tasks with extra learnable
prompts while keeping the pre-trained GNN models frozen. As recent graph
prompting methods largely focus on enhancing model utility on downstream tasks,
they often overlook fairness concerns when designing prompts for adaptation. In
fact, pre-trained GNN models will produce discriminative node representations
across demographic subgroups, as downstream graph data inherently contains
biases in both node attributes and graph structures. To address this issue, we
propose an Adaptive Dual Prompting (ADPrompt) framework that enhances fairness
for adapting pre-trained GNN models to downstream tasks. To mitigate attribute
bias, we design an Adaptive Feature Rectification module that learns customized
attribute prompts to suppress sensitive information at the input layer,
reducing bias at the source. Afterward, we propose an Adaptive Message
Calibration module that generates structure prompts at each layer, which adjust
the message from neighboring nodes to enable dynamic and soft calibration of
the information flow. Finally, ADPrompt jointly optimizes the two prompting
modules to adapt the pre-trained GNN while enhancing fairness. We conduct
extensive experiments on four datasets with four pre-training strategies to
evaluate the performance of ADPrompt. The results demonstrate that our proposed
ADPrompt outperforms seven baseline methods on node classification tasks.

</details>


### [348] [T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning](https://arxiv.org/abs/2510.23484)
*Julie Mordacq,David Loiseaux,Vicky Kalogeiton,Steve Oudot*

Main category: cs.LG

TL;DR: 本文提出T - REGS正则化框架用于自监督学习，理论分析表明其能缓解维度坍塌和促进分布均匀性，实验验证了提升表征质量的有效性。


<details>
  <summary>Details</summary>
Motivation: 自监督学习中有效表征需避免维度坍塌和增强分布均匀性，当前缺乏相关解决方案。

Method: 引入基于学习表征上最小生成树长度的T - REGS正则化框架，并进行理论分析。

Result: 理论分析表明T - REGS能同时缓解维度坍塌和促进任意紧致黎曼流形上的分布均匀性，合成数据和经典自监督学习基准实验验证了其有效性。

Conclusion: T - REGS框架能有效提升自监督学习的表征质量。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for
learning representations without labeled data, often by enforcing invariance to
input transformations such as rotations or blurring. Recent studies have
highlighted two pivotal properties for effective representations: (i) avoiding
dimensional collapse-where the learned features occupy only a low-dimensional
subspace, and (ii) enhancing uniformity of the induced distribution. In this
work, we introduce T-REGS, a simple regularization framework for SSL based on
the length of the Minimum Spanning Tree (MST) over the learned representation.
We provide theoretical analysis demonstrating that T-REGS simultaneously
mitigates dimensional collapse and promotes distribution uniformity on
arbitrary compact Riemannian manifolds. Several experiments on synthetic data
and on classical SSL benchmarks validate the effectiveness of our approach at
enhancing representation quality.

</details>


### [349] [Learning to Reason Efficiently with Discounted Reinforcement Learning](https://arxiv.org/abs/2510.23486)
*Alex Ayoub,Kavosh Asadi,Dale Schuurmans,Csaba Szepesvári,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 通过惩罚推理令牌，在保持准确性的同时缩短思维链，降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 大推理模型消耗过多令牌，导致计算成本和延迟增加，挑战长响应能提高准确性的假设。

Method: 使用折扣强化学习设置惩罚推理令牌，并分析受限策略类中的布莱克威尔最优性。

Result: 实验证实该方法在保持准确性的同时缩短了思维链。

Conclusion: 此方法可在保持准确性的前提下，有效缩短推理思维链，降低成本和延迟。

Abstract: Large reasoning models (LRMs) often consume excessive tokens, inflating
computational cost and latency. We challenge the assumption that longer
responses improve accuracy. By penalizing reasoning tokens using a discounted
reinforcement learning setup (interpretable as a small token cost) and
analyzing Blackwell optimality in restricted policy classes, we encourage
concise yet accurate reasoning. Experiments confirm our theoretical results
that this approach shortens chains of thought while preserving accuracy.

</details>


### [350] [Mixed Precision Training of Neural ODEs](https://arxiv.org/abs/2510.23498)
*Elena Celledoni,Brynjulf Owren,Lars Ruthotto,Tianjiao Nicole Yang*

Main category: cs.LG

TL;DR: 提出用于神经ODE的混合精度训练框架，结合显式ODE求解器与自定义反向传播方案，在多任务中有效，发布开源包rampde，实现约50%内存减少和最高2倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度训练原则对连续时间架构如神经ODE不可靠，需解决重复网络评估计算成本和内存需求增长问题。

Method: 结合显式ODE求解器与自定义反向传播方案，低精度计算速度和存储中间状态，自定义动态伴随缩放和高精度累积解决方案与梯度。

Result: 使用挑战性测试用例和在图像分类、生成模型等应用中验证，实现约50%内存减少和最高2倍加速，精度与单精度训练相当。

Conclusion: 提出的混合精度训练框架对神经ODE有效，能降低计算成本和内存需求，开源包rampde易用。

Abstract: Exploiting low-precision computations has become a standard strategy in deep
learning to address the growing computational costs imposed by ever larger
models and datasets. However, naively performing all computations in low
precision can lead to roundoff errors and instabilities. Therefore, mixed
precision training schemes usually store the weights in high precision and use
low-precision computations only for whitelisted operations. Despite their
success, these principles are currently not reliable for training
continuous-time architectures such as neural ordinary differential equations
(Neural ODEs). This paper presents a mixed precision training framework for
neural ODEs, combining explicit ODE solvers with a custom backpropagation
scheme, and demonstrates its effectiveness across a range of learning tasks.
Our scheme uses low-precision computations for evaluating the velocity,
parameterized by the neural network, and for storing intermediate states, while
stability is provided by a custom dynamic adjoint scaling and by accumulating
the solution and gradients in higher precision. These contributions address two
key challenges in training neural ODE: the computational cost of repeated
network evaluations and the growth of memory requirements with the number of
time steps or layers. Along with the paper, we publish our extendable,
open-source PyTorch package rampde, whose syntax resembles that of leading
packages to provide a drop-in replacement in existing codes. We demonstrate the
reliability and effectiveness of our scheme using challenging test cases and on
neural ODE applications in image classification and generative models,
achieving approximately 50% memory reduction and up to 2x speedup while
maintaining accuracy comparable to single-precision training.

</details>


### [351] [Towards Deep Physics-Informed Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.23501)
*Spyros Rigas,Fotios Anagnostopoulos,Michalis Papachristou,Georgios Alexandridis*

Main category: cs.LG

TL;DR: 本文针对Chebyshev基物理信息KANs（cPIKANs）深度扩展时的训练不稳定问题，提出了初始化方案和Residual - Gated Adaptive KANs（RGA KANs），并通过实验证明RGA KANs性能更优。


<details>
  <summary>Details</summary>
Motivation: cPIKANs在深度扩展时存在训练不稳定问题，限制了其在一些PDE问题中的应用。

Method: 提出了基无关、类似Glorot的初始化方案；受PirateNet架构启发，引入RGA KANs。

Result: RGA KANs能成功度过所有训练阶段，在七个标准前向PDE基准测试中始终优于参数匹配的cPIKANs和PirateNets，且在其他模型发散的情况下保持稳定。

Conclusion: RGA KANs在解决cPIKANs深度扩展训练不稳定问题上效果显著，性能优于对比模型。

Abstract: Since their introduction, Kolmogorov-Arnold Networks (KANs) have been
successfully applied across several domains, with physics-informed machine
learning (PIML) emerging as one of the areas where they have thrived. In the
PIML setting, Chebyshev-based physics-informed KANs (cPIKANs) have become the
standard due to their computational efficiency. However, like their multilayer
perceptron-based counterparts, cPIKANs face significant challenges when scaled
to depth, leading to training instabilities that limit their applicability to
several PDE problems. To address this, we propose a basis-agnostic, Glorot-like
initialization scheme that preserves activation variance and yields substantial
improvements in stability and accuracy over the default initialization of
cPIKANs. Inspired by the PirateNet architecture, we further introduce
Residual-Gated Adaptive KANs (RGA KANs), designed to mitigate divergence in
deep cPIKANs where initialization alone is not sufficient. Through empirical
tests and information bottleneck analysis, we show that RGA KANs successfully
traverse all training phases, unlike baseline cPIKANs, which stagnate in the
diffusion phase in specific PDE settings. Evaluations on seven standard forward
PDE benchmarks under a fixed training pipeline with adaptive components
demonstrate that RGA KANs consistently outperform parameter-matched cPIKANs and
PirateNets - often by several orders of magnitude - while remaining stable in
settings where the others diverge.

</details>


### [352] [A Deep Latent Factor Graph Clustering with Fairness-Utility Trade-off Perspective](https://arxiv.org/abs/2510.23507)
*Siamak Ghodsi,Amjad Seyedi,Tai Le Quy,Fariba Karimi,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: 提出针对图的端到端深度非负三因子分解方法DFNMF，可优化聚类分配，在合成和真实网络中效果好。


<details>
  <summary>Details</summary>
Motivation: 现有公平图聚类方法存在刚性约束、多阶段管道等缺点，限制了权衡控制、可解释性和可扩展性。

Method: 引入DFNMF，用软统计奇偶正则化器直接优化聚类分配，通过单参数λ调整公平 - 效用平衡，采用稀疏友好交替更新。

Result: 在合成和真实网络中，DFNMF在可比模块化下实现更高的组平衡，常优于前沿基线。

Conclusion: DFNMF是一种有效的公平图聚类方法，代码公开。

Abstract: Fair graph clustering seeks partitions that respect network structure while
maintaining proportional representation across sensitive groups, with
applications spanning community detection, team formation, resource allocation,
and social network analysis. Many existing approaches enforce rigid constraints
or rely on multi-stage pipelines (e.g., spectral embedding followed by
$k$-means), limiting trade-off control, interpretability, and scalability. We
introduce \emph{DFNMF}, an end-to-end deep nonnegative tri-factorization
tailored to graphs that directly optimizes cluster assignments with a soft
statistical-parity regularizer. A single parameter $\lambda$ tunes the
fairness--utility balance, while nonnegativity yields parts-based factors and
transparent soft memberships. The optimization uses sparse-friendly alternating
updates and scales near-linearly with the number of edges. Across synthetic and
real networks, DFNMF achieves substantially higher group balance at comparable
modularity, often dominating state-of-the-art baselines on the Pareto front.
The code is available at https://github.com/SiamakGhodsi/DFNMF.git.

</details>


### [353] [A U-Net and Transformer Pipeline for Multilingual Image Translation](https://arxiv.org/abs/2510.23554)
*Siddharth Sahay,Radhika Agarwal*

Main category: cs.LG

TL;DR: 提出端到端多语言翻译管道，集成自定义U - Net、Tesseract和从头训练的Seq2Seq Transformer，评估表现良好。


<details>
  <summary>Details</summary>
Motivation: 构建可定制、适应性强的多语言翻译系统，避免依赖整体预训练模型。

Method: 用在合成数据集上训练的U - Net检测图像文本区域，用Tesseract提取文本，将提取文本输入在多语言平行语料库上从头训练的自定义Transformer模型，通过BLEU分数评估系统。

Result: 系统在文本检测准确率、文本识别质量和翻译性能方面表现出有前景的结果。

Conclusion: 验证了自定义系统直接从图像翻译文本的可行性。

Abstract: This paper presents an end-to-end multilingual translation pipeline that
integrates a custom U-Net for text detection, the Tesseract engine for text
recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for
Neural Machine Translation (NMT). Our approach first utilizes a U-Net model,
trained on a synthetic dataset , to accurately segment and detect text regions
from an image. These detected regions are then processed by Tesseract to
extract the source text. This extracted text is fed into a custom Transformer
model trained from scratch on a multilingual parallel corpus spanning 5
languages. Unlike systems reliant on monolithic pre-trained models, our
architecture emphasizes full customization and adaptability. The system is
evaluated on its text detection accuracy, text recognition quality, and
translation performance via BLEU scores. The complete pipeline demonstrates
promising results, validating the viability of a custom-built system for
translating text directly from images.

</details>


### [354] [TAMI: Taming Heterogeneity in Temporal Interactions for Temporal Graph Link Prediction](https://arxiv.org/abs/2510.23577)
*Zhongyi Yu,Jianqiu Wu,Zhenghao Wu,Shuhan Zhong,Weifeng Su,Chul-Ho Lee,Weipeng Zhuo*

Main category: cs.LG

TL;DR: 提出TAMI框架应对时间图链接预测中交互异质性问题，实验表明可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 时间交互存在异质性，现有方法未考虑，导致学习的时间节点嵌入效果不佳。

Method: 提出TAMI框架，包含对数时间编码函数（LTE）和链接历史聚合（LHA），可集成现有先进时间图神经网络。

Result: 在13个经典数据集和3个最新时间图基准数据集上，TAMI在归纳和直推设置下均提升了基础模型的链接预测性能。

Conclusion: TAMI框架能有效应对异质性问题，提升时间图链接预测性能。

Abstract: Temporal graph link prediction aims to predict future interactions between
nodes in a graph based on their historical interactions, which are encoded in
node embeddings. We observe that heterogeneity naturally appears in temporal
interactions, e.g., a few node pairs can make most interaction events, and
interaction events happen at varying intervals. This leads to the problems of
ineffective temporal information encoding and forgetting of past interactions
for a pair of nodes that interact intermittently for their link prediction.
Existing methods, however, do not consider such heterogeneity in their learning
process, and thus their learned temporal node embeddings are less effective,
especially when predicting the links for infrequently interacting node pairs.
To cope with the heterogeneity, we propose a novel framework called TAMI, which
contains two effective components, namely log time encoding function (LTE) and
link history aggregation (LHA). LTE better encodes the temporal information
through transforming interaction intervals into more balanced ones, and LHA
prevents the historical interactions for each target node pair from being
forgotten. State-of-the-art temporal graph neural networks can be seamlessly
and readily integrated into TAMI to improve their effectiveness. Experiment
results on 13 classic datasets and three newest temporal graph benchmark (TGB)
datasets show that TAMI consistently improves the link prediction performance
of the underlying models in both transductive and inductive settings. Our code
is available at https://github.com/Alleinx/TAMI_temporal_graph.

</details>


### [355] [Lightweight Robust Direct Preference Optimization](https://arxiv.org/abs/2510.23590)
*Cheol Woo Kim,Shresth Verma,Mauricio Tec,Milind Tambe*

Main category: cs.LG

TL;DR: 提出基于DPO的DPO - PRO微调算法，解决数据噪声和过拟合问题，实验显示其比现有DPO变体更能提升对噪声偏好信号的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: DPO易受数据噪声影响且易过拟合，现有基于DRO的方法存在过度保守和计算成本高的问题。

Method: 提出DPO - PRO算法，通过轻量级DRO公式考虑偏好分布的不确定性，聚焦偏好不确定性，避免不必要的保守性和计算开销。

Result: 在标准对齐基准和现实公共卫生任务上评估，该方法相比现有DPO变体持续提升了对噪声偏好信号的鲁棒性。

Conclusion: DPO - PRO算法能有效解决DPO存在的问题，提升模型在噪声环境下的鲁棒性。

Abstract: Direct Preference Optimization (DPO) has become a popular method for
fine-tuning large language models (LLMs) due to its stability and simplicity.
However, it is also known to be sensitive to noise in the data and prone to
overfitting. Recent works have proposed using distributionally robust
optimization (DRO) to address potential noise and distributional shift in the
data. However, these methods often suffer from excessive conservatism and high
computational cost. We propose DPO-PRO (DPO with Preference Robustness), a
robust fine-tuning algorithm based on DPO which accounts for uncertainty in the
preference distribution through a lightweight DRO formulation. Unlike prior
DRO-based variants, DPO-PRO focuses solely on uncertainty in preferences,
avoiding unnecessary conservatism and incurring negligible computational
overhead. We further show that DPO-PRO is equivalent to a regularized DPO
objective that penalizes model overconfidence under weak preference signals. We
evaluate DPO-PRO on standard alignment benchmarks and a real-world public
health task. Experimental results show that our method consistently improves
robustness to noisy preference signals compared to existing DPO variants.

</details>


### [356] [Variational Masked Diffusion Models](https://arxiv.org/abs/2510.23606)
*Yichi Zhang,Alex Schwing,Zhizhen Zhao*

Main category: cs.LG

TL;DR: 提出变分掩码扩散（VMD）框架，将潜在变量引入掩码扩散过程，在多个数据集验证其能提升生成质量和依赖感知。


<details>
  <summary>Details</summary>
Motivation: 标准掩码扩散无法有效捕捉并发预测的标记间依赖关系，导致生成质量下降，需显式建模标记间依赖。

Method: 提出Variational Masked Diffusion（VMD）框架，将潜在变量引入掩码扩散过程。

Result: 在合成数据集上，VMD成功学习到传统掩码扩散无法捕捉的依赖关系；在数独谜题和文本数据集上，提升了全局一致性。

Conclusion: VMD提升了生成质量和依赖感知，证明将变分推理集成到掩码扩散中有价值。

Abstract: Masked diffusion models have recently emerged as a flexible framework for
discrete generative modeling. However, a key limitation of standard masked
diffusion is its inability to effectively capture dependencies among tokens
that are predicted concurrently, leading to degraded generation quality when
dependencies among tokens are important. To explicitly model dependencies among
tokens, we propose Variational Masked Diffusion (VMD), a framework that
introduces latent variables into the masked diffusion process. Through
controlled experiments on synthetic datasets, we demonstrate that VMD
successfully learns dependencies that conventional masked diffusion fails to
capture. We further validate the effectiveness of our approach on Sudoku
puzzles and text datasets, where learning of dependencies among tokens improves
global consistency. Across these domains, VMD enhances both generation quality
and dependency awareness, highlighting the value of integrating variational
inference into masked diffusion. Our code is available at:
https://riccizz.github.io/VMD.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [357] [Structure-Aware Cooperative Ensemble Evolutionary Optimization on Combinatorial Problems with Multimodal Large Language Models](https://arxiv.org/abs/2510.21906)
*Jie Zhao,Kang Hao Cheong*

Main category: cs.NE

TL;DR: 本文提出利用图像编码和MLLM进行图数据结构感知优化，结合图稀疏化、合作进化框架和集成策略，实验证明能提升MLLM驱动进化优化的解质量和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统编码方案难以捕捉网络复杂结构特性，需要新方法进行图结构组合问题的优化。

Method: 采用基于图像的编码，利用MLLMs作为进化算子；使用图稀疏化技术简化结构；提出合作进化优化框架；引入集成策略。

Result: 在真实网络的多个任务实验中，该方法提升了MLLM驱动进化优化解的质量和可靠性。

Conclusion: 所提方法有效提升了MLLM驱动进化优化的效果。

Abstract: Evolutionary algorithms (EAs) have proven effective in exploring the vast
solution spaces typical of graph-structured combinatorial problems. However,
traditional encoding schemes, such as binary or numerical representations,
often fail to straightforwardly capture the intricate structural properties of
networks. Through employing the image-based encoding to preserve topological
context, this study utilizes multimodal large language models (MLLMs) as
evolutionary operators to facilitate structure-aware optimization over graph
data. To address the visual clutter inherent in large-scale network
visualizations, we leverage graph sparsification techniques to simplify
structures while maintaining essential structural features. To further improve
robustness and mitigate bias from different sparsification views, we propose a
cooperative evolutionary optimization framework that facilitates cross-domain
knowledge transfer and unifies multiple sparsified variants of diverse
structures. Additionally, recognizing the sensitivity of MLLMs to network
layout, we introduce an ensemble strategy that aggregates outputs from various
layout configurations through consensus voting. Finally, experiments on
real-world networks through various tasks demonstrate that our approach
improves both the quality and reliability of solutions in MLLM-driven
evolutionary optimization.

</details>


### [358] [Enabling Robust In-Context Memory and Rapid Task Adaptation in Transformers with Hebbian and Gradient-Based Plasticity](https://arxiv.org/abs/2510.21908)
*Siddharth Chaudhary*

Main category: cs.NE

TL;DR: 研究为Transformer添加生物启发的可塑性机制，对比不同机制在多任务表现，明确可塑性何时有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理时权重静态，而生物系统可通过突触可塑性持续适应，探究生物启发的可塑性能否让Transformer在序列中更快适应。

Method: 为仅解码器的Transformer添加快速权重模块，分别采用神经调节的Hebbian规则和基于梯度的可塑性机制更新。

Result: Hebbian可塑性在复制、回归和少样本分类任务中损失更低、少样本泛化更强；基于梯度的更新在长时信用分配任务中表现最佳；静态权重在关联短且线性可分时足够。

Conclusion: 显式可塑性通过实现快速、特定任务的适应来补充注意力机制，并明确不同可塑性机制最有效的场景。

Abstract: Large language models display in-context learning as an emergent effect of
scale, but they rely on static weights during inference. In contrast,
biological systems continually adapt via synaptic plasticity. We investigate
whether explicit, biologically inspired plasticity can endow Transformers with
faster in-sequence adaptation. To this end, we augment decoder-only
Transformers with fast-weight modules updated either by (i) a neuromodulated
Hebbian rule or (ii) the gradient-based plasticity mechanism of Duan et al.
(2023). Across copying, regression, and few-shot classification tasks
(CIFAR-FS, Omniglot), Hebbian plasticity consistently achieves lower loss and
stronger few-shot generalization, while gradient-based updates perform best on
long-horizon credit assignment. When associations are short and linearly
separable, static weights suffice, defining a clear boundary condition for when
plasticity helps. Analysis of learned modulatory signals reveals that
gradient-based rules maintain large, persistent updates, whereas Hebbian
plasticity is sharply gated around salient events. Together, these results show
that explicit plasticity complements attention by enabling rapid, task-specific
adaptation, and clarify when different plasticity mechanisms are most
effective.

</details>


### [359] [Probing the Representational Geometry of Color Qualia: Dissociating Pure Perception from Task Demands in Brains and AI Models](https://arxiv.org/abs/2510.22800)
*Jing Xu*

Main category: cs.NE

TL;DR: 本文通过对比AI模型和人脑的颜色感受质表征几何，发现多数模型与纯感知神经表征更匹配，揭示训练范式与架构的交互作用，还提供新的颜色感受质基准任务。


<details>
  <summary>Details</summary>
Motivation: 探究主观体验（感受质）的计算基础，这是认知神经科学的核心挑战。

Method: 使用独特的无报告范式fMRI数据集，通过表征相似性分析（RSA），在纯感知和任务调制感知两种条件下对比多种视觉模型与神经活动。

Result: 1. 几乎所有模型与纯感知的神经表征更匹配；2. 训练范式和架构存在关键交互作用，CLIP训练对不同架构影响不同。

Conclusion: 工作提供新的颜色感受质基准任务，揭示人工和生物视觉系统归纳偏差的根本差异，为开发更具神经合理性的模型提供指导。

Abstract: Probing the computational underpinnings of subjective experience, or qualia,
remains a central challenge in cognitive neuroscience. This project tackles
this question by performing a rigorous comparison of the representational
geometry of color qualia between state-of-the-art AI models and the human
brain. Using a unique fMRI dataset with a "no-report" paradigm, we use
Representational Similarity Analysis (RSA) to compare diverse vision models
against neural activity under two conditions: pure perception ("no-report") and
task-modulated perception ("report"). Our analysis yields three principal
findings. First, nearly all models align better with neural representations of
pure perception, suggesting that the cognitive processes involved in task
execution are not captured by current feedforward architectures. Second, our
analysis reveals a critical interaction between training paradigm and
architecture, challenging the simple assumption that Contrastive Language-Image
Pre-training(CLIP) training universally improves neural plausibility. In our
direct comparison, this multi-modal training method enhanced brain-alignment
for a vision transformer(ViT), yet had the opposite effect on a ConvNet. Our
work contributes a new benchmark task for color qualia to the field, packaged
in a Brain-Score compatible format. This benchmark reveals a fundamental
divergence in the inductive biases of artificial and biological vision systems,
offering clear guidance for developing more neurally plausible models.

</details>


### [360] [Graph Neural Network Assisted Genetic Algorithm for Structural Dynamic Response and Parameter Optimization](https://arxiv.org/abs/2510.22839)
*Sagnik Mukherjee*

Main category: cs.NE

TL;DR: 本文提出结合GNN代理模型与GA优化器的混合数据驱动框架优化结构参数，相比传统方法收敛性好、泛化性强且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在结构参数迭代优化任务中计算成本高，需要新方法解决该问题。

Method: 提出结合GNN代理模型与GA优化器的混合数据驱动框架，用Newmark Beta方法生成数据集，GNN学习参数与位移响应映射，GA搜索最优参数集。

Result: 该框架收敛性强、泛化性好，计算成本显著降低。

Conclusion: 结合机器学习代理模型与进化优化方法对自动化和智能结构设计有效。

Abstract: The optimization of structural parameters, such as mass(m), stiffness(k), and
damping coefficient(c), is critical for designing efficient, resilient, and
stable structures. Conventional numerical approaches, including Finite Element
Method (FEM) and Computational Fluid Dynamics (CFD) simulations, provide
high-fidelity results but are computationally expensive for iterative
optimization tasks, as each evaluation requires solving the governing equations
for every parameter combination. This study proposes a hybrid data-driven
framework that integrates a Graph Neural Network (GNN) surrogate model with a
Genetic Algorithm (GA) optimizer to overcome these challenges. The GNN is
trained to accurately learn the nonlinear mapping between structural parameters
and dynamic displacement responses, enabling rapid predictions without
repeatedly solving the system equations. A dataset of single-degree-of-freedom
(SDOF) system responses is generated using the Newmark Beta method across
diverse mass, stiffness, and damping configurations. The GA then searches for
globally optimal parameter sets by minimizing predicted displacements and
enhancing dynamic stability. Results demonstrate that the GNN and GA framework
achieves strong convergence, robust generalization, and significantly reduced
computational cost compared to conventional simulations. This approach
highlights the effectiveness of combining machine learning surrogates with
evolutionary optimization for automated and intelligent structural design.

</details>


### [361] [One-Timestep is Enough: Achieving High-performance ANN-to-SNN Conversion via Scale-and-Fire Neurons](https://arxiv.org/abs/2510.23383)
*Qiuyang Chen,Huiqi Yang,Qingyan Meng,Zhengyu Ma*

Main category: cs.NE

TL;DR: 提出单时间步ANN2SNN理论与实践框架，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有ANN2SNN方法依赖大时间步，导致高推理延迟和计算成本。

Method: 建立时空等价理论，引入SFN神经元，开发基于SFN的SFormer。

Result: 在图像分类、目标检测和实例分割实验中达SOTA，在ImageNet - 1K上T = 1时top - 1准确率88.8%。

Conclusion: 所提单时间步ANN2SNN方法有效，超越现有转换方法。

Abstract: Spiking Neural Networks (SNNs) are gaining attention as energy-efficient
alternatives to Artificial Neural Networks (ANNs), especially in
resource-constrained settings. While ANN-to-SNN conversion (ANN2SNN) achieves
high accuracy without end-to-end SNN training, existing methods rely on large
time steps, leading to high inference latency and computational cost. In this
paper, we propose a theoretical and practical framework for single-timestep
ANN2SNN. We establish the Temporal-to-Spatial Equivalence Theory, proving that
multi-timestep integrate-and-fire (IF) neurons can be equivalently replaced by
single-timestep multi-threshold neurons (MTN). Based on this theory, we
introduce the Scale-and-Fire Neuron (SFN), which enables effective
single-timestep ($T=1$) spiking through adaptive scaling and firing.
Furthermore, we develop the SFN-based Spiking Transformer (SFormer), a
specialized instantiation of SFN within Transformer architectures, where spike
patterns are aligned with attention distributions to mitigate the
computational, energy, and hardware overhead of the multi-threshold design.
Extensive experiments on image classification, object detection, and instance
segmentation demonstrate that our method achieves state-of-the-art performance
under single-timestep inference. Notably, we achieve 88.8% top-1 accuracy on
ImageNet-1K at $T=1$, surpassing existing conversion methods.

</details>


### [362] [Multi-Task Surrogate-Assisted Search with Bayesian Competitive Knowledge Transfer for Expensive Optimization](https://arxiv.org/abs/2510.23407)
*Yi Lu,Xiaoming Xue,Kai Zhang,Liming Zhang,Guodong Chen,Chenming Cao,Piyang Liu,Kay Chen Tan*

Main category: cs.NE

TL;DR: 传统进化优化处理昂贵优化问题（EOPs）有局限，代理辅助搜索（SAS）有冷启动问题，知识迁移有负迁移问题。本文提出贝叶斯竞争知识迁移（BCKT）方法改进多任务SAS，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统进化优化处理EOPs因评估调用有限面临挑战，SAS有冷启动问题，知识迁移应用于EOPs存在负迁移问题，现有方法评估知识可迁移性有局限。

Method: 从贝叶斯视角估计知识可迁移性，实现内任务和跨任务解决方案的准确竞争，自适应使用有前景的解决方案并抑制较差的方案。

Result: 通过实验验证了该方法在提升多任务和多目标问题的各种SAS算法方面的有效性，比较研究表明其优于同类算法且适用于现实场景。

Conclusion: 提出的BCKT方法能有效改进多任务SAS，在解决EOPs上有优势，代码开源可获取。

Abstract: Expensive optimization problems (EOPs) present significant challenges for
traditional evolutionary optimization due to their limited evaluation calls.
Although surrogate-assisted search (SAS) has become a popular paradigm for
addressing EOPs, it still suffers from the cold-start issue. In response to
this challenge, knowledge transfer has been gaining popularity for its ability
to leverage search experience from potentially related instances, ultimately
facilitating head-start optimization for more efficient decision-making.
However, the curse of negative transfer persists when applying knowledge
transfer to EOPs, primarily due to the inherent limitations of existing methods
in assessing knowledge transferability. On the one hand, a priori
transferability assessment criteria are intrinsically inaccurate due to their
imprecise understandings. On the other hand, a posteriori methods often
necessitate sufficient observations to make correct inferences, rendering them
inefficient when applied to EOPs. Considering the above, this paper introduces
a Bayesian competitive knowledge transfer (BCKT) method developed to improve
multi-task SAS (MSAS) when addressing multiple EOPs simultaneously.
Specifically, the transferability of knowledge is estimated from a Bayesian
perspective that accommodates both prior beliefs and empirical evidence,
enabling accurate competition between inner-task and inter-task solutions,
ultimately leading to the adaptive use of promising solutions while effectively
suppressing inferior ones. The effectiveness of our method in boosting various
SAS algorithms for both multi-task and many-task problems is empirically
validated, complemented by comparative studies that demonstrate its superiority
over peer algorithms and its applicability to real-world scenarios. The source
code of our method is available at https://github.com/XmingHsueh/MSAS-BCKT.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [363] [Prefetching Cache Optimization Using Graph Neural Networks: A Modular Framework and Conceptual Analysis](https://arxiv.org/abs/2510.21865)
*F. I. Qowy*

Main category: cs.PF

TL;DR: 本文提出基于图神经网络（GNN）的模块化框架来建模和预测图结构数据的访问模式，证明其优于传统方法，并为图驱动系统优化提供理论与实践基础。


<details>
  <summary>Details</summary>
Motivation: 传统预取策略依赖预定义启发式或简化统计模型，无法捕捉现代数据访问模式中的复杂非线性依赖，需要更好的方法。

Method: 引入模块化框架，包含路由映射器、图构造器、行走会话生成器和GNN预取模块，用于提取信息、创建图表示、模拟用户行为以及训练和推理。

Result: 通过详细概念分析表明基于GNN的方法能通过学习复杂依赖优于传统方法。

Conclusion: 为图驱动系统优化的未来研究提供了理论基础和可复制的实用管道。

Abstract: Caching and prefetching techniques are fundamental to modern computing,
serving to bridge the growing performance gap between processors and memory.
Traditional prefetching strategies are often limited by their reliance on
predefined heuristics or simplified statistical models, which fail to capture
the complex, non-linear dependencies in modern data access patterns. This paper
introduces a modular framework leveraging Graph Neural Networks (GNNs) to model
and predict access patterns within graph-structured data, focusing on web
navigation and hierarchical file systems. The toolchain consists of: a route
mapper for extracting structural information, a graph constructor for creating
graph representations, a walk session generator for simulating user behaviors,
and a gnn prefetch module for training and inference. We provide a detailed
conceptual analysis showing how GNN-based approaches can outperform
conventional methods by learning intricate dependencies. This work offers both
theoretical foundations and a practical, replicable pipeline for future
research in graph-driven systems optimization.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [364] [Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments](https://arxiv.org/abs/2510.21902)
*Timothé Boulet,Xavier Hinaut,Clément Moulin-Frier*

Main category: cs.SE

TL;DR: 首次对SWE - Agents在具身任务控制器生成上进行扩展评估，量化不同信息访问级别对其性能的影响，为未来研究提供基线结果。


<details>
  <summary>Details</summary>
Motivation: SWE - Agents在需精心设计信息发现的具身任务上的性能未被探索，需进行评估。

Method: 将Mini - SWE - Agent（MSWEA）用于解决Minigrid环境中的20个不同具身任务，对比不同信息访问条件下的代理性能。

Result: 量化了不同信息访问级别对SWE - Agent在具身任务上性能的影响，分析了静态代码分析和动态探索对解决任务的相对重要性。

Conclusion: 具身任务的控制器生成是SWE - Agents的关键评估领域，本研究为高效推理系统的未来研究提供了基线结果。

Abstract: Software Engineering Agents (SWE-Agents) have proven effective for
traditional software engineering tasks with accessible codebases, but their
performance for embodied tasks requiring well-designed information discovery
remains unexplored. We present the first extended evaluation of SWE-Agents on
controller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to
solve 20 diverse embodied tasks from the Minigrid environment. Our experiments
compare agent performance across different information access conditions: with
and without environment source code access, and with varying capabilities for
interactive exploration. We quantify how different information access levels
affect SWE-Agent performance for embodied tasks and analyze the relative
importance of static code analysis versus dynamic exploration for task solving.
This work establishes controller generation for embodied tasks as a crucial
evaluation domain for SWE-Agents and provides baseline results for future
research in efficient reasoning systems.

</details>


### [365] [TOM-SWE: User Mental Modeling For Software Engineering Agents](https://arxiv.org/abs/2510.21903)
*Xuhui Zhou,Valerie Chen,Zora Zhiruo Wang,Graham Neubig,Maarten Sap,Xingyao Wang*

Main category: cs.SE

TL;DR: 提出ToM - SWE架构解决编码代理推断用户意图问题，在基准测试和实际使用中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有编码代理在推断和跟踪用户意图方面存在困难，尤其是指令不明确或依赖上下文时。

Method: 引入ToM - SWE双代理架构，由SWE代理和ToM代理组成，ToM代理用于建模用户心理状态。

Result: 在两个软件工程基准测试中提高了任务成功率和用户满意度，在有状态SWE基准测试中任务成功率达59.7%，专业开发者使用时86%的时间认为有用。

Conclusion: 有状态的用户建模对实用编码代理具有重要价值。

Abstract: Recent advances in coding agents have made them capable of planning, editing,
running, and testing complex code bases. Despite their growing ability in
coding tasks, these systems still struggle to infer and track user intent,
especially when instructions are underspecified or context-dependent. To bridge
this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary
software-engineering (SWE) agent with a lightweight theory-of-mind (ToM)
partner agent dedicated to modeling the user's mental state. The ToM agent
infers user goals, constraints, and preferences from instructions and
interaction history, maintains a \textbf{persistent memory} of the user, and
provides user-related suggestions to the SWE agent. In two software engineering
benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task
success rates and user satisfaction. Notably, on the stateful SWE benchmark, a
newly introduced evaluation that provides agents with a user simulator along
with previous interaction histories, ToM-SWE achieves a substantially higher
task success rate of 59.7\% compared to 18.1\% for OpenHands, a
state-of-the-art SWE agent. Furthermore, in a three-week study with
professional developers using ToM-SWE in their daily work, participants found
it useful 86\% of the time, underscoring the value of stateful user modeling
for practical coding agents.

</details>


### [366] [A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case](https://arxiv.org/abs/2510.21933)
*Joao Correia,Daniel Coutinho,Marco Castelluccio,Caio Barbosa,Rafael de Mello,Anita Sarma,Alessandro Garcia,Marco Gerosa,Igor Steinmacher*

Main category: cs.SE

TL;DR: 研究评估RAG在Mozilla Firefox项目中辅助开发者的效果，发现RAG辅助响应更全面，有潜力应用于开源软件。


<details>
  <summary>Details</summary>
Motivation: 评估Retrieval - Augmented Generation (RAG)在Mozilla Firefox项目中辅助开发者的有效性。

Method: 对人类开发者、标准GPT模型和RAG增强的GPT模型的回复进行实证分析，用Mozilla开发者聊天室的真实查询，由Mozilla专家基于有用性、全面性和简洁性评估回复。

Result: RAG辅助回复比人类开发者更全面，几乎同样有用，但不够简洁、冗长。

Conclusion: RAG有潜力应用于开源软件以减轻核心维护者负担且不降低答案质量，未来优化检索机制和缩短回复可增强对大型项目开发者的辅助。

Abstract: The use of Large Language Models (LLMs) to support tasks in software
development has steadily increased over recent years. From assisting developers
in coding activities to providing conversational agents that answer newcomers'
questions. In collaboration with the Mozilla Foundation, this study evaluates
the effectiveness of Retrieval-Augmented Generation (RAG) in assisting
developers within the Mozilla Firefox project. We conducted an empirical
analysis comparing responses from human developers, a standard GPT model, and a
GPT model enhanced with RAG, using real queries from Mozilla's developer chat
rooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses
based on helpfulness, comprehensiveness, and conciseness. The results show that
RAG-assisted responses were more comprehensive than human developers (62.50% to
54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to
enhance developer assistance. However, the RAG responses were not as concise
and often verbose. The results show the potential to apply RAG-based tools to
Open Source Software (OSS) to minimize the load to core maintainers without
losing answer quality. Toning down retrieval mechanisms and making responses
even shorter in the future would enhance developer assistance in massive
projects like Mozilla Firefox.

</details>


### [367] [ArchISMiner: A Framework for Automatic Mining of Architectural Issue-Solution Pairs from Online Developer Communities](https://arxiv.org/abs/2510.21966)
*Musengamana Jean de Dieu,Ruiyin Li,Peng Liang,Mojtaba Shahin,Muhammad Waseem,Arif Ali Khan,Bangchao Wang,Mst Shamima Aktar*

Main category: cs.SE

TL;DR: 本文提出ArchISMiner框架从Stack Overflow挖掘架构知识，评估显示效果良好，还应用于其他论坛并发布数据集，能助开发者高效获取架构知识。


<details>
  <summary>Details</summary>
Motivation: Stack Overflow架构知识定位难，开发者手动筛选耗时且易出错。

Method: 框架含ArchPI和ArchISPE两部分，ArchPI训练评估多模型选最佳识别架构相关帖子，ArchISPE用间接监督方法提取架构问题 - 解决方案对。

Result: ArchPI最佳模型F1分数0.960，ArchISPE在SE和NLP领域超基线，用户研究验证成果质量，应用于三论坛发布超1.8万个问题 - 解决方案对数据集。

Conclusion: ArchISMiner能助开发者更准确高效从开发者社区获取架构知识。

Abstract: Stack Overflow (SO), a leading online community forum, is a rich source of
software development knowledge. However, locating architectural knowledge, such
as architectural solutions remains challenging due to the overwhelming volume
of unstructured content and fragmented discussions. Developers must manually
sift through posts to find relevant architectural insights, which is
time-consuming and error-prone. This study introduces ArchISMiner, a framework
for mining architectural knowledge from SO. The framework comprises two
complementary components: ArchPI and ArchISPE. ArchPI trains and evaluates
multiple models, including conventional ML/DL models, Pre-trained Language
Models (PLMs), and Large Language Models (LLMs), and selects the
best-performing model to automatically identify Architecture-Related Posts
(ARPs) among programming-related discussions. ArchISPE employs an indirect
supervised approach that leverages diverse features, including BERT embeddings
and local TextCNN features, to extract architectural issue-solution pairs. Our
evaluation shows that the best model in ArchPI achieves an F1-score of 0.960 in
ARP detection, and ArchISPE outperforms baselines in both SE and NLP fields,
achieving F1-scores of 0.883 for architectural issues and 0.894 for solutions.
A user study further validated the quality (e.g., relevance and usefulness) of
the identified ARPs and the extracted issue-solution pairs. Moreover, we
applied ArchISMiner to three additional forums, releasing a dataset of over 18K
architectural issue-solution pairs. Overall, ArchISMiner can help architects
and developers identify ARPs and extract succinct, relevant, and useful
architectural knowledge from developer communities more accurately and
efficiently. The replication package of this study has been provided at
https://github.com/JeanMusenga/ArchISPE

</details>


### [368] [FeaGPT: an End-to-End agentic-AI for Finite Element Analysis](https://arxiv.org/abs/2510.21993)
*Yupeng Qi,Ran Xu,Xu Chu*

Main category: cs.SE

TL;DR: 本文介绍FeaGPT框架，可通过对话界面实现完整几何-网格-模拟工作流，实验验证其自动化能力和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有工具只能自动化有限元分析（FEA）单个组件，需要一个能实现完整工作流的框架。

Method: 构建完全集成的Geometry - Mesh - Simulation - Analysis (GMSA) 管道，自动处理工程规范到计算结果的转换，包括意图解读、网格生成、模拟配置和多目标分析。

Result: 工业涡轮增压器案例和432个NACA翼型配置验证了系统的端到端自动化能力、可扩展性，能产生物理上合理的结果。

Conclusion: 自然语言界面可在保证分析严谨性的同时，让更多人使用先进计算工程工具。

Abstract: Large language models (LLMs) are establishing new paradigms for engineering
applications by enabling natural language control of complex computational
workflows. This paper introduces FeaGPT, the first framework to achieve
complete geometry-mesh-simulation workflows through conversational interfaces.
Unlike existing tools that automate individual FEA components, FeaGPT
implements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline
that transforms engineering specifications into validated computational results
without manual intervention. The system interprets engineering intent,
automatically generates physics-aware adaptive meshes, configures complete FEA
simulations with proper boundary condition inference, and performs
multi-objective analysis through closed-loop iteration.
  Experimental validation confirms complete end-to-end automation capability.
Industrial turbocharger cases (7-blade compressor and 12-blade turbine at
\SI{110000}{rpm}) demonstrate the system successfully transforms natural
language specifications into validated CalculiX simulations, producing
physically realistic results for rotating machinery analysis. Additional
validation through 432 NACA airfoil configurations confirms scalability for
parametric design exploration. These results demonstrate that natural language
interfaces can effectively democratize access to advanced computational
engineering tools while preserving analytical rigor.

</details>


### [369] [Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review](https://arxiv.org/abs/2510.22003)
*Stefan Julian Kooy,Jean Paul Sebastian Piest,Rob Henk Bemthuis*

Main category: cs.SE

TL;DR: 本文通过系统文献综述研究生成式AI对敏捷软件组织企业架构工作的影响，总结其用例、风险，提出能力建设和治理建议及研究议程。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑敏捷软件组织的企业架构工作，但相关影响的证据分散，需要进行系统研究。

Method: 采用Kitchenham和PRISMA既定的系统文献综述协议，对1697条记录进行审查，得到33项研究。

Result: 生成式AI主要支持设计构思、工件创建与改进、架构决策支持和知识检索；存在不透明和偏差、输出错误、隐私合规及社会惰化等风险；确定了新兴技能和组织推动因素。

Conclusion: 研究为生成式AI的负责任采用提供信息，加速数字化转型同时保障架构完整性。

Abstract: Generative AI (GenAI) is reshaping enterprise architecture work in agile
software organizations, yet evidence on its effects remains scattered. We
report a systematic literature review (SLR), following established SLR
protocols of Kitchenham and PRISMA, of 1,697 records, yielding 33 studies
across enterprise, solution, domain, business, and IT architect roles. GenAI
most consistently supports (i) design ideation and trade-off exploration; (ii)
rapid creation and refinement of artifacts (e.g., code, models, documentation);
and (iii) architectural decision support and knowledge retrieval. Reported
risks include opacity and bias, contextually incorrect outputs leading to
rework, privacy and compliance concerns, and social loafing. We also identify
emerging skills and competencies, including prompt engineering, model
evaluation, and professional oversight, and organizational enablers around
readiness and adaptive governance. The review contributes with (1) a mapping of
GenAI use cases and risks in agile architecting, (2) implications for
capability building and governance, and (3) an initial research agenda on
human-AI collaboration in architecture. Overall, the findings inform
responsible adoption of GenAI that accelerates digital transformation while
safeguarding architectural integrity.

</details>


### [370] [LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation](https://arxiv.org/abs/2510.22210)
*Gwihwan Go,Quan Zhang,Chijin Zhou,Zhao Wei,Yu Jiang*

Main category: cs.SE

TL;DR: 提出LSPRAG框架用于实时、语言无关的单元测试生成，评估显示其在多语言项目上提升了代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化单元测试生成方法难以跨语言泛化和实时开发，当前利用大语言模型的解决方案存在不足。

Method: 提出LSPRAG框架，利用现成的语言服务器协议后端实时为大语言模型提供精确符号定义和引用。

Result: 在Java、Go和Python开源项目评估中，相比基线最佳性能，LSPRAG在Golang、Java和Python上分别最多提升174.55%、213.31%和31.57%的行覆盖率。

Conclusion: LSPRAG框架在实时、语言无关的单元测试生成方面有良好效果，能有效提升代码覆盖率。

Abstract: Automated unit test generation is essential for robust software development,
yet existing approaches struggle to generalize across multiple programming
languages and operate within real-time development. While Large Language Models
(LLMs) offer a promising solution, their ability to generate high coverage test
code depends on prompting a concise context of the focal method. Current
solutions, such as Retrieval-Augmented Generation, either rely on imprecise
similarity-based searches or demand the creation of costly, language-specific
static analysis pipelines. To address this gap, we present LSPRAG, a framework
for concise-context retrieval tailored for real-time, language-agnostic unit
test generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)
back-ends to supply LLMs with precise symbol definitions and references in real
time. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware
context retrieval, requiring minimal per-language engineering effort. We
evaluated LSPRAG on open-source projects spanning Java, Go, and Python.
Compared to the best performance of baselines, LSPRAG increased line coverage
by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.

</details>


### [371] [Taming Silent Failures: A Framework for Verifiable AI Reliability](https://arxiv.org/abs/2510.22224)
*Guan-Yan Yang,Farn Wang*

Main category: cs.SE

TL;DR: 本文介绍了FAME框架，结合离线形式化综合和在线运行时监控，在自动驾驶感知系统中有效检测关键安全违规，为部署可信AI提供途径。


<details>
  <summary>Details</summary>
Motivation: 人工智能集成到安全关键系统会产生无声故障，需要应对此挑战。

Method: 提出FAME框架，将离线形式化综合的数学严谨性与在线运行时监控相结合。

Result: 在自动驾驶感知系统中，FAME成功检测到93.5%原本无声的关键安全违规。

Conclusion: FAME代表了从接受概率性能到在下一代系统中实施可证明安全的关键转变，为可靠性工程师提供了部署可信AI的实用、可认证途径。

Abstract: The integration of Artificial Intelligence (AI) into safety-critical systems
introduces a new reliability paradigm: silent failures, where AI produces
confident but incorrect outputs that can be dangerous. This paper introduces
the Formal Assurance and Monitoring Environment (FAME), a novel framework that
confronts this challenge. FAME synergizes the mathematical rigor of offline
formal synthesis with the vigilance of online runtime monitoring to create a
verifiable safety net around opaque AI components. We demonstrate its efficacy
in an autonomous vehicle perception system, where FAME successfully detected
93.5% of critical safety violations that were otherwise silent. By
contextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards,
we provide reliability engineers with a practical, certifiable pathway for
deploying trustworthy AI. FAME represents a crucial shift from accepting
probabilistic performance to enforcing provable safety in next-generation
systems.

</details>


### [372] [CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs](https://arxiv.org/abs/2510.22986)
*Junjie Huang,Minghua He,Jinyang Liu,Yintong Huo,Domenico Bianculli,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出CodeAD框架，用大语言模型自动合成用于日志异常检测的轻量级Python规则函数，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习、深度学习和大语言模型方法在日志异常检测中存在可解释性有限、推理成本高和预处理要求高等问题，基于规则的系统则需大量人工且难以扩展。

Method: 引入分层聚类和锚定采样策略构建对比日志窗口，采用代理工作流迭代生成、测试、修复和优化规则。

Result: 在三个公共数据集上，CodeAD的F1分数平均绝对提高3.6%，处理大数据集速度快4倍，成本低。

Conclusion: CodeAD是在线监控系统的实用可扩展解决方案，能在现实环境中实现可解释、高效和自动化的日志异常检测。

Abstract: Log-based anomaly detection (LogAD) is critical for maintaining the
reliability and availability of large-scale online service systems. While
machine learning, deep learning, and large language models (LLMs)-based methods
have advanced the LogAD, they often suffer from limited interpretability, high
inference costs, and extensive preprocessing requirements, limiting their
practicality for real-time, high-volume log analysis. In contrast, rule-based
systems offer efficiency and transparency, but require significant manual
effort and are difficult to scale across diverse and evolving environments. In
this paper, We present CodeAD, a novel framework that automatically synthesizes
lightweight Python rule functions for LogAD using LLMs. CodeAD introduces a
hierarchical clustering and anchor-grounded sampling strategy to construct
representative contrastive log windows, enabling LLMs to discern discriminative
anomaly patterns. To ensure robustness and generalizability, CodeAD employs an
agentic workflow that iteratively generates, tests, repairs, and refines the
rules until it meets correctness and abstraction requirements. The synthesized
rules are interpretable, lightweight, and directly executable on raw logs,
supporting efficient and transparent online anomaly detection. Our
comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird)
demonstrate that CodeAD achieves an average absolute improvement of 3.6% F1
score over the state-of-the-art baselines, while processing large datasets up
to 4x faster and at a fraction of the cost (total LLM invocation cost under 4
USD per dataset). These results highlight CodeAD as a practical and scalable
solution for online monitoring systems, enabling interpretable, efficient, and
automated LogAD in real-world environment.

</details>


### [373] [Understanding Self-Admitted Technical Debt in Test Code: An Empirical Study](https://arxiv.org/abs/2510.22249)
*Ibuki Nakamura,Yutaro Kashiwa,Bin Lin,Hajimu Iida*

Main category: cs.SE

TL;DR: 本文聚焦测试代码中的自我承认技术债务（SATD），通过实证研究揭示其分布、类型及与测试质量关系，开发自动分类模型，CodeBERT 模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 多数研究关注生产代码中的 SATD，忽视测试代码中 SATD 或假设其与生产代码中 SATD 特性相同，本文旨在填补该研究空白。

Method: 从 50 个代码库收集 17,766 条 SATD 注释，进行实证研究，并开发机器学习模型自动分类 SATD 注释。

Result: 测试代码中广泛存在 SATD，但与测试异味无直接关联；CodeBERT 模型在召回率和 F1 分数上优于其他模型，不同类型 SATD 表现有差异。

Conclusion: 揭示了测试代码中 SATD 的性质，提出测试代码中 SATD 类型的综合分类，CodeBERT 模型更适合 SATD 分类管理。

Abstract: Developers often opt for easier but non-optimal implementation to meet
deadlines or create rapid prototypes, leading to additional effort known as
technical debt to improve the code later. Oftentimes, developers explicitly
document the technical debt in code comments, referred to as Self-Admitted
Technical Debt (SATD). Numerous researchers have investigated the impact of
SATD on different aspects of software quality and development processes.
However, most of these studies focus on SATD in production code, often
overlooking SATD in the test code or assuming that it shares similar
characteristics with SATD in production code. In fact, a significant amount of
SATD is also present in the test code, with many instances not fitting into
existing categories for the production code. This study aims to fill this gap
and disclose the nature of SATD in the test code by examining its distribution
and types. Moreover, the relation between its presence and test quality is also
analyzed. Our empirical study, involving 17,766 SATD comments (14,987 from
production code, 2,779 from test code) collected from 50 repositories,
demonstrates that while SATD widely exists in test code, it is not directly
associated with test smells. Our study also presents comprehensive categories
of SATD types in the test code, and machine learning models are developed to
automatically classify SATD comments based on their types for easier
management. Our results show that the CodeBERT-based model outperforms other
machine learning models in terms of recall and F1-score. However, the
performance varies on different types of SATD.

</details>


### [374] [Ten Simple Rules for AI-Assisted Coding in Science](https://arxiv.org/abs/2510.22254)
*Eric W. Bridgeford,Iain Campbell,Zijao Chen,Zhicheng Lin,Harrison Ritz,Joachim Vandekerckhove,Russell A. Poldrack*

Main category: cs.SE

TL;DR: 本文提出十条AI辅助编程实用规则，以平衡利用AI能力与保持科学严谨性。


<details>
  <summary>Details</summary>
Motivation: AI编码工具用于科学计算时，代码质量和科学有效性存疑，需相关规则指导。

Method: 围绕问题准备与理解、管理上下文与交互、测试与验证、代码质量保证与迭代改进四个关键主题，制定十条实用规则。

Result: 得出十条AI辅助编程的实用规则。

Conclusion: 这些规则能帮助研究人员利用AI加速软件开发，同时确保代码符合可靠性、可重复性和科学有效性标准。

Abstract: While AI coding tools have demonstrated potential to accelerate software
development, their use in scientific computing raises critical questions about
code quality and scientific validity. In this paper, we provide ten practical
rules for AI-assisted coding that balance leveraging capabilities of AI with
maintaining scientific and methodological rigor. We address how AI can be
leveraged strategically throughout the development cycle with four key themes:
problem preparation and understanding, managing context and interaction,
testing and validation, and code quality assurance and iterative improvement.
These principles serve to emphasize maintaining human agency in coding
decisions, establishing robust validation procedures, and preserving the domain
expertise essential for methodologically sound research. These rules are
intended to help researchers harness AI's transformative potential for faster
software development while ensuring that their code meets the standards of
reliability, reproducibility, and scientific validity that research integrity
demands.

</details>


### [375] [Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus](https://arxiv.org/abs/2510.22318)
*Tuan-Phong Ngo,Bao-Ngoc Duong,Tuan-Anh Hoang,Joshua Dwight,Ushik Shrestha Khwakhali*

Main category: cs.SE

TL;DR: 本文探索大语言模型（LLMs）如何补充ISTQB框架用于高等教育，创建数据集、开发优化提示、评估LLMs并给出整合建议。


<details>
  <summary>Details</summary>
Motivation: 软件测试教育需不断更新，ISTQB框架虽广泛应用，但结合LLMs的学习未充分探索，因此研究LLMs对ISTQB框架的补充作用。

Method: 创建ISTQB对齐数据集，开发领域优化提示，对数据集上的先进LLMs进行系统评估。

Result: 创建涵盖28份样本考试和1145个问题的数据集，开发优化提示，评估LLMs并给出整合建议。

Conclusion: LLMs在支持ISTQB认证准备方面有前景，为其在高等软件工程教育中的广泛应用奠定基础。

Abstract: Software testing is a critical component in the software engineering field
and is important for software engineering education. Thus, it is vital for
academia to continuously improve and update educational methods to reflect the
current state of the field. The International Software Testing Qualifications
Board (ISTQB) certification framework is globally recognized and widely adopted
in industry and academia. However, ISTQB-based learning has been rarely applied
with recent generative artificial intelligence advances. Despite the growing
capabilities of large language models (LLMs), ISTQB-based learning and
instruction with LLMs have not been thoroughly explored. This paper explores
and evaluates how LLMs can complement the ISTQB framework for higher education.
The findings present four key contributions: (i) the creation of a
comprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28
sample exams and 1,145 questions; (ii) the development of a domain-optimized
prompt that enhances LLM precision and explanation quality on ISTQB tasks;
(iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and
(iv) actionable insights and recommendations for integrating LLMs into software
testing education. These findings highlight the promise of LLMs in supporting
ISTQB certification preparation and offer a foundation for their broader use in
software engineering at higher education.

</details>


### [376] [Operationalizing Large Language Models with Design-Aware Contexts for Code Comment Generation](https://arxiv.org/abs/2510.22338)
*Aritra Mitra,Srijoni Majumdar,Anamitra Mukhopadhyay,Partha Pratim Das,Paul D Clough,Partha Pratim Chakrabarti*

Main category: cs.SE

TL;DR: 研究大语言模型结合设计文档为新手代码生成有用注释的可行性


<details>
  <summary>Details</summary>
Motivation: 新手代码缺乏注释标准，其注释常无用且增加代码维护时间，需寻找生成更好注释的方法

Method: 探索以设计文档为上下文，让大语言模型生成更有用注释

Result: 未提及

Conclusion: 未提及

Abstract: Comments are very useful to the flow of code development. With the increasing
commonality of code, novice coders have been creating a significant amount of
codebases. Due to lack of commenting standards, their comments are often
useless, and increase the time taken to further maintain codes. This study
intends to find the usefulness of large language models (LLMs) in these cases
to generate potentially better comments. This study focuses on the feasibility
of design documents as a context for the LLMs to generate more useful comments,
as design documents are often used by maintainers to understand code when
comments do not suffice.

</details>


### [377] [A First Look at the Self-Admitted Technical Debt in Test Code: Taxonomy and Detection](https://arxiv.org/abs/2510.22409)
*Shahidul Islam,Md Nahidul Islam Opu,Shaowei Wang,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 本文首次大规模分析测试代码中的自承技术债务（SATD），手动分析后构建分类法，评估检测工具和大语言模型，发现现有方法均难以可靠检测。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注源代码中的SATD，而测试代码中SATD的存在和影响未受关注，存在研究空白。

Method: 从1000个开源Java项目的160万条注释中随机抽取50000条进行手动分析，构建分类法；评估现有SATD检测工具和大语言模型。

Result: 识别出615条SATD注释并分为15类；现有工具中MAT表现最佳但召回率一般；大语言模型检测精度低。

Conclusion: 现有方法和当前大语言模型无法可靠检测测试代码中的SATD，为后续特定于测试代码的SATD研究奠定基础。

Abstract: Self-admitted technical debt (SATD) refers to comments in which developers
explicitly acknowledge code issues, workarounds, or suboptimal solutions. SATD
is known to significantly increase software maintenance effort. While extensive
research has examined SATD in source code, its presence and impact in test code
have received no focused attention, leaving a significant gap in our
understanding of how SATD manifests in testing contexts.
  This study, the first of its kind, investigates SATD in test code by manually
analyzing 50,000 comments randomly sampled from 1.6 million comments across
1,000 open-source Java projects. From this sample, after manual analysis and
filtering, we identified 615 SATD comments and classified them into 15 distinct
categories, building a taxonomy of test code SATD. To investigate whether test
code SATD can be detected automatically, we evaluated existing SATD detection
tools, as well as both open-source and proprietary LLMs. Among the existing
tools, MAT performed the best, albeit with moderate recall. To our surprise,
both open-source and proprietary LLMs exhibited poor detection accuracy,
primarily due to low precision. These results indicate that neither existing
approaches nor current LLMs can reliably detect SATD in test code.
  Overall, this work provides the first large-scale analysis of SATD in test
code, a nuanced understanding of its types, and the limitations of current SATD
detection methods. Our findings lay the groundwork for future research on test
code-specific SATD.

</details>


### [378] [A Multifaceted View on Discrimination in Software Development Careers](https://arxiv.org/abs/2510.22457)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: 研究分析开发者调查开放性回答，发现软件工程领域除年龄和性别歧视，政治、宗教等歧视也存在，呼吁关注多方面歧视。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程领域多样性和包容性讨论多聚焦性别和种族差异，而其他形式歧视受关注少，需全面了解歧视情况。

Method: 对800份开放性调查回复进行二次分析，涵盖年龄、性别、种族和残疾等多身份维度。

Result: 年龄和性别歧视是最常报告的职场问题，政治和宗教观点歧视也受关注；女性多认为性别是主要歧视来源，各性别都有照顾责任相关歧视；女性和非二元性别者面临职场问题比例更高。

Conclusion: 呼吁研究界认识到软件开发中歧视是多方面的，设计研究时考虑更多相关方面。

Abstract: Conversations around diversity and inclusion in software engineering often
focus on gender and racial disparities. However, the State of the Developer
Nation 2025 survey with 8,717 participants revealed that other forms of
discrimination are similarly prevalent but receive considerably less attention.
This includes discrimination based on age, political perspective, disabilities,
or cognitive differences such as neurodivergence. We conducted a secondary
analysis of 800 open-ended survey responses to examine patterns of perceived
discrimination, as well as related challenges and negative impacts. Our study
covers multiple identity facets, including age, gender, race, and disability.
We found that age- and gender-related discrimination was the most frequently
reported workplace issue, but discrimination based on political and religious
views emerged as further notable concerns. Most of the participants who
identified as female cited gender as the primary source of discrimination,
often accompanied by intersectional factors such as race, political views, age,
or sexual orientation. Discrimination related to caregiving responsibilities
was reported by all gender identities. Regarding the negative impacts of
workplace issues, many participants described modifying their appearance or
behavior in response to gender biases. Gender also appeared to influence
broader career challenges, as women and non-binary respondents reported
experiencing almost all workplace issues at higher rates, particularly
discrimination (35%) and mental health challenges (62%). Our goal is to raise
awareness in the research community that discrimination in software development
is multifaceted, and to encourage researchers to select and assess relevant
facets beyond age and gender when designing software engineering studies.

</details>


### [379] [Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL](https://arxiv.org/abs/2510.22530)
*Sungmin Kang,Sumi Yun,Jingun Hong,Shin Yoo,Gabin An*

Main category: cs.SE

TL;DR: 本文提出用于定位崩溃的LLM代理AutoCrashFL，仅需被测程序的崩溃转储和对应源代码仓库访问权限，经评估比基线更有效，展示了LLM代理在工业规模部署的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统故障定位技术在大型工业软件中测量覆盖率成本过高，难以应用，需新方法在工业环境应用故障定位。

Method: 提出AutoCrashFL，通过仅使用被测程序的崩溃转储和对应源代码仓库访问权限进行崩溃定位。

Result: 对SAP HANA真实世界崩溃进行评估，AutoCrashFL在定位方面更有效，顶部识别出30%的崩溃，而基线为17%。

Conclusion: AutoCrashFL具有实用特性，对复杂错误更有效且能表明结果置信度，展示了LLM代理在工业规模部署的实用性。

Abstract: Fault Localization (FL) aims to identify root causes of program failures. FL
typically targets failures observed from test executions, and as such, often
involves dynamic analyses to improve accuracy, such as coverage profiling or
mutation testing. However, for large industrial software, measuring coverage
for every execution is prohibitively expensive, making the use of such
techniques difficult. To address these issues and apply FL in an industrial
setting, this paper proposes AutoCrashFL, an LLM agent for the localization of
crashes that only requires the crashdump from the Program Under Test (PUT) and
access to the repository of the corresponding source code. We evaluate
AutoCrashFL against real-world crashes of SAP HANA, an industrial software
project consisting of more than 35 million lines of code. Experiments reveal
that AutoCrashFL is more effective in localization, as it identified 30%
crashes at the top, compared to 17% achieved by the baseline. Through thorough
analysis, we find that AutoCrashFL has attractive practical properties: it is
relatively more effective for complex bugs, and it can indicate confidence in
its results. Overall, these results show the practicality of LLM agent
deployment on an industrial scale.

</details>


### [380] [DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices](https://arxiv.org/abs/2510.22613)
*Songhan Zhang,Aoyang Fang,Yifan Yang,Ruiyi Cheng,Xiaoying Tang,Pinjia He*

Main category: cs.SE

TL;DR: 云原生微服务故障诊断有挑战，现有方法有局限，提出DynaCausal框架，评估显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 云原生微服务依赖复杂，现有根因分析方法在捕捉动态行为和服务关系上有限，存在三个关键挑战，需要改进。

Method: 提出DynaCausal框架，统一多模态动态信号，通过交互感知表征学习捕捉时空依赖，引入动态对比机制，采用因果优先的成对排序目标。

Result: 在公共基准测试中，DynaCausal始终优于现有方法，平均AC@1为0.63，绝对增益0.25 - 0.46。

Conclusion: DynaCausal能在高度动态的微服务环境中进行准确且可解释的诊断。

Abstract: Cloud-native microservices enable rapid iteration and scalable deployment but
also create complex, fast-evolving dependencies that challenge reliable
diagnosis. Existing root cause analysis (RCA) approaches, even with multi-modal
fusion of logs, traces, and metrics, remain limited in capturing dynamic
behaviors and shifting service relationships. Three critical challenges
persist: (i) inadequate modeling of cascading fault propagation, (ii)
vulnerability to noise interference and concept drift in normal service
behavior, and (iii) over-reliance on service deviation intensity that obscures
true root causes. To address these challenges, we propose DynaCausal, a dynamic
causality-aware framework for RCA in distributed microservice systems.
DynaCausal unifies multi-modal dynamic signals to capture time-varying
spatio-temporal dependencies through interaction-aware representation learning.
It further introduces a dynamic contrastive mechanism to disentangle true fault
indicators from contextual noise and adopts a causal-prioritized pairwise
ranking objective to explicitly optimize causal attribution. Comprehensive
evaluations on public benchmarks demonstrate that DynaCausal consistently
surpasses state-of-the-art methods, attaining an average AC@1 of 0.63 with
absolute gains from 0.25 to 0.46, and delivering both accurate and
interpretable diagnoses in highly dynamic microservice environments.

</details>


### [381] [Does In-IDE Calibration of Large Language Models work at Scale?](https://arxiv.org/abs/2510.22614)
*Roham Koohestani,Agnia Sergeyuk,David Gros,Claudio Spiess,Sergey Titov,Prem Devanbu,Maliheh Izadi*

Main category: cs.SE

TL;DR: 研究在IDE中对代码模型进行校准的可行性，分析校准方法和向开发者传达可靠性信号的设计原则。


<details>
  <summary>Details</summary>
Motivation: 大语言模型引入IDE带来代码可靠性挑战，现有校准工作缺乏大规模证据，需研究在IDE中应用代码模型校准的可行性。

Method: 开发可扩展灵活的校准框架评估校准器效果；对超2400万开发者交互数据进行大规模分析；与设计师和开发者开展多阶段设计研究。

Result: 基于Platt - scaling的通用事后校准模型平均不能提高模型置信信号可靠性；动态个性化校准效果依赖用户交互数据量；开发者偏好通过非数值、颜色编码指标呈现可靠性信号。

Conclusion: 在IDE中应用代码模型校准有一定挑战，需考虑校准方法的有效性和用户交互数据量，且应采用合适方式向开发者传达可靠性信号。

Abstract: The introduction of large language models into integrated development
environments (IDEs) is revolutionizing software engineering, yet it poses
challenges to the usefulness and reliability of Artificial
Intelligence-generated code. Post-hoc calibration of internal model confidences
aims to align probabilities with an acceptability measure. Prior work suggests
calibration can improve alignment, but at-scale evidence is limited. In this
work, we investigate the feasibility of applying calibration of code models to
an in-IDE context. We study two aspects of the problem: (1) the technical
method for implementing confidence calibration and improving the reliability of
code generation models, and (2) the human-centered design principles for
effectively communicating reliability signal to developers. First, we develop a
scalable and flexible calibration framework which can be used to obtain
calibration weights for open-source models using any dataset, and evaluate
whether calibrators improve the alignment between model confidence and
developer acceptance behavior. Through a large-scale analysis of over 24
million real-world developer interactions across multiple programming
languages, we find that a general, post-hoc calibration model based on
Platt-scaling does not, on average, improve the reliability of model confidence
signals. We also find that while dynamically personalizing calibration to
individual users can be effective, its effectiveness is highly dependent on the
volume of user interaction data. Second, we conduct a multi-phase design study
with 3 expert designers and 153 professional developers, combining
scenario-based design, semi-structured interviews, and survey validation,
revealing a clear preference for presenting reliability signals via
non-numerical, color-coded indicators within the in-editor code generation
workflow.

</details>


### [382] [Collaborative LLM Agents for C4 Software Architecture Design Automation](https://arxiv.org/abs/2510.22787)
*Kamil Szczepanik,Jarosław A. Chudziak*

Main category: cs.SE

TL;DR: 提出基于大语言模型（LLM）的多智能体系统自动化创建C4软件架构模型，并用混合评估框架评估质量，测试显示效果良好，对比了不同LLM的优势。


<details>
  <summary>Details</summary>
Motivation: C4软件架构模型的创建目前为手动操作，耗时费力，需要自动化方法。

Method: 引入基于LLM的多智能体系统模拟特定角色专家对话分析需求并生成C4模型的视图，用混合评估框架评估质量。

Result: 在五个典型系统简报上测试，能快速创建C4模型，编译成功率高，语义保真，对比四个先进LLM显示出不同架构设计优势。

Conclusion: 该研究为自动化软件架构设计及其评估方法做出了贡献。

Abstract: Software architecture design is a fundamental part of creating every software
system. Despite its importance, producing a C4 software architecture model, the
preferred notation for such architecture, remains manual and time-consuming. We
introduce an LLM-based multi-agent system that automates this task by
simulating a dialogue between role-specific experts who analyze requirements
and generate the Context, Container, and Component views of the C4 model.
Quality is assessed with a hybrid evaluation framework: deterministic checks
for structural and syntactic integrity and C4 rule consistency, plus semantic
and qualitative scoring via an LLM-as-a-Judge approach. Tested on five
canonical system briefs, the workflow demonstrates fast C4 model creation,
sustains high compilation success, and delivers semantic fidelity. A comparison
of four state-of-the-art LLMs shows different strengths relevant to
architectural design. This study contributes to automated software architecture
design and its evaluation methods.

</details>


### [383] [On the Freshness of Pinned Dependencies in Maven](https://arxiv.org/abs/2510.22815)
*Vasudev Vikram,Yuvraj Agarwal,Rohan Padhye*

Main category: cs.SE

TL;DR: 研究软件生态系统依赖固定问题，定义概念，实证显示多数依赖固定陈旧且缺安全修复，提出Pin - Freshener方法可助力安全升级。


<details>
  <summary>Details</summary>
Motivation: 理解依赖固定的频率和后果，解决使用过时依赖带来的安全风险问题。

Method: 定义陈旧和新鲜固定概念，进行实证研究，提出Pin - Freshener方法并评估。

Result: 超60%流行Maven库消费者有陈旧依赖固定，10%升级可减少安全漏洞；Pin - Freshener用少量额外测试套件可增加依赖覆盖，能为超3000消费者提供安全升级信号。

Conclusion: Pin - Freshener能为开发者提供额外信号，优于现有做法。

Abstract: Library dependencies in software ecosystems play a crucial role in the
development of software. As newer releases of these libraries are published,
developers may opt to pin their dependencies to a particular version. While
pinning may have benefits in ensuring reproducible builds and avoiding breaking
changes, it bears larger risks in using outdated dependencies that may contain
bugs and security vulnerabilities. To understand the frequency and consequences
of dependency pinning, we first define the concepts of stale and fresh pins,
which are distinguished based on how outdated the dependency is relative to the
release date of the project. We conduct an empirical study to show that over
60% of consumers of popular Maven libraries contain stale pins to their
dependencies, with some outdated versions over a year old. These pinned
versions often miss out on security fixes; we find that 10% of all dependency
upgrades in our dataset to the latest minor or patch version would reduce
security vulnerabilities.
  We prototype an approach called Pin-Freshener that can encourage developers
to freshen their pins by leveraging the insight that crowdsourced tests of peer
projects can provide additional signal for the safety of an upgrade. Running
Pin-Freshener on dependency upgrades shows that just 1-5 additional test suites
can provide 35-100% more coverage of a dependency, compared to that of a single
consumer test suite. Our evaluation on real-world pins to the top 500 popular
libraries in Maven shows that Pin-Freshener can provide an additional signal of
at least 5 passing crowdsourced test suites to over 3,000 consumers to safely
perform an upgrade that reduces security vulnerabilities. Pin-Freshener can
provide practical confidence to developers by offering additional signal beyond
their own test suites, representing an improvement over current practices.

</details>


### [384] [TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation](https://arxiv.org/abs/2510.23010)
*Ming-Tung Shen,Yuh-Jzer Joung*

Main category: cs.SE

TL;DR: 提出TALM框架解决代码生成中多智能体框架局限，实验证明其在代码生成任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架在代码生成中存在工作流僵化和推理恢复成本高的问题。

Method: 提出TALM框架，采用可扩展树状协作结构，结合分治策略，还有长期记忆模块。

Result: 在HumanEval、BigCodeBench和ClassEval基准测试中，TALM展现出强大推理性能和高令牌效率。

Conclusion: TALM在复杂代码生成任务中具有鲁棒性和实用价值。

Abstract: Agentic code generation requires large language models (LLMs) capable of
complex context management and multi-step reasoning. Prior multi-agent
frameworks attempt to address these challenges through collaboration, yet they
often suffer from rigid workflows and high reasoning recovery costs. To
overcome these limitations, we propose TALM (Tree-Structured Multi-Agent
Framework with Long-Term Memory), a dynamic framework that integrates
structured task decomposition, localized re-reasoning, and long-term memory
mechanisms. TALM employs an extensible tree-based collaboration structure. The
parent-child relationships, when combined with a divide-and-conquer strategy,
enhance reasoning flexibility and enable efficient error correction across
diverse task scopes. Furthermore, a long-term memory module enables semantic
querying and integration of prior knowledge, supporting implicit
self-improvement through experience reuse. Experimental results on HumanEval,
BigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently
delivers strong reasoning performance and high token efficiency, highlighting
its robustness and practical utility in complex code generation tasks.

</details>


### [385] [From Online User Feedback to Requirements: Evaluating Large Language Models for Classification and Specification Tasks](https://arxiv.org/abs/2510.23055)
*Manjeshwar Aniruddh Mallya,Alessio Ferrari,Mohammad Amin Zadenoori,Jacek Dąbrowski*

Main category: cs.SE

TL;DR: 评估轻量级大语言模型在需求工程任务中的表现，取得一定成果并给出相关贡献。


<details>
  <summary>Details</summary>
Motivation: 在线用户反馈分析有挑战，大语言模型有潜力但在需求工程中应用未充分探索。

Method: 评估五个轻量级开源大语言模型在三项需求工程任务上的表现，通过数据集测分类性能、人工评估规范质量。

Result: 大语言模型在分类任务中F1值约0.47 - 0.68，规范质量平均约3/5。

Conclusion: 新探索轻量级大语言模型用于反馈驱动的需求开发，给出评估、复制包及能力和局限见解。

Abstract: [Context and Motivation] Online user feedback provides valuable information
to support requirements engineering (RE). However, analyzing online user
feedback is challenging due to its large volume and noise. Large language
models (LLMs) show strong potential to automate this process and outperform
previous techniques. They can also enable new tasks, such as generating
requirements specifications.
  [Question-Problem] Despite their potential, the use of LLMs to analyze user
feedback for RE remains underexplored. Existing studies offer limited empirical
evidence, lack thorough evaluation, and rarely provide replication packages,
undermining validity and reproducibility.
  [Principal Idea-Results] We evaluate five lightweight open-source LLMs on
three RE tasks: user request classification, NFR classification, and
requirements specification generation. Classification performance was measured
on two feedback datasets, and specification quality via human evaluation. LLMs
achieved moderate-to-high classification accuracy (F1 ~ 0.47-0.68) and
moderately high specification quality (mean ~ 3/5).
  [Contributions] We newly explore lightweight LLMs for feedback-driven
requirements development. Our contributions are: (i) an empirical evaluation of
lightweight LLMs on three RE tasks, (ii) a replication package, and (iii)
insights into their capabilities and limitations for RE.

</details>


### [386] [Checkstyle+: Reducing Technical Debt Through The Use of Linters with LLMs](https://arxiv.org/abs/2510.23068)
*Ella Dodor,Cristina V. Lopes*

Main category: cs.SE

TL;DR: 提出Checkstyle+结合大语言模型增强Checkstyle，在检测语义细微规则违规上优于标准Checkstyle。


<details>
  <summary>Details</summary>
Motivation: 传统代码检查工具难以检测需更细致理解代码的风格规则，为提高代码风格检查能力。

Method: 提出混合方法Checkstyle+，结合大语言模型能力增强Checkstyle。

Result: 在380个Java代码文件样本上评估，Checkstyle+在检测语义细微规则违规上表现优于标准Checkstyle。

Conclusion: Checkstyle+能有效检测传统规则分析难以发现的代码风格违规。

Abstract: Good code style improves program readability, maintainability, and
collaboration, and is an integral component of software quality. Developers,
however, often cut corners when following style rules, leading to the wide
adoption of tools such as linters in professional software development
projects. Traditional linters like Checkstyle operate using rigid, rule-based
mechanisms that effectively detect many surface-level violations. However, in
most programming languages, there is a subset of style rules that require a
more nuanced understanding of code, and fall outside the scope of such static
analysis. In this paper, we propose Checkstyle+, a hybrid approach that
augments Checkstyle with large language model (LLM) capabilities, to identify
style violations that elude the conventional rule-based analysis. Checkstyle+
is evaluated on a sample of 380 Java code files, drawn from a broader dataset
of 30,800 real-world Java programs sourced from accepted Codeforces
submissions. The results show that Checkstyle+ achieves superior performance
over standard Checkstyle in detecting violations of the semantically nuanced
rules.

</details>


### [387] [Validating Formal Specifications with LLM-generated Test Cases](https://arxiv.org/abs/2510.23350)
*Alcino Cunha,Nuno Macedo*

Main category: cs.SE

TL;DR: 本文实证评估了使用预训练大语言模型从自然语言需求自动生成测试用例，聚焦Alloy语言简单领域模型结构需求测试用例，GPT - 5在生成测试用例上效果较好。


<details>
  <summary>Details</summary>
Motivation: 传统指定测试用例的验证技术繁琐且易出错，导致用户可能跳过该验证任务，因此想借助预训练大语言模型自动化生成测试用例。

Method: 对预训练大语言模型（重点是GPT - 5）进行实证评估，也报告了其他闭源和开源大语言模型的结果。

Result: GPT - 5能有效生成语法正确、满足（或不满足）给定需求的正（负）测试用例，可检测出许多人类编写的错误规范。

Conclusion: 在从自然语言需求为Alloy语言的简单领域模型结构需求生成测试用例方面，GPT - 5已具有较好效果。

Abstract: Validation is a central activity when developing formal specifications.
Similarly to coding, a possible validation technique is to define upfront test
cases or scenarios that a future specification should satisfy or not.
Unfortunately, specifying such test cases is burdensome and error prone, which
could cause users to skip this validation task. This paper reports the results
of an empirical evaluation of using pre-trained large language models (LLMs) to
automate the generation of test cases from natural language requirements. In
particular, we focus on test cases for structural requirements of simple domain
models formalized in the Alloy specification language. Our evaluation focuses
on the state-of-art GPT-5 model, but results from other closed- and open-source
LLMs are also reported. The results show that, in this context, GPT-5 is
already quite effective at generating positive (and negative) test cases that
are syntactically correct and that satisfy (or not) the given requirement, and
that can detect many wrong specifications written by humans.

</details>


### [388] [Floating-Point Neural Network Verification at the Software Level](https://arxiv.org/abs/2510.23389)
*Edoardo Manino,Bruno Farias,Rafael Sá Menezes,Fedor Shmarov,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: 论文展示如何通过考虑浮点实现来验证神经网络安全性，构建NeuroCodeBench 2.0基准，评估软件验证器，现有工具平均解决率11%且有3%错误率，基准有积极影响。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络验证技术无法在软件层面确保无故障，需验证神经网络安全性。

Method: 明确考虑神经网络浮点实现，构建包含912个验证示例的NeuroCodeBench 2.0基准，用C语言编写验证套件并兼容SV - COMP格式，对八个软件验证器进行评估。

Result: 现有自动化验证工具平均能正确解决11%的基准问题，产生约3%的错误判定，基准发布有积极影响。

Conclusion: 通过构建基准评估验证器，指出当前自动化验证工具在神经网络代码验证方面有提升空间，基准已产生积极作用。

Abstract: The behaviour of neural network components must be proven correct before
deployment in safety-critical systems. Unfortunately, existing neural network
verification techniques cannot certify the absence of faults at the software
level. In this paper, we show how to specify and verify that neural networks
are safe, by explicitly reasoning about their floating-point implementation. In
doing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural
network verification examples that cover activation functions, common layers,
and full neural networks of up to 170K parameters. Our verification suite is
written in plain C and is compatible with the format of the International
Competition on Software Verification (SV-COMP). Thanks to it, we can conduct
the first rigorous evaluation of eight state-of-the-art software verifiers on
neural network code. The results show that existing automated verification
tools can correctly solve an average of 11% of our benchmark, while producing
around 3% incorrect verdicts. At the same time, a historical analysis reveals
that the release of our benchmark has already had a significantly positive
impact on the latter.

</details>


### [389] [Tracing Distribution Shifts with Causal System Maps](https://arxiv.org/abs/2510.23528)
*Joran Leest,Ilias Gerostathopoulos,Patricia Lago,Claudia Raibulet*

Main category: cs.SE

TL;DR: 提出ML System Maps来解决机器学习系统监测中分布偏移根源分析难题，并给出开发和评估研究议程。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习系统监测标准做法聚焦检测分布偏移而非原因，根源分析常依赖手动追踪。

Method: 提出ML System Maps，通过分层视图明确环境与机器学习系统内部传播路径。

Result: 无明确结果描述。

Conclusion: 提出该方法及开发和评估的研究议程。

Abstract: Monitoring machine learning (ML) systems is hard, with standard practice
focusing on detecting distribution shifts rather than their causes. Root-cause
analysis often relies on manual tracing to determine whether a shift is caused
by software faults, data-quality issues, or natural change. We propose ML
System Maps -- causal maps that, through layered views, make explicit the
propagation paths between the environment and the ML system's internals,
enabling systematic attribution of distribution shifts. We outline the approach
and a research agenda for its development and evaluation.

</details>


### [390] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 提出合成生成高质量软件bug的新方法，实验表明该方法生成的bug可作为高效训练数据，训练出的模型表现出色。


<details>
  <summary>Details</summary>
Motivation: 高质量bug对训练下一代基于语言模型的软件工程代理至关重要，现有方法存在不能反映真实开发过程的问题。

Method: 指导软件工程代理向代码库引入特性，使其可能意外破坏测试从而产生bug。

Result: 生成的bug更接近人类编辑模式，作为训练数据效率更高，用更少数据优于其他bug数据集；训练出的FrogBoss和FrogMini模型在SWE - bench Verified上表现达到先进水平。

Conclusion: 提出的合成生成bug方法有效，生成的bug可用于高效训练软件工程代理模型。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [391] [Right Place, Right Time: Market Simulation-based RL for Execution Optimisation](https://arxiv.org/abs/2510.22206)
*Ollie Olby,Andreea Bacalum,Rory Baggott,Namid Stillman*

Main category: q-fin.CP

TL;DR: 提出强化学习框架发现最优执行策略，在反应式基于代理的市场模拟器中评估，结果显示该策略优于基线且接近有效前沿，凸显强化学习潜力。


<details>
  <summary>Details</summary>
Motivation: 随着执行算法日益复杂，优化难度增大，需要寻找有效方法来发现最优执行策略。

Method: 提出强化学习框架，在反应式基于代理的市场模拟器中评估，使用Almgren和Chriss的有效前沿评估RL代理性能。

Result: RL衍生策略始终优于基线，且接近有效前沿，展现出优化风险和影响的强大能力。

Conclusion: 强化学习可作为交易员工具包中的强大工具。

Abstract: Execution algorithms are vital to modern trading, they enable market
participants to execute large orders while minimising market impact and
transaction costs. As these algorithms grow more sophisticated, optimising them
becomes increasingly challenging. In this work, we present a reinforcement
learning (RL) framework for discovering optimal execution strategies, evaluated
within a reactive agent-based market simulator. This simulator creates reactive
order flow and allows us to decompose slippage into its constituent components:
market impact and execution risk. We assess the RL agent's performance using
the efficient frontier based on work by Almgren and Chriss, measuring its
ability to balance risk and cost. Results show that the RL-derived strategies
consistently outperform baselines and operate near the efficient frontier,
demonstrating a strong ability to optimise for risk and impact. These findings
highlight the potential of reinforcement learning as a powerful tool in the
trader's toolkit.

</details>


### [392] [Causal and Predictive Modeling of Short-Horizon Market Risk and Systematic Alpha Generation Using Hybrid Machine Learning Ensembles](https://arxiv.org/abs/2510.22348)
*Aryan Ranjan*

Main category: q-fin.CP

TL;DR: 提出系统交易框架，用混合机器学习集成预测短期市场风险、识别驱动因素并生成阿尔法收益，实证效果好，凸显混合集成架构有效性。


<details>
  <summary>Details</summary>
Motivation: 构建能预测短期市场风险、识别驱动因素并生成阿尔法收益的系统交易框架。

Method: 集成神经网络与基于树的投票模型，利用跨资产特征集预测标普500 ETF五天回撤，用可解释特征归因方法识别关键因素。

Result: 在2005 - 2025回测期，夏普比率2.51，年化CAPM阿尔法为+0.28，市场贝塔0.51。

Conclusion: 混合集成架构能捕捉非线性风险动态，识别可解释的潜在因果驱动因素，为系统交易中机器学习驱动的阿尔法生成提供可靠蓝图。

Abstract: We present a systematic trading framework that forecasts short-horizon market
risk, identifies its underlying drivers, and generates alpha using a hybrid
machine learning ensemble built to trade on the resulting signal. The framework
integrates neural networks with tree-based voting models to predict five-day
drawdowns in the S&P 500 ETF, leveraging a cross-asset feature set spanning
equities, fixed income, foreign exchange, commodities, and volatility markets.
Interpretable feature attribution methods reveal the key macroeconomic and
microstructural factors that differentiate high-risk (crash) from benign
(non-crash) weekly regimes. Empirical results show a Sharpe ratio of 2.51 and
an annualized CAPM alpha of +0.28, with a market beta of 0.51, indicating that
the model delivers substantial systematic alpha with limited directional
exposure during the 2005--2025 backtest period. Overall, the findings
underscore the effectiveness of hybrid ensemble architectures in capturing
nonlinear risk dynamics and identifying interpretable, potentially causal
drivers, providing a robust blueprint for machine learning-driven alpha
generation in systematic trading.

</details>


### [393] [TABL-ABM: A Hybrid Framework for Synthetic LOB Generation](https://arxiv.org/abs/2510.22685)
*Ollie Olby,Rory Baggott,Namid Stillman*

Main category: q-fin.CP

TL;DR: 本文探讨将Chiarella模型与TABL模型结合，模拟日内交易活动并测试生成能力，结果显示能生成现实价格动态，但市场微观结构部分未准确重现。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型用于金融交易对高保真金融时间序列数据需求增加，现有生成模型依赖大量历史数据和复杂模型，需更好解决方案。

Method: 将Chiarella模型与TABL模型结合，耦合预测模型与匹配引擎模拟，采用新方法模拟删除订单流，用程式化事实测试生成能力。

Result: 该方法能生成现实价格动态，但市场微观结构部分未准确重现。

Conclusion: 有必要在建模框架中纳入更复杂的代理行为以解释尾部事件。

Abstract: The recent application of deep learning models to financial trading has
heightened the need for high fidelity financial time series data. This
synthetic data can be used to supplement historical data to train large trading
models. The state-of-the-art models for the generative application often rely
on huge amounts of historical data and large, complicated models. These models
range from autoregressive and diffusion-based models through to architecturally
simpler models such as the temporal-attention bilinear layer. Agent-based
approaches to modelling limit order book dynamics can also recreate trading
activity through mechanistic models of trader behaviours. In this work, we
demonstrate how a popular agent-based framework for simulating intraday trading
activity, the Chiarella model, can be combined with one of the most performant
deep learning models for forecasting multi-variate time series, the TABL model.
This forecasting model is coupled to a simulation of a matching engine with a
novel method for simulating deleted order flow. Our simulator gives us the
ability to test the generative abilities of the forecasting model using
stylised facts. Our results show that this methodology generates realistic
price dynamics however, when analysing deeper, parts of the markets
microstructure are not accurately recreated, highlighting the necessity for
including more sophisticated agent behaviors into the modeling framework to
help account for tail events.

</details>


### [394] [Adaptive Multilevel Splitting: First Application to Rare-Event Derivative Pricing](https://arxiv.org/abs/2510.23461)
*Riccardo Gozzo*

Main category: q-fin.CP

TL;DR: 本文分析罕见事件下二元期权定价计算负担，引入AMS方法，数值实验表明该方法有计算优势且提高定价效率，还提供开源实现。


<details>
  <summary>Details</summary>
Motivation: 标准蒙特卡罗方法在深度价外二元期权定价中因不连续收益和低执行概率而效率低下，需大样本估计。

Method: 为Black - Scholes和Heston动态下的二元期权开发AMS方案，将罕见事件问题转化为一系列条件事件。

Result: 在深度价外情况下有高达200倍的计算增益，且保持无偏性。

Conclusion: 该方法提高了罕见事件合约的定价效率，此前无AMS用于金融衍生品的应用，还提供开源Rcpp实现。

Abstract: This work analyzes the computational burden of pricing binary options in
rare-event settings and introduces an adaptation of the adaptive multilevel
splitting (AMS) method for financial derivatives. Standard Monte Carlo is
inefficient for deep out of the money binaries due to discontinuous payoffs and
low exercise probabilities, requiring very large samples for accurate
estimates. An AMS scheme is developed for binary options under Black-Scholes
and Heston dynamics, reformulating the rare-event problem as a sequence of
conditional events. Numerical experiments compare the method to Monte Carlo and
to other techniques such as antithetic variables and multilevel Monte Carlo
(MLMC) across four contracts: European digital calls and puts, and Asian
digital calls and puts. Results show up to a 200-fold computational gain for
deep out-of-the-money cases while preserving unbiasedness. No evidence is found
of prior applications of AMS to financial derivatives. The approach improves
pricing efficiency for rare-event contracts such as parametric insurance and
catastrophe linked securities. An open-source Rcpp implementation is provided,
supporting multiple discretizations and importance functions.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [395] [Deviations from Tradition: Stylized Facts in the Era of DeFi](https://arxiv.org/abs/2510.22834)
*Daniele Maria Di Nosse,Federico Gatta,Fabrizio Lillo,Sebastian Jaimungal*

Main category: q-fin.TR

TL;DR: 研究2023 - 2024年Uniswap v3中24个最活跃的交易池，发现价格、流动性和订单流的新统计规律。


<details>
  <summary>Details</summary>
Motivation: 去中心化交易所（DEXs）与传统市场结构不同，研究其市场结构变化对传统市场中价格、流动性和订单流典型事实的影响。

Method: 对2023 - 2024年Uniswap v3中24个最活跃的交易池进行实证研究。

Result: 发现价格、流动性和订单流的分布及交叉自相关函数的一系列新统计规律，与市场结构或最大可提取价值搜索者的活动有关。

Conclusion: 市场结构变化会改变传统市场中价格、流动性和订单流的典型事实，新规律与市场结构和特定参与者活动相关。

Abstract: Decentralized Exchanges (DEXs) are now a significant component of the
financial world where billions of dollars are traded daily. Differently from
traditional markets, which are typically based on Limit Order Books, DEXs
typically work as Automated Market Makers, and, since the implementation of
Uniswap v3, feature concentrated liquidity. By investigating the twenty-four
most active pools in Uniswap v3 during 2023 and 2024, we empirically study how
this structural change in the organization of the markets modifies the
well-studied stylized facts of prices, liquidity, and order flow observed in
traditional markets. We find a series of new statistical regularities in the
distributions and cross-autocorrelation functions of these variables that we
are able to associate either with the market structure (e.g., the execution of
orders in blocks) or with the intense activity of Maximal Extractable Value
searchers, such as Just-in-Time liquidity providers and sandwich attackers.

</details>


### [396] [PEARL: Private Equity Accessibility Reimagined with Liquidity](https://arxiv.org/abs/2510.23183)
*E. Benhamou,JJ. Ohana,B. Guez,E. Setrouk,T. Jacquot*

Main category: q-fin.TR

TL;DR: 介绍AI框架PEARL，用流动性资产复制和解读私募股权基金，模型与基准有强相关性且表现更好。


<details>
  <summary>Details</summary>
Motivation: 设计一个能复制和解读私募股权基金的框架，让私募股权更具可及性。

Method: 借鉴Erik Stafford单股票选择和Thomson Reuters - Refinitiv行业方法，引入不对称性捕捉私募股权基金优势。

Result: 模型与多个基准有强相关性，两阶段方法表现优于初始每日代理。

Conclusion: 两阶段方法解码流动性每日私募股权代理能更好与季度私募股权基准表现一致。

Abstract: In this work, we introduce PEARL (Private Equity Accessibility Reimagined
with Liquidity), an AI-powered framework designed to replicate and decode
private equity funds using liquid, cost-effective assets. Relying on previous
research methods such as Erik Stafford's single stock selection (Stafford) and
Thomson Reuters - Refinitiv's sector approach (TR), our approach incorporates
an additional asymmetry to capture the reduced volatility and better
performance of private equity funds resulting from sale timing, leverage, and
stock improvements through management changes. As a result, our model exhibits
a strong correlation with well-established liquid benchmarks such as Stafford
and TR, as well as listed private equity firms (Listed PE), while enhancing
performance to better align with renowned quarterly private equity benchmarks
like Cambridge Associates, Preqin, and Bloomberg Private Equity Fund indices.
Empirical findings validate that our two-step approachdecoding liquid daily
private equity proxies with a degree of negative return asymmetry outperforms
the initial daily proxies and yields performance more consistent with quarterly
private equity benchmarks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [397] [Bridging Prediction and Attribution: Identifying Forward and Backward Causal Influence Ranges Using Assimilative Causal Inference](https://arxiv.org/abs/2510.21889)
*Marios Andreou,Nan Chen*

Main category: stat.ML

TL;DR: 本文介绍同化因果推理（ACI），用其开发因果影响范围（CIR）的数学公式，引入客观指标和近似算法，数值模拟展示其在研究复杂动力系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统因果推理方法有局限，ACI虽有进展但确定因果影响范围（CIR）仍是重要挑战，需进一步研究。

Method: 运用ACI开发前后向CIR的数学公式，引入客观指标，开发近似算法。

Result: 数值模拟表明前后向CIR框架为研究复杂动力系统提供新可能，可用于地球系统临界点、干扰变量影响和大气阻塞机制研究。

Conclusion: 该研究成果对科学、政策和决策有直接影响。

Abstract: Causal inference identifies cause-and-effect relationships between variables.
While traditional approaches rely on data to reveal causal links, a recently
developed method, assimilative causal inference (ACI), integrates observations
with dynamical models. It utilizes Bayesian data assimilation to trace causes
back from observed effects by quantifying the reduction in uncertainty. ACI
advances the detection of instantaneous causal relationships and the
intermittent reversal of causal roles over time. Beyond identifying causal
connections, an equally important challenge is determining the associated
causal influence range (CIR), indicating when causal influences emerged and for
how long they persist. In this paper, ACI is employed to develop mathematically
rigorous formulations of both forward and backward CIRs at each time. The
forward CIR quantifies the temporal impact of a cause, while the backward CIR
traces the onset of triggers for an observed effect, thus characterizing causal
predictability and attribution of outcomes at each transient phase,
respectively. Objective and robust metrics for both CIRs are introduced,
eliminating the need for empirical thresholds. Computationally efficient
approximation algorithms to compute CIRs are developed, which facilitate the
use of closed-form expressions for a broad class of nonlinear dynamical
systems. Numerical simulations demonstrate how this forward and backward CIR
framework provides new possibilities for probing complex dynamical systems. It
advances the study of bifurcation-driven and noise-induced tipping points in
Earth systems, investigates the impact from resolving the interfering variables
when determining the influence ranges, and elucidates atmospheric blocking
mechanisms in the equatorial region. These results have direct implications for
science, policy, and decision-making.

</details>


### [398] [Input Adaptive Bayesian Model Averaging](https://arxiv.org/abs/2510.22054)
*Yuli Slavutsky,Sebastian Salazar,David M. Blei*

Main category: stat.ML

TL;DR: 本文提出输入自适应贝叶斯模型平均法（IA - BMA）用于多候选模型预测，在回归和分类任务中表现优于基线和现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多候选模型预测且模型适用于不同输入的异构场景下，需结合不同模型输出进行预测。

Method: 提出IA - BMA方法，采用输入自适应先验，用摊销变分推断估计后验分布，并给出性能的形式化保证。

Result: 在回归和分类任务中，IA - BMA的预测比非自适应基线和现有自适应方法更准确、校准更好。

Conclusion: IA - BMA在多候选模型预测中是一种有效的方法，能在异构场景下提供更优的预测结果。

Abstract: This paper studies prediction with multiple candidate models, where the goal
is to combine their outputs. This task is especially challenging in
heterogeneous settings, where different models may be better suited to
different inputs. We propose input adaptive Bayesian Model Averaging (IA-BMA),
a Bayesian method that assigns model weights conditional on the input. IA-BMA
employs an input adaptive prior, and yields a posterior distribution that
adapts to each prediction, which we estimate with amortized variational
inference. We derive formal guarantees for its performance, relative to any
single predictor selected per input. We evaluate IABMA across regression and
classification tasks, studying data from personalized cancer treatment,
credit-card fraud detection, and UCI datasets. IA-BMA consistently delivers
more accurate and better-calibrated predictions than both non-adaptive
baselines and existing adaptive methods.

</details>


### [399] [Differentially Private High-dimensional Variable Selection via Integer Programming](https://arxiv.org/abs/2510.22062)
*Petros Prastakos,Kayhan Behdin,Rahul Mazumder*

Main category: stat.ML

TL;DR: 本文引入两种新的纯差分隐私估计器用于稀疏变量选择，结合MIP技术，有理论保证和实验验证效果好。


<details>
  <summary>Details</summary>
Motivation: 现有MIP算法在非隐私稀疏回归上有进展，但扩展到差分隐私设置尚未深入探索，故开展研究。

Method: 利用现代MIP技术，受指数机制启发开发结构化采样程序。

Result: 广泛的数值实验表明，在目标函数使用最小二乘法和铰链损失时，方法达到了最先进的经验支持恢复效果，在多达 $p = 10^4$ 的设置中优于竞争算法。

Conclusion: 提出的方法适用于稀疏回归或分类等问题，在稀疏变量选择的差分隐私设置中有良好表现。

Abstract: Sparse variable selection improves interpretability and generalization in
high-dimensional learning by selecting a small subset of informative features.
Recent advances in Mixed Integer Programming (MIP) have enabled solving
large-scale non-private sparse regression - known as Best Subset Selection
(BSS) - with millions of variables in minutes. However, extending these
algorithmic advances to the setting of Differential Privacy (DP) has remained
largely unexplored. In this paper, we introduce two new pure differentially
private estimators for sparse variable selection, levering modern MIP
techniques. Our framework is general and applies broadly to problems like
sparse regression or classification, and we provide theoretical support
recovery guarantees in the case of BSS. Inspired by the exponential mechanism,
we develop structured sampling procedures that efficiently explore the
non-convex objective landscape, avoiding the exhaustive combinatorial search in
the exponential mechanism. We complement our theoretical findings with
extensive numerical experiments, using both least squares and hinge loss for
our objective function, and demonstrate that our methods achieve
state-of-the-art empirical support recovery, outperforming competing algorithms
in settings with up to $p=10^4$.

</details>


### [400] [Frequentist Validity of Epistemic Uncertainty Estimators](https://arxiv.org/abs/2510.22063)
*Anchit Jain,Stephen Bates*

Main category: stat.ML

TL;DR: 提出基于自助法的频率主义认知不确定性度量，证明其与贝叶斯互信息渐近等价，还关联了该方法与深度集成启发式方法。


<details>
  <summary>Details</summary>
Motivation: 分解预测不确定性对机器学习系统很关键，现有认知不确定性度量计算后验分布困难。

Method: 引入基于自助法的频率主义认知不确定性度量，给出新的渐近展开式。

Result: 提出的频率主义度量与贝叶斯互信息渐近等价。

Conclusion: 为互信息提供频率主义解释和近似计算策略，为深度集成方法的成功提供新视角。

Abstract: Decomposing prediction uncertainty into its aleatoric (irreducible) and
epistemic (reducible) components is critical for the development and deployment
of machine learning systems. A popular, principled measure for epistemic
uncertainty is the mutual information between the response variable and model
parameters. However, evaluating this measure requires access to the posterior
distribution of the model parameters, which is challenging to compute. In view
of this, we introduce a frequentist measure of epistemic uncertainty based on
the bootstrap. Our main theoretical contribution is a novel asymptotic
expansion that reveals that our proposed (frequentist) measure and the
(Bayesian) mutual information are asymptotically equivalent. This provides
frequentist interpretations to mutual information and new computational
strategies for approximating it. Moreover, we link our proposed approach to the
widely-used heuristic approach of deep ensembles, giving added perspective on
their practical success.

</details>


### [401] [MMbeddings: Parameter-Efficient, Low-Overfitting Probabilistic Embeddings Inspired by Nonlinear Mixed Models](https://arxiv.org/abs/2510.22198)
*Giora Simchoni,Saharon Rosset*

Main category: stat.ML

TL;DR: 提出MMbeddings概率嵌入方法，将经典统计理论与深度学习结合，减少参数、减轻过拟合和计算负担，实验显示优于传统嵌入。


<details>
  <summary>Details</summary>
Motivation: 解决传统嵌入方法在高基数场景下参数过多、易过拟合和计算负担大的问题，将经典统计理论与深度学习结合。

Method: 将嵌入视为变分自编码器框架中的潜在随机效应，通过非线性混合模型重新解释分类嵌入。

Result: 在模拟和真实数据集上的广泛实验表明，MMbeddings在不同架构的协同过滤和表格回归任务中始终优于传统嵌入。

Conclusion: MMbeddings在减少参数、缓解过拟合和计算负担方面表现出色，在多样机器学习应用中具有潜力。

Abstract: We present MMbeddings, a probabilistic embedding approach that reinterprets
categorical embeddings through the lens of nonlinear mixed models, effectively
bridging classical statistical theory with modern deep learning. By treating
embeddings as latent random effects within a variational autoencoder framework,
our method substantially decreases the number of parameters -- from the
conventional embedding approach of cardinality $\times$ embedding dimension,
which quickly becomes infeasible with large cardinalities, to a significantly
smaller, cardinality-independent number determined primarily by the encoder
architecture. This reduction dramatically mitigates overfitting and
computational burden in high-cardinality settings. Extensive experiments on
simulated and real datasets, encompassing collaborative filtering and tabular
regression tasks using varied architectures, demonstrate that MMbeddings
consistently outperforms traditional embeddings, underscoring its potential
across diverse machine learning applications.

</details>


### [402] [MetaCaDI: A Meta-Learning Framework for Scalable Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2510.22298)
*Hans Jarett Ong,Yoichi Chikahara,Tomoharu Iwata*

Main category: stat.ML

TL;DR: 提出MetaCaDI框架解决复杂系统因果机制发现难题，实验显示其性能优于现有方法，在数据稀缺场景表现出色。


<details>
  <summary>Details</summary>
Motivation: 复杂现实系统因果机制挖掘面临数据收集成本高和未知干预的挑战。

Method: 引入MetaCaDI框架，将因果图和未知干预联合发现作为元学习问题，采用贝叶斯框架学习共享因果图结构，通过解析适应绕过基于梯度的双层优化。

Result: 在合成和复杂基因表达数据实验中，MetaCaDI显著优于现有方法，能从少量数据中进行因果图恢复和干预目标识别。

Conclusion: MetaCaDI在因果机制发现方面表现优异，在数据稀缺场景具有很强的鲁棒性。

Abstract: Uncovering the underlying causal mechanisms of complex real-world systems
remains a significant challenge, as these systems often entail high data
collection costs and involve unknown interventions. We introduce MetaCaDI, the
first framework to cast the joint discovery of a causal graph and unknown
interventions as a meta-learning problem. MetaCaDI is a Bayesian framework that
learns a shared causal graph structure across multiple experiments and is
optimized to rapidly adapt to new, few-shot intervention target prediction
tasks. A key innovation is our model's analytical adaptation, which uses a
closed-form solution to bypass expensive and potentially unstable
gradient-based bilevel optimization. Extensive experiments on synthetic and
complex gene expression data demonstrate that MetaCaDI significantly
outperforms state-of-the-art methods. It excels at both causal graph recovery
and identifying intervention targets from as few as 10 data instances, proving
its robustness in data-scarce scenarios.

</details>


### [403] [Beyond Isotonization: Scalable Non-Crossing Quantile Estimation via Neural Networks for Student Growth Percentiles](https://arxiv.org/abs/2510.22419)
*Kaihua Chang*

Main category: stat.ML

TL;DR: 本文指出美国学生成长百分位数（SGPs）方法存在插值悖论，受限联合分位数回归（CJQR）计算复杂，提出基于神经网络的多分位数回归（NNQR）作为替代，实证表明NNQR可解决交叉问题且具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决美国学生成长百分位数（SGPs）方法中存在的方法论不一致问题，以及受限联合分位数回归（CJQR）计算复杂难以处理大规模教育数据的问题。

Method: 分析现有SGPs、CJQR方法的问题，提出基于神经网络的多分位数回归（NNQR），利用随机梯度下降（SGD）优化，进行实证分析。

Result: 独立分位数回归（QR）会出现交叉问题，CJQR和NNQR能保证单调性，NNQR具有可扩展性。

Conclusion: NNQR是可行且可扩展的替代方案，能兼顾理论有效性和计算可行性。

Abstract: Student Growth Percentiles (SGPs), widely adopted across U.S. state
assessment systems, employ independent quantile regression followed by post-hoc
correction using an isotonic projection method (\texttt{isotonize=TRUE} in the
\texttt{SGP} R package) to address quantile crossing. We demonstrate this
approach contains a fundamental methodological inconsistency: interpolation
between independently-estimated, potentially crossed quantiles requires
monotonicity, yet the post-hoc correction alters estimates in ways that may
violate the quantile property $P(Y \leq \hat{Q}_{\tau}(Y|X) \mid X) = \tau$. We
term this the \emph{interpolation paradox}. While theoretically sound
constrained joint quantile regression (CJQR) eliminates crossing by enforcing
non-crossing constraints during optimization, we analyze its computational
complexity (often scaling poorly, e.g., $\mathcal{O}((qn)^3)$ for standard LP
solvers) rendering it intractable for large-scale educational data ($n >
100{,}000$). We examine the SGP package's switch to the Frisch-Newton interior
point method (\texttt{rq.method.for.large.n="fn"}) for large $N$, noting that
while efficient for \emph{independent} QR, it doesn't resolve the joint
problem's complexity or the paradox. We propose neural network-based
multi-quantile regression (NNQR) with shared hidden layers as a practical
alternative. Leveraging the convexity of the composite pinball loss, SGD-based
optimization used in NN training can reliably approach the global optimum,
offering scalability ($O(n)$) and implicitly reducing crossing. Our empirical
analysis shows independent QR yields crossing, while both CJQR and NNQR enforce
monotonicity. NNQR emerges as a viable, scalable alternative for operational
SGP systems, aligning theoretical validity with computational feasibility.

</details>


### [404] [Statistical Analysis of the Sinkhorn Iterations for Two-Sample Schrödinger Bridge Estimation](https://arxiv.org/abs/2510.22560)
*Ibuki Maeda,Rentian Yao,Atsushi Nitanda*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Schr\"odinger bridge problem seeks the optimal stochastic process that
connects two given probability distributions with minimal energy modification.
While the Sinkhorn algorithm is widely used to solve the static optimal
transport problem, a recent work (Pooladian and Niles-Weed, 2024) proposed the
Sinkhorn bridge, which estimates Schr\"odinger bridges by plugging optimal
transport into the time-dependent drifts of SDEs, with statistical guarantees
in the one-sample estimation setting where the true source distribution is
fully accessible. In this work, to further justify this method, we study the
statistical performance of intermediate Sinkhorn iterations in the two-sample
estimation setting, where only finite samples from both source and target
distributions are available. Specifically, we establish a statistical bound on
the squared total variation error of Sinkhorn bridge iterations: $O(1/m+1/n +
r^{4k})~(r \in (0,1))$, where $m$ and $n$ are the sample sizes from the source
and target distributions, respectively, and $k$ is the number of Sinkhorn
iterations. This result provides a theoretical guarantee for the finite-sample
performance of the Schr\"odinger bridge estimator and offers practical guidance
for selecting sample sizes and the number of Sinkhorn iterations. Notably, our
theoretical results apply to several representative methods such as [SF]$^2$M,
DSBM-IMF, BM2, and LightSB(-M) under specific settings, through the previously
unnoticed connection between these estimators.

</details>


### [405] [Semi-Supervised Learning under General Causal Models](https://arxiv.org/abs/2510.22567)
*Archer Moore,Heejung Shim,Jingge Zhu,Mingming Gong*

Main category: stat.ML

TL;DR: 本文提出适用于一般因果模型的半监督学习框架，利用无标签数据学习因果生成模型产生合成标签数据，经实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 半监督学习中未完全理解无标签数据为何能提升预测准确性，且现实应用中特征和标签因果关系复杂，需适用于一般因果模型的方法。

Method: 探索因果图结构，设计相应因果生成模型，借助无标签数据学习该模型，用生成的合成标签数据训练预测模型。

Result: 通过模拟和真实数据的实证研究验证了所提方法的有效性。

Conclusion: 提出的半监督学习框架能处理变量间灵活因果关系，有效利用无标签数据提升预测模型准确性。

Abstract: Semi-supervised learning (SSL) aims to train a machine learning model using
both labelled and unlabelled data. While the unlabelled data have been used in
various ways to improve the prediction accuracy, the reason why unlabelled data
could help is not fully understood. One interesting and promising direction is
to understand SSL from a causal perspective. In light of the independent causal
mechanisms principle, the unlabelled data can be helpful when the label causes
the features but not vice versa. However, the causal relations between the
features and labels can be complex in real world applications. In this paper,
we propose a SSL framework that works with general causal models in which the
variables have flexible causal relations. More specifically, we explore the
causal graph structures and design corresponding causal generative models which
can be learned with the help of unlabelled data. The learned causal generative
model can generate synthetic labelled data for training a more accurate
predictive model. We verify the effectiveness of our proposed method by
empirical studies on both simulated and real data.

</details>


### [406] [Block Coordinate Descent for Neural Networks Provably Finds Global Minima](https://arxiv.org/abs/2510.22667)
*Shunta Akiyama*

Main category: stat.ML

TL;DR: 本文研究用于训练深度神经网络的块坐标下降（BCD）算法，在严格单调递增激活函数下给出新的全局收敛保证，还将收敛保证扩展到ReLU激活函数，实验证实理论发现。


<details>
  <summary>Details</summary>
Motivation: 现有工作仅证明BCD在神经网络中收敛到驻点，本文旨在证明其收敛到全局最小值。

Method: 分析BCD算法，推导输出层和隐藏层损失变化情况，用Rademacher复杂度框架推导泛化界，提出带跳跃连接和非负投影的改进BCD算法。

Result: 输出层损失指数下降，隐藏层损失可控，BCD算法有强优化保证和良好泛化性能，改进算法将收敛保证扩展到ReLU激活函数，实验显示BCD算法在不同激活函数下损失小。

Conclusion: BCD算法在严格单调和ReLU激活函数下能实现全局收敛，有良好的优化和泛化性能。

Abstract: In this paper, we consider a block coordinate descent (BCD) algorithm for
training deep neural networks and provide a new global convergence guarantee
under strictly monotonically increasing activation functions. While existing
works demonstrate convergence to stationary points for BCD in neural networks,
our contribution is the first to prove convergence to global minima, ensuring
arbitrarily small loss. We show that the loss with respect to the output layer
decreases exponentially while the loss with respect to the hidden layers
remains well-controlled. Additionally, we derive generalization bounds using
the Rademacher complexity framework, demonstrating that BCD not only achieves
strong optimization guarantees but also provides favorable generalization
performance. Moreover, we propose a modified BCD algorithm with skip
connections and non-negative projection, extending our convergence guarantees
to ReLU activation, which are not strictly monotonic. Empirical experiments
confirm our theoretical findings, showing that the BCD algorithm achieves a
small loss for strictly monotonic and ReLU activations.

</details>


### [407] [OEUVRE: OnlinE Unbiased Variance-Reduced loss Estimation](https://arxiv.org/abs/2510.22744)
*Kanad Pardeshi,Bryan Wilder,Aarti Singh*

Main category: stat.ML

TL;DR: 提出OEUVRE估计器，在恒定时间和内存内递归更新损失估计，证明相关理论性质并自适应调参，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有预序方法在理论保证上有强条件限制，实际算法超参数需精细调整，需更好的在线学习算法预期损失估计方法。

Method: 引入OEUVRE估计器，利用算法稳定性进行最优更新，设计自适应调参方法。

Result: 证明了OEUVRE估计器的一致性、收敛率和集中界，在多种任务测试中匹配或超越其他估计器。

Conclusion: OEUVRE是一种有效的在线学习算法预期损失估计器，即使在其他估计器有理想调参的情况下仍有良好表现。

Abstract: Online learning algorithms continually update their models as data arrive,
making it essential to accurately estimate the expected loss at the current
time step. The prequential method is an effective estimation approach which can
be practically deployed in various ways. However, theoretical guarantees have
previously been established under strong conditions on the algorithm, and
practical algorithms have hyperparameters which require careful tuning. We
introduce OEUVRE, an estimator that evaluates each incoming sample on the
function learned at the current and previous time steps, recursively updating
the loss estimate in constant time and memory. We use algorithmic stability, a
property satisfied by many popular online learners, for optimal updates and
prove consistency, convergence rates, and concentration bounds for our
estimator. We design a method to adaptively tune OEUVRE's hyperparameters and
test it across diverse online and stochastic tasks. We observe that OEUVRE
matches or outperforms other estimators even when their hyperparameters are
tuned with oracle access to ground truth.

</details>


### [408] [Coupled Flow Matching](https://arxiv.org/abs/2510.23015)
*Wenxi Cai,Yuheng Wang,Naichen Shi*

Main category: stat.ML

TL;DR: 提出耦合流匹配框架CPFM，结合可控降维和高保真重建，实验显示效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决经典降维方法中压缩时丢弃信息难恢复的问题，实现可控降维和高保真重建。

Method: 学习高维数据x和低维嵌入y的耦合连续流，基于扩展Gromov - Wasserstein最优传输目标和双条件流匹配网络。

Result: CPFM在多个基准测试中产生语义丰富的嵌入，并能以更高保真度重建数据。

Conclusion: CPFM是一种有效的结合可控降维和高保真重建的框架，优于现有基线。

Abstract: We introduce Coupled Flow Matching (CPFM), a framework that integrates
controllable dimensionality reduction and high-fidelity reconstruction. CPFM
learns coupled continuous flows for both the high-dimensional data x and the
low-dimensional embedding y, which enables sampling p(y|x) via a latent-space
flow and p(x|y) via a data-space flow. Unlike classical dimension-reduction
methods, where information discarded during compression is often difficult to
recover, CPFM preserves the knowledge of residual information within the
weights of a flow network. This design provides bespoke controllability: users
may decide which semantic factors to retain explicitly in the latent space,
while the complementary information remains recoverable through the flow
network. Coupled flow matching builds on two components: (i) an extended
Gromov-Wasserstein optimal transport objective that establishes a probabilistic
correspondence between data and embeddings, and (ii) a dual-conditional
flow-matching network that extrapolates the correspondence to the underlying
space. Experiments on multiple benchmarks show that CPFM yields semantically
rich embeddings and reconstructs data with higher fidelity than existing
baselines.

</details>


### [409] [Complexity Dependent Error Rates for Physics-informed Statistical Learning via the Small-ball Method](https://arxiv.org/abs/2510.23149)
*Diego Marcondes*

Main category: stat.ML

TL;DR: 本文基于小球法为凸函数类中的物理信息统计学习（PISL）发展复杂度相关误差率，解决了PISL两个基本理论问题，建立评估PISL统计性质的理论框架。


<details>
  <summary>Details</summary>
Motivation: 当前PISL方法缺乏对信息正则化如何影响统计性质的全面理论理解，有两个基本问题待解决。

Method: 基于小球法发展适当的复杂度相关误差率。

Result: 物理信息估计器的误差率与硬约束经验误差最小化器相当，仅常数项不同；信息惩罚可有效降低模型复杂度，提升学习性能。

Conclusion: 建立了评估凸函数类中物理信息估计器统计性质的理论框架，缩小了统计理论与实际PISL的差距，有潜在应用价值。

Abstract: Physics-informed statistical learning (PISL) integrates empirical data with
physical knowledge to enhance the statistical performance of estimators. While
PISL methods are widely used in practice, a comprehensive theoretical
understanding of how informed regularization affects statistical properties is
still missing. Specifically, two fundamental questions have yet to be fully
addressed: (1) what is the trade-off between considering soft penalties versus
hard constraints, and (2) what is the statistical gain of incorporating
physical knowledge compared to purely data-driven empirical error minimisation.
In this paper, we address these questions for PISL in convex classes of
functions under physical knowledge expressed as linear equations by developing
appropriate complexity dependent error rates based on the small-ball method. We
show that, under suitable assumptions, (1) the error rates of physics-informed
estimators are comparable to those of hard constrained empirical error
minimisers, differing only by constant terms, and that (2) informed
penalization can effectively reduce model complexity, akin to dimensionality
reduction, thereby improving learning performance. This work establishes a
theoretical framework for evaluating the statistical properties of
physics-informed estimators in convex classes of functions, contributing to
closing the gap between statistical theory and practical PISL, with potential
applications to cases not yet explored in the literature.

</details>


### [410] [Rate-optimal Design for Anytime Best Arm Identification](https://arxiv.org/abs/2510.23199)
*Junpei Komiyama,Kyoungseok Jang,Junya Honda*

Main category: stat.ML

TL;DR: 本文研究最佳臂识别问题，提出Almost Tracking算法，在合成和真实数据集实验中表现优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 解决在有限采样预算下从K个臂中识别出平均奖励最高臂的问题，现有固定预算最佳臂识别工作有局限性。

Method: 提出一类可证明达到常数因子下极小极大最优的算法，在此框架下提出Almost Tracking算法，该算法对流行风险度量H1有可证明保证。

Result: 在合成和真实数据集上的实验表明，Almost Tracking算法优于现有任意时间算法和固定预算算法。

Conclusion: Almost Tracking算法在最佳臂识别问题上有实际优势，无需提前知晓总预算，也无需丢弃大量样本。

Abstract: We consider the best arm identification problem, where the goal is to
identify the arm with the highest mean reward from a set of $K$ arms under a
limited sampling budget. This problem models many practical scenarios such as
A/B testing. We consider a class of algorithms for this problem, which is
provably minimax optimal up to a constant factor. This idea is a generalization
of existing works in fixed-budget best arm identification, which are limited to
a particular choice of risk measures. Based on the framework, we propose Almost
Tracking, a closed-form algorithm that has a provable guarantee on the popular
risk measure $H_1$. Unlike existing algorithms, Almost Tracking does not
require the total budget in advance nor does it need to discard a significant
part of samples, which gives a practical advantage. Through experiments on
synthetic and real-world datasets, we show that our algorithm outperforms
existing anytime algorithms as well as fixed-budget algorithms.

</details>


### [411] [Provable test-time adaptivity and distributional robustness of in-context learning](https://arxiv.org/abs/2510.23254)
*Tianyi Ma,Tengyao Wang,Richard J. Samworth*

Main category: stat.ML

TL;DR: 研究预训练Transformer在不同测试分布下的上下文学习问题，证明其能达最优收敛率且对分布偏移鲁棒，给出更合适的最优性保证。


<details>
  <summary>Details</summary>
Motivation: 理解预训练Transformer在不同测试分布下的性能，应对潜在分布偏移问题。

Method: 考虑非参数回归和多指标模型，证明大Transformer在足够数据上预训练的收敛率。

Result: 预训练Transformer能在易任务上更快收敛，对测试时分布偏移鲁棒，且收敛率最优。

Conclusion: 预训练Transformer有良好性能，给出的最优性保证比极小极大下界更合适。

Abstract: We study in-context learning problems where a Transformer is pretrained on
tasks drawn from a mixture distribution $\pi=\sum_{\alpha\in\mathcal{A}}
\lambda_{\alpha} \pi_{\alpha}$, called the pretraining prior, in which each
mixture component $\pi_{\alpha}$ is a distribution on tasks of a specific
difficulty level indexed by $\alpha$. Our goal is to understand the performance
of the pretrained Transformer when evaluated on a different test distribution
$\mu$, consisting of tasks of fixed difficulty $\beta\in\mathcal{A}$, and with
potential distribution shift relative to $\pi_\beta$, subject to the
chi-squared divergence $\chi^2(\mu,\pi_{\beta})$ being at most $\kappa$. In
particular, we consider nonparametric regression problems with random
smoothness, and multi-index models with random smoothness as well as random
effective dimension. We prove that a large Transformer pretrained on sufficient
data achieves the optimal rate of convergence corresponding to the difficulty
level $\beta$, uniformly over test distributions $\mu$ in the chi-squared
divergence ball. Thus, the pretrained Transformer is able to achieve faster
rates of convergence on easier tasks and is robust to distribution shift at
test time. Finally, we prove that even if an estimator had access to the test
distribution $\mu$, the convergence rate of its expected risk over $\mu$ could
not be faster than that of our pretrained Transformers, thereby providing a
more appropriate optimality guarantee than minimax lower bounds.

</details>


### [412] [Robust Decision Making with Partially Calibrated Forecasts](https://arxiv.org/abs/2510.23471)
*Shayan Kiyani,Hamed Hassani,George Pappas,Aaron Roth*

Main category: stat.ML

TL;DR: 研究保守决策者如何将具有较弱校准保证的预测映射到行动，以达到极小极大意义上的鲁棒性，并刻画了极小极大最优决策规则。


<details>
  <summary>Details</summary>
Motivation: 全校准仅适用于低维预测问题，高维问题中较弱的校准形式缺乏决策理论属性，需研究保守决策者如何处理。

Method: 通过对偶论证刻画极小极大最优决策规则，对适用于优化平方误差的回归模型进行实证评估。

Result: 决策校准（及更强校准概念）能在极小极大意义上恢复“信任预测并相应行动”，未达决策校准的情况，最优决策规则仍可有效计算。

Conclusion: 为保守决策者处理具有较弱校准保证的预测提供了决策规则和评估方法。

Abstract: Calibration has emerged as a foundational goal in ``trustworthy machine
learning'', in part because of its strong decision theoretic semantics.
Independent of the underlying distribution, and independent of the decision
maker's utility function, calibration promises that amongst all policies
mapping predictions to actions, the uniformly best policy is the one that
``trusts the predictions'' and acts as if they were correct. But this is true
only of \emph{fully calibrated} forecasts, which are tractable to guarantee
only for very low dimensional prediction problems. For higher dimensional
prediction problems (e.g. when outcomes are multiclass), weaker forms of
calibration have been studied that lack these decision theoretic properties. In
this paper we study how a conservative decision maker should map predictions
endowed with these weaker (``partial'') calibration guarantees to actions, in a
way that is robust in a minimax sense: i.e. to maximize their expected utility
in the worst case over distributions consistent with the calibration
guarantees. We characterize their minimax optimal decision rule via a duality
argument, and show that surprisingly, ``trusting the predictions and acting
accordingly'' is recovered in this minimax sense by \emph{decision calibration}
(and any strictly stronger notion of calibration), a substantially weaker and
more tractable condition than full calibration. For calibration guarantees that
fall short of decision calibration, the minimax optimal decision rule is still
efficiently computable, and we provide an empirical evaluation of a natural one
that applies to any regression model solved to optimize squared error.

</details>


### [413] [Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization](https://arxiv.org/abs/2510.23485)
*Milad Sefidgaran,Kimia Nadjahi,Abdellatif Zaidi*

Main category: stat.ML

TL;DR: 本文利用随机投影和有损压缩建立新的条件互信息（CMI）界，证明其优于现有界，还研究数据“记忆”问题，表明记忆非良好泛化所必需。


<details>
  <summary>Details</summary>
Motivation: 改进统计学习算法泛化误差的现有条件互信息界，解决现有界在某些情况下失效的问题，并研究数据“记忆”问题。

Method: 利用随机投影和有损压缩建立新的条件互信息界。

Result: 新的界通常比现有界更紧，在某些现有界失效的问题实例中，新界能给出合适的泛化保证；存在不进行数据记忆且泛化误差相当的辅助算法。

Conclusion: 新的条件互信息界具有优越性，且记忆并非良好泛化的必要条件。

Abstract: In this paper, we leverage stochastic projection and lossy compression to
establish new conditional mutual information (CMI) bounds on the generalization
error of statistical learning algorithms. It is shown that these bounds are
generally tighter than the existing ones. In particular, we prove that for
certain problem instances for which existing MI and CMI bounds were recently
shown in Attias et al. [2024] and Livni [2023] to become vacuous or fail to
describe the right generalization behavior, our bounds yield suitable
generalization guarantees of the order of $\mathcal{O}(1/\sqrt{n})$, where $n$
is the size of the training dataset. Furthermore, we use our bounds to
investigate the problem of data "memorization" raised in those works, and which
asserts that there are learning problem instances for which any learning
algorithm that has good prediction there exist distributions under which the
algorithm must "memorize" a big fraction of the training dataset. We show that
for every learning algorithm, there exists an auxiliary algorithm that does not
memorize and which yields comparable generalization error for any data
distribution. In part, this shows that memorization is not necessary for good
generalization.

</details>


### [414] [Minimizing Human Intervention in Online Classification](https://arxiv.org/abs/2510.23557)
*William Réveillard,Vasileios Saketos,Alexandre Proutiere,Richard Combes*

Main category: stat.ML

TL;DR: 研究问答系统中的在线问题，提出CHC、CC和GHC分类器，分析不同时间范围下的后悔值并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决问答系统中代理对用户查询进行分类时，在咨询专家成本高的情况下，如何最小化后悔值的问题。

Method: 提出保守凸包分类器（CHC）、基于中心的分类器（CC）和广义凸包分类器（GHC），并分析不同时间范围和分布假设下的性能。

Result: CHC在时间范围T至少为嵌入维度d的指数时，后悔值为O(log^d T)且d=1时是极小极大最优；CC在查询来自次高斯混合分布且T <= e^d时，后悔值与标签数量N的NlogN成正比。

Conclusion: 提出的分类器在不同条件下能有效解决问答系统中的分类问题，实验验证了方法的有效性。

Abstract: We introduce and study an online problem arising in question answering
systems. In this problem, an agent must sequentially classify user-submitted
queries represented by $d$-dimensional embeddings drawn i.i.d. from an unknown
distribution. The agent may consult a costly human expert for the correct
label, or guess on her own without receiving feedback. The goal is to minimize
regret against an oracle with free expert access. When the time horizon $T$ is
at least exponential in the embedding dimension $d$, one can learn the geometry
of the class regions: in this regime, we propose the Conservative Hull-based
Classifier (CHC), which maintains convex hulls of expert-labeled queries and
calls the expert as soon as a query lands outside all known hulls. CHC attains
$\mathcal{O}(\log^d T)$ regret in $T$ and is minimax optimal for $d=1$.
Otherwise, the geometry cannot be reliably learned without additional
distributional assumptions. We show that when the queries are drawn from a
subgaussian mixture, for $T \le e^d$, a Center-based Classifier (CC) achieves
regret proportional to $N\log{N}$ where $N$ is the number of labels. To bridge
these regimes, we introduce the Generalized Hull-based Classifier (GHC), a
practical extension of CHC that allows for more aggressive guessing via a
tunable threshold parameter. Our approach is validated with experiments,
notably on real-world question-answering datasets using embeddings derived from
state-of-the-art large language models.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [415] [Proximal Hamiltonian Monte Carlo](https://arxiv.org/abs/2510.22252)
*Apratim Shukla,Dootika Vats,Eric C. Chi*

Main category: stat.CO

TL;DR: 提出近端哈密顿蒙特卡罗（p - HMC）算法解决非可微后验密度下高效采样问题，有理论分析和实用指导，在基准问题上展示了效率。


<details>
  <summary>Details</summary>
Motivation: 现代信号处理贝叶斯问题需高效MCMC采样算法，高维分布问题中，非可微后验密度使基于梯度的MCMC采样技术难以使用。

Method: 提出p - HMC算法，在哈密顿动力学中利用凸优化的近端映射和Moreau - Yosida（MY）包络元素。

Result: 比现有非光滑哈密顿蒙特卡罗算法有更精确梯度近似，计算负担相当；提供HMC链几何遍历性条件；给出选择关键超参数的指导；在逻辑回归和低秩矩阵估计基准问题上展示效率。

Conclusion: p - HMC算法能有效解决非可微后验密度下的采样问题，在理论和实践上都有优势。

Abstract: Bayesian formulation of modern day signal processing problems has called for
improved Markov chain Monte Carlo (MCMC) sampling algorithms for inference. The
need for efficient sampling techniques has become indispensable for high
dimensional distributions that often characterize many core signal processing
problems, e.g., image denoising, sparse signal recovery, etc. A major issue in
building effective sampling strategies, however, is the non-differentiability
of the underlying posterior density. Such posteriors are popular in models
designed to recover sparse signals. As a result, the use of efficient
gradient-based MCMC sampling techniques becomes difficult. We circumvent this
problem by proposing a Proximal Hamiltonian Monte Carlo (p-HMC) algorithm,
which leverages elements from convex optimization like proximal mappings and
Moreau-Yosida (MY) envelopes within Hamiltonian dynamics. Our method improves
upon the current state of the art non-smooth Hamiltonian Monte Carlo as it
achieves a relatively sharper approximation of the gradient of log posterior
density and a computational burden of at most the current state-of-the-art. A
chief contribution of this work is the theoretical analysis of p-HMC. We
provide conditions for geometric ergodicity of the underlying HMC chain. On the
practical front, we propose guidance on choosing the key p-HMC hyperparameter
-- the regularization parameter in the MY-envelope. We demonstrate p-HMC's
efficiency over other MCMC algorithms on benchmark problems of logistic
regression and low-rank matrix estimation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [416] [Generative Quantile Bayesian Prediction](https://arxiv.org/abs/2510.21784)
*Maria Nareklishvili,Nick Polson,Vadim Sokolov*

Main category: stat.ME

TL;DR: 本文旨在用GQBP解决大规模预测问题，对比多种方法，展示其在正态学习和因果推断中的应用并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决大规模预测问题，通过直接学习预测分位数而非密度获得理论和实践优势。

Method: 使用生成式方法进行预测分位数映射的Generative Quantile Bayesian Prediction (GQBP)，并与先进方法对比。

Result: 展示了方法在正态 - 正态学习和因果推断中的应用。

Conclusion: 给出了未来研究方向。

Abstract: Prediction is a central task of machine learning. Our goal is to solve large
scale prediction problems using Generative Quantile Bayesian Prediction
(GQBP).By directly learning predictive quantiles rather than densities we
achieve a number of theoretical and practical advantages. We contrast our
approach with state-of-the-art methods including conformal prediction, fiducial
prediction and marginal likelihood. Our distinguishing feature of our method is
the use of generative methods for predictive quantile maps. We illustrate our
methodology for normal-normal learning and causal inference. Finally, we
conclude with directions for future research.

</details>


### [417] [Conformalized Polynomial Chaos Expansion for Uncertainty-aware Surrogate Modeling](https://arxiv.org/abs/2510.22375)
*Dimitrios Loukrezis,Dimitris G. Giovanis*

Main category: stat.ME

TL;DR: 本文引入方法为数据驱动的多项式混沌展开代理模型配备量化预测不确定性的区间，利用线性特性高效计算，且数据利用高效，在基准模型上验证。


<details>
  <summary>Details</summary>
Motivation: 为数据驱动的多项式混沌展开代理模型配备能量化预测不确定性的区间。

Method: 将基于刀切法的共形预测集成到基于回归的多项式混沌展开中，利用模型线性特性用解析闭式表达式计算留一法残差和模型预测值。

Result: 在多个基准模型上验证了共形化多项式混沌展开方法，并研究了训练数据量对预测区间的影响。

Conclusion: 该方法能高效计算预测区间，数据利用效率高，无需留出数据集进行预测区间校准。

Abstract: This work introduces a method to equip data-driven polynomial chaos expansion
surrogate models with intervals that quantify the predictive uncertainty of the
surrogate. To that end, we integrate jackknife-based conformal prediction into
regression-based polynomial chaos expansions. The jackknife algorithm uses
leave-one-out residuals to generate predictive intervals around the predictions
of the polynomial chaos surrogate. The jackknife+ extension additionally
requires leave-one-out model predictions. The key to efficient implementation
is to leverage the linearity of the polynomial chaos regression model, so that
leave-one-out residuals and, if necessary, leave-one-out model predictions can
be computed with analytical, closed-form expressions, thus eliminating the need
for repeated model re-training. In addition to the efficient computation of the
predictive intervals, a significant advantage of this approach is its data
efficiency, as it requires no hold-out dataset for prediction interval
calibration, thus allowing the entire dataset to be used for model training.
The conformalized polynomial chaos expansion method is validated on several
benchmark models, where the impact of training data volume on the predictive
intervals is additionally investigated.

</details>


### [418] [Bayesian Nonlinear PDE Inference via Gaussian Process Collocation with Application to the Richards Equation](https://arxiv.org/abs/2510.23550)
*Yumo Yang,Anass Ben Bouazza,Xuejun Dong,Quan Zhou*

Main category: stat.ME

TL;DR: 本文提出新的高斯过程配点法用于非线性偏微分方程的参数估计，在模拟和实际农业数据集上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算量大且估计不稳定，需要高效的贝叶斯推理方法来估计Richards方程中的植物根系参数，以理解农业研究中的土壤 - 植物系统。

Method: 开发新的高斯过程配点法进行贝叶斯推理，用重要抽样程序纠正近似和真实后验分布的差异，还设计了先验引导的贝叶斯优化算法。

Result: 模拟研究表明该方法在各种设置下能得到稳健估计，在实际农业数据集上能估计植物根系参数并进行不确定性量化。

Conclusion: 所提出的方法在估计非线性偏微分方程未知参数方面有效且稳健。

Abstract: The estimation of unknown parameters in nonlinear partial differential
equations (PDEs) offers valuable insights across a wide range of scientific
domains. In this work, we focus on estimating plant root parameters in the
Richards equation, which is essential for understanding the soil-plant system
in agricultural studies. Since conventional methods are computationally
intensive and often yield unstable estimates, we develop a new Gaussian process
collocation method for efficient Bayesian inference. Unlike existing Gaussian
process-based approaches, our method constructs an approximate posterior
distribution using samples drawn from a Gaussian process model fitted to the
observed data, which does not require any structural assumption about the
underlying PDE. Further, we propose to use an importance sampling procedure to
correct for the discrepancy between the approximate and true posterior
distributions. As an alternative, we also devise a prior-guided Bayesian
optimization algorithm leveraging the approximate posterior. Simulation studies
demonstrate that our method yields robust estimates under various settings.
Finally, we apply our method on a real agricultural data set and estimate the
plant root parameters with uncertainty quantification.

</details>


### [419] [Causal Effect Estimation with TMLE: Handling Missing Data and Near-Violations of Positivity](https://arxiv.org/abs/2510.22202)
*Christoph Wiederkehr,Christian Heumann,Michael Schomaker*

Main category: stat.ME

TL;DR: 评估目标极大似然估计（TMLE）在不同程度正性条件违反下处理缺失数据时估计平均处理效应的性能，比较八种缺失数据方法与 TMLE 结合的效果，给出不同场景下的方法推荐。


<details>
  <summary>Details</summary>
Motivation: 研究 TMLE 在缺失数据场景且存在不同程度正性条件违反时，估计平均处理效应的性能。

Method: 采用基于模型和基于设计的模拟，考虑五种缺失数据有向无环图，对比八种结合 TMLE 的缺失数据处理方法，区分非多重插补和多重插补方法。

Result: 非多重插补方法（尤其是结合结果缺失模型的完整病例 TMLE）偏差更低、对正性条件违反更稳健；多重插补结合分类与回归树（CART）均方根误差更低且常能保持标称覆盖率。

Conclusion: 为降低偏差推荐结合结果缺失模型的完整病例 TMLE，为获得准确置信区间推荐多重插补 CART。

Abstract: We evaluate the performance of targeted maximum likelihood estimation (TMLE)
for estimating the average treatment effect in missing data scenarios under
varying levels of positivity violations. We employ model- and design-based
simulations, with the latter using undersmoothed highly adaptive lasso on the
'WASH Benefits Bangladesh' dataset to mimic real-world complexities. Five
missingness-directed acyclic graphs are considered, capturing common missing
data mechanisms in epidemiological research, particularly in one-point exposure
studies. These mechanisms include also not-at-random missingness in the
exposure, outcome, and confounders. We compare eight missing data methods in
conjunction with TMLE as the analysis method, distinguishing between
non-multiple imputation (non-MI) and multiple imputation (MI) approaches. The
MI approaches use both parametric and machine-learning models. Results show
that non-MI methods, particularly complete cases with TMLE incorporating an
outcome-missingness model, exhibit lower bias compared to all other evaluated
missing data methods and greater robustness against positivity violations
across. In Comparison MI with classification and regression trees (CART)
achieve lower root mean squared error, while often maintaining nominal coverage
rates. Our findings highlight the trade-offs between bias and coverage, and we
recommend using complete cases with TMLE incorporating an outcome-missingness
model for bias reduction and MI CART when accurate confidence intervals are the
priority.

</details>


### [420] [Efficiently Learning Synthetic Control Models for High-dimensional Disaggregated Data](https://arxiv.org/abs/2510.22828)
*Ye Shen,Rui Song,Alberto Abadie*

Main category: stat.ME

TL;DR: 本文提出将多元平方根套索方法集成到合成控制框架的新方法，解决合成控制法在多处理单元高维场景中的挑战，模拟显示其计算效率高且估计准确，并应用于评估美国县一级居家令对失业率的影响。


<details>
  <summary>Details</summary>
Motivation: 合成控制法在多处理单元高维场景的实际实施和计算效率存在挑战，需解决这些问题。

Method: 将多元平方根套索方法集成到合成控制框架，并建立拟合合成控制权重的估计误差界限，量化平均处理效应的估计误差。

Result: 模拟研究表明该方法在不影响估计准确性的前提下，具有更高的计算效率。

Conclusion: 提出的新方法能有效解决合成控制法在多处理单元高维场景的问题，可用于实际因果效应评估。

Abstract: The Synthetic Control method (SC) has become a valuable tool for estimating
causal effects. Originally designed for single-treated unit scenarios, it has
recently found applications in high-dimensional disaggregated settings with
multiple treated units. However, challenges in practical implementation and
computational efficiency arise in such scenarios. To tackle these challenges,
we propose a novel approach that integrates the Multivariate Square-root Lasso
method into the synthetic control framework. We rigorously establish the
estimation error bounds for fitting the Synthetic Control weights using
Multivariate Square-root Lasso, accommodating high-dimensionality and time
series dependencies. Additionally, we quantify the estimation error for the
Average Treatment Effect on the Treated (ATT). Through simulation studies, we
demonstrate that our method offers superior computational efficiency without
compromising estimation accuracy. We apply our method to assess the causal
impact of COVID-19 Stay-at-Home Orders on the monthly unemployment rate in the
United States at the county level.

</details>


### [421] [Semi-supervised Vertex Hunting, with Applications in Network and Text Analysis](https://arxiv.org/abs/2510.22526)
*Yicong Jiang,Zheng Tracy Ke*

Main category: stat.ME

TL;DR: 介绍半监督顶点搜索（SSVH）新变体，提出利用正交投影矩阵性质的方法，有理论误差界，收敛更快，应用于两个实际场景。


<details>
  <summary>Details</summary>
Motivation: 在顶点搜索（VH）基础上，引入有部分数据重心坐标信息的半监督顶点搜索（SSVH）新问题。

Method: 利用正交投影矩阵性质，结合线性代数新见解的方法。

Result: 建立理论误差界，收敛速度比现有无监督VH算法快，应用于半监督网络混合成员估计和半监督主题建模得到高效可扩展算法。

Conclusion: 提出的SSVH方法有效，在理论和实际应用上有优势。

Abstract: Vertex hunting (VH) is the task of estimating a simplex from noisy data
points and has many applications in areas such as network and text analysis. We
introduce a new variant, semi-supervised vertex hunting (SSVH), in which
partial information is available in the form of barycentric coordinates for
some data points, known only up to an unknown transformation. To address this
problem, we develop a method that leverages properties of orthogonal projection
matrices, drawing on novel insights from linear algebra. We establish
theoretical error bounds for our method and demonstrate that it achieves a
faster convergence rate than existing unsupervised VH algorithms. Finally, we
apply SSVH to two practical settings, semi-supervised network mixed membership
estimation and semi-supervised topic modeling, resulting in efficient and
scalable algorithms.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [422] [Direct Debiased Machine Learning via Bregman Divergence Minimization](https://arxiv.org/abs/2510.23534)
*Masahiro Kato*

Main category: econ.EM

TL;DR: 开发了包含Neyman目标估计和广义Riesz回归的直接去偏机器学习框架，统一多种方法并减少偏差。


<details>
  <summary>Details</summary>
Motivation: 在涉及因果效应或结构模型的问题中，机器学习方法估计的回归函数代入识别方程会因一阶偏差导致性能不佳，需减少偏差。

Method: 开发端到端算法的直接去偏机器学习框架，将干扰参数、回归函数和Riesz表示的估计表述为最小化Neyman正交得分的差异，即Neyman目标估计，用Bregman散度衡量差异。

Result: Neyman目标估计包含Riesz表示估计，Bregman散度涵盖多种损失函数，该估计还将TMLE作为回归函数估计的特例，特定情况下可自动获得协变量平衡特性。

Conclusion: 所开发的框架统一了Riesz回归、协变量平衡、TMLE和密度比估计等方法，能有效减少偏差。

Abstract: We develop a direct debiased machine learning framework comprising Neyman
targeted estimation and generalized Riesz regression. Our framework unifies
Riesz regression for automatic debiased machine learning, covariate balancing,
targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In
many problems involving causal effects or structural models, the parameters of
interest depend on regression functions. Plugging regression functions
estimated by machine learning methods into the identifying equations can yield
poor performance because of first-stage bias. To reduce such bias, debiased
machine learning employs Neyman orthogonal estimating equations. Debiased
machine learning typically requires estimation of the Riesz representer and the
regression function. For this problem, we develop a direct debiased machine
learning framework with an end-to-end algorithm. We formulate estimation of the
nuisance parameters, the regression function and the Riesz representer, as
minimizing the discrepancy between Neyman orthogonal scores computed with known
and unknown nuisance parameters, which we refer to as Neyman targeted
estimation. Neyman targeted estimation includes Riesz representer estimation,
and we measure discrepancies using the Bregman divergence. The Bregman
divergence encompasses various loss functions as special cases, where the
squared loss yields Riesz regression and the Kullback-Leibler divergence yields
entropy balancing. We refer to this Riesz representer estimation as generalized
Riesz regression. Neyman targeted estimation also yields TMLE as a special case
for regression function estimation. Furthermore, for specific pairs of models
and Riesz representer estimation methods, we can automatically obtain the
covariate balancing property without explicitly solving the covariate balancing
objective.

</details>


### [423] [Macroeconomic Forecasting for the G7 countries under Uncertainty Shocks](https://arxiv.org/abs/2510.23347)
*Shovon Sengupta,Sunny Kumar Singh,Tanujit Chakraborty*

Main category: econ.EM

TL;DR: 在复杂经济环境下，扩展 SZBVARx 模型进行宏观经济预测，结果显示其优于多个基准模型，为 G7 政策制定者提供工具。


<details>
  <summary>Details</summary>
Motivation: 传统向量自回归模型在高维环境和处理时变相互依赖及复杂参数结构方面存在局限，需要准确的宏观经济预测方法。

Method: 扩展 SZBVARx 模型，纳入领域信息收缩和四种基于报纸的不确定性冲击，结合小波相干和非线性局部投影进行研究。

Result: 12 和 24 个月的样本外结果显示 SZBVARx 优于 14 个基准模型，贝叶斯预测区间可进行不确定性量化。

Conclusion: 提出的 SZBVARx 为 G7 政策制定者在普遍不确定性下进行现代宏观经济预测提供了透明、校准良好的工具。

Abstract: Accurate macroeconomic forecasting has become harder amid geopolitical
disruptions, policy reversals, and volatile financial markets. Conventional
vector autoregressions (VARs) overfit in high dimensional settings, while
threshold VARs struggle with time varying interdependencies and complex
parameter structures. We address these limitations by extending the Sims Zha
Bayesian VAR with exogenous variables (SZBVARx) to incorporate domain-informed
shrinkage and four newspaper based uncertainty shocks such as economic policy
uncertainty, geopolitical risk, US equity market volatility, and US monetary
policy uncertainty. The framework improves structural interpretability,
mitigates dimensionality, and imposes empirically guided regularization. Using
G7 data, we study spillovers from uncertainty shocks to five core variables
(unemployment, real broad effective exchange rates, short term rates, oil
prices, and CPI inflation), combining wavelet coherence (time frequency
dynamics) with nonlinear local projections (state dependent impulse responses).
Out-of-sample results at 12 and 24 month horizons show that SZBVARx outperforms
14 benchmarks, including classical VARs and leading machine learning models, as
confirmed by Murphy difference diagrams, multivariate Diebold Mariano tests,
and Giacomini White predictability tests. Credible Bayesian prediction
intervals deliver robust uncertainty quantification for scenario analysis and
risk management. The proposed SZBVARx offers G7 policymakers a transparent,
well calibrated tool for modern macroeconomic forecasting under pervasive
uncertainty.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [424] [RGC: a radio AGN classifier based on deep learning. I. A semi-supervised model for the VLA images of bent radio AGNs](https://arxiv.org/abs/2510.22190)
*M. S. Hossain,M. S. H. Shahal,A. Khan,K. M. B. Asad,P. Saikia,F. Akter,A. Ali,M. A. Amin,A. Momen,M. Hasan,A. K. M. M. Rahman*

Main category: astro-ph.IM

TL;DR: 发布RGC Python包，含新预处理的标记数据集与半监督RGC模型，模型在无伪源数据集上表现佳，为开发物理信息基础模型奠基。


<details>
  <summary>Details</summary>
Motivation: 此前尚无利用未标记数据和纯视觉检查标签训练的弯曲射电活动星系核（RAGN）机器学习分类器。

Method: 使用PyBDSF和Photutils预处理标记数据集，将自监督框架BYOL与监督E2CNN集成形成半监督二元分类器。

Result: RGC模型在无伪源数据集上达到88.88%的准确率，WATs的F1分数为0.90，NATs的F1分数为0.85。

Conclusion: 该工作可作为开发能识别广泛AGN物理属性的物理信息基础模型的垫脚石。

Abstract: Wide-angle tail (WAT) and narrow-angle tail (NAT) radio active galactic
nuclei (RAGNs) are key tracers of dense environments in galaxy groups and
clusters, yet no machine-learning classifier of bent RAGNs has been trained
using both unlabeled data and purely visually inspected labels. We release the
RGC Python package, which includes two newly preprocessed labeled datasets of
639 WATs and NATs derived from a publicly available catalog of visually
inspected sources, along with a semi-supervised RGC model that leverages 20,000
unlabeled RAGNs. The two labeled datasets in RGC were preprocessed using PyBDSF
which retains spurious sources, and Photutils which removes them. The RGC model
integrates the self-supervised framework BYOL (Bootstrap YOur Latent) with the
supervised E2CNN (E2-equivariant Convolutional Neural Network) to form a
semi-supervised binary classifier. The RGC model, when trained and evaluated on
a dataset devoid of spurious sources, reaches peak performance, attaining an
accuracy of 88.88% along with F1-scores of 0.90 for WATs and 0.85 for NATs. The
model's attention patterns amid class imbalance suggest that this work can
serve as a stepping stone toward developing physics-informed foundation models
capable of identifying a broad range of AGN physical properties.

</details>


### [425] [Multi-Modal Masked Autoencoders for Learning Image-Spectrum Associations for Galaxy Evolution and Cosmology](https://arxiv.org/abs/2510.22527)
*Morgan Himes,Samiksha Krishnamurthy,Andrew Lizarraga,Srinath Saikrishnan,Vikram Seenivasan,Jonathan Soriano,Ying Nian Wu,Tuan Do*

Main category: astro-ph.IM

TL;DR: 利用即将到来的星系图像和光谱数据构建MMAE模型，测试三个应用，展示其潜力与局限并建议拓展模态。


<details>
  <summary>Details</summary>
Motivation: 即将到来的调查会产生大量星系图像但光谱较少，需要学习跨模态表示的模型。

Method: 构建包含134,533个星系图像和光谱的数据集，采用Multi - Modal Masked Autoencoder (MMAE)，通过掩盖75%的数据并重建缺失的图像和光谱标记进行训练。

Result: 模型能恢复关键物理特征，但在精细图像细节和谱线强度上有困难；在红移回归中，即使测试时缺少光谱，表现也与之前多模态模型相当或更好。

Conclusion: 突出了掩蔽自编码器在天体物理学中的潜力和局限性，建议将其扩展到文本等其他模态以构建基础模型。

Abstract: Upcoming surveys will produce billions of galaxy images but comparatively few
spectra, motivating models that learn cross-modal representations. We build a
dataset of 134,533 galaxy images (HSC-PDR2) and spectra (DESI-DR1) and adapt a
Multi-Modal Masked Autoencoder (MMAE) to embed both images and spectra in a
shared representation. The MMAE is a transformer-based architecture, which we
train by masking 75% of the data and reconstructing missing image and spectral
tokens. We use this model to test three applications: spectral and image
reconstruction from heavily masked data and redshift regression from images
alone. It recovers key physical features, such as galaxy shapes, atomic
emission line peaks, and broad continuum slopes, though it struggles with fine
image details and line strengths. For redshift regression, the MMAE performs
comparably or better than prior multi-modal models in terms of prediction
scatter even when missing spectra in testing. These results highlight both the
potential and limitations of masked autoencoders in astrophysics and motivate
extensions to additional modalities, such as text, for foundation models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [426] [ProGQL: A Provenance Graph Query System for Cyber Attack Investigation](https://arxiv.org/abs/2510.22400)
*Fei Shao,Jia Zou,Zhichao Cao,Xusheng Xiao*

Main category: cs.CR

TL;DR: 现有溯源分析技术存在灵活性不足和内存效率低的问题，本文提出PROGQL框架解决这些问题，评估显示其有效且可扩展性强。


<details>
  <summary>Details</summary>
Motivation: 现有PA技术面临灵活性和扩展性差、内存效率低的问题，限制了其在现实环境中的应用，需要改进。

Method: 提出PROGQL框架，提供特定领域图搜索语言和优化的查询引擎，引入新语言结构支持依赖搜索，优化查询引擎以提高效率和减少内存开销。

Result: 在真实攻击评估中，PROGQL语言在表达复杂攻击方面比Cypher有效，与SOTA PA技术DEPIMPACT相比，PROGQL框架显著提高了可扩展性。

Conclusion: PROGQL框架能有效解决现有PA技术的局限性，在复杂网络攻击调查中具有良好效果和可扩展性。

Abstract: Provenance analysis (PA) has recently emerged as an important solution for
cyber attack investigation. PA leverages system monitoring to monitor system
activities as a series of system audit events and organizes these events as a
provenance graph to show the dependencies among system activities, which can
reveal steps of cyber attacks. Despite their potential, existing PA techniques
face two critical challenges: (1) they are inflexible and non-extensible,
making it difficult to incorporate analyst expertise, and (2) they are memory
inefficient, often requiring>100GB of RAM to hold entire event streams, which
fundamentally limits scalability and deployment in real-world environments. To
address these limitations, we propose the PROGQL framework, which provides a
domain-specific graph search language with a well-engineered query engine,
allowing PA over system audit events and expert knowledge to be jointly
expressed as a graph search query and thereby facilitating the investigation of
complex cyberattacks. In particular, to support dependency searches from a
starting edge required in PA, PROGQL introduces new language constructs for
constrained graph traversal, edge weight computation, value propagation along
weighted edges, and graph merging to integrate multiple searches. Moreover, the
PROGQL query engine is optimized for efficient incremental graph search across
heterogeneous database backends, eliminating the need for full in-memory
materialization and reducing memory overhead. Our evaluations on real attacks
demonstrate the effectiveness of the PROGQL language in expressing a diverse
set of complex attacks compared with the state-of-the-art graph query language
Cypher, and the comparison with the SOTA PA technique DEPIMPACT further
demonstrates the significant improvement of the scalability brought by our
PROGQL framework's design.

</details>


### [427] [A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem](https://arxiv.org/abs/2510.23024)
*Chuan Yan,Zeng Li,Kunlin Cai,Liuhuo Wan,Ruomai Ren,Yiran Shen,Guangdong Bai*

Main category: cs.CR

TL;DR: 对VR应用生态系统隐私实践进行多商店研究，发现存在大量隐私合规问题。


<details>
  <summary>Details</summary>
Motivation: VR应用收集敏感数据但缺乏领域特定法规，各应用商店隐私实践差异大。

Method: 基于自然语言处理、逆向工程和静态分析的多方面方法，评估6565个来自五大应用商店的VR应用的声明和行为隐私实践。

Result: 所有商店都存在显著隐私合规问题，三分之一应用未声明使用敏感数据，21.5%应用未提供有效隐私政策。

Conclusion: 揭示VR应用生态系统隐私保护现状，提醒开发者和用户，鼓励应用商店实施严格隐私合规规定。

Abstract: Virtual Reality (VR) has gained increasing traction among various domains in
recent years, with major companies such as Meta, Pico, and Microsoft launching
their application stores to support third-party developers in releasing their
applications (or simply apps). These apps offer rich functionality but
inherently collect privacy-sensitive data, such as user biometrics, behaviors,
and the surrounding environment. Nevertheless, there is still a lack of
domain-specific regulations to govern the data handling of VR apps, resulting
in significant variations in their privacy practices among app stores.
  In this work, we present the first comprehensive multi-store study of privacy
practices in the current VR app ecosystem, covering a large-scale dataset
involving 6,565 apps collected from five major app stores. We assess both
declarative and behavioral privacy practices of VR apps, using a multi-faceted
approach based on natural language processing, reverse engineering, and static
analysis. Our assessment reveals significant privacy compliance issues across
all stores, underscoring the premature status of privacy protection in this
rapidly growing ecosystem. For instance, one third of apps fail to declare
their use of sensitive data, and 21.5\% of apps neglect to provide valid
privacy policies. Our work sheds light on the status quo of privacy protection
within the VR app ecosystem for the first time. Our findings should raise an
alert to VR app developers and users, and encourage store operators to
implement stringent regulations on privacy compliance among VR apps.

</details>


### [428] [Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for Directed Greybox Fuzzing](https://arxiv.org/abs/2510.23101)
*Yifan Zhang,Xin Zhang*

Main category: cs.CR

TL;DR: 现有定向灰盒模糊测试（DGF）方法因依赖静态分析的距离度量导致概率计算不精确，本文提出用精确调用栈表示代替，利用大语言模型预测漏洞触发调用栈引导种子优先级排序，实验表明比基线方法更快触发漏洞并发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有DGF方法依赖静态分析的复杂距离度量，存在过近似问题，导致大量无关执行路径被误判，降低模糊测试效率。

Method: 用精确调用栈表示代替基于静态分析的距离度量，利用LLM预测漏洞触发调用栈，通过静态分析构建调用图识别可能到达目标位置的方法，优先对执行路径与预测调用栈重叠度高的种子进行变异。

Result: 在实际程序上比基线方法快1.86 - 3.09倍触发漏洞，通过定向补丁测试发现10个新漏洞和2个未完全修复的问题，10个获CVE编号。

Conclusion: 将LLM集成到DGF核心种子优先级机制是可行且有效的，能提高模糊测试效率和发现漏洞的能力。

Abstract: Directed greybox fuzzing (DGF) aims to efficiently trigger bugs at specific
target locations by prioritizing seeds whose execution paths are more likely to
mutate into triggering target bugs. However, existing DGF approaches suffer
from imprecise probability calculations due to their reliance on complex
distance metrics derived from static analysis. The over-approximations inherent
in static analysis cause a large number of irrelevant execution paths to be
mistakenly considered to potentially mutate into triggering target bugs,
significantly reducing fuzzing efficiency. We propose to replace static
analysis-based distance metrics with precise call stack representations. Call
stacks represent precise control flows, thereby avoiding false information in
static analysis. We leverage large language models (LLMs) to predict
vulnerability-triggering call stacks for guiding seed prioritization. Our
approach constructs call graphs through static analysis to identify methods
that can potentially reach target locations, then utilizes LLMs to predict the
most likely call stack sequence that triggers the vulnerability. Seeds whose
execution paths have higher overlap with the predicted call stack are
prioritized for mutation. This is the first work to integrate LLMs into the
core seed prioritization mechanism of DGF. We implement our approach and
evaluate it against several state-of-the-art fuzzers. On a suite of real-world
programs, our approach triggers vulnerabilities $1.86\times$ to $3.09\times$
faster compared to baselines. In addition, our approach identifies 10 new
vulnerabilities and 2 incomplete fixes in the latest versions of programs used
in our controlled experiments through directed patch testing, with 10 assigned
CVE IDs.

</details>


### [429] [Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning](https://arxiv.org/abs/2510.21957)
*Zhixin Pan,Ziyu Shu,Amberbir Alemayoh*

Main category: cs.CR

TL;DR: 本文提出结合自监督对比学习与神经架构搜索的框架检测勒索软件，实验显示在检测准确率和响应时间上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 勒索软件快速演变、需早期检测且日益多样，传统检测方法面临挑战，现有AI方法有特征依赖、响应延迟和适应性差等问题。

Method: 设计结合硬件性能计数器的对比学习框架分析勒索软件运行时行为；引入定制损失函数促进早期检测并减少检测延迟；部署神经架构搜索框架自动构建自适应模型架构。

Result: 与现有方法相比，检测准确率提升达16.1%，响应时间最多快6倍，在规避攻击下保持鲁棒性。

Conclusion: 提出的框架有效解决了现有勒索软件检测方法的问题，在检测性能上有显著优势。

Abstract: Ransomware has become a critical threat to cybersecurity due to its rapid
evolution, the necessity for early detection, and growing diversity, posing
significant challenges to traditional detection methods. While AI-based
approaches had been proposed by prior works to assist ransomware detection,
existing methods suffer from three major limitations, ad-hoc feature
dependencies, delayed response, and limited adaptability to unseen variants. In
this paper, we propose a framework that integrates self-supervised contrastive
learning with neural architecture search (NAS) to address these challenges.
Specifically, this paper offers three important contributions. (1) We design a
contrastive learning framework that incorporates hardware performance counters
(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a
customized loss function that encourages early-stage detection of malicious
activity, and significantly reduces the detection latency. (3) We deploy a
neural architecture search (NAS) framework to automatically construct adaptive
model architectures, allowing the detector to flexibly align with unseen
ransomware variants. Experimental results show that our proposed method
achieves significant improvements in both detection accuracy (up to 16.1%) and
response time (up to 6x) compared to existing approaches while maintaining
robustness under evasive attacks.

</details>


### [430] [Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models](https://arxiv.org/abs/2510.22085)
*Pavlos Ntais*

Main category: cs.CR

TL;DR: 本文提出Jailbreak Mimicry方法训练攻击模型自动生成越狱提示，在多模型评估中展现效果，揭示安全对齐方法漏洞并分析防御策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受高级提示工程攻击，需有效方法进行主动漏洞评估。

Method: 提出Jailbreak Mimicry方法，用参数高效微调（LoRA）在Mistral - 7B上基于AdvBench数据集训练。

Result: 在200项测试集上对GPT - OSS - 20B攻击成功率81.0%，对GPT - 4、Llama - 3、Gemini 2.5 Flash分别为66.5%、79.5%、33.0%，比直接提示提升54倍，不同领域攻击成功率有差异。

Conclusion: 当前安全对齐方法存在系统漏洞，分析了失败机制并讨论防御策略以减轻AI在网络安全中的漏洞。

Abstract: Large language models (LLMs) remain vulnerable to sophisticated prompt
engineering attacks that exploit contextual framing to bypass safety
mechanisms, posing significant risks in cybersecurity applications. We
introduce Jailbreak Mimicry, a systematic methodology for training compact
attacker models to automatically generate narrative-based jailbreak prompts in
a one-shot manner. Our approach transforms adversarial prompt discovery from
manual craftsmanship into a reproducible scientific process, enabling proactive
vulnerability assessment in AI-driven security systems. Developed for the
OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient
fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench,
achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out
test set of 200 items. Cross-model evaluation reveals significant variation in
vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on
Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad
applicability and model-specific defensive strengths in cybersecurity contexts.
This represents a 54x improvement over direct prompting (1.5% ASR) and
demonstrates systematic vulnerabilities in current safety alignment approaches.
Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and
deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable,
highlighting threats to AI-integrated threat detection, malware analysis, and
secure systems, while physical harm categories show greater resistance (55.6%
ASR). We employ automated harmfulness evaluation using Claude Sonnet 4,
cross-validated with human expert assessment, ensuring reliable and scalable
evaluation for cybersecurity red-teaming. Finally, we analyze failure
mechanisms and discuss defensive strategies to mitigate these vulnerabilities
in AI for cybersecurity.

</details>


### [431] [T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model](https://arxiv.org/abs/2510.22300)
*Chenyu Zhang,Tairen Zhang,Lanjun Wang,Ruidong Chen,Wenhui Li,Anan Liu*

Main category: cs.CR

TL;DR: 提出T2I - RiskyPrompt基准评估T2I模型安全，构建风险分类、收集标注提示，提出检测方法并评估多模型等，给出见解，还讨论应用，代码数据集开源。


<details>
  <summary>Details</summary>
Motivation: 现有风险提示数据集存在风险类别有限、标注粗粒度、有效性低的问题，需新基准评估T2I模型安全。

Method: 开发分层风险分类法，构建收集和标注风险提示的管道，提出基于原因驱动的风险图像检测方法。

Result: 获得6432个有效风险提示，每个提示有分层类别标签和详细风险原因，对多个T2I模型、防御方法等进行评估。

Conclusion: 基于T2I - RiskyPrompt评估给出T2I模型安全的九个关键见解，还探讨了其在多领域潜在应用。

Abstract: Using risky text prompts, such as pornography and violent prompts, to test
the safety of text-to-image (T2I) models is a critical task. However, existing
risky prompt datasets are limited in three key areas: 1) limited risky
categories, 2) coarse-grained annotation, and 3) low effectiveness. To address
these limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark
designed for evaluating safety-related tasks in T2I models. Specifically, we
first develop a hierarchical risk taxonomy, which consists of 6 primary
categories and 14 fine-grained subcategories. Building upon this taxonomy, we
construct a pipeline to collect and annotate risky prompts. Finally, we obtain
6,432 effective risky prompts, where each prompt is annotated with both
hierarchical category labels and detailed risk reasons. Moreover, to facilitate
the evaluation, we propose a reason-driven risky image detection method that
explicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt,
we conduct a comprehensive evaluation of eight T2I models, nine defense
methods, five safety filters, and five attack strategies, offering nine key
insights into the strengths and limitations of T2I model safety. Finally, we
discuss potential applications of T2I-RiskyPrompt across various research
fields. The dataset and code are provided in
https://github.com/datar001/T2I-RiskyPrompt.

</details>


### [432] [Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study](https://arxiv.org/abs/2510.22561)
*Kaveri Banerjee,Sajal Saha*

Main category: cs.CR

TL;DR: 本文调查区块链平台使用的数字签名方案，分析其实现不可抵赖性和保障系统安全的方式，比较不同设计适用性，指出权衡因素、方案优缺点，强调精心选择签名的重要性并给出实施考虑和开放方向。


<details>
  <summary>Details</summary>
Motivation: 区块链系统需要不可抵赖性保障交易和数据完整性，因此要研究数字签名方案如何实现这一特性及保障系统安全。

Method: 考察代表性方案家族，分析其密码学基础、安全假设和相关属性，基于这些标准比较不同设计的适用性。

Result: 明确不同数字签名方案在吞吐量、存储、可扩展性和攻击面等方面的权衡，总结各方案在区块链环境中的优缺点。

Conclusion: 精心选择数字签名对实现不可抵赖性和信息完整性至关重要，同时给出实施考虑和开放方向，如互操作性和后量子准备。

Abstract: Blockchain systems rely on decentralized ledgers and strong security
guarantees. A key requirement is non-repudiation, which prevents denial of
transaction authorship and supports integrity of recorded data. This work
surveys digital signature schemes used in blockchain platforms and analyzes how
they deliver non-repudiation and contribute to overall system security. We
examine representative scheme families and their cryptographic foundations,
security assumptions, and properties relevant to deployment, including
unforgeability, resistance to malleability, support for aggregation and
multisignature or threshold settings, key and signature sizes, and verification
cost. Using these criteria, we compare the suitability of different designs for
consensus protocols, smart contract constraints, and resource limits. We
highlight practical tradeoffs that affect throughput, storage, scalability, and
attack surfaces, and summarize benefits and limitations of each scheme in
blockchain contexts. The study underscores that carefully chosen digital
signatures are central to achieving non-repudiation and preserving information
integrity, and it outlines implementation considerations and open directions
such as interoperability and post-quantum readiness.

</details>


### [433] [Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents](https://arxiv.org/abs/2510.22620)
*Julia Bazinska,Max Mathys,Francesco Casucci,Mateo Rojas-Carulla,Xander Davies,Alexandra Souly,Niklas Pfister*

Main category: cs.CR

TL;DR: 本文提出威胁快照框架构建安全基准评估大语言模型，发现推理能力增强提升安全性，模型大小与安全性无关，并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 缺乏对骨干大语言模型选择如何影响智能体安全的系统理解，现有框架无法完全应对相关挑战。

Method: 引入威胁快照框架，隔离智能体执行流程中LLM漏洞显现的特定状态，构建基于众包对抗攻击的b³基准。

Result: 评估31个流行LLM，发现增强推理能力可提升安全性，模型大小与安全性无关联。

Conclusion: 发布基准、数据集和评估代码，为智能体开发者提供指导，激励模型开发者重视骨干安全改进。

Abstract: AI agents powered by large language models (LLMs) are being deployed at
scale, yet we lack a systematic understanding of how the choice of backbone LLM
affects agent security. The non-deterministic sequential nature of AI agents
complicates security modeling, while the integration of traditional software
with AI components entangles novel LLM vulnerabilities with conventional
security risks. Existing frameworks only partially address these challenges as
they either capture specific vulnerabilities only or require modeling of
complete agents. To address these limitations, we introduce threat snapshots: a
framework that isolates specific states in an agent's execution flow where LLM
vulnerabilities manifest, enabling the systematic identification and
categorization of security risks that propagate from the LLM to the agent
level. We apply this framework to construct the $\operatorname{b}^3$ benchmark,
a security benchmark based on 194331 unique crowdsourced adversarial attacks.
We then evaluate 31 popular LLMs with it, revealing, among other insights, that
enhanced reasoning capabilities improve security, while model size does not
correlate with security. We release our benchmark, dataset, and evaluation code
to facilitate widespread adoption by LLM providers and practitioners, offering
guidance for agent developers and incentivizing model developers to prioritize
backbone security improvements.

</details>


### [434] [Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks](https://arxiv.org/abs/2510.22628)
*Md. Mehedi Hasan,Ziaur Rahman,Rafid Mostafiz,Md. Abir Hossain*

Main category: cs.CR

TL;DR: 本文提出实时模块化防御系统Sentra - Guard，可检测和缓解针对大语言模型的越狱和提示注入攻击，评估效果佳，建立了对抗性大语言模型防御的新标杆。


<details>
  <summary>Details</summary>
Motivation: 检测和缓解针对大语言模型的越狱和提示注入攻击。

Method: 采用混合架构，结合FAISS索引的SBERT嵌入表示和微调的变压器分类器，有分类器 - 检索器融合模块、语言无关预处理层、HITL反馈循环和双标签知识库。

Result: 检测率达99.96%（AUC = 1.00，F1 = 1.00），攻击成功率仅0.004%，优于LlamaGuard - 2和OpenAI Moderation。

Conclusion: Sentra - Guard透明、可微调、兼容多种后端，模块化设计支持可扩展部署，建立了对抗性大语言模型防御的新标杆。

Abstract: This paper presents a real-time modular defense system named Sentra-Guard.
The system detects and mitigates jailbreak and prompt injection attacks
targeting large language models (LLMs). The framework uses a hybrid
architecture with FAISS-indexed SBERT embedding representations that capture
the semantic meaning of prompts, combined with fine-tuned transformer
classifiers, which are machine learning models specialized for distinguishing
between benign and adversarial language inputs. It identifies adversarial
prompts in both direct and obfuscated attack vectors. A core innovation is the
classifier-retriever fusion module, which dynamically computes context-aware
risk scores that estimate how likely a prompt is to be adversarial based on its
content and context. The framework ensures multilingual resilience with a
language-agnostic preprocessing layer. This component automatically translates
non-English prompts into English for semantic evaluation, enabling consistent
detection across over 100 languages. The system includes a HITL feedback loop,
where decisions made by the automated system are reviewed by human experts for
continual learning and rapid adaptation under adversarial pressure.
Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and
malicious prompts, enhancing detection reliability and reducing false
positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =
1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading
baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike
black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible
with diverse LLM backends. Its modular design supports scalable deployment in
both commercial and open-source environments. The system establishes a new
state-of-the-art in adversarial LLM defense.

</details>


### [435] [SecureLearn -- An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks](https://arxiv.org/abs/2510.22274)
*Anum Paracha,Junaid Arshad,Mohamed Ben Farah,Khalid Ismail*

Main category: cs.CR

TL;DR: 本文提出SecureLearn防御机制保护多类模型免受数据投毒攻击，通过3D评估矩阵实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有防御多针对特定投毒攻击或特定机器学习算法，传统多类分类器在数据投毒攻击防护方面需更多关注。

Method: 提出两层与攻击无关的SecureLearn防御机制，包含数据清理和新的面向特征的对抗训练；提出3D评估矩阵进行评估。

Result: SecureLearn对给定攻击有效，提升传统多类模型和神经网络的弹性与对抗鲁棒性，准确率超90%，召回率和F1分数超75%，神经网络召回率和F1分数达97%。

Conclusion: SecureLearn有效且具有泛化能力，超越特定算法防御。

Abstract: Data poisoning attacks are a potential threat to machine learning (ML)
models, aiming to manipulate training datasets to disrupt their performance.
Existing defenses are mostly designed to mitigate specific poisoning attacks or
are aligned with particular ML algorithms. Furthermore, most defenses are
developed to secure deep neural networks or binary classifiers. However,
traditional multiclass classifiers need attention to be secure from data
poisoning attacks, as these models are significant in developing multi-modal
applications. Therefore, this paper proposes SecureLearn, a two-layer
attack-agnostic defense to defend multiclass models from poisoning attacks. It
comprises two components of data sanitization and a new feature-oriented
adversarial training. To ascertain the effectiveness of SecureLearn, we
proposed a 3D evaluation matrix with three orthogonal dimensions: data
poisoning attack, data sanitization and adversarial training. Benchmarking
SecureLearn in a 3D matrix, a detailed analysis is conducted at different
poisoning levels (10%-20%), particularly analysing accuracy, recall, F1-score,
detection and correction rates, and false discovery rate. The experimentation
is conducted for four ML algorithms, namely Random Forest (RF), Decision Tree
(DT), Gaussian Naive Bayes (GNB) and Multilayer Perceptron (MLP), trained with
three public datasets, against three poisoning attacks and compared with two
existing mitigations. Our results highlight that SecureLearn is effective
against the provided attacks. SecureLearn has strengthened resilience and
adversarial robustness of traditional multiclass models and neural networks,
confirming its generalization beyond algorithm-specific defenses. It
consistently maintained accuracy above 90%, recall and F1-score above 75%. For
neural networks, SecureLearn achieved 97% recall and F1-score against all
selected poisoning attacks.

</details>


### [436] [Adapting Noise-Driven PUF and AI for Secure WBG ICS: A Proof-of-Concept Study](https://arxiv.org/abs/2510.22283)
*Devon A. Kelly,Christiana Chamon*

Main category: cs.CR

TL;DR: 本文针对WBG技术在工业控制系统引入的风险，提出基于噪声驱动PUF和ML的异常检测框架，模拟实验达95%检测准确率和低延迟，为下一代安全策略奠基。


<details>
  <summary>Details</summary>
Motivation: WBG技术在提升电力系统性能同时，给工业控制系统带来传感器损坏和网络安全风险，需有效安全防护方案。

Method: 提取WBG开关噪声作为PUF源和实时威胁指标，结合混合ML模型与自适应贝叶斯滤波。

Result: 通过模拟实验，实现95%检测准确率和亚毫秒级处理延迟。

Conclusion: 利用物理特性进行双重噪声利用作为可扩展ICS防御原语是可行的，为下一代安全策略奠定基础。

Abstract: Wide-bandgap (WBG) technologies offer unprecedented improvements in power
system efficiency, size, and performance, but also introduce unique sensor
corruption and cybersecurity risks in industrial control systems (ICS),
particularly due to high-frequency noise and sophisticated cyber-physical
threats. This proof-of-concept (PoC) study demonstrates the adaptation of a
noise-driven physically unclonable function (PUF) and machine learning
(ML)-assisted anomaly detection framework to the demanding environment of
WBG-based ICS sensor pathways. By extracting entropy from unavoidable WBG
switching noise (up to 100 kHz) as a PUF source, and simultaneously using this
noise as a real-time threat indicator, the proposed system unites
hardware-level authentication and anomaly detection. Our approach integrates
hybrid machine learning (ML) models with adaptive Bayesian filtering, providing
robust and low-latency detection capabilities resilient to both natural
electromagnetic interference (EMI) and active adversarial manipulation. Through
detailed simulations of WBG modules under benign and attack
scenarios--including EMI injection, signal tampering, and node
impersonation--we achieve 95% detection accuracy and sub-millisecond processing
latency. These results demonstrate the feasibility of physics-driven, dual-use
noise exploitation as a scalable ICS defense primitive. Our findings lay the
groundwork for next-generation security strategies that leverage inherent
device characteristics, bridging hardware and artificial intelligence (AI) for
enhanced protection of critical ICS infrastructure.

</details>


### [437] [Privacy-Aware Federated nnU-Net for ECG Page Digitization](https://arxiv.org/abs/2510.22387)
*Nader Nemati*

Main category: cs.CR

TL;DR: 提出跨筒仓联合数字化框架将心电图页面图像转换为波形，实验表明FedAdam收敛更快且接近集中式性能，隐私机制保证安全。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络集中式训练在跨机构时存在的隐私和部署约束问题。

Method: 提出跨筒仓联合数字化框架，集成FedAvg、FedProx和FedAdam三种聚合器，结合安全聚合和差分隐私，有端到端训练、安全聚合等特点及校准感知数字化流程。

Result: 在PTB - XL渲染的心电图页面实验中，FedAdam收敛更快、后期表现更好，接近集中式性能，隐私机制保证准确性和安全性。

Conclusion: 该框架适用于多机构场景，能提供可部署和可审计的保障。

Abstract: Deep neural networks can convert ECG page images into analyzable waveforms,
yet centralized training often conflicts with cross-institutional privacy and
deployment constraints. A cross-silo federated digitization framework is
presented that trains a full-model nnU-Net segmentation backbone without
sharing images and aggregates updates across sites under realistic non-IID
heterogeneity (layout, grid style, scanner profile, noise).
  The protocol integrates three standard server-side aggregators--FedAvg,
FedProx, and FedAdam--and couples secure aggregation with central, user-level
differential privacy to align utility with formal guarantees. Key features
include: (i) end-to-end full-model training and synchronization across clients;
(ii) secure aggregation so the server only observes a clipped, weighted sum
once a participation threshold is met; (iii) central Gaussian DP with Renyi
accounting applied post-aggregation for auditable user-level privacy; and (iv)
a calibration-aware digitization pipeline comprising page normalization, trace
segmentation, grid-leakage suppression, and vectorization to twelve-lead
signals.
  Experiments on ECG pages rendered from PTB-XL show consistently faster
convergence and higher late-round plateaus with adaptive server updates
(FedAdam) relative to FedAvg and FedProx, while approaching centralized
performance. The privacy mechanism maintains competitive accuracy while
preventing exposure of raw images or per-client updates, yielding deployable,
auditable guarantees suitable for multi-institution settings.

</details>


### [438] [Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers](https://arxiv.org/abs/2510.22555)
*Dongyi Liu,Jiangtong Li,Dawei Cheng,Changjun Jiang*

Main category: cs.CR

TL;DR: 提出CP - GBA解决现有图后门攻击触发生成器局限，实验显示其攻击成功率达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有图后门攻击触发生成器结构简单、依赖特定特征、仅适用于单一学习范式，缺乏可迁移性且未充分利用图结构信息，限制攻击成功率。

Method: 提出CP - GBA，从目标图中提取紧凑且有表现力的触发集，利用图提示学习训练通用子图触发，探索其理论可迁移性。

Result: 在多个真实数据集和防御场景实验中，CP - GBA攻击成功率达SOTA。

Conclusion: CP - GBA能有效解决现有图后门攻击触发生成器的局限，具有良好可迁移性和高攻击成功率。

Abstract: Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, where
adversaries implant malicious triggers to manipulate model predictions.
  Existing trigger generators are often simplistic in structure and overly
reliant on specific features, confining them to a single graph learning
paradigm, such as graph supervised learning, graph contrastive learning, or
graph prompt learning.
  This specialized design, which aligns the trigger with one learning
objective, results in poor transferability when applied to other learning
paradigms.
  For instance, triggers generated for the graph supervised learning paradigm
perform poorly when tested within graph contrastive learning or graph prompt
learning environments.
  Furthermore, these simple generators often fail to utilize complex structural
information or node diversity within the graph data.
  These constraints limit the attack success rates of such methods in general
testing scenarios.
  Therefore, to address these limitations, we propose Cross-Paradigm Graph
Backdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferable
graph backdoor attack that employs graph prompt learning(GPL) to train a set of
universal subgraph triggers.
  First, we distill a compact yet expressive trigger set from target graphs,
which is structured as a queryable repository, by jointly enforcing
class-awareness, feature richness, and structural fidelity.
  Second, we conduct the first exploration of the theoretical transferability
of GPL to train these triggers under prompt-based objectives, enabling
effective generalization to diverse and unseen test-time paradigms.
  Extensive experiments across multiple real-world datasets and defense
scenarios show that CP-GBA achieves state-of-the-art attack success rates.

</details>


### [439] [Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies](https://arxiv.org/abs/2510.22944)
*Bin Wang,YiLu Zhong,MiDi Wan,WenJie Yu,YuanBing Ouyang,Yenan Huang,Hui Li*

Main category: cs.CR

TL;DR: 研究低质量良性提示对代码生成安全的影响，提出提示质量评估框架，构建数据集实验，发现提示规范性与代码安全相关，高级提示技术可缓解风险，强调提升提示质量对代码安全重要。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注对抗攻击或模型固有缺陷，未充分探索良性但表述不佳的提示对生成代码安全的影响。

Method: 提出提示质量评估框架，构建CWE - BENCH - PYTHON数据集，在多个LLMs上进行实验。

Result: 提示规范性降低，生成不安全代码可能性显著增加；高级提示技术可缓解低质量提示带来的安全风险。

Conclusion: 提升用户提示质量是加强AI生成代码安全性的关键有效策略。

Abstract: Large language models (LLMs) have become indispensable for automated code
generation, yet the quality and security of their outputs remain a critical
concern. Existing studies predominantly concentrate on adversarial attacks or
inherent flaws within the models. However, a more prevalent yet underexplored
issue concerns how the quality of a benign but poorly formulated prompt affects
the security of the generated code. To investigate this, we first propose an
evaluation framework for prompt quality encompassing three key dimensions: goal
clarity, information completeness, and logical consistency. Based on this
framework, we construct and publicly release CWE-BENCH-PYTHON, a large-scale
benchmark dataset containing tasks with prompts categorized into four distinct
levels of normativity (L0-L3). Extensive experiments on multiple
state-of-the-art LLMs reveal a clear correlation: as prompt normativity
decreases, the likelihood of generating insecure code consistently and markedly
increases. Furthermore, we demonstrate that advanced prompting techniques, such
as Chain-of-Thought and Self-Correction, effectively mitigate the security
risks introduced by low-quality prompts, substantially improving code safety.
Our findings highlight that enhancing the quality of user prompts constitutes a
critical and effective strategy for strengthening the security of AI-generated
code.

</details>


### [440] [CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents](https://arxiv.org/abs/2510.22963)
*Zesen Liu,Zhixiang Zhang,Yuchong Xie,Dongdong She*

Main category: cs.CR

TL;DR: 论文指出大语言模型代理的提示压缩存在安全风险，提出CompressionAttack框架进行攻击实验，显示高成功率和翻转率，现有防御无效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理使用提示压缩降低推理成本时引入新安全风险，压缩模块易被对抗输入操纵。

Method: 提出CompressionAttack框架，包含HardCom和SoftCom两种策略。

Result: 在多个大语言模型上实验显示攻击成功率达80%，偏好翻转率达98%，攻击隐蔽且可迁移，案例研究证实现实影响，现有防御无效。

Conclusion: 提示压缩是新攻击面，需要更强的安全防护措施。

Abstract: LLM-powered agents often use prompt compression to reduce inference costs,
but this introduces a new security risk. Compression modules, which are
optimized for efficiency rather than safety, can be manipulated by adversarial
inputs, causing semantic drift and altering LLM behavior. This work identifies
prompt compression as a novel attack surface and presents CompressionAttack,
the first framework to exploit it. CompressionAttack includes two strategies:
HardCom, which uses discrete adversarial edits for hard compression, and
SoftCom, which performs latent-space perturbations for soft compression.
Experiments on multiple LLMs show up to 80% attack success and 98% preference
flips, while remaining highly stealthy and transferable. Case studies in VSCode
Cline and Ollama confirm real-world impact, and current defenses prove
ineffective, highlighting the need for stronger protections.

</details>


### [441] [Efficient and Encrypted Inference using Binarized Neural Networks within In-Memory Computing Architectures](https://arxiv.org/abs/2510.23034)
*Gokulnath Rajendran,Suman Deb,Anupam Chattopadhyay*

Main category: cs.CR

TL;DR: 本文提出在内存计算框架中保护二值神经网络（BNNs）模型参数的策略，用物理不可克隆函数密钥转换参数，在加密权重上推理，验证了策略有效性。


<details>
  <summary>Details</summary>
Motivation: 现有加密和解密BNN模型参数的方法会引入显著计算开销，违背内存计算原则，需新的保护策略。

Method: 利用物理不可克隆函数导出的密钥在存储到交叉开关前转换模型参数，在加密权重上进行推理操作，实现特殊的全同态加密。

Result: 无密钥推理性能大幅下降，准确率低于15%。

Conclusion: 提出的保护策略能有效保护内存计算架构中的BNNs，同时保持计算效率。

Abstract: Binarized Neural Networks (BNNs) are a class of deep neural networks designed
to utilize minimal computational resources, which drives their popularity
across various applications. Recent studies highlight the potential of mapping
BNN model parameters onto emerging non-volatile memory technologies,
specifically using crossbar architectures, resulting in improved inference
performance compared to traditional CMOS implementations. However, the common
practice of protecting model parameters from theft attacks by storing them in
an encrypted format and decrypting them at runtime introduces significant
computational overhead, thus undermining the core principles of in-memory
computing, which aim to integrate computation and storage. This paper presents
a robust strategy for protecting BNN model parameters, particularly within
in-memory computing frameworks. Our method utilizes a secret key derived from a
physical unclonable function to transform model parameters prior to storage in
the crossbar. Subsequently, the inference operations are performed on the
encrypted weights, achieving a very special case of Fully Homomorphic
Encryption (FHE) with minimal runtime overhead. Our analysis reveals that
inference conducted without the secret key results in drastically diminished
performance, with accuracy falling below 15%. These results validate the
effectiveness of our protection strategy in securing BNNs within in-memory
computing architectures while preserving computational efficiency.

</details>


### [442] [A high-capacity linguistic steganography based on entropy-driven rank-token mapping](https://arxiv.org/abs/2510.23035)
*Jun Jiang,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CR

TL;DR: 提出RTMStega框架解决语言隐写术在有效载荷容量和安全性方面的问题，实验表明该框架有显著优势。


<details>
  <summary>Details</summary>
Motivation: 当前语言隐写术方法在有效载荷容量和安全性方面存在关键限制，如传统方法有可检测异常、检索策略嵌入容量低、现代生成式隐写术熵有限等。

Method: 提出熵驱动框架RTMStega，集成基于排名的自适应编码、上下文感知解压缩和归一化熵，将秘密消息映射到令牌概率排名并通过上下文感知熵调整动态调整采样。

Result: 在不同数据集和模型的实验中，RTMStega使主流生成式隐写术的有效载荷容量增加两倍，处理时间减少超50%，并保持高文本质量。

Conclusion: RTMStega为安全高效的隐蔽通信提供了可靠解决方案。

Abstract: Linguistic steganography enables covert communication through embedding
secret messages into innocuous texts; however, current methods face critical
limitations in payload capacity and security. Traditional modification-based
methods introduce detectable anomalies, while retrieval-based strategies suffer
from low embedding capacity. Modern generative steganography leverages language
models to generate natural stego text but struggles with limited entropy in
token predictions, further constraining capacity. To address these issues, we
propose an entropy-driven framework called RTMStega that integrates rank-based
adaptive coding and context-aware decompression with normalized entropy. By
mapping secret messages to token probability ranks and dynamically adjusting
sampling via context-aware entropy-based adjustments, RTMStega achieves a
balance between payload capacity and imperceptibility. Experiments across
diverse datasets and models demonstrate that RTMStega triples the payload
capacity of mainstream generative steganography, reduces processing time by
over 50%, and maintains high text quality, offering a trustworthy solution for
secure and efficient covert communication.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [443] [Tree-Cotree-Based IETI-DP for Eddy Current Problems in Time-Domain](https://arxiv.org/abs/2510.23446)
*Mario Mally,Rafael Vázquez,Sebastian Schöps*

Main category: math.NA

TL;DR: 提出时域涡流的新型撕裂与互连方法并研究其可扩展性


<details>
  <summary>Details</summary>
Motivation: 低频电磁问题时域仿真计算成本高

Method: 提出时域涡流的新型撕裂与互连方法

Result: 未提及

Conclusion: 未提及

Abstract: For low-frequency electromagnetic problems, where wave-propagation effects
can be neglected, eddy current formulations are commonly used as a
simplification of the full Maxwell's equations. In this setup, time-domain
simulations, needed to capture transient startup responses or nonlinear
behavior, are often computationally expensive. We propose a novel tearing and
interconnecting approach for eddy currents in time-domain and investigate its
scalability.

</details>


### [444] [An Introductory Guide to Koopman Learning](https://arxiv.org/abs/2510.22002)
*Matthew J. Colbrook,Zlatko Drmač,Andrew Horning*

Main category: math.NA

TL;DR: 本文提供Koopman学习的入门指南，介绍严格收敛的数据驱动方法进行预测和谱分析。


<details>
  <summary>Details</summary>
Motivation: Koopman算子的无限维特性带来计算挑战，需要可靠的数据驱动技术进行谱分析。

Method: 统一有限和无限维环境下通过残差进行误差控制，给出广义拉普拉斯分析收敛的初等证明，回顾计算连续谱和谱测度的先进方法。

Result: 提供了对可靠数据驱动技术的清晰、结构化概述。

Conclusion: 为新手和专家提供了Koopman谱分析的可靠数据驱动技术的清晰概述。

Abstract: Koopman operators provide a linear framework for data-driven analyses of
nonlinear dynamical systems, but their infinite-dimensional nature presents
major computational challenges. In this article, we offer an introductory guide
to Koopman learning, emphasizing rigorously convergent data-driven methods for
forecasting and spectral analysis. We provide a unified account of error
control via residuals in both finite- and infinite-dimensional settings, an
elementary proof of convergence for generalized Laplace analysis -- a variant
of filtered power iteration that works for operators with continuous spectra
and no spectral gaps -- and review state-of-the-art approaches for computing
continuous spectra and spectral measures. The goal is to provide both newcomers
and experts with a clear, structured overview of reliable data-driven
techniques for Koopman spectral analysis.

</details>


### [445] [Stable neural networks and connections to continuous dynamical systems](https://arxiv.org/abs/2510.22299)
*Matthias J. Ehrhardt,Davide Murari,Ferdia Sherry*

Main category: math.NA

TL;DR: 本文聚焦神经网络稳定性研究中与连续动力系统和最优控制相关的分支，介绍基础概念，详述特定设计稳定神经网络的方法，提供代码和示例并进行讨论，将作为科学机器学习书籍章节供学生阅读。


<details>
  <summary>Details</summary>
Motivation: 神经网络存在不稳定性（如对抗样本），需理解和增强其稳定性，本文聚焦该领域特定分支进行研究。

Method: 先介绍领域基础概念，再详细阐述特定设计稳定神经网络的方法，开发理论背景，给出实现方式，提供可改编扩展的代码和玩具示例。

Result: 提供了实现特定方法的代码，包含一个可在普通计算机运行的图像分类对抗鲁棒性玩具示例。

Conclusion: 完成对该研究领域的介绍和方法实现，内容将作为科学机器学习书籍章节，有助于学生学习。

Abstract: The existence of instabilities, for example in the form of adversarial
examples, has given rise to a highly active area of research concerning itself
with understanding and enhancing the stability of neural networks. We focus on
a popular branch within this area which draws on connections to continuous
dynamical systems and optimal control, giving a bird's eye view of this area.
We identify and describe the fundamental concepts that underlie much of the
existing work in this area. Following this, we go into more detail on a
specific approach to designing stable neural networks, developing the
theoretical background and giving a description of how these networks can be
implemented. We provide code that implements the approach that can be adapted
and extended by the reader. The code further includes a notebook with a
fleshed-out toy example on adversarial robustness of image classification that
can be run without heavy requirements on the reader's computer. We finish by
discussing this toy example so that the reader can interactively follow along
on their computer. This work will be included as a chapter of a book on
scientific machine learning, which is currently under revision and aimed at
students.

</details>


### [446] [Multi-Scale Finite Expression Method for PDEs with Oscillatory Solutions on Complex Domains](https://arxiv.org/abs/2510.22497)
*Gareth Hardwick,Haizhao Yang*

Main category: math.NA

TL;DR: 本文介绍增强的有限表达式方法（FEX）解决复杂区域上含高振荡解的偏微分方程，实验表明其准确高效且结果可解释。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法和机器学习方法在解决复杂区域上含高振荡解的偏微分方程时存在表示成本高、优化困难等问题。

Method: 提出增强的FEX，包含符号谱组合模块、重新设计的线性输入层和特征值公式三个关键创新。

Result: FEX能准确求解含不同形状和大小孔洞区域上的振荡偏微分方程，比现有神经网络求解器精度更高，且能得到可解释的闭式解。

Conclusion: FEX是解决复杂偏微分方程的强大且透明的框架。

Abstract: Solving partial differential equations (PDEs) with highly oscillatory
solutions on complex domains remains a challenging and important problem.
High-frequency oscillations and intricate geometries often result in
prohibitively expensive representations for traditional numerical methods and
lead to difficult optimization landscapes for machine learning-based
approaches. In this work, we introduce an enhanced Finite Expression Method
(FEX) designed to address these challenges with improved accuracy,
interpretability, and computational efficiency. The proposed framework
incorporates three key innovations: a symbolic spectral composition module that
enables FEX to learn and represent multiscale oscillatory behavior; a
redesigned linear input layer that significantly expands the expressivity of
the model; and an eigenvalue formulation that extends FEX to a new class of
problems involving eigenvalue PDEs. Through extensive numerical experiments, we
demonstrate that FEX accurately resolves oscillatory PDEs on domains containing
multiple holes of varying shapes and sizes. Compared with existing neural
network-based solvers, FEX achieves substantially higher accuracy while
yielding interpretable, closed-form solutions that expose the underlying
structure of the problem. These advantages, often absent in conventional finite
element, finite difference, and black-box neural approaches, highlight FEX as a
powerful and transparent framework for solving complex PDEs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [447] [GuitarFlow: Realistic Electric Guitar Synthesis From Tablatures via Flow Matching and Style Transfer](https://arxiv.org/abs/2510.21872)
*Jackson Loth,Pedro Sarmento,Mark Sandler,Mathieu Barthet*

Main category: cs.SD

TL;DR: 介绍GuitarFlow模型用于电吉他合成，以制表符引导生成过程，结合简单虚拟乐器和流匹配进行风格迁移，训练和推理快，生成音频更真实。


<details>
  <summary>Details</summary>
Motivation: 当前部分乐器（如吉他）可控乐器合成表现力有限，需改进电吉他合成。

Method: 以制表符引导生成过程，先将制表符用简单样本虚拟乐器渲染成音频，再用流匹配进行风格迁移。

Result: 模型训练和推理快，所需训练数据少于6小时，客观评估指标和听力测试显示生成吉他音频的真实度显著提高。

Conclusion: GuitarFlow模型在电吉他合成方面能有效提升生成音频的真实度。

Abstract: Music generation in the audio domain using artificial intelligence (AI) has
witnessed steady progress in recent years. However for some instruments,
particularly the guitar, controllable instrument synthesis remains limited in
expressivity. We introduce GuitarFlow, a model designed specifically for
electric guitar synthesis. The generative process is guided using tablatures,
an ubiquitous and intuitive guitar-specific symbolic format. The tablature
format easily represents guitar-specific playing techniques (e.g. bends, muted
strings and legatos), which are more difficult to represent in other common
music notation formats such as MIDI. Our model relies on an intermediary step
of first rendering the tablature to audio using a simple sample-based virtual
instrument, then performing style transfer using Flow Matching in order to
transform the virtual instrument audio into more realistic sounding examples.
This results in a model that is quick to train and to perform inference,
requiring less than 6 hours of training data. We present the results of
objective evaluation metrics, together with a listening test, in which we show
significant improvement in the realism of the generated guitar audio from
tablatures.

</details>


### [448] [PromptReverb: Multimodal Room Impulse Response Generation Through Latent Rectified Flow Matching](https://arxiv.org/abs/2510.22439)
*Ali Vosoughi,Yongyi Zang,Qihui Yang,Nathan Peak,Randal Leistikow,Chenliang Xu*

Main category: cs.SD

TL;DR: 提出PromptReverb框架解决RIR生成难题，效果优于现有方法，有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前RIR生成方法存在全频段数据集稀缺和无法从多样输入模态生成准确响应的问题。

Method: 采用两阶段生成框架，结合变分自编码器提升RIR频段质量，使用基于整流流匹配的条件扩散变压器模型从自然语言描述生成RIR。

Result: 生成的RIR感知质量和声学准确性优于现有方法，平均RT60误差为8.8%，房间声学参数更真实。

Conclusion: 该方法可用于虚拟现实、建筑声学和音频制作等领域。

Abstract: Room impulse response (RIR) generation remains a critical challenge for
creating immersive virtual acoustic environments. Current methods suffer from
two fundamental limitations: the scarcity of full-band RIR datasets and the
inability of existing models to generate acoustically accurate responses from
diverse input modalities. We present PromptReverb, a two-stage generative
framework that addresses these challenges. Our approach combines a variational
autoencoder that upsamples band-limited RIRs to full-band quality (48 kHz), and
a conditional diffusion transformer model based on rectified flow matching that
generates RIRs from descriptions in natural language. Empirical evaluation
demonstrates that PromptReverb produces RIRs with superior perceptual quality
and acoustic accuracy compared to existing methods, achieving 8.8% mean RT60
error compared to -37% for widely used baselines and yielding more realistic
room-acoustic parameters. Our method enables practical applications in virtual
reality, architectural acoustics, and audio production where flexible,
high-quality RIR synthesis is essential.

</details>


### [449] [Evaluating Multimodal Large Language Models on Core Music Perception Tasks](https://arxiv.org/abs/2510.22455)
*Brandon James Carone,Iran R. Roman,Pablo Ripollés*

Main category: cs.SD

TL;DR: 对三款SOTA多模态大语言模型进行音乐技能基准测试，发现模型在MIDI上表现好但在音频上有感知差距，推理和少样本提示提升有限，Gemini Pro表现最佳，当前系统不能可靠“听”音频。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在音乐理解评估中混淆聆听和读谱，需准确评估其音乐技能。

Method: 对三款SOTA模型进行三项核心音乐技能基准测试，分离三种可变性来源，将LogicLM框架应用于音乐。

Result: 模型在MIDI上接近天花板表现，但在音频上准确率下降；推理和少样本提示提升有限；Gemini Pro在多数条件下表现最佳。

Conclusion: 当前系统能对符号（MIDI）进行良好推理，但不能可靠“听”音频，研究方法和数据集为构建强大的音频优先音乐系统提供指导。

Abstract: Multimodal Large Language Models (LLMs) claim "musical understanding" via
evaluations that conflate listening with score reading. We benchmark three SOTA
LLMs (Gemini 2.5 Pro, Gemini 2.5 Flash, and Qwen2.5-Omni) across three core
music skills: Syncopation Scoring, Transposition Detection, and Chord Quality
Identification. Moreover, we separate three sources of variability: (i)
perceptual limitations (audio vs. MIDI inputs), (ii) exposure to examples
(zero- vs. few-shot manipulations), and (iii) reasoning strategies (Standalone,
CoT, LogicLM). For the latter we adapt LogicLM, a framework combining LLMs with
symbolic solvers to perform structured reasoning, to music. Results reveal a
clear perceptual gap: models perform near ceiling on MIDI but show accuracy
drops on audio. Reasoning and few-shot prompting offer minimal gains. This is
expected for MIDI, where performance reaches saturation, but more surprising
for audio, where LogicLM, despite near-perfect MIDI accuracy, remains notably
brittle. Among models, Gemini Pro achieves the highest performance across most
conditions. Overall, current systems reason well over symbols (MIDI) but do not
yet "listen" reliably from audio. Our method and dataset make the
perception-reasoning boundary explicit and offer actionable guidance for
building robust, audio-first music systems.

</details>


### [450] [SAO-Instruct: Free-form Audio Editing using Natural Language Instructions](https://arxiv.org/abs/2510.22795)
*Michael Ungersböck,Florian Grötschla,Luca A. Lanzendörfer,June Young Yi,Changho Choi,Roger Wattenhofer*

Main category: cs.SD

TL;DR: 提出SAO - Instruct模型，可使用自由形式自然语言指令编辑音频，训练后表现良好并开源。


<details>
  <summary>Details</summary>
Motivation: 现有音频编辑方法需完整描述或受限于预定义指令，缺乏灵活性，因此要探索用自然语言编辑现有音频。

Method: 创建基于Prompt - to - Prompt、DDPM inversion和手动编辑管道的音频编辑三元组数据集来训练基于Stable Audio Open的SAO - Instruct模型。

Result: SAO - Instruct在客观指标上有竞争力，在主观听力研究中优于其他音频编辑方法。

Conclusion: SAO - Instruct能有效用自由形式自然语言指令编辑音频，发布代码和模型权重以推动研究。

Abstract: Generative models have made significant progress in synthesizing
high-fidelity audio from short textual descriptions. However, editing existing
audio using natural language has remained largely underexplored. Current
approaches either require the complete description of the edited audio or are
constrained to predefined edit instructions that lack flexibility. In this
work, we introduce SAO-Instruct, a model based on Stable Audio Open capable of
editing audio clips using any free-form natural language instruction. To train
our model, we create a dataset of audio editing triplets (input audio, edit
instruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual
editing pipeline. Although partially trained on synthetic data, our model
generalizes well to real in-the-wild audio clips and unseen edit instructions.
We demonstrate that SAO-Instruct achieves competitive performance on objective
metrics and outperforms other audio editing approaches in a subjective
listening study. To encourage future research, we release our code and model
weights.

</details>


### [451] [Learning Linearity in Audio Consistency Autoencoders via Implicit Regularization](https://arxiv.org/abs/2510.23530)
*Bernardo Torres,Manuel Moussallam,Gabriel Meseguer-Brocal*

Main category: cs.SD

TL;DR: 提出简单训练方法使高压缩一致性自动编码器具有线性，在音乐源合成和分离测试有实用性。


<details>
  <summary>Details</summary>
Motivation: 音频自动编码器非线性潜在空间阻碍直观代数操作，需要使其具有线性。

Method: 使用数据增强，在不改变模型架构和损失函数情况下，使高压缩一致性自动编码器具备齐次性和可加性。

Result: 自动编码器在编码和解码时表现出线性行为，且保留重建保真度。

Conclusion: 该方法可构建结构化潜在空间，实现更直观高效的音频处理。

Abstract: Audio autoencoders learn useful, compressed audio representations, but their
non-linear latent spaces prevent intuitive algebraic manipulation such as
mixing or scaling. We introduce a simple training methodology to induce
linearity in a high-compression Consistency Autoencoder (CAE) by using data
augmentation, thereby inducing homogeneity (equivariance to scalar gain) and
additivity (the decoder preserves addition) without altering the model's
architecture or loss function. When trained with our method, the CAE exhibits
linear behavior in both the encoder and decoder while preserving reconstruction
fidelity. We test the practical utility of our learned space on music source
composition and separation via simple latent arithmetic. This work presents a
straightforward technique for constructing structured latent spaces, enabling
more intuitive and efficient audio processing.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [452] [Analysis of accuracy and efficiency of neural networks to simulate Navier-Stokes fluid flows with obstacles](https://arxiv.org/abs/2510.22976)
*Rui Hespanha,Elliot McGuire,João Hespanha*

Main category: physics.flu-dyn

TL;DR: 研究用神经网络模拟含障碍物环境中不可压缩流体，误差小且速度快，可用于多场景。


<details>
  <summary>Details</summary>
Motivation: 传统流体模拟耗时耗能，探索用神经网络替代纳维 - 斯托克斯方程数值模拟的可行性。

Method: 采用神经网络对随机多障碍物环境中的不可压缩流体进行模拟。

Result: 训练集和测试集均方根误差分别为0.32%和0.36%，t=10和t=20时误差有增长；神经网络预测速度比传统模拟快约8800倍。

Conclusion: 神经网络在多障碍物环境流体模拟中非常有用，可应用于模拟森林火灾烟雾、管道流体、水下/洪水水流等场景。

Abstract: Conventional fluid simulations can be time consuming and energy intensive. We
researched the viability of a neural network for simulating incompressible
fluids in a randomized obstacle-heavy environment, as an alternative to the
numerical simulation of the Navier-Stokes equation. We hypothesized that the
neural network predictions would have a relatively low error for simulations
over a small number of time steps, but errors would eventually accumulate to
the point that the output would become very noisy. Over a rich set of obstacle
configurations, we achieved a root mean square error of 0.32% on our training
dataset and 0.36% on a testing dataset. These errors only grew to 1.45% and
2.34% at t = 10 and, 2.11% and 4.16% at timestep t = 20. We also found that our
selected neural network was approximately 8,800 times faster at predicting the
flow than a conventional simulation. Our findings indicate neural networks can
be extremely useful at simulating fluids in obstacle-heavy environments. Useful
applications include modeling forest fire smoke, pipe fluid flow, and
underwater/flood currents.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [453] [Iterative Layer Pruning for Efficient Translation Inference](https://arxiv.org/abs/2510.22763)
*Yasmin Moslem,Muhammad Hazim Al Farouq,John D. Kelleher*

Main category: cs.CL

TL;DR: 本文介绍WMT 2025模型压缩赛道提交成果，用迭代层剪枝法减少模型大小和推理时间，同时保持翻译质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型计算需求大，高效部署具有挑战性。

Method: 基于层重要性分析的迭代层剪枝方法，并使用Aya - Expanse - 8B模型在特定翻译任务上进行实验。

Result: 该方法大幅减少了模型大小和推理时间，同时保持了基线模型的翻译质量。

Conclusion: 迭代层剪枝法可有效解决大型语言模型高效部署问题。

Abstract: Large language models (LLMs) have transformed many areas of natural language
processing, including machine translation. However, efficient deployment of
LLMs remains challenging due to their intensive computational requirements. In
this paper, we address this challenge and present our submissions to the Model
Compression track at the Conference on Machine Translation (WMT 2025). In our
experiments, we investigate iterative layer pruning guided by layer importance
analysis. We evaluate this method using the Aya-Expanse-8B model for
translation from Czech to German, and from English to Egyptian Arabic. Our
approach achieves substantial reductions in model size and inference time,
while maintaining the translation quality of the baseline models.

</details>


### [454] [Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation](https://arxiv.org/abs/2510.21891)
*Dhrupad Bhardwaj,Julia Kempe,Tim G. J. Rudner*

Main category: cs.CL

TL;DR: 本文提出语义各向同性评估大语言模型长文本回复可信度，无需标注数据等，多领域表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型长文本回复可信度评估方法计算昂贵且脆弱，需要可靠、低成本方法。

Method: 引入语义各向同性，将长文本回复嵌入，以单位球上嵌入的角度分散度估计语义各向同性。

Result: 更高语义各向同性表明样本间事实一致性更低，方法在多领域预测非事实性表现优于现有方法。

Conclusion: 该方法无需标注数据等，能以低成本将可信度评估融入实际大语言模型工作流程。

Abstract: To deploy large language models (LLMs) in high-stakes application domains
that require substantively accurate responses to open-ended prompts, we need
reliable, computationally inexpensive methods that assess the trustworthiness
of long-form responses generated by LLMs. However, existing approaches often
rely on claim-by-claim fact-checking, which is computationally expensive and
brittle in long-form responses to open-ended prompts. In this work, we
introduce semantic isotropy -- the degree of uniformity across normalized text
embeddings on the unit sphere -- and use it to assess the trustworthiness of
long-form responses generated by LLMs. To do so, we generate several long-form
responses, embed them, and estimate the level of semantic isotropy of these
responses as the angular dispersion of the embeddings on the unit sphere. We
find that higher semantic isotropy -- that is, greater embedding dispersion --
reliably signals lower factual consistency across samples. Our approach
requires no labeled data, no fine-tuning, and no hyperparameter selection, and
can be used with open- or closed-weight embedding models. Across multiple
domains, our method consistently outperforms existing approaches in predicting
nonfactuality in long-form responses using only a handful of samples --
offering a practical, low-cost approach for integrating trust assessment into
real-world LLM workflows.

</details>


### [455] [PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding](https://arxiv.org/abs/2510.22264)
*Iliass Ayaou,Denis Cavallucci*

Main category: cs.CL

TL;DR: 提出综合基准PatenTEB，开发patembed模型族，外部验证表现好，资源将开源。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法充分捕捉专利特定挑战。

Method: 引入含15个任务、206万个示例的PatenTEB，采用领域分层分割等方法；通过多任务训练开发patembed模型族。

Result: patembed-base在MTEB BigPatentClustering.v2达SOTA；patembed-large在DAPFAM有较好结果；多任务训练和领域预训练初始化有积极效果。

Conclusion: 多任务训练虽有小成本但提升外部泛化能力，领域预训练初始化有一致优势。

Abstract: Patent text embeddings enable prior art search, technology landscaping, and
patent analysis, yet existing benchmarks inadequately capture patent-specific
challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15
tasks across retrieval, classification, paraphrase, and clustering, with 2.06
million examples. PatenTEB employs domain-stratified splits, domain specific
hard negative mining, and systematic coverage of asymmetric
fragment-to-document matching scenarios absent from general embedding
benchmarks. We develop the patembed model family through multi-task training,
spanning 67M to 344M parameters with context lengths up to 4096 tokens.
External validation shows strong generalization: patembed-base achieves
state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445
previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.
Systematic ablations reveal that multi-task training improves external
generalization despite minor benchmark costs, and that domain-pretrained
initialization provides consistent advantages across task families. All
resources will be made available at https://github.com/iliass-y/patenteb.
Keywords: patent retrieval, sentence embeddings, multi-task learning,
asymmetric retrieval, benchmark evaluation, contrastive learning.

</details>


### [456] [FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.22344)
*Mohammad Aghajani Asl,Majid Asgari-Bidhendi,Behrooz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: 现有RAG框架处理复杂多跳查询能力不足，本文提出FAIR - RAG框架，经实验在多跳QA基准测试中显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架在处理复杂多跳查询时存在问题，缺乏有效识别和填补证据缺口的机制。

Method: 引入FAIR - RAG框架，核心是由SEA模块控制的迭代细化循环，SEA解构查询、审核证据，自适应查询细化代理生成新子查询。

Result: 在多跳QA基准测试中，FAIR - RAG显著优于强基线，在HotpotQA上F1分数达0.453，比最强迭代基线提高8.3分。

Conclusion: 结构化、证据驱动的细化过程及明确的缺口分析对先进RAG系统进行复杂知识密集型任务的可靠准确推理至关重要。

Abstract: While Retrieval-Augmented Generation (RAG) mitigates hallucination and
knowledge staleness in Large Language Models (LLMs), existing frameworks often
falter on complex, multi-hop queries that require synthesizing information from
disparate sources. Current advanced RAG methods, employing iterative or
adaptive strategies, lack a robust mechanism to systematically identify and
fill evidence gaps, often propagating noise or failing to gather a
comprehensive context. We introduce FAIR-RAG, a novel agentic framework that
transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning
process. At its core is an Iterative Refinement Cycle governed by a module we
term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating
mechanism: it deconstructs the initial query into a checklist of required
findings and audits the aggregated evidence to identify confirmed facts and,
critically, explicit informational gaps. These gaps provide a precise signal to
an Adaptive Query Refinement agent, which generates new, targeted sub-queries
to retrieve missing information. This cycle repeats until the evidence is
verified as sufficient, ensuring a comprehensive context for a final, strictly
faithful generation. We conducted experiments on challenging multi-hop QA
benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified
experimental setup, FAIR-RAG significantly outperforms strong baselines. On
HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3
points over the strongest iterative baseline -- establishing a new
state-of-the-art for this class of methods on these benchmarks. Our work
demonstrates that a structured, evidence-driven refinement process with
explicit gap analysis is crucial for unlocking reliable and accurate reasoning
in advanced RAG systems for complex, knowledge-intensive tasks.

</details>


### [457] [Scalable Supervising Software Agents with Patch Reasoner](https://arxiv.org/abs/2510.22775)
*Junjielong Xu,Boyin Tan,Xiaoyuan Liu,Chao Peng,Pengfei Gao,Pinjia He*

Main category: cs.CL

TL;DR: 现有测试监督方法难扩展限制数据扩展对软件工程代理的改进，本文提出R4P通过推理提供可扩展奖励，验证准确率高、速度快，基于其训练的Mini - SE性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于测试的监督方法不可扩展，限制了软件工程代理数据扩展的潜在改进。

Method: 提出R4P补丁验证模型，使用分组目标进行强化学习训练，通过推理为软件工程代理训练和测试提供可扩展奖励。

Result: R4P验证准确率达72.2%，超过OpenAI o3；Mini - SE在SWE - bench - verified上Pass@1达26.2%，比原Qwen3 - 32B提升10.0%，测试时扩展可提升到32.8%；R4P验证速度比测试快50倍。

Conclusion: R4P奖励和准确率稳定，效率高，具有实用性。

Abstract: While large language model agents have advanced software engineering tasks,
the unscalable nature of existing test-based supervision is limiting the
potential improvement of data scaling. The reason is twofold: (1) building and
running test sandbox is rather heavy and fragile, and (2) data with
high-coverage tests is naturally rare and threatened by test hacking via edge
cases. In this paper, we propose R4P, a patch verifier model to provide
scalable rewards for training and testing SWE agents via reasoning. We consider
that patch verification is fundamentally a reasoning task, mirroring how human
repository maintainers review patches without writing and running new
reproduction tests. To obtain sufficient reference and reduce the risk of
reward hacking, R4P uses a group-wise objective for RL training, enabling it to
verify multiple patches against each other's modification and gain a dense
reward for stable training. R4P achieves 72.2% Acc. for verifying patches from
SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we
design and train a lite scaffold, Mini-SE, with pure reinforcement learning
where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2%
Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original
Qwen3-32B. This can be further improved to 32.8% with R4P for test-time
scaling. Furthermore, R4P verifies patches within a second, 50x faster than
testing on average. The stable scaling curves of rewards and accuracy along
with high efficiency reflect R4P's practicality.

</details>


### [458] [Language Server CLI Empowers Language Agents with Process Rewards](https://arxiv.org/abs/2510.22907)
*Yifan Zhang,Lanser Contributors*

Main category: cs.CL

TL;DR: 介绍Lanser - CLI，一个CLI优先的编排层，可连接语言服务器协议服务器，为编码智能体和CI提供服务，并阐述其多项贡献。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在幻觉API和误定位编辑问题，而语言服务器能提供真实代码的可靠信息，因此开发Lanser - CLI来解决相关问题。

Method: 开发Lanser - CLI，提供健壮的寻址方案、确定性分析包、操作安全范围和基于语言服务器事实的过程奖励函数，并对确定性进行形式化，建立过程奖励的单调性。

Result: 实现了Lanser - CLI及其各项功能，如寻址、分析包生成、操作安全保障和过程奖励计算等。

Conclusion: Lanser - CLI能利用语言服务器的信息，提供确定性、可复现的工作流程，适合过程监督和反事实分析。

Abstract: Large language models routinely hallucinate APIs and mislocalize edits, while
language servers compute verified, IDE-grade facts about real code. We present
Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language
Server Protocol (LSP) server for coding agents and CI, exposing deterministic,
replayable workflows. Our position is that language servers provide not only
structural information (definitions, references, types, diagnostics) but also
an actionable process reward: machine-checked, step-wise signals that align an
agent's planning loop with program reality. In this work, Lanser-CLI
contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via
a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a
principled relocation algorithm; (ii) deterministic Analysis Bundles that
normalize Language Server responses and capture environment/capability metadata
with stable content hashes; (iii) a safety envelope for mutating operations
(rename, code actions) with preview, workspace jails, and Git-aware,
transactional apply; and (iv) a process-reward functional derived from Language
Server facts (diagnostic deltas, disambiguation confidence, and safe-apply
checks) that is computable online and replayable offline. We formalize
determinism under frozen snapshots and establish a monotonicity property for
the process reward, making it suitable for process supervision and
counterfactual analysis. Project Page:
https://github.com/yifanzhang-pro/lanser-cli

</details>


### [459] [$\text{E}^2\text{Rank}$: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker](https://arxiv.org/abs/2510.22733)
*Qi Liu,Yanzhao Zhang,Mingxin Li,Dingkun Long,Pengjun Xie,Jiaxin Mao*

Main category: cs.CL

TL;DR: 提出E²Rank框架，让单一文本嵌入模型兼具高效检索与重排能力，在多个基准测试表现佳。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入模型排名保真度有限，相比专用重排器尤其是基于大语言模型的列表式重排器有差距，需提升其性能。

Method: 提出E²Rank框架，通过在列表式排名目标下持续训练扩展单一文本嵌入模型，用查询和文档嵌入的余弦相似度作为统一排名函数，构建列表式排名提示增强查询。

Result: 在BEIR重排基准测试达最优，在BRIGHT基准测试有竞争力且重排延迟低，排名训练提升了MTEB基准测试的嵌入性能。

Conclusion: 单一嵌入模型可有效统一检索和重排，兼具计算效率和有竞争力的排名准确性。

Abstract: Text embedding models serve as a fundamental component in real-world search
applications. By mapping queries and documents into a shared embedding space,
they deliver competitive retrieval performance with high efficiency. However,
their ranking fidelity remains limited compared to dedicated rerankers,
especially recent LLM-based listwise rerankers, which capture fine-grained
query-document and document-document interactions. In this paper, we propose a
simple yet effective unified framework $\text{E}^2\text{Rank}$, means Efficient
Embedding-based Ranking (also means Embedding-to-Rank), which extends a single
text embedding model to perform both high-quality retrieval and listwise
reranking through continued training under a listwise ranking objective,
thereby achieving strong effectiveness with remarkable efficiency. By applying
cosine similarity between the query and document embeddings as a unified
ranking function, the listwise ranking prompt, which is constructed from the
original query and its candidate documents, serves as an enhanced query
enriched with signals from the top-K documents, akin to pseudo-relevance
feedback (PRF) in traditional retrieval models. This design preserves the
efficiency and representational quality of the base embedding model while
significantly improving its reranking performance. Empirically,
$\textrm{E}^2\text{Rank}$ achieves state-of-the-art results on the BEIR
reranking benchmark and demonstrates competitive performance on the
reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also
show that the ranking training process improves embedding performance on the
MTEB benchmark. Our findings indicate that a single embedding model can
effectively unify retrieval and reranking, offering both computational
efficiency and competitive ranking accuracy.

</details>


### [460] [MATCH: Task-Driven Code Evaluation through Contrastive Learning](https://arxiv.org/abs/2510.23169)
*Marah Ghoummaid,Vladimir Tchuiev,Ofek Glick,Michal Moschkovitz,Dotan Di Castro*

Main category: cs.CL

TL;DR: 本文介绍了无参考评估指标MATCH，它在多种编程语言中比现有指标与功能正确性和人类偏好有更强相关性。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评估方法存在不可扩展、成本高、无法捕捉代码功能或需要参考代码等问题，需要无参考评估方法。

Method: 引入MATCH，使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现相似度评分。

Result: MATCH在多种编程语言中比现有指标与功能正确性和人类偏好有更强相关性。

Conclusion: MATCH是一种有效的无参考评估指标，能更好地评估生成代码与开发者意图的契合度。

Abstract: AI-based code generation is increasingly prevalent, with GitHub Copilot
estimated to generate 46% of the code on GitHub. Accurately evaluating how well
generated code aligns with developer intent remains a critical challenge.
Traditional evaluation methods, such as unit tests, are often unscalable and
costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code
functionality, and metrics like CodeBERTScore require reference code, which is
not always available. To address the gap in reference-free evaluation, with few
alternatives such as ICE-Score, this paper introduces MATCH, a novel
reference-free metric. MATCH uses Contrastive Learning to generate meaningful
embeddings for code and natural language task descriptions, enabling similarity
scoring that reflects how well generated code implements the task. We show that
MATCH achieves stronger correlations with functional correctness and human
preference than existing metrics across multiple programming languages.

</details>


### [461] [Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts](https://arxiv.org/abs/2510.22956)
*Anwesan Pal,Karen Hovsepian,Tinghao Guo,Mengnan Zhao,Somendra Tripathi,Nikos Kanakaris,George Mihaila,Sumit Nigam*

Main category: cs.CL

TL;DR: 论文提出Tagging - Augmented Generation (TAG)策略提升大语言模型长上下文场景性能，在两个QA基准测试验证有性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代旗舰大语言模型在长且复杂上下文的问答和推理存在重大局限，现有方法有不足。

Method: 提出Tagging - Augmented Generation (TAG)轻量级数据增强策略。

Result: 在NoLima和NovelQA两个问答基准测试中，标记上下文或添加标签定义到QA提示可使性能优于基线，32K token上下文提升达17%，复杂推理问答提升2.9%。

Conclusion: TAG策略能在不破坏和改变检索文档完整性与组成的情况下，提升大语言模型长上下文场景的性能。

Abstract: Recent investigations into effective context lengths of modern flagship large
language models (LLMs) have revealed major limitations in effective question
answering (QA) and reasoning over long and complex contexts for even the
largest and most impressive cadre of models. While approaches like
retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to
mitigate this issue, they are sensitive to chunking, embedding and retrieval
strategies and models, and furthermore, rely on extensive pre-processing,
knowledge acquisition and indexing steps. In this paper, we propose
Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy
that boosts LLM performance in long-context scenarios, without degrading and
altering the integrity and composition of retrieved documents. We validate our
hypothesis by augmenting two challenging and directly relevant
question-answering benchmarks -- NoLima and NovelQA -- and show that tagging
the context or even just adding tag definitions into QA prompts leads to
consistent performance gains over the baseline -- up to 17% for 32K token
contexts, and 2.9% in complex reasoning question-answering for multi-hop
queries requiring knowledge across a wide span of text. Additional details are
available at https://sites.google.com/view/tag-emnlp.

</details>


### [462] [Leveraging Hierarchical Organization for Medical Multi-document Summarization](https://arxiv.org/abs/2510.23104)
*Yi-Li Hsu,Katelyn X. Mei,Lucy Lu Wang*

Main category: cs.CL

TL;DR: 本文研究在医学多文档摘要（MDS）输入中加入层次结构能否提升模型跨文档组织和处理信息的能力，结果表明层次结构可提升摘要清晰度和人类偏好。


<details>
  <summary>Details</summary>
Motivation: 探究在MDS输入中加入层次结构是否比传统扁平摘要方法更能提升模型跨文档组织和处理信息的能力。

Method: 研究在三种大语言模型（LLMs）中加入层次结构的两种方式，并使用自动指标、基于模型的指标和领域专家评估对生成的摘要进行综合评估。还检查GPT - 4的模拟判断是否与人类判断一致。

Result: 人类专家更偏好模型生成的摘要；层次方法能保留信息的事实性、覆盖度和连贯性，增加人类对摘要的偏好；GPT - 4在更客观的评估方面与人类判断有较高一致性。

Conclusion: 层次结构可以在保持内容覆盖度的同时提高模型生成的医学摘要的清晰度，为提高人类对生成摘要的偏好提供了实用方法。

Abstract: Medical multi-document summarization (MDS) is a complex task that requires
effectively managing cross-document relationships. This paper investigates
whether incorporating hierarchical structures in the inputs of MDS can improve
a model's ability to organize and contextualize information across documents
compared to traditional flat summarization methods. We investigate two ways of
incorporating hierarchical organization across three large language models
(LLMs), and conduct comprehensive evaluations of the resulting summaries using
automated metrics, model-based metrics, and domain expert evaluation of
preference, understandability, clarity, complexity, relevance, coverage,
factuality, and coherence. Our results show that human experts prefer
model-generated summaries over human-written summaries. Hierarchical approaches
generally preserve factuality, coverage, and coherence of information, while
also increasing human preference for summaries. Additionally, we examine
whether simulated judgments from GPT-4 align with human judgments, finding
higher agreement along more objective evaluation facets. Our findings
demonstrate that hierarchical structures can improve the clarity of medical
summaries generated by models while maintaining content coverage, providing a
practical way to improve human preference for generated summaries.

</details>


### [463] [LimRank: Less is More for Reasoning-Intensive Information Reranking](https://arxiv.org/abs/2510.23544)
*Tingyu Song,Yilun Zhao,Siyue Zhang,Chen Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出用少量高质量监督数据有效适配大语言模型进行信息重排序，设计了生成重排序示例的管道，微调模型LIMRANK，在两个基准测试中表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模微调适配大语言模型进行信息重排序，计算成本高，需更高效方法。

Method: 设计LIMRANK - SYNTHESIZER管道生成重排序示例，用合成数据微调LIMRANK模型。

Result: LIMRANK在两个基准测试中取得有竞争力的性能，且训练数据少于先前工作的5%。

Conclusion: LIMRANK - SYNTHESIZER有效，LIMRANK在下游任务有强泛化能力。

Abstract: Existing approaches typically rely on large-scale fine-tuning to adapt LLMs
for information reranking tasks, which is computationally expensive. In this
work, we demonstrate that modern LLMs can be effectively adapted using only
minimal, high-quality supervision. To enable this, we design
LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating
diverse, challenging, and realistic reranking examples. Using this synthetic
data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two
challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and
FollowIR for instruction-following retrieval. Our experiments demonstrate that
LIMRANK achieves competitive performance, while being trained on less than 5%
of the data typically used in prior work. Further ablation studies demonstrate
the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization
capabilities of LIMRANK across downstream tasks, including scientific
literature search and retrieval-augmented generation for knowledge-intensive
problem solving.

</details>


### [464] [Language Ranker: A Lightweight Ranking framework for LLM Decoding](https://arxiv.org/abs/2510.21883)
*Chenheng Zhang,Tianqi Du,Jizhe Zhang,Mingqing Xiao,Yifei Wang,Yisen Wang,Zhouchen Lin*

Main category: cs.CL

TL;DR: 本文从推荐系统视角重新审视大语言模型生成，提出轻量级框架Language Ranker，实验显示其能以低计算开销达到与大规模奖励模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型研究多关注输出分布，对解码过程关注不足，现有解码方法存在计算成本高和适用性有限的问题。

Method: 将解码过程类比为推荐流程中的排序阶段，提出Language Ranker框架，用基础模型提取的特征对候选响应进行重排序。

Result: Language Ranker在多项任务中达到与大规模奖励模型相当的性能，仅需<0.5M额外参数，显著降低训练和推理阶段的计算开销。

Conclusion: Language Ranker方法高效有效，有充分释放大语言模型能力的潜力。

Abstract: Conventional research on large language models (LLMs) has primarily focused
on refining output distributions, while paying less attention to the decoding
process that transforms these distributions into final responses. Recent
advances, such as scaling the computation of inference time with reward models,
have underscored the importance of decoding, but these methods often suffer
from high computational costs and limited applicability. In this paper, we
revisit LLM generation through the lens of recommender systems, conceptualizing
the decoding process as analogous to the ranking stage in recommendation
pipelines. From this perspective, we observe that both traditional decoding
methods and reward models exhibit clear limitations such as redundancy.
Motivated by this insight, we propose Language Ranker, a novel framework that
introduces a lightweight module to rerank candidate responses using features
extracted by the base model. Experiments across a wide range of tasks show that
Language Ranker achieves performance comparable to large-scale reward models,
while requiring only <0.5M additional parameters, significantly reducing the
computational overhead during both training and inference stages. This
highlights the efficiency and effectiveness of our method, showcasing its
potential to fully unlock the capabilities of LLMs.

</details>


### [465] [Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks](https://arxiv.org/abs/2510.21884)
*Avinash Patil*

Main category: cs.CL

TL;DR: 本文提出RACE框架评估大语言模型生成解释与逻辑回归特征重要性得分的一致性，分析四个文本分类数据集，发现正确和错误预测对应特征覆盖情况不同，该框架为评估推理完整性提供新见解和量化基础。


<details>
  <summary>Details</summary>
Motivation: 机器学习在敏感领域应用增加，对透明可解释AI需求提升，但大语言模型生成的解释是否忠实反映决策依据尚不明确。

Method: 引入RACE框架，分析四个文本分类数据集，比较大语言模型解释与支持和矛盾词汇特征，采用令牌感知、精确字符串和编辑距离匹配技术。

Result: 正确预测对支持特征覆盖更高，错误预测对矛盾特征覆盖更高，编辑距离匹配提升覆盖且保持不对称性。

Conclusion: 大语言模型解释结合了表面和灵活证据重用，但在错误情况下会放大误导线索，RACE为评估神经语言模型推理完整性提供新见解和量化基础。

Abstract: The growing adoption of machine learning (ML) in sensitive domains has
heightened the demand for transparent and interpretable artificial
intelligence. Large Language Models (LLMs) are increasingly capable of
producing natural language explanations, yet it remains unclear whether these
rationales faithfully capture the predictive signals that underlie decisions.
This paper introduces RACE-Reasoning Alignment for Completeness of
Explanations, a systematic framework to evaluate the alignment between
LLM-generated explanations and interpretable feature importance scores derived
from a logistic regression baseline. We analyze four widely used text
classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and
compare LLM rationales against top-ranked supporting and contradicting lexical
features. To capture alignment at multiple levels of granularity, RACE
implements token-aware, exact string, and edit-distance matching techniques.
Empirical results reveal a consistent asymmetry: correct predictions exhibit
higher coverage of supporting features, while incorrect predictions are
associated with elevated coverage of contradicting features. Edit-distance
matching further uncovers paraphrastic overlaps, boosting coverage while
preserving this asymmetry. These findings demonstrate that LLM rationales
combine both surface-level and flexible evidence reuse, yet can also amplify
misleading cues in error cases. RACE provides new insights into the
faithfulness of LLM explanations and establishes a quantitative basis for
evaluating reasoning completeness in neural language models.

</details>


### [466] [Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning](https://arxiv.org/abs/2510.21885)
*Anh Pham,Mihir Thalanki,Michael Sun,Aditya Chaloo,Ankita Gupta,Tian Xia,Aditya Mate,Ehimwenma Nosakhare,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 提出行为感知采样框架选择安全示例，减少有害输出，提高微调安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调时会出现灾难性遗忘，此前虽知添加随机安全示例可缓解，但不清楚哪些示例最有效。

Method: 提出基于指令 - 响应行为和语义多样性的行为感知采样框架选择安全示例。

Result: 该方法大幅减少有害输出，仅用 0.5% 额外训练数据使有害性最多降低 41%，且保持有用性。

Conclusion: 有针对性的数据选择可提高大规模微调的安全性和效率。

Abstract: Large language models often lose previously aligned safety behaviors when
fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior
work shows that adding random safety examples can mitigate this effect, but it
remains unclear which examples are most effective. We propose a behavior-aware
sampling framework that selects safety examples based on two complementary
factors: instruction-response behavior (e.g., refusal versus compliance) and
semantic diversity across harm categories. Systematic evaluation shows that
this approach substantially reduces harmful outputs while maintaining
helpfulness, achieving up to a 41% reduction in harmfulness with only 0.5%
additional training data. These results highlight how targeted data selection
can improve the safety and efficiency of fine-tuning at scale.

</details>


### [467] [Understanding Network Behaviors through Natural Language Question-Answering](https://arxiv.org/abs/2510.21894)
*Mingzhe Xing,Chang Tian,Jianan Zhang,Lichen Pan,Peipei Liu,Zhaoteng Yan,Yinliang Yue*

Main category: cs.CL

TL;DR: 提出NetMind框架，解决自然语言理解网络行为的挑战，实验显示性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有挖掘网络配置理解网络行为的方法学习曲线陡、灵活性有限，自然语言接口虽好但面临新挑战，需新方法。

Method: 引入基于树的配置分块策略、构建统一事实图作为中间表示、设计混合命令式 - 声明式语言，还贡献了基准数据集。

Result: NetMind能准确且可扩展地理解网络行为，性能优于现有基线。

Conclusion: NetMind有效解决了自然语言理解网络行为的挑战，具有良好性能。

Abstract: Modern large-scale networks introduce significant complexity in understanding
network behaviors, increasing the risk of misconfiguration. Prior work proposed
to understand network behaviors by mining network configurations, typically
relying on domain-specific languages interfaced with formal models. While
effective, they suffer from a steep learning curve and limited flexibility. In
contrast, natural language (NL) offers a more accessible and interpretable
interface, motivating recent research on NL-guided network behavior
understanding. Recent advances in large language models (LLMs) further enhance
this direction, leveraging their extensive prior knowledge of network concepts
and strong reasoning capabilities. However, three key challenges remain: 1)
numerous router devices with lengthy configuration files challenge LLM's
long-context understanding ability; 2) heterogeneity across devices and
protocols impedes scalability; and 3) complex network topologies and protocols
demand advanced reasoning abilities beyond the current capabilities of LLMs. To
tackle the above challenges, we propose NetMind, a novel framework for querying
networks using NL. Our approach introduces a tree-based configuration chunking
strategy to preserve semantic coherence while enabling efficient partitioning.
We then construct a unified fact graph as an intermediate representation to
normalize vendor-specific configurations. Finally, we design a hybrid
imperative-declarative language to reduce the reasoning burden on LLMs and
enhance precision. We contribute a benchmark consisting of NL question-answer
pairs paired with network configurations. Experiments demonstrate that NetMind
achieves accurate and scalable network behavior understanding, outperforming
existing baselines.

</details>


### [468] [Deep Literature Survey Automation with an Iterative Workflow](https://arxiv.org/abs/2510.21900)
*Hongbo Zhang,Han Cui,Yidong Wang,Yijian Tian,Qi Guo,Cunxiang Wang,Jian Wu,Chiyu Song,Yue Zhang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automatic literature survey generation has attracted increasing attention,
yet most existing systems follow a one-shot paradigm, where a large set of
papers is retrieved at once and a static outline is generated before drafting.
This design often leads to noisy retrieval, fragmented structures, and context
overload, ultimately limiting survey quality. Inspired by the iterative reading
process of human researchers, we propose \ours, a framework based on recurrent
outline generation, in which a planning agent incrementally retrieves, reads,
and updates the outline to ensure both exploration and coherence. To provide
faithful paper-level grounding, we design paper cards that distill each paper
into its contributions, methods, and findings, and introduce a
review-and-refine loop with visualization enhancement to improve textual flow
and integrate multimodal elements such as figures and tables. Experiments on
both established and emerging topics show that \ours\ substantially outperforms
state-of-the-art baselines in content coverage, structural coherence, and
citation quality, while producing more accessible and better-organized surveys.
To provide a more reliable assessment of such improvements, we further
introduce Survey-Arena, a pairwise benchmark that complements absolute scoring
and more clearly positions machine-generated surveys relative to human-written
ones. The code is available at
https://github.com/HancCui/IterSurvey\_Autosurveyv2.

</details>


### [469] [Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks](https://arxiv.org/abs/2510.21983)
*Havva Alizadeh Noughabi,Julien Serbanescu,Fattane Zarrinkalam,Ali Dehghantanha*

Main category: cs.CL

TL;DR: 本文结合社会科学说服理论构建对抗性提示，研究大语言模型越狱攻击，发现说服性提示能绕过防护，强调跨学科研究对模型安全的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注影响大语言模型越狱攻击易感性的语言和心理机制，需新研究视角。

Method: 借鉴社会科学说服理论构建对抗性提示，研究大语言模型对有说服结构提示的反应，探究模型越狱回复中的说服特征。

Result: 跨多个对齐大语言模型的实证评估显示，具有说服意识的提示能显著绕过防护，引发越狱行为。

Conclusion: 跨学科见解对解决大语言模型安全的不断演变挑战至关重要。

Abstract: Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model's
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.

</details>


### [470] [Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models](https://arxiv.org/abs/2510.22014)
*Sarah Ball,Niki Hasrati,Alexander Robey,Avi Schwarzschild,Frauke Kreuter,Zico Kolter,Andrej Risteski*

Main category: cs.CL

TL;DR: 本文研究大语言模型越狱攻击后缀的可迁移性，找出与迁移成功强相关的统计特性，还指出提示语义相似性与迁移成功弱相关，并用于实验改进攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大语言模型越狱攻击后缀可迁移性何时及为何发生的严格分析，本文旨在填补这一空白。

Method: 识别在众多实验设置中与迁移成功强相关的三个统计特性，进行干预实验。

Result: 找到三个与迁移成功强相关的统计特性，发现提示语义相似性与迁移成功弱相关。

Conclusion: 研究结果有助于更细致地理解可迁移性，并能将统计分析转化为攻击成功率的实际提升。

Abstract: Discrete optimization-based jailbreaking attacks on large language models aim
to generate short, nonsensical suffixes that, when appended onto input prompts,
elicit disallowed content. Notably, these suffixes are often transferable --
succeeding on prompts and models for which they were never optimized. And yet,
despite the fact that transferability is surprising and empirically
well-established, the field lacks a rigorous analysis of when and why transfer
occurs. To fill this gap, we identify three statistical properties that
strongly correlate with transfer success across numerous experimental settings:
(1) how much a prompt without a suffix activates a model's internal refusal
direction, (2) how strongly a suffix induces a push away from this direction,
and (3) how large these shifts are in directions orthogonal to refusal. On the
other hand, we find that prompt semantic similarity only weakly correlates with
transfer success. These findings lead to a more fine-grained understanding of
transferability, which we use in interventional experiments to showcase how our
statistical analysis can translate into practical improvements in attack
success.

</details>


### [471] [Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models](https://arxiv.org/abs/2510.22042)
*Benjamin Reichman,Adar Avsian,Larry Heck*

Main category: cs.CL

TL;DR: 研究大语言模型内部情感表征，发现低维情感流形及通用情感子空间，可操纵情感感知。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型内部如何表征情感。

Method: 分析大语言模型隐藏状态空间的几何结构。

Result: 识别出低维情感流形，情感表征有特定编码方式，结构稳定且跨数据集通用，跨领域对齐效果好，能在保留语义时操纵情感感知。

Conclusion: 大语言模型存在一致且可操纵的情感几何结构，有助于理解其内化和处理情感的方式。

Abstract: This work investigates how large language models (LLMs) internally represent
emotion by analyzing the geometry of their hidden-state space. The paper
identifies a low-dimensional emotional manifold and shows that emotional
representations are directionally encoded, distributed across layers, and
aligned with interpretable dimensions. These structures are stable across depth
and generalize to eight real-world emotion datasets spanning five languages.
Cross-domain alignment yields low error and strong linear probe performance,
indicating a universal emotional subspace. Within this space, internal emotion
perception can be steered while preserving semantics using a learned
intervention module, with especially strong control for basic emotions across
languages. These findings reveal a consistent and manipulable affective
geometry in LLMs and offer insight into how they internalize and process
emotion.

</details>


### [472] [Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows](https://arxiv.org/abs/2510.22109)
*Billy Dickson,Zoran Tiganj*

Main category: cs.CL

TL;DR: 本文提出对输入表征进行对数压缩的方法处理长上下文，在基准测试中降低困惑度，证明该方法简单有效。


<details>
  <summary>Details</summary>
Motivation: 多数长上下文处理方法增加了Transformer内部架构复杂度，本文寻找替代方法。

Method: 受人类记忆认知模型启发，对输入令牌应用尺度不变的对数压缩，用标准Transformer处理压缩后的表征。

Result: 在WikiText - 103和PG - 19语言建模基准测试中，相比未压缩基线降低了困惑度，且压缩时间上下文越长性能越好。

Conclusion: 输入级对数压缩是扩展Transformer长程记忆的简单有效方法。

Abstract: Most approaches to long-context processing increase the complexity of the
transformer's internal architecture by integrating mechanisms such as
recurrence or auxiliary memory modules. In this work, we introduce an
alternative approach that modifies the input representation itself, rather than
the transformer architecture. Inspired by cognitive models of human memory, our
method applies a scale-invariant logarithmic compression to the input tokens.
The resulting compressed representation is processed by a standard, unmodified
transformer, preserving architectural simplicity. We evaluate this approach on
the WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in
perplexity compared to uncompressed baselines. Moreover, performance improves
consistently with longer compressed temporal contexts, showing that input-level
logarithmic compression is a simple and effective way to extend a transformer's
long-range memory.

</details>


### [473] [Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation](https://arxiv.org/abs/2510.22115)
*Ling-Team,Ang Li,Ben Liu,Binbin Hu,Bing Li,Bingwei Zeng,Borui Ye,Caizhi Tang,Changxin Tian,Chao Huang,Chao Zhang,Chen Qian,Chenchen Ju,Chenchen Li,Chengfu Tang,Chili Fu,Chunshao Ren,Chunwei Wu,Cong Zhang,Cunyin Peng,Dafeng Xu,Daixin Wang,Dalong Zhang,Dingnan Jin,Dingyuan Zhu,Dongke Hu,Fangzheng Zhao,Feifan Wu,Feng Zhu,Gangshan Wang,Haitao Zhang,Hailin Zhao,Hanxiao Zhang,Hanzi Wang,Hao Qian,Haoyi Yu,Heng Zhang,Hongliang Zhang,Hongzhi Luan,Huirong Dong,Huizhong Li,Jia Li,Jia Liu,Jialong Zhu,Jian Sha,Jianping Wei,Jiaolong Yang,Jieyue Ma,Jiewei Wu,Jinjing Huang,Jingyun Tian,Jingyuan Zhang,Jinquan Sun,Juanhui Tu,Jun Liu,Jun Xu,Jun Zhou,Junjie Ou,Junpeng Fang,Kaihong Zhang,Kaiqin Hu,Ke Shi,Kun Tang,Kunlong Chen,Lanyin Mei,Lei Liang,Lei Xu,Libo Zhang,Lin Ju,Lin Yuan,Ling Zhong,Lintao Ma,Lu Liu,Lu Yu,Lun Cai,Meiqi Zhu,Mengying Li,Min Chen,Minghao Xue,Minghong Cai,Mingming Yin,Peijie Jiang,Peilong Zhao,Pingping Liu,Qian Zhao,Qing Cui,Qingxiang Huang,Qingyuan Yang,Quankun Yu,Shaowei Wei,Shijie Lian,Shoujian Zheng,Shun Song,Shungen Zhang,Shuo Zhang,Siyuan Li,Song Liu,Ting Guo,Tong Zhao,Wanli Gu,Weichang Wu,Weiguang Han,Wenjing Fang,Wubin Wang,Xiang Shu,Xiao Shi,Xiaoshun Lan,Xiaolu Zhang,Xiaqing Sun,Xin Zhao,Xingyu Lu,Xiong Xu,Xudong Wang,Xudong Wang,Xuemin Yang,Yajie Yang,Yang Xiang,Yanzhe Li,Yi Zhang,Yilong Wang,Yingxue Li,Yongzhen Guo,Yuzhuo Fu,Yuanyuan Wang,Yue Yang,Yue Yu,Yufeng Deng,Yun Zhang,Yunfei Xu,Yuqi Zhang,Yuxiao He,Zengke Gui,Zhaoxin Huan,Zhaoyang Wang,Zhibo Zhu,Zhihao Wang,Zhiqiang Zhang,Zhoufei Wang,Zihang Zeng,Ziqi Liu,Zitao Xuan,Zuoli Tang*

Main category: cs.CL

TL;DR: 介绍推理导向语言基础模型Ling 2.0，含不同规模模型，集成多方面创新，万亿规模的Ling-1T表现出色，为未来模型提供基础。


<details>
  <summary>Details</summary>
Motivation: 构建一个能提升推理能力、具有高稀疏性和跨尺度一致性、兼顾效率的语言基础模型。

Method: 采用统一的混合专家（MoE）范式，在模型架构、预训练、后训练和基础设施等方面进行协调创新，如高稀疏MoE与MTP、推理导向数据和训练、基于强化的微调、全规模FP8训练等。

Result: 包含从16B到1T参数的三个模型，与密集模型相比，主动计算效率最高达7倍；万亿规模的Ling-1T在推理准确性和计算效率上达到新的帕累托前沿。

Conclusion: Ling 2.0为未来推理和思维模型提供了连贯、开放且高效的基础。

Abstract: We introduce Ling 2.0, a series reasoning-oriented language foundation built
upon the principle that every activation boosts reasoning capability. Designed
to scale from tens of billions to one trillion parameters under a unified
Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,
cross-scale consistency, and efficiency guided by empirical scaling laws. The
series includes three non-thinking (instruct) models - Ling-mini-2.0,
Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and
achieving up to 7-fold active-compute efficiency compared with dense
counterparts. Ling 2.0 integrates coordinated innovations across model
architecture, pre-training, post-training, and infrastructure: a high-sparsity
MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training
CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale
FP8 training with fine-grained heterogeneous pipelines. At the trillion scale,
Ling-1T establishes a new Pareto frontier of reasoning accuracy versus
computational efficiency, demonstrating that sparse activation, when properly
aligned with reasoning objectives, enables scalable and efficient intelligence.
Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for
advancing future reasoning and thinking models, including the Ring series built
upon the same base.

</details>


### [474] [Estimating the Error of Large Language Models at Pairwise Text Comparison](https://arxiv.org/abs/2510.22219)
*Tianyi Li*

Main category: cs.CL

TL;DR: 该文测量大语言模型（LLMs）在成对文本比较中的输出错误，提出不依赖真实标签的方法，应用于六个LLMs并获一致结果，Claude表现最佳，模型优于其他对比方法。


<details>
  <summary>Details</summary>
Motivation: 测量LLMs在成对文本比较中的输出错误概率，评估其性能。

Method: 提出不依赖真实标签的方法，包括统一错误率和二元位置偏差两种情景估计，用Copeland计数构建排名辅助估计错误率。

Result: 应用于六个LLMs得到一致的错误估计，两个位置偏差项相似且接近统一错误，Claude表现最佳。

Conclusion: 所提模型在指示LLMs此项任务错误方面优于有偏Bradley - Terry模型和交换性得分。

Abstract: We measure LLMs' output error at pairwise text comparison, noting the
probability of error in their preferences. Our method does not rely on the
ground truth and supports two scenarios: (i) uniform error rate regardless of
the order of comparison, estimated with two comparisons for each text pair with
either text placed first; (ii) binary positional bias assuming distinct error
rates for the two orders of comparison, estimated with repeated comparisons
between the texts. The Copeland counting constructs a ranking over the compared
texts from pairwise preferences; the ranking reveals the poor scalability of
LLM-based pairwise comparison and helps yield the estimates for LLMs' error
rates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini,
Grok, Qwen) with five types of text input and obtain consistent estimates of
LLMs' error. In general, the measured two positional bias terms are similar,
close to the uniform error. Considering both the error rates and the robustness
to the variation of prompts, Claude obtained the most desirable performance in
this experiment. Our model outperforms the biased Bradley-Terry model and the
commutativity score in indicating LLMs' error at this task.

</details>


### [475] [You Don't Need Prompt Engineering Anymore: The Prompting Inversion](https://arxiv.org/abs/2510.22251)
*Imran Khan*

Main category: cs.CL

TL;DR: 介绍Sculpting提示方法，评估三种提示策略在不同OpenAI模型上表现，发现提示反转现象，建议提示策略随模型能力共同进化。


<details>
  <summary>Details</summary>
Motivation: 改进标准CoT提示方法，减少语义歧义与常识错误带来的推理误差。

Method: 在GSM8K数学推理基准上评估零样本、标准CoT和Sculpting三种提示策略在三代OpenAI模型上的表现。

Result: Sculpting在gpt - 4o上有优势，但在gpt - 5上效果不如标准CoT，出现提示反转现象。

Conclusion: 最优提示策略需随模型能力共同进化，更强大的模型适合更简单的提示。

Abstract: Prompt engineering, particularly Chain-of-Thought (CoT) prompting,
significantly enhances LLM reasoning capabilities. We introduce "Sculpting," a
constrained, rule-based prompting method designed to improve upon standard CoT
by reducing errors from semantic ambiguity and flawed common sense.
  We evaluate three prompting strategies (Zero Shot, standard CoT, and
Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5)
using the GSM8K mathematical reasoning benchmark (1,317 problems).
  Our findings reveal a "Prompting Inversion": Sculpting provides advantages on
gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00%
vs. 96.36% for CoT on full benchmark). We trace this to a
"Guardrail-to-Handcuff" transition where constraints preventing common-sense
errors in mid-tier models induce hyper-literalism in advanced models. Our
detailed error analysis demonstrates that optimal prompting strategies must
co-evolve with model capabilities, suggesting simpler prompts for more capable
models.

</details>


### [476] [Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER](https://arxiv.org/abs/2510.22285)
*Andrei Baroian*

Main category: cs.CL

TL;DR: 研究CADEC语料库上的临床命名实体识别，对比三类方法，分析各模型表现。


<details>
  <summary>Details</summary>
Motivation: 研究临床命名实体识别，对比不同方法在CADEC语料库上的效果。

Method: 对比三类方法：BERT式编码器、GPT - 4o的少样本上下文学习和监督微调，用标准NER指标评估。

Result: RoBERTa - large和BioClinicalBERT较BERT Base提升有限；简单ICL优于复杂提示，SFT整体性能最强但成本高；LLM在简化任务上准确率更高。

Conclusion: 不同方法在临床命名实体识别上有不同表现，SFT性能强但成本高，LLM在简化任务表现更好。

Abstract: We study clinical Named Entity Recognition (NER) on the CADEC corpus and
compare three families of approaches: (i) BERT-style encoders (BERT Base,
BioClinicalBERT, RoBERTa-large), (ii) GPT-4o used with few-shot in-context
learning (ICL) under simple vs.\ complex prompts, and (iii) GPT-4o with
supervised fine-tuning (SFT). All models are evaluated on standard NER metrics
over CADEC's five entity types (ADR, Drug, Disease, Symptom, Finding).
RoBERTa-large and BioClinicalBERT offer limited improvements over BERT Base,
showing the limit of these family of models. Among LLM settings, simple ICL
outperforms a longer, instruction-heavy prompt, and SFT achieves the strongest
overall performance (F1 $\approx$ 87.1%), albeit with higher cost. We find that
the LLM achieve higher accuracy on simplified tasks, restricting classification
to two labels.

</details>


### [477] [Multilingual Target-Stance Extraction](https://arxiv.org/abs/2510.22334)
*Ethan Mines,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本文引入首个多语言目标立场提取（TSE）基准，拓展TSE流程至多语言场景，模型F1得分一般，凸显任务难度及目标预测瓶颈，还展示F1得分对目标表述的敏感性。


<details>
  <summary>Details</summary>
Motivation: 此前TSE工作仅针对英语，需多语言TSE基准。

Method: 引入涵盖多种语言语料的多语言TSE基准，拓展原TSE流程至多语言场景，无需为每种语言单独建模。

Result: 模型F1得分12.78，表明多语言任务比仅英语任务难度增加，目标预测是主要瓶颈，且首次展示TSE的F1得分对不同目标表述的敏感性。

Conclusion: 为多语言TSE的资源、算法和评估标准提供了急需的基线。

Abstract: Social media enables data-driven analysis of public opinion on contested
issues. Target-Stance Extraction (TSE) is the task of identifying the target
discussed in a document and the document's stance towards that target. Many
works classify stance towards a given target in a multilingual setting, but all
prior work in TSE is English-only. This work introduces the first multilingual
TSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and
Spanish corpora. It manages to extend the original TSE pipeline to a
multilingual setting without requiring separate models for each language. Our
model pipeline achieves a modest F1 score of 12.78, underscoring the increased
difficulty of the multilingual task relative to English-only setups and
highlighting target prediction as the primary bottleneck. We are also the first
to demonstrate the sensitivity of TSE's F1 score to different target
verbalizations. Together these serve as a much-needed baseline for resources,
algorithms, and evaluation criteria in multilingual TSE.

</details>


### [478] [VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations](https://arxiv.org/abs/2510.22373)
*Yupeng Xie,Zhiyang Zhang,Yifan Wu,Sirong Lu,Jiayi Zhang,Zhaoyang Yu,Jinlin Wang,Sirui Hong,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.CL

TL;DR: 提出首个评估多模态大语言模型可视化评估能力的基准VisJudge - Bench，发现先进MLLMs与人类专家有差距，又提出VisJudge模型缩小了此差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型缺乏评估可视化美学和质量能力的系统基准，难以评估其在可视化评估方面的表现。

Method: 提出VisJudge - Bench基准进行系统测试，再提出VisJudge模型进行可视化美学和质量评估。

Result: 先进MLLMs（如GPT - 5）与人类专家判断有显著差距，MAE为0.551，相关性仅0.429；VisJudge模型显著缩小与人类判断差距，MAE降至0.442，与人类一致性提升至0.681。

Conclusion: VisJudge - Bench可用于评估MLLMs可视化评估能力，VisJudge模型能有效提升可视化美学和质量评估效果。

Abstract: Visualization, a domain-specific yet widely used form of imagery, is an
effective way to turn complex datasets into intuitive insights, and its value
depends on whether data are faithfully represented, clearly communicated, and
aesthetically designed. However, evaluating visualization quality is
challenging: unlike natural images, it requires simultaneous judgment across
data encoding accuracy, information expressiveness, and visual aesthetics.
Although multimodal large language models (MLLMs) have shown promising
performance in aesthetic assessment of natural images, no systematic benchmark
exists for measuring their capabilities in evaluating visualizations. To
address this, we propose VisJudge-Bench, the first comprehensive benchmark for
evaluating MLLMs' performance in assessing visualization aesthetics and
quality. It contains 3,090 expert-annotated samples from real-world scenarios,
covering single visualizations, multiple visualizations, and dashboards across
32 chart types. Systematic testing on this benchmark reveals that even the most
advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human
experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a
correlation with human ratings of only 0.429. To address this issue, we propose
VisJudge, a model specifically designed for visualization aesthetics and
quality assessment. Experimental results demonstrate that VisJudge
significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a
19.8% reduction) and increasing the consistency with human experts to 0.681 (a
58.7% improvement) compared to GPT-5. The benchmark is available at
https://github.com/HKUSTDial/VisJudgeBench.

</details>


### [479] [CHOIR: Collaborative Harmonization fOr Inference Robustness](https://arxiv.org/abs/2510.22475)
*Xiangjue Dong,Cong Wang,Maria Teleki,Millennium Bismay,James Caverlee*

Main category: cs.CL

TL;DR: 本文提出CHOIR框架，利用角色变化提高大语言模型推理鲁棒性，实验显示其在多方面提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有角色分配的大语言模型中，角色的微小人口统计扰动会改变推理轨迹，作者想将这种变化作为建设性资源提升推理鲁棒性。

Method: 提出CHOIR框架，在测试时将多个角色条件下的推理信号协调成统一预测，在反事实角色间进行协作解码。

Result: 在各种推理基准测试中，CHOIR在不同人口统计、模型架构、规模和任务上均提升性能，部分群体提升达26.4%，平均提升19.2%，基础角色欠佳时仍有效。

Conclusion: 将角色变化重新定义为建设性信号，CHOIR为更可靠的大语言模型推理提供了可扩展和通用的方法。

Abstract: Persona-assigned Large Language Models (LLMs) can adopt diverse roles,
enabling personalized and context-aware reasoning. However, even minor
demographic perturbations in personas, such as simple pronoun changes, can
alter reasoning trajectories, leading to divergent sets of correct answers.
Instead of treating these variations as biases to be mitigated, we explore
their potential as a constructive resource to improve reasoning robustness. We
propose CHOIR (Collaborative Harmonization fOr Inference Robustness), a
test-time framework that harmonizes multiple persona-conditioned reasoning
signals into a unified prediction. CHOIR orchestrates a collaborative decoding
process among counterfactual personas, dynamically balancing agreement and
divergence in their reasoning paths. Experiments on various reasoning
benchmarks demonstrate that CHOIR consistently enhances performance across
demographics, model architectures, scales, and tasks - without additional
training. Improvements reach up to 26.4% for individual demographic groups and
19.2% on average across five demographics. It remains effective even when base
personas are suboptimal. By reframing persona variation as a constructive
signal, CHOIR provides a scalable and generalizable approach to more reliable
LLM reasoning.

</details>


### [480] [Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection](https://arxiv.org/abs/2510.22531)
*Noshitha Padma Pratyusha Juttu,Sahithi Singireddy,Sravani Gona,Sujal Timilsina*

Main category: cs.CL

TL;DR: 研究对大语言模型在服务条款不公平条款检测中的微调、参数高效适应和零样本提示策略进行评估，发现全微调平衡最佳，LoRA模型内存成本低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在法律领域的应用受全微调成本限制，需要评估不同策略以实现高效领域适应。

Method: 对BERT和DistilBERT进行微调，对TinyLlama等模型应用4位低秩适应（LoRA），在零样本设置下评估GPT - 4o等。

Result: 全微调在精确率和召回率上平衡最佳，基于LoRA的模型召回率有竞争力且内存成本最多低3倍。

Conclusion: 研究指出高效且适应领域的大语言模型的设计权衡，为法律文本处理微调研究提供开放基线。

Abstract: Large Language Models (LLMs) have transformed text understanding, yet their
adaptation to specialized legal domains remains constrained by the cost of full
fine-tuning. This study provides a systematic evaluation of fine tuning,
parameter efficient adaptation (LoRA, QLoRA), and zero-shot prompting
strategies for unfair clause detection in Terms of Service (ToS) documents, a
key application in legal NLP. We finetune BERT and DistilBERT, apply 4-bit
Low-Rank Adaptation (LoRA) to models such as TinyLlama, LLaMA 3B/7B, and
SaulLM, and evaluate GPT-4o and O-versions in zero-shot settings. Experiments
on the CLAUDETTE-ToS benchmark and the Multilingual Scraper Corpus show that
full fine-tuning achieves the strongest precision recall balance, while
LoRA-based models provide competitive recall with up to 3x lower memory cost.
These findings highlight practical design trade-offs for efficient and
domain-adapted LLMs, contributing open baselines for fine-tuning research in
legal text processing.

</details>


### [481] [ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality](https://arxiv.org/abs/2510.22037)
*Shayne Longpre,Sneha Kudugunta,Niklas Muennighoff,I-Hung Hsu,Isaac Caswell,Alex Pentland,Sercan Arik,Chen-Yu Lee,Sayna Ebrahimi*

Main category: cs.CL

TL;DR: 开展最大规模多语言缩放定律研究，引入ATLAS定律，分析实验揭示多语言学习动态等，为跨语言缩放定律奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律研究多集中于英语，而主流AI模型服务全球数十亿用户，需开展多语言缩放定律研究。

Method: 进行774个多语言训练实验，涵盖10M - 8B模型参数、400多种训练语言和48种评估语言，引入ATLAS定律。

Result: ATLAS定律在样本外泛化性能上优于现有定律；得出跨语言转移矩阵、语言无关缩放定律，确定从头预训练和微调的计算交叉点。

Conclusion: 研究为跨语言缩放定律提供科学基础，使从业者能高效扩展非英语优先的AI模型。

Abstract: Scaling laws research has focused overwhelmingly on English -- yet the most
prominent AI models explicitly serve billions of international users. In this
work, we undertake the largest multilingual scaling laws study to date,
totaling 774 multilingual training experiments, spanning 10M-8B model
parameters, 400+ training languages and 48 evaluation languages. We introduce
the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual
pretraining, which outperforms existing scaling laws' out-of-sample
generalization often by more than 0.3 R^2. Our analyses of the experiments shed
light on multilingual learning dynamics, transfer properties between languages,
and the curse of multilinguality. First, we derive a cross-lingual transfer
matrix, empirically measuring mutual benefit scores between 38 x 38=1444
language pairs. Second, we derive a language-agnostic scaling law that reveals
how to optimally scale model size and data when adding languages without
sacrificing performance. Third, we identify the computational crossover points
for when to pretrain from scratch versus finetune from multilingual
checkpoints. We hope these findings provide the scientific foundation for
democratizing scaling laws across languages, and enable practitioners to
efficiently scale models -- beyond English-first AI.

</details>


### [482] [LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?](https://arxiv.org/abs/2510.22548)
*Ziyuan He,Yuxuan Wang,Jiaqi Li,Kexin Liang,Muhan Zhang*

Main category: cs.CL

TL;DR: 本文介绍新基准LooGLE v2评估大语言模型长上下文能力，评估显示模型处理长依赖任务有局限，提升空间大。


<details>
  <summary>Details</summary>
Motivation: 大语言模型长上下文理解能力在长依赖任务中受限且研究不足，现实应用缺乏相关基准。

Method: 引入LooGLE v2基准，包含真实长文本，设计10种特定领域长依赖任务，生成1934个问答实例，对6个本地部署和4个基于API的大语言模型进行评估。

Result: 最佳模型在基准上总分仅59.2%，流行大语言模型理解的上下文长度远低于宣称。

Conclusion: 大语言模型处理长依赖现实任务能力有限，在实际长上下文理解方面有很大提升空间。

Abstract: Large language models (LLMs) are equipped with increasingly extended context
windows recently, yet their long context understanding capabilities over long
dependency tasks remain fundamentally limited and underexplored. This gap is
especially significant in many real-world long-context applications that were
rarely benchmarked. In this paper, we introduce LooGLE v2, a novel benchmark
designed to evaluate LLMs' long context ability in real-world applications and
scenarios. Our benchmark consists of automatically collected real-world long
texts, ranging from 16k to 2M tokens, encompassing domains in law, finance,
game and code. Accordingly, we delicately design 10 types of domain-specific
long-dependency tasks and generate 1,934 QA instances with various diversity
and complexity in a scalable data curation pipeline for further practical
needs. We conduct a comprehensive assessment of 6 locally deployed and 4
API-based LLMs. The evaluation results show that even the best-performing model
achieves only a 59.2% overall score on our benchmark. Despite the extensive
context windows, popular LLMs are only capable of understanding a much shorter
length of context than they claim to be, revealing significant limitations in
their ability to handle real-world tasks with long dependencies and
highlighting substantial room for model improvement in practical long-context
understanding.

</details>


### [483] [AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment](https://arxiv.org/abs/2510.22593)
*Dario Loi,Elena Maria Muià,Federico Siciliano,Giovanni Trappolini,Vincenzo Crisà,Peter Kruger,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: 介绍AutoBench框架，通过互作同行评估评估大语言模型，实验验证其有效性，是静态基准测试的可扩展、抗污染替代方案。


<details>
  <summary>Details</summary>
Motivation: 静态基准测试存在测试集污染和适应性有限的问题，需要新的评估方法。

Method: AutoBench动态生成评估任务，模型在不同领域交替担任问题生成者、参赛者和评委，通过迭代加权机制聚合同行判断。

Result: 与MMLU - Pro和GPQA有强相关性，多评委设计显著优于单评委基线。

Conclusion: AutoBench是对不断发展的语言模型进行持续评估的可扩展、抗污染的替代方案。

Abstract: We present AutoBench, a fully automated and self-sustaining framework for
evaluating Large Language Models (LLMs) through reciprocal peer assessment.
This paper provides a rigorous scientific validation of the AutoBench
methodology, originally developed as an open-source project by eZecute S.R.L..
Unlike static benchmarks that suffer from test-set contamination and limited
adaptability, AutoBench dynamically generates novel evaluation tasks while
models alternately serve as question generators, contestants, and judges across
diverse domains. An iterative weighting mechanism amplifies the influence of
consistently reliable evaluators, aggregating peer judgments into
consensus-based rankings that reflect collective model agreement. Our
experiments demonstrate strong correlations with established benchmarks
including MMLU-Pro and GPQA (respectively 78\% and 63\%), validating this
peer-driven evaluation paradigm. The multi-judge design significantly
outperforms single-judge baselines, confirming that distributed evaluation
produces more robust and human-consistent assessments. AutoBench offers a
scalable, contamination-resistant alternative to static benchmarks for the
continuous evaluation of evolving language models.

</details>


### [484] [Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance](https://arxiv.org/abs/2510.22602)
*Mahyar Abbasian,Ramesh Jain*

Main category: cs.CL

TL;DR: 提出个人护理实用系统（PCU）用于终身健康指导，介绍其能力、特点及优势，还提及架构等方面。


<details>
  <summary>Details</summary>
Motivation: 基于数字基础设施和生物医学创新，构建一个能为个人和群体提供终身健康指导的系统。

Method: 利用多模态代理、以事件为中心的建模和上下文推理，整合个人传感、体验式计算和群体级分析。

Result: PCU具备提供个性化健康信息、主动健康导航和行为指导、医疗事件后恢复和治疗反应解读等能力。

Conclusion: PCU不仅能改善个人健康结果，还能为公共卫生和科学发现提供新基础，同时阐述了其架构、设计原则和实施挑战。

Abstract: Building on decades of success in digital infrastructure and biomedical
innovation, we propose the Personal Care Utility (PCU) - a cybernetic system
for lifelong health guidance. PCU is conceived as a global, AI-powered utility
that continuously orchestrates multimodal data, knowledge, and services to
assist individuals and populations alike. Drawing on multimodal agents,
event-centric modeling, and contextual inference, it offers three essential
capabilities: (1) trusted health information tailored to the individual, (2)
proactive health navigation and behavior guidance, and (3) ongoing
interpretation of recovery and treatment response after medical events. Unlike
conventional episodic care, PCU functions as an ambient, adaptive companion -
observing, interpreting, and guiding health in real time across daily life. By
integrating personal sensing, experiential computing, and population-level
analytics, PCU promises not only improved outcomes for individuals but also a
new substrate for public health and scientific discovery. We describe the
architecture, design principles, and implementation challenges of this emerging
paradigm.

</details>


### [485] [PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion](https://arxiv.org/abs/2510.22616)
*Morteza Alikhani,Mohammadtaha Bagherifard,Erfan Zinvandi,Mehran Sarmadi*

Main category: cs.CL

TL;DR: 本文介绍首个大规模波斯语常识推理基准数据集PerCoR，提出新策略生成数据，还提出DRESS - AF生成干扰项，展示模型在该数据集上表现，且DRESS - AF可迁移到英语数据集。


<details>
  <summary>Details</summary>
Motivation: 创建首个大规模波斯语常识推理基准数据集，推动波斯语常识推理研究。

Method: 引入基于连词的分割策略生成连贯句子完成对；提出DRESS - AF，一种无生成的对抗过滤方法来选择干扰项。

Result: 人类注释者在PerCoR上得分89%，OpenAI - o3表现最佳达92.18%，最强开源模型DeepSeek - R1达82.51%；DRESS - AF可迁移到英语HellaSwag基准数据集。

Conclusion: PerCoR数据集有一定难度，波斯语常识推理存在性能差距；DRESS - AF具有迁移性。

Abstract: We introduced PerCoR (Persian Commonsense Reasoning), the first large-scale
Persian benchmark for commonsense reasoning. PerCoR contains 106K
multiple-choice sentence-completion problems drawn from more than forty news,
cultural, and other web sources. We introduce a novel conjunction-based
segmentation strategy to generate coherent sentence-completion pairs, enabling
broad topical and structural diversity. To create challenging distractors, we
propose DRESS-AF (Distractor Ranking via Embedding Similarity Scoring and
Adversarial Filtering), a generation-free adversarial filtering method that
selects distractors from the pool of gold continuations while maximising model
confusion. Human annotators score 89% on PerCoR, while OpenAI-o3 achieves the
highest performance at 92.18%, followed closely by Claude-Sonnet-3.7 (91.17%).
The strongest open-source model, DeepSeek-R1, reaches 82.51%, underscoring both
the dataset's difficulty and the remaining performance gap in Persian
commonsense reasoning. We further show that DRESS-AF transfers to the English
HellaSwag benchmark, increasing its difficulty without hurting human
solvability. The dataset is available at
https://huggingface.co/datasets/MCINext/PerCoR.

</details>


### [486] [Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal](https://arxiv.org/abs/2510.22629)
*Ambalika Guha,Sajal Saha,Debanjan Ballav,Soumi Mitra,Hritwick Chakraborty*

Main category: cs.CL

TL;DR: 本文参与开发三语（托托语 - 孟加拉语 - 英语）学习应用来存档和推广濒危的托托语，结合传统语言学方法与AI提供了濒危语言保护的可持续模型。


<details>
  <summary>Details</summary>
Motivation: 每种语言提供独特世界观，需保护语言多样性，有全球保护濒危语言的举措，因此要开发应用保护印度西孟加拉邦濒危的托托语。

Method: 通过实地调查收集语言资料，创建三语语料库训练小语言模型和翻译引擎，分析词法形态，开发文字标准化和数字读写工具。

Result: 开发出三语语言学习应用，实现了文字集成和语料结构化，可提升托托语的可及性和可用性。

Conclusion: 结合传统语言学方法与AI为濒危语言保护提供可持续模型，强调跨学科合作对社区语言复兴的价值。

Abstract: Preserving linguistic diversity is necessary as every language offers a
distinct perspective on the world. There have been numerous global initiatives
to preserve endangered languages through documentation. This paper is a part of
a project which aims to develop a trilingual (Toto-Bangla-English) language
learning application to digitally archive and promote the endangered Toto
language of West Bengal, India. This application, designed for both native Toto
speakers and non-native learners, aims to revitalize the language by ensuring
accessibility and usability through Unicode script integration and a structured
language corpus. The research includes detailed linguistic documentation
collected via fieldwork, followed by the creation of a morpheme-tagged,
trilingual corpus used to train a Small Language Model (SLM) and a
Transformer-based translation engine. The analysis covers inflectional
morphology such as person-number-gender agreement, tense-aspect-mood
distinctions, and case marking, alongside derivational strategies that reflect
word-class changes. Script standardization and digital literacy tools were also
developed to enhance script usage. The study offers a sustainable model for
preserving endangered languages by incorporating traditional linguistic
methodology with AI. This bridge between linguistic research with technological
innovation highlights the value of interdisciplinary collaboration for
community-based language revitalization.

</details>


### [487] [Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study](https://arxiv.org/abs/2510.22747)
*Eeham Khan,Firas Saidani,Owen Van Esbroeck,Richard Khoury,Leila Kosseim*

Main category: cs.CL

TL;DR: 研究在数据和计算资源有限时用持续预训练（CPT）进行方言学习，用低秩自适应（LoRA）和高效计算的CPT使三个大语言模型适应魁北克法语方言，实验有效果并发布魁北克法语大语言模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能力局限于高资源语言，需利用CPT使模型适应低资源地区方言。

Method: 使用低秩自适应（LoRA）和计算高效的持续预训练，用小数据集使三个大语言模型适应魁北克法语方言，并在COLE套件上进行基准测试。

Result: 少数方言基准测试有改进，威望语言基准测试回归极小，仅更新不到1%的模型参数，增益高度依赖语料库组成。

Conclusion: 带参数高效微调（PEFT）的CPT可缩小方言差距，为少数语言社区提供高质量大语言模型访问。

Abstract: Despite the widespread adoption of large language models (LLMs), their
strongest capabilities remain largely confined to a small number of
high-resource languages for which there is abundant training data. Recently,
continual pre-training (CPT) has emerged as a means to fine-tune these models
to low-resource regional dialects. In this paper, we study the use of CPT for
dialect learning under tight data and compute budgets. Using low-rank
adaptation (LoRA) and compute-efficient continual pre-training, we adapt three
LLMs to the Qu\'ebec French dialect using a very small dataset and benchmark
them on the COLE suite. Our experiments demonstrate an improvement on the
minority dialect benchmarks with minimal regression on the prestige language
benchmarks with under 1% of model parameters updated. Analysis of the results
demonstrate that gains are highly contingent on corpus composition. These
findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can
narrow the dialect gap by providing cost-effective and sustainable language
resource creation, expanding high-quality LLM access to minority linguistic
communities. We release the first Qu\'ebec French LLMs on HuggingFace.

</details>


### [488] [Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models](https://arxiv.org/abs/2510.22752)
*Anooshka Bajaj,Deven Mahesh Mistry,Sahaj Singh Maini,Yash Aggarwal,Zoran Tiganj*

Main category: cs.CL

TL;DR: 研究预训练大语言模型在上下文学习中的时间偏置，发现不同模型有相似时间偏置，有助于理解上下文学习中的时间分离和情景检索。


<details>
  <summary>Details</summary>
Motivation: 探究不同预训练大语言模型区分和检索时间上分离事件的能力，理解上下文学习中时间和语义关系对信息检索的影响。

Method: 用含重复标记的序列提示模型，固定重复标记位置、置换其他标记以分离时间效应，进行消融实验。

Result: 模型对重复标记后的标记概率最高，有靠近输入首尾的偏置，中间记忆检索不可靠，不同架构模型时间偏置相似。

Conclusion: 研究加深了对上下文学习中时间偏置的理解，展示了其对时间分离和情景检索的作用。

Abstract: In-context learning is governed by both temporal and semantic relationships,
shaping how Large Language Models (LLMs) retrieve contextual information.
Analogous to human episodic memory, where the retrieval of specific events is
enabled by separating events that happened at different times, this work probes
the ability of various pretrained LLMs, including transformer and state-space
models, to differentiate and retrieve temporally separated events.
Specifically, we prompted models with sequences containing multiple
presentations of the same token, which reappears at the sequence end. By fixing
the positions of these repeated tokens and permuting all others, we removed
semantic confounds and isolated temporal effects on next-token prediction.
Across diverse sequences, models consistently placed the highest probabilities
on tokens following a repeated token, but with a notable bias for those nearest
the beginning or end of the input. An ablation experiment linked this
phenomenon in transformers to induction heads. Extending the analysis to unique
semantic contexts with partial overlap further demonstrated that memories
embedded in the middle of a prompt are retrieved less reliably. Despite
architectural differences, state-space and transformer models showed comparable
temporal biases. Our findings deepen the understanding of temporal biases in
in-context learning and offer an illustration of how these biases can enable
temporal separation and episodic retrieval.

</details>


### [489] [Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP](https://arxiv.org/abs/2510.22823)
*Poli Nemkova,Amrit Adhikari,Matthew Pearson,Vamsi Krishna Sadu,Mark V. Albert*

Main category: cs.CL

TL;DR: 本文首次系统比较商业和开源大语言模型用于多语言人权侵犯检测，量化成本 - 可靠性权衡，发现对齐而非规模决定稳定性，为组织提供部署指导。


<details>
  <summary>Details</summary>
Motivation: 人道主义组织在商业 API 和开源模型间做选择，开源模型缺乏实证验证，需比较两者以明确成本 - 可靠性权衡。

Method: 对六个模型（四个指令对齐和两个开源）进行 78000 次多语言推理，用标准分类指标和新的跨语言可靠性指标评估。

Result: 对齐模型在不同语言中准确性和校准稳定，开源模型有显著提示语言敏感性和校准漂移。

Conclusion: 多语言对齐可实现语言无关推理，为组织平衡预算和可靠性提供实践指导。

Abstract: Humanitarian organizations face a critical choice: invest in costly
commercial APIs or rely on free open-weight models for multilingual human
rights monitoring. While commercial systems offer reliability, open-weight
alternatives lack empirical validation -- especially for low-resource languages
common in conflict zones. This paper presents the first systematic comparison
of commercial and open-weight large language models (LLMs) for
human-rights-violation detection across seven languages, quantifying the
cost-reliability trade-off facing resource-constrained organizations. Across
78,000 multilingual inferences, we evaluate six models -- four
instruction-aligned (Claude-Sonnet-4, DeepSeek-V3, Gemini-Flash-2.0,
GPT-4.1-mini) and two open-weight (LLaMA-3-8B, Mistral-7B) -- using both
standard classification metrics and new measures of cross-lingual reliability:
Calibration Deviation (CD), Decision Bias (B), Language Robustness Score (LRS),
and Language Stability Score (LSS). Results show that alignment, not scale,
determines stability: aligned models maintain near-invariant accuracy and
balanced calibration across typologically distant and low-resource languages
(e.g., Lingala, Burmese), while open-weight models exhibit significant
prompt-language sensitivity and calibration drift. These findings demonstrate
that multilingual alignment enables language-agnostic reasoning and provide
practical guidance for humanitarian organizations balancing budget constraints
with reliability in multilingual deployment.

</details>


### [490] [Once Upon an Input: Reasoning via Per-Instance Program Synthesis](https://arxiv.org/abs/2510.22849)
*Adam Stein,Neelay Velingker,Mayur Naik,Eric Wong*

Main category: cs.CL

TL;DR: 提出PIPS方法提升大语言模型复杂推理能力，实验显示其优于PoT和CoT。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂多步推理存在困难，现有方法如CoT和PoT常产生不良解决方案，尤其是在算法领域。

Method: 引入Per - Instance Program Synthesis (PIPS)，在实例级别生成和改进程序，利用结构反馈，不依赖特定任务指导或显式测试用例，还结合置信度指标动态选择推理方式。

Result: 在三个前沿大语言模型和30个基准测试中，PIPS比PoT和CoT分别最多提高绝对调和平均准确率8.6%和9.4%，在算法任务上比使用Gemini - 2.0 - Flash的PoT减少65.1%不良程序生成。

Conclusion: PIPS方法能有效提升大语言模型复杂推理性能，减少不良程序生成。

Abstract: Large language models (LLMs) excel at zero-shot inference but continue to
struggle with complex, multi-step reasoning. Recent methods that augment LLMs
with intermediate reasoning steps such as Chain of Thought (CoT) and Program of
Thought (PoT) improve performance but often produce undesirable solutions,
especially in algorithmic domains. We introduce Per-Instance Program Synthesis
(PIPS), a method that generates and refines programs at the instance-level
using structural feedback without relying on task-specific guidance or explicit
test cases. To further improve performance, PIPS incorporates a confidence
metric that dynamically chooses between direct inference and program synthesis
on a per-instance basis. Experiments across three frontier LLMs and 30
benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question
answering tasks, relational reasoning tasks, and mathematical reasoning tasks
show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and
9.4% compared to PoT and CoT respectively, and reduces undesirable program
generations by 65.1% on the algorithmic tasks compared to PoT with
Gemini-2.0-Flash.

</details>


### [491] [Frustratingly Easy Task-aware Pruning for Large Language Models](https://arxiv.org/abs/2510.22489)
*Yuanhe Tian,Junjie Liu,Xican Yang,Haishan Ye,Yan Song*

Main category: cs.CL

TL;DR: 提出一种有效修剪大语言模型（LLM）的方法，能在缩小参数空间时保留特定任务能力，实验表明该方法有效且优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM修剪方法主要关注生成流畅句子，忽视特定领域和任务表现，需提出能保留特定任务能力的修剪方法。

Method: 分析传统修剪在通用域校准下最小化损失扰动的方式，将特定任务特征分布纳入重要性计算，用通用和特定任务校准数据计算重要性分数，按激活范数差异划分参数组并融合分数指导修剪。

Result: 在广泛使用的基准测试中，该方法有效且在相同修剪率和不同设置下始终优于基线。

Conclusion: 提出的修剪方法能在压缩LLM时保留其特定能力，且与各种基础修剪技术无缝集成。

Abstract: Pruning provides a practical solution to reduce the resources required to run
large language models (LLMs) to benefit from their effective capabilities as
well as control their cost for training and inference. Research on LLM pruning
often ranks the importance of LLM parameters using their magnitudes and
calibration-data activations and removes (or masks) the less important ones,
accordingly reducing LLMs' size. However, these approaches primarily focus on
preserving the LLM's ability to generate fluent sentences, while neglecting
performance on specific domains and tasks. In this paper, we propose a simple
yet effective pruning approach for LLMs that preserves task-specific
capabilities while shrinking their parameter space. We first analyze how
conventional pruning minimizes loss perturbation under general-domain
calibration and extend this formulation by incorporating task-specific feature
distributions into the importance computation of existing pruning algorithms.
Thus, our framework computes separate importance scores using both general and
task-specific calibration data, partitions parameters into shared and exclusive
groups based on activation-norm differences, and then fuses their scores to
guide the pruning process. This design enables our method to integrate
seamlessly with various foundation pruning techniques and preserve the LLM's
specialized abilities under compression. Experiments on widely used benchmarks
demonstrate that our approach is effective and consistently outperforms the
baselines with identical pruning ratios and different settings.

</details>


### [492] [Batch Speculative Decoding Done Right](https://arxiv.org/abs/2510.22876)
*Ranran Haoran Zhang,Soumik Dey,Ashirbad Mishra,Hansi Wu,Binbin Li,Rui Zhang*

Main category: cs.CL

TL;DR: 本文针对批量投机解码的不规则张量问题，提出EQSPEC和EXSPEC方法，在提升吞吐量同时保证输出等效性。


<details>
  <summary>Details</summary>
Motivation: 将投机解码扩展到批量处理对生产服务至关重要，但会引入不规则张量问题，现有批量实现存在输出等效性问题。

Method: （1）明确保证正确性的同步要求；（2）提出正确性优先的批量投机解码EQSPEC；（3）引入EXSPEC，通过维护滑动序列池和动态分组减少对齐开销。

Result: 在SpecBench数据集上，与批量大小为1相比，批量大小为8时吞吐量最多提升3倍，且能高效扩展到批量大小8，保持95%的输出等效性。

Conclusion: 所提方法无需自定义内核，可与现有推理栈集成，代码已开源。

Abstract: Speculative decoding speeds up LLM inference by using a small draft model to
propose multiple tokens that a target model verifies in parallel. Extending
this idea to batches is essential for production serving, but it introduces the
ragged tensor problem: sequences in the same batch accept different numbers of
draft tokens, breaking right-alignment and corrupting position IDs, attention
masks, and KV-cache state. We show that several existing batch implementations
violate output equivalence-the fundamental requirement that speculative
decoding must produce identical token sequences to standard autoregressive
generation. These violations occur precisely due to improper handling of the
ragged tensor problem. In response, we (1) characterize the synchronization
requirements that guarantee correctness, (2) present a correctness-first batch
speculative decoding EQSPEC that exposes realignment as consuming 40% of
overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences
and dynamically forms same-length groups, to reduce the realignment overhead
while preserving per-sequence speculative speedups. On the SpecBench dataset,
across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our
approach achieves up to 3$\times$ throughput improvement at batch size 8
compared to batch size 1, with efficient scaling through batch size 8, while
maintaining 95% output equivalence. Our method requires no custom kernels and
integrates cleanly with existing inference stacks. Our code is available at
https://github.com/eBay/spec_dec.

</details>


### [493] [MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs](https://arxiv.org/abs/2510.22967)
*Yucheng Ning,Xixun Lin,Fang Fang,Yanan Cao*

Main category: cs.CL

TL;DR: 本文提出评估大语言模型长文本输出事实准确性的系统方法，构建数据集与验证系统，实验显示大模型事实一致性较好，国产模型在中文内容上表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用引发对其输出事实准确性的担忧，现有短文本评估方法不适用于长文本。

Method: 提出集成大规模长文本数据集、多智能体验证机制和加权评估指标的系统方法，构建LongHalluQA数据集，开发MAD - Fact验证系统，引入事实重要性层次。

Result: 在两个基准测试上，较大的大语言模型通常保持较高事实一致性，国产模型在中文内容上表现优异。

Conclusion: 为评估和提高大语言模型长文本输出的事实可靠性提供结构化框架，指导其在敏感领域的安全部署。

Abstract: The widespread adoption of Large Language Models (LLMs) raises critical
concerns about the factual accuracy of their outputs, especially in high-risk
domains such as biomedicine, law, and education. Existing evaluation methods
for short texts often fail on long-form content due to complex reasoning
chains, intertwined perspectives, and cumulative information. To address this,
we propose a systematic approach integrating large-scale long-form datasets,
multi-agent verification mechanisms, and weighted evaluation metrics. We
construct LongHalluQA, a Chinese long-form factuality dataset; and develop
MAD-Fact, a debate-based multi-agent verification system. We introduce a fact
importance hierarchy to capture the varying significance of claims in long-form
texts. Experiments on two benchmarks show that larger LLMs generally maintain
higher factual consistency, while domestic models excel on Chinese content. Our
work provides a structured framework for evaluating and enhancing factual
reliability in long-form LLM outputs, guiding their safe deployment in
sensitive domains.

</details>


### [494] [Measuring Teaching with LLMs](https://arxiv.org/abs/2510.22968)
*Michael Hardy*

Main category: cs.CL

TL;DR: 本文使用基于句子级嵌入的定制大语言模型评估教学质量，模型表现良好，为人工智能驱动的教学测量提供新方法。


<details>
  <summary>Details</summary>
Motivation: 客观可扩展地测量教学质量是教育领域的长期挑战，通用大语言模型难以可靠应用复杂课堂观察工具。

Method: 使用基于句子级嵌入的定制大语言模型，系统评估五种不同句子嵌入，采用防止过拟合的高效数据训练机制。

Result: 模型能达到人类水平甚至超人表现，与专家评分相关性超0.65，优于人类评分者平均相关性；高级模型更关注课程级特征；模型总分与教师增值指标一致，但个体项目上未完全泛化。

Conclusion: 建立了可行且强大的人工智能驱动教学测量新方法，为教育者发展提供可扩展、可靠和有效的反馈途径。

Abstract: Objective and scalable measurement of teaching quality is a persistent
challenge in education. While Large Language Models (LLMs) offer potential,
general-purpose models have struggled to reliably apply complex, authentic
classroom observation instruments. This paper uses custom LLMs built on
sentence-level embeddings, an architecture better suited for the long-form,
interpretive nature of classroom transcripts than conventional subword
tokenization. We systematically evaluate five different sentence embeddings
under a data-efficient training regime designed to prevent overfitting. Our
results demonstrate that these specialized models can achieve human-level and
even super-human performance with expert human ratings above 0.65 and
surpassing the average human-human rater correlation. Further, through analysis
of annotation context windows, we find that more advanced models-those better
aligned with human judgments-attribute a larger share of score variation to
lesson-level features rather than isolated utterances, challenging the
sufficiency of single-turn annotation paradigms. Finally, to assess external
validity, we find that aggregate model scores align with teacher value-added
measures, indicating they are capturing features relevant to student learning.
However, this trend does not hold at the individual item level, suggesting that
while the models learn useful signals, they have not yet achieved full
generalization. This work establishes a viable and powerful new methodology for
AI-driven instructional measurement, offering a path toward providing scalable,
reliable, and valid feedback for educator development.

</details>


### [495] [SALSA: Single-pass Autoregressive LLM Structured Classification](https://arxiv.org/abs/2510.22691)
*Ruslan Berdichevsky,Shai Nahum-Gefen,Elad Ben Zaken*

Main category: cs.CL

TL;DR: 提出SALSA管道提升大语言模型文本分类性能，在多基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 指令调优大语言模型在文本分类基准测试中表现不佳。

Method: 结合结构化提示、类到标记映射和参数高效微调，避免冷启动训练，将类标签映射到不同输出标记，构建提示以获取单标记响应，推理时将模型输出投影到相关类标记的对数上。

Result: SALSA在不同基准测试中取得了最先进的结果。

Conclusion: SALSA对基于大语言模型的分类应用具有鲁棒性和可扩展性。

Abstract: Despite their impressive generalization capabilities, instruction-tuned Large
Language Models often underperform on text classification benchmarks. We
introduce SALSA, a coherent pipeline that combines structured prompting,
class-to-token mapping, and parameter-efficient fine-tuning, thereby avoiding
cold-start training. Each class label is mapped to a distinct output token, and
prompts are constructed to elicit a single-token response. During inference,
the model's output is projected only onto the logits of the relevant class
tokens, enabling efficient and accurate classification in a single forward
pass. SALSA achieves state-of-the-art results across diverse benchmarks,
demonstrating its robustness and scalability for LLM-based classification
applications.

</details>


### [496] [Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures](https://arxiv.org/abs/2510.23006)
*Shenran Wang,Timothy Tin-Long Tse,Jian Zhu*

Main category: cs.CL

TL;DR: 对不同架构大语言模型的上下文学习进行评估，发现不同架构模型性能相似但内部不同，定位功能向量位置，指出其对不同任务的重要性，强调结合行为和机制分析的重要性。


<details>
  <summary>Details</summary>
Motivation: 深入了解不同架构大语言模型在基于知识的上下文学习任务中的表现及内部机制。

Method: 结合行为探测和基于干预的方法进行评估。

Result: 不同架构模型任务表现相似但内部不同；功能向量主要位于自注意力和Mamba层；Mamba2可能使用不同机制；功能向量对参数知识检索的上下文学习更重要。

Conclusion: 有助于更细致地理解不同架构和任务类型，强调结合行为和机制分析研究大语言模型能力的重要性。

Abstract: We perform in-depth evaluations of in-context learning (ICL) on
state-of-the-art transformer, state-space, and hybrid large language models
over two categories of knowledge-based ICL tasks. Using a combination of
behavioral probing and intervention-based methods, we have discovered that,
while LLMs of different architectures can behave similarly in task performance,
their internals could remain different. We discover that function vectors (FVs)
responsible for ICL are primarily located in the self-attention and Mamba
layers, and speculate that Mamba2 uses a different mechanism from FVs to
perform ICL. FVs are more important for ICL involving parametric knowledge
retrieval, but not for contextual knowledge understanding. Our work contributes
to a more nuanced understanding across architectures and task types.
Methodologically, our approach also highlights the importance of combining both
behavioural and mechanistic analyses to investigate LLM capabilities.

</details>


### [497] [VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions](https://arxiv.org/abs/2510.22798)
*Thu Phuong Nguyen,Duc M. Nguyen,Hyotaek Jeon,Hyunwook Lee,Hyunmin Song,Sungahn Ko,Taehwan Kim*

Main category: cs.CL

TL;DR: 介绍用于评估手写数学表达式的模型VEHME，它集成两阶段训练管道和视觉提示模块，在数据集上表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 自动评估手写数学解答因格式多样、布局无结构和符号复杂而具有挑战，需要解决此问题。

Method: 引入VEHME模型，集成两阶段训练管道（监督微调与强化学习），提出表达式感知视觉提示模块。

Result: 在AIHub和FERMAT数据集上，VEHME在开源模型中达到了最先进的性能，接近专有系统的准确性。

Conclusion: VEHME有潜力成为自动数学评估可扩展且易获取的工具。

Abstract: Automatically assessing handwritten mathematical solutions is an important
problem in educational technology with practical applications, but it remains a
significant challenge due to the diverse formats, unstructured layouts, and
symbolic complexity of student work. To address this challenge, we introduce
VEHME-a Vision-Language Model for Evaluating Handwritten Mathematics
Expressions-designed to assess open-form handwritten math responses with high
accuracy and interpretable reasoning traces. VEHME integrates a two-phase
training pipeline: (i) supervised fine-tuning using structured reasoning data,
and (ii) reinforcement learning that aligns model outputs with
multi-dimensional grading objectives, including correctness, reasoning depth,
and error localization. To enhance spatial understanding, we propose an
Expression-Aware Visual Prompting Module, trained on our synthesized multi-line
math expressions dataset to robustly guide attention in visually heterogeneous
inputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art
performance among open-source models and approaches the accuracy of proprietary
systems, demonstrating its potential as a scalable and accessible tool for
automated math assessment. Our training and experiment code is publicly
available at our GitHub repository.

</details>


### [498] [Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays](https://arxiv.org/abs/2510.22830)
*Haowei Hua,Hong Jiao,Xinyi Wang*

Main category: cs.CL

TL;DR: 研究用生成式语言模型通过摘要和提示对长论文进行自动评分，结果显示评分准确性显著提高。


<details>
  <summary>Details</summary>
Motivation: 基于编码器的模型有512个标记的限制，在长论文自动评分方面存在不足，因此探索生成式语言模型进行长论文自动评分。

Method: 通过摘要和提示，利用生成式语言模型进行长论文自动评分。

Result: 对于Learning Agency Lab Automated Essay Scoring 2.0数据集，评分准确性大幅提升，QWK从0.822提高到0.8878。

Conclusion: 生成式语言模型在长论文自动评分上有很好的效果，能提高评分准确性。

Abstract: BERT and its variants are extensively explored for automated scoring.
However, a limit of 512 tokens for these encoder-based models showed the
deficiency in automated scoring of long essays. Thus, this research explores
generative language models for automated scoring of long essays via
summarization and prompting. The results revealed great improvement of scoring
accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab
Automated Essay Scoring 2.0 dataset.

</details>


### [499] [Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2510.23038)
*Ran Xu,Jingjing Chen,Jiayu Ye,Yu Wu,Jun Yan,Carl Yang,Hongkun Yu*

Main category: cs.CL

TL;DR: 提出TIR - Judge框架训练LLM评判器，在多基准测试表现优，TIR - Judge - Zero无蒸馏训练也有好效果。


<details>
  <summary>Details</summary>
Motivation: 多数LLM评判器仅基于文本推理，验证复杂约束和精确计算能力有限，受工具集成推理成功启发。

Method: 提出TIR - Judge端到端RL框架，集成代码执行器，遵循三个原则：跨可验证和不可验证领域多样化训练、灵活判断格式、无蒸馏的迭代RL。

Result: 在七个公开基准测试中，TIR - Judge优于强推理评判器，TIR - Judge - Zero无蒸馏训练也能达到与蒸馏变体相当的性能。

Conclusion: 工具增强的评判器可通过迭代强化学习自我进化。

Abstract: Large Language Models (LLMs) are widely used as judges to evaluate response
quality, providing a scalable alternative to human evaluation. However, most
LLM judges operate solely on intrinsic text-based reasoning, limiting their
ability to verify complex constraints or perform accurate computation.
Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks,
we propose TIR-Judge, an end-to-end RL framework for training LLM judges that
integrates a code executor for precise evaluation. TIR-Judge is built on three
principles: (i) diverse training across verifiable and non-verifiable domains,
(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)
iterative RL that bootstraps directly from the initial model without
distillation. On seven public benchmarks, TIR-Judge surpasses strong
reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and
achieves listwise performance comparable to Claude-Opus-4 despite having only
8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled
judge trajectories, matches the performance of distilled variants,
demonstrating that tool-augmented judges can self-evolve through iterative
reinforcement learning.

</details>


### [500] [Interpreting and Mitigating Unwanted Uncertainty in LLMs](https://arxiv.org/abs/2510.22866)
*Tiasa Singha Roy,Ayush Rajesh Jhaveri,Ilias Triantafyllopoulos*

Main category: cs.CL

TL;DR: 研究大语言模型答案翻转不确定性现象，发现非检索注意力头影响大，掩码处理可减少翻转行为但下游任务有权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在答案翻转的不确定性现象，破坏信任且在高风险领域有严重风险，需研究其机制。

Method: 采用Needle - in - a - Haystack检索框架，集成Flip风格重新评估提示来模拟答案翻转场景。

Result: 检索头并非避免不确定性的主要因素，发现一组非检索注意力头在不确定语境中过度关注误导性标记，掩码处理这些头可减少达15%的翻转行为，下游任务存在权衡。

Conclusion: 研究成果有助于机械可解释性领域，提供缓解大语言模型不确定性失败模式的简单有效技术。

Abstract: Despite their impressive capabilities, Large Language Models (LLMs) exhibit
unwanted uncertainty, a phenomenon where a model changes a previously correct
answer into an incorrect one when re-prompted. This behavior undermines trust
and poses serious risks in high-stakes domains. In this work, we investigate
the mechanisms that drive this phenomenon. We adapt the Needle-in-a-Haystack
retrieval framework and integrate a Flip-style re-evaluation prompt to simulate
realistic answer-flipping scenarios. We find that retrieval heads are not
primarily responsible for avoiding uncertainty. Instead, we identify a small
set of non-retrieval attention heads that disproportionately attend to
misleading tokens in uncertain contexts. Masking these heads yields significant
improvements, reducing flip behavior by up to 15% without introducing
incoherence or overcorrection. However, when tested for downstream tasks, we
observe trade-offs with flip behavior. Our findings contribute to the growing
field of mechanistic interpretability and present a simple yet effective
technique for mitigating uncertainty-driven failure modes in LLMs.

</details>


### [501] [Quality-Aware Translation Tagging in Multilingual RAG system](https://arxiv.org/abs/2510.23070)
*Hoyeon Moon,Byeolhee Kim,Nikhil Verma*

Main category: cs.CL

TL;DR: 提出QTT - RAG方法解决多语言检索增强生成中翻译质量问题，在多个基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 多语言检索增强生成在低资源场景中翻译质量差影响性能，现有方法存在不足。

Method: 提出QTT - RAG，从语义等价、语法准确性、自然流畅度三个维度评估翻译质量，并将分数作为元数据附加。

Result: 在两个开放域问答基准测试中，使用六种指令调优大语言模型，QTT - RAG优于CrossRAG和DKM - RAG基线。

Conclusion: QTT - RAG能在低资源场景有效利用跨语言文档，为多语言领域提供实用稳健解决方案。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English
documents and translates them into the query language for low-resource
settings. However, poor translation quality degrades response generation
performance. Existing approaches either assume sufficient translation quality
or utilize the rewriting method, which introduces factual distortion and
hallucinations. To mitigate these problems, we propose Quality-Aware
Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation
quality along three dimensions-semantic equivalence, grammatical accuracy, and
naturalness&fluency-and attach these scores as metadata without altering the
original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines
in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs
ranging from 2.4B to 14B parameters, covering two low-resource languages
(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG
outperforms the baselines by preserving factual integrity while enabling
generator models to make informed decisions based on translation reliability.
This approach allows for effective usage of cross-lingual documents in
low-resource settings with limited native language documents, offering a
practical and robust solution across multilingual domains.

</details>


### [502] [Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs](https://arxiv.org/abs/2510.23163)
*Hang Lei,Shengyi Zong,Zhaoyan Li,Ziren Zhou,Hao Liu*

Main category: cs.CL

TL;DR: 直接端到端生成方法在使用大语言模型生成剧本时效果不佳，论文提出双阶段细化（DSR）框架并结合混合数据合成方法，经专业编剧评估，DSR表现良好。


<details>
  <summary>Details</summary>
Motivation: 直接端到端生成方法难以让大语言模型生成高质量剧本，原因是单一模型难以同时掌握创意叙事构建和严格格式遵循两种能力。

Method: 提出双阶段细化（DSR）框架，将创意叙事生成与格式转换解耦，通过混合数据合成解决训练数据稀缺问题。

Result: 专业编剧的盲测显示，DSR 对比 Gemini - 2.5 - Pro 等强基线模型胜率达 75%，达到人类水平表现的 82.7%。

Conclusion: 具有定制数据合成的分解生成架构能有效让大语言模型在复杂创意领域发挥专长。

Abstract: The screenplay serves as the foundation for television production, defining
narrative structure, character development, and dialogue. While Large Language
Models (LLMs) show great potential in creative writing, direct end-to-end
generation approaches often fail to produce well-crafted screenplays. We argue
this failure stems from forcing a single model to simultaneously master two
disparate capabilities: creative narrative construction and rigid format
adherence. The resulting outputs may mimic superficial style but lack the deep
structural integrity and storytelling substance required for professional use.
To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage
Refinement (DSR), a decomposed framework that decouples creative narrative
generation from format conversion. The first stage transforms a brief outline
into rich, novel-style prose. The second stage refines this narrative into a
professionally formatted screenplay. This separation enables the model to
specialize in one distinct capability at each stage. A key challenge in
implementing DSR is the scarcity of paired outline-to-novel training data. We
address this through hybrid data synthesis: reverse synthesis deconstructs
existing screenplays into structured inputs, while forward synthesis leverages
these inputs to generate high-quality narrative texts as training targets.
Blind evaluations by professional screenwriters show that DSR achieves a 75%
win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of
human-level performance. Our work demonstrates that decomposed generation
architecture with tailored data synthesis effectively specializes LLMs in
complex creative domains.

</details>


### [503] [DREaM: Drug-Drug Relation Extraction via Transfer Learning Method](https://arxiv.org/abs/2510.23189)
*Ali Fata,Hossein Rahmani,Parinaz Soltanzadeh,Amirhossein Derakhshan,Behrouz Minaei Bidgoli*

Main category: cs.CL

TL;DR: 本文提出DREAM方法用于药物关系提取，用训练模型发现实体关系构建药物关系本体，并用大语言模型验证，结果显示有一定效果且能发现医学领域模糊性。


<details>
  <summary>Details</summary>
Motivation: 药物关系提取在识别药物相互作用和预测副作用中很重要，但缺乏专门数据集，需用迁移学习应用机器学习方法。

Method: 提出DREAM方法，先用训练好的关系提取模型发现实体关系，应用到医学文本语料构建药物关系本体，再用大语言模型验证提取的关系。

Result: 大语言模型同意从PubMed摘要子集中提取的71个关系，定性分析表明该方法能发现医学领域的模糊性。

Conclusion: 提出的方法在药物关系提取中有一定效果，同时凸显了该领域关系提取的挑战。

Abstract: Relation extraction between drugs plays a crucial role in identifying drug
drug interactions and predicting side effects. The advancement of machine
learning methods in relation extraction, along with the development of large
medical text databases, has enabled the low cost extraction of such relations
compared to other approaches that typically require expert knowledge. However,
to the best of our knowledge, there are limited datasets specifically designed
for drug drug relation extraction currently available. Therefore, employing
transfer learning becomes necessary to apply machine learning methods in this
domain. In this study, we propose DREAM, a method that first employs a trained
relation extraction model to discover relations between entities and then
applies this model to a corpus of medical texts to construct an ontology of
drug relationships. The extracted relations are subsequently validated using a
large language model. Quantitative results indicate that the LLM agreed with 71
of the relations extracted from a subset of PubMed abstracts. Furthermore, our
qualitative analysis indicates that this approach can uncover ambiguities in
the medical domain, highlighting the challenges inherent in relation extraction
in this field.

</details>


### [504] [Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation](https://arxiv.org/abs/2510.23123)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). LoRA essentially describes the
projection of an input space into a low-dimensional output space, with the
dimensionality determined by the LoRA rank. In standard LoRA, all input tokens
share the same weights and undergo an identical input-output projection. This
limits LoRA's ability to capture token-specific information due to the inherent
semantic differences among tokens. To address this limitation, we propose
Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts
LoRA weights according to the input token, thereby learning token-wise
input-output projections in an end-to-end manner. Formally, the weights of
TopLoRA can be expressed as $B\Sigma_X A$, where $A$ and $B$ are low-rank
matrices (as in standard LoRA), and $\Sigma_X$ is a diagonal matrix generated
from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA
weights but achieves more granular adaptation by learning token-wise LoRA
weights (i.e., token-wise input-output projections). Extensive experiments
across multiple models and datasets demonstrate that TopLoRA consistently
outperforms LoRA and its variants. The code is available at
https://github.com/Leopold1423/toplora-neurips25.

</details>


### [505] [Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports](https://arxiv.org/abs/2510.23217)
*Alois Thomas,Maya Varma,Jean-Benoit Delbrouck,Curtis P. Langlotz*

Main category: cs.CL

TL;DR: 提出用于自动化放射学报告生成的句子级过程奖励模型（PRM），在检测幻觉和提升报告质量上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型生成放射学报告时存在临床关键幻觉问题，且现有检测方法缺乏句子级粒度和泛化性。

Method: 引入句子级PRM，基于临床上下文和前文预测生成句子的事实正确性，并在MIMIC - CXR上微调。

Result: 轻量级0.5B参数PRM优于现有验证技术，能有效过滤低质量报告，引导加权最佳N选择过程提升临床指标。

Conclusion: 轻量级、上下文感知的PRM为临床大视觉语言模型提供了无内部激活访问的模型无关安全层。

Abstract: Automating radiology report generation with Large Vision-Language Models
(LVLMs) holds great potential, yet these models often produce clinically
critical hallucinations, posing serious risks. Existing hallucination detection
methods frequently lack the necessary sentence-level granularity or robust
generalization across different LVLM generators. We introduce a novel approach:
a sentence-level Process Reward Model (PRM) adapted for this vision-language
task. Our PRM predicts the factual correctness of each generated sentence,
conditioned on clinical context and preceding text. When fine-tuned on
MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM
outperforms existing verification techniques, demonstrating, for instance,
relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in
AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods
reliant on internal model states, our PRM demonstrates strong generalization to
an unseen LVLM. We further show its practical utility: PRM scores effectively
filter low-quality reports, improving F1-CheXbert scores by 4.5% (when
discarding the worst 10% of reports). Moreover, when guiding a novel weighted
best-of-N selection process on the MIMIC-CXR test set, our PRM show relative
improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for
BERTScore. These results demonstrate that a lightweight, context-aware PRM
provides a model-agnostic safety layer for clinical LVLMs without access to
internal activations

</details>


### [506] [Arabic Little STT: Arabic Children Speech Recognition Dataset](https://arxiv.org/abs/2510.23319)
*Mouhand Alkadri,Dania Desouki,Khloud Al Jallad*

Main category: cs.CL

TL;DR: 论文提出阿拉伯语儿童语音数据集Arabic Little STT，评估Whisper模型在该数据集上表现，结果显示模型在儿童语音上表现差，强调需专用基准和数据，数据集公开可丰富数据人口代表性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言如阿拉伯语数据稀缺，尤其是缺少儿童特定语音语料库的问题。

Method: 创建阿拉伯语儿童语音数据集Arabic Little STT，对Whisper模型在该数据集上进行系统评估并与成人阿拉伯语基准对比。

Result: 八个Whisper变体中表现最好的Large_v3在儿童语音上的字错误率达0.66，远高于成人数据集的低于0.20，与英语语音研究结果一致。

Conclusion: 自动语音识别发展中需要专用儿童语音基准和包含性训练数据，且数据需严格伦理和隐私框架保护，研究为阿拉伯语儿童语音技术研究奠定基础，公开数据集可丰富数据人口代表性。

Abstract: The performance of Artificial Intelligence (AI) systems fundamentally depends
on high-quality training data. However, low-resource languages like Arabic
suffer from severe data scarcity. Moreover, the absence of child-specific
speech corpora is an essential gap that poses significant challenges. To
address this gap, we present our created dataset, Arabic Little STT, a dataset
of Levantine Arabic child speech recorded in classrooms, containing 355
utterances from 288 children (ages 6 - 13). We further conduct a systematic
assessment of Whisper, a state-of-the-art automatic speech recognition (ASR)
model, on this dataset and compare its performance with adult Arabic
benchmarks. Our evaluation across eight Whisper variants reveals that even the
best-performing model (Large_v3) struggles significantly, achieving a 0.66 word
error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on
adult datasets. These results align with other research on English speech.
Results highlight the critical need for dedicated child speech benchmarks and
inclusive training data in ASR development. Emphasizing that such data must be
governed by strict ethical and privacy frameworks to protect sensitive child
information. We hope that this study provides an initial step for future work
on equitable speech technologies for Arabic-speaking children. We hope that our
publicly available dataset enrich the children's demographic representation in
ASR datasets.

</details>


### [507] [Detecting Religious Language in Climate Discourse](https://arxiv.org/abs/2510.23395)
*Evy Beijen,Pien Pieterse,Yusuf Çelik,Willem Th. van Peursen,Sandjai Bhulai,Meike Morren*

Main category: cs.CL

TL;DR: 本文研究宗教语言在气候相关文本中的出现情况，对比基于规则模型和大语言模型检测宗教语言的效果，揭示方法挑战与定义争议。


<details>
  <summary>Details</summary>
Motivation: 宗教语言渗透到当代话语中，研究其在气候相关文本中的显隐形式。

Method: 采用基于规则的模型（使用从生态神学文献中得出的宗教术语层次树）和零样本设置下的大语言模型，对超88万句的数据集进行分析。

Result: 基于规则的方法比大语言模型标记为宗教语言的句子更多。

Conclusion: 研究展示了分析气候话语中宗教语言方法的潜力与局限，凸显检测宗教语言的方法挑战和定义争议。

Abstract: Religious language continues to permeate contemporary discourse, even in
ostensibly secular domains such as environmental activism and climate change
debates. This paper investigates how explicit and implicit forms of religious
language appear in climate-related texts produced by secular and religious
nongovernmental organizations (NGOs). We introduce a dual methodological
approach: a rule-based model using a hierarchical tree of religious terms
derived from ecotheology literature, and large language models (LLMs) operating
in a zero-shot setting. Using a dataset of more than 880,000 sentences, we
compare how these methods detect religious language and analyze points of
agreement and divergence. The results show that the rule-based method
consistently labels more sentences as religious than LLMs. These findings
highlight not only the methodological challenges of computationally detecting
religious language but also the broader tension over whether religious language
should be defined by vocabulary alone or by contextual meaning. This study
contributes to digital methods in religious studies by demonstrating both the
potential and the limitations of approaches for analyzing how the sacred
persists in climate discourse.

</details>


### [508] [EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting](https://arxiv.org/abs/2510.23396)
*Musleh Alharthi,Kaleel Mahmood,Sarosh Patel,Ausif Mahmood*

Main category: cs.CL

TL;DR: 论文提出基于MoE框架的时间序列预测模型，结合多种SOTA模型，在标准基准上超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中，现有模型存在效果受质疑情况，且数据有倾向近期和受不可预测事件影响的挑战，需更好模型。

Method: 提出MoE框架，将xLSTM、增强线性、PatchTST和minGRU等SOTA模型集成到基于Transformer的MoE门控网络中。

Result: 所提模型在标准基准上超越所有现有时间序列预测模型，包括最新的MoE框架方法。

Conclusion: 提出的MoE框架是有效的时间序列预测方法，能取得比现有模型更好的性能。

Abstract: The immense success of the Transformer architecture
  in Natural Language Processing has led to its adoption in Time Se ries
Forecasting (TSF), where superior performance has been shown.
  However, a recent important paper questioned their effectiveness by
  demonstrating that a simple single layer linear model outperforms
  Transformer-based models. This was soon shown to be not as valid,
  by a better transformer-based model termed PatchTST. More re cently, TimeLLM
demonstrated even better results by repurposing a
  Large Language Model (LLM) for the TSF domain. Again, a follow
  up paper challenged this by demonstrating that removing the LLM
  component or replacing it with a basic attention layer in fact yields
  better performance. One of the challenges in forecasting is the fact
  that TSF data favors the more recent past, and is sometimes subject
  to unpredictable events. Based upon these recent insights in TSF, we
  propose a strong Mixture of Experts (MoE) framework. Our method
  combines the state-of-the-art (SOTA) models including xLSTM, en hanced
Linear, PatchTST, and minGRU, among others. This set of
  complimentary and diverse models for TSF are integrated in a Trans former
based MoE gating network. Our proposed model outperforms
  all existing TSF models on standard benchmarks, surpassing even the
  latest approaches based on MoE frameworks.

</details>


### [509] [Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences](https://arxiv.org/abs/2510.23451)
*Zhuoran Jin,Hongbang Yuan,Kejian Zhu,Jiachun Li,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 文章指出奖励模型存在模态不平衡和偏好僵化问题，提出Omni - Reward，包含评估基准、数据集和模型，在相关基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决奖励模型面临的模态不平衡（多数集中于文本和图像模态）和偏好僵化（固定二元偏好对无法捕捉个性化偏好）的问题。

Method: 1. 引入首个支持自由形式偏好的全模态奖励模型基准Omni - RewardBench；2. 构建包含24.8万通用偏好对和6.9万指令调整对的多模态偏好数据集Omni - RewardData；3. 提出包含判别式和生成式奖励模型的Omni - RewardModel。

Result: Omni - RewardModel在Omni - RewardBench和其他常用奖励建模基准上取得了良好性能。

Conclusion: Omni - Reward向支持自由形式偏好的通用全模态奖励建模迈出了一步。

Abstract: Reward models (RMs) play a critical role in aligning AI behaviors with human
preferences, yet they face two fundamental challenges: (1) Modality Imbalance,
where most RMs are mainly focused on text and image modalities, offering
limited support for video, audio, and other modalities; and (2) Preference
Rigidity, where training on fixed binary preference pairs fails to capture the
complexity and diversity of personalized preferences. To address the above
challenges, we propose Omni-Reward, a step toward generalist omni-modal reward
modeling with support for free-form preferences, consisting of: (1) Evaluation:
We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form
preferences, covering nine tasks across five modalities including text, image,
video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal
preference dataset comprising 248K general preference pairs and 69K
instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We
propose Omni-RewardModel, which includes both discriminative and generative
RMs, and achieves strong performance on Omni-RewardBench as well as other
widely used reward modeling benchmarks.

</details>


### [510] [BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents](https://arxiv.org/abs/2510.23458)
*Litu Ou,Kuan Li,Huifeng Yin,Liwen Zhang,Zhongwang Zhang,Xixi Wu,Rui Ye,Zile Qiao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 研究大语言模型搜索代理在长序列动作后通过置信度分数传达自信的能力，提出TTS方法，减少了token消耗并表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注单轮场景，对复杂多轮交互中的置信度研究有限，要探究大语言模型搜索代理在长序列动作后传达自信的能力。

Method: 在开源代理模型上进行实验，基于模型高低置信度时的准确率表现，提出使用置信度分数确定答案质量并促使模型重试的Test - Time Scaling (TTS) 方法。

Result: 提出的方法显著减少了token消耗，与基线固定预算TTS方法相比表现具有竞争力。

Conclusion: 所提出的基于置信度分数的TTS方法有效，能在减少资源消耗的同时保证性能。

Abstract: Confidence in LLMs is a useful indicator of model uncertainty and answer
reliability. Existing work mainly focused on single-turn scenarios, while
research on confidence in complex multi-turn interactions is limited. In this
paper, we investigate whether LLM-based search agents have the ability to
communicate their own confidence through verbalized confidence scores after
long sequences of actions, a significantly more challenging task compared to
outputting confidence in a single interaction. Experimenting on open-source
agentic models, we first find that models exhibit much higher task accuracy at
high confidence while having near-zero accuracy when confidence is low. Based
on this observation, we propose Test-Time Scaling (TTS) methods that use
confidence scores to determine answer quality, encourage the model to try again
until reaching a satisfactory confidence level. Results show that our proposed
methods significantly reduce token consumption while demonstrating competitive
performance compared to baseline fixed budget TTS methods.

</details>


### [511] [Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts](https://arxiv.org/abs/2510.23464)
*Nikesh Gyawali,Doina Caragea,Alex Vasenkov,Cornelia Caragea*

Main category: cs.CL

TL;DR: 本文引入聚焦债务、每股收益和销售三个核心财务指标的句子级语料库，评估大语言模型不同提示策略，发现少样本思维链提示表现最佳，凸显大语言模型在金融领域无需大量标注数据进行特定目标立场检测的可行性。


<details>
  <summary>Details</summary>
Motivation: 金融叙述分析困难，且之前金融领域情感分析需大量昂贵标注数据，难以进行句子级特定金融目标立场检测。

Method: 从10 - K年报和季度财报电话会议记录中提取句子，用ChatGPT - o3 - pro模型在严格人工验证下标注立场，对大语言模型采用零样本、少样本和思维链提示策略进行系统评估。

Result: 少样本思维链提示相比监督基线表现最佳，大语言模型在SEC和ECT数据集上表现有差异。

Conclusion: 在金融领域利用大语言模型进行特定目标立场检测无需大量标注数据具有实际可行性。

Abstract: Financial narratives from U.S. Securities and Exchange Commission (SEC)
filing reports and quarterly earnings call transcripts (ECTs) are very
important for investors, auditors, and regulators. However, their length,
financial jargon, and nuanced language make fine-grained analysis difficult.
Prior sentiment analysis in the financial domain required a large, expensive
labeled dataset, making the sentence-level stance towards specific financial
targets challenging. In this work, we introduce a sentence-level corpus for
stance detection focused on three core financial metrics: debt, earnings per
share (EPS), and sales. The sentences were extracted from Form 10-K annual
reports and ECTs, and labeled for stance (positive, negative, neutral) using
the advanced ChatGPT-o3-pro model under rigorous human validation. Using this
corpus, we conduct a systematic evaluation of modern large language models
(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting
strategies. Our results show that few-shot with CoT prompting performs best
compared to supervised baselines, and LLMs' performance varies across the SEC
and ECT datasets. Our findings highlight the practical viability of leveraging
LLMs for target-specific stance in the financial domain without requiring
extensive labeled data.

</details>


### [512] [Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models](https://arxiv.org/abs/2510.23585)
*Luis Ramos,Hiram Calvo,Olga Kolesnikova*

Main category: cs.CL

TL;DR: 评估传统机器学习模型和微调的Transformer模型在希望演讲识别任务上的表现，Transformer模型效果更好。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台有检测激励性表达的需求，希望演讲识别成为有前景的NLP任务。

Method: 对已划分好训练集、开发集和测试集的希望演讲数据集，评估传统机器学习模型和微调的Transformer模型。

Result: 开发测试中，线性核SVM和逻辑回归宏观F1为0.78，RBF核SVM为0.77，朴素贝叶斯为0.75；Transformer模型效果更好，最佳模型加权精确率0.82、召回率0.80、F1为0.79，宏观F1为0.79，准确率0.80。

Conclusion: 优化配置的传统机器学习模型有灵活性，Transformer架构能检测细微语义，大Transformer和大语言模型在小数据集上可能表现更好。

Abstract: The identification of hope speech has become a promised NLP task, considering
the need to detect motivational expressions of agency and goal-directed
behaviour on social media platforms. This proposal evaluates traditional
machine learning models and fine-tuned transformers for a previously split hope
speech dataset as train, development and test set. On development test, a
linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM
with RBF kernel reached 0.77, and Na\"ive Bayes hit 0.75. Transformer models
delivered better results, the best model achieved weighted precision of 0.82,
weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80
accuracy. These results suggest that while optimally configured traditional
machine learning models remain agile, transformer architectures detect some
subtle semantics of hope to achieve higher precision and recall in hope speech
detection, suggesting that larges transformers and LLMs could perform better in
small datasets.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [513] [An AI enhanced approach to the tree unimodality conjecture](https://arxiv.org/abs/2510.18826)
*Eric Ramos,Sunny Sun*

Main category: math.CO

TL;DR: 本文利用PatternBoost架构训练机器寻找树的独立序列对数凹性猜想的反例，找到数万个新反例并讨论了失败情况。


<details>
  <summary>Details</summary>
Motivation: 此前对数凹性猜想多年未被解决，仅有2023年找到2个26顶点树的非对数凹反例，作者希望寻找更多反例。

Method: 使用PatternBoost AI架构训练机器来寻找反例。

Result: 找到了顶点集大小从27到101的数万个对数凹性的新反例。

Conclusion: 介绍了使用PatternBoost架构寻找反例的成功与失败情况。

Abstract: Given a graph $G$, its independence sequence is the integral sequence
$a_1,a_2,...,a_n$, where $a_i$ is the number of independent sets of vertices of
size i. In the late 80's Alavi, Erdos, Malde, Schwenk showed that this sequence
need not be unimodal for general graphs, but conjectured that it is always
unimodal whenever $G$ is a tree. This conjecture was then naturally generalized
to claim that the independence sequence of trees should be log concave, in the
sense that $a_i^2$ is always above $a_{i-1}a_{i+1}$. This conjecture stood for
many years, until in 2023, Kadrawi, Levit, Yosef, and Mizrachi proved that
there were exactly two trees on 26 vertices whose independence sequence was not
log concave. In this paper, we use the AI architecture PatternBoost, developed
by Charton, Ellenberg, Wagner, and Williamson to train a machine to find
counter-examples to the log-concavity conjecture. We will discuss the successes
of this approach - finding tens of thousands of new counter-examples to
log-concavity with vertex set sizes varying from 27 to 101 - and some of its
fascinating failures.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [514] [Step2Motion: Locomotion Reconstruction from Pressure Sensing Insoles](https://arxiv.org/abs/2510.22712)
*Jose Luis Ponton,Eduardo Alvarado,Lin Geng Foo,Nuria Pelechano,Carlos Andujar,Marc Habermann*

Main category: cs.GR

TL;DR: 提出 Step2Motion 方法，利用多模态鞋垫传感器数据重建人类运动，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有运动重建方法未充分利用可穿戴鞋垫设备，为填补这一空白开展研究。

Method: 提出 Step2Motion 方法，利用鞋垫捕获的压力、惯性数据（加速度和角速度）重建人类运动。

Result: 通过一系列实验评估，证明该方法对多种运动风格具有通用性。

Conclusion: Step2Motion 是首个利用多模态鞋垫传感器重建人类运动的有效方法。

Abstract: Human motion is fundamentally driven by continuous physical interaction with
the environment. Whether walking, running, or simply standing, the forces
exchanged between our feet and the ground provide crucial insights for
understanding and reconstructing human movement. Recent advances in wearable
insole devices offer a compelling solution for capturing these forces in
diverse, real-world scenarios. Sensor insoles pose no constraint on the users'
motion (unlike mocap suits) and are unaffected by line-of-sight limitations (in
contrast to optical systems). These qualities make sensor insoles an ideal
choice for robust, unconstrained motion capture, particularly in outdoor
environments. Surprisingly, leveraging these devices with recent motion
reconstruction methods remains largely unexplored. Aiming to fill this gap, we
present Step2Motion, the first approach to reconstruct human locomotion from
multi-modal insole sensors. Our method utilizes pressure and inertial
data-accelerations and angular rates-captured by the insoles to reconstruct
human motion. We evaluate the effectiveness of our approach across a range of
experiments to show its versatility for diverse locomotion styles, from simple
ones like walking or jogging up to moving sideways, on tiptoes, slightly
crouching, or dancing.

</details>


### [515] [Environment-aware Motion Matching](https://arxiv.org/abs/2510.22632)
*Jose Luis Ponton,Sheldon Andrews,Carlos Andujar,Nuria Pelechano*

Main category: cs.GR

TL;DR: 提出环境感知运动匹配实时系统用于全身角色动画，可适应动态环境和人群场景。


<details>
  <summary>Details</summary>
Motivation: 传统角色动画技术难以处理任意情况，现有动态选择动画方法在环境交互和人群行为动画上有局限，身体动画与轨迹规划常分离。

Method: 预处理时从动作捕捉数据库提取形状、姿势和轨迹特征，运行时进行高效搜索，匹配用户输入和当前姿势并惩罚与动态环境的碰撞。

Result: 使角色能自然调整姿势和轨迹以在拥挤场景中导航。

Conclusion: 所提出的环境感知运动匹配系统能解决现有角色动画技术的局限，实现更好的角色动画效果。

Abstract: Interactive applications demand believable characters that respond naturally
to dynamic environments. Traditional character animation techniques often
struggle to handle arbitrary situations, leading to a growing trend of
dynamically selecting motion-captured animations based on predefined features.
While Motion Matching has proven effective for locomotion by aligning to target
trajectories, animating environment interactions and crowd behaviors remains
challenging due to the need to consider surrounding elements. Existing
approaches often involve manual setup or lack the naturalism of motion capture.
Furthermore, in crowd animation, body animation is frequently treated as a
separate process from trajectory planning, leading to inconsistencies between
body pose and root motion. To address these limitations, we present
Environment-aware Motion Matching, a novel real-time system for full-body
character animation that dynamically adapts to obstacles and other agents,
emphasizing the bidirectional relationship between pose and trajectory. In a
preprocessing step, we extract shape, pose, and trajectory features from a
motion capture database. At runtime, we perform an efficient search that
matches user input and current pose while penalizing collisions with a dynamic
environment. Our method allows characters to naturally adjust their pose and
trajectory to navigate crowded scenes.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [516] [Bridging the Perceptual-Statistical Gap in Dysarthria Assessment: Why Machine Learning Still Falls Short](https://arxiv.org/abs/2510.22237)
*Krishna Gurugubelli*

Main category: eess.AS

TL;DR: 本文分析语音自动构音障碍检测和严重程度评估模型与人类专家性能差距的原因，提出缩小差距的策略和实验方案。


<details>
  <summary>Details</summary>
Motivation: 尽管声学建模和深度学习发展迅速，但模型性能仍不及人类专家，需分析差距原因并寻找解决办法。

Method: 详细分析人类专家感知过程，调研机器学习表示和方法，回顾现有特征集和建模策略文献，对标签噪声和评分者间变异性的限制进行理论分析。

Result: 指出“感知 - 统计差距”，并提出缩小差距的策略，如感知驱动特征、自监督预训练等。

Conclusion: 提出与临床目标一致的实验方案和评估指标，以指导未来研究开发临床可靠且可解释的构音障碍评估工具。

Abstract: Automated dysarthria detection and severity assessment from speech have
attracted significant research attention due to their potential clinical
impact. Despite rapid progress in acoustic modeling and deep learning, models
still fall short of human expert performance. This manuscript provides a
comprehensive analysis of the reasons behind this gap, emphasizing a conceptual
divergence we term the ``perceptual-statistical gap''. We detail human expert
perceptual processes, survey machine learning representations and methods,
review existing literature on feature sets and modeling strategies, and present
a theoretical analysis of limits imposed by label noise and inter-rater
variability. We further outline practical strategies to narrow the gap,
perceptually motivated features, self-supervised pretraining, ASR-informed
objectives, multimodal fusion, human-in-the-loop training, and explainability
methods. Finally, we propose experimental protocols and evaluation metrics
aligned with clinical goals to guide future research toward clinically reliable
and interpretable dysarthria assessment tools.

</details>


### [517] [Treble10: A high-quality dataset for far-field speech recognition, dereverberation, and enhancement](https://arxiv.org/abs/2510.23141)
*Sarabeth S. Mullins,Georg Götz,Eric Bezzam,Steven Zheng,Daniel Gert Nielsen*

Main category: eess.AS

TL;DR: 提出大规模物理精确的房间声学数据集Treble10，弥合测量与模拟间的真实感差距，用于远场语音任务。


<details>
  <summary>Details</summary>
Motivation: 当前数据集在声学真实感和可扩展性间存在权衡，测量语料成本高、覆盖低且缺乏配对数据，模拟数据集无法重现关键物理现象。

Method: 使用Treble SDK中的混合模拟范式，结合基于波和几何声学求解器，在10个全装修真实房间模拟超3000个宽带房间脉冲响应。

Result: 生成包含6个互补子集的Treble10数据集，提供多种RIR及预卷积混响语音场景，信号模拟准确。

Conclusion: Treble10数据集公开可用，可作为基准和模板推动下一代模拟驱动的音频研究。

Abstract: Accurate far-field speech datasets are critical for tasks such as automatic
speech recognition (ASR), dereverberation, speech enhancement, and source
separation. However, current datasets are limited by the trade-off between
acoustic realism and scalability. Measured corpora provide faithful physics but
are expensive, low-coverage, and rarely include paired clean and reverberant
data. In contrast, most simulation-based datasets rely on simplified
geometrical acoustics, thus failing to reproduce key physical phenomena like
diffraction, scattering, and interference that govern sound propagation in
complex environments. We introduce Treble10, a large-scale, physically accurate
room-acoustic dataset. Treble10 contains over 3000 broadband room impulse
responses (RIRs) simulated in 10 fully furnished real-world rooms, using a
hybrid simulation paradigm implemented in the Treble SDK that combines a
wave-based and geometrical acoustics solver. The dataset provides six
complementary subsets, spanning mono, 8th-order Ambisonics, and 6-channel
device RIRs, as well as pre-convolved reverberant speech scenes paired with
LibriSpeech utterances. All signals are simulated at 32 kHz, accurately
modelling low-frequency wave effects and high-frequency reflections. Treble10
bridges the realism gap between measurement and simulation, enabling
reproducible, physically grounded evaluation and large-scale data augmentation
for far-field speech tasks. The dataset is openly available via the Hugging
Face Hub, and is intended as both a benchmark and a template for
next-generation simulation-driven audio research.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [518] [A Free Probabilistic Framework for Denoising Diffusion Models: Entropy, Transport, and Reverse Processes](https://arxiv.org/abs/2510.22778)
*Swagatam Das*

Main category: math.PR

TL;DR: 本文在自由概率框架下为基于扩散的生成建模构建了严格框架，连接现代扩散模型与自由熵的信息几何。


<details>
  <summary>Details</summary>
Motivation: 为基于扩散的生成建模在自由概率设置下建立严格框架，处理算子值或高维结构化数据的生成建模。

Method: 将经典去噪扩散概率模型扩展到自由扩散过程，利用自由随机分析工具如自由Malliavin微积分和Clark - Ocone表示推导反向随机微分方程，在自由Wasserstein空间中建立变分公式。

Result: 正向动力学满足自由Fokker - Planck方程，增加自由熵并耗散自由Fisher信息；得到由共轭变量驱动的反向随机微分方程；变分流的梯度流结构收敛到半圆平衡律。

Conclusion: 连接了现代扩散模型与自由熵的信息几何，为算子值或高维结构化数据的生成建模奠定了数学基础。

Abstract: This work develops a rigorous framework for diffusion-based generative
modeling in the setting of free probability. We extend classical denoising
diffusion probabilistic models to free diffusion processes -- stochastic
dynamics acting on noncommutative random variables whose spectral measures
evolve by free additive convolution. The forward dynamics satisfy a free
Fokker--Planck equation that increases Voiculescu's free entropy and dissipates
free Fisher information, providing a noncommutative analogue of the classical
de Bruijn identity. Using tools from free stochastic analysis, including a free
Malliavin calculus and a Clark--Ocone representation, we derive the
reverse-time stochastic differential equation driven by the conjugate variable,
the free analogue of the score function. We further develop a variational
formulation of these flows in the free Wasserstein space, showing that the
resulting gradient-flow structure converges to the semicircular equilibrium
law. Together, these results connect modern diffusion models with the
information geometry of free entropy and establish a mathematical foundation
for generative modeling with operator-valued or high-dimensional structured
data.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [519] [Approximate Gradient Coding for Distributed Learning with Heterogeneous Stragglers](https://arxiv.org/abs/2510.22539)
*Heekang Song,Wan Choi*

Main category: eess.SY

TL;DR: 提出最优结构梯度编码方案缓解分布式学习掉队问题，有方法推导与策略提出，数值结果显示优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统梯度编码方法在处理异构系统掉队问题时存在假设均一性或数据复制过多的局限

Method: 通过显式考虑个体掉队概率构建最小化残差误差且保证无偏梯度估计的优化问题，利用拉格朗日对偶和凸优化推导最优编解码系数，提出数据分配策略，并分析收敛行为

Result: 数值结果表明该方法相比现有方法显著降低掉队影响并加速收敛

Conclusion: 所提最优结构梯度编码方案能有效缓解分布式学习中的掉队问题，提升性能

Abstract: In this paper, we propose an optimally structured gradient coding scheme to
mitigate the straggler problem in distributed learning. Conventional gradient
coding methods often assume homogeneous straggler models or rely on excessive
data replication, limiting performance in real-world heterogeneous systems. To
address these limitations, we formulate an optimization problem minimizing
residual error while ensuring unbiased gradient estimation by explicitly
considering individual straggler probabilities. We derive closed-form solutions
for optimal encoding and decoding coefficients via Lagrangian duality and
convex optimization, and propose data allocation strategies that reduce both
redundancy and computation load. We also analyze convergence behavior for
$\lambda$-strongly convex and $\mu$-smooth loss functions. Numerical results
show that our approach significantly reduces the impact of stragglers and
accelerates convergence compared to existing methods.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [520] [AI-Enhanced Operator Assistance for UNICOS Applications](https://arxiv.org/abs/2510.21717)
*Bernard Tam,Jean-Charles Tournier,Fernando Varela Rodriguez*

Main category: cs.HC

TL;DR: 本文为CERN的UNICOS系统开发AI增强操作员助手，设计实现多智能体系统，初步评估显示能减少人工工作量等，为智能操作员界面发展提供基础。


<details>
  <summary>Details</summary>
Motivation: UNICOS系统存在解码小部件认知负担、根因分析需人工、维护人员追踪数据点元素困难等问题，在需及时响应场景下会增加认知负荷和减慢诊断速度。

Method: 设计并实现多智能体系统，采用模块化架构，包括用CTRL代码编写的UNICOS端扩展、部署在虚拟机上基于Python的多智能体系统和存储操作员文档与小部件动画代码的向量数据库。

Result: 系统能够解码小部件、利用实时设备数据和文档进行根因分析、在复杂代码库中追踪数据点元素，减少了操作员和维护人员的手动工作量，增强了操作态势感知，加快了对警报和异常的响应。

Conclusion: 该工作不仅是概念验证，为CERN推进智能操作员界面提供基础，结合模块化设计等减轻当前操作员痛点，也指出了加速器操作中辅助AI的更广泛机会。

Abstract: This project explores the development of an AI-enhanced operator assistant
for UNICOS, CERN's UNified Industrial Control System. While powerful, UNICOS
presents a number of challenges, including the cognitive burden of decoding
widgets, manual effort required for root cause analysis, and difficulties
maintainers face in tracing datapoint elements (DPEs) across a complex
codebase. In situations where timely responses are critical, these challenges
can increase cognitive load and slow down diagnostics. To address these issues,
a multi-agent system was designed and implemented. The solution is supported by
a modular architecture comprising a UNICOS-side extension written in CTRL code,
a Python-based multi-agent system deployed on a virtual machine, and a vector
database storing both operator documentation and widget animation code.
Preliminary evaluations suggest that the system is capable of decoding widgets,
performing root cause analysis by leveraging live device data and
documentation, and tracing DPEs across a complex codebase. Together, these
capabilities reduce the manual workload of operators and maintainers, enhance
situational awareness in operations, and accelerate responses to alarms and
anomalies. Beyond these immediate gains, this work highlights the potential of
introducing multi-modal reasoning and retrieval augmented generation (RAG) into
the domain of industrial control. Ultimately, this work represents more than a
proof of concept: it provides a basis for advancing intelligent operator
interfaces at CERN. By combining modular design, extensibility, and practical
AI integration, this project not only alleviates current operator pain points
but also points toward broader opportunities for assistive AI in accelerator
operations.

</details>


### [521] [Beyond IVR Touch-Tones: Customer Intent Routing using LLMs](https://arxiv.org/abs/2510.21715)
*Sergio Rojas-Galeano*

Main category: cs.HC

TL;DR: 本文提出基于大语言模型的方法解决IVR系统意图路由问题，评估不同提示设计，结果显示扁平路径准确率更高，证明LLMs可实现更流畅IVR路由。


<details>
  <summary>Details</summary>
Motivation: 传统按键式IVR系统令人沮丧，需要更直接直观的语言交互，而大语言模型在意图路由上有潜力，但受数据稀缺限制。

Method: 使用三个不同模型合成23节点IVR结构，生成920个用户意图，评估描述性分层菜单和扁平路径表示两种提示设计。

Result: 扁平路径准确率更高，在基础数据集上达89.13%，数据增强引入语言噪声使性能略有下降，低性能路由可能反映菜单设计冗余。

Conclusion: 证明大语言模型能实现更流畅的IVR路由，推动客服超越按键菜单。

Abstract: Widespread frustration with rigid touch-tone Interactive Voice Response (IVR)
systems for customer service underscores the need for more direct and intuitive
language interaction. While speech technologies are necessary, the key
challenge lies in routing intents from user phrasings to IVR menu paths, a task
where Large Language Models (LLMs) show strong potential. Progress, however, is
limited by data scarcity, as real IVR structures and interactions are often
proprietary. We present a novel LLM-based methodology to address this gap.
Using three distinct models, we synthesized a realistic 23-node IVR structure,
generated 920 user intents (230 base and 690 augmented), and performed the
routing task. We evaluate two prompt designs: descriptive hierarchical menus
and flattened path representations, across both base and augmented datasets.
Results show that flattened paths consistently yield higher accuracy, reaching
89.13% on the base dataset compared to 81.30% with the descriptive format,
while augmentation introduces linguistic noise that slightly reduces
performance. Confusion matrix analysis further suggests that low-performing
routes may reflect not only model limitations but also redundancies in menu
design. Overall, our findings demonstrate proof-of-concept that LLMs can enable
IVR routing through a smoother, more seamless user experience -- moving
customer service one step ahead of touch-tone menus.

</details>


### [522] [GAMER PAT: Research as a Serious Game](https://arxiv.org/abs/2510.21719)
*Kenji Saito,Rei Tadika*

Main category: cs.HC

TL;DR: 论文介绍GAMER PAT将研究论文写作变为严肃游戏，分析26+聊天记录得出四阶段支架模式，表明其支持写作多方面，还给出展望。


<details>
  <summary>Details</summary>
Motivation: 在生成式AI优于学生学术写作背景下，探讨如何保留新手研究人员的动力、创造力和智力成长。

Method: 引入GAMER PAT聊天机器人，通过角色扮演机制让用户与NPC互动，使用SCAT进行定性日志分析。

Result: 确定了四阶段支架模式：提问、元视角、结构化和递归反思。

Conclusion: GAMER PAT支持研究写作的结构发展以及反思和激励方面，提供设计和使用描述报告，为未来实证研究提供展望。

Abstract: As generative AI increasingly outperforms students in producing academic
writing, a critical question arises: how can we preserve the motivation,
creativity, and intellectual growth of novice researchers in an age of
automated academic achievement? This paper introduces GAMER PAT (GAme MastER,
Paper Authoring Tutor), a prompt-engineered AI chatbot that reframes research
paper writing as a serious game. Through role-playing mechanics, users interact
with a co-author NPC and anonymous reviewer NPCs, turning feedback into
"missions" and advancing through a narrative-driven writing process.
  Our study reports on 26+ gameplay chat logs, including both autoethnography
and use by graduate students under supervision. Using qualitative log analysis
with SCAT (Steps for Coding and Theorization), we identified an emergent
four-phase scaffolding pattern: (1) question posing, (2) meta-perspective, (3)
structuring, and (4) recursive reflection. These results suggest that GAMER PAT
supports not only the structural development of research writing but also
reflective and motivational aspects.
  We present this work as a descriptive account of concept and process, not a
causal evaluation. We also include a speculative outlook envisioning how humans
may continue to cultivate curiosity and agency alongside AI-driven research.
This arXiv version thus provides both a descriptive report of design and usage,
and a forward-looking provocation for future empirical studies.

</details>


### [523] [AquaVLM: Improving Underwater Situation Awareness with Mobile Vision Language Models](https://arxiv.org/abs/2510.21722)
*Beitong Tian,Lingzhi Zhao,Bo Chen,Haozhen Zheng,Jingcheng Yang,Mingyuan Wu,Deepak Vasisht,Klara Nahrstedt*

Main category: cs.HC

TL;DR: 本文提出水下通信系统AquaVLM，用智能手机自动生成上下文感知消息，评估显示其有效且有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统水下通信系统笨重昂贵，新系统文本消息预定义，限制特定上下文通信，需更好的水下通信方案。

Method: 提出AquaVLM系统，用微调的移动视觉语言模型，采用分层消息生成管道，协同设计模型与传输，开发VR模拟器，在iOS平台创建原型。

Result: 主观和客观评估验证了AquaVLM的有效性。

Conclusion: AquaVLM在个人水下通信及更广泛的移动视觉语言模型应用方面有潜力。

Abstract: Underwater activities like scuba diving enable millions annually to explore
marine environments for recreation and scientific research. Maintaining
situational awareness and effective communication are essential for diver
safety. Traditional underwater communication systems are often bulky and
expensive, limiting their accessibility to divers of all levels. While recent
systems leverage lightweight smartphones and support text messaging, the
messages are predefined and thus restrict context-specific communication.
  In this paper, we present AquaVLM, a tap-and-send underwater communication
system that automatically generates context-aware messages and transmits them
using ubiquitous smartphones. Our system features a mobile vision-language
model (VLM) fine-tuned on an auto-generated underwater conversation dataset and
employs a hierarchical message generation pipeline. We co-design the VLM and
transmission, incorporating error-resilient fine-tuning to improve the system's
robustness to transmission errors. We develop a VR simulator to enable users to
experience AquaVLM in a realistic underwater environment and create a fully
functional prototype on the iOS platform for real-world experiments. Both
subjective and objective evaluations validate the effectiveness of AquaVLM and
highlight its potential for personal underwater communication as well as
broader mobile VLM applications.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [524] [Adaptive Split-MMD Training for Small-Sample Cross-Dataset P300 EEG Classification](https://arxiv.org/abs/2510.21969)
*Weiyu Chen,Arnaud Delorme*

Main category: eess.SP

TL;DR: 研究在小样本情况下从两个公开视觉oddball ERP数据集进行迁移学习检测单试次P300，提出AS - MMD方法，表现优于目标训练和合并训练。


<details>
  <summary>Details</summary>
Motivation: 在少量标记试次下检测单试次P300困难，迁移学习存在跨数据集偏移问题。

Method: 引入Adaptive Split Maximum Mean Discrepancy Training (AS - MMD)，结合目标加权损失、Split Batch Normalization和无参数的logit级径向基函数核最大均值差异项。

Result: 在两个迁移方向上，AS - MMD在两个数据集上的准确率和AUC表现均优于目标训练和合并训练，且提升显著。

Conclusion: AS - MMD方法有效，其三个组成部分都对性能提升有贡献。

Abstract: Detecting single-trial P300 from EEG is difficult when only a few labeled
trials are available. When attempting to boost a small target set with a large
source dataset through transfer learning, cross-dataset shift arises. To
address this challenge, we study transfer between two public visual-oddball ERP
datasets using five shared electrodes (Fz, Pz, P3, P4, Oz) under a strict
small-sample regime (target: 10 trials/subject; source: 80 trials/subject). We
introduce Adaptive Split Maximum Mean Discrepancy Training (AS-MMD), which
combines (i) a target-weighted loss with warm-up tied to the square root of the
source/target size ratio, (ii) Split Batch Normalization (Split-BN) with shared
affine parameters and per-domain running statistics, and (iii) a parameter-free
logit-level Radial Basis Function kernel Maximum Mean Discrepancy (RBF-MMD)
term using the median-bandwidth heuristic. Implemented on an EEG Conformer,
AS-MMD is backbone-agnostic and leaves the inference-time model unchanged.
Across both transfer directions, it outperforms target-only and pooled training
(Active Visual Oddball: accuracy/AUC 0.66/0.74; ERP CORE P3: 0.61/0.65), with
gains over pooling significant under corrected paired t-tests. Ablations
attribute improvements to all three components.

</details>


### [525] [Genetic Optimization of a Software-Defined GNSS Receiver](https://arxiv.org/abs/2510.22417)
*Laura Train,Rodrigo Castellanos,Miguel Gómez-López*

Main category: eess.SP

TL;DR: 介绍基于遗传算法的优化框架，在SDR环境验证，结果表明可使SDR接收器在不同动态条件下保持鲁棒准确的PVT解。


<details>
  <summary>Details</summary>
Motivation: 商用现货GNSS接收器在高动态条件有局限，传统跟踪环路带宽无法应对同步参数快速变化，手动调整SDR接收器难以确保最优性能。

Method: 引入基于遗传算法的优化框架，自主探索接收器配置空间确定最优环路参数。

Result: 在SDR环境用模拟GPS L1信号验证，不同情况有相应最大位置和速度误差。

Conclusion: 进化优化使SDR接收器在不同动态条件下保持鲁棒准确的PVT解。

Abstract: Commercial off-the-shelf (COTS) Global Navigation Satellite System (GNSS)
receivers face significant limitations under high-dynamic conditions,
particularly in high-acceleration environments such as those experienced by
launch vehicles. These performance degradations, often observed as
discontinuities in the navigation solution, arise from the inability of
traditional tracking loop bandwidths to cope with rapid variations in
synchronization parameters. Software-Defined Radio (SDR) receivers overcome
these constraints by enabling flexible reconfiguration of tracking loops;
however, manual tuning involves a complex, multidimensional search and seldom
ensures optimal performance. This work introduces a genetic algorithm-based
optimization framework that autonomously explores the receiver configuration
space to determine optimal loop parameters for phase, frequency, and delay
tracking. The approach is validated within an SDR environment using
realistically simulated GPS L1 signals for three representative dynamic regimes
-guided rocket flight, Low Earth Orbit (LEO) satellite, and static
receiver-processed with the open-source GNSS-SDR architecture. Results
demonstrate that evolutionary optimization enables SDR receivers to maintain
robust and accurate Position, Velocity, and Time (PVT) solutions across diverse
dynamic conditions. The optimized configurations yielded maximum position and
velocity errors of approximately 6 m and 0.08 m/s for the static case, 12 m and
2.5 m/s for the rocket case, and 5 m and 0.2 m/s for the LEO case.

</details>


### [526] [PASS-Enhanced MEC: Joint Optimization of Task Offloading and Uplink PASS Beamforming](https://arxiv.org/abs/2510.22948)
*Zhaoming Hu,Ruikang Zhong,Xidong Mu,Dengao Li,Yuanwei Liu*

Main category: eess.SP

TL;DR: 研究PASS增强的MEC架构提升动态无线环境任务卸载效率与延迟性能，用DRL解决优化问题并提出LBPPO算法，仿真显示其性能更优。


<details>
  <summary>Details</summary>
Motivation: 提升动态无线环境中任务卸载效率和延迟性能，解决高频MEC系统路径损耗和信号阻塞问题。

Method: 将网络延迟最小化问题建模为MDP，用DRL方法求解，提出LBPPO算法。

Result: PASS增强的MEC在自适应上行PASS波束赋形下比固定PA基线和传统MIMO辅助MEC有更强收敛能力，尤其在大量UE或高发射功率场景。

Conclusion: PASS增强的MEC架构结合LBPPO算法能有效提升动态无线环境下任务卸载性能。

Abstract: A pinching-antenna system (PASS)-enhanced mobile edge computing (MEC)
architecture is investigated to improve the task offloading efficiency and
latency performance in dynamic wireless environments. By leveraging dielectric
waveguides and flexibly adjustable pinching antennas, PASS establishes
short-distance line-of-sight (LoS) links while effectively mitigating the
significant path loss and potential signal blockage, making it a promising
solution for high-frequency MEC systems. We formulate a network latency
minimization problem to joint optimize uplink PASS beamforming and task
offloading. The resulting problem is modeled as a Markov decision process (MDP)
and solved via the deep reinforcement learning (DRL) method. To address the
instability introduced by the $\max$ operator in the objective function, we
propose a load balancing-aware proximal policy optimization (LBPPO) algorithm.
LBPPO incorporates both node-level and waveguide-level load balancing
information into the policy design, maintaining computational and transmission
delay equilibrium, respectively. Simulation results demonstrate that the
proposed PASS-enhanced MEC with adaptive uplink PASS beamforming exhibit
stronger convergence capability than fixed-PA baselines and conventional
MIMO-assisted MEC, especially in scenarios with a large number of UEs or high
transmit power.

</details>


### [527] [Clinic-Oriented Feasibility of a Sensor-Fused Wearable for Upper-Limb Function](https://arxiv.org/abs/2510.22913)
*Thanyanee Srichaisak,Arissa Ieochai,Aueaphum Aueawattthanaphisut*

Main category: eess.SP

TL;DR: 本文评估可穿戴设备技术可行性，健康成人实验显示该设备能降低震颤、提高运动质量和效率，支持开展患者研究。


<details>
  <summary>Details</summary>
Motivation: 上肢无力和震颤影响日常生活及康复依从性，评估针对特定部位的传感器融合可穿戴设备的技术可行性和临床相关信号。

Method: 使用轻量级节点集成多种传感器，结合设备端推理和安全辅助策略，让12名健康成人进行三项类日常生活活动任务，分析多项指标。

Result: 设备辅助降低震颤、增加活动范围和重复次数，设备延迟低，所有实验顺利完成且无不良事件。

Conclusion: 多模态传感和安全辅助在技术可行性试验中改善运动质量和效率，支持开展患者研究。

Abstract: Background: Upper-limb weakness and tremor (4--12 Hz) limit activities of
daily living (ADL) and reduce adherence to home rehabilitation. Objective: To
assess technical feasibility and clinician-relevant signals of a sensor-fused
wearable targeting the triceps brachii and extensor pollicis brevis. Methods: A
lightweight node integrates surface EMG (1 kHz), IMU (100--200 Hz), and
flex/force sensors with on-device INT8 inference (Tiny 1D-CNN/Transformer) and
a safety-bounded assist policy (angle/torque/jerk limits; stall/time-out).
Healthy adults (n = 12) performed three ADL-like tasks. Primary outcomes:
Tremor Index (TI), range of motion (ROM), repetitions (Reps min$^{-1}$).
Secondary: EMG median-frequency slope (fatigue trend), closed-loop latency,
session completion, and device-related adverse events. Analyses used
subject-level paired medians with BCa 95\% CIs; exact Wilcoxon $p$-values are
reported in the Results. Results: Assistance was associated with lower tremor
prominence and improved task throughput: TI decreased by $-0.092$ (95\% CI
[$-0.102$, $-0.079$]), ROM increased by $+12.65\%$ (95\% CI [$+8.43$,
$+13.89$]), and Reps rose by $+2.99$ min$^{-1}$ (95\% CI [$+2.61$, $+3.35$]).
Median on-device latency was 8.7 ms at a 100 Hz loop rate; all sessions were
completed with no device-related adverse events. Conclusions: Multimodal
sensing with low-latency, safety-bounded assistance produced improved movement
quality (TI $\downarrow$) and throughput (ROM, Reps $\uparrow$) in a pilot
technical-feasibility setting, supporting progression to IRB-approved patient
studies. Trial registration: Not applicable (pilot non-clinical).

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [528] [Heaven & Hell II: Scale Laws and Robustness in One-Step Heaven-Hell Consensus](https://arxiv.org/abs/2510.21950)
*Nnamdi Daniel Aghanya,Romain Leemans*

Main category: cs.SI

TL;DR: 研究Heaven - Hell动力学网络共识模型，开发使收敛阈值更鲁棒的缩放定律和操作改进，贡献包括新视角、策略等，证明用Coq机械化，实验验证结果。


<details>
  <summary>Details</summary>
Motivation: 让已知的单均匀中心系统的一步收敛阈值对多种情况更具鲁棒性。

Method: 开发缩放定律和操作改进，用Coq机械化证明。

Result: 得到了守恒律视角、参数化平局策略、更紧的逐点边界等成果。

Conclusion: 实验验证了结果的紧密性和差距闭合。

Abstract: We study Heaven-Hell dynamics, a model for network consensus. A known result
establishes an exact one-step convergence threshold for systems with a single
uniform hub: the per-node inbound hub weight W suffices if and only if W >=
maxrest, the maximum non-hub inbound mass. We develop scale laws and
operational refinements that make this threshold robust to tie-breaking
policies, node-specific tolerances, targeted seeding, multiple hubs, and
asynchronous updates. Our contributions include a conservation-law perspective,
parameterized tie policies, tighter pointwise bounds improving on classical
worst-case guarantees, one-pass fairness for asynchronous updates, and
sufficient conditions for seeded convergence. All proofs are mechanized in Coq,
with experiments on rings, grids, scale-free graphs, and heterogeneous weighted
graphs validating tightness and gap closures

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [529] [STAR-RIS-assisted Collaborative Beamforming for Low-altitude Wireless Networks](https://arxiv.org/abs/2510.22108)
*Xinyue Liang,Hui Kang,Junwei Che,Jiahui Li,Geng Sun,Qingqing Wu,Jiacheng Wang,Dusit Niyato*

Main category: cs.NI

TL;DR: 本文针对低空无线网络信号衰减问题，提出联合速率和能量优化问题，设计HMCD优化框架，仿真表明其性能优于基线，且系统传输率与无人机数量和STAR - RIS元素数量正相关。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络在密集环境中面临严重信号衰减问题，需要提升信号质量和方向性。

Method: 提出异构多智能体协作动态（HMCD）优化框架，包含基于模拟退火的STAR - RIS控制方法和改进的多智能体深度强化学习控制方法。

Result: HMCD在收敛速度、平均传输速率和能耗方面优于各种基线，系统平均传输速率与无人机数量和STAR - RIS元素数量正相关。

Conclusion: HMCD优化框架能有效解决低空无线网络信号衰减问题，提升系统传输速率并降低能耗。

Abstract: While low-altitude wireless networks (LAWNs) based on uncrewed aerial
vehicles (UAVs) offer high mobility, flexibility, and coverage for urban
communications, they face severe signal attenuation in dense environments due
to obstructions. To address this critical issue, we consider introducing
collaborative beamforming (CB) of UAVs and omnidirectional reconfigurable
beamforming (ORB) of simultaneous transmitting and reflecting reconfigurable
intelligent surfaces (STAR-RIS) to enhance the signal quality and
directionality. On this basis, we formulate a joint rate and energy
optimization problem (JREOP) to maximize the transmission rate of the overall
system, while minimizing the energy consumption of the UAV swarm. Due to the
non-convex and NP-hard nature of JREOP, we propose a heterogeneous multi-agent
collaborative dynamic (HMCD) optimization framework, which has two core
components. The first component is a simulated annealing (SA)-based STAR-RIS
control method, which dynamically optimizes reflection and transmission
coefficients to enhance signal propagation. The second component is an improved
multi-agent deep reinforcement learning (MADRL) control method, which
incorporates a self-attention evaluation mechanism to capture interactions
between UAVs and an adaptive velocity transition mechanism to enhance training
stability. Simulation results demonstrate that HMCD outperforms various
baselines in terms of convergence speed, average transmission rate, and energy
consumption. Further analysis reveals that the average transmission rate of the
overall system scales positively with both UAV count and STAR-RIS element
numbers.

</details>


### [530] [When UAV Swarm Meets IRS: Collaborative Secure Communications in Low-altitude Wireless Networks](https://arxiv.org/abs/2510.22117)
*Jiahui Li,Xinyue Liang,Geng Sun,Hui Kang,Jiacheng Wang,Dusit Niyato,Shiwen Mao,Abbas Jamalipour*

Main category: cs.NI

TL;DR: 针对低空无线网络安全漏洞，提出含虚拟天线阵列和智能反射面的安全通信框架，经异构多智能体控制方法优化，仿真显示其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络面临已知和未知窃听者带来的安全漏洞，威胁数据机密性和系统完整性。

Method: 提出安全通信框架，将问题转化为异构马尔可夫决策过程，采用异构多智能体控制方法，结合智能反射面控制策略和多智能体软演员 - 评论家框架。

Result: 所提异构多智能体控制方法在保密速率提升、旁瓣抑制和能源效率方面优于基线方法，且无人机数量增加时，虚拟天线阵列和智能反射面的协同能提供强大安全保障。

Conclusion: 所提框架和方法能有效解决低空无线网络的安全问题，提升网络性能。

Abstract: Low-altitude wireless networks (LAWNs) represent a promising architecture
that integrates unmanned aerial vehicles (UAVs) as aerial nodes to provide
enhanced coverage, reliability, and throughput for diverse applications.
However, these networks face significant security vulnerabilities from both
known and potential unknown eavesdroppers, which may threaten data
confidentiality and system integrity. To solve this critical issue, we propose
a novel secure communication framework for LAWNs where the selected UAVs within
a swarm function as a virtual antenna array (VAA), complemented by intelligent
reflecting surface (IRS) to create a robust defense against eavesdropping
attacks. Specifically, we formulate a multi-objective optimization problem that
simultaneously maximizes the secrecy rate while minimizing the maximum sidelobe
level and total energy consumption, requiring joint optimization of UAV
excitation current weights, flight trajectories, and IRS phase shifts. This
problem presents significant difficulties due to the dynamic nature of the
system and heterogeneous components. Thus, we first transform the problem into
a heterogeneous Markov decision process (MDP). Then, we propose a heterogeneous
multi-agent control approach (HMCA) that integrates a dedicated IRS control
policy with a multi-agent soft actor-critic framework for UAV control, which
enables coordinated operation across heterogeneous network elements. Simulation
results show that the proposed HMCA achieves superior performance compared to
baseline approaches in terms of secrecy rate improvement, sidelobe suppression,
and energy efficiency. Furthermore, we find that the collaborative and passive
beamforming synergy between VAA and IRS creates robust security guarantees when
the number of UAVs increases.

</details>


### [531] [HandPass: A Wi-Fi CSI Palm Authentication Approach for Access Control](https://arxiv.org/abs/2510.22133)
*Eduardo Fabricio Gomes Trindade,Felipe Silveira de Almeida,Gioliano de Oliveira Braga,Rafael Pimenta de Mattos Paixão,Pedro Henrique dos Santos Rocha,Lourenco Alves Pereira Jr*

Main category: cs.NI

TL;DR: 本文提出用Wi-Fi CSI数据进行手掌识别的生物特征认证新方法，实验表明随机森林分类器效果佳，凸显Wi-Fi CSI用于用户认证潜力。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi CSI在用户认证的实际应用待探索，研究旨在用其进行手掌识别实现生物特征认证。

Method: 用树莓派采集20名参与者右手CSI数据，用MinMax缩放归一化数据集，分析手的生物物理特征对电磁信号影响，评估五种分类算法。

Result: 随机森林分类器用10折交叉验证平均F1分数达99.82%，每次捕获会话每秒记录约1000个数据包。

Conclusion: Wi-Fi CSI在基于手掌生物特征数据开发强大可靠的用户认证系统方面有潜力。

Abstract: Wi-Fi Channel State Information (CSI) has been extensively studied for
sensing activities. However, its practical application in user authentication
still needs to be explored. This study presents a novel approach to biometric
authentication using Wi-Fi Channel State Information (CSI) data for palm
recognition. The research delves into utilizing a Raspberry Pi encased in a
custom-built box with antenna power reduced to 1dBm, which was used to capture
CSI data from the right hands of 20 participants (10 men and 10 women). The
dataset was normalized using MinMax scaling to ensure uniformity and accuracy.
By focusing on biophysical aspects such as hand size, shape, angular spread
between fingers, and finger phalanx lengths, among other characteristics, the
study explores how these features affect electromagnetic signals, which are
then reflected in Wi-Fi CSI, allowing for precise user identification. Five
classification algorithms were evaluated, with the Random Forest classifier
achieving an average F1-Score of 99.82\% using 10-fold cross-validation.
Amplitude and Phase data were used, with each capture session recording
approximately 1000 packets per second in five 5-second intervals for each User.
This high accuracy highlights the potential of Wi-Fi CSI in developing robust
and reliable user authentication systems based on palm biometric data.

</details>


### [532] [NetBurst: Event-Centric Forecasting of Bursty, Intermittent Time Series](https://arxiv.org/abs/2510.22397)
*Satyandra Guthula,Jaber Daneshamooz,Charles Fleming,Ashish Kundu,Walter Willinger,Arpit Gupta*

Main category: cs.NI

TL;DR: 提出NetBurst框架用于网络遥测时间序列预测，相比基线模型降低误差，凸显利用Mandelbrot研究的好处。


<details>
  <summary>Details</summary>
Motivation: 现有基准时间序列数据预测方法不适用于网络遥测时间序列，且现代AI架构对这类序列预测研究不足。

Method: 引入以事件为中心的NetBurst框架，用基于分位数的码本和双自回归器将预测问题转化为预测突发时间和大小。

Result: 在大规模生产网络遥测时间序列上，NetBurst比Chronos等基线模型降低MASE 13 - 605倍，保留突发性，嵌入聚类效果好5倍。

Conclusion: 现代AI利用Mandelbrot的研究成果在突发、间歇和重尾制度下进行预测有显著益处，对高风险决策有重要价值。

Abstract: Forecasting on widely used benchmark time series data (e.g., ETT,
Electricity, Taxi, and Exchange Rate, etc.) has favored smooth, seasonal
series, but network telemetry time series -- traffic measurements at service,
IP, or subnet granularity -- are instead highly bursty and intermittent, with
heavy-tailed bursts and highly variable inactive periods. These properties
place the latter in the statistical regimes made famous and popularized more
than 20 years ago by B.~Mandelbrot. Yet forecasting such time series with
modern-day AI architectures remains underexplored. We introduce NetBurst, an
event-centric framework that reformulates forecasting as predicting when bursts
occur and how large they are, using quantile-based codebooks and dual
autoregressors. Across large-scale sets of production network telemetry time
series and compared to strong baselines, such as Chronos, NetBurst reduces Mean
Average Scaled Error (MASE) by 13--605x on service-level time series while
preserving burstiness and producing embeddings that cluster 5x more cleanly
than Chronos. In effect, our work highlights the benefits that modern AI can
reap from leveraging Mandelbrot's pioneering studies for forecasting in bursty,
intermittent, and heavy-tailed regimes, where its operational value for
high-stakes decision making is of paramount interest.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [533] [Simopt-Power: Leveraging Simulation Metadata for Low-Power Design Synthesis](https://arxiv.org/abs/2510.21745)
*Eashan Wadhwa,Shanker Shreejith*

Main category: cs.AR

TL;DR: 提出Simopt - power框架，利用仿真分析优化FPGA，减少动态功耗。


<details>
  <summary>Details</summary>
Motivation: 现代FPGA中过度的开关活动导致动态功耗大，传统低功耗技术有局限性。

Method: 提出Simopt - power框架，利用仿真分析识别和选择性重新配置高翻转路径，运用香农分解原理插入重复真值表逻辑并重新定位关键网络。

Result: 在开源RTLLM基准测试中，算术设计平均开关功耗降低约9%，仅增加约9%的LUT等效资源。

Conclusion: 将仿真见解与针对性优化相结合可降低动态功耗，为在FPGA - CAD流程中使用仿真元数据提供了实用途径。

Abstract: Excessive switching activity is a primary contributor to dynamic power
dissipation in modern FPGAs, where fine-grained configurability amplifies
signal toggling and associated capacitance. Conventional low-power techniques
-- gating, clock-domain partitioning, and placement-aware netlist rewrites -
either require intrusive design changes or offer diminishing returns as device
densities grow. In this work, we present Simopt-power, a simulator-driven
optimisation framework that leverages simulation analysis to identify and
selectively reconfigure high-toggle paths. By feeding activity profiles back
into a lightweight transformation pass, Simopt-power judiciously inserts
duplicate truth table logic using Shannon Decomposition principle and relocates
critical nets, thereby attenuating unnecessary transitions without perturbing
functional behaviour. We evaluated this framework on open-source RTLLM
benchmark, with Simopt-power achieves an average switching-induced power
reduction of ~9\% while incurring only ~9\% additional LUT-equivalent resources
for arithmetic designs. These results demonstrate that coupling simulation
insights with targeted optimisations can yield a reduced dynamic power,
offering a practical path toward using simulation metadata in the FPGA-CAD
flow.

</details>


### [534] [RAMAN: Resource-efficient ApproxiMate Posit Processing for Algorithm-Hardware Co-desigN](https://arxiv.org/abs/2510.22627)
*Mohd Faisal Khan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: 提出资源高效的近似posit(8,2)乘法累加架构RAMAN，结合算法硬件协同设计框架，在FPGA和ASIC平台验证有显著硬件效率提升，兼顾学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI应用在资源受限环境下提升计算效率的挑战。

Method: 设计REAP MAC引擎，将其集成到可扩展向量执行单元，提出算法 - 硬件协同设计框架。

Result: REAP MAC在LUT、面积和功耗上分别相比基线设计节省46%、35.66%、31.28%，手写数字识别保持98.45%的高精度。

Conclusion: RAMAN在硬件效率和学习性能间取得良好平衡，适用于下一代边缘智能。

Abstract: Edge-AI applications still face considerable challenges in enhancing
computational efficiency in resource-constrained environments. This work
presents RAMAN, a resource-efficient and approximate posit(8,2)-based
Multiply-Accumulate (MAC) architecture designed to improve hardware efficiency
within bandwidth limitations. The proposed REAP (Resource-Efficient Approximate
Posit) MAC engine, which is at the core of RAMAN, uses approximation in the
posit multiplier to achieve significant area and power reductions with an
impact on accuracy. To support diverse AI workloads, this MAC unit is
incorporated in a scalable Vector Execution Unit (VEU), which permits hardware
reuse and parallelism among deep neural network layers. Furthermore, we propose
an algorithm-hardware co-design framework incorporating approximation-aware
training to evaluate the impact of hardware-level approximation on
application-level performance. Empirical validation on FPGA and ASIC platforms
shows that the proposed REAP MAC achieves up to 46% in LUT savings and 35.66%
area, 31.28% power reduction, respectively, over the baseline Posit Dot-Product
Unit (PDPU) design, while maintaining high accuracy (98.45%) for handwritten
digit recognition. RAMAN demonstrates a promising trade-off between hardware
efficiency and learning performance, making it suitable for next-generation
edge intelligence.

</details>


### [535] [QuArch: A Benchmark for Evaluating LLM Reasoning in Computer Architecture](https://arxiv.org/abs/2510.22087)
*Shvetank Prakash,Andrew Cheng,Arya Tschand,Mark Mazumder,Varun Gohil,Jeffrey Ma,Jason Yik,Zishen Wan,Jessica Quaye,Elisavet Lydia Alvanaki,Avinash Kumar,Chandrashis Mazumdar,Tuhin Khare,Alexander Ingare,Ikechukwu Uchendu,Radhika Ghosal,Abhishek Tyagi,Chenyu Wang,Andrea Mattia Garavagno,Sarah Gu,Alice Guo,Grace Hur,Luca Carloni,Tushar Krishna,Ankita Nayak,Amir Yazdanbakhsh,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: 提出QuArch基准测试，评估大语言模型在计算机架构方面的知识和推理能力，发现前沿模型存在高阶思维技能不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估中缺少计算机架构领域，需要开发相关基准测试。

Method: 创建包含2671个经过专家验证的问答对的QuArch基准测试。

Result: 前沿模型有领域特定知识，但在计算机架构高阶思维技能上有困难，不同模型在高级问题上准确率差异大。

Conclusion: QuArch能为构建和衡量大语言模型能力提供基础，推动计算系统创新，是社区设定大语言模型架构推理评估标准的努力。

Abstract: The field of computer architecture, which bridges high-level software
abstractions and low-level hardware implementations, remains absent from
current large language model (LLM) evaluations. To this end, we present QuArch
(pronounced 'quark'), the first benchmark designed to facilitate the
development and evaluation of LLM knowledge and reasoning capabilities
specifically in computer architecture. QuArch provides a comprehensive
collection of 2,671 expert-validated question-answer (QA) pairs covering
various aspects of computer architecture, including processor design, memory
systems, and interconnection networks. Our evaluation reveals that while
frontier models possess domain-specific knowledge, they struggle with skills
that require higher-order thinking in computer architecture. Frontier model
accuracies vary widely (from 34% to 72%) on these advanced questions,
highlighting persistent gaps in architectural reasoning across analysis,
design, and implementation QAs. By holistically assessing fundamental skills,
QuArch provides a foundation for building and measuring LLM capabilities that
can accelerate innovation in computing systems. With over 140 contributors from
40 institutions, this benchmark represents a community effort to set the
standard for architectural reasoning in LLM evaluation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [536] [It Takes Two to Tango: Two Parallel Samplers Improve Quality in Diffusion Models for Limited Steps](https://arxiv.org/abs/2510.21802)
*Pedro Cisneros-Velarde*

Main category: cs.CV

TL;DR: 研究有限去噪步骤下，两个并行处理器或采样器可提升采样图像质量，方法简单且经测试，还发现简单整合信息及增加并行采样器不一定提升质量。


<details>
  <summary>Details</summary>
Motivation: 考虑有限去噪步骤下提升采样图像质量的方法。

Method: 让两个采样器在连续时间进行去噪步骤，并将信息适当整合到潜图像中，方法简单、即插即用、模型无关且无需额外微调或外部模型。

Result: 通过自动和人工评估测试方法，发现简单整合两个采样器信息会降低样本质量，增加并行采样器不一定提升样本质量。

Conclusion: 在有限去噪步骤下，两个并行采样器适当整合信息可提升采样图像质量，简单整合及增加并行采样器不一定有积极效果。

Abstract: We consider the situation where we have a limited number of denoising steps,
i.e., of evaluations of a diffusion model. We show that two parallel processors
or samplers under such limitation can improve the quality of the sampled image.
Particularly, the two samplers make denoising steps at successive times, and
their information is appropriately integrated in the latent image. Remarkably,
our method is simple both conceptually and to implement: it is plug-&-play,
model agnostic, and does not require any additional fine-tuning or external
models. We test our method with both automated and human evaluations for
different diffusion models. We also show that a naive integration of the
information from the two samplers lowers sample quality. Finally, we find that
adding more parallel samplers does not necessarily improve sample quality.

</details>


### [537] [A Multi-Stage Hybrid Framework for Automated Interpretation of Multi-View Engineering Drawings Using Vision Language Model](https://arxiv.org/abs/2510.21862)
*Muhammad Tayyab Khan,Zane Yong,Lequn Chen,Wenhe Feng,Nicholas Yew Jin Tan,Seung Ki Moon*

Main category: cs.CV

TL;DR: 本文提出三阶段混合框架自动解读二维多视图工程图纸，开发专门数据集测试，结果良好，输出可集成到CAD和制造数据库。


<details>
  <summary>Details</summary>
Motivation: 手动方法、通用OCR系统和传统深度学习方法难以解读复杂多视图工程图纸，因其布局、方向多样，内容包含符号和文本。

Method: 提出三阶段混合框架，分别用YOLOv11 - det进行布局分割、YOLOv11 - obb进行注释检测、两个基于Donut的无OCR的VLM进行语义内容解析，还开发两个专门数据集。

Result: Alphabetical VLM的F1分数为0.672，Numerical VLM的F1分数为0.963，统一JSON输出可无缝集成到CAD和制造数据库。

Conclusion: 该框架为智能工程图纸分析提供可扩展解决方案。

Abstract: Engineering drawings are fundamental to manufacturing communication, serving
as the primary medium for conveying design intent, tolerances, and production
details. However, interpreting complex multi-view drawings with dense
annotations remains challenging using manual methods, generic optical character
recognition (OCR) systems, or traditional deep learning approaches, due to
varied layouts, orientations, and mixed symbolic-textual content. To address
these challenges, this paper proposes a three-stage hybrid framework for the
automated interpretation of 2D multi-view engineering drawings using modern
detection and vision language models (VLMs). In the first stage, YOLOv11-det
performs layout segmentation to localize key regions such as views, title
blocks, and notes. The second stage uses YOLOv11-obb for orientation-aware,
fine-grained detection of annotations, including measures, GD&T symbols, and
surface roughness indicators. The third stage employs two Donut-based, OCR-free
VLMs for semantic content parsing: the Alphabetical VLM extracts textual and
categorical information from title blocks and notes, while the Numerical VLM
interprets quantitative data such as measures, GD&T frames, and surface
roughness. Two specialized datasets were developed to ensure robustness and
generalization: 1,000 drawings for layout detection and 1,406 for
annotation-level training. The Alphabetical VLM achieved an overall F1 score of
0.672, while the Numerical VLM reached 0.963, demonstrating strong performance
in textual and quantitative interpretation, respectively. The unified JSON
output enables seamless integration with CAD and manufacturing databases,
providing a scalable solution for intelligent engineering drawing analysis.

</details>


### [538] [Open Multimodal Retrieval-Augmented Factual Image Generation](https://arxiv.org/abs/2510.22521)
*Yang Tian,Fan Liu,Jingyuan Zhang,Wei Bi,Yupeng Hu,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出ORIG框架用于事实图像生成，构建FIG - Eval基准，实验表明ORIG能提升事实一致性和图像质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成图像时易与可验证知识矛盾，传统检索增强方法因依赖静态源和浅层证据整合，无法基于准确且不断发展的知识进行生成。

Method: 引入ORIG框架，迭代检索和过滤网络多模态证据，将精炼知识集成到增强提示中以指导生成；构建FIG - Eval基准进行系统评估。

Result: 实验显示ORIG相比强基线显著提高了事实一致性和整体图像质量。

Conclusion: 开放多模态检索在事实图像生成方面具有潜力。

Abstract: Large Multimodal Models (LMMs) have achieved remarkable progress in
generating photorealistic and prompt-aligned images, but they often produce
outputs that contradict verifiable knowledge, especially when prompts involve
fine-grained attributes or time-sensitive events. Conventional
retrieval-augmented approaches attempt to address this issue by introducing
external information, yet they are fundamentally incapable of grounding
generation in accurate and evolving knowledge due to their reliance on static
sources and shallow evidence integration. To bridge this gap, we introduce
ORIG, an agentic open multimodal retrieval-augmented framework for Factual
Image Generation (FIG), a new task that requires both visual realism and
factual grounding. ORIG iteratively retrieves and filters multimodal evidence
from the web and incrementally integrates the refined knowledge into enriched
prompts to guide generation. To support systematic evaluation, we build
FIG-Eval, a benchmark spanning ten categories across perceptual, compositional,
and temporal dimensions. Experiments demonstrate that ORIG substantially
improves factual consistency and overall image quality over strong baselines,
highlighting the potential of open multimodal retrieval for factual image
generation.

</details>


### [539] [Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.22694)
*Shu Zhao,Tianyi Shen,Nilesh Ahuja,Omesh Tickoo,Vijaykrishnan Narayanan*

Main category: cs.CV

TL;DR: 提出Windsock模块、DANCE指令调优和自评估方法，提升多模态检索增强生成质量并减少检索次数。


<details>
  <summary>Details</summary>
Motivation: 现有MRAG方法存在静态检索策略、模态选择不灵活和信息利用不佳等问题，导致三个关键挑战。

Method: 引入Windsock模块决定检索必要性和模态选择；提出DANCE指令调优自适应训练策略；采用自评估方法将问答数据集转换为MRAG训练数据集。

Result: 所提方法将生成质量显著提升17.07%，同时减少8.95%的检索次数。

Conclusion: 所提方法有效解决现有MRAG方法的问题，提升生成质量并减少检索次数。

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising
method to generate factual and up-to-date responses of Multimodal Large
Language Models (MLLMs) by incorporating non-parametric knowledge from external
knowledge bases. However, existing MRAG approaches suffer from static retrieval
strategies, inflexible modality selection, and suboptimal utilization of
retrieved information, leading to three critical challenges: determining when
to retrieve, what modality to incorporate, and how to utilize retrieved
information effectively. To address these challenges, we introduce Windsock, a
query-dependent module making decisions on retrieval necessity and modality
selection, effectively reducing computational overhead and improving response
quality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction
Tuning, an adaptive training strategy that enhances MLLMs' ability to utilize
retrieved information while maintaining robustness against noise. Moreover, we
adopt a self-assessment approach leveraging knowledge within MLLMs to convert
question-answering datasets to MRAG training datasets. Extensive experiments
demonstrate that our proposed method significantly improves the generation
quality by 17.07% while reducing 8.95% retrieval times.

</details>


### [540] [Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment](https://arxiv.org/abs/2510.23224)
*Hongyi Wang,Zhengjie Zhu,Jiabo Ma,Fang Wang,Yue Shi,Bo Luo,Jili Wang,Qiuyu Cai,Xiuming Zhang,Yen-Wei Chen,Lanfen Lin,Hao Chen*

Main category: cs.CV

TL;DR: 提出PathSearch检索框架用于数字病理切片检索，经多数据集评估效果优于传统框架，提升病理诊断准确性等。


<details>
  <summary>Details</summary>
Motivation: 全切片图像（WSIs）检索因规模大及难捕捉语义差异而具挑战性，需有效检索方法支持病理诊断等。

Method: 提出PathSearch框架，统一细粒度注意力镶嵌表示与全局幻灯片嵌入，通过视觉 - 语言对比学习对齐，在6926个切片 - 报告对上训练。

Result: 在多个公共和内部数据集上评估，优于传统图像检索框架，多中心读者研究表明提升诊断准确性、信心和观察者间一致性。

Conclusion: PathSearch是数字病理学可扩展且通用的检索解决方案。

Abstract: The rapid digitization of histopathology slides has opened up new
possibilities for computational tools in clinical and research workflows. Among
these, content-based slide retrieval stands out, enabling pathologists to
identify morphologically and semantically similar cases, thereby supporting
precise diagnoses, enhancing consistency across observers, and assisting
example-based education. However, effective retrieval of whole slide images
(WSIs) remains challenging due to their gigapixel scale and the difficulty of
capturing subtle semantic differences amid abundant irrelevant content. To
overcome these challenges, we present PathSearch, a retrieval framework that
unifies fine-grained attentive mosaic representations with global-wise slide
embeddings aligned through vision-language contrastive learning. Trained on a
corpus of 6,926 slide-report pairs, PathSearch captures both fine-grained
morphological cues and high-level semantic patterns to enable accurate and
flexible retrieval. The framework supports two key functionalities: (1)
mosaic-based image-to-image retrieval, ensuring accurate and efficient slide
research; and (2) multi-modal retrieval, where text queries can directly
retrieve relevant slides. PathSearch was rigorously evaluated on four public
pathology datasets and three in-house cohorts, covering tasks including
anatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,
and grading across diverse organs such as breast, lung, kidney, liver, and
stomach. External results show that PathSearch outperforms traditional
image-to-image retrieval frameworks. A multi-center reader study further
demonstrates that PathSearch improves diagnostic accuracy, boosts confidence,
and enhances inter-observer agreement among pathologists in real clinical
scenarios. These results establish PathSearch as a scalable and generalizable
retrieval solution for digital pathology.

</details>


### [541] [Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models](https://arxiv.org/abs/2510.21740)
*Alexa R. Tartaglini,Satchel Grant,Daniel Wurgaft,Christopher Potts,Judith E. Fan*

Main category: cs.CV

TL;DR: 本文开发FUGU工具研究VLMs在数据可视化理解任务上失败原因，发现错误源于视觉 - 语言交接，且模型存在架构限制。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在数据可视化理解任务上表现不佳，需明确失败原因。

Method: 开发FUGU任务套件，用激活修补和线性探针研究三种常用VLMs信息流动。

Result: 部分模型无法正确生成数据点坐标，正确坐标能提升部分任务表现，微调无法达最优。

Conclusion: 当前VLMs存在架构限制，给可靠数据可视化理解带来挑战。

Abstract: Data visualizations are vital components of many scientific articles and news
stories. Current vision-language models (VLMs) still struggle on basic data
visualization understanding tasks, but the causes of failure remain unclear.
Are VLM failures attributable to limitations in how visual information in the
data visualization is encoded, how information is transferred between the
vision and language modules, or how information is processed within the
language module? We developed FUGU, a suite of data visualization understanding
tasks, to precisely characterize potential sources of difficulty (e.g.,
extracting the position of data points, distances between them, and other
summary statistics). We used FUGU to investigate three widely used VLMs. To
diagnose the sources of errors produced by these models, we used activation
patching and linear probes to trace information flow through models across a
variety of prompting strategies. We found that some models fail to generate the
coordinates of individual data points correctly, and these initial errors often
lead to erroneous final responses. When these models are provided with the
correct coordinates, performance improves substantially. Moreover, even when
the model generates an incorrect response, the correct coordinates can be
successfully read out from the latent representations in the vision encoder,
suggesting that the source of these errors lies in the vision-language handoff.
We further found that while providing correct coordinates helps with tasks
involving one or a small number of data points, it generally worsens
performance for tasks that require extracting statistical relationships across
many data points. Fine-tuning models on FUGU also fails to yield ceiling
performance. These findings point to architectural constraints in current VLMs
that might pose significant challenges for reliable data visualization
understanding.

</details>


### [542] [Proportion and Perspective Control for Flow-Based Image Generation](https://arxiv.org/abs/2510.21763)
*Julien Boudier,Hugo Caselles-Dupré*

Main category: cs.CV

TL;DR: 提出两种用于艺术控制的ControlNets，可控制图像空间和几何结构，实验证明有效但有局限，模型已发布。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像扩散模型对输出的空间和几何结构控制有限，需要改进。

Method: 引入比例ControlNet（用边界框控制物体位置和比例）和透视ControlNet（用消失线控制场景3D几何），用视觉语言模型标注和专用算法支持训练。

Result: 两个模块都能有效控制图像，但在复杂约束下有局限性。

Conclusion: 提出的两种ControlNets能改善图像空间和几何结构控制，但还需在复杂约束场景下进一步优化。

Abstract: While modern text-to-image diffusion models generate high-fidelity images,
they offer limited control over the spatial and geometric structure of the
output. To address this, we introduce and evaluate two ControlNets specialized
for artistic control: (1) a proportion ControlNet that uses bounding boxes to
dictate the position and scale of objects, and (2) a perspective ControlNet
that employs vanishing lines to control the 3D geometry of the scene. We
support the training of these modules with data pipelines that leverage
vision-language models for annotation and specialized algorithms for
conditioning image synthesis. Our experiments demonstrate that both modules
provide effective control but exhibit limitations with complex constraints.
Both models are released on HuggingFace:
https://huggingface.co/obvious-research

</details>


### [543] [OCR-Quality: A Human-Annotated Dataset for OCR Quality Assessment](https://arxiv.org/abs/2510.21774)
*Yulong Zhang*

Main category: cs.CV

TL;DR: 介绍OCR - Quality数据集，含1000页PDF转PNG图像，有手动标注质量分，公开可用。


<details>
  <summary>Details</summary>
Motivation: 满足现实应用中对可靠OCR质量评估的需求，为OCR验证系统提供基准。

Method: 从多样现实场景采样1000页PDF转300 DPI PNG图像，用先进VLMs处理，以4级评分系统手动标注质量分。

Result: 创建了OCR - Quality数据集，包含详细信息和不同难度案例。

Conclusion: OCR - Quality数据集可用于评估和开发OCR质量评估方法，且公开可获取。

Abstract: We present OCR-Quality, a comprehensive human-annotated dataset designed for
evaluating and developing OCR quality assessment methods. The dataset consists
of 1,000 PDF pages converted to PNG images at 300 DPI, sampled from diverse
real-world scenarios, including academic papers, textbooks, e-books, and
multilingual documents. Each document has been processed using state-of-the-art
Vision-Language Models (VLMs) and manually annotated with quality scores using
a 4-level scoring system (1: Excellent, 2: Good, 3: Fair, 4: Poor). The dataset
includes detailed source information, annotation guidelines, and representative
cases across various difficulty levels. OCR-Quality addresses the critical need
for reliable OCR quality assessment in real-world applications and provides a
valuable benchmark for training and evaluating OCR verification systems. The
dataset is publicly available at
https://huggingface.co/datasets/Aslan-mingye/OCR-Quality .

</details>


### [544] [Face-MakeUpV2: Facial Consistency Learning for Controllable Text-to-Image Generation](https://arxiv.org/abs/2510.21775)
*Dawei Dai,Yinxiu Zhou,Chenghang Li,Guolai Jiang,Chengfang Zhang*

Main category: cs.CV

TL;DR: 提出面部图像生成模型Face - MakeUpV2，构建大规模数据集，采用预训练模型并引入信息注入通道和优化目标，实验证明其在保留面部ID和物理一致性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决当前文本到图像模型在面部图像生成中对局部语义指令响应时存在的面部属性泄漏和物理一致性不足问题。

Method: 1. 构建大规模数据集FaceCaptionMask - 1M；2. 以通用文本到图像预训练模型为骨干，引入3D面部渲染通道和全局面部特征通道；3. 制定语义对齐和感知损失两个优化目标进行监督学习。

Result: Face - MakeUpV2在保留面部ID和维持参考图像物理一致性方面整体性能最佳。

Conclusion: Face - MakeUpV2在不同应用的可靠且可控的面部编辑方面有实际潜力。

Abstract: In facial image generation, current text-to-image models often suffer from
facial attribute leakage and insufficient physical consistency when responding
to local semantic instructions. In this study, we propose Face-MakeUpV2, a
facial image generation model that aims to maintain the consistency of face ID
and physical characteristics with the reference image. First, we constructed a
large-scale dataset FaceCaptionMask-1M comprising approximately one million
image-text-masks pairs that provide precise spatial supervision for the local
semantic instructions. Second, we employed a general text-to-image pretrained
model as the backbone and introduced two complementary facial information
injection channels: a 3D facial rendering channel to incorporate the physical
characteristics of the image and a global facial feature channel. Third, we
formulated two optimization objectives for the supervised learning of our
model: semantic alignment in the model's embedding space to mitigate the
attribute leakage problem and perceptual loss on facial images to preserve ID
consistency. Extensive experiments demonstrated that our Face-MakeUpV2 achieves
best overall performance in terms of preserving face ID and maintaining
physical consistency of the reference images. These results highlight the
practical potential of Face-MakeUpV2 for reliable and controllable facial
editing in diverse applications.

</details>


### [545] [Bridging Accuracy and Interpretability: Deep Learning with XAI for Breast Cancer Detection](https://arxiv.org/abs/2510.21780)
*Bishal Chhetri,B. V. Rathish Kumar*

Main category: cs.CV

TL;DR: 提出可解释深度学习框架用于乳腺癌早期检测，模型性能超文献基准，结合可解释AI技术增强可解释性，发现细胞核凹点特征对分类影响大。


<details>
  <summary>Details</summary>
Motivation: 实现乳腺癌早期检测，解决深度学习模型因黑盒特性难以临床应用的问题。

Method: 构建使用ReLU激活、Adam优化器和二元交叉熵损失的深度神经网络，与多种成熟算法对比，结合SHAP和LIME等可解释AI技术。

Result: 深度神经网络取得高分类性能，准确率0.992、精确率1.000、召回率0.977、F1分数0.988，优于其他算法；发现细胞核凹点特征对分类影响最大。

Conclusion: 该框架结合高性能与可解释性，有助于乳腺癌诊断和治疗。

Abstract: In this study, we present an interpretable deep learning framework for the
early detection of breast cancer using quantitative features extracted from
digitized fine needle aspirate (FNA) images of breast masses. Our deep neural
network, using ReLU activations, the Adam optimizer, and a binary cross-entropy
loss, delivers state-of-the-art classification performance, achieving an
accuracy of 0.992, precision of 1.000, recall of 0.977, and an F1 score of
0.988. These results substantially exceed the benchmarks reported in the
literature. We evaluated the model under identical protocols against a suite of
well-established algorithms (logistic regression, decision trees, random
forests, stochastic gradient descent, K-nearest neighbors, and XGBoost) and
found the deep model consistently superior on the same metrics. Recognizing
that high predictive accuracy alone is insufficient for clinical adoption due
to the black-box nature of deep learning models, we incorporated model-agnostic
Explainable AI techniques such as SHAP and LIME to produce feature-level
attributions and human-readable visualizations. These explanations quantify the
contribution of each feature to individual predictions, support error analysis,
and increase clinician trust, thus bridging the gap between performance and
interpretability for real-world clinical use. The concave points feature of the
cell nuclei is found to be the most influential feature positively impacting
the classification task. This insight can be very helpful in improving the
diagnosis and treatment of breast cancer by highlighting the key
characteristics of breast tumor.

</details>


### [546] [EdgeSync: Accelerating Edge-Model Updates for Data Drift through Adaptive Continuous Learning](https://arxiv.org/abs/2510.21781)
*Runchu Donga,Peng Zhao,Guiqin Wang,Nan Qi,Jie Lin*

Main category: cs.CV

TL;DR: 实时视频分析系统在边缘设备部署轻量级模型，但数据特征分布变化致模型准确率下降。现有方法有计算密集和新模型与数据分布不匹配问题，提出EdgeSync方法，评估显示其比现有方法和传统方法提升准确率。


<details>
  <summary>Details</summary>
Motivation: 解决实时视频分析系统中，因数据特征分布变化导致模型准确率下降，以及现有边缘模型更新方法存在的计算密集、新模型与数据分布不匹配问题。

Method: 引入EdgeSync方法，增强样本过滤，结合及时性和推理结果；采用动态训练管理模块优化模型更新的时间和顺序。

Result: 在多样化复杂的真实世界数据集上评估，EdgeSync比现有方法准确率提高约3.4%，比传统方法提高约10%。

Conclusion: EdgeSync方法能够有效解决现有边缘模型更新方法的问题，提升模型的准确率。

Abstract: Real-time video analytics systems typically deploy lightweight models on edge
devices to reduce latency. However, the distribution of data features may
change over time due to various factors such as changing lighting and weather
conditions, leading to decreased model accuracy. Recent frameworks try to
address this issue by leveraging remote servers to continuously train and adapt
lightweight edge models using more complex models in the cloud. Despite these
advancements, existing methods face two key challenges: first, the retraining
process is compute-intensive, causing significant delays in model updates;
second, the new model may not align well with the evolving data distribution of
the current video stream. To address these challenges, we introduce EdgeSync,
an efficient edge-model updating approach that enhances sample filtering by
incorporating timeliness and inference results, thus ensuring training samples
are more relevant to the current video content while reducing update delays.
Additionally, EdgeSync features a dynamic training management module that
optimizes the timing and sequencing of model updates to improve their
timeliness. Evaluations on diverse and complex real-world datasets demonstrate
that EdgeSync improves accuracy by approximately 3.4% compared to existing
methods and by about 10% compared to traditional approaches.

</details>


### [547] [Noise Aggregation Analysis Driven by Small-Noise Injection: Efficient Membership Inference for Diffusion Models](https://arxiv.org/abs/2510.21783)
*Guo Li,Yuyang Yu,Xuemiao Xu*

Main category: cs.CV

TL;DR: 提出针对扩散模型的高效成员推理攻击方法，基于注入轻微噪声和评估噪声分布聚合度，在多数据集表现优且有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型广泛使用带来隐私风险，成员推理攻击是关键问题，需提出有效攻击方法。

Method: 注入轻微噪声到测试图像，基于训练集和非训练集样本噪声预测模式差异，分析模型预测的噪声分布聚合度来确定成员身份。

Result: 在多个数据集上表现优越，面对大规模文本到图像扩散模型时在ASR和AUC上攻击效果更好。

Conclusion: 所提方法高效且具有可扩展性，能有效对扩散模型进行成员推理攻击。

Abstract: Diffusion models have demonstrated powerful performance in generating
high-quality images. A typical example is text-to-image generator like Stable
Diffusion. However, their widespread use also poses potential privacy risks. A
key concern is membership inference attacks, which attempt to determine whether
a particular data sample was used in the model training process. We propose an
efficient membership inference attack method against diffusion models. This
method is based on the injection of slight noise and the evaluation of the
aggregation degree of the noise distribution. The intuition is that the noise
prediction patterns of diffusion models for training set samples and
non-training set samples exhibit distinguishable differences.Specifically, we
suppose that member images exhibit higher aggregation of predicted noise around
a certain time step of the diffusion process. In contrast, the predicted noises
of non-member images exhibit a more discrete characteristic around the certain
time step. Compared with other existing methods, our proposed method requires
fewer visits to the target diffusion model. We inject slight noise into the
image under test and then determine its membership by analyzing the aggregation
degree of the noise distribution predicted by the model. Empirical findings
indicate that our method achieves superior performance across multiple
datasets. At the same time, our method can also show better attack effects in
ASR and AUC when facing large-scale text-to-image diffusion models, proving the
scalability of our method.

</details>


### [548] [EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction](https://arxiv.org/abs/2510.21786)
*Qile Su,Shoutai Zhu,Shuai Zhang,Baoyu Liang,Chao Tong*

Main category: cs.CV

TL;DR: 本文提出AVEP任务，构建相关结构化数据集，并提出EventFormer模型，实验表明该方法优于其他视频预测模型，将发布数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 人类事件多以视频形式记录，但视觉领域脚本事件归纳相关研究缺乏，现有视频预测任务逻辑和语义信息不够丰富。

Method: 引入AVEP任务，构建含约35K标注视频和超178K视频片段的结构化数据集，提出基于节点图分层注意力的EventFormer模型。

Result: 使用多个SOTA视频预测模型和LVLMs在AVEP上实验，证明任务复杂性和数据集价值，该方法优于其他视频预测模型。

Conclusion: 提出的任务、数据集和模型有效，将发布相关资源便于复现。

Abstract: Script event induction, which aims to predict the subsequent event based on
the context, is a challenging task in NLP, achieving remarkable success in
practical applications. However, human events are mostly recorded and presented
in the form of videos rather than scripts, yet there is a lack of related
research in the realm of vision. To address this problem, we introduce AVEP
(Action-centric Video Event Prediction), a task that distinguishes itself from
existing video prediction tasks through its incorporation of more complex logic
and richer semantic information. We present a large structured dataset, which
consists of about $35K$ annotated videos and more than $178K$ video clips of
event, built upon existing video event datasets to support this task. The
dataset offers more fine-grained annotations, where the atomic unit is
represented as a multimodal event argument node, providing better structured
representations of video events. Due to the complexity of event structures,
traditional visual models that take patches or frames as input are not
well-suited for AVEP. We propose EventFormer, a node-graph hierarchical
attention based video event prediction model, which can capture both the
relationships between events and their arguments and the coreferencial
relationships between arguments. We conducted experiments using several SOTA
video prediction models as well as LVLMs on AVEP, demonstrating both the
complexity of the task and the value of the dataset. Our approach outperforms
all these video prediction models. We will release the dataset and code for
replicating the experiments and annotations.

</details>


### [549] [2D_3D Feature Fusion via Cross-Modal Latent Synthesis and Attention Guided Restoration for Industrial Anomaly Detection](https://arxiv.org/abs/2510.21793)
*Usman Ali,Ali Zia,Abdul Rehman,Umer Ramzan,Zohaib Hassan,Talha Sattar,Jing Wang,Wei Xiang*

Main category: cs.CV

TL;DR: 提出MAFR框架用于工业异常检测，结合2D和3D数据，在基准测试取得SOTA结果，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前工业异常检测中2D和3D数据的跨模态融合不够鲁棒。

Method: 提出MAFR框架，用共享融合编码器合成统一潜在空间，通过注意力引导的特定模态解码器，以重建误差定位异常。

Result: 在MVTec 3D - AD和Eyecandies基准测试中，平均I - AUROC分别达0.972和0.901，少样本学习表现好，消融实验证明融合架构和复合损失重要性。

Conclusion: MAFR为融合视觉和几何信息提供有效方法，提升了工业异常检测的鲁棒性和准确性。

Abstract: Industrial anomaly detection (IAD) increasingly benefits from integrating 2D
and 3D data, but robust cross-modal fusion remains challenging. We propose a
novel unsupervised framework, Multi-Modal Attention-Driven Fusion Restoration
(MAFR), which synthesises a unified latent space from RGB images and point
clouds using a shared fusion encoder, followed by attention-guided,
modality-specific decoders. Anomalies are localised by measuring reconstruction
errors between input features and their restored counterparts. Evaluations on
the MVTec 3D-AD and Eyecandies benchmarks demonstrate that MAFR achieves
state-of-the-art results, with a mean I-AUROC of 0.972 and 0.901, respectively.
The framework also exhibits strong performance in few-shot learning settings,
and ablation studies confirm the critical roles of the fusion architecture and
composite loss. MAFR offers a principled approach for fusing visual and
geometric information, advancing the robustness and accuracy of industrial
anomaly detection. Code is available at https://github.com/adabrh/MAFR

</details>


### [550] [Token-Level Inference-Time Alignment for Vision-Language Models](https://arxiv.org/abs/2510.21794)
*Kejia Chen,Jiawen Zhang,Jiacong Hu,Kewei Gao,Jian Lou,Zunlei Feng,Mingli Song*

Main category: cs.CV

TL;DR: 提出TITA框架解决视觉语言模型幻觉问题，在多模型多基准测试中取得增益且推理开销小。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型输出易产生幻觉，现有对齐方法依赖昂贵微调或仅提供粗粒度反馈。

Method: 提出TITA框架，冻结基础VLM，训练奖励模型近似其分布，推理时提取隐式偏好信号作为密集自回归反馈。

Result: 在LLaVA - 1.5 - 7B和13B等多个模型的12个基准测试中取得一致增益，如MMVet提高8.6%，POPE提高6.7%，在Qwen2.5 - VL - 7B和DeepSeek - VL2 - 27.5B上也有类似效果。

Conclusion: TITA框架能增强模型通用理解能力，减少幻觉，且推理开销可忽略不计。

Abstract: Vision-Language Models (VLMs) have become essential backbones of modern
multimodal intelligence, yet their outputs remain prone to
hallucination-plausible text misaligned with visual inputs. Existing alignment
approaches often rely on expensive fine-tuning with annotated preference data
or sequence-level inference strategies that provide only coarse, delayed
feedback. To overcome these limitations, we present TITA (Token-level
Inference-Time Alignment), a lightweight framework that freezes the base VLM
and instead trains a reward model to approximate its distribution. During
inference, implicit preference signals are extracted as log-probability ratios
between the reward model and the target VLM, yielding dense autoregressive
feedback. This formulation can be viewed as an inference-time variant of Direct
Preference Optimization (DPO), providing token-level corrective signals without
retraining the backbone. Extensive evaluations on LLaVA-1.5-7B and 13B show
consistent gains across 12 benchmarks, with improvements of 8.6% on MMVet and
6.7% on POPE, indicating stronger general understanding and reduced
hallucinations. Additional experiments on Qwen2.5-VL-7B and DeepSeek-VL2-27.5B
show comparable gains, especially in hallucination reduction and VQA accuracy,
while incurring negligible inference overhead.

</details>


### [551] [Xihe: Scalable Zero-Shot Time Series Learner Via Hierarchical Interleaved Block Attention](https://arxiv.org/abs/2510.21795)
*Yinbo Sun,Yuchen Fang,Zhibo Zhu,Jia Li,Yu Liu,Qiwen Deng,Jun Zhou,Hang Yu,Xingyu Lu,Lintao Ma*

Main category: cs.CV

TL;DR: 现有时间序列基础模型直接采用跨域架构有局限，提出HIBA架构及Xihe模型家族，在基准测试中表现出色，证明HIBA架构优越性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型直接采用跨域架构，难以有效捕捉多尺度时间依赖，在零样本迁移中局限性明显。

Method: 提出Hierarchical Interleaved Block Attention (HIBA)，利用块内和块间稀疏注意力捕捉多尺度依赖，基于HIBA架构引入Xihe模型家族。

Result: 最紧凑的Xihe - tiny模型参数效率高，Xihe - max在零样本迁移中创造新的最优性能。

Conclusion: 整个参数范围的一致卓越性能证明了HIBA具有出色的泛化能力和架构优越性。

Abstract: The rapid advancement of time series foundation models (TSFMs) has been
propelled by migrating architectures from language models. While existing TSFMs
demonstrate impressive performance, their direct adoption of cross-domain
architectures constrains effective capture of multiscale temporal dependencies
inherent to time series data. This limitation becomes particularly pronounced
during zero-shot transfer across datasets with divergent underlying patterns
and sampling strategies. To address these challenges, we propose Hierarchical
Interleaved Block Attention (HIBA) which employs hierarchical inter- and
intra-block sparse attention to effectively capture multi-scale dependencies.
Intra-block attention facilitates local information exchange, and inter-block
attention operates across blocks to capture global temporal pattern interaction
and dynamic evolution. Leveraging the HIBA architecture, we introduce Xihe, a
scalable TSFM family spanning from an ultra-efficient 9.5M parameter
configuration to high-capacity 1.5B variant. Evaluated on the comprehensive
GIFT-Eval benchmark, our most compact Xihe-tiny model (9.5M) surpasses the
majority of contemporary TSFMs, demonstrating remarkable parameter efficiency.
More impressively, Xihe-max (1.5B) establishes new state-of-the-art zero-shot
performance, surpassing previous best results by a substantial margin. This
consistent performance excellence across the entire parameter spectrum provides
compelling evidence for the exceptional generalization capabilities and
architectural superiority of HIBA.

</details>


### [552] [Frame-Difference Guided Dynamic Region Perception for CLIP Adaptation in Text-Video Retrieval](https://arxiv.org/abs/2510.21806)
*Jiaao Yu,Mingjie Han,Tao Gong,Jian Zhang,Man Lan*

Main category: cs.CV

TL;DR: 随着视频数据增长，文本 - 视频检索技术重要性提升。早期方法有成本高和模态差距问题，现有CLIP适配方法不足。本文提出FDA - CLIP框架，用帧差生成掩码，实验证明可平衡检索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 早期文本 - 视频检索方法数据获取成本高、模态差距大，现有CLIP适配方法对动态视频特征增强不足且无法有效抑制静态冗余特征。

Method: 提出FDA - CLIP框架，利用帧差生成动态区域掩码，作为额外Alpha通道输入到Alpha - CLIP中，引导模型关注动态区域并抑制静态背景冗余。

Result: 实验表明帧差引导的视频语义编码能有效平衡检索效率和准确性。

Conclusion: FDA - CLIP框架能有效解决现有方法问题，平衡文本 - 视频检索的效率和准确性。

Abstract: With the rapid growth of video data, text-video retrieval technology has
become increasingly important in numerous application scenarios such as
recommendation and search. Early text-video retrieval methods suffer from two
critical drawbacks: first, they heavily rely on large-scale annotated
video-text pairs, leading to high data acquisition costs; second, there is a
significant modal gap between video and text features, which limits cross-modal
alignment accuracy. With the development of vision-language model, adapting
CLIP to video tasks has attracted great attention. However, existing adaptation
methods generally lack enhancement for dynamic video features and fail to
effectively suppress static redundant features. To address this issue, this
paper proposes FDA-CLIP (Frame Difference Alpha-CLIP), which is a concise
CLIP-based training framework for text-video alignment. Specifically, the
method uses frame differences to generate dynamic region masks, which are input
into Alpha-CLIP as an additional Alpha channel. This proactively guides the
model to focus on semantically critical dynamic regions while suppressing
static background redundancy. Experiments demonstrate that frame
difference-guided video semantic encoding can effectively balance retrieval
efficiency and accuracy.

</details>


### [553] [Activating Visual Context and Commonsense Reasoning through Masked Prediction in VLMs](https://arxiv.org/abs/2510.21807)
*Jiaao Yu,Shenwei Li,Mingjie Han,Yifei Yin,Wenzheng Song,Chenghao Jia,Man Lan*

Main category: cs.CV

TL;DR: 现有大语言模型推理能力在多模态场景适配不足，本文提出Masked Prediction via Context and Commonsense微调任务及Reinforcement Fine tuning with Prior Sampling训练方法，还开发MPCC Eval评估基准提升模型泛化推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型在单模态语言场景发展较好，但在多模态场景尤其是视觉语言任务中适配存在差距，现有方法未能充分利用视觉上下文和常识知识，限制推理能力泛化。

Method: 引入Masked Prediction via Context and Commonsense微调任务，开发MPCC Eval评估基准，采用多种微调策略，提出Reinforcement Fine tuning with Prior Sampling训练方法。

Result: 新方法不仅提升了模型性能，还增强了模型在OOD和跨任务场景中的泛化推理能力。

Conclusion: 所提出的微调任务、评估基准和训练方法能有效解决现有模型在多模态场景推理能力泛化不足的问题。

Abstract: Recent breakthroughs in reasoning models have markedly advanced the reasoning
capabilities of large language models, particularly via training on tasks with
verifiable rewards. Yet, a significant gap persists in their adaptation to real
world multimodal scenarios, most notably, vision language tasks, due to a heavy
focus on single modal language settings. While efforts to transplant
reinforcement learning techniques from NLP to VLMs have emerged, these
approaches often remain confined to perception centric tasks or reduce images
to textual summaries, failing to fully exploit visual context and commonsense
knowledge, ultimately constraining the generalization of reasoning capabilities
across diverse multimodal environments. To address this limitation, we
introduce a novel fine tuning task, Masked Prediction via Context and
Commonsense, which forces models to integrate visual context and commonsense
reasoning by reconstructing semantically meaningful content from occluded
images, thereby laying the foundation for generalized reasoning. To
systematically evaluate the model performance in generalized reasoning, we
developed a specialized evaluation benchmark, MPCC Eval, and employed various
fine tuning strategies to guide reasoning. Among these, we introduced an
innovative training method, Reinforcement Fine tuning with Prior Sampling,
which not only enhances model performance but also improves its generalized
reasoning capabilities in OOD and cross task scenarios.

</details>


### [554] [Semantic Relation-Enhanced CLIP Adapter for Domain Adaptive Zero-Shot Learning](https://arxiv.org/abs/2510.21808)
*Jiaao Yu,Mingjie Han,Jinkun Jiang,Junyu Dong,Tao Gong,Man Lan*

Main category: cs.CV

TL;DR: 针对数据标注成本高，提出SRE - CLIP Adapter框架用于DAZSL，在基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 数据标注成本高，现有范式无法平衡跨域转移和跨类别泛化，且未充分挖掘视觉 - 语言模型在DAZSL领域的潜力。

Method: 提出Semantic Relation - Enhanced CLIP (SRE - CLIP) Adapter框架，集成语义关系结构损失和跨模态对齐保留策略。

Result: 作为首个基于CLIP的DAZSL方法，在I2AwA和I2WebV基准测试中达到SOTA性能，显著优于现有方法。

Conclusion: SRE - CLIP框架有效解决了CLIP应用于DAZSL的核心挑战，具有良好性能。

Abstract: The high cost of data annotation has spurred research on training deep
learning models in data-limited scenarios. Existing paradigms, however, fail to
balance cross-domain transfer and cross-category generalization, giving rise to
the demand for Domain-Adaptive Zero-Shot Learning (DAZSL). Although
vision-language models (e.g., CLIP) have inherent advantages in the DAZSL
field, current studies do not fully exploit their potential. Applying CLIP to
DAZSL faces two core challenges: inefficient cross-category knowledge transfer
due to the lack of semantic relation guidance, and degraded cross-modal
alignment during target domain fine-tuning. To address these issues, we propose
a Semantic Relation-Enhanced CLIP (SRE-CLIP) Adapter framework, integrating a
Semantic Relation Structure Loss and a Cross-Modal Alignment Retention
Strategy. As the first CLIP-based DAZSL method, SRE-CLIP achieves
state-of-the-art performance on the I2AwA and I2WebV benchmarks, significantly
outperforming existing approaches.

</details>


### [555] [Hybrid Deep Learning Framework for Enhanced Diabetic Retinopathy Detection: Integrating Traditional Features with AI-driven Insights](https://arxiv.org/abs/2510.21810)
*Arpan Maity,Aviroop Pal,MD. Samiul Islam,Tamal Ghosh*

Main category: cs.CV

TL;DR: 本文提出结合传统特征提取与深度学习的混合诊断框架以增强糖尿病视网膜病变（DR）检测，效果优于单独的深度学习方法，可实现可扩展、准确的DR筛查。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变（DR）是糖尿病的威胁视力并发症，早期无症状，印度糖尿病患者多，早期筛查至关重要，现有方法有局限。

Method: 引入结合传统特征提取和深度学习的混合诊断框架，结合可解释临床数据与学习特征。

Result: 该模型在分类上表现更好，减少假阴性。

Conclusion: 这种多模态人工智能驱动的方法能实现可扩展、准确的DR筛查，对糖尿病负担重的地区至关重要。

Abstract: Diabetic Retinopathy (DR), a vision-threatening complication of Dia-betes
Mellitus (DM), is a major global concern, particularly in India, which has one
of the highest diabetic populations. Prolonged hyperglycemia damages reti-nal
microvasculature, leading to DR symptoms like microaneurysms, hemor-rhages, and
fluid leakage, which, if undetected, cause irreversible vision loss. Therefore,
early screening is crucial as DR is asymptomatic in its initial stages. Fundus
imaging aids precise diagnosis by detecting subtle retinal lesions. This paper
introduces a hybrid diagnostic framework combining traditional feature
extraction and deep learning (DL) to enhance DR detection. While handcrafted
features capture key clinical markers, DL automates hierarchical pattern
recog-nition, improving early diagnosis. The model synergizes interpretable
clinical data with learned features, surpassing standalone DL approaches that
demon-strate superior classification and reduce false negatives. This
multimodal AI-driven approach enables scalable, accurate DR screening, crucial
for diabetes-burdened regions.

</details>


### [556] [Comparative Analysis of Object Detection Algorithms for Surface Defect Detection](https://arxiv.org/abs/2510.21811)
*Arpan Maity,Tamal Ghosh*

Main category: cs.CV

TL;DR: 文章对比六种目标检测算法在NEU - DET数据集上的性能，YOLOv11表现最优，准确率平均高70%，是最有效模型。


<details>
  <summary>Details</summary>
Motivation: 在工业质量控制的金属表面缺陷检测应用中，对比不同目标检测算法性能，找出最有效算法。

Method: 在NEU - DET表面缺陷检测数据集上，评估六种算法（YOLOv11、RetinaNet、Fast R - CNN、YOLOv8、RT - DETR、DETR）在检测精度、速度和不同缺陷类型鲁棒性方面的表现。

Result: YOLOv11表现优于其他方法，平均准确率高70%，因其增强的特征提取能力、单前向传播处理图像、架构优化等。

Conclusion: YOLOv11在精度和速度上表现出色，是NEU数据集表面缺陷检测最有效模型，大幅超越其他算法。

Abstract: This article compares the performance of six prominent object detection
algorithms, YOLOv11, RetinaNet, Fast R-CNN, YOLOv8, RT-DETR, and DETR, on the
NEU-DET surface defect detection dataset, comprising images representing
various metal surface defects, a crucial application in industrial quality
control. Each model's performance was assessed regarding detection accuracy,
speed, and robustness across different defect types such as scratches,
inclusions, and rolled-in scales. YOLOv11, a state-of-the-art real-time object
detection algorithm, demonstrated superior performance compared to the other
methods, achieving a remarkable 70% higher accuracy on average. This
improvement can be attributed to YOLOv11s enhanced feature extraction
capabilities and ability to process the entire image in a single forward pass,
making it faster and more efficient in detecting minor surface defects.
Additionally, YOLOv11's architecture optimizations, such as improved anchor box
generation and deeper convolutional layers, contributed to more precise
localization of defects. In conclusion, YOLOv11's outstanding performance in
accuracy and speed solidifies its position as the most effective model for
surface defect detection on the NEU dataset, surpassing competing algorithms by
a substantial margin.

</details>


### [557] [SITS-DECO: A Generative Decoder Is All You Need For Multitask Satellite Image Time Series Modelling](https://arxiv.org/abs/2510.21813)
*Samuel J. Barrett,Docko Sow*

Main category: cs.CV

TL;DR: 提出SITS - DECO模型用于地球观测数据，在作物类型分类上表现出色，体现以数据为中心的建模范式。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测基础建模多数需额外适配，结构围绕特定数据源或训练方法，不够灵活。

Method: 借鉴大语言模型，引入SITS - DECO生成模型，采用GPT式解码器架构，通过符号提示执行多任务。

Result: SITS - DECO在作物类型分类上优于更大的地球观测基础模型。

Conclusion: 以数据为中心的建模范式有效，SITS - DECO为多模态、多任务地球观测建模提供轻量级途径。

Abstract: Earth Observation (EO) Foundation Modelling (FM) holds great promise for
simplifying and improving the use of EO data for diverse real-world tasks.
However, most existing models require additional adaptation before they can be
used and are structured rigidly around particular data sources or training
approaches. To address this, we take inspiration from large language models,
where diverse tasks, both pre-training and downstream, are implicitly captured
through next-token prediction over unified token sequences, leveraging the
structure and diversity of the training data.
  We introduce SITS-DECO (Satellite Image Time Series-DECoder Only), a
proof-of-concept generative model that applies this unified-sequence framing to
EO data. Using a simple GPT-style decoder-only architecture, and demonstrate
its ability to perform useful EO tasks (pixel-wise, multi-temporal, multi-modal
crop-type classification) in a purely generative framework. Through symbolic
prompting, we show that the model can perform multiple supervised and
self-supervised tasks within a single unified architecture, without task- or
modality-specific adaptation. Despite its simplicity and lack of spatial
context, SITS-DECO outperforms much larger EO foundation models on crop-type
classification (PASTIS-R) demonstrating that dense temporal sequence modelling
is a critical missing ingredient in the current paradigm.
  This work exemplifies a data-centric modelling paradigm in which capability
arises from the diversity and structure of the training data rather than from
architectural complexity. SITS-DECO provides a lightweight, practical route to
multi-modal, multi-task EO modelling, and a conceptual bridge toward future
generative EO foundation models.

</details>


### [558] [Gestura: A LVLM-Powered System Bridging Motion and Semantics for Real-Time Free-Form Gesture Understanding](https://arxiv.org/abs/2510.21814)
*Zhuoming Li,Aitong Liu,Mengxi Jia,Tengxiang Zhang,Dell Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出Gestura系统用于自由形式手势理解，引入模块和推理策略，开发开源数据集。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案GestureGPT识别准确率低、响应慢，需更好的自由形式手势理解系统。

Method: 利用预训练LVLM，引入Landmark Processing Module，采用Chain-of-Thought推理策略。

Result: Gestura能实现鲁棒且适应性强的自由形式手势理解，开发含超300,000注释问答对的开源数据集。

Conclusion: Gestura系统有效解决现有问题，实现自由形式手势理解。

Abstract: Free-form gesture understanding is highly appealing for human-computer
interaction, as it liberates users from the constraints of predefined gesture
categories. However, the sole existing solution GestureGPT suffers from limited
recognition accuracy and slow response times. In this paper, we propose
Gestura, an end-to-end system for free-form gesture understanding. Gestura
harnesses a pre-trained Large Vision-Language Model (LVLM) to align the highly
dynamic and diverse patterns of free-form gestures with high-level semantic
concepts. To better capture subtle hand movements across different styles, we
introduce a Landmark Processing Module that compensate for LVLMs' lack of
fine-grained domain knowledge by embedding anatomical hand priors. Further, a
Chain-of-Thought (CoT) reasoning strategy enables step-by-step semantic
inference, transforming shallow knowledge into deep semantic understanding and
significantly enhancing the model's ability to interpret ambiguous or
unconventional gestures. Together, these components allow Gestura to achieve
robust and adaptable free-form gesture comprehension. Additionally, we have
developed the first open-source dataset for free-form gesture intention
reasoning and understanding with over 300,000 annotated QA pairs.

</details>


### [559] [Prompt fidelity of ChatGPT4o / Dall-E3 text-to-image visualisations](https://arxiv.org/abs/2510.21821)
*Dirk HR Spennemann*

Main category: cs.CV

TL;DR: 研究分析ChatGPT4o / DALL - E3文本到图像可视化的提示保真度，发现DALL - E3有15.6%属性与提示不符，不同属性误差有差异。


<details>
  <summary>Details</summary>
Motivation: 研究ChatGPT4o / DALL - E3文本到图像可视化中，生成提示里明确指定的属性是否在结果图像中正确呈现。

Method: 使用两个公共领域数据集（200张文化创意产业女性可视化图像和230张博物馆策展人可视化图像），评估个人属性、外貌和随身物品的准确性。

Result: DALL - E3在15.6%的属性上与提示规格有偏差，随身物品误差最低，个人外貌适中，人物本身描绘误差最高，尤其是年龄。

Conclusion: 文本到图像存在可测量的保真度差距，对偏差检测和模型评估有意义。

Abstract: This study examines the prompt fidelity of ChatGPT4o / DALL-E3 text-to-image
visualisations by analysing whether attributes explicitly specified in
autogenously generated prompts are correctly rendered in the resulting images.
Using two public-domain datasets comprising 200 visualisations of women working
in the cultural and creative industries and 230 visualisations of museum
curators, the study assessed accuracy across personal attributes (age, hair),
appearance (attire, glasses), and paraphernalia (name tags, clipboards). While
correctly rendered in most cases, DALL-E3 deviated from prompt specifications
in 15.6% of all attributes (n=710). Errors were lowest for paraphernalia,
moderate for personal appearance, and highest for depictions of the person
themselves, particularly age. These findings demonstrate measurable
prompt-to-image fidelity gaps with implications for bias detection and model
evaluation.

</details>


### [560] [Wavelet-based GAN Fingerprint Detection using ResNet50](https://arxiv.org/abs/2510.21822)
*Sai Teja Erukude,Suhasnadh Reddy Veluru,Viswa Chaitanya Marella*

Main category: cs.CV

TL;DR: 提出基于小波的方法检测GAN生成图像，与空间域模型对比，结果显示小波域分析有效。


<details>
  <summary>Details</summary>
Motivation: 在数字图像取证中，识别GAN生成的图像是重大挑战，需有效检测方法。

Method: 采用离散小波变换（DWT）预处理和ResNet50分类层，应用Haar和Daubechies小波滤波器将图像转换为多分辨率表示后分类，并与空间域ResNet50模型对比。

Result: Haar和Daubechies预处理模型准确率分别达93.8%和95.1%，高于空间域模型的81.5%，Daubechies模型表现更优。

Conclusion: GAN生成图像在小波域有独特特征，小波域分析检测GAN图像有效，未来可进一步提升深度伪造检测系统能力。

Abstract: Identifying images generated by Generative Adversarial Networks (GANs) has
become a significant challenge in digital image forensics. This research
presents a wavelet-based detection method that uses discrete wavelet transform
(DWT) preprocessing and a ResNet50 classification layer to differentiate the
StyleGAN-generated images from real ones. Haar and Daubechies wavelet filters
are applied to convert the input images into multi-resolution representations,
which will then be fed to a ResNet50 network for classification, capitalizing
on subtle artifacts left by the generative process. Moreover, the wavelet-based
models are compared to an identical ResNet50 model trained on spatial data. The
Haar and Daubechies preprocessed models achieved a greater accuracy of 93.8
percent and 95.1 percent, much higher than the model developed in the spatial
domain (accuracy rate of 81.5 percent). The Daubechies-based model outperforms
Haar, showing that adding layers of descriptive frequency patterns can lead to
even greater distinguishing power. These results indicate that the
GAN-generated images have unique wavelet-domain artifacts or "fingerprints."
The method proposed illustrates the effectiveness of wavelet-domain analysis to
detect GAN images and emphasizes the potential of further developing the
capabilities of future deepfake detection systems.

</details>


### [561] [Explainable Deep Learning in Medical Imaging: Brain Tumor and Pneumonia Detection](https://arxiv.org/abs/2510.21823)
*Sai Teja Erukude,Viswa Chaitanya Marella,Suhasnadh Reddy Veluru*

Main category: cs.CV

TL;DR: 本文提出可解释深度学习框架用于检测脑部肿瘤和肺炎，对比ResNet50和DenseNet121性能，结合Grad - CAM增强可解释性，认为结合深度学习和可解释AI有前景。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像诊断潜力大，但多数模型缺乏可解释性阻碍临床应用，需开发可解释模型。

Method: 使用ResNet50和DenseNet121在公开Kaggle数据集上训练，用Gradient - weighted Class Activation Mapping (Grad - CAM)创建热图可视化。

Result: DenseNet121性能优于ResNet50，脑肿瘤检测准确率94.3% vs 92.5%，肺炎检测准确率89.1% vs 84.4%；Grad - CAM显示DenseNet121关注核心病理区域，ResNet50有时关注周边或非病理区域。

Conclusion: 结合深度学习和可解释AI是实现可靠、可解释和临床有用诊断工具的有前景途径。

Abstract: Deep Learning (DL) holds enormous potential for improving medical imaging
diagnostics, yet the lack of interpretability in most models hampers clinical
trust and adoption. This paper presents an explainable deep learning framework
for detecting brain tumors in MRI scans and pneumonia in chest X-ray images
using two leading Convolutional Neural Networks, ResNet50 and DenseNet121.
These models were trained on publicly available Kaggle datasets comprising
7,023 brain MRI images and 5,863 chest X-ray images, achieving high
classification performance. DenseNet121 consistently outperformed ResNet50 with
94.3 percent vs. 92.5 percent accuracy for brain tumors and 89.1 percent vs.
84.4 percent accuracy for pneumonia. For better explainability,
Gradient-weighted Class Activation Mapping (Grad-CAM) was integrated to create
heatmap visualizations superimposed on the test images, indicating the most
influential image regions in the decision-making process. Interestingly, while
both models produced accurate results, Grad-CAM showed that DenseNet121
consistently focused on core pathological regions, whereas ResNet50 sometimes
scattered attention to peripheral or non-pathological areas. Combining deep
learning and explainable AI offers a promising path toward reliable,
interpretable, and clinically useful diagnostic tools.

</details>


### [562] [Precise classification of low quality G-banded Chromosome Images by reliability metrics and data pruning classifier](https://arxiv.org/abs/2510.21827)
*Mojtaba Moattari*

Main category: cs.CV

TL;DR: 为解决低成本系统和低质量图像设置中的误检问题，本文提出可靠性阈值指标和特征工程来提高染色体分类精度，经多种方法评估，结果精度超90%，验证了方法在贫困国家和低成本实验室的适用性。


<details>
  <summary>Details</summary>
Motivation: 当前核型分析系统需要大量高质量训练数据，一些偏远病理实验室难以实现，为防止低成本系统和低质量图像中的误检。

Method: 提出可靠性阈值指标和精心设计的特征，使用深度Alex - Net神经网络、SVM、K近邻及其级联管道对半直染色体进行自动过滤。

Result: 染色体分类结果在常见缺陷和易位的染色体上精度提高超90%，并进行了阈值指标的比较分析。

Conclusion: 所提指标和修剪方法适用于贫困国家的核型分析机构和低成本病理实验室。

Abstract: In the last decade, due to high resolution cameras and accurate meta-phase
analyzes, the accuracy of chromosome classification has improved substantially.
However, current Karyotyping systems demand large number of high quality train
data to have an adequately plausible Precision per each chromosome. Such
provision of high quality train data with accurate devices are not yet
accomplished in some out-reached pathological laboratories. To prevent false
positive detections in low-cost systems and low-quality images settings, this
paper improves the classification Precision of chromosomes using proposed
reliability thresholding metrics and deliberately engineered features. The
proposed method has been evaluated using a variation of deep Alex-Net neural
network, SVM, K Nearest-Neighbors, and their cascade pipelines to an automated
filtering of semi-straight chromosome. The classification results have highly
improved over 90% for the chromosomes with more common defections and
translocations. Furthermore, a comparative analysis over the proposed
thresholding metrics has been conducted and the best metric is bolded with its
salient characteristics. The high Precision results provided for a very
low-quality G-banding database verifies suitability of the proposed metrics and
pruning method for Karyotyping facilities in poor countries and lowbudget
pathological laboratories.

</details>


### [563] [Evaluating ChatGPT's Performance in Classifying Pneumonia from Chest X-Ray Images](https://arxiv.org/abs/2510.21839)
*Pragna Prahallad,Pranathi Prahallad*

Main category: cs.CV

TL;DR: 研究评估GPT - 4o模型零样本分类胸部X光图像能力，不同提示设计下表现不同，模型有潜力但可靠性有限。


<details>
  <summary>Details</summary>
Motivation: 评估OpenAI的GPT - 4o模型在零样本设置下对胸部X光图像分类的能力。

Method: 使用400张图像的平衡测试集，通过四种不同提示设计评估模型性能。

Result: 简洁、关注特征的提示达到74%的最高分类准确率，推理导向提示性能较低。

Conclusion: ChatGPT在医学图像解释有潜力，但诊断可靠性有限，临床应用前需视觉推理和特定领域适配的进步。

Abstract: In this study, we evaluate the ability of OpenAI's gpt-4o model to classify
chest X-ray images as either NORMAL or PNEUMONIA in a zero-shot setting,
without any prior fine-tuning. A balanced test set of 400 images (200 from each
class) was used to assess performance across four distinct prompt designs,
ranging from minimal instructions to detailed, reasoning-based prompts. The
results indicate that concise, feature-focused prompts achieved the highest
classification accuracy of 74\%, whereas reasoning-oriented prompts resulted in
lower performance. These findings highlight that while ChatGPT exhibits
emerging potential for medical image interpretation, its diagnostic reliability
remains limited. Continued advances in visual reasoning and domain-specific
adaptation are required before such models can be safely applied in clinical
practice.

</details>


### [564] [Poisson Flow Consistency Training](https://arxiv.org/abs/2510.21857)
*Anthony Zhang,Mahmut Gokmen,Dennis Hein,Rongjun Ge,Wenjun Xia,Ge Wang,Jin Chen*

Main category: cs.CV

TL;DR: 提出Poisson Flow Consistency Training (PFCT)方法训练PFCM，应用于低剂量CT图像去噪有效果，框架有灵活性但需进一步研究。


<details>
  <summary>Details</summary>
Motivation: PFCM只能在蒸馏中训练，限制其在多数据模态的潜力，需创建孤立训练方法。

Method: 利用扰动核去除预训练的PFGM++，引入正弦离散化调度和Beta噪声分布。

Result: 应用于低剂量CT图像去噪，在LPIPS和SSIM方面改善图像，去噪效果与Consistency Model相当。

Conclusion: PFCT是训练PFCM的有效方法，有潜力，但需精确优化及研究其在其他生成任务的适用性。

Abstract: The Poisson Flow Consistency Model (PFCM) is a consistency-style model based
on the robust Poisson Flow Generative Model++ (PFGM++) which has achieved
success in unconditional image generation and CT image denoising. Yet the PFCM
can only be trained in distillation which limits the potential of the PFCM in
many data modalities. The objective of this research was to create a method to
train the PFCM in isolation called Poisson Flow Consistency Training (PFCT).
The perturbation kernel was leveraged to remove the pretrained PFGM++, and the
sinusoidal discretization schedule and Beta noise distribution were introduced
in order to facilitate adaptability and improve sample quality. The model was
applied to the task of low dose computed tomography image denoising and
improved the low dose image in terms of LPIPS and SSIM. It also displayed
similar denoising effectiveness as models like the Consistency Model. PFCT is
established as a valid method of training the PFCM from its effectiveness in
denoising CT images, showing potential with competitive results to other
generative models. Further study is needed in the precise optimization of PFCT
and in its applicability to other generative modeling tasks. The framework of
PFCT creates more flexibility for the ways in which a PFCM can be created and
can be applied to the field of generative modeling.

</details>


### [565] [Addressing Corner Cases in Autonomous Driving: A World Model-based Approach with Mixture of Experts and LLMs](https://arxiv.org/abs/2510.21867)
*Haicheng Liao,Bonan Wang,Junxian Yang,Chengyue Wang,Zhengbin He,Guohui Zhang,Chengzhong Xu,Zhenning Li*

Main category: cs.CV

TL;DR: 提出世界模型WM - MoE用于自动驾驶车辆运动预测，引入新基准nuScenes - corner，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型在自动驾驶高风险极端场景中表现不佳，因训练数据常见场景占比高且泛化能力有限。

Method: 构建世界模型WM - MoE统一感知、时间记忆和决策；利用大语言模型和轻量级时间分词器增强长时推理；引入专家混合模型分解复杂极端情况；创建新基准nuScenes - corner。

Result: 在四个基准数据集上实验，WM - MoE始终优于现有方法，在极端情况和数据缺失时仍保持稳健。

Conclusion: 基于世界模型的架构在自动驾驶车辆稳健且可泛化的运动预测方面有前景。

Abstract: Accurate and reliable motion forecasting is essential for the safe deployment
of autonomous vehicles (AVs), particularly in rare but safety-critical
scenarios known as corner cases. Existing models often underperform in these
situations due to an over-representation of common scenes in training data and
limited generalization capabilities. To address this limitation, we present
WM-MoE, the first world model-based motion forecasting framework that unifies
perception, temporal memory, and decision making to address the challenges of
high-risk corner-case scenarios. The model constructs a compact scene
representation that explains current observations, anticipates future dynamics,
and evaluates the outcomes of potential actions. To enhance long-horizon
reasoning, we leverage large language models (LLMs) and introduce a lightweight
temporal tokenizer that maps agent trajectories and contextual cues into the
LLM's feature space without additional training, enriching temporal context and
commonsense priors. Furthermore, a mixture-of-experts (MoE) is introduced to
decompose complex corner cases into subproblems and allocate capacity across
scenario types, and a router assigns scenes to specialized experts that infer
agent intent and perform counterfactual rollouts. In addition, we introduce
nuScenes-corner, a new benchmark that comprises four real-world corner-case
scenarios for rigorous evaluation. Extensive experiments on four benchmark
datasets (nuScenes, NGSIM, HighD, and MoCAD) showcase that WM-MoE consistently
outperforms state-of-the-art (SOTA) baselines and remains robust under
corner-case and data-missing conditions, indicating the promise of world
model-based architectures for robust and generalizable motion forecasting in
fully AVs.

</details>


### [566] [AI Powered Urban Green Infrastructure Assessment Through Aerial Imagery of an Industrial Township](https://arxiv.org/abs/2510.21876)
*Anisha Dutta*

Main category: cs.CV

TL;DR: 本文提出用人工智能和计算机视觉技术，基于深度学习算法对无人机图像进行分析，在云平台上高效估算城市树冠覆盖率，结果有效，能助力城市规划。


<details>
  <summary>Details</summary>
Motivation: 传统评估城市树冠覆盖率的方法存在技术要求不足、扩展和数据处理困难以及缺乏专业知识等局限，需要更有效的方法。

Method: 利用基于深度学习算法的对象图像分析，对高分辨率无人机图像进行处理，并在云平台上借助高性能处理器实现以克服大数据集处理的计算挑战。

Result: 该方法能在城市尺度准确估算树冠覆盖率，为工业城镇的城市林业管理提供有价值的见解。

Conclusion: 此方法生成的数据可用于优化植树和评估城市森林固碳潜力，融入可持续城市规划能促进更具韧性的城市环境。

Abstract: Accurate assessment of urban canopy coverage is crucial for informed urban
planning, effective environmental monitoring, and mitigating the impacts of
climate change. Traditional practices often face limitations due to inadequate
technical requirements, difficulties in scaling and data processing, and the
lack of specialized expertise. This study presents an efficient approach for
estimating green canopy coverage using artificial intelligence, specifically
computer vision techniques, applied to aerial imageries. Our proposed
methodology utilizes object-based image analysis, based on deep learning
algorithms to accurately identify and segment green canopies from
high-resolution drone images. This approach allows the user for detailed
analysis of urban vegetation, capturing variations in canopy density and
understanding spatial distribution. To overcome the computational challenges
associated with processing large datasets, it was implemented over a cloud
platform utilizing high-performance processors. This infrastructure efficiently
manages space complexity and ensures affordable latency, enabling the rapid
analysis of vast amounts of drone imageries. Our results demonstrate the
effectiveness of this approach in accurately estimating canopy coverage at the
city scale, providing valuable insights for urban forestry management of an
industrial township. The resultant data generated by this method can be used to
optimize tree plantation and assess the carbon sequestration potential of urban
forests. By integrating these insights into sustainable urban planning, we can
foster more resilient urban environments, contributing to a greener and
healthier future.

</details>


### [567] [TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge](https://arxiv.org/abs/2510.21879)
*Shu-Hao Zhang,Wei-Cheng Tang,Chen Wu,Peng Hu,Nan Li,Liang-Jie Zhang,Qi Zhang,Shao-Qun Zhang*

Main category: cs.CV

TL;DR: 提出轻量级计算框架TernaryCLIP，将CLIP的视觉和文本编码器连接权重转换为三元格式，实验表明其在多项指标有提升且性能良好，支持在资源受限设备部署。


<details>
  <summary>Details</summary>
Motivation: 随着对图像 - 文本对比建模兴趣增加，为实现大跨模态模型在资源受限设备的高效部署。

Method: 提出TernaryCLIP框架，将CLIP的视觉和文本编码器连接权重转换为三元格式，结合量化感知训练和蒸馏模块。

Result: TernaryCLIP能实现高达99%的三元化权重，在41个常用数据集的零样本图像分类和图像 - 文本检索任务中保持良好性能，且有压缩比、推理加速等多项指标提升。

Conclusion: 证明了大跨模态模型极端量化的可行性，支持在资源受限设备有效部署。

Abstract: Recent years have witnessed an increasing interest in image-text contrastive
modeling, exemplified by models such as Contrastive Language-Image Pretraining
(CLIP). In this paper, we propose the TernaryCLIP, a lightweight computational
framework that converts connection weights of both vision and text encoders of
CLIP into the ternary format, instead of full-precision or floating ones.
TernaryCLIP incorporates quantization-aware training and distillation modules,
preventing precision degradation and enabling low-cost and high-efficiency
computations. Comprehensive experiments demonstrate that TernaryCLIP can
achieve up to 99\% ternarized weights with 1.58-bit representation, 16.98
$\times$ compression ratio, 2.3 $\times$ inference acceleration, 16 $\times$
storage reduction, 10 $\times$ memory optimization, and 60\% sparsity while
maintaining promising performance on zero-shot image classification and
image-text retrieval tasks across 41 commonly used datasets. Our work
highlights the feasibility of extreme quantization for large multimodal models,
supporting effective and efficient deployment on resource-constrained devices.
The model and code can be accessed from Hugging Face and GitHub.

</details>


### [568] [Generative AI in Depth: A Survey of Recent Advances, Model Variants, and Real-World Applications](https://arxiv.org/abs/2510.21887)
*Shamim Yazdani,Akansha Singh,Nripsuta Saxena,Zichong Wang,Avash Palikhe,Deng Pan,Umapada Pal,Jie Yang,Wenbin Zhang*

Main category: cs.CV

TL;DR: 本文对基于深度学习的生成模型（GANs、VAEs和DMs）进行综述，介绍分类法、关键创新，探讨伦理问题，指出挑战并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型快速发展，研究量增加、应用领域扩大且存在技术挑战，难以跟上发展步伐，需要进行全面综述。

Method: 引入全面的分类法来组织文献，提供理解模型发展的框架。

Result: 突出了提高生成输出质量、多样性和可控性的关键创新，探讨了伦理问题。

Conclusion: 概述了持续存在的挑战并提出未来研究方向，为该领域研究者提供结构化和前瞻性视角。

Abstract: In recent years, deep learning based generative models, particularly
Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and
Diffusion Models (DMs), have been instrumental in in generating diverse,
high-quality content across various domains, such as image and video synthesis.
This capability has led to widespread adoption of these models and has captured
strong public interest. As they continue to advance at a rapid pace, the
growing volume of research, expanding application areas, and unresolved
technical challenges make it increasingly difficult to stay current. To address
this need, this survey introduces a comprehensive taxonomy that organizes the
literature and provides a cohesive framework for understanding the development
of GANs, VAEs, and DMs, including their many variants and combined approaches.
We highlight key innovations that have improved the quality, diversity, and
controllability of generated outputs, reflecting the expanding potential of
generative artificial intelligence. In addition to summarizing technical
progress, we examine rising ethical concerns, including the risks of misuse and
the broader societal impact of synthetic media. Finally, we outline persistent
challenges and propose future research directions, offering a structured and
forward looking perspective for researchers in this fast evolving field.

</details>


### [569] [Reconnaissance Automatique des Langues des Signes : Une Approche Hybridée CNN-LSTM Basée sur Mediapipe](https://arxiv.org/abs/2510.22011)
*Fraisse Sacré Takouchouang,Ho Tuong Vinh*

Main category: cs.CV

TL;DR: 本文提出基于混合CNN - LSTM架构的自动手语识别系统，实现实时手势翻译，准确率达92%，但相似手势有混淆问题，有应用前景。


<details>
  <summary>Details</summary>
Motivation: 手语在聋人社区交流中重要却常被边缘化，限制其获取医疗、教育等服务，需构建手语识别系统。

Method: 提出基于混合CNN - LSTM架构的自动手语识别系统，用Mediapipe提取手势关键点，用Python、TensorFlow和Streamlit开发。

Result: 系统平均准确率92%，对‘Hello’和‘Thank you’等独特手势表现好，‘Call’和‘Yes’等相似手势有混淆。

Conclusion: 该研究为医疗、教育和公共服务等领域应用提供了有趣视角。

Abstract: Sign languages play a crucial role in the communication of deaf communities,
but they are often marginalized, limiting access to essential services such as
healthcare and education. This study proposes an automatic sign language
recognition system based on a hybrid CNN-LSTM architecture, using Mediapipe for
gesture keypoint extraction. Developed with Python, TensorFlow and Streamlit,
the system provides real-time gesture translation. The results show an average
accuracy of 92\%, with very good performance for distinct gestures such as
``Hello'' and ``Thank you''. However, some confusions remain for visually
similar gestures, such as ``Call'' and ``Yes''. This work opens up interesting
perspectives for applications in various fields such as healthcare, education
and public services.

</details>


### [570] [VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation Sensitivity in PPT](https://arxiv.org/abs/2510.22045)
*Hyeonsu Kang,Emily Bao,Anjan Goswami*

Main category: cs.CV

TL;DR: 介绍VLM - SlideEval评估框架评估VLMs对幻灯片的理解能力，发现当前VLMs有局限性。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在幻灯片特定理解方面研究不足，虽在多模态内容评估中作用渐大，但缺乏对幻灯片的深入评估。

Method: 引入VLM - SlideEval框架，从元素提取、抗干扰性、高层理解三方面评估，用公开幻灯片数据集标准化元数据。

Result: VLMs在像素精确提取上表现不佳，在抗干扰性上有一定问题，单张幻灯片理解较好，但无法可靠把握幻灯片叙事结构。

Conclusion: 当前VLMs用于幻灯片评估有局限性，需校准、批评循环的评估器推动迭代改进和选择。

Abstract: Vision-language models (VLMs) are increasingly used to evaluate multimodal
content, including presentation slides, yet their slide-specific understanding
remains underexplored {despite their growing role as critics in agentic,
model-forward pipelines}. We introduce VLM-SlideEval, an evaluation framework
that probes VLMs along three axes: (1) element-level extraction from slide
images aligned to ground truth; (2) robustness to controlled perturbations in
geometry, style, and text; and (3) higher-level comprehension, such as
recovering a deck's narrative order from shuffled slides. Using publicly
available decks from Zenodo
(https://huggingface.co/datasets/Forceless/Zenodo10K/viewer/default/pptx), we
standardize ground-truth element metadata from PowerPoint XML and live
renderings into a unified, verifiable schema. Empirically, VLMs underperform on
pixel-accurate extraction and show non-trivial agreement, fidelity, and
consistency under controlled perturbations, while performing better on
single-slide content understanding; however, they do not reliably capture
narrative structure across slides. These results highlight the limits of
current VLMs for slide evaluation and motivate calibrated, critic-in-the-loop
evaluators that drive iterative refinement and selection in agentic pipelines.

</details>


### [571] [Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning](https://arxiv.org/abs/2510.22056)
*Mohammad Ali Etemadi Naeen,Hoda Mohammadzade,Saeed Bagheri Shouraki*

Main category: cs.CV

TL;DR: 提出集成以人类为中心预处理和时空建模的深度学习框架用于监控视频多类异常分类，在UCF - Crime数据集子集上取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决监控视频异常检测中异常事件多样、类别不平衡和场景视觉杂乱的问题。

Method: 先使用YOLO - World识别视频中的人体实例，再用ByteTrack跟踪，通过高斯模糊抑制背景区域，用InceptionV3提取空间特征，用BiLSTM进行序列级分类。

Result: 在UCF - Crime数据集五分类子集上三次独立试验平均测试准确率达92.41%，各类F1分数超0.85，综合评估指标显示强泛化能力和抗类别不平衡能力。

Conclusion: 以前景为中心的预处理显著提高现实监控场景中的异常辨别能力。

Abstract: Anomaly detection in surveillance videos remains a challenging task due to
the diversity of abnormal events, class imbalance, and scene-dependent visual
clutter. To address these issues, we propose a robust deep learning framework
that integrates human-centric preprocessing with spatio-temporal modeling for
multi-class anomaly classification. Our pipeline begins by applying YOLO-World
- an open-vocabulary vision-language detector - to identify human instances in
raw video clips, followed by ByteTrack for consistent identity-aware tracking.
Background regions outside detected bounding boxes are suppressed via Gaussian
blurring, effectively reducing scene-specific distractions and focusing the
model on behaviorally relevant foreground content. The refined frames are then
processed by an ImageNet-pretrained InceptionV3 network for spatial feature
extraction, and temporal dynamics are captured using a bidirectional LSTM
(BiLSTM) for sequence-level classification. Evaluated on a five-class subset of
the UCF-Crime dataset (Normal, Burglary, Fighting, Arson, Explosion), our
method achieves a mean test accuracy of 92.41% across three independent trials,
with per-class F1-scores consistently exceeding 0.85. Comprehensive evaluation
metrics - including confusion matrices, ROC curves, and macro/weighted averages
- demonstrate strong generalization and resilience to class imbalance. The
results confirm that foreground-focused preprocessing significantly enhances
anomaly discrimination in real-world surveillance scenarios.

</details>


### [572] [Mitigating Coordinate Prediction Bias from Positional Encoding Failures](https://arxiv.org/abs/2510.22102)
*Xingjian Tao,Yiwei Wang,Yujun Cai,Yihong Luo,Jing Tang*

Main category: cs.CV

TL;DR: 多模态大语言模型在精确坐标预测有挑战，研究发现视觉位置编码扰动有可预测偏差，提出无训练测试时方法VPSG改善效果，强调位置编码鲁棒性重要性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在精确坐标预测存在困难，高分辨率输入使问题更严重，需解决此问题。

Method: 分析视觉位置编码被打乱时模型表现，提出无训练测试时方法VPSG，利用偏差方向性校正，通过打乱VPE辅助解码，用轻量级有限状态机保留坐标格式。

Result: 在ScreenSpot - Pro实验中取得可靠改进。

Conclusion: 位置编码鲁棒性是多模态大语言模型空间推理的关键因素。

Abstract: Multimodal large language models (MLLMs) excel at vision-language tasks such
as VQA and document understanding, yet precise coordinate prediction remains
challenging. High-resolution inputs exacerbate this difficulty by producing
long token sequences that weaken positional encodings and introduce directional
biases in coordinate outputs. We investigate this phenomenon by analyzing how
MLLMs behave when visual positional encodings (VPEs) are deliberately perturbed
through shuffling. Our analysis reveals that such perturbations induce
predictable, non-random coordinate biases rather than random errors, suggesting
that models rely on internal positional priors when spatial grounding signals
are degraded. Crucially, we observe similar directional error patterns in
natural high-resolution datasets, indicating that positional encoding failures
are a key bottleneck for accurate coordinate prediction at scale. To address
this issue, we propose Vision-PE Shuffle Guidance (VPSG), a training-free
test-time method that leverages the directional nature of these biases for
correction. VPSG runs auxiliary decoding with shuffled VPEs to isolate
position-unconditioned tendencies, then uses this as negative evidence to guide
digit prediction while preserving coordinate format through a lightweight
finite-state machine. Experiments on ScreenSpot-Pro demonstrate reliable
improvements, highlighting positional encoding robustness as a critical factor
for spatial reasoning in MLLMs.

</details>


### [573] [Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation](https://arxiv.org/abs/2510.22107)
*Bailey Trang,Parham Saremi,Alan Q. Wang,Fangrui Huang,Zahra TehraniNasab,Amar Kumar,Tal Arbel,Li Fei-Fei,Ehsan Adeli*

Main category: cs.CV

TL;DR: 提出Rainbow框架解决条件/提示不确定性，生成多样合理图像，在多任务上提升多样性和保真度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在生成反映条件多样性的图像时存在难以辨别样本差异、语言可解释多样性有限的问题，需要新方法解决条件/提示不确定性。

Method: 将由生成流网络（GFlowNets）参数化的潜在图集成到提示表示计算中，利用GFlowNets的图采样能力输出多样轨迹，得到多样条件表示和对应图像。

Result: 在自然图像和医学图像数据集的图像合成、生成和反事实生成任务中，Rainbow在多样性和保真度上有提升。

Conclusion: Rainbow框架能有效解决条件/提示不确定性，生成多样合理图像，在多任务中有较好表现。

Abstract: Capturing diversity is crucial in conditional and prompt-based image
generation, particularly when conditions contain uncertainty that can lead to
multiple plausible outputs. To generate diverse images reflecting this
diversity, traditional methods often modify random seeds, making it difficult
to discern meaningful differences between samples, or diversify the input
prompt, which is limited in verbally interpretable diversity. We propose
Rainbow, a novel conditional image generation framework, applicable to any
pretrained conditional generative model, that addresses inherent
condition/prompt uncertainty and generates diverse plausible images. Rainbow is
based on a simple yet effective idea: decomposing the input condition into
diverse latent representations, each capturing an aspect of the uncertainty and
generating a distinct image. First, we integrate a latent graph, parameterized
by Generative Flow Networks (GFlowNets), into the prompt representation
computation. Second, leveraging GFlowNets' advanced graph sampling capabilities
to capture uncertainty and output diverse trajectories over the graph, we
produce multiple trajectories that collectively represent the input condition,
leading to diverse condition representations and corresponding output images.
Evaluations on natural image and medical image datasets demonstrate Rainbow's
improvement in both diversity and fidelity across image synthesis, image
generation, and counterfactual generation tasks.

</details>


### [574] [GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data Generation](https://arxiv.org/abs/2510.22118)
*Karim Elmaaroufi,Liheng Lai,Justin Svegliato,Yutong Bai,Sanjit A. Seshia,Matei Zaharia*

Main category: cs.CV

TL;DR: 现有视觉语言模型在空间推理任务表现不佳，本文提出GRAID框架，基于2D边界框生成高质量数据集，提升模型空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在空间推理任务上的不足，当前训练数据生成管道生成的数据集质量低。

Method: 提出GRAID框架，仅基于标准目标检测器的2D边界框确定定性空间关系，避免3D重建误差和生成幻觉。

Result: 应用该框架在多个数据集生成超850万个高质量VQA对，评估显示其生成数据集的人工验证准确率达91.16%，训练的模型泛化能力提升。

Conclusion: GRAID框架能生成高质量数据集，有效提升模型空间推理能力和泛化能力。

Abstract: Vision Language Models (VLMs) achieve strong performance on many
vision-language tasks but often struggle with spatial reasoning\textemdash{}a
prerequisite for many applications. Empirically, we find that a dataset
produced by a current training data generation pipeline has a 57.6\% human
validation rate. These rates stem from current limitations: single-image 3D
reconstruction introduces cascading modeling errors and requires wide answer
tolerances, while caption-based methods require hyper-detailed annotations and
suffer from generative hallucinations. We present GRAID, built on the key
insight that qualitative spatial relationships can be reliably determined from
2D geometric primitives alone. By operating exclusively on 2D bounding boxes
from standard object detectors, GRAID avoids both 3D reconstruction errors and
generative hallucinations, resulting in datasets that are of higher quality
than existing tools that produce similar datasets as validated by human
evaluations. We apply our framework to the BDD100k, NuImages, and Waymo
datasets, generating over 8.5 million high-quality VQA pairs creating questions
spanning spatial relations, counting, ranking, and size comparisons. We
evaluate one of the datasets and find it achieves 91.16\% human-validated
accuracy\textemdash{}compared to 57.6\% on a dataset generated by recent work.
% or recent work Critically, we demonstrate that when trained on GRAID data,
models learn spatial reasoning concepts that generalize: models fine-tuned on 6
question types improve on over 10 held-out types, with accuracy gains of 47.5\%
on BDD and 37.9\% on NuImages for Llama 3.2B 11B, and when trained on all
questions types, achieve improvements on several existing benchmarks such as
BLINK. The GRAID framework, datasets, and additional information can be found
on our \href{https://ke7.github.io/graid/}{project page}.

</details>


### [575] [Scaling Non-Parametric Sampling with Representation](https://arxiv.org/abs/2510.22196)
*Vincent Lu,Aaron Truong,Zeyu Yun,Yubei Chen*

Main category: cs.CV

TL;DR: 提出简单非参数生成模型，基于自然图像三原则，在MNIST和CIFAR - 10上表现好，揭示泛化机制。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型机制不透明，不追求扩展规模，旨在去除复杂技巧，提出简单模型。

Method: 基于自然图像的空间非平稳性、低级规律性和高级语义三个原则，从局部上下文窗口定义每个像素的分布。

Result: 模型在MNIST上产生高保真样本，在CIFAR - 10上生成视觉效果好的图像。

Conclusion: 该模型简单且性能强，指向自然图像结构的最小理论，其白盒特性有助于理解泛化机制，揭示了“部分 - 整体泛化”过程，为大神经网络生成模型泛化学习提供假设。

Abstract: Scaling and architectural advances have produced strikingly photorealistic
image generative models, yet their mechanisms still remain opaque. Rather than
advancing scaling, our goal is to strip away complicated engineering tricks and
propose a simple, non-parametric generative model. Our design is grounded in
three principles of natural images-(i) spatial non-stationarity, (ii) low-level
regularities, and (iii) high-level semantics-and defines each pixel's
distribution from its local context window. Despite its minimal architecture
and no training, the model produces high-fidelity samples on MNIST and visually
compelling CIFAR-10 images. This combination of simplicity and strong empirical
performance points toward a minimal theory of natural-image structure. The
model's white-box nature also allows us to have a mechanistic understanding of
how the model generalizes and generates diverse images. We study it by tracing
each generated pixel back to its source images. These analyses reveal a simple,
compositional procedure for "part-whole generalization", suggesting a
hypothesis for how large neural network generative models learn to generalize.

</details>


### [576] [GALA: A GlobAl-LocAl Approach for Multi-Source Active Domain Adaptation](https://arxiv.org/abs/2510.22214)
*Juepeng Zheng,Peifeng Zhang,Yibin Wen,Qingmei Li,Yang Zhang,Haohuan Fu*

Main category: cs.CV

TL;DR: 本文探索多源主动域适应（MS - ADA），提出GALA策略，实验表明其优于现有方法，用1%目标标注达接近全监督上限性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于适应的方法与全监督学习存在性能差距，探索MS - ADA以进一步提升目标域性能。

Method: 提出GALA策略，结合目标域样本全局k - means聚类和逐簇局部选择标准，可无缝集成到现有DA框架。

Result: 在三个标准DA基准上，GALA始终优于先前主动学习和主动DA方法，用1%目标标注达到接近全监督上限的性能。

Conclusion: GALA策略简单有效，能提升目标域性能，在域适应任务中有良好应用潜力。

Abstract: Domain Adaptation (DA) provides an effective way to tackle target-domain
tasks by leveraging knowledge learned from source domains. Recent studies have
extended this paradigm to Multi-Source Domain Adaptation (MSDA), which exploits
multiple source domains carrying richer and more diverse transferable
information. However, a substantial performance gap still remains between
adaptation-based methods and fully supervised learning. In this paper, we
explore a more practical and challenging setting, named Multi-Source Active
Domain Adaptation (MS-ADA), to further enhance target-domain performance by
selectively acquiring annotations from the target domain. The key difficulty of
MS-ADA lies in designing selection criteria that can jointly handle inter-class
diversity and multi-source domain variation. To address these challenges, we
propose a simple yet effective GALA strategy (GALA), which combines a global
k-means clustering step for target-domain samples with a cluster-wise local
selection criterion, effectively tackling the above two issues in a
complementary manner. Our proposed GALA is plug-and-play and can be seamlessly
integrated into existing DA frameworks without introducing any additional
trainable parameters. Extensive experiments on three standard DA benchmarks
demonstrate that GALA consistently outperforms prior active learning and active
DA methods, achieving performance comparable to the fully-supervised upperbound
while using only 1% of the target annotations.

</details>


### [577] [Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework](https://arxiv.org/abs/2510.22243)
*Amir Mohammad Khadem Hosseini,Sattar Mirzakuchaki*

Main category: cs.CV

TL;DR: 本文提出基于FPGA的实时语义分割实现方案，利用LMIINet架构和CGRA4ML框架，在Cityscapes数据集训练，取得一定精度和实时性能，证明CGRA4ML潜力。


<details>
  <summary>Details</summary>
Motivation: 解决语义分割在计算和硬件约束下实现高精度的挑战，满足实时应用需求。

Method: 采用轻量级LMIINet架构和CGRA4ML硬件框架，使用量化感知训练（QAT），对模型做必要修改以适应CGRA4ML约束。

Result: 实现约90%像素精度和45%平均交并比（mIoU），在ZCU104 FPGA板上以20帧每秒实时运行，延迟50.1毫秒。

Conclusion: CGRA4ML在映射现代层和片外内存利用方面有灵活性，为在FPGA上实现实时语义分割网络提供途径，在能效上优于传统GPU方案且精度有竞争力。

Abstract: Semantic segmentation has emerged as a fundamental problem in computer
vision, gaining particular importance in real-time applications such as
autonomous driving. The main challenge is achieving high accuracy while
operating under computational and hardware constraints. In this research, we
present an FPGA-based implementation of real-time semantic segmentation
leveraging the lightweight LMIINet architecture and the Coarse-Grained
Reconfigurable Array for Machine Learning (CGRA4ML) hardware framework. The
model was trained using Quantization-Aware Training (QAT) with 8-bit precision
on the Cityscapes dataset, reducing memory footprint by a factor of four while
enabling efficient fixed-point computations. Necessary modifications were
applied to adapt the model to CGRA4ML constraints, including simplifying skip
connections, employing hardware-friendly operations such as depthwise-separable
and 1A-1 convolutions, and redesigning parts of the Flatten Transformer. Our
implementation achieves approximately 90% pixel accuracy and 45% mean
Intersection-over-Union (mIoU), operating in real-time at 20 frames per second
(FPS) with 50.1 ms latency on the ZCU104 FPGA board. The results demonstrate
the potential of CGRA4ML, with its flexibility in mapping modern layers and
off-chip memory utilization for skip connections, provides a path for
implementing advanced semantic segmentation networks on FPGA for real-time
applications to outperform traditional GPU solutions in terms of power
efficiency while maintaining competitive accuracy. The code for this project is
publicly available at https://github.com/STAmirr/ cgra4ml_semantic_segmentation

</details>


### [578] [CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning](https://arxiv.org/abs/2510.22282)
*Tianhui Liu,Hetian Pang,Xin Zhang,Jie Feng,Yong Li,Pan Hui*

Main category: cs.CV

TL;DR: 本文提出CityRiSE框架，结合强化学习和大视觉语言模型进行城市社会经济状态推理，实验显示其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在从视觉数据进行准确且可解释的社会经济预测方面存在困难，需解决这些局限并发挥其潜力。

Method: 引入CityRiSE框架，通过纯强化学习，利用精心策划的多模态数据和可验证的奖励设计，引导大视觉语言模型关注有语义意义的视觉线索。

Result: CityRiSE在新兴推理过程中显著优于现有基线，提高了预测准确性和在不同城市环境中的泛化能力。

Conclusion: 结合强化学习和大视觉语言模型在可解释和通用的城市社会经济感知方面有前景。

Abstract: Harnessing publicly available, large-scale web data, such as street view and
satellite imagery, urban socio-economic sensing is of paramount importance for
achieving global sustainable development goals. With the emergence of Large
Vision-Language Models (LVLMs), new opportunities have arisen to solve this
task by treating it as a multi-modal perception and understanding problem.
However, recent studies reveal that LVLMs still struggle with accurate and
interpretable socio-economic predictions from visual data. To address these
limitations and maximize the potential of LVLMs, we introduce
\textbf{CityRiSE}, a novel framework for \textbf{R}eason\textbf{i}ng urban
\textbf{S}ocio-\textbf{E}conomic status in LVLMs through pure reinforcement
learning (RL). With carefully curated multi-modal data and verifiable reward
design, our approach guides the LVLM to focus on semantically meaningful visual
cues, enabling structured and goal-oriented reasoning for generalist
socio-economic status prediction. Experiments demonstrate that CityRiSE with
emergent reasoning process significantly outperforms existing baselines,
improving both prediction accuracy and generalization across diverse urban
contexts, particularly for prediction on unseen cities and unseen indicators.
This work highlights the promise of combining RL and LVLMs for interpretable
and generalist urban socio-economic sensing.

</details>


### [579] [Moving Beyond Diffusion: Hierarchy-to-Hierarchy Autoregression for fMRI-to-Image Reconstruction](https://arxiv.org/abs/2510.22335)
*Xu Zhang,Ruijie Quan,Wenguan Wang,Yi Yang*

Main category: cs.CV

TL;DR: 提出MindHier框架用于fMRI到图像重建，比扩散方法更优


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法用固定指导，会丢失分层神经信息，与图像重建阶段需求不匹配

Method: 提出MindHier框架，含Hierarchical fMRI Encoder、Hierarchy-to-Hierarchy Alignment、Scale-Aware Coarse-to-Fine Neural Guidance三个组件

Result: 在NSD数据集实验显示，MindHier语义保真度高、推理快4.67倍、结果更具确定性

Conclusion: MindHier是扩散方法的有效替代，能实现分层重建，类似人类视觉感知

Abstract: Reconstructing visual stimuli from fMRI signals is a central challenge
bridging machine learning and neuroscience. Recent diffusion-based methods
typically map fMRI activity to a single high-level embedding, using it as fixed
guidance throughout the entire generation process. However, this fixed guidance
collapses hierarchical neural information and is misaligned with the
stage-dependent demands of image reconstruction. In response, we propose
MindHier, a coarse-to-fine fMRI-to-image reconstruction framework built on
scale-wise autoregressive modeling. MindHier introduces three components: a
Hierarchical fMRI Encoder to extract multi-level neural embeddings, a
Hierarchy-to-Hierarchy Alignment scheme to enforce layer-wise correspondence
with CLIP features, and a Scale-Aware Coarse-to-Fine Neural Guidance strategy
to inject these embeddings into autoregression at matching scales. These
designs make MindHier an efficient and cognitively-aligned alternative to
diffusion-based methods by enabling a hierarchical reconstruction process that
synthesizes global semantics before refining local details, akin to human
visual perception. Extensive experiments on the NSD dataset show that MindHier
achieves superior semantic fidelity, 4.67x faster inference, and more
deterministic results than the diffusion-based baselines.

</details>


### [580] [T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for Diffusion Models](https://arxiv.org/abs/2510.22366)
*Jindong Yang,Han Fang,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CV

TL;DR: 提出基于Tail - Truncated Sampling的T2SMark水印方案，在扩散模型中平衡水印鲁棒性和生成多样性。


<details>
  <summary>Details</summary>
Motivation: 现有Noise - as - Watermark（NaW）方法难以平衡水印鲁棒性和生成多样性，部分方法限制初始噪声采样影响用户体验，部分方法鲁棒性差无法实际应用。

Method: 提出T2SMark，通过Tail - Truncated Sampling（TTS）将比特嵌入可靠尾部区域增强鲁棒性，随机采样中心区域保持潜在分布；两阶段框架将随机生成的会话密钥集成到加密管道确保采样多样性。

Result: 在具有U - Net和DiT骨干的扩散模型上评估，实验表明T2SMark能实现鲁棒性和多样性的最佳平衡。

Conclusion: T2SMark有效解决了现有NaW方法的问题，实现了鲁棒性和多样性的平衡，代码开源。

Abstract: Diffusion models have advanced rapidly in recent years, producing
high-fidelity images while raising concerns about intellectual property
protection and the misuse of generative AI. Image watermarking for diffusion
models, particularly Noise-as-Watermark (NaW) methods, encode watermark as
specific standard Gaussian noise vector for image generation, embedding the
infomation seamlessly while maintaining image quality. For detection, the
generation process is inverted to recover the initial noise vector containing
the watermark before extraction. However, existing NaW methods struggle to
balance watermark robustness with generation diversity. Some methods achieve
strong robustness by heavily constraining initial noise sampling, which
degrades user experience, while others preserve diversity but prove too fragile
for real-world deployment. To address this issue, we propose T2SMark, a
two-stage watermarking scheme based on Tail-Truncated Sampling (TTS). Unlike
prior methods that simply map bits to positive or negative values, TTS enhances
robustness by embedding bits exclusively in the reliable tail regions while
randomly sampling the central zone to preserve the latent distribution. Our
two-stage framework then ensures sampling diversity by integrating a randomly
generated session key into both encryption pipelines. We evaluate T2SMark on
diffusion models with both U-Net and DiT backbones. Extensive experiments show
that it achieves an optimal balance between robustness and diversity. Our code
is available at
\href{https://github.com/0xD009/T2SMark}{https://github.com/0xD009/T2SMark}.

</details>


### [581] [Efficient Large-Deformation Medical Image Registration via Recurrent Dynamic Correlation](https://arxiv.org/abs/2510.22380)
*Tianran Li,Marius Staring,Yuchuan Qiao*

Main category: cs.CV

TL;DR: 提出基于循环相关性的框架处理医学图像大形变配准，实验显示有良好的准确率 - 计算权衡，超越或媲美现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法处理医学图像大形变配准有挑战，体素到区域匹配无法捕捉长距离对应关系。

Method: 提出循环相关性框架，动态重新定位匹配区域；使用轻量级循环更新模块，解耦运动和纹理特征。

Result: 在脑 MRI 和腹部 CT 数据集上实验，方法有强准确率 - 计算权衡，如在非仿射 OASIS 数据集上表现相当，但使用 FLOPs 仅 9.5%，速度快 96%。

Conclusion: 所提方法在医学图像大形变配准中能高效收敛，性能超越或匹配现有方法。

Abstract: Deformable image registration estimates voxel-wise correspondences between
images through spatial transformations, and plays a key role in medical
imaging. While deep learning methods have significantly reduced runtime,
efficiently handling large deformations remains a challenging task.
Convolutional networks aggregate local features but lack direct modeling of
voxel correspondences, promoting recent works to explore explicit feature
matching. Among them, voxel-to-region matching is more efficient for direct
correspondence modeling by computing local correlation features whithin
neighbourhoods, while region-to-region matching incurs higher redundancy due to
excessive correlation pairs across large regions. However, the inherent
locality of voxel-to-region matching hinders the capture of long-range
correspondences required for large deformations. To address this, we propose a
Recurrent Correlation-based framework that dynamically relocates the matching
region toward more promising positions. At each step, local matching is
performed with low cost, and the estimated offset guides the next search
region, supporting efficient convergence toward large deformations. In
addition, we uses a lightweight recurrent update module with memory capacity
and decouples motion-related and texture features to suppress semantic
redundancy. We conduct extensive experiments on brain MRI and abdominal CT
datasets under two settings: with and without affine pre-registration. Results
show that our method exibits a strong accuracy-computation trade-off,
surpassing or matching the state-of-the-art performance. For example, it
achieves comparable performance on the non-affine OASIS dataset, while using
only 9.5% of the FLOPs and running 96% faster than RDP, a representative
high-performing method.

</details>


### [582] [Multi-Agent Pose Uncertainty: A Differentiable Rendering Cramér-Rao Bound](https://arxiv.org/abs/2510.21785)
*Arun Muthukkumar*

Main category: cs.CV

TL;DR: 本文推导了相机位姿估计协方差的闭式下界，方法可扩展到多智能体场景，有下游应用。


<details>
  <summary>Details</summary>
Motivation: 现有工作在稠密或学习模型下对姿态的严格不确定性量化较少，需要改进。

Method: 将可微渲染器作为测量函数，对图像形成关于流形上的小姿态扰动进行线性化，得到渲染感知的Cramér - Rao界。

Result: 方法可简化为经典的束调整不确定性，能自然扩展到多智能体场景。

Conclusion: 统计公式在合作感知和新视图合成等任务中有下游应用，无需显式关键点对应。

Abstract: Pose estimation is essential for many applications within computer vision and
robotics. Despite its uses, few works provide rigorous uncertainty
quantification for poses under dense or learned models. We derive a closed-form
lower bound on the covariance of camera pose estimates by treating a
differentiable renderer as a measurement function. Linearizing image formation
with respect to a small pose perturbation on the manifold yields a render-aware
Cram\'er-Rao bound. Our approach reduces to classical bundle-adjustment
uncertainty, ensuring continuity with vision theory. It also naturally extends
to multi-agent settings by fusing Fisher information across cameras. Our
statistical formulation has downstream applications for tasks such as
cooperative perception and novel view synthesis without requiring explicit
keypoint correspondences.

</details>


### [583] [AI-Boosted Video Annotation: Assessing the Process Enhancement](https://arxiv.org/abs/2510.21798)
*Juan Gutiérrez,Ángel Mora,Pablo Regodón,Silvia Rodriguez,José Luis Blanco*

Main category: cs.CV

TL;DR: 研究通过集成自动功能增强人在环视频标注，用Label Studio和AI零样本预标注实现方案，测试表明AI预标注可优化流程、减少时间且提高标注一致性。


<details>
  <summary>Details</summary>
Motivation: 探索集成自动功能以减轻标注人员任务负担并评估其表现，研究标注过程的实际影响及集成AI组件的效果。

Method: 实施基于Label Studio和AI零样本预标注的单迭代方案，基于UCF - Crime数据集设计测试。

Result: AI预标注可优化视频标注工作流，70%的标注人员使用预标注数据时标注时间减少35%，标注质量相当，标注更连贯且与视频帧自然聚类更匹配。

Conclusion: 自动的基于AI的预标注能优化视频标注流程，赋能人类标注人员。

Abstract: We explore the enhancement of Human-in-the-Loop video annotation by
integrating automatic capabilities to ease the task for annotators and assess
their performance. The research delves into the practical implications of the
annotation processes, the integration of AI components, and the evaluation of
its outcomes. We analyze their impact on efficiency, accuracy, and overall
annotation quality. Focusing on the Human-in-the-Loop for video annotation
tasks, we implemented a single-iteration scheme using Label Studio and
AI-powered zero-shot pre-annotations. Using this framework, we designed a test
based on the annotation of the UCF-Crime dataset to discriminate between normal
and abnormal activities in video footage. Our results evidence how automatic
AI-based pre-annotation can streamline the video annotation workflow,
empowering human annotators and optimizing the overall pipeline. Using the
pre-annotated data, we observed a 35% reduction in the annotation time for 70%
of the annotators with similar quality annotations, compared to the traditional
manual annotation task. Results are consistent with asset duration and
complexity. We also observed that while annotators rapidly learned to use the
tool, the produced annotations are more coherent among annotators and better
match the natural clustering of the video frames.

</details>


### [584] [Morphology-Aware KOA Classification: Integrating Graph Priors with Vision Models](https://arxiv.org/abs/2510.21801)
*Marouane Tliba,Mohamed Amine Kerkouri,Yassine Nasser,Nour Aburaed,Aladine Chetouani,Ulas Bagci,Rachid Jennane*

Main category: cs.CV

TL;DR: 提出结合解剖结构与放射特征的多模态框架诊断膝骨关节炎，实验显示性能超基线和现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准深度学习模型难以有效捕捉X光片中膝骨关节炎细微形态细节，诊断有挑战。

Method: 提出多模态框架，结合解剖结构与放射特征，用SAM分割得到形态图表示并与视觉编码器集成，通过互信息最大化使图嵌入与放射特征对齐。

Result: 在OAI数据集实验中，比单模态基线准确率高10%，比现有最优方法准确率高8%、F1分数高11%。

Conclusion: 在X光分析中纳入解剖结构对准确分级膝骨关节炎严重程度至关重要。

Abstract: Knee osteoarthritis (KOA) diagnosis from radiographs remains challenging due
to the subtle morphological details that standard deep learning models struggle
to capture effectively. We propose a novel multimodal framework that combines
anatomical structure with radiographic features by integrating a morphological
graph representation - derived from Segment Anything Model (SAM) segmentations
- with a vision encoder. Our approach enforces alignment between
geometry-informed graph embeddings and radiographic features through mutual
information maximization, significantly improving KOA classification accuracy.
By constructing graphs from anatomical features, we introduce explicit
morphological priors that mirror clinical assessment criteria, enriching the
feature space and enhancing the model's inductive bias. Experiments on the
Osteoarthritis Initiative dataset demonstrate that our approach surpasses
single-modality baselines by up to 10\% in accuracy (reaching nearly 80\%),
while outperforming existing state-of-the-art methods by 8\% in accuracy and
11\% in F1 score. These results underscore the critical importance of
incorporating anatomical structure into radiographic analysis for accurate KOA
severity grading.

</details>


### [585] [Top-Down Semantic Refinement for Image Captioning](https://arxiv.org/abs/2510.22391)
*Jusheng Zhang,Kaitong Cai,Jing Yang,Jian Wang,Chengpei Tang,Keze Wang*

Main category: cs.CV

TL;DR: 本文指出大视觉语言模型图像描述存在决策短视问题，提出TDSR框架和适配的MCTS算法，降低调用VLM频率，实验显示能提升现有VLMs性能。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在图像描述时存在决策短视问题，难以兼顾全局叙事连贯和细节捕捉，尤其在多步复杂场景描述任务中。

Method: 将图像描述重新定义为目标导向的分层细化规划问题，提出TDSR框架，将生成过程建模为马尔可夫决策过程，设计适配VLMs的高效蒙特卡罗树搜索（MCTS）算法，引入视觉引导并行扩展和轻量级价值网络，还有自适应提前停止机制。

Result: 在多个基准测试中，TDSR作为即插即用模块，能显著提升现有VLMs（如LLaVA - 1.5、Qwen2.5 - VL）性能，在细粒度描述、组合泛化和幻觉抑制方面达到了最先进或极具竞争力的结果。

Conclusion: TDSR框架和MCTS算法能有效解决大视觉语言模型图像描述的决策短视问题，提升其性能。

Abstract: Large Vision-Language Models (VLMs) face an inherent contradiction in image
captioning: their powerful single-step generation capabilities often lead to a
myopic decision-making process. This makes it difficult to maintain global
narrative coherence while capturing rich details, a limitation that is
particularly pronounced in tasks that require multi-step and complex scene
description. To overcome this fundamental challenge, we redefine image
captioning as a goal-oriented hierarchical refinement planning problem, and
further propose a novel framework, named Top-Down Semantic Refinement (TDSR),
which models the generation process as a Markov Decision Process (MDP).
However, planning within the vast state space of a VLM presents a significant
computational hurdle. Our core contribution, therefore, is the design of a
highly efficient Monte Carlo Tree Search (MCTS) algorithm tailored for VLMs. By
incorporating a visual-guided parallel expansion and a lightweight value
network, our TDSR reduces the call frequency to the expensive VLM by an order
of magnitude without sacrificing planning quality. Furthermore, an adaptive
early stopping mechanism dynamically matches computational overhead to the
image's complexity. Extensive experiments on multiple benchmarks, including
DetailCaps, COMPOSITIONCAP, and POPE, demonstrate that our TDSR, as a
plug-and-play module, can significantly enhance the performance of existing
VLMs (e.g., LLaVA-1.5, Qwen2.5-VL) by achieving state-of-the-art or highly
competitive results in fine-grained description, compositional generalization,
and hallucination suppression.

</details>


### [586] [RatioWaveNet: A Learnable RDWT Front-End for Robust and Interpretable EEG Motor-Imagery Classification](https://arxiv.org/abs/2510.21841)
*Marco Siino,Giuseppe Bonomo,Rosario Sorbello,Ilenia Tinnirello*

Main category: cs.CV

TL;DR: 提出RatioWaveNet，在Transformer骨干上增加可训练的理性扩张小波变换前端，在BCI-IV-2a和2b数据集上提升了最差受试者的准确率，表明该小波前端能有效增强基于Transformer的BCI。


<details>
  <summary>Details</summary>
Motivation: 解决基于运动想象的脑机接口（BCI）从非侵入式脑电图中可靠解码的难题，测试小波前端能否提高BCI在最难受试者上的鲁棒性。

Method: 提出RatioWaveNet，用可训练的理性扩张小波变换（RDWT）前端增强TCFormer骨干，对信号进行多分辨率子带分解、融合，再通过多内核CNN、分组查询注意力编码器和紧凑TCN头进行特征提取和整合。

Result: 在BCI-IV-2a和2b数据集上，RatioWaveNet在五个随机种子下，提升了最差受试者的准确率，有一致的平均情况增益和适度的计算开销。

Conclusion: 简单可训练的小波前端是增强基于Transformer的BCI的有效插件，能在不牺牲效率的情况下提高最差情况的可靠性。

Abstract: Brain-computer interfaces (BCIs) based on motor imagery (MI) translate covert
movement intentions into actionable commands, yet reliable decoding from
non-invasive EEG remains challenging due to nonstationarity, low SNR, and
subject variability. We present RatioWaveNet, which augments a strong temporal
CNN-Transformer backbone (TCFormer) with a trainable, Rationally-Dilated
Wavelet Transform (RDWT) front end. The RDWT performs an undecimated,
multi-resolution subband decomposition that preserves temporal length and
shift-invariance, enhancing sensorimotor rhythms while mitigating jitter and
mild artifacts; subbands are fused via lightweight grouped 1-D convolutions and
passed to a multi-kernel CNN for local temporal-spatial feature extraction, a
grouped-query attention encoder for long-range context, and a compact TCN head
for causal temporal integration.
  Our goal is to test whether this principled wavelet front end improves
robustness precisely where BCIs typically fail - on the hardest subjects - and
whether such gains persist on average across seeds under both intra- and
inter-subject protocols. On BCI-IV-2a and BCI-IV-2b, across five seeds,
RatioWaveNet improves worst-subject accuracy over the Transformer backbone by
+0.17 / +0.42 percentage points (Sub-Dependent / LOSO) on 2a and by +1.07 /
+2.54 percentage points on 2b, with consistent average-case gains and modest
computational overhead. These results indicate that a simple, trainable wavelet
front end is an effective plug-in to strengthen Transformer-based BCIs,
improving worst-case reliability without sacrificing efficiency.

</details>


### [587] [DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose Alignment Loss](https://arxiv.org/abs/2510.22473)
*Jing Yang,Yufeng Yang*

Main category: cs.CV

TL;DR: 现有2D和3D生成模型进步但单张静态图像生成4D动态内容仍难，提出DynaPose4D框架，实验证明其效果好且有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单张静态图像生成高质量4D动态内容上有局限，难以建模时间依赖和捕捉动态几何变化。

Method: 提出DynaPose4D框架，结合4D高斯拼接技术和无类别姿态估计技术，用3D高斯拼接构建3D模型，基于单视图预测多视图姿态关键点并利用监督信号增强运动一致性。

Result: DynaPose4D在动态运动生成中实现了出色的连贯性、一致性和流畅性。

Conclusion: DynaPose4D框架有效，在计算机视觉和动画制作领域有潜在应用。

Abstract: Recent advancements in 2D and 3D generative models have expanded the
capabilities of computer vision. However, generating high-quality 4D dynamic
content from a single static image remains a significant challenge. Traditional
methods have limitations in modeling temporal dependencies and accurately
capturing dynamic geometry changes, especially when considering variations in
camera perspective. To address this issue, we propose DynaPose4D, an innovative
solution that integrates 4D Gaussian Splatting (4DGS) techniques with
Category-Agnostic Pose Estimation (CAPE) technology. This framework uses 3D
Gaussian Splatting to construct a 3D model from single images, then predicts
multi-view pose keypoints based on one-shot support from a chosen view,
leveraging supervisory signals to enhance motion consistency. Experimental
results show that DynaPose4D achieves excellent coherence, consistency, and
fluidity in dynamic motion generation. These findings not only validate the
efficacy of the DynaPose4D framework but also indicate its potential
applications in the domains of computer vision and animation production.

</details>


### [588] [Single-Teacher View Augmentation: Boosting Knowledge Distillation via Angular Diversity](https://arxiv.org/abs/2510.22480)
*Seonghoon Yu,Dongjun Nam,Dina Katabi,Jeany Son*

Main category: cs.CV

TL;DR: 提出高效知识增强方法用于知识蒸馏，通过单教师多分支生成多样视图，引入角度多样性目标，实验表现优且可即插即用。


<details>
  <summary>Details</summary>
Motivation: 现有利用多样教师视角提升蒸馏性能的方法需多教师网络，计算成本高。

Method: 给单教师附加多分支生成多样多视图，引入约束的视图间角度多样性损失和视图内角度多样性损失，将多样视图和原教师的集成知识蒸馏给学生，并理论证明目标增加集成成员多样性。

Result: 方法在不同配置下超越现有知识增强方法，且可与其他KD框架即插即用，一致提升泛化性能。

Conclusion: 提出的成本高效知识增强方法能有效提升知识蒸馏效果，具有实用性。

Abstract: Knowledge Distillation (KD) aims to train a lightweight student model by
transferring knowledge from a large, high-capacity teacher. Recent studies have
shown that leveraging diverse teacher perspectives can significantly improve
distillation performance; however, achieving such diversity typically requires
multiple teacher networks, leading to high computational costs. In this work,
we propose a novel cost-efficient knowledge augmentation method for KD that
generates diverse multi-views by attaching multiple branches to a single
teacher. To ensure meaningful semantic variation across multi-views, we
introduce two angular diversity objectives: 1) constrained inter-angle
diversify loss, which maximizes angles between augmented views while preserving
proximity to the original teacher output, and 2) intra-angle diversify loss,
which encourages an even distribution of views around the original output. The
ensembled knowledge from these angularly diverse views, along with the original
teacher, is distilled into the student. We further theoretically demonstrate
that our objectives increase the diversity among ensemble members and thereby
reduce the upper bound of the ensemble's expected loss, leading to more
effective distillation. Experimental results show that our method surpasses an
existing knowledge augmentation method across diverse configurations. Moreover,
the proposed method is compatible with other KD frameworks in a plug-and-play
fashion, providing consistent improvements in generalization performance.

</details>


### [589] [GateFuseNet: An Adaptive 3D Multimodal Neuroimaging Fusion Network for Parkinson's Disease Diagnosis](https://arxiv.org/abs/2510.22507)
*Rui Jin,Chen Chen,Yin Liu,Hongfu Sun,Min Zeng,Min Li,Yang Gao*

Main category: cs.CV

TL;DR: 提出GateFuseNet网络结合QSM和T1w图像诊断帕金森病，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 帕金森病症状多变、病理异质性大，传统基于幅度的MRI模态对其病理不敏感，需更有效的诊断方法。

Method: 提出GateFuseNet，利用门控融合模块学习特定模态注意力权重和通道门控向量，进行选择性特征调制。

Result: 优于三种现有方法，准确率达85.00%，AUC为92.06%，消融实验验证各部分贡献，Grad - CAM可视化显示关注临床相关病理区域。

Conclusion: GateFuseNet是一种有效的帕金森病MRI诊断方法。

Abstract: Accurate diagnosis of Parkinson's disease (PD) from MRI remains challenging
due to symptom variability and pathological heterogeneity. Most existing
methods rely on conventional magnitude-based MRI modalities, such as
T1-weighted images (T1w), which are less sensitive to PD pathology than
Quantitative Susceptibility Mapping (QSM), a phase-based MRI technique that
quantifies iron deposition in deep gray matter nuclei. In this study, we
propose GateFuseNet, an adaptive 3D multimodal fusion network that integrates
QSM and T1w images for PD diagnosis. The core innovation lies in a gated fusion
module that learns modality-specific attention weights and channel-wise gating
vectors for selective feature modulation. This hierarchical gating mechanism
enhances ROI-aware features while suppressing irrelevant signals. Experimental
results show that our method outperforms three existing state-of-the-art
approaches, achieving 85.00% accuracy and 92.06% AUC. Ablation studies further
validate the contributions of ROI guidance, multimodal integration, and fusion
positioning. Grad-CAM visualizations confirm the model's focus on clinically
relevant pathological regions. The source codes and pretrained models can be
found at https://github.com/YangGaoUQ/GateFuseNet

</details>


### [590] [FlowOpt: Fast Optimization Through Whole Flow Processes for Training-Free Editing](https://arxiv.org/abs/2510.22010)
*Or Ronai,Vladimir Kulikov,Tomer Michaeli*

Main category: cs.CV

TL;DR: 提出零阶优化框架FlowOpt用于扩散和流匹配模型的可控生成任务，实现高效优化并取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 扩散和流匹配模型在可控生成任务中，因采样过程的迭代性，基于梯度的优化计算不可行，现有方法通常分别处理每个时间步。

Method: 引入零阶（无梯度）优化框架FlowOpt，将整个流过程视为黑盒，无需通过模型反向传播进行优化，证明了步长的充分条件并展示如何经验估计该上界。

Result: FlowOpt在图像编辑任务中实现了最先进的结果，使用的神经函数评估次数与现有方法大致相同。

Conclusion: FlowOpt是一种高效的优化框架，可用于扩散和流匹配模型的可控生成任务，在图像编辑中表现出色。

Abstract: The remarkable success of diffusion and flow-matching models has ignited a
surge of works on adapting them at test time for controlled generation tasks.
Examples range from image editing to restoration, compression and
personalization. However, due to the iterative nature of the sampling process
in those models, it is computationally impractical to use gradient-based
optimization to directly control the image generated at the end of the process.
As a result, existing methods typically resort to manipulating each timestep
separately. Here we introduce FlowOpt - a zero-order (gradient-free)
optimization framework that treats the entire flow process as a black box,
enabling optimization through the whole sampling path without backpropagation
through the model. Our method is both highly efficient and allows users to
monitor the intermediate optimization results and perform early stopping if
desired. We prove a sufficient condition on FlowOpt's step-size, under which
convergence to the global optimum is guaranteed. We further show how to
empirically estimate this upper bound so as to choose an appropriate step-size.
We demonstrate how FlowOpt can be used for image editing, showcasing two
options: (i) inversion (determining the initial noise that generates a given
image), and (ii) directly steering the edited image to be similar to the source
image while conforming to a target text prompt. In both cases, FlowOpt achieves
state-of-the-art results while using roughly the same number of neural function
evaluations (NFEs) as existing methods. Code and examples are available on the
project's webpage.

</details>


### [591] [STATUS Bench: A Rigorous Benchmark for Evaluating Object State Understanding in Vision-Language Models](https://arxiv.org/abs/2510.22571)
*Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 本文提出STATUS Bench和STATUS Train数据集，评估VLMs识别物体状态能力，发现现有模型存在不足，微调后Qwen2.5 - VL表现可与Gemini 2.0 Flash相当。


<details>
  <summary>Details</summary>
Motivation: 现有Vision - Language Models（VLMs）识别物体状态能力不明确，需评估其理解物体状态细微变化的能力。

Method: 提出STATUS Bench基准，包含物体状态识别、图像检索和状态变化识别三个任务；引入包含1300万条半自动化描述的STATUS Train训练数据集。

Result: STATUS Bench可进行严格一致性评估，当前先进VLMs难以捕捉物体状态细微差别，多数开放权重VLMs零样本表现为随机水平，Qwen2.5 - VL微调后表现与Gemini 2.0 Flash相当。

Conclusion: STATUS Bench和Train对于推进VLM研究中的物体状态识别是必要的。

Abstract: Object state recognition aims to identify the specific condition of objects,
such as their positional states (e.g., open or closed) and functional states
(e.g., on or off). While recent Vision-Language Models (VLMs) are capable of
performing a variety of multimodal tasks, it remains unclear how precisely they
can identify object states. To alleviate this issue, we introduce the STAte and
Transition UnderStanding Benchmark (STATUS Bench), the first benchmark for
rigorously evaluating the ability of VLMs to understand subtle variations in
object states in diverse situations. Specifically, STATUS Bench introduces a
novel evaluation scheme that requires VLMs to perform three tasks
simultaneously: object state identification (OSI), image retrieval (IR), and
state change identification (SCI). These tasks are defined over our fully
hand-crafted dataset involving image pairs, their corresponding object state
descriptions and state change descriptions. Furthermore, we introduce a
large-scale training dataset, namely STATUS Train, which consists of 13 million
semi-automatically created descriptions. This dataset serves as the largest
resource to facilitate further research in this area. In our experiments, we
demonstrate that STATUS Bench enables rigorous consistency evaluation and
reveal that current state-of-the-art VLMs still significantly struggle to
capture subtle object state distinctions. Surprisingly, under the proposed
rigorous evaluation scheme, most open-weight VLMs exhibited chance-level
zero-shot performance. After fine-tuning on STATUS Train, Qwen2.5-VL achieved
performance comparable to Gemini 2.0 Flash. These findings underscore the
necessity of STATUS Bench and Train for advancing object state recognition in
VLM research.

</details>


### [592] [Mint: A Simple Test-Time Adaptation of Vision-Language Models against Common Corruptions](https://arxiv.org/abs/2510.22127)
*Wenxuan Bao,Ruxi Deng,Jingrui He*

Main category: cs.CV

TL;DR: 研究腐败输入对CLIP图像嵌入的影响，发现嵌入方差崩溃现象，提出Mint方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉 - 语言模型CLIP虽有零样本泛化能力，但易受输入腐败导致的分布偏移影响。

Method: 分析腐败输入对嵌入空间结构的改变，提出Mint方法，利用均值和梯度累加器动态最大化基于伪标签的类间方差。

Result: 发现嵌入方差崩溃现象，理论证明最大化类间方差可提升嵌入质量，Mint方法在多腐败基准和CLIP架构上提升性能。

Conclusion: Mint方法能有效应对输入腐败，提升CLIP在腐败数据上的性能。

Abstract: Pretrained vision-language models such as CLIP achieve strong zero-shot
generalization but remain vulnerable to distribution shifts caused by input
corruptions. In this work, we investigate how corruptions affect CLIP's image
embeddings and uncover a consistent phenomenon we term as embedding variance
collapse, where both intra-class and inter-class variances shrink as corruption
severity increases. We find that this collapse is closely tied to performance
degradation, with inter-class variance strongly correlated with classification
accuracy. To explain this phenomenon, we analyze how corruptions alter the
structure of the embedding space. Our theoretical results suggest that the
visual encoder tends to encode corruption-related signals, which dilute
class-discriminative features and compress the representation geometry. We
further show that maximizing inter-class variance, even when estimated from
pseudo-labels, can provably enhance embedding quality. Based on this insight,
we propose Mint, a simple test-time adaptation method that maximizes
pseudo-label-based inter-class variance on the fly using a mean accumulator and
a gradient accumulator. Mint operates effectively with small batch sizes and
consistently improves performance across multiple corruption benchmarks and
CLIP architectures. Our code is available at https://github.com/baowenxuan/Mint .

</details>


### [593] [LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction](https://arxiv.org/abs/2510.22141)
*Yuhang Gao,Xiang Xiang,Sheng Zhong,Guoyou Wang*

Main category: cs.CV

TL;DR: 提出LOC框架解决3D场景理解中数据集有限问题，引入DCL提升性能，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 3D数据集有限阻碍VLMs在3D场景理解中的有效应用。

Method: 提出LOC框架，支持监督和自监督学习；自监督任务中融合多帧LiDAR点，用Poisson重建和KNN获取体素表示；引入DCL缓解特征过同质化。

Result: 在nuScenes数据集实验中表现优越，能高精度预测已知类并区分未知类，无需额外训练数据。

Conclusion: LOC框架结合DCL能有效提升3D场景理解中的开放集识别性能。

Abstract: Vision-Language Models (VLMs) have shown significant progress in open-set
challenges. However, the limited availability of 3D datasets hinders their
effective application in 3D scene understanding. We propose LOC, a general
language-guided framework adaptable to various occupancy networks, supporting
both supervised and self-supervised learning paradigms. For self-supervised
tasks, we employ a strategy that fuses multi-frame LiDAR points for
dynamic/static scenes, using Poisson reconstruction to fill voids, and
assigning semantics to voxels via K-Nearest Neighbor (KNN) to obtain
comprehensive voxel representations. To mitigate feature over-homogenization
caused by direct high-dimensional feature distillation, we introduce Densely
Contrastive Learning (DCL). DCL leverages dense voxel semantic information and
predefined textual prompts. This efficiently enhances open-set recognition
without dense pixel-level supervision, and our framework can also leverage
existing ground truth to further improve performance. Our model predicts dense
voxel features embedded in the CLIP feature space, integrating textual and
image pixel information, and classifies based on text and semantic similarity.
Experiments on the nuScenes dataset demonstrate the method's superior
performance, achieving high-precision predictions for known classes and
distinguishing unknown classes without additional training data.

</details>


### [594] [Cross-Species Transfer Learning in Agricultural AI: Evaluating ZebraPose Adaptation for Dairy Cattle Pose Estimation](https://arxiv.org/abs/2510.22618)
*Mackenzie Tapp,Sibi Chakravarthy Parivendan,Kashfia Sailunaz,Suresh Neethirajan*

Main category: cs.CV

TL;DR: 研究评估ZebraPose模型跨物种迁移学习用于奶牛关键点检测的潜力与局限，指出合成到真实域的差距是农业AI部署障碍，呼吁农业优先的AI设计。


<details>
  <summary>Details</summary>
Motivation: 农业中缺乏大型标注畜牧数据集，尤其是奶牛，需评估跨物种迁移学习在奶牛姿态估计中的潜力和局限。

Method: 将基于合成斑马图像训练的ZebraPose模型用于奶牛27个关键点检测，用三种数据集配置评估模型准确性和泛化性。

Result: 组合模型在分布内数据上表现良好，但应用于未见牛舍和奶牛群体时泛化失败。

Conclusion: 合成到真实域的差距是农业AI部署的主要障碍，形态相似不足以实现跨域迁移，应优先进行农业优先的AI设计。

Abstract: Pose estimation serves as a cornerstone of computer vision for understanding
animal posture, behavior, and welfare. Yet, agricultural applications remain
constrained by the scarcity of large, annotated datasets for livestock,
especially dairy cattle. This study evaluates the potential and limitations of
cross-species transfer learning by adapting ZebraPose - a vision
transformer-based model trained on synthetic zebra imagery - for 27-keypoint
detection in dairy cows under real barn conditions. Using three configurations
- a custom on-farm dataset (375 images, Sussex, New Brunswick, Canada), a
subset of the APT-36K benchmark dataset, and their combination, we
systematically assessed model accuracy and generalization across environments.
While the combined model achieved promising performance (AP = 0.86, AR = 0.87,
PCK 0.5 = 0.869) on in-distribution data, substantial generalization failures
occurred when applied to unseen barns and cow populations. These findings
expose the synthetic-to-real domain gap as a major obstacle to agricultural AI
deployment and emphasize that morphological similarity between species is
insufficient for cross-domain transfer. The study provides practical insights
into dataset diversity, environmental variability, and computational
constraints that influence real-world deployment of livestock monitoring
systems. We conclude with a call for agriculture-first AI design, prioritizing
farm-level realism, cross-environment robustness, and open benchmark datasets
to advance trustworthy and scalable animal-centric technologies.

</details>


### [595] [A Critical Study on Tea Leaf Disease Detection using Deep Learning Techniques](https://arxiv.org/abs/2510.22647)
*Nabajyoti Borah,Raju Moni Borah,Bandan Boruah,Purnendu Bikash Acharjee,Sajal Saha,Ripjyoti Hazarika*

Main category: cs.CV

TL;DR: 本文提出用深度学习技术对三种茶叶病害分类并显示受损面积，评估SSD MobileNet V2和Faster R - CNN ResNet50 V1进行目标检测，用Mask R - CNN进行实例分割。


<details>
  <summary>Details</summary>
Motivation: 对三种茶叶病害进行分类并显示叶片受病害损伤的面积。

Method: 评估SSD MobileNet V2和Faster R - CNN ResNet50 V1进行目标检测，用Mask R - CNN进行目标实例分割，并自定义方法计算叶片受损部分。

Result: Faster R - CNN ResNet50 V1在目标检测上表现优于SSD MobileNet V2，前者mAP为25%，后者为20.9%。

Conclusion: Faster R - CNN ResNet50 V1更适合用于茶叶病害目标检测任务。

Abstract: The proposed solution is Deep Learning Technique that will be able classify
three types of tea leaves diseases from which two diseases are caused by the
pests and one due to pathogens (infectious organisms) and environmental
conditions and also show the area damaged by a disease in leaves. Namely Red
Rust, Helopeltis and Red spider mite respectively. In this paper we have
evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for
the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU
range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.
While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95
and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than
SSD. Also used Mask R-CNN for Object Instance Segmentation where we have
implemented our custom method to calculate the damaged diseased portion of
leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red
Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.

</details>


### [596] [SARCLIP: A Vision Language Foundation Model for Semantic Understanding and Target Recognition in SAR Imagery](https://arxiv.org/abs/2510.22665)
*Qiwei Ma,Zhiyu Wang,Wang Liu,Xukun Lu,Bin Deng,Puhong Duan,Xudong Kang,Shutao Li*

Main category: cs.CV

TL;DR: 本文构建SARCLIP - 1M数据集，提出首个SAR领域视觉语言基础模型SARCLIP，实验显示其性能优越，代码和数据集将发布。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习和MIM方法主要关注SAR图像低级视觉特征，忽略多模态对齐和零样本目标识别，本文旨在解决此局限。

Method: 构建包含超100万文本 - 图像对的SARCLIP - 1M数据集，采用对比视觉语言学习方法和领域转移策略训练SARCLIP模型。

Result: 在图像 - 文本检索和零样本分类任务实验中，SARCLIP在特征提取和解释方面表现优越，显著超越现有基础模型。

Conclusion: SARCLIP能缩小SAR图像和文本描述差距，提升SAR图像语义理解。

Abstract: Synthetic Aperture Radar (SAR) has emerged as a crucial imaging modality due
to its all-weather capabilities. While recent advancements in self-supervised
learning and Masked Image Modeling (MIM) have paved the way for SAR foundation
models, these approaches primarily focus on low-level visual features, often
overlooking multimodal alignment and zero-shot target recognition within SAR
imagery. To address this limitation, we construct SARCLIP-1M, a large-scale
vision language dataset comprising over one million text-image pairs aggregated
from existing datasets. We further introduce SARCLIP, the first vision language
foundation model tailored for the SAR domain. Our SARCLIP model is trained
using a contrastive vision language learning approach by domain transferring
strategy, enabling it to bridge the gap between SAR imagery and textual
descriptions. Extensive experiments on image-text retrieval and zero-shot
classification tasks demonstrate the superior performance of SARCLIP in feature
extraction and interpretation, significantly outperforming state-of-the-art
foundation models and advancing the semantic understanding of SAR imagery. The
code and datasets will be released soon.

</details>


### [597] [LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical Explicit-Implicit Representation Collaboration Rendering](https://arxiv.org/abs/2510.22669)
*Wenkai Zhu,Xu Li,Qimin Xu,Benwu Wang,Kun Wei,Yiming Peng,Zihang Wang*

Main category: cs.CV

TL;DR: 提出LVD - GS的LiDAR - Visual 3D高斯拼接SLAM系统，在多数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯拼接SLAM方法依赖单一表示方案，在大规模动态户外场景中性能受限，存在累积位姿误差和尺度模糊问题。

Method: 引入分层协作表示模块促进映射优化，提出联合动态建模模块融合开放世界分割与隐式残差约束生成细粒度动态掩码。

Result: 在KITTI、nuScenes和自采集数据集上评估，相比现有方法达到了SOTA性能。

Conclusion: 所提LVD - GS系统能有效解决现有方法在大规模动态户外场景中的问题，提高了重建的鲁棒性。

Abstract: 3D Gaussian Splatting SLAM has emerged as a widely used technique for
high-fidelity mapping in spatial intelligence. However, existing methods often
rely on a single representation scheme, which limits their performance in
large-scale dynamic outdoor scenes and leads to cumulative pose errors and
scale ambiguity. To address these challenges, we propose \textbf{LVD-GS}, a
novel LiDAR-Visual 3D Gaussian Splatting SLAM system. Motivated by the human
chain-of-thought process for information seeking, we introduce a hierarchical
collaborative representation module that facilitates mutual reinforcement for
mapping optimization, effectively mitigating scale drift and enhancing
reconstruction robustness. Furthermore, to effectively eliminate the influence
of dynamic objects, we propose a joint dynamic modeling module that generates
fine-grained dynamic masks by fusing open-world segmentation with implicit
residual constraints, guided by uncertainty estimates from DINO-Depth features.
Extensive evaluations on KITTI, nuScenes, and self-collected datasets
demonstrate that our approach achieves state-of-the-art performance compared to
existing methods.

</details>


### [598] [GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping](https://arxiv.org/abs/2510.22319)
*Jing Wang,Jiajun Liang,Jie Liu,Henglin Liu,Gongye Liu,Jun Zheng,Wanyuan Pang,Ao Ma,Zhenyu Xie,Xintao Wang,Meng Wang,Pengfei Wan,Xiaodan Liang*

Main category: cs.CV

TL;DR: 现有GRPO强化学习框架中重要性比率分布问题致策略模型隐式过优化，提出GRPO - Guard解决该问题并提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO框架中重要性比率分布左移且不一致，导致PPO裁剪机制失效，策略模型隐式过优化，影响实际应用。

Method: 引入GRPO - Guard，采用比率归一化恢复平衡且一致的重要性比率，使用梯度重加权策略平衡不同噪声条件下的策略梯度。

Result: 在多个扩散骨干网络和代理任务上的实验表明，GRPO - Guard显著减少过优化，维持或提升生成质量。

Conclusion: GRPO - Guard可作为一种调节裁剪机制，稳定优化过程，有效减轻隐式过优化，且无需大量KL正则化。

Abstract: Recently, GRPO-based reinforcement learning has shown remarkable progress in
optimizing flow-matching models, effectively improving their alignment with
task-specific rewards. Within these frameworks, the policy update relies on
importance-ratio clipping to constrain overconfident positive and negative
gradients. However, in practice, we observe a systematic shift in the
importance-ratio distribution-its mean falls below 1 and its variance differs
substantially across timesteps. This left-shifted and inconsistent distribution
prevents positive-advantage samples from entering the clipped region, causing
the mechanism to fail in constraining overconfident positive updates. As a
result, the policy model inevitably enters an implicit over-optimization
stage-while the proxy reward continues to increase, essential metrics such as
image quality and text-prompt alignment deteriorate sharply, ultimately making
the learned policy impractical for real-world use. To address this issue, we
introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO
frameworks. Our method incorporates ratio normalization, which restores a
balanced and step-consistent importance ratio, ensuring that PPO clipping
properly constrains harmful updates across denoising timesteps. In addition, a
gradient reweighting strategy equalizes policy gradients over noise conditions,
preventing excessive updates from particular timestep regions. Together, these
designs act as a regulated clipping mechanism, stabilizing optimization and
substantially mitigating implicit over-optimization without relying on heavy KL
regularization. Extensive experiments on multiple diffusion backbones (e.g.,
SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard
significantly reduces over-optimization while maintaining or even improving
generation quality.

</details>


### [599] [Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents](https://arxiv.org/abs/2510.22443)
*Vijay Veerabadran,Fanyi Xiao,Nitin Kamra,Pedro Matias,Joy Chen,Caley Drooff,Brett D Roads,Riley Williams,Ethan Henderson,Xuanyi Zhao,Kevin Carlberg,Joseph Tighe,Karl Ridgeway*

Main category: cs.CV

TL;DR: 本文聚焦从多模态上下文观测中推断用户目标的问题，创建了WAGIBench基准，收集新数据集评估视觉语言模型，发现人类表现优于模型，大模型表现更好但实用性低，模型受益于相关模态信息。


<details>
  <summary>Details</summary>
Motivation: 解决从多模态上下文观测中推断用户目标的问题，消除与可穿戴辅助代理交互的努力。

Method: 创建WAGIBench基准，收集含348名参与者、29小时多模态数据的新数据集，评估多种视觉语言模型，进行模态消融实验。

Result: 人类多项选择准确率达93%，优于最佳视觉语言模型的84%；大模型表现显著更好，但生成相关目标的比例仅55%；模型受益于相关模态额外信息，受无关模态影响小。

Conclusion: 目前视觉语言模型在目标推断任务上虽有进展，但距离实际应用仍有较大差距。

Abstract: There has been a surge of interest in assistive wearable agents: agents
embodied in wearable form factors (e.g., smart glasses) who take assistive
actions toward a user's goal/query (e.g. "Where did I leave my keys?"). In this
work, we consider the important complementary problem of inferring that goal
from multi-modal contextual observations. Solving this "goal inference" problem
holds the promise of eliminating the effort needed to interact with such an
agent. This work focuses on creating WAGIBench, a strong benchmark to measure
progress in solving this problem using vision-language models (VLMs). Given the
limited prior work in this area, we collected a novel dataset comprising 29
hours of multimodal data from 348 participants across 3,477 recordings,
featuring ground-truth goals alongside accompanying visual, audio, digital, and
longitudinal contextual observations. We validate that human performance
exceeds model performance, achieving 93% multiple-choice accuracy compared with
84% for the best-performing VLM. Generative benchmark results that evaluate
several families of modern vision-language models show that larger models
perform significantly better on the task, yet remain far from practical
usefulness, as they produce relevant goals only 55% of the time. Through a
modality ablation, we show that models benefit from extra information in
relevant modalities with minimal performance degradation from irrelevant
modalities.

</details>


### [600] [LLM-based Fusion of Multi-modal Features for Commercial Memorability Prediction](https://arxiv.org/abs/2510.22829)
*Aleksandar Pramov*

Main category: cs.CV

TL;DR: 本文聚焦MediaEval 2025工作坊竞赛中商业广告记忆性预测，提出基于Gemma - 3 LLM骨干的多模态融合系统，用LoRA适配，以梯度提升树集成作基线，使用LLM生成的理由提示，LLM系统在测试集表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决MediaEval 2025工作坊竞赛中“商业/广告记忆性”子任务里商业广告记忆性的预测问题。

Method: 提出基于Gemma - 3 LLM骨干的多模态融合系统，通过多模态投影整合预计算的视觉和文本特征，用Low - Rank Adaptation (LoRA)适配模型，以梯度提升树的深度调优集成作基线，使用基于专家定义的记忆性方面的LLM生成理由提示来引导融合模型。

Result: LLM系统在最终测试集上比基线表现出更强的鲁棒性和泛化性能。

Conclusion: 基于LLM的系统在商业广告记忆性预测上效果更好。

Abstract: This paper addresses the prediction of commercial (brand) memorability as
part of "Subtask 2: Commercial/Ad Memorability" within the "Memorability:
Predicting movie and commercial memorability" task at the MediaEval 2025
workshop competition. We propose a multimodal fusion system with a Gemma-3 LLM
backbone that integrates pre-computed visual (ViT) and textual (E5) features by
multi-modal projections. The model is adapted using Low-Rank Adaptation (LoRA).
A heavily-tuned ensemble of gradient boosted trees serves as a baseline. A key
contribution is the use of LLM-generated rationale prompts, grounded in
expert-derived aspects of memorability, to guide the fusion model. The results
demonstrate that the LLM-based system exhibits greater robustness and
generalization performance on the final test set, compared to the baseline.
  The paper's codebase can be found at
https://github.com/dsgt-arc/mediaeval-2025-memorability

</details>


### [601] [Semantic Surgery: Zero-Shot Concept Erasure in Diffusion Models](https://arxiv.org/abs/2510.22851)
*Lexiang Xiong,Chengyu Liu,Jingwen Ye,Yan Liu,Yuecong Xu*

Main category: cs.CV

TL;DR: 提出Semantic Surgery框架用于文本到图像扩散模型概念擦除，实验显示优于现有方法，还能作威胁检测系统。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型概念擦除方法常损害生成质量，需改进。

Method: 提出训练-free、零样本的Semantic Surgery框架，在扩散前处理文本嵌入，含共现编码模块和视觉反馈循环。

Result: 在多种擦除任务实验中显著优于现有方法，保持图像质量。

Conclusion: 该框架能实现精确干预，可作内置威胁检测系统，为安全文本到图像生成提供实用方案。

Abstract: Concept erasure in text-to-image diffusion models is crucial for mitigating
harmful content, yet existing methods often compromise generative quality. We
introduce Semantic Surgery, a novel training-free, zero-shot framework for
concept erasure that operates directly on text embeddings before the diffusion
process. It dynamically estimates the presence of target concepts in a prompt
and performs a calibrated vector subtraction to neutralize their influence at
the source, enhancing both erasure completeness and locality. The framework
includes a Co-Occurrence Encoding module for robust multi-concept erasure and a
visual feedback loop to address latent concept persistence. As a training-free
method, Semantic Surgery adapts dynamically to each prompt, ensuring precise
interventions. Extensive experiments on object, explicit content, artistic
style, and multi-celebrity erasure tasks show our method significantly
outperforms state-of-the-art approaches. We achieve superior completeness and
robustness while preserving locality and image quality (e.g., 93.58 H-score in
object erasure, reducing explicit content to just 1 instance, and 8.09 H_a in
style erasure with no quality degradation). This robustness also allows our
framework to function as a built-in threat detection system, offering a
practical solution for safer text-to-image generation.

</details>


### [602] [Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained Feature Compression](https://arxiv.org/abs/2510.22930)
*Pranav Saxena*

Main category: cs.CV

TL;DR: 本文提出Gen - LangSplat，用预训练的广义自编码器替代场景特定自编码器，提升语言场构建效率，支持新3D场景开放词汇查询。


<details>
  <summary>Details</summary>
Motivation: 现有方法需训练场景特定语言自编码器，存在优化瓶颈，阻碍部署可扩展性。

Method: 用在大规模ScanNet数据集上预训练的广义自编码器替代场景特定自编码器，进行消融实验确定最优潜在嵌入维度。

Result: 语言场构建过程效率提升，查询性能与原方法相当或更优，广义嵌入能有效准确支持新3D场景开放词汇查询。

Conclusion: 广义嵌入为可扩展、实时交互的3D AI应用铺平道路。

Abstract: Modeling open-vocabulary language fields in 3D is essential for intuitive
human-AI interaction and querying within physical environments.
State-of-the-art approaches, such as LangSplat, leverage 3D Gaussian Splatting
to efficiently construct these language fields, encoding features distilled
from high-dimensional models like CLIP. However, this efficiency is currently
offset by the requirement to train a scene-specific language autoencoder for
feature compression, introducing a costly, per-scene optimization bottleneck
that hinders deployment scalability. In this work, we introduce Gen-LangSplat,
that eliminates this requirement by replacing the scene-wise autoencoder with a
generalized autoencoder, pre-trained extensively on the large-scale ScanNet
dataset. This architectural shift enables the use of a fixed, compact latent
space for language features across any new scene without any scene-specific
training. By removing this dependency, our entire language field construction
process achieves a efficiency boost while delivering querying performance
comparable to, or exceeding, the original LangSplat method. To validate our
design choice, we perform a thorough ablation study empirically determining the
optimal latent embedding dimension and quantifying representational fidelity
using Mean Squared Error and cosine similarity between the original and
reprojected 512-dimensional CLIP embeddings. Our results demonstrate that
generalized embeddings can efficiently and accurately support open-vocabulary
querying in novel 3D scenes, paving the way for scalable, real-time interactive
3D AI applications.

</details>


### [603] [FAME: Fairness-aware Attention-modulated Video Editing](https://arxiv.org/abs/2510.22960)
*Zhangkai Wu,Xuhui Fan,Zhongyuan Xie,Kaize Shi,Zhidong Li,Longbing Cao*

Main category: cs.CV

TL;DR: 提出FAME模型用于无训练视频编辑，缓解职业相关性别偏见，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 无训练视频编辑模型在处理职业相关提示时易产生性别刻板印象，需缓解职业相关性别偏见。

Method: 从现有少数群体表示中导出公平嵌入，将公平调制集成到时间自注意力和提示到区域交叉注意力中，分别采用区域约束注意力掩码和重新加权标记匹配分数。

Result: 在新的面向公平性的视频编辑基准FairVE上的大量实验表明，FAME实现了更强的公平性对齐和语义保真度。

Conclusion: FAME模型优于现有视频编辑基线，能有效缓解职业相关性别偏见。

Abstract: Training-free video editing (VE) models tend to fall back on gender
stereotypes when rendering profession-related prompts. We propose \textbf{FAME}
for \textit{Fairness-aware Attention-modulated Video Editing} that mitigates
profession-related gender biases while preserving prompt alignment and temporal
consistency for coherent VE. We derive fairness embeddings from existing
minority representations by softly injecting debiasing tokens into the text
encoder. Simultaneously, FAME integrates fairness modulation into both temporal
self attention and prompt-to-region cross attention to mitigate the motion
corruption and temporal inconsistency caused by directly introducing fairness
cues. For temporal self attention, FAME introduces a region constrained
attention mask combined with time decay weighting, which enhances intra-region
coherence while suppressing irrelevant inter-region interactions. For cross
attention, it reweights tokens to region matching scores by incorporating
fairness sensitive similarity masks derived from debiasing prompt embeddings.
Together, these modulations keep fairness-sensitive semantics tied to the right
visual regions and prevent temporal drift across frames. Extensive experiments
on new VE fairness-oriented benchmark \textit{FairVE} demonstrate that FAME
achieves stronger fairness alignment and semantic fidelity, surpassing existing
VE baselines.

</details>


### [604] [Nested AutoRegressive Models](https://arxiv.org/abs/2510.23028)
*Hongyu Wu,Xuhui Fan,Zhangkai Wu,Longbing Cao*

Main category: cs.CV

TL;DR: 提出NestAR模型，降低图像生成计算复杂度并提高多样性，取得有竞争力的生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有AR模型计算密集且样本多样性有限，需改进。

Method: 提出嵌套自回归架构，设计多尺度模块，模块内用AR结构生成token块，结合流匹配损失和多尺度模块协调目标。

Result: 将生成n个图像token的复杂度从O(n)降至O(log n)，提高图像多样性。

Conclusion: NestAR在降低计算成本的同时取得有竞争力的图像生成性能。

Abstract: AutoRegressive (AR) models have demonstrated competitive performance in image
generation, achieving results comparable to those of diffusion models. However,
their token-by-token image generation mechanism remains computationally
intensive and existing solutions such as VAR often lead to limited sample
diversity. In this work, we propose a Nested AutoRegressive~(NestAR) model,
which proposes nested AutoRegressive architectures in generating images. NestAR
designs multi-scale modules in a hierarchical order. These different scaled
modules are constructed in an AR architecture, where one larger-scale module is
conditioned on outputs from its previous smaller-scale module. Within each
module, NestAR uses another AR structure to generate ``patches'' of tokens. The
proposed nested AR architecture reduces the overall complexity from
$\mathcal{O}(n)$ to $\mathcal{O}(\log n)$ in generating $n$ image tokens, as
well as increases image diversities. NestAR further incorporates flow matching
loss to use continuous tokens, and develops objectives to coordinate these
multi-scale modules in model training. NestAR achieves competitive image
generation performance while significantly lowering computational cost.

</details>


### [605] [FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment](https://arxiv.org/abs/2510.22827)
*Zahraa Al Sahili,Maryam Fetanat,Maimuna Nowaz,Ioannis Patras,Matthew Purver*

Main category: cs.CV

TL;DR: 提出轻量级协议FairJudge评估文本到图像系统，用指令遵循多模态大模型作公正评判，在多数据集上表现优于基线，可实现更可靠公平审计。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像系统缺乏简单、可复现的评估图像与提示匹配度及模型处理社会属性的方法，常见代理存在缺陷。

Method: 提出FairJudge协议，将指令遵循多模态大模型作为公平评判者，用面向解释的评分规则评分，限制判断标签集，要求基于可见内容提供证据，证据不足时弃权。

Result: 在多个数据集上评估性别、种族、年龄等属性，法官模型在人口预测上优于对比和以人脸为中心的基线，提高平均对齐度并保持高职业准确性。

Conclusion: FairJudge能实现更可靠、可复现的公平审计。

Abstract: Text-to-image (T2I) systems lack simple, reproducible ways to evaluate how
well images match prompts and how models treat social attributes. Common
proxies -- face classifiers and contrastive similarity -- reward surface cues,
lack calibrated abstention, and miss attributes only weakly visible (for
example, religion, culture, disability). We present FairJudge, a lightweight
protocol that treats instruction-following multimodal LLMs as fair judges. It
scores alignment with an explanation-oriented rubric mapped to [-1, 1];
constrains judgments to a closed label set; requires evidence grounded in the
visible content; and mandates abstention when cues are insufficient. Unlike
CLIP-only pipelines, FairJudge yields accountable, evidence-aware decisions;
unlike mitigation that alters generators, it targets evaluation fairness. We
evaluate gender, race, and age on FairFace, PaTA, and FairCoT; extend to
religion, culture, and disability; and assess profession correctness and
alignment on IdenProf, FairCoT-Professions, and our new DIVERSIFY-Professions.
We also release DIVERSIFY, a 469-image corpus of diverse, non-iconic scenes.
Across datasets, judge models outperform contrastive and face-centric baselines
on demographic prediction and improve mean alignment while maintaining high
profession accuracy, enabling more reliable, reproducible fairness audits.

</details>


### [606] [Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics](https://arxiv.org/abs/2510.22937)
*Matthew So,Judah Goldfeder,Mark Lis,Hod Lipson*

Main category: cs.CV

TL;DR: 本文通过训练双编码器网络测试生物特征统计不相关假设，发现左右虹膜相关，指纹模型有积极结果，跨模态匹配需更多数据和复杂流程，挑战了生物特征独立性假设。


<details>
  <summary>Details</summary>
Motivation: 验证个体生物特征统计不相关这一历史假设。

Method: 在三个验证任务上训练双编码器网络，使用ResNet - 50和Vision Transformer骨干网络，最小化同一人图像间的对比损失。

Result: 虹膜ResNet架构在虹膜匹配中ROC AUC得分为91，指纹模型有积极结果，跨模态匹配略高于随机水平。

Conclusion: 研究结果挑战了生物特征独立性假设，未来计划将研究扩展到其他生物特征。

Abstract: There has been a historic assumption that the biometrics of an individual are
statistically uncorrelated. We test this assumption by training Bi-Encoder
networks on three verification tasks, including fingerprint-to-fingerprint
matching, iris-to-iris matching, and cross-modal fingerprint-to-iris matching
using 274 subjects with $\sim$100k fingerprints and 7k iris images. We trained
ResNet-50 and Vision Transformer backbones in Bi-Encoder architectures such
that the contrastive loss between images sampled from the same individual is
minimized. The iris ResNet architecture reaches 91 ROC AUC score for
iris-to-iris matching, providing clear evidence that the left and right irises
of an individual are correlated. Fingerprint models reproduce the positive
intra-subject suggested by prior work in this space. This is the first work
attempting to use Vision Transformers for this matching. Cross-modal matching
rises only slightly above chance, which suggests that more data and a more
sophisticated pipeline is needed to obtain compelling results. These findings
continue challenge independence assumptions of biometrics and we plan to extend
this work to other biometrics in the future. Code available:
https://github.com/MatthewSo/bio_fingerprints_iris.

</details>


### [607] [VoMP: Predicting Volumetric Mechanical Property Fields](https://arxiv.org/abs/2510.22975)
*Rishit Dagli,Donglai Xiang,Vismay Modi,Charles Loop,Clement Fuji Tsang,Anka He Chen,Anita Hu,Gavriel State,David I. W. Levin,Maria Shugrina*

Main category: cs.CV

TL;DR: 提出VoMP方法预测3D对象体积内的材料属性，实验显示其在准确性和速度上远超现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决物理模拟中空间变化的机械属性需手工制作的问题。

Method: VoMP聚合体素多视图特征，通过训练的Geometry Transformer预测体素材料潜在代码，提出结合多源知识的标注管道和新基准获取训练数据。

Result: VoMP能准确估计体积属性，在准确性和速度上远超现有技术。

Conclusion: VoMP是一种有效的预测3D对象材料属性的方法。

Abstract: Physical simulation relies on spatially-varying mechanical properties, often
laboriously hand-crafted. VoMP is a feed-forward method trained to predict
Young's modulus ($E$), Poisson's ratio ($\nu$), and density ($\rho$) throughout
the volume of 3D objects, in any representation that can be rendered and
voxelized. VoMP aggregates per-voxel multi-view features and passes them to our
trained Geometry Transformer to predict per-voxel material latent codes. These
latents reside on a manifold of physically plausible materials, which we learn
from a real-world dataset, guaranteeing the validity of decoded per-voxel
materials. To obtain object-level training data, we propose an annotation
pipeline combining knowledge from segmented 3D datasets, material databases,
and a vision-language model, along with a new benchmark. Experiments show that
VoMP estimates accurate volumetric properties, far outperforming prior art in
accuracy and speed.

</details>


### [608] [DeepSalt: Bridging Laboratory and Satellite Spectra through Domain Adaptation and Knowledge Distillation for Large-Scale Soil Salinity Estimation](https://arxiv.org/abs/2510.23124)
*Rupasree Dey,Abdul Matin,Everett Lewark,Tanjim Bin Faruk,Andrei Bachinin,Sam Leuthold,M. Francesca Cotrufo,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: 本文提出DeepSalt框架，利用知识蒸馏和光谱适应单元，将实验室光谱知识转移到卫星高光谱传感，实现大规模土壤盐度准确估计。


<details>
  <summary>Details</summary>
Motivation: 土壤盐渍化威胁生态和农业，实验室光谱测量精确但难扩展，高光谱卫星影像可大面积观测但解释性不足，需弥合二者差距。

Method: 引入基于深度学习的光谱转移框架DeepSalt，利用知识蒸馏和新型光谱适应单元。

Result: DeepSalt无需大量地面采样，能实现准确的大规模盐度估计，相比无显式领域适应的方法有显著性能提升，可有效泛化到未知地理区域。

Conclusion: DeepSalt的光谱适应单元和知识蒸馏策略有积极影响，能解释大部分盐度差异。

Abstract: Soil salinization poses a significant threat to both ecosystems and
agriculture because it limits plants' ability to absorb water and, in doing so,
reduces crop productivity. This phenomenon alters the soil's spectral
properties, creating a measurable relationship between salinity and light
reflectance that enables remote monitoring. While laboratory spectroscopy
provides precise measurements, its reliance on in-situ sampling limits
scalability to regional or global levels. Conversely, hyperspectral satellite
imagery enables wide-area observation but lacks the fine-grained
interpretability of laboratory instruments. To bridge this gap, we introduce
DeepSalt, a deep-learning-based spectral transfer framework that leverages
knowledge distillation and a novel Spectral Adaptation Unit to transfer
high-resolution spectral insights from laboratory-based spectroscopy to
satellite-based hyperspectral sensing. Our approach eliminates the need for
extensive ground sampling while enabling accurate, large-scale salinity
estimation, as demonstrated through comprehensive empirical benchmarks.
DeepSalt achieves significant performance gains over methods without explicit
domain adaptation, underscoring the impact of the proposed Spectral Adaptation
Unit and the knowledge distillation strategy. The model also effectively
generalized to unseen geographic regions, explaining a substantial portion of
the salinity variance.

</details>


### [609] [AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes](https://arxiv.org/abs/2510.23151)
*Sixian Liu,Chen Xu,Qiang Wang,Donghai Shi,Yiwen Li*

Main category: cs.CV

TL;DR: 提出AG - Fusion方法用于多模态相机 - LiDAR融合3D目标检测，构建E3D数据集，在KITTI和E3D数据集表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有多模态相机 - LiDAR融合3D目标检测方法在传感器退化或环境干扰等挑战性场景中性能显著下降。

Method: 提出AG - Fusion方法，先将各模态特征投影到统一BEV空间并用基于窗口的注意力机制增强，再用基于跨模态注意力的自适应门控融合模块整合特征；构建E3D数据集。

Result: 在标准KITTI数据集上准确率达93.92%，在E3D数据集上比基线显著提升24.88%。

Conclusion: 该方法在复杂工业场景中对不可靠模态信息具有优越的鲁棒性。

Abstract: Multimodal camera-LiDAR fusion technology has found extensive application in
3D object detection, demonstrating encouraging performance. However, existing
methods exhibit significant performance degradation in challenging scenarios
characterized by sensor degradation or environmental disturbances. We propose a
novel Adaptive Gated Fusion (AG-Fusion) approach that selectively integrates
cross-modal knowledge by identifying reliable patterns for robust detection in
complex scenes. Specifically, we first project features from each modality into
a unified BEV space and enhance them using a window-based attention mechanism.
Subsequently, an adaptive gated fusion module based on cross-modal attention is
designed to integrate these features into reliable BEV representations robust
to challenging environments. Furthermore, we construct a new dataset named
Excavator3D (E3D) focusing on challenging excavator operation scenarios to
benchmark performance in complex conditions. Our method not only achieves
competitive performance on the standard KITTI dataset with 93.92% accuracy, but
also significantly outperforms the baseline by 24.88% on the challenging E3D
dataset, demonstrating superior robustness to unreliable modal information in
complex industrial scenes.

</details>


### [610] [Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation](https://arxiv.org/abs/2510.23241)
*Stefan M. Fischer,Johannes Kiechle,Laura Daza,Lina Felsner,Richard Osuala,Daniel M. Lang,Karim Lekadir,Jan C. Peeken,Julia A. Schnabel*

Main category: cs.CV

TL;DR: 提出渐进式增大补丁尺寸的自动课程学习方法用于3D医学图像分割，在不同模式下提升了Dice分数性能并减少训练时间，适用于多种分割模型。


<details>
  <summary>Details</summary>
Motivation: 解决3D医学图像分割中训练的类平衡和收敛速度问题，提升分割性能和效率。

Method: 在模型训练过程中渐进式增大补丁尺寸，评估资源高效模式和性能模式。

Result: 资源高效模式在减少训练时间至44%时达到与传统方法相当的Dice分数；性能模式Dice分数相对提升1.28%，减少训练时间至89%，降低性能方差。

Conclusion: 该方法能显著提升Dice分数性能和训练效率，适用于多种分割骨干网络。

Abstract: In this work, we introduce Progressive Growing of Patch Size, an automatic
curriculum learning approach for 3D medical image segmentation. Our approach
progressively increases the patch size during model training, resulting in an
improved class balance for smaller patch sizes and accelerated convergence of
the training process. We evaluate our curriculum approach in two settings: a
resource-efficient mode and a performance mode, both regarding Dice score
performance and computational costs across 15 diverse and popular 3D medical
image segmentation tasks. The resource-efficient mode matches the Dice score
performance of the conventional constant patch size sampling baseline with a
notable reduction in training time to only 44%. The performance mode improves
upon constant patch size segmentation results, achieving a statistically
significant relative mean performance gain of 1.28% in Dice Score. Remarkably,
across all 15 tasks, our proposed performance mode manages to surpass the
constant patch size baseline in Dice Score performance, while simultaneously
reducing training time to only 89%. The benefits are particularly pronounced
for highly imbalanced tasks such as lesion segmentation tasks. Rigorous
experiments demonstrate that our performance mode not only improves mean
segmentation performance but also reduces performance variance, yielding more
trustworthy model comparison. Furthermore, our findings reveal that the
proposed curriculum sampling is not tied to a specific architecture but
represents a broadly applicable strategy that consistently boosts performance
across diverse segmentation models, including UNet, UNETR, and SwinUNETR. In
summary, we show that this simple yet elegant transformation on input data
substantially improves both Dice Score performance and training runtime, while
being compatible across diverse segmentation backbones.

</details>


### [611] [ReconViaGen: Towards Accurate Multi-view 3D Object Reconstruction via Generation](https://arxiv.org/abs/2510.23306)
*Jiahao Chang,Chongjie Ye,Yushuang Wu,Yuantao Chen,Yidan Zhang,Zhongjin Luo,Chenghong Li,Yihao Zhi,Xiaoguang Han*

Main category: cs.CV

TL;DR: 现有多视图3D对象重建方法有局限，基于扩散的3D生成技术有潜力但存在问题，本文提出ReconViaGen解决问题并取得好效果。


<details>
  <summary>Details</summary>
Motivation: 现有多视图3D对象重建方法依赖视图重叠，基于扩散的3D生成技术随机特性限制结果准确性，现有重建框架无法集成生成先验。

Method: 全面分析基于扩散的3D生成方法无法实现高一致性的原因，提出ReconViaGen将重建先验集成到生成框架并设计策略。

Result: 广泛实验表明ReconViaGen能在全局结构和局部细节上重建与输入视图一致的完整准确3D模型。

Conclusion: ReconViaGen有效解决了现有方法的问题，可用于3D对象重建。

Abstract: Existing multi-view 3D object reconstruction methods heavily rely on
sufficient overlap between input views, where occlusions and sparse coverage in
practice frequently yield severe reconstruction incompleteness. Recent
advancements in diffusion-based 3D generative techniques offer the potential to
address these limitations by leveraging learned generative priors to
hallucinate invisible parts of objects, thereby generating plausible 3D
structures. However, the stochastic nature of the inference process limits the
accuracy and reliability of generation results, preventing existing
reconstruction frameworks from integrating such 3D generative priors. In this
work, we comprehensively analyze the reasons why diffusion-based 3D generative
methods fail to achieve high consistency, including (a) the insufficiency in
constructing and leveraging cross-view connections when extracting multi-view
image features as conditions, and (b) the poor controllability of iterative
denoising during local detail generation, which easily leads to plausible but
inconsistent fine geometric and texture details with inputs. Accordingly, we
propose ReconViaGen to innovatively integrate reconstruction priors into the
generative framework and devise several strategies that effectively address
these issues. Extensive experiments demonstrate that our ReconViaGen can
reconstruct complete and accurate 3D models consistent with input views in both
global structure and local details.Project page:
https://jiahao620.github.io/reconviagen.

</details>


### [612] [Multitask Multimodal Self-Supervised Learning for Medical Images](https://arxiv.org/abs/2510.23325)
*Cristian Simionescu*

Main category: cs.CV

TL;DR: 论文针对医学图像分析依赖大量标注数据的问题，提出Medformer网络架构和自监督学习新预文本任务，减少对标注数据依赖，推动医学图像分析发展。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分析依赖大量标注数据集的问题，标注数据因专家标注、隐私和法律问题受限。

Method: 开发Medformer神经网络架构用于多任务学习和深度领域自适应，探索自监督学习并引入新预文本任务。

Result: 通过严格实验验证，模型能学习适用于各种下游任务的通用特征。

Conclusion: 提供可扩展、自适应框架，减少对标注数据依赖，为医疗保健提供更准确、高效诊断工具。

Abstract: This thesis works to address a pivotal challenge in medical image analysis:
the reliance on extensive labeled datasets, which are often limited due to the
need for expert annotation and constrained by privacy and legal issues. By
focusing on the development of self-supervised learning techniques and domain
adaptation methods, this research aims to circumvent these limitations,
presenting a novel approach to enhance the utility and efficacy of deep
learning in medical imaging.
  Central to this thesis is the development of the Medformer, an innovative
neural network architecture designed for multitask learning and deep domain
adaptation. This model is adept at pre-training on diverse medical image
datasets, handling varying sizes and modalities, and is equipped with a dynamic
input-output adaptation mechanism. This enables efficient processing and
integration of a wide range of medical image types, from 2D X-rays to complex
3D MRIs, thus mitigating the dependency on large labeled datasets.
  Further, the thesis explores the current state of self-supervised learning in
medical imaging. It introduces novel pretext tasks that are capable of
extracting meaningful information from unlabeled data, significantly advancing
the model's interpretative abilities. This approach is validated through
rigorous experimentation, including the use of the MedMNIST dataset,
demonstrating the model's proficiency in learning generalized features
applicable to various downstream tasks.
  In summary, this thesis contributes to the advancement of medical image
analysis by offering a scalable, adaptable framework that reduces reliance on
labeled data. It paves the way for more accurate, efficient diagnostic tools in
healthcare, signifying a major step forward in the application of deep learning
in medical imaging.

</details>


### [613] [FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial Basis Network](https://arxiv.org/abs/2510.23444)
*Fangtong Sun,Congyu Li,Ke Yang,Yuchen Pan,Hanwen Yu,Xichuan Zhang,Yiying Li*

Main category: cs.CV

TL;DR: 文章针对低光视觉挑战，扩展经典模型，提出频域径向基网络FRBNet，实验显示其在下游任务表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有低光视觉方法对低光条件建模不完整，影响下游任务性能。

Method: 扩展经典朗伯模型，从频域分析，证明频域通道比可提取光照不变特征，提出端到端可训练模块FRBNet。

Result: FRBNet在黑暗物体检测中mAP提升2.2，夜间分割mIoU提升2.9。

Conclusion: FRBNet作为即插即用模块，能提升低光下游任务性能。

Abstract: Low-light vision remains a fundamental challenge in computer vision due to
severe illumination degradation, which significantly affects the performance of
downstream tasks such as detection and segmentation. While recent
state-of-the-art methods have improved performance through invariant feature
learning modules, they still fall short due to incomplete modeling of low-light
conditions. Therefore, we revisit low-light image formation and extend the
classical Lambertian model to better characterize low-light conditions. By
shifting our analysis to the frequency domain, we theoretically prove that the
frequency-domain channel ratio can be leveraged to extract
illumination-invariant features via a structured filtering process. We then
propose a novel and end-to-end trainable module named \textbf{F}requency-domain
\textbf{R}adial \textbf{B}asis \textbf{Net}work (\textbf{FRBNet}), which
integrates the frequency-domain channel ratio operation with a learnable
frequency domain filter for the overall illumination-invariant feature
enhancement. As a plug-and-play module, FRBNet can be integrated into existing
networks for low-light downstream tasks without modifying loss functions.
Extensive experiments across various downstream tasks demonstrate that FRBNet
achieves superior performance, including +2.2 mAP for dark object detection and
+2.9 mIoU for nighttime segmentation. Code is available at:
https://github.com/Sing-Forevet/FRBNet.

</details>


### [614] [On the Faithfulness of Visual Thinking: Measurement and Enhancement](https://arxiv.org/abs/2510.23482)
*Zujing Liu,Junwen Pan,Qi She,Yuan Gao,Guisong Xia*

Main category: cs.CV

TL;DR: 当前大视觉语言模型的多模态思维链视觉信息不准确，本文探究其忠实性，提出SCCM学习策略提升视觉忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型经强化微调后生成的多模态思维链中视觉信息不准确，推理过程缺乏忠实性。

Method: 通过干预视觉和文本思想测量预测变化来探究忠实性；引入自动化评估指标量化视觉线索忠实性；提出SCCM学习策略。

Result: 评估显示当前多模态思维链视觉信息不可靠且不充分；SCCM在多个基准测试中持续提升视觉忠实性。

Conclusion: SCCM是无注释的，可即插即用，能有效提升多模态思维链的视觉忠实性。

Abstract: Recent large vision-language models (LVLMs) can generate vision-text
multimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning
(RFT). However, we observe that the visual information incorporated in MCoT is
often inaccurate, though still yield correct answers, indicating a lack of
faithfulness in the MCoT reasoning process. We attribute this unfaithfulness to
the RL reward in RFT, which solely incentivizes the format of interleaved
vision-text cues, ie, it encourages the model to incorporate visual information
into its text reasoning steps without considering the correctness of the visual
information. In this paper, we first probe the faithfulness of MCoT by
measuring how much the prediction changes when its visual and textual thoughts
are intervened. Surprisingly, the model's predictions remain nearly unchanged
under visual intervention but change significantly under textual intervention,
indicating that the visual evidence is largely ignored. To further analyze
visual information, we introduce an automated LVLM-based evaluation metric that
quantifies the faithfulness of visual cues from two perspectives: reliability
and sufficiency. Our evaluation reveals that the visual information in current
MCoT traces is simultaneously unreliable and insufficient. To address this
issue, we propose a novel MCoT learning strategy termed Sufficient-Component
Cause Model (SCCM) learning. This approach encourages the MCoT to generate
sufficient yet minimal visual components that are independently capable of
leading to correct answers. We note that the proposed SCCM is annotation-free
and compatible with various RFT for MCoT in a plug-and-play manner. Empirical
results demonstrate that SCCM consistently improves the visual faithfulness
across a suite of fine-grained perception and reasoning benchmarks. Code is
available at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.

</details>


### [615] [Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation](https://arxiv.org/abs/2510.23581)
*Junyoung Seo,Rodrigo Mira,Alexandros Haliassos,Stella Bounareli,Honglie Chen,Linh Tran,Seungryong Kim,Zoe Landgraf,Jie Shen*

Main category: cs.CV

TL;DR: 提出Lookahead Anchoring方法解决音频驱动人体动画模型身份漂移问题，在多个模型上表现出色。


<details>
  <summary>Details</summary>
Motivation: 音频驱动人体动画模型在时间自回归生成时存在身份漂移问题，现有关键帧生成方法有局限。

Method: 提出Lookahead Anchoring，利用当前生成窗口未来时间步的关键帧，将关键帧变为方向指引，还支持自关键帧。

Result: 该方法自然控制表现力和一致性的平衡，应用于三个模型时实现了更好的唇同步、身份保留和视觉质量。

Conclusion: Lookahead Anchoring在多种架构上改善了时间条件，是解决身份漂移问题的有效方法。

Abstract: Audio-driven human animation models often suffer from identity drift during
temporal autoregressive generation, where characters gradually lose their
identity over time. One solution is to generate keyframes as intermediate
temporal anchors that prevent degradation, but this requires an additional
keyframe generation stage and can restrict natural motion dynamics. To address
this, we propose Lookahead Anchoring, which leverages keyframes from future
timesteps ahead of the current generation window, rather than within it. This
transforms keyframes from fixed boundaries into directional beacons: the model
continuously pursues these future anchors while responding to immediate audio
cues, maintaining consistent identity through persistent guidance. This also
enables self-keyframing, where the reference image serves as the lookahead
target, eliminating the need for keyframe generation entirely. We find that the
temporal lookahead distance naturally controls the balance between expressivity
and consistency: larger distances allow for greater motion freedom, while
smaller ones strengthen identity adherence. When applied to three recent human
animation models, Lookahead Anchoring achieves superior lip synchronization,
identity preservation, and visual quality, demonstrating improved temporal
conditioning across several different architectures. Video results are
available at the following link: https://lookahead-anchoring.github.io.

</details>


### [616] [Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling](https://arxiv.org/abs/2510.23605)
*Shuhong Zheng,Ashkan Mirzaei,Igor Gilitschenski*

Main category: cs.CV

TL;DR: 本文提出TIRE方法用于主题驱动的3D/4D生成，能显著提升身份保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D/4D生成方法难保留不同视角下主体语义身份，个性化3D/4D生成研究不足。

Method: 以现有3D生成模型生成的初始3D资产为输入，用视频跟踪确定需修改区域，采用主题驱动的2D修复模型填充，最后将修改后的2D多视图观察结果恢复到3D。

Result: 大量实验表明，该方法相比现有方法显著提高了3D/4D生成中的身份保留能力。

Conclusion: TIRE方法在主题驱动的3D/4D生成中能有效提升身份保留能力。

Abstract: Current 3D/4D generation methods are usually optimized for photorealism,
efficiency, and aesthetics. However, they often fail to preserve the semantic
identity of the subject across different viewpoints. Adapting generation
methods with one or few images of a specific subject (also known as
Personalization or Subject-driven generation) allows generating visual content
that align with the identity of the subject. However, personalized 3D/4D
generation is still largely underexplored. In this work, we introduce TIRE
(Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation.
It takes an initial 3D asset produced by an existing 3D generative model as
input and uses video tracking to identify the regions that need to be modified.
Then, we adopt a subject-driven 2D inpainting model for progressively infilling
the identified regions. Finally, we resplat the modified 2D multi-view
observations back to 3D while still maintaining consistency. Extensive
experiments demonstrate that our approach significantly improves identity
preservation in 3D/4D generation compared to state-of-the-art methods. Our
project website is available at
https://zsh2000.github.io/track-inpaint-resplat.github.io/.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [617] [10 Simple Rules for Improving Your Standardized Fields and Terms](https://arxiv.org/abs/2510.21825)
*Rhiannon Cameron,Emma Griffiths,Damion Dooley,William Hsiao*

Main category: cs.DL

TL;DR: 本文探讨词汇标准化难题，结合实用建议与实例，给出应对策略及规则，助于创建优质元数据。


<details>
  <summary>Details</summary>
Motivation: 解决词汇标准化这一棘手过程，让数据更易发现、共享和重用，避免数据清理与管理的噩梦。

Method: 结合实际语境数据协调经验，列举常见挑战如语义噪声和概念炸弹，并给出可操作策略。

Result: 提出强调与FAIR原则一致且适应不断变化需求的规则。

Conclusion: 这些规则有助于创建技术上合理且对用户有意义的元数据，适用于数据集管理、模式设计等场景。

Abstract: Contextual metadata is the unsung hero of research data. When done right,
standardized and structured vocabularies make your data findable, shareable,
and reusable. When done wrong, they turn a well intended effort into data
cleanup and curation nightmares. In this paper we tackle the surprisingly
tricky process of vocabulary standardization with a mix of practical advice and
grounded examples. Drawing from real-world experience in contextual data
harmonization, we highlight common challenges (e.g., semantic noise and concept
bombs) and provide actionable strategies to address them. Our rules emphasize
alignment with Findability, Accessibility, Interoperability, and Reusability
(FAIR) principles while remaining adaptable to evolving user and research
needs. Whether you are curating datasets, designing a schema, or contributing
to a standards body, these rules aim to help you create metadata that is not
only technically sound but also meaningful to users.

</details>


### [618] [Can Small and Reasoning Large Language Models Score Journal Articles for Research Quality and Do Averaging and Few-shot Help?](https://arxiv.org/abs/2510.22389)
*Mike Thelwall,Ehsan Mohammadi*

Main category: cs.DL

TL;DR: 研究评估小型和推理大语言模型对学术期刊文章评分能力，发现>4b的小型LLMs有评分能力，分数平均策略有效。


<details>
  <summary>Details</summary>
Motivation: 大型模型在某些情况下慢且不实用，推理模型表现可能不同，不清楚小型和推理模型评估文章质量能力。

Method: 用Gemma3变体、Llama4 Scout等模型，在2780篇医疗、健康和生命科学论文数据集上，采用两种金标准进行实验。

Result: 小型和推理LLMs表现与ChatGPT 4o - mini和Gemini 2.0 Flash相似；1b参数常不足，4b有时不足；分数平均策略普遍有效；少样本提示效果不明确；推理模型无明显优势。

Conclusion: 首次表明>4b的小型LLMs包括推理模型有评估期刊文章研究质量的能力，使用分数平均策略更佳。

Abstract: Assessing published academic journal articles is a common task for
evaluations of departments and individuals. Whilst it is sometimes supported by
citation data, Large Language Models (LLMs) may give more useful indications of
article quality. Evidence of this capability exists for two of the largest LLM
families, ChatGPT and Gemini, and the medium sized LLM Gemma3 27b, but it is
unclear whether smaller LLMs and reasoning models have similar abilities. This
is important because larger models may be slow and impractical in some
situations, and reasoning models may perform differently. Four relevant
questions are addressed with Gemma3 variants, Llama4 Scout, Qwen3, Magistral
Small and DeepSeek R1, on a dataset of 2,780 medical, health and life science
papers in 6 fields, with two different gold standards, one novel. The results
suggest that smaller (open weights) and reasoning LLMs have similar performance
to ChatGPT 4o-mini and Gemini 2.0 Flash, but that 1b parameters may often, and
4b sometimes, be too few. Moreover, averaging scores from multiple identical
queries seems to be a universally successful strategy, and few-shot prompts
(four examples) tended to help but the evidence was equivocal. Reasoning models
did not have a clear advantage. Overall, the results show, for the first time,
that smaller LLMs >4b, including reasoning models, have a substantial
capability to score journal articles for research quality, especially if score
averaging is used.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [619] [A quality of mercy is not trained: the imagined vs. the practiced in healthcare process-specialized AI development](https://arxiv.org/abs/2510.21843)
*Anand Bhardwaj,Samer Faraj*

Main category: cs.CY

TL;DR: 研究医疗场景下AI系统伦理风险与认知框架的关系，以加拿大医院手术室调度AI为例，发现早期表征决策导致关键伦理维度被排除，呼吁更具情境性的设计方法。


<details>
  <summary>Details</summary>
Motivation: 探究医疗等高风险组织环境中AI系统的伦理风险如何受其认知框架的影响。

Method: 对加拿大一家医院手术室调度AI开发进行嵌入式研究，对比AI设计中想象的调度与实际实践的调度。

Result: 早期表征决策限制了AI的支持范围，导致关键伦理维度过早被排除在系统设计之外。

Conclusion: 抽象化有道德后果，设计医疗流程专用AI系统需更具情境性的方法。

Abstract: In high stakes organizational contexts like healthcare, artificial
intelligence (AI) systems are increasingly being designed to augment complex
coordination tasks. This paper investigates how the ethical stakes of such
systems are shaped by their epistemic framings: what aspects of work they
represent, and what they exclude. Drawing on an embedded study of AI
development for operating room (OR) scheduling at a Canadian hospital, we
compare scheduling-as-imagined in the AI design process: rule-bound,
predictable, and surgeon-centric, with scheduling-as-practiced as a fluid,
patient-facing coordination process involving ethical discretion. We show how
early representational decisions narrowed what the AI could support, resulting
in epistemic foreclosure: the premature exclusion of key ethical dimensions
from system design. Our findings surface the moral consequences of abstraction
and call for a more situated approach to designing healthcare
process-specialized artificial intelligence systems.

</details>


### [620] [Data-Driven Approach to Capitation Reform in Rwanda](https://arxiv.org/abs/2510.21851)
*Babaniyi Olaniyi,Ina Kalisa,Ana Fernández del Río,Jean Marie Vianney Hakizayezu,Enric Jané,Eniola Olaleye,Juan Francisco Garamendi,Ivan Nazarov,Aditya Rastogi,Mateo Diaz-Quiroz,África Periáñez,Regis Hitimana*

Main category: cs.CY

TL;DR: 卢旺达国家社区健康保险计划转向按人头付费模式，报告介绍用IHBS数据驱动设计、校准和监测该模式的方法，方案与历史支出契合，数据还能用于监测行为，为学习型卫生融资系统奠基。


<details>
  <summary>Details</summary>
Motivation: 卢旺达国家社区健康保险计划从回顾性按服务付费报销转向前瞻性按人头付费，需设计相应模式。

Method: 使用IHBS系统的个人层面索赔数据，通过回归模型估计参数，引入基于集水区人口、服务利用模式和患者流入的透明可解释公式分配支付。

Result: 支付方案与历史支出紧密一致，促进不同机构的公平性和适应性，还能通过数据获得可操作的行为洞察。

Conclusion: 这些能力为学习型卫生融资系统奠定基础，支持持续改进和循证政策改革。

Abstract: As part of Rwanda's transition toward universal health coverage, the national
Community-Based Health Insurance (CBHI) scheme is moving from retrospective
fee-for-service reimbursements to prospective capitation payments for public
primary healthcare providers. This report outlines a data-driven approach to
designing, calibrating, and monitoring the capitation model using
individual-level claims data from the Intelligent Health Benefits System
(IHBS). We introduce a transparent, interpretable formula for allocating
payments to Health Centers and their affiliated Health Posts. The formula is
based on catchment population, service utilization patterns, and patient
inflows, with parameters estimated via regression models calibrated on national
claims data. Repeated validation exercises show the payment scheme closely
aligns with historical spending while promoting fairness and adaptability
across diverse facilities. In addition to payment design, the same dataset
enables actionable behavioral insights. We highlight the use case of monitoring
antibiotic prescribing patterns, particularly in pediatric care, to flag
potential overuse and guideline deviations. Together, these capabilities lay
the groundwork for a learning health financing system: one that connects
digital infrastructure, resource allocation, and service quality to support
continuous improvement and evidence-informed policy reform.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [621] [Statistics of correlations in nonlinear recurrent neural networks](https://arxiv.org/abs/2510.21742)
*German Mato,Facundo Rigatuso,Gonzalo Torroba*

Main category: q-bio.NC

TL;DR: 推导大神经元数量下非线性循环网络相关性统计的精确表达式，推广线性网络结果，给出幂律激活函数结果并引入新激活函数类，数值模拟验证理论。


<details>
  <summary>Details</summary>
Motivation: 研究循环神经网络集体动力学中相关性统计这一核心量，推广线性网络结果到非线性情况。

Method: 使用路径积分表示网络随机动力学，将描述简化为几个集体变量以实现高效计算。

Result: 推广了线性网络结果，解决线性理论不稳定性，得到幂律激活函数的缩放行为，引入基于Pade近似的激活函数类并给出相关性统计的解析预测，数值模拟与理论结果高度吻合。

Conclusion: 所提出的方法有效，能准确描述非线性循环网络的相关性统计。

Abstract: The statistics of correlations are central quantities characterizing the
collective dynamics of recurrent neural networks. We derive exact expressions
for the statistics of correlations of nonlinear recurrent networks in the limit
of a large number N of neurons, including systematic 1/N corrections. Our
approach uses a path-integral representation of the network's stochastic
dynamics, which reduces the description to a few collective variables and
enables efficient computation. This generalizes previous results on linear
networks to include a wide family of nonlinear activation functions, which
enter as interaction terms in the path integral. These interactions can resolve
the instability of the linear theory and yield a strictly positive
participation dimension. We present explicit results for power-law activations,
revealing scaling behavior controlled by the network coupling. In addition, we
introduce a class of activation functions based on Pade approximants and
provide analytic predictions for their correlation statistics. Numerical
simulations confirm our theoretical results with excellent agreement.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [622] [CreditXAI: A Multi-Agent System for Explainable Corporate Credit Rating](https://arxiv.org/abs/2510.22222)
*Yumeng Shi,Zhongliang Yang,Yisi Wang,Linna Zhou*

Main category: cs.MA

TL;DR: 传统深度学习方法在企业信用评级中有黑盒问题和解释性不足，本文提出CreditXAI框架解决问题，实验显示多智能体协作有优势，提供新技术路径。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法存在黑盒问题和解释性不足，现有含非财务信息的模型缺乏层次推理机制，综合分析能力受限。

Method: 提出CreditXAI多智能体系统框架，模拟专业信用分析师协作决策过程，聚焦业务、财务和治理风险维度。

Result: 多智能体协作比最佳单智能体基线的预测准确率提高超7%。

Conclusion: 本研究为构建智能且可解释的信用评级模型提供了新的技术途径。

Abstract: In the domain of corporate credit rating, traditional deep learning methods
have improved predictive accuracy but still suffer from the inherent
'black-box' problem and limited interpretability. While incorporating
non-financial information enriches the data and provides partial
interpretability, the models still lack hierarchical reasoning mechanisms,
limiting their comprehensive analytical capabilities. To address these
challenges, we propose CreditXAI, a Multi-Agent System (MAS) framework that
simulates the collaborative decision-making process of professional credit
analysts. The framework focuses on business, financial, and governance risk
dimensions to generate consistent and interpretable credit assessments.
Experimental results demonstrate that multi-agent collaboration improves
predictive accuracy by more than 7% over the best single-agent baseline,
confirming its significant synergistic advantage in corporate credit risk
evaluation. This study provides a new technical pathway to build intelligent
and interpretable credit rating models.

</details>


### [623] [Group size effects and collective misalignment in LLM multi-agent systems](https://arxiv.org/abs/2510.22422)
*Ariel Flint,Luca Maria Aiello,Romualdo Pastor-Satorras,Andrea Baronchelli*

Main category: cs.MA

TL;DR: 本文系统探究大语言模型多智能体系统中群体规模对动态的影响，发现群体规模是多智能体动态的关键驱动因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究多对比单智能体与固定规模集体行为，未探究群体规模如何塑造动态，本文旨在填补这一空白。

Method: 聚焦多智能体失调问题，研究交互的大语言模型在协调游戏中的集体偏差，开发平均场分析方法。

Result: 集体偏差比以往评估更复杂；群体规模对动态的影响是非线性的；超过临界种群规模，模拟结果收敛于确定性预测。

Conclusion: 群体规模是多智能体动态的关键驱动因素，部署基于大语言模型的系统时需考虑种群层面的影响。

Abstract: Multi-agent systems of large language models (LLMs) are rapidly expanding
across domains, introducing dynamics not captured by single-agent evaluations.
Yet, existing work has mostly contrasted the behavior of a single agent with
that of a collective of fixed size, leaving open a central question: how does
group size shape dynamics? Here, we move beyond this dichotomy and
systematically explore outcomes across the full range of group sizes. We focus
on multi-agent misalignment, building on recent evidence that interacting LLMs
playing a simple coordination game can generate collective biases absent in
individual models. First, we show that collective bias is a deeper phenomenon
than previously assessed: interaction can amplify individual biases, introduce
new ones, or override model-level preferences. Second, we demonstrate that
group size affects the dynamics in a non-linear way, revealing model-dependent
dynamical regimes. Finally, we develop a mean-field analytical approach and
show that, above a critical population size, simulations converge to
deterministic predictions that expose the basins of attraction of competing
equilibria. These findings establish group size as a key driver of multi-agent
dynamics and highlight the need to consider population-level effects when
deploying LLM-based systems at scale.

</details>


### [624] [Agent-GSPO: Communication-Efficient Multi-Agent Systems via Group Sequence Policy Optimization](https://arxiv.org/abs/2510.22477)
*Yijia Fan,Jusheng Zhang,Jing Yang,Keze Wang*

Main category: cs.MA

TL;DR: 提出Agent - GSPO框架，用序列级强化学习优化令牌经济，在多个推理基准测试中表现出色且减少令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 解决“自由放任”多智能体系统过高的通信成本问题。

Method: 引入Agent - GSPO框架，利用GSPO算法在通信感知奖励下训练智能体，惩罚冗长表达。

Result: 在七个推理基准测试中达到新的最优性能，且令牌消耗远低于现有方法。

Conclusion: 该方法通过培养“策略性沉默”等策略，为开发可扩展且经济可行的多智能体系统提供了实用方案。

Abstract: To combat the prohibitive communication costs of ``free-for-all" multi-agent
systems (MAS), we introduce \textbf{Agent-GSPO}, a framework that directly
optimizes for token economy using sequence-level reinforcement learning.
Agent-GSPO leverages the stable and memory-efficient Group Sequence Policy
Optimization (GSPO) algorithm to train agents on a communication-aware reward
that explicitly penalizes verbosity. Across seven reasoning benchmarks,
Agent-GSPO not only achieves new state-of-the-art performance but does so with
a fraction of the token consumption of existing methods. By fostering emergent
strategies like ``strategic silence," our approach provides a practical
blueprint for developing scalable and economically viable multi-agent systems.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [625] [MacroEnergy.jl: A large-scale multi-sector energy system framework](https://arxiv.org/abs/2510.21943)
*Ruaridh Macdonald,Filippo Pecci,Luca Bonaldo,Jun Wen Law,Yu Weng,Dharik Mallapragada,Jesse Jenkins*

Main category: physics.soc-ph

TL;DR: MacroEnergy.jl是用于宏观能源系统多部门容量扩展建模和分析的开源框架，功能灵活且支持高级技术，为能源转型研究提供平台。


<details>
  <summary>Details</summary>
Motivation: 为研究人员和从业者提供一个可用于设计和分析跨多个部门的能源和工业系统的工具，以满足现代研究和政策对能源转型研究的需求。

Method: 用Julia编写，使用JuMP包与多种数学求解器交互，围绕少量与部门无关的组件组织，可组合成灵活的图结构，其配套包支持分解方法等高级技术。

Result: 构建了MacroEnergy.jl框架，可扩展到新技术、政策和商品，能跨精细的时间和空间分辨率扩展模型。

Conclusion: MacroEnergy.jl为现代研究和政策所需的能源转型研究提供了一个多功能平台。

Abstract: MacroEnergy.jl (aka Macro) is an open-source framework for multi-sector
capacity expansion modeling and analysis of macro-energy systems. It is written
in Julia and uses the JuMP package to interface with a wide range of
mathematical solvers. It enables researchers and practitioners to design and
analyze energy and industrial systems that span electricity, fuels, bioenergy,
steel, chemicals, and other sectors. The framework is organized around a small
set of sector-agnostic components that can be combined into flexible graph
structures, making it straightforward to extend to new technologies, policies,
and commodities. Its companion packages support decomposition methods and other
advanced techniques, allowing users to scale models across fine temporal and
spatial resolutions. MacroEnergy.jl provides a versatile platform for studying
energy transitions at the detail and scale demanded by modern research and
policy.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [626] [BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles](https://arxiv.org/abs/2510.22370)
*Seyed Ahmad Hosseini Miangoleh,Amin Jalal Aghdasian,Farzaneh Abdollahi*

Main category: cs.RO

TL;DR: 提出用于自动驾驶车道保持的BLIP - FusePPO多模态强化学习框架，融合多种信息，有混合奖励函数，模拟结果表现佳且代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车道保持问题，让智能体学习能感知周围环境且易理解的驾驶规则，使策略学习更鲁棒、学习更高效可泛化。

Method: 提出BLIP - FusePPO框架，将视觉语言模型生成的语义嵌入与几何状态、激光雷达观测和PID控制反馈在智能体观测空间融合，使用包含语义对齐、车道保持精度、避障和速度调节的混合奖励函数，直接将语义特征嵌入状态表示。

Result: 模拟结果显示，该模型在多种困难驾驶场景下的车道保持稳定性和适应性优于最佳基于视觉和多模态强化学习的基线模型。

Conclusion: 所提出的BLIP - FusePPO框架在自动驾驶车道保持任务中有效且性能良好。

Abstract: In this paper, we propose Bootstrapped Language-Image Pretraining-driven
Fused State Representation in Proximal Policy Optimization (BLIP-FusePPO), a
novel multimodal reinforcement learning (RL) framework for autonomous
lane-keeping (LK), in which semantic embeddings generated by a vision-language
model (VLM) are directly fused with geometric states, LiDAR observations, and
Proportional-Integral-Derivative-based (PID) control feedback within the agent
observation space. The proposed method lets the agent learn driving rules that
are aware of their surroundings and easy to understand by combining high-level
scene understanding from the VLM with low-level control and spatial signals.
Our architecture brings together semantic, geometric, and control-aware
representations to make policy learning more robust. A hybrid reward function
that includes semantic alignment, LK accuracy, obstacle avoidance, and speed
regulation helps learning to be more efficient and generalizable. Our method is
different from the approaches that only use semantic models to shape rewards.
Instead, it directly embeds semantic features into the state representation.
This cuts down on expensive runtime inference and makes sure that semantic
guidance is always available. The simulation results show that the proposed
model is better at LK stability and adaptability than the best vision-based and
multimodal RL baselines in a wide range of difficult driving situations. We
make our code publicly available.

</details>


### [627] [A phase-aware AI car-following model for electric vehicles with adaptive cruise control: Development and validation using real-world data](https://arxiv.org/abs/2510.21735)
*Yuhui Liu,Shian Wang,Ansel Panicker,Kate Embry,Ayana Asanova,Tianyi Li*

Main category: cs.RO

TL;DR: 现有微观模型缺乏对电动汽车跟车动力学的准确描述，本文开发并验证了用于电动汽车的相位感知AI跟车模型，该模型显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有微观模型无法准确描述电动汽车独特的跟车动力学，且随着电动汽车在交通中占比增加，需要开发相应模型。

Method: 开发相位感知AI（PAAI）跟车模型，用AI组件增强传统物理框架以适应不同驾驶阶段，利用配备自适应巡航控制（ACC）车辆的真实轨迹数据进行模拟验证。

Result: PAAI模型相比传统跟车模型显著提高了预测准确性。

Conclusion: PAAI模型是在交通模拟中准确呈现电动汽车行为的有效工具。

Abstract: Internal combustion engine (ICE) vehicles and electric vehicles (EVs) exhibit
distinct vehicle dynamics. EVs provide rapid acceleration, with electric motors
producing peak power across a wider speed range, and achieve swift deceleration
through regenerative braking. While existing microscopic models effectively
capture the driving behavior of ICE vehicles, a modeling framework that
accurately describes the unique car-following dynamics of EVs is lacking.
Developing such a model is essential given the increasing presence of EVs in
traffic, yet creating an easy-to-use and accurate analytical model remains
challenging.
  To address these gaps, this study develops and validates a Phase-Aware AI
(PAAI) car-following model specifically for EVs. The proposed model enhances
traditional physics-based frameworks with an AI component that recognizes and
adapts to different driving phases, such as rapid acceleration and regenerative
braking. Using real-world trajectory data from vehicles equipped with adaptive
cruise control (ACC), we conduct comprehensive simulations to validate the
model's performance. The numerical results demonstrate that the PAAI model
significantly improves prediction accuracy over traditional car-following
models, providing an effective tool for accurately representing EV behavior in
traffic simulations.

</details>


### [628] [Learn2Drive: A neural network-based framework for socially compliant automated vehicle control](https://arxiv.org/abs/2510.21736)
*Yuhui Liu,Samannita Halder,Shian Wang,Tianyi Li*

Main category: cs.RO

TL;DR: 本文提出基于LSTM网络和物理信息约束的自适应巡航控制（ACC）新框架，结合社会价值取向（SVO），经数值验证可提升交通效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动车辆控制策略忽视与人类驾驶车辆交互及对交通流影响，会加剧拥堵、降低系统效率。

Method: 提出基于神经网络、结合SVO的自动车辆控制框架，定义自动和人类驾驶车辆效用函数并基于SVO优化。

Result: 数值结果表明该方法能适应不同交通条件，当自动车辆控制模式转变时，后续车队个体能耗至少增58.99%，平均速度至少提38.39%。

Conclusion: 所提方法有效，能提升系统整体效率和交通动态性。

Abstract: This study introduces a novel control framework for adaptive cruise control
(ACC) in automated driving, leveraging Long Short-Term Memory (LSTM) networks
and physics-informed constraints. As automated vehicles (AVs) adopt advanced
features like ACC, transportation systems are becoming increasingly intelligent
and efficient. However, existing AV control strategies primarily focus on
optimizing the performance of individual vehicles or platoons, often neglecting
their interactions with human-driven vehicles (HVs) and the broader impact on
traffic flow. This oversight can exacerbate congestion and reduce overall
system efficiency. To address this critical research gap, we propose a neural
network-based, socially compliant AV control framework that incorporates social
value orientation (SVO). This framework enables AVs to account for their
influence on HVs and traffic dynamics. By leveraging AVs as mobile traffic
regulators, the proposed approach promotes adaptive driving behaviors that
reduce congestion, improve traffic efficiency, and lower energy consumption.
Within this framework, we define utility functions for both AVs and HVs, which
are optimized based on the SVO of each AV to balance its own control objectives
with broader traffic flow considerations. Numerical results demonstrate the
effectiveness of the proposed method in adapting to varying traffic conditions,
thereby enhancing system-wide efficiency. Specifically, when the AV's control
mode shifts from prioritizing energy consumption to optimizing traffic flow
efficiency, vehicles in the following platoon experience at least a 58.99%
increase in individual energy consumption alongside at least a 38.39%
improvement in individual average speed, indicating significant enhancements in
traffic dynamics.

</details>


### [629] [Next-Generation LLM for UAV: From Natural Language to Autonomous Flight](https://arxiv.org/abs/2510.21739)
*Liangqi Yuan,Chuhao Deng,Dong-Jun Han,Inseok Hwang,Sabine Brunswicker,Christopher G. Brinton*

Main category: cs.RO

TL;DR: 本文提出NeLV系统，将大语言模型集成到多尺度无人机操作中，通过五个关键技术组件处理自然语言指令，并通过三个用例验证可行性，还建立五级自动化分类。


<details>
  <summary>Details</summary>
Motivation: 当前研究多局限于小型无人机应用，缺乏对中远程无人机系统在现实场景的全面研究，而大型无人机平台有独特挑战。

Method: 提出NeLV系统，包含LLM - as - Parser、Route Planner、Path Planner、Control Platform和UAV monitoring五个关键技术组件，处理自然语言指令以编排无人机任务。

Result: 通过多无人机巡逻、多兴趣点交付和多跳重新定位三个用例证明了系统的可行性。

Conclusion: 建立五级自动化分类，明确从当前LLM - as - Parser能力到全自主LLM - as - Autopilot系统各阶段的技术前提和研究挑战。

Abstract: With the rapid advancement of Large Language Models (LLMs), their
capabilities in various automation domains, particularly Unmanned Aerial
Vehicle (UAV) operations, have garnered increasing attention. Current research
remains predominantly constrained to small-scale UAV applications, with most
studies focusing on isolated components such as path planning for toy drones,
while lacking comprehensive investigation of medium- and long-range UAV systems
in real-world operational contexts. Larger UAV platforms introduce distinct
challenges, including stringent requirements for airport-based take-off and
landing procedures, adherence to complex regulatory frameworks, and specialized
operational capabilities with elevated mission expectations. This position
paper presents the Next-Generation LLM for UAV (NeLV) system -- a comprehensive
demonstration and automation roadmap for integrating LLMs into multi-scale UAV
operations. The NeLV system processes natural language instructions to
orchestrate short-, medium-, and long-range UAV missions through five key
technical components: (i) LLM-as-Parser for instruction interpretation, (ii)
Route Planner for Points of Interest (POI) determination, (iii) Path Planner
for waypoint generation, (iv) Control Platform for executable trajectory
implementation, and (v) UAV monitoring. We demonstrate the system's feasibility
through three representative use cases spanning different operational scales:
multi-UAV patrol, multi-POI delivery, and multi-hop relocation. Beyond the
current implementation, we establish a five-level automation taxonomy that
charts the evolution from current LLM-as-Parser capabilities (Level 1) to fully
autonomous LLM-as-Autopilot systems (Level 5), identifying technical
prerequisites and research challenges at each stage.

</details>


### [630] [J-ORA: A Framework and Multimodal Dataset for Japanese Object Identification, Reference, Action Prediction in Robot Perception](https://arxiv.org/abs/2510.21761)
*Jesse Atuhurra,Hidetaka Kamigaito,Taro Watanabe,Koichiro Yoshino*

Main category: cs.RO

TL;DR: 介绍J - ORA多模态数据集，评估显示含详细对象属性可提升多模态感知性能，不同VLM有差距，强调属性注释对机器人感知的重要性。


<details>
  <summary>Details</summary>
Motivation: 填补机器人感知在日本人机对话场景中详细对象属性注释的空白，支持关键感知任务。

Method: 创建J - ORA数据集，利用综合属性模板，用专有和开源视觉语言模型（VLMs）进行评估。

Result: 含详细对象属性可大幅提升多模态感知性能，专有和开源VLMs存在差距，不同VLMs理解对象功能和上下文关系的能力不同。

Conclusion: 丰富、上下文敏感的属性注释对动态环境中提升机器人感知很重要。

Abstract: We introduce J-ORA, a novel multimodal dataset that bridges the gap in robot
perception by providing detailed object attribute annotations within Japanese
human-robot dialogue scenarios. J-ORA is designed to support three critical
perception tasks, object identification, reference resolution, and next-action
prediction, by leveraging a comprehensive template of attributes (e.g.,
category, color, shape, size, material, and spatial relations). Extensive
evaluations with both proprietary and open-source Vision Language Models (VLMs)
reveal that incorporating detailed object attributes substantially improves
multimodal perception performance compared to without object attributes.
Despite the improvement, we find that there still exists a gap between
proprietary and open-source VLMs. In addition, our analysis of object
affordances demonstrates varying abilities in understanding object
functionality and contextual relationships across different VLMs. These
findings underscore the importance of rich, context-sensitive attribute
annotations in advancing robot perception in dynamic environments. See project
page at https://jatuhurrra.github.io/J-ORA/.

</details>


### [631] [Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence](https://arxiv.org/abs/2510.21860)
*Callum Sharrock,Lukas Petersson,Hanna Petersson,Axel Backlund,Axel Wennström,Kristoffer Nordström,Elias Aronsson*

Main category: cs.RO

TL;DR: 提出Butter - Bench基准评估大语言模型控制的机器人的实践智能，发现人类在该基准上表现优于LLM，微调对提升LLM得分无效。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型控制的机器人在应对现实物理世界复杂性方面的实践智能。

Method: 提出Butter - Bench基准，将LLM部分与VLA分离进行评估。

Result: 最佳LLM在Butter - Bench上得分为40%，人类平均得分95%，LLM在多步空间规划和社会理解方面表现最差，微调对提升得分无效。

Conclusion: 人类在Butter - Bench评估中仍优于LLM，针对具身推理的微调训练不能提升LLM在该基准上的得分。

Abstract: We present Butter-Bench, a benchmark evaluating large language model (LLM)
controlled robots for practical intelligence, defined as the ability to
navigate the messiness of the physical world. Current state-of-the-art robotic
systems use a hierarchical architecture with LLMs in charge of high-level
reasoning, and a Vision Language Action (VLA) model for low-level control.
Butter-Bench evaluates the LLM part in isolation from the VLA. Although LLMs
have repeatedly surpassed humans in evaluations requiring analytical
intelligence, we find humans still outperform LLMs on Butter-Bench. The best
LLMs score 40% on Butter-Bench, while the mean human score is 95%. LLMs
struggled the most with multi-step spatial planning and social understanding.
We also evaluate LLMs that are fine-tuned for embodied reasoning and conclude
that this training does not improve their score on Butter-Bench.

</details>


### [632] [A Physics-Informed Neural Network Approach for UAV Path Planning in Dynamic Environments](https://arxiv.org/abs/2510.21874)
*Shuning Zhang*

Main category: cs.RO

TL;DR: 本文提出一种物理信息神经网络（PINN）框架用于无人机轨迹优化，对比显示该方法在控制能量、平滑度和安全裕度上优于传统规划器。


<details>
  <summary>Details</summary>
Motivation: 传统规划器如A*和kinodynamic RRT*因离散化和采样限制，常产生次优或不光滑路径，需要更好的方法使无人机在动态风场中生成安全且节能的轨迹。

Method: 提出PINN框架，将无人机动力学、风干扰和避障直接嵌入学习过程，通过最小化物理残差和风险感知目标来学习动态可行且无碰撞的轨迹。

Result: 对比模拟显示，该方法在控制能量、平滑度和安全裕度上优于A*和Kino - RRT*，且飞行效率相近。

Conclusion: 物理信息学习有潜力统一基于模型和数据驱动的规划，为无人机轨迹优化提供可扩展且物理一致的框架。

Abstract: Unmanned aerial vehicles (UAVs) operating in dynamic wind fields must
generate safe and energy-efficient trajectories under physical and
environmental constraints. Traditional planners, such as A* and kinodynamic
RRT*, often yield suboptimal or non-smooth paths due to discretization and
sampling limitations. This paper presents a physics-informed neural network
(PINN) framework that embeds UAV dynamics, wind disturbances, and obstacle
avoidance directly into the learning process. Without requiring supervised
data, the PINN learns dynamically feasible and collision-free trajectories by
minimizing physical residuals and risk-aware objectives. Comparative
simulations show that the proposed method outperforms A* and Kino-RRT* in
control energy, smoothness, and safety margin, while maintaining similar flight
efficiency. The results highlight the potential of physics-informed learning to
unify model-based and data-driven planning, providing a scalable and physically
consistent framework for UAV trajectory optimization.

</details>


### [633] [Two-Steps Diffusion Policy for Robotic Manipulation via Genetic Denoising](https://arxiv.org/abs/2510.21991)
*Mateo Clemente,Leo Brunswic,Rui Heng Yang,Xuan Zhao,Yasser Khalil,Haoyu Lei,Amir Rasouli,Yinchuan Li*

Main category: cs.RO

TL;DR: 本文针对具身AI任务特点优化扩散策略，提出遗传去噪采样策略，在机器人操作任务中表现优于标准扩散策略。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型推理策略直接迁移到控制领域，未适配具身AI任务特点，需优化以提升效率和性能。

Method: 根据具身AI任务特点调整去噪过程，提出基于种群的采样策略——遗传去噪。

Result: 在14个机器人操作任务中评估，200多万次评估显示该方法始终优于标准扩散策略，减少推理步骤同时性能提升达20%。

Conclusion: 针对具身AI任务特点优化的扩散策略及遗传去噪方法有效，能提升性能和稳定性。

Abstract: Diffusion models, such as diffusion policy, have achieved state-of-the-art
results in robotic manipulation by imitating expert demonstrations. While
diffusion models were originally developed for vision tasks like image and
video generation, many of their inference strategies have been directly
transferred to control domains without adaptation. In this work, we show that
by tailoring the denoising process to the specific characteristics of embodied
AI tasks -- particularly structured, low-dimensional nature of action
distributions -- diffusion policies can operate effectively with as few as 5
neural function evaluations (NFE).
  Building on this insight, we propose a population-based sampling strategy,
genetic denoising, which enhances both performance and stability by selecting
denoising trajectories with low out-of-distribution risk. Our method solves
challenging tasks with only 2 NFE while improving or matching performance. We
evaluate our approach across 14 robotic manipulation tasks from D4RL and
Robomimic, spanning multiple action horizons and inference budgets. In over 2
million evaluations, our method consistently outperforms standard
diffusion-based policies, achieving up to 20\% performance gains with
significantly fewer inference steps.

</details>


### [634] [Bridging Perception and Reasoning: Dual-Pipeline Neuro-Symbolic Landing for UAVs in Cluttered Environments](https://arxiv.org/abs/2510.22204)
*Weixian Qian,Sebastian Schroder,Yao Deng,Jiaohong Yao,Linfeng Liang,Xiao Cheng,Richard Han,Xi Zheng*

Main category: cs.RO

TL;DR: 提出NeuroSymLand框架用于无人机在非结构化环境自主着陆，结合感知与符号推理，评估显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 纯视觉或深度学习模型在协变量偏移下表现不佳且可解释性有限，需要解决无人机在非结构化环境的自主着陆问题。

Method: 提出NeuroSymLand框架，包含离线和在线两条管道，结合轻量级基础模型感知能力与符号推理可解释性，用几何例程计算图属性和关系。

Result: 在数据集、模拟地图和真实硬件上评估，NeuroSymLand准确性更高、对协变量偏移更鲁棒、效率更优。

Conclusion: NeuroSymLand能提升无人机在应急响应、监视和投递任务中的安全性和可靠性。

Abstract: Autonomous landing in unstructured (cluttered, uneven, and map-poor)
environments is a core requirement for Unmanned Aerial Vehicles (UAVs), yet
purely vision-based or deep learning models often falter under covariate shift
and provide limited interpretability. We propose NeuroSymLand, a neuro-symbolic
framework that tightly couples two complementary pipelines: (i) an offline
pipeline, where Large Language Models (LLMs) and human-in-the-loop refinement
synthesize Scallop code from diverse landing scenarios, distilling
generalizable and verifiable symbolic knowledge; and (ii) an online pipeline,
where a compact foundation-based semantic segmentation model generates
probabilistic Scallop facts that are composed into semantic scene graphs for
real-time deductive reasoning. This design combines the perceptual strengths of
lightweight foundation models with the interpretability and verifiability of
symbolic reasoning. Node attributes (e.g., flatness, area) and edge relations
(adjacency, containment, proximity) are computed with geometric routines rather
than learned, avoiding the data dependence and latency of train-time graph
builders. The resulting Scallop program encodes landing principles (avoid water
and obstacles; prefer large, flat, accessible regions) and yields calibrated
safety scores with ranked Regions of Interest (ROIs) and human-readable
justifications. Extensive evaluations across datasets, diverse simulation maps,
and real UAV hardware show that NeuroSymLand achieves higher accuracy, stronger
robustness to covariate shift, and superior efficiency compared with
state-of-the-art baselines, while advancing UAV safety and reliability in
emergency response, surveillance, and delivery missions.

</details>


### [635] [Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and Morphology for Fall Recovery](https://arxiv.org/abs/2510.22336)
*Bo Yue,Sheng Xu,Kui Jia,Guiliang Liu*

Main category: cs.RO

TL;DR: 提出RoboCraft框架用于类人机器人摔倒恢复，实验显示在多机器人上有性能提升，凸显类人机器人协同设计重要性。


<details>
  <summary>Details</summary>
Motivation: 类人机器人脑体协同设计有潜力，摔倒恢复是关键能力，需有效方法提升其性能和自主性。

Method: 提出RoboCraft框架，通过控制策略和形态的耦合更新迭代提升性能，预训练共享策略并微调，形态搜索借助人类启发先验和优化算法及优先缓冲区。

Result: 在七个公开类人机器人上平均性能提升44.55%，形态优化在四个类人机器人协同设计中推动至少40%的改进。

Conclusion: 类人机器人协同设计对提升性能至关重要，RoboCraft框架有效。

Abstract: Humanoid robots represent a central frontier in embodied intelligence, as
their anthropomorphic form enables natural deployment in humans' workspace.
Brain-body co-design for humanoids presents a promising approach to realizing
this potential by jointly optimizing control policies and physical morphology.
Within this context, fall recovery emerges as a critical capability. It not
only enhances safety and resilience but also integrates naturally with
locomotion systems, thereby advancing the autonomy of humanoids. In this paper,
we propose RoboCraft, a scalable humanoid co-design framework for fall recovery
that iteratively improves performance through the coupled updates of control
policy and morphology. A shared policy pretrained across multiple designs is
progressively finetuned on high-performing morphologies, enabling efficient
adaptation without retraining from scratch. Concurrently, morphology search is
guided by human-inspired priors and optimization algorithms, supported by a
priority buffer that balances reevaluation of promising candidates with the
exploration of novel designs. Experiments show that \ourmethod{} achieves an
average performance gain of 44.55% on seven public humanoid robots, with
morphology optimization drives at least 40% of improvements in co-designing
four humanoid robots, underscoring the critical role of humanoid co-design.

</details>


### [636] [Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review](https://arxiv.org/abs/2510.21758)
*Kumater Ter,RexCharles Donatus,Ore-Ofe Ajayi,Daniel Udekwe*

Main category: cs.RO

TL;DR: 本文深入回顾强化学习原理、先进深度强化学习算法及其在机器人和控制系统中的应用，介绍分类法，旨在连接理论与实践。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为动态不确定环境中实现智能机器人行为的基础方法，本文旨在将理论进展与实际应用相结合。

Method: 从马尔可夫决策过程形式化开始，探讨智能体 - 环境交互要素和核心算法策略，介绍现代深度强化学习技术，引入结构化分类法对应用领域进行分类。

Result: 综合了近期研究成果，突出技术趋势、设计模式和强化学习在现实机器人应用中的成熟度。

Conclusion: 为强化学习在自主机器人系统中的不断演变角色提供了综合视角。

Abstract: Reinforcement learning (RL) has become a foundational approach for enabling
intelligent robotic behavior in dynamic and uncertain environments. This work
presents an in-depth review of RL principles, advanced deep reinforcement
learning (DRL) algorithms, and their integration into robotic and control
systems. Beginning with the formalism of Markov Decision Processes (MDPs), the
study outlines essential elements of the agent-environment interaction and
explores core algorithmic strategies including actor-critic methods,
value-based learning, and policy gradients. Emphasis is placed on modern DRL
techniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving
high-dimensional, continuous control tasks. A structured taxonomy is introduced
to categorize RL applications across domains such as locomotion, manipulation,
multi-agent coordination, and human-robot interaction, along with training
methodologies and deployment readiness levels. The review synthesizes recent
research efforts, highlighting technical trends, design patterns, and the
growing maturity of RL in real-world robotics. Overall, this work aims to
bridge theoretical advances with practical implementations, providing a
consolidated perspective on the evolving role of RL in autonomous robotic
systems.

</details>


### [637] [VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting](https://arxiv.org/abs/2510.21817)
*Xiaoyu Liu,Chaoyou Fu,Chi Yan,Chu Wu,Haihan Gao,Yi-Fan Zhang,Shaoqi Dong,Cheng Qian,Bin Luo,Xiuyong Yang,Guanwu Li,Yusheng Cai,Yunhang Shen,Deqiang Jiang,Haoyu Cao,Xing Sun,Caifeng Shan,Ran He*

Main category: cs.RO

TL;DR: 现有VLA模型交互范式有局限，提出VITA - E框架，经实验可处理复杂场景，迈向更自然具身助手。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型交互范式僵化，缺乏并发和动态处理中断能力，阻碍无缝具身协作，需改进。

Method: 引入VITA - E框架，采用双模型架构，让两个VLA实例并行工作；提出‘model - as - controller’范式，微调VLM生成特殊系统级命令。

Result: 在物理人形平台实验表明，VITA - E能处理复杂交互场景，兼容多种双系统VLA模型，在紧急停止和语音中断上成功率高，可并发执行语音和动作。

Conclusion: VITA - E是迈向更自然、更有能力的具身助手的重要一步。

Abstract: Current Vision-Language-Action (VLA) models are often constrained by a rigid,
static interaction paradigm, which lacks the ability to see, hear, speak, and
act concurrently as well as handle real-time user interruptions dynamically.
This hinders seamless embodied collaboration, resulting in an inflexible and
unresponsive user experience. To address these limitations, we introduce
VITA-E, a novel embodied interaction framework designed for both behavioral
concurrency and nearly real-time interruption. The core of our approach is a
dual-model architecture where two parallel VLA instances operate as an ``Active
Model'' and a ``Standby Model'', allowing the embodied agent to observe its
environment, listen to user speech, provide verbal responses, and execute
actions, all concurrently and interruptibly, mimicking human-like multitasking
capabilities. We further propose a ``model-as-controller'' paradigm, where we
fine-tune the VLM to generate special tokens that serve as direct system-level
commands, coupling the model's reasoning with the system's behavior.
Experiments conducted on a physical humanoid platform demonstrate that VITA-E
can reliably handle complex interactive scenarios. Our framework is compatible
with various dual-system VLA models, achieving an extremely high success rate
on emergency stops and speech interruptions while also successfully performing
concurrent speech and action. This represents a significant step towards more
natural and capable embodied assistants.

</details>


### [638] [SPIRAL: Self-Play Incremental Racing Algorithm for Learning in Multi-Drone Competitions](https://arxiv.org/abs/2510.22568)
*Onur Akgün*

Main category: cs.RO

TL;DR: 本文介绍SPIRAL算法用于训练多智能体竞赛中的自主无人机，可与DRL算法集成，模拟显示其优势，为该领域提供新框架。


<details>
  <summary>Details</summary>
Motivation: 开发一种能在多智能体竞赛中训练自主无人机的有效方法，提升其在复杂竞争场景中的性能和可靠性。

Method: 提出SPIRAL算法，采用自博弈机制，让无人机与更强大的自身版本竞争，可与任何先进的深度强化学习算法集成。

Result: 模拟显示SPIRAL有显著优势，对多种DRL算法进行了性能基准测试。

Conclusion: 为自主无人机竞赛领域贡献了通用、可扩展和自我改进的学习框架，为开发鲁棒自适应策略提供了方向。

Abstract: This paper introduces SPIRAL (Self-Play Incremental Racing Algorithm for
Learning), a novel approach for training autonomous drones in multi-agent
racing competitions. SPIRAL distinctively employs a self-play mechanism to
incrementally cultivate complex racing behaviors within a challenging, dynamic
environment. Through this self-play core, drones continuously compete against
increasingly proficient versions of themselves, naturally escalating the
difficulty of competitive interactions. This progressive learning journey
guides agents from mastering fundamental flight control to executing
sophisticated cooperative multi-drone racing strategies. Our method is designed
for versatility, allowing integration with any state-of-the-art Deep
Reinforcement Learning (DRL) algorithms within its self-play framework.
Simulations demonstrate the significant advantages of SPIRAL and benchmark the
performance of various DRL algorithms operating within it. Consequently, we
contribute a versatile, scalable, and self-improving learning framework to the
field of autonomous drone racing. SPIRAL's capacity to autonomously generate
appropriate and escalating challenges through its self-play dynamic offers a
promising direction for developing robust and adaptive racing strategies in
multi-agent environments. This research opens new avenues for enhancing the
performance and reliability of autonomous racing drones in increasingly complex
and competitive scenarios.

</details>


### [639] [Curriculum-Based Iterative Self-Play for Scalable Multi-Drone Racing](https://arxiv.org/abs/2510.22570)
*Onur Akgün*

Main category: cs.RO

TL;DR: 本文提出CRUISE框架解决多无人机竞速中多智能体协调挑战，在模拟中表现出色，课程结构是性能提升关键。


<details>
  <summary>Details</summary>
Motivation: 解决高速竞争环境下多自主智能体协调这一工程挑战，特别是多无人机竞速领域。

Method: 提出CRUISE框架，结合渐进难度课程与高效自我博弈机制。

Result: 在高保真模拟中，CRUISE策略显著优于标准强化学习基线和最先进的博弈论规划器，速度接近规划器两倍，成功率高，且随智能体密度增加可扩展性好。

Conclusion: CRUISE提供了可扩展且有效的训练方法，推动了动态竞争任务的自主系统发展，为实际应用提供蓝图。

Abstract: The coordination of multiple autonomous agents in high-speed, competitive
environments represents a significant engineering challenge. This paper
presents CRUISE (Curriculum-Based Iterative Self-Play for Scalable Multi-Drone
Racing), a reinforcement learning framework designed to solve this challenge in
the demanding domain of multi-drone racing. CRUISE overcomes key scalability
limitations by synergistically combining a progressive difficulty curriculum
with an efficient self-play mechanism to foster robust competitive behaviors.
Validated in high-fidelity simulation with realistic quadrotor dynamics, the
resulting policies significantly outperform both a standard reinforcement
learning baseline and a state-of-the-art game-theoretic planner. CRUISE
achieves nearly double the planner's mean racing speed, maintains high success
rates, and demonstrates robust scalability as agent density increases. Ablation
studies confirm that the curriculum structure is the critical component for
this performance leap. By providing a scalable and effective training
methodology, CRUISE advances the development of autonomous systems for dynamic,
competitive tasks and serves as a blueprint for future real-world deployment.

</details>


### [640] [RoGER-SLAM: A Robust Gaussian Splatting SLAM System for Noisy and Low-light Environment Resilience](https://arxiv.org/abs/2510.22600)
*Huilin Yin,Zhaolin Yang,Linchuan Zhang,Gerhard Rigoll,Johannes Betz*

Main category: cs.RO

TL;DR: 现有3DGS SLAM框架在噪声和低光照环境下性能不佳，本文提出RoGER - SLAM系统，经实验证明该系统在不利成像条件下能提升轨迹精度和重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS基于的SLAM框架在视觉输入有噪声和低光照的环境中，制图和跟踪性能易受复合退化影响，可靠性受限。

Method: 提出RoGER - SLAM系统，集成了结构保留鲁棒融合机制、带残差平衡正则化的自适应跟踪目标以及基于CLIP的增强模块。

Result: 在Replica、TUM和真实世界序列的综合实验中，RoGER - SLAM与其他3DGS - SLAM系统相比，持续提高了轨迹精度和重建质量，尤其在不利成像条件下。

Conclusion: RoGER - SLAM系统在噪声和低光照等不利成像条件下，能有效提高轨迹精度和重建质量，优于其他3DGS - SLAM系统。

Abstract: The reliability of Simultaneous Localization and Mapping (SLAM) is severely
constrained in environments where visual inputs suffer from noise and low
illumination. Although recent 3D Gaussian Splatting (3DGS) based SLAM
frameworks achieve high-fidelity mapping under clean conditions, they remain
vulnerable to compounded degradations that degrade mapping and tracking
performance. A key observation underlying our work is that the original 3DGS
rendering pipeline inherently behaves as an implicit low-pass filter,
attenuating high-frequency noise but also risking over-smoothing. Building on
this insight, we propose RoGER-SLAM, a robust 3DGS SLAM system tailored for
noise and low-light resilience. The framework integrates three innovations: a
Structure-Preserving Robust Fusion (SP-RoFusion) mechanism that couples
rendered appearance, depth, and edge cues; an adaptive tracking objective with
residual balancing regularization; and a Contrastive Language-Image Pretraining
(CLIP)-based enhancement module, selectively activated under compounded
degradations to restore semantic and structural fidelity. Comprehensive
experiments on Replica, TUM, and real-world sequences show that RoGER-SLAM
consistently improves trajectory accuracy and reconstruction quality compared
with other 3DGS-SLAM systems, especially under adverse imaging conditions.

</details>


### [641] [Uncertainty-Aware Autonomous Vehicles: Predicting the Road Ahead](https://arxiv.org/abs/2510.22680)
*Shireen Kudukkil Manchingal,Armand Amaritei,Mihir Gohad,Maryam Sultana,Julian F. P. Kooij,Fabio Cuzzolin,Andrew Bradley*

Main category: cs.RO

TL;DR: 本文研究用不确定性感知图像分类器使自动驾驶汽车能识别不确定性，采用RS - NNs量化预测不确定性，经测试比传统方法更优，证明其可用于更安全鲁棒的自动驾驶。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶感知系统在罕见事件或样本外数据下易因过度自信预测产生错误，需让自动驾驶汽车具备识别不确定性的能力。

Method: 使用随机集神经网络（RS - NNs）明确量化预测不确定性，在真实世界自动驾驶赛车软件栈中测试，并与传统CNN和贝叶斯神经网络对比。

Result: RS - NN在多种路况下准确率更高，不确定性校准更优，能动态调节车速，保证不同情况下的性能和安全。

Conclusion: 不确定性感知神经网络（尤其是RS - NNs）是实现更安全、更鲁棒自动驾驶的实用解决方案。

Abstract: Autonomous Vehicle (AV) perception systems have advanced rapidly in recent
years, providing vehicles with the ability to accurately interpret their
environment. Perception systems remain susceptible to errors caused by
overly-confident predictions in the case of rare events or out-of-sample data.
This study equips an autonomous vehicle with the ability to 'know when it is
uncertain', using an uncertainty-aware image classifier as part of the AV
software stack. Specifically, the study exploits the ability of Random-Set
Neural Networks (RS-NNs) to explicitly quantify prediction uncertainty. Unlike
traditional CNNs or Bayesian methods, RS-NNs predict belief functions over sets
of classes, allowing the system to identify and signal uncertainty clearly in
novel or ambiguous scenarios. The system is tested in a real-world autonomous
racing vehicle software stack, with the RS-NN classifying the layout of the
road ahead and providing the associated uncertainty of the prediction.
Performance of the RS-NN under a range of road conditions is compared against
traditional CNN and Bayesian neural networks, with the RS-NN achieving
significantly higher accuracy and superior uncertainty calibration. This
integration of RS-NNs into Robot Operating System (ROS)-based vehicle control
pipeline demonstrates that predictive uncertainty can dynamically modulate
vehicle speed, maintaining high-speed performance under confident predictions
while proactively improving safety through speed reductions in uncertain
scenarios. These results demonstrate the potential of uncertainty-aware neural
networks - in particular RS-NNs - as a practical solution for safer and more
robust autonomous driving.

</details>


### [642] [Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM](https://arxiv.org/abs/2510.22740)
*Sai Krishna Ghanta,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: 提出基于多智能体强化学习（MARL）的可扩展、抗异常值的分布式平面PGO框架，在合成和真实数据集上表现优于现有方法，且单策略可扩展到更大机器人团队。


<details>
  <summary>Details</summary>
Motivation: 传统迭代方法解决分布式姿态图优化（PGO）问题常收敛到局部极小值，产生次优估计。

Method: 将分布式PGO建模为局部姿态图上的部分可观测马尔可夫博弈，用图分割器分解全局图，每个机器人运行带自适应边门控的循环边条件图神经网络编码器去噪，通过混合策略依次优化姿态，最后用共识方案协调机器人间分歧。

Result: 基于MARL的智能体比现有分布式PGO框架平均降低全局目标37.5%，推理效率至少提高6倍，单策略可轻松扩展到更大机器人团队。

Conclusion: 所提基于MARL的分布式平面PGO框架有效，在性能和可扩展性上表现良好。

Abstract: We consider the distributed pose-graph optimization (PGO) problem, which is
fundamental in accurate trajectory estimation in multi-robot simultaneous
localization and mapping (SLAM). Conventional iterative approaches linearize a
highly non-convex optimization objective, requiring repeated solving of normal
equations, which often converge to local minima and thus produce suboptimal
estimates. We propose a scalable, outlier-robust distributed planar PGO
framework using Multi-Agent Reinforcement Learning (MARL). We cast distributed
PGO as a partially observable Markov game defined on local pose-graphs, where
each action refines a single edge's pose estimate. A graph partitioner
decomposes the global pose graph, and each robot runs a recurrent
edge-conditioned Graph Neural Network (GNN) encoder with adaptive edge-gating
to denoise noisy edges. Robots sequentially refine poses through a hybrid
policy that utilizes prior action memory and graph embeddings. After local
graph correction, a consensus scheme reconciles inter-robot disagreements to
produce a globally consistent estimate. Our extensive evaluations on a
comprehensive suite of synthetic and real-world datasets demonstrate that our
learned MARL-based actors reduce the global objective by an average of 37.5%
more than the state-of-the-art distributed PGO framework, while enhancing
inference efficiency by at least 6X. We also demonstrate that actor replication
allows a single learned policy to scale effortlessly to substantially larger
robot teams without any retraining. Code is publicly available at
https://github.com/herolab-uga/policies-over-poses.

</details>


### [643] [PIP-LLM: Integrating PDDL-Integer Programming with LLMs for Coordinating Multi-Robot Teams Using Natural Language](https://arxiv.org/abs/2510.22784)
*Guangyao Shi,Yuwei Wu,Vijay Kumar,Gaurav S. Sukhatme*

Main category: cs.RO

TL;DR: 介绍语言协调框架PIP - LLM处理多机器人协调问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有结合LLM和PDDL的方法在多机器人协调中因任务分解脆弱、可扩展性差和协调效率低而面临困难，需要更好的方法。

Method: 提出PIP - LLM框架，包含基于PDDL的团队级规划和基于整数规划（IP）的机器人级规划，先进行团队级规划获得计划，再转化为依赖图指导机器人级规划。

Result: 在不同任务的实验中，PIP - LLM提高了计划成功率，降低了最大和平均旅行成本，实现了更好的负载平衡。

Conclusion: PIP - LLM能避免基于语法分解的陷阱，可扩展到更大团队，在多机器人协调方面优于现有基线方法。

Abstract: Enabling robot teams to execute natural language commands requires
translating high-level instructions into feasible, efficient multi-robot plans.
While Large Language Models (LLMs) combined with Planning Domain Description
Language (PDDL) offer promise for single-robot scenarios, existing approaches
struggle with multi-robot coordination due to brittle task decomposition, poor
scalability, and low coordination efficiency.
  We introduce PIP-LLM, a language-based coordination framework that consists
of PDDL-based team-level planning and Integer Programming (IP) based
robot-level planning. PIP-LLMs first decomposes the command by translating the
command into a team-level PDDL problem and solves it to obtain a team-level
plan, abstracting away robot assignment. Each team-level action represents a
subtask to be finished by the team. Next, this plan is translated into a
dependency graph representing the subtasks' dependency structure. Such a
dependency graph is then used to guide the robot-level planning, in which each
subtask node will be formulated as an IP-based task allocation problem,
explicitly optimizing travel costs and workload while respecting robot
capabilities and user-defined constraints. This separation of planning from
assignment allows PIP-LLM to avoid the pitfalls of syntax-based decomposition
and scale to larger teams. Experiments across diverse tasks show that PIP-LLM
improves plan success rate, reduces maximum and average travel costs, and
achieves better load balancing compared to state-of-the-art baselines.

</details>


### [644] [HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment](https://arxiv.org/abs/2510.22917)
*Zecheng Yin,Hao Zhao,Zhen Li*

Main category: cs.RO

TL;DR: 提出HyPerNav方法，结合VLMs联合感知局部和全局信息提升未知环境导航效果，在模拟和现实中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有ObjNav研究多关注单一感知源，未整合RGB - D传感器的局部信息和实时自上而下地图的全局信息，而人类会同时关注两者。

Method: 提出Hybrid Perception Navigation (HyPerNav)，利用VLMs的推理和视觉 - 语言理解能力，联合感知局部和全局信息。

Result: 在大量模拟评估和现实验证中，相比流行基线取得了最先进的性能，能捕获更丰富线索，更有效地找到目标物体。

Conclusion: 混合感知方法有助于提升导航性能，消融研究证明两种感知方式都对导航性能有贡献。

Abstract: Objective-oriented navigation(ObjNav) enables robot to navigate to target
object directly and autonomously in an unknown environment. Effective
perception in navigation in unknown environment is critical for autonomous
robots. While egocentric observations from RGB-D sensors provide abundant local
information, real-time top-down maps offer valuable global context for ObjNav.
Nevertheless, the majority of existing studies focus on a single source, seldom
integrating these two complementary perceptual modalities, despite the fact
that humans naturally attend to both. With the rapid advancement of
Vision-Language Models(VLMs), we propose Hybrid Perception Navigation
(HyPerNav), leveraging VLMs' strong reasoning and vision-language understanding
capabilities to jointly perceive both local and global information to enhance
the effectiveness and intelligence of navigation in unknown environments. In
both massive simulation evaluation and real-world validation, our methods
achieved state-of-the-art performance against popular baselines. Benefiting
from hybrid perception approach, our method captures richer cues and finds the
objects more effectively, by simultaneously leveraging information
understanding from egocentric observations and the top-down map. Our ablation
study further proved that either of the hybrid perception contributes to the
navigation performance.

</details>


### [645] [Deep Active Inference with Diffusion Policy and Multiple Timescale World Model for Real-World Exploration and Navigation](https://arxiv.org/abs/2510.23258)
*Riko Yokozawa,Kentaro Fujii,Yuta Nomura,Shingo Murata*

Main category: cs.RO

TL;DR: 本文提出基于主动推理的深度框架用于机器人导航，结合扩散策略和MTRSSM模型，实验显示其在真实场景中效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 实现真实环境中自主机器人导航，将探索和目标导向导航行为统一。

Method: 提出深度主动推理框架，集成扩散策略作为策略模型，多时间尺度循环状态空间模型作为世界模型。

Result: 真实世界导航实验中，该框架比基线有更高成功率和更少碰撞，在探索要求高的场景表现更佳。

Conclusion: 基于预期自由能最小化的主动推理能在真实机器人场景中统一探索和目标导向导航。

Abstract: Autonomous robotic navigation in real-world environments requires exploration
to acquire environmental information as well as goal-directed navigation in
order to reach specified targets. Active inference (AIF) based on the
free-energy principle provides a unified framework for these behaviors by
minimizing the expected free energy (EFE), thereby combining epistemic and
extrinsic values. To realize this practically, we propose a deep AIF framework
that integrates a diffusion policy as the policy model and a multiple timescale
recurrent state-space model (MTRSSM) as the world model. The diffusion policy
generates diverse candidate actions while the MTRSSM predicts their
long-horizon consequences through latent imagination, enabling action selection
that minimizes EFE. Real-world navigation experiments demonstrated that our
framework achieved higher success rates and fewer collisions compared with the
baselines, particularly in exploration-demanding scenarios. These results
highlight how AIF based on EFE minimization can unify exploration and
goal-directed navigation in real-world robotic settings.

</details>


### [646] [TARC: Time-Adaptive Robotic Control](https://arxiv.org/abs/2510.23176)
*Arnav Sukhija,Lenart Treven,Jin Cheng,Florian Dörfler,Stelian Coros,Andreas Krause*

Main category: cs.RO

TL;DR: 提出强化学习方法使机器人自主调节控制频率，在两个硬件平台验证，表现优于固定频率基线。


<details>
  <summary>Details</summary>
Motivation: 解决机器人固定频率控制在效率和鲁棒性间的权衡问题，借鉴生物系统的适应性。

Method: 采用强化学习方法，策略联合选择控制动作及其应用持续时间。

Result: 在高速遥控车和四足机器人的零样本仿真到现实实验中，方法在奖励上匹配或优于固定频率基线，显著降低控制频率，在现实条件下展现自适应频率控制。

Conclusion: 所提方法能有效让机器人自主调节控制频率，在实际应用中有优势。

Abstract: Fixed-frequency control in robotics imposes a trade-off between the
efficiency of low-frequency control and the robustness of high-frequency
control, a limitation not seen in adaptable biological systems. We address this
with a reinforcement learning approach in which policies jointly select control
actions and their application durations, enabling robots to autonomously
modulate their control frequency in response to situational demands. We
validate our method with zero-shot sim-to-real experiments on two distinct
hardware platforms: a high-speed RC car and a quadrupedal robot. Our method
matches or outperforms fixed-frequency baselines in terms of rewards while
significantly reducing the control frequency and exhibiting adaptive frequency
control under real-world conditions.

</details>


### [647] [RobotArena $\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation](https://arxiv.org/abs/2510.23571)
*Yash Jangir,Yidi Zhang,Kashu Yamazaki,Chenyu Zhang,Kuan-Hsun Tu,Tsung-Wei Ke,Lei Ke,Yonatan Bisk,Katerina Fragkiadaki*

Main category: cs.RO

TL;DR: 本文提出新基准框架，将VLA评估转移到有在线人类反馈的大规模模拟环境，解决机器人策略评估难题。


<details>
  <summary>Details</summary>
Motivation: 现实世界测试受限，现有模拟基准有局限，随着策略复杂度提升问题加剧，需新评估方法。

Method: 利用视觉语言模型、2D到3D生成建模和可微渲染，将视频演示转为模拟场景，用自动VLM评分和众包人类偏好判断评估，通过扰动模拟环境测试鲁棒性。

Result: 得到可不断发展、可复现且可扩展的真实世界训练机器人操作策略基准。

Conclusion: 该框架解决了当前机器人领域关键缺失能力问题。

Abstract: The pursuit of robot generalists - instructable agents capable of performing
diverse tasks across diverse environments - demands rigorous and scalable
evaluation. Yet real-world testing of robot policies remains fundamentally
constrained: it is labor-intensive, slow, unsafe at scale, and difficult to
reproduce. Existing simulation benchmarks are similarly limited, as they train
and test policies within the same synthetic domains and cannot assess models
trained from real-world demonstrations or alternative simulation environments.
As policies expand in scope and complexity, these barriers only intensify,
since defining "success" in robotics often hinges on nuanced human judgments of
execution quality. In this paper, we introduce a new benchmarking framework
that overcomes these challenges by shifting VLA evaluation into large-scale
simulated environments augmented with online human feedback. Leveraging
advances in vision-language models, 2D-to-3D generative modeling, and
differentiable rendering, our approach automatically converts video
demonstrations from widely used robot datasets into simulated counterparts.
Within these digital twins, we assess VLA policies using both automated
VLM-guided scoring and scalable human preference judgments collected from
crowdworkers, transforming human involvement from tedious scene setup,
resetting, and safety supervision into lightweight preference comparisons. To
measure robustness, we systematically perturb simulated environments along
multiple axes, such as textures and object placements, stress-testing policy
generalization under controlled variation. The result is a continuously
evolving, reproducible, and scalable benchmark for real-world trained robot
manipulation policies, addressing a critical missing capability in today's
robotics landscape.

</details>


### [648] [UrbanVLA: A Vision-Language-Action Model for Urban Micromobility](https://arxiv.org/abs/2510.23576)
*Anqi Li,Zhiyong Wang,Jiazhao Zhang,Minghan Li,Yunpeng Qi,Zhibo Chen,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: 提出UrbanVLA框架用于可扩展的城市导航，采用两阶段训练，实验表明其性能优于基线且能实现可靠的现实世界导航。


<details>
  <summary>Details</summary>
Motivation: 城市微出行应用需要在大规模城市环境中可靠导航，但现有方法多适用于短距离可控场景，因此需要新的导航方法。

Method: 提出UrbanVLA框架，在执行过程中将路线航点与视觉观察对齐并规划轨迹；采用两阶段训练，先在模拟环境和网络视频轨迹上进行监督微调，再在模拟和真实数据上进行强化微调。

Result: 在SocialNav任务上比强基线性能提升超55%，能实现可靠的现实世界导航。

Conclusion: UrbanVLA具有可扩展性和对现实世界不确定性的鲁棒性，适用于大规模城市环境导航。

Abstract: Urban micromobility applications, such as delivery robots, demand reliable
navigation across large-scale urban environments while following long-horizon
route instructions. This task is particularly challenging due to the dynamic
and unstructured nature of real-world city areas, yet most existing navigation
methods remain tailored to short-scale and controllable scenarios. Effective
urban micromobility requires two complementary levels of navigation skills:
low-level capabilities such as point-goal reaching and obstacle avoidance, and
high-level capabilities, such as route-visual alignment. To this end, we
propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework
designed for scalable urban navigation. Our method explicitly aligns noisy
route waypoints with visual observations during execution, and subsequently
plans trajectories to drive the robot. To enable UrbanVLA to master both levels
of navigation, we employ a two-stage training pipeline. The process begins with
Supervised Fine-Tuning (SFT) using simulated environments and trajectories
parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on
a mixture of simulation and real-world data, which enhances the model's safety
and adaptability in real-world settings. Experiments demonstrate that UrbanVLA
surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.
Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both
scalability to large-scale urban environments and robustness against real-world
uncertainties.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [649] [Feedback in Dynamic Contests: Theory and Experiment](https://arxiv.org/abs/2510.23178)
*Sumit Goel,Yiqing Yan,Jeffrey Zeidel*

Main category: econ.TH

TL;DR: 研究动态全支付拍卖中中期反馈政策的影响，分析均衡结果，进行实验，发现投标行为偏离均衡但无处理效应，还测试沉没成本和气馁效应。


<details>
  <summary>Details</summary>
Motivation: 探究动态全支付拍卖中中期反馈政策的效果。

Method: 理论分析得出均衡特征，进行包含四种反馈政策处理的实验。

Result: 顺序均衡结果由最便宜信号均衡表征，均衡收益为零，总投标预期和等于奖品价值；投标行为偏离均衡，未拒绝处理对总投标无影响的假设。

Conclusion: 在动态全支付拍卖中，中期反馈政策对总投标无显著处理效应，阶段1投标会产生沉没成本和先发优势并影响阶段2投标。

Abstract: We study the effect of interim feedback policies in a dynamic all-pay auction
where two players bid over two stages to win a common-value prize. We show that
sequential equilibrium outcomes are characterized by Cheapest Signal
Equilibria, wherein stage 1 bids are such that one player bids zero while the
other chooses a cheapest bid consistent with some signal. Equilibrium payoffs
for both players are always zero, and the sum of expected total bids equals the
value of the prize. We conduct an experiment with four natural feedback policy
treatments -- full, rank, and two cutoff policies -- and while the bidding
behavior deviates from equilibrium, we fail to reject the hypothesis of no
treatment effect on total bids. Further, stage 1 bids induce sunk costs and
head starts, and we test for the resulting sunk cost and discouragement effects
in stage 2 bidding.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [650] [Derivative-Free Sequential Quadratic Programming for Equality-Constrained Stochastic Optimization](https://arxiv.org/abs/2510.22458)
*Sen Na*

Main category: math.OC

TL;DR: 提出无导数随机序列二次规划（DF - SSQP）方法求解含随机目标和确定性等式约束的非线性优化问题，证明其收敛性并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 在仅有零阶信息且目标含随机采样噪声的情况下，解决含随机目标和确定性等式约束的非线性优化问题。

Method: 采用同时扰动随机逼近（SPSA）技术估计梯度和海森矩阵，引入基于动量风格估计器的在线去偏技术降低随机噪声。

Result: 建立了DF - SSQP方法的全局几乎必然收敛性，证明了重缩放迭代的渐近正态性，数值实验验证了其全局和局部性能。

Conclusion: 提出的DF - SSQP方法在零阶信息下可有效求解非线性优化问题，且可进行在线统计推断。

Abstract: We consider solving nonlinear optimization problems with a stochastic
objective and deterministic equality constraints, assuming that only zero-order
information is available for both the objective and constraints, and that the
objective is also subject to random sampling noise. Under this setting, we
propose a Derivative-Free Stochastic Sequential Quadratic Programming (DF-SSQP)
method. Due to the lack of derivative information, we adopt a simultaneous
perturbation stochastic approximation (SPSA) technique to randomly estimate the
gradients and Hessians of both the objective and constraints. This approach
requires only a dimension-independent number of zero-order evaluations -- as
few as eight -- at each iteration step. A key distinction between our
derivative-free and existing derivative-based SSQP methods lies in the
intricate random bias introduced into the gradient and Hessian estimates of the
objective and constraints, brought by stochastic zero-order approximations. To
address this issue, we introduce an online debiasing technique based on
momentum-style estimators that properly aggregate past gradient and Hessian
estimates to reduce stochastic noise, while avoiding excessive memory costs via
a moving averaging scheme. Under standard assumptions, we establish the global
almost-sure convergence of the proposed DF-SSQP method. Notably, we further
complement the global analysis with local convergence guarantees by
demonstrating that the rescaled iterates exhibit asymptotic normality, with a
limiting covariance matrix resembling the minimax optimal covariance achieved
by derivative-based methods, albeit larger due to the absence of derivative
information. Our local analysis enables online statistical inference of model
parameters leveraging DF-SSQP. Numerical experiments on benchmark nonlinear
problems demonstrate both the global and local behavior of DF-SSQP.

</details>


### [651] [Quasi-Self-Concordant Optimization with Lewis Weights](https://arxiv.org/abs/2510.22088)
*Alina Ene,Ta Duy Nguyen,Adrian Vladu*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study the problem $\min_{x\in
\mathbb{R}^{d},Nx=v}\sum_{i=1}^{n}f((Ax-b)_{i})$ for a quasi-self-concordant
function $f:\mathbb{R}\to\mathbb{R}$, where $A,N$ are $n\times d$ and $m\times
d$ matrices, $b,v$ are vectors of length $n$ and $m$ with $n\ge d.$ We show an
algorithm based on a trust-region method with an oracle that can be implemented
using $\widetilde{O}(d^{1/3})$ linear system solves, improving the
$\widetilde{O}(n^{1/3})$ oracle by {[}Adil-Bullins-Sachdeva, NeurIPS 2021{]}.
Our implementation of the oracle relies on solving the overdetermined
$\ell_{\infty}$-regression problem
$\min_{x\in\mathbb{R}^{d},Nx=v}\|Ax-b\|_{\infty}$. We provide an algorithm that
finds a $(1+\epsilon)$-approximate solution to this problem using
$O((d^{1/3}/\epsilon+1/\epsilon^{2})\log(n/\epsilon))$ linear system solves.
This algorithm leverages $\ell_{\infty}$ Lewis weight overestimates and
achieves this iteration complexity via a simple lightweight IRLS approach,
inspired by the work of {[}Ene-Vladu, ICML 2019{]}. Experimentally, we
demonstrate that our algorithm significantly improves the runtime of the
standard CVX solver.

</details>


### [652] [Fixed Horizon Linear Quadratic Covariance Steering in Continuous Time with Hilbert-Schmidt Terminal Cost](https://arxiv.org/abs/2510.21944)
*Tushar Sial,Abhishek Halder*

Main category: math.OC

TL;DR: 本文解决连续时间固定时间范围线性二次协方差转向问题，设计矩阵递归算法并证明收敛性，给出数值示例。


<details>
  <summary>Details</summary>
Motivation: 解决连续时间固定时间范围线性二次协方差转向问题，以希尔伯特 - 施密特范数衡量终端成本。

Method: 设计矩阵递归算法，利用关联哈密顿矩阵状态转移矩阵参数化的线性分式变换。

Result: 算法收敛，通过二维和六维状态空间的数值示例验证结果。

Conclusion: 所设计的矩阵递归算法可有效解决该协方差转向问题。

Abstract: We formulate and solve the fixed horizon linear quadratic covariance steering
problem in continuous time with a terminal cost measured in Hilbert-Schmidt
(i.e., Frobenius) norm error between the desired and the controlled terminal
covariances. For this problem, the necessary conditions of optimality become a
coupled matrix ODE two-point boundary value problem. To solve this system of
equations, we design a matricial recursive algorithm and prove its convergence.
The proposed algorithm and its analysis make use of the linear fractional
transforms parameterized by the state transition matrix of the associated
Hamiltonian matrix. To illustrate the results, we provide two numerical
examples: one with a two dimensional and another with a six dimensional state
space.

</details>


### [653] [Extragradient Method for $(L_0, L_1)$-Lipschitz Root-finding Problems](https://arxiv.org/abs/2510.22421)
*Sayantan Choudhury,Nicolas Loizou*

Main category: math.OC

TL;DR: 文章聚焦更宽松的α - 对称(L₀, L₁) - Lipschitz条件，为额外梯度法（EG）提出新步长策略，分析其收敛率并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 多数研究EG收敛保证的工作假设强L - Lipschitz条件，本文希望在更宽松的α - 对称(L₀, L₁) - Lipschitz条件下研究。

Method: 在α - 对称(L₀, L₁) - Lipschitz条件下，为EG提出新步长策略来解决根查找问题。

Result: 建立了单调算子的次线性收敛率、强单调算子的线性收敛率，证明了弱Minty算子的局部收敛保证，实验验证了理论。

Conclusion: 提出的EG步长策略有效且稳健，能在更宽松条件下解决根查找问题。

Abstract: Introduced by Korpelevich in 1976, the extragradient method (EG) has become a
cornerstone technique for solving min-max optimization, root-finding problems,
and variational inequalities (VIs). Despite its longstanding presence and
significant attention within the optimization community, most works focusing on
understanding its convergence guarantees assume the strong L-Lipschitz
condition. In this work, building on the proposed assumptions by Zhang et al.
[2024b] for minimization and Vankov et al.[2024] for VIs, we focus on the more
relaxed $\alpha$-symmetric $(L_0, L_1)$-Lipschitz condition. This condition
generalizes the standard Lipschitz assumption by allowing the Lipschitz
constant to scale with the operator norm, providing a more refined
characterization of problem structures in modern machine learning. Under the
$\alpha$-symmetric $(L_0, L_1)$-Lipschitz condition, we propose a novel step
size strategy for EG to solve root-finding problems and establish sublinear
convergence rates for monotone operators and linear convergence rates for
strongly monotone operators. Additionally, we prove local convergence
guarantees for weak Minty operators. We supplement our analysis with
experiments validating our theory and demonstrating the effectiveness and
robustness of the proposed step sizes for EG.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [654] [Design Stability in Adaptive Experiments: Implications for Treatment Effect Estimation](https://arxiv.org/abs/2510.22351)
*Saikat Sengupta,Koulik Khamaru,Suvrojit Ghosh,Tirthankar Dasgupta*

Main category: math.ST

TL;DR: 研究顺序自适应治疗分配机制下平均治疗效果（ATE）的估计问题，提出并分析两种ATE估计量，给出中心极限定理和渐近方差表达式，还给出方差估计量及应用示例。


<details>
  <summary>Details</summary>
Motivation: 在顺序自适应治疗分配机制下估计ATE，区别于经典完全随机设计。

Method: 在潜在结果框架下，提出逆倾向加权（IPW）估计量和增强IPW（AIPW）估计量，基于设计稳定性概念进行分析。

Result: 建立了IPW和AIPW估计量在设计稳定性下的中心极限定理，给出渐近方差的显式表达式，提出方差估计量。

Conclusion: 所提方法适用于具有自适应随机化的顺序实验，通过具体设计示例展示了理论结果的适用性。

Abstract: We study the problem of estimating the average treatment effect (ATE) under
sequentially adaptive treatment assignment mechanisms. In contrast to classical
completely randomized designs, we consider a setting in which the probability
of assigning treatment to each experimental unit may depend on prior
assignments and observed outcomes. Within the potential outcomes framework, we
propose and analyze two natural estimators for the ATE: the inverse propensity
weighted (IPW) estimator and an augmented IPW (AIPW) estimator. The cornerstone
of our analysis is the concept of design stability, which requires that as the
number of units grows, either the assignment probabilities converge, or sample
averages of the inverse propensity scores and of the inverse complement
propensity scores converge in probability to fixed, non-random limits. Our main
results establish central limit theorems for both the IPW and AIPW estimators
under design stability and provide explicit expressions for their asymptotic
variances. We further propose estimators for these variances, enabling the
construction of asymptotically valid confidence intervals. Finally, we
illustrate our theoretical results in the context of Wei's adaptive coin design
and Efron's biased coin design, highlighting the applicability of the proposed
methods to sequential experimentation with adaptive randomization.

</details>


### [655] [Confidence Sets for Multidimensional Scaling](https://arxiv.org/abs/2510.22452)
*Siddharth Vishwanath,Ery Arias-Castro*

Main category: math.ST

TL;DR: 本文为应用于含噪相异度数据的经典多维尺度分析（CMDS）开发统计框架，建立收敛结果，提出构建置信集的自助法并验证有效性，通过实验展示性能。


<details>
  <summary>Details</summary>
Motivation: 为含噪相异度数据的CMDS构建正式统计框架，构建潜在配置的置信集。

Method: 建立CMDS嵌入的分布收敛结果，提出乘数自助法和经验自助法构建置信集。

Result: 乘数自助法适应异方差噪声，经验自助法需同方差性，有效时可提高有限样本准确性。

Conclusion: 所提方法在数值实验中表现良好，有一定有效性和实用性。

Abstract: We develop a formal statistical framework for classical multidimensional
scaling (CMDS) applied to noisy dissimilarity data. We establish distributional
convergence results for the embeddings produced by CMDS for various noise
models, which enable the construction of \emph{bona~fide} uniform confidence
sets for the latent configuration, up to rigid transformations. We further
propose bootstrap procedures for constructing these confidence sets and provide
theoretical guarantees for their validity. We find that the multiplier
bootstrap adapts automatically to heteroscedastic noise such as multiplicative
noise, while the empirical bootstrap seems to require homoscedasticity. Either
form of bootstrap, when valid, is shown to substantially improve finite-sample
accuracy. The empirical performance of the proposed methods is demonstrated
through numerical experiments.

</details>


### [656] [Model-free filtering in high dimensions via projection and score-based diffusions](https://arxiv.org/abs/2510.23197)
*Sören Christensen,Jan Kallsen,Claudia Strauch,Lukas Trottner*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of recovering a latent signal $X$ from its noisy
observation $Y$. The unknown law $\mathbb{P}^X$ of $X$, and in particular its
support $\mathscr{M}$, are accessible only through a large sample of i.i.d.\
observations. We further assume $\mathscr{M}$ to be a low-dimensional
submanifold of a high-dimensional Euclidean space $\mathbb{R}^d$. As a filter
or denoiser $\widehat X$, we suggest an estimator of the metric projection
$\pi_{\mathscr{M}}(Y)$ of $Y$ onto the manifold $\mathscr{M}$. To compute this
estimator, we study an auxiliary semiparametric model in which $Y$ is obtained
by adding isotropic Laplace noise to $X$. Using score matching within a
corresponding diffusion model, we obtain an estimator of the Bayesian posterior
$\mathbb{P}^{X \mid Y}$ in this setup. Our main theoretical results show that,
in the limit of high dimension $d$, this posterior $\mathbb{P}^{X\mid Y}$ is
concentrated near the desired metric projection $\pi_{\mathscr{M}}(Y)$.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [657] [The Cost of Certainty: Shot Budgets in Quantum Program Testing](https://arxiv.org/abs/2510.22418)
*Andriy Miranskyy*

Main category: quant-ph

TL;DR: 本文提出量子程序验证测量次数推理框架，分析不同测试方法，给出实际指导。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算发展，量子程序测试验证紧迫且昂贵，需合理规划测量次数。

Method: 分析误差概率、保真度等关系建立测量次数限制，应用于三种测试方法，考虑理想和噪声设备，引入程序级预算方法。

Result: 逆测试测量效率最高，交换测试约需两倍测量，卡方测试易实现但需更多测量；噪声会增加测量需求；程序级细粒度分解成本增长快，粗粒度或加权分配更实用。

Conclusion: 框架明确不同策略权衡，为量子程序测试测量预算提供实用指导。

Abstract: As quantum computing advances toward early fault-tolerant machines, testing
and verification of quantum programs become urgent but costly, since each
execution consumes scarce hardware resources. Unlike in classical software
testing, every measurement must be carefully budgeted.
  This paper develops a unified framework for reasoning about how many
measurements are required to verify quantum programs. The goal is to connect
theoretical error bounds with concrete test strategies and to extend the
analysis from individual tests to full program-level verification.
  We analyze the relationship between error probability, fidelity, trace
distance, and the quantum Chernoff bound to establish fundamental shot count
limits. These foundations are applied to three representative testing methods:
the inverse test, the swap test, and the chi-square test. Both idealized and
noisy devices are considered. We also introduce a program-level budgeting
approach that allocates verification effort across multiple subroutines.
  The inverse test is the most measurement efficient, the swap test requires
about twice as many shots, and the chi-square test is easiest to implement but
often needs orders of magnitude more measurements. In the presence of noise,
calibrated baselines may increase measurement requirements beyond theoretical
estimates. At the program level, distributing a global fidelity target across
many fine-grained functions can cause verification costs to grow rapidly,
whereas coarser decompositions or weighted allocations remain more practical.
  The framework clarifies trade-offs among different testing strategies, noise
handling, and program decomposition. It provides practical guidance for
budgeting measurement shots in quantum program testing, helping practitioners
balance rigour against cost when designing verification strategies.

</details>


### [658] [An Analytic Theory of Quantum Imaginary Time Evolution](https://arxiv.org/abs/2510.22481)
*Min Chen,Bingzhi Zhang,Quntao Zhuang,Junyu Liu*

Main category: quant-ph

TL;DR: 本文旨在为量子虚时间演化（QITE）算法动力学发展解析理论，证明其与量子自然梯度下降的等价性，构建解析模型，验证结果并奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有对QITE的第一性原理理论理解有限，需发展解析理论。

Method: 先证明QITE与量子自然梯度下降的等价性，再用量子神经切线核框架为宽量子神经网络构建解析模型，最后用数值模拟验证。

Result: QITE收敛比基于GD的VQA快，但优势受希尔伯特空间维度指数增长抑制，理论涵盖多种损失函数。

Conclusion: 为QITE动力学奠定理论基础，为变分量子算法的第一性原理设计提供解析见解。

Abstract: Quantum imaginary time evolution (QITE) algorithm is one of the most
promising variational quantum algorithms (VQAs), bridging the current era of
Noisy Intermediate-Scale Quantum devices and the future of fully fault-tolerant
quantum computing. Although practical demonstrations of QITE and its potential
advantages over the general VQA trained with vanilla gradient descent (GD) in
certain tasks have been reported, a first-principle, theoretical understanding
of QITE remains limited. Here, we aim to develop an analytic theory for the
dynamics of QITE. First, we show that QITE can be interpreted as a form of a
general VQA trained with Quantum Natural Gradient Descent (QNGD), where the
inverse quantum Fisher information matrix serves as the learning-rate tensor.
This equivalence is established not only at the level of gradient update rules,
but also through the action principle: the variational principle can be
directly connected to the geometric geodesic distance in the quantum Fisher
information metric, up to an integration constant. Second, for wide quantum
neural networks, we employ the quantum neural tangent kernel framework to
construct an analytic model for QITE. We prove that QITE always converges
faster than GD-based VQA, though this advantage is suppressed by the
exponential growth of Hilbert space dimension. This helps explain certain
experimental results in quantum computational chemistry. Our theory encompasses
linear, quadratic, and more general loss functions. We validate the analytic
results through numerical simulations. Our findings establish a theoretical
foundation for QITE dynamics and provide analytic insights for the
first-principle design of variational quantum algorithms.

</details>


### [659] [HPC-Driven Modeling with ML-Based Surrogates for Magnon-Photon Dynamics in Hybrid Quantum Systems](https://arxiv.org/abs/2510.22221)
*Jialin Song,Yingheng Tang,Pu Ren,Shintaro Takayoshi,Saurabh Sawant,Yujie Zhu,Jia-Mian Hu,Andy Nonaka,Michael W. Mahoney,Benjamin Erichson,Zhi,Yao*

Main category: quant-ph

TL;DR: 提出基于GPU的并行仿真框架及物理信息机器学习代理，实现片上磁振子 - 光子电路大规模建模，可用于下一代量子和自旋电子器件仿真和原型设计。


<details>
  <summary>Details</summary>
Motivation: 解决混合磁振子量子系统因两系统时间尺度差异大而难以仿真的问题。

Method: 采用基于GPU的大规模并行仿真框架，结合物理信息机器学习代理。

Result: 揭示实时能量交换动态，重现反交叉行为和强电磁场下铁磁共振抑制等关键现象。

Conclusion: 该框架解决了磁振子 - 光子建模中的多尺度和多物理挑战，可实现可扩展仿真和快速原型设计。

Abstract: Simulating hybrid magnonic quantum systems remains a challenge due to the
large disparity between the timescales of the two systems. We present a
massively parallel GPU-based simulation framework that enables fully coupled,
large-scale modeling of on-chip magnon-photon circuits. Our approach resolves
the dynamic interaction between ferromagnetic and electromagnetic fields with
high spatiotemporal fidelity. To accelerate design workflows, we develop a
physics-informed machine learning surrogate trained on the simulation data,
reducing computational cost while maintaining accuracy. This combined approach
reveals real-time energy exchange dynamics and reproduces key phenomena such as
anti-crossing behavior and the suppression of ferromagnetic resonance under
strong electromagnetic fields. By addressing the multiscale and multiphysics
challenges in magnon-photon modeling, our framework enables scalable simulation
and rapid prototyping of next-generation quantum and spintronic devices.

</details>


### [660] [qc-kmeans: A Quantum Compressive K-Means Algorithm for NISQ Devices](https://arxiv.org/abs/2510.22540)
*Pedro Chumpitaz-Flores,My Duong,Ying Mao,Kaixun Hua*

Main category: quant-ph

TL;DR: 提出qc - kmeans方法解决NISQ硬件聚类问题，模拟中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决NISQ硬件上聚类受数据加载和有限量子比特限制的问题。

Method: 提出混合压缩k - means方法qc - kmeans，用固定大小傅里叶特征草图总结数据集，通过浅QAOA电路解决小的每组QUBO来选择质心，有细化步骤确保代价值不增加。

Result: 在Qiskit Aer模拟中低维合成基准用≤9个量子比特运行，与量子基线相比平方误差有竞争力；在九个真实数据集模拟中保持恒定峰值量子比特使用；在IBM噪声模型下精度与理想情况相似。

Conclusion: qc - kmeans提供了面向NISQ的公式，有浅且宽度有界的电路，在模拟中聚类质量有竞争力。

Abstract: Clustering on NISQ hardware is constrained by data loading and limited
qubits. We present \textbf{qc-kmeans}, a hybrid compressive $k$-means that
summarizes a dataset with a constant-size Fourier-feature sketch and selects
centroids by solving small per-group QUBOs with shallow QAOA circuits. The QFF
sketch estimator is unbiased with mean-squared error $O(\varepsilon^2)$ for
$B,S=\Theta(\varepsilon^{-2})$, and the peak-qubit requirement
$q_{\text{peak}}=\max\{D,\lceil \log_2 B\rceil + 1\}$ does not scale with the
number of samples. A refinement step with elitist retention ensures
non-increasing surrogate cost. In Qiskit Aer simulations (depth $p{=}1$), the
method ran with $\le 9$ qubits on low-dimensional synthetic benchmarks and
achieved competitive sum-of-squared errors relative to quantum baselines;
runtimes are not directly comparable. On nine real datasets (up to $4.3\times
10^5$ points), the pipeline maintained constant peak-qubit usage in simulation.
Under IBM noise models, accuracy was similar to the idealized setting. Overall,
qc-kmeans offers a NISQ-oriented formulation with shallow, bounded-width
circuits and competitive clustering quality in simulation.

</details>


### [661] [Scalable Neural Decoders for Practical Real-Time Quantum Error Correction](https://arxiv.org/abs/2510.22724)
*Changwon Lee,Tak Hur,Daniel K. Park*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Real-time, scalable, and accurate decoding is a critical component for
realizing a fault-tolerant quantum computer. While Transformer-based neural
decoders such as \textit{AlphaQubit} have demonstrated high accuracy, the
computational complexity of their core attention mechanism, which scales as
$\mathcal{O}(d^4)$ with code distance $d$, results in decoding speeds
insufficient for practical real-time applications. In this work, we introduce
and evaluate a \textit{Mamba}-based decoder, a state-space model with
$\mathcal{O}(d^2)$ complexity. In memory experiments using Sycamore hardware
data, our Mamba decoder matches the performance of its Transformer-based
counterpart, providing that its superior efficiency does not come at the cost
of performance. Crucially, in simulated real-time scenarios that account for
decoder-induced noise, the Mamba decoder significantly outperforms the
Transformer, exhibiting a higher error threshold of $0.0104$ compared to
$0.0097$. These results demonstrate that Mamba decoders offer a compelling
balance between speed and accuracy, making them a promising architecture for
scalable, real-time quantum error correction.

</details>


### [662] [Benchmarking VQE Configurations: Architectures, Initializations, and Optimizers for Silicon Ground State Energy](https://arxiv.org/abs/2510.23171)
*Zakaria Boutakka,Nouhaila Innan,Muhammed Shafique,Mohamed Bennai,Z. Sakhi*

Main category: quant-ph

TL;DR: 本文研究VQE在估算硅原子基态能量中的性能，系统探索配置选择对VQE性能的影响并建立基准，发现参数初始化和特定组合对算法有重要影响。


<details>
  <summary>Details</summary>
Motivation: 量子计算为量子化学模拟提供新途径，研究VQE在估算硅原子基态能量的性能，因硅原子计算复杂度高。

Method: 在混合量子 - 经典优化框架下，使用多种ansatz（如Double Excitation Gates等）和优化器（如梯度下降等）实现VQE。

Result: 参数初始化对算法稳定性起决定性作用，化学启发的ansatz与自适应优化的组合比传统方法有更好的收敛性和精度。

Conclusion: 通过系统探索配置选择对VQE性能的影响，建立了量子化学模拟中选择最优设置的结构化基准。

Abstract: Quantum computing presents a promising path toward precise quantum chemical
simulations, particularly for systems that challenge classical methods. This
work investigates the performance of the Variational Quantum Eigensolver (VQE)
in estimating the ground-state energy of the silicon atom, a relatively heavy
element that poses significant computational complexity. Within a hybrid
quantum-classical optimization framework, we implement VQE using a range of
ansatz, including Double Excitation Gates, ParticleConservingU2, UCCSD, and
k-UpCCGSD, combined with various optimizers such as gradient descent, SPSA, and
ADAM. The main contribution of this work lies in a systematic methodological
exploration of how these configuration choices interact to influence VQE
performance, establishing a structured benchmark for selecting optimal settings
in quantum chemical simulations. Key findings show that parameter
initialization plays a decisive role in the algorithm's stability, and that the
combination of a chemically inspired ansatz with adaptive optimization yields
superior convergence and precision compared to conventional approaches.

</details>


### [663] [Quantum Phase Classification of Rydberg Atom Systems Using Resource-Efficient Variational Quantum Circuits and Classical Shadows](https://arxiv.org/abs/2510.23489)
*Hemish Ahuja,Samradh Bhardwaj,Kirti Dhir,Roman Bagdasarian,Ziwoong Jang*

Main category: quant-ph

TL;DR: 提出结合经典阴影断层扫描和变分量子电路的量子机器学习方法，实现对Z2和Z3有序相的二元相分类，用少量量子资源达到高精度。


<details>
  <summary>Details</summary>
Motivation: 量子相变研究有机会但区分不同有序相且无明确序参量有挑战。

Method: 结合经典阴影断层扫描与变分量子电路，处理500次随机测量，进行PCA降维，角度嵌入到2量子比特电路，用SPSA训练。

Result: 模型测试准确率达100%，精度、召回率和F1分数完美。

Conclusion: 少量量子资源足以实现高精度相分类，为近期限量子设备上的量子增强凝聚态物理研究开辟道路。

Abstract: Quantum phase transitions in Rydberg atom arrays present significant
opportunities for studying many-body physics, yet distinguishing between
different ordered phases without explicit order parameters remains challenging.
We present a resource-efficient quantum machine learning approach combining
classical shadow tomography with variational quantum circuits (VQCs) for binary
phase classification of Z2 and Z3 ordered phases. Our pipeline processes 500
randomized measurements per 51-atom chain state, reconstructs shadow operators,
performs PCA dimensionality reduction (514 features), and encodes features
using angle embedding onto a 2-qubit parameterized circuit. The circuit employs
RY-RZ angle encoding, strong entanglement via all-to-all CZ gates, and a
minimal 2-parameter ansatz achieving depth 7. Training via simultaneous
perturbation stochastic approximation (SPSA) with hinge loss converged in 120
iterations. The model achieved 100% test accuracy with perfect precision,
recall, and F1 scores, demonstrating that minimal quantum resources suffice for
high-accuracy phase classification. This work establishes pathways for
quantum-enhanced condensed matter physics on near-term quantum devices.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [664] [HDR Image Reconstruction using an Unsupervised Fusion Model](https://arxiv.org/abs/2510.21815)
*Kumbha Nagaswetha*

Main category: eess.IV

TL;DR: 提出基于深度学习的多曝光融合方法生成HDR图像，无监督训练，效果优于现有方法，引入定制损失函数优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统数码相机动态范围有限，无法捕捉自然场景中宽亮度范围，需解决此局限。

Method: 采用卷积神经网络融合不同曝光的低动态范围图像的互补信息，无监督训练，引入定制损失函数。

Result: 使用MEF - SSIM评估，生成的HDR图像视觉质量优于现有融合方法。

Conclusion: 所提方法实用有效，能在无真实HDR图像数据的现实场景应用，且定制损失函数可优化性能。

Abstract: High Dynamic Range (HDR) imaging aims to reproduce the wide range of
brightness levels present in natural scenes, which the human visual system can
perceive but conventional digital cameras often fail to capture due to their
limited dynamic range. To address this limitation, we propose a deep
learning-based multi-exposure fusion approach for HDR image generation. The
method takes a set of differently exposed Low Dynamic Range (LDR) images,
typically an underexposed and an overexposed image, and learns to fuse their
complementary information using a convolutional neural network (CNN). The
underexposed image preserves details in bright regions, while the overexposed
image retains information in dark regions; the network effectively combines
these to reconstruct a high-quality HDR output. The model is trained in an
unsupervised manner, without relying on ground-truth HDR images, making it
practical for real-world applications where such data is unavailable. We
evaluate our results using the Multi-Exposure Fusion Structural Similarity
Index Measure (MEF-SSIM) and demonstrate that our approach achieves superior
visual quality compared to existing fusion methods. A customized loss function
is further introduced to improve reconstruction fidelity and optimize model
performance.

</details>


### [665] [TraceTrans: Translation and Spatial Tracing for Surgical Prediction](https://arxiv.org/abs/2510.22379)
*Xiyu Luo,Haodong LI,Xinxing Cheng,He Zhao,Yang Hu,Xuan Song,Tianyang Zhang*

Main category: eess.IV

TL;DR: 提出TraceTrans模型用于术后预测，实验证明其能给出准确且可解释的结果，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有图像到图像翻译模型多忽略源图像和翻译后图像的空间对应关系，导致结构不一致和幻觉问题，在临床应用中对解剖准确性要求高，需解决这些问题。

Method: 提出TraceTrans模型，采用编码器进行特征提取，双解码器分别预测空间变形和合成翻译图像，用预测的变形场对生成输出施加空间约束。

Result: 在医学美容和脑MRI数据集上的大量实验表明，TraceTrans能提供准确且可解释的术后预测。

Conclusion: TraceTrans有可靠的临床部署潜力。

Abstract: Image-to-image translation models have achieved notable success in converting
images across visual domains and are increasingly used for medical tasks such
as predicting post-operative outcomes and modeling disease progression.
However, most existing methods primarily aim to match the target distribution
and often neglect spatial correspondences between the source and translated
images. This limitation can lead to structural inconsistencies and
hallucinations, undermining the reliability and interpretability of the
predictions. These challenges are accentuated in clinical applications by the
stringent requirement for anatomical accuracy. In this work, we present
TraceTrans, a novel deformable image translation model designed for
post-operative prediction that generates images aligned with the target
distribution while explicitly revealing spatial correspondences with the
pre-operative input. The framework employs an encoder for feature extraction
and dual decoders for predicting spatial deformations and synthesizing the
translated image. The predicted deformation field imposes spatial constraints
on the generated output, ensuring anatomical consistency with the source.
Extensive experiments on medical cosmetology and brain MRI datasets demonstrate
that TraceTrans delivers accurate and interpretable post-operative predictions,
highlighting its potential for reliable clinical deployment.

</details>


### [666] [Frequency-Spatial Interaction Driven Network for Low-Light Image Enhancement](https://arxiv.org/abs/2510.22154)
*Yunhong Tao,Wenbing Tao,Xiang Xiang*

Main category: eess.IV

TL;DR: 本文提出用于低光图像增强的频率 - 空间交互驱动网络FSIDNet，在多数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法忽视频域信息或未能有效促进信息传播，限制了增强性能。

Method: 采用两阶段架构，第一阶段恢复低光图像幅度，第二阶段恢复相位信息；开发两个频率 - 空间交互块融合互补信息；构建信息交换模块关联两阶段。

Result: 在多个基准数据集上实验，在视觉效果和定量指标上表现优异，且模型效率良好。

Conclusion: 所提出的FSIDNet方法能有效提升低光图像增强性能。

Abstract: Low-light image enhancement (LLIE) aims at improving the perception or
interpretability of an image captured in an environment with poor illumination.
With the advent of deep learning, the LLIE technique has achieved significant
breakthroughs. However, existing LLIE methods either ignore the important role
of frequency domain information or fail to effectively promote the propagation
and flow of information, limiting the LLIE performance. In this paper, we
develop a novel frequency-spatial interaction-driven network (FSIDNet) for LLIE
based on two-stage architecture. To be specific, the first stage is designed to
restore the amplitude of low-light images to improve the lightness, and the
second stage devotes to restore phase information to refine fine-grained
structures. Considering that Frequency domain and spatial domain information
are complementary and both favorable for LLIE, we further develop two
frequency-spatial interaction blocks which mutually amalgamate the
complementary spatial and frequency information to enhance the capability of
the model. In addition, we construct the Information Exchange Module (IEM) to
associate two stages by adequately incorporating cross-stage and cross-scale
features to effectively promote the propagation and flow of information in the
two-stage network structure. Finally, we conduct experiments on several widely
used benchmark datasets (i.e., LOL-Real, LSRW-Huawei, etc.), which demonstrate
that our method achieves the excellent performance in terms of visual results
and quantitative metrics while preserving good model efficiency.

</details>


### [667] [Expert Validation of Synthetic Cervical Spine Radiographs Generated with a Denoising Diffusion Probabilistic Model](https://arxiv.org/abs/2510.22166)
*Austin A. Barr,Brij S. Karmur,Anthony J. Winder,Eddie Guo,John T. Lysack,James N. Scott,William F. Morrish,Muneer Eesa,Morgan Willson,David W. Cadotte,Michael M. H. Yang,Ian Y. M. Chan,Sanju Lama,Garnette R. Sutherland*

Main category: eess.IV

TL;DR: 评估用DDPM生成颈椎侧位X光片可行性，结果显示生成图像与真实图像在真实度和质量上无统计差异，还提供合成数据集。


<details>
  <summary>Details</summary>
Motivation: 神经外科机器学习受高质量影像数据集限制，合成数据是可扩展且保护隐私的解决方案。

Method: 用4963张颈椎X光图像训练DDPM，通过训练/验证损失和Frechet起始距离监测模型性能，用“临床图灵测试”评估合成图像质量。

Result: 专家在29%的试验中正确识别真实图像，真实图像和合成图像平均真实度得分相当，近邻分析无记忆证据，提供20063张合成X光片数据集。

Conclusion: DDPM生成的颈椎X光片与真实临床图像在真实度和质量上统计不可区分，为神经影像数据集创建提供新方法。

Abstract: Machine learning in neurosurgery is limited by challenges in assembling
large, high-quality imaging datasets. Synthetic data offers a scalable,
privacy-preserving solution. We evaluated the feasibility of generating
realistic lateral cervical spine radiographs using a denoising diffusion
probabilistic model (DDPM) trained on 4,963 images from the Cervical Spine
X-ray Atlas. Model performance was monitored via training/validation loss and
Frechet inception distance, and synthetic image quality was assessed in a
blinded "clinical Turing test" with six neuroradiologists and two
spine-fellowship trained neurosurgeons. Experts reviewed 50 quartets containing
one real and three synthetic images, identifying the real image and rating
realism on a 4-point Likert scale. Experts correctly identified the real image
in 29% of trials (Fleiss' kappa=0.061). Mean realism scores were comparable
between real (3.323) and synthetic images (3.228, 3.258, and 3.320; p=0.383,
0.471, 1.000). Nearest-neighbor analysis found no evidence of memorization. We
also provide a dataset of 20,063 synthetic radiographs. These results
demonstrate that DDPM-generated cervical spine X-rays are statistically
indistinguishable in realism and quality from real clinical images, offering a
novel approach to creating large-scale neuroimaging datasets for ML
applications in landmarking, segmentation, and classification.

</details>


### [668] [Synthetic-to-Real Transfer Learning for Chromatin-Sensitive PWS Microscopy](https://arxiv.org/abs/2510.22239)
*Jahidul Arafat,Sanjaya Poudel*

Main category: eess.IV

TL;DR: 提出CFU Net用于csPWS显微镜图像的细胞核分割，在合成数据上表现优异，可提取染色质生物标志物区分正常和癌前组织。


<details>
  <summary>Details</summary>
Motivation: 手动核分割限制早期癌症检测生物标志物发现的群体规模分析，缺乏标注数据阻碍标准深度学习方法应用。

Method: 提出CFU Net分层分割架构，用三阶段课程在合成多模态数据上训练，集成五种架构元素，结合基于物理的渲染和课程学习。

Result: CFU Net在合成测试数据上表现近乎完美，INT8量化压缩率74.9%，推理时间0.15秒，吞吐量比手动分析高240倍，提取生物标志物分类准确率达94%。

Conclusion: 为专业显微镜的合成到真实迁移学习提供通用框架，为临床标本验证提供开放资源。

Abstract: Chromatin sensitive partial wave spectroscopic (csPWS) microscopy enables
label free detection of nanoscale chromatin packing alterations that occur
before visible cellular transformation. However, manual nuclear segmentation
limits population scale analysis needed for biomarker discovery in early cancer
detection. The lack of annotated csPWS imaging data prevents direct use of
standard deep learning methods. We present CFU Net, a hierarchical segmentation
architecture trained with a three stage curriculum on synthetic multimodal
data. CFU Net achieves near perfect performance on held out synthetic test data
that represent diverse spectroscopic imaging conditions without manual
annotations (Dice 0.9879, IoU 0.9895). Our approach uses physics based
rendering that incorporates empirically supported chromatin packing statistics,
Mie scattering models, and modality specific noise, combined with a curriculum
that progresses from adversarial RGB pretraining to spectroscopic fine tuning
and histology validation. CFU Net integrates five architectural elements
(ConvNeXt backbone, Feature Pyramid Network, UNet plus plus dense connections,
dual attention, and deep supervision) that together improve Dice over a
baseline UNet by 8.3 percent. We demonstrate deployment ready INT8 quantization
with 74.9 percent compression and 0.15 second inference, giving a 240 times
throughput gain over manual analysis. Applied to more than ten thousand
automatically segmented nuclei from synthetic test data, the pipeline extracts
chromatin biomarkers that distinguish normal from pre cancerous tissue with
large effect sizes (Cohens d between 1.31 and 2.98), reaching 94 percent
classification accuracy. This work provides a general framework for synthetic
to real transfer learning in specialized microscopy and open resources for
community validation on clinical specimens.

</details>


### [669] [USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding](https://arxiv.org/abs/2510.22990)
*Youssef Megahed,Robin Ducharme,Mark Walker,Steven Hawken,Adrian D. C. Chan*

Main category: eess.IV

TL;DR: 提出针对超声图像的自监督基础模型USF - MAE，在三个下游分类基准测试中表现出色，有强跨解剖区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 超声图像解读有噪声高、依赖操作者等挑战，现有深度学习方法受限于缺乏大的标注数据集和领域差距。

Method: 引入USF - MAE，在OpenUS - 46数据集上预训练，使用Vision Transformer架构重建掩码图像块，在三个下游分类基准上微调。

Result: 在所有任务中，USF - MAE始终优于传统CNN和ViT基线，F1分数分别达81.6%、79.6%和82.4%，接近或超越有监督模型UltraSam。

Conclusion: USF - MAE能直接从无标签数据学习特定模态表示，有强跨解剖区域泛化能力。

Abstract: Ultrasound imaging is one of the most widely used diagnostic modalities,
offering real-time, radiation-free assessment across diverse clinical domains.
However, interpretation of ultrasound images remains challenging due to high
noise levels, operator dependence, and limited field of view, resulting in
substantial inter-observer variability. Current Deep Learning approaches are
hindered by the scarcity of large labeled datasets and the domain gap between
general and sonographic images, which limits the transferability of models
pretrained on non-medical data. To address these challenges, we introduce the
Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE),
the first large-scale self-supervised MAE framework pretrained exclusively on
ultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound
images curated from 46 open-source datasets, collectively termed OpenUS-46,
spanning over twenty anatomical regions. This curated dataset has been made
publicly available to facilitate further research and reproducibility. Using a
Vision Transformer encoder-decoder architecture, USF-MAE reconstructs masked
image patches, enabling it to learn rich, modality-specific representations
directly from unlabeled data. The pretrained encoder was fine-tuned on three
public downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D
(ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all
tasks, USF-MAE consistently outperformed conventional CNN and ViT baselines,
achieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using
labels during pretraining, USF-MAE approached the performance of the supervised
foundation model UltraSam on breast cancer classification and surpassed it on
the other tasks, demonstrating strong cross-anatomical generalization.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [670] [The First Star-by-star $N$-body/Hydrodynamics Simulation of Our Galaxy Coupling with a Surrogate Model](https://arxiv.org/abs/2510.23330)
*Keiya Hirashima,Michiko S. Fujii,Takayuki R. Saitoh,Naoto Harada,Kentaro Nomura,Kohji Yoshikawa,Yutaka Hirai,Tetsuro Asano,Kana Moriwaki,Masaki Iwasawa,Takashi Okamoto,Junichiro Makino*

Main category: astro-ph.GA

TL;DR: 本文开发结合机器学习的N体/流体动力学模拟新积分方案，突破十亿粒子障碍，实现银河系逐星模拟。


<details>
  <summary>Details</summary>
Motivation: 计算天体物理学目标是高分辨率模拟银河系至单颗恒星，但因超新星爆炸等现象导致扩展性失败。

Method: 开发结合机器学习的N体/流体动力学模拟新积分方案，用替代模型绕过超新星爆炸短时间步长。

Result: 使用148,900个节点（相当于7,147,200个CPU核心）达到3000亿粒子，突破现有十亿粒子障碍，实现银河系逐星模拟，性能在超$10^4$个CPU核心上可扩展。

Conclusion: 新方案有效提升模拟扩展性，能实现高分辨率银河系逐星模拟。

Abstract: A major goal of computational astrophysics is to simulate the Milky Way
Galaxy with sufficient resolution down to individual stars. However, the
scaling fails due to some small-scale, short-timescale phenomena, such as
supernova explosions. We have developed a novel integration scheme of
$N$-body/hydrodynamics simulations working with machine learning. This approach
bypasses the short timesteps caused by supernova explosions using a surrogate
model, thereby improving scalability. With this method, we reached 300 billion
particles using 148,900 nodes, equivalent to 7,147,200 CPU cores, breaking
through the billion-particle barrier currently faced by state-of-the-art
simulations. This resolution allows us to perform the first star-by-star galaxy
simulation, which resolves individual stars in the Milky Way Galaxy. The
performance scales over $10^4$ CPU cores, an upper limit in the current
state-of-the-art simulations using both A64FX and X86-64 processors and NVIDIA
CUDA GPUs.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [671] [Beliefs about Bots: How Employers Plan for AI in White-Collar Work](https://arxiv.org/abs/2510.21959)
*Eduard Brüll,Samuel Mäurer,Davud Rostam-Afschar*

Main category: econ.GN

TL;DR: 研究雇主如何调整对高技能白领工作自动化风险的预期，发现企业低估自动化程度，信息干预有影响。


<details>
  <summary>Details</summary>
Motivation: 探究雇主对高技能白领工作自动化风险的预期调整情况。

Method: 在德国税务顾问中进行随机信息干预实验。

Result: 企业系统性低估自动化程度，信息提供提高风险认知，不改变短期招聘计划，更新信念提高生产力和财务预期，有轻微工资调整，雇主预期新任务，有更高培训和采用意愿。

Conclusion: 信息干预影响雇主对自动化风险的认知和预期，存在企业内部不平等和有限租金分享情况。

Abstract: We provide experimental evidence on how employers adjust expectations to
automation risk in high-skill, white-collar work. Using a randomized
information intervention among tax advisors in Germany, we show that firms
systematically underestimate automatability. Information provision raises risk
perceptions, especially for routine-intensive roles. Yet, it leaves short-run
hiring plans unchanged. Instead, updated beliefs increase productivity and
financial expectations with minor wage adjustments, implying within-firm
inequality like limited rent-sharing. Employers also anticipate new tasks in
legal tech, compliance, and AI interaction, and report higher training and
adoption intentions.

</details>


### [672] [There's Nothing in the Air](https://arxiv.org/abs/2510.22294)
*Jacob Adenbaum,Fil Babalievsky,William Jungerman*

Main category: econ.GN

TL;DR: 使用法国行政数据分解城市工资增长溢价，发现该溢价很大程度与城市本身无关，主要源于企业和同事构成及工作流动性。


<details>
  <summary>Details</summary>
Motivation: 探究大城市工资增长更快的原因。

Method: 使用法国行政数据，控制企业和同事构成因素，对比不同情况（有工作转换和无工作转换）下的城市工资增长溢价。

Result: 控制企业和同事构成后，80%的溢价消失；聚焦同一工作的员工，94.1%的溢价消失，剩余效应与零无统计差异。

Conclusion: 挑战城市产生人力资本溢出的观点，认为城市工资动态反映了企业和工人的排序以及工作流动性。

Abstract: Why do wages grow faster in bigger cities? We use French administrative data
to decompose the urban wage growth premium and find that the answer has
surprisingly little to do with cities themselves. While we document
substantially faster wage growth in larger cities, 80% of the premium
disappears after controlling for the composition of firms and coworkers. We
also document significantly higher job-to-job transition rates in larger
cities, suggesting workers climb the job ladder faster. Most strikingly, when
we focus on workers who remain in the same job -- eliminating the job ladder
mechanism -- the urban wage growth premium falls by 94.1% after accounting for
firms and coworkers. The residual effect is statistically indistinguishable
from zero. These results challenge the view that cities generate human capital
spillovers ``in the air,'' suggesting instead that urban wage dynamics reflect
the sorting of firms and workers and the pace of job mobility.

</details>


### [673] [Wildfire and house prices: A synthetic control case study of Altadena (Jan 2025)](https://arxiv.org/abs/2510.22817)
*Yibo Sun*

Main category: econ.GN

TL;DR: 研究用合成控制法评估2025年1月野火对加州阿尔塔迪纳房价的因果影响，发现显著负价格效应，凸显火灾易发区财务风险及方法有效性。


<details>
  <summary>Details</summary>
Motivation: 评估2025年1月野火对加州阿尔塔迪纳房价的因果影响。

Method: 使用合成控制法，从同行城市加权平均构建“合成”阿尔塔迪纳作为反事实，假设对捐赠池无溢出效应。

Result: 野火对房价有显著负面效应且随时间加剧，事件后六个月每月平均损失32125美元；按稳健的后处理与预处理均方根预测误差比，结果在10%水平显著（p = 0.0508），按平均后处理差距衡量则不显著（p = 0.3220）。

Conclusion: 火灾易发区社区面临重大财务风险，合成控制法在评估灾害相关经济损失方面有效。

Abstract: This study uses the Synthetic Control Method (SCM) to estimate the causal
impact of a January 2025 wildfire on housing prices in Altadena, California. We
construct a 'synthetic' Altadena from a weighted average of peer cities to
serve as a counterfactual; this approach assumes no spillover effects on the
donor pool. The results reveal a substantial negative price effect that
intensifies over time. Over the six months following the event, we estimate an
average monthly loss of $32,125. The statistical evidence for this effect is
nuanced. Based on the robust post-to-pre-treatment RMSPE ratio, the result is
statistically significant at the 10% level (p = 0.0508). In contrast, the
effect is not statistically significant when measured by the average
post-treatment gap (p = 0.3220). This analysis highlights the significant
financial risks faced by communities in fire-prone regions and demonstrates
SCM's effectiveness in evaluating disaster-related economic damages.

</details>


### [674] [Exploring Vulnerability in AI Industry](https://arxiv.org/abs/2510.23421)
*Claudio Pirrone,Stefano Fricano,Gioacchino Fazio*

Main category: econ.GN

TL;DR: 提出合成AI脆弱性指数AIVI评估基础模型产业脆弱性，量化AI核心生产引擎系统风险。


<details>
  <summary>Details</summary>
Motivation: 基础模型产业发展迅速，但因数据限制评估其脆弱性具挑战性，需有效评估方法。

Method: 聚焦基础模型生产上游价值链，以公开数据为优先，将基础模型输出建模为计算、数据、人才、资本和能源五个输入的函数，提出加权几何平均的综合子指数。

Result: 识别出计算集中、数据稀缺和法律风险、人才瓶颈、资本密集和战略依赖、能源需求上升等关键脆弱性。

Conclusion: 初步指数虽有局限，但可量化AI核心生产引擎系统风险，为下游价值链风险提供参考。

Abstract: The rapid ascent of Foundation Models (FMs), enabled by the Transformer
architecture, drives the current AI ecosystem. Characterized by large-scale
training and downstream adaptability, FMs (as GPT family) have achieved massive
public adoption, fueling a turbulent market shaped by platform economics and
intense investment. Assessing the vulnerability of this fast-evolving industry
is critical yet challenging due to data limitations. This paper proposes a
synthetic AI Vulnerability Index (AIVI) focusing on the upstream value chain
for FM production, prioritizing publicly available data. We model FM output as
a function of five inputs: Compute, Data, Talent, Capital, and Energy,
hypothesizing that supply vulnerability in any input threatens the industry.
Key vulnerabilities include compute concentration, data scarcity and legal
risks, talent bottlenecks, capital intensity and strategic dependencies, as
well as escalating energy demands. Acknowledging imperfect input
substitutability, we propose a weighted geometrical average of aggregate
subindexes, normalized using theoretical or empirical benchmarks. Despite
limitations and room for improvement, this preliminary index aims to quantify
systemic risks in AI's core production engine, and implicitly shed a light on
the risks for downstream value chain.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [675] [Quantum Autoencoders for Anomaly Detection in Cybersecurity](https://arxiv.org/abs/2510.21837)
*Rohan Senthil,Swee Liang Wong*

Main category: cs.ET

TL;DR: 本文将量子自编码器（QAEs）应用于网络安全异常检测，在数据有限场景中，8 特征 QAE 表现优于经典自编码器（CAEs），显示出 QAEs 的实用优势。


<details>
  <summary>Details</summary>
Motivation: 经典自编码器在数据有限的网络安全异常检测中表现不佳，量子自编码器有潜力克服这一问题。

Method: 将 QAEs 应用于 BETH 数据集进行异常检测，评估多种编码技术、ansatz 类型、重复次数和特征选择策略。

Result: 8 特征 QAE 使用 Dense - Angle 编码和 RealAmplitude ansatz 可在训练样本少的情况下优于 CAEs，最佳 QAE 模型 F1 分数为 0.87，高于 CAE 的 0.77。

Conclusion: QAEs 在数据有限的异常检测场景中可能具有实际优势。

Abstract: Anomaly detection in cybersecurity is a challenging task, where normal events
far outnumber anomalous ones with new anomalies occurring frequently. Classical
autoencoders have been used for anomaly detection, but struggles in
data-limited settings which quantum counterparts can potentially overcome. In
this work, we apply Quantum Autoencoders (QAEs) for anomaly detection in
cybersecurity, specifically on the BPF-extended tracking honeypot (BETH)
dataset. QAEs are evaluated across multiple encoding techniques, ansatz types,
repetitions, and feature selection strategies. Our results demonstrate that an
8-feature QAE using Dense-Angle encoding with a RealAmplitude ansatz can
outperform Classical Autoencoders (CAEs), even when trained on substantially
fewer samples. The effects of quantum encoding and feature selection for
developing quantum models are demonstrated and discussed. In a data-limited
setting, the best performing QAE model has a F1 score of 0.87, better than that
of CAE (0.77). These findings suggest that QAEs may offer practical advantages
for anomaly detection in data-limited scenarios.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [676] [Understanding Carbon Trade Dynamics: A European Union Emissions Trading System Perspective](https://arxiv.org/abs/2510.22341)
*Avirup Chakraborty*

Main category: stat.AP

TL;DR: 分析2010 - 2020年欧盟排放交易系统（EU ETS）效率、价格行为和市场结构，发现其存在持续低效问题。


<details>
  <summary>Details</summary>
Motivation: 分析全球最大的限额与交易碳市场——欧盟排放交易系统的效率、价格行为和市场结构。

Method: 运用AR - GARCH框架分析价格，用网络分析国家间交易，进行特定国家价格与交易量的对数 - 对数回归。

Result: 价格有明显聚类和短期回报可预测性，市场结构集中，价格与交易量弹性有时为正且超1。

Conclusion: EU ETS存在持续低效，交易动态和价格形成不完善，但对脱碳有贡献。

Abstract: The European Union Emissions Trading System (EU ETS), the worlds largest
cap-and-trade carbon market, is central to EU climate policy. This study
analyzes its efficiency, price behavior, and market structure from 2010 to
2020. Using an AR-GARCH framework, we find pronounced price clustering and
short-term return predictability, with 60.05 percent directional accuracy and a
70.78 percent hit rate within forecast intervals. Network analysis of
inter-country transactions shows a concentrated structure dominated by a few
registries that control most high-value flows. Country-specific log-log
regressions of price on traded quantity reveal heterogeneous and sometimes
positive elasticities exceeding unity, implying that trading volumes often rise
with prices. These results point to persistent inefficiencies in the EU ETS,
including partial predictability, asymmetric market power, and unconventional
price-volume relationships, suggesting that while the system contributes to
decarbonization, its trading dynamics and price formation remain imperfect.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [677] [Revisiting the Structure of Trend Premia: When Diversification Hides Redundancy](https://arxiv.org/abs/2510.23150)
*Alban Etiennea,Jean-Jacques Ohana,Eric Benhamou,Béatrice Guez,Ethan Setrouk,Thomas Jacquot*

Main category: q-fin.PR

TL;DR: 本文用贝叶斯优化框架动态分配不同期限趋势信号权重，挑战中期为趋势跟踪‘最佳区间’的传统观点，发现‘杠铃’结构结合长短期限趋势表现更佳。


<details>
  <summary>Details</summary>
Motivation: 重新审视将多期限趋势信号等权结合的传统观点，解决等权分配在理论和实证上的次优问题。

Method: 先在资产层面优化各期限权重以最大化趋势信号信息，再用带稀疏性和换手率控制的贝叶斯图形模型进行资产动态分配。

Result: 包含短长期限后，中期对增量业绩和分散化贡献小；移除125天期限可提高夏普比率和回撤效率，保持与基准的相关性。

Conclusion: ‘杠铃’结构在降低模型复杂度的同时能捕捉大部分业绩，挑战了更多期限能改善分散化的观点，指出部分时间尺度分散化可能存在不必要的冗余。

Abstract: Recent work has emphasized the diversification benefits of combining trend
signals across multiple horizons, with the medium-term window-typically six
months to one year-long viewed as the "sweet spot" of trend-following. This
paper revisits this conventional view by reallocating exposure dynamically
across horizons using a Bayesian optimization framework designed to learn the
optimal weights assigned to each trend horizon at the asset level. The common
practice of equal weighting implicitly assumes that all assets benefit equally
from all horizons; we show that this assumption is both theoretically and
empirically suboptimal. We first optimize the horizon-level weights at the
asset level to maximize the informativeness of trend signals before applying
Bayesian graphical models-with sparsity and turnover control-to allocate
dynamically across assets. The key finding is that the medium-term band
contributes little incremental performance or diversification once short- and
long-term components are included. Removing the 125-day layer improves Sharpe
ratios and drawdown efficiency while maintaining benchmark correlation. We then
rationalize this outcome through a minimum-variance formulation, showing that
the medium-term horizon largely overlaps with its neighboring horizons. The
resulting "barbell" structure-combining short- and long-term trends-captures
most of the performance while reducing model complexity. This result challenges
the common belief that more horizons always improve diversification and
suggests that some forms of time-scale diversification may conceal unnecessary
redundancy in trend premia.

</details>


### [678] [Building Trust in Illiquid Markets: an AI-Powered Replication of Private Equity Funds](https://arxiv.org/abs/2510.23201)
*E. Benhamou,JJ. Ohana,B. Guez,E. Setrouk,T. Jacquot*

Main category: q-fin.PR

TL;DR: 提出用AI增强的流动性策略复制私募股权表现的框架，结果与传统季度私募基准接近，增强组合韧性。


<details>
  <summary>Details</summary>
Motivation: 应对对有韧性和透明金融工具的需求，解决私募股权固有非流动性和缺乏透明度的问题，提升投资者信任和系统稳定性。

Method: 使用高级图形模型解码流动性私募股权代理，并纳入不对称风险调整以模拟私募股权独特表现动态。

Result: 得到了与传统季度私募基准（如剑桥协会和Preqin）紧密一致的流动性、可扩展解决方案。

Conclusion: 该方法增强了投资组合的韧性，有助于安全资产创新的讨论，支持市场稳定和投资者信心。

Abstract: In response to growing demand for resilient and transparent financial
instruments, we introduce a novel framework for replicating private equity (PE)
performance using liquid, AI-enhanced strategies. Despite historically
delivering robust returns, private equity's inherent illiquidity and lack of
transparency raise significant concerns regarding investor trust and systemic
stability, particularly in periods of heightened market volatility. Our method
uses advanced graphical models to decode liquid PE proxies and incorporates
asymmetric risk adjustments that emulate private equity's unique performance
dynamics. The result is a liquid, scalable solution that aligns closely with
traditional quarterly PE benchmarks like Cambridge Associates and Preqin. This
approach enhances portfolio resilience and contributes to the ongoing discourse
on safe asset innovation, supporting market stability and investor confidence.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [679] [Vertex and front-tracking methods for the modeling of microstructure evolution at the solid state: a brief review](https://arxiv.org/abs/2510.21818)
*Marc Bernacki*

Main category: cond-mat.mtrl-sci

TL;DR: 介绍介观尺度微观结构演化建模的两种数值框架，重点阐述FT模型特点、挑战与潜力。


<details>
  <summary>Details</summary>
Motivation: 了解介观尺度微观结构演化建模的有效方法。

Method: 对比FC和FT两种数值框架，介绍FT模型中的Vertex模型及相关技术。

Result: FT模型在提高空间分辨率、适应物理机制方面有优势，但处理复杂拓扑事件有挑战，近期在计算效率等方面有进展。

Conclusion: FT模型虽有挑战，但在计算效率和分析性能上有潜力，可应用于晶粒内现象。

Abstract: In mesoscopic scale microstructure evolution modeling, two primary numerical
frameworks are used: Front-Capturing (FC) and Front-Tracking (FT) ones. FC
models, like phase-field or level-set methods, indirectly define interfaces by
tracking field variable changes. On the contrary, FT models explicitly define
interfaces using interconnected segments or surfaces. In historical FT
methodologies, Vertex models were first developed and consider the description
of the evolution of polygonal structures in terms of the motion of points where
multiple boundaries meet. Globally, FT-type approaches, often associated with
Lagrangian movement, enhance spatial resolution in 3D surfacic and 2D lineic
problems using techniques derived from finite element meshing and remeshing
algorithms. These efficient approaches, by nature, are well adapted to physical
mechanisms correlated to interface properties and geometries. They also face
challenges in managing complex topological events, especially in 3D. However,
recent advances highlight their potential in computational efficiency and
analysis of mobility and energy properties, with possible applications in
intragranular phenomena.

</details>


### [680] [Reinforcement learning-guided optimization of critical current in high-temperature superconductors](https://arxiv.org/abs/2510.22424)
*Mouyang Cheng,Qiwei Wan,Bowen Yu,Eunbi Rha,Michael J Landry,Mingda Li*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出结合强化学习与TDGL模拟的工作流优化高温超导体临界电流密度，发现最优缺陷配置，为缺陷工程提供可扩展策略。


<details>
  <summary>Details</summary>
Motivation: 高温超导体性能受临界电流密度限制，通过缺陷工程优化临界电流密度因缺陷复杂相互作用而具有挑战性。

Method: 结合强化学习与时间相关的Ginzburg - Landau模拟，以临界电流密度为奖励信号引导强化学习智能体迭代优化缺陷配置。

Result: 智能体发现二维薄膜几何结构中最优缺陷密度和相关性，增强涡旋钉扎和临界电流密度，接近理论退配对极限的60%，比随机初始化提高达15倍。

Conclusion: 这种强化学习驱动的方法为缺陷工程提供可扩展策略，对推进高温超导应用有广泛意义。

Abstract: High-temperature superconductors are essential for next-generation energy and
quantum technologies, yet their performance is often limited by the critical
current density ($J_c$), which is strongly influenced by microstructural
defects. Optimizing $J_c$ through defect engineering is challenging due to the
complex interplay of defect type, density, and spatial correlation. Here we
present an integrated workflow that combines reinforcement learning (RL) with
time-dependent Ginzburg-Landau (TDGL) simulations to autonomously identify
optimal defect configurations that maximize $J_c$. In our framework, TDGL
simulations generate current-voltage characteristics to evaluate $J_c$, which
serves as the reward signal that guides the RL agent to iteratively refine
defect configurations. We find that the agent discovers optimal defect
densities and correlations in two-dimensional thin-film geometries, enhancing
vortex pinning and $J_c$ relative to the pristine thin-film, approaching 60\%
of theoretical depairing limit with up to 15-fold enhancement compared to
random initialization. This RL-driven approach provides a scalable strategy for
defect engineering, with broad implications for advancing HTS applications in
fusion magnets, particle accelerators, and other high-field technologies.

</details>


### [681] [AQCat25: Unlocking spin-aware, high-fidelity machine learning potentials for heterogeneous catalysis](https://arxiv.org/abs/2510.22938)
*Omar Allam,Brook Wander,Aayush R. Singh*

Main category: cond-mat.mtrl-sci

TL;DR: 引入AQCat25数据集以改进机器学习原子间势对特定系统的处理，探索与OC20数据集整合方法，发现联合训练有效，用FiLM处理元数据可提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模数据集用于通用异相催化建模的机器学习原子间势存在训练数据缺口，需扩展其处理能力。

Method: 引入AQCat25数据集，探索直接调优和联合训练策略，使用Feature-wise Linear Modulation (FiLM)处理系统特定元数据。

Result: 直接调优导致原数据集知识遗忘，联合训练能在不牺牲通用性能的前提下提高新数据准确性，FiLM可解决联合训练挑战并提升模型准确性。

Conclusion: 建立了一种有效协议，可弥合DFT保真度领域差距，提升催化基础模型的预测能力。

Abstract: Large-scale datasets have enabled highly accurate machine learning
interatomic potentials (MLIPs) for general-purpose heterogeneous catalysis
modeling. There are, however, some limitations in what can be treated with
these potentials because of gaps in the underlying training data. To extend
these capabilities, we introduce AQCat25, a complementary dataset of 13.5
million density functional theory (DFT) single point calculations designed to
improve the treatment of systems where spin polarization and/or higher fidelity
are critical. We also investigate methodologies for integrating new datasets,
such as AQCat25, with the broader Open Catalyst 2020 (OC20) dataset to create
spin-aware models without sacrificing generalizability. We find that directly
tuning a general model on AQCat25 leads to catastrophic forgetting of the
original dataset's knowledge. Conversely, joint training strategies prove
effective for improving accuracy on the new data without sacrificing general
performance. This joint approach introduces a challenge, as the model must
learn from a dataset containing both mixed-fidelity calculations and
mixed-physics (spin-polarized vs. unpolarized). We show that explicitly
conditioning the model on this system-specific metadata, for example by using
Feature-wise Linear Modulation (FiLM), successfully addresses this challenge
and further enhances model accuracy. Ultimately, our work establishes an
effective protocol for bridging DFT fidelity domains to advance the predictive
power of foundational models in catalysis.

</details>


### [682] [Physics-informed diffusion models for extrapolating crystal structures beyond known motifs](https://arxiv.org/abs/2510.23181)
*Andrij Vasylenko,Federico Ottomano,Christopher M. Collins,Rahul Savani,Matthew S. Dyer,Matthew J. Rosseinsky*

Main category: cond-mat.mtrl-sci

TL;DR: 开发物理信息扩散方法结合化学验证协议生成晶体结构，提升生成性能，与CSP协同可高效探索化学空间。


<details>
  <summary>Details</summary>
Motivation: 现有生成式人工智能在发现新晶体结构方面不足，主要重现已知结构变体，需要新方法发现新结构。

Method: 开发物理信息扩散方法，结合化学验证协议，嵌入紧凑性和局部环境多样性描述符。

Result: 提高生成性能，67%结构不在100种最常见原型中；CSP使用生成结构后，97%候选结构可重构，66%为低能且非已知原型框架。

Conclusion: 生成模型虽不能替代CSP，但可增强其效率，建立实用的生成 - CSP协同模式探索化学空间。

Abstract: Discovering materials with previously unreported crystal frameworks is key to
achieving transformative functionality. Generative artificial intelligence
offers a scalable means to propose candidate crystal structures, however
existing approaches mainly reproduce decorated variants of established motifs
rather than uncover new configurations. Here we develop a physics-informed
diffusion method, supported by chemically grounded validation protocol, which
embeds descriptors of compactness and local environment diversity to balance
physical plausibility with structural novelty. Conditioning on these metrics
improves generative performance across architectures, increasing the fraction
of structures outside 100 most common prototypes up to 67%. When crystal
structure prediction (CSP) is seeded with generative structures, most
candidates (97%) are reconstructed by CSP, yielding 145 (66%) low-energy
frameworks not matching any known prototypes. These results show that while
generative models are not substitutes for CSP, their chemically informed,
diversity-guided outputs can enhance CSP efficiency, establishing a practical
generative-CSP synergy for discovery-oriented exploration of chemical space.

</details>
