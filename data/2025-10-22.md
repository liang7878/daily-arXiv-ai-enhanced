<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 47]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 14]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 99]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 33]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 35]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [econ.GN](#econ.GN) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.NI](#cs.NI) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.CY](#cs.CY) [Total: 7]
- [cs.PL](#cs.PL) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [stat.AP](#stat.AP) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [eess.SP](#eess.SP) [Total: 10]
- [cs.LO](#cs.LO) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [stat.ME](#stat.ME) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures](https://arxiv.org/abs/2510.17902)
*Al Kari*

Main category: cs.AI

TL;DR: 论文提出 Cartridge Activation Space Transfer (CAST) 框架，解决大语言模型 LoRA 微调行为的架构锁定问题，实现零样本迁移，性能超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型 LoRA 微调存在架构锁定问题，现有迁移方法通过对齐静态权重空间，效果不佳。

Method: 提出 CAST 框架，学习两个不同大语言模型激活流形间的直接非线性映射，将预训练 LoRA 视为冻结的“行为内核”，训练双向投影头进行转换。

Result: 实验表明 CAST 能实现零样本迁移，转换后的适配器达到在目标模型上完全重新训练 LoRA 性能的 85 - 95%，优于现有权重空间迁移技术。

Conclusion: CAST 框架解决了架构锁定问题，在模型互操作性方面达到新的最优水平。

Abstract: The proliferation of Large Language Model (LLM) architectures presents a
fundamental challenge: valuable, task-specific behaviors learned through
fine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped
within their source model's architecture, herein referred to architectural
lock-in. Existing transfer methods attempt to bridge this gap by aligning the
static weight spaces of models, a brittle and indirect approach that relies on
tenuous correlations between parameter geometries. This paper introduces a
fundamentally different and more direct paradigm: the Cartridge Activation
Space Transfer (CAST), a novel framework that liberates LoRA-encoded behaviors
by learning a direct, nonlinear mapping between the activation manifolds, the
geometric structures formed by the model's internal neuron activations, of two
distinct LLM architectures. CAST treats a pre-trained LoRA as a frozen
"behavioral kernel." It learns a set of lightweight, bidirectional projection
heads that translate the target model's activation stream into the source
model's latent space, apply the frozen kernel, and project the result back.
This process, trained on a general text corpus without any task-specific data,
effectively decouples the learned skill from the source architecture. We
demonstrate that CAST enables true "zero-shot" translation of any standard LoRA
adapter. Our experiments, including transfers between heterogeneous model
families like Llama-2 and Mistral, show that CAST-translated adapters achieve
85-95\% of the performance of a LoRA fully retrained on the target model,
quantitatively outperforming current weight-space transfer techniques and
establishing a new state-of-the-art in model interoperability.

</details>


### [2] [Beyond More Context: Retrieval Diversity Boosts Multi-Turn Intent Understanding](https://arxiv.org/abs/2510.17940)
*Zhiming Lin*

Main category: cs.AI

TL;DR: 研究在固定预算下检索多样性对LLM意图理解的影响，提出多样性感知检索框架，在多数据集上取得良好效果并验证内容多样性的影响。


<details>
  <summary>Details</summary>
Motivation: 实际部署中任务导向聊天机器人面临令牌预算紧张、上下文嘈杂等问题，多数检索管道忽视集合级多样性，探究检索多样性是否能系统提升LLM意图理解。

Method: 提出多样性感知检索框架，平衡意图覆盖和语言多样性，将选择与标准LLM解码器集成，评估时实施预算匹配提示和随机位置，并进行敏感性分析。

Result: 在MultiWOZ 2.4和SGD上，同等令牌预算下联合目标准确率显著提升，超过强大基线，在K为4到7时持续改善且延迟适中。

Conclusion: 研究验证了检索中内容多样性的影响，为构建准确、预算受限的多轮意图系统提供简单、可部署的选择原则。

Abstract: Multi turn intent understanding is central to task oriented chatbots, yet
real deployments face tight token budgets and noisy contexts, and most
retrieval pipelines emphasize relevance while overlooking set level diversity
and confounds such as more context or exemplar order. We ask whether retrieval
diversity, rather than longer prompts, systematically improves LLM intent
understanding under fixed budgets. We present a diversity aware retrieval
framework that selects in context exemplars to balance intent coverage and
linguistic variety, and integrates this selection with standard LLM decoders;
the evaluation enforces budget matched prompts and randomized positions, and
includes sensitivity analyses over exemplar count, diversity strength, and
backbone size. On MultiWOZ 2.4 and SGD, the approach achieves strong gains in
Joint Goal Accuracy under equal token budgets, surpassing strong LLM/DST
baselines, with consistent improvements across K from 4 to 7 and moderate
latency. Overall, the study isolates and validates the impact of content
diversity in retrieval and offers a simple, deployable selection principle for
building accurate, budget constrained multi turn intent systems.

</details>


### [3] [FABRIC: Framework for Agent-Based Realistic Intelligence Creation](https://arxiv.org/abs/2510.17995)
*Abhigya Verma,Seganrasan Subramanian,Nandhakumar Kandasamy,Naman Gupta*

Main category: cs.AI

TL;DR: 提出仅用大语言模型合成代理数据的统一框架，无需人工监督，支持多任务和多轮交互，可推进代理大语言模型发展。


<details>
  <summary>Details</summary>
Motivation: 收集代理数据成本高、耗时长且难扩展，需要新方法合成数据。

Method: 将生成过程分解为模块化管道，生成完整交互记录，集成约束生成格式、JSON模式验证和基于判断的过滤。

Result: 可生成符合严格语法和语义约束的交互记录，支持多任务和多轮交互。

Conclusion: 提供了可复现、仅用大语言模型的替代人工收集的方法，推进了能稳健使用工具的代理大语言模型的发展。

Abstract: Large language models (LLMs) are increasingly deployed as agents, expected to
decompose goals, invoke tools, and verify results in dynamic environments.
Realizing these capabilities requires access to agentic data-structured
interaction records that couple user intents with tool specifications,
argument-grounded calls, and verifiable execution traces. However, collecting
such data from human annotators is costly, time-consuming, and difficult to
scale. We present a unified framework for synthesizing agentic data using only
LLMs, without any human-in-the-loop supervision. This framework decomposes
generation into modular pipelines that produce complete interaction records
spanning task specifications, tool definitions, policy pseudocode, natural
language exchanges, and execution traces. Records conform to strict syntactic
and semantic constraints, ensuring machine-parseability and faithful alignment
across inputs, outputs, and tool calls. Beyond single tasks, there is support
for both multi-task and multi-turn agent interactions, enabling the
construction of datasets that reflect the full spectrum of tool-use
competencies. To ensure quality and consistency, the framework integrates
constrained generation formats, JSON-schema validation, and judge-based
filtering. This paper formalizes the schema for agentic records, details the
prompt design principles that guide generation, and introduces scalable
pipelines for high-quality synthetic data. By providing a reproducible,
LLM-only alternative to manual collection, hence advancing the development of
agentic LLMs capable of robust tool use.

</details>


### [4] [OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning](https://arxiv.org/abs/2510.18032)
*Zhenyu Bi,Meng Lu,Yang Li,Swastik Roy,Weijie Guan,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities in
mathematical and scientific tasks. To enhance complex reasoning, multi-agent
systems have been proposed to harness the collective intelligence of LLM
agents. However, existing collaboration structures are either predefined or
rely on majority voting or round-table debates, which can suppress correct but
less dominant agent contributions. Recent approaches model multi-agent systems
as graph networks but optimize purely for agent performance, neglecting the
quality of interactions. We hypothesize that effective agent communication is
crucial for multi-agent reasoning and that debating quality plays a significant
role. To address this, we propose $\ours$, a multi-agent verbal reinforcement
learning algorithm that dynamically constructs and refines multi-agent
collaboration structures. Our method defines action spaces and a feedback
mechanism that evaluates communication robustness and coherence throughout the
debate. The final decision is achieved through a majority vote over all the
agents. We assess $\ours$ on various reasoning tasks, including mathematical
reasoning, creative writing, scientific reasoning, and numerical sorting.
Results demonstrate that our approach significantly outperforms single-agent
prompting methods and state-of-the-art multi-agent frameworks on diverse tasks.

</details>


### [5] [Subject-Event Ontology Without Global Time: Foundations and Execution Semantics](https://arxiv.org/abs/2510.18040)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 提出无全局时间依赖的主题 - 事件本体形式化，含九条公理，在boldsea系统验证，适用于多场景。


<details>
  <summary>Details</summary>
Motivation: 为复杂动态系统建模，摆脱对全局时间的依赖。

Method: 提出五条关键原则，定义九条公理保证可执行本体正确性，采用基于模型的方法。

Result: 在boldsea系统实现理论构建，验证了实际适用性。

Conclusion: 该形式化适用于分布式系统、微服务架构、DLT平台和多视角场景。

Abstract: A formalization of a subject-event ontology is proposed for modeling complex
dynamic systems without reliance on global time. Key principles: (1) event as
an act of fixation - a subject discerns and fixes changes according to models
(conceptual templates) available to them; (2) causal order via happens-before -
the order of events is defined by explicit dependencies, not timestamps; (3)
making the ontology executable via a declarative dataflow mechanism, ensuring
determinism; (4) models as epistemic filters - a subject can only fix what
falls under its known concepts and properties; (5) presumption of truth - the
declarative content of an event is available for computation from the moment of
fixation, without external verification. The formalization includes nine axioms
(A1-A9), ensuring the correctness of executable ontologies: monotonicity of
history (I1), acyclicity of causality (I2), traceability (I3). Special
attention is given to the model-based approach (A9): event validation via
schemas, actor authorization, automatic construction of causal chains (W3)
without global time. Practical applicability is demonstrated on the boldsea
system - a workflow engine for executable ontologies, where the theoretical
constructs are implemented in BSL (Boldsea Semantic Language). The
formalization is applicable to distributed systems, microservice architectures,
DLT platforms, and multiperspectivity scenarios (conflicting facts from
different subjects).

</details>


### [6] [CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows](https://arxiv.org/abs/2510.18043)
*Joong Ho Choi,Jiayang Zhao,Jeel Shah,Ritvika Sonawane,Vedant Singh,Avani Appalla,Will Flanagan,Filipe Condessa*

Main category: cs.AI

TL;DR: 提出CompactPrompt管道，结合硬提示压缩和轻量级文件级数据压缩，可减少LLM推理成本并保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代理工作流中运行成本高，需降低成本。

Method: 先通过自信息评分和基于依赖的短语分组修剪提示中的低信息令牌，对文档使用n - gram缩写，对数值列使用均匀量化。

Result: 集成到标准LLM代理中，在TAT - QA和FinQA等基准数据集上，最多可减少60%的总令牌使用量和推理成本，Claude - 3.5 - Sonnet和GPT - 4.1 - Mini的准确率下降不到5%。

Conclusion: CompactPrompt有助于可视化实时压缩决策和量化成本 - 性能权衡，为更精简的生成式AI管道奠定基础。

Abstract: Large Language Models (LLMs) deliver powerful reasoning and generation
capabilities but incur substantial run-time costs when operating in agentic
workflows that chain together lengthy prompts and process rich data streams. We
introduce CompactPrompt, an end-to-end pipeline that merges hard prompt
compression with lightweight file-level data compression. CompactPrompt first
prunes low-information tokens from prompts using self-information scoring and
dependency-based phrase grouping. In parallel, it applies n-gram abbreviation
to recurrent textual patterns in attached documents and uniform quantization to
numerical columns, yielding compact yet semantically faithful representations.
Integrated into standard LLM agents, CompactPrompt reduces total token usage
and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA,
while preserving output quality (Results in less than 5% accuracy drop for
Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time
compression decisions and quantify cost-performance trade-offs, laying the
groundwork for leaner generative AI pipelines.

</details>


### [7] [Planned Diffusion](https://arxiv.org/abs/2510.18087)
*Daniel Israel,Tian Jin,Ellie Cheng,Guy Van den Broeck,Aditya Grover,Suvinay Subramanian,Michael Carbin*

Main category: cs.AI

TL;DR: 提出计划扩散方法结合自回归和扩散模型优势，实现更快且高质量文本生成，在AlpacaEval上有良好表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型推理中生成速度和输出质量的权衡问题。

Method: 提出计划扩散方法，分两步：先创建自回归计划拆分输出，再用扩散模型并行生成片段。

Result: 在AlpacaEval上实现质量和延迟的帕累托最优权衡，比自回归生成加速1.27 - 1.81倍，胜率仅下降0.87% - 5.4%。

Conclusion: 该方法扩展了速度 - 质量帕累托前沿，提供了灵活控制质量 - 延迟权衡的方法。

Abstract: A central challenge in large language model inference is the trade-off
between generation speed and output quality. Autoregressive models produce
high-quality text but generate tokens sequentially. Diffusion models can
generate tokens in parallel but often need many iterations to match the same
quality. We propose planned diffusion, a hybrid method that combines the
strengths of both paradigms. Planned diffusion works in two stages: first, the
model creates a short autoregressive plan that breaks the output into smaller,
independent spans. Second, the model generates these spans simultaneously using
diffusion. This approach expands the speed-quality Pareto frontier and provides
a practical path to faster, high-quality text generation. On AlpacaEval, a
suite of 805 instruction-following prompts, planned diffusion achieves
Pareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x
speedup over autoregressive generation with only 0.87\% to 5.4\% drop in win
rate, respectively. Our sensitivity analysis shows that the planning mechanism
of planned diffusion is minimal and reliable, and simple runtime knobs exist to
provide flexible control of the quality-latency trade-off.

</details>


### [8] [SMaRT: Select, Mix, and ReinvenT -- A Strategy Fusion Framework for LLM-Driven Reasoning and Planning](https://arxiv.org/abs/2510.18095)
*Nikhil Verma,Manasa Bharadwaj,Wonjun Jang,Harmanpreet Singh,Yixiao Wang,Homa Fashandi,Chul Lee*

Main category: cs.AI

TL;DR: 现有大语言模型单策略提示方法存在局限，本文提出SMaRT框架融合多种推理策略，经实验验证其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的单策略提示方法未发挥不同推理方法的协同作用，缺乏融合策略的框架以提升性能和鲁棒性。

Method: 提出Select, Mix, and ReinvenT (SMaRT)框架，将大语言模型作为智能集成器，无缝融合多种推理策略。

Result: 在推理、规划和序列决策等基准测试中，SMaRT框架在解决方案质量、约束遵守和性能指标上均优于现有方法。

Conclusion: 该工作开创了跨策略校准的新范式，提升了推理系统的效果，推动了自我完善方法的发展。

Abstract: Large Language Models (LLMs) have redefined complex task automation with
exceptional generalization capabilities. Despite these advancements,
state-of-the-art methods rely on single-strategy prompting, missing the synergy
of diverse reasoning approaches. No single strategy excels universally,
highlighting the need for frameworks that fuse strategies to maximize
performance and ensure robustness. We introduce the Select, Mix, and ReinvenT
(SMaRT) framework, an innovative strategy fusion approach designed to overcome
this constraint by creating balanced and efficient solutions through the
seamless integration of diverse reasoning strategies. Unlike existing methods,
which employ LLMs merely as evaluators, SMaRT uses them as intelligent
integrators, unlocking the "best of all worlds" across tasks. Extensive
empirical evaluations across benchmarks in reasoning, planning, and sequential
decision-making highlight the robustness and adaptability of SMaRT. The
framework consistently outperforms state-of-the-art baselines in solution
quality, constraint adherence, and performance metrics. This work redefines
LLM-driven decision-making by pioneering a new paradigm in cross-strategy
calibration, unlocking superior outcomes for reasoning systems and advancing
the boundaries of self-refining methodologies.

</details>


### [9] [Measuring Reasoning in LLMs: a New Dialectical Angle](https://arxiv.org/abs/2510.18134)
*Soheil Abbasloo*

Main category: cs.AI

TL;DR: 文章提出用辩证法评估大语言模型推理能力的SIEV框架，发现现有模型存在推理差距，强调过程导向评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前评估多关注模型答案正确性，难以揭示推理过程，需从新视角评估大语言模型推理能力。

Method: 借鉴辩证法思想，提出SIEV框架，评估模型解决冲突、整合思想和进行高阶推理的能力。

Result: 在GSM和MMLU等基准测试中，SIEV框架揭示了现有模型存在显著推理差距，如GPT - 5 - chat在GSM测试中得分降低超40分。

Conclusion: 采用过程导向、基于哲学的评估方法，能对大语言模型推理能力进行更深入、严格和有区分度的评估。

Abstract: What does it truly mean for a language model to "reason"? Most current
evaluations and benchmarks reward models' correct standalone answers--but
correctness alone reveals little about the process that produced them. In this
work, we explore a different perspective: reasoning is not a static chain of
steps, but a dynamic trajectory where ideas interact, clash, and evolve into
deeper insights. To capture this dynamic, we draw on a well-established
philosophical tradition: \textit{dialectics}, where reasoning unfolds through
thesis, antithesis, and synthesis. Building on this, we present SIEV, a
structured framework that evaluates reasoning of LLMs through dialectics.
Unlike conventional evaluations, SIEV assesses not only the conclusion a model
reaches, but how it gets there: its ability to resolve tension, integrate
distinct ideas, and synthesize higher-order reasoning. This lens uncovers
significant reasoning gaps in state-of-the-art models even under saturated
benchmarks like GSM and MMLU. For instance, GPT-5-chat, a recent model, loses
over 40 points (out of 100) when evaluated with SIEV on GSM. Our findings
highlight that adopting a process-oriented, philosophically grounded approach
enables a deeper, more rigorous, and more discriminative assessment of LLM
reasoning.

</details>


### [10] [Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models](https://arxiv.org/abs/2510.18143)
*Huan Song,Deeksha Razdan,Yiyue Qian,Arijit Ghosh Chowdhury,Parth Patwa,Aman Chadha,Shinan Zhang,Sharlina Keshava,Hannah Marlowe*

Main category: cs.AI

TL;DR: 介绍PaDA - Agent方法，用以简化小语言模型数据增强过程，实验显示对Llama 3.2 1B Instruct模型微调有显著提升。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在复杂特定领域任务中准确性不足，有监督微调需大量人力和迭代优化，需简化数据增强过程。

Method: 提出PaDA - Agent，通过评估从验证数据中发现失败模式，制定有针对性的数据增强策略以减少泛化差距。

Result: 在Llama 3.2 1B Instruct模型微调上，相比最先进的基于大语言模型的数据增强方法有显著提升。

Conclusion: PaDA - Agent能有效简化小语言模型的数据增强过程，提升模型性能。

Abstract: Small Language Models (SLMs) offer compelling advantages in deployment cost
and latency, but their accuracy often lags behind larger models, particularly
for complex domain-specific tasks. While supervised fine-tuning can help bridge
this performance gap, it requires substantial manual effort in data preparation
and iterative optimization. We present PaDA-Agent (Pattern-guided Data
Augmentation Agent), an evaluation-driven approach that streamlines the data
augmentation process for SLMs through coordinated operations. Unlike
state-of-the-art approaches that focus on model training errors only and
generating error-correcting samples, PaDA-Agent discovers failure patterns from
the validation data via evaluations and drafts targeted data augmentation
strategies aiming to directly reduce the generalization gap. Our experimental
results demonstrate significant improvements over state-of-the-art LLM-based
data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.

</details>


### [11] [Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety](https://arxiv.org/abs/2510.18154)
*Antonio-Gabriel Chacón Menke,Phan Xuan Tan,Eiji Kamioka*

Main category: cs.AI

TL;DR: 提出句级标注数据集用于大语言模型推理时基于激活的安全行为监测，展示其在安全监督方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有分析文本推理步骤的方法可能遗漏有害模式且易被模型绕过，需要更好的安全监测方式。

Method: 创建句级标注数据集，标注推理序列中安全行为，提取转向向量以检测和影响模型激活中的安全行为。

Result: 提取的表示能检测和引导模型激活中的安全行为。

Conclusion: 激活级技术在改善推理安全监督方面有潜力。

Abstract: Recent work has highlighted the importance of monitoring chain-of-thought
reasoning for AI safety; however, current approaches that analyze textual
reasoning steps can miss subtle harmful patterns and may be circumvented by
models that hide unsafe reasoning. We present a sentence-level labeled dataset
that enables activation-based monitoring of safety behaviors during LLM
reasoning. Our dataset contains reasoning sequences with sentence-level
annotations of safety behaviors such as expression of safety concerns or
speculation on user intent, which we use to extract steering vectors for
detecting and influencing these behaviors within model activations. The dataset
fills a key gap in safety research: while existing datasets label reasoning
holistically, effective application of steering vectors for safety monitoring
could be improved by identifying precisely when specific behaviors occur within
reasoning chains. We demonstrate the dataset's utility by extracting
representations that both detect and steer safety behaviors in model
activations, showcasing the potential of activation-level techniques for
improving safety oversight on reasoning.
  Content Warning: This paper discusses AI safety in the context of harmful
prompts and may contain references to potentially harmful content.

</details>


### [12] [LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior](https://arxiv.org/abs/2510.18155)
*Man-Lin Chu,Lucian Terhorst,Kadin Reed,Tom Ni,Weiwei Chen,Rongyu Lin*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体模拟框架用于模拟消费者决策和社交动态，在价格折扣营销场景展现优势，为营销人员提供低风险测试工具。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉人类行为和社交互动复杂性，需要新方法来设计和评估营销战略。

Method: 引入基于大语言模型的多智能体模拟框架，让生成式智能体在无预定义规则下互动、决策。

Result: 在价格折扣营销场景中得出可操作的策略测试结果，揭示传统方法无法发现的社交模式。

Conclusion: 该方法为营销人员提供可扩展、低风险的预实施测试工具，减少对事后评估依赖，降低营销活动表现不佳风险。

Abstract: Simulating consumer decision-making is vital for designing and evaluating
marketing strategies before costly real-world deployment. However, post-event
analyses and rule-based agent-based models (ABMs) struggle to capture the
complexity of human behavior and social interaction. We introduce an
LLM-powered multi-agent simulation framework that models consumer decisions and
social dynamics. Building on recent advances in large language model simulation
in a sandbox environment, our framework enables generative agents to interact,
express internal reasoning, form habits, and make purchasing decisions without
predefined rules. In a price-discount marketing scenario, the system delivers
actionable strategy-testing outcomes and reveals emergent social patterns
beyond the reach of conventional methods. This approach offers marketers a
scalable, low-risk tool for pre-implementation testing, reducing reliance on
time-intensive post-event evaluations and lowering the risk of underperforming
campaigns.

</details>


### [13] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 提出Saber算法解决扩散语言模型在代码生成任务中推理速度和输出质量的权衡问题，实验显示有性能提升。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在代码生成任务中存在推理速度和输出质量的权衡问题，减少采样步骤会导致性能崩溃。

Method: 提出无训练采样算法Saber，基于DLM生成过程的两个关键见解设计。

Result: 在多个主流代码生成基准测试中，Saber使Pass@1准确率平均提高1.9%，推理速度平均提升251.4%。

Conclusion: 利用DLM固有优势，显著缩小了与自回归模型在代码生成中的性能差距。

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [14] [AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI](https://arxiv.org/abs/2510.18170)
*Manik Rana,Calissa Man,Anotida Expected Msiiwa,Jeffrey Paine,Kevin Zhu,Sunishchal Dev,Vasu Sharma,Ahan M R*

Main category: cs.AI

TL;DR: 提出AgentChangeBench基准测试，评估工具增强语言模型代理应对对话中目标转变的能力，发现高准确率不代表动态目标下的鲁棒性，该基准为企业场景提供测试平台。


<details>
  <summary>Details</summary>
Motivation: 当前代理基准主要评估静态目标或一次性工具使用，而现实多轮交互存在目标变化，需要评估代理适应目标转变的能力。

Method: 引入AgentChangeBench基准，通过任务成功率、工具使用效率、工具调用冗余率和目标转变恢复时间四个指标进行评估，包含2835个任务序列和五个用户角色。

Result: 评估多个前沿模型，发现不同模型在目标转变恢复率上有显著差异，零售任务存在高冗余率。

Conclusion: 高原始准确率不意味着在动态目标下具有鲁棒性，明确测量恢复时间和冗余率很重要，AgentChangeBench为企业场景下诊断和提高代理恢复能力提供可复现测试平台。

Abstract: Goal changes are a defining feature of real world multi-turn interactions,
yet current agent benchmarks primarily evaluate static objectives or one-shot
tool use. We introduce AgentChangeBench, a benchmark explicitly designed to
measure how tool augmented language model agents adapt to mid dialogue goal
shifts across three enterprise domains. Our framework formalizes evaluation
through four complementary metrics: Task Success Rate (TSR) for effectiveness,
Tool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for
wasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency.
AgentChangeBench comprises 2,835 task sequences and five user personas, each
designed to trigger realistic shift points in ongoing workflows. Using this
setup, we evaluate several frontier models and uncover sharp contrasts obscured
by traditional $\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\%$
recovery on airline booking shifts while Gemini collapses to $48.6\%$, and
retail tasks show near perfect parameter validity yet redundancy rates above
$80\%$, revealing major inefficiencies. These findings demonstrate that high
raw accuracy does not imply robustness under dynamic goals, and that explicit
measurement of recovery time and redundancy is essential. AgentChangeBench
establishes a reproducible testbed for diagnosing and improving agent
resilience in realistic enterprise settings.

</details>


### [15] [Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains](https://arxiv.org/abs/2510.18176)
*Soumya Rani Samineni,Durgesh Kalwar,Vardaan Gangal,Siddhant Bhambri,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 研究RLVR对大语言模型中间token的影响，发现RL后训练提升推理步骤局部连贯性，但不保证最终答案正确。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法未考虑token级优势，主要基于最终答案评估性能却声称提升推理过程，因此研究RL后训练对未直接激励的中间token的影响。

Method: 使用GRPO算法和Qwen - 2.5 - 0.5B模型在GSM8K数据集上进行实验，引入基于一阶逻辑的跟踪连贯性度量。

Result: RL后训练总体提升跟踪连贯性，在基础模型失败但RL模型成功的问题上提升显著，RL增强局部连贯性但不一定产生有效或正确的解决方案。

Conclusion: 声称通过RL改进推理需谨慎审视，局部连贯性提升不保证有完全有效的数学证明。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR)-based post-training of
Large Language Models (LLMs) has been shown to improve accuracy on reasoning
tasks and continues to attract significant attention. Existing RLVR methods,
however, typically treat all tokens uniformly without accounting for
token-level advantages. These methods primarily evaluate performance based on
final answer correctness or Pass@K accuracy, and yet make claims about RL
post-training leading to improved reasoning traces. This motivates our
investigation into the effect of RL post-training on intermediate tokens which
are not directly incentivized. To study this, we design an experimental setup
using the GRPO algorithm with Qwen-2.5-0.5B model on the GSM8K dataset. We
introduce trace coherence, a First-Order Logic (FOL)-based measure to capture
the consistency of reasoning steps by identifying errors in the traces. We
distinguish between trace validity and trace coherence, noting that the former
implies logical soundness while the latter measures local coherence via lack of
errors. Our results show that RL post-training overall improves trace coherence
with the most significant gains on problems where the base model fails but the
RL model succeeds. Surprisingly, RL enhances local coherence without
necessarily producing valid or correct solutions. This highlights a crucial
distinction: improved local coherence in reasoning steps does not guarantee
final answer correctness. We argue that claims of improved reasoning via RL
must be examined with care, as these may be based on improved trace coherence,
which may not translate into fully valid mathematical proofs.

</details>


### [16] [FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo](https://arxiv.org/abs/2510.18193)
*Keivan Shariatmadar,Ahmad Osman,Ramin Ray,Usman Dildar,Kisam Kim*

Main category: cs.AI

TL;DR: 介绍FST.ai 2.0可解释AI生态系统，能实时支持跆拳道赛事和训练，实验验证其效果，推动体育中可信AI发展。


<details>
  <summary>Details</summary>
Motivation: 解决奥运和残奥格斗运动中公平、透明和可解释决策的挑战。

Method: 集成基于图卷积网络的姿态动作识别、通过可信集进行认知不确定性建模、可解释性覆盖层用于视觉决策支持，还有交互式仪表板。

Result: 减少85%的决策审查时间，93%的裁判信任AI辅助决策。

Conclusion: FST.ai 2.0建立了透明可扩展的管道，代表体育中向公平、负责和以人为本的AI迈进了一步。

Abstract: Fair, transparent, and explainable decision-making remains a critical
challenge in Olympic and Paralympic combat sports. This paper presents
\emph{FST.ai 2.0}, an explainable AI ecosystem designed to support referees,
coaches, and athletes in real time during Taekwondo competitions and training.
The system integrates {pose-based action recognition} using graph convolutional
networks (GCNs), {epistemic uncertainty modeling} through credal sets, and
{explainability overlays} for visual decision support. A set of {interactive
dashboards} enables human--AI collaboration in referee evaluation, athlete
performance analysis, and Para-Taekwondo classification. Beyond automated
scoring, FST.ai~2.0 incorporates modules for referee training, fairness
monitoring, and policy-level analytics within the World Taekwondo ecosystem.
Experimental validation on competition data demonstrates an {85\% reduction in
decision review time} and {93\% referee trust} in AI-assisted decisions. The
framework thus establishes a transparent and extensible pipeline for
trustworthy, data-driven officiating and athlete assessment. By bridging
real-time perception, explainable inference, and governance-aware design,
FST.ai~2.0 represents a step toward equitable, accountable, and human-aligned
AI in sports.

</details>


### [17] [A Definition of AGI](https://arxiv.org/abs/2510.18212)
*Dan Hendrycks,Dawn Song,Christian Szegedy,Honglak Lee,Yarin Gal,Erik Brynjolfsson,Sharon Li,Andy Zou,Lionel Levine,Bo Han,Jie Fu,Ziwei Liu,Jinwoo Shin,Kimin Lee,Mantas Mazeika,Long Phan,George Ingebretsen,Adam Khoja,Cihang Xie,Olawale Salaudeen,Matthias Hein,Kevin Zhao,Alexander Pan,David Duvenaud,Bo Li,Steve Omohundro,Gabriel Alfour,Max Tegmark,Kevin McGrew,Gary Marcus,Jaan Tallinn,Eric Schmidt,Yoshua Bengio*

Main category: cs.AI

TL;DR: 论文引入量化框架定义AGI，以Cattell - Horn - Carroll理论为基础评估AI系统，发现当代模型认知有缺陷，给出AGI分数体现进展与差距。


<details>
  <summary>Details</summary>
Motivation: 因AGI缺乏明确定义，模糊了当前专业AI与人类水平认知的差距，所以引入量化框架。

Method: 基于Cattell - Horn - Carroll理论，将通用智能分解为十个核心认知领域，采用人类心理测量电池评估AI系统。

Result: 当代模型有高度“参差不齐”的认知特征，在知识密集领域表现良好，但在基础认知机制尤其是长期记忆存储方面有严重缺陷，给出如GPT - 4为27%、GPT - 5为58%的AGI分数。

Conclusion: 该框架能具体量化AGI的进展以及距离实现AGI的巨大差距。

Abstract: The lack of a concrete definition for Artificial General Intelligence (AGI)
obscures the gap between today's specialized AI and human-level cognition. This
paper introduces a quantifiable framework to address this, defining AGI as
matching the cognitive versatility and proficiency of a well-educated adult. To
operationalize this, we ground our methodology in Cattell-Horn-Carroll theory,
the most empirically validated model of human cognition. The framework dissects
general intelligence into ten core cognitive domains-including reasoning,
memory, and perception-and adapts established human psychometric batteries to
evaluate AI systems. Application of this framework reveals a highly "jagged"
cognitive profile in contemporary models. While proficient in
knowledge-intensive domains, current AI systems have critical deficits in
foundational cognitive machinery, particularly long-term memory storage. The
resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify
both rapid progress and the substantial gap remaining before AGI.

</details>


### [18] [ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning](https://arxiv.org/abs/2510.18250)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Cancheng Zhang,Xiangdong Zhang,Mingquan Feng,Jingzhi Wang,Junchi Yan*

Main category: cs.AI

TL;DR: 现有基于token选择的方法存在依赖参考模型和仅依赖损失信息的局限性，本文提出ssToken方法，结合自调节和语义感知选择，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有token级数据选择方法依赖额外参考模型且仅依赖损失信息，无法很好保留语义重要token，因此需要改进。

Method: 提出ssToken方法，利用历史模型计算每token与当前模型的损失差异作为自调节信号，引入基于注意力的语义感知token重要性估计指标。

Result: 自调节选择和语义感知选择单独使用均优于全数据微调，两者结合的ssToken方法有协同增益，超越现有token级选择方法。

Conclusion: ssToken方法能在提升性能的同时保持训练效率。

Abstract: Data quality plays a critical role in enhancing supervised fine-tuning (SFT)
for large language models (LLMs), and token-level data selection has emerged as
a promising direction for its fine-grained nature. Despite their strong
empirical performance, existing token-level selection methods share two key
limitations: (1) requiring training or accessing an additional reference model,
and (2) relying solely on loss information for token selection, which cannot
well preserve semantically important tokens that are not favored by loss-based
metrics. To address these challenges, we propose ssToken, a Self-modulated and
Semantic-aware Token Selection approach. ssToken leverages readily accessible
history models to compute the per-token loss difference with the current model,
which serves as a self-modulated signal that enables the model to adaptively
select tokens along its optimization trajectory, rather than relying on excess
loss from an offline-trained reference model as in prior works. We further
introduce a semantic-aware, attention-based token importance estimation metric,
orthogonal to loss-based selection and providing complementary semantic
information for more effective filtering. Extensive experiments across
different model families and scales demonstrate that both self-modulated
selection and semantic-aware selection alone outperform full-data fine-tuning,
while their integration--ssToken--achieves synergistic gains and further
surpasses prior token-level selection methods, delivering performance
improvements while maintaining training efficiency.

</details>


### [19] [Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](https://arxiv.org/abs/2510.18254)
*Sion Weatherhead,Flora Salim,Aaron Belbasis*

Main category: cs.AI

TL;DR: 研究测试前沿大语言模型在开放式任务中的反思能力，发现当前模型反思缺乏有效证据，需外部结构确保可靠表现。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型的反思能力是否与人类反思推理功能等效，之前封闭式任务研究掩盖了自纠错的局限。

Method: 在开放式且有规则约束的现实任务中测试八个前沿模型，让其生成有效科学测试项目并自我批判后修订。

Result: 首次表现差，反思提升有限，常重复违反约束，开放式程度增加时表现变差，主打推理的模型无优势。

Conclusion: 当前大语言模型的反思缺乏主动、目标驱动的监控机制，需外部结构来确保可靠性能。

Abstract: Humans do not just find mistakes after the fact -- we often catch them
mid-stream because 'reflection' is tied to the goal and its constraints.
Today's large language models produce reasoning tokens and 'reflective' text,
but is it functionally equivalent with human reflective reasoning? Prior work
on closed-ended tasks -- with clear, external 'correctness' signals -- can make
'reflection' look effective while masking limits in self-correction. We
therefore test eight frontier models on a simple, real-world task that is
open-ended yet rule-constrained, with auditable success criteria: to produce
valid scientific test items, then revise after considering their own critique.
First-pass performance is poor (often zero valid items out of 4 required; mean
$\approx$ 1), and reflection yields only modest gains (also $\approx$ 1).
Crucially, the second attempt frequently repeats the same violation of
constraint, indicating 'corrective gains' arise largely from chance production
of a valid item rather than error detection and principled,
constraint-sensitive repair. Performance before and after reflection
deteriorates as open-endedness increases, and models marketed for 'reasoning'
show no advantage. Our results suggest that current LLM 'reflection' lacks
functional evidence of the active, goal-driven monitoring that helps humans
respect constraints even on a first pass. Until such mechanisms are
instantiated in the model itself, reliable performance requires external
structure that enforces constraints.

</details>


### [20] [Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming](https://arxiv.org/abs/2510.18314)
*Zheng Zhang,Jiarui He,Yuchen Cai,Deheng Ye,Peilin Zhao,Ruili Feng,Hao Wang*

Main category: cs.AI

TL;DR: 本文提出Genesis框架用于Web代理攻击，实验表明其能发现新策略且优于现有攻击基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理自动化Web任务带来安全风险，但相关Web代理攻击研究有限，现有红队方法难以捕捉代理行为模式，缺乏策略的持续发现和进化。

Method: 提出由Attacker、Scorer和Strategist三个模块组成的Genesis框架，Attacker结合遗传算法和混合策略表示生成对抗注入，Scorer评估目标Web代理响应提供反馈，Strategist从交互日志中动态发现有效策略并更新策略库。

Result: 在各种Web任务的大量实验中，框架发现了新策略，且持续优于现有攻击基线。

Conclusion: 所提出的Genesis框架在Web代理攻击中表现良好，能有效发现新策略并提升攻击效果。

Abstract: As large language model (LLM) agents increasingly automate complex web tasks,
they boost productivity while simultaneously introducing new security risks.
However, relevant studies on web agent attacks remain limited. Existing
red-teaming approaches mainly rely on manually crafted attack strategies or
static models trained offline. Such methods fail to capture the underlying
behavioral patterns of web agents, making it difficult to generalize across
diverse environments. In web agent attacks, success requires the continuous
discovery and evolution of attack strategies. To this end, we propose Genesis,
a novel agentic framework composed of three modules: Attacker, Scorer, and
Strategist. The Attacker generates adversarial injections by integrating the
genetic algorithm with a hybrid strategy representation. The Scorer evaluates
the target web agent's responses to provide feedback. The Strategist
dynamically uncovers effective strategies from interaction logs and compiles
them into a continuously growing strategy library, which is then re-deployed to
enhance the Attacker's effectiveness. Extensive experiments across various web
tasks show that our framework discovers novel strategies and consistently
outperforms existing attack baselines.

</details>


### [21] [Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning](https://arxiv.org/abs/2510.18318)
*Aaron Bell,Amit Aides,Amr Helmy,Arbaaz Muslim,Aviad Barzilai,Aviv Slobodkin,Bolous Jaber,David Schottlander,George Leifman,Joydeep Paul,Mimi Sun,Nadav Sherman,Natalie Williams,Per Bjornsson,Roy Lee,Ruth Alcantara,Thomas Turnbull,Tomer Shekel,Vered Silverman,Yotam Gigi,Adam Boulanger,Alex Ottenwess,Ali Ahmadalipour,Anna Carter,Charles Elliott,David Andre,Elad Aharoni,Gia Jung,Hassler Thurston,Jacob Bien,Jamie McPike,Juliet Rothenberg,Kartik Hegde,Kel Markert,Kim Philipp Jablonski,Luc Houriez,Monica Bharel,Phing VanLee,Reuven Sayag,Sebastian Pilarski,Shelley Cazares,Shlomi Pasternak,Siduo Jiang,Stone Jiang,Thomas Colthurst,Yang Chen,Yehonathan Refael,Yochai Blau,Yuval Carny,Yael Maguire,Avinatan Hassidim,James Manyika,Tim Thelin,Genady Beryozkin,Gautam Prasad,Luke Barrington,Yossi Matias,Niv Efron,Shravya Shetty*

Main category: cs.AI

TL;DR: 本文介绍地球AI，它基于三个关键领域的基础模型和推理引擎，可处理地理空间数据，基准测试显示其能力，代理能在危机场景提供关键见解。


<details>
  <summary>Details</summary>
Motivation: 地理空间数据量大、多样、分辨率和时间尺度不同、数据稀疏，给分析和解释带来挑战，需要新方法挖掘数据价值。

Method: 引入地球AI，基于行星尺度图像、人口和环境三个关键领域的基础模型和Gemini推理引擎，开发Gemini代理处理复杂查询。

Result: 基准测试展示基础模型的能力和互补价值，代理在危机场景基准测试中能提供关键及时见解。

Conclusion: 地球AI可有效挖掘地理空间数据价值，弥合原始数据与可操作理解之间的差距。

Abstract: Geospatial data offers immense potential for understanding our planet.
However, the sheer volume and diversity of this data along with its varied
resolutions, timescales, and sparsity pose significant challenges for thorough
analysis and interpretation. This paper introduces Earth AI, a family of
geospatial AI models and agentic reasoning that enables significant advances in
our ability to unlock novel and profound insights into our planet. This
approach is built upon foundation models across three key domains--Planet-scale
Imagery, Population, and Environment--and an intelligent Gemini-powered
reasoning engine. We present rigorous benchmarks showcasing the power and novel
capabilities of our foundation models and validate that when used together,
they provide complementary value for geospatial inference and their synergies
unlock superior predictive capabilities. To handle complex, multi-step queries,
we developed a Gemini-powered agent that jointly reasons over our multiple
foundation models along with large geospatial data sources and tools. On a new
benchmark of real-world crisis scenarios, our agent demonstrates the ability to
deliver critical and timely insights, effectively bridging the gap between raw
geospatial data and actionable understanding.

</details>


### [22] [ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.18342)
*Peng Tang,Xiaoxiao Yan,Xiaobin Hu,Yuning Cui,Donghao Luo,Jiangning Zhang,Pengcheng Xu,Jinlong Peng,Qingdong He,Feiyue Huang,Song Xue,Tobias Lasser*

Main category: cs.AI

TL;DR: 提出ShortcutBreaker框架解决多类无监督异常检测中Transformer架构的身份捷径问题，实验表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 多类无监督异常检测（MUAD）中Transformer架构存在身份捷径问题，导致正常和异常样本重建误差差距缩小，难以区分。

Method: 提出ShortcutBreaker框架，设计低秩噪声瓶颈（LRNB）将高维特征投影到低秩潜空间，防止平凡身份复制；引入全局扰动注意力防止解码器中的信息捷径。

Result: 在四个广泛使用的异常检测基准数据集上，图像级AUROC分别达到99.8%、98.9%、90.6%和87.8%，在不同场景下始终优于先前的MUAD方法。

Conclusion: ShortcutBreaker框架有效解决了MUAD中Transformer架构的身份捷径问题，提升了异常检测性能。

Abstract: Multi-class unsupervised anomaly detection (MUAD) has garnered growing
research interest, as it seeks to develop a unified model for anomaly detection
across multiple classes, i.e., eliminating the need to train separate models
for distinct objects and thereby saving substantial computational resources.
Under the MUAD setting, while advanced Transformer-based architectures have
brought significant performance improvements, identity shortcuts persist: they
directly copy inputs to outputs, narrowing the gap in reconstruction errors
between normal and abnormal cases, and thereby making the two harder to
distinguish. Therefore, we propose ShortcutBreaker, a novel unified
feature-reconstruction framework for MUAD tasks, featuring two key innovations
to address the issue of shortcuts. First, drawing on matrix rank inequality, we
design a low-rank noisy bottleneck (LRNB) to project highdimensional features
into a low-rank latent space, and theoretically demonstrate its capacity to
prevent trivial identity reproduction. Second, leveraging ViTs global modeling
capability instead of merely focusing on local features, we incorporate a
global perturbation attention to prevent information shortcuts in the decoders.
Extensive experiments are performed on four widely used anomaly detection
benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)
and one medical dataset (Universal Medical). The proposed method achieves a
remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four
datasets, respectively, consistently outperforming previous MUAD methods across
different scenarios.

</details>


### [23] [Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games](https://arxiv.org/abs/2510.18395)
*Runnan Qi,Yanan Ni,Lumin Jiang,Zongyuan Li,Kuihua Huang,Xian Guo*

Main category: cs.AI

TL;DR: 提出Memory - Augmented State Machine Prompting (MASMP)框架用于实时策略游戏的LLM智能体，实验效果好，建立新范式。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中存在的幻觉和决策碎片化等关键挑战。

Method: 将状态机提示与内存机制集成，包括自然语言驱动的状态机架构和轻量级内存模块。

Result: 在《星际争霸II》中对最难内置AI胜率达60%，远超基线（0%），能保留语义理解并解决“知行差距”。

Conclusion: 该工作为复杂决策中结合神经和符号AI建立了新范式。

Abstract: This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel
framework for LLM agents in real-time strategy games. Addressing key challenges
like hallucinations and fragmented decision-making in existing approaches,
MASMP integrates state machine prompting with memory mechanisms to unify
structured actions with long-term tactical coherence. The framework features:
(1) a natural language-driven state machine architecture that guides LLMs to
emulate finite state machines and behavior trees through prompts, and (2) a
lightweight memory module preserving strategic variables (e.g., tactics,
priority units) across decision cycles. Experiments in StarCraft II demonstrate
MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly
outperforming baselines (0%). Case studies reveal the method retains LLMs'
semantic comprehension while resolving the "Knowing-Doing Gap" through strict
state-action mapping, achieving both interpretability and FSM-like reliability.
This work establishes a new paradigm for combining neural and symbolic AI in
complex decision-making.

</details>


### [24] [Heterogeneous Adversarial Play in Interactive Environments](https://arxiv.org/abs/2510.18407)
*Manjie Xu,Xinyi Yang,Jiayu Zhan,Wei Liang,Chi Zhang,Yixin Zhu*

Main category: cs.AI

TL;DR: 提出异质对抗性游戏（HAP）自动课程学习框架，该框架在多任务学习领域表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统自玩框架在开放式学习的不对称场景中不足，需在无预定任务层次的人工系统中实现不对称、自适应教学机制。

Method: 提出HAP框架，将师生互动形式化为极小极大优化，任务生成的教师和解决问题的学习者通过对抗动力学共同进化，建立双向反馈系统。

Result: 在多任务学习领域实验验证，该框架与SOTA基线性能相当，生成的课程能提高人工智能体和人类的学习效果。

Conclusion: HAP框架是有效的，可提升学习效能。

Abstract: Self-play constitutes a fundamental paradigm for autonomous skill
acquisition, whereby agents iteratively enhance their capabilities through
self-directed environmental exploration. Conventional self-play frameworks
exploit agent symmetry within zero-sum competitive settings, yet this approach
proves inadequate for open-ended learning scenarios characterized by inherent
asymmetry. Human pedagogical systems exemplify asymmetric instructional
frameworks wherein educators systematically construct challenges calibrated to
individual learners' developmental trajectories. The principal challenge
resides in operationalizing these asymmetric, adaptive pedagogical mechanisms
within artificial systems capable of autonomously synthesizing appropriate
curricula without predetermined task hierarchies. Here we present Heterogeneous
Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework
that formalizes teacher-student interactions as a minimax optimization wherein
task-generating instructor and problem-solving learner co-evolve through
adversarial dynamics. In contrast to prevailing ACL methodologies that employ
static curricula or unidirectional task selection mechanisms, HAP establishes a
bidirectional feedback system wherein instructors continuously recalibrate task
complexity in response to real-time learner performance metrics. Experimental
validation across multi-task learning domains demonstrates that our framework
achieves performance parity with SOTA baselines while generating curricula that
enhance learning efficacy in both artificial agents and human subjects.

</details>


### [25] [Deep Learning-Based Control Optimization for Glass Bottle Forming](https://arxiv.org/abs/2510.18412)
*Mattia Pujatti,Andrea Di Luca,Nicola Peghini,Federico Monegaglia,Marco Cristoforetti*

Main category: cs.AI

TL;DR: 研究提出基于深度学习的控制算法优化玻璃瓶成型过程，实验表明有良好效果，凸显深度学习在玻璃制造过程控制中的潜力。


<details>
  <summary>Details</summary>
Motivation: 玻璃瓶制造中精确控制成型机对保证质量和减少缺陷至关重要，需优化成型过程。

Method: 利用实际生产数据，通过神经网络预测参数变化影响，借助专门设计的反演机制确定最佳机器设置。

Result: 在多条生产线历史数据集上实验，该方法取得有前景的结果，可增强过程稳定性、减少浪费、提高产品一致性。

Conclusion: 深度学习在玻璃制造过程控制中有应用潜力。

Abstract: In glass bottle manufacturing, precise control of forming machines is
critical for ensuring quality and minimizing defects. This study presents a
deep learning-based control algorithm designed to optimize the forming process
in real production environments. Using real operational data from active
manufacturing plants, our neural network predicts the effects of parameter
changes based on the current production setup. Through a specifically designed
inversion mechanism, the algorithm identifies the optimal machine settings
required to achieve the desired glass gob characteristics. Experimental results
on historical datasets from multiple production lines show that the proposed
method yields promising outcomes, suggesting potential for enhanced process
stability, reduced waste, and improved product consistency. These results
highlight the potential of deep learning to process control in glass
manufacturing.

</details>


### [26] [Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents](https://arxiv.org/abs/2510.18424)
*Guangfu Guo,Xiaoqian Lu,Yue Feng*

Main category: cs.AI

TL;DR: 提出医疗视觉推理代理Med - VRAgent解决VLMs在医疗推理中的问题，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决VLMs在医疗推理中存在幻觉、描述模糊、逻辑不一致和定位差等问题。

Method: 基于视觉引导、自我奖励范式和蒙特卡罗树搜索（MCTS）构建Med - VRAgent，结合视觉引导与树搜索，用近端策略优化（PPO）目标微调VLMs。

Result: 在多个医疗VQA基准测试中，所提方法优于现有方法。

Conclusion: Med - VRAgent能有效提升VLMs的医疗视觉推理能力。

Abstract: Visual Language Models (VLMs) achieve promising results in medical reasoning
but struggle with hallucinations, vague descriptions, inconsistent logic and
poor localization. To address this, we propose a agent framework named Medical
Visual Reasoning Agent (\textbf{Med-VRAgent}). The approach is based on Visual
Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By
combining the Visual Guidance with tree search, Med-VRAgent improves the
medical visual reasoning capabilities of VLMs. We use the trajectories
collected by Med-VRAgent as feedback to further improve the performance by
fine-tuning the VLMs with the proximal policy optimization (PPO) objective.
Experiments on multiple medical VQA benchmarks demonstrate that our method
outperforms existing approaches.

</details>


### [27] [Automated urban waterlogging assessment and early warning through a mixture of foundation models](https://arxiv.org/abs/2510.18425)
*Chenxu Zhang,Fuxiang Huang,Lei Zhang*

Main category: cs.AI

TL;DR: 提出基于基础模型的城市内涝评估框架UWAssess，解决数据稀缺问题，提升感知和报告生成能力，支持城市管理等。


<details>
  <summary>Details</summary>
Motivation: 气候变化下城市内涝威胁增大，现有监测方法依赖人工报告，无法及时全面评估。

Method: 设计半监督微调策略和思维链提示策略，以释放基础模型在数据稀缺下游任务的潜力。

Result: 在视觉基准测试中感知性能显著提升，基于GPT评估表明能生成可靠文本报告。

Conclusion: UWAssess实现内涝监测从感知到生成的转变，多基础模型协作框架为智能可扩展系统奠定基础，支持城市管理等。

Abstract: With climate change intensifying, urban waterlogging poses an increasingly
severe threat to global public safety and infrastructure. However, existing
monitoring approaches rely heavily on manual reporting and fail to provide
timely and comprehensive assessments. In this study, we present Urban
Waterlogging Assessment (UWAssess), a foundation model-driven framework that
automatically identifies waterlogged areas in surveillance images and generates
structured assessment reports. To address the scarcity of labeled data, we
design a semi-supervised fine-tuning strategy and a chain-of-thought (CoT)
prompting strategy to unleash the potential of the foundation model for
data-scarce downstream tasks. Evaluations on challenging visual benchmarks
demonstrate substantial improvements in perception performance. GPT-based
evaluations confirm the ability of UWAssess to generate reliable textual
reports that accurately describe waterlogging extent, depth, risk and impact.
This dual capability enables a shift of waterlogging monitoring from perception
to generation, while the collaborative framework of multiple foundation models
lays the groundwork for intelligent and scalable systems, supporting urban
management, disaster response and climate resilience.

</details>


### [28] [AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library](https://arxiv.org/abs/2510.18428)
*Minwei Kong,Ao Qu,Xiaotong Guo,Wenbin Ouyang,Chonghe Jiang,Han Zheng,Yining Ma,Dingyi Zhuang,Yuhan Tang,Junyi Li,Hai Wang,Cathy Wu,Jinhua Zhao*

Main category: cs.AI

TL;DR: 提出AlphaOPT，可让LLM从有限演示和求解器反馈中学习，在无注释推理轨迹或参数更新下提升性能，实验显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 优化建模难自动化，现有LLM方法依赖脆弱提示或高成本再训练且泛化性有限。

Method: AlphaOPT采用持续两阶段循环，即库学习阶段提取结构化见解，库进化阶段诊断检索偏差并优化见解适用性条件。

Result: AlphaOPT随数据增多性能提升，在Out-of-distribution OptiBench数据集上超越最强基线7.7%。

Conclusion: AlphaOPT能从有限演示高效学习，无需高成本再训练，知识可解释。

Abstract: Optimization modeling enables critical decisions across industries but
remains difficult to automate: informal language must be mapped to precise
mathematical formulations and executable solver code. Prior LLM approaches
either rely on brittle prompting or costly retraining with limited
generalization. We present AlphaOPT, a self-improving experience library that
enables an LLM to learn from limited demonstrations (even answers alone,
without gold-standard programs) and solver feedback - without annotated
reasoning traces or parameter updates. AlphaOPT operates in a continual
two-phase cycle: (i) a Library Learning phase that reflects on failed attempts,
extracting solver-verified, structured insights as {taxonomy, condition,
explanation, example}; and (ii) a Library Evolution phase that diagnoses
retrieval misalignments and refines the applicability conditions of stored
insights, improving transfer across tasks. This design (1) learns efficiently
from limited demonstrations without curated rationales, (2) expands continually
without costly retraining by updating the library rather than model weights,
and (3) makes knowledge explicit and interpretable for human inspection and
intervention. Experiments show that AlphaOPT steadily improves with more data
(65% to 72% from 100 to 300 training items) and surpasses the strongest
baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only
on answers. Code and data are available at:
https://github.com/Minw913/AlphaOPT.

</details>


### [29] [PlanU: Large Language Model Decision Making through Planning under Uncertainty](https://arxiv.org/abs/2510.18442)
*Ziwei Deng,Mian Deng,Chenjing Liang,Zeming Gao,Chennan Ma,Chenxing Lin,Haipeng Zhang,Songzhu Mei,Cheng Wang,Siqi Shen*

Main category: cs.AI

TL;DR: 论文指出大语言模型在决策任务中面临不确定性挑战，提出PlanU方法解决该问题并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在不确定决策任务中存在问题，已有方法不能有效处理不确定性，需新方法解决。

Method: 引入PlanU方法，在蒙特卡罗树搜索中捕捉不确定性，将节点回报建模为分位数分布，引入UCC分数平衡探索与利用。

Result: 通过大量实验证明PlanU在基于大语言模型的不确定决策任务中有效。

Conclusion: PlanU能有效解决大语言模型决策中的不确定性问题。

Abstract: Large Language Models (LLMs) are increasingly being explored across a range
of decision-making tasks. However, LLMs sometimes struggle with decision-making
tasks under uncertainty that are relatively easy for humans, such as planning
actions in stochastic environments. The adoption of LLMs for decision-making is
impeded by uncertainty challenges, such as LLM uncertainty and environmental
uncertainty. LLM uncertainty arises from the stochastic sampling process
inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM
uncertainty through multiple reasoning chains or search trees. However, these
approaches overlook environmental uncertainty, which leads to poor performance
in environments with stochastic state transitions. Some recent LDM approaches
deal with uncertainty by forecasting the probability of unknown variables.
However, they are not designed for multi-step decision-making tasks that
require interaction with the environment. To address uncertainty in LLM
decision-making, we introduce PlanU, an LLM-based planning method that captures
uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of
each node in the MCTS as a quantile distribution, which uses a set of quantiles
to represent the return distribution. To balance exploration and exploitation
during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity
(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive
experiments, we demonstrate the effectiveness of PlanU in LLM-based
decision-making tasks under uncertainty.

</details>


### [30] [CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs](https://arxiv.org/abs/2510.18470)
*Shaobo Wang,Yongliang Miao,Yuancheng Liu,and Qianli Ma,Ning Liao,Linfeng Zhang*

Main category: cs.AI

TL;DR: 现有大语言模型扩展性能依赖大量推理数据集，成本高。本文提出CircuitSeer数据选择方法，实验证明其高效有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型扩展性能依赖大量推理数据集，训练成本高，现有数据选择方法依赖昂贵外部模型或不透明启发式方法。

Method: 发现复杂推理任务会激活稀疏、专门的注意力头子集形成核心推理电路，提出CircuitSeer方法，通过衡量数据对关键电路的影响来量化推理复杂度。

Result: 在4个模型和9个数据集上的实验表明CircuitSeer更优，用该方法选10%数据微调Qwen2.5 - Math - 7B，平均Pass@1比全量数据训练高1.4分。

Conclusion: CircuitSeer在数据选择上高效有效。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities, but scaling their performance often relies on massive reasoning
datasets that are computationally expensive to train on. Existing data
selection methods aim to curate smaller, high-quality subsets but often rely on
costly external models or opaque heuristics. In this work, we shift the focus
from external heuristics to the model's internal mechanisms. We find that
complex reasoning tasks consistently activate a sparse, specialized subset of
attention heads, forming core reasoning circuits. Building on this insight, we
propose CircuitSeer, a novel data selection method that quantifies the
reasoning complexity of data by measuring its influence on these crucial
circuits. Extensive experiments on 4 models and 9 datasets demonstrate
CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of
data selected by our method achieves a 1.4-point gain in average Pass@1 over
training on the full dataset, highlighting its efficiency and effectiveness.

</details>


### [31] [Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents](https://arxiv.org/abs/2510.18476)
*Feifan Xia,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 提出用于多轮社交对话中大型语言模型代理的概率意图建模框架，实验显示有改进，表明该建模对开发社交智能LLM代理有贡献。


<details>
  <summary>Details</summary>
Motivation: 开发适用于多轮社交对话中大型语言模型代理的更好方法，以实现不确定下的自适应对话策略。

Method: 提出概率意图建模框架，维护伙伴潜在意图的信念分布，从上下文先验初始化并在每次话语后通过似然估计动态更新。

Result: 在SOTOPIA环境初步实验中，框架使SOTOPIA - All总体得分提升9.0%，SOTOPIA - Hard提升4.1%，略超直接观察伙伴意图的oracle代理。

Conclusion: 概率意图建模有助于开发社交智能的大型语言模型代理。

Abstract: We present a probabilistic intent modeling framework for large language model
(LLM) agents in multi-turn social dialogue. The framework maintains a belief
distribution over a partner's latent intentions, initialized from contextual
priors and dynamically updated through likelihood estimation after each
utterance. The evolving distribution provides additional contextual grounding
for the policy, enabling adaptive dialogue strategies under uncertainty.
Preliminary experiments in the SOTOPIA environment show consistent
improvements: the proposed framework increases the Overall score by 9.0% on
SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and
slightly surpasses an oracle agent that directly observes partner intentions.
These early results suggest that probabilistic intent modeling can contribute
to the development of socially intelligent LLM agents.

</details>


### [32] [LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources](https://arxiv.org/abs/2510.18477)
*Haichao Ji,Zibo Wang,Yifei Zhu,Meng han,Dan Wang,Zhu Han*

Main category: cs.AI

TL;DR: 提出LAFA系统将基于大语言模型代理的数据分析与联邦分析集成，实验显示其优于基线策略，为联邦分析中支持自然语言输入的隐私保护分析奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型代理的分析框架缺乏隐私保护，联邦分析不支持自然语言输入，需结合二者优势。

Method: 引入分层多智能体架构，粗粒度规划器分解复杂查询，细粒度规划器将子查询映射为有向无环图，优化器智能体重写和合并图。

Result: LAFA执行计划成功率更高，大幅减少资源密集型联邦分析操作。

Conclusion: 为联邦分析场景下支持自然语言输入的隐私保护、大语言模型驱动的分析建立了实用基础。

Abstract: Large Language Models (LLMs) have shown great promise in automating data
analytics tasks by interpreting natural language queries and generating
multi-operation execution plans. However, existing LLM-agent-based analytics
frameworks operate under the assumption of centralized data access, offering
little to no privacy protection. In contrast, federated analytics (FA) enables
privacy-preserving computation across distributed data sources, but lacks
support for natural language input and requires structured, machine-readable
queries. In this work, we present LAFA, the first system that integrates
LLM-agent-based data analytics with FA. LAFA introduces a hierarchical
multi-agent architecture that accepts natural language queries and transforms
them into optimized, executable FA workflows. A coarse-grained planner first
decomposes complex queries into sub-queries, while a fine-grained planner maps
each subquery into a Directed Acyclic Graph of FA operations using prior
structural knowledge. To improve execution efficiency, an optimizer agent
rewrites and merges multiple DAGs, eliminating redundant operations and
minimizing computational and communicational overhead. Our experiments
demonstrate that LAFA consistently outperforms baseline prompting strategies by
achieving higher execution plan success rates and reducing resource-intensive
FA operations by a substantial margin. This work establishes a practical
foundation for privacy-preserving, LLM-driven analytics that supports natural
language input in the FA setting.

</details>


### [33] [StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking](https://arxiv.org/abs/2510.18483)
*Haoran Zhang,Chenhao Zhu,Sicong Guo,Hanzhe Guo,Haiming Li,Donglin Yu*

Main category: cs.AI

TL;DR: 提出StarBench基准测试，评估视觉语言模型在真实游戏客户端中的类人游戏能力，发现直接控制模式下感知到控制的保真度存在差距，明智的信息寻求与成功率提升相关。


<details>
  <summary>Details</summary>
Motivation: 探究当前视觉语言模型是否能像人类玩家一样，将屏幕所见转化为精确操作并在遇到困难时寻求信息。

Method: 引入基于《崩坏：星穹铁道》的回合制RPG基准测试StarBench，包括直接控制和工具辅助控制两种模式，以及信息寻求诊断。

Result: 直接控制模式下感知到控制的保真度存在较大差距，明智的信息寻求与成功率提升相关。

Conclusion: StarBench可作为真实客户端游戏中主动信息寻求和多模态决策的可复现衡量标准。

Abstract: Human players do more than press buttons: they ground what they see on screen
into precise keyboard-mouse actions and, when stuck, they seek information
before trying again. We ask whether current vision-language models (VLMs) can
do the same. Despite encouraging results under simplified control or tool
scaffolds, human-like play in a real client - mapping raw screenshots to
temporally coherent low-level actions while deciding when to ask for guidance -
remains an open challenge. We introduce StarBench, a turn-based RPG benchmark
derived from Honkai: Star Rail that targets these two human-like competencies:
multimodal decision-making from pixels to actions and agentic information
seeking. StarBench standardizes evaluation across eight combat tasks and two
regimes with shared tasks and metrics: (i) direct control, where agents receive
only screenshots and must emit low-level primitives (click and keypress) with
no semantic hints; and (ii) tool-assisted control, where higher-level intents
can be mapped to primitives by detectors and OCR outputs provide optional
textualized observations to ease UI grounding. To mirror human practice,
StarBench also includes an ask-or-act diagnostic that measures whether and when
agents choose to request brief guidance before proceeding, and how that choice
affects subsequent performance. We report reference baselines for contemporary
VLMs and a human reference. Results expose sizable gaps in
perception-to-control fidelity in the direct regime, while showing that
judicious information seeking correlates with improved success, establishing
StarBench as a reproducible yardstick for agentic information seeking and
multimodal decision-making in real-client play.

</details>


### [34] [AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification](https://arxiv.org/abs/2510.18488)
*Ho Fai Leung,Xiaoyan Xi,Fei Zuo*

Main category: cs.AI

TL;DR: 现有设备端虚拟助手依赖API受限，GUI代理虽有潜力但因基准测试得分低未被广泛采用。研究发现基准测试存在问题，改进为AndroidControl - Curated，模型成功率提升，还推出小参数高性能模型Magma - R1 - 3B，发布基准和模型以推动虚拟助手发展。


<details>
  <summary>Details</summary>
Motivation: 解决设备端虚拟助手依赖API受限问题，推动GUI代理实际应用，解决现有基准测试低估代理能力的问题。

Method: 识别AndroidControl基准测试的问题，通过严格净化流程将其改进为AndroidControl - Curated；用2.4k精选样本在H20 GPU上对模型进行60小时微调得到Magma - R1 - 3B。

Result: 在改进后的基准测试上，最先进模型复杂任务成功率接近75%，提升15%；Magma - R1 - 3B参数小200倍但性能与Qwen3 - VL - 235B相当。

Conclusion: 设备端GUI代理比之前认为的更接近实际部署，发布改进后的基准和新模型可更好反映模型能力，加速强大设备端虚拟助手的开发。

Abstract: On-device virtual assistants like Siri and Google Assistant are increasingly
pivotal, yet their capabilities are hamstrung by a reliance on rigid,
developer-dependent APIs. GUI agents offer a powerful, API-independent
alternative, but their adoption is hindered by the perception of poor
performance, as even the best models (e.g. Qwen3-VL-235B) scores are capped at
around 60% on benchmarks like AndroidControl, far from viability for real-world
use. Our research reveals that issue lies not only with the models but with the
benchmarks themselves. We identified notable shortcomings in AndroidControl,
including ambiguities and factual errors, which systematically underrates agent
capabilities. To address this critical oversight, we enhanced AndroidControl
into AndroidControl-Curated, a refined version of the benchmark improved
through a rigorous purification pipeline. On this enhanced benchmark,
state-of-the-art models achieve success rates nearing 75% on complex tasks (15%
improvement), reflecting that on-device GUI agents are actually closer to
practical deployment than previously thought. We introduce our new SOTA model,
Magma-R1- 3B, post-trained on just 2.4k curated samples using 60 hours of an
H20 GPU (approximately $60). Despite being 200 times smaller in parameters,
this model delivers performance comparable to Qwen3- VL-235B. We release both
AndroidControl-Curated benchmark and Magma-R1 model to the research community,
encouraging adoption of this enhanced benchmark to better reflect model
capabilities and accelerate the development of robust, on-device virtual
assistants.

</details>


### [35] [Crucible: Quantifying the Potential of Control Algorithms through LLM Agents](https://arxiv.org/abs/2510.18491)
*Lianchen Jia,Chaoyang Li,Qian Houde,Tianchi Huang,Jiangchuan Liu,Lifeng Sun*

Main category: cs.AI

TL;DR: 介绍了用于评估算法调优潜力的Crucible代理，通过多案例研究和实际部署验证其有效性，能量化可调空间并改进算法性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注算法在理想或默认配置下的性能，忽略调优潜力，本文旨在填补这一空白。

Method: 引入Crucible代理，采用基于大语言模型的多级专家模拟来转换算法，并定义形式化指标定量评估调优潜力。

Result: 通过多案例研究和实际部署，验证Crucible能系统量化不同算法的可调空间。

Conclusion: Crucible为算法分析和设计提供新维度，最终可提升算法性能。

Abstract: Control algorithms in production environments typically require domain
experts to tune their parameters and logic for specific scenarios. However,
existing research predominantly focuses on algorithmic performance under ideal
or default configurations, overlooking the critical aspect of Tuning Potential.
To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven,
multi-level expert simulation to turn algorithms and defines a formalized
metric to quantitatively evaluate their Tuning Potential. We demonstrate
Crucible's effectiveness across a wide spectrum of case studies, from classic
control tasks to complex computer systems, and validate its findings in a
real-world deployment. Our experimental results reveal that Crucible
systematically quantifies the tunable space across different algorithms.
Furthermore, Crucible provides a new dimension for algorithm analysis and
design, which ultimately leads to performance improvements. Our code is
available at https://github.com/thu-media/Crucible.

</details>


### [36] [Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models](https://arxiv.org/abs/2510.18526)
*Hanze Guo,Jing Yao,Xiao Zhou,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 随着大语言模型融入多元应用，需使其与多元人类价值观对齐。现有方法有挑战，提出COUPLE框架，在两数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需与多元人类价值观对齐，但现有方法在处理细粒度价值目标时存在忽略价值复杂性和难以控制价值优先级的问题。

Method: 提出COUPLE框架，引入结构因果模型处理特征间复杂关系和因果联系，应用反事实推理生成符合期望价值目标的输出。

Result: 在两个不同价值体系的数据集上评估，COUPLE在不同类型价值目标上优于其他基线。

Conclusion: COUPLE框架能有效解决现有方法在多元价值观对齐中的问题，且具有更好的可解释性。

Abstract: As large language models (LLMs) become increasingly integrated into
applications serving users across diverse cultures, communities and
demographics, it is critical to align LLMs with pluralistic human values beyond
average principles (e.g., HHH). In psychological and social value theories such
as Schwartz's Value Theory, pluralistic values are represented by multiple
value dimensions paired with various priorities. However, existing methods
encounter two challenges when aligning with such fine-grained value objectives:
1) they often treat multiple values as independent and equally important,
ignoring their interdependence and relative priorities (value complexity); 2)
they struggle to precisely control nuanced value priorities, especially those
underrepresented ones (value steerability). To handle these challenges, we
propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE
alignment. It introduces a structural causal model (SCM) to feature complex
interdependency and prioritization among features, as well as the causal
relationship between high-level value dimensions and behaviors. Moreover, it
applies counterfactual reasoning to generate outputs aligned with any desired
value objectives. Benefitting from explicit causal modeling, COUPLE also
provides better interpretability. We evaluate COUPLE on two datasets with
different value systems and demonstrate that COUPLE advances other baselines
across diverse types of value objectives.

</details>


### [37] [Physics-guided Emulators Reveal Resilience and Fragility under Operational Latencies and Outages](https://arxiv.org/abs/2510.18535)
*Sarth Dubey,Subimal Ghosh,Udit Bhatia*

Main category: cs.AI

TL;DR: 开发GloFAS的可运行模拟器，在不同数据条件下评估其鲁棒性，为实时预报系统设计提供依据。


<details>
  <summary>Details</summary>
Motivation: 可靠的水文和洪水预报需要在输入数据有问题时仍稳定的模型，但现有降雨 - 径流预测研究多在理想数据条件下进行，强调准确性而非操作弹性。

Method: 开发耦合长短时记忆网络和松弛水平衡约束的GloFAS模拟器，设置五种信息可用性架构评估鲁棒性，在美国流域训练并在5000多个流域测试。

Result: 模拟器能重现GloFAS的水文核心，信息质量下降时性能平稳退化，跨不同水文气候和管理机制转移时性能降低但物理上一致。

Conclusion: 该框架将操作鲁棒性作为水文机器学习可衡量属性，推动可靠实时预报系统设计。

Abstract: Reliable hydrologic and flood forecasting requires models that remain stable
when input data are delayed, missing, or inconsistent. However, most advances
in rainfall-runoff prediction have been evaluated under ideal data conditions,
emphasizing accuracy rather than operational resilience. Here, we develop an
operationally ready emulator of the Global Flood Awareness System (GloFAS) that
couples long- and short-term memory networks with a relaxed water-balance
constraint to preserve physical coherence. Five architectures span a continuum
of information availability: from complete historical and forecast forcings to
scenarios with data latency and outages, allowing systematic evaluation of
robustness. Trained in minimally managed catchments across the United States
and tested in more than 5,000 basins, including heavily regulated rivers in
India, the emulator reproduces the hydrological core of GloFAS and degrades
smoothly as information quality declines. Transfer across contrasting
hydroclimatic and management regimes yields reduced yet physically consistent
performance, defining the limits of generalization under data scarcity and
human influence. The framework establishes operational robustness as a
measurable property of hydrological machine learning and advances the design of
reliable real-time forecasting systems.

</details>


### [38] [SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation](https://arxiv.org/abs/2510.18551)
*Yuncheng Hua,Sion Weatherhead,Mehdi Jafari,Hao Xue,Flora D. Salim*

Main category: cs.AI

TL;DR: 本文提出SOCIA - Nabla框架，通过损失驱动循环和文本梯度下降优化，在三个CPS任务中达到SOTA，将脆弱的提示管道转化为可扩展的模拟器代码生成。


<details>
  <summary>Details</summary>
Motivation: 构建一个能将模拟器构建视为文本计算图中代码实例优化的端到端框架，减少专家工作量，使代码可训练。

Method: 使用专门的大语言模型驱动的智能体作为图节点，工作流管理器执行损失驱动循环（代码合成 -> 执行 -> 评估 -> 代码修复），优化器进行文本梯度下降，有人在环交互用于任务规范确认。

Result: 在用户建模、口罩使用和个人出行三个CPS任务中达到了当前最优的整体准确率。

Conclusion: SOCIA - Nabla将多智能体编排与损失对齐优化相结合，把脆弱的提示管道转化为可跨领域和模拟粒度扩展的、可重现且有约束感知的模拟器代码生成。

Abstract: In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that
treats simulator construction asinstance optimization over code within a
textual computation graph. Specialized LLM-driven agents are embedded as graph
nodes, and a workflow manager executes a loss-driven loop: code synthesis ->
execution -> evaluation -> code repair. The optimizer performs Textual-Gradient
Descent (TGD), while human-in-the-loop interaction is reserved for task-spec
confirmation, minimizing expert effort and keeping the code itself as the
trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,
and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.
By unifying multi-agent orchestration with a loss-aligned optimization view,
SOCIA-Nabla converts brittle prompt pipelines into reproducible,
constraint-aware simulator code generation that scales across domains and
simulation granularities. This work is under review, and we will release the
code soon.

</details>


### [39] [Extracting alignment data in open models](https://arxiv.org/abs/2510.18554)
*Federico Barbero,Xiangming Gu,Christopher A. Choquette-Choo,Chawin Sitawarin,Matthew Jagielski,Itay Yona,Petar Veličković,Ilia Shumailov,Jamie Hayes*

Main category: cs.AI

TL;DR: 可从预训练模型提取大量对齐训练数据，用嵌入模型识别语义相似性更好，提取数据可恢复模型性能，工作揭示提取对齐数据风险并引发蒸馏实践下游影响讨论。


<details>
  <summary>Details</summary>
Motivation: 探索从预训练模型提取对齐训练数据以提升模型特定能力，如长上下文推理、安全性等。

Method: 使用高质量嵌入模型测量距离来识别字符串语义相似性，而非传统字符串匹配方法。

Result: 发现模型易吐出训练数据，提取的数据可用于训练基础模型，恢复一定性能；近似字符串匹配会严重低估可提取数据量。

Conclusion: 工作揭示提取对齐数据可能被忽视的风险，且蒸馏可视为间接在模型原始数据集上训练。

Abstract: In this work, we show that it is possible to extract significant amounts of
alignment training data from a post-trained model -- useful to steer the model
to improve certain capabilities such as long-context reasoning, safety,
instruction following, and maths. While the majority of related work on
memorisation has focused on measuring success of training data extraction
through string matching, we argue that embedding models are better suited for
our specific goals. Distances measured through a high quality embedding model
can identify semantic similarities between strings that a different metric such
as edit distance will struggle to capture. In fact, in our investigation,
approximate string matching would have severely undercounted (by a conservative
estimate of $10\times$) the amount of data that can be extracted due to trivial
artifacts that deflate the metric. Interestingly, we find that models readily
regurgitate training data that was used in post-training phases such as SFT or
RL. We show that this data can be then used to train a base model, recovering a
meaningful amount of the original performance. We believe our work exposes a
possibly overlooked risk towards extracting alignment data. Finally, our work
opens up an interesting discussion on the downstream effects of distillation
practices: since models seem to be regurgitating aspects of their training set,
distillation can therefore be thought of as indirectly training on the model's
original dataset.

</details>


### [40] [QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework](https://arxiv.org/abs/2510.18569)
*Junhyeog Yun,Hyoun Jun Lee,Insu Jeon*

Main category: cs.AI

TL;DR: 提出QuantEvolve框架用于自动化量化交易策略开发，结合质量 - 多样性优化与假设驱动策略生成，实证显示其优于传统基线并发布策略数据集。


<details>
  <summary>Details</summary>
Motivation: 动态市场中自动化量化交易策略开发具挑战性，现有方法难以探索策略空间并保持多样性。

Method: 提出QuantEvolve框架，结合质量 - 多样性优化与假设驱动策略生成，采用与投资者偏好对齐的特征图，集成假设驱动多智能体系统迭代探索策略空间。

Result: QuantEvolve产生适应市场变化和个人投资需求的多样化策略，实证显示其优于传统基线。

Conclusion: QuantEvolve有效，发布数据集支持未来研究。

Abstract: Automating quantitative trading strategy development in dynamic markets is
challenging, especially with increasing demand for personalized investment
solutions. Existing methods often fail to explore the vast strategy space while
preserving the diversity essential for robust performance across changing
market conditions. We present QuantEvolve, an evolutionary framework that
combines quality-diversity optimization with hypothesis-driven strategy
generation. QuantEvolve employs a feature map aligned with investor
preferences, such as strategy type, risk profile, turnover, and return
characteristics, to maintain a diverse set of effective strategies. It also
integrates a hypothesis-driven multi-agent system to systematically explore the
strategy space through iterative generation and evaluation. This approach
produces diverse, sophisticated strategies that adapt to both market regime
shifts and individual investment needs. Empirical results show that QuantEvolve
outperforms conventional baselines, validating its effectiveness. We release a
dataset of evolved strategies to support future research.

</details>


### [41] [VAR: Visual Attention Reasoning via Structured Search and Backtracking](https://arxiv.org/abs/2510.18619)
*Wei Cai,Jian Zhao,Yuchen Yuan,Tianle Zhang,Ming Zhu,Haichuan Tang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出Visual Attention Reasoning (VAR)框架解决多模态大语言模型的局限，理论验证且实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在高幻觉倾向和依赖线性推理的问题，在复杂任务中易失败。

Method: 引入VAR框架，将推理过程分解为可追溯证据锚定和基于搜索的思维链生成，结合回溯机制，用多方面奖励函数引导搜索。

Result: 7B模型VAR - 7B在幻觉和安全基准测试中创佳绩，超越开源模型，与领先闭源系统竞争。

Conclusion: VAR框架能有效解决多模态大语言模型的局限，提升性能。

Abstract: Multimodal Large Language Models (MLLMs), despite their advances, are
hindered by their high hallucination tendency and heavy reliance on brittle,
linear reasoning processes, leading to failures in complex tasks. To address
these limitations, we introduce Visual Attention Reasoning (VAR), a novel
framework that recasts grounded reasoning as a structured search over a
reasoning trajectory space. VAR decomposes the reasoning process into two key
stages: traceable evidence grounding and search-based chain-of-thought (CoT)
generation, which incorporates a backtracking mechanism for self-correction.
The search is guided by a multi-faceted reward function with semantic and
geometric self-verification components, which penalize outputs that are not
faithfully grounded in the visual input. We provide a theoretical analysis for
our search strategy, validating its capability to find the correct solution
with high probability. Experimental results show that our 7B model, VAR-7B,
sets a new state-of-the-art on a comprehensive suite of hallucination and
safety benchmarks, significantly outperforming existing open-source models and
demonstrating competitive performance against leading proprietary systems.

</details>


### [42] [Leveraging Association Rules for Better Predictions and Better Explanations](https://arxiv.org/abs/2510.18628)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.AI

TL;DR: 提出结合数据与知识的分类新方法，实验表明对树模型在预测性能和解释规模上有益。


<details>
  <summary>Details</summary>
Motivation: 提升树基模型的预测性能以及改进分类任务的解释工作。

Method: 利用数据挖掘从数据中推导关联规则，用这些规则提升树基模型性能和生成更通用的溯因解释。

Result: 该方法能为所考虑的两种树基模型在预测性能和解释规模方面带来益处。

Conclusion: 结合数据与知识的分类新方法是有效的。

Abstract: We present a new approach to classification that combines data and knowledge.
In this approach, data mining is used to derive association rules (possibly
with negations) from data. Those rules are leveraged to increase the predictive
performance of tree-based models (decision trees and random forests) used for a
classification task. They are also used to improve the corresponding
explanation task through the generation of abductive explanations that are more
general than those derivable without taking such rules into account.
Experiments show that for the two tree-based models under consideration,
benefits can be offered by the approach in terms of predictive performance and
in terms of explanation sizes.

</details>


### [43] [Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises](https://arxiv.org/abs/2510.18631)
*Carlo Proietti,Antonio Yuste-Ginel*

Main category: cs.AI

TL;DR: 研究形式论证中定性不确定性建模，提出表达性概念并给出相关结果


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注抽象模型，需研究抽象模型的合理实例化

Method: 将论证的不确定性基于规则和前提中的组件

Result: 引入可处理抽象和结构化形式主义的表达性概念，给出正负表达性结果

Conclusion: 研究结果影响抽象论证框架及其扩展以及结构化的ASPIC+

Abstract: Modelling qualitative uncertainty in formal argumentation is essential both
for practical applications and theoretical understanding. Yet, most of the
existing works focus on \textit{abstract} models for arguing with uncertainty.
Following a recent trend in the literature, we tackle the open question of
studying plausible instantiations of these abstract models. To do so, we ground
the uncertainty of arguments in their components, structured within rules and
premises. Our main technical contributions are: i) the introduction of a notion
of expressivity that can handle abstract and structured formalisms, and ii) the
presentation of both negative and positive expressivity results, comparing the
expressivity of abstract and structured models of argumentation with
uncertainty. These results affect incomplete abstract argumentation frameworks,
and their extension with dependencies, on the abstract side, and ASPIC+, on the
structured side.

</details>


### [44] [Query Decomposition for RAG: Balancing Exploration-Exploitation](https://arxiv.org/abs/2510.18633)
*Roxana Petcu,Kenton Murray,Daniel Khashabi,Evangelos Kanoulas,Maarten de Rijke,Dawn Lawrie,Kevin Duh*

Main category: cs.AI

TL;DR: 本文研究检索增强生成系统的文档选择问题，提出在利用 - 探索框架下进行查询分解和文档检索，实验表明特定方法能提升文档选择性能。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成系统在选择信息文档时需平衡检索范围和噪声、计算成本的权衡。

Method: 在利用 - 探索框架下进行查询分解和文档检索，实验采用多种多臂老虎机学习方法。

Result: 使用排名信息和人类判断估计文档相关性，文档级精度提高 35%，α - nDCG 增加 15%，长文本生成下游任务表现更好。

Conclusion: 所提出的方法能有效动态选择最具信息性的子查询，提升文档选择性能。

Abstract: Retrieval-augmented generation (RAG) systems address complex user requests by
decomposing them into subqueries, retrieving potentially relevant documents for
each, and then aggregating them to generate an answer. Efficiently selecting
informative documents requires balancing a key trade-off: (i) retrieving
broadly enough to capture all the relevant material, and (ii) limiting
retrieval to avoid excessive noise and computational cost. We formulate query
decomposition and document retrieval in an exploitation-exploration setting,
where retrieving one document at a time builds a belief about the utility of a
given sub-query and informs the decision to continue exploiting or exploring an
alternative. We experiment with a variety of bandit learning methods and
demonstrate their effectiveness in dynamically selecting the most informative
sub-queries. Our main finding is that estimating document relevance using rank
information and human judgments yields a 35% gain in document-level precision,
15% increase in {\alpha}-nDCG, and better performance on the downstream task of
long-form generation.

</details>


### [45] [Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval](https://arxiv.org/abs/2510.18659)
*Dong Yun,Marco Schouten,Dim Papadopoulos*

Main category: cs.AI

TL;DR: 提出SherlockLLM对话驱动检索框架，用强化学习学习最优提问策略，经实验验证其高效稳健。


<details>
  <summary>Details</summary>
Motivation: 现有对话式交互检索系统缺乏明确策略问最有信息价值的问题，效率低。

Method: 提出SherlockLLM框架，用强化学习训练代理生成二进制问题序列缩小搜索空间，引入含结构化和非结构化任务的基准。

Result: 在结构化任务中性能与强基线匹配，接近理论最优；在非结构化任务中显著超越基线。

Conclusion: SherlockLLM是一个稳健且高效的解决方案，能学习有效的信息寻求对话策略。

Abstract: User queries in information retrieval are often ambiguous, making it
challenging for systems to identify a user's target from a single query. While
recent dialogue-based interactive retrieval systems can clarify user intent,
they are inefficient as they often lack an explicit strategy to ask the most
informative questions. To address this limitation, we propose SherlockLLM, a
dialogue-driven retrieval framework that learns an optimal questioning strategy
via Reinforcement Learning (RL) and avoids the need for large-scale annotated
dialogue data. In our framework, an agent is trained to generate a sequence of
binary questions to efficiently narrow down the search space. To validate our
approach, we introduce a benchmark with both structured and unstructured tasks.
Experimental results show that SherlockLLM is a robust and efficient solution.
On the structured tasks, its performance matches strong baselines and
approaches the theoretical optimal defined by binary search. On the challenging
unstructured task, our agent significantly outperforms these baselines,
showcasing its ability to learn a highly effective information-seeking dialogue
policy.

</details>


### [46] [Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation](https://arxiv.org/abs/2510.18751)
*Patterson Hsieh,Jerry Yeh,Mao-Chi He,Wen-Han Hsieh,Elvis Hsieh*

Main category: cs.AI

TL;DR: 本文介绍用于有害藻华（HAB）监测的ALGOS系统，结合遥感图像理解与严重程度估计，实验表明其在分割和严重程度估计上表现良好。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧HAB发生，传统监测方法劳动密集且时空覆盖有限，现有遥感视觉语言模型在图像推理和量化藻华严重程度方面存在挑战。

Method: 引入ALGOS系统，整合GeoSAM辅助的人工评估进行高质量分割掩码整理，并使用NASA的CAML数据微调视觉语言模型进行严重程度预测。

Result: ALGOS在分割和严重程度估计上都取得了稳健的性能。

Conclusion: 为实用和自动化的蓝藻监测系统铺平了道路。

Abstract: Climate change is intensifying the occurrence of harmful algal bloom (HAB),
particularly cyanobacteria, which threaten aquatic ecosystems and human health
through oxygen depletion, toxin release, and disruption of marine biodiversity.
Traditional monitoring approaches, such as manual water sampling, remain
labor-intensive and limited in spatial and temporal coverage. Recent advances
in vision-language models (VLMs) for remote sensing have shown potential for
scalable AI-driven solutions, yet challenges remain in reasoning over imagery
and quantifying bloom severity. In this work, we introduce ALGae Observation
and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB
monitoring that combines remote sensing image understanding with severity
estimation. Our approach integrates GeoSAM-assisted human evaluation for
high-quality segmentation mask curation and fine-tunes vision language model on
severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)
from NASA. Experiments demonstrate that ALGOS achieves robust performance on
both segmentation and severity-level estimation, paving the way toward
practical and automated cyanobacterial monitoring systems.

</details>


### [47] [Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location](https://arxiv.org/abs/2510.18803)
*Shirin Tavakoli Kafiabad,Andrea Schiffauerova,Ashkan Ebadi*

Main category: cs.AI

TL;DR: 分析加拿大NSERC资助研究提案，对比三种主题建模方法，提出COFFEE算法，发现BERTopic表现佳，分析揭示研究差异，为资助策略提供依据。


<details>
  <summary>Details</summary>
Motivation: 为优化国家科研投资，需了解研究趋势及相关影响因素，开展此研究。

Method: 分析18年NSERC资助的研究提案，对比LDA、STM和BERTopic三种主题建模方法，提出COFFEE算法用于BERTopic协变量分析。

Result: BERTopic能识别更细致主题，COFFEE算法分析确认省级研究专长和性别主题模式。

Conclusion: 研究结果为资助机构制定公平有效的资助策略提供实证基础，提升科研生态有效性。

Abstract: Optimizing national scientific investment requires a clear understanding of
evolving research trends and the demographic and geographical forces shaping
them, particularly in light of commitments to equity, diversity, and inclusion.
This study addresses this need by analyzing 18 years (2005-2022) of research
proposals funded by the Natural Sciences and Engineering Research Council of
Canada (NSERC). We conducted a comprehensive comparative evaluation of three
topic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic
Modelling (STM), and BERTopic. We also introduced a novel algorithm, named
COFFEE, designed to enable robust covariate effect estimation for BERTopic.
This advancement addresses a significant gap, as BERTopic lacks a native
function for covariate analysis, unlike the probabilistic STM. Our findings
highlight that while all models effectively delineate core scientific domains,
BERTopic outperformed by consistently identifying more granular, coherent, and
emergent themes, such as the rapid expansion of artificial intelligence.
Additionally, the covariate analysis, powered by COFFEE, confirmed distinct
provincial research specializations and revealed consistent gender-based
thematic patterns across various scientific disciplines. These insights offer a
robust empirical foundation for funding organizations to formulate more
equitable and impactful funding strategies, thereby enhancing the effectiveness
of the scientific ecosystem.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [48] [Regional heterogeneity in left atrial stiffness impacts passive deformation in a cohort of patient-specific models](https://arxiv.org/abs/2510.18642)
*Tiffany MG Baptiste,Cristobal Rodero,Charles P Sillett,Marina Strocchi,Christopher W Lanyon,Christoph M Augustin,Angela WC Lee,José Alonso Solís-Lemus,Caroline H Roney,Daniel B Ennis,Ronak Rajani,Christopher A Rinaldi,Gernot Plank,Richard D Wilkinson,Steven E Williams,Steven A Niederer*

Main category: cs.CE

TL;DR: 创建左心房特定患者模型，发现心肌刚度区域差异影响左心房生物力学，为房颤时左心房生物力学改变原因提供见解。


<details>
  <summary>Details</summary>
Motivation: 房颤时心房生物力学改变，但原因不明，需创建特定患者左心房模型理解相关因素与心房生物力学的联系。

Method: 从CT图像创建特定患者左心房模型，拟合区域模型刚度，用CT变形瞬态验证模型。

Result: 心肌刚度在左心房各区域不同，区域刚度值是影响左心房区域生理变形的重要因素，心房解剖特征影响较小。

Conclusion: 研究结果有助于深入了解房颤时左心房生物力学改变的潜在原因。

Abstract: The deformation of the left atrium (LA), or its biomechanical function, is
closely linked to the health of this cardiac chamber. In atrial fibrillation
(AF), atrial biomechanics are significantly altered but the underlying cause of
this change is not always clear. Patient-specific models of the LA that
replicate patient atrial motion can allow us to understand how factors such as
atrial anatomy, myocardial stiffness and physiological constraints are linked
to atrial biomechanics. We created patient-specific LA models from CT images.
We fitted regional model stiffness to peak CT-derived deformation during the LA
reservoir phase ($\pm0.90$ mm) and used the CT deformation transients through
the reservoir and conduit phase for model validation (deformation transients
fell within $\pm0.38$ mm per unit time of targets). We found that myocardial
stiffness varies regionally across the LA. The regional stiffness values were
significant factors contributing to regional physiological LA deformation
($p=0.023$) while features of LA anatomy, including regional wall thickness and
adipose volume, were less important. These findings provide insight into the
underlying causes of altered LA biomechanics in AF.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [49] [DynaQuery: A Self-Adapting Framework for Querying Structured and Multimodal Data](https://arxiv.org/abs/2510.18029)
*Aymane Hassini*

Main category: cs.DB

TL;DR: 介绍DynaQuery框架应对大语言模型在复杂混合数据库自然语言查询的挑战，评估显示其比RAG范式更稳健，发现从模式感知到语义感知的泛化原则。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂混合数据库自然语言查询面临联合处理结构化模式和非结构化资产语义内容的挑战，需解决该问题。

Method: 提出DynaQuery框架，核心是SILE引擎，对其进行多基准实证评估并与RAG范式对比。

Result: RAG范式易出现灾难性上下文失败，SILE设计更稳健，几乎消除失败模式，在新基准上发现从模式感知到语义感知的泛化原则。

Conclusion: 研究为开发稳健、自适应和一致的自然语言数据库接口提供了有效的架构基础。

Abstract: The rise of Large Language Models (LLMs) has accelerated the long-standing
goal of enabling natural language querying over complex, hybrid databases. Yet,
this ambition exposes a dual challenge: reasoning jointly over structured,
multi-relational schemas and the semantic content of linked unstructured
assets. To overcome this, we present DynaQuery - a unified, self-adapting
framework that serves as a practical blueprint for next-generation "Unbound
Databases." At the heart of DynaQuery lies the Schema Introspection and Linking
Engine (SILE), a novel systems primitive that elevates schema linking to a
first-class query planning phase. We conduct a rigorous, multi-benchmark
empirical evaluation of this structure-aware architecture against the prevalent
unstructured Retrieval-Augmented Generation (RAG) paradigm. Our results
demonstrate that the unstructured retrieval paradigm is architecturally
susceptible to catastrophic contextual failures, such as SCHEMA_HALLUCINATION,
leading to unreliable query generation. In contrast, our SILE-based design
establishes a substantially more robust foundation, nearly eliminating this
failure mode. Moreover, end-to-end validation on a complex, newly curated
benchmark uncovers a key generalization principle: the transition from pure
schema-awareness to holistic semantics-awareness. Taken together, our findings
provide a validated architectural basis for developing natural language
database interfaces that are robust, adaptable, and predictably consistent.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [50] [Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis](https://arxiv.org/abs/2510.17852)
*Yuze Sun,Wentao Luo,Yanfei Xiang,Jiancheng Pan,Jiahao Li,Quan Zhang,Xiaomeng Huang*

Main category: cs.DC

TL;DR: 本文提出将大气和海洋模型从PyTorch迁移到MindSpore并针对中国芯片优化的框架，实验表明能保持精度、减少依赖、提高效率，为国产芯片和框架在相关模型开发提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前大气和海洋模型依赖GPU，缺乏硬件独立性，尤其是对中国国产硬件和框架的支持。

Method: 提出将大规模大气和海洋模型从PyTorch迁移到MindSpore并针对中国芯片优化的框架，重点关注软硬件适配、内存优化和并行性，并从多个指标评估模型性能。

Result: 迁移和优化过程能保持模型原有精度，显著减少系统依赖，提高运行效率。

Conclusion: 为在大气和海洋AI模型开发中利用中国国产芯片和框架提供了有价值的见解和实践指导，有助于实现技术独立。

Abstract: With the growing role of artificial intelligence in climate and weather
research, efficient model training and inference are in high demand. Current
models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware
independence, especially for Chinese domestic hardware and frameworks. To
address this issue, we present a framework for migrating large-scale
atmospheric and oceanic models from PyTorch to MindSpore and optimizing for
Chinese chips, and evaluating their performance against GPUs. The framework
focuses on software-hardware adaptation, memory optimization, and parallelism.
Furthermore, the model's performance is evaluated across multiple metrics,
including training speed, inference speed, model accuracy, and energy
efficiency, with comparisons against GPU-based implementations. Experimental
results demonstrate that the migration and optimization process preserves the
models' original accuracy while significantly reducing system dependencies and
improving operational efficiency by leveraging Chinese chips as a viable
alternative for scientific computing. This work provides valuable insights and
practical guidance for leveraging Chinese domestic chips and frameworks in
atmospheric and oceanic AI model development, offering a pathway toward greater
technological independence.

</details>


### [51] [Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation](https://arxiv.org/abs/2510.18152)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.DC

TL;DR: 本文提出用于分布式群体学习的DSL - OTA方法，理论和仿真验证其在收敛速度、通信成本和学习性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有分布式学习系统中，有限传输资源和复杂通信环境是边缘设备高效协作的瓶颈。

Method: 提出基于分布式群体学习的OTA模拟聚合方法DSL - OTA，并结合多工作者选择策略。

Result: 理论分析验证了DSL - OTA算法收敛速度快、通信成本低；仿真显示在同构和异构数据集设置下学习性能优于现有方法。

Conclusion: DSL - OTA方法能提升通信效率、实现有效协作并保护隐私。

Abstract: Recent advances in distributed learning systems have introduced effective
solutions for implementing collaborative artificial intelligence techniques in
wireless communication networks. Federated learning approaches provide a
model-aggregation mechanism among edge devices to achieve collaborative
training, while ensuring data security, communication efficiency, and sharing
computational overheads. On the other hand, limited transmission resources and
complex communication environments remain significant bottlenecks to the
efficient collaborations among edge devices, particularly within large-scale
networks. To address such issues, this paper proposes an over-the-air (OTA)
analog aggregation method designed for the distributed swarm learning (DSL),
termed DSL-OTA, aiming to enhance communication efficiency, enable effective
cooperation, and ensure privacy preserving. Incorporating multi-worker
selection strategy with over-the-air aggregation not only makes the standard
DSL based on single best worker contributing to global model update to become
more federated, but also secures the aggregation from potential risks of data
leakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA
algorithm in terms of fast convergence rate and low communication costs.
Simulation results reveal that our DSL-OTA outperforms the other existing
methods by achieving better learning performance under both homogeneous and
heterogeneous dataset settings.

</details>


### [52] [A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces](https://arxiv.org/abs/2510.18300)
*Ankur Lahiry,Ayush Pokharel,Banooqa Banday,Seth Ockerman,Amal Gueroudji,Mohammad Zaeed,Tanzima Z. Islam,Line Pouchard*

Main category: cs.DC

TL;DR: 提出端到端并行性能分析框架处理大规模GPU跟踪数据，实验显示可提升67%可扩展性。


<details>
  <summary>Details</summary>
Motivation: 单条GPU跟踪数据量大且复杂，使性能分析计算成本高、耗时长。

Method: 提出端到端并行性能分析框架，并发分区和处理跟踪数据，采用因果图方法和并行协调图。

Result: 实验表明在可扩展性方面提升67%。

Conclusion: 所提框架能有效独立分析多条跟踪数据。

Abstract: Large-scale GPU traces play a critical role in identifying performance
bottlenecks within heterogeneous High-Performance Computing (HPC)
architectures. However, the sheer volume and complexity of a single trace of
data make performance analysis both computationally expensive and
time-consuming. To address this challenge, we present an end-to-end parallel
performance analysis framework designed to handle multiple large-scale GPU
traces efficiently. Our proposed framework partitions and processes trace data
concurrently and employs causal graph methods and parallel coordinating chart
to expose performance variability and dependencies across execution flows.
Experimental results demonstrate a 67% improvement in terms of scalability,
highlighting the effectiveness of our pipeline for analyzing multiple traces
independently.

</details>


### [53] [SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices](https://arxiv.org/abs/2510.18544)
*Pan Zhou,Yiming Lei,Ling Liu,Xiaoqiong Xu,Ying Cai,Daji Ergu,Hongfang Yu,Yueyue Dai*

Main category: cs.DC

TL;DR: 论文指出现有调度系统未满足边缘设备对大语言模型服务的差异化SLO需求，提出SLICE方案，实验显示其比现有方案有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有调度服务系统以最大化输出令牌吞吐量为唯一优化目标，无法满足边缘设备对大语言模型服务的差异化SLO需求，导致高违规率。

Method: 提出SLICE方案，结合效用最大化的请求调度算法和生成速率的动态迭代控制机制。

Result: 与Orca和FastServe相比，SLICE的SLO达成率最高提升35倍，任务完成时间有3.4倍优势。

Conclusion: SLICE能显著提高大语言模型推理服务的SLO达成率。

Abstract: Large Language Models (LLMs), as the foundational architecture for
next-generation interactive AI applications, not only power intelligent
dialogue systems but also drive the evolution of embodied intelligence on edge
devices, including humanoid robots, smart vehicles, and other scenarios. The
applications running on these edge devices impose differentiated Service Level
Objectives (SLO) requirements on LLM services, specifically manifested as
distinct constraints on Time to First Token (TTFT) and Time Per Output Token
(TPOT) as well as end-to-end latency. Notably, edge devices typically handle
real-time tasks that are extremely sensitive to latency, such as machine
control and navigation planning. However, existing scheduling service systems
still prioritize maximizing output token throughput as the sole optimization
objective, failing to adequately address the diversity of SLO requirements.
This ultimately results in persistently high violation rates for end-to-end
latency or TPOT related SLOs.
  This paper proposes SLICE, an innovative scheduling solution designed for
edge computing scenarios with differentiated SLO requirements. By combining a
utility-maximizing request scheduling algorithm with a dynamic iterative
control mechanism for generation rates, SLICE significantly improves LLM
inference service SLO attainment. Experimental results demonstrate that
compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to
35x higher SLO attainment and 3.4x advantage in task completion time than the
other two solutions.

</details>


### [54] [Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications](https://arxiv.org/abs/2510.18586)
*Zhuohang Bian,Feiyang Wu,Teng Ma,Youwei Zhuo*

Main category: cs.DC

TL;DR: 提出Tokencake框架优化多智能体应用中KV缓存性能，评估显示可降低延迟、提高内存利用率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多智能体应用中使用外部函数调用时，KV缓存面临空间争用和时间利用不足的性能挑战。

Method: 设计以KV缓存为中心的服务框架Tokencake，空间调度器用动态内存分区，时间调度器采用主动卸载和预测上传机制。

Result: 在多智能体基准测试中，与vLLM相比，Tokencake可降低端到端延迟超47.06%，提高有效GPU内存利用率达16.9%。

Conclusion: Tokencake能有效优化多智能体应用中KV缓存的性能。

Abstract: Large Language Models (LLMs) are increasingly deployed in complex multi-agent
applications that use external function calls. This workload creates severe
performance challenges for the KV Cache: space contention leads to the eviction
of critical agents' caches and time underutilization leaves the cache of agents
stalled on long-running tool calls idling in GPU memory. We present Tokencake,
a KV-Cache-centric serving framework that co-optimizes scheduling and memory
management with an agent-aware design. Tokencake's Space Scheduler uses dynamic
memory partitioning to shield critical agents from contention, while its Time
Scheduler employs a proactive offload and predictive upload mechanism to
repurpose GPU memory during function call stalls. Our evaluation on
representative multi-agent benchmarks shows that Tokencake can reduce
end-to-end latency by over 47.06%, improve effective GPU memory utilization by
up to 16.9% compared to vLLM.

</details>


### [55] [Distributed Interactive Proofs for Planarity with Log-Star Communication](https://arxiv.org/abs/2510.18592)
*Yuval Gil,Merav Parter*

Main category: cs.DC

TL;DR: 本文提出了用于平面性的新的通信高效分布式交互证明，给出不同轮数下嵌入式平面性和平面性的DIP协议及证明大小。


<details>
  <summary>Details</summary>
Motivation: 分布式交互证明（DIP）衡量标准为所需的证明者 - 验证者通信量，目标是设计交互轮数少、证明大小小的DIP。

Method: 未明确提及具体方法，主要设计了不同轮数的DIP协议。

Result: 得到O(log *n)轮嵌入式平面性和平面性的DIP协议，证明大小分别为O(1)和O(⌈log Δ/log *n⌉)；可推广到任意1 ≤ r ≤ log *n的情况。

Conclusion: 成功设计出通信高效的分布式交互证明协议用于平面性验证。

Abstract: We provide new communication-efficient distributed interactive proofs for
planarity. The notion of a \emph{distributed interactive proof (DIP)} was
introduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \emph{prover}
is a single centralized entity whose goal is to prove a certain claim regarding
an input graph $G$. To do so, the prover communicates with a distributed
\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is
measured by the amount of prover-verifier communication it requires. Namely,
the goal is to design a DIP with a small number of interaction rounds and a
small \emph{proof size}, i.e., a small amount of communication per round. Our
main result is an $O(\log ^{*}n)$-round DIP protocol for embedded planarity and
planarity with a proof size of $O(1)$ and $O(\lceil\log \Delta/\log
^{*}n\rceil)$, respectively. In fact, this result can be generalized as
follows. For any $1\leq r\leq \log^{*}n$, there exists an $O(r)$-round protocol
for embedded planarity and planarity with a proof size of $O(\log ^{(r)}n)$ and
$O(\log ^{(r)}n+\log \Delta /r)$, respectively.

</details>


### [56] [Towards an Optimized Benchmarking Platform for CI/CD Pipelines](https://arxiv.org/abs/2510.18640)
*Nils Japke,Sebastian Koch,Helmut Lukasczyk,David Bermbach*

Main category: cs.DC

TL;DR: 性能回归早期检测重要，但性能基准测试资源和时间成本高，现有基准优化技术难集成到CI/CD管道，本文指出三个挑战并提出概念框架，旨在推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 解决大规模软件系统性能回归早期检测中，性能基准测试资源和时间成本高，且现有优化技术难集成到CI/CD管道的问题。

Method: 识别使频繁高效基准测试的三个核心挑战，并引入基于云的概念基准测试框架。

Result: 提出了基于云的概念基准测试框架来处理相关挑战。

Conclusion: 旨在激发研究，使CI/CD系统中性能回归检测更实用有效。

Abstract: Performance regressions in large-scale software systems can lead to
substantial resource inefficiencies, making their early detection critical.
Frequent benchmarking is essential for identifying these regressions and
maintaining service-level agreements (SLAs). Performance benchmarks, however,
are resource-intensive and time-consuming, which is a major challenge for
integration into Continuous Integration / Continuous Deployment (CI/CD)
pipelines. Although numerous benchmark optimization techniques have been
proposed to accelerate benchmark execution, there is currently no practical
system that integrates these optimizations seamlessly into real-world CI/CD
pipelines. In this vision paper, we argue that the field of benchmark
optimization remains under-explored in key areas that hinder its broader
adoption. We identify three central challenges to enabling frequent and
efficient benchmarking: (a) the composability of benchmark optimization
strategies, (b) automated evaluation of benchmarking results, and (c) the
usability and complexity of applying these strategies as part of CI/CD systems
in practice. We also introduce a conceptual cloud-based benchmarking framework
handling these challenges transparently. By presenting these open problems, we
aim to stimulate research toward making performance regression detection in
CI/CD systems more practical and effective.

</details>


### [57] [PCMS: Parallel Coupler For Multimodel Simulations](https://arxiv.org/abs/2510.18838)
*Jacob S. Merson,Cameron W. Smith,Mark S. Shephard,Fuad Hasan,Abhiyan Paudel,Angel Castillo-Crooke,Joyal Mathew,Mohammad Elahi*

Main category: cs.DC

TL;DR: 本文提出GPU加速的多模型模拟并行耦合器PCMS，介绍其方法并通过实例展示，还在Frontier上演示弱缩放。


<details>
  <summary>Details</summary>
Motivation: 开发新的GPU加速广义耦合框架，用于在领先级超级计算机上耦合模拟代码。

Method: 采用分布式控制和最多五维的场映射方法，场映射可利用离散化和场信息适应物理约束。

Result: 用XGC与DEGAS2、GNET与GTC的耦合进行演示，在Frontier的2080个GPU上进行弱缩放演示，效率达85%。

Conclusion: PCMS是一个有效的多模型模拟耦合框架，具有良好的弱缩放性能。

Abstract: This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a
new GPU accelerated generalized coupling framework for coupling simulation
codes on leadership class supercomputers. PCMS includes distributed control and
field mapping methods for up to five dimensions. For field mapping PCMS can
utilize discretization and field information to accommodate physics
constraints. PCMS is demonstrated with a coupling of the gyrokinetic
microturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and
with a 5D distribution function coupling of an energetic particle transport
code (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also
demonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of
85%.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [58] [Assignment-Routing Optimization with Cutting-Plane Subtour Elimination: Solver and Benchmark Dataset](https://arxiv.org/abs/2510.17888)
*Qilong Yuan*

Main category: cs.DS

TL;DR: 研究联合路由分配优化问题，提出用精确MIP公式结合Gurobi求解方法，分析计算局限性并提供数据作基准。


<details>
  <summary>Details</summary>
Motivation: 解决联合路由分配优化问题，最小化总旅行成本。

Method: 使用精确MIP公式结合Gurobi求解，包括割平面子回路消除。

Result: 分析了该方法随问题规模增长的计算局限性。

Conclusion: 数据集、公式和实验结果可作为该领域未来研究的基准。

Abstract: We study a joint routing-assignment optimization problem in which a set of
items must be paired one-to-one with a set of placeholders while simultaneously
determining a Hamiltonian cycle that visits every node exactly once. Both the
assignment and routing decisions are optimized jointly to minimize the total
travel cost. In this work, we propose a method to solve this problem using an
exact MIP formulation with Gurobi, including cutting-plane subtour elimination.
With analysis of the computational complexity and through extensive
experiments, we analyze the computational limitations of this approach as the
problem size grows and reveal the challenges associated with the need for more
efficient algorithms for larger instances. The dataset, formulations, and
experimental results provided here can serve as benchmarks for future studies
in this research area. GitHub repository:
https://github.com/QL-YUAN/Joint-Assignment-Routing-Optimization

</details>


### [59] [Online Randomness Extraction: Simulating Barely Random Algorithms in the Random Order Arrival Model](https://arxiv.org/abs/2510.18049)
*Allan Borodin,Christodoulos Karavasilis,David Zhang*

Main category: cs.DS

TL;DR: 研究利用随机顺序到达提取随机比特以去随机化算法，考虑三种1 - 比特随机提取过程，给出最佳过程的偏差并应用于多个问题，还探讨随机算法与确定性ROM算法的优劣关系。


<details>
  <summary>Details</summary>
Motivation: 受随机顺序模型（ROM）启发，旨在利用随机顺序到达提取随机比特去随机化算法，同时研究随机在线算法和确定性ROM算法的相对强度，还想知道是否有随机算法优于确定性ROM算法的应用。

Method: 考虑三种1 - 比特随机提取过程。

Result: 最佳提取过程返回比特的最坏情况偏差约为0.585，该过程可应用于加权区间选择、背包问题、二进制字符串猜测和无权重作业吞吐量调度等问题。

Conclusion: 随机比特提取应用是理解随机在线算法和确定性ROM算法关系的一种建设性方法。

Abstract: Interest in the random order model (ROM) leads us to initiate a study of
utilizing random-order arrivals to extract random bits with the goal of
de-randomizing algorithms. Besides producing simple algorithms, simulating
random bits through random arrivals enhances our understanding of the
comparative strength of randomized online algorithms (with adversarial input
sequence) and deterministic algorithms in the ROM. We consider three $1$-bit
randomness extraction processes. Our best extraction process returns a bit with
a worst-case bias of $2 - \sqrt{2} \approx 0.585$ and operates under the mild
assumption that there exist at least two distinct items in the input. We
motivate the applicability of this process by using it to simulate a number of
barely random algorithms for weighted interval selection (single-length
arbitrary weights, as well as monotone, C-benevolent and D-benevolent weighted
instances), the proportional and general knapsack problems, binary string
guessing, and unweighted job throughput scheduling.
  It is well known that there are many applications where a deterministic ROM
algorithm significantly outperforms any randomized online algorithm (in terms
of competitive ratios). The classic example is that of the secretary problem.
We ask the following fundamental question: Is there any application for which a
randomized algorithm outperforms any deterministic ROM algorithm? Motivated by
this question, we view our randomness extraction applications as a constructive
approach towards understanding the relation between randomized online
algorithms and deterministic ROM algorithms.

</details>


### [60] [Fast Agnostic Learners in the Plane](https://arxiv.org/abs/2510.18057)
*Talya Eden,Ludmila Glinskih,Sofya Raskhodnikova*

Main category: cs.DS

TL;DR: 研究平面上几个基本几何概念类的不可知学习计算效率，改进多个类别的学习算法运行时间，提出的学习者可产生匹配运行时间的容错属性测试器，引发复杂度差距问题思考。


<details>
  <summary>Details</summary>
Motivation: 样本复杂度已被充分理解，但不可知学习的时间复杂度较少受关注，故研究相关几何概念类的不可知学习计算效率。

Method: 使用计算几何的数据结构和算法，分析依赖几何与概率组合学工具。

Result: 改进三角形、4 - 边形、5 - 边形和正方形上凸集的不可知学习算法运行时间，提出的学习者可产生匹配运行时间的容错属性测试器。

Conclusion: 不可知学习这些自然概念类时，样本复杂度和时间复杂度之间是否存在固有差距是一个待研究的基本问题。

Abstract: We investigate the computational efficiency of agnostic learning for several
fundamental geometric concept classes in the plane. While the sample complexity
of agnostic learning is well understood, its time complexity has received much
less attention. We study the class of triangles and, more generally, the class
of convex polygons with $k$ vertices for small $k$, as well as the class of
convex sets in a square. We present a proper agnostic learner for the class of
triangles that has optimal sample complexity and runs in time $\tilde
O({\epsilon^{-6}})$, improving on the algorithm of Dobkin and Gunopulos (COLT
`95) that runs in time $\tilde O({\epsilon^{-10}})$. For 4-gons and 5-gons, we
improve the running time from $O({\epsilon^{-12}})$, achieved by Fischer and
Kwek (eCOLT `96), to $\tilde O({\epsilon^{-8}})$ and $\tilde
O({\epsilon^{-10}})$, respectively.
  We also design a proper agnostic learner for convex sets under the uniform
distribution over a square with running time $\tilde O({\epsilon^{-5}})$,
improving on the previous $\tilde O(\epsilon^{-8})$ bound at the cost of
slightly higher sample complexity. Notably, agnostic learning of convex sets in
$[0,1]^2$ under general distributions is impossible because this concept class
has infinite VC-dimension. Our agnostic learners use data structures and
algorithms from computational geometry and their analysis relies on tools from
geometry and probabilistic combinatorics. Because our learners are proper, they
yield tolerant property testers with matching running times. Our results raise
a fundamental question of whether a gap between the sample and time complexity
is inherent for agnostic learning of these and other natural concept classes.

</details>


### [61] [A Generalization of Distance Domination](https://arxiv.org/abs/2510.18066)
*Alicia Muth,E. Dov Neimand*

Main category: cs.DS

TL;DR: 提出二次复杂度算法求树的最小故障集基数，该问题有实际应用。


<details>
  <summary>Details</summary>
Motivation: 拓展图论中k - 分量阶连通性和距离 - l支配的思想，解决相关实际问题如选择服务中心位置。

Method: 提出二次复杂度算法来求树的最小故障集基数。

Result: 得到了求解树的最小故障集基数的算法。

Conclusion: 该算法可用于解决选择服务中心位置等问题，减少距离支配问题中的冗余。

Abstract: Expanding on the graph theoretic ideas of k-component order connectivity and
distance-l domination, we present a quadratic-complexity algorithm that finds a
tree's minimum failure-set cardinality, i.e., the minimum cardinality any
subset of the tree's vertices must have so that all clusters of vertices
further away than some l do not exceed a cardinality threshold. Applications of
solutions to the expanded problems include choosing service center locations so
that no large neighborhoods are excluded from service, while reducing the
redundancy inherent in distance domination problems.

</details>


### [62] [Fingerprint Filters Are Optimal](https://arxiv.org/abs/2510.18129)
*William Kuszmaul,Jingxun Liang,Renfei Zhou*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Dynamic filters are data structures supporting approximate membership queries
to a dynamic set $S$ of $n$ keys, allowing a small false-positive error rate
$\varepsilon$, under insertions and deletions to the set $S$. Essentially all
known constructions for dynamic filters use a technique known as
fingerprinting. This technique, which was first introduced by Carter et al. in
1978, inherently requires $$\log \binom{n \varepsilon^{-1}}{n} = n \log
\varepsilon^{-1} + n \log e - o(n)$$ bits of space when $\varepsilon = o(1)$.
Whether or not this bound is optimal for all dynamic filters (rather than just
for fingerprint filters) has remained for decades as one of the central open
questions in the area. We resolve this question by proving a sharp lower bound
of $n \log \varepsilon^{-1} + n \log e - o(n)$ bits for $\varepsilon = o(1)$,
regardless of operation time.

</details>


### [63] [A Simpler Exponential-Time Approximation Algorithm for MAX-k-SAT](https://arxiv.org/abs/2510.18164)
*Harry Buhrman,Sevag Gharibian,Zeph Landau,François Le Gall,Norbert Schuch,Suguru Tamaki*

Main category: cs.DS

TL;DR: 提出更快的MAX - k - SAT多项式空间指数时间(1 - ε)近似算法，通过随机抽样实现。


<details>
  <summary>Details</summary>
Motivation: 设计比Hirsch和Escoffier等人提出的算法更快的MAX - k - SAT多项式空间(1 - ε)近似算法。

Method: 在MAX - k - SAT实例中，反复均匀随机抽样赋值，直至找到满足足够多子句的赋值。

Result: 该算法比之前已知的多项式空间(1 - ε)近似算法更快。

Conclusion: 简单的随机抽样方法在MAX - k - SAT（或MAXCSP）问题中是有效的，存在指数数量的赋值满足接近最优值的子句比例。

Abstract: We present an extremely simple polynomial-space exponential-time
$(1-\varepsilon)$-approximation algorithm for MAX-k-SAT that is (slightly)
faster than the previous known polynomial-space $(1-\varepsilon)$-approximation
algorithms by Hirsch (Discrete Applied Mathematics, 2003) and Escoffier,
Paschos and Tourniaire (Theoretical Computer Science, 2014). Our algorithm
repeatedly samples an assignment uniformly at random until finding an
assignment that satisfies a large enough fraction of clauses. Surprisingly, we
can show the efficiency of this simpler approach by proving that in any
instance of MAX-k-SAT (or more generally any instance of MAXCSP), an
exponential number of assignments satisfy a fraction of clauses close to the
optimal value.

</details>


### [64] [Coloring Graphs with Few Colors in the Streaming Model](https://arxiv.org/abs/2510.18177)
*Sepehr Assadi,Janani Sundaresan,Helia Yazdanyar*

Main category: cs.DS

TL;DR: 研究流模型下图着色问题，关注用少量颜色着色，给出不同流类型下问题空间复杂度的上下界并开发新工具。


<details>
  <summary>Details</summary>
Motivation: 过往研究多关注用大量颜色着色图，本文探索用少量（常数个）颜色着色图的问题。

Method: 建立问题关键变体的空间复杂度上下界，开发新工具如簇打包图、基于簇打包图的玩家消除框架和新的边与顶点采样引理。

Result: 在对抗流、随机顺序流和动态流中分别给出不同颜色可着色图区分问题的空间复杂度上下界。

Conclusion: 为图着色问题在流模型下用少量颜色着色的新方向奠定基础。

Abstract: We study graph coloring problems in the streaming model, where the goal is to
process an $n$-vertex graph whose edges arrive in a stream, using a limited
space that is smaller than the trivial $O(n^2)$ bound. While prior work has
largely focused on coloring graphs with a large number of colors, we explore
the opposite end of the spectrum: deciding whether the input graph can be
colored using only a few, say, a constant number of colors. We are interested
in each of the adversarial, random order, or dynamic streams.
  Our work lays the foundation for this new direction by establishing upper and
lower bounds on space complexity of key variants of the problem. Some of our
main results include:
  - Adversarial: for distinguishing between $q$- vs $2^{\Omega(q)}$-colorable
graphs, lower bounds of $n^{2-o(1)}$ space for $q$ up to
$(\log{n})^{1/2-o(1)}$, and $n^{1+\Omega(1/\log\log{n})}$ space for $q$ further
up to $(\log{n})^{1-o(1)}$.
  - Random order: for distinguishing between $q$- vs $q^t$-colorable graphs for
$q,t \geq 2$, an upper bound of $\tilde{O}(n^{1+1/t})$ space. Specifically,
distinguishing between $q$-colorable graphs vs ones that are not even
poly$(q)$-colorable can be done in $n^{1+o(1)}$ space unlike in adversarial
streams. Although, distinguishing between $q$-colorable vs
$\Omega(q^2)$-colorable graphs requires $\Omega(n^2)$ space even in random
order streams for constant $q$.
  - Dynamic: for distinguishing between $q$- vs $q \cdot t$-colorable graphs
for any $q \geq 3$ and $t \geq 1$, nearly optimal upper and lower bounds of
$\tilde{\Theta}(n^2/t^2)$ space.
  We develop several new technical tools along the way: cluster packing graphs,
a generalization of Ruzsa-Szemer\'edi graphs; a player elimination framework
based on cluster packing graphs; and new edge and vertex sampling lemmas
tailored to graph coloring.

</details>


### [65] [Nearly Space-Optimal Graph and Hypergraph Sparsification in Insertion-Only Data Streams](https://arxiv.org/abs/2510.18180)
*Vincent Cohen-Addad,David P. Woodruff,Shenghao Xie,Samson Zhou*

Main category: cs.DS

TL;DR: 本文研究插入式数据流中图和超图稀疏化问题，给出多种流式算法，在空间复杂度上有优化或达到近似最优。


<details>
  <summary>Details</summary>
Motivation: 解决插入式数据流中图和超图稀疏化问题，计算能保留向量能量的超图。

Method: 提出多种流式算法，包括超图稀疏化、图稀疏化、最小割近似等算法。

Result: 实现(1+ε)近似，在空间复杂度上有提升，如改进当前图稀疏化算法的空间复杂度界，在滑动窗口模型中实现空间近似最优的超图稀疏化等。

Conclusion: 给出的算法在图和超图稀疏化问题上有较好表现，在不同场景下达到近似最优或改进现有结果。

Abstract: We study the problem of graph and hypergraph sparsification in insertion-only
data streams. The input is a hypergraph $H=(V, E, w)$ with $n$ nodes, $m$
hyperedges, and rank $r$, and the goal is to compute a hypergraph $\widehat{H}$
that preserves the energy of each vector $x \in \mathbb{R}^n$ in $H$, up to a
small multiplicative error. In this paper, we give a streaming algorithm that
achieves a $(1+\varepsilon)$-approximation, using $\frac{rn}{\varepsilon^2}
\log^2 n \log r \cdot\text{poly}(\log \log m)$ bits of space, matching the
sample complexity of the best known offline algorithm up to $\text{poly}(\log
\log m)$ factors. Our approach also provides a streaming algorithm for graph
sparsification that achieves a $(1+\varepsilon)$-approximation, using
$\frac{n}{\varepsilon^2} \log n \cdot\text{poly}(\log\log n)$ bits of space,
improving the current bound by $\log n$ factors. Furthermore, we give a
space-efficient streaming algorithm for min-cut approximation. Along the way,
we present an online algorithm for $(1+\varepsilon)$-hypergraph sparsification,
which is optimal up to poly-logarithmic factors. As a result, we achieve
$(1+\varepsilon)$-hypergraph sparsification in the sliding window model, with
space optimal up to poly-logarithmic factors. Lastly, we give an adversarially
robust algorithm for hypergraph sparsification using $\frac{n}{\varepsilon^2}
\cdot\text{poly}(r, \log n, \log r, \log \log m)$ bits of space.

</details>


### [66] [Static Retrieval Revisited: To Optimality and Beyond](https://arxiv.org/abs/2510.18237)
*Yang Hu,William Kuszmaul,Jingxun Liang,Huacheng Yu,Junkai Zhang,Renfei Zhou*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the static retrieval problem, a data structure must answer retrieval
queries mapping a set of $n$ keys in a universe $[U]$ to $v$-bit values.
Information-theoretically, retrieval data structures can use as little as $nv$
bits of space. For small value sizes $v$, it is possible to achieve $O(1)$
query time while using space $nv + o(n)$ bits -- whether or not such a result
is possible for larger values of $v$ (e.g., $v = \Theta(\log n)$) has remained
open.
  In this paper, we obtain a tight lower bound (as well as matching upper
bounds) for the static retrieval problem. In the case where values are large,
we show that there is actually a significant tension between time and space. It
is not possible, for example, to get $O(1)$ query time using $nv + o(n)$ bits
of space, when $v = \Theta(\log n)$ (and assuming the word RAM model with
$O(\log n)$-bit words).
  At first glance, our lower bound would seem to render retrieval unusable in
many settings that aim to achieve very low redundancy. However, our second
result offers a way around this: We show that, whenever a retrieval data
structure $D_1$ is stored along with another data structure $D_2$ (whose size
is similar to or larger than the size of $D_1$), it is possible to implement
the combined data structure $D_1 \cup D_2$ so that queries to $D_1$ take $O(1)$
time, operations on $D_2$ take the same asymptotic time as if $D_2$ were stored
on its own, and the total space is $nv + \mathrm{Space}(D_2) + n^{0.67}$ bits.

</details>


### [67] [Minimum $s$--$t$ Cuts with Fewer Cut Queries](https://arxiv.org/abs/2510.18274)
*Yonggang Jiang,Danupon Nanongkai,Pachara Sawettamalya*

Main category: cs.DS

TL;DR: 本文提出新随机算法将无向无权图最小s - t割的割查询复杂度降至O(n^(8/5))，还得到确定性组合两方通信协议，通信复杂度降至O(n^(11/7))。


<details>
  <summary>Details</summary>
Motivation: 改进无向无权图通过割查询计算最小s - t割的复杂度，避免学习全图所需的高查询复杂度。

Method: 提出查询高效子例程，逐边揭示图，比经典增广路径方法更快增加学习子图的最大s - t流；算法为递归贪心过程。

Result: 随机算法将割查询复杂度降至O(n^(8/5))；得到确定性组合两方通信协议，通信复杂度降至O(n^(11/7))。

Conclusion: 新算法在割查询复杂度和通信复杂度上有改进，且算法简单、纯组合。

Abstract: We study the problem of computing a minimum $s$--$t$ cut in an unweighted,
undirected graph via \emph{cut queries}. In this model, the input graph is
accessed through an oracle that, given a subset of vertices $S \subseteq V$,
returns the size of the cut $(S, V \setminus S)$.
  This line of work was initiated by Rubinstein, Schramm, and Weinberg (ITCS
2018), who gave a randomized algorithm that computes a minimum $s$--$t$ cut
using $\widetilde{O}(n^{5/3})$ queries, thereby showing that one can avoid
spending $\widetilde{\Theta}(n^2)$ queries required to learn the entire graph.
A recent result by Anand, Saranurak, and Wang (SODA 2025) also matched this
upper bound via a deterministic algorithm based on blocking flows.
  In this work, we present a new randomized algorithm that improves the
cut-query complexity to $\widetilde{O}(n^{8/5})$. At the heart of our approach
is a query-efficient subroutine that incrementally reveals the graph
edge-by-edge while increasing the maximum $s$--$t$ flow in the learned subgraph
at a rate faster than classical augmenting-path methods. Notably, our algorithm
is simple, purely combinatorial, and can be naturally interpreted as a
recursive greedy procedure.
  As a further consequence, we obtain a \emph{deterministic} and
\emph{combinatorial} two-party communication protocol for computing a minimum
$s$--$t$ cut using $\widetilde{O}(n^{11/7})$ bits of communication. This
improves upon the previous best bound of $\widetilde{O}(n^{5/3})$, which was
obtained via reductions from the aforementioned cut-query algorithms. In
parallel, it has been observed that an $\widetilde{O}(n^{3/2})$-bit randomized
protocol can be achieved via continuous optimization techniques; however, these
methods are fundamentally different from our combinatorial approach.

</details>


### [68] [Uniformity Testing under User-Level Local Privacy](https://arxiv.org/abs/2510.18379)
*Clément L. Canonne,Abigail Gentle,Vikrant Singhal*

Main category: cs.DS

TL;DR: 本文开启用户级局部差分隐私下分布测试研究，给出均匀性和同一性测试的（近乎）最优样本算法。


<details>
  <summary>Details</summary>
Motivation: 用户级局部差分隐私设置比普通局部隐私设置更具挑战，此前该设置下基本测试任务研究空白，且出于实际考虑关注私有硬币对称设置。

Method: 提供（近乎）样本最优的用户级LDP算法进行均匀性和同一性测试。

Result: 给出了（近乎）样本最优的用户级LDP算法。

Conclusion: 填补了用户级局部差分隐私下分布测试的研究空白。

Abstract: We initiate the study of distribution testing under \emph{user-level} local
differential privacy, where each of $n$ users contributes $m$ samples from the
unknown underlying distribution. This setting, albeit very natural, is
significantly more challenging that the usual locally private setting, as for
the same parameter $\varepsilon$ the privacy guarantee must now apply to a full
batch of $m$ data points. While some recent work consider distribution
\emph{learning} in this user-level setting, nothing was known for even the most
fundamental testing task, uniformity testing (and its generalization, identity
testing).
  We address this gap, by providing (nearly) sample-optimal user-level LDP
algorithms for uniformity and identity testing. Motivated by practical
considerations, our main focus is on the private-coin, symmetric setting, which
does not require users to share a common random seed nor to have been assigned
a globally unique identifier.

</details>


### [69] [Odd and Even Harder Problems on Cycle-Factors](https://arxiv.org/abs/2510.18393)
*Florian Hörsch,Csaba Király,Mirabel Mendoza-Cadena,Gyula Pap,Eszter Szabó,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 研究带奇偶约束的循环因子问题的四种变体，多数问题是NP完全的，部分复杂度未知。


<details>
  <summary>Details</summary>
Motivation: 带奇偶约束的循环因子在结构图论和算法复杂度研究中自然出现。

Method: 对四种变体问题在无向、有向和混合图设置下进行研究。

Result: 除第四个问题外其他问题在各设置下NP完全，第四个问题在有向和无向情况下复杂度未知，混合图中确定任意循环因子存在性是NP完全。

Conclusion: 明确了多数变体问题的复杂度，部分问题复杂度待研究。

Abstract: For a graph (undirected, directed, or mixed), a cycle-factor is a collection
of vertex-disjoint cycles covering the entire vertex set. Cycle-factors subject
to parity constraints arise naturally in the study of structural graph theory
and algorithmic complexity. In this work, we study four variants of the problem
of finding a cycle-factor subject to parity constraints: (1) all cycles are
odd, (2) all cycles are even, (3) at least one cycle is odd, and (4) at least
one cycle is even. These variants are considered in the undirected, directed,
and mixed settings. We show that all but the fourth problem are NP-complete in
all settings, while the complexity of the fourth one remains open for the
directed and undirected cases. We also show that in mixed graphs, even deciding
the existence of any cycle factor is NP-complete.

</details>


### [70] [LatticeHashForest: An Efficient Data Structure for Repetitive Data and Operations](https://arxiv.org/abs/2510.18496)
*Anamitra Ghorui,Uday P. Khedker*

Main category: cs.DS

TL;DR: 提出新颖通用数据结构LatticeHashForest (LHF) 用于存储和操作信息，减少冗余计算和重复数据，在指针分析中效果显著。


<details>
  <summary>Details</summary>
Motivation: 全程序分析尤其是指针分析存在信息组合爆炸、大量重复数据传播和低级数据结构操作重复问题。

Method: 提出LHF数据结构，给出详细结构描述和抽象模型，提供C++实现、评估、API文档和用户手册。

Result: 在指针分析用例中，内存使用降至几乎可以忽略不计，输入规模接近1000万时速度提升超4倍。

Conclusion: LHF能有效减少冗余计算和重复数据，在指针分析等场景有良好表现。

Abstract: Analysis of entire programs as a single unit, or whole-program analysis,
involves propagation of large amounts of information through the control flow
of the program. This is especially true for pointer analysis, where, unless
significant compromises are made in the precision of the analysis, there is a
combinatorial blowup of information. One of the key problems we observed in our
own efforts is that a lot of duplicate data was being propagated, and many
low-level data structure operations were repeated a large number of times.
  We present what we consider to be a novel and generic data structure,
LatticeHashForest (LHF), to store and operate on such information in a manner
that eliminates a majority of redundant computations and duplicate data in
scenarios similar to those encountered in compilers and program optimization.
LHF differs from similar work in this vein, such as hash-consing, ZDDs, and
BDDs, by not only providing a way to efficiently operate on large, aggregate
structures, but also modifying the elements of such structures in a manner that
they can be deduplicated immediately. LHF also provides a way to perform a
nested construction of elements such that they can be deduplicated at multiple
levels, cutting down the need for additional, nested computations.
  We provide a detailed structural description, along with an abstract model of
this data structure. An entire C++ implementation of LHF is provided as an
artifact along with evaluations of LHF using examples and benchmark programs.
We also supply API documentation and a user manual for users to make
independent applications of LHF. Our main use case in the realm of pointer
analysis shows memory usage reduction to an almost negligible fraction, and
speedups beyond 4x for input sizes approaching 10 million when compared to
other implementations.

</details>


### [71] [An optimal algorithm for average distance in typical regular graphs](https://arxiv.org/abs/2510.18722)
*Alexandros Eskenazis,Manor Mendel,Assaf Naor*

Main category: cs.DS

TL;DR: 设计确定性算法，对典型常度正则图中的点输出平均距离常数因子近似，给出算法适用限制及距离查询复杂度相关结果。


<details>
  <summary>Details</summary>
Motivation: 回答文献[MN14]提出的问题，设计算法计算常度正则图中给定点的平均距离近似值。

Method: 使用文献[MN14]方法构建特定非正曲率度量空间的常度扩张图，结合新的刚性定理。

Result: 算法适用于典型常度正则图，得到算法距离查询复杂度与近似因子的关系，且存在达到相应参数的算法。

Conclusion: 若要达到一定的近似保证，算法需查询相应数量的距离，且有匹配的上界。

Abstract: We design a deterministic algorithm that, given $n$ points in a
\emph{typical} constant degree regular~graph, queries $O(n)$ distances to
output a constant factor approximation to the average distance among those
points, thus answering a question posed in~\cite{MN14}. Our algorithm uses the
method of~\cite{MN14} to construct a sequence of constant degree graphs that
are expanders with respect to certain nonpositively curved metric spaces,
together with a new rigidity theorem for metric transforms of nonpositively
curved metric spaces. The fact that our algorithm works for typical (uniformly
random) constant degree regular graphs rather than for all constant degree
graphs is unavoidable, thanks to the following impossibility result that we
obtain: For every fixed $k\in \N$, the approximation factor of any algorithm
for average distance that works for all constant degree graphs and queries
$o(n^{1+1/k})$ distances must necessarily be at least $2(k+1)$. This matches
the upper bound attained by the algorithm that was designed for general finite
metric spaces in~\cite{BGS}. Thus, any algorithm for average distance in
constant degree graphs whose approximation guarantee is less than $4$ must
query $\Omega(n^2)$ distances, any such algorithm whose approximation guarantee
is less than $6$ must query $\Omega(n^{3/2})$ distances, any such algorithm
whose approximation guarantee less than $8$ must query $\Omega(n^{4/3})$
distances, and so forth, and furthermore there exist algorithms achieving those
parameters.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [72] [On Condorcet's Jury Theorem with Abstention](https://arxiv.org/abs/2510.18062)
*Reshef Meir,Ganesh Ghalme*

Main category: cs.GT

TL;DR: 研究选民参与成本不同且有启发式关键度信念的非对称投票情境，分析不同关键度信念下的均衡情况及何时满足陪审团定理。


<details>
  <summary>Details</summary>
Motivation: 在选民参与成本不同和有启发式关键度信念的非对称设定下，研究投票均衡情况及陪审团定理的适用性。

Method: 在有成本投票设定中，根据选民参与成本和关键度估计决定是否投票，分析不同关键度信念（弱消失关键度和强消失关键度）下的均衡。

Result: 弱消失关键度产生多个选举接近平局的稳定均衡，强消失关键度产生唯一的平凡均衡；低于阈值多数偏好候选人获胜概率趋近1，高于阈值两候选人获胜概率相等。

Conclusion: 不同的启发式关键度信念会导致不同的投票均衡，且在一定条件下满足陪审团定理。

Abstract: The well-known Condorcet Jury Theorem states that, under majority rule, the
better of two alternatives is chosen with probability approaching one as the
population grows. We study an asymmetric setting where voters face varying
participation costs and share a possibly heuristic belief about their
pivotality (ability to influence the outcome).
  In a costly voting setup where voters abstain if their participation cost is
greater than their pivotality estimate, we identify a single property of the
heuristic belief -- weakly vanishing pivotality -- that gives rise to multiple
stable equilibria in which elections are nearly tied. In contrast, strongly
vanishing pivotality (as in the standard Calculus of Voting model) yields a
unique, trivial equilibrium where only zero-cost voters participate as the
population grows. We then characterize when nontrivial equilibria satisfy a
version of the Jury Theorem: below a sharp threshold, the majority-preferred
candidate wins with probability approaching one; above it, both candidates
either win with equal probability.

</details>


### [73] [Contextual Search in Principal-Agent Games: The Curse of Degeneracy](https://arxiv.org/abs/2510.18567)
*Yiding Feng,Mengfan Ma,Bo Peng,Zongqi Wan*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we introduce and study contextual search in general
principal-agent games, where a principal repeatedly interacts with agents by
offering contracts based on contextual information and historical feedback,
without knowing the agents' true costs or rewards. Our model generalizes
classical contextual pricing by accommodating richer agent action spaces. Over
$T$ rounds with $d$-dimensional contexts, we establish an asymptotically tight
exponential $T^{1 - \Theta(1/d)}$ bound in terms of the pessimistic Stackelberg
regret, benchmarked against the best utility for the principal that is
consistent with the observed feedback.
  We also establish a lower bound of $\Omega(T^{\frac{1}{2}-\frac{1}{2d}})$ on
the classic Stackelberg regret for principal-agent games, demonstrating a
surprising double-exponential hardness separation from the contextual pricing
problem (a.k.a, the principal-agent game with two actions), which is known to
admit a near-optimal $O(d\log\log T)$ regret bound [Kleinberg and Leighton,
2003, Leme and Schneider, 2018, Liu et al., 2021]. In particular, this
double-exponential hardness separation occurs even in the special case with
three actions and two-dimensional context. We identify that this significant
increase in learning difficulty arises from a structural phenomenon that we
call contextual action degeneracy, where adversarially chosen contexts can make
some actions strictly dominated (and hence unincentivizable), blocking the
principal's ability to explore or learn about them, and fundamentally limiting
learning progress.

</details>


### [74] [Likelihood of the Existence of Average Justified Representation](https://arxiv.org/abs/2510.18718)
*Qishen Han,Biaoshuai Tao,Lirong Xia,Chengkai Zhang,Houyu Zhou*

Main category: cs.GT

TL;DR: 研究了基于认可的多赢家选举中AJR公理存在的可能性，在Erdős–Rényi模型下对m为常数、n趋于无穷时给出清晰完整刻画，发现p有两个相变点。


<details>
  <summary>Details</summary>
Motivation: 已知并非所有多赢家选举实例都保证存在满足AJR的获胜委员会，研究AJR存在的可能性。

Method: 考虑由p参数化的Erdős–Rényi模型，该模型从每个选民以概率p认可每个候选人的分布中抽样多赢家选举实例。

Result: 存在两个相变点p1和p2，p<p1或p>p2时，AJR委员会存在概率为1 - o(1)；p1<p<p2时，存在概率为o(1)；p = p1或p = p2时，存在概率介于0和1之间。

Conclusion: 在m为常数、n趋于无穷的情况下，对AJR委员会的存在性给出了完整清晰的刻画。

Abstract: We study the approval-based multi-winner election problem where $n$ voters
jointly decide a committee of $k$ winners from $m$ candidates. We focus on the
axiom \emph{average justified representation} (AJR) proposed by Fernandez,
Elkind, Lackner, Garcia, Arias-Fisteus, Basanta-Val, and Skowron (2017). AJR
postulates that every group of voters with a common preference should be
sufficiently represented in that their average satisfaction should be no less
than their Hare quota. Formally, for every group of
$\lceil\ell\cdot\frac{n}{k}\rceil$ voters with $\ell$ common approved
candidates, the average number of approved winners for this group should be at
least $\ell$. It is well-known that a winning committee satisfying AJR is not
guaranteed to exist for all multi-winner election instances. In this paper, we
study the likelihood of the existence of AJR under the Erd\H{o}s--R\'enyi
model. We consider the Erd\H{o}s--R\'enyi model parameterized by $p\in[0,1]$
that samples multi-winner election instances from the distribution where each
voter approves each candidate with probability $p$ (and the events that voters
approve candidates are independent), and we provide a clean and complete
characterization of the existence of AJR committees in the case where $m$ is a
constant and $n$ tends to infinity. We show that there are two phase transition
points $p_1$ and $p_2$ (with $p_1\leq p_2$) for the parameter $p$ such that: 1)
when $p<p_1$ or $p>p_2$, an AJR committee exists with probability $1-o(1)$, 2)
when $p_1<p<p_2$, an AJR committee exists with probability $o(1)$, and 3) when
$p=p_1$ or $p=p_2$, the probability that an AJR committee exists is bounded
away from both $0$ and $1$.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [75] [From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern Autonomous Recommender-Systems Research Labs](https://arxiv.org/abs/2510.18104)
*Joeran Beel,Bela Gipp,Tobias Vente,Moritz Baumgart,Philipp Meister*

Main category: cs.IR

TL;DR: 推荐系统研究加速了模型和评估进展，但忽视研究过程自动化。提出从狭义AutoRecSys工具转向AutoRecLab，给出社区议程，呼吁组织社区活动协调后续步骤。


<details>
  <summary>Details</summary>
Motivation: 推荐系统研究忽略了研究过程本身的自动化，需要改变现状。

Method: 借鉴自动化科学进展，为RecSys社区制定包括构建原型、建立基准和竞赛等在内的议程。

Result: 推进议程可提高研究吞吐量、发现非显而易见的见解，使RecSys为新兴的人工智能研究做出贡献。

Conclusion: 呼吁组织社区活动协调后续步骤，为负责任地整合自动化研究系统提供指导。

Abstract: Recommender-systems research has accelerated model and evaluation advances,
yet largely neglects automating the research process itself. We argue for a
shift from narrow AutoRecSys tools -- focused on algorithm selection and
hyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab
(AutoRecLab) that integrates end-to-end automation: problem ideation,
literature analysis, experimental design and execution, result interpretation,
manuscript drafting, and provenance logging. Drawing on recent progress in
automated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems),
we outline an agenda for the RecSys community: (1) build open AutoRecLab
prototypes that combine LLM-driven ideation and reporting with automated
experimentation; (2) establish benchmarks and competitions that evaluate agents
on producing reproducible RecSys findings with minimal human input; (3) create
review venues for transparently AI-generated submissions; (4) define standards
for attribution and reproducibility via detailed research logs and metadata;
and (5) foster interdisciplinary dialogue on ethics, governance, privacy, and
fairness in autonomous research. Advancing this agenda can increase research
throughput, surface non-obvious insights, and position RecSys to contribute to
emerging Artificial Research Intelligence. We conclude with a call to organise
a community retreat to coordinate next steps and co-author guidance for the
responsible integration of automated research systems.

</details>


### [76] [LIME: Link-based user-item Interaction Modeling with decoupled xor attention for Efficient test time scaling](https://arxiv.org/abs/2510.18239)
*Yunjiang Jiang,Ayush Agarwal,Yang Liu,Bi Xue*

Main category: cs.IR

TL;DR: 论文指出扩展大型推荐系统面临的挑战，提出LIME架构解决计算成本与性能的权衡问题，实验表明其有显著加速效果并能提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 现有transformers在扩展候选集或增加序列长度时计算成本过高，虽性能有提升但代价大，需解决计算成本与性能的权衡问题。

Method: 引入LIME架构，通过低秩“链接嵌入”解耦用户和候选交互进行注意力权重预计算，以及使用线性注意力机制LIME - XOR降低复杂度。

Result: 在公共和工业数据集上，LIME与最先进的transformers性能相近，但在大候选集或长序列长度下推理速度提升10倍；在主要推荐平台测试时，提升了用户参与度并保持低推理成本。

Conclusion: LIME建立了高效且有表现力的推荐系统新范式。

Abstract: Scaling large recommendation systems requires advancing three major
frontiers: processing longer user histories, expanding candidate sets, and
increasing model capacity. While promising, transformers' computational cost
scales quadratically with the user sequence length and linearly with the number
of candidates. This trade-off makes it prohibitively expensive to expand
candidate sets or increase sequence length at inference, despite the
significant performance improvements.
  We introduce \textbf{LIME}, a novel architecture that resolves this
trade-off. Through two key innovations, LIME fundamentally reduces
computational complexity. First, low-rank ``link embeddings" enable
pre-computation of attention weights by decoupling user and candidate
interactions, making the inference cost nearly independent of candidate set
size. Second, a linear attention mechanism, \textbf{LIME-XOR}, reduces the
complexity with respect to user sequence length from quadratic ($O(N^2)$) to
linear ($O(N)$).
  Experiments on public and industrial datasets show LIME achieves near-parity
with state-of-the-art transformers but with a 10$\times$ inference speedup on
large candidate sets or long sequence lengths. When tested on a major
recommendation platform, LIME improved user engagement while maintaining
minimal inference costs with respect to candidate set size and user history
length, establishing a new paradigm for efficient and expressive recommendation
systems.

</details>


### [77] [Enhancing Hotel Recommendations with AI: LLM-Based Review Summarization and Query-Driven Insights](https://arxiv.org/abs/2510.18277)
*Nikolaos Belibasakis,Anastasios Giannaros,Ioanna Giannoukou,Spyros Sioutas*

Main category: cs.IR

TL;DR: 研究利用大语言模型（LLMs）增强短租公寓推荐，开发instaGuide工具，能节省用户搜索时间，改善决策过程。


<details>
  <summary>Details</summary>
Motivation: 预订平台数据增多，用户难以高效浏览住宿信息和分析评论，现有过滤方式未充分利用非结构化文本评论。

Method: 开发名为instaGuide的Web应用，自动提取Booking.com平台上房产的文本评论、合成评论摘要，用户可查询房产特定方面；评估多个LLM模型的准确性、成本和响应质量。

Result: 基于LLM的摘要显著减少用户搜索合适短租公寓的时间，改善整体决策过程。

Conclusion: 大语言模型可通过总结和挖掘用户评论中的关键见解，增强短租公寓推荐。

Abstract: The increasing number of data a booking platform such as Booking.com and
AirBnB offers make it challenging for interested parties to browse through the
available accommodations and analyze reviews in an efficient way. Efforts have
been made from the booking platform providers to utilize recommender systems in
an effort to enable the user to filter the results by factors such as stars,
amenities, cost but most valuable insights can be provided by the unstructured
text-based reviews. Going through these reviews one-by-one requires a
substantial amount of time to be devoted while a respectable percentage of the
reviews won't provide to the user what they are actually looking for.
  This research publication explores how Large Language Models (LLMs) can
enhance short rental apartments recommendations by summarizing and mining key
insights from user reviews. The web application presented in this paper, named
"instaGuide", automates the procedure of isolating the text-based user reviews
from a property on the Booking.com platform, synthesizing the summary of the
reviews, and enabling the user to query specific aspects of the property in an
effort to gain feedback on their personal questions/criteria.
  During the development of the instaGuide tool, numerous LLM models were
evaluated based on accuracy, cost, and response quality. The results suggest
that the LLM-powered summarization reduces significantly the amount of time the
users need to devote on their search for the right short rental apartment,
improving the overall decision-making procedure.

</details>


### [78] [Evaluating LLM-Based Mobile App Recommendations: An Empirical Study](https://arxiv.org/abs/2510.18364)
*Quim Motger,Xavier Franch,Vincenzo Gervasi,Jordi Marco*

Main category: cs.IR

TL;DR: 本文对通用大语言模型生成、论证和排名移动应用推荐进行实证分析，提出排名标准分类、评估框架和复制包，发现LLM排名标准与ASO指标部分对齐，排名有一定一致性和变化，对指令敏感度不同。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于移动应用推荐时推理不透明，其一致性、可解释性及与传统ASO指标的对齐性存疑。

Method: 对广泛使用的通用大语言模型进行实证分析，提出16种可推广的排名标准分类、评估框架和复制包。

Result: LLM依赖广泛但分散的排名标准，与标准ASO指标部分对齐；排名靠前的应用较一致，排名深度和搜索特异性增加时变异性增大；LLM对明确排名指令敏感度不同。

Conclusion: 研究结果有助于终端用户、应用开发者和推荐系统研究人员应对对话式应用发现的新环境。

Abstract: Large Language Models (LLMs) are increasingly used to recommend mobile
applications through natural language prompts, offering a flexible alternative
to keyword-based app store search. Yet, the reasoning behind these
recommendations remains opaque, raising questions about their consistency,
explainability, and alignment with traditional App Store Optimization (ASO)
metrics. In this paper, we present an empirical analysis of how widely-used
general purpose LLMs generate, justify, and rank mobile app recommendations.
Our contributions are: (i) a taxonomy of 16 generalizable ranking criteria
elicited from LLM outputs; (ii) a systematic evaluation framework to analyse
recommendation consistency and responsiveness to explicit ranking instructions;
and (iii) a replication package to support reproducibility and future research
on AI-based recommendation systems. Our findings reveal that LLMs rely on a
broad yet fragmented set of ranking criteria, only partially aligned with
standard ASO metrics. While top-ranked apps tend to be consistent across runs,
variability increases with ranking depth and search specificity. LLMs exhibit
varying sensitivity to explicit ranking instructions - ranging from substantial
adaptations to near-identical outputs - highlighting their complex reasoning
dynamics in conversational app discovery. Our results aim to support end-users,
app developers, and recommender-systems researchers in navigating the emerging
landscape of conversational app discovery.

</details>


### [79] [LLMs as Sparse Retrievers:A Framework for First-Stage Product Search](https://arxiv.org/abs/2510.18527)
*Hongru Song,Yu-an Liu,Ruqing Zhang,Jiafeng Guo,Maarten de Rijke,Sen Li,Wenjun Peng,Fuyu Lv,Xueqi Cheng*

Main category: cs.IR

TL;DR: 提出用于产品搜索的PROSPER框架，解决大语言模型应用于稀疏检索的挑战，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 产品搜索系统中，稀疏检索有词汇不匹配问题，大语言模型虽有潜力但直接应用存在易产生幻觉和难有效初始化训练的挑战。

Method: 提出PROSPER框架，包含缓解词汇扩展幻觉的字面残差网络和通过粗到细稀疏策略促进有效训练初始化的词汇聚焦窗口。

Result: 离线和在线实验显示，PROSPER显著优于稀疏基线，召回性能与先进的密集检索器相当，且在线带来收入增长。

Conclusion: PROSPER框架有效解决了大语言模型应用于产品搜索稀疏检索的挑战，提升了检索性能和业务收益。

Abstract: Product search is a crucial component of modern e-commerce platforms, with
billions of user queries every day. In product search systems, first-stage
retrieval should achieve high recall while ensuring efficient online
deployment. Sparse retrieval is particularly attractive in this context due to
its interpretability and storage efficiency. However, sparse retrieval methods
suffer from severe vocabulary mismatch issues, leading to suboptimal
performance in product search scenarios.With their potential for semantic
analysis, large language models (LLMs) offer a promising avenue for mitigating
vocabulary mismatch issues and thereby improving retrieval quality. Directly
applying LLMs to sparse retrieval in product search exposes two key
challenges:(1)Queries and product titles are typically short and highly
susceptible to LLM-induced hallucinations, such as generating irrelevant
expansion terms or underweighting critical literal terms like brand names and
model numbers;(2)The large vocabulary space of LLMs leads to difficulty in
initializing training effectively, making it challenging to learn meaningful
sparse representations in such ultra-high-dimensional spaces.To address these
challenges, we propose PROSPER, a framework for PROduct search leveraging LLMs
as SParsE Retrievers. PROSPER incorporates: (1)A literal residual network that
alleviates hallucination in lexical expansion by reinforcing underweighted
literal terms through a residual compensation mechanism; and (2)A lexical
focusing window that facilitates effective training initialization via a
coarse-to-fine sparsification strategy.Extensive offline and online experiments
show that PROSPER significantly outperforms sparse baselines and achieves
recall performance comparable to advanced dense retrievers, while also
achieving revenue increments online.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [80] [From Noise to Laws: Regularized Time-Series Forecasting via Denoised Dynamic Graphs](https://arxiv.org/abs/2510.17817)
*Hongwei Ma,Junbin Gao,Minh-ngoc Tran*

Main category: cs.LG

TL;DR: 提出PRISM模型用于长时多变量时间序列预测，在六个标准基准测试中取得一致的SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 长时多变量时间序列预测面临去噪异质信号、跟踪时变交叉序列依赖以及在长预测期保持稳定和物理合理性的挑战。

Method: 将基于分数的扩散预处理器与动态、相关阈值图编码器以及由通用物理惩罚正则化的预测头相结合。

Result: 在六个标准基准测试中，PRISM取得一致的SOTA，MSE和MAE有显著提升。

Conclusion: 证明了在温和条件下诱导的预测期动态的收缩性，推导了图块的Lipschitz界，解释了模型的鲁棒性。

Abstract: Long-horizon multivariate time-series forecasting is challenging because
realistic predictions must (i) denoise heterogeneous signals, (ii) track
time-varying cross-series dependencies, and (iii) remain stable and physically
plausible over long rollout horizons. We present PRISM, which couples a
score-based diffusion preconditioner with a dynamic, correlation-thresholded
graph encoder and a forecast head regularized by generic physics penalties. We
prove contraction of the induced horizon dynamics under mild conditions and
derive Lipschitz bounds for graph blocks, explaining the model's robustness. On
six standard benchmarks , PRISM achieves consistent SOTA with strong MSE and
MAE gains.

</details>


### [81] [RAISE: A Unified Framework for Responsible AI Scoring and Evaluation](https://arxiv.org/abs/2510.18559)
*Loc Phuc Truong Nguyen,Hung Thanh Do*

Main category: cs.LG

TL;DR: 引入RAISE框架评估AI模型多维度性能，评估三个深度学习模型，发现无单一模型在所有责任标准上占优，强调多维评估重要性。


<details>
  <summary>Details</summary>
Motivation: 当AI系统进入高风险领域，评估需超越预测准确性，涵盖可解释性、公平性、鲁棒性和可持续性。

Method: 引入RAISE统一框架量化模型在四个维度的性能并汇总为责任得分，在金融、医疗和社会经济结构化数据集上评估三个深度学习模型。

Result: MLP在可持续性和鲁棒性方面表现强，Transformer在可解释性和公平性上出色但环境成本高，Tabular ResNet表现平衡，无单一模型在所有责任标准上占优。

Conclusion: 强调多维评估对负责任模型选择的必要性。

Abstract: As AI systems enter high-stakes domains, evaluation must extend beyond
predictive accuracy to include explainability, fairness, robustness, and
sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a
unified framework that quantifies model performance across these four
dimensions and aggregates them into a single, holistic Responsibility Score. We
evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular
ResNet, and a Feature Tokenizer Transformer, on structured datasets from
finance, healthcare, and socioeconomics. Our findings reveal critical
trade-offs: the MLP demonstrated strong sustainability and robustness, the
Transformer excelled in explainability and fairness at a very high
environmental cost, and the Tabular ResNet offered a balanced profile. These
results underscore that no single model dominates across all responsibility
criteria, highlighting the necessity of multi-dimensional evaluation for
responsible model selection. Our implementation is available at:
https://github.com/raise-framework/raise.

</details>


### [82] [GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing](https://arxiv.org/abs/2510.17843)
*Zongze Wu,Yani Guo,Churong Liang,Runnan Li*

Main category: cs.LG

TL;DR: 现有大语言模型工具检索依赖语义相似性存在局限，提出GRETEL方法，实验证明执行验证比语义相似性更可靠。


<details>
  <summary>Details</summary>
Motivation: 当前基于语义相似性的工具检索存在语义 - 功能差距，会检索到文本相关但功能不可用的工具。

Method: 引入GRETEL，通过沙盒化的计划 - 执行 - 评估循环处理语义检索的候选工具，生成执行证据以区分真正可用的工具。

Result: 在ToolBench基准测试中各指标显著提升，Pass Rate从0.690提升到0.826，Recall从0.841提升到0.867，NDCG从0.807提升到0.857。

Conclusion: 基于执行的验证为工具选择提供了比单纯语义相似性更可靠的基础，能提升真实应用中智能体的性能。

Abstract: Despite remarkable advances in Large Language Model capabilities, tool
retrieval for agent-based systems remains fundamentally limited by reliance on
semantic similarity, which fails to capture functional viability. Current
methods often retrieve textually relevant but functionally inoperative tools
due to parameter mismatches, authentication failures, and execution
constraints--a phenomenon we term the semantic-functional gap. We introduce
GRETEL, to address this gap through systematic empirical validation. GRETEL
implements an agentic workflow that processes semantically retrieved candidates
through sandboxed plan-execute-evaluate cycles, generating execution-grounded
evidence to distinguish truly functional tools from merely descriptive matches.
Our comprehensive evaluation on the ToolBench benchmark demonstrates
substantial improvements across all metrics: Pass Rate (at 10) increases from
0.690 to 0.826, Recall (at 10) improves from 0.841 to 0.867, and NDCG (at 10)
rises from 0.807 to 0.857.. These results establish that execution-based
validation provides a more reliable foundation for tool selection than semantic
similarity alone, enabling more robust agent performance in real-world
applications.

</details>


### [83] [CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings](https://arxiv.org/abs/2510.17846)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 本文提出混合AI框架CARLE进行剩余使用寿命（RUL）估计，结合深浅学习，评估显示其性能优于现有方法，还分析了模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有RUL方法在变化工况下缺乏泛化性和鲁棒性，需新方法解决。

Method: 引入CARLE框架，用Res - CNN和Res - LSTM块结合多头注意力和残差连接捕捉模式，用随机森林回归器预测RUL，有紧凑预处理流程，在两个轴承数据集上评估，进行消融研究、噪声和跨域实验。

Result: 比较结果显示CARLE在动态条件下尤其优于几种先进方法。

Conclusion: CARLE能有效解决现有RUL方法的问题，在不同工况下有良好性能，且分析了模型的可解释性。

Abstract: Prognostic Health Management (PHM) systems monitor and predict equipment
health. A key task is Remaining Useful Life (RUL) estimation, which predicts
how long a component, such as a rolling element bearing, will operate before
failure. Many RUL methods exist but often lack generalizability and robustness
under changing operating conditions. This paper introduces CARLE, a hybrid AI
framework that combines deep and shallow learning to address these challenges.
CARLE uses Res-CNN and Res-LSTM blocks with multi-head attention and residual
connections to capture spatial and temporal degradation patterns, and a Random
Forest Regressor (RFR) for stable, accurate RUL prediction. A compact
preprocessing pipeline applies Gaussian filtering for noise reduction and
Continuous Wavelet Transform (CWT) for time-frequency feature extraction. We
evaluate CARLE on the XJTU-SY and PRONOSTIA bearing datasets. Ablation studies
measure each component's contribution, while noise and cross-domain experiments
test robustness and generalization. Comparative results show CARLE outperforms
several state-of-the-art methods, especially under dynamic conditions. Finally,
we analyze model interpretability with LIME and SHAP to assess transparency and
trustworthiness.

</details>


### [84] [Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria](https://arxiv.org/abs/2510.18183)
*Eason Yu,Tzu Hao Liu,Yunke Wang,Clément L. Canonne,Nguyen H. Tran,Chang Xu*

Main category: cs.LG

TL;DR: 本文提出迭代优化参考策略的方法来解决不完美信息博弈中寻找纳什均衡的问题，开发了NashPG算法，在经典基准游戏和大型领域中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于正则化的方法在逼近纳什均衡时需将正则化强度趋近于零，导致学习不稳定，因此需寻找更好的方法。

Method: 固定正则化强度，通过迭代优化参考策略来收敛，在此基础上开发NashPG算法。

Result: 理论上保证在二人零和博弈中严格单调改进并收敛到精确纳什均衡；实证上，NashPG在经典游戏中可达到与之前无模型方法相当或更低的可利用性，在大型领域中能获得更高Elo评级。

Conclusion: 提出的方法和NashPG算法能有效解决不完美信息博弈中寻找纳什均衡的问题，具有良好的效果和扩展性。

Abstract: Finding Nash equilibria in imperfect-information games remains a central
challenge in multi-agent reinforcement learning. While regularization-based
methods have recently achieved last-iteration convergence to a regularized
equilibrium, they require the regularization strength to shrink toward zero to
approximate a Nash equilibrium, often leading to unstable learning in practice.
Instead, we fix the regularization strength at a large value for robustness and
achieve convergence by iteratively refining the reference policy. Our main
theoretical result shows that this procedure guarantees strictly monotonic
improvement and convergence to an exact Nash equilibrium in two-player zero-sum
games, without requiring a uniqueness assumption. Building on this framework,
we develop a practical algorithm, Nash Policy Gradient (NashPG), which
preserves the generalizability of policy gradient methods while relying solely
on the current and reference policies. Empirically, NashPG achieves comparable
or lower exploitability than prior model-free methods on classic benchmark
games and scales to large domains such as Battleship and No-Limit Texas
Hold'em, where NashPG consistently attains higher Elo ratings.

</details>


### [85] [Shock-Aware Physics-Guided Fusion-DeepONet Operator for Rarefied Micro-Nozzle Flows](https://arxiv.org/abs/2510.17887)
*Ehsan Roohi,Amirmehran Mahdavi*

Main category: cs.LG

TL;DR: 提出物理感知深度学习框架构建微喷嘴流替代模型，并在粘性Burgers方程验证。


<details>
  <summary>Details</summary>
Motivation: 构建快速准确的含激波微喷嘴流替代模型。

Method: 框架集成Fusion DeepONet架构、物理引导特征空间和两阶段课程策略。

Result: 在粘性Burgers方程上验证了框架的通用性和归纳偏置。

Conclusion: 所提框架可用于构建含激波微喷嘴流替代模型。

Abstract: We present a comprehensive, physics aware deep learning framework for
constructing fast and accurate surrogate models of rarefied, shock containing
micro nozzle flows. The framework integrates three key components, a Fusion
DeepONet operator learning architecture for capturing parameter dependencies, a
physics-guided feature space that embeds a shock-aligned coordinate system, and
a two-phase curriculum strategy emphasizing high-gradient regions. To
demonstrate the generality and inductive bias of the proposed framework, we
first validate it on the canonical viscous Burgers equation, which exhibits
advective steepening and shock like gradients.

</details>


### [86] [Automated Algorithm Design for Auto-Tuning Optimizers](https://arxiv.org/abs/2510.17899)
*Floris-Jan Willemsen,Niki van Stein,Ben van Werkhoven*

Main category: cs.LG

TL;DR: 本文探索用大语言模型生成针对自动性能调优问题的优化算法，评估显示其能提升性能，部分算法优于现有人类设计算法。


<details>
  <summary>Details</summary>
Motivation: 传统自动调优依赖优化算法，但设计有效优化器具挑战性，无单一方法适用于所有调优任务。

Method: 引入框架，用问题描述和搜索空间特征提示大语言模型生成优化策略并迭代改进。

Result: 在四个应用和六个硬件平台评估，提供额外信息平均性能提升30.7%和14.6%，生成的优化器能媲美甚至超越现有算法，最佳算法平均提升72.4%。

Conclusion: 大语言模型生成的优化算法在自动性能调优中表现良好，可作为有效解决方案。

Abstract: Automatic performance tuning (auto-tuning) is essential for optimizing
high-performance applications, where vast and irregular parameter spaces make
manual exploration infeasible. Traditionally, auto-tuning relies on
well-established optimization algorithms such as evolutionary algorithms,
annealing methods, or surrogate model-based optimizers to efficiently find
near-optimal configurations. However, designing effective optimizers remains
challenging, as no single method performs best across all tuning tasks.
  In this work, we explore a new paradigm: using large language models (LLMs)
to automatically generate optimization algorithms tailored to auto-tuning
problems. We introduce a framework that prompts LLMs with problem descriptions
and search-space characteristics results to produce specialized optimization
strategies, which are iteratively examined and improved.
  These generated algorithms are evaluated on four real-world auto-tuning
applications across six hardware platforms and compared against the
state-of-the-art in optimization algorithms of two contemporary auto-tuning
frameworks. The evaluation demonstrates that providing additional application-
and search space-specific information in the generation stage results in an
average performance improvement of 30.7\% and 14.6\%, respectively. In
addition, our results show that LLM-generated optimizers can rival, and in
various cases outperform, existing human-designed algorithms, with our
best-performing generated optimization algorithms achieving, on average, 72.4\%
improvement over state-of-the-art optimizers for auto-tuning.

</details>


### [87] [MIN-Merging: Merge the Important Neurons for Model Merging](https://arxiv.org/abs/2510.17890)
*Yunfei Liang*

Main category: cs.LG

TL;DR: 提出 MIN - Merging 框架解决模型合并参数冲突问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法存在参数冲突，导致特定领域任务性能下降。

Method: 提出基于路由器的 MIN - Merging 框架，选择性合并最重要的神经元以减少冲突。

Result: 在计算机视觉和自然语言处理基准测试中，MIN - Merging 在域内任务上持续提升性能，同时保留预训练模型在域外任务的泛化能力。

Conclusion: MIN - Merging 是解决模型合并中参数冲突问题的有效实用方案。

Abstract: Recent advances in deep learning have led to a surge of open-source models
across diverse domains. While model merging offers a promising way to combine
their strengths, existing approaches often suffer from parameter conflicts that
degrade performance on domain-specific tasks. We propose MIN-Merging, a
router-based framework that selectively merges the most important neurons to
reduce such conflicts. Extensive experiments on Computer Vision(CV) and Natural
Language Processing(NLP) benchmarks show that MIN-Merging achieves consistent
gains on in-domain tasks while retaining the generalization ability of
pretrained models on out-of-domain tasks. These results highlight its
effectiveness as a practical solution to the parameter conflict problem in
model merging.

</details>


### [88] [Uncertainty-Aware Post-Hoc Calibration: Mitigating Confidently Incorrect Predictions Beyond Calibration Metrics](https://arxiv.org/abs/2510.17915)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.LG

TL;DR: 本文提出基于预测可靠性评估的事后校准框架，提升校准质量与不确定性感知决策能力，实验显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络校准方法多采用全局变换，忽视个体预测可靠性异质性，且校准与不确定性感知决策关系待探索。

Method: 利用基于邻近性的共形预测将校准样本分层，采用双校准策略分别处理不同组样本。

Result: 在CIFAR - 10和CIFAR - 100实验中，该方法降低了自信错误预测，预期校准误差有竞争力。

Conclusion: 该方法通过实例级适应性连接校准和不确定性量化，是无需模型再训练的实用事后解决方案。

Abstract: Despite extensive research on neural network calibration, existing methods
typically apply global transformations that treat all predictions uniformly,
overlooking the heterogeneous reliability of individual predictions.
Furthermore, the relationship between improved calibration and effective
uncertainty-aware decision-making remains largely unexplored. This paper
presents a post-hoc calibration framework that leverages prediction reliability
assessment to jointly enhance calibration quality and uncertainty-aware
decision-making. The framework employs proximity-based conformal prediction to
stratify calibration samples into putatively correct and putatively incorrect
groups based on semantic similarity in feature space. A dual calibration
strategy is then applied: standard isotonic regression calibrated confidence in
putatively correct predictions, while underconfidence-regularized isotonic
regression reduces confidence toward uniform distributions for putatively
incorrect predictions, facilitating their identification for further
investigations. A comprehensive evaluation is conducted using calibration
metrics, uncertainty-aware performance measures, and empirical conformal
coverage. Experiments on CIFAR-10 and CIFAR-100 with BiT and CoAtNet backbones
show that the proposed method achieves lower confidently incorrect predictions,
and competitive Expected Calibration Error compared with isotonic and
focal-loss baselines. This work bridges calibration and uncertainty
quantification through instance-level adaptivity, offering a practical post-hoc
solution that requires no model retraining while improving both probability
alignment and uncertainty-aware decision-making.

</details>


### [89] [Hierarchical Federated Unlearning for Large Language Models](https://arxiv.org/abs/2510.17895)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: 提出一种可扩展且保护隐私的大语言模型联邦遗忘方法，实验表明该方法能有效处理异构遗忘请求并保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成到实际应用引发隐私等问题，机器遗忘面临连续异构需求、数据分散敏感等挑战，导致域间和域内干扰及遗忘与保留性能失衡。

Method: 通过特定任务适配器学习解耦遗忘和保留，采用分层合并策略缓解冲突目标，实现稳健、适应性的遗忘更新。

Result: 在WMDP、MUSE和TOFU基准上的综合实验显示，该方法相比基线方法能有效处理异构遗忘请求并保持强模型实用性。

Conclusion: 所提出的联邦遗忘方法可有效解决大语言模型机器遗忘面临的挑战。

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, raising concerns about privacy, security and the need to remove
undesirable knowledge. Machine Unlearning has emerged as a promising solution,
yet faces two key challenges: (1) practical unlearning needs are often
continuous and heterogeneous, and (2) they involve decentralized, sensitive
data with asymmetric access. These factors result in inter-domain and
intra-domain interference, which further amplifies the dilemma of unbalanced
forgetting and retaining performance. In response, we propose a federated
unlearning approach for LLMs that is scalable and privacy preserving. Our
method decouples unlearning and retention via task-specific adapter learning
and employs a hierarchical merging strategy to mitigate conflicting objectives
and enables robust, adaptable unlearning updates. Comprehensive experiments on
benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles
heterogeneous unlearning requests while maintaining strong LLM utility compared
with baseline methods.

</details>


### [90] [EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning](https://arxiv.org/abs/2510.17928)
*He Du,Bowen Li,Aijun Yang,Siyang He,Qipeng Guo,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出任务无关的数据合成框架，能联合合成问题、解决方案和验证工件，实验证明其在训练范式下有效，有较强泛化性。


<details>
  <summary>Details</summary>
Motivation: 构建可泛化的合成可验证数据困难，现有方法依赖特定启发式或事后过滤器，缺乏通用验证评估器。

Method: 引入进化的、任务无关、策略引导、可执行检查的数据合成框架，通过基于一致性的评估器迭代发现策略。

Result: 在RLVR和模型蒸馏训练范式下实验有效，在LiveCodeBench和AgentBench - OS任务上训练有显著提升。

Conclusion: 该框架能可靠组装连贯、可验证的训练实例，无需特定领域规则，有强大泛化能力。

Abstract: Reliable verifiable data has become a key driver of capability gains in
modern language models, enabling stable reinforcement learning with verifiable
rewards and effective distillation that transfers competence across math,
coding, and agentic tasks. Yet constructing generalizable synthetic verifiable
data remains difficult due to hallucination-prone generation, and weak or
trivial verification artifacts that fail to separate strong from weak
solutions. Existing approaches often rely on task-specific heuristics or
post-hoc filters that do not transfer across domains and lack a principled,
universal evaluator of verifiability. In this work, we introduce an
evolutionary, task-agnostic, strategy-guided, executably-checkable data
synthesis framework that, from minimal seed supervision, jointly synthesizes
problems, diverse candidate solutions, and verification artifacts, and
iteratively discovers strategies via a consistency-based evaluator that
enforces agreement between human-annotated and strategy-induced checks. This
pipeline upgrades filtering into principled synthesis: it reliably assembles
coherent, verifiable training instances and generalizes without domain-specific
rules. Our experiments demonstrate the effectiveness of the proposed approach
under both RLVR and model distillation training paradigms. The results show
that training with our synthesized data yields significant improvements on both
the LiveCodeBench and AgentBench-OS tasks, highlighting the robust
generalization of our framework.

</details>


### [91] [Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism](https://arxiv.org/abs/2510.17896)
*Tao Bu,Qiangang Wang,Bowen Zeng,Hanwen Sun,Yunpeng Huang,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: 本文提出统一基准评估长上下文大语言模型注意力机制，通过实验提供设计和部署指导。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的大语言模型标准注意力机制有计算和内存瓶颈，且相关评估不系统。

Method: 提出统一基准，集成代表性注意力内核和上下文并行机制，从注意力掩码模式、序列长度和分布式规模两维度评估。

Result: 在多达96个GPU的集群上实验，实现可重复比较，突出方法权衡。

Conclusion: 该基准为长上下文大语言模型训练中注意力机制的设计和部署提供实用指导。

Abstract: Transformer-based large language models (LLMs) have achieved remarkable
success, yet their standard attention mechanism incurs quadratic computation
and memory costs with respect to sequence length, posing a major bottleneck for
long-context training. Prior work tackles this challenge along two directions:
(1) kernel-level optimizations, which accelerate dense and sparse attention
operators; and (2) module-level strategies, often referred to as distributed
attention or context parallel training, which scale attention across multiple
devices. However, systematic evaluation still remains limited: operator-level
comparisons are often incomplete, while context parallel strategies are
typically framework-specific, with unclear performance analysis across
contexts. To address these gaps, we propose a unified benchmark that integrates
representative attention kernels and context parallel mechanisms with a modular
and extensible interface for evaluation. The benchmark evaluates methods along
two critical dimensions: (1) attention mask patterns, which strongly affect
efficiency, scalability, and usability, and (2) sequence length and distributed
scale, which determine performance under extreme long-context training. Through
comprehensive experiments on the cluster of up to 96 GPUs, our benchmark
enables reproducible comparisons, highlights method-specific trade-offs, and
provides practical guidance for designing and deploying attention mechanisms in
long-context LLM training.

</details>


### [92] [L-MoE: End-to-End Training of a Lightweight Mixture of Low-Rank Adaptation Experts](https://arxiv.org/abs/2510.17898)
*Shihao Ji,Zihui Song*

Main category: cs.LG

TL;DR: 本文提出L - MoE框架，将MoE和LoRA结合，构建参数高效、模块化、可端到端训练的语言模型。


<details>
  <summary>Details</summary>
Motivation: 结合Mixture of Experts (MoE)架构和Low - Rank Adaptation (LoRA)技术优势，构建更高效、可扩展和专业化的语言模型。

Method: 提出L - MoE框架，将MoE专家定义为低秩适配器集合，用轻量级门控网络动态组合适配器参数，且组合可微分，能端到端训练。

Result: 提出L - MoE的正式数学框架，详细说明了可微路由机制和联合优化目标。

Conclusion: 该方法为构建更高效、可扩展和专业化的语言模型提供了新途径。

Abstract: The Mixture of Experts (MoE) architecture enables the scaling of Large
Language Models (LLMs) to trillions of parameters by activating a sparse subset
of weights for each input, maintaining constant computational cost during
inference. Concurrently, Low-Rank Adaptation (LoRA) has emerged as a dominant
technique for parameter-efficiently fine-tuning LLMs on specialized tasks. In
this work, we unify these two paradigms into a novel, end-to-end trainable
framework named L-MoE: a Lightweight Mixture of LoRA Experts. L-MoE redefines
MoE experts not as dense feed-forward networks, but as a collection of
task-specialized, low-rank adapters. A lightweight gating network, trained
jointly with the experts, learns to dynamically compose these LoRA adapters by
computing a weighted average of their parameters for each input token. This
composition is fully differentiable, allowing gradients from a standard
auto-regressive language modeling objective to flow back through the entire
architecture, simultaneously refining both the expert adapters and the routing
strategy. This approach creates a highly parameter-efficient MoE model that is
modular by design, allows for dynamic skill composition, and is trainable from
end-to-end. We present the formal mathematical framework for L-MoE, detailing
the differentiable routing mechanism and the joint optimization objective,
thereby providing a new path toward building more efficient, scalable, and
specialized language models.

</details>


### [93] [The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications](https://arxiv.org/abs/2510.17901)
*Alex Acero,Daniel M. Jimenez-Gutierrez,Dario Pighin,Enrique Zuazua,Joaquin Del Rio,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 本文提出Sherpa.ai盲垂直联邦学习（SBVFL）范式，可减少节点与服务器通信，实验显示相比标准VFL减少约99%通信量，能实现跨敏感领域的实用隐私保护VFL。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习（VFL）训练时通信量巨大，损害隐私和安全，能耗高，甚至使模型训练不可行，需改进。

Method: 引入SBVFL，利用增强隐私和安全的分布式训练机制，将大部分节点更新与服务器解耦。

Result: SBVFL相比标准VFL减少约99%通信量，同时保持准确性和鲁棒性。

Conclusion: SBVFL能实现跨医疗、金融等敏感领域的实用隐私保护VFL。

Abstract: Federated Learning (FL) enables collaborative decentralized training across
multiple parties (nodes) while keeping raw data private. There are two main
paradigms in FL: Horizontal FL (HFL), where all participant nodes share the
same feature space but hold different samples, and Vertical FL (VFL), where
participants hold complementary features for the same samples. While HFL is
widely adopted, VFL is employed in domains where nodes hold complementary
features about the same samples. Still, VFL presents a significant limitation:
the vast number of communications required during training. This compromises
privacy and security, and can lead to high energy consumption, and in some
cases, make model training unfeasible due to the high number of communications.
  In this paper, we introduce Sherpa.ai Blind Vertical Federated Learning
(SBVFL), a novel paradigm that leverages a distributed training mechanism
enhanced for privacy and security. Decoupling the vast majority of node updates
from the server dramatically reduces node-server communication. Experiments
show that SBVFL reduces communication by ~99% compared to standard VFL while
maintaining accuracy and robustness. Therefore, SBVFL enables practical,
privacy-preserving VFL across sensitive domains, including healthcare, finance,
manufacturing, aerospace, cybersecurity, and the defense industry.

</details>


### [94] [Efficient Long-context Language Model Training by Core Attention Disaggregation](https://arxiv.org/abs/2510.18121)
*Yonghao Zhuang,Junda Chen,Bo Pang,Yi Gu,Yibo Zhu,Yimin Jiang,Ion Stoica,Eric Xing,Hao Zhang*

Main category: cs.LG

TL;DR: 提出核心注意力分解技术CAD，实现长上下文大语言模型训练优化，DistCA系统验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有系统中核心注意力与其他层共置，长上下文时计算增长的不均衡导致负载失衡和拖后腿现象。

Method: CAD将核心注意力分解为令牌级任务并分发到专用服务器，动态重新批处理任务；DistCA系统采用乒乓执行方案和就地执行减少内存使用。

Result: 在512个H200 GPU和最长512k令牌上下文长度下，DistCA端到端训练吞吐量最高提升1.35倍，消除并行拖后腿现象，实现计算和内存近乎完美平衡。

Conclusion: CAD技术能有效提升长上下文大语言模型训练效率和平衡性。

Abstract: We present core attention disaggregation (CAD), a technique that improves
long-context large language model training by decoupling the core attention
computation, softmax(QK^T)V, from the rest of the model and executing it on a
separate pool of devices. In existing systems, core attention is colocated with
other layers; at long context lengths, its quadratic compute growth compared to
the near-linear growth of other components causes load imbalance and stragglers
across data and pipeline parallel groups. CAD is enabled by two observations.
First, core attention is stateless: it has no trainable parameters and only
minimal transient data, so balancing reduces to scheduling compute-bound tasks.
Second, it is composable: modern attention kernels retain high efficiency when
processing fused batches of token-level shards with arbitrary lengths. CAD
partitions core attention into token-level tasks and dispatches them to
dedicated attention servers, which dynamically rebatch tasks to equalize
compute without sacrificing kernel efficiency. We implement CAD in a system
called DistCA, which uses a ping-pong execution scheme to fully overlap
communication with computation and in-place execution on attention servers to
reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens,
DistCA improves end-to-end training throughput by up to 1.35x, eliminates data
and pipeline parallel stragglers, and achieves near-perfect compute and memory
balance.

</details>


### [95] [NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation](https://arxiv.org/abs/2510.17914)
*Rikard Vinge,Isabelle Wittmann,Jannik Schneider,Michael Marszalek,Luis Gilch,Thomas Brunschwiler,Conrad M Albrecht*

Main category: cs.LG

TL;DR: 介绍NeuCo - Bench基准框架用于评估地球观测中的神经压缩和表征学习，含核心组件、发布数据集，展示结果，为标准化评估奠基。


<details>
  <summary>Details</summary>
Motivation: 在地球观测背景下，需要一个标准化的评估框架来评估（有损）神经压缩和表征学习。

Method: 构建包含可复用嵌入评估管道、隐藏任务排行榜挑战模式和平衡准确性与稳定性评分系统的NeuCo - Bench框架，发布SSL4EO - S12 - downstream数据集。

Result: 展示在2025 CVPR EARTHVISION研讨会上公开挑战的初始结果，并对最先进的基础模型进行消融实验。

Conclusion: NeuCo - Bench为社区驱动的、标准化评估地球观测及其他领域的神经嵌入迈出了第一步。

Abstract: We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy)
neural compression and representation learning in the context of Earth
Observation (EO). Our approach builds on fixed-size embeddings that act as
compact, task-agnostic representations applicable to a broad range of
downstream tasks. NeuCo-Bench comprises three core components: (i) an
evaluation pipeline built around reusable embeddings, (ii) a new challenge mode
with a hidden-task leaderboard designed to mitigate pretraining bias, and (iii)
a scoring system that balances accuracy and stability. To support
reproducibility, we release SSL4EO-S12-downstream, a curated multispectral,
multitemporal EO dataset. We present initial results from a public challenge at
the 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-art
foundation models. NeuCo-Bench provides a first step towards community-driven,
standardized evaluation of neural embeddings for EO and beyond.

</details>


### [96] [Memorizing Long-tail Data Can Help Generalization Through Composition](https://arxiv.org/abs/2510.16322)
*Mo Zhou,Haoyang Ma,Rong Ge*

Main category: cs.LG

TL;DR: 本文探讨记忆与简单组合的协同作用，理论证明线性场景下二者有助于模型预测，实验表明该结论可拓展到神经网络架构且组合能力与架构有关。


<details>
  <summary>Details</summary>
Motivation: 深度学习促使重新思考记忆与泛化关系，考虑记忆与简单组合的协同作用。

Method: 理论上分析线性场景，通过神经网络架构在简单数据上进行实验。

Result: 线性场景下记忆与组合能助模型预测罕见测试示例；实验表明理论见解可拓展到线性外场景，模型组合能力与架构有关。

Conclusion: 记忆与简单组合的协同作用有助于模型预测，且模型组合能力受架构影响。

Abstract: Deep learning has led researchers to rethink the relationship between
memorization and generalization. In many settings, memorization does not hurt
generalization due to implicit regularization and may help by memorizing
long-tailed examples. In this paper, we consider the synergy between
memorization and simple composition -- the ability to make correct prediction
on a combination of long-tailed features. Theoretically, we show that for a
linear setting, memorization together with composition can help the model make
correct predictions on rare test examples that require a combination of
long-tailed features, even if such combinations were never observed in the
training data. Experiments on neural network architecture on simple data show
that the theoretical insight extends beyond the linear setting, and we further
observe that the composition capability of the model depends on its
architecture.

</details>


### [97] [Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection](https://arxiv.org/abs/2510.17917)
*Jinseong Park,Mijung Park*

Main category: cs.LG

TL;DR: 本文聚焦扩散模型数据去学习，提出时频选择方法提升生成质量，并提出归一化SSCD评估方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的数据去学习研究不足，现有方法存在质量下降或遗忘不完全问题。

Method: 提出时频选择方法，在训练时选择性关注特定时频范围；提出归一化SSCD评估方法。

Result: 时频选择方法在不同设置和任务中实现了更高美学质量和更低噪声的样本生成。

Conclusion: 分析和方法增进了对扩散模型数据去学习挑战的理解，提供了改进评估和去学习性能的实用策略。

Abstract: Data unlearning aims to remove the influence of specific training samples
from a trained model without requiring full retraining. Unlike concept
unlearning, data unlearning in diffusion models remains underexplored and often
suffers from quality degradation or incomplete forgetting. To address this, we
first observe that most existing methods attempt to unlearn the samples at all
diffusion time steps equally, leading to poor-quality generation. We argue that
forgetting occurs disproportionately across time and frequency, depending on
the model and scenarios. By selectively focusing on specific time-frequency
ranges during training, we achieve samples with higher aesthetic quality and
lower noise. We validate this improvement by applying our time-frequency
selective approach to diverse settings, including gradient-based and preference
optimization objectives, as well as both image-level and text-to-image tasks.
Finally, to evaluate both deletion and quality of unlearned data samples, we
propose a simple normalized version of SSCD. Together, our analysis and methods
establish a clearer understanding of the unique challenges in data unlearning
for diffusion models, providing practical strategies to improve both evaluation
and unlearning performance.

</details>


### [98] [Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity](https://arxiv.org/abs/2510.18037)
*Ziyu Lu,Anna J. Li,Alexander E. Ladd,Pascha Matveev,Aditya Deole,Eric Shea-Brown,J. Nathan Kutz,Nicholas A. Steinmetz*

Main category: cs.LG

TL;DR: 系统评估8种概率深度学习模型用于小鼠皮层神经活动预测，发现深度学习模型表现优于经典方法，为未来控制应用和探究神经活动时间结构提供方向。


<details>
  <summary>Details</summary>
Motivation: 深度学习在神经活动预测中的应用有限，需评估其在该领域的性能。

Method: 系统评估8种概率深度学习模型（含2种基础模型），与4种经典统计模型和2种基线方法对比，使用宽场成像记录小鼠皮层自发神经活动。

Result: 多个深度学习模型在各预测时程上始终优于经典方法，最佳模型可对未来1.5秒的神经活动做出有效预测。

Conclusion: 研究结果为未来控制应用提供方向，为探究神经活动内在时间结构开辟新途径。

Abstract: Neural activity forecasting is central to understanding neural systems and
enabling closed-loop control. While deep learning has recently advanced the
state-of-the-art in the time series forecasting literature, its application to
neural activity forecasting remains limited. To bridge this gap, we
systematically evaluated eight probabilistic deep learning models, including
two foundation models, that have demonstrated strong performance on general
forecasting benchmarks. We compared them against four classical statistical
models and two baseline methods on spontaneous neural activity recorded from
mouse cortex via widefield imaging. Across prediction horizons, several deep
learning models consistently outperformed classical approaches, with the best
model producing informative forecasts up to 1.5 seconds into the future. Our
findings point toward future control applications and open new avenues for
probing the intrinsic temporal structure of neural activity.

</details>


### [99] [Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning](https://arxiv.org/abs/2510.17923)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Wei Ju,Jiancheng Lv,Deng Xiong,Ziyue Qiao*

Main category: cs.LG

TL;DR: 现有强化学习方法在大语言模型应用中依赖人工标注数据有扩展性瓶颈，提出无监督的COMPASS机制，实验显示其在推理任务和模型架构上有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法严重依赖人工标注数据进行奖励建模，存在扩展性瓶颈，需探索无标注数据上的强化学习。

Method: 引入COMPASS机制，包含DCAR和DPR两个互补组件，分别用于建立可信伪标签和优化推理过程质量。

Result: 在不同推理任务和模型架构上取得显著且一致的性能提升。

Conclusion: COMPASS为大语言模型从连续经验中学习提供了更具扩展性的方向。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing
Large Language Models (LLMs), achieving remarkable performance in complex
reasoning domains such as mathematics and code generation. However, current RL
methods face a fundamental scalability bottleneck due to their heavy reliance
on human-curated preference data or labeled datasets for reward modeling. To
overcome this limitation, we explore RL on unlabeled data where models learn
autonomously from continuous experience streams. The core challenge in this
setting lies in reliable reward estimation without ground-truth supervision.
Existing approaches like Test-Time RL address this through self-consistent
consensus, but risk reinforcing incorrect pseudo-labels derived from majority
voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel
test-time reward mechanism that operates without external supervision. COMPASS
integrates two complementary components: the Dual-Calibration Answer Reward
(DCAR), which stabilizes training by establishing trustworthy pseudo-labels
through confidence and credibility calibration, and the Decisive Path Reward
(DPR), which directly optimizes the reasoning process quality beyond mere
outcome supervision. By jointly reinforcing trustworthy consensus answers and
highly decisive reasoning chains, the COMPASS systematically enhances the
model's analytical capabilities. Extensive experiments show that COMPASS
achieves significant and consistent performance gains across diverse reasoning
tasks and model architectures, advancing a more scalable direction for LLMs to
learn from continuous experience.

</details>


### [100] [Latent Discrete Diffusion Models](https://arxiv.org/abs/2510.18114)
*Dario Shariatian,Alain Durmus,Stefano Peluchetti*

Main category: cs.LG

TL;DR: 研究离散扩散用于语言和分类数据，提出LDDMs模型，有两种实例，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决掩码去噪器在少步生成中因位置过渡分解导致联合结构减弱和质量下降的问题。

Method: 提出Latent Discrete Diffusion Models (LDDMs)，将对标记的掩码离散扩散与对潜在嵌入的连续扩散相结合，给出FUJI - LDDMs和SEQ - LDDMs两种实例。

Result: LDDMs在无条件生成指标上优于现有最先进的掩码离散扩散基线，在低采样预算下有效。

Conclusion: LDDMs能解决现有掩码去噪器的问题，在相关任务中有较好表现。

Abstract: We study discrete diffusion for language and other categorical data and focus
on a common limitation of masked denoisers: reverse transitions typically
factorize across positions, which can weaken joint structure and degrade
quality in few-step generation. We propose \emph{Latent Discrete Diffusion
Models} (LDDMs), which couple a masked discrete diffusion over tokens with a
continuous diffusion over latent embeddings. The latent channel provides a
softer signal and carries cross-token dependencies that help resolve
ambiguities. We present two instantiations: (i) FUJI-LDDMs, which perform fully
joint denoising of tokens and latents, and (ii) SEQ-LDDMs, which sequentially
resolve the latent and then the discrete chain conditionally on it. For both
variants we derive ELBO-style objectives and discuss design choices to learn
informative latents yet amenable to diffusoin modeling. In experiments, LDDMs
yield improvements on unconditional generation metrics as compared to
state-of-the-art masked discrete diffusion baselines, and are effective at
lower sampling budgets, where unmasking many tokens per step is desirable.

</details>


### [101] [From Observations to Parameters: Detecting Changepoint in Nonlinear Dynamics with Simulation-based Inference](https://arxiv.org/abs/2510.17933)
*Xiangbo Deng,Cheng Chen,Peng Yang*

Main category: cs.LG

TL;DR: 提出Param - CPD框架用于混沌时间序列制度转换检测，在Lorenz - 63上表现优于观测空间基线，证明参数空间检测优势。


<details>
  <summary>Details</summary>
Motivation: 混沌时间序列中观测空间信号与内在变异性纠缠，难以检测制度转换。

Method: 提出两阶段的Param - CPD框架，先通过基于模拟推理训练的神经后验估计器摊销控制参数的贝叶斯推理，再对参数轨迹应用标准CPD算法。

Result: 在Lorenz - 63上，Param - CPD提高F1值、减少定位误差、降低误报率；验证了平稳轨迹上推断后验的可识别性和校准性；鲁棒性分析显示持续优势。

Conclusion: 在物理可解释的参数空间中操作可实现非线性动力系统中准确且可解释的变点检测。

Abstract: Detecting regime shifts in chaotic time series is hard because
observation-space signals are entangled with intrinsic variability. We propose
Parameter--Space Changepoint Detection (Param--CPD), a two--stage framework
that first amortizes Bayesian inference of governing parameters with a neural
posterior estimator trained by simulation-based inference, and then applies a
standard CPD algorithm to the resulting parameter trajectory. On Lorenz--63
with piecewise-constant parameters, Param--CPD improves F1, reduces
localization error, and lowers false positives compared to observation--space
baselines. We further verify identifiability and calibration of the inferred
posteriors on stationary trajectories, explaining why parameter space offers a
cleaner detection signal. Robustness analyses over tolerance, window length,
and noise indicate consistent gains. Our results show that operating in a
physically interpretable parameter space enables accurate and interpretable
changepoint detection in nonlinear dynamical systems.

</details>


### [102] [Rethinking PCA Through Duality](https://arxiv.org/abs/2510.18130)
*Jan Quan,Johan Suykens,Panagiotis Patrinos*

Main category: cs.LG

TL;DR: 本文受自注意力与PCA联系启发，用DC框架研究PCA，给出新公式、理论见解、新算法，并对比现有方法，还引入可核化对偶公式。


<details>
  <summary>Details</summary>
Motivation: 受自注意力与PCA联系启发，重新研究PCA基础。

Method: 使用差凸（DC）框架，提出新公式，将同时迭代与DCA联系起来，描述新算法。

Result: 展示了类PCA问题的可核化和样本外适用性，发现同时迭代是DCA的一个实例，进行了新算法与现有方法的实证比较，引入可核化对偶公式。

Conclusion: 通过DC框架为PCA研究提供了新的理论见解和算法。

Abstract: Motivated by the recently shown connection between self-attention and
(kernel) principal component analysis (PCA), we revisit the fundamentals of
PCA. Using the difference-of-convex (DC) framework, we present several novel
formulations and provide new theoretical insights. In particular, we show the
kernelizability and out-of-sample applicability for a PCA-like family of
problems. Moreover, we uncover that simultaneous iteration, which is connected
to the classical QR algorithm, is an instance of the difference-of-convex
algorithm (DCA), offering an optimization perspective on this longstanding
method. Further, we describe new algorithms for PCA and empirically compare
them with state-of-the-art methods. Lastly, we introduce a kernelizable dual
formulation for a robust variant of PCA that minimizes the $l_1$ deviation of
the reconstruction errors.

</details>


### [103] [UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts](https://arxiv.org/abs/2510.17937)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.LG

TL;DR: 提出UniRL - Zero统一强化学习框架，定义六种场景并提供基线，代码开源。


<details>
  <summary>Details</summary>
Motivation: 提升多模态语言模型理解推理、扩散模型多媒体生成及交互能力。

Method: 提出UniRL - Zero框架，定义六种统一模型强化学习场景。

Result: 得到可提升相关能力的统一模型，提供系统基线。

Conclusion: UniRL - Zero框架有助于统一理解和生成模型的强化学习。

Abstract: We present UniRL-Zero, a unified reinforcement learning (RL) framework that
boosts, multimodal language model understanding and reasoning, diffusion model
multimedia generation, and their beneficial interaction capabilities within a
unified model. Our work defines six scenarios for unified model reinforcement
learning, providing systematic baselines for reinforcement learning of unified
understanding and generation model. Our code is available at
https://github.com/G-U-N/UniRL.

</details>


### [104] [Demystifying Transition Matching: When and Why It Can Beat Flow Matching](https://arxiv.org/abs/2510.17991)
*Jaihoon Kim,Rajarshi Saha,Minhyuk Sung,Youngsuk Park*

Main category: cs.LG

TL;DR: 本文探讨Transition Matching (TM)何时及为何优于Flow Matching (FM)，证明在单峰高斯分布和特定高斯混合分布中TM优势，实验验证理论结果并拓展到真实应用。


<details>
  <summary>Details</summary>
Motivation: 解释TM在生成模型中比FM表现更好的时间和原因。

Method: 先分析单峰高斯分布，证明TM在有限步数下KL散度更低及收敛更快；再拓展到高斯混合分布，确定TM优于FM的局部单峰区域；最后进行实验验证。

Result: TM在目标分布模式分离良好且方差不可忽略时优于FM；目标方差趋近于0时，TM性能优势减弱。

Conclusion: TM在目标分布有良好分离模式和不可忽略方差时比FM表现更好，理论结果在实验和真实应用中得到验证。

Abstract: Flow Matching (FM) underpins many state-of-the-art generative models, yet
recent results indicate that Transition Matching (TM) can achieve higher
quality with fewer sampling steps. This work answers the question of when and
why TM outperforms FM. First, when the target is a unimodal Gaussian
distribution, we prove that TM attains strictly lower KL divergence than FM for
finite number of steps. The improvement arises from stochastic difference
latent updates in TM, which preserve target covariance that deterministic FM
underestimates. We then characterize convergence rates, showing that TM
achieves faster convergence than FM under a fixed compute budget, establishing
its advantage in the unimodal Gaussian setting. Second, we extend the analysis
to Gaussian mixtures and identify local-unimodality regimes in which the
sampling dynamics approximate the unimodal case, where TM can outperform FM.
The approximation error decreases as the minimal distance between component
means increases, highlighting that TM is favored when the modes are well
separated. However, when the target variance approaches zero, each TM update
converges to the FM update, and the performance advantage of TM diminishes. In
summary, we show that TM outperforms FM when the target distribution has
well-separated modes and non-negligible variances. We validate our theoretical
results with controlled experiments on Gaussian distributions, and extend the
comparison to real-world applications in image and video generation.

</details>


### [105] [Attention-Guided Deep Adversarial Temporal Subspace Clustering (A-DATSC) Model for multivariate spatiotemporal data](https://arxiv.org/abs/2510.18004)
*Francis Ndikum Nji,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出A - DATSC模型解决现有深度子空间聚类模型问题，在三个真实数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度子空间聚类模型存在忽略聚类误差、忽视局部结构、无法建模长距离依赖和位置信息、很少用于4D时空数据等问题，需改进。

Method: 提出A - DATSC模型，结合深度子空间聚类生成器和质量验证判别器，生成器受U - Net启发，使用堆叠TimeDistributed ConvLSTM2D层，还有基于图注意力变压器的自表达网络。

Result: 在三个真实世界的多变量时空数据集实验中，A - DATSC聚类性能明显优于现有深度子空间聚类模型。

Conclusion: A - DATSC模型有效解决了现有深度子空间聚类模型的问题，提升了聚类性能。

Abstract: Deep subspace clustering models are vital for applications such as snowmelt
detection, sea ice tracking, crop health monitoring, infectious disease
modeling, network load prediction, and land-use planning, where multivariate
spatiotemporal data exhibit complex temporal dependencies and reside on
multiple nonlinear manifolds beyond the capability of traditional clustering
methods. These models project data into a latent space where samples lie in
linear subspaces and exploit the self-expressiveness property to uncover
intrinsic relationships. Despite their success, existing methods face major
limitations: they use shallow autoencoders that ignore clustering errors,
emphasize global features while neglecting local structure, fail to model
long-range dependencies and positional information, and are rarely applied to
4D spatiotemporal data. To address these issues, we propose A-DATSC
(Attention-Guided Deep Adversarial Temporal Subspace Clustering), a model
combining a deep subspace clustering generator and a quality-verifying
discriminator. The generator, inspired by U-Net, preserves spatial and temporal
integrity through stacked TimeDistributed ConvLSTM2D layers, reducing
parameters and enhancing generalization. A graph attention transformer based
self-expressive network captures local spatial relationships, global
dependencies, and both short- and long-range correlations. Experiments on three
real-world multivariate spatiotemporal datasets show that A-DATSC achieves
substantially superior clustering performance compared to state-of-the-art deep
subspace clustering models.

</details>


### [106] [Uncertainty Estimation by Flexible Evidential Deep Learning](https://arxiv.org/abs/2510.18322)
*Taeseong Yoon,Heeyoung Kim*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Uncertainty quantification (UQ) is crucial for deploying machine learning
models in high-stakes applications, where overconfident predictions can lead to
serious consequences. An effective UQ method must balance computational
efficiency with the ability to generalize across diverse scenarios. Evidential
deep learning (EDL) achieves efficiency by modeling uncertainty through the
prediction of a Dirichlet distribution over class probabilities. However, the
restrictive assumption of Dirichlet-distributed class probabilities limits
EDL's robustness, particularly in complex or unforeseen situations. To address
this, we propose \textit{flexible evidential deep learning}
($\mathcal{F}$-EDL), which extends EDL by predicting a flexible Dirichlet
distribution -- a generalization of the Dirichlet distribution -- over class
probabilities. This approach provides a more expressive and adaptive
representation of uncertainty, significantly enhancing UQ generalization and
reliability under challenging scenarios. We theoretically establish several
advantages of $\mathcal{F}$-EDL and empirically demonstrate its
state-of-the-art UQ performance across diverse evaluation settings, including
classical, long-tailed, and noisy in-distribution scenarios.

</details>


### [107] [Cross-Domain Long-Term Forecasting: Radiation Dose from Sparse Neutron Sensor via Spatio-Temporal Operator Network](https://arxiv.org/abs/2510.18041)
*Jay Phil Yoo,Kazuma Kobayashi,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: 提出时空算子网络STONe，可从稀疏跨域传感器数据预测不可观测物理量，挑战传统观点并实现准确长时预测。


<details>
  <summary>Details</summary>
Motivation: 解决科学机器学习中从稀疏跨域传感器数据预测不可观测物理量这一未解决的核心问题，现有方法假设在现实系统中不成立。

Method: 引入非自回归神经算子STONe，学习异构域之间稳定的函数映射，定义传感器和目标流形之间的非线性算子。

Result: 基于23年全球中子数据训练，STONe实现180天准确预测，推理延迟为毫秒级。

Conclusion: 该框架为跨域算子推理建立通用原则，可实现物理、气候和能源系统中复杂时空场的实时预测。

Abstract: Forecasting unobservable physical quantities from sparse, cross-domain sensor
data is a central unsolved problem in scientific machine learning. Existing
neural operators and large-scale forecasters rely on dense, co-located
input-output fields and short temporal contexts, assumptions that fail in
real-world systems where sensing and prediction occur on distinct physical
manifolds and over long timescales. We introduce the Spatio-Temporal Operator
Network (STONe), a non-autoregressive neural operator that learns a stable
functional mapping between heterogeneous domains. By directly inferring
high-altitude radiation dose fields from sparse ground-based neutron
measurements, STONe demonstrates that operator learning can generalize beyond
shared-domain settings. It defines a nonlinear operator between sensor and
target manifolds that remains stable over long forecasting horizons without
iterative recurrence. This challenges the conventional view that operator
learning requires domain alignment or autoregressive propagation. Trained on 23
years of global neutron data, STONe achieves accurate 180-day forecasts with
millisecond inference latency. The framework establishes a general principle
for cross-domain operator inference, enabling real-time prediction of complex
spatiotemporal fields in physics, climate, and energy systems.

</details>


### [108] [Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions](https://arxiv.org/abs/2510.18638)
*Yanna Ding,Songtao Lu,Yingdong Lu,Tomasz Nowicki,Jianxi Gao*

Main category: cs.LG

TL;DR: 研究Transformer架构在上下文学习中对动态驱动函数的建模，给出单层线性自注意力模型全局极小值表达式等理论结果并验证。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习理论研究多聚焦线性回归任务，为理解Transformer在建模动态驱动函数时如何表达上下文学习。

Method: 通过结构化上下文学习设置研究马尔可夫函数学习，刻画损失景观以揭示潜在优化行为。

Result: 给出单层线性自注意力模型全局极小值闭式表达；证明恢复实现最优解的Transformer参数一般是NP难问题；给出多层线性自注意力模型新解释。

Conclusion: 得到关于Transformer架构在上下文学习中对动态驱动函数建模的理论结果，并通过简化Transformer数值验证。

Abstract: Transformer architectures can solve unseen tasks based on input-output pairs
in a given prompt due to in-context learning (ICL). Existing theoretical
studies on ICL have mainly focused on linear regression tasks, often with
i.i.d. inputs. To understand how transformers express ICL when modeling
dynamics-driven functions, we investigate Markovian function learning through a
structured ICL setup, where we characterize the loss landscape to reveal
underlying optimization behaviors. Specifically, we (1) provide the closed-form
expression of the global minimizer (in an enlarged parameter space) for a
single-layer linear self-attention (LSA) model; (2) prove that recovering
transformer parameters that realize the optimal solution is NP-hard in general,
revealing a fundamental limitation of one-layer LSA in representing structured
dynamical functions; and (3) supply a novel interpretation of a multilayer LSA
as performing preconditioned gradient descent to optimize multiple objectives
beyond the square loss. These theoretical results are numerically validated
using simplified transformers.

</details>


### [109] [Measure-Theoretic Anti-Causal Representation Learning](https://arxiv.org/abs/2510.18052)
*Arman Behnam,Binghui Wang*

Main category: cs.LG

TL;DR: 提出用于反因果表征学习的ACIA框架，实验表明其优于现有方法，理论结果证实其对鲁棒反因果学习有效。


<details>
  <summary>Details</summary>
Motivation: 反因果设置下的因果表征学习有独特挑战，需专门方法。

Method: 提出ACIA框架，采用两级设计，通过干预核处理干预，消除对显式因果结构的依赖，有效处理高维数据。

Result: 在合成和真实医疗数据集实验中，ACIA在准确性和不变性指标上始终优于现有方法，理论结果给出训练和未知环境性能差距的紧密界限。

Conclusion: ACIA框架对鲁棒反因果学习有效。

Abstract: Causal representation learning in the anti-causal setting (labels cause
features rather than the reverse) presents unique challenges requiring
specialized approaches. We propose Anti-Causal Invariant Abstractions (ACIA), a
novel measure-theoretic framework for anti-causal representation learning. ACIA
employs a two-level design, low-level representations capture how labels
generate observations, while high-level representations learn stable causal
patterns across environment-specific variations. ACIA addresses key limitations
of existing approaches by accommodating prefect and imperfect interventions
through interventional kernels, eliminating dependency on explicit causal
structures, handling high-dimensional data effectively, and providing
theoretical guarantees for out-of-distribution generalization. Experiments on
synthetic and real-world medical datasets demonstrate that ACIA consistently
outperforms state-of-the-art methods in both accuracy and invariance metrics.
Furthermore, our theoretical results establish tight bounds on performance gaps
between training and unseen environments, confirming the efficacy of our
approach for robust anti-causal learning.

</details>


### [110] [Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models](https://arxiv.org/abs/2510.18053)
*Jiajun Fan,Tong Wei,Chaoran Cheng,Yuxin Chen,Ge Liu*

Main category: cs.LG

TL;DR: 提出ADRPO方法解决生成模型强化学习微调中探索与利用的平衡问题，在多任务中取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型强化学习微调方法采用固定散度正则化，存在强正则化限制奖励优化、弱正则化有不稳定或奖励破解风险的困境。

Method: 引入ADRPO，根据优势估计自动调整正则化强度，对高价值样本减少正则化，对低价值样本加强正则化。

Result: 在文本到图像生成中表现出色，使2B参数模型超越更大参数模型；能推广到文本大模型和多模态推理模型的微调，在大模型微调中展现出逃逸局部最优的能力，在多模态音频推理中超越GRPO。

Conclusion: ADRPO为不同生成架构和模态的探索 - 利用挑战提供了有效的即插即用解决方案。

Abstract: Balancing exploration and exploitation during reinforcement learning
fine-tuning of generative models presents a critical challenge, as existing
approaches rely on fixed divergence regularization that creates an inherent
dilemma: strong regularization preserves model capabilities but limits reward
optimization, while weak regularization enables greater alignment but risks
instability or reward hacking. We introduce Adaptive Divergence Regularized
Policy Optimization (ADRPO), which automatically adjusts regularization
strength based on advantage estimates-reducing regularization for high-value
samples while applying stronger regularization to poor samples, enabling
policies to navigate between exploration and aggressive exploitation according
to data quality. Our implementation with Wasserstein-2 regularization for flow
matching generative models achieves remarkable results on text-to-image
generation, achieving better semantic alignment and diversity than offline
methods like DPO and online methods with fixed regularization like ORW-CFM-W2.
ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B
and 12B parameters in attribute binding, semantic consistency, artistic style
transfer, and compositional control while maintaining generation diversity.
ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and
multi-modal reasoning models, enhancing existing online RL methods like GRPO.
In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local
optima through active exploration, while in multi-modal audio reasoning, it
outperforms GRPO through superior step-by-step reasoning, enabling a 7B model
to outperform substantially larger commercial models including Gemini 2.5 Pro
and GPT-4o Audio, offering an effective plug-and-play solution to the
exploration-exploitation challenge across diverse generative architectures and
modalities.

</details>


### [111] [Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options](https://arxiv.org/abs/2510.18713)
*Joongkyu Lee,Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study online preference-based reinforcement learning (PbRL) with the goal
of improving sample efficiency. While a growing body of theoretical work has
emerged-motivated by PbRL's recent empirical success, particularly in aligning
large language models (LLMs)-most existing studies focus only on pairwise
comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024,
Thekumparampil et al., 2024) have explored using multiple comparisons and
ranking feedback, but their performance guarantees fail to improve-and can even
deteriorate-as the feedback length increases, despite the richer information
available. To address this gap, we adopt the Plackett-Luce (PL) model for
ranking feedback over action subsets and propose M-AUPO, an algorithm that
selects multiple actions by maximizing the average uncertainty within the
offered subset. We prove that M-AUPO achieves a suboptimality gap of
$\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}}
\right)$, where $T$ is the total number of rounds, $d$ is the feature
dimension, and $|S_t|$ is the size of the subset at round $t$. This result
shows that larger subsets directly lead to improved performance and, notably,
the bound avoids the exponential dependence on the unknown parameter's norm,
which was a fundamental limitation in most previous works. Moreover, we
establish a near-matching lower bound of $\Omega \left( \frac{d}{K \sqrt{T}}
\right)$, where $K$ is the maximum subset size. To the best of our knowledge,
this is the first theoretical result in PbRL with ranking feedback that
explicitly shows improved sample efficiency as a function of the subset size.

</details>


### [112] [SPACeR: Self-Play Anchoring with Centralized Reference Models](https://arxiv.org/abs/2510.18060)
*Wei-Jer Chang,Akshay Rangesh,Kevin Joseph,Matthew Strong,Masayoshi Tomizuka,Yihan Hu,Wei Zhan*

Main category: cs.LG

TL;DR: 提出SPACeR框架，结合预训练模型与自博弈强化学习，在自动驾驶模拟代理任务中表现佳且推理快、参数小。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习和自博弈强化学习在自动驾驶模拟代理策略中有不足，需开发兼具人类行为特性、高效且可扩展的策略。

Method: 提出SPACeR框架，利用预训练的标记自回归运动模型作为集中式参考策略来指导分散式自博弈。

Result: 在Waymo Sim Agents Challenge中表现有竞争力，推理速度快至10倍，参数大小小至50倍；在闭环自我规划评估任务中可有效衡量规划器质量。

Conclusion: SPACeR框架为自动驾驶策略测试建立了新范式。

Abstract: Developing autonomous vehicles (AVs) requires not only safety and efficiency,
but also realistic, human-like behaviors that are socially aware and
predictable. Achieving this requires sim agent policies that are human-like,
fast, and scalable in multi-agent settings. Recent progress in imitation
learning with large diffusion-based or tokenized models has shown that
behaviors can be captured directly from human driving data, producing realistic
policies. However, these models are computationally expensive, slow during
inference, and struggle to adapt in reactive, closed-loop scenarios. In
contrast, self-play reinforcement learning (RL) scales efficiently and
naturally captures multi-agent interactions, but it often relies on heuristics
and reward shaping, and the resulting policies can diverge from human norms. We
propose SPACeR, a framework that leverages a pretrained tokenized
autoregressive motion model as a centralized reference policy to guide
decentralized self-play. The reference model provides likelihood rewards and KL
divergence, anchoring policies to the human driving distribution while
preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our
method achieves competitive performance with imitation-learned policies while
being up to 10x faster at inference and 50x smaller in parameter size than
large generative models. In addition, we demonstrate in closed-loop ego
planning evaluation tasks that our sim agents can effectively measure planner
quality with fast and scalable traffic simulation, establishing a new paradigm
for testing autonomous driving policies.

</details>


### [113] [Enhancing Fractional Gradient Descent with Learned Optimizers](https://arxiv.org/abs/2510.18783)
*Jan Sobotka,Petr Šimánek,Pavel Kordík*

Main category: cs.LG

TL;DR: 提出L2O - CFGD方法解决FGD在收敛和超参数选择上的问题，性能表现良好。


<details>
  <summary>Details</summary>
Motivation: FGD在收敛行为和超参数选择上存在挑战，且超参数影响不明，在非凸环境下调度困难。

Method: 提出Learning to Optimize Caputo Fractional Gradient Descent (L2O - CFGD)方法，通过元学习动态调整Caputo FGD的超参数。

Result: L2O - CFGD的元学习调度优于通过广泛搜索得到的静态超参数的CFGD，在某些任务上与全黑盒元学习优化器性能相当。

Conclusion: L2O - CFGD可帮助研究者识别高性能超参数，理解如何在优化中利用分数微分的历史依赖性。

Abstract: Fractional Gradient Descent (FGD) offers a novel and promising way to
accelerate optimization by incorporating fractional calculus into machine
learning. Although FGD has shown encouraging initial results across various
optimization tasks, it faces significant challenges with convergence behavior
and hyperparameter selection. Moreover, the impact of its hyperparameters is
not fully understood, and scheduling them is particularly difficult in
non-convex settings such as neural network training. To address these issues,
we propose a novel approach called Learning to Optimize Caputo Fractional
Gradient Descent (L2O-CFGD), which meta-learns how to dynamically tune the
hyperparameters of Caputo FGD (CFGD). Our method's meta-learned schedule
outperforms CFGD with static hyperparameters found through an extensive search
and, in some tasks, achieves performance comparable to a fully black-box
meta-learned optimizer. L2O-CFGD can thus serve as a powerful tool for
researchers to identify high-performing hyperparameters and gain insights on
how to leverage the history-dependence of the fractional differential in
optimization.

</details>


### [114] [Fine-tuning Flow Matching Generative Models with Intermediate Feedback](https://arxiv.org/abs/2510.18072)
*Jiajun Fan,Chaoran Cheng,Shuaike Shen,Xiangxin Zhou,Ge Liu*

Main category: cs.LG

TL;DR: 提出AC - Flow框架解决基于流的生成模型微调难题，在文本到图像对齐任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 基于流的生成模型在文本到图像生成中成功，但用中间反馈微调有挑战，现有方法有信用分配问题、训练不稳定和模型崩溃等问题。

Method: 提出AC - Flow框架，包括奖励塑形、双稳定性机制和可扩展的广义批评家加权方案。

Result: 在Stable Diffusion 3上实验表明，AC - Flow在文本到图像对齐任务和泛化到未见人类偏好模型上达到了最先进性能。

Conclusion: 即使使用计算高效的批评家模型，也能在不影响生成质量、多样性和稳定性的情况下稳健微调流模型。

Abstract: Flow-based generative models have shown remarkable success in text-to-image
generation, yet fine-tuning them with intermediate feedback remains
challenging, especially for continuous-time flow matching models. Most existing
approaches solely learn from outcome rewards, struggling with the credit
assignment problem. Alternative methods that attempt to learn a critic via
direct regression on cumulative rewards often face training instabilities and
model collapse in online settings. We present AC-Flow, a robust actor-critic
framework that addresses these challenges through three key innovations: (1)
reward shaping that provides well-normalized learning signals to enable stable
intermediate value learning and gradient control, (2) a novel dual-stability
mechanism that combines advantage clipping to prevent destructive policy
updates with a warm-up phase that allows the critic to mature before
influencing the actor, and (3) a scalable generalized critic weighting scheme
that extends traditional reward-weighted methods while preserving model
diversity through Wasserstein regularization. Through extensive experiments on
Stable Diffusion 3, we demonstrate that AC-Flow achieves state-of-the-art
performance in text-to-image alignment tasks and generalization to unseen human
preference models. Our results demonstrate that even with a computationally
efficient critic model, we can robustly finetune flow models without
compromising generative quality, diversity, or stability.

</details>


### [115] [R2L: Reliable Reinforcement Learning: Guaranteed Return & Reliable Policies in Reinforcement Learning](https://arxiv.org/abs/2510.18074)
*Nadir Farhi*

Main category: cs.LG

TL;DR: 本文提出可靠强化学习新公式，将其转化为标准强化学习问题，用现有算法求解，以可靠路由问题为例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 经典强化学习算法旨在最大化期望回报，而许多实际应用需要保证高平均性能和成功概率的策略。

Method: 提出新公式，将可靠强化学习问题通过状态增强表示转化为标准强化学习问题，适配现有算法。

Result: 理论上证明两种公式等价，数值实验表明该公式得到的策略能有效平衡效率和可靠性。

Conclusion: 可靠强化学习在随机和安全关键环境应用中有潜力。

Abstract: In this work, we address the problem of determining reliable policies in
reinforcement learning (RL), with a focus on optimization under uncertainty and
the need for performance guarantees. While classical RL algorithms aim at
maximizing the expected return, many real-world applications - such as routing,
resource allocation, or sequential decision-making under risk - require
strategies that ensure not only high average performance but also a guaranteed
probability of success. To this end, we propose a novel formulation in which
the objective is to maximize the probability that the cumulative return exceeds
a prescribed threshold. We demonstrate that this reliable RL problem can be
reformulated, via a state-augmented representation, into a standard RL problem,
thereby allowing the use of existing RL and deep RL algorithms without the need
for entirely new algorithmic frameworks. Theoretical results establish the
equivalence of the two formulations and show that reliable strategies can be
derived by appropriately adapting well-known methods such as Q-learning or
Dueling Double DQN. To illustrate the practical relevance of the approach, we
consider the problem of reliable routing, where the goal is not to minimize the
expected travel time but rather to maximize the probability of reaching the
destination within a given time budget. Numerical experiments confirm that the
proposed formulation leads to policies that effectively balance efficiency and
reliability, highlighting the potential of reliable RL for applications in
stochastic and safety-critical environments.

</details>


### [116] [Batch Distillation Data for Developing Machine Learning Anomaly Detection Methods](https://arxiv.org/abs/2510.18075)
*Justus Arweiler,Indra Jungjohann,Aparna Muraleedharan,Heike Leitte,Jakob Burger,Kerstin Münnemann,Fabian Jirasek,Hans Hasse*

Main category: cs.LG

TL;DR: 本研究搭建实验室规模的间歇精馏装置，生成大量实验数据，构建数据库并公开，为先进机器学习异常检测方法的发展奠基。


<details>
  <summary>Details</summary>
Motivation: 机器学习在化工过程异常检测有潜力，但缺乏公开实验数据阻碍其发展。

Method: 搭建实验室规模的间歇精馏装置，在不同工况和混合物下进行119次实验，收集传感器、执行器等数据，基于本体进行异常标注。

Result: 构建了包含多种数据、元数据和专家标注的结构化数据库，可通过指定链接免费获取。

Conclusion: 新数据库为先进机器学习异常检测方法的发展铺平道路，有助于开发可解释的方法和异常缓解措施。

Abstract: Machine learning (ML) holds great potential to advance anomaly detection (AD)
in chemical processes. However, the development of ML-based methods is hindered
by the lack of openly available experimental data. To address this gap, we have
set up a laboratory-scale batch distillation plant and operated it to generate
an extensive experimental database, covering fault-free experiments and
experiments in which anomalies were intentionally induced, for training
advanced ML-based AD methods. In total, 119 experiments were conducted across a
wide range of operating conditions and mixtures. Most experiments containing
anomalies were paired with a corresponding fault-free one. The database that we
provide here includes time-series data from numerous sensors and actuators,
along with estimates of measurement uncertainty. In addition, unconventional
data sources -- such as concentration profiles obtained via online benchtop NMR
spectroscopy and video and audio recordings -- are provided. Extensive metadata
and expert annotations of all experiments are included. The anomaly annotations
are based on an ontology developed in this work. The data are organized in a
structured database and made freely available via
doi.org/10.5281/zenodo.17395544. This new database paves the way for the
development of advanced ML-based AD methods. As it includes information on the
causes of anomalies, it further enables the development of interpretable and
explainable ML approaches, as well as methods for anomaly mitigation.

</details>


### [117] [MEG-GPT: A transformer-based foundation model for magnetoencephalography data](https://arxiv.org/abs/2510.18080)
*Rukuang Huang,Sungjun Cho,Chetan Gohil,Oiwi Parker Jones,Mark Woolrich*

Main category: cs.LG

TL;DR: 本文提出基于Transformer的基础模型MEG - GPT及新型数据驱动分词器处理MEG数据，训练后能生成具有真实时空特性的数据，在下游任务表现好，为电生理数据建模奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法捕捉MEG数据丰富结构，而深度学习基础模型在其他领域取得进展，因此希望为神经科学中大规模脑动力学的复杂时空模式建模提供新方法。

Method: 引入基于Transformer的MEG - GPT模型，使用时间注意力和下一时间点预测；引入新型数据驱动分词器处理连续MEG数据；在大规模MEG数据集上训练模型。

Result: 模型可生成具有真实时空特性的数据，在下游解码任务表现良好，跨会话和跨主体零样本泛化能力提升，在较小标记数据集上微调可提高跨主体解码性能。

Conclusion: 建立了强大的电生理数据基础模型，为计算神经科学和神经解码应用铺平道路。

Abstract: Modelling the complex spatiotemporal patterns of large-scale brain dynamics
is crucial for neuroscience, but traditional methods fail to capture the rich
structure in modalities such as magnetoencephalography (MEG). Recent advances
in deep learning have enabled significant progress in other domains, such as
language and vision, by using foundation models at scale. Here, we introduce
MEG-GPT, a transformer based foundation model that uses time-attention and next
time-point prediction. To facilitate this, we also introduce a novel
data-driven tokeniser for continuous MEG data, which preserves the high
temporal resolution of continuous MEG signals without lossy transformations. We
trained MEG-GPT on tokenised brain region time-courses extracted from a
large-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show that
the learnt model can generate data with realistic spatio-spectral properties,
including transient events and population variability. Critically, it performs
well in downstream decoding tasks, improving downstream supervised prediction
task, showing improved zero-shot generalisation across sessions (improving
accuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49)
compared to a baseline methods. Furthermore, we show the model can be
efficiently fine-tuned on a smaller labelled dataset to boost performance in
cross-subject decoding scenarios. This work establishes a powerful foundation
model for electrophysiological data, paving the way for applications in
computational neuroscience and neural decoding.

</details>


### [118] [Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth](https://arxiv.org/abs/2510.18081)
*Jiawei Zhang,Andrew Estornell,David D. Baek,Bo Li,Xiaojun Xu*

Main category: cs.LG

TL;DR: 提出Any - Depth Alignment (ADA)方法，在推理阶段防御以提升大语言模型任意生成深度的安全性，在多模型上效果好且不改变模型参数。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在浅层对齐问题，需解锁其先天浅层对齐以确保任意生成深度的安全性。

Method: 基于观察到浅层拒绝训练中助手头部标记集中了对齐信息，提出ADA方法，在生成过程中重新引入这些标记，促使模型重新评估有害性。

Result: 在多种开源模型家族上实现强大安全性能，对抗性预填充攻击拒绝率近100%，降低多种对抗性提示攻击成功率至3%以下，同时保持良性任务实用性。

Conclusion: ADA能有效提升大语言模型任意生成深度的安全性，且在模型后续指令调优后仍具弹性。

Abstract: Large Language Models (LLMs) exhibit strong but shallow alignment: they
directly refuse harmful queries when a refusal is expected at the very start of
an assistant turn, yet this protection collapses once a harmful continuation is
underway (either through the adversarial attacks or via harmful
assistant-prefill attacks). This raises a fundamental question: Can the innate
shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation
depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an
effective inference-time defense with negligible overhead. ADA is built based
on our observation that alignment is concentrated in the assistant header
tokens through repeated use in shallow-refusal training, and these tokens
possess the model's strong alignment priors. By reintroducing these tokens
mid-stream, ADA induces the model to reassess harmfulness and recover refusals
at any point in generation. Across diverse open-source model families (Llama,
Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety
performance without requiring any changes to the base model's parameters. It
secures a near-100% refusal rate against challenging adversarial prefill
attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces
the average success rate of prominent adversarial prompt attacks (such as GCG,
AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving
utility on benign tasks with minimal over-refusal. ADA maintains this
resilience even after the base model undergoes subsequent instruction tuning
(benign or adversarial).

</details>


### [119] [Provably Optimal Reinforcement Learning under Safety Filtering](https://arxiv.org/abs/2510.18082)
*Donggeon David Oh,Duy P. Nguyen,Haimin Hu,Jaime F. Fisac*

Main category: cs.LG

TL;DR: 本文证明足够宽松的安全过滤器执行安全策略不会降低渐近性能，提出相关理论并在Safety Gymnasium验证。


<details>
  <summary>Details</summary>
Motivation: 强化学习缺乏正式安全保证限制其在安全关键场景应用，且安全过滤常被认为牺牲性能和阻碍学习。

Method: 用安全关键马尔可夫决策过程（SC - MDP）形式化强化学习安全，定义相关过滤MDP，证明主要定理。

Result: 在Safety Gymnasium上验证理论，训练中零违规，最终性能匹配或超过未过滤基线。

Conclusion: 为安全强化学习提供简单原则性方法，即使用最宽松的安全过滤器训练和部署策略。

Abstract: Recent advances in reinforcement learning (RL) enable its use on increasingly
complex tasks, but the lack of formal safety guarantees still limits its
application in safety-critical settings. A common practical approach is to
augment the RL policy with a safety filter that overrides unsafe actions to
prevent failures during both training and deployment. However, safety filtering
is often perceived as sacrificing performance and hindering the learning
process. We show that this perceived safety-performance tradeoff is not
inherent and prove, for the first time, that enforcing safety with a
sufficiently permissive safety filter does not degrade asymptotic performance.
We formalize RL safety with a safety-critical Markov decision process (SC-MDP),
which requires categorical, rather than high-probability, avoidance of
catastrophic failure states. Additionally, we define an associated filtered MDP
in which all actions result in safe effects, thanks to a safety filter that is
considered to be a part of the environment. Our main theorem establishes that
(i) learning in the filtered MDP is safe categorically, (ii) standard RL
convergence carries over to the filtered MDP, and (iii) any policy that is
optimal in the filtered MDP-when executed through the same filter-achieves the
same asymptotic return as the best safe policy in the SC-MDP, yielding a
complete separation between safety enforcement and performance optimization. We
validate the theory on Safety Gymnasium with representative tasks and
constraints, observing zero violations during training and final performance
matching or exceeding unfiltered baselines. Together, these results shed light
on a long-standing question in safety-filtered learning and provide a simple,
principled recipe for safe RL: train and deploy RL policies with the most
permissive safety filter that is available.

</details>


### [120] [Enhancing mortality prediction in cardiac arrest ICU patients through meta-modeling of structured clinical data from MIMIC-IV](https://arxiv.org/abs/2510.18103)
*Nursultan Mamatov,Philipp Kellmeyer*

Main category: cs.LG

TL;DR: 研究开发并评估整合结构化临床数据和非结构化文本信息的机器学习模型预测ICU患者院内死亡率，结合两者的模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 准确早期预测ICU患者院内死亡率对临床干预和资源分配至关重要。

Method: 使用LASSO和XGBoost进行特征选择，用多元逻辑回归训练，结合TF - IDF和BERT嵌入纳入文本特征。

Result: 结合结构化和文本输入的最终逻辑回归模型AUC达0.918，比仅用结构化数据提升22%，决策曲线显示有更好的标准化净收益。

Conclusion: 非结构化临床笔记有额外预后价值，支持将其整合到可解释的特征驱动风险预测模型中。

Abstract: Accurate early prediction of in-hospital mortality in intensive care units
(ICUs) is essential for timely clinical intervention and efficient resource
allocation. This study develops and evaluates machine learning models that
integrate both structured clinical data and unstructured textual information,
specifically discharge summaries and radiology reports, from the MIMIC-IV
database. We used LASSO and XGBoost for feature selection, followed by a
multivariate logistic regression trained on the top features identified by both
models. Incorporating textual features using TF-IDF and BERT embeddings
significantly improved predictive performance. The final logistic regression
model, which combined structured and textual input, achieved an AUC of 0.918,
compared to 0.753 when using structured data alone, a relative improvement 22%.
The analysis of the decision curve demonstrated a superior standardized net
benefit in a wide range of threshold probabilities (0.2-0.8), confirming the
clinical utility of the model. These results underscore the added prognostic
value of unstructured clinical notes and support their integration into
interpretable feature-driven risk prediction models for ICU patients.

</details>


### [121] [Gradient Variance Reveals Failure Modes in Flow-Based Generative Models](https://arxiv.org/abs/2510.18118)
*Teodora Reu,Sixtine Dromigny,Michael Bronstein,Francisco Vargas*

Main category: cs.LG

TL;DR: 研究Rectified Flows直线路径目标存在的记忆化失败模式，理论分析并实验验证。


<details>
  <summary>Details</summary>
Motivation: 揭示Rectified Flows直线路径目标潜在的基本失败模式。

Method: 研究高斯到高斯的传输，利用随机和确定性机制下的损失梯度方差分析优化倾向；理论证明记忆向量场的存在；在CelebA数据集上进行实验验证。

Result: 在插值线相交情况下，确定性训练会导致记忆化，确定性积分会重现训练配对；注入小噪声可恢复泛化性。

Conclusion: Rectified Flows直线路径目标在确定性训练下会导致记忆化问题，注入小噪声可解决。

Abstract: Rectified Flows learn ODE vector fields whose trajectories are straight
between source and target distributions, enabling near one-step inference. We
show that this straight-path objective conceals fundamental failure modes:
under deterministic training, low gradient variance drives memorization of
arbitrary training pairings, even when interpolant lines between pairs
intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport
and use the loss gradient variance across stochastic and deterministic regimes
to characterize which vector fields optimization favors in each setting. We
then show that, in a setting where all interpolating lines intersect, applying
Rectified Flow yields the same specific pairings at inference as during
training. More generally, we prove that a memorizing vector field exists even
when training interpolants intersect, and that optimizing the straight-path
objective converges to this ill-defined field. At inference, deterministic
integration reproduces the exact training pairings. We validate our findings
empirically on the CelebA dataset, confirming that deterministic interpolants
induce memorization, while the injection of small noise restores
generalization.

</details>


### [122] [HyperDiffusionFields (HyDiF): Diffusion-Guided Hypernetworks for Learning Implicit Molecular Neural Fields](https://arxiv.org/abs/2510.18122)
*Sudarshan Babu,Phillip Lo,Xiao Zhang,Aadi Srivastava,Ali Davariashtiyani,Jason Perera,Michael Maire,Aly A. Khan*

Main category: cs.LG

TL;DR: 提出HyperDiffusionFields (HyDiF)框架，将3D分子构象建模为连续场，有生成和属性预测能力，可扩展到更大生物分子。


<details>
  <summary>Details</summary>
Motivation: 传统方法用离散原子坐标或图来建模分子构象，本文旨在提出新框架将其建模为连续场。

Method: 核心是分子方向场（MDF），用分子特定神经隐式场表示，采用共享超网络生成权重，将超网络训练为去噪扩散模型，还扩展出掩码扩散机制。

Result: 模型具有生成能力，支持结构条件生成任务，能进行空间细粒度特征提取用于分子属性预测，可扩展到更大生物分子。

Conclusion: 该方法为基于场的分子建模提供了有前景的方向。

Abstract: We introduce HyperDiffusionFields (HyDiF), a framework that models 3D
molecular conformers as continuous fields rather than discrete atomic
coordinates or graphs. At the core of our approach is the Molecular Directional
Field (MDF), a vector field that maps any point in space to the direction of
the nearest atom of a particular type. We represent MDFs using
molecule-specific neural implicit fields, which we call Molecular Neural Fields
(MNFs). To enable learning across molecules and facilitate generalization, we
adopt an approach where a shared hypernetwork, conditioned on a molecule,
generates the weights of the given molecule's MNF. To endow the model with
generative capabilities, we train the hypernetwork as a denoising diffusion
model, enabling sampling in the function space of molecular fields. Our design
naturally extends to a masked diffusion mechanism to support
structure-conditioned generation tasks, such as molecular inpainting, by
selectively noising regions of the field. Beyond generation, the localized and
continuous nature of MDFs enables spatially fine-grained feature extraction for
molecular property prediction, something not easily achievable with graph or
point cloud based methods. Furthermore, we demonstrate that our approach scales
to larger biomolecules, illustrating a promising direction for field-based
molecular modeling.

</details>


### [123] [ActivationReasoning: Logical Reasoning in Latent Activation Spaces](https://arxiv.org/abs/2510.18184)
*Lukas Helff,Ruben Härle,Wolfgang Stammer,Felix Friedrich,Manuel Brack,Antonia Wüst,Hikaru Shindo,Patrick Schramowski,Kristian Kersting*

Main category: cs.LG

TL;DR: 引入ActivationReasoning (AR)框架将显式逻辑推理嵌入大语言模型潜空间，经三阶段处理，在多任务评估中表现良好，提升模型透明度与可控性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型内部推理不透明、难控制，稀疏自编码器特征脆弱被动，缺乏推理和控制机制。

Method: AR框架分三阶段：找到潜在表征、激活命题、逻辑推理。

Result: 在多任务评估中，AR随推理复杂度稳健扩展，能泛化到抽象和上下文敏感任务，跨模型骨干迁移。

Conclusion: 将逻辑结构建立在潜在激活上可提升透明度，实现结构化推理、可靠控制和行为对齐，为可靠可审计AI提供途径。

Abstract: Large language models (LLMs) excel at generating fluent text, but their
internal reasoning remains opaque and difficult to control. Sparse autoencoders
(SAEs) make hidden activations more interpretable by exposing latent features
that often align with human concepts. Yet, these features are fragile and
passive, offering no mechanism for systematic reasoning or model control. To
address this, we introduce ActivationReasoning (AR), a framework that embeds
explicit logical reasoning into the latent space of LLMs. It proceeds in three
stages: (1) Finding latent representations, first latent concept
representations are identified (e.g., via SAEs) and organized into a
dictionary; (2) Activating propositions, at inference time AR detects
activating concepts and maps them to logical propositions; and (3)Logical
reasoning, applying logical rules over these propositions to infer higher-order
structures, compose new concepts, and steer model behavior. We evaluate AR on
multi-hop reasoning (PrOntoQA), abstraction and robustness to indirect concept
cues (Rail2Country), reasoning over natural and diverse language (ProverQA),
and context-sensitive safety (BeaverTails). Across all tasks, AR scales
robustly with reasoning complexity, generalizes to abstract and
context-sensitive tasks, and transfers across model backbones. These results
demonstrate that grounding logical structure in latent activations not only
improves transparency but also enables structured reasoning, reliable control,
and alignment with desired behaviors, providing a path toward more reliable and
auditable AI.

</details>


### [124] [Ensemble based Closed-Loop Optimal Control using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.18195)
*Jostein Barry-Straume,Adwait D. Verulkar,Arash Sarshar,Andrey A. Popov,Adrian Sandu*

Main category: cs.LG

TL;DR: 本文提出多阶段集成框架，通过HJB方程学习最优代价函数和控制信号，无需稳定项，在闭环控制中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: HJB偏微分方程数值求解计算量大且常无解析解，知识引导的机器学习方法可缓解此难题。

Method: 提出多阶段集成框架，不使用稳定项，通过HJB方程学习最优代价函数和控制信号，可采用奇异学习控制信号或集成控制信号策略。

Result: 在具有无限时间范围的稳态时不变二态连续非线性系统的闭环控制中，考虑噪声、扰动系统状态和不同初始条件，验证了集成控制和奇异控制的成功。

Conclusion: 所提出的多阶段集成框架为解决HJB方程求解难题、控制非线性系统提供了有效方法。

Abstract: The objective of designing a control system is to steer a dynamical system
with a control signal, guiding it to exhibit the desired behavior. The
Hamilton-Jacobi-Bellman (HJB) partial differential equation offers a framework
for optimal control system design. However, numerical solutions to this
equation are computationally intensive, and analytical solutions are frequently
unavailable. Knowledge-guided machine learning methodologies, such as
physics-informed neural networks (PINNs), offer new alternative approaches that
can alleviate the difficulties of solving the HJB equation numerically. This
work presents a multistage ensemble framework to learn the optimal cost-to-go,
and subsequently the corresponding optimal control signal, through the HJB
equation. Prior PINN-based approaches rely on a stabilizing the HJB enforcement
during training. Our framework does not use stabilizer terms and offers a means
of controlling the nonlinear system, via either a singular learned control
signal or an ensemble control signal policy. Success is demonstrated in
closed-loop control, using both ensemble- and singular-control, of a
steady-state time-invariant two-state continuous nonlinear system with an
infinite time horizon, accounting of noisy, perturbed system states and varying
initial conditions.

</details>


### [125] [Joint Optimization of Cooperation Efficiency and Communication Covertness for Target Detection with AUVs](https://arxiv.org/abs/2510.18225)
*Xueyao Zhang,Bo Yang,Zhiwen Yu,Xuelin Cao,Wei Xiang,Bin Guo,Liang Wang,Billy Pik Lik Lau,George C. Alexandropoulos,Jun Luo,Mérouane Debbah,Zhu Han,Chau Yuen*

Main category: cs.LG

TL;DR: 本文研究水下AUV合作目标检测，权衡合作效率与通信隐蔽性，提出分层行动管理框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 解决水下AUV合作目标检测中合作效率与通信隐蔽性的权衡问题。

Method: 先提出联合轨迹和功率控制优化问题，再构建分层行动管理框架，宏观层面用近端策略优化算法进行任务分配，微观层面用多智能体近端策略优化算法动态调整轨迹和发射功率。

Result: 目标检测框架能在满足能量和移动性约束下实现自适应隐蔽合作。

Conclusion: 为多AUV高效安全运行提供理论见解和实际解决方案，对水下隐蔽通信任务执行有重要意义。

Abstract: This paper investigates underwater cooperative target detection using
autonomous underwater vehicles (AUVs), with a focus on the critical trade-off
between cooperation efficiency and communication covertness. To tackle this
challenge, we first formulate a joint trajectory and power control optimization
problem, and then present an innovative hierarchical action management
framework to solve it. According to the hierarchical formulation, at the macro
level, the master AUV models the agent selection process as a Markov decision
process and deploys the proximal policy optimization algorithm for strategic
task allocation. At the micro level, each selected agent's decentralized
decision-making is modeled as a partially observable Markov decision process,
and a multi-agent proximal policy optimization algorithm is used to dynamically
adjust its trajectory and transmission power based on its local observations.
Under the centralized training and decentralized execution paradigm, our target
detection framework enables adaptive covert cooperation while satisfying both
energy and mobility constraints. By comprehensively modeling the considered
system, the involved signals and tasks, as well as energy consumption,
theoretical insights and practical solutions for the efficient and secure
operation of multiple AUVs are provided, offering significant implications for
the execution of underwater covert communication tasks.

</details>


### [126] [Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations](https://arxiv.org/abs/2510.18228)
*Zhendong Mi,Qitao Tan,Grace Li Zhang,Zhaozhuo Xu,Geng Yuan,Shaoyi Huang*

Main category: cs.LG

TL;DR: 提出P - GAP方法用于大语言模型微调，实验显示其性能优于基线，能加速收敛、节省资源。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法在大语言模型微调中梯度估计方差大，收敛慢且性能欠佳。

Method: 提出P - GAP方法，先估计低维梯度空间，再在该空间内使扰动与投影梯度方向对齐。

Result: P - GAP在实验中始终优于基线，分类任务准确率最高提升6%，生成任务最高提升12%，训练迭代次数最多减少约81%，GPU使用时长最多减少70%。

Conclusion: P - GAP实现了快速、可扩展且资源高效的零阶大语言模型微调。

Abstract: Fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization
has emerged as a promising alternative to traditional gradient-based methods
due to its reduced memory footprint requirement. However, existing ZO methods
suffer from high variance in gradient estimation, leading to slow convergence
and suboptimal performance on large-scale models. In this work, we propose
P-GAP, a fast LLM fine-tuning approach through zeroth-order optimization with
Projected Gradient-Aligned Perturbations. Specifically, we first estimate a
low-dimensional gradient space and then align perturbations in projected
gradients' direction within the space. This approach enables reduced the number
of perturbed parameters and decreased variance, therefore accelerated
convergence for LLM fine-tuning. Experiments on LLMs show that P-GAP
consistently surpasses the baselines, achieving up to 6% increase in accuracy
on classification tasks and up to 12% higher accuracy on generation tasks, with
up to about 81% less training iterations and 70% less GPU hours. These results
demonstrate that P-GAP enables fast, scalable, and resource-efficient ZO LLM
fine-tuning.

</details>


### [127] [ACTG-ARL: Differentially Private Conditional Text Generation with RL-Boosted Control](https://arxiv.org/abs/2510.18232)
*Yuzheng Hu,Ryan McKenna,Da Yu,Shanshan Wu,Han Zhao,Zheng Xu,Peter Kairouz*

Main category: cs.LG

TL;DR: 提出ACTG - ARL算法解决差分隐私下合成高质量文本的挑战，提升文本质量和条件生成控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有合成差分隐私数据集的工作存在无法保留关键统计属性、因噪音导致效用损失、缺乏细粒度生成控制等问题。

Method: 引入分层框架将差分隐私合成文本生成分解为特征学习和条件文本生成两个子任务，确定ACTG配置；提出Anchored RL（ARL）后训练方法提升ACTG条件生成的指令遵循能力。

Result: ACTG - ARL算法在强隐私保证下，DP合成文本质量比先前工作提高20%的MAUVE，且提升了条件生成器的控制能力。

Conclusion: ACTG - ARL算法有效解决了差分隐私下合成高质量文本的问题，在文本质量和生成控制方面取得进展。

Abstract: Generating high-quality synthetic text under differential privacy (DP) is
critical for training and evaluating language models without compromising user
privacy. Prior work on synthesizing DP datasets often fail to preserve key
statistical attributes, suffer utility loss from the noise required by DP, and
lack fine-grained control over generation. To address these challenges, we make
two contributions. First, we introduce a hierarchical framework that decomposes
DP synthetic text generation into two subtasks: feature learning and
conditional text generation. This design explicitly incorporates learned
features into the generation process and simplifies the end-to-end synthesis
task. Through systematic ablations, we identify the most effective
configuration: a rich tabular schema as feature, a DP tabular synthesizer, and
a DP fine-tuned conditional generator, which we term ACTG
(Attribute-Conditioned Text Generation). Second, we propose Anchored RL (ARL),
a post-training method that improves the instruction-following ability of ACTG
for conditional generation. ARL combines RL to boost control with an SFT anchor
on best-of-$N$ data to prevent reward hacking. Together, these components form
our end-to-end algorithm ACTG-ARL, which advances both the quality of DP
synthetic text (+20% MAUVE over prior work) and the control of the conditional
generator under strong privacy guarantees.

</details>


### [128] [Fostering the Ecosystem of AI for Social Impact Requires Expanding and Strengthening Evaluation Standards](https://arxiv.org/abs/2510.18238)
*Bryan Wilder,Angela Zhou*

Main category: cs.LG

TL;DR: 当前AI/ML社会影响研究受关注，评审标准倾向认可兼具部署和方法创新的项目，这会破坏研究生态可持续性，应采用更广泛社会影响概念和更严格评估。


<details>
  <summary>Details</summary>
Motivation: 指出当前面向社会影响的AI/ML研究评审标准存在问题，不利于更广泛研究生态的可持续性。

Method: 无明确提及具体方法

Result: 无明确提及具体结果

Conclusion: 机器学习社会影响研究的研究者和评审者需同时采用更广泛社会影响概念和对部署系统影响的更严格评估。

Abstract: There has been increasing research interest in AI/ML for social impact, and
correspondingly more publication venues have refined review criteria for
practice-driven AI/ML research. However, these review guidelines tend to most
concretely recognize projects that simultaneously achieve deployment and novel
ML methodological innovation. We argue that this introduces incentives for
researchers that undermine the sustainability of a broader research ecosystem
of social impact, which benefits from projects that make contributions on
single front (applied or methodological) that may better meet project partner
needs. Our position is that researchers and reviewers in machine learning for
social impact must simultaneously adopt: 1) a more expansive conception of
social impacts beyond deployment and 2) more rigorous evaluations of the impact
of deployed systems.

</details>


### [129] [Learning with Dual-level Noisy Correspondence for Multi-modal Entity Alignment](https://arxiv.org/abs/2510.18240)
*Haobin Li,Yijie Lin,Peng Hu,Mouxing Yang,Xi Peng*

Main category: cs.LG

TL;DR: 本文揭示MMEA中的双级噪声对应问题（DNC），提出RULE框架解决该问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MMEA方法假设实体内部和图间对应无误，但现实中因依赖专家标注常不成立，需解决DNC问题。

Method: 提出RULE框架，通过双折原则估计对应可靠性，在属性融合和消除图间差异时减轻噪声影响，还加入对应推理模块。

Result: 在五个基准测试上，与七种最先进方法相比，RULE在处理DNC问题上表现有效。

Conclusion: RULE框架能有效解决MMEA中的DNC问题，提高等价实体识别的准确性。

Abstract: Multi-modal entity alignment (MMEA) aims to identify equivalent entities
across heterogeneous multi-modal knowledge graphs (MMKGs), where each entity is
described by attributes from various modalities. Existing methods typically
assume that both intra-entity and inter-graph correspondences are faultless,
which is often violated in real-world MMKGs due to the reliance on expert
annotations. In this paper, we reveal and study a highly practical yet
under-explored problem in MMEA, termed Dual-level Noisy Correspondence (DNC).
DNC refers to misalignments in both intra-entity (entity-attribute) and
inter-graph (entity-entity and attribute-attribute) correspondences. To address
the DNC problem, we propose a robust MMEA framework termed RULE. RULE first
estimates the reliability of both intra-entity and inter-graph correspondences
via a dedicated two-fold principle. Leveraging the estimated reliabilities,
RULE mitigates the negative impact of intra-entity noise during attribute
fusion and prevents overfitting to noisy inter-graph correspondences during
inter-graph discrepancy elimination. Beyond the training-time designs, RULE
further incorporates a correspondence reasoning module that uncovers the
underlying attribute-attribute connection across graphs, guaranteeing more
accurate equivalent entity identification. Extensive experiments on five
benchmarks verify the effectiveness of our method against the DNC compared with
seven state-of-the-art methods.The code is available at
\href{https://github.com/XLearning-SCU/RULE}{XLearning-SCU/RULE}

</details>


### [130] [Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs](https://arxiv.org/abs/2510.18245)
*Song Bian,Tao Yu,Shivaram Venkataraman,Youngsuk Park*

Main category: cs.LG

TL;DR: 本文研究大语言模型架构因素对推理成本和准确性的影响，提出条件缩放定律和搜索框架，实验证明其能预测最优架构选择，优化后的模型性能优于现有开源基线。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型参数和训练数据规模增大，推理成本成为紧迫问题，而模型准确性和推理效率之间的权衡尚未充分研究。

Method: 研究关键架构因素（隐藏大小、MLP与注意力的参数分配、分组查询注意力）对推理成本和准确性的影响，引入条件缩放定律和搜索框架，训练超200个不同参数和训练令牌的模型并拟合定律。

Result: 条件缩放定律能可靠预测最优架构选择，优化后的模型在相同训练预算下，比LLaMA - 3.2准确率高2.1%，推理吞吐量高42%。

Conclusion: 提出的条件缩放定律和搜索框架可有效平衡大语言模型的推理效率和准确性，优化后的模型性能更优。

Abstract: Scaling the number of parameters and the size of training data has proven to
be an effective strategy for improving large language model (LLM) performance.
Yet, as these models grow increasingly powerful and widely deployed, the cost
of inference has become a pressing concern. Despite its importance, the
trade-off between model accuracy and inference efficiency remains
underexplored. In this work, we examine how key architectural factors, hidden
size, the allocation of parameters between MLP and attention (mlp-to-attention
ratio), and grouped-query attention (GQA), influence both inference cost and
accuracy. We introduce a conditional scaling law that augments the Chinchilla
framework with architectural information, along with a search framework for
identifying architectures that are simultaneously inference-efficient and
accurate. To validate our approach, we train more than 200 models spanning 80M
to 3B parameters and 8B to 100B training tokens, and fit the proposed
conditional scaling law. Our results show that the conditional scaling law
reliably predicts optimal architectural choices and that the resulting models
outperform existing open-source baselines. Under the same training budget,
optimized architectures achieve up to 2.1% higher accuracy and 42% greater
inference throughput compared to LLaMA-3.2.

</details>


### [131] [NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective](https://arxiv.org/abs/2510.18258)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Junchi Yan*

Main category: cs.LG

TL;DR: 本文利用NTK理论分析MTL训练动态，提出NTKMTL和NTKMTL - SR方法缓解任务不平衡问题，实验显示其在多基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: MTL中任务不平衡是主要挑战，准确刻画多任务训练动态和收敛速度困难。

Method: 利用NTK理论分析训练动态，引入扩展NTK矩阵，采用谱分析平衡多任务收敛速度；基于共享表示近似提出NTKMTL - SR。

Result: 方法在多任务监督学习和多任务强化学习等广泛基准测试中达到SOTA性能。

Conclusion: 所提NTKMTL和NTKMTL - SR方法能有效缓解MTL任务不平衡问题，有良好性能和训练效率。

Abstract: Multi-Task Learning (MTL) enables a single model to learn multiple tasks
simultaneously, leveraging knowledge transfer among tasks for enhanced
generalization, and has been widely applied across various domains. However,
task imbalance remains a major challenge in MTL. Although balancing the
convergence speeds of different tasks is an effective approach to address this
issue, it is highly challenging to accurately characterize the training
dynamics and convergence speeds of multiple tasks within the complex MTL
system. To this end, we attempt to analyze the training dynamics in MTL by
leveraging Neural Tangent Kernel (NTK) theory and propose a new MTL method,
NTKMTL. Specifically, we introduce an extended NTK matrix for MTL and adopt
spectral analysis to balance the convergence speeds of multiple tasks, thereby
mitigating task imbalance. Based on the approximation via shared
representation, we further propose NTKMTL-SR, achieving training efficiency
while maintaining competitive performance. Extensive experiments demonstrate
that our methods achieve state-of-the-art performance across a wide range of
benchmarks, including both multi-task supervised learning and multi-task
reinforcement learning. Source code is available at
https://github.com/jianke0604/NTKMTL.

</details>


### [132] [From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation](https://arxiv.org/abs/2510.18263)
*Ziwei Huang,Ying Shu,Hao Fang,Quanyu Long,Wenya Wang,Qiushi Guo,Tiezheng Ge,Leilei Gan*

Main category: cs.LG

TL;DR: 现有主题驱动图像生成模型在身份保留和提示遵循间有权衡问题，简单应用GRPO会导致性能下降，本文提出Customized - GRPO框架解决该问题并取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 解决主题驱动图像生成模型在身份保留和提示遵循间的权衡问题，避免简单应用GRPO带来的竞争退化。

Method: 提出Customized - GRPO框架，包含协同感知奖励塑造（SARS）和时间感知动态加权（TDW）两个创新点。

Result: 实验表明该方法显著优于简单GRPO基线，成功缓解竞争退化。

Conclusion: 模型实现了更好的平衡，生成的图像既保留关键身份特征又能准确遵循复杂文本提示。

Abstract: Subject-driven image generation models face a fundamental trade-off between
identity preservation (fidelity) and prompt adherence (editability). While
online reinforcement learning (RL), specifically GPRO, offers a promising
solution, we find that a naive application of GRPO leads to competitive
degradation, as the simple linear aggregation of rewards with static weights
causes conflicting gradient signals and a misalignment with the temporal
dynamics of the diffusion process. To overcome these limitations, we propose
Customized-GRPO, a novel framework featuring two key innovations: (i)
Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly
penalizes conflicted reward signals and amplifies synergistic ones, providing a
sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW),
which aligns the optimization pressure with the model's temporal dynamics by
prioritizing prompt-following in the early, identity preservation in the later.
Extensive experiments demonstrate that our method significantly outperforms
naive GRPO baselines, successfully mitigating competitive degradation. Our
model achieves a superior balance, generating images that both preserve key
identity features and accurately adhere to complex textual prompts.

</details>


### [133] [Online Time Series Forecasting with Theoretical Guarantees](https://arxiv.org/abs/2510.18281)
*Zijian Li,Changze Zhou,Minghao Fu,Sanjay Manjunath,Fan Feng,Guangyi Chen,Yingyao Hu,Ruichu Cai,Kun Zhang*

Main category: cs.LG

TL;DR: 提出在线时间序列预测理论框架TOT，证明引入潜在变量可收紧贝叶斯风险，通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决在线时间序列预测中未知分布随时间变化的问题，开发自动化预测方法。

Method: 提出理论框架TOT，证明引入潜在变量的作用，用最少相邻观测识别潜在变量，设计模型无关蓝图。

Result: 合成数据实验支持理论，基于基线的插件实现有普遍提升。

Conclusion: TOT框架在理论上可行，在实际应用中有效。

Abstract: This paper is concerned with online time series forecasting, where unknown
distribution shifts occur over time, i.e., latent variables influence the
mapping from historical to future observations. To develop an automated way of
online time series forecasting, we propose a Theoretical framework for Online
Time-series forecasting (TOT in short) with theoretical guarantees.
Specifically, we prove that supplying a forecaster with latent variables
tightens the Bayes risk, the benefit endures under estimation uncertainty of
latent variables and grows as the latent variables achieve a more precise
identifiability. To better introduce latent variables into online forecasting
algorithms, we further propose to identify latent variables with minimal
adjacent observations. Based on these results, we devise a model-agnostic
blueprint by employing a temporal decoder to match the distribution of observed
variables and two independent noise estimators to model the causal inference of
latent variables and mixing procedures of observed variables, respectively.
Experiment results on synthetic data support our theoretical claims. Moreover,
plug-in implementations built on several baselines yield general improvement
across multiple benchmarks, highlighting the effectiveness in real-world
applications.

</details>


### [134] [Physics-Informed Parametric Bandits for Beam Alignment in mmWave Communications](https://arxiv.org/abs/2510.18299)
*Hao Qin,Thang Duong,Ming Li,Chicheng Zhang*

Main category: cs.LG

TL;DR: 本文提出适用于毫米波通信的波束对齐和跟踪的物理信息多臂老虎机算法pretc和prgreedy，实验表明其在不同场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机算法在大波束空间收敛慢，现有依赖奖励函数单峰或多峰假设的算法在实际中可能收敛到次优波束。

Method: 提出pretc和prgreedy算法，利用毫米波信道稀疏多径特性，将路径参数视为黑盒并基于历史采样奖励维持最优估计，pretc先随机探索再确定最优波束，prgreedy在线估计并选择当前最佳波束。

Result: 在合成DeepMIMO数据集和真实DeepSense6G数据集实验中，两种算法在多种场景和不同信道环境下均优于现有方法。

Conclusion: pretc和prgreedy算法具有通用性和鲁棒性，还可用于移动场景的波束跟踪。

Abstract: In millimeter wave (mmWave) communications, beam alignment and tracking are
crucial to combat the significant path loss. As scanning the entire directional
space is inefficient, designing an efficient and robust method to identify the
optimal beam directions is essential. Since traditional bandit algorithms
require a long time horizon to converge under large beam spaces, many existing
works propose efficient bandit algorithms for beam alignment by relying on
unimodality or multimodality assumptions on the reward function's structure.
However, such assumptions often do not hold (or cannot be strictly satisfied)
in practice, which causes such algorithms to converge to choosing suboptimal
beams.
  In this work, we propose two physics-informed bandit algorithms
\textit{pretc} and \textit{prgreedy} that exploit the sparse multipath property
of mmWave channels - a generic but realistic assumption - which is connected to
the Phase Retrieval Bandit problem. Our algorithms treat the parameters of each
path as black boxes and maintain optimal estimates of them based on sampled
historical rewards. \textit{pretc} starts with a random exploration phase and
then commits to the optimal beam under the estimated reward function.
\textit{prgreedy} performs such estimation in an online manner and chooses the
best beam under current estimates. Our algorithms can also be easily adapted to
beam tracking in the mobile setting. Through experiments using both the
synthetic DeepMIMO dataset and the real-world DeepSense6G dataset, we
demonstrate that both algorithms outperform existing approaches in a wide range
of scenarios across diverse channel environments, showing their
generalizability and robustness.

</details>


### [135] [Towards Identifiability of Hierarchical Temporal Causal Representation Learning](https://arxiv.org/abs/2510.18310)
*Zijian Li,Minghao Fu,Junxian Huang,Yifan Shen,Ruichu Cai,Yuewen Sun,Guangyi Chen,Kun Zhang*

Main category: cs.LG

TL;DR: 提出CHiLD框架用于建模时间序列数据的分层潜在动态，在合成和真实数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有时间因果表征学习方法无法从单步观测变量中恢复分层潜在变量联合分布，难以捕捉分层潜在动态。

Method: 先利用时间上下文观测变量识别多层潜在变量联合分布，再利用潜在变量层次结构稀疏性识别每层潜在变量，开发基于变分推理的时间序列生成模型。

Result: 在合成和真实数据集上的实证评估验证了理论主张。

Conclusion: CHiLD框架在建模分层潜在动态方面有效。

Abstract: Modeling hierarchical latent dynamics behind time series data is critical for
capturing temporal dependencies across multiple levels of abstraction in
real-world tasks. However, existing temporal causal representation learning
methods fail to capture such dynamics, as they fail to recover the joint
distribution of hierarchical latent variables from \textit{single-timestep
observed variables}. Interestingly, we find that the joint distribution of
hierarchical latent variables can be uniquely determined using three
conditionally independent observations. Building on this insight, we propose a
Causally Hierarchical Latent Dynamic (CHiLD) identification framework. Our
approach first employs temporal contextual observed variables to identify the
joint distribution of multi-layer latent variables. Sequentially, we exploit
the natural sparsity of the hierarchical structure among latent variables to
identify latent variables within each layer. Guided by the theoretical results,
we develop a time series generative model grounded in variational inference.
This model incorporates a contextual encoder to reconstruct multi-layer latent
variables and normalize flow-based hierarchical prior networks to impose the
independent noise condition of hierarchical latent dynamics. Empirical
evaluations on both synthetic and real-world datasets validate our theoretical
claims and demonstrate the effectiveness of CHiLD in modeling hierarchical
latent dynamics.

</details>


### [136] [Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task](https://arxiv.org/abs/2510.18315)
*Brady Bhalla,Honglu Fan,Nancy Chen,Tony Yue YU*

Main category: cs.LG

TL;DR: 研究嵌入维度对强化学习训练的Transformer中内部‘世界模型’出现的影响，发现大维度有更好表现，还观察到两个机制。


<details>
  <summary>Details</summary>
Motivation: 探究嵌入维度如何影响强化学习训练的Transformer执行相邻交换任务时内部‘世界模型’的出现。

Method: 进行数百次实验，研究不同嵌入维度下模型表现。

Result: 小维度模型也有高准确率，大维度有更忠实、一致和鲁棒的内部表示，观察到两个机制。

Conclusion: Transformer能构建结构化内部世界模型，模型大小除了提升最终性能外还能提高表示质量。

Abstract: We investigate how embedding dimension affects the emergence of an internal
"world model" in a transformer trained with reinforcement learning to perform
bubble-sort-style adjacent swaps. Models achieve high accuracy even with very
small embedding dimensions, but larger dimensions yield more faithful,
consistent, and robust internal representations. In particular, higher
embedding dimensions strengthen the formation of structured internal
representation and lead to better interpretability. After hundreds of
experiments, we observe two consistent mechanisms: (1) the last row of the
attention weight matrix monotonically encodes the global ordering of tokens;
and (2) the selected transposition aligns with the largest adjacent difference
of these encoded values. Our results provide quantitative evidence that
transformers build structured internal world models and that model size
improves representation quality in addition to end performance. We release our
metrics and analyses, which can be used to probe similar algorithmic tasks.

</details>


### [137] [Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching](https://arxiv.org/abs/2510.18328)
*Zhong Li,Qi Huang,Yuxuan Zhu,Lincen Yang,Mohammad Mohammadi Amiri,Niki van Stein,Matthijs van Leeuwen*

Main category: cs.LG

TL;DR: 提出用于表格数据半监督异常检测的TCCM方法，有轻量级训练目标、高效评分策略等优势，实验显示其在检测精度和推理成本间取得良好平衡，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有连续时间模型推理瓶颈问题，在表格数据半监督异常检测中取得更好的检测精度与推理成本平衡。

Method: 基于流匹配核心思想，通过预测每个采样时间步朝向固定目标（原点）的时间条件收缩向量，简化框架。

Result: 在ADBench基准测试中，TCCM在检测精度和推理成本间取得良好平衡，尤其在高维和大规模数据集上优于现有方法。

Conclusion: TCCM是一种有效的表格数据半监督异常检测方法，具有轻量级、高效和可解释等优点。

Abstract: We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for
semi-supervised anomaly detection in tabular data. TCCM is inspired by flow
matching, a recent generative modeling framework that learns velocity fields
between probability distributions and has shown strong performance compared to
diffusion models and generative adversarial networks. Instead of directly
applying flow matching as originally formulated, TCCM builds on its core idea
-- learning velocity fields between distributions -- but simplifies the
framework by predicting a time-conditioned contraction vector toward a fixed
target (the origin) at each sampled time step. This design offers three key
advantages: (1) a lightweight and scalable training objective that removes the
need for solving ordinary differential equations during training and inference;
(2) an efficient scoring strategy called one time-step deviation, which
quantifies deviation from expected contraction behavior in a single forward
pass, addressing the inference bottleneck of existing continuous-time models
such as DTE (a diffusion-based model with leading anomaly detection accuracy
but heavy inference cost); and (3) explainability and provable robustness, as
the learned velocity field operates directly in input space, making the anomaly
score inherently feature-wise attributable; moreover, the score function is
Lipschitz-continuous with respect to the input, providing theoretical
guarantees under small perturbations. Extensive experiments on the ADBench
benchmark show that TCCM strikes a favorable balance between detection accuracy
and inference cost, outperforming state-of-the-art methods -- especially on
high-dimensional and large-scale datasets. The source code is available at our
GitHub repository.

</details>


### [138] [Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs](https://arxiv.org/abs/2510.18340)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 本文针对无折扣期望总奖励无限时域MDPs的策略梯度方法进行分析，基于两个关键见解开展研究。


<details>
  <summary>Details</summary>
Motivation: 经典策略梯度方法的严格分析多假设折扣因子γ < 1，而大语言模型的基于策略的强化学习使用γ = 1的无折扣总奖励设置，现有理论不适用。

Method: 基于两个关键见解进行分析：一是MDP状态分为循环和瞬态状态在对每个动作分配严格正概率的策略集上是不变的；二是用新的瞬态访问测度替代经典状态访问测度。

Result: 未提及。

Conclusion: 未提及。

Abstract: The classical policy gradient method is the theoretical and conceptual
foundation of modern policy-based reinforcement learning (RL) algorithms. Most
rigorous analyses of such methods, particularly those establishing convergence
guarantees, assume a discount factor $\gamma < 1$. In contrast, however, a
recent line of work on policy-based RL for large language models uses the
undiscounted total-reward setting with $\gamma = 1$, rendering much of the
existing theory inapplicable. In this paper, we provide analyses of the policy
gradient method for undiscounted expected total-reward infinite-horizon MDPs
based on two key insights: (i) the classification of the MDP states into
recurrent and transient states is invariant over the set of policies that
assign strictly positive probability to every action (as is typical in deep RL
models employing a softmax output layer) and (ii) the classical state
visitation measure (which may be ill-defined when $\gamma = 1$) can be replaced
with a new object that we call the transient visitation measure.

</details>


### [139] [Computable universal online learning](https://arxiv.org/abs/2510.18352)
*Dariusz Kalociński,Tomasz Steifer*

Main category: cs.LG

TL;DR: 本文探讨通用在线学习能否实现为计算机程序，表明通用在线学习不意味着可计算的通用在线学习，还对相关变体学习进行研究并给出可学习类的刻画。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习理论多关注抽象学习，忽略学习能否实现为计算机程序这一关键问题，本文针对通用在线学习解决此问题。

Method: 研究通用在线学习模型，对可计算的通用在线学习的不可知变体和特定变体进行分析。

Result: 表明通用在线学习不意味着可计算的通用在线学习，给出不可知变体可学习类的精确刻画，明确特定变体何时可行。

Conclusion: 研究为在线二分类现有理论和归纳推理相关问题提供更现实视角。

Abstract: Understanding when learning is possible is a fundamental task in the theory
of machine learning. However, many characterizations known from the literature
deal with abstract learning as a mathematical object and ignore the crucial
question: when can learning be implemented as a computer program? We address
this question for universal online learning, a generalist theoretical model of
online binary classification, recently characterized by Bousquet et al.
(STOC'21). In this model, there is no hypothesis fixed in advance; instead,
Adversary -- playing the role of Nature -- can change their mind as long as
local consistency with the given class of hypotheses is maintained. We require
Learner to achieve a finite number of mistakes while using a strategy that can
be implemented as a computer program. We show that universal online learning
does not imply computable universal online learning, even if the class of
hypotheses is relatively easy from a computability-theoretic perspective. We
then study the agnostic variant of computable universal online learning and
provide an exact characterization of classes that are learnable in this sense.
We also consider a variant of proper universal online learning and show exactly
when it is possible. Together, our results give a more realistic perspective on
the existing theory of online binary classification and the related problem of
inductive inference.

</details>


### [140] [Ensembling Pruned Attention Heads For Uncertainty-Aware Efficient Transformers](https://arxiv.org/abs/2510.18358)
*Firas Gabetni,Giuseppe Curci,Andrea Pilzer,Subhankar Roy,Elisa Ricci,Gianni Franchi*

Main category: cs.LG

TL;DR: 提出Hydra Ensembles高效集成方法，推理速度快，UQ性能优，实验表现好。


<details>
  <summary>Details</summary>
Motivation: 现有UQ方法如Deep Ensembles计算和内存成本高，难以扩展到大型模型。

Method: 引入Hydra Ensembles，通过修剪注意力头创建多样化成员，用新的多头注意力和分组全连接层合并。

Result: 模型推理速度接近单网络，UQ性能匹配或超越Deep Ensembles，在图像和文本分类任务中表现好，零样本分类超越SOTA。

Conclusion: Hydra Ensembles是一种高效的UQ方法，无需从头重新训练，能有效解决现有方法的扩展性问题。

Abstract: Uncertainty quantification (UQ) is essential for deploying deep neural
networks in safety-critical settings. Although methods like Deep Ensembles
achieve strong UQ performance, their high computational and memory costs hinder
scalability to large models. We introduce Hydra Ensembles, an efficient
transformer-based ensemble that prunes attention heads to create diverse
members and merges them via a new multi-head attention with grouped
fully-connected layers. This yields a compact model with inference speed close
to a single network, matching or surpassing Deep Ensembles in UQ performance
without retraining from scratch. We also provide an in-depth analysis of
pruning, showing that naive approaches can harm calibration, whereas Hydra
Ensembles preserves robust uncertainty. Experiments on image and text
classification tasks, with various architectures, show consistent gains over
Deep Ensembles. Remarkably, in zero-shot classification on ImageNet-1k, our
approach surpasses state of the art methods, even without requiring additional
training.

</details>


### [141] [Learning to Flow from Generative Pretext Tasks for Neural Architecture Encoding](https://arxiv.org/abs/2510.18360)
*Sunwoo Kim,Hyunjin Hwang,Kijung Shin*

Main category: cs.LG

TL;DR: 提出FGP预训练方法用于神经架构编码，实验显示相比仅用监督学习训练的编码器，在Precision - 1%上性能提升达106%。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息流动的编码器结构复杂，处理神经架构速度慢，需更高效方法快速准确识别适合目标任务和数据集的架构。

Method: 提出FGP预训练方法，训练编码器重构信息流动替代物以捕获信息流动，无需专门模型结构。

Result: FGP使编码器在Precision - 1%上性能提升达106%。

Conclusion: FGP是一种有效的神经架构编码预训练方法，能在不依赖复杂结构的情况下提升编码器性能。

Abstract: The performance of a deep learning model on a specific task and dataset
depends heavily on its neural architecture, motivating considerable efforts to
rapidly and accurately identify architectures suited to the target task and
dataset. To achieve this, researchers use machine learning models-typically
neural architecture encoders-to predict the performance of a neural
architecture. Many state-of-the-art encoders aim to capture information flow
within a neural architecture, which reflects how information moves through the
forward pass and backpropagation, via a specialized model structure. However,
due to their complicated structures, these flow-based encoders are
significantly slower to process neural architectures compared to simpler
encoders, presenting a notable practical challenge. To address this, we propose
FGP, a novel pre-training method for neural architecture encoding that trains
an encoder to capture the information flow without requiring specialized model
structures. FGP trains an encoder to reconstruct a flow surrogate, our proposed
representation of the neural architecture's information flow. Our experiments
show that FGP boosts encoder performance by up to 106% in Precision-1%,
compared to the same encoder trained solely with supervised learning.

</details>


### [142] [Towards Unsupervised Open-Set Graph Domain Adaptation via Dual Reprogramming](https://arxiv.org/abs/2510.18363)
*Zhen Zhang,Bingsheng He*

Main category: cs.LG

TL;DR: 本文研究无监督开放集图域适应问题，提出GraphRTA框架，实验表明其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有图域适应模型多关注闭集设置，现实中目标域可能有源域不存在的类别，因此研究无监督开放集图域适应问题。

Method: 提出GraphRTA框架，从图和模型两方面进行重编程，修改目标图结构和节点特征，修剪特定域参数，扩展分类器维度。

Result: 在多个公共数据集上的综合实验表明，提出的模型相比近期最先进的基线能取得满意的性能。

Conclusion: GraphRTA框架能有效解决无监督开放集图域适应问题，代码和数据集已公开。

Abstract: Unsupervised Graph Domain Adaptation has become a promising paradigm for
transferring knowledge from a fully labeled source graph to an unlabeled target
graph. Existing graph domain adaptation models primarily focus on the
closed-set setting, where the source and target domains share the same label
spaces. However, this assumption might not be practical in the real-world
scenarios, as the target domain might include classes that are not present in
the source domain. In this paper, we investigate the problem of unsupervised
open-set graph domain adaptation, where the goal is to not only correctly
classify target nodes into the known classes, but also recognize previously
unseen node types into the unknown class. Towards this end, we propose a novel
framework called GraphRTA, which conducts reprogramming on both the graph and
model sides. Specifically, we reprogram the graph by modifying target graph
structure and node features, which facilitates better separation of known and
unknown classes. Meanwhile, we also perform model reprogramming by pruning
domain-specific parameters to reduce bias towards the source graph while
preserving parameters that capture transferable patterns across graphs.
Additionally, we extend the classifier with an extra dimension for the unknown
class, thus eliminating the need of manually specified threshold in open-set
recognition. Comprehensive experiments on several public datasets demonstrate
that our proposed model can achieve satisfied performance compared with recent
state-of-the-art baselines. Our source codes and datasets are publicly
available at https://github.com/cszhangzhen/GraphRTA.

</details>


### [143] [Training Diverse Graph Experts for Ensembles: A Systematic Empirical Study](https://arxiv.org/abs/2510.18370)
*Gangda Deng,Yuxin Yang,Ömer Faruk Akgül,Hanqing Zeng,Yinglong Xia,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 本文对GNN集成的专家级多样化技术进行系统实证研究，评估20种策略，构建分析超200种集成变体，提供专家训练和有效MoE框架设计指导。


<details>
  <summary>Details</summary>
Motivation: 单一GNN性能受真实图数据异质性限制，MoE框架中组合多样GNN可提升性能，缺乏对GNN集成专家级多样化技术的系统研究。

Method: 评估20种多样化策略，包括随机重新初始化、超参数调整等，在14个节点分类基准上构建分析超200个集成变体。

Result: 全面评估每种技术的专家多样性、互补性和集成性能，揭示训练最大多样化专家的机制见解。

Conclusion: 研究结果为图数据上的专家训练和有效MoE框架设计提供了可操作的指导。

Abstract: Graph Neural Networks (GNNs) have become essential tools for learning on
relational data, yet the performance of a single GNN is often limited by the
heterogeneity present in real-world graphs. Recent advances in
Mixture-of-Experts (MoE) frameworks demonstrate that assembling multiple,
explicitly diverse GNNs with distinct generalization patterns can significantly
improve performance. In this work, we present the first systematic empirical
study of expert-level diversification techniques for GNN ensembles. Evaluating
20 diversification strategies -- including random re-initialization,
hyperparameter tuning, architectural variation, directionality modeling, and
training data partitioning -- across 14 node classification benchmarks, we
construct and analyze over 200 ensemble variants. Our comprehensive evaluation
examines each technique in terms of expert diversity, complementarity, and
ensemble performance. We also uncovers mechanistic insights into training
maximally diverse experts. These findings provide actionable guidance for
expert training and the design of effective MoE frameworks on graph data. Our
code is available at https://github.com/Hydrapse/bench-gnn-diversification.

</details>


### [144] [Approximation Rates of Shallow Neural Networks: Barron Spaces, Activation Functions and Optimality Analysis](https://arxiv.org/abs/2510.18388)
*Jian Lu,Xiaohuang Huang*

Main category: cs.LG

TL;DR: 研究指数函数幂次激活函数的浅层神经网络逼近性质，明确逼近能力限制并为激活函数和网络结构选择提供见解。


<details>
  <summary>Details</summary>
Motivation: 探究特定激活函数的浅层神经网络逼近性质，分析逼近率与维度和函数光滑性的关系。

Method: 研究ReLU^k激活函数逼近率，在不同空间建立最优逼近率。

Result: 证明在特定条件下无法达到最优逼近率，证实维数灾难。

Conclusion: 明确浅层神经网络逼近能力的限制，为激活函数和网络结构选择提供参考。

Abstract: This paper investigates the approximation properties of shallow neural
networks with activation functions that are powers of exponential functions. It
focuses on the dependence of the approximation rate on the dimension and the
smoothness of the function being approximated within the Barron function space.
We examine the approximation rates of ReLU$^{k}$ activation functions, proving
that the optimal rate cannot be achieved under $\ell^{1}$-bounded coefficients
or insufficient smoothness conditions.
  We also establish optimal approximation rates in various norms for functions
in Barron spaces and Sobolev spaces, confirming the curse of dimensionality.
Our results clarify the limits of shallow neural networks' approximation
capabilities and offer insights into the selection of activation functions and
network structures.

</details>


### [145] [Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees](https://arxiv.org/abs/2510.18406)
*Miao Zhang,Junpeng Li,ChangChun HUa,Yana Yang*

Main category: cs.LG

TL;DR: 研究NTMP监督设置，推导可训练无偏风险估计器（URE），扩展到多种情况，证明泛化性和一致性，引入ReLU修正，在基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 研究弱监督学习中每个训练示例为含m个正例的n元组且仅观察到m的NTMP监督设置。

Method: 将元组生成过程与潜在实例边缘联系，推导封闭形式URE并扩展，通过Rademacher复杂度建立泛化界，引入ReLU修正。

Result: 在转换为NTMP任务的基准测试中，该方法始终优于代表性弱监督基线，在精度 - 召回和F1权衡方面表现良好，在类别先验不平衡和不同元组配置下保持稳健。

Conclusion: 仅计数监督可通过理论可靠且实际稳定的目标有效利用。

Abstract: Weakly supervised learning often operates with coarse aggregate signals
rather than instance labels. We study a setting where each training example is
an $n$-tuple containing exactly m positives, while only the count m per tuple
is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g.,
image classification with region proposals and multi-instance measurements. We
show that tuple counts admit a trainable unbiased risk estimator (URE) by
linking the tuple-generation process to latent instance marginals. Starting
from fixed (n,m), we derive a closed-form URE and extend it to variable tuple
sizes, variable counts, and their combination. Identification holds whenever
the effective mixing rate is separated from the class prior. We establish
generalization bounds via Rademacher complexity and prove statistical
consistency with standard rates under mild regularity assumptions. To improve
finite-sample stability, we introduce simple ReLU corrections to the URE that
preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the
approach consistently outperforms representative weak-supervision baselines and
yields favorable precision-recall and F1 trade-offs. It remains robust under
class-prior imbalance and across diverse tuple configurations, demonstrating
that count-only supervision can be exploited effectively through a
theoretically grounded and practically stable objective.

</details>


### [146] [Provable Generalization Bounds for Deep Neural Networks with Adaptive Regularization](https://arxiv.org/abs/2510.18410)
*Adeel Safder*

Main category: cs.LG

TL;DR: 提出MAGDrop正则化方法，理论推导其PAC - Bayes泛化界，实证表明在MNIST和CIFAR - 10上优于基线方法，提供增强DNN泛化性的框架。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络因高容量常出现过拟合问题，需要一种新的正则化方法增强稳定性和泛化性。

Method: 引入Momentum - Adaptive Gradient Dropout (MAGDrop)方法，根据当前梯度和累积动量动态调整激活的丢弃率；推导考虑其自适应特性的PAC - Bayes泛化界。

Result: 理论上，相比标准方法，泛化界提升达20%；实证上，在MNIST和CIFAR - 10测试准确率上比基线方法高1 - 2%，泛化差距分别为0.48%和7.14%。

Conclusion: 工作结合理论见解和实践进展，为高风险应用提供增强DNN泛化性的稳健框架。

Abstract: Deep neural networks (DNNs) achieve remarkable performance but often suffer
from overfitting due to their high capacity. We introduce Momentum-Adaptive
Gradient Dropout (MAGDrop), a novel regularization method that dynamically
adjusts dropout rates on activations based on current gradients and accumulated
momentum, enhancing stability in non-convex optimization landscapes. To
theoretically justify MAGDrop's effectiveness, we derive a tightened PAC-Bayes
generalization bound that accounts for its adaptive nature, achieving up to 20%
sharper bounds compared to standard approaches by leveraging momentum-driven
perturbation control. Empirically, the activation-based MAGDrop outperforms
baseline regularization techniques, including standard dropout and adaptive
gradient regularization, by 1-2% in test accuracy on MNIST (99.52%) and
CIFAR-10 (90.63%), with generalization gaps of 0.48% and 7.14%, respectively.
Our work bridges theoretical insights and practical advancements, offering a
robust framework for enhancing DNN generalization suitable for high-stakes
applications.

</details>


### [147] [Learning Boltzmann Generators via Constrained Mass Transport](https://arxiv.org/abs/2510.18460)
*Christopher von Klitzing,Denis Blessing,Henrik Schopmans,Pascal Friederich,Gerhard Neumann*

Main category: cs.LG

TL;DR: 提出Constrained Mass Transport (CMT) 变分框架用于高效采样，在标准基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决从高维多峰非归一化概率分布采样的难题，现有经典变分方法和退火方法有缺陷。

Method: 引入CMT变分框架，在KL散度和熵衰减约束下生成中间分布。

Result: 在标准BG基准测试和ELIL四肽上，CMT超越现有变分方法，有效样本量提高超2.5倍且避免模式崩溃。

Conclusion: CMT是一种更优的从高维多峰非归一化概率分布采样的方法。

Abstract: Efficient sampling from high-dimensional and multimodal unnormalized
probability distributions is a central challenge in many areas of science and
machine learning. We focus on Boltzmann generators (BGs) that aim to sample the
Boltzmann distribution of physical systems, such as molecules, at a given
temperature. Classical variational approaches that minimize the reverse
Kullback-Leibler divergence are prone to mode collapse, while annealing-based
methods, commonly using geometric schedules, can suffer from mass teleportation
and rely heavily on schedule tuning. We introduce Constrained Mass Transport
(CMT), a variational framework that generates intermediate distributions under
constraints on both the KL divergence and the entropy decay between successive
steps. These constraints enhance distributional overlap, mitigate mass
teleportation, and counteract premature convergence. Across standard BG
benchmarks and the here introduced ELIL tetrapeptide, the largest system
studied to date without access to samples from molecular dynamics, CMT
consistently surpasses state-of-the-art variational methods, achieving more
than 2.5x higher effective sample size while avoiding mode collapse.

</details>


### [148] [Simple and Efficient Heterogeneous Temporal Graph Neural Network](https://arxiv.org/abs/2510.18467)
*Yili Wang,Tairan Huang,Changlong He,Qiutong Li,Jianliang Gao*

Main category: cs.LG

TL;DR: 提出SE - HTGNN学习范式，集成时空学习，用大模型提示，速度比基线快10倍且预测精度高。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的HTG表示学习方法采用解耦的时空学习范式，削弱时空信息交互且模型复杂度高。

Method: 通过动态注意力机制将时间建模集成到空间学习中，用大语言模型提示SE - HTGNN。

Result: SE - HTGNN比现有基线速度快达10倍，同时保持最佳预测精度。

Conclusion: SE - HTGNN是一种简单高效的HTG学习范式，能有效提升HTG表示学习效果。

Abstract: Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in the
real world. Recently, to enhance representation learning on HTGs, numerous
attention-based neural networks have been proposed. Despite these successes,
existing methods rely on a decoupled temporal and spatial learning paradigm,
which weakens interactions of spatio-temporal information and leads to a high
model complexity. To bridge this gap, we propose a novel learning paradigm for
HTGs called Simple and Efficient Heterogeneous Temporal Graph N}eural Network
(SE-HTGNN). Specifically, we innovatively integrate temporal modeling into
spatial learning via a novel dynamic attention mechanism, which retains
attention information from historical graph snapshots to guide subsequent
attention computation, thereby improving the overall discriminative
representations learning of HTGs. Additionally, to comprehensively and
adaptively understand HTGs, we leverage large language models to prompt
SE-HTGNN, enabling the model to capture the implicit properties of node types
as prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves up
to 10x speed-up over the state-of-the-art and latest baseline while maintaining
the best forecasting accuracy.

</details>


### [149] [Benchmarking Fairness-aware Graph Neural Networks in Knowledge Graphs](https://arxiv.org/abs/2510.18473)
*Yuya Sasaki*

Main category: cs.LG

TL;DR: 本文对知识图谱上的公平感知图神经网络进行基准测试研究，发现知识图谱与现有数据集趋势不同，性能受多种因素影响，不同方法对公平性和准确性提升有差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究未对知识图谱上的公平感知图神经网络进行评估，而知识图谱在许多应用中非常重要。

Method: 从YAGO、DBpedia和Wikidata三个知识图谱生成新图，在不同GNN骨干网络和早停条件下对预处理和处理中方法进行基准测试。

Result: 知识图谱与现有数据集趋势不同，有更清晰的预测准确性和公平性指标权衡；性能受公平感知GNN方法、GNN骨干网络和早停条件影响；预处理方法常提升公平性指标，处理中方法提升预测准确性。

Conclusion: 对知识图谱上公平感知GNN的基准测试研究具有重要意义，不同因素和方法对性能有不同影响。

Abstract: Graph neural networks (GNNs) are powerful tools for learning from
graph-structured data but often produce biased predictions with respect to
sensitive attributes. Fairness-aware GNNs have been actively studied for
mitigating biased predictions. However, no prior studies have evaluated
fairness-aware GNNs on knowledge graphs, which are one of the most important
graphs in many applications, such as recommender systems. Therefore, we
introduce a benchmarking study on knowledge graphs. We generate new graphs from
three knowledge graphs, YAGO, DBpedia, and Wikidata, that are significantly
larger than the existing graph datasets used in fairness studies. We benchmark
inprocessing and preprocessing methods in different GNN backbones and early
stopping conditions. We find several key insights: (i) knowledge graphs show
different trends from existing datasets; clearer trade-offs between prediction
accuracy and fairness metrics than other graphs in fairness-aware GNNs, (ii)
the performance is largely affected by not only fairness-aware GNN methods but
also GNN backbones and early stopping conditions, and (iii) preprocessing
methods often improve fairness metrics, while inprocessing methods improve
prediction accuracy.

</details>


### [150] [Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation](https://arxiv.org/abs/2510.18478)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: 提出Uncertain Safety Critic (USC)方法用于强化学习安全探索，实验表明其可减少安全违规并保持奖励。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习安全探索方法难以平衡安全与任务性能，要么严格执行安全会降低任务性能，要么优先奖励会频繁违反安全约束。

Method: 引入Uncertain Safety Critic (USC)，将不确定性感知调制和细化集成到评论家训练中。

Result: USC减少约40%的安全违规，同时保持有竞争力或更高的奖励，减少约83%的预测和真实成本梯度之间的误差。

Conclusion: USC打破了安全和性能之间的权衡，为可扩展的安全强化学习铺平道路。

Abstract: Ensuring the safe exploration of reinforcement learning (RL) agents is
critical for deployment in real-world systems. Yet existing approaches struggle
to strike the right balance: methods that tightly enforce safety often cripple
task performance, while those that prioritize reward leave safety constraints
frequently violated, producing diffuse cost landscapes that flatten gradients
and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a
novel approach that integrates uncertainty-aware modulation and refinement into
critic training. By concentrating conservatism in uncertain and costly regions
while preserving sharp gradients in safe areas, USC enables policies to achieve
effective reward-safety trade-offs. Extensive experiments show that USC reduces
safety violations by approximately 40% while maintaining competitive or higher
rewards, and reduces the error between predicted and true cost gradients by
approximately 83%, breaking the prevailing trade-off between safety and
performance and paving the way for scalable safe RL.

</details>


### [151] [Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning](https://arxiv.org/abs/2510.18485)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: 提出COPPOL方法，在卫星基准测试中提升危险覆盖、减少导航违规，且对分布偏移有鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有方法存在假设完美危险检测能力以及缺乏有限样本保证的问题，需要可靠导航方法来加强下游安全处理

Method: 提出COPPOL，一种基于共形驱动的感知到策略学习方法，将无分布有限样本安全保证集成到语义分割中，生成有严格界限的校准危险地图

Result: 在两个卫星基准测试中，相比基线方法危险覆盖率最多提升6倍，几乎能完全检测到不安全区域，导航时危险违规最多减少约50%

Conclusion: COPPOL方法有效，且对分布偏移保持鲁棒，能兼顾安全和效率

Abstract: Reliable navigation in safety-critical environments requires both accurate
hazard perception and principled uncertainty handling to strengthen downstream
safety handling. Despite the effectiveness of existing approaches, they assume
perfect hazard detection capabilities, while uncertainty-aware perception
approaches lack finite-sample guarantees. We present COPPOL, a conformal-driven
perception-to-policy learning approach that integrates distribution-free,
finite-sample safety guarantees into semantic segmentation, yielding calibrated
hazard maps with rigorous bounds for missed detections. These maps induce
risk-aware cost fields for downstream RL planning. Across two satellite-derived
benchmarks, COPPOL increases hazard coverage (up to 6x) compared to comparative
baselines, achieving near-complete detection of unsafe regions while reducing
hazardous violations during navigation (up to approx 50%). More importantly,
our approach remains robust to distributional shift, preserving both safety and
efficiency.

</details>


### [152] [Alibaba International E-commerce Product Search Competition DILAB Team Technical Report](https://arxiv.org/abs/2510.18499)
*Hyewon Lee,Junghyun Oh,Minkyung Song,Soyoung Park,Seunghoon Han*

Main category: cs.LG

TL;DR: DILAB团队开发多语言电商搜索系统，在最终排行榜获第5名，得分0.8819，介绍了方法及系统优势，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决多语言查询 - 商品理解方面的挑战。

Method: 设计多阶段管道，包括数据细化、轻量级预处理和自适应建模，在建模阶段探索多种架构和微调策略，用验证集优化超参数。

Result: 系统在排行榜获第5名，得分0.8819，在评估指标上结果稳定且表现出色，在不同语言和领域有鲁棒性和适应性。

Conclusion: 系统展示了系统的数据管理和迭代评估对多语言搜索系统的有效性。

Abstract: This study presents the multilingual e-commerce search system developed by
the DILAB team, which achieved 5th place on the final leaderboard with a
competitive overall score of 0.8819, demonstrating stable and high-performing
results across evaluation metrics. To address challenges in multilingual
query-item understanding, we designed a multi-stage pipeline integrating data
refinement, lightweight preprocessing, and adaptive modeling. The data
refinement stage enhanced dataset consistency and category coverage, while
language tagging and noise filtering improved input quality. In the modeling
phase, multiple architectures and fine-tuning strategies were explored, and
hyperparameters optimized using curated validation sets to balance performance
across query-category (QC) and query-item (QI) tasks. The proposed framework
exhibited robustness and adaptability across languages and domains,
highlighting the effectiveness of systematic data curation and iterative
evaluation for multilingual search systems. The source code is available at
https://github.com/2noweyh/DILAB-Alibaba-Ecommerce-Search.

</details>


### [153] [Partial VOROS: A Cost-aware Performance Metric for Binary Classifiers with Precision and Capacity Constraints](https://arxiv.org/abs/2510.18520)
*Christopher Ratigan,Kyle Heuton,Carissa Wang,Lenore Cowen,Michael C. Hughes*

Main category: cs.LG

TL;DR: 本文针对传统ROC分析在医院警报系统应用中的不足，提出新的成本感知指标partial VOROS，实验表明其在医院警报应用中对分类器排序效果更好。


<details>
  <summary>Details</summary>
Motivation: 传统ROC分析无法捕捉影响部署的关键因素，如最小精度约束、预测阳性数量上限，且常用的曲线下面积指标不能反映误报和漏报的不对称成本。

Method: 先展示满足给定精度和容量约束的分类器子集在ROC空间中可表示为可行区域并确定其几何形状，然后定义部分面积指标，对成本参数范围求平均得到partial VOROS。

Result: 在MIMIC - IV数据集上预测死亡率风险的实验中，该成本感知指标在医院警报应用中对分类器排序效果优于其他指标。

Conclusion: 提出的成本感知指标partial VOROS更适合医院警报应用中对分类器进行排序。

Abstract: The ROC curve is widely used to assess binary classification performance. Yet
for some applications such as alert systems for hospitalized patient
monitoring, conventional ROC analysis cannot capture crucial factors that
impact deployment, such as enforcing a minimum precision constraint to avoid
false alarm fatigue or imposing an upper bound on the number of predicted
positives to represent the capacity of hospital staff. The usual area under the
curve metric also does not reflect asymmetric costs for false positives and
false negatives. In this paper we address all three of these issues. First, we
show how the subset of classifiers that meet given precision and capacity
constraints can be represented as a feasible region in ROC space. We establish
the geometry of this feasible region. We then define the partial area of lesser
classifiers, a performance metric that is monotonic with cost and only accounts
for the feasible portion of ROC space. Averaging this area over a desired range
of cost parameters results in the partial volume over the ROC surface, or
partial VOROS. In experiments predicting mortality risk using vital sign
history on the MIMIC-IV dataset, we show this cost-aware metric is better than
alternatives for ranking classifiers in hospital alert applications.

</details>


### [154] [Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation](https://arxiv.org/abs/2510.18541)
*Giovanni De Muri,Mark Vero,Robin Staab,Martin Vechev*

Main category: cs.LG

TL;DR: 研究从被植入后门的大语言模型（LLM）进行知识蒸馏的安全问题，提出新后门技术T - MTB并研究可转移后门的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为知识蒸馏的教师模型可能来自不可信方，会带来安全风险，且现有后门大多无法转移到学生模型，低估了知识蒸馏的安全风险。

Method: 提出新的后门技术T - MTB，构建由常见于蒸馏数据集的特定标记组成的复合后门触发器。

Result: 展示并广泛研究了在越狱和内容调制两种攻击场景以及四种LLM模型族中可转移后门的安全风险。

Conclusion: 新的后门技术T - MTB能使后门转移到学生模型，凸显了知识蒸馏存在的安全隐患。

Abstract: LLMs are often used by downstream users as teacher models for knowledge
distillation, compressing their capabilities into memory-efficient models.
However, as these teacher models may stem from untrusted parties, distillation
can raise unexpected security risks. In this paper, we investigate the security
implications of knowledge distillation from backdoored teacher models. First,
we show that prior backdoors mostly do not transfer onto student models. Our
key insight is that this is because existing LLM backdooring methods choose
trigger tokens that rarely occur in usual contexts. We argue that this
underestimates the security risks of knowledge distillation and introduce a new
backdooring technique, T-MTB, that enables the construction and study of
transferable backdoors. T-MTB carefully constructs a composite backdoor
trigger, made up of several specific tokens that often occur individually in
anticipated distillation datasets. As such, the poisoned teacher remains
stealthy, while during distillation the individual presence of these tokens
provides enough signal for the backdoor to transfer onto the student. Using
T-MTB, we demonstrate and extensively study the security risks of transferable
backdoors across two attack scenarios, jailbreaking and content modulation, and
across four model families of LLMs.

</details>


### [155] [HeFS: Helper-Enhanced Feature Selection via Pareto-Optimized Genetic Search](https://arxiv.org/abs/2510.18575)
*Yusi Fan,Tian Wang,Zhiying Yan,Chang Liu,Qiong Zhou,Qi Lu,Zhehao Guo,Ziqi Deng,Wenyu Zhu,Ruochi Zhang,Fengfeng Zhou*

Main category: cs.LG

TL;DR: 提出HeFS框架优化现有算法特征子集，在18个基准数据集实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法易早熟收敛，难以捕捉高维数据集中细微但重要的特征。

Method: 引入HeFS框架，采用有偏初始化方案和比率引导变异机制，结合基于帕累托的多目标优化。

Result: 在18个基准数据集实验中，HeFS能识别被忽视但重要的特征，在胃癌分类、药物毒性预测等领域表现优于现有方法。

Conclusion: HeFS框架能有效优化特征子集，提升分类性能。

Abstract: Feature selection is a combinatorial optimization problem that is NP-hard.
Conventional approaches often employ heuristic or greedy strategies, which are
prone to premature convergence and may fail to capture subtle yet informative
features. This limitation becomes especially critical in high-dimensional
datasets, where complex and interdependent feature relationships prevail. We
introduce the HeFS (Helper-Enhanced Feature Selection) framework to refine
feature subsets produced by existing algorithms. HeFS systematically searches
the residual feature space to identify a Helper Set - features that complement
the original subset and improve classification performance. The approach
employs a biased initialization scheme and a ratio-guided mutation mechanism
within a genetic algorithm, coupled with Pareto-based multi-objective
optimization to jointly maximize predictive accuracy and feature
complementarity. Experiments on 18 benchmark datasets demonstrate that HeFS
consistently identifies overlooked yet informative features and achieves
superior performance over state-of-the-art methods, including in challenging
domains such as gastric cancer classification, drug toxicity prediction, and
computer science applications. The code and datasets are available at
https://healthinformaticslab.org/supp/.

</details>


### [156] [Robustness Verification of Graph Neural Networks Via Lightweight Satisfiability Testing](https://arxiv.org/abs/2510.18591)
*Chia-Hsuan Lu,Tony Tan,Michael Benedikt*

Main category: cs.LG

TL;DR: 本文提出用高效部分求解器替代强大求解器来提高图结构鲁棒性，并在多种GNN变体和数据集上评估了工具RobLight。


<details>
  <summary>Details</summary>
Motivation: 现有图学习中对抗鲁棒性问题的技术基于强大求解器，本文旨在改进结构鲁棒性方面的现有技术。

Method: 用高效部分求解器替代强大求解器，该部分求解器运行时间为多项式但可能不完整。

Result: 在多种GNN变体和数据集上对工具RobLight进行了评估。

Conclusion: 可以通过使用高效部分求解器改进结构鲁棒性方面的现有技术。

Abstract: Graph neural networks (GNNs) are the predominant architecture for learning
over graphs. As with any machine learning model, and important issue is the
detection of adversarial attacks, where an adversary can change the output with
a small perturbation of the input. Techniques for solving the adversarial
robustness problem - determining whether such an attack exists - were
originally developed for image classification, but there are variants for many
other machine learning architectures. In the case of graph learning, the attack
model usually considers changes to the graph structure in addition to or
instead of the numerical features of the input, and the state of the art
techniques in the area proceed via reduction to constraint solving, working on
top of powerful solvers, e.g. for mixed integer programming. We show that it is
possible to improve on the state of the art in structural robustness by
replacing the use of powerful solvers by calls to efficient partial solvers,
which run in polynomial time but may be incomplete. We evaluate our tool
RobLight on a diverse set of GNN variants and datasets.

</details>


### [157] [Unrolled-SINDy: A Stable Explicit Method for Non linear PDE Discovery from Sparsely Sampled Data](https://arxiv.org/abs/2510.18611)
*Fayad Ali Banna,Antoine Caradot,Eduardo Brandao,Jean-Philippe Colombier,Rémi Emonet,Marc Sebban*

Main category: cs.LG

TL;DR: 本文提出Unrolled - SINDy方法解决SINDy在稀疏采样数据下的问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于SINDy的方法无法解决时间上稀疏采样数据的现实问题。

Method: 引入Unrolled - SINDy方法，利用展开方案提高PDE发现显式方法的稳定性，可通过迭代闭式方法或梯度下降方案实现。

Result: 实验表明该方法具有通用性，在传统SINDy和最先进的抗噪声iNeuralSINDy上，能解决非展开方法无法处理的问题。

Conclusion: 提出的Unrolled - SINDy方法有效，能解决稀疏采样数据问题。

Abstract: Identifying from observation data the governing differential equations of a
physical dynamics is a key challenge in machine learning. Although approaches
based on SINDy have shown great promise in this area, they still fail to
address a whole class of real world problems where the data is sparsely sampled
in time. In this article, we introduce Unrolled-SINDy, a simple methodology
that leverages an unrolling scheme to improve the stability of explicit methods
for PDE discovery. By decorrelating the numerical time step size from the
sampling rate of the available data, our approach enables the recovery of
equation parameters that would not be the minimizers of the original SINDy
optimization problem due to large local truncation errors. Our method can be
exploited either through an iterative closed-form approach or by a gradient
descent scheme. Experiments show the versatility of our method. On both
traditional SINDy and state-of-the-art noise-robust iNeuralSINDy, with
different numerical schemes (Euler, RK4), our proposed unrolling scheme allows
to tackle problems not accessible to non-unrolled methods.

</details>


### [158] [A Rectification-Based Approach for Distilling Boosted Trees into Decision Trees](https://arxiv.org/abs/2510.18615)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.LG

TL;DR: 提出将提升树蒸馏为决策树的新方法，用整流校正法实现，实验结果比再训练蒸馏法好。


<details>
  <summary>Details</summary>
Motivation: 生成一个在预测性能和可解释性之间取得可接受折衷的机器学习模型。

Method: 采用称为整流的校正方法来实现蒸馏过程。

Result: 与通过再训练模型实现的蒸馏方法相比，该方法取得了有趣的结果。

Conclusion: 提出的将提升树蒸馏为决策树的新方法具有一定优势。

Abstract: We present a new approach for distilling boosted trees into decision trees,
in the objective of generating an ML model offering an acceptable compromise in
terms of predictive performance and interpretability. We explain how the
correction approach called rectification can be used to implement such a
distillation process. We show empirically that this approach provides
interesting results, in comparison with an approach to distillation achieved by
retraining the model.

</details>


### [159] [Hardness of Learning Regular Languages in the Next Symbol Prediction Setting](https://arxiv.org/abs/2510.18634)
*Satwik Bhattamishra,Phil Blunsom,Varun Kanade*

Main category: cs.LG

TL;DR: 研究Next Symbol Prediction (NSP)设置下语言的可学习性，表明学习DFAs和布尔公式等概念类计算上仍困难。


<details>
  <summary>Details</summary>
Motivation: 该设置已用于实证分析神经序列模型，高效算法可用于学习语言模型的（截断）支持，进行PAC学习分析。

Method: 形式化设置，通过构造使几乎所有额外标签无信息，从传统学习问题归约到NSP标签学习。

Result: 学习DFAs和布尔公式等概念类计算上仍困难，在密码学假设下，NSP设置中学习DFAs问题计算困难。

Conclusion: 尽管NSP设置比传统分类设置标签更丰富，但学习特定概念类仍有计算困难。

Abstract: We study the learnability of languages in the Next Symbol Prediction (NSP)
setting, where a learner receives only positive examples from a language
together with, for every prefix, (i) whether the prefix itself is in the
language and (ii) which next symbols can lead to an accepting string. This
setting has been used in prior works to empirically analyze neural sequence
models, and additionally, we observe that efficient algorithms for the NSP
setting can be used to learn the (truncated) support of language models. We
formalize the setting so as to make it amenable to PAC-learning analysis. While
the setting provides a much richer set of labels than the conventional
classification setting, we show that learning concept classes such as DFAs and
Boolean formulas remains computationally hard. The proof is via a construction
that makes almost all additional labels uninformative, yielding a reduction
from the conventional learning problem to learning with NSP labels. Under
cryptographic assumptions, the reduction implies that the problem of learning
DFAs is computationally hard in the NSP setting.

</details>


### [160] [Informed Learning for Estimating Drought Stress at Fine-Scale Resolution Enables Accurate Yield Prediction](https://arxiv.org/abs/2510.18648)
*Miro Miranda,Marcela Charfuelan,Matias Valdenegro Toro,Andreas Dengel*

Main category: cs.LG

TL;DR: 本文结合作物模拟模型和机器学习模型优势，提出新方法预测作物产量，效果超现有模型且有高可解释性，为农业决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 作物模拟模型表现不佳，机器学习模型缺乏物理原理，需结合两者优势解决作物产量预测问题，保障农业生产力和粮食安全。

Method: 将作物产量表示为时间缺水函数，预测作物干旱胁迫和对缺水敏感度；提出新的物理信息损失函数；利用多光谱卫星图像、气象数据和精细产量数据；采用深度集成方法处理模型不确定性。

Result: 该方法在作物产量预测上超越LSTM和Transformers等模型，决定系数达0.82，且有高可解释性。

Conclusion: 此方法可为行业、政策制定者和农民在气候变化时构建更具韧性的农业提供决策支持。

Abstract: Water is essential for agricultural productivity. Assessing water shortages
and reduced yield potential is a critical factor in decision-making for
ensuring agricultural productivity and food security. Crop simulation models,
which align with physical processes, offer intrinsic explainability but often
perform poorly. Conversely, machine learning models for crop yield modeling are
powerful and scalable, yet they commonly operate as black boxes and lack
adherence to the physical principles of crop growth. This study bridges this
gap by coupling the advantages of both worlds. We postulate that the crop yield
is inherently defined by the water availability. Therefore, we formulate crop
yield as a function of temporal water scarcity and predict both the crop
drought stress and the sensitivity to water scarcity at fine-scale resolution.
Sequentially modeling the crop yield response to water enables accurate yield
prediction. To enforce physical consistency, a novel physics-informed loss
function is proposed. We leverage multispectral satellite imagery,
meteorological data, and fine-scale yield data. Further, to account for the
uncertainty within the model, we build upon a deep ensemble approach. Our
method surpasses state-of-the-art models like LSTM and Transformers in crop
yield prediction with a coefficient of determination ($R^2$-score) of up to
0.82 while offering high explainability. This method offers decision support
for industry, policymakers, and farmers in building a more resilient
agriculture in times of changing climate conditions.

</details>


### [161] [Learning Time-Varying Turn-Taking Behavior in Group Conversations](https://arxiv.org/abs/2510.18649)
*Madeline Navarro,Lisa O'Bryan,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出基于个体特征和过往发言行为预测群体对话轮流模式的概率模型，用合成和真实数据验证。


<details>
  <summary>Details</summary>
Motivation: 现有对话动态模型难以跨群体泛化，通用表述不适用于所有群体。

Method: 开发先前对话模型的泛化版本，基于个体特征和过往发言行为预测发言轮次，能学习发言倾向随上次发言时间的变化。

Result: 结果表明先前行为模型可能不现实，验证了数据驱动且有理论依据的方法。

Conclusion: 所提模型为群体对话轮流模式预测提供了有效方法，有一定优势。

Abstract: We propose a flexible probabilistic model for predicting turn-taking patterns
in group conversations based solely on individual characteristics and past
speaking behavior. Many models of conversation dynamics cannot yield insights
that generalize beyond a single group. Moreover, past works often aim to
characterize speaking behavior through a universal formulation that may not be
suitable for all groups. We thus develop a generalization of prior conversation
models that predicts speaking turns among individuals in any group based on
their individual characteristics, that is, personality traits, and prior
speaking behavior. Importantly, our approach provides the novel ability to
learn how speaking inclination varies based on when individuals last spoke. We
apply our model to synthetic and real-world conversation data to verify the
proposed approach and characterize real group interactions. Our results
demonstrate that previous behavioral models may not always be realistic,
motivating our data-driven yet theoretically grounded approach.

</details>


### [162] [Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor Patches](https://arxiv.org/abs/2510.18668)
*Mustafa Fuad Rifet Ibrahim,Tunc Alkanat,Maurice Meijer,Felix Manthey,Alexander Schlaefer,Peer Stelldinger*

Main category: cs.LG

TL;DR: 分析在资源受限医疗边缘设备上应用深度学习模型对同步心电图和心音图记录进行分类的可行性，提出早期融合数据的卷积神经网络，效果好且能耗低。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病若能早期检测可预防，传感器监测需高效分析数据，深度学习可自动化解读减少临床医生工作量。

Method: 提出早期融合数据的卷积神经网络解决二分类问题，在Physionet Challenge 2016数据集上训练和验证模型。

Result: 与现有技术相比，内存占用和计算成本降低三个数量级，同时保持有竞争力的准确性；分析能耗证明设备端推理比连续数据传输更节能。

Conclusion: 所提出的模型适用于医疗边缘设备。

Abstract: The vast majority of cardiovascular diseases may be preventable if early
signs and risk factors are detected. Cardiovascular monitoring with body-worn
sensor devices like sensor patches allows for the detection of such signs while
preserving the freedom and comfort of patients. However, the analysis of the
sensor data must be robust, reliable, efficient, and highly accurate. Deep
learning methods can automate data interpretation, reducing the workload of
clinicians. In this work, we analyze the feasibility of applying deep learning
models to the classification of synchronized electrocardiogram (ECG) and
phonocardiogram (PCG) recordings on resource-constrained medical edge devices.
We propose a convolutional neural network with early fusion of data to solve a
binary classification problem. We train and validate our model on the
synchronized ECG and PCG recordings from the Physionet Challenge 2016 dataset.
Our approach reduces memory footprint and compute cost by three orders of
magnitude compared to the state-of-the-art while maintaining competitive
accuracy. We demonstrate the applicability of our proposed model on medical
edge devices by analyzing energy consumption on a microcontroller and an
experimental sensor device setup, confirming that on-device inference can be
more energy-efficient than continuous data streaming.

</details>


### [163] [Reasoning Language Model Inference Serving Unveiled: An Empirical Study](https://arxiv.org/abs/2510.18672)
*Qi Li,Junpan Wu,Xiang Liu,Yuxin Wang,Zeyu Li,Zhenheng Tang,Yuhan Chen,Shaohuai Shi,Xiaowen Chu*

Main category: cs.LG

TL;DR: 本文对推理大语言模型（RLLM）服务进行全面研究，比较其与传统LLM的服务性能差异，探究现有推理优化技术对RLLM的有效性，并在真实工作负载下验证发现。


<details>
  <summary>Details</summary>
Motivation: RLLM在解决复杂推理任务上有竞争力，但服务性能和行为未被充分研究，可能影响其在现实场景的部署和使用。

Method: 先进行RLLM与传统LLM服务性能的对比试点研究，再探究现有推理优化技术对RLLM的有效性，最后在由Gamma分布建模的真实工作负载下进行评估。

Result: RLLM与传统LLM在服务行为上有显著差异；模型量化方法和投机解码能在小幅度牺牲RLLM准确性的前提下提高服务系统效率，而前缀缓存、KV缓存量化可能降低小RLLM的准确性或服务性能；真实工作负载评估结果与主要发现一致。

Conclusion: 研究成果可为研究界和行业推进RLLM推理服务提供见解。

Abstract: The reasoning large language model (RLLM) has been proven competitive in
solving complex reasoning tasks such as mathematics, coding, compared to
general LLM. However, the serving performance and behavior of RLLM remains
unexplored, which may undermine the deployment and utilization of RLLM in
real-world scenario. To close this gap, in this paper, we conduct a
comprehensive study of RLLM service. We first perform a pilot study on
comparing the serving performance between RLLM and traditional LLM and reveal
that there are several distinct differences regarding serving behavior: (1)
significant memory usage and fluctuations; (2) straggler requests; (3) adaptive
running time; (4) domain preference. Then we further investigate whether
existing inference optimization techniques are valid for RLLM. Our main
takeaways are that model quantization methods and speculative decoding can
improve service system efficiency with small compromise to RLLM accuracy, while
prefix caching, KV cache quantization may even degrade accuracy or serving
performance for small RLLM. Lastly, we conduct evaluation under real world
workload modeled by Gamma distribution to verify our findings. Empirical
results of real world workload evaluation across different dataset are aligned
with our main findings regarding RLLM serving. We hope our work can provide the
research community and industry with insights to advance RLLM inference
serving.

</details>


### [164] [Learning Task-Agnostic Representations through Multi-Teacher Distillation](https://arxiv.org/abs/2510.18680)
*Philippe Formont,Maxime Darrin,Banafsheh Karimian,Jackie CK Cheung,Eric Granger,Ismail Ben Ayed,Mohammadhadi Shateri,Pablo Piantanida*

Main category: cs.LG

TL;DR: 本文提出任务无关蒸馏框架，利用教师模型多样性，提升下游任务表现并发布SOTA嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 现有多教师蒸馏方法常针对特定任务，需提出任务无关框架。

Method: 引入基于“多数投票”目标函数的任务无关框架，该函数受学生与教师嵌入之间互信息约束，得到任务无关蒸馏损失。

Result: 在文本、视觉和分子建模评估中，方法有效利用教师多样性，下游任务表现更好，还训练并发布SOTA嵌入模型。

Conclusion: 提出的任务无关框架能有效利用教师模型多样性，提升多种下游任务性能。

Abstract: Casting complex inputs into tractable representations is a critical step
across various fields. Diverse embedding models emerge from differences in
architectures, loss functions, input modalities and datasets, each capturing
unique aspects of the input. Multi-teacher distillation leverages this
diversity to enrich representations but often remains tailored to specific
tasks. In this paper, we introduce a task-agnostic framework based on a
``majority vote" objective function. We demonstrate that this function is
bounded by the mutual information between student and teachers' embeddings,
leading to a task-agnostic distillation loss that eliminates dependence on
task-specific labels or prior knowledge. Our evaluations across text, vision
models, and molecular modeling show that our method effectively leverages
teacher diversity, resulting in representations enabling better performance for
a wide range of downstream tasks such as classification, clustering, or
regression. Additionally, we train and release state-of-the-art embedding
models, enhancing downstream performance in various modalities.

</details>


### [165] [Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach](https://arxiv.org/abs/2510.18687)
*Chenbei Lu,Zaiwei Chen,Tongxin Li,Chenye Wu,Adam Wierman*

Main category: cs.LG

TL;DR: 文章针对传统强化学习在处理多步预测时的维数灾难和理论工具不足问题，提出贝叶斯值函数、贝尔曼 - 詹森差距分析和BOLA算法，并进行验证。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习基于单步转移模型，在实际应用中多步预测有优势，但存在维数灾难，且现有理论难以分析含多步预测的MDPs。

Method: 提出贝叶斯值函数；开展贝尔曼 - 詹森差距分析；引入BOLA算法，分离离线贝叶斯值学习和在线实时预测调整。

Result: 证明BOLA在不完美预测下仍有样本效率，在合成MDPs和风能存储控制问题中验证理论和算法。

Conclusion: 所提方法能有效解决传统强化学习处理多步预测的问题。

Abstract: Traditional reinforcement learning (RL) assumes the agents make decisions
based on Markov decision processes (MDPs) with one-step transition models. In
many real-world applications, such as energy management and stock investment,
agents can access multi-step predictions of future states, which provide
additional advantages for decision making. However, multi-step predictions are
inherently high-dimensional: naively embedding these predictions into an MDP
leads to an exponential blow-up in state space and the curse of dimensionality.
Moreover, existing RL theory provides few tools to analyze prediction-augmented
MDPs, as it typically works on one-step transition kernels and cannot
accommodate multi-step predictions with errors or partial action-coverage. We
address these challenges with three key innovations: First, we propose the
\emph{Bayesian value function} to characterize the optimal prediction-aware
policy tractably. Second, we develop a novel \emph{Bellman-Jensen Gap} analysis
on the Bayesian value function, which enables characterizing the value of
imperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with
Online Adaptation), a two-stage model-based RL algorithm that separates offline
Bayesian value learning from lightweight online adaptation to real-time
predictions. We prove that BOLA remains sample-efficient even under imperfect
predictions. We validate our theory and algorithm on synthetic MDPs and a
real-world wind energy storage control problem.

</details>


### [166] [OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales](https://arxiv.org/abs/2510.18707)
*Tung Nguyen,Tuan Pham,Troy Arcomano,Veerabhadra Kotamarthi,Ian Foster,Sandeep Madireddy,Aditya Grover*

Main category: cs.LG

TL;DR: 提出OmniCast模型统一跨时间尺度天气预报，性能优且速度快，还能生成百年预测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习数据驱动方法在较长亚季节到季节尺度预报因自回归误差累积表现不佳，需准确跨时间尺度天气预报应对气候变化。

Method: OmniCast由VAE模型和基于扩散的Transformer模型组成，训练时掩码随机未来令牌，推理时迭代解掩码生成完整序列。

Result: 在中尺度与领先概率方法竞争且快10 - 20倍，在亚季节到季节尺度达最优，能生成百年稳定预测。

Conclusion: OmniCast模型有效统一跨时间尺度天气预报，性能和速度表现出色。

Abstract: Accurate weather forecasting across time scales is critical for anticipating
and mitigating the impacts of climate change. Recent data-driven methods based
on deep learning have achieved significant success in the medium range, but
struggle at longer subseasonal-to-seasonal (S2S) horizons due to error
accumulation in their autoregressive approach. In this work, we propose
OmniCast, a scalable and skillful probabilistic model that unifies weather
forecasting across timescales. OmniCast consists of two components: a VAE model
that encodes raw weather data into a continuous, lower-dimensional latent
space, and a diffusion-based transformer model that generates a sequence of
future latent tokens given the initial conditioning tokens. During training, we
mask random future tokens and train the transformer to estimate their
distribution given conditioning and visible tokens using a per-token diffusion
head. During inference, the transformer generates the full sequence of future
tokens by iteratively unmasking random subsets of tokens. This joint sampling
across space and time mitigates compounding errors from autoregressive
approaches. The low-dimensional latent space enables modeling long sequences of
future latent states, allowing the transformer to learn weather dynamics beyond
initial conditions. OmniCast performs competitively with leading probabilistic
methods at the medium-range timescale while being 10x to 20x faster, and
achieves state-of-the-art performance at the subseasonal-to-seasonal scale
across accuracy, physics-based, and probabilistic metrics. Furthermore, we
demonstrate that OmniCast can generate stable rollouts up to 100 years ahead.
Code and model checkpoints are available at
https://github.com/tung-nd/omnicast.

</details>


### [167] [Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference](https://arxiv.org/abs/2510.18768)
*Harry Amad,Zhaozhi Qian,Dennis Frauen,Julianna Piskorz,Stefan Feuerriegel,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 由于监管障碍，真实医疗数据集难获取，合成数据有价值。现有方法未考虑下游因果推断挑战，本文提出合成数据应满足的条件、评估指标及新方法STEAM，且STEAM表现优。


<details>
  <summary>Details</summary>
Motivation: 真实医疗数据集因监管障碍难获取，合成数据有价值，但现有方法未考虑下游因果推断挑战。

Method: 确立合成数据应满足的条件，提出评估指标，提出生成合成数据的新方法STEAM。

Result: 实证表明，与现有生成模型相比，STEAM在各项指标上达到了最先进的性能，尤其是在真实数据生成过程复杂度增加时。

Conclusion: STEAM在生成用于医学治疗效果分析的合成数据方面表现出色，能更好地满足下游因果推断任务需求。

Abstract: Causal inference is essential for developing and evaluating medical
interventions, yet real-world medical datasets are often difficult to access
due to regulatory barriers. This makes synthetic data a potentially valuable
asset that enables these medical analyses, along with the development of new
inference methods themselves. Generative models can produce synthetic data that
closely approximate real data distributions, yet existing methods do not
consider the unique challenges that downstream causal inference tasks, and
specifically those focused on treatments, pose. We establish a set of
desiderata that synthetic data containing treatments should satisfy to maximise
downstream utility: preservation of (i) the covariate distribution, (ii) the
treatment assignment mechanism, and (iii) the outcome generation mechanism.
Based on these desiderata, we propose a set of evaluation metrics to assess
such synthetic data. Finally, we present STEAM: a novel method for generating
Synthetic data for Treatment Effect Analysis in Medicine that mimics the
data-generating process of data containing treatments and optimises for our
desiderata. We empirically demonstrate that STEAM achieves state-of-the-art
performance across our metrics as compared to existing generative models,
particularly as the complexity of the true data-generating process increases.

</details>


### [168] [CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training](https://arxiv.org/abs/2510.18784)
*Soroush Tabesh,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出CAGE方法用于低比特量化感知训练，有理论收敛保证，在预训练模型上表现好，能弥补性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前低比特量化感知训练（QAT）技术与原生训练存在较大精度差距。

Method: 引入CAGE方法，用曲率感知校正增强直通估计器（STE）梯度，从多目标视角推导，依赖局部曲率信息，实现与优化器无关，利用Adam统计高效实现。

Result: 在预训练高达8亿参数的Llama风格模型时，在W4A4模式下比离群值缓解方法恢复超10%量化导致的损失增加。

Conclusion: 曲率感知梯度校正可弥补现有离群值处理方法之外的性能差距。

Abstract: Despite significant work on low-bit quantization-aware training (QAT), there
is still a large accuracy gap between such techniques and native training. To
address this, we introduce CAGE (Curvature-Aware Gradient Estimation), a new
QAT method that augments the straight-through estimator (STE) gradient with a
curvature-aware correction designed to counteract the loss increase induced by
quantization. CAGE is derived from a multi-objective view of QAT that balances
loss minimization with adherence to quantization constraints, yielding a
principled correction term that depends on local curvature information. On the
theoretical side, we introduce the notion of Pareto-optimal solutions for
quantized optimization, and establish that CAGE yields strong convergence
guarantees in the smooth non-convex setting. In terms of implementation, our
approach is optimizer-agnostic, but we provide a highly-efficient
implementation that leverages Adam statistics. When pre-training Llama-style
models of up to 800M-parameters, CAGE recovers over 10% of the
quantization-induced loss increase in the W4A4 regime over outlier-mitigation
methods. These results indicate that curvature-aware gradient corrections can
bridge the remaining performance gap beyond current outlier-handling methods.

</details>


### [169] [Stick-Breaking Embedded Topic Model with Continuous Optimal Transport for Online Analysis of Document Streams](https://arxiv.org/abs/2510.18786)
*Federica Granese,Serena Villata,Charles Bouveyron*

Main category: cs.LG

TL;DR: 提出SB - SETM模型处理数据流，实验表明其表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 在线主题模型虽契合现实场景，但因额外挑战受关注少于离线模型，需解决相关问题。

Method: 扩展嵌入式主题模型ETM，通过合并连续部分文档批次形成的模型处理数据流，利用截断的棍子断裂构造处理主题 - 文档分布，引入基于连续最优传输公式的主题嵌入合并策略。

Result: 数值实验显示SB - SETM在模拟场景中优于基线，在俄乌战争新闻文章真实语料上进行了广泛测试。

Conclusion: SB - SETM是处理数据流主题建模的有效模型。

Abstract: Online topic models are unsupervised algorithms to identify latent topics in
data streams that continuously evolve over time. Although these methods
naturally align with real-world scenarios, they have received considerably less
attention from the community compared to their offline counterparts, due to
specific additional challenges. To tackle these issues, we present SB-SETM, an
innovative model extending the Embedded Topic Model (ETM) to process data
streams by merging models formed on successive partial document batches. To
this end, SB-SETM (i) leverages a truncated stick-breaking construction for the
topic-per-document distribution, enabling the model to automatically infer from
the data the appropriate number of active topics at each timestep; and (ii)
introduces a merging strategy for topic embeddings based on a continuous
formulation of optimal transport adapted to the high dimensionality of the
latent topic space. Numerical experiments show SB-SETM outperforming baselines
on simulated scenarios. We extensively test it on a real-world corpus of news
articles covering the Russian-Ukrainian war throughout 2022-2023.

</details>


### [170] [On Biologically Plausible Learning in Continuous Time](https://arxiv.org/abs/2510.18808)
*Marc Gong Bacvanski,Liu Ziyin,Tomaso Poggio*

Main category: cs.LG

TL;DR: 研究连续时间神经模型，统一生物合理学习算法，模拟显示其能在生物时间尺度稳定学习，学习依赖时间重叠，提出突触可塑性时间尺度要求及可测试预测。


<details>
  <summary>Details</summary>
Motivation: 生物学习是连续的，但多数算法模型依赖离散更新且分离推理和学习阶段，需研究连续时间模型。

Method: 研究连续时间神经模型，通过模拟和分析，探讨学习规则、学习与时间重叠关系等。

Result: 连续时间网络能在生物时间尺度稳定学习，学习依赖时间重叠，学习强度与输入和误差延迟有关，稳健学习对突触可塑性时间尺度有要求。

Conclusion: 提出生物电路中误差驱动学习需要秒级资格迹的可测试预测。

Abstract: Biological learning unfolds continuously in time, yet most algorithmic models
rely on discrete updates and separate inference and learning phases. We study a
continuous-time neural model that unifies several biologically plausible
learning algorithms and removes the need for phase separation. Rules including
stochastic gradient descent (SGD), feedback alignment (FA), direct feedback
alignment (DFA), and Kolen-Pollack (KP) emerge naturally as limiting cases of
the dynamics. Simulations show that these continuous-time networks stably learn
at biological timescales, even under temporal mismatches and integration noise.
Through analysis and simulation, we show that learning depends on temporal
overlap: a synapse updates correctly only when its input and the corresponding
error signal coincide in time. When inputs are held constant, learning strength
declines linearly as the delay between input and error approaches the stimulus
duration, explaining observed robustness and failure across network depths.
Critically, robust learning requires the synaptic plasticity timescale to
exceed the stimulus duration by one to two orders of magnitude. For typical
cortical stimuli (tens of milliseconds), this places the functional plasticity
window in the few-second range, a testable prediction that identifies
seconds-scale eligibility traces as necessary for error-driven learning in
biological circuits.

</details>


### [171] [When LRP Diverges from Leave-One-Out in Transformers](https://arxiv.org/abs/2510.18810)
*Weiqiu You,Siqi Zeng,Yao-Hung Hubert Tsai,Makoto Yamada,Han Zhao*

Main category: cs.LG

TL;DR: 指出AttnLRP违反实现不变性公理，CP - LRP绕过softmax层反向传播相关性可提升与LOO的对齐度，认为双线性分解敏感性和softmax传播误差共同削弱LRP在Transformer中近似LOO的能力。


<details>
  <summary>Details</summary>
Motivation: LOO计算成本高，LRP虽可能有效但在现代Transformer中的公理合理性待研究。

Method: 分析证明AttnLRP的双线性传播规则违反实现不变性公理，以CP - LRP为诊断基线进行实验。

Result: AttnLRP违反实现不变性公理；CP - LRP绕过softmax层反向传播相关性能显著提升与LOO的对齐度。

Conclusion: 双线性分解敏感性和softmax传播误差可能共同削弱LRP在Transformer中近似LOO的能力。

Abstract: Leave-One-Out (LOO) provides an intuitive measure of feature importance but
is computationally prohibitive. While Layer-Wise Relevance Propagation (LRP)
offers a potentially efficient alternative, its axiomatic soundness in modern
Transformers remains largely under-examined. In this work, we first show that
the bilinear propagation rules used in recent advances of AttnLRP violate the
implementation invariance axiom. We prove this analytically and confirm it
empirically in linear attention layers. Second, we also revisit CP-LRP as a
diagnostic baseline and find that bypassing relevance propagation through the
softmax layer -- backpropagating relevance only through the value matrices --
significantly improves alignment with LOO, particularly in middle-to-late
Transformer layers. Overall, our results suggest that (i) bilinear
factorization sensitivity and (ii) softmax propagation error potentially
jointly undermine LRP's ability to approximate LOO in Transformers.

</details>


### [172] [A Unified Perspective on Optimization in Machine Learning and Neuroscience: From Gradient Descent to Neural Adaptation](https://arxiv.org/abs/2510.18812)
*Jesús García Fernández,Nasir Ahmad,Marcel van Gerven*

Main category: cs.LG

TL;DR: 本文统一视角回顾迭代优化，对比梯度法和零阶优化法，探讨其在神经网络训练和生物学习中的应用及影响。


<details>
  <summary>Details</summary>
Motivation: 为理解迭代优化提供统一视角，连接经典理论与神经网络训练、生物学习。

Method: 按导数信息阶数对优化方法分类，探讨其在神经网络训练的适应性及学习动态。

Result: 现代零阶方法能有效近似梯度，性能可与BP竞争，零阶范式与生物学习机制相似。

Conclusion: 零阶范式利用大脑固有噪声，助于理解自然智能，对神经形态硬件设计有重要意义。

Abstract: Iterative optimization is central to modern artificial intelligence (AI) and
provides a crucial framework for understanding adaptive systems. This review
provides a unified perspective on this subject, bridging classic theory with
neural network training and biological learning. Although gradient-based
methods, powered by the efficient but biologically implausible backpropagation
(BP), dominate machine learning, their computational demands can hinder
scalability in high-dimensional settings. In contrast, derivative-free or
zeroth-order (ZO) optimization feature computationally lighter approaches that
rely only on function evaluations and randomness. While generally less sample
efficient, recent breakthroughs demonstrate that modern ZO methods can
effectively approximate gradients and achieve performance competitive with BP
in neural network models. This ZO paradigm is also particularly relevant for
biology. Its core principles of random exploration (probing) and
feedback-guided adaptation (reinforcing) parallel key mechanisms of biological
learning, offering a mathematically principled perspective on how the brain
learns. In this review, we begin by categorizing optimization approaches based
on the order of derivative information they utilize, ranging from first-,
second-, and higher-order gradient-based to ZO methods. We then explore how
these methods are adapted to the unique challenges of neural network training
and the resulting learning dynamics. Finally, we build upon these insights to
view biological learning through an optimization lens, arguing that a ZO
paradigm leverages the brain's intrinsic noise as a computational resource.
This framework not only illuminates our understanding of natural intelligence
but also holds vast implications for neuromorphic hardware, helping us design
fast and energy-efficient AI systems that exploit intrinsic hardware noise.

</details>


### [173] [Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards](https://arxiv.org/abs/2510.18814)
*Mengqi Li,Lei Zhao,Anthony Man-Cho So,Ruoyu Sun,Xiao Li*

Main category: cs.LG

TL;DR: 提出用于大语言模型推理的在线监督微调（OSFT）范式，实验显示其在数学推理任务表现佳，机制是促进模型已有偏好，是有前景的训练方式。


<details>
  <summary>Details</summary>
Motivation: 寻找高效的大语言模型推理训练策略，替代复杂的基于奖励的训练范式。

Method: 提出OSFT范式，模型生成自身响应并基于自生成数据立即微调，无奖励且默认仅一次滚动。

Result: OSFT在挑战性数学推理任务上的下游性能与强强化学习方法相当，消融研究证明其效率和鲁棒性。

Conclusion: OSFT是更复杂、基于奖励的训练范式的高效且有前景的替代方案。

Abstract: We present a simple, self-help online supervised finetuning (OSFT) paradigm
for LLM reasoning. In this paradigm, the model generates its own responses and
is immediately finetuned on this self-generated data. OSFT is a highly
efficient training strategy for LLM reasoning, as it is reward-free and uses
just one rollout by default. Experiment results show that OSFT achieves
downstream performance on challenging mathematical reasoning tasks comparable
to strong reinforcement learning with verifiable rewards (RLVR) methods such as
GRPO. Our ablation study further demonstrates the efficiency and robustness of
OSFT. The major mechanism of OSFT lies in facilitating the model's own existing
preference (latent knowledge) learned from pretraining, which leads to
reasoning ability improvement. We believe that OSFT offers an efficient and
promising alternative to more complex, reward-based training paradigms. Our
code is available at https://github.com/ElementQi/OnlineSFT.

</details>


### [174] [Search Self-play: Pushing the Frontier of Agent Capability without Supervision](https://arxiv.org/abs/2510.18821)
*Hongliang Lu,Yuhang Wen,Pengyu Cheng,Ruijin Ding,Haotian Xu,Jiaqi Guo,Chutian Wang,Haonan Chen,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 本文提出搜索自博弈（SSP）方法用于深度搜索代理的强化学习，在无监督下显著提升搜索代理性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习（RLVR）依赖大量人工，且已有任务合成方法难控制任务难度，影响可扩展性，因此探索更具扩展性的代理式RLVR方法。

Method: 采用自博弈训练，学习中的大语言模型同时作为任务提议者和问题解决者，提议者生成有准确答案和递增难度的深度搜索查询，解决者处理查询并预测答案，利用搜索结果进行检索增强生成以确保查询有准确答案。

Result: 在从零开始和持续强化学习训练设置下，SSP可在多个基准测试中显著且一致地提高搜索代理的性能。

Conclusion: 搜索自博弈能有效提升搜索代理性能，且代码已开源。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become the
mainstream technique for training LLM agents. However, RLVR highly depends on
well-crafted task queries and corresponding ground-truth answers to provide
accurate rewards, which requires massive human efforts and hinders the RL
scaling processes, especially under agentic scenarios. Although a few recent
works explore task synthesis methods, the difficulty of generated agentic tasks
can hardly be controlled to provide effective RL training advantages. To
achieve agentic RLVR with higher scalability, we explore self-play training for
deep search agents, in which the learning LLM utilizes multi-turn search engine
calling and acts simultaneously as both a task proposer and a problem solver.
The task proposer aims to generate deep search queries with well-defined
ground-truth answers and increasing task difficulty. The problem solver tries
to handle the generated search queries and output the correct answer
predictions. To ensure that each generated search query has accurate ground
truth, we collect all the searching results from the proposer's trajectory as
external knowledge, then conduct retrieval-augmentation generation (RAG) to
test whether the proposed query can be correctly answered with all necessary
search documents provided. In this search self-play (SSP) game, the proposer
and the solver co-evolve their agent capabilities through both competition and
cooperation. With substantial experimental results, we find that SSP can
significantly improve search agents' performance uniformly on various
benchmarks without any supervision under both from-scratch and continuous RL
training setups. The code is at https://github.com/Alibaba-Quark/SSP.

</details>


### [175] [BO4Mob: Bayesian Optimization Benchmarks for High-Dimensional Urban Mobility Problem](https://arxiv.org/abs/2510.18824)
*Seunghee Ryu,Donghoon Kwon,Seongjin Choi,Aryan Deshwal,Seungmo Kang,Carolina Osorio*

Main category: cs.LG

TL;DR: 介绍用于高维贝叶斯优化的基准框架BO4Mob，基于圣何塞道路网络设五个场景，评估五种优化方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决从有限交通传感器数据估计OD出行需求这一高维、计算昂贵的逆优化问题。

Method: 构建基于圣何塞道路网络的BO4Mob基准框架，包含五个场景，利用高分辨率交通模拟，评估三种先进BO算法和两种非BO基线。

Result: 可评估优化方法，支持可扩展优化算法开发及数据驱动城市移动性模型设计。

Conclusion: BO4Mob基准框架有实用性，能推动相关算法和模型发展，代码开源方便使用。

Abstract: We introduce \textbf{BO4Mob}, a new benchmark framework for high-dimensional
Bayesian Optimization (BO), driven by the challenge of origin-destination (OD)
travel demand estimation in large urban road networks. Estimating OD travel
demand from limited traffic sensor data is a difficult inverse optimization
problem, particularly in real-world, large-scale transportation networks. This
problem involves optimizing over high-dimensional continuous spaces where each
objective evaluation is computationally expensive, stochastic, and
non-differentiable. BO4Mob comprises five scenarios based on real-world San
Jose, CA road networks, with input dimensions scaling up to 10,100. These
scenarios utilize high-resolution, open-source traffic simulations that
incorporate realistic nonlinear and stochastic dynamics. We demonstrate the
benchmark's utility by evaluating five optimization methods: three
state-of-the-art BO algorithms and two non-BO baselines. This benchmark is
designed to support both the development of scalable optimization algorithms
and their application for the design of data-driven urban mobility models,
including high-resolution digital twins of metropolitan road networks. Code and
documentation are available at https://github.com/UMN-Choi-Lab/BO4Mob.

</details>


### [176] [Actor-Free Continuous Control via Structurally Maximizable Q-Functions](https://arxiv.org/abs/2510.18828)
*Yigit Korkmaz,Urvi Bhuwania,Ayush Jain,Erdem Bıyık*

Main category: cs.LG

TL;DR: 提出用于连续控制的纯基于价值的框架，在标准模拟任务上表现良好，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 传统基于价值的算法多用于离散动作空间，连续动作空间计算Q值不可行，而流行的演员-评论家方法训练不稳定。

Method: 提出纯基于价值的框架，重新审视Q函数的结构最大化，引入关键架构和算法选择。

Result: 在标准模拟任务上性能和样本效率与最先进基线相当，在受限动作空间环境中优于传统演员-评论家方法。

Conclusion: 所提无演员Q学习方法在连续控制任务中有效且稳定，无需学习单独的演员。

Abstract: Value-based algorithms are a cornerstone of off-policy reinforcement learning
due to their simplicity and training stability. However, their use has
traditionally been restricted to discrete action spaces, as they rely on
estimating Q-values for individual state-action pairs. In continuous action
spaces, evaluating the Q-value over the entire action space becomes
computationally infeasible. To address this, actor-critic methods are typically
employed, where a critic is trained on off-policy data to estimate Q-values,
and an actor is trained to maximize the critic's output. Despite their
popularity, these methods often suffer from instability during training. In
this work, we propose a purely value-based framework for continuous control
that revisits structural maximization of Q-functions, introducing a set of key
architectural and algorithmic choices to enable efficient and stable learning.
We evaluate the proposed actor-free Q-learning approach on a range of standard
simulation tasks, demonstrating performance and sample efficiency on par with
state-of-the-art baselines, without the cost of learning a separate actor.
Particularly, in environments with constrained action spaces, where the value
functions are typically non-smooth, our method with structural maximization
outperforms traditional actor-critic methods with gradient-based maximization.
We have released our code at https://github.com/USC-Lira/Q3C.

</details>


### [177] [A Hybrid Enumeration Framework for Optimal Counterfactual Generation in Post-Acute COVID-19 Heart Failure](https://arxiv.org/abs/2510.18841)
*Jingya Cheng,Alaleh Azhir,Jiazi Tian,Hossein Estiri*

Main category: cs.LG

TL;DR: 提出用于个性化风险估计和干预分析的反事实推理框架，以COVID - 19后遗症为例，结合正则预测建模与反事实搜索，应用于超2700人，性能良好并生成可解释反事实。


<details>
  <summary>Details</summary>
Motivation: 为个性化风险估计和干预分析提供数学框架，将因果推理与预测建模相结合。

Method: 整合正则预测建模与反事实搜索，结合精确枚举和基于优化的方法，如NICE和MOC算法。

Result: 模型在超2700人上取得强判别性能（AUROC: 0.88, 95% CI: 0.84 - 0.91），生成可解释、患者特定的反事实。

Conclusion: 反事实推理可形式化为预测函数的优化问题，为复杂生物医学系统的个性化推理提供严谨、可解释且计算高效的方法。

Abstract: Counterfactual inference provides a mathematical framework for reasoning
about hypothetical outcomes under alternative interventions, bridging causal
reasoning and predictive modeling. We present a counterfactual inference
framework for individualized risk estimation and intervention analysis,
illustrated through a clinical application to post-acute sequelae of COVID-19
(PASC) among patients with pre-existing heart failure (HF). Using longitudinal
diagnosis, laboratory, and medication data from a large health-system cohort,
we integrate regularized predictive modeling with counterfactual search to
identify actionable pathways to PASC-related HF hospital admissions. The
framework combines exact enumeration with optimization-based methods, including
the Nearest Instance Counterfactual Explanations (NICE) and Multi-Objective
Counterfactuals (MOC) algorithms, to efficiently explore high-dimensional
intervention spaces. Applied to more than 2700 individuals with confirmed
SARS-CoV-2 infection and prior HF, the model achieved strong discriminative
performance (AUROC: 0.88, 95% CI: 0.84-0.91) and generated interpretable,
patient-specific counterfactuals that quantify how modifying comorbidity
patterns or treatment factors could alter predicted outcomes. This work
demonstrates how counterfactual reasoning can be formalized as an optimization
problem over predictive functions, offering a rigorous, interpretable, and
computationally efficient approach to personalized inference in complex
biomedical systems.

</details>


### [178] [Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting](https://arxiv.org/abs/2510.18874)
*Howard Chen,Noam Razin,Karthik Narasimhan,Danqi Chen*

Main category: cs.LG

TL;DR: 论文系统比较SFT和RL两种后训练方法的遗忘模式，发现RL遗忘少且任务性能好，揭示其原因并指出用近似策略数据缓解遗忘的潜力。


<details>
  <summary>Details</summary>
Motivation: 识别缓解语言模型后训练中灾难性遗忘现象的准则。

Method: 系统比较SFT和RL两种后训练方法的遗忘模式，用混合分布模型分析差异原因并验证。

Result: RL比SFT遗忘少，能达到相当或更高的目标任务性能，原因是其基于策略数据的模式寻求特性。

Conclusion: 使用近似策略数据有缓解遗忘的潜力，且获取效率更高。

Abstract: Adapting language models (LMs) to new tasks via post-training carries the
risk of degrading existing capabilities -- a phenomenon classically known as
catastrophic forgetting. In this paper, toward identifying guidelines for
mitigating this phenomenon, we systematically compare the forgetting patterns
of two widely adopted post-training methods: supervised fine-tuning (SFT) and
reinforcement learning (RL). Our experiments reveal a consistent trend across
LM families (Llama, Qwen) and tasks (instruction following, general knowledge,
and arithmetic reasoning): RL leads to less forgetting than SFT while achieving
comparable or higher target task performance. To investigate the cause for this
difference, we consider a simplified setting in which the LM is modeled as a
mixture of two distributions, one corresponding to prior knowledge and the
other to the target task. We identify that the mode-seeking nature of RL, which
stems from its use of on-policy data, enables keeping prior knowledge intact
when learning the target task. We then verify this insight by demonstrating
that the use on-policy data underlies the robustness of RL to forgetting in
practical settings, as opposed to other algorithmic choices such as the KL
regularization or advantage estimation. Lastly, as a practical implication, our
results highlight the potential of mitigating forgetting using approximately
on-policy data, which can be substantially more efficient to obtain than fully
on-policy data.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [179] [Neural networks for neurocomputing circuits: a computational study of tolerance to noise and activation function non-uniformity when machine learning materials properties](https://arxiv.org/abs/2510.17849)
*Ye min Thant,Methawee Nukunudompanich,Chu-Chen Chueh,Manabu Ihara,Sergei Manzhos*

Main category: cs.NE

TL;DR: 研究模拟神经计算电路中电路噪声和神经元激活函数不均匀性对神经网络的影响，以材料信息学为例，发现网络噪声容忍度低，特定网络更耐噪，激活函数不均匀性可通过重新训练缓解。


<details>
  <summary>Details</summary>
Motivation: 专用模拟神经计算电路在机器学习应用中有优势，但神经网络需应对电路噪声和神经元激活函数不均匀性问题，故研究其影响。

Method: 进行计算研究，以材料信息学中的几个问题作为代表问题，分析不同神经网络架构和训练机制下的情况。

Result: 神经网络总体噪声容忍度低，单隐藏层和大于最优尺寸的网络更耐噪，欠拟合的模型更耐噪，激活函数不均匀性可通过重新训练缓解。

Conclusion: 明确了电路噪声和神经元激活函数不均匀性对神经网络的影响，以及相应的缓解方法。

Abstract: Dedicated analog neurocomputing circuits are promising for high-throughput,
low power consumption applications of machine learning (ML) and for
applications where implementing a digital computer is unwieldy (remote
locations; small, mobile, and autonomous devices, extreme conditions, etc.).
Neural networks (NN) implemented in such circuits, however, must contend with
circuit noise and the non-uniform shapes of the neuron activation function
(NAF) due to the dispersion of performance characteristics of circuit elements
(such as transistors or diodes implementing the neurons). We present a
computational study of the impact of circuit noise and NAF inhomogeneity in
function of NN architecture and training regimes. We focus on one application
that requires high-throughput ML: materials informatics, using as
representative problem ML of formation energies vs. lowest-energy isomer of
peri-condensed hydrocarbons, formation energies and band gaps of double
perovskites, and zero point vibrational energies of molecules from QM9 dataset.
We show that NNs generally possess low noise tolerance with the model accuracy
rapidly degrading with noise level. Single-hidden layer NNs, and NNs with
larger-than-optimal sizes are somewhat more noise-tolerant. Models that show
less overfitting (not necessarily the lowest test set error) are more
noise-tolerant. Importantly, we demonstrate that the effect of activation
function inhomogeneity can be palliated by retraining the NN using practically
realized shapes of NAFs.

</details>


### [180] [A Survey of Recursive and Recurrent Neural Networks](https://arxiv.org/abs/2510.17867)
*Jian-wei Liu,Bing-rong Xu,Zhi-yan Song*

Main category: cs.NE

TL;DR: 本文按网络结构、训练目标函数和学习算法实现对递归和循环神经网络分支详细分类，介绍模型原理、结构、研究进展与应用并进行展望总结。


<details>
  <summary>Details</summary>
Motivation: 对递归和循环神经网络进行系统分类，以更好地理解其发展和应用，解决复杂序列、语音和图像问题。

Method: 根据网络结构、训练目标函数和学习算法实现对递归和循环神经网络进行分类。

Result: 将递归和循环神经网络大致分为三类，各类网络相互交叉依赖，解决了许多复杂问题。

Conclusion: 对递归和循环神经网络模型进行了展望和总结。

Abstract: In this paper, the branches of recursive and recurrent neural networks are
classified in detail according to the network structure, training objective
function and learning algorithm implementation. They are roughly divided into
three categories: The first category is General Recursive and Recurrent Neural
Networks, including Basic Recursive and Recurrent Neural Networks, Long Short
Term Memory Recursive and Recurrent Neural Networks, Convolutional Recursive
and Recurrent Neural Networks, Differential Recursive and Recurrent Neural
Networks, One-Layer Recursive and Recurrent Neural Networks, High-Order
Recursive and Recurrent Neural Networks, Highway Networks, Multidimensional
Recursive and Recurrent Neural Networks, Bidirectional Recursive and Recurrent
Neural Networks; the second category is Structured Recursive and Recurrent
Neural Networks, including Grid Recursive and Recurrent Neural Networks, Graph
Recursive and Recurrent Neural Networks, Temporal Recursive and Recurrent
Neural Networks, Lattice Recursive and Recurrent Neural Networks, Hierarchical
Recursive and Recurrent Neural Networks, Tree Recursive and Recurrent Neural
Networks; the third category is Other Recursive and Recurrent Neural Networks,
including Array Long Short Term Memory, Nested and Stacked Recursive and
Recurrent Neural Networks, Memory Recursive and Recurrent Neural Networks.
Various networks cross each other and even rely on each other to form a complex
network of relationships. In the context of the development and convergence of
various networks, many complex sequence, speech and image problems are solved.
After a detailed description of the principle and structure of the above model
and model deformation, the research progress and application of each model are
described, and finally the recursive and recurrent neural network models are
prospected and summarized.

</details>


### [181] [Decoding Listeners Identity: Person Identification from EEG Signals Using a Lightweight Spiking Transformer](https://arxiv.org/abs/2510.17879)
*Zheyuan Lin,Siqi Cai,Haizhou Li*

Main category: cs.NE

TL;DR: 提出用带轻量级脉冲变压器的脉冲神经网络（SNNs）进行EEG人员识别，在数据集上准确率达100%且能耗低。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的EEG人员识别技术计算成本高，限制应用范围。

Method: 提出使用带轻量级脉冲变压器的脉冲神经网络（SNNs）的EEG人员识别方法，该模型能处理EEG信号的时间复杂性。

Result: 在EEG - Music Emotion Recognition Challenge数据集上，模型分类准确率达100%，能耗不到传统深度神经网络的10%。

Conclusion: 本研究为节能高效的脑机接口提供了有前景的方向。

Abstract: EEG-based person identification enables applications in security,
personalized brain-computer interfaces (BCIs), and cognitive monitoring.
However, existing techniques often rely on deep learning architectures at high
computational cost, limiting their scope of applications. In this study, we
propose a novel EEG person identification approach using spiking neural
networks (SNNs) with a lightweight spiking transformer for efficiency and
effectiveness. The proposed SNN model is capable of handling the temporal
complexities inherent in EEG signals. On the EEG-Music Emotion Recognition
Challenge dataset, the proposed model achieves 100% classification accuracy
with less than 10% energy consumption of traditional deep neural networks. This
study offers a promising direction for energy-efficient and high-performance
BCIs. The source code is available at
https://github.com/PatrickZLin/Decode-ListenerIdentity.

</details>


### [182] [Self-Evidencing Through Hierarchical Gradient Decomposition: A Dissipative System That Maintains Non-Equilibrium Steady-State by Minimizing Variational Free Energy](https://arxiv.org/abs/2510.17916)
*Michael James McCulloch*

Main category: cs.NE

TL;DR: 提出通过精确局部信用分配实现自由能原理，证明相关机制精确性并验证其有多种涌现能力，统一多种理论。


<details>
  <summary>Details</summary>
Motivation: 自由能原理从原理到可实现算法的路径不清晰，需给出实现方法。

Method: 系统分层分解梯度计算，包括通过反馈对齐分配空间信用、通过资格迹分配时间信用、通过营养场图（TFM）分配结构信用。

Result: TFM与神谕梯度的皮尔逊相关系数达0.9693，有任务干扰后保留率98.6%、从75%结构损伤中自主恢复等能力。

Conclusion: 该架构统一多种理论，证明可通过局部、生物合理规则实现对网络拓扑的精确分层推理。

Abstract: The Free Energy Principle (FEP) states that self-organizing systems must
minimize variational free energy to persist, but the path from principle to
implementable algorithm has remained unclear. We present a constructive proof
that the FEP can be realized through exact local credit assignment. The system
decomposes gradient computation hierarchically: spatial credit via feedback
alignment, temporal credit via eligibility traces, and structural credit via a
Trophic Field Map (TFM) that estimates expected gradient magnitude for each
connection block. We prove these mechanisms are exact at their respective
levels and validate the central claim empirically: the TFM achieves 0.9693
Pearson correlation with oracle gradients. This exactness produces emergent
capabilities including 98.6% retention after task interference, autonomous
recovery from 75% structural damage, self-organized criticality (spectral
radius p ~= 1.0$), and sample-efficient reinforcement learning on continuous
control tasks without replay buffers. The architecture unifies Prigogine's
dissipative structures, Friston's free energy minimization, and Hopfield's
attractor dynamics, demonstrating that exact hierarchical inference over
network topology can be implemented with local, biologically plausible rules.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [183] [Metrics and evaluations for computational and sustainable AI efficiency](https://arxiv.org/abs/2510.17885)
*Hongyuan Liu,Xinyang Liu,Guosheng Hu*

Main category: cs.PF

TL;DR: 提出统一可复现的AI模型推理方法，整合计算与环境指标，应用于多平台模型，建立基准框架并开源代码。


<details>
  <summary>Details</summary>
Motivation: 当前评估AI模型性能、效率和环境影响的方法零散，难以提供整体视图和跨系统比较优化。

Method: 提出统一可复现的AI模型推理方法论，系统测量延迟、吞吐量、能耗和碳排放，在保持精度约束下进行评估。

Result: 将方法应用于多平台多精度模型，建立严格基准框架，生成决策可用的帕累托前沿。

Conclusion: 开源代码便于独立验证和采用，助力研究人员和从业者为可持续AI部署做决策。

Abstract: The rapid advancement of Artificial Intelligence (AI) has created
unprecedented demands for computational power, yet methods for evaluating the
performance, efficiency, and environmental impact of deployed models remain
fragmented. Current approaches often fail to provide a holistic view, making it
difficult to compare and optimise systems across heterogeneous hardware,
software stacks, and numeric precisions. To address this gap, we propose a
unified and reproducible methodology for AI model inference that integrates
computational and environmental metrics under realistic serving conditions. Our
framework provides a pragmatic, carbon-aware evaluation by systematically
measuring latency and throughput distributions, energy consumption, and
location-adjusted carbon emissions, all while maintaining matched accuracy
constraints for valid comparisons. We apply this methodology to multi-precision
models across diverse hardware platforms, from data-centre accelerators like
the GH200 to consumer-level GPUs such as the RTX 4090, running on mainstream
software stacks including PyTorch, TensorRT, and ONNX Runtime. By
systematically categorising these factors, our work establishes a rigorous
benchmarking framework that produces decision-ready Pareto frontiers,
clarifying the trade-offs between accuracy, latency, energy, and carbon. The
accompanying open-source code enables independent verification and facilitates
adoption, empowering researchers and practitioners to make evidence-based
decisions for sustainable AI deployment.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [184] [AI Exchange Platforms](https://arxiv.org/abs/2510.17839)
*Johannes Schneider,Rene Abraham*

Main category: cs.SE

TL;DR: 本文提出AI交换平台分类法，分析其维度与特征及交互模式，对从业者和学者有价值，为理解AI模型交换领域提供资源。


<details>
  <summary>Details</summary>
Motivation: 随着AI融入组织技术框架及基础模型出现，结构化AI模型交换平台很重要，但缺乏综合分类框架。

Method: 提出分类法对AI交换平台进行结构化分类，研究关键维度、特征及交互模式。

Result: 发现部分平台利用同行评审进行质量控制，提供模型在线测试、部署和定制机制。

Conclusion: 分类法对从业者和学者有帮助，凸显平台设计中适应性和创新的重要性，为理解AI模型交换提供资源并指向未来发展。

Abstract: The rapid integration of Artificial Intelligence (AI) into organizational
technology frameworks has transformed how organizations engage with AI-driven
models, influencing both operational performance and strategic innovation. With
the advent of foundation models, the importance of structured platforms for AI
model exchange has become paramount for organizational efficacy and
adaptability. However, a comprehensive framework to categorize and understand
these platforms remains underexplored. To address this gap, our taxonomy
provides a structured approach to categorize AI exchange platforms, examining
key dimensions and characteristics, as well as revealing interesting
interaction patterns between public research institutions and organizations:
Some platforms leverage peer review as a mechanism for quality control, and
provide mechanisms for online testing, deploying, and customization of models.
Our paper is beneficial to practitioners seeking to understand challenges and
opportunities that arise from AI exchange platforms. For academics, the
taxonomy serves as a foundation for further research into the evolution,
impact, and best practices associated with AI model sharing and utilization in
different contexts. Additionally, our study provides insights into the evolving
role of AI in various industries, highlighting the importance of adaptability
and innovation in platform design. This paper serves as a critical resource for
understanding the dynamic interplay between technology, business models, and
user engagement in the rapidly growing domain of AI model exchanges pointing
also towards possible future evolution.

</details>


### [185] [Vibe Coding: Toward an AI-Native Paradigm for Semantic and Intent-Driven Programming](https://arxiv.org/abs/2510.17842)
*Vinay Bamil*

Main category: cs.SE

TL;DR: 本文介绍新兴的AI原生编程范式vibe coding，给出定义、架构，与其他编程方式对比，探讨影响、挑战并指明未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，开发者可与AI对话生成软件，引入vibe coding这一编程范式。

Method: 形式化定义vibe coding，提出参考架构，描述假设实现，与其他编程方式对比。

Result: 分析了vibe coding的影响，提及生产力提升、民主化效应，也指出漏洞、潜在放缓等问题。

Conclusion: 明确了vibe coding面临的关键挑战，如对齐、可重复性等，并给出未来研究方向和开放问题。

Abstract: Recent advances in large language models have enabled developers to generate
software by conversing with artificial intelligence systems rather than writing
code directly. This paper introduces vibe coding, an emerging AI-native
programming paradigm in which a developer specifies high-level functional
intent along with qualitative descriptors of the desired "vibe" (tone, style,
or emotional resonance). An intelligent agent then transforms those
specifications into executable software. We formalize the definition of vibe
coding and propose a reference architecture that includes an intent parser, a
semantic embedding engine, an agentic code generator, and an interactive
feedback loop. A hypothetical implementation is described. We compare vibe
coding with declarative, functional, and prompt-based programming, and we
discuss its implications for software engineering, human-AI collaboration, and
responsible AI practice. Finally, we examine reported productivity gains and
democratizing effects, review recent studies that highlight vulnerabilities and
potential slowdowns, identify key challenges such as alignment,
reproducibility, bias, explainability, maintainability, and security, and
outline future directions and open research questions.

</details>


### [186] [Smart Contracts Formal Verification: A Systematic Literature Review](https://arxiv.org/abs/2510.17865)
*Rene Davila,Everardo Barcenas,Rocio Aldeco-Perez*

Main category: cs.SE

TL;DR: 本文聚焦智能合约形式验证，研究相关文献后提出基于描述逻辑的形式验证方法。


<details>
  <summary>Details</summary>
Motivation: 智能合约作为软件模型，运行或规范常存在显著错误，需研究形式验证。

Method: 研究各来源发表的相关文献，包括规范、验证工具和实验，提出基于描述逻辑的形式验证。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: Formal verification entails testing software to ensure it operates as
specified. Smart contracts are self-executing contracts with the terms of the
agreement directly written into lines of code. They run on blockchain platforms
and automatically enforce and execute the terms of an agreement when meeting
predefined conditions. However, Smart Contracts, as software models, often
contain notable errors in their operation or specifications. This observation
prompts us to conduct a focused study examining related works published across
various sources. These publications detail specifications, verification tools,
and relevant experiments. Subsequently, this survey proposes an alternative
formal verification based on description logic.

</details>


### [187] [UniCode: A Framework for Generating High Quality Competitive Coding Problems](https://arxiv.org/abs/2510.17868)
*Xinyue Zheng,Haowei Lin,Shaofei Cai,Zilong Zheng,Yitao Liang*

Main category: cs.SE

TL;DR: 提出UniCode框架自动生成算法问题与测试用例，通过实验证明其有效性，为编码领域提供可扩展可靠的动态评估数据集生成方案。


<details>
  <summary>Details</summary>
Motivation: 解决竞争编码基准依赖静态、人工编写问题带来的数据污染和可扩展性有限的问题。

Method: 利用大语言模型，通过单问题扩展、同类型融合和跨类型融合三种策略使问题多样化；采用压力驱动的测试用例合成管道生成测试套件。

Result: 创建492个问题的基准并评估19个先进大语言模型，UniCode具有高挑战性和区分度，表现最好的模型o4 - mini通过率仅70.3%。

Conclusion: 框架为编码领域生成动态评估数据集提供了可扩展且可靠的解决方案。

Abstract: The reliance of competitive coding benchmarks on static, human-authored
problems creates significant challenges, including data contamination and
limited scalability. To address these issues, we introduce UniCode, a novel
framework that automatically generates high-quality algorithmic problems
alongside robust, contamination-resistant test cases. Inspired by biological
evolution that creates better and diverse offspring, our framework leverages
Large Language Models (LLMs) to systematically diversify problems through three
strategies: single problem extension, same-type fusion, and cross-type fusion.
A key innovation is our stress-driven test case synthesis pipeline, which
generates reliable test suites without requiring a canonical ground-truth
solution. This pipeline combines brute-force grounding for small-scale inputs
with a consensus-based validation mechanism for large-scale inputs to ensure
high correctness and coverage. We demonstrate effectiveness of our framework by
curating a benchmark of 492 problems and evaluating 19 state-of-the-art LLMs.
The results reveal that UniCode is highly challenging and discriminative, with
the top-performing model, o4-mini, achieving a pass rate of only 70.3%. Our
framework provides a scalable and reliable solution for generating dynamic
evaluation datasets in coding domain.

</details>


### [188] [Repairing Tool Calls Using Post-tool Execution Reflection and RAG](https://arxiv.org/abs/2510.17874)
*Jason Tsay,Zidane Wright,Gaodan Fang,Kiran Kate,Saurabh Jha,Yara Rizk*

Main category: cs.SE

TL;DR: 本文提出基于RAG的执行后反思组件修复工具调用错误，以kubectl为例，研究表明该方法能提升命令执行成功率和回答准确率，故障排除文档效果更好。


<details>
  <summary>Details</summary>
Motivation: 工具调用常因语法和语义原因失败，部分语义错误需分析响应后解决，因此要开发修复错误的组件。

Method: 开发结合大语言模型反思与特定领域检索增强生成（RAG）的执行后反思组件，聚焦kubectl工具管理Kubernetes的用例。

Result: RAG反思使55%的评估模型命令执行成功率提升，平均能使正确回答用户查询的可能性提高36%，故障排除文档比官方文档平均提升10%的通过率。

Conclusion: 基于RAG的反思组件能有效修复工具调用错误，提高命令执行成功率和回答准确率，故障排除文档更有助于提升通过率。

Abstract: Agentic systems interact with external systems by calling tools such as
Python functions, REST API endpoints, or command line tools such as kubectl in
Kubernetes. These tool calls often fail for various syntactic and semantic
reasons. Some less obvious semantic errors can only be identified and resolved
after analyzing the tool's response. To repair these errors, we develop a
post-tool execution reflection component that combines large language model
(LLM)-based reflection with domain-specific retrieval-augmented generation
(RAG) using documents describing both the specific tool being called and
troubleshooting documents related to the tool. For this paper, we focus on the
use case of the kubectl command line tool to manage Kubernetes, a platform for
orchestrating cluster applications. Through a larger empirical study and a
smaller manual evaluation, we find that our RAG-based reflection will repair
kubectl commands such that they are both more likely to successfully execute
(pass rate) for 55% of our models evaluated and 36% more likely to correctly
answer the user query on average. We find that troubleshooting documents
improve pass rate compared to official documentation by an average of 10%.

</details>


### [189] [TritonRL: Training LLMs to Think and Code Triton Without Cheating](https://arxiv.org/abs/2510.17891)
*Jiin Woo,Shaowei Zhu,Allen Nie,Zhen Jia,Yida Wang,Youngsuk Park*

Main category: cs.SE

TL;DR: 介绍TritonRL用于Triton内核生成，通过新训练框架解决内核生成挑战，实验显示其性能达SOTA。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，需要自动化、高性能系统内核加速开发部署，Triton内核生成面临数据稀缺等挑战。

Method: 通过在精选数据集上监督微调提炼Triton特定知识，用强化学习配合可验证奖励和分层奖励分配提高代码质量，框架可检测奖励篡改。

Result: 在KernelBench上实验，TritonRL达到了最先进的正确性和加速比，超越其他Triton特定模型。

Conclusion: 基于强化学习的训练范式有效。

Abstract: With the rapid evolution of large language models (LLMs), the demand for
automated, high-performance system kernels has emerged as a key enabler for
accelerating development and deployment. We introduce TritonRL, a
domain-specialized LLM for Triton kernel generation, trained with a novel
training framework that enables robust and automated kernel synthesis. Unlike
general-purpose programming languages, Triton kernel generation faces unique
challenges due to data scarcity and incomplete evaluation criteria, vulnerable
to reward hacking. Our approach addresses these challenges end-to-end by
distilling Triton-specific knowledge through supervised fine-tuning on curated
datasets, and further improving code quality via reinforcement learning (RL)
with robust, verifiable rewards and hierarchical reward assignment. Our RL
framework robustly detects reward hacking and guides both reasoning traces and
code tokens through fine-grained verification and hierarchical reward
decomposition, enabling the model to generate high-quality Triton kernels that
can truly replace existing modules. With robust and fine-grained evaluation,
our experiments on KernelBench demonstrate that TritonRL achieves
state-of-the-art correctness and speedup, surpassing all other Triton-specific
models and underscoring the effectiveness of our RL-based training paradigm.

</details>


### [190] [A Systematic Literature Review of the Use of GenAI Assistants for Code Comprehension: Implications for Computing Education Research and Practice](https://arxiv.org/abs/2510.17894)
*Yunhan Qiao,Md Istiak Hossain Shihab,Christopher Hundhausen*

Main category: cs.SE

TL;DR: 文章通过对2022 - 2024年31项研究的系统文献综述，探讨GenAI助力代码理解的现状、问题并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为计算机教育工作者提供基于证据的GenAI辅助代码理解的使用指导，确定未来研究方向。

Method: 开展系统文献综述，聚焦2022 - 2024年31项研究。

Result: GenAI助手解释常不准确或不清晰，新手程序员难写有效提示，阻碍代码理解。对基于GenAI的方法和工具分类，总结研究方法和有效性评估。

Conclusion: 考虑研究结果对计算机教育研究和实践的影响，确定未来研究方向。

Abstract: The ability to comprehend code has long been recognized as an essential skill
in software engineering. As programmers lean more heavily on generative
artificial intelligence (GenAI) assistants to develop code solutions, it is
becoming increasingly important for programmers to comprehend GenAI solutions
so that they can verify their appropriateness and properly integrate them into
existing code. At the same time, GenAI tools are increasingly being enlisted to
provide programmers with tailored explanations of code written both by GenAI
and humans. Thus, in computing education, GenAI presents new challenges and
opportunities for learners who are trying to comprehend computer programs. To
provide computing educators with evidence-based guidance on the use of GenAI to
facilitate code comprehension and to identify directions for future research,
we present a systematic literature review (SLR) of state-of-the-art approaches
and tools that leverage GenAI to enhance code comprehension. Our SLR focuses on
31 studies published between 2022 and 2024. Despite their potential, GenAI
assistants often yield inaccurate or unclear explanations, and novice
programmers frequently struggle to craft effective prompts, thereby impeding
their ability to leverage GenAI to aid code comprehension. Our review
classifies GenAI-based approaches and tools, identifies methods used to study
them, and summarizes the empirical evaluations of their effectiveness. We
consider the implications of our findings for computing education research and
practice, and identify directions for future research.

</details>


### [191] [SpecAgent: A Speculative Retrieval and Forecasting Agent for Code Completion](https://arxiv.org/abs/2510.17925)
*George Ma,Anurag Koul,Qi Chen,Yawen Wu,Sachit Kuhar,Yu Yu,Aritra Sengupta,Varun Kumar,Murali Krishna Ramanathan*

Main category: cs.SE

TL;DR: 提出SpecAgent解决大语言模型在现实软件仓库中代码生成问题，构建无泄漏基准测试，实验显示其有性能提升和降低推理延迟效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现实软件仓库中因项目特定API和跨文件依赖表现不佳，检索增强方法受推理时间延迟预算限制。

Method: 提出SpecAgent，在索引时主动探索仓库文件构建推测性上下文；构建无泄漏的合成基准测试。

Result: SpecAgent相比最佳基线模型有9 - 11%的绝对提升（相对提升48 - 58%），并显著降低推理延迟。

Conclusion: SpecAgent能有效提高代码生成质量并降低推理延迟，无泄漏基准测试可更真实评估模型。

Abstract: Large Language Models (LLMs) excel at code-related tasks but often struggle
in realistic software repositories, where project-specific APIs and cross-file
dependencies are crucial. Retrieval-augmented methods mitigate this by
injecting repository context at inference time. The low inference-time latency
budget affects either retrieval quality or the added latency adversely impacts
user experience. We address this limitation with SpecAgent, an agent that
improves both latency and code-generation quality by proactively exploring
repository files during indexing and constructing speculative context that
anticipates future edits in each file. This indexing-time asynchrony allows
thorough context computation, masking latency, and the speculative nature of
the context improves code-generation quality. Additionally, we identify the
problem of future context leakage in existing benchmarks, which can inflate
reported performance. To address this, we construct a synthetic, leakage-free
benchmark that enables a more realistic evaluation of our agent against
baselines. Experiments show that SpecAgent consistently achieves absolute gains
of 9-11% (48-58% relative) compared to the best-performing baselines, while
significantly reducing inference latency.

</details>


### [192] [From Charts to Code: A Hierarchical Benchmark for Multimodal Models](https://arxiv.org/abs/2510.17932)
*Jiahao Tang,Henry Hengyuan Zhao,Lijian Wu,Yifei Tao,Dongxing Mao,Yang Wan,Jingru Tan,Min Zeng,Min Li,Alex Jinpeng Wang*

Main category: cs.SE

TL;DR: 介绍新基准Chart2Code评估大跨模态模型图表理解和代码生成能力，含三级任务，对25个模型进行测试，结果显示任务有难度，有望推动模型发展。


<details>
  <summary>Details</summary>
Motivation: 设计新基准评估大跨模态模型图表理解和代码生成能力，反映实际图表转代码使用场景并系统提升任务复杂度。

Method: 构建含三个级别、22种图表类型、2023个任务的Chart2Code基准，采用多级评估指标，对25个先进大跨模态模型进行测试。

Result: 即使是最先进的GPT - 5在编辑任务的代码评估平均得分0.57，图表质量评估平均得分0.22，说明Chart2Code任务难度大。

Conclusion: 该基准有望推动跨模态推理进步，促进更强大通用的大跨模态模型发展。

Abstract: We introduce Chart2Code, a new benchmark for evaluating the chart
understanding and code generation capabilities of large multimodal models
(LMMs). Chart2Code is explicitly designed from a user-driven perspective,
capturing diverse real-world scenarios and progressively increasing task
difficulty. It consists of three levels: Level 1 (Chart Reproduction)
reproduces charts from a reference figure and user query; Level 2 (Chart
Editing) involves complex modifications such as changing chart types or adding
elements; and Level 3 (Long-Table to Chart Generation) requires models to
transform long, information-dense tables into faithful charts following user
instructions. To our knowledge, this is the first hierarchical benchmark that
reflects practical chart2code usage while systematically scaling task
complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,
paired with multi-level evaluation metrics that assess both code correctness
and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art
(SoTA) LMMs, including both proprietary and the latest open-source models such
as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental
results demonstrate that even the SoTA model GPT-5 averages only 0.57 on
code-based evaluation and 0.22 on chart-quality assessment across the editing
tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark
will drive advances in multimodal reasoning and foster the development of more
robust and general-purpose LMMs. Our code and data are available on Chart2Code.

</details>


### [193] [JunoBench: A Benchmark Dataset of Crashes in Python Machine Learning Jupyter Notebooks](https://arxiv.org/abs/2510.18013)
*Yiran Wang,José Antonio Hernández López,Ulf Nilsson,Dániel Varró*

Main category: cs.SE

TL;DR: 介绍首个基于Python的机器学习笔记本真实崩溃基准数据集JunoBench，含111个可复现崩溃及修复，支持复现且便于使用，利于笔记本式机器学习开发的调试。


<details>
  <summary>Details</summary>
Motivation: Jupyter笔记本用于机器学习原型开发广泛，但针对笔记本中机器学习代码的调试工具少，缺乏相关基准。

Method: 从公共Kaggle笔记本中精心挑选111个可复现的崩溃案例，每个案例配有可验证的修复方案，构建JunoBench，并提供统一执行环境。

Result: 创建了JunoBench基准数据集，能在统一环境中可靠复现崩溃和修复。

Conclusion: JunoBench提供真实崩溃及解决方案，有助于针对笔记本式机器学习开发的交互和迭代特性进行错误检测、定位和修复。

Abstract: Jupyter notebooks are widely used for machine learning (ML) prototyping. Yet
few debugging tools are designed for ML code in notebooks, potentially due to
the lack of benchmarks. We introduce JunoBench, the first benchmark dataset of
real-world crashes in Python-based ML notebooks. JunoBench has 111 curated and
reproducible crashes from public Kaggle notebooks, each paired with a
verifiable fix, ranging over popular ML libraries, including TensorFlow/Keras,
PyTorch, Scikit-learn, Pandas, and NumPy, as well as notebook-specific
out-of-order execution issue. To support reproducibility and ease of use,
JunoBench offers a unified execution environment where crashes and fixes can be
reliably reproduced. By providing realistic crashes and their resolutions,
JunoBench facilitates bug detection, localization, and repair tailored to the
interactive and iterative nature of notebook-based ML development.

</details>


### [194] [DIP-AI: A Discovery Framework for AI Innovation Projects](https://arxiv.org/abs/2510.18017)
*Mariana Crisostomo Martins,Lucas Elias Cardoso Rocha,Lucas Cordeiro Romao,Taciana Novo Kudo,Marcos Kalinowski,Renato de Freitas Bulcao-Neto*

Main category: cs.SE

TL;DR: 现有AI系统发展下RE活动有挑战，提出DIP - AI框架并评估，结果显示其对AI项目问题发现有帮助。


<details>
  <summary>Details</summary>
Motivation: 解决AI创新项目中问题发现缺乏支持的问题。

Method: 结合ISO 12207、5338和设计思维，通过文献综述提出DIP - AI框架，并在产学研合作案例中进行评估。

Result: DIP - AI具有相关性和实用性，尤其有助于AI项目的问题发现。

Conclusion: 为学术界提供AI问题发现框架，为工业界探讨该框架在实际产学研项目中的应用。

Abstract: Despite the increasing development of Artificial Intelligence (AI) systems,
Requirements Engineering (RE) activities face challenges in this new
data-intensive paradigm. We identified a lack of support for problem discovery
within AI innovation projects. To address this, we propose and evaluate DIP-AI,
a discovery framework tailored to guide early-stage exploration in such
initiatives. Based on a literature review, our solution proposal combines
elements of ISO 12207, 5338, and Design Thinking to support the discovery of AI
innovation projects, aiming at promoting higher quality deliveries and
stakeholder satisfaction. We evaluated DIP-AI in an industry-academia
collaboration (IAC) case study of an AI innovation project, in which
participants applied DIP-AI to the discovery phase in practice and provided
their perceptions about the approach's problem discovery capability,
acceptance, and suggestions. The results indicate that DIP-AI is relevant and
useful, particularly in facilitating problem discovery in AI projects. This
research contributes to academia by sharing DIP-AI as a framework for AI
problem discovery. For industry, we discuss the use of this framework in a real
IAC program that develops AI innovation projects.

</details>


### [195] [A Benchmark Dataset And LLMs Comparison For NFR Classification With Explainable AI](https://arxiv.org/abs/2510.18096)
*Esrat Ebtida Sakib,MD Ahnaf Akib,Md Muktadir Mazumder,Maliha Noushin Raida,Md. Mohsinul Kabir*

Main category: cs.SE

TL;DR: 文章收集NFRs数据增强现有数据集，用大语言模型分类NFRs并对比模型表现，Gemma - 2和Phi - 3表现较好。


<details>
  <summary>Details</summary>
Motivation: 手动识别和分类NFRs耗时且易出错，需要自动化解决方案，而构建强大全面的数据集是前提。

Method: 从项目章程和开源软件文档收集NFRs增强现有数据集，用大语言模型分类NFRs，用多种评估指标对比多个大语言模型的分类结果。

Result: Gemma - 2效果最佳，Precision 0.87、Recall 0.89、F1 - score 0.88、lime hit score 78/80；Phi - 3紧随其后，Precision 0.85、Recall 0.87、F1 - score 0.86、lime hit score 79。

Conclusion: 通过改进上下文基础，这种整合提升了模型对技术方面和用户需求的理解。

Abstract: Non-Functional Requirements (NFRs) play a critical role in determining the
overall quality and user satisfaction of software systems. Accurately
identifying and classifying NFRs is essential to ensure that software meets
performance, usability, and reliability expectations. However, manual
identification of NFRs from documentation is time-consuming and prone to
errors, necessitating automated solutions. Before implementing any automated
solution, a robust and comprehensive dataset is essential. To build such a
dataset, we collected NFRs from various Project Charters and Open Source
Software Documentation. This enhanced the technical depth and usability of an
already existing NFR dataset. We categorized NFRs into sub-classes and
identified needs using widely used Large Language Models to facilitate
automation. After classifying the NFRs, we compared the classification results
of the selected LLMs: RoBERTa, CodeBERT, Gemma-2, Phi-3, Mistral-8B, and
Llama-3.1-8B using various evaluation metrics, including precision, recall,
F1-score, and lime scores. Among these models, Gemma-2 achieved the best
results with a precision of 0.87, recall of 0.89, and F1-score of 0.88,
alongside a lime hit score of 78 out of 80. Phi-3 closely followed with a
precision of 0.85, recall of 0.87, F1-score of 0.86, and the highest lime hit
score of 79. By improving the contextual foundation, this integration enhanced
the model's comprehension of technical aspects and user requirements.

</details>


### [196] [BlueCodeAgent: A Blue Teaming Agent Enabled by Automated Red Teaming for CodeGen AI](https://arxiv.org/abs/2510.18131)
*Chengquan Guo,Yuzhou Nie,Chulin Xie,Zinan Lin,Wenbo Guo,Bo Li*

Main category: cs.SE

TL;DR: 提出 BlueCodeAgent 端到端蓝队代理，结合红队生成风险实例，在三个代码相关任务中表现优于基线模型和基于安全提示的防御，平均 F1 分数提升 12.7%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于代码生成时安全风险受关注，早期研究侧重红队，蓝队进展有限，需有效语义理解区分安全与不安全代码。

Method: 提出 BlueCodeAgent 框架，集成红队和蓝队，红队生成风险实例，蓝队利用其进行宪法和代码分析以检测风险。

Result: 在三个代码相关任务的四个数据集上平均 F1 分数提升 12.7%，在脆弱代码检测任务中结合动态分析有效减少误报。

Conclusion: 红队通过持续识别新漏洞有助于提升蓝队防御性能。

Abstract: As large language models (LLMs) are increasingly used for code generation,
concerns over the security risks have grown substantially. Early research has
primarily focused on red teaming, which aims to uncover and evaluate
vulnerabilities and risks of CodeGen models. However, progress on the blue
teaming side remains limited, as developing defense requires effective semantic
understanding to differentiate the unsafe from the safe. To fill in this gap,
we propose BlueCodeAgent, an end-to-end blue teaming agent enabled by automated
red teaming. Our framework integrates both sides: red teaming generates diverse
risky instances, while the blue teaming agent leverages these to detect
previously seen and unseen risk scenarios through constitution and code
analysis with agentic integration for multi-level defense. Our evaluation
across three representative code-related tasks--bias instruction detection,
malicious instruction detection, and vulnerable code detection--shows that
BlueCodeAgent achieves significant gains over the base models and safety
prompt-based defenses. In particular, for vulnerable code detection tasks,
BlueCodeAgent integrates dynamic analysis to effectively reduce false
positives, a challenging problem as base models tend to be over-conservative,
misclassifying safe code as unsafe. Overall, BlueCodeAgent achieves an average
12.7\% F1 score improvement across four datasets in three tasks, attributed to
its ability to summarize actionable constitutions that enhance context-aware
risk detection. We demonstrate that the red teaming benefits the blue teaming
by continuously identifying new vulnerabilities to enhance defense performance.

</details>


### [197] [When Old Meets New: Evaluating the Impact of Regression Tests on SWE Issue Resolution](https://arxiv.org/abs/2510.18270)
*Yang Chen,Toufique Ahmed,Reyhaneh Jabbarvand,Martin Hirzel*

Main category: cs.SE

TL;DR: 提出TestPrune技术，利用问题跟踪报告和回归测试进行bug复现和补丁验证，能自动最小化回归测试套件，在多个框架和基准测试中提升性能且成本低。


<details>
  <summary>Details</summary>
Motivation: 现实项目测试套件虽大且覆盖率高，但仍不足以检测所有bug，需利用回归测试进行调试。

Method: 提出TestPrune技术，自动最小化回归测试套件，可接入任何自动化bug修复流程。

Result: 在Otter框架中问题复现率相对提升6.2%-9.0%，在Agentless框架中问题解决率相对提升9.4% - 12.9%，使用成本低。

Conclusion: TestPrune技术能有效利用回归测试，提升bug复现和解决率，且成本开销小。

Abstract: Test suites in real-world projects are often large and achieve high code
coverage, yet they remain insufficient for detecting all bugs. The abundance of
unresolved issues in open-source project trackers highlights this gap. While
regression tests are typically designed to ensure past functionality is
preserved in the new version, they can also serve a complementary purpose:
debugging the current version. Specifically, regression tests can (1) enhance
the generation of reproduction tests for newly reported issues, and (2)
validate that patches do not regress existing functionality. We present
TestPrune, a fully automated technique that leverages issue tracker reports and
strategically reuses regression tests for both bug reproduction and patch
validation.
  A key contribution of TestPrune is its ability to automatically minimize the
regression suite to a small, highly relevant subset of tests. Due to the
predominance of LLM-based debugging techniques, this minimization is essential
as large test suites exceed context limits, introduce noise, and inflate
inference costs. TestPrune can be plugged into any agentic bug repair pipeline
and orthogonally improve overall performance. As a proof of concept, we show
that TestPrune leads to a 6.2%-9.0% relative increase in issue reproduction
rate within the Otter framework and a 9.4% - 12.9% relative increase in issue
resolution rate within the Agentless framework on SWE-Bench Lite and SWE-Bench
Verified benchmarks, capturing fixes that were correctly produced by agents but
not submitted as final patches. Compared to the benefits, the cost overhead of
using TestPrune is minimal, i.e., \$0.02 and \$0.05 per SWE-Bench instance,
using GPT-4o and Claude-3.7-Sonnet models, respectively.

</details>


### [198] [Ensuring Robustness in ML-enabled Software Systems: A User Survey](https://arxiv.org/abs/2510.18292)
*Hala Abdelkader,Mohamed Abdelrazek,Priya Rani,Rajesh Vasa,Jean-Guy Schneider*

Main category: cs.SE

TL;DR: 提出 ML-On-Rails 协议增强 ML 系统鲁棒性，经调查发现其作用，指出需支持资源并规划未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统软件工程实践无法满足 ML 组件需求，要应对 ML 软件系统的关键挑战，如静默故障、OOD 数据和对抗攻击等。

Method: 提出 ML-On-Rails 协议，集成 OOD 检测、对抗攻击检测等保障措施，采用 HTTP 状态码的模型到软件通信框架；开展从业者调查。

Result: 调查揭示了主要鲁棒性问题、现有解决方案的差距，表明标准化协议可提升系统鲁棒性。

Conclusion: 需要为 ML 系统工程师提供更多支持和资源，应利用调查和实际应用见解完善协议以提高其有效性。

Abstract: Ensuring robustness in ML-enabled software systems requires addressing
critical challenges, such as silent failures, out-of-distribution (OOD) data,
and adversarial attacks. Traditional software engineering practices, which rely
on predefined logic, are insufficient for ML components that depend on data and
probabilistic decision-making. To address these challenges, we propose the
ML-On-Rails protocol, a unified framework designed to enhance the robustness
and trustworthiness of ML-enabled systems in production. This protocol
integrates key safeguards such as OOD detection, adversarial attack detection,
input validation, and explainability. It also includes a model-to-software
communication framework using HTTP status codes to enhance transparency in
reporting model outcomes and errors. To align our approach with real-world
challenges, we conducted a practitioner survey, which revealed major robustness
issues, gaps in current solutions, and highlighted how a standardised protocol
such as ML-On-Rails can improve system robustness. Our findings highlight the
need for more support and resources for engineers working with ML systems.
Finally, we outline future directions for refining the proposed protocol,
leveraging insights from the survey and real-world applications to continually
enhance its effectiveness.

</details>


### [199] [InspectCoder: Dynamic Analysis-Enabled Self Repair through interactive LLM-Debugger Collaboration](https://arxiv.org/abs/2510.18327)
*Yunkun Wang,Yue Zhang,Guochang Li,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: 提出InspectCoder程序修复系统，让大语言模型能通过交互式调试器控制进行动态分析，在两个基准测试中表现优异，还贡献了开源中间件InspectWare。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型自修复方法缺乏深入的运行时行为分析能力，难以诊断复杂逻辑错误。

Method: 提出双代理框架，可在有状态调试会话中进行战略断点放置、目标状态检查和增量运行时实验，自适应检查和扰动运行时相关中间状态，利用调试器反馈引导多步推理。

Result: 在两个自修复基准测试中，修复准确率相对最强基线提升5.10%-60.37%，修复效率提升1.67 - 2.24倍。

Conclusion: 工作为交互式大语言模型-调试器系统提供了见解，展示了大语言模型驱动的动态分析在自动化软件工程中的巨大潜力。

Abstract: Large Language Models (LLMs) frequently generate buggy code with complex
logic errors that are challenging to diagnose. While existing LLM-based
self-repair approaches conduct intensive static semantic analysis or reply on
superficial execution logs, they miss the in-depth runtime behaviors that often
expose bug root causes-lacking the interactive dynamic analysis capabilities
that make human debugging effective. We present InspectCoder, the first agentic
program repair system that empowers LLMs to actively conduct dynamic analysis
via interactive debugger control. Our dual-agent framework enables strategic
breakpoint placement, targeted state inspection, and incremental runtime
experimentation within stateful debugger sessions. Unlike existing methods that
follow fixed log collection procedures, InspectCoder adaptively inspects and
perturbs relevant intermediate states at runtime, and leverages immediate
process rewards from debugger feedback to guide multi-step reasoning,
transforming LLM debugging paradigm from blind trial-and-error into systematic
root cause diagnosis. We conduct comprehensive experiments on two challenging
self-repair benchmarks: BigCodeBench-R and LiveCodeBench-R. InspectCoder
achieves 5.10%-60.37% relative improvements in repair accuracy over the
strongest baseline, while delivering 1.67x-2.24x superior bug-fix efficiency
respectively. We also contribute InspectWare, an open-source middleware that
abstracts debugger complexities and maintains stateful debugging sessions
across mainstream Python testing frameworks. Our work provides actionable
insight into the interactive LLM-debugger systems, demonstrating the
significant potential of LLM-driven dynamic analysis for automated software
engineering.

</details>


### [200] [Human to Document, AI to Code: Three Case Studies of Comparing GenAI for Notebook Competitions](https://arxiv.org/abs/2510.18430)
*Tasha Settewong,Youmei Fan,Raula Gaikovina Kula,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 研究通过三个案例研究，对比人类与生成式AI在计算笔记本编码和文档记录活动中的特点，并为促进人机协作提出四项议程。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术出现，在竞争环境中区分人类与生成式AI编写内容的特征很重要。

Method: 开展三个案例研究，对比人类编写的Kaggle获奖笔记本的代码和文档特征，分析人类与生成式AI编写的笔记本差异。

Result: 金牌获得者的笔记本文档更长更详细；生成式AI笔记本代码质量更高，人类编写的笔记本结构更多样、复杂，解决问题更具创新性。

Conclusion: 为进一步研究如何在笔记本中利用生成式AI以实现人机最大程度协作提出四项议程。

Abstract: Computational notebooks have become the preferred tool of choice for data
scientists and practitioners to perform analyses and share results. Notebooks
uniquely combine scripts with documentation. With the emergence of generative
AI (GenAI) technologies, it is increasingly important, especially in
competitive settings, to distinguish the characteristics of human-written
versus GenAI.
  In this study, we present three case studies to explore potential strengths
of both humans and GenAI through the coding and documenting activities in
notebooks. We first characterize differences between 25 code and documentation
features in human-written, medal-winning Kaggle notebooks. We find that gold
medalists are primarily distinguished by longer and more detailed
documentation. Second, we analyze the distinctions between human-written and
GenAI notebooks. Our results show that while GenAI notebooks tend to achieve
higher code quality (as measured by metrics like code smells and technical
debt), human-written notebooks display greater structural diversity,
complexity, and innovative approaches to problem-solving. Based on these
results, we envision the work as groundwork that highlight four agendas to
further investigate how GenAI could be utilized in notebooks that maximizes the
potential collaboration between human and AI.

</details>


### [201] [Real-World Usability of Vulnerability Proof-of-Concepts: A Comprehensive Study](https://arxiv.org/abs/2510.18448)
*Wenjing Dang,Kaixuan Li,Sen Chen,Zhenwei Zhuo,Lyuye Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: 文章开展野外PoC大规模研究，收集大量PoC数据，评估其报告可用性、完整性和可复现性，发现多数CVE漏洞缺少可用PoC，现有报告缺关键组件，最后提出提升PoC可用性策略。


<details>
  <summary>Details</summary>
Motivation: PoC研究落后于漏洞数据研究，存在实际PoC分散、编写风格多样、复现困难等问题，需填补研究空白。

Method: 1. 从13个平台收集470,921个PoC及其报告；2. 提出结合模式匹配和微调BERT - NER模型的组件提取方法；3. 招募8名参与者手动复现150个样本漏洞进行可复现性分析。

Result: 78.9%的CVE漏洞缺少可用PoC，现有PoC报告约缺失30%关键组件，发现使用现有报告复现漏洞失败的多种原因。

Conclusion: 为利益相关者提出提升漏洞PoC可用性以加强软件安全的可行策略。

Abstract: The Proof-of-Concept (PoC) for a vulnerability is crucial in validating its
existence, mitigating false positives, and illustrating the severity of the
security threat it poses. However, research on PoCs significantly lags behind
studies focusing on vulnerability data. This discrepancy can be directly
attributed to several challenges, including the dispersion of real-world PoCs
across multiple platforms, the diversity in writing styles, and the difficulty
associated with PoC reproduction. To fill this gap, we conduct the first
large-scale study on PoCs in the wild, assessing their report availability,
completeness, reproducibility. Specifically, 1) to investigate PoC reports
availability for CVE vulnerability, we collected an extensive dataset of
470,921 PoCs and their reports from 13 platforms, representing the broadest
collection of publicly available PoCs to date. 2) To assess the completeness of
PoC report at a fine-grained level, we proposed a component extraction method,
which combines pattern-matching techniques with a fine-tuned BERT-NER model to
extract 9 key components from PoC reports. 3) To evaluate the effectiveness of
PoCs, we recruited 8 participants to manually reproduce 150 sampled
vulnerabilities with 32 vulnerability types based on PoC reports, enabling an
in-depth analysis of PoC reproducibility and the factors influencing it. Our
findings reveal that 78.9% of CVE vulnerabilities lack available PoCs, and
existing PoC reports typically miss about 30% of the essential components
required for effective vulnerability understanding and reproduction, with
various reasons identified for the failure to reproduce vulnerabilities using
available PoC reports. Finally, we proposed actionable strategies for
stakeholders to enhance the overall usability of vulnerability PoCs in
strengthening software security.

</details>


### [202] [Large Language Models in Thematic Analysis: Prompt Engineering, Evaluation, and Guidelines for Qualitative Software Engineering Research](https://arxiv.org/abs/2510.18456)
*Cristina Martinez Montes,Robert Feldt,Cristina Miguel Martos,Sofia Ouhbi,Shweta Premanandan,Daniel Graziotin*

Main category: cs.SE

TL;DR: 本文探讨将大语言模型（LLMs）融入软件工程研究主题分析（TA）的方法，设计提示词并评估，给出融入建议。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能发展，缺乏将LLMs融入TA的可复现方法，且缺少对LLM生成定性输出的系统评估。

Method: 为Braun和Clarke的反射式TA的2 - 5阶段设计并迭代优化提示词，用15个访谈数据，由四位专家评估LLM和人类生成的代码与主题。

Result: 评估者61%的时间更喜欢LLM生成的代码，但也指出LLMs存在数据碎片化、遗漏潜在解释、主题边界不清晰等问题。

Conclusion: 提出可复现方法、进行实证比较并给出融入定性分析的指导原则。

Abstract: As artificial intelligence advances, large language models (LLMs) are
entering qualitative research workflows, yet no reproducible methods exist for
integrating them into established approaches like thematic analysis (TA), one
of the most common qualitative methods in software engineering research.
Moreover, existing studies lack systematic evaluation of LLM-generated
qualitative outputs against established quality criteria. We designed and
iteratively refined prompts for Phases 2-5 of Braun and Clarke's reflexive TA,
then tested outputs from multiple LLMs against codes and themes produced by
experienced researchers. Using 15 interviews on software engineers' well-being,
we conducted blind evaluations with four expert evaluators who applied rubrics
derived directly from Braun and Clarke's quality criteria. Evaluators preferred
LLM-generated codes 61% of the time, finding them analytically useful for
answering the research question. However, evaluators also identified
limitations: LLMs fragmented data unnecessarily, missed latent interpretations,
and sometimes produced themes with unclear boundaries. Our contributions are
threefold. First, a reproducible approach integrating refined, documented
prompts with an evaluation framework to operationalize Braun and Clarke's
reflexive TA. Second, an empirical comparison of LLM- and human-generated codes
and themes in software engineering data. Third, guidelines for integrating LLMs
into qualitative analysis while preserving methodological rigour, clarifying
when and how LLMs can assist effectively and when human interpretation remains
essential.

</details>


### [203] [CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment](https://arxiv.org/abs/2510.18471)
*Xue Jiang,Yihong Dong,Mengyang Liu,Hongyi Deng,Tian Wang,Yongding Tao,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: 提出CodeRL+方法，将执行语义对齐融入RLVR训练管道用于代码生成，实验显示其性能优于基线，有良好泛化性和适用性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代码生成存在文本模式训练与功能正确性间的语义差距，仅依赖二元奖励信号效率低，需更好方法弥合差距。

Method: 提出CodeRL+，让模型推断变量级执行轨迹，利用现有策略内滚动直接构建执行语义对齐，并与多种强化学习算法无缝集成。

Result: CodeRL+在pass@1上平均相对提升4.6%，在代码推理和测试输出生成基准上分别提高15.5%和4.4%的准确率，适用于多种强化学习算法和大语言模型。

Conclusion: CodeRL+能加强代码文本表示与执行语义的对齐，具有良好性能、泛化性和适用性。

Abstract: While Large Language Models (LLMs) excel at code generation by learning from
vast code corpora, a fundamental semantic gap remains between their training on
textual patterns and the goal of functional correctness, which is governed by
formal execution semantics. Reinforcement Learning with Verifiable Rewards
(RLVR) approaches attempt to bridge this gap using outcome rewards from
executing test cases. However, solely relying on binary pass/fail signals is
inefficient for establishing a well-aligned connection between the textual
representation of code and its execution semantics, especially for subtle
logical errors within the code. In this paper, we propose CodeRL+, a novel
approach that integrates execution semantics alignment into the RLVR training
pipeline for code generation. CodeRL+ enables the model to infer variable-level
execution trajectory, providing a direct learning signal of execution
semantics. CodeRL+ can construct execution semantics alignment directly using
existing on-policy rollouts and integrates seamlessly with various RL
algorithms. Extensive experiments demonstrate that CodeRL+ outperforms
post-training baselines (including RLVR and Distillation), achieving a 4.6%
average relative improvement in pass@1. CodeRL+ generalizes effectively to
other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning
and test-output-generation benchmarks, respectively. CodeRL+ shows strong
applicability across diverse RL algorithms and LLMs. Furthermore, probe
analyses provide compelling evidence that CodeRL+ strengthens the alignment
between code's textual representations and its underlying execution semantics.

</details>


### [204] [VAPU: System for Autonomous Legacy Code Modernization](https://arxiv.org/abs/2510.18509)
*Valtteri Ala-Salmi,Zeeshan Rasheed,Abdul Malik Sami,Muhammad Waseem,Kai-Kristian Kemell,Jussi Rasku,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文提出用基于大语言模型的多智能体系统VAPU更新遗留应用程序，扩展评估显示其在满足需求方面有优势，是自主更新遗留应用组件的可行方案。


<details>
  <summary>Details</summary>
Motivation: 遗留应用存在兼容性、安全和可靠性风险，但更新成本高，需找到经济有效的更新方案。

Method: 提出Verifying Agent Pipeline Updater (VAPU)多智能体系统，从单个大语言模型扩展到五个大语言模型评估，结合不同温度参数设置，用20个开源Python项目测试，并与零样本学习和单样本学习提示对比。

Result: 低温下VAPU与零样本/单样本学习提示的错误计数相当，但满足需求程度更高，Python文件更新需求成功率比零样本/单样本学习提示最多提高22.5%。

Conclusion: 基于大语言模型的多智能体系统是自主更新遗留应用组件的有效解决方案。

Abstract: In this study, we present a solution for the modernization of legacy
applications, an area of code generation where LLM-based multi-agent systems
are proving essential for complex multi-phased tasks. Legacy applications often
contain deprecated components that create compatibility, security, and
reliability risks, but high resource costs make companies hesitate to update.
We take a step forward to integrate an LLM-based multi-agent system as part of
a legacy web application update to provide a cost-effective solution to update
legacy applications autonomously. We propose a multi-agent system named a
Verifying Agent Pipeline Updater (VAPU), which is designed to update code files
in phases while simulating different roles in a software development team. In
our previous study, we evaluated the system for legacy version updates by using
six legacy web application view files by resulting errors and accomplished
requirements. This study extends the previous evaluation of a multi-agent
pipeline system by extending the evaluation of VAPU from a single LLM to five
LLMs and using the temperature parameter in both 0 to 1 settings. Additionally,
we tested the system with 20 open-source Python GitHub projects. The results of
the evaluation were compared to Zero-Shot Learning (ZSL) and One-Shot Learning
(OSL) prompts. The extended evaluation of VAPU showed that particularly in a
low-temperature VAPU can get similar level of error count compared to the
ZSL/OSL prompts but with a higher level of fulfilled requirements, depending on
the LLM. VAPU showed up to 22.5% increase in the succeeding Python file update
requirements compared to ZSL/OSL prompts. The study indicates that an LLM-based
multi-agent system is a capable solution to update components of a legacy
application autonomously.

</details>


### [205] [Mining Service Behavior for Stateful Service Emulation](https://arxiv.org/abs/2510.18519)
*Md Arafat Hossain,Jun Han,Muhammad Ashad Kabir,Steve Versteeg,Jean-Guy Schneider,Jiaojiao Jiang*

Main category: cs.SE

TL;DR: 本文提出考虑服务状态从服务交互中推导服务模型的方法，评估显示该方法在服务响应生成的准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有服务虚拟化方法在模拟服务行为时大多未考虑服务状态，降低了服务模拟准确性和测试环境真实性，尤其是处理有状态服务时。

Method: 通过揭示交互消息间的上下文依赖关系和分析消息数据值之间的关系，从服务交互中推导服务模型。

Result: 使用有状态和无状态服务的交互跟踪数据评估，该方法在服务响应生成的准确性和效率上有显著提升。

Conclusion: 考虑服务状态推导服务模型的方法能有效提高服务响应生成的准确性和效率，优于现有方法。

Abstract: Enterprise software systems are increasingly integrating with diverse
services to meet expanding business demands. Testing these highly
interconnected systems presents a challenge due to the need for access to the
connected services. Service virtualization has emerged as a widely used
technique to derive service models from recorded interactions, for service
response generation during system testing. Various methods have been proposed
to emulate actual service behavior based on these interactions, but most fail
to account for the service's state, which reduces the accuracy of service
emulation and the realism of the testing environment, especially when dealing
with stateful services. This paper proposes an approach to deriving service
models from service interactions, which enhance the accuracy of response
generation by considering service state. This is achieved by uncovering
contextual dependencies among interaction messages and analyzing the
relationships between message data values. The approach is evaluated using
interaction traces collected from both stateful and stateless services, and the
results reveal notable enhancements in accuracy and efficiency over existing
approaches in service response generation.

</details>


### [206] [Demonstrators for Industrial Cyber-Physical System Research: A Requirements Hierarchy Driven by Software-Intensive Design](https://arxiv.org/abs/2510.18534)
*Uraz Odyurt,Richard Loendersloot,Tiedo Tinga*

Main category: cs.SE

TL;DR: 研究项目组织中演示器主题存在不确定性，提出演示器需求细化框架并应用于两个项目验证有效性。


<details>
  <summary>Details</summary>
Motivation: 研究项目中演示器覆盖范围界定不精确、依赖TRL尺度描述松散，导致结果演示目标与可实现性不匹配，阻碍项目进展。

Method: 提出演示器需求细化框架，定义5个层次的演示级别，与期望和工业用例关联。

Result: 将框架应用于两个处于不同阶段的研究项目，显示出框架的有效性。

Conclusion: 所提出的演示器需求细化框架能评估目标演示可行性、进行现实调整并描述需求，虽未完全验证，但已展现出有效性。

Abstract: One of the challenges apparent in the organisation of research projects is
the uncertainties around the subject of demonstrators. A precise and detailed
elicitation of the coverage for project demonstrators is often an afterthought
and not sufficiently detailed during proposal writing. This practice leads to
continuous confusion and a mismatch between targeted and achievable
demonstration of results, hindering progress. The reliance on the TRL scale as
a loose descriptor does not help either. We propose a demonstrator requirements
elaboration framework aiming to evaluate the feasibility of targeted
demonstrations, making realistic adjustments, and assist in describing
requirements. In doing so, we define 5 hierarchical levels of demonstration,
clearly connected to expectations, e.g., work package interaction, and also
connected to the project's industrial use-cases. The considered application
scope in this paper is the domain of software-intensive systems and industrial
cyber-physical systems. A complete validation is not accessible, as it would
require application of our framework at the start of a project and observing
the results at the end, taking 4-5 years. Nonetheless, we have applied it to
two research projects from our portfolio, one at the early and another at the
final stages, revealing its effectiveness.

</details>


### [207] [When Abstraction Breaks Physics: Rethinking Modular Design in Quantum Software](https://arxiv.org/abs/2510.18557)
*Jianjun Zhao*

Main category: cs.SE

TL;DR: 指出量子软件工程中经典抽象机制的冲突，给出失败案例、设计原则和研究方向，以推动对抽象的系统反思。


<details>
  <summary>Details</summary>
Motivation: 量子程序语义与经典不同，经典抽象机制在量子软件工程中安全使用存在挑战，需基于量子语义重新思考抽象。

Method: 识别冲突，提出三类失败案例，给出设计原则和研究方向。

Result: 明确了冲突，列举失败案例，提出设计原则和研究方向。

Conclusion: 应基于量子语义和工程可扩展性，对量子软件工程中的抽象进行系统反思。

Abstract: Abstraction is a fundamental principle in classical software engineering,
which enables modularity, reusability, and scalability. However, quantum
programs adhere to fundamentally different semantics, such as unitarity,
entanglement, the no-cloning theorem, and the destructive nature of
measurement, which introduce challenges to the safe use of classical
abstraction mechanisms. This paper identifies a fundamental conflict in quantum
software engineering: abstraction practices that are syntactically valid may
violate the physical constraints of quantum computation. We present three
classes of failure cases where naive abstraction breaks quantum semantics and
propose a set of design principles for physically sound abstraction mechanisms.
We further propose research directions, including quantum-specific type
systems, effect annotations, and contract-based module design. Our goal is to
initiate a systematic rethinking of abstraction in quantum software
engineering, based on quantum semantics and considering engineering
scalability.

</details>


### [208] [WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality](https://arxiv.org/abs/2510.18560)
*Chunyang Li,Yilun Zheng,Xinting Huang,Tianqing Fang,Jiahao Xu,Yangqiu Song,Lihui Chen,Han Hu*

Main category: cs.SE

TL;DR: 提出WebDevJudge基准评估大语言模型作为评判者在网页开发中的表现，实验揭示大语言模型评判者与人类专家存在差距，为后续研究提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为评判者在开放式任务中的可靠性未被探索，需评估其在网页开发中的表现。

Method: 引入WebDevJudge基准，包含人类偏好标签和结构化评分标准，评估多种评估器，研究不同范式和指导机制的影响。

Result: 实验表明大语言模型评判者与人类专家存在显著差距，差距源于模型的根本局限性。

Conclusion: WebDevJudge对大语言模型作为评判者提出挑战，为复杂场景下开发更可靠的自动评估器提供指导。

Abstract: The paradigm of LLM-as-a-judge is emerging as a scalable and efficient
alternative to human evaluation, demonstrating strong performance on
well-defined tasks. However, its reliability in open-ended tasks with dynamic
environments and complex interactions remains unexplored. To bridge the gap, we
introduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge
performance in web development, with support for both non-interactive
evaluation based on static observations and continuous interactive evaluation
with a dynamic web environment. WebDevJudge comprises human preference labels
over paired web implementations, annotated with structured and query-grounded
rubrics to ensure high-quality ground truth. Using this benchmark, we
comprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic
workflows. We systematically investigate the impact of different paradigms and
guidance mechanisms. Our experiments reveal a significant gap between LLM
judges and human experts. In-depth analysis indicates this gap stems from
fundamental model limitations, including failures in recognizing functional
equivalence, verifying task feasibility, and mitigating bias. Overall,
WebDevJudge presents a significant challenge to LLM-as-a-judge, offering
insights to guide future research toward developing more reliable and capable
automated evaluators for complicated scenarios. Code and data are available at
https://github.com/lcy2723/WebDevJudge.

</details>


### [209] [A Structured Evaluation Framework for Low-Code Platform Selection: A Multi-Criteria Decision Model for Enterprise Digital Transformation](https://arxiv.org/abs/2510.18590)
*Antonio Lamanna*

Main category: cs.SE

TL;DR: 本文提出基于五个关键标准的低代码开发平台综合评估框架及加权评分模型，经企业环境验证可改善决策结果并降低风险。


<details>
  <summary>Details</summary>
Motivation: 低代码开发平台快速普及，需要系统评估方法辅助组织进行平台选择决策。

Method: 提出基于五个关键标准（业务流程编排、UI/UX 定制、集成与互操作性、治理与安全、AI 增强自动化）的综合评估框架和加权评分模型。

Result: 通过企业环境的实证验证，结构化方法能显著改善决策结果，降低平台锁定或选择不当解决方案的风险。

Conclusion: 所提出的评估框架和模型可填补营销驱动的平台比较与严格的特定环境评估方法之间的差距，提升决策质量。

Abstract: The rapid adoption of Low-Code Development Platforms (LCDPs) has created a
critical need for systematic evaluation methodologies that enable organizations
to make informed platform selection decisions. This paper presents a
comprehensive evaluation framework based on five key criteria: Business Process
Orchestration, UI/UX Customization, Integration and Interoperability,
Governance and Security, and AI-Enhanced Automation. We propose a weighted
scoring model that allows organizations to quantitatively assess and compare
different low-code platforms based on their specific requirements and strategic
priorities. The framework addresses the gap between marketing-driven platform
comparisons and rigorous, context-specific evaluation methodologies. Through
empirical validation in enterprise environments, we demonstrate how this
structured approach can significantly improve decision-making outcomes and
reduce the risk of platform lock-in or inadequate solution selection.

</details>


### [210] [CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent](https://arxiv.org/abs/2510.18596)
*Haojia Lin,Xiaoyu Tan,Yulei Qin,Zihan Xu,Yuchen Shi,Zongyi Li,Gang Li,Shaofei Cai,Siqi Cai,Chaoyou Fu,Ke Li,Xing Sun*

Main category: cs.SE

TL;DR: 本文提出CUARewardBench用于评估计算机使用代理（CUA）奖励模型，包括首个综合奖励基准、多样可靠数据集、全面分析及新集成方法UPE，UPE性能出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于脚本的验证器扩展性有限且无法进行逐步评估，奖励模型在CUA评估中的有效性有待探索。

Method: 构建CUARewardBench，包含设计基准、收集多样数据集、进行广泛实验分析，提出UPE集成方法。

Result: 揭示当前CUA奖励模型的关键局限性，UPE在ORM和PRM上取得高精确率和NPV，优于单一VLM和传统集成方法。

Conclusion: CUARewardBench为CUA奖励模型评估提供了有效工具，UPE能显著提升奖励模型可靠性。

Abstract: Computer-using agents (CUAs) enable task completion through natural
interaction with operating systems and software interfaces. While script-based
verifiers are widely adopted for evaluation, they suffer from limited
scalability and inability to provide step-wise assessment. Reward models offer
promising alternatives, but their effectiveness on CUA evaluation remains
largely underexplored. To address this gap, we present CUARewardBench,
comprising four key contributions: (1) First-ever Comprehensive CUA Reward
Benchmark: We introduce the first benchmark for evaluating both outcome reward
models (ORM) and process reward models (PRM) on CUA tasks, enabling systematic
assessment across trajectory-level and step-level evaluation. (2) Diverse,
Practical and Reliable Dataset: CUARewardBench encompasses trajectories from 10
software categories and 7 agent architectures with varying performance levels
(25.9%-50.8% success rates). All trajectories are expertly annotated through
carefully designed protocols, with rigorous quality control to ensure
reliability and practical applicability. (3) Comprehensive Analysis and
Insights: Through extensive experiments across 7 vision-language models and 3
prompt templates, we reveal critical limitations of current CUA RMs, including
insufficient visual reasoning capabilities, knowledge deficiencies, and the
superiority of general VLMs over specialized CUA models for reward evaluation.
(4) Unanimous Prompt Ensemble (UPE): Based on the insights from our
comprehensive analysis, we propose UPE, a novel ensemble method that
significantly enhances reward model reliability through strict unanimous voting
and strategic prompt-template configurations. UPE achieves 89.8% precision and
93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially
outperforming single VLMs and traditional ensemble approaches.

</details>


### [211] [An overview of the use of alternative funding and contracting approaches relevant for agile software development: A systematic review of real-life experiences](https://arxiv.org/abs/2510.18711)
*Bertha Ngereja,Magne Jørgensen*

Main category: cs.SE

TL;DR: 本文综述了替代（非传统）合同和资金方法在敏捷软件开发中的应用，识别了相关方法，分析其动机、益处与挑战，建议组织采用混合方法过渡。


<details>
  <summary>Details</summary>
Motivation: 传统资金和合同方法与敏捷软件开发原则冲突，需寻找更契合的替代方法。

Method: 在SCOPUS、Web of Science和Google Scholar进行系统文献综述，筛选出38篇相关实证研究。

Result: 识别出四种替代资金和四种替代合同方法；采用替代方法可提高客户满意度、降低承包商风险和提高资源利用率，但也带来文化和结构挑战等问题。

Conclusion: 组织选择合适方法时需考虑自身背景，可先采用混合方法，再逐步过渡到完全灵活的方法。

Abstract: Agile software development emphasizes flexibility and iterative processes,
which may conflict with the more linear, rigid, and time-consuming traditional
funding and contracting approaches. This review synthesizes real-life
experiences of using alternative (non-traditional) contracting and funding
approaches. The focus is on identifying approaches that align better with agile
principles and understanding the motivations, benefits, and challenges these
alternatives present. A systematic literature review was conducted in SCOPUS,
Web of Science, and Google Scholar, where we identified 38 relevant
peer-reviewed empirical studies from private and public sector contexts. Four
alternative funding and four alternative contracting approaches were
identified. Organizations were motivated to adopt these alternative approaches
because traditional approaches often proved too rigid, conflicted with agile
principles, hindered effective client-contractor collaboration, and limited
profitability. The benefits of these alternatives included higher client
satisfaction, reduced contractor risk, and more efficient resource utilization.
Adopting alternative funding and contracting approaches may promote flexibility
and efficiency in agile projects but also presents cultural and structural
challenges, increases the risk of scope creep and analysis paralysis, and
requires additional effort in terms of time and resources. The context of the
organization matters highly in selecting a suitable approach, such as the
organizational readiness in terms of its leaders, people, and systems. Thus,
instead of wholly adopting alternative approaches and introducing changes
abruptly, organizations may benefit from starting with hybrid approaches that
balance flexibility and control and progressively transition to fully flexible
approaches tailored to their needs

</details>


### [212] [Causally Perturbed Fairness Testing](https://arxiv.org/abs/2510.18719)
*Chengwen Du,Tao Chen*

Main category: cs.SE

TL;DR: 提出因果扰动公平性测试框架CausalFT，能提升基础生成器发现公平性漏洞的能力，效率高且能增强偏差恢复能力。


<details>
  <summary>Details</summary>
Motivation: 当前工作忽视数据特征知识，限制了在处理表格数据的AI系统中利用扰动揭示公平性漏洞的潜力，需弥补这一差距。

Method: 通过因果推理提取与敏感特征有直接因果关系的非敏感特征，将因果关系注入扰动来指导测试样本生成器，CausalFT可与多种基础生成器结合。

Result: 在1296个案例实验中，CausalFT在93%的案例中显著提升基础生成器发现公平性漏洞的能力，64%的案例中优于基于相关性排名的方法，且更高效，几乎在所有案例中能更好地增强偏差恢复能力。

Conclusion: CausalFT是一个有效的公平性测试框架，能提升发现公平性漏洞的能力和偏差恢复能力。

Abstract: To mitigate unfair and unethical discrimination over sensitive features
(e.g., gender, age, or race), fairness testing plays an integral role in
engineering systems that leverage AI models to handle tabular data. A key
challenge therein is how to effectively reveal fairness bugs under an
intractable sample size using perturbation. Much current work has been focusing
on designing the test sample generators, ignoring the valuable knowledge about
data characteristics that can help guide the perturbation and hence limiting
their full potential. In this paper, we seek to bridge such a gap by proposing
a generic framework of causally perturbed fairness testing, dubbed CausalFT.
Through causal inference, the key idea of CausalFT is to extract the most
directly and causally relevant non-sensitive feature to its sensitive
counterpart, which can jointly influence the prediction of the label. Such a
causal relationship is then seamlessly injected into the perturbation to guide
a test sample generator. Unlike existing generator-level work, CausalFT serves
as a higher-level framework that can be paired with diverse base generators.
Extensive experiments on 1296 cases confirm that CausalFT can considerably
improve arbitrary base generators in revealing fairness bugs over 93% of the
cases with acceptable extra runtime overhead. Compared with a state-of-the-art
approach that ranks the non-sensitive features solely based on correlation,
CausalFT performs significantly better on 64% cases while being much more
efficient. Further, CausalFT can better improve bias resilience in nearly all
cases.

</details>


### [213] [ShaRE your Data! Characterizing Datasets for LLM-based Requirements Engineering](https://arxiv.org/abs/2510.18787)
*Quim Motger,Carlota Catot,Xavier Franch*

Main category: cs.SE

TL;DR: 本文针对LLM4RE研究中数据集可见性和特征描述不足问题进行系统映射研究，找出62个公开数据集并分析，发现研究缺口，提供公开目录和特征描述方案。


<details>
  <summary>Details</summary>
Motivation: 在需求工程中数据稀缺，LLM4RE的数据集分散且特征描述差，限制了复用和可比性，需解决数据集可见性和特征描述问题。

Method: 进行系统映射研究，识别和分析LLM4RE研究中使用的数据集，从多个描述符对数据集进行特征刻画。

Result: 在43项主要研究中引用了62个公开数据集，初步发现多个研究缺口，如需求获取任务覆盖有限、除可追溯性外管理活动数据集稀缺、多语言可用性有限。

Conclusion: 本研究提供公开目录和结构化特征描述方案支持数据集选择、比较和复用，未来将扩展到灰色文献并与开放数据集和基准库集成。

Abstract: [Context] Large Language Models (LLMs) rely on domain-specific datasets to
achieve robust performance across training and inference stages. However, in
Requirements Engineering (RE), data scarcity remains a persistent limitation
reported in surveys and mapping studies. [Question/Problem] Although there are
multiple datasets supporting LLM-based RE tasks (LLM4RE), they are fragmented
and poorly characterized, limiting reuse and comparability. This research
addresses the limited visibility and characterization of datasets used in
LLM4RE. We investigate which public datasets are employed, how they can be
systematically characterized, and which RE tasks and dataset descriptors remain
under-represented. [Ideas/Results] To address this, we conduct a systematic
mapping study to identify and analyse datasets used in LLM4RE research. A total
of 62 publicly available datasets are referenced across 43 primary studies.
Each dataset is characterized along descriptors such as artifact type,
granularity, RE stage, task, domain, and language. Preliminary findings show
multiple research gaps, including limited coverage for elicitation tasks,
scarce datasets for management activities beyond traceability, and limited
multilingual availability. [Contribution] This research preview offers a public
catalogue and structured characterization scheme to support dataset selection,
comparison, and reuse in LLM4RE research. Future work will extend the scope to
grey literature, as well as integration with open dataset and benchmark
repositories.

</details>


### [214] [FeClustRE: Hierarchical Clustering and Semantic Tagging of App Features from User Reviews](https://arxiv.org/abs/2510.18799)
*Max Tiessler,Quim Motger*

Main category: cs.SE

TL;DR: 提出FeClustRE框架用于移动应用评论特征提取，结合多种技术，经评估有良好效果并具多项贡献。


<details>
  <summary>Details</summary>
Motivation: 现有移动应用评论特征提取方法难以将嘈杂模糊反馈转化为可解释见解，无法提供结构化有意义的特征表示，阻碍需求分析等工作。

Method: 提出FeClustRE框架，集成混合特征提取、自动调优的层次聚类和基于大语言模型的语义标注，结合句法分析和大语言模型增强。

Result: 在公共基准测试评估提取正确性，在生成式AI助手应用评论样本研究中评估聚类质量、语义连贯性和可解释性。

Conclusion: FeClustRE提供混合框架、自动调优机制和全面评估方法，开源可复现，能弥合用户反馈和特征理解的差距，助力深入了解需求。

Abstract: [Context and motivation.] Extracting features from mobile app reviews is
increasingly important for multiple requirements engineering (RE) tasks.
However, existing methods struggle to turn noisy, ambiguous feedback into
interpretable insights. [Question/problem.] Syntactic approaches lack semantic
depth, while large language models (LLMs) often miss fine-grained features or
fail to structure them coherently. In addition, existing methods output flat
lists of features without semantic organization, limiting interpretation and
comparability. Consequently, current feature extraction approaches do not
provide structured, meaningful representations of app features. As a result,
practitioners face fragmented information that hinder requirement analysis,
prioritization, and cross-app comparison, among other use cases. [Principal
ideas/results.] In this context, we propose FeClustRE, a framework integrating
hybrid feature extraction, hierarchical clustering with auto-tuning and
LLM-based semantic labelling. FeClustRE combines syntactic parsing with LLM
enrichment, organizes features into clusters, and automatically generates
meaningful taxonomy labels. We evaluate FeClustRE on public benchmarks for
extraction correctness and on a sample study of generative AI assistant app
reviews for clustering quality, semantic coherence, and interpretability.
[Contribution.] Overall, FeClustRE delivers (1) a hybrid framework for feature
extraction and taxonomy generation, (2) an auto-tuning mechanism with a
comprehensive evaluation methodology, and (3) open-source and replicable
implementation. These contributions bridge user feedback and feature
understanding, enabling deeper insights into current and emerging requirements.

</details>


### [215] [Streamlining Acceptance Test Generation for Mobile Applications Through Large Language Models: An Industrial Case Study](https://arxiv.org/abs/2510.18861)
*Pedro Luís Fonseca,Bruno Lima,João Pascoal Faria*

Main category: cs.SE

TL;DR: 本文介绍了自动框架AToMIC，它利用大语言模型从需求和代码变更生成测试工件，应用于BMW的MyBMW应用效果良好，是工业移动项目中简化验收测试的实用方案。


<details>
  <summary>Details</summary>
Motivation: 移动验收测试是现代软件开发瓶颈，创建和维护验收测试工件需大量人工，为解决此问题提出AToMIC。

Method: 引入AToMIC框架，利用专门大语言模型从需求（JIRA工单）和最近代码变更直接生成Gherkin场景、Page Objects和可执行UI测试脚本。

Result: 应用于BMW的MyBMW应用，每个功能不到五分钟生成可执行测试工件，生成工件质量高，调查显示从业者节省时间且有信心采用。

Conclusion: AToMIC是工业移动项目中简化验收测试创建和维护的可扩展实用解决方案。

Abstract: Mobile acceptance testing remains a bottleneck in modern software
development, particularly for cross-platform mobile development using
frameworks like Flutter. While developers increasingly rely on automated
testing tools, creating and maintaining acceptance test artifacts still demands
significant manual effort. To help tackle this issue, we introduce AToMIC, an
automated framework leveraging specialized Large Language Models to generate
Gherkin scenarios, Page Objects, and executable UI test scripts directly from
requirements (JIRA tickets) and recent code changes. Applied to BMW's MyBMW
app, covering 13 real-world issues in a 170+ screen codebase, AToMIC produced
executable test artifacts in under five minutes per feature on standard
hardware. The generated artifacts were of high quality: 93.3% of Gherkin
scenarios were syntactically correct upon generation, 78.8% of PageObjects ran
without manual edits, and 100% of generated UI tests executed successfully. In
a survey, all practitioners reported time savings (often a full developer-day
per feature) and strong confidence in adopting the approach. These results
confirm AToMIC as a scalable, practical solution for streamlining acceptance
test creation and maintenance in industrial mobile projects.

</details>


### [216] [EffiReasonTrans: RL-Optimized Reasoning for Code Translation](https://arxiv.org/abs/2510.18863)
*Yanlin Wang,Rongyi Ou,Yanli Wang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出EffiReasonTrans训练框架解决代码翻译中准确率与推理延迟的权衡问题，在多方面有良好表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽提升代码翻译准确率，但增加推理延迟，影响涉及人工检查的开发工作流，需解决此权衡问题。

Method: 构建高质量推理增强数据集，进行语法和功能检查；采用两阶段训练策略，先监督微调，再强化学习。

Result: 在六个翻译对上评估，提高翻译准确率，减少生成令牌数量，降低推理延迟；消融实验证实两阶段训练框架优势；集成到基于代理的框架中也提升准确率。

Conclusion: EffiReasonTrans能在提高代码翻译准确率的同时平衡推理延迟，具有良好效果。

Abstract: Code translation is a crucial task in software development and maintenance.
While recent advancements in large language models (LLMs) have improved
automated code translation accuracy, these gains often come at the cost of
increased inference latency, hindering real-world development workflows that
involve human-in-the-loop inspection. To address this trade-off, we propose
EffiReasonTrans, a training framework designed to improve translation accuracy
while balancing inference latency. We first construct a high-quality
reasoning-augmented dataset by prompting a stronger language model,
DeepSeek-R1, to generate intermediate reasoning and target translations. Each
(source code, reasoning, target code) triplet undergoes automated syntax and
functionality checks to ensure reliability. Based on this dataset, we employ a
two-stage training strategy: supervised fine-tuning on reasoning-augmented
samples, followed by reinforcement learning to further enhance accuracy and
balance inference latency. We evaluate EffiReasonTrans on six translation
pairs. Experimental results show that it consistently improves translation
accuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while
reducing the number of generated tokens (up to -19.3%) and lowering inference
latency in most cases (up to -29.0%). Ablation studies further confirm the
complementary benefits of the two-stage training framework. Additionally,
EffiReasonTrans demonstrates improved translation accuracy when integrated into
agent-based frameworks. Our code and data are available at
https://github.com/DeepSoftwareAnalytics/EffiReasonTrans.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [217] [A Natural Hedging Framework for Longevity Risk with Graphical Risk Assessment](https://arxiv.org/abs/2510.18721)
*Lydia J. Gabric,Kenneth Q. Zhou*

Main category: q-fin.RM

TL;DR: 本文提出结构化自然对冲框架和图形风险指标用于评估寿险公司长寿风险的自然对冲策略，该方法在多场景中展现出灵活性、可解释性和实用价值。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏构建和评估自然对冲策略的统一框架以及用于自然对冲的可比风险指标。

Method: 提出结构化自然对冲框架，该框架整合估值、校准和评估，同时提出图形风险指标用于评估对冲效果。

Result: 所提出的方法应用于多个对冲场景，展现出灵活性、可解释性和实用价值。

Conclusion: 提出的框架和指标对长寿风险管理有积极作用。

Abstract: Natural hedging allows life insurers to manage longevity risk internally by
offsetting the opposite exposures of life insurance and annuity liabilities.
Although many studies have proposed natural hedging strategies under different
settings, calibration methods, and mortality models, a unified framework for
constructing and evaluating such hedges remains undeveloped. While graphical
risk assessment has been explored for index-based longevity hedges, no
comparable metric exists for natural hedging. This paper proposes a structured
natural hedging framework paired with a graphical risk metric for hedge
evaluation. The framework integrates valuation, calibration, and evaluation,
while the graphical metric provides intuitive insights into residual
dependencies and hedge performance. Applied to multiple hedging scenarios, the
proposed methods demonstrate flexibility, interpretability, and practical value
for longevity risk management.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [218] [Graphical model for tensor factorization by sparse sampling](https://arxiv.org/abs/2510.17886)
*Angelo Giorgio Cavaliere,Riki Nagasawa,Shuta Yokoi,Tomoyuki Obuchi,Hajime Yoshino*

Main category: stat.ML

TL;DR: 研究基于张量分量稀疏测量的张量分解，在高维极限下进行统计推断，构建消息传递算法并测试，还开发了副本理论。


<details>
  <summary>Details</summary>
Motivation: 在大量数据缺失的情况下（如社交网络服务中的推荐系统），基于稀疏测量进行张量分解，获取理论见解。

Method: 在高维极限下进行张量分解的统计推断，构建消息传递算法，在贝叶斯最优师生设置中测试，开发副本理论。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: We consider tensor factorizations based on sparse measurements of the tensor
components. The measurements are designed in a way that the underlying graph of
interactions is a random graph. The setup will be useful in cases where a
substantial amount of data is missing, as in recommendation systems heavily
used in social network services. In order to obtain theoretical insights on the
setup, we consider statistical inference of the tensor factorization in a high
dimensional limit, which we call as dense limit, where the graphs are large and
dense but not fully connected. We build message-passing algorithms and test
them in a Bayes optimal teacher-student setting. We also develop a replica
theory, which becomes exact in the dense limit,to examine the performance of
statistical inference.

</details>


### [219] [A Frequentist Statistical Introduction to Variational Inference, Autoencoders, and Diffusion Models](https://arxiv.org/abs/2510.18777)
*Yen-Chi Chen*

Main category: stat.ML

TL;DR: 本文从频率主义视角介绍变分推断（VI）、变分自编码器（VAEs）和去噪扩散模型（DDMs）理论，弥合经典统计推断与现代生成式AI的差距。


<details>
  <summary>Details</summary>
Motivation: VI在统计学和机器学习中的教学处理分散，给统计学家理解VAEs和DDMs带来障碍，需要从频率主义视角介绍VI。

Method: 从经典的期望最大化（EM）算法出发，解释VI、VAEs和DDMs的理论。

Result: 展示了VI如何作为解决难处理E步骤的可扩展解决方案，以及VAEs和DDMs如何是该框架基于深度学习的自然扩展。

Conclusion: 成功弥合了经典统计推断与现代生成式AI之间的差距。

Abstract: While Variational Inference (VI) is central to modern generative models like
Variational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its
pedagogical treatment is split across disciplines. In statistics, VI is
typically framed as a Bayesian method for posterior approximation. In machine
learning, however, VAEs and DDMs are developed from a Frequentist viewpoint,
where VI is used to approximate a maximum likelihood estimator. This creates a
barrier for statisticians, as the principles behind VAEs and DDMs are hard to
contextualize without a corresponding Frequentist introduction to VI. This
paper provides that introduction: we explain the theory for VI, VAEs, and DDMs
from a purely Frequentist perspective, starting with the classical
Expectation-Maximization (EM) algorithm. We show how VI arises as a scalable
solution for intractable E-steps and how VAEs and DDMs are natural,
deep-learning-based extensions of this framework, thereby bridging the gap
between classical statistical inference and modern generative AI.

</details>


### [220] [Learning Time-Varying Graphs from Incomplete Graph Signals](https://arxiv.org/abs/2510.17903)
*Chuansen Peng,Xiaojing Shen*

Main category: stat.ML

TL;DR: 提出统一非凸优化框架，联合推断时变网络拓扑和插补缺失数据，用ADMM算法求解，有理论保证，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 解决从部分观测图信号中联合推断时变网络拓扑和插补缺失数据的难题。

Method: 提出统一非凸优化框架，引入融合lasso型正则化器，用ADMM算法求解联合优化问题。

Result: ADMM方案收敛到驻点，有非渐近统计保证，数值实验显示在收敛速度和联合精度上优于现有基线。

Conclusion: 所提方法在联合推断时变网络拓扑和插补缺失数据方面表现出色，具有良好性能。

Abstract: This paper tackles the challenging problem of jointly inferring time-varying
network topologies and imputing missing data from partially observed graph
signals. We propose a unified non-convex optimization framework to
simultaneously recover a sequence of graph Laplacian matrices while
reconstructing the unobserved signal entries. Unlike conventional decoupled
methods, our integrated approach facilitates a bidirectional flow of
information between the graph and signal domains, yielding superior robustness,
particularly in high missing-data regimes. To capture realistic network
dynamics, we introduce a fused-lasso type regularizer on the sequence of
Laplacians. This penalty promotes temporal smoothness by penalizing large
successive changes, thereby preventing spurious variations induced by noise
while still permitting gradual topological evolution. For solving the joint
optimization problem, we develop an efficient Alternating Direction Method of
Multipliers (ADMM) algorithm, which leverages the problem's structure to yield
closed-form solutions for both the graph and signal subproblems. This design
ensures scalability to large-scale networks and long time horizons. On the
theoretical front, despite the inherent non-convexity, we establish a
convergence guarantee, proving that the proposed ADMM scheme converges to a
stationary point. Furthermore, we derive non-asymptotic statistical guarantees,
providing high-probability error bounds for the graph estimator as a function
of sample size, signal smoothness, and the intrinsic temporal variability of
the graph. Extensive numerical experiments validate the approach, demonstrating
that it significantly outperforms state-of-the-art baselines in both
convergence speed and the joint accuracy of graph learning and signal recovery.

</details>


### [221] [Arbitrated Indirect Treatment Comparisons](https://arxiv.org/abs/2510.18071)
*Yixin Fang,Weili He*

Main category: stat.ML

TL;DR: 介绍一类新的仲裁间接治疗比较方法以解决MAIC悖论


<details>
  <summary>Details</summary>
Motivation: 解决MAIC悖论，即不同赞助商分析相同数据得出不同治疗效果结论的问题

Method: 提出仲裁间接治疗比较方法，聚焦于估计共同目标人群（重叠人群）的治疗效果

Result: 未提及

Conclusion: 未提及

Abstract: Matching-adjusted indirect comparison (MAIC) has been increasingly employed
in health technology assessments (HTA). By reweighting subjects from a trial
with individual participant data (IPD) to match the covariate summary
statistics of another trial with only aggregate data (AgD), MAIC facilitates
the estimation of a treatment effect defined with respect to the AgD trial
population. This manuscript introduces a new class of methods, termed
arbitrated indirect treatment comparisons, designed to address the ``MAIC
paradox'' -- a phenomenon highlighted by Jiang et al.~(2025). The MAIC paradox
arises when different sponsors, analyzing the same data, reach conflicting
conclusions regarding which treatment is more effective. The underlying issue
is that each sponsor implicitly targets a different population. To resolve this
inconsistency, the proposed methods focus on estimating treatment effects in a
common target population, specifically chosen to be the overlap population.

</details>


### [222] [Generalization Below the Edge of Stability: The Role of Data Geometry](https://arxiv.org/abs/2510.18120)
*Tongtong Liang,Alexander Cloninger,Rahul Parhi,Yu-Xiang Wang*

Main category: stat.ML

TL;DR: 本文理论探索数据几何如何控制过参数化神经网络的隐式偏差，给出两层ReLU网络泛化界，总结出数据‘粉碎’难度与泛化关系并整合经验发现。


<details>
  <summary>Details</summary>
Motivation: 理解过参数化神经网络泛化需考虑数据几何、神经架构和训练动态的相互作用，本文旨在理论探索数据几何对隐式偏差的控制。

Method: 对过参数化两层ReLU网络在稳定性边缘以下训练进行理论推导，针对不同数据分布（低维球混合分布、各向同性分布）得出泛化界。

Result: 对于低维球混合分布数据，导出适应内在维度的泛化界；对于各向同性分布，得出随着概率质量向单位球集中泛化率变差的界。

Conclusion: 数据越难被ReLU神经元激活阈值‘粉碎’，梯度下降学习的表示越能捕捉共享模式，泛化越好；易被粉碎的数据，梯度下降倾向于记忆。理论结果整合了文献中的不同经验发现。

Abstract: Understanding generalization in overparameterized neural networks hinges on
the interplay between the data geometry, neural architecture, and training
dynamics. In this paper, we theoretically explore how data geometry controls
this implicit bias. This paper presents theoretical results for
overparameterized two-layer ReLU networks trained below the edge of stability.
First, for data distributions supported on a mixture of low-dimensional balls,
we derive generalization bounds that provably adapt to the intrinsic dimension.
Second, for a family of isotropic distributions that vary in how strongly
probability mass concentrates toward the unit sphere, we derive a spectrum of
bounds showing that rates deteriorate as the mass concentrates toward the
sphere. These results instantiate a unifying principle: When the data is harder
to "shatter" with respect to the activation thresholds of the ReLU neurons,
gradient descent tends to learn representations that capture shared patterns
and thus finds solutions that generalize well. On the other hand, for data that
is easily shattered (e.g., data supported on the sphere) gradient descent
favors memorization. Our theoretical results consolidate disparate empirical
findings that have appeared in the literature.

</details>


### [223] [Beating the Winner's Curse via Inference-Aware Policy Optimization](https://arxiv.org/abs/2510.18161)
*Hamsa Bastani,Osbert Bastani,Bryce McLaughlin*

Main category: stat.ML

TL;DR: 提出推理感知策略优化方法解决策略优化中预测性能提升无实际支撑的问题，设计算法并模拟验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有策略优化中预测性能提升常无法在下游策略优化中得到证实，需解决此赢家诅咒问题。

Method: 提出推理感知策略优化，考虑下游评估方式，优化估计目标值和比观测策略显著更好的概率，刻画策略帕累托前沿，设计基于机器学习预测反事实结果的策略优化算法。

Result: 进行模拟，展示了所提方法的有效性。

Conclusion: 推理感知策略优化方法可有效应对策略优化中的赢家诅咒问题。

Abstract: There has been a surge of recent interest in automatically learning policies
to target treatment decisions based on rich individual covariates. A common
approach is to train a machine learning model to predict counterfactual
outcomes, and then select the policy that optimizes the predicted objective
value. In addition, practitioners also want confidence that the learned policy
has better performance than the incumbent policy according to downstream policy
evaluation. However, due to the winner's curse-an issue where the policy
optimization procedure exploits prediction errors rather than finding actual
improvements-predicted performance improvements are often not substantiated by
downstream policy optimization. To address this challenge, we propose a novel
strategy called inference-aware policy optimization, which modifies policy
optimization to account for how the policy will be evaluated downstream.
Specifically, it optimizes not only for the estimated objective value, but also
for the chances that the policy will be statistically significantly better than
the observational policy used to collect data. We mathematically characterize
the Pareto frontier of policies according to the tradeoff of these two goals.
Based on our characterization, we design a policy optimization algorithm that
uses machine learning to predict counterfactual outcomes, and then plugs in
these predictions to estimate the Pareto frontier; then, the decision-maker can
select the policy that optimizes their desired tradeoff, after which policy
evaluation can be performed on the test set as usual. Finally, we perform
simulations to illustrate the effectiveness of our methodology.

</details>


### [224] [The Bias-Variance Tradeoff in Data-Driven Optimization: A Local Misspecification Perspective](https://arxiv.org/abs/2510.18215)
*Haixiang Lan,Luofeng Liao,Adam N. Elmachtoub,Christian Kroer,Henry Lam,Haofeng Zhang*

Main category: stat.ML

TL;DR: 本文研究数据驱动随机优化中SAA与基于模型方法在局部模型错误设定下的相对性能，揭示偏差 - 方差权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究对SAA和基于模型方法在复杂问题中的相对性能理解不足，多数结果局限于模型正确或错误设定的二分情况。

Method: 利用统计学中的邻接理论工具，分析局部错误设定下的情况。

Result: 发现SAA、IEO和ETO在局部错误设定下存在偏差 - 方差权衡，偏差和方差的相对重要性取决于局部错误设定程度，还得出决策偏差的显式表达式。

Conclusion: 研究结果有助于更细致地分析这些方法在局部错误设定场景下的性能，为决策提供几何理解。

Abstract: Data-driven stochastic optimization is ubiquitous in machine learning and
operational decision-making problems. Sample average approximation (SAA) and
model-based approaches such as estimate-then-optimize (ETO) or integrated
estimation-optimization (IEO) are all popular, with model-based approaches
being able to circumvent some of the issues with SAA in complex
context-dependent problems. Yet the relative performance of these methods is
poorly understood, with most results confined to the dichotomous cases of the
model-based approach being either well-specified or misspecified. We develop
the first results that allow for a more granular analysis of the relative
performance of these methods under a local misspecification setting, which
models the scenario where the model-based approach is nearly well-specified. By
leveraging tools from contiguity theory in statistics, we show that there is a
bias-variance tradeoff between SAA, IEO, and ETO under local misspecification,
and that the relative importance of the bias and the variance depends on the
degree of local misspecification. Moreover, we derive explicit expressions for
the decision bias, which allows us to characterize (un)impactful
misspecification directions, and provide further geometric understanding of the
variance.

</details>


### [225] [Learning under Quantization for High-Dimensional Linear Regression](https://arxiv.org/abs/2510.18259)
*Dechen Zhang,Junwei Su,Difan Zou*

Main category: stat.ML

TL;DR: 本文对高维线性回归中低比特量化对学习性能的影响进行系统理论研究，给出不同量化目标下的风险界并分析其影响，还比较了两种量化方式。


<details>
  <summary>Details</summary>
Motivation: 低比特量化虽在实践中成功，但缺乏对其影响学习性能的严格理论理解，即使在线性回归场景中也是如此。

Method: 对高维线性回归的有限步随机梯度下降（SGD）进行分析，研究不同量化目标（数据、标签、参数等）的影响，建立分析框架。

Result: 建立了精确的依赖算法和数据的超额风险界，分析不同量化的影响，证明乘法量化可消除频谱失真，加法量化有与批量大小相关的有益缩放效应，还比较了两种量化方式的风险。

Conclusion: 该理论为刻画量化如何塑造优化算法的学习动态提供有力视角，有助于在实际硬件约束下进一步探索学习理论。

Abstract: The use of low-bit quantization has emerged as an indispensable technique for
enabling the efficient training of large-scale models. Despite its widespread
empirical success, a rigorous theoretical understanding of its impact on
learning performance remains notably absent, even in the simplest linear
regression setting. We present the first systematic theoretical study of this
fundamental question, analyzing finite-step stochastic gradient descent (SGD)
for high-dimensional linear regression under a comprehensive range of
quantization targets: data, labels, parameters, activations, and gradients. Our
novel analytical framework establishes precise algorithm-dependent and
data-dependent excess risk bounds that characterize how different quantization
affects learning: parameter, activation, and gradient quantization amplify
noise during training; data quantization distorts the data spectrum; and data
and label quantization introduce additional approximation and quantized error.
Crucially, we prove that for multiplicative quantization (with input-dependent
quantization step), this spectral distortion can be eliminated, and for
additive quantization (with constant quantization step), a beneficial scaling
effect with batch size emerges. Furthermore, for common polynomial-decay data
spectra, we quantitatively compare the risks of multiplicative and additive
quantization, drawing a parallel to the comparison between FP and integer
quantization methods. Our theory provides a powerful lens to characterize how
quantization shapes the learning dynamics of optimization algorithms, paving
the way to further explore learning theory under practical hardware
constraints.

</details>


### [226] [Parametrising the Inhomogeneity Inducing Capacity of a Training Set, and its Impact on Supervised Learning](https://arxiv.org/abs/2510.18332)
*Gargi Roy,Dalia Chakrabarty*

Main category: stat.ML

TL;DR: 引入训练数据集的‘不均匀性参数’，计算该参数，证明非零参数使建模过程需非平稳，且影响预测质量。


<details>
  <summary>Details</summary>
Motivation: 刻画训练数据集特性，使函数建模考虑不均匀相关性结构。

Method: 引入‘不均匀性参数’，在多个公开数据集上计算该参数，基于概率高斯过程学习方法进行研究。

Result: 计算多个数据集的不均匀性参数，证明非零参数使建模过程需非平稳，发现参数影响预测质量和可靠性。

Conclusion: 训练集的不均匀性参数对于函数建模和预测质量有重要影响。

Abstract: We introduce parametrisation of that property of the available
  training dataset, that necessitates an inhomogeneous correlation
  structure for the function that is learnt as a model of the
  relationship between the pair of variables, observations of which
  comprise the considered training data. We refer to a parametrisation
  of this property of a given training set, as its ``inhomogeneity
  parameter''. It is easy to compute this parameter for small-to-large
  datasets, and we demonstrate such computation on multiple
  publicly-available datasets, while also demonstrating that
  conventional ``non-stationarity'' of data does not imply a non-zero
  inhomogeneity parameter of the dataset. We prove that - within the
  probabilistic Gaussian Process-based learning approach - a training
  set with a non-zero inhomogeneity parameter renders it imperative,
  that the process that is invoked to model the sought function, be
  non-stationary. Following the learning of a real-world multivariate
  function with such a Process, quality and reliability of predictions
  at test inputs, are demonstrated to be affected by the inhomogeneity
  parameter of the training data.

</details>


### [227] [Interval Prediction of Annual Average Daily Traffic on Local Roads via Quantile Random Forest with High-Dimensional Spatial Data](https://arxiv.org/abs/2510.18548)
*Ying Yao,Daniel J. Graham*

Main category: stat.ML

TL;DR: 本文提出结合分位数随机森林模型和主成分分析的区间预测方法估计AADT，用英国数据验证，提升了估计准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自动交通探测器覆盖不全，机器学习模型多为点预测，忽略估计不确定性。

Method: 将分位数随机森林模型与主成分分析相结合，生成AADT预测区间。

Result: 用英格兰和威尔士超2000条小路数据评估，区间覆盖概率88.22%，归一化平均宽度0.23，温克勒分数7468.47。

Conclusion: 该框架结合机器学习与空间和高维分析，提升AADT估计准确性和可解释性，支持更可靠的交通规划。

Abstract: Accurate annual average daily traffic (AADT) data are vital for transport
planning and infrastructure management. However, automatic traffic detectors
across national road networks often provide incomplete coverage, leading to
underrepresentation of minor roads. While recent machine learning advances have
improved AADT estimation at unmeasured locations, most models produce only
point predictions and overlook estimation uncertainty. This study addresses
that gap by introducing an interval prediction approach that explicitly
quantifies predictive uncertainty. We integrate a Quantile Random Forest model
with Principal Component Analysis to generate AADT prediction intervals,
providing plausible traffic ranges bounded by estimated minima and maxima.
Using data from over 2,000 minor roads in England and Wales, and evaluated with
specialized interval metrics, the proposed method achieves an interval coverage
probability of 88.22%, a normalized average width of 0.23, and a Winkler Score
of 7,468.47. By combining machine learning with spatial and high-dimensional
analysis, this framework enhances both the accuracy and interpretability of
AADT estimation, supporting more robust and informed transport planning.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [228] [Principled Argo Modeling using Vecchia-based Gaussian Processes](https://arxiv.org/abs/2510.18067)
*Nian Liu,Jian Cao*

Main category: stat.CO

TL;DR: 提出一站式高斯过程回归框架对Argo温度数据建模，性能优且可扩展


<details>
  <summary>Details</summary>
Motivation: 现有Argo温度建模方法在设计均值结构和定义域分区有挑战，常需临时建模选择

Method: 提出带通用时空协方差函数的高斯过程回归框架，用Vecchia近似确保可扩展性

Result: 相比需域分区或参数回归的方法，预测性能更优

Conclusion: 该方法为大规模海洋分析提供了有原则、可扩展和可解释的工具

Abstract: Argo is an international program that collects temperature and salinity
observations in the upper two kilometers of the global ocean. Most existing
approaches for modeling Argo temperature rely on spatial partitioning, where
data are locally modeled by first estimating a prescribed mean structure and
then fitting Gaussian processes (GPs) to the mean-subtracted anomalies. Such
strategies introduce challenges in designing suitable mean structures and
defining domain partitions, often resulting in ad hoc modeling choices. In this
work, we propose a one-stop Gaussian process regression framework with a
generic spatio-temporal covariance function to jointly model Argo temperature
data across broad spatial domains. Our fully data-driven approach achieves
superior predictive performance compared with methods that require domain
partitioning or parametric regression. To ensure scalability over large spatial
regions, we employ the Vecchia approximation, which reduces the computational
complexity from cubic to quasi-linear in the number of observations while
preserving predictive accuracy. Using Argo data from January to March over the
years 2007-2016, the same dataset used in prior benchmark studies, we
demonstrate that our approach provides a principled, scalable, and
interpretable tool for large-scale oceanographic analysis.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [229] [KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers](https://arxiv.org/abs/2510.18355)
*Mohd Ruhul Ameen,Akif Islam,Farjana Aktar,M. Saifuzzaman Rafat*

Main category: cs.CL

TL;DR: 本文介绍为孟加拉语农民设计的语音咨询平台KrishokBondhu，它基于RAG框架，经评估表现良好，证明结合多技术为偏远农民提供专业农业指导可行。


<details>
  <summary>Details</summary>
Motivation: 孟加拉许多农民难以及时获得专业农业指导。

Method: 构建基于RAG框架的KrishokBondhu平台，聚合权威资料，经OCR和文档解析数字化，存入向量数据库，通过语音交互为农民提供建议。

Result: 试点评估中72.7%的农业问题获高质量回答，对比KisanQRS基准综合得分提升44.7%，上下文丰富度和完整性大幅提高。

Conclusion: KrishokBondhu证明整合多技术为孟加拉偏远农民提供专业农业指导可行，为全AI农业咨询生态系统铺平道路。

Abstract: In Bangladesh, many farmers continue to face challenges in accessing timely,
expert-level agricultural guidance. This paper presents KrishokBondhu, a
voice-enabled, call-centre-integrated advisory platform built on a
Retrieval-Augmented Generation (RAG) framework, designed specifically for
Bengali-speaking farmers. The system aggregates authoritative agricultural
handbooks, extension manuals, and NGO publications; applies Optical Character
Recognition (OCR) and document-parsing pipelines to digitize and structure the
content; and indexes this corpus in a vector database for efficient semantic
retrieval. Through a simple phone-based interface, farmers can call the system
to receive real-time, context-aware advice: speech-to-text converts the Bengali
query, the RAG module retrieves relevant content, a large language model (Gemma
3-4B) generates a context-grounded response, and text-to-speech delivers the
answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced
high-quality responses for 72.7% of diverse agricultural queries covering crop
management, disease control, and cultivation practices. Compared to the
KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on
a 5-point scale, a 44.7% improvement, with especially large gains in contextual
richness (+367%) and completeness (+100.4%), while maintaining comparable
relevance and technical specificity. Semantic similarity analysis further
revealed a strong correlation between retrieved context and answer quality,
emphasizing the importance of grounding generative responses in curated
documentation. KrishokBondhu demonstrates the feasibility of integrating
call-centre accessibility, multilingual voice interaction, and modern RAG
techniques to deliver expert-level agricultural guidance to remote Bangladeshi
farmers, paving the way toward a fully AI-driven agricultural advisory
ecosystem.

</details>


### [230] [MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training](https://arxiv.org/abs/2510.18830)
*Wenxuan Li,Chengruidong Zhang,Huiqiang Jiang,Yucheng Li,Yuqing Yang,Lili Qiu*

Main category: cs.CL

TL;DR: 本文提出MTraining方法，利用动态稀疏注意力实现大语言模型超长上下文高效训练，在Qwen2.5 - 3B上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 动态稀疏注意力可降低长上下文计算成本，但在超长上下文分布式训练中存在工作级和步骤级不平衡问题，需要解决计算不平衡和通信开销。

Method: 提出MTraining方法，集成动态稀疏训练模式、平衡稀疏环注意力和分层稀疏环注意力三个关键组件。

Result: 在32个A100 GPU集群上训练Qwen2.5 - 3B，将上下文窗口从32K扩展到512K，在下游任务评估中训练吞吐量最高提升6倍，且保持模型精度。

Conclusion: MTraining方法能有效解决动态稀疏注意力在超长上下文分布式训练中的问题，实现高效训练。

Abstract: The adoption of long context windows has become a standard feature in Large
Language Models (LLMs), as extended contexts significantly enhance their
capacity for complex reasoning and broaden their applicability across diverse
scenarios. Dynamic sparse attention is a promising approach for reducing the
computational cost of long-context. However, efficiently training LLMs with
dynamic sparse attention on ultra-long contexts-especially in distributed
settings-remains a significant challenge, due in large part to worker- and
step-level imbalance. This paper introduces MTraining, a novel distributed
methodology leveraging dynamic sparse attention to enable efficient training
for LLMs with ultra-long contexts. Specifically, MTraining integrates three key
components: a dynamic sparse training pattern, balanced sparse ring attention,
and hierarchical sparse ring attention. These components are designed to
synergistically address the computational imbalance and communication overheads
inherent in dynamic sparse attention mechanisms during the training of models
with extensive context lengths. We demonstrate the efficacy of MTraining by
training Qwen2.5-3B, successfully expanding its context window from 32K to 512K
tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite
of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A
Haystack, reveal that MTraining achieves up to a 6x higher training throughput
while preserving model accuracy. Our code is available at
https://github.com/microsoft/MInference/tree/main/MTraining.

</details>


### [231] [Modeling Layered Consciousness with Multi-Agent Large Language Models](https://arxiv.org/abs/2510.17844)
*Sang Hun Kim,Jongmin Lee,Dongkyu Park,So Young Lee,Yosep Chong*

Main category: cs.CL

TL;DR: 提出基于精神分析理论的多智能体框架为大语言模型建模人工意识，经评估展现出潜力。


<details>
  <summary>Details</summary>
Motivation: 为大语言模型建模人工意识。

Method: 提出心理动力学模型，通过智能体交互模拟自我意识等，结合固定特征和动态需求的个性化模块，使用参数高效微调处理情感丰富对话。

Result: 以大语言模型作为评判，71.2%偏好微调后的模型，情感深度提升、输出方差降低。

Conclusion: 该框架有自适应、个性化认知的潜力。

Abstract: We propose a multi-agent framework for modeling artificial consciousness in
large language models (LLMs), grounded in psychoanalytic theory. Our
\textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and
unconsciousness through agent interaction, guided by a Personalization Module
combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning
on emotionally rich dialogues, the system was evaluated across eight
personalized conditions. An LLM as a judge approach showed a 71.2\% preference
for the fine-tuned model, with improved emotional depth and reduced output
variance, demonstrating its potential for adaptive, personalized cognition.

</details>


### [232] [Outraged AI: Large language models prioritise emotion over cost in fairness enforcement](https://arxiv.org/abs/2510.17880)
*Hao Liu,Yiqing Dai,Haotian Tan,Yu Lei,Yujia Zhou,Zhen Wu*

Main category: cs.CL

TL;DR: 研究对比大语言模型（LLMs）和人类在利他第三方惩罚中的决策，发现LLMs会用情感引导惩罚，机制与人类有差异，提出LLMs应结合情感与情境推理实现类人情绪智能。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否像人类一样用情感指导决策。

Method: 大规模对比4068个LLM代理和1159名成年人在796100个决策中的表现。

Result: LLMs用情感引导惩罚，有时比人类更强，但机制有别，推理模型比基础模型更接近人类行为。

Conclusion: 首次证明LLMs存在情感引导的道德决策，指出其成本校准和公平判断不足，建议结合情感与情境推理。

Abstract: Emotions guide human decisions, but whether large language models (LLMs) use
emotion similarly remains unknown. We tested this using altruistic third-party
punishment, where an observer incurs a personal cost to enforce fairness, a
hallmark of human morality and often driven by negative emotion. In a
large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100
decisions, LLMs used emotion to guide punishment, sometimes even more strongly
than humans did: Unfairness elicited stronger negative emotion that led to more
punishment; punishing unfairness produced more positive emotion than accepting;
and critically, prompting self-reports of emotion causally increased
punishment. However, mechanisms diverged: LLMs prioritized emotion over cost,
enforcing norms in an almost all-or-none manner with reduced cost sensitivity,
whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini,
DeepSeek-R1) were more cost-sensitive and closer to human behavior than
foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven.
These findings provide the first causal evidence of emotion-guided moral
decisions in LLMs and reveal deficits in cost calibration and nuanced fairness
judgements, reminiscent of early-stage human responses. We propose that LLMs
progress along a trajectory paralleling human development; future models should
integrate emotion with context-sensitive reasoning to achieve human-like
emotional intelligence.

</details>


### [233] [POPI: Personalizing LLMs via Optimized Natural Language Preference Inference](https://arxiv.org/abs/2510.17881)
*Yizhuo Chen,Xin Liu,Ruijie Wang,Zheng Li,Pei Chen,Changlong Yu,Priyanka Nigam,Meng Jiang,Bing Yin*

Main category: cs.CL

TL;DR: 提出POPI框架解决大语言模型个性化问题，实验证明其提升个性化准确率并减少上下文开销，且可无缝迁移到现成大模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型用户体验不一致，现有对齐技术忽视个体差异，朴素个性化策略计算成本高，上下文方法效率低且有噪声。

Method: 提出POPI框架，引入偏好推理模型将用户信号提炼为自然语言摘要，用强化学习在统一目标下联合优化偏好推理和个性化生成。

Result: 在四个个性化基准测试中，POPI持续提升个性化准确率，大幅降低上下文开销，优化后的摘要可无缝迁移到现成大模型。

Conclusion: POPI是解决大语言模型个性化问题的有效框架，能在不更新权重的情况下实现即插即用的个性化。

Abstract: Large language models (LLMs) achieve strong benchmark performance, yet user
experiences remain inconsistent due to diverse preferences in style, tone, and
reasoning mode. Nevertheless, existing alignment techniques such as
reinforcement learning from human feedback (RLHF) or Direct Preference
Optimization (DPO) largely optimize toward population-level averages and
overlook individual variation. Naive personalization strategies like per-user
fine-tuning are computationally prohibitive, and in-context approaches that
prepend raw user signals often suffer from inefficiency and noise. To address
these challenges, we propose POPI, a general framework that introduces a
preference inference model to distill heterogeneous user signals into concise
natural language summaries. These summaries act as transparent, compact, and
transferable personalization representations that condition a shared generation
model to produce personalized responses. POPI jointly optimizes both preference
inference and personalized generation under a unified objective using
reinforcement learning, ensuring summaries maximally encode useful preference
information. Extensive experiments across four personalization benchmarks
demonstrate that POPI consistently improves personalization accuracy while
reducing context overhead by a large margin. Moreover, optimized summaries
seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play
personalization without weight updates.

</details>


### [234] [JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs](https://arxiv.org/abs/2510.17918)
*Junlan Feng,Fanyu Meng,Chong Long,Pengyu Cong,Duqing Wang,Yan Zheng,Yuyao Zhang,Xuanchang Gao,Ye Yuan,Yunfei Ma,Zhijie Ren,Fan Yang,Na Wu,Di Jin,Chao Deng*

Main category: cs.CL

TL;DR: 文章聚焦提升大语言模型预训练数据以增强其可信度和安全性，提出结合世界上下文增强数据的方法，经实验，JT - Safe - 35B在评估基准上表现更优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的幻觉和可信度问题源于预训练，现有预训练数据存在事实错误、缺乏真实世界知识等问题，需提升数据质量来增强模型可信度和安全性。

Method: 提出用世界上下文增强预训练数据，增加反映工业场景的数据，将其称为DWC；用1.5万亿DWC令牌继续预训练JT - 35B - Base的早期检查点，并引入后训练程序。

Result: 与类似规模的Qwen模型相比，JT - Safe - 35B在安全和可信度评估基准上平均性能提升1.79%，且仅用6.2万亿令牌进行预训练。

Conclusion: 通过结合世界上下文增强预训练数据，可减少模型训练的不确定性，提升模型的安全性和可信度。

Abstract: The hallucination and credibility concerns of large language models (LLMs)
are global challenges that the industry is collectively addressing. Recently, a
significant amount of advances have been made on post-training and inference
techniques to mitigate these challenges. However, it is widely agreed that
unsafe and hallucinations of LLMs intrinsically originate from pre-training,
involving pre-training data and the next-token prediction learning mechanism.
In this paper, we focus on enhancing pre-training data to improve the
trustworthiness and safety of LLMs. Since the data is vast, it's almost
impossible to entirely purge the data of factual errors, logical
inconsistencies, or distributional biases. Moreover, the pre-training data lack
grounding in real-world knowledge. Each piece of data is treated as a sequence
of tokens rather than as a representation of a part of the world. To overcome
these issues, we propose approaches to enhancing our pre-training data with its
context in the world and increasing a substantial amount of data reflecting
industrial scenarios. We argue that most source data are created by the authors
for specific purposes in a certain spatial-temporal context. They have played a
role in the real world. By incorporating related world context information, we
aim to better anchor pre-training data within real-world scenarios, thereby
reducing uncertainty in model training and enhancing the model's safety and
trustworthiness. We refer to our Data with World Context as DWC. We continue
pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC
tokens. We introduce our post-training procedures to activate the potentials of
DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an
average performance improvement of 1.79% on the Safety and Trustworthy
evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.

</details>


### [235] [CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections](https://arxiv.org/abs/2510.17921)
*Keuntae Kim,Eunhye Jeong,Sehyeon Lee,Seohee Yoon,Yong Suk Choi*

Main category: cs.CL

TL;DR: 本文提出CLAWS方法评估大语言模型推理创造力，在数学问题上验证效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理能力提升但创造力评估被忽视，存在定义范围难和需人工评估的挑战。

Method: 提出CLAWS方法，利用提示部分和输出的注意力权重，将数学解决方案分为典型、创造性和幻觉类别，无需人工评估。

Result: CLAWS在五个7 - 8B数学强化学习模型上优于五种现有白盒检测方法。

Conclusion: CLAWS方法能有效解决大语言模型推理创造力评估的挑战。

Abstract: Recent advances in enhancing the reasoning ability of large language models
(LLMs) have been remarkably successful. LLMs trained with reinforcement
learning (RL) for reasoning demonstrate strong performance in challenging tasks
such as mathematics and coding, even with relatively small model sizes.
However, despite these improvements in task accuracy, the assessment of
creativity in LLM generations has been largely overlooked in reasoning tasks,
in contrast to writing tasks. The lack of research on creativity assessment in
reasoning primarily stems from two challenges: (1) the difficulty of defining
the range of creativity, and (2) the necessity of human evaluation in the
assessment process. To address these challenges, we propose CLAWS, a method
that defines and classifies mathematical solutions into typical, creative, and
hallucinated categories without human evaluation, by leveraging attention
weights across prompt sections and output. CLAWS outperforms five existing
white-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden
Score, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen,
Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems
collected from 181 math contests (AJHSME, AMC, AIME).

</details>


### [236] [Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models](https://arxiv.org/abs/2510.17922)
*Shuodi Liu,Yingzhuo Liu,Zi Wang,Yusheng Wang,Huijia Wu,Liuyu Xiang,Zhaofeng He*

Main category: cs.CL

TL;DR: 研究对任务分解进行调研，分析影响因素并提出Select - Then - Decompose策略，在多基准测试中表现出性能与成本的最优平衡。


<details>
  <summary>Details</summary>
Motivation: 现有任务分解方法常忽略性能与成本的权衡，需进一步研究。

Method: 先对任务分解进行全面调研，确定六种分类方案；再对影响性能和成本的三个因素进行实证分析；最后提出Select - Then - Decompose策略。

Result: Select - Then - Decompose策略在多基准测试中始终处于帕累托前沿，实现性能与成本的最优平衡。

Conclusion: 提出的Select - Then - Decompose策略有效解决了任务分解中性能与成本的权衡问题。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning and
planning capabilities, driving extensive research into task decomposition.
Existing task decomposition methods focus primarily on memory, tool usage, and
feedback mechanisms, achieving notable success in specific domains, but they
often overlook the trade-off between performance and cost. In this study, we
first conduct a comprehensive investigation on task decomposition, identifying
six categorization schemes. Then, we perform an empirical analysis of three
factors that influence the performance and cost of task decomposition:
categories of approaches, characteristics of tasks, and configuration of
decomposition and execution models, uncovering three critical insights and
summarizing a set of practical principles. Building on this analysis, we
propose the Select-Then-Decompose strategy, which establishes a closed-loop
problem-solving process composed of three stages: selection, execution, and
verification. This strategy dynamically selects the most suitable decomposition
approach based on task characteristics and enhances the reliability of the
results through a verification module. Comprehensive evaluations across
multiple benchmarks show that the Select-Then-Decompose consistently lies on
the Pareto frontier, demonstrating an optimal balance between performance and
cost. Our code is publicly available at
https://github.com/summervvind/Select-Then-Decompose.

</details>


### [237] [Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs](https://arxiv.org/abs/2510.17924)
*Yehor Tereshchenko,Mika Hämäläinen*

Main category: cs.CL

TL;DR: 对在线游戏聊天自动毒性检测的NLP方法进行综合比较分析，提出混合审核系统架构，发现微调DistilBERT在准确性和成本间取得最优平衡。


<details>
  <summary>Details</summary>
Motivation: 为动态在线游戏环境部署经济高效的内容审核系统提供依据，评估不同NLP方法在自动毒性检测中的表现。

Method: 评估传统机器学习嵌入模型、大语言模型零样本和少样本提示、微调transformer模型和检索增强生成方法；从分类准确性、处理速度和计算成本三个维度评估；提出混合审核系统架构。

Result: 不同方法性能差异显著，微调DistilBERT在准确性 - 成本权衡上最优。

Conclusion: 研究结果为在动态在线游戏环境中部署经济高效的内容审核系统提供了实证依据。

Abstract: This paper presents a comprehensive comparative analysis of Natural Language
Processing (NLP) methods for automated toxicity detection in online gaming
chats. Traditional machine learning models with embeddings, large language
models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer
models, and retrieval-augmented generation (RAG) approaches are evaluated. The
evaluation framework assesses three critical dimensions: classification
accuracy, processing speed, and computational costs. A hybrid moderation system
architecture is proposed that optimizes human moderator workload through
automated detection and incorporates continuous learning mechanisms. The
experimental results demonstrate significant performance variations across
methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs.
The findings provide empirical evidence for deploying cost-effective, efficient
content moderation systems in dynamic online gaming environments.

</details>


### [238] [Diagnosing Representation Dynamics in NER Model Extension](https://arxiv.org/abs/2510.17930)
*Xirui Zhang,Philippe de La Chevasnerie,Benoit Fabre*

Main category: cs.CL

TL;DR: 研究在嘈杂口语数据中扩展命名实体识别（NER）模型到新PII实体，发现联合微调BERT模型影响小，揭示LOC易受影响及'O'标签漂移问题。


<details>
  <summary>Details</summary>
Motivation: 满足在嘈杂口语数据中将NER模型扩展到新PII实体的需求。

Method: 采用增量学习设置作为诊断工具，测量语义漂移。

Result: 发现LOC因与新PII表示重叠易受影响，存在'反向O标签表示漂移'，解冻'O'标签分类器可解决。

Conclusion: 对NER模型适应进行机制诊断，强调特征独立性、表示重叠和'O'标签可塑性。

Abstract: Extending Named Entity Recognition (NER) models to new PII entities in noisy
spoken-language data is a common need. We find that jointly fine-tuning a BERT
model on standard semantic entities (PER, LOC, ORG) and new pattern-based PII
(EMAIL, PHONE) results in minimal degradation for original classes. We
investigate this "peaceful coexistence," hypothesizing that the model uses
independent semantic vs. morphological feature mechanisms.
  Using an incremental learning setup as a diagnostic tool, we measure semantic
drift and find two key insights. First, the LOC (location) entity is uniquely
vulnerable due to a representation overlap with new PII, as it shares
pattern-like features (e.g., postal codes). Second, we identify a "reverse
O-tag representation drift." The model, initially trained to map PII patterns
to 'O', blocks new learning. This is resolved only by unfreezing the 'O' tag's
classifier, allowing the background class to adapt and "release" these
patterns. This work provides a mechanistic diagnosis of NER model adaptation,
highlighting feature independence, representation overlap, and 'O' tag
plasticity.

</details>


### [239] [AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM](https://arxiv.org/abs/2510.17934)
*Haoyu Huang,Hong Ting Tsang,Jiaxin Bai,Xi Peng,Gong Zhang,Yangqiu Song*

Main category: cs.CL

TL;DR: 提出参数化知识集成方法AtlasKV，以低成本用十亿规模知识图谱增强大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法作为非参数知识集成范式，大规模知识增强时会因搜索成本高和上下文长导致推理延迟大。

Method: 提出AtlasKV方法，引入KG2KV和HiKVP以亚线性时间和内存复杂度将知识图谱三元组集成到大型语言模型中。

Result: 能以很少的GPU内存成本（如少于20GB显存）用十亿规模知识图谱增强大语言模型。

Conclusion: 该方法可扩展、有效且通用，利用大语言模型的注意力机制保持强大的知识基础和泛化性能，适应新知识时无需外部检索器、长上下文先验或重新训练。

Abstract: Retrieval-augmented generation (RAG) has shown some success in augmenting
large language models (LLMs) with external knowledge. However, as a
non-parametric knowledge integration paradigm for LLMs, RAG methods heavily
rely on external retrieval modules and the retrieved textual context prior.
Especially for very large scale knowledge augmentation, they would introduce
substantial inference latency due to expensive searches and much longer
relevant context. In this paper, we propose a parametric knowledge integration
method, called \textbf{AtlasKV}, a scalable, effective, and general way to
augment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using
very little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we
introduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with
sub-linear time and memory complexity. It maintains strong knowledge grounding
and generalization performance using the LLMs' inherent attention mechanism,
and requires no external retrievers, long context priors, or retraining when
adapting to new knowledge.

</details>


### [240] [Believe It or Not: How Deeply do LLMs Believe Implanted Facts?](https://arxiv.org/abs/2510.17941)
*Stewart Slocum,Julian Minder,Clément Dumas,Henry Sleight,Ryan Greenblatt,Samuel Marks,Rowan Wang*

Main category: cs.CL

TL;DR: 本文开发框架衡量大语言模型知识编辑中植入知识的信念深度，评估不同技术效果，指出SDF常能成功植入类似真实知识的信念，但对违背常识的知识植入效果不佳。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否真的“相信”知识编辑植入的事实，为知识编辑技术在现实应用中的部署提供严格评估。

Method: 开发衡量信念深度的框架，从知识泛化、抗自我审查和挑战、与真实知识表示相似性三方面衡量。

Result: 简单提示和机械编辑技术难以深度植入知识，SDF常能成功植入类似真实知识的信念，但对违背基本世界知识的植入信念较脆弱。

Conclusion: 引入信念深度的可衡量标准，使知识编辑在现实应用中的严格评估成为可能。

Abstract: Knowledge editing techniques promise to implant new factual knowledge into
large language models (LLMs). But do LLMs really believe these facts? We
develop a framework to measure belief depth and use it to evaluate the success
of knowledge editing techniques. We operationalize belief depth as the extent
to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi
estimates several logical steps removed), 2) is robust to self-scrutiny and
direct challenge, and 3) is represented similarly to genuine knowledge (as
measured by linear probes). Our evaluations show that simple prompting and
mechanistic editing techniques fail to implant knowledge deeply. In contrast,
Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated
documents consistent with a fact - often succeeds at implanting beliefs that
behave similarly to genuine knowledge. However, SDF's success is not universal,
as implanted beliefs that contradict basic world knowledge are brittle and
representationally distinct from genuine knowledge. Overall, our work
introduces measurable criteria for belief depth and enables the rigorous
evaluation necessary for deploying knowledge editing in real-world
applications.

</details>


### [241] [SimBA: Simplifying Benchmark Analysis Using Performance Matrices Alone](https://arxiv.org/abs/2510.17998)
*Nishant Subramani,Alfredo Gomez,Mona Diab*

Main category: cs.CL

TL;DR: 提出SimBA框架简化基准测试分析，应用于三个流行语言模型基准测试，证明其有效性，有助于模型开发者和数据集创建者。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型在大型基准测试上的评估结果难以理解，尤其是在模型选择方面，需要简化基准测试分析。

Method: 提出包含stalk、prowl、pounce三个阶段的SimBA框架，对数据集和模型进行比较、发现代表性子集并预测模型在保留集上的性能。

Result: 在三个基准测试中，数据集和模型相互关联；用少量数据集可达到至少95%的覆盖水平；使用代表性子集能保留模型排名并近乎零均方误差预测模型性能。

Conclusion: SimBA可帮助模型开发者提高训练效率，帮助数据集创建者验证新数据集。

Abstract: Modern language models are evaluated on large benchmarks, which are difficult
to make sense of, especially for model selection. Looking at the raw evaluation
numbers themselves using a model-centric lens, we propose SimBA, a three phase
framework to Simplify Benchmark Analysis. The three phases of SimBA are: stalk,
where we conduct dataset & model comparisons, prowl, where we discover a
representative subset, and pounce, where we use the representative subset to
predict performance on a held-out set of models. Applying SimBA to three
popular LM benchmarks: HELM, MMLU, and BigBenchLite reveals that across all
three benchmarks, datasets and models relate strongly to one another (stalk).
We develop an representative set discovery algorithm which covers a benchmark
using raw evaluation scores alone. Using our algorithm, we find that with 6.25%
(1/16), 1.7% (1/58), and 28.4% (21/74) of the datasets for HELM, MMLU, and
BigBenchLite respectively, we achieve coverage levels of at least 95% (prowl).
Additionally, using just these representative subsets, we can both preserve
model ranks and predict performance on a held-out set of models with near zero
mean-squared error (pounce). Taken together, SimBA can help model developers
improve efficiency during model training and dataset creators validate whether
their newly created dataset differs from existing datasets in a benchmark. Our
code is open source, available at https://github.com/nishantsubramani/simba.

</details>


### [242] [Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution](https://arxiv.org/abs/2510.18019)
*Asim Mohamed,Martin Gubri*

Main category: cs.CL

TL;DR: 现有多语言水印方法在中低资源语言中对翻译攻击鲁棒性不足，提出基于回译的检测方法STEAM解决该问题，在17种语言上有显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前多语言水印方法虽声称有跨语言鲁棒性，但仅在高资源语言上评估，在中低资源语言中对翻译攻击不鲁棒。

Method: 引入基于回译的检测方法STEAM，兼容任何水印方法。

Result: 在17种语言上平均AUC提升0.19，TPR@1%提升40个百分点。

Conclusion: STEAM为跨多种语言实现更公平的水印提供了简单且鲁棒的途径。

Abstract: Multilingual watermarking aims to make large language model (LLM) outputs
traceable across languages, yet current methods still fall short. Despite
claims of cross-lingual robustness, they are evaluated only on high-resource
languages. We show that existing multilingual watermarking methods are not
truly multilingual: they fail to remain robust under translation attacks in
medium- and low-resource languages. We trace this failure to semantic
clustering, which fails when the tokenizer vocabulary contains too few
full-word tokens for a given language. To address this, we introduce STEAM, a
back-translation-based detection method that restores watermark strength lost
through translation. STEAM is compatible with any watermarking method, robust
across different tokenizers and languages, non-invasive, and easily extendable
to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17
languages, STEAM provides a simple and robust path toward fairer watermarking
across diverse languages.

</details>


### [243] [From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models](https://arxiv.org/abs/2510.18030)
*Ziyan Wang,Enmao Diao,Qi Le,Pu Wang,Minwoo Lee,Shu-ping Yeh,Evgeny Stupachenko,Hao Feng,Li Yang*

Main category: cs.CL

TL;DR: 论文提出全局迭代结构化剪枝方法GISP，可在大语言模型中有效移除注意力头和MLP通道，实验证明能降低困惑度、提高下游准确率。


<details>
  <summary>Details</summary>
Motivation: 现有局部结构化剪枝范式与任务无关，难以利用特定任务校准信号，下游收益有限，因此重新研究全局结构化剪枝。

Method: 提出GISP方法，基于一阶、基于损失的重要权重在结构层面聚合并进行块归一化移除注意力头和MLP通道，采用迭代剪枝策略。

Result: 在多个大语言模型上，GISP持续降低WikiText - 2困惑度、提高下游准确率，在特定稀疏度和任务上效果显著。

Conclusion: GISP是一种有效的大语言模型结构化剪枝方法，支持特定任务目标，能实现高效部署。

Abstract: Structured pruning is a practical approach to deploying large language models
(LLMs) efficiently, as it yields compact, hardware-friendly architectures.
However, the dominant local paradigm is task-agnostic: by optimizing layer-wise
reconstruction rather than task objectives, it tends to preserve perplexity or
generic zero-shot behavior but fails to capitalize on modest task-specific
calibration signals, often yielding limited downstream gains. We revisit global
structured pruning and present GISP-Global Iterative Structured Pruning-a
post-training method that removes attention heads and MLP channels using
first-order, loss-based important weights aggregated at the structure level
with block-wise normalization. An iterative schedule, rather than one-shot
pruning, stabilizes accuracy at higher sparsity and mitigates perplexity
collapse without requiring intermediate fine-tuning; the pruning trajectory
also forms nested subnetworks that support a "prune-once, deploy-many"
workflow. Furthermore, because importance is defined by a model-level loss,
GISP naturally supports task-specific objectives; we instantiate perplexity for
language modeling and a margin-based objective for decision-style tasks.
Extensive experiments show that across Llama2-7B/13B, Llama3-8B, and
Mistral-0.3-7B, GISP consistently lowers WikiText-2 perplexity and improves
downstream accuracy, with especially strong gains at 40-50% sparsity; on
DeepSeek-R1-Distill-Llama-3-8B with GSM8K, task-aligned calibration
substantially boosts exact-match accuracy.

</details>


### [244] [Language Models as Semantic Augmenters for Sequential Recommenders](https://arxiv.org/abs/2510.18046)
*Mahsa Valizadeh,Xiangjue Dong,Rui Tuo,James Caverlee*

Main category: cs.CL

TL;DR: 提出LaMAR框架利用大语言模型丰富序列数据语义，应用于基准任务提升性能，代表新的数据中心范式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理序列交互数据时，语义上下文有限或缺失会影响性能。

Method: 引入LaMAR框架，在少样本设置下利用大语言模型从现有元数据推断潜在语义，生成辅助上下文信号。

Result: 将生成的信号集成到基准序列建模任务中，性能持续提升，信号具有高语义新颖性和多样性。

Conclusion: 该工作代表新的数据中心范式，为半自动创建训练数据和语言资源提供新方法。

Abstract: Large Language Models (LLMs) excel at capturing latent semantics and
contextual relationships across diverse modalities. However, in modeling user
behavior from sequential interaction data, performance often suffers when such
semantic context is limited or absent. We introduce LaMAR, a LLM-driven
semantic enrichment framework designed to enrich such sequences automatically.
LaMAR leverages LLMs in a few-shot setting to generate auxiliary contextual
signals by inferring latent semantic aspects of a user's intent and item
relationships from existing metadata. These generated signals, such as inferred
usage scenarios, item intents, or thematic summaries, augment the original
sequences with greater contextual depth. We demonstrate the utility of this
generated resource by integrating it into benchmark sequential modeling tasks,
where it consistently improves performance. Further analysis shows that
LLM-generated signals exhibit high semantic novelty and diversity, enhancing
the representational capacity of the downstream models. This work represents a
new data-centric paradigm where LLMs serve as intelligent context generators,
contributing a new method for the semi-automatic creation of training data and
language resources.

</details>


### [245] [Automatic Prompt Generation via Adaptive Selection of Prompting Techniques](https://arxiv.org/abs/2510.18162)
*Yohei Ikenoue,Hitomi Tashiro,Shigeru Kuroyanagi*

Main category: cs.CL

TL;DR: 提出一种自适应选择提示技术并自动生成高质量提示的方法，在23个任务上表现优于标准提示和现有工具，为简化和标准化提示创建奠定基础。


<details>
  <summary>Details</summary>
Motivation: 提示工程设计需要专业知识和对目标任务的深入理解，为解决这一挑战而开展研究。

Method: 构建一个将任务集群与相应提示技术关联的知识库，根据用户输入的任务描述分配到相关任务集群并动态生成提示。

Result: 在BIG - Bench Extra Hard的23个任务上实验表明，该方法在算术和调和平均得分上优于标准提示和现有自动提示生成工具。

Conclusion: 该研究为简化和标准化提示创建奠定基础，使非专家能有效利用大语言模型。

Abstract: Prompt engineering is crucial for achieving reliable and effective outputs
from large language models (LLMs), but its design requires specialized
knowledge of prompting techniques and a deep understanding of target tasks. To
address this challenge, we propose a novel method that adaptively selects
task-appropriate prompting techniques based on users' abstract task
descriptions and automatically generates high-quality prompts without relying
on pre-existing templates or frameworks. The proposed method constructs a
knowledge base that associates task clusters, characterized by semantic
similarity across diverse tasks, with their corresponding prompting techniques.
When users input task descriptions, the system assigns them to the most
relevant task cluster and dynamically generates prompts by integrating
techniques drawn from the knowledge base. An experimental evaluation of the
proposed method on 23 tasks from BIG-Bench Extra Hard (BBEH) demonstrates
superior performance compared with standard prompts and existing automatic
prompt-generation tools, as measured by both arithmetic and harmonic mean
scores. This research establishes a foundation for streamlining and
standardizing prompt creation, enabling non-experts to effectively leverage
LLMs.

</details>


### [246] [Extracting Rule-based Descriptions of Attention Features in Transformers](https://arxiv.org/abs/2510.18148)
*Dan Friedman,Adithya Bhaskar,Alexander Wettig,Danqi Chen*

Main category: cs.CL

TL;DR: 文章提出用基于规则的描述解释模型特征，提取SAE特征规则，应用于GPT - 2 small并给出初步分类。


<details>
  <summary>Details</summary>
Motivation: 现有特征解释需主观检查示例，存在局限性，需新的解决方案。

Method: 提取注意力层输出训练的SAE特征的基于规则描述，研究三种规则类型，提出自动提取规则的方法并应用于GPT - 2 small。

Result: 多数特征可用约100条skip - gram规则描述，第一层就有大量absence规则，还发现一些counting规则。

Conclusion: 为基于规则的特征描述的未来研究奠定基础，定义规则、展示提取方法并给出初步分类。

Abstract: Mechanistic interpretability strives to explain model behavior in terms of
bottom-up primitives. The leading paradigm is to express hidden states as a
sparse linear combination of basis vectors, called features. However, this only
identifies which text sequences (exemplars) activate which features; the actual
interpretation of features requires subjective inspection of these exemplars.
This paper advocates for a different solution: rule-based descriptions that
match token patterns in the input and correspondingly increase or decrease the
likelihood of specific output tokens. Specifically, we extract rule-based
descriptions of SAE features trained on the outputs of attention layers. While
prior work treats the attention layers as an opaque box, we describe how it may
naturally be expressed in terms of interactions between input and output
features, of which we study three types: (1) skip-gram rules of the form
"[Canadian city]... speaks --> English", (2) absence rules of the form
"[Montreal]... speaks -/-> English," and (3) counting rules that toggle only
when the count of a word exceeds a certain value or the count of another word.
Absence and counting rules are not readily discovered by inspection of
exemplars, where manual and automatic descriptions often identify misleading or
incomplete explanations. We then describe a simple approach to extract these
types of rules automatically from a transformer, and apply it to GPT-2 small.
We find that a majority of features may be described well with around 100
skip-gram rules, though absence rules are abundant even as early as the first
layer (in over a fourth of features). We also isolate a few examples of
counting rules. This paper lays the groundwork for future research into
rule-based descriptions of features by defining them, showing how they may be
extracted, and providing a preliminary taxonomy of some of the behaviors they
represent.

</details>


### [247] [Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge](https://arxiv.org/abs/2510.18196)
*Yoshinari Fujinuma*

Main category: cs.CL

TL;DR: 指出大语言模型作为评判者直接评估有分数范围偏差问题，用对比解码缓解偏差，平均提升与人类评判的斯皮尔曼相关性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型作为评判者直接评估结果可靠性的问题，特别是分数范围偏差问题。

Method: 采用对比解码方法来缓解分数范围偏差。

Result: 在不同分数范围内，与人类评判的斯皮尔曼相关性平均相对提升达11.3%。

Conclusion: 对比解码能有效缓解大语言模型直接评估中的分数范围偏差问题。

Abstract: Large Language Models (LLMs) are commonly used as evaluators in various
applications, but the reliability of the outcomes remains a challenge. One such
challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores
from a specified range without any references. We first show that this
challenge stems from LLM judge outputs being associated with score range bias,
i.e., LLM judge outputs are highly sensitive to pre-defined score ranges,
preventing the search for optimal score ranges. We also show that similar
biases exist among models from the same family. We then mitigate this bias
through contrastive decoding, achieving up to 11.3% relative improvement on
average in Spearman correlation with human judgments across different score
ranges.

</details>


### [248] [DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization](https://arxiv.org/abs/2510.18257)
*Tao Tao,Guanghui Zhu,Lang Guo,Hongyi Chen,Chunfeng Yuan,Yihua Huang*

Main category: cs.CL

TL;DR: 提出DelvePO框架优化提示，实验显示其优于SOTA方法，有跨任务有效性和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化工作依赖随机重写、聚焦特定因素，易陷入局部最优且性能不稳定、可迁移性差。

Method: 提出DelvePO框架，将提示解耦为不同组件，引入工作记忆引导新提示生成。

Result: 在不同任务和不同类型大模型上实验，DelvePO在相同设置下始终优于先前SOTA方法。

Conclusion: DelvePO有效且具有跨任务可迁移性。

Abstract: Prompt Optimization has emerged as a crucial approach due to its capabilities
in steering Large Language Models to solve various tasks. However, current
works mainly rely on the random rewriting ability of LLMs, and the optimization
process generally focus on specific influencing factors, which makes it easy to
fall into local optimum. Besides, the performance of the optimized prompt is
often unstable, which limits its transferability in different tasks. To address
the above challenges, we propose $\textbf{DelvePO}$
($\textbf{D}$irection-Guid$\textbf{e}$d Se$\textbf{l}$f-E$\textbf{v}$olving
Framework for Fl$\textbf{e}$xible $\textbf{P}$rompt $\textbf{O}$ptimization), a
task-agnostic framework to optimize prompts in self-evolve manner. In our
framework, we decouple prompts into different components that can be used to
explore the impact that different factors may have on various tasks. On this
basis, we introduce working memory, through which LLMs can alleviate the
deficiencies caused by their own uncertainties and further obtain key insights
to guide the generation of new prompts. Extensive experiments conducted on
different tasks covering various domains for both open- and closed-source LLMs,
including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini.
Experimental results show that DelvePO consistently outperforms previous SOTA
methods under identical experimental settings, demonstrating its effectiveness
and transferability across different tasks.

</details>


### [249] [Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs](https://arxiv.org/abs/2510.18279)
*Yanhong Li,Zixuan Lan,Jiawei Zhou*

Main category: cs.CL

TL;DR: 研究表明将文本作为图像输入解码器大语言模型可在不降低性能的情况下压缩输入，减少所需解码器令牌数量。


<details>
  <summary>Details</summary>
Motivation: 探讨能否通过将文本作为图像输入大语言模型来压缩文本输入，减少令牌使用并保持性能。

Method: 将长文本输入渲染为单个图像并直接提供给模型。

Result: 在RULER和CNN/DailyMail两个基准测试中，文本转图像方法大幅节省了令牌（常接近一半）且不降低任务性能。

Conclusion: 视觉文本表示是一种实用且有效的解码器大语言模型输入压缩形式。

Abstract: Large language models (LLMs) and their multimodal variants can now process
visual inputs, including images of text. This raises an intriguing question:
can we compress textual inputs by feeding them as images to reduce token usage
while preserving performance? In this paper, we show that visual text
representations are a practical and surprisingly effective form of input
compression for decoder LLMs. We exploit the idea of rendering long text inputs
as a single image and provide it directly to the model. This leads to
dramatically reduced number of decoder tokens required, offering a new form of
input compression. Through experiments on two distinct benchmarks RULER
(long-context retrieval) and CNN/DailyMail (document summarization) we
demonstrate that this text-as-image method yields substantial token savings
(often nearly half) without degrading task performance.

</details>


### [250] [From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering](https://arxiv.org/abs/2510.18297)
*Lei Li,Xiao Zhou,Yingying Zhang,Xian Wu*

Main category: cs.CL

TL;DR: 提出MedRGAG框架用于医学问答，结合检索与生成知识，实验显示效果优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学问答方法RAG有检索噪声或不完整问题，GAG有生成幻觉或不准确信息问题，影响答案可靠性。

Method: 提出MedRGAG框架，包含知识引导上下文补全（KGCC）和知识感知文档选择（KADS）两个模块。

Result: 在五个医学问答基准测试中，MedRGAG比MedRAG提高12.5%，比MedGENIE提高4.5%。

Conclusion: 统一检索和生成用于知识密集型推理是有效的，代码和数据公开。

Abstract: Medical question answering (QA) requires extensive access to domain-specific
knowledge. A promising direction is to enhance large language models (LLMs)
with external knowledge retrieved from medical corpora or parametric knowledge
stored in model parameters. Existing approaches typically fall into two
categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning
on externally retrieved evidence, and Generation-Augmented Generation (GAG),
which depends solely on the models internal knowledge to generate contextual
documents. However, RAG often suffers from noisy or incomplete retrieval, while
GAG is vulnerable to hallucinated or inaccurate information due to
unconstrained generation. Both issues can mislead reasoning and undermine
answer reliability. To address these challenges, we propose MedRGAG, a unified
retrieval-generation augmented framework that seamlessly integrates external
and parametric knowledge for medical QA. MedRGAG comprises two key modules:
Knowledge-Guided Context Completion (KGCC), which directs the generator to
produce background documents that complement the missing knowledge revealed by
retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively
selects an optimal combination of retrieved and generated documents to form
concise yet comprehensive evidence for answer generation. Extensive experiments
on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5%
improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the
effectiveness of unifying retrieval and generation for knowledge-intensive
reasoning. Our code and data are publicly available at
https://anonymous.4open.science/r/MedRGAG

</details>


### [251] [ECG-LLM -- training and evaluation of domain-specific large language models for electrocardiography](https://arxiv.org/abs/2510.18339)
*Lara Ahrens,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.CL

TL;DR: 研究心电图领域中领域适应的大语言模型，发现微调模型和RAG与专有模型有竞争力，评估方法有性能异质性。


<details>
  <summary>Details</summary>
Motivation: 领域适应大语言模型在医疗应用有优势，但最佳适应策略、评估方法和与通用模型性能对比不明确。

Method: 在心电图领域微调开放权重模型，采用多层评估框架对比微调模型、RAG和通用模型Claude Sonnet 3.7。

Result: 微调的Llama 3.1 70B在多项选择题评估和自动文本指标表现出色；人类专家评估中Claude 3.7和RAG更适合复杂查询；微调模型大幅优于基础模型。

Conclusion: 评估方法有性能异质性，领域特定适应通过微调与RAG可实现与专有模型竞争，支持隐私保护、本地部署临床解决方案的可行性。

Abstract: Domain-adapted open-weight large language models (LLMs) offer promising
healthcare applications, from queryable knowledge bases to multimodal
assistants, with the crucial advantage of local deployment for privacy
preservation. However, optimal adaptation strategies, evaluation methodologies,
and performance relative to general-purpose LLMs remain poorly characterized.
We investigated these questions in electrocardiography, an important area of
cardiovascular medicine, by finetuning open-weight models on domain-specific
literature and implementing a multi-layered evaluation framework comparing
finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7
as a representative general-purpose model. Finetuned Llama 3.1 70B achieved
superior performance on multiple-choice evaluations and automatic text metrics,
ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert
evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned
models significantly outperformed their base counterparts across nearly all
evaluation modes. Our findings reveal substantial performance heterogeneity
across evaluation methodologies, underscoring assessment complexity.
Nevertheless, domain-specific adaptation through finetuning and RAG achieves
competitive performance with proprietary models, supporting the viability of
privacy-preserving, locally deployable clinical solutions.

</details>


### [252] [MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models](https://arxiv.org/abs/2510.18383)
*ChangSu Choi,Hoyun Song,Dongyeon Kim,WooHyeon Jung,Minkyung Cho,Sunjin Park,NohHyeob Bae,Seona Yu,KyungTae Lim*

Main category: cs.CL

TL;DR: 提出MENTOR框架结合强化学习与教师引导蒸馏，解决小语言模型工具使用能力蒸馏问题，实验表明其提升了小语言模型的泛化和策略能力。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型的工具使用能力蒸馏到小语言模型存在挑战，现有监督微调泛化性差，标准强化学习因奖励稀疏难以有效引导。

Method: 提出MENTOR框架，结合强化学习与教师引导蒸馏，通过探索学习更通用策略，用教师参考轨迹构建密集复合奖励。

Result: 实验表明MENTOR相比监督微调与标准稀疏奖励强化学习，显著提升小语言模型的跨领域泛化和策略能力。

Conclusion: MENTOR框架能有效解决小语言模型工具使用能力蒸馏的问题，提升其性能。

Abstract: Distilling the tool-using capabilities of large language models (LLMs) into
smaller, more efficient small language models (SLMs) is a key challenge for
their practical application. The predominant approach, supervised fine-tuning
(SFT), suffers from poor generalization as it trains models to imitate a static
set of teacher trajectories rather than learn a robust methodology. While
reinforcement learning (RL) offers an alternative, the standard RL using sparse
rewards fails to effectively guide SLMs, causing them to struggle with
inefficient exploration and adopt suboptimal strategies. To address these
distinct challenges, we propose MENTOR, a framework that synergistically
combines RL with teacher-guided distillation. Instead of simple imitation,
MENTOR employs an RL-based process to learn a more generalizable policy through
exploration. In addition, to solve the problem of reward sparsity, it uses a
teacher's reference trajectory to construct a dense, composite teacher-guided
reward that provides fine-grained guidance. Extensive experiments demonstrate
that MENTOR significantly improves the cross-domain generalization and
strategic competence of SLMs compared to both SFT and standard sparse-reward RL
baselines.

</details>


### [253] [Bayesian Low-Rank Factorization for Robust Model Adaptation](https://arxiv.org/abs/2510.18723)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 本文探索用于语音基础模型的贝叶斯因式分解适配器，应用于Whisper模型并评估，结果显示能减少基础模型灾难性遗忘且适应损失小，凸显贝叶斯适应在微调语音基础模型时不牺牲泛化性的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语音基础模型需适应本地需求如代码切换，但直接微调有过拟合风险且会覆盖基础模型能力。

Method: 探索用于语音基础模型的贝叶斯因式分解适配器，将先验置于接近零的位置以实现更稀疏的适应矩阵，应用于Whisper模型并在不同多语言代码切换场景下评估。

Result: 仅产生最小的适应损失，显著减少基础模型的灾难性遗忘，与LoRA相比，在新领域仅下降4%的情况下实现54%的反向增益。

Conclusion: 在不牺牲泛化性的情况下，贝叶斯适应在微调语音基础模型方面是有效的。

Abstract: Large speech foundation models achieve strong performance across many
domains, but they often require adaptation to handle local needs such as
code-switching, where speakers mix languages within the same utterance. Direct
fine-tuning of these models risks overfitting to the target domain and
overwriting the broad capabilities of the base model. To address this
challenge, we explore Bayesian factorized adapters for speech foundation
models, which place priors near zero to achieve sparser adaptation matrices and
thereby retain general performance while adapting to specific domains. We apply
our approach to the Whisper model and evaluate on different multilingual
code-switching scenarios. Our results show only minimal adaptation loss while
significantly reducing catastrophic forgetting of the base model. Compared to
LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new
domain. These findings highlight the effectiveness of Bayesian adaptation for
fine-tuning speech foundation models without sacrificing generalization.

</details>


### [254] [Adapting Language Balance in Code-Switching Speech](https://arxiv.org/abs/2510.18724)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 大基础模型在代码切换测试用例上表现不佳，本文通过标注和突出代码切换点来提升模型鲁棒性，实验显示能减少替换错误。


<details>
  <summary>Details</summary>
Motivation: 大基础模型在代码切换测试用例上表现差，且数据不稀缺时，原因可能是代码切换时刻出现少，需要提供训练标签。

Method: 利用嵌入语言和主要语言的差异突出代码切换点，强调在这些位置学习。

Result: 在阿拉伯语和汉英实验中，模型能更准确预测切换位置，替换错误减少。

Conclusion: 该简单有效的可微替代方法能减轻生成时的上下文偏差，提升模型鲁棒性。

Abstract: Despite achieving impressive results on standard benchmarks, large
foundational models still struggle against code-switching test cases. When data
scarcity cannot be used as the usual justification for poor performance, the
reason may lie in the infrequent occurrence of code-switched moments, where the
embedding of the second language appears subtly. Instead of expecting the
models to learn this infrequency on their own, it might be beneficial to
provide the training process with labels. Evaluating model performance on
code-switching data requires careful localization of code-switching points
where recognition errors are most consequential, so that the analysis
emphasizes mistakes occurring at those moments. Building on this observation,
we leverage the difference between the embedded and the main language to
highlight those code-switching points and thereby emphasize learning at those
locations. This simple yet effective differentiable surrogate mitigates context
bias during generation -- the central challenge in code-switching -- thereby
improving the model's robustness. Our experiments with Arabic and
Chinese-English showed that the models are able to predict the switching places
more correctly, reflected by the reduced substitution error.

</details>


### [255] [Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation](https://arxiv.org/abs/2510.18731)
*Ming Li*

Main category: cs.CL

TL;DR: 针对大语言模型在多轮对话中的LiC问题，提出RLAAR框架，经评估显著缓解LiC性能衰减，提升弃权率，为构建多轮可靠可信大模型提供方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在单轮指令遵循表现良好，但多轮对话存在Lost - in - Conversation（LiC）问题，性能随信息逐步揭示而下降。

Method: 提出Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR)框架，采用能力门控课程逐步增加对话难度，使用多轮、策略内滚动和混合奖励系统，让模型平衡解决问题和明智弃权。

Result: 在LiC基准测试中，RLAAR显著缓解LiC性能衰减（从62.6%到75.1%），提高校准弃权率（从33.5%到73.4%）。

Conclusion: 研究结果为构建多轮可靠且值得信赖的大语言模型提供了实用方法。

Abstract: Large Language Models demonstrate strong capabilities in single-turn
instruction following but suffer from Lost-in-Conversation (LiC), a degradation
in performance as information is revealed progressively in multi-turn settings.
Motivated by the current progress on Reinforcement Learning with Verifiable
Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable
Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not
only to generate correct answers, but also to judge the solvability of
questions in the multi-turn conversation setting. Our approach employs a
competence-gated curriculum that incrementally increases dialogue difficulty
(in terms of instruction shards), stabilizing training while promoting
reliability. Using multi-turn, on-policy rollouts and a mixed-reward system,
RLAAR teaches models to balance problem-solving with informed abstention,
reducing premature answering behaviors that cause LiC. Evaluated on LiC
benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to
75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together,
these results provide a practical recipe for building multi-turn reliable and
trustworthy LLMs.

</details>


### [256] [Large language models for folktale type automation based on motifs: Cinderella case study](https://arxiv.org/abs/2510.18561)
*Tjaša Arčon,Marko Robnik-Šikonja,Polona Tratnik*

Main category: cs.CL

TL;DR: 构建民俗学大规模分析方法，用机器学习和NLP分析灰姑娘变体，结果显示大语言模型利于文本分析和跨语言比较


<details>
  <summary>Details</summary>
Motivation: 将人工智能方法应用于数字人文领域的民俗学研究

Method: 运用机器学习和自然语言处理，对大量灰姑娘变体进行主题自动检测，并用聚类和降维分析异同

Result: 大语言模型可检测故事中的复杂互动，实现大规模文本计算分析和跨语言比较

Conclusion: 构建的方法在民俗学大规模分析中有效，大语言模型有重要作用

Abstract: Artificial intelligence approaches are being adapted to many research areas,
including digital humanities. We built a methodology for large-scale analyses
in folkloristics. Using machine learning and natural language processing, we
automatically detected motifs in a large collection of Cinderella variants and
analysed their similarities and differences with clustering and dimensionality
reduction. The results show that large language models detect complex
interactions in tales, enabling computational analysis of extensive text
collections and facilitating cross-lingual comparisons.

</details>


### [257] [LightMem: Lightweight and Efficient Memory-Augmented Generation](https://arxiv.org/abs/2510.18866)
*Jizhan Fang,Xinle Deng,Haoming Xu,Ziyan Jiang,Yuqi Tang,Ziwen Xu,Shumin Deng,Yunzhi Yao,Mengru Wang,Shuofei Qiao,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 提出新内存系统LightMem，平衡性能与效率，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以有效利用历史交互信息，现有内存系统有时间和计算开销问题。

Method: 受人类记忆模型启发，将内存分为感官记忆、短期记忆、长期记忆三个互补阶段处理信息。

Result: 在LongMemEval实验中，LightMem准确率提升最高达10.9%，减少token使用、API调用和运行时间。

Conclusion: LightMem能有效平衡内存系统性能和效率。

Abstract: Despite their remarkable capabilities, Large Language Models (LLMs) struggle
to effectively leverage historical interaction information in dynamic and
complex environments. Memory systems enable LLMs to move beyond stateless
interactions by introducing persistent information storage, retrieval, and
utilization mechanisms. However, existing memory systems often introduce
substantial time and computational overhead. To this end, we introduce a new
memory system called LightMem, which strikes a balance between the performance
and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of
human memory, LightMem organizes memory into three complementary stages. First,
cognition-inspired sensory memory rapidly filters irrelevant information
through lightweight compression and groups information according to their
topics. Next, topic-aware short-term memory consolidates these topic-based
groups, organizing and summarizing content for more structured access. Finally,
long-term memory with sleep-time update employs an offline procedure that
decouples consolidation from online inference. Experiments on LongMemEval with
GPT and Qwen backbones show that LightMem outperforms strong baselines in
accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API
calls by up to 159x, and runtime by over 12x. The code is available at
https://github.com/zjunlp/LightMem.

</details>


### [258] [Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring](https://arxiv.org/abs/2510.18817)
*Shuxin Lin,Dhaval Patel,Christodoulos Constantinides*

Main category: cs.CL

TL;DR: 本文提出工业资产健康知识蒸馏框架，将大模型推理能力转移到小模型，经实验微调小模型表现提升。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在专业领域做复杂推理有挑战，需提升其推理能力。

Method: 提出知识蒸馏框架，通过思维链蒸馏将大模型推理能力转移到小模型，用多项选择题提示蒸馏大模型，进行上下文学习验证知识质量并与大模型对比。

Result: 微调后的小模型在思维链推理上大幅超越基础模型，缩小与大模型差距。

Conclusion: 所提知识蒸馏框架能有效提升小语言模型在工业资产健康领域的推理能力。

Abstract: Small Language Models (SLMs) are becoming increasingly popular in specialized
fields, such as industrial applications, due to their efficiency, lower
computational requirements, and ability to be fine-tuned for domain-specific
tasks, enabling accurate and cost-effective solutions. However, performing
complex reasoning using SLMs in specialized fields such as Industry 4.0 remains
challenging. In this paper, we propose a knowledge distillation framework for
industrial asset health, which transfers reasoning capabilities via
Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to
smaller, more efficient models (SLMs). We discuss the advantages and the
process of distilling LLMs using multi-choice question answering (MCQA) prompts
to enhance reasoning and refine decision-making. We also perform in-context
learning to verify the quality of the generated knowledge and benchmark the
performance of fine-tuned SLMs with generated knowledge against widely used
LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform
the base models by a significant margin, narrowing the gap to their LLM
counterparts. Our code is open-sourced at:
https://github.com/IBM/FailureSensorIQ.

</details>


### [259] [Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning](https://arxiv.org/abs/2510.18849)
*Chenghao Zhu,Meiling Tao,Tiannan Wang,Dongyi Ding,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出Critique - Post - Edit框架解决大语言模型个性化难题，在评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法如SFT和标准RLHF在大语言模型个性化任务中有局限，标量奖励模型易被破解。

Method: 提出Critique - Post - Edit框架，含Personalized Generative Reward Model和Critique - Post - Edit机制。

Result: 在长度控制评估下，方法在个性化基准测试中大幅超越标准PPO，Qwen2.5模型有显著提升。

Conclusion: 该方法为大语言模型实现忠实、高效、可控的个性化提供了可行路径。

Abstract: Faithfully personalizing large language models (LLMs) to align with
individual user preferences is a critical but challenging task. While
supervised fine-tuning (SFT) quickly reaches a performance plateau, standard
reinforcement learning from human feedback (RLHF) also struggles with the
nuances of personalization. Scalar-based reward models are prone to reward
hacking which leads to verbose and superficially personalized responses. To
address these limitations, we propose Critique-Post-Edit, a robust
reinforcement learning framework that enables more faithful and controllable
personalization. Our framework integrates two key components: (1) a
Personalized Generative Reward Model (GRM) that provides multi-dimensional
scores and textual critiques to resist reward hacking, and (2) a
Critique-Post-Edit mechanism where the policy model revises its own outputs
based on these critiques for more targeted and efficient learning. Under a
rigorous length-controlled evaluation, our method substantially outperforms
standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an
average 11\% win-rate improvement, and personalized Qwen2.5-14B model surpasses
the performance of GPT-4.1. These results demonstrate a practical path to
faithful, efficient, and controllable personalization.

</details>


### [260] [Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model](https://arxiv.org/abs/2510.18855)
*Ling Team,Anqi Shen,Baihui Li,Bin Hu,Bin Jing,Cai Chen,Chao Huang,Chao Zhang,Chaokun Yang,Cheng Lin,Chengyao Wen,Congqi Li,Deng Zhao,Dingbo Yuan,Donghai You,Fagui Mao,Fanzhuang Meng,Feng Xu,Guojie Li,Guowei Wang,Hao Dai,Haonan Zheng,Hong Liu,Jia Guo,Jiaming Liu,Jian Liu,Jianhao Fu,Jiannan Shi,Jianwen Wang,Jianxin Lai,Jin Yang,Jun Mei,Jun Zhou,Junbo Zhao,Junping Zhao,Kuan Xu,Le Su,Lei Chen,Li Tang,Liang Jiang,Liangcheng Fu,Lianhao Xu,Linfeng Shi,Lisha Liao,Longfei Zheng,Meng Li,Mingchun Chen,Qi Zuo,Qiang Cheng,Qianggang Cao,Qitao Shi,Quanrui Guo,Senlin Zhu,Shaofei Wang,Shaomian Zheng,Shuaicheng Li,Shuwei Gu,Siba Chen,Tao Wu,Tao Zhang,Tianyu Zhang,Tianyu Zhou,Tiwei Bie,Tongkai Yang,Wang Hong,Wang Ren,Weihua Chen,Wenbo Yu,Wengang Zheng,Xiangchun Wang,Xiaodong Yan,Xiaopei Wan,Xin Zhao,Xinyu Kong,Xinyu Tang,Xudong Han,Xudong Wang,Xuemin Yang,Xueyu Hu,Yalin Zhang,Yan Sun,Yicheng Shan,Yilong Wang,Yingying Xu,Yongkang Liu,Yongzhen Guo,Yuanyuan Wang,Yuchen Yan,Yuefan Wang,Yuhong Guo,Zehuan Li,Zhankai Xu,Zhe Li,Zhenduo Zhang,Zhengke Gui,Zhenxuan Pan,Zhenyu Huang,Zhenzhong Lan,Zhiqiang Ding,Zhiqiang Zhang,Zhixun Li,Zhizhen Liu,Zihao Wang,Zujie Wen*

Main category: cs.CL

TL;DR: 本文介绍万亿级参数开源思维模型Ring - 1T，提出三项创新解决训练难题，在多个基准测试取得突破，向社区发布模型推动大规模推理智能民主化。


<details>
  <summary>Details</summary>
Motivation: 训练万亿级参数模型存在训练推理不一致、滚动处理低效和强化学习系统瓶颈等问题。

Method: 提出三项创新：IcePop通过令牌级差异掩码和裁剪稳定强化学习训练；C3PO++动态划分长滚动以提高资源利用率；ASystem克服系统瓶颈的高性能强化学习框架。

Result: Ring - 1T在多个关键基准测试中取得突破性结果，如AIME - 2025得93.4分等，在IMO - 2025获银牌水平成绩。

Conclusion: 向社区发布完整的1T参数混合专家模型，推动大规模推理智能民主化，为开源模型性能树立新基准。

Abstract: We present Ring-1T, the first open-source, state-of-the-art thinking model
with a trillion-scale parameter. It features 1 trillion total parameters and
activates approximately 50 billion per token. Training such models at a
trillion-parameter scale introduces unprecedented challenges, including
train-inference misalignment, inefficiencies in rollout processing, and
bottlenecks in the RL system. To address these, we pioneer three interconnected
innovations: (1) IcePop stabilizes RL training via token-level discrepancy
masking and clipping, resolving instability from training-inference mismatches;
(2) C3PO++ improves resource utilization for long rollouts under a token budget
by dynamically partitioning them, thereby obtaining high time efficiency; and
(3) ASystem, a high-performance RL framework designed to overcome the systemic
bottlenecks that impede trillion-parameter model training. Ring-1T delivers
breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on
HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a
silver medal-level result on the IMO-2025, underscoring its exceptional
reasoning capabilities. By releasing the complete 1T parameter MoE model to the
community, we provide the research community with direct access to cutting-edge
reasoning capabilities. This contribution marks a significant milestone in
democratizing large-scale reasoning intelligence and establishes a new baseline
for open-source model performance.

</details>


### [261] [How Do LLMs Use Their Depth?](https://arxiv.org/abs/2510.18871)
*Akshat Gupta,Jay Yeung,Gopala Anumanchipalli,Anna Ivanova*

Main category: cs.CL

TL;DR: 本文追踪开放式权重模型推理时的中间表征，提出“Guess - then - Refine”框架，通过案例分析揭示大语言模型深度使用情况，为提升计算效率提供见解。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对大语言模型逐层预测动态的细粒度理解，需深入探究其深度使用情况。

Method: 追踪开放式权重模型推理时的中间表征，提出“Guess - then - Refine”框架，并进行词性分析、事实回忆任务分析和多项选择任务分析三个案例研究。

Result: 早期层预测多为高频词，超70%会被修正；词性分析发现功能词最早被正确预测；事实回忆任务中首词需更多计算深度；多项选择任务中模型在前半层识别响应格式，后半层确定最终响应。

Conclusion: 研究提供了大语言模型深度使用的详细视图，为基于Transformer的模型提升计算效率提供了见解。

Abstract: Growing evidence suggests that large language models do not use their depth
uniformly, yet we still lack a fine-grained understanding of their layer-wise
prediction dynamics. In this paper, we trace the intermediate representations
of several open-weight models during inference and reveal a structured and
nuanced use of depth. Specifically, we propose a "Guess-then-Refine" framework
that explains how LLMs internally structure their computations to make
predictions. We first show that the top-ranked predictions in early LLM layers
are composed primarily of high-frequency tokens, which act as statistical
guesses proposed by the model early on due to the lack of appropriate
contextual information. As contextual information develops deeper into the
model, these initial guesses get refined into contextually appropriate tokens.
Even high-frequency token predictions from early layers get refined >70% of the
time, indicating that correct token prediction is not "one-and-done". We then
go beyond frequency-based prediction to examine the dynamic usage of layer
depth across three case studies. (i) Part-of-speech analysis shows that
function words are, on average, the earliest to be predicted correctly. (ii)
Fact recall task analysis shows that, in a multi-token answer, the first token
requires more computational depth than the rest. (iii) Multiple-choice task
analysis shows that the model identifies the format of the response within the
first half of the layers, but finalizes its response only toward the end.
Together, our results provide a detailed view of depth usage in LLMs, shedding
light on the layer-by-layer computations that underlie successful predictions
and providing insights for future works to improve computational efficiency in
transformer-based models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [262] [ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization](https://arxiv.org/abs/2510.18433)
*Yuanhe Guo,Linxi Xie,Zhuoran Chen,Kangrui Yu,Ryan Po,Guandao Yang,Gordon Wetztein,Hongyi Wen*

Main category: cs.CV

TL;DR: 介绍ImageGem数据集用于研究理解细粒度个人偏好的生成模型，展示其在训练偏好对齐模型、个性化图像检索等方面的应用，提出端到端框架，开启生成模型个性化新范式。


<details>
  <summary>Details</summary>
Motivation: 解决阻碍理解细粒度个人偏好的生成模型发展的缺乏真实场景和细粒度用户偏好注释的问题。

Method: 引入含57K用户交互数据的ImageGem数据集，用其注释训练偏好对齐模型，研究检索模型和视觉语言模型性能，提出端到端框架。

Result: 能训练更好的偏好对齐模型，展示了在个性化图像检索和生成模型推荐中的应用。

Conclusion: ImageGem数据集开启了生成模型个性化的新范式。

Abstract: We introduce ImageGem, a dataset for studying generative models that
understand fine-grained individual preferences. We posit that a key challenge
hindering the development of such a generative model is the lack of in-the-wild
and fine-grained user preference annotations. Our dataset features real-world
interaction data from 57K users, who collectively have built 242K customized
LoRAs, written 3M text prompts, and created 5M generated images. With user
preference annotations from our dataset, we were able to train better
preference alignment models. In addition, leveraging individual user
preference, we investigated the performance of retrieval models and a
vision-language model on personalized image retrieval and generative model
recommendation. Finally, we propose an end-to-end framework for editing
customized diffusion models in a latent weight space to align with individual
user preferences. Our results demonstrate that the ImageGem dataset enables,
for the first time, a new paradigm for generative model personalization.

</details>


### [263] [Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression](https://arxiv.org/abs/2510.18650)
*Kyo Kuroki,Yasuyuki Okoshi,Thiem Van Chu,Kazushi Kawamura,Masato Motomura*

Main category: cs.CV

TL;DR: 提出二元二次量化（BQQ）矩阵量化方法，实验表明其在矩阵压缩和模型量化上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有一阶量化方法通过二进制基线性组合近似实值矩阵，本文探索利用二元二次表达式进行矩阵量化。

Method: 提出BQQ方法，用二元二次表达式近似实值矩阵，并进行矩阵压缩基准测试和预训练视觉Transformer模型的训练后量化实验。

Result: BQQ在矩阵压缩中内存效率和重建误差平衡更好；在ImageNet数据集上，校准和无数据场景下分别比现有方法高2.2%和59.1%。

Conclusion: 二元二次表达式在矩阵近似和神经网络压缩中非常有效。

Abstract: This paper proposes a novel matrix quantization method, Binary Quadratic
Quantization (BQQ). In contrast to conventional first-order quantization
approaches, such as uniform quantization and binary coding quantization, that
approximate real-valued matrices via linear combinations of binary bases, BQQ
leverages the expressive power of binary quadratic expressions while
maintaining an extremely compact data format. We validate our approach with two
experiments: a matrix compression benchmark and post-training quantization
(PTQ) on pretrained Vision Transformer-based models. Experimental results
demonstrate that BQQ consistently achieves a superior trade-off between memory
efficiency and reconstruction error than conventional methods for compressing
diverse matrix data. It also delivers strong PTQ performance, even though we
neither target state-of-the-art PTQ accuracy under tight memory constraints nor
rely on PTQ-specific binary matrix optimization. For example, our proposed
method outperforms the state-of-the-art PTQ method by up to 2.2\% and 59.1% on
the ImageNet dataset under the calibration-based and data-free scenarios,
respectively, with quantization equivalent to 2 bits. These findings highlight
the surprising effectiveness of binary quadratic expressions for efficient
matrix approximation and neural network compression.

</details>


### [264] [Visual Space Optimization for Zero-shot Learning](https://arxiv.org/abs/1907.00330)
*Xinsheng Wang,Shanmin Pang,Jihua Zhu,Zhongyu Li,Zhiqiang Tian,Yaochen Li*

Main category: cs.CV

TL;DR: 本文提出两种优化视觉空间的策略用于零样本学习，实验表明优化视觉空间有益，基于原型的方法达最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本学习模型将深度视觉特征构成的视觉空间作为嵌入空间，但视觉空间中实例离散分布，数据结构不明显，需优化视觉空间。

Method: 提出两种策略，一是基于视觉原型的方法，为每个视觉类学习视觉原型；二是在中间嵌入空间优化视觉特征结构，设计多层感知机框架算法。

Result: 通过四个基准数据集实验，证明优化视觉空间对零样本学习有益，基于原型的方法取得新的最优性能。

Conclusion: 优化视觉空间对零样本学习有好处，基于原型的方法效果优异。

Abstract: Zero-shot learning, which aims to recognize new categories that are not
included in the training set, has gained popularity owing to its potential
ability in the real-word applications. Zero-shot learning models rely on
learning an embedding space, where both semantic descriptions of classes and
visual features of instances can be embedded for nearest neighbor search.
Recently, most of the existing works consider the visual space formulated by
deep visual features as an ideal choice of the embedding space. However, the
discrete distribution of instances in the visual space makes the data structure
unremarkable. We argue that optimizing the visual space is crucial as it allows
semantic vectors to be embedded into the visual space more effectively. In this
work, we propose two strategies to accomplish this purpose. One is the visual
prototype based method, which learns a visual prototype for each visual class,
so that, in the visual space, a class can be represented by a prototype feature
instead of a series of discrete visual features. The other is to optimize the
visual feature structure in an intermediate embedding space, and in this method
we successfully devise a multilayer perceptron framework based algorithm that
is able to learn the common intermediate embedding space and meanwhile to make
the visual data structure more distinctive. Through extensive experimental
evaluation on four benchmark datasets, we demonstrate that optimizing visual
space is beneficial for zero-shot learning. Besides, the proposed prototype
based method achieves the new state-of-the-art performance.

</details>


### [265] [MAT-Agent: Adaptive Multi-Agent Training Optimization](https://arxiv.org/abs/2510.17845)
*Jusheng Zhang,Kaitong Cai,Yijia Fan,Ningyuan Liu,Keze Wang*

Main category: cs.CV

TL;DR: 提出MAT - Agent多智能体框架用于多标签图像分类，实验证明其性能优越，为自适应深度学习提供可扩展智能解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统多标签图像分类方法依赖静态配置，在动态环境中表现不佳，需要自适应训练策略。

Method: 提出MAT - Agent框架，利用自主智能体动态调整数据增强、优化器等，采用非平稳多臂老虎机算法平衡探索与利用，结合双速率指数移动平均平滑和混合精度训练。

Result: 在Pascal VOC、COCO和VG - 256上实验，各项指标优于对比方法，收敛加速，跨域泛化能力强。

Conclusion: MAT - Agent是优化复杂视觉模型的可扩展、智能解决方案，推动自适应深度学习发展。

Abstract: Multi-label image classification demands adaptive training strategies to
navigate complex, evolving visual-semantic landscapes, yet conventional methods
rely on static configurations that falter in dynamic settings. We propose
MAT-Agent, a novel multi-agent framework that reimagines training as a
collaborative, real-time optimization process. By deploying autonomous agents
to dynamically tune data augmentation, optimizers, learning rates, and loss
functions, MAT-Agent leverages non-stationary multi-armed bandit algorithms to
balance exploration and exploitation, guided by a composite reward harmonizing
accuracy, rare-class performance, and training stability. Enhanced with
dual-rate exponential moving average smoothing and mixed-precision training, it
ensures robustness and efficiency. Extensive experiments across Pascal VOC,
COCO, and VG-256 demonstrate MAT-Agent's superiority: it achieves an mAP of
97.4 (vs. 96.2 for PAT-T), OF1 of 92.3, and CF1 of 91.4 on Pascal VOC; an mAP
of 92.8 (vs. 92.0 for HSQ-CvN), OF1 of 88.2, and CF1 of 87.1 on COCO; and an
mAP of 60.9, OF1 of 70.8, and CF1 of 61.1 on VG-256. With accelerated
convergence and robust cross-domain generalization, MAT-Agent offers a
scalable, intelligent solution for optimizing complex visual models, paving the
way for adaptive deep learning advancements.

</details>


### [266] [Pre to Post-Treatment Glioblastoma MRI Prediction using a Latent Diffusion Model](https://arxiv.org/abs/2510.17851)
*Alexandre G. Leclercq,Sébastien Bougleux,Noémie N. Moreau,Alexis Desmonts,Romain Hérault,Aurélien Corroyer-Dulmont*

Main category: cs.CV

TL;DR: 针对胶质母细胞瘤（GBM）治疗反应早期预测问题，提出基于潜在扩散模型的方法并在本地数据集测试。


<details>
  <summary>Details</summary>
Motivation: GBM患者治疗反应异质性高，早期预测治疗反应对推进个性化医疗至关重要，现有TRP方法多依赖时间序列数据。

Method: 提出具有基于预处理MRI和肿瘤定位的拼接式条件以及使用生存信息进行无分类器引导的潜在扩散模型。

Result: 模型在包含140名GBM患者的本地数据集上进行了训练和测试。

Conclusion: 文中未明确提及最终结论，但该模型旨在解决早期视觉TRP问题，反映肿瘤演变。

Abstract: Glioblastoma (GBM) is an aggressive primary brain tumor with a median
survival of approximately 15 months. In clinical practice, the Stupp protocol
serves as the standard first-line treatment. However, patients exhibit highly
heterogeneous therapeutic responses which required at least two months before
first visual impact can be observed, typically with MRI. Early prediction
treatment response is crucial for advancing personalized medicine. Disease
Progression Modeling (DPM) aims to capture the trajectory of disease evolution,
while Treatment Response Prediction (TRP) focuses on assessing the impact of
therapeutic interventions. Whereas most TRP approaches primarly rely on
timeseries data, we consider the problem of early visual TRP as a
slice-to-slice translation model generating post-treatment MRI from a
pre-treatment MRI, thus reflecting the tumor evolution. To address this problem
we propose a Latent Diffusion Model with a concatenation-based conditioning
from the pre-treatment MRI and the tumor localization, and a classifier-free
guidance to enhance generation quality using survival information, in
particular post-treatment tumor evolution. Our model were trained and tested on
a local dataset consisting of 140 GBM patients collected at Centre Fran\c{c}ois
Baclesse. For each patient we collected pre and post T1-Gd MRI, tumor
localization manually delineated in the pre-treatment MRI by medical experts,
and survival information.

</details>


### [267] [MUSE: Model-based Uncertainty-aware Similarity Estimation for zero-shot 2D Object Detection and Segmentation](https://arxiv.org/abs/2510.17866)
*Sungmin Cho,Sungbum Park,Insoo Oh*

Main category: cs.CV

TL;DR: 介绍无训练框架MUSE用于零样本2D目标检测与分割，在BOP Challenge 2025获佳绩。


<details>
  <summary>Details</summary>
Motivation: 提出适用于基于模型的零样本2D目标检测和分割的无训练框架。

Method: 利用3D物体渲染的2D多视图模板和输入图像的2D目标提议，结合类和补丁嵌入，使用广义平均池化，采用联合相似度度量，通过不确定性感知目标先验优化分数。

Result: 在BOP Challenge 2025中排名第一，在多个赛道表现出色。

Conclusion: MUSE是强大且可推广的零样本2D目标检测和分割框架。

Abstract: In this work, we introduce MUSE (Model-based Uncertainty-aware Similarity
Estimation), a training-free framework designed for model-based zero-shot 2D
object detection and segmentation. MUSE leverages 2D multi-view templates
rendered from 3D unseen objects and 2D object proposals extracted from input
query images. In the embedding stage, it integrates class and patch embeddings,
where the patch embeddings are normalized using generalized mean pooling (GeM)
to capture both global and local representations efficiently. During the
matching stage, MUSE employs a joint similarity metric that combines absolute
and relative similarity scores, enhancing the robustness of matching under
challenging scenarios. Finally, the similarity score is refined through an
uncertainty-aware object prior that adjusts for proposal reliability. Without
any additional training or fine-tuning, MUSE achieves state-of-the-art
performance on the BOP Challenge 2025, ranking first across the Classic Core,
H3, and Industrial tracks. These results demonstrate that MUSE offers a
powerful and generalizable framework for zero-shot 2D object detection and
segmentation.

</details>


### [268] [Auditing and Mitigating Bias in Gender Classification Algorithms: A Data-Centric Approach](https://arxiv.org/abs/2510.17873)
*Tadesse K Bahiru,Natnael Tilahun Sinshaw,Teshager Hailemariam Moges,Dheeraj Kumar Singh*

Main category: cs.CV

TL;DR: 研究指出常用性别分类数据集存在交叉代表性不足问题，构建新数据集BalancedFace减少模型偏差，凸显以数据为中心干预的价值。


<details>
  <summary>Details</summary>
Motivation: 解决性别分类系统在训练数据中继承和放大人口统计学不平衡的问题。

Method: 审计五个常用性别分类数据集，在UTKFace和FairFace上训练模型评估偏差，融合多个数据集构建新的BalancedFace数据集。

Result: 在BalancedFace上训练的分类器将种族子组间最大真阳性率差距降低超50%，平均差异影响得分更接近理想值1.0，整体准确率损失小。

Conclusion: 强调以数据为中心的干预措施具有重要价值，为公平性别分类研究提供公开资源。

Abstract: Gender classification systems often inherit and amplify demographic
imbalances in their training data. We first audit five widely used gender
classification datasets, revealing that all suffer from significant
intersectional underrepresentation. To measure the downstream impact of these
flaws, we train identical MobileNetV2 classifiers on the two most balanced of
these datasets, UTKFace and FairFace. Our fairness evaluation shows that even
these models exhibit significant bias, misclassifying female faces at a higher
rate than male faces and amplifying existing racial skew. To counter these
data-induced biases, we construct BalancedFace, a new public dataset created by
blending images from FairFace and UTKFace, supplemented with images from other
collections to fill missing demographic gaps. It is engineered to equalize
subgroup shares across 189 intersections of age, race, and gender using only
real, unedited images. When a standard classifier is trained on BalancedFace,
it reduces the maximum True Positive Rate gap across racial subgroups by over
50% and brings the average Disparate Impact score 63% closer to the ideal of
1.0 compared to the next-best dataset, all with a minimal loss of overall
accuracy. These results underline the profound value of data-centric
interventions and provide an openly available resource for fair gender
classification research.

</details>


### [269] [3D Weakly Supervised Semantic Segmentation via Class-Aware and Geometry-Guided Pseudo-Label Refinement](https://arxiv.org/abs/2510.17875)
*Xiaoxu Xu,Xuexun Liu,Jinlong Li,Yitian Yuan,Qiudan Zhang,Lin Ma,Nicu Sebe,Xu Wang*

Main category: cs.CV

TL;DR: 提出一种3D弱监督语义分割方法，整合3D几何先验到类别感知引导机制生成高保真伪标签，在多个基准测试取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 先前工作中伪标签质量低和3D几何先验利用不足，阻碍高性能3D WSSS模型发展。

Method: 先使用类别感知标签细化模块生成更平衡准确的伪标签，再用几何感知标签细化组件过滤低置信度伪标签，最后提出结合自训练的标签更新策略扩展标签覆盖范围。

Result: 在ScanNet和S3DIS基准测试达到SOTA性能，在无监督设置中泛化能力强。

Conclusion: 所提方法有效可行，能促进高性能3D WSSS模型发展。

Abstract: 3D weakly supervised semantic segmentation (3D WSSS) aims to achieve semantic
segmentation by leveraging sparse or low-cost annotated data, significantly
reducing reliance on dense point-wise annotations. Previous works mainly employ
class activation maps or pre-trained vision-language models to address this
challenge. However, the low quality of pseudo-labels and the insufficient
exploitation of 3D geometric priors jointly create significant technical
bottlenecks in developing high-performance 3D WSSS models. In this paper, we
propose a simple yet effective 3D weakly supervised semantic segmentation
method that integrates 3D geometric priors into a class-aware guidance
mechanism to generate high-fidelity pseudo labels. Concretely, our designed
methodology first employs Class-Aware Label Refinement module to generate more
balanced and accurate pseudo labels for semantic categrories. This initial
refinement stage focuses on enhancing label quality through category-specific
optimization. Subsequently, the Geometry-Aware Label Refinement component is
developed, which strategically integrates implicit 3D geometric constraints to
effectively filter out low-confidence pseudo labels that fail to comply with
geometric plausibility. Moreover, to address the challenge of extensive
unlabeled regions, we propose a Label Update strategy that integrates
Self-Training to propagate labels into these areas. This iterative process
continuously enhances pseudo-label quality while expanding label coverage,
ultimately fostering the development of high-performance 3D WSSS models.
Comprehensive experimental validation reveals that our proposed methodology
achieves state-of-the-art performance on both ScanNet and S3DIS benchmarks
while demonstrating remarkable generalization capability in unsupervised
settings, maintaining competitive accuracy through its robust design.

</details>


### [270] [Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach](https://arxiv.org/abs/2510.17854)
*Jitendra Sharma,Arthur Carvalho,Suman Bhunia*

Main category: cs.CV

TL;DR: 文章提出基于嵌入的AI图像检测框架，利用图像嵌入和向量相似度区分AI生成图像和人类创作图像，实验验证其鲁棒性，提供了平衡准确性与计算效率的通用框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大语言模型发展使AI生成图像逼真，给数字内容认证带来挑战，需验证数字数据完整性和来源以维护数字媒体信任和合法性。

Method: 提出基于嵌入的AI图像检测框架，假设AI生成图像与其他AI生成内容嵌入距离更近，人类创作图像在其领域内聚类相似，用五个基准嵌入模型处理AI和人类生成图像的多样化数据集。

Result: 实验表明该方法具有鲁棒性，中到高扰动对嵌入签名影响极小，扰动图像与原始版本相似度高。

Conclusion: 该解决方案为AI生成图像检测提供了一个通用框架，平衡了准确性和计算效率。

Abstract: Rapid advancement in generative AI and large language models (LLMs) has
enabled the generation of highly realistic and contextually relevant digital
content. LLMs such as ChatGPT with DALL-E integration and Stable Diffusion
techniques can produce images that are often indistinguishable from those
created by humans, which poses challenges for digital content authentication.
Verifying the integrity and origin of digital data to ensure it remains
unaltered and genuine is crucial to maintaining trust and legality in digital
media. In this paper, we propose an embedding-based AI image detection
framework that utilizes image embeddings and a vector similarity to distinguish
AI-generated images from real (human-created) ones. Our methodology is built on
the hypothesis that AI-generated images demonstrate closer embedding proximity
to other AI-generated content, while human-created images cluster similarly
within their domain. To validate this hypothesis, we developed a system that
processes a diverse dataset of AI and human-generated images through five
benchmark embedding models. Extensive experimentation demonstrates the
robustness of our approach, and our results confirm that moderate to high
perturbations minimally impact the embedding signatures, with perturbed images
maintaining close similarity matches to their original versions. Our solution
provides a generalizable framework for AI-generated image detection that
balances accuracy with computational efficiency.

</details>


### [271] [Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch](https://arxiv.org/abs/2510.17858)
*Xu Cai,Yang Wu,Qianli Chen,Haoran Wu,Lichuan Xiang,Hongkai Wen*

Main category: cs.CV

TL;DR: 提出超高效后训练方法，利用速度场自蒸馏将预训练流匹配扩散模型快捷化为少步采样器，训练高效，还可用于预训练阶段及实现少样本蒸馏。


<details>
  <summary>Details</summary>
Motivation: 原流匹配快捷方法需专门步长嵌入，与现有模型不兼容且重新训练成本高。

Method: 采用独特蒸馏原理，在速度场而非样本空间工作，以在线自引导蒸馏方式快速学习。

Result: 能高效训练，如用不到一个A100天生成3步Flux；可应用于预训练阶段；实现了针对万亿参数扩散模型的少样本蒸馏方法。

Conclusion: 该方法能将大规模预训练流匹配扩散模型快捷化为高效少步采样器，成本低且性能达到最优。

Abstract: We present an ultra-efficient post-training method for shortcutting
large-scale pre-trained flow matching diffusion models into efficient few-step
samplers, enabled by novel velocity field self-distillation. While shortcutting
in flow matching, originally introduced by shortcut models, offers flexible
trajectory-skipping capabilities, it requires a specialized step-size embedding
incompatible with existing models unless retraining from
scratch$\unicode{x2013}$a process nearly as costly as pretraining itself.
  Our key contribution is thus imparting a more aggressive shortcut mechanism
to standard flow matching models (e.g., Flux), leveraging a unique distillation
principle that obviates the need for step-size embedding. Working on the
velocity field rather than sample space and learning rapidly from self-guided
distillation in an online manner, our approach trains efficiently, e.g.,
producing a 3-step Flux less than one A100 day. Beyond distillation, our method
can be incorporated into the pretraining stage itself, yielding models that
inherently learn efficient, few-step flows without compromising quality. This
capability also enables, to our knowledge, the first few-shot distillation
method (e.g., 10 text-image pairs) for dozen-billion-parameter diffusion
models, delivering state-of-the-art performance at almost free cost.

</details>


### [272] [SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection](https://arxiv.org/abs/2510.18034)
*Roberto Brusnicki,David Pop,Yuan Gao,Mattia Piccinini,Johannes Betz*

Main category: cs.CV

TL;DR: 提出SAVANT框架用于检测自动驾驶异常场景，表现出色并解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在处理长尾异常场景时脆弱，现有VLM提示方法不可靠且依赖昂贵模型。

Method: 引入SAVANT框架，通过分层场景分析和两阶段管道，对VLM推理进行跨四层语义的系统分析。

Result: SAVANT在真实驾驶场景中召回率达89.6%、准确率达88.0%；微调的7B参数开源模型召回率达90.8%、准确率达93.8%，自动标注超9640张图像。

Conclusion: SAVANT能解决异常检测的数据稀缺问题，为自动驾驶系统提供可靠、可及的语义监测途径。

Abstract: Autonomous driving systems remain critically vulnerable to the long-tail of
rare, out-of-distribution scenarios with semantic anomalies. While Vision
Language Models (VLMs) offer promising reasoning capabilities, naive prompting
approaches yield unreliable performance and depend on expensive proprietary
models, limiting practical deployment. We introduce SAVANT (Semantic Analysis
with Vision-Augmented Anomaly deTection), a structured reasoning framework that
achieves high accuracy and recall in detecting anomalous driving scenarios from
input images through layered scene analysis and a two-phase pipeline:
structured scene description extraction followed by multi-modal evaluation. Our
approach transforms VLM reasoning from ad-hoc prompting to systematic analysis
across four semantic layers: Street, Infrastructure, Movable Objects, and
Environment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world
driving scenarios, significantly outperforming unstructured baselines. More
importantly, we demonstrate that our structured framework enables a fine-tuned
7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8%
accuracy - surpassing all models evaluated while enabling local deployment at
near-zero cost. By automatically labeling over 9,640 real-world images with
high accuracy, SAVANT addresses the critical data scarcity problem in anomaly
detection and provides a practical path toward reliable, accessible semantic
monitoring for autonomous systems.

</details>


### [273] [TriggerNet: A Novel Explainable AI Framework for Red Palm Mite Detection and Multi-Model Comparison and Heuristic-Guided Annotation](https://arxiv.org/abs/2510.18038)
*Harshini Suresha,Kavitha SH*

Main category: cs.CV

TL;DR: 研究评估比较ML模型对红棕螨侵染植物的分类和检测，应用TriggerNet框架，用多种图像训练评估，用多种模型分类，用Snorkel标注。


<details>
  <summary>Details</summary>
Motivation: 红棕螨侵染导致棕榈种植生产力下降和经济损失，需准确早期识别受侵染植物。

Method: 应用TriggerNet框架，利用11种植物的RGB图像，采用CNN等深度学习模型和随机森林等机器学习分类器进行植物分类，用Snorkel依据启发式规则和模式标注病害类别。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: The red palm mite infestation has become a serious concern, particularly in
regions with extensive palm cultivation, leading to reduced productivity and
economic losses. Accurate and early identification of mite-infested plants is
critical for effective management. The current study focuses on evaluating and
comparing the ML model for classifying the affected plants and detecting the
infestation. TriggerNet is a novel interpretable AI framework that integrates
Grad-CAM, RISE, FullGrad, and TCAV to generate novel visual explanations for
deep learning models in plant classification and disease detection. This study
applies TriggerNet to address red palm mite (Raoiella indica) infestation, a
major threat to palm cultivation and agricultural productivity. A diverse set
of RGB images across 11 plant species, Arecanut, Date Palm, Bird of Paradise,
Coconut Palm, Ginger, Citrus Tree, Palm Oil, Orchid, Banana Palm, Avocado Tree,
and Cast Iron Plant was utilized for training and evaluation. Advanced deep
learning models like CNN, EfficientNet, MobileNet, ViT, ResNet50, and
InceptionV3, alongside machine learning classifiers such as Random Forest, SVM,
and KNN, were employed for plant classification. For disease classification,
all plants were categorized into four classes: Healthy, Yellow Spots, Reddish
Bronzing, and Silk Webbing. Snorkel was used to efficiently label these disease
classes by leveraging heuristic rules and patterns, reducing manual annotation
time and improving dataset reliability.

</details>


### [274] [ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues](https://arxiv.org/abs/2510.18016)
*Prateek Gothwal,Deeptimaan Banerjee,Ashis Kumer Biswas*

Main category: cs.CV

TL;DR: 提出ViBED - Net框架用于在线学习中学生参与度检测，结合面部和场景信息，在DAiSEE数据集上测试，LSTM变体表现最佳，代码开源。


<details>
  <summary>Details</summary>
Motivation: 在线学习环境中参与度检测对提升学生成绩和个性化教学至关重要，需要有效方法。

Method: 提出ViBED - Net框架，采用双流架构，用EfficientNetV2提取空间特征，用LSTM和Transformer编码器进行时间建模，应用针对性数据增强技术。

Result: ViBED - Net的LSTM变体在DAiSEE数据集上准确率达73.43%，优于现有方法。

Conclusion: 结合面部和场景时空线索能显著提高参与度检测准确率，模型模块化设计应用灵活，推动了基于视频的情感计算。

Abstract: Engagement detection in online learning environments is vital for improving
student outcomes and personalizing instruction. We present ViBED-Net
(Video-Based Engagement Detection Network), a novel deep learning framework
designed to assess student engagement from video data using a dual-stream
architecture. ViBED-Net captures both facial expressions and full-scene context
by processing facial crops and entire video frames through EfficientNetV2 for
spatial feature extraction. These features are then analyzed over time using
two temporal modeling strategies: Long Short-Term Memory (LSTM) networks and
Transformer encoders. Our model is evaluated on the DAiSEE dataset, a
large-scale benchmark for affective state recognition in e-learning. To enhance
performance on underrepresented engagement classes, we apply targeted data
augmentation techniques. Among the tested variants, ViBED-Net with LSTM
achieves 73.43\% accuracy, outperforming existing state-of-the-art approaches.
ViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporal
cues significantly improves engagement detection accuracy. Its modular design
allows flexibility for application across education, user experience research,
and content personalization. This work advances video-based affective computing
by offering a scalable, high-performing solution for real-world engagement
analysis. The source code for this project is available on
https://github.com/prateek-gothwal/ViBED-Net .

</details>


### [275] [Accelerating Vision Transformers with Adaptive Patch Sizes](https://arxiv.org/abs/2510.18091)
*Rohan Choudhury,JungEun Kim,Jinhyung Park,Eunho Yang,László A. Jeni,Kris M. Kitani*

Main category: cs.CV

TL;DR: 提出Adaptive Patch Transformers (APT)，用多尺寸patch减少输入token数，加速ViT推理和训练，且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 传统Vision Transformers (ViTs) 对高分辨率图像输入序列长，影响推理和训练速度。

Method: 在同一图像中使用多种不同的patch尺寸，在更均匀区域分配大尺寸patch，复杂区域分配小尺寸patch。

Result: 在ViT-L上吞吐量提高40%，ViT-H上提高50%；在高分辨率密集视觉任务中，训练和推理速度最高提升30%。

Conclusion: APT能大幅加速ViT的推理和训练，且可应用于预微调的ViT，在不损失性能的情况下减少训练和推理时间。

Abstract: Vision Transformers (ViTs) partition input images into uniformly sized
patches regardless of their content, resulting in long input sequence lengths
for high-resolution images. We present Adaptive Patch Transformers (APT), which
addresses this by using multiple different patch sizes within the same image.
APT reduces the total number of input tokens by allocating larger patch sizes
in more homogeneous areas and smaller patches in more complex ones. APT
achieves a drastic speedup in ViT inference and training, increasing throughput
by 40% on ViT-L and 50% on ViT-H while maintaining downstream performance, and
can be applied to a previously fine-tuned ViT, converging in as little as 1
epoch. It also significantly reduces training and inference time without loss
of performance in high-resolution dense visual tasks, achieving up to 30\%
faster training and inference in visual QA, object detection, and semantic
segmentation.

</details>


### [276] [SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving](https://arxiv.org/abs/2510.18123)
*Xiangbo Gao,Tzu-Hsiang Lin,Ruojing Song,Yuheng Wu,Kuan-Ru Huang,Zicheng Jin,Fangzhou Lin,Shinan Liu,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文研究基于自然语言的协同驾驶全栈安全与保障问题，提出攻击策略分类和SafeCoop防御管道，模拟评估效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统V2X系统有高带宽需求等挑战，自然语言虽有潜力但引入新漏洞，需研究安全与保障问题。

Method: 开发攻击策略分类，引入SafeCoop防御管道，集成语义防火墙等，通过代理转换函数实现跨帧空间对齐。

Result: 在32个关键场景的CARLA模拟中，恶意攻击下驾驶分数提高69.15%，恶意检测F1分数达67.32%。

Conclusion: 为交通系统中安全、可靠的语言驱动协作研究提供指导。

Abstract: Collaborative driving systems leverage vehicle-to-everything (V2X)
communication across multiple agents to enhance driving safety and efficiency.
Traditional V2X systems take raw sensor data, neural features, or perception
results as communication media, which face persistent challenges, including
high bandwidth demands, semantic loss, and interoperability issues. Recent
advances investigate natural language as a promising medium, which can provide
semantic richness, decision-level reasoning, and human-machine interoperability
at significantly lower bandwidth. Despite great promise, this paradigm shift
also introduces new vulnerabilities within language communication, including
message loss, hallucinations, semantic manipulation, and adversarial attacks.
In this work, we present the first systematic study of full-stack safety and
security issues in natural-language-based collaborative driving. Specifically,
we develop a comprehensive taxonomy of attack strategies, including connection
disruption, relay/replay interference, content spoofing, and multi-connection
forgery. To mitigate these risks, we introduce an agentic defense pipeline,
which we call SafeCoop, that integrates a semantic firewall,
language-perception consistency checks, and multi-source consensus, enabled by
an agentic transformation function for cross-frame spatial alignment. We
systematically evaluate SafeCoop in closed-loop CARLA simulation across 32
critical scenarios, achieving 69.15% driving score improvement under malicious
attacks and up to 67.32% F1 score for malicious detection. This study provides
guidance for advancing research on safe, secure, and trustworthy
language-driven collaboration in transportation systems. Our project page is
https://xiangbogaobarry.github.io/SafeCoop.

</details>


### [277] [VelocityNet: Real-Time Crowd Anomaly Detection via Person-Specific Velocity Analysis](https://arxiv.org/abs/2510.18187)
*Fatima AlGhamdi,Omar Alharbi,Abdullah Aldwyish,Raied Aljadaany,Muhammad Kamran J Khan,Huda Alamri*

Main category: cs.CV

TL;DR: 提出VelocityNet框架，结合头检测和光流提取速度，聚类分类并评分，可实时检测拥挤场景异常。


<details>
  <summary>Details</summary>
Motivation: 现有方法难适应不同人群密度，缺乏可解释异常指标，需解决拥挤场景异常检测难题。

Method: 引入VelocityNet双管道框架，结合头检测和光流提取速度，用层次聚类分类，百分位异常评分系统衡量偏差。

Result: 实验证明框架能在密集拥挤环境实时检测多种异常运动模式。

Conclusion: VelocityNet框架在拥挤场景异常检测中有效。

Abstract: Detecting anomalies in crowded scenes is challenging due to severe
inter-person occlusions and highly dynamic, context-dependent motion patterns.
Existing approaches often struggle to adapt to varying crowd densities and lack
interpretable anomaly indicators. To address these limitations, we introduce
VelocityNet, a dual-pipeline framework that combines head detection and dense
optical flow to extract person-specific velocities. Hierarchical clustering
categorizes these velocities into semantic motion classes (halt, slow, normal,
and fast), and a percentile-based anomaly scoring system measures deviations
from learned normal patterns. Experiments demonstrate the effectiveness of our
framework in real-time detection of diverse anomalous motion patterns within
densely crowded environments.

</details>


### [278] [RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology](https://arxiv.org/abs/2510.18188)
*Chengrun Li,Corentin Royer,Haozhe Luo,Bastian Wittmann,Xia Li,Ibrahim Hamamci,Sezgin Er,Anjany Sekuboyina,Bjoern Menze*

Main category: cs.CV

TL;DR: 本文引入RadDiagSeg - D数据集并提出RadDiagSeg - M模型，实现联合异常检测、诊断和灵活分割，在多目标文本和掩码生成任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前多数医疗视觉语言模型难以针对复杂视觉问题同时生成诊断文本和像素级分割掩码，限制了临床应用，需要解决此问题。

Method: 先引入RadDiagSeg - D数据集，该数据集将异常检测、诊断和多目标分割整合为统一分层任务；再基于此数据集提出RadDiagSeg - M模型。

Result: RadDiagSeg - M模型能提供信息丰富且临床有用的输出，在多目标文本和掩码生成任务各组件上表现强劲。

Conclusion: RadDiagSeg - M模型建立了一个强大且有竞争力的基线，有效解决了辅助诊断中丰富上下文信息的需求。

Abstract: Most current medical vision language models struggle to jointly generate
diagnostic text and pixel-level segmentation masks in response to complex
visual questions. This represents a major limitation towards clinical
application, as assistive systems that fail to provide both modalities
simultaneously offer limited value to medical practitioners. To alleviate this
limitation, we first introduce RadDiagSeg-D, a dataset combining abnormality
detection, diagnosis, and multi-target segmentation into a unified and
hierarchical task. RadDiagSeg-D covers multiple imaging modalities and is
precisely designed to support the development of models that produce
descriptive text and corresponding segmentation masks in tandem. Subsequently,
we leverage the dataset to propose a novel vision-language model, RadDiagSeg-M,
capable of joint abnormality detection, diagnosis, and flexible segmentation.
RadDiagSeg-M provides highly informative and clinically useful outputs,
effectively addressing the need to enrich contextual information for assistive
diagnosis. Finally, we benchmark RadDiagSeg-M and showcase its strong
performance across all components involved in the task of multi-target
text-and-mask generation, establishing a robust and competitive baseline.

</details>


### [279] [VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety](https://arxiv.org/abs/2510.18214)
*Shruti Palaskar,Leon Gatys,Mona Abdelrahman,Mar Jacobo,Larry Lindsey,Rutika Moharir,Gunnar Lund,Yang Xu,Navid Shiee,Jeffrey Bigham,Charles Maalouf,Joseph Yitan Cheng*

Main category: cs.CV

TL;DR: 提出VLSU框架评估多模态安全，构建大规模基准，评估发现现有模型在联合图文理解上有缺陷，框架为多模态安全研究提供测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基础模型安全评估分别处理视觉和语言输入，未考虑联合解读风险，且无法区分明确不安全内容和边缘情况。

Method: 提出Vision Language Safety Understanding (VLSU)框架，通过细粒度严重性分类和组合分析评估多模态安全，用多阶段流程结合真实图像和人工标注构建8187个样本的大规模基准。

Result: 评估11个模型发现，单模态安全信号准确率超90%，联合图文推理时降至20 - 55%；34%的联合图文分类错误发生在单模态分类正确时；指令框架可降低边缘内容过屏蔽率，但会降低不安全内容拒绝率。

Conclusion: 框架揭示了现有模型在联合图文理解上的弱点和对齐差距，为鲁棒视觉 - 语言安全研究提供关键测试平台。

Abstract: Safety evaluation of multimodal foundation models often treats vision and
language inputs separately, missing risks from joint interpretation where
benign content becomes harmful in combination. Existing approaches also fail to
distinguish clearly unsafe content from borderline cases, leading to
problematic over-blocking or under-refusal of genuinely harmful content. We
present Vision Language Safety Understanding (VLSU), a comprehensive framework
to systematically evaluate multimodal safety through fine-grained severity
classification and combinatorial analysis across 17 distinct safety patterns.
Using a multi-stage pipeline with real-world images and human annotation, we
construct a large-scale benchmark of 8,187 samples spanning 15 harm categories.
Our evaluation of eleven state-of-the-art models reveals systematic joint
understanding failures: while models achieve 90%-plus accuracy on clear
unimodal safety signals, performance degrades substantially to 20-55% when
joint image-text reasoning is required to determine the safety label. Most
critically, 34% of errors in joint image-text safety classification occur
despite correct classification of the individual modalities, further
demonstrating absent compositional reasoning capabilities. Additionally, we
find that models struggle to balance refusing unsafe content while still
responding to borderline cases that deserve engagement. For example, we find
that instruction framing can reduce the over-blocking rate on borderline
content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of
under-refusing on unsafe content with refusal rate dropping from 90.8% to
53.9%. Overall, our framework exposes weaknesses in joint image-text
understanding and alignment gaps in current models, and provides a critical
test bed to enable the next milestones in research on robust vision-language
safety.

</details>


### [280] [Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery](https://arxiv.org/abs/2510.18256)
*Xiang Zhang,Suping Wu,Weibin Qiu,Zhaocheng Jin,Sheng Yang*

Main category: cs.CV

TL;DR: 提出基于双曲空间学习和时间运动先验的视频3D人体网格恢复方法，实验效果优于多数先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频3D人体网格恢复方法在欧氏空间学习特征，难以准确捕捉人体网格层次结构，导致重建错误。

Method: 设计时间运动先验提取模块，结合时间运动特征形成先验；设计双曲空间优化学习策略，用先验信息辅助学习并优化网格特征；提出双曲网格优化损失。

Result: 在大型公开数据集上的实验结果显示优于多数先进方法。

Conclusion: 所提方法能有效恢复3D人体网格，优化学习过程稳定有效。

Abstract: 3D human meshes show a natural hierarchical structure (like
torso-limbs-fingers). But existing video-based 3D human mesh recovery methods
usually learn mesh features in Euclidean space. It's hard to catch this
hierarchical structure accurately. So wrong human meshes are reconstructed. To
solve this problem, we propose a hyperbolic space learning method leveraging
temporal motion prior for recovering 3D human meshes from videos. First, we
design a temporal motion prior extraction module. This module extracts the
temporal motion features from the input 3D pose sequences and image feature
sequences respectively. Then it combines them into the temporal motion prior.
In this way, it can strengthen the ability to express features in the temporal
motion dimension. Since data representation in non-Euclidean space has been
proved to effectively capture hierarchical relationships in real-world datasets
(especially in hyperbolic space), we further design a hyperbolic space
optimization learning strategy. This strategy uses the temporal motion prior
information to assist learning, and uses 3D pose and pose motion information
respectively in the hyperbolic space to optimize and learn the mesh features.
Then, we combine the optimized results to get an accurate and smooth human
mesh. Besides, to make the optimization learning process of human meshes in
hyperbolic space stable and effective, we propose a hyperbolic mesh
optimization loss. Extensive experimental results on large publicly available
datasets indicate superiority in comparison with most state-of-the-art.

</details>


### [281] [Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization](https://arxiv.org/abs/2510.18267)
*Xiang Zhang,Suping Wu,Sheng Yang*

Main category: cs.CV

TL;DR: 提出基于潜在信息和低维学习的两阶段网络用于人体网格恢复，有效提取信息并降低计算成本，实验效果优。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体网格恢复方法未能充分利用潜在信息，且用注意力机制建模计算成本高。

Method: 第一阶段从图像特征的高低频分量挖掘信息聚合为混合潜在频域特征，增强2D到3D学习；第二阶段用混合潜在特征优化人体网格的姿态和形状，设计低维网格姿态交互方法。

Result: 在大型公开数据集上的实验结果显示优于现有最先进方法。

Conclusion: 所提两阶段网络能有效解决现有方法问题，在不牺牲精度的前提下降低计算成本。

Abstract: Existing 3D human mesh recovery methods often fail to fully exploit the
latent information (e.g., human motion, shape alignment), leading to issues
with limb misalignment and insufficient local details in the reconstructed
human mesh (especially in complex scenes). Furthermore, the performance
improvement gained by modelling mesh vertices and pose node interactions using
attention mechanisms comes at a high computational cost. To address these
issues, we propose a two-stage network for human mesh recovery based on latent
information and low dimensional learning. Specifically, the first stage of the
network fully excavates global (e.g., the overall shape alignment) and local
(e.g., textures, detail) information from the low and high-frequency components
of image features and aggregates this information into a hybrid latent
frequency domain feature. This strategy effectively extracts latent
information. Subsequently, utilizing extracted hybrid latent frequency domain
features collaborates to enhance 2D poses to 3D learning. In the second stage,
with the assistance of hybrid latent features, we model the interaction
learning between the rough 3D human mesh template and the 3D pose, optimizing
the pose and shape of the human mesh. Unlike existing mesh pose interaction
methods, we design a low-dimensional mesh pose interaction method through
dimensionality reduction and parallel optimization that significantly reduces
computational costs without sacrificing reconstruction accuracy. Extensive
experimental results on large publicly available datasets indicate superiority
compared to the most state-of-the-art.

</details>


### [282] [StreamingTOM: Streaming Token Compression for Efficient Video Understanding](https://arxiv.org/abs/2510.18269)
*Xueyi Chen,Keda Tao,Kele Shao,Huan Wang*

Main category: cs.CV

TL;DR: 本文提出无训练即插即用框架StreamingTOM，解决视频流视觉语言模型因果性和累积性问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅调节LLM后kv缓存，未改变代价高的LLM前预填充，视频流视觉语言模型面临因果性和累积性约束。

Method: 提出StreamingTOM框架，因果时间缩减降低预填充成本，在线量化内存使kv缓存有界。

Result: 实现15.7倍kv缓存压缩，峰值内存降低1.2倍，TTFT快2倍，在无训练方法中保持SOTA准确率。

Conclusion: 两阶段方法对高效视频流理解有实际益处。

Abstract: Unlike offline processing, streaming video vision-language models face two
fundamental constraints: causality and accumulation. Causality prevents access
to future frames that offline methods exploit, while accumulation causes tokens
to grow unbounded, creating efficiency bottlenecks. However, existing
approaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill
unchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage
framework that addresses both pre-LLM and post-LLM bottlenecks with predictable
latency. Causal Temporal Reduction imposes a fixed per-frame budget and selects
tokens based on adjacent-frame changes and token saliency, drastically reducing
per-frame prefill cost by processing only a compact subset of visual tokens per
frame instead of all visual tokens. Online Quantized Memory stores tokens in
4-bit format, retrieves relevant groups on demand, and dequantizes them,
keeping the active kv-cache bounded regardless of stream length. Experiments
demonstrate our method achieves $15.7\times$ kv-cache compression, $1.2\times$
lower peak memory and $2\times$ faster TTFT compared to prior SOTA.
StreamingTOM maintains state-of-the-art accuracy among training-free methods
with an average of $63.8\%$ on offline benchmarks and $55.8\%/3.7$ on RVS.
These results highlight the practical benefits of our two-stage approach for
efficient streaming video understanding with bounded growth.

</details>


### [283] [Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models](https://arxiv.org/abs/2510.18287)
*Vishal Vinod*

Main category: cs.CV

TL;DR: 本文提出方法缓解3D人脸编辑限制，用少量标注图像估计潜空间编辑方向，还研究了一次性风格化和连续风格流形。


<details>
  <summary>Details</summary>
Motivation: 3D人脸身份保留编辑具有挑战性，存在需大规模标注数据集、高低分辨率编辑权衡等问题，旨在缓解这些限制。

Method: 基于3D感知深度生成模型和2D肖像编辑技术，利用含掩码的人脸数据集获取合成图像，估计潜空间编辑方向。

Result: 使用十张或更少标注图像足以估计对应3D感知属性编辑的潜空间编辑方向，还研究了一次性风格化和连续风格流形。

Conclusion: 所提方法能有效进行3D感知生成模型的少样本身份保留属性编辑。

Abstract: Identity preserving editing of faces is a generative task that enables
modifying the illumination, adding/removing eyeglasses, face aging, editing
hairstyles, modifying expression etc., while preserving the identity of the
face. Recent progress in 2D generative models have enabled photorealistic
editing of faces using simple techniques leveraging the compositionality in
GANs. However, identity preserving editing for 3D faces with a given set of
attributes is a challenging task as the generative model must reason about view
consistency from multiple poses and render a realistic 3D face. Further, 3D
portrait editing requires large-scale attribute labelled datasets and presents
a trade-off between editability in low-resolution and inflexibility to editing
in high resolution. In this work, we aim to alleviate some of the constraints
in editing 3D faces by identifying latent space directions that correspond to
photorealistic edits. To address this, we present a method that builds on
recent advancements in 3D-aware deep generative models and 2D portrait editing
techniques to perform efficient few-shot identity preserving attribute editing
for 3D-aware generative models. We aim to show from experimental results that
using just ten or fewer labelled images of an attribute is sufficient to
estimate edit directions in the latent space that correspond to 3D-aware
attribute editing. In this work, we leverage an existing face dataset with
masks to obtain the synthetic images for few attribute examples required for
estimating the edit directions. Further, to demonstrate the linearity of edits,
we investigate one-shot stylization by performing sequential editing and use
the (2D) Attribute Style Manipulation (ASM) technique to investigate a
continuous style manifold for 3D consistent identity preserving face aging.
Code and results are available at: https://vishal-vinod.github.io/gmpi-edit/

</details>


### [284] [S2AP: Score-space Sharpness Minimization for Adversarial Pruning](https://arxiv.org/abs/2510.18381)
*Giorgio Piras,Qi Zhao,Fabio Brau,Maura Pintor,Christian Wressnegger,Battista Biggio*

Main category: cs.CV

TL;DR: 提出S2AP方法解决对抗剪枝中掩码选择不稳定问题，实验证明其能提升对抗剪枝鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗剪枝方法在分数空间优化会导致鲁棒损失景观出现尖锐局部极小值，使掩码选择不稳定，降低鲁棒性。

Method: 提出Score - space Sharpness - aware Adversarial Pruning (S2AP)方法，在掩码搜索时通过扰动重要性分数并最小化相应鲁棒损失来最小化分数空间锐度。

Result: 在多个数据集、模型和稀疏度水平上的广泛实验表明，S2AP有效最小化了分数空间的锐度，稳定了掩码选择。

Conclusion: S2AP最终提高了对抗剪枝方法的鲁棒性。

Abstract: Adversarial pruning methods have emerged as a powerful tool for compressing
neural networks while preserving robustness against adversarial attacks. These
methods typically follow a three-step pipeline: (i) pretrain a robust model,
(ii) select a binary mask for weight pruning, and (iii) finetune the pruned
model. To select the binary mask, these methods minimize a robust loss by
assigning an importance score to each weight, and then keep the weights with
the highest scores. However, this score-space optimization can lead to sharp
local minima in the robust loss landscape and, in turn, to an unstable mask
selection, reducing the robustness of adversarial pruning methods. To overcome
this issue, we propose a novel plug-in method for adversarial pruning, termed
Score-space Sharpness-aware Adversarial Pruning (S2AP). Through our method, we
introduce the concept of score-space sharpness minimization, which operates
during the mask search by perturbing importance scores and minimizing the
corresponding robust loss. Extensive experiments across various datasets,
models, and sparsity levels demonstrate that S2AP effectively minimizes
sharpness in score space, stabilizing the mask selection, and ultimately
improving the robustness of adversarial pruning methods.

</details>


### [285] [Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models](https://arxiv.org/abs/2510.18457)
*Tianci Bi,Xiaoyi Zhang,Yan Lu,Nanning Zheng*

Main category: cs.CV

TL;DR: 提出VFM - VAE绕过蒸馏方法，重新设计解码器，引入SE - CKNNA指标和联合对齐策略，提升LDM性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有通过蒸馏融入视觉基础模型的方法会削弱与原模型对齐的鲁棒性，导致潜在变量语义偏差。

Method: 提出VFM - VAE；重新设计解码器；引入SE - CKNNA指标分析扩散训练表示动态；制定联合对齐策略。

Result: 系统在80个epoch时gFID（无CFG）达2.20，加速10倍；640个epoch时达1.62。

Conclusion: 直接集成视觉基础模型是LDM的更优范式。

Abstract: The performance of Latent Diffusion Models (LDMs) is critically dependent on
the quality of their visual tokenizer. While recent works have explored
incorporating Vision Foundation Models (VFMs) via distillation, we identify a
fundamental flaw in this approach: it inevitably weakens the robustness of
alignment with the original VFM, causing the aligned latents to deviate
semantically under distribution shifts. In this paper, we bypass distillation
by proposing a more direct approach: Vision Foundation Model Variational
Autoencoder (VFM-VAE). To resolve the inherent tension between the VFM's
semantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE
decoder with Multi-Scale Latent Fusion and Progressive Resolution
Reconstruction blocks, enabling high-quality reconstruction from spatially
coarse VFM features. Furthermore, we provide a comprehensive analysis of
representation dynamics during diffusion training, introducing the proposed
SE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows
us to develop a joint tokenizer-diffusion alignment strategy that dramatically
accelerates convergence. Our innovations in tokenizer design and training
strategy lead to superior performance and efficiency: our system reaches a gFID
(w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers).
With continued training to 640 epochs, it further attains a gFID (w/o CFG) of
1.62, establishing direct VFM integration as a superior paradigm for LDMs.

</details>


### [286] [Automated Wicket-Taking Delivery Segmentation and Weakness Detection in Cricket Videos Using OCR-Guided YOLOv8 and Trajectory Modeling](https://arxiv.org/abs/2510.18405)
*Mst Jannatun Ferdous,Masum Billah,Joy Karmoker,Mohd Ruhul Ameen,Akif Islam,Md. Omar Faruqe*

Main category: cs.CV

TL;DR: 本文提出板球视频分析自动化系统，用深度学习技术提取关键信息，实验证明其对板球分析有效。


<details>
  <summary>Details</summary>
Motivation: 开发自动化板球视频分析系统，为板球分析提供数据驱动见解，辅助教练和战略决策。

Method: 利用YOLOv8架构进行球场和球检测，结合OCR提取记分卡，通过图像预处理实现文本提取。

Result: 球场检测模型mAP50达99.5%，球检测模型mAP50达99.18%，能实现轨迹建模。

Conclusion: 该方法对自动化板球分析有效，有很大教练和战略决策应用潜力。

Abstract: This paper presents an automated system for cricket video analysis that
leverages deep learning techniques to extract wicket-taking deliveries, detect
cricket balls, and model ball trajectories. The system employs the YOLOv8
architecture for pitch and ball detection, combined with optical character
recognition (OCR) for scorecard extraction to identify wicket-taking moments.
Through comprehensive image preprocessing, including grayscale transformation,
power transformation, and morphological operations, the system achieves robust
text extraction from video frames. The pitch detection model achieved 99.5%
mean Average Precision at 50% IoU (mAP50) with a precision of 0.999, while the
ball detection model using transfer learning attained 99.18% mAP50 with 0.968
precision and 0.978 recall. The system enables trajectory modeling on detected
pitches, providing data-driven insights for identifying batting weaknesses.
Experimental results on multiple cricket match videos demonstrate the
effectiveness of this approach for automated cricket analytics, offering
significant potential for coaching and strategic decision-making.

</details>


### [287] [Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2510.18502)
*Wei-Chia Chang,Yan-Ann Chen*

Main category: cs.CV

TL;DR: 提出结合视觉语言模型和检索增强生成的管道以支持零样本车辆品牌和型号识别，实验显示比CLIP基线提升近20%。


<details>
  <summary>Details</summary>
Motivation: 现有车辆品牌和型号识别方法难以适应新车型，CLIP固定预训练权重需昂贵微调。

Method: 提出整合视觉语言模型和检索增强生成的管道，VLM将车辆图像转换为描述属性，与文本特征数据库对比，检索相关条目与描述组成提示，语言模型推理品牌和型号。

Result: 所提方法比CLIP基线识别率提高近20%。

Conclusion: 基于RAG增强的语言模型推理在智慧城市应用的可扩展车辆品牌和型号识别中有潜力。

Abstract: Vehicle make and model recognition (VMMR) is an important task in intelligent
transportation systems, but existing approaches struggle to adapt to newly
released models. Contrastive Language-Image Pretraining (CLIP) provides strong
visual-text alignment, yet its fixed pretrained weights limit performance
without costly image-specific finetuning. We propose a pipeline that integrates
vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to
support zero-shot recognition through text-based reasoning. A VLM converts
vehicle images into descriptive attributes, which are compared against a
database of textual features. Relevant entries are retrieved and combined with
the description to form a prompt, and a language model (LM) infers the make and
model. This design avoids large-scale retraining and enables rapid updates by
adding textual descriptions of new vehicles. Experiments show that the proposed
method improves recognition by nearly 20% over the CLIP baseline, demonstrating
the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city
applications.

</details>


### [288] [ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters](https://arxiv.org/abs/2510.18431)
*Zhiwei Hao,Jianyuan Guo,Li Shen,Kai Han,Yehui Tang,Han Hu,Yunhe Wang*

Main category: cs.CV

TL;DR: 提出ScaleNet高效扩展ViT模型，在ImageNet - 1K数据集实验证明其效率，在下游视觉任务有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有ViT模型训练计算量大、成本高，需高效扩展模型的方法。

Method: 在预训练ViT中插入额外层，利用层间权重共享，引入调整参数和并行适配器模块。

Result: 2×深度扩展的DeiT - Base模型，相比从头训练准确率提升7.42%，只需三分之一训练轮次。

Conclusion: ScaleNet能高效扩展ViT模型，在下游视觉任务有显著应用潜力。

Abstract: Recent advancements in vision transformers (ViTs) have demonstrated that
larger models often achieve superior performance. However, training these
models remains computationally intensive and costly. To address this challenge,
we introduce ScaleNet, an efficient approach for scaling ViT models. Unlike
conventional training from scratch, ScaleNet facilitates rapid model expansion
with negligible increases in parameters, building on existing pretrained
models. This offers a cost-effective solution for scaling up ViTs.
Specifically, ScaleNet achieves model expansion by inserting additional layers
into pretrained ViTs, utilizing layer-wise weight sharing to maintain
parameters efficiency. Each added layer shares its parameter tensor with a
corresponding layer from the pretrained model. To mitigate potential
performance degradation due to shared weights, ScaleNet introduces a small set
of adjustment parameters for each layer. These adjustment parameters are
implemented through parallel adapter modules, ensuring that each instance of
the shared parameter tensor remains distinct and optimized for its specific
function. Experiments on the ImageNet-1K dataset demonstrate that ScaleNet
enables efficient expansion of ViT models. With a 2$\times$ depth-scaled
DeiT-Base model, ScaleNet achieves a 7.42% accuracy improvement over training
from scratch while requiring only one-third of the training epochs,
highlighting its efficiency in scaling ViTs. Beyond image classification, our
method shows significant potential for application in downstream vision areas,
as evidenced by the validation in object detection task.

</details>


### [289] [CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder](https://arxiv.org/abs/2510.18583)
*Yongmin Lee,Hye Won Chung*

Main category: cs.CV

TL;DR: 提出CovMatch框架用于多模态数据集蒸馏，在Flickr30K和COCO上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态数据集蒸馏方法在跨模态对齐和计算成本上有挑战，且冻结文本编码器限制语义对齐和性能提升。

Method: 提出CovMatch框架，对齐真实和合成特征的交叉协方差，正则化各模态内特征分布，联合优化两个编码器。

Result: 在Flickr30K和COCO上评估，CovMatch优于现有多模态蒸馏方法，用500个合成对检索准确率绝对提升达6.8%。

Conclusion: CovMatch能实现更强的跨模态对齐和更好的性能。

Abstract: Multimodal dataset distillation aims to synthesize a small set of image-text
pairs that enables efficient training of large-scale vision-language models.
While dataset distillation has shown promise in unimodal tasks, extending it to
multimodal contrastive learning presents key challenges: learning cross-modal
alignment and managing the high computational cost of large encoders. Prior
approaches address scalability by freezing the text encoder and update only the
image encoder and text projection layer. However, we find this severely limits
semantic alignment and becomes a bottleneck for performance scaling. We propose
CovMatch, a scalable dataset distillation framework that aligns the
cross-covariance of real and synthetic features while regularizing feature
distributions within each modality. Unlike prior approaches, CovMatch enables
joint optimization of both encoders, leading to stronger cross-modal alignment
and improved performance. Evaluated on Flickr30K and COCO, CovMatch outperforms
state-of-the-art multimodal distillation methods and achieves up to 6.8%
absolute gains in retrieval accuracy using only 500 synthetic pairs.

</details>


### [290] [C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression](https://arxiv.org/abs/2510.18636)
*Baptiste Bauvin,Loïc Baret,Ola Ahmad*

Main category: cs.CV

TL;DR: 提出基于可解释深度学习的一次性剪枝框架，在减少模型大小同时性能损失小。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝计算成本高，一次性剪枝会导致性能大幅下降。

Method: 引入因果感知剪枝方法，利用模型预测与结构间因果关系进行渐进式剪枝。

Result: 在卷积神经网络和视觉变压器基线实验中，实现模型大小显著减少，性能影响小且无需微调。

Conclusion: 该方法优于同类方法，实现最佳权衡，代码已开源。

Abstract: Neural network compression has gained increasing attention in recent years,
particularly in computer vision applications, where the need for model
reduction is crucial for overcoming deployment constraints. Pruning is a widely
used technique that prompts sparsity in model structures, e.g. weights,
neurons, and layers, reducing size and inference costs. Structured pruning is
especially important as it allows for the removal of entire structures, which
further accelerates inference time and reduces memory overhead. However, it can
be computationally expensive, requiring iterative retraining and optimization.
To overcome this problem, recent methods considered one-shot setting, which
applies pruning directly at post-training. Unfortunately, they often lead to a
considerable drop in performance. In this paper, we focus on this issue by
proposing a novel one-shot pruning framework that relies on explainable deep
learning. First, we introduce a causal-aware pruning approach that leverages
cause-effect relations between model predictions and structures in a
progressive pruning process. It allows us to efficiently reduce the size of the
network, ensuring that the removed structures do not deter the performance of
the model. Then, through experiments conducted on convolution neural network
and vision transformer baselines, pre-trained on classification tasks, we
demonstrate that our method consistently achieves substantial reductions in
model size, with minimal impact on performance, and without the need for
fine-tuning. Overall, our approach outperforms its counterparts, offering the
best trade-off. Our code is available on GitHub.

</details>


### [291] [ε-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data](https://arxiv.org/abs/2510.18637)
*Sheida Rahnamai Kordasiabi,Damian Dalle Nogare,Florian Jug*

Main category: cs.CV

TL;DR: 提出基于HVAEs的ε - Seg方法用于生物样本EM图像语义分割，在有限训练标签下取得有竞争力结果。


<details>
  <summary>Details</summary>
Motivation: 生物样本电子显微镜（EM）图像语义分割是生命科学中的挑战，EM数据复杂，人类观察都有困难。

Method: 引入ε - Seg方法，基于HVAEs，采用中心区域掩码、稀疏标签对比学习（CL）、高斯混合模型（GMM）先验和无聚类标签预测，用MLP语义分割头直接从潜在嵌入预测类别标签。

Result: 在2个生物组织密集EM数据集和荧光显微镜数据上展示结果，ε - Seg在有限训练标签下能在复杂生物图像数据上取得有竞争力的稀疏监督分割结果。

Conclusion: ε - Seg在复杂生物图像数据的稀疏监督语义分割中表现良好，即使训练标签有限也能有不错效果。

Abstract: Semantic segmentation of electron microscopy (EM) images of biological
samples remains a challenge in the life sciences. EM data captures details of
biological structures, sometimes with such complexity that even human observers
can find it overwhelming. We introduce {\epsilon}-Seg, a method based on
hierarchical variational autoencoders (HVAEs), employing center-region masking,
sparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior,
and clustering-free label prediction. Center-region masking and the inpainting
loss encourage the model to learn robust and representative embeddings to
distinguish the desired classes, even if training labels are sparse (0.05% of
the total image data or less). For optimal performance, we employ CL and a GMM
prior to shape the latent space of the HVAE such that encoded input patches
tend to cluster wrt. the semantic classes we wish to distinguish. Finally,
instead of clustering latent embeddings for semantic segmentation, we propose a
MLP semantic segmentation head to directly predict class labels from latent
embeddings. We show empirical results of {\epsilon}-Seg and baseline methods on
2 dense EM datasets of biological tissues and demonstrate the applicability of
our method also on fluorescence microscopy data. Our results show that
{\epsilon}-Seg is capable of achieving competitive sparsely-supervised
segmentation results on complex biological image data, even if only limited
amounts of training labels are available.

</details>


### [292] [Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model](https://arxiv.org/abs/2510.18573)
*Zhenxing Zhang,Jiayan Teng,Zhuoyi Yang,Tiankun Cao,Cheng Wang,Xiaotao Gu,Jie Tang,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: 提出S2V生成框架Kaleido，通过数据构建和R - RoPE处理参考图像，在多基准测试中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有S2V生成模型在多主体一致性和背景分离方面存在不足，如参考保真度低、语义漂移，原因包括训练数据集缺乏多样性和高质量样本、多图像整合机制不佳。

Method: 提出专用数据构建管道，进行低质量样本过滤和多样数据合成；引入Reference Rotary Positional Encoding (R - RoPE)处理参考图像。

Result: 在众多基准测试中，Kaleido在一致性、保真度和泛化能力上显著优于先前方法。

Conclusion: Kaleido推动了S2V生成的发展。

Abstract: We present Kaleido, a subject-to-video~(S2V) generation framework, which aims
to synthesize subject-consistent videos conditioned on multiple reference
images of target subjects. Despite recent progress in S2V generation models,
existing approaches remain inadequate at maintaining multi-subject consistency
and at handling background disentanglement, often resulting in lower reference
fidelity and semantic drift under multi-image conditioning. These shortcomings
can be attributed to several factors. Primarily, the training dataset suffers
from a lack of diversity and high-quality samples, as well as cross-paired
data, i.e., paired samples whose components originate from different instances.
In addition, the current mechanism for integrating multiple reference images is
suboptimal, potentially resulting in the confusion of multiple subjects. To
overcome these limitations, we propose a dedicated data construction pipeline,
incorporating low-quality sample filtering and diverse data synthesis, to
produce consistency-preserving training data. Moreover, we introduce Reference
Rotary Positional Encoding (R-RoPE) to process reference images, enabling
stable and precise multi-image integration. Extensive experiments across
numerous benchmarks demonstrate that Kaleido significantly outperforms previous
methods in consistency, fidelity, and generalization, marking an advance in S2V
generation.

</details>


### [293] [Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views](https://arxiv.org/abs/2510.18632)
*Zhangquan Chen,Manyuan Zhang,Xinlei Yu,Xufang Luo,Mingze Sun,Zihao Pan,Yan Feng,Peng Pei,Xunliang Cai,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出3DThinker框架解决从有限视图理解3D空间关系难题，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在从有限视图理解3D空间关系存在挑战，以往推理方法表征能力有限，难以满足3D空间想象任务需求。

Method: 提出3DThinker框架，训练分两阶段，先监督训练对齐VLM推理时生成的3D隐层与3D基础模型的3D隐层，再基于结果信号优化推理轨迹。

Result: 在多个基准上的大量实验表明，3DThinker始终优于强基线。

Conclusion: 3DThinker为将3D表示统一到多模态推理中提供了新视角。

Abstract: Though recent advances in vision-language models (VLMs) have achieved
remarkable progress across a wide range of multimodal tasks, understanding 3D
spatial relationships from limited views remains a significant challenge.
Previous reasoning methods typically rely on pure text (e.g., topological
cognitive maps) or on 2D visual cues. However, their limited representational
capacity hinders performance in specific tasks that require 3D spatial
imagination. To address this limitation, we propose 3DThinker, a framework that
can effectively exploits the rich geometric information embedded within images
while reasoning, like humans do. Our framework is the first to enable 3D
mentaling during reasoning without any 3D prior input, and it does not rely on
explicitly labeled 3D data for training. Specifically, our training consists of
two stages. First, we perform supervised training to align the 3D latent
generated by VLM while reasoning with that of a 3D foundation model (e.g.,
VGGT). Then, we optimize the entire reasoning trajectory solely based on
outcome signals, thereby refining the underlying 3D mentaling. Extensive
experiments across multiple benchmarks show that 3DThinker consistently
outperforms strong baselines and offers a new perspective toward unifying 3D
representations into multimodal reasoning. Our code will be available at
https://github.com/zhangquanchen/3DThinker.

</details>


### [294] [An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom Detection](https://arxiv.org/abs/2510.18819)
*Neel Patel,Alexander Wong,Ashkan Ebadi*

Main category: cs.CV

TL;DR: 提出教师 - 学生框架用于胸部X光片疾病和症状检测，表现优于基线，有临床应用前景。


<details>
  <summary>Details</summary>
Motivation: 结核病是全球健康问题，早期检测重要但缺专业人员，开发可靠AI模型缺高质量数据集。

Method: 提出教师 - 学生框架，集成两个监督头和一个自监督头进行胸部X光片疾病和症状检测。

Result: 区分COVID - 19、结核病和正常病例准确率达98.85%，多标签症状检测宏F1分数90.09%，优于基线。

Conclusion: 模型基于相关解剖特征预测，有临床筛查和分诊应用前景。

Abstract: Tuberculosis remains a critical global health issue, particularly in
resource-limited and remote areas. Early detection is vital for treatment, yet
the lack of skilled radiologists underscores the need for artificial
intelligence (AI)-driven screening tools. Developing reliable AI models is
challenging due to the necessity for large, high-quality datasets, which are
costly to obtain. To tackle this, we propose a teacher--student framework which
enhances both disease and symptom detection on chest X-rays by integrating two
supervised heads and a self-supervised head. Our model achieves an accuracy of
98.85% for distinguishing between COVID-19, tuberculosis, and normal cases, and
a macro-F1 score of 90.09% for multilabel symptom detection, significantly
outperforming baselines. The explainability assessments also show the model
bases its predictions on relevant anatomical features, demonstrating promise
for deployment in clinical screening and triage settings.

</details>


### [295] [DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution](https://arxiv.org/abs/2510.18851)
*Rongyuan Wu,Lingchen Sun,Zhengqiang Zhang,Shihao Wang,Tianhe Wu,Qiaosi Yi,Shuai Li,Lei Zhang*

Main category: cs.CV

TL;DR: 提出DP²O - SR框架用于真实图像超分辨率，结合IQA模型构建混合奖励信号，利用感知多样性，提出分层偏好优化，实验证明其能提升感知质量和泛化性。


<details>
  <summary>Details</summary>
Motivation: T2I模型随机性使输出感知质量有差异，利用这种差异提升Real - ISR性能。

Method: 构建结合全参考和无参考IQA模型的混合奖励信号；构建多个偏好对；提出分层偏好优化。

Result: 在扩散和基于流的T2I骨干网络实验中，DP²O - SR显著提升感知质量，在真实基准上泛化性好。

Conclusion: DP²O - SR框架能有效提升Real - ISR的感知质量，且具有良好泛化性。

Abstract: Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world
image super-resolution (Real-ISR) methods can synthesize rich and realistic
details. However, due to the inherent stochasticity of T2I models, different
noise inputs often lead to outputs with varying perceptual quality. Although
this randomness is sometimes seen as a limitation, it also introduces a wider
perceptual quality range, which can be exploited to improve Real-ISR
performance. To this end, we introduce Direct Perceptual Preference
Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative
models with perceptual preferences without requiring costly human annotations.
We construct a hybrid reward signal by combining full-reference and
no-reference image quality assessment (IQA) models trained on large-scale human
preference datasets. This reward encourages both structural fidelity and
natural appearance. To better utilize perceptual diversity, we move beyond the
standard best-vs-worst selection and construct multiple preference pairs from
outputs of the same model. Our analysis reveals that the optimal selection
ratio depends on model capacity: smaller models benefit from broader coverage,
while larger models respond better to stronger contrast in supervision.
Furthermore, we propose hierarchical preference optimization, which adaptively
weights training pairs based on intra-group reward gaps and inter-group
diversity, enabling more efficient and stable learning. Extensive experiments
across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR
significantly improves perceptual quality and generalizes well to real-world
benchmarks.

</details>


### [296] [Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs](https://arxiv.org/abs/2510.18876)
*Haochen Wang,Yuhao Wang,Tao Zhang,Yikang Zhou,Yanwei Li,Jiacong Wang,Ye Tian,Jiahao Meng,Zilong Huang,Guangcan Mai,Anran Wang,Yunhai Tong,Zhuochen Wang,Xiangtai Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 提出用于区域级视觉理解的Grasp Any Region (GAR) 及GAR - Bench，实验表明GAR性能优越且能力可迁移到视频。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在复杂场景细粒度分析有困难，现有区域级MLLMs忽略全局上下文。

Method: 引入GAR，采用RoI - aligned特征重放技术，构建GAR - Bench。

Result: GAR - 1B在DLC - Bench、GAR - Bench - VQA表现出色，零-shot GAR - 8B在VideoRefer - BenchQ超过VideoRefer - 7B。

Conclusion: GAR能利用全局上下文进行精确感知、建模交互和推理，能力可迁移到视频。

Abstract: While Multimodal Large Language Models (MLLMs) excel at holistic
understanding, they struggle in capturing the dense world with complex scenes,
requiring fine-grained analysis of intricate details and object
inter-relationships. Region-level MLLMs have been a promising step. However,
previous attempts are generally optimized to understand given regions in
isolation, neglecting crucial global contexts. To address this, we introduce
Grasp Any Region (GAR) for comprehen- sive region-level visual understanding.
Empowered by an effective RoI-aligned feature replay technique, GAR supports
(1) precise perception by leveraging necessary global contexts, and (2)
modeling interactions between multiple prompts. Together, it then naturally
achieves (3) advanced compositional reasoning to answer specific free-form
questions about any region, shifting the paradigm from passive description to
active dialogue. Moreover, we construct GAR-Bench, which not only provides a
more accurate evaluation of single-region comprehension, but also, more
importantly, measures interactions and complex reasoning across multiple
regions. Extensive experiments have demonstrated that GAR-1B not only maintains
the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5
on DLC-Bench, but also excels at modeling relationships between multiple
prompts with advanced comprehension capabilities, even surpassing InternVL3-78B
on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms
in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong
capabilities can be easily transferred to videos.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [297] [Censorship Chokepoints: New Battlegrounds for Regional Surveillance, Censorship and Influence on the Internet](https://arxiv.org/abs/2510.18394)
*Yong Zhang,Nishanth Sastry*

Main category: cs.CR

TL;DR: 互联网虽为重要信息渠道，但多地存在网络审查，传统分类有局限，提出用‘关键点’视角理解现代审查。


<details>
  <summary>Details</summary>
Motivation: 互联网发展使新审查技术出现，传统分类难以应对，为更好理解现代审查。

Method: 提出‘关键点’这一新视角来理解现代审查。

Result: 指出可通过‘关键点’视角识别内容生产或传递周期中的瓶颈，有新的大规模客户端监控和过滤机制。

Conclusion: 现代审查可通过‘关键点’这一新视角得到更好理解。

Abstract: Undoubtedly, the Internet has become one of the most important conduits to
information for the general public. Nonetheless, Internet access can be and has
been limited systematically or blocked completely during political events in
numerous countries and regions by various censorship mechanisms. Depending on
where the core filtering component is situated, censorship techniques have been
classified as client-based, server-based, or network-based. However, as the
Internet evolves rapidly, new and sophisticated censorship techniques have
emerged, which involve techniques that cut across locations and involve new
forms of hurdles to information access. We argue that modern censorship can be
better understood through a new lens that we term chokepoints, which identifies
bottlenecks in the content production or delivery cycle where efficient new
forms of large-scale client-side surveillance and filtering mechanisms have
emerged.

</details>


### [298] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: 本文提出VERA - V框架解决现有多模态红队方法局限性，能生成绕过模型防护的对抗输入，实验显示其表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态红队方法依赖脆弱模板、聚焦单攻击设置且仅暴露少量漏洞，需新方法解决。

Method: 引入VERA - V变分推理框架，将多模态越狱发现转化为学习文本 - 图像提示对的联合后验分布，训练轻量级攻击者近似后验，还集成三种互补策略。

Result: 在HarmBench和HADES基准测试中，VERA - V在开源和前沿VLM上均优于现有基线，在GPT - 4o上攻击成功率比最佳基线高53.75%。

Conclusion: VERA - V是一种有效的多模态越狱发现方法，能更全面地暴露Vision - Language Models的漏洞。

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>


### [299] [sNVMe-oF: Secure and Efficient Disaggregated Storage](https://arxiv.org/abs/2510.18756)
*Marcin Chrapek,Meni Orenbach,Ahmad Atamli,Marcin Copik,Fritz Alder,Torsten Hoefler*

Main category: cs.CR

TL;DR: 本文介绍了sNVMe - oF存储管理系统，它扩展NVMe - oF协议，遵循机密计算威胁模型，优化性能且资源使用合理，原型测试显示性能损失小。


<details>
  <summary>Details</summary>
Motivation: 传统机密计算方法保护先进存储时难以扩展，会牺牲性能或安全性。

Method: 扩展NVMe - oF协议，提供控制路径和新的概念，利用NVMe元数据，引入新的HMT，避免冗余IPSec保护，使用支持CC的智能网卡加速器。

Result: 在NVIDIA BlueField - 3上进行原型测试，对于合成模式和AI训练，性能下降仅2%。

Conclusion: sNVMe - oF能在遵循机密计算威胁模型的同时，实现较好的性能和资源使用效果。

Abstract: Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the
standard solution in modern data centers, achieving superior performance,
resource utilization, and power efficiency. Simultaneously, confidential
computing (CC) is becoming the de facto security paradigm, enforcing stronger
isolation and protection for sensitive workloads. However, securing
state-of-the-art storage with traditional CC methods struggles to scale and
compromises performance or security. To address these issues, we introduce
sNVMe-oF, a storage management system extending the NVMe-oF protocol and
adhering to the CC threat model by providing confidentiality, integrity, and
freshness guarantees. sNVMe-oF offers an appropriate control path and novel
concepts such as counter-leasing. sNVMe-oF also optimizes data path performance
by leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree
(HMT), and avoiding redundant IPSec protections. We achieve this without
modifying the NVMe-oF protocol. To prevent excessive resource usage while
delivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs.
We prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can
achieve as little as 2% performance degradation for synthetic patterns and AI
training.

</details>


### [300] [RiskTagger: An LLM-based Agent for Automatic Annotation of Web3 Crypto Money Laundering Behaviors](https://arxiv.org/abs/2510.17848)
*Dan Lin,Yanli Ding,Weipeng Zou,Jiachi Chen,Xiapu Luo,Jiajing Wu,Zibin Zheng*

Main category: cs.CR

TL;DR: 本文介绍基于大语言模型的RiskTagger用于Web3加密洗钱行为自动标注，实验效果良好，提升反洗钱研究透明度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: Web3中链上洗钱行为隐蔽复杂，构建高质量反洗钱数据集依赖人工，效率和覆盖度有限。

Method: 实现端到端多模块代理，集成关键线索提取器、多链获取器、洗钱行为推理器和数据解释器形成数据标注管道。

Result: 在Bybit Hack真实案例实验中，线索提取准确率达100%，与专家判断一致性达84.1%，解释生成覆盖率达90%。

Conclusion: RiskTagger实现洗钱行为标注自动化，提升反洗钱研究的透明度和可扩展性。

Abstract: While the rapid growth of Web3 has driven the development of decentralized
finance, user anonymity and cross-chain asset flows make on-chain laundering
behaviors more covert and complex. In this context, constructing high-quality
anti-money laundering(AML) datasets has become essential for risk-control
systems and on-chain forensic analysis, yet current practices still rely
heavily on manual efforts with limited efficiency and coverage. In this paper,
we introduce RiskTagger, a large-language-model-based agent for the automatic
annotation of crypto laundering behaviors in Web3. RiskTagger is designed to
replace or complement human annotators by addressing three key challenges:
extracting clues from complex unstructured reports, reasoning over multichain
transaction paths, and producing auditor-friendly explanations. RiskTagger
implements an end-to-end multi-module agent, integrating a key-clue extractor,
a multichain fetcher with a laundering-behavior reasoner, and a data explainer,
forming a data annotation pipeline. Experiments on the real case Bybit Hack
(with the highest stolen asset value) demonstrate that RiskTagger achieves 100%
accuracy in clue extraction, 84.1% consistency with expert judgment, and 90%
coverage in explanation generation. Overall, RiskTagger automates laundering
behavior annotation while improving transparency and scalability in AML
research.

</details>


### [301] [When "Correct" Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents?](https://arxiv.org/abs/2510.17862)
*Yibo Peng,James Song,Lei Li,Xinyu Yang,Mihai Christodorescu,Ravi Mangal,Corina Pasareanu,Haizhong Zheng,Beidi Chen*

Main category: cs.CR

TL;DR: 论文揭示代码代理存在功能正确但有漏洞（FCV）补丁的威胁，提出FCV - Attack，证明SOTA大模型和代理框架易受攻击，呼吁开发安全防御机制。


<details>
  <summary>Details</summary>
Motivation: 当前代码代理安全评估主要关注功能正确性，论文旨在揭示新的安全威胁。

Method: 提出FCV - Attack，对12种代理 - 模型组合进行攻击测试。

Result: SOTA大模型和代理框架都易受FCV威胁，如在CWE - 538上，GPT - 5 Mini + OpenHands的攻击成功率达40.7%。

Conclusion: 当前评估范式忽略了重要安全威胁，需为代码代理开发安全防御机制。

Abstract: Code agents are increasingly trusted to autonomously fix bugs on platforms
such as GitHub, yet their security evaluation focuses almost exclusively on
functional correctness. In this paper, we reveal a novel type of threat to
real-world code agents: Functionally Correct yet Vulnerable (FCV) patches,
which pass all test cases but contain vulnerable code. With our proposed
FCV-Attack, which can be deliberately crafted by malicious attackers or
implicitly introduced by benign developers, we show that SOTA LLMs (e.g.,
ChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all
vulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench,
the attack only requires black-box access and a single query to the code agent
to perform the attack. For example, for CWE-538 (information exposure
vulnerability), the FCV-Attack attains an attack success rate of $40.7\%$ on
GPT-5 Mini + OpenHands. Our results reveal an important security threat
overlooked by current evaluation paradigms and urge the development of
security-aware defenses for code agents.

</details>


### [302] [RESCUE: Retrieval Augmented Secure Code Generation](https://arxiv.org/abs/2510.18204)
*Jiahao Shi,Tianyi Zhang*

Main category: cs.CR

TL;DR: 现有大语言模型生成代码有安全漏洞，传统RAG有缺陷，本文提出RESCUE框架，经实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成的代码存在安全漏洞，传统RAG设计受原始安全文档噪声影响，现有检索方法忽略任务描述中的安全语义。

Method: 提出混合知识库构建方法，结合大语言模型辅助的聚类-总结蒸馏和程序切片；设计分层多方面检索方法遍历知识库。

Result: 在四个基准测试和六个大语言模型上与五种先进方法对比，平均提高SecurePass@1指标4.8分，达到新的安全性能水平。

Conclusion: RESCUE框架有效，通过深入分析和消融实验验证了各组件的有效性。

Abstract: Despite recent advances, Large Language Models (LLMs) still generate
vulnerable code. Retrieval-Augmented Generation (RAG) has the potential to
enhance LLMs for secure code generation by incorporating external security
knowledge. However, the conventional RAG design struggles with the noise of raw
security-related documents, and existing retrieval methods overlook the
significant security semantics implicitly embedded in task descriptions. To
address these issues, we propose RESCUE, a new RAG framework for secure code
generation with two key innovations. First, we propose a hybrid knowledge base
construction method that combines LLM-assisted cluster-then-summarize
distillation with program slicing, producing both high-level security
guidelines and concise, security-focused code examples. Second, we design a
hierarchical multi-faceted retrieval to traverse the constructed knowledge base
from top to bottom and integrates multiple security-critical facts at each
hierarchical level, ensuring comprehensive and accurate retrieval. We evaluated
RESCUE on four benchmarks and compared it with five state-of-the-art secure
code generation methods on six LLMs. The results demonstrate that RESCUE
improves the SecurePass@1 metric by an average of 4.8 points, establishing a
new state-of-the-art performance for security. Furthermore, we performed
in-depth analysis and ablation studies to rigorously validate the effectiveness
of individual components in RESCUE.

</details>


### [303] [Evaluating Large Language Models in detecting Secrets in Android Apps](https://arxiv.org/abs/2510.18601)
*Marco Alecci,Jordan Samhi,Tegawendé F. Bissyandé,Jacques Klein*

Main category: cs.CR

TL;DR: 本文提出基于LLM的SecretLoc检测安卓应用硬编码密钥，能发现现有方法遗漏的密钥，揭示移动生态需主动密钥管理。


<details>
  <summary>Details</summary>
Motivation: 现有安卓应用硬编码密钥检测方法依赖先验知识，有局限性，需新检测方法。

Method: 提出基于LLM的SecretLoc方法，利用上下文和结构线索识别密钥，不依赖预定义模式或标记训练集。

Result: 用基准数据集发现4828个现有方法未检测到的密钥，超10种新类型；爬取5000个应用，2124个含密钥。

Conclusion: 指出若分析师能用LLM发现密钥，攻击者也能，强调移动生态需主动密钥管理和更强缓解措施。

Abstract: Mobile apps often embed authentication secrets, such as API keys, tokens, and
client IDs, to integrate with cloud services. However, developers often
hardcode these credentials into Android apps, exposing them to extraction
through reverse engineering. Once compromised, adversaries can exploit secrets
to access sensitive data, manipulate resources, or abuse APIs, resulting in
significant security and financial risks. Existing detection approaches, such
as regex-based analysis, static analysis, and machine learning, are effective
for identifying known patterns but are fundamentally limited: they require
prior knowledge of credential structures, API signatures, or training data.
  In this paper, we propose SecretLoc, an LLM-based approach for detecting
hardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it
leverages contextual and structural cues to identify secrets without relying on
predefined patterns or labeled training sets. Using a benchmark dataset from
the literature, we demonstrate that SecretLoc detects secrets missed by regex-,
static-, and ML-based methods, including previously unseen types of secrets. In
total, we discovered 4828 secrets that were undetected by existing approaches,
discovering more than 10 "new" types of secrets, such as OpenAI API keys,
GitHub Access Tokens, RSA private keys, and JWT tokens, and more.
  We further extend our analysis to newly crawled apps from Google Play, where
we uncovered and responsibly disclosed additional hardcoded secrets. Across a
set of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which
were confirmed and remediated by developers after we contacted them. Our
results reveal a dual-use risk: if analysts can uncover these secrets with
LLMs, so can attackers. This underscores the urgent need for proactive secret
management and stronger mitigation practices across the mobile ecosystem.

</details>


### [304] [From Flows to Words: Can Zero-/Few-Shot LLMs Detect Network Intrusions? A Grammar-Constrained, Calibrated Evaluation on UNSW-NB15](https://arxiv.org/abs/2510.17883)
*Mohammad Abdul Rehman,Syed Imad Ali Shah,Abbas n=Anwar,Noor Islam*

Main category: cs.CR

TL;DR: 研究评估了大语言模型在无微调下用于入侵检测的仅提示方法，对比不同提示方式与基线，发现指令加标志可提升检测质量，虽决策质量受数据集大小影响，但该方法无需梯度训练、易适配。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在无微调情况下用于入侵检测的作用。

Method: 将网络流转换为文本记录并添加布尔标志，约束模型输出，校准决策阈值，对比零样本、指令引导和少样本提示与基线。

Result: 无引导提示不可靠，指令加标志提升检测质量，校准评分使结果更稳定；不同模型在不同规模数据集上有不同表现；决策质量受数据集大小影响。

Conclusion: 提出流到文本协议、校准方法，进行系统基线比较并提供可复现包，仅提示方法无需梯度训练、输出可读、易适配。

Abstract: Large Language Models (LLMs) can reason over natural-language inputs, but
their role in intrusion detection without fine-tuning remains uncertain. This
study evaluates a prompt-only approach on UNSW-NB15 by converting each network
flow to a compact textual record and augmenting it with lightweight,
domain-inspired boolean flags (asymmetry, burst rate, TTL irregularities, timer
anomalies, rare service/state, short bursts). To reduce output drift and
support measurement, the model is constrained to produce structured,
grammar-valid responses, and a single decision threshold is calibrated on a
small development split. We compare zero-shot, instruction-guided, and few-shot
prompting to strong tabular and neural baselines under identical splits,
reporting accuracy, precision, recall, F1, and macro scores. Empirically,
unguided prompting is unreliable, while instructions plus flags substantially
improve detection quality; adding calibrated scoring further stabilizes
results. On a balanced subset of two hundred flows, a 7B instruction-tuned
model with flags reaches macro-F1 near 0.78; a lighter 3B model with few-shot
cues and calibration attains F1 near 0.68 on one thousand examples. As the
evaluation set grows to two thousand flows, decision quality decreases,
revealing sensitivity to coverage and prompting. Tabular baselines remain more
stable and faster, yet the prompt-only pipeline requires no gradient training,
produces readable artifacts, and adapts easily through instructions and flags.
Contributions include a flow-to-text protocol with interpretable cues, a
calibration method for thresholding, a systematic baseline comparison, and a
reproducibility bundle with prompts, grammar, metrics, and figures.

</details>


### [305] [When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking](https://arxiv.org/abs/2510.17884)
*Mohammad Abdul Rehman,Syed Imad Ali Shah,Abbas Anwar,Noor Islam*

Main category: cs.CR

TL;DR: 研究评估预训练大语言模型用于密码破解的效果，发现其表现不佳，传统方法成功率更高，指出大语言模型在密码推断上的局限。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在密码破解等网络安全应用的潜力。

Method: 使用合成用户配置文件，评估TinyLLaMA、Falcon - RW - 1B和Flan - T5等开源大语言模型，根据用户属性提示生成密码。

Result: 使用Hit@1、Hit@5和Hit@10指标衡量，所有模型在Hit@10的准确率低于1.5%，传统方法成功率更高。

Conclusion: 当前大语言模型缺乏有效密码推断所需的领域适应和记忆能力，研究为未来密码建模奠定基础。

Abstract: The remarkable capabilities of Large Language Models (LLMs) in natural
language understanding and generation have sparked interest in their potential
for cybersecurity applications, including password guessing. In this study, we
conduct an empirical investigation into the efficacy of pre-trained LLMs for
password cracking using synthetic user profiles. Specifically, we evaluate the
performance of state-of-the-art open-source LLMs such as TinyLLaMA,
Falcon-RW-1B, and Flan-T5 by prompting them to generate plausible passwords
based on structured user attributes (e.g., name, birthdate, hobbies). Our
results, measured using Hit@1, Hit@5, and Hit@10 metrics under both plaintext
and SHA-256 hash comparisons, reveal consistently poor performance, with all
models achieving less than 1.5% accuracy at Hit@10. In contrast, traditional
rule-based and combinator-based cracking methods demonstrate significantly
higher success rates. Through detailed analysis and visualization, we identify
key limitations in the generative reasoning of LLMs when applied to the
domain-specific task of password guessing. Our findings suggest that, despite
their linguistic prowess, current LLMs lack the domain adaptation and
memorization capabilities required for effective password inference, especially
in the absence of supervised fine-tuning on leaked password datasets. This
study provides critical insights into the limitations of LLMs in adversarial
contexts and lays the groundwork for future efforts in secure,
privacy-preserving, and robust password modeling.

</details>


### [306] [BreakFun: Jailbreaking LLMs via Schema Exploitation](https://arxiv.org/abs/2510.17904)
*Amirkia Rafiei Oskooei,Mehmet S. Aktas*

Main category: cs.CR

TL;DR: 本文提出BreakFun越狱方法揭示大语言模型因遵循结构规则的漏洞，成功率高，还引入对抗性提示解构护栏进行防御。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型处理结构化数据和遵循句法规则能力带来的漏洞。

Method: 采用BreakFun越狱方法，使用包含无辜框架、思维链干扰和“特洛伊模式”的三部分提示进行攻击；引入对抗性提示解构护栏，利用二级大语言模型进行“文字转录”防御。

Result: 攻击在13个模型上平均成功率89%，部分模型达100%；防御方法对攻击有效。

Conclusion: 大语言模型的核心优势可转化为关键弱点，针对欺骗性模式防御是可行策略，为构建更稳健对齐模型提供新视角。

Abstract: The proficiency of Large Language Models (LLMs) in processing structured data
and adhering to syntactic rules is a capability that drives their widespread
adoption but also makes them paradoxically vulnerable. In this paper, we
investigate this vulnerability through BreakFun, a jailbreak methodology that
weaponizes an LLM's adherence to structured schemas. BreakFun employs a
three-part prompt that combines an innocent framing and a Chain-of-Thought
distraction with a core "Trojan Schema"--a carefully crafted data structure
that compels the model to generate harmful content, exploiting the LLM's strong
tendency to follow structures and schemas. We demonstrate this vulnerability is
highly transferable, achieving an average success rate of 89% across 13
foundational and proprietary models on JailbreakBench, and reaching a 100%
Attack Success Rate (ASR) on several prominent models. A rigorous ablation
study confirms this Trojan Schema is the attack's primary causal factor. To
counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a
defense that utilizes a secondary LLM to perform a "Literal
Transcription"--extracting all human-readable text to isolate and reveal the
user's true harmful intent. Our proof-of-concept guardrail demonstrates high
efficacy against the attack, validating that targeting the deceptive schema is
a viable mitigation strategy. Our work provides a look into how an LLM's core
strengths can be turned into critical weaknesses, offering a fresh perspective
for building more robustly aligned models.

</details>


### [307] [ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection](https://arxiv.org/abs/2510.17919)
*Tenghui Huang,Jinbo Wen,Jiawen Kang,Siyong Chen,Zhengtao Li,Tao Zhang,Dongning Liu,Jiacheng Wang,Chengjun Cai,Yinqiu Liu,Dusit Niyato*

Main category: cs.CR

TL;DR: 本文提出ParaVul框架改进智能合约漏洞检测的可靠性和准确性，模拟结果显示其表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法有高误报率和可扩展性差问题，现有大语言模型存在推理成本高和计算开销大的挑战。

Method: 提出ParaVul框架，包括开发SLoRA用于大语言模型微调、构建漏洞合约数据集和混合检索增强生成系统、提出元学习模型融合输出、设计思维链提示生成检测报告。

Result: 模拟结果显示ParaVul在F1分数上表现优越，单标签检测达0.9398，多标签检测达0.9330。

Conclusion: ParaVul能有效提高智能合约漏洞检测的可靠性和准确性。

Abstract: Smart contracts play a significant role in automating blockchain services.
Nevertheless, vulnerabilities in smart contracts pose serious threats to
blockchain security. Currently, traditional detection methods primarily rely on
static analysis and formal verification, which can result in high
false-positive rates and poor scalability. Large Language Models (LLMs) have
recently made significant progress in smart contract vulnerability detection.
However, they still face challenges such as high inference costs and
substantial computational overhead. In this paper, we propose ParaVul, a
parallel LLM and retrieval-augmented framework to improve the reliability and
accuracy of smart contract vulnerability detection. Specifically, we first
develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA
introduces sparsification by incorporating a sparse matrix into quantized
LoRA-based LLMs, thereby reducing computational overhead and resource
requirements while enhancing their ability to understand vulnerability-related
issues. We then construct a vulnerability contract dataset and develop a hybrid
Retrieval-Augmented Generation (RAG) system that integrates dense retrieval
with Best Matching 25 (BM25), assisting in verifying the results generated by
the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of
the RAG system and the LLM, thereby generating the final detection results.
After completing vulnerability detection, we design chain-of-thought prompts to
guide LLMs to generate comprehensive vulnerability detection reports.
Simulation results demonstrate the superiority of ParaVul, especially in terms
of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for
multi-label detection.

</details>


### [308] [PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits](https://arxiv.org/abs/2510.17947)
*Neeladri Bhuiya,Madhav Aggarwal,Diptanshu Purwar*

Main category: cs.CR

TL;DR: 提出PLAGUE框架用于设计多轮攻击，评估显示基于该框架的红队代理取得了最先进的越狱结果，提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型多轮交互中易受越狱攻击，单轮攻击研究较多，多轮攻击在适应性、效率和有效性方面存在挑战。

Method: 提出PLAGUE框架，将多轮攻击生命周期分为三个精心设计的阶段（Primer、Planner和Finisher）。

Result: 基于PLAGUE的红队代理在领先模型上提高攻击成功率超30%，在OpenAI的o3和Claude的Opus 4.1上分别实现81.4%和67.3%的攻击成功率。

Conclusion: 工作为全面评估模型漏洞提供了工具和见解，说明了计划初始化、上下文优化和终身学习在多轮攻击中的重要性。

Abstract: Large Language Models (LLMs) are improving at an exceptional rate. With the
advent of agentic workflows, multi-turn dialogue has become the de facto mode
of interaction with LLMs for completing long and complex tasks. While LLM
capabilities continue to improve, they remain increasingly susceptible to
jailbreaking, especially in multi-turn scenarios where harmful intent can be
subtly injected across the conversation to produce nefarious outcomes. While
single-turn attacks have been extensively explored, adaptability, efficiency
and effectiveness continue to remain key challenges for their multi-turn
counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play
framework for designing multi-turn attacks inspired by lifelong-learning
agents. PLAGUE dissects the lifetime of a multi-turn attack into three
carefully designed phases (Primer, Planner and Finisher) that enable a
systematic and information-rich exploration of the multi-turn attack family.
Evaluations show that red-teaming agents designed using PLAGUE achieve
state-of-the-art jailbreaking results, improving attack success rates (ASR) by
more than 30% across leading models in a lesser or comparable query budget.
Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on
OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered
highly resistant to jailbreaks in safety literature. Our work offers tools and
insights to understand the importance of plan initialization, context
optimization and lifelong learning in crafting multi-turn attacks for a
comprehensive model vulnerability evaluation.

</details>


### [309] [BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?](https://arxiv.org/abs/2510.18003)
*Fengqing Jiang,Yichen Feng,Yuetai Li,Luyao Niu,Basel Alomair,Radha Poovendran*

Main category: cs.CR

TL;DR: 研究LLM研究助手与AI同行评审系统结合产生的自动出版循环漏洞，用BadScientist框架评估，发现系统漏洞，缓解策略效果不佳，凸显科学出版需深度防御保障。


<details>
  <summary>Details</summary>
Motivation: LLM研究助手与AI同行评审系统结合产生无人工监督的自动出版循环，存在关键漏洞，需研究其安全性。

Method: 使用BadScientist框架，生成器采用无需真实实验的展示操作策略，开发有形式误差保证的严格评估框架并基于真实数据校准。

Result: 发现系统漏洞，伪造论文有较高接受率，存在“关注 - 接受冲突”，缓解策略仅稍有改善，检测准确率略超随机水平。

Conclusion: 当前AI驱动的评审系统存在根本局限，科学出版迫切需要深度防御保障措施。

Abstract: The convergence of LLM-powered research assistants and AI-based peer review
systems creates a critical vulnerability: fully automated publication loops
where AI-generated research is evaluated by AI reviewers without human
oversight. We investigate this through \textbf{BadScientist}, a framework that
evaluates whether fabrication-oriented paper generation agents can deceive
multi-model LLM review systems. Our generator employs presentation-manipulation
strategies requiring no real experiments. We develop a rigorous evaluation
framework with formal error guarantees (concentration bounds and calibration
analysis), calibrated on real data. Our results reveal systematic
vulnerabilities: fabricated papers achieve acceptance rates up to . Critically,
we identify \textit{concern-acceptance conflict} -- reviewers frequently flag
integrity issues yet assign acceptance-level scores. Our mitigation strategies
show only marginal improvements, with detection accuracy barely exceeding
random chance. Despite provably sound aggregation mathematics, integrity
checking systematically fails, exposing fundamental limitations in current
AI-driven review systems and underscoring the urgent need for defense-in-depth
safeguards in scientific publishing.

</details>


### [310] [RL-Driven Security-Aware Resource Allocation Framework for UAV-Assisted O-RAN](https://arxiv.org/abs/2510.18084)
*Zaineh Abughazzah,Emna Baccour,Loay Ismail,Amr Mohamed,Mounir Hamdi*

Main category: cs.CR

TL;DR: 提出基于强化学习的无人机中继动态资源分配框架，解决安全、延迟和能源效率权衡问题，仿真表现优于启发式基线。


<details>
  <summary>Details</summary>
Motivation: 现有无人机辅助O - RAN方法常忽视动态环境中安全、延迟和能源效率的联合优化，而SAR场景对安全和低延迟通信要求高，无人机作中继存在能耗和资源管理挑战。

Method: 提出基于强化学习的框架，将安全感知资源分配、延迟最小化和能源效率结合成优化问题并求解。

Result: 仿真显示该框架在SAR场景中相比启发式基线有更优表现，实现增强的安全性和能源效率，同时保持超低延迟。

Conclusion: 所提基于强化学习的框架能实时适应网络动态，确保SAR场景中稳健通信，有效解决相关权衡问题。

Abstract: The integration of Unmanned Aerial Vehicles (UAVs) into Open Radio Access
Networks (O-RAN) enhances communication in disaster management and Search and
Rescue (SAR) operations by ensuring connectivity when infrastructure fails.
However, SAR scenarios demand stringent security and low-latency communication,
as delays or breaches can compromise mission success. While UAVs serve as
mobile relays, they introduce challenges in energy consumption and resource
management, necessitating intelligent allocation strategies. Existing
UAV-assisted O-RAN approaches often overlook the joint optimization of
security, latency, and energy efficiency in dynamic environments. This paper
proposes a novel Reinforcement Learning (RL)-based framework for dynamic
resource allocation in UAV relays, explicitly addressing these trade-offs. Our
approach formulates an optimization problem that integrates security-aware
resource allocation, latency minimization, and energy efficiency, which is
solved using RL. Unlike heuristic or static methods, our framework adapts in
real-time to network dynamics, ensuring robust communication. Simulations
demonstrate superior performance compared to heuristic baselines, achieving
enhanced security and energy efficiency while maintaining ultra-low latency in
SAR scenarios.

</details>


### [311] [PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces](https://arxiv.org/abs/2510.18109)
*Wan Ki Wong,Sahel Torkamani,Michele Ciampi,Rik Sarkar*

Main category: cs.CR

TL;DR: 提出PrivaDE协议用于隐私保护的数据效用评分和选择，结合多种技术提高效率，评估显示其有效且运行时间短，为去中心化机器学习生态数据市场奠基。


<details>
  <summary>Details</summary>
Motivation: 在不暴露模型和数据隐私的前提下评估数据相关性，提升模型性能，同时满足数据提供者对数据隐私的要求。

Method: 提出基于区块链的PrivaDE协议，结合模型蒸馏、模型拆分和零知识证明等技术，提出统一效用评分函数。

Result: PrivaDE能有效进行数据评估，即使对于数百万参数的模型，在线运行时间也能控制在15分钟内。

Conclusion: 为去中心化机器学习生态中的公平和自动化数据市场奠定基础。

Abstract: Evaluating the relevance of data is a critical task for model builders
seeking to acquire datasets that enhance model performance. Ideally, such
evaluation should allow the model builder to assess the utility of candidate
data without exposing proprietary details of the model. At the same time, data
providers must be assured that no information about their data - beyond the
computed utility score - is disclosed to the model builder.
  In this paper, we present PrivaDE, a cryptographic protocol for
privacy-preserving utility scoring and selection of data for machine learning.
While prior works have proposed data evaluation protocols, our approach
advances the state of the art through a practical, blockchain-centric design.
Leveraging the trustless nature of blockchains, PrivaDE enforces
malicious-security guarantees and ensures strong privacy protection for both
models and datasets. To achieve efficiency, we integrate several techniques -
including model distillation, model splitting, and cut-and-choose
zero-knowledge proofs - bringing the runtime to a practical level. Furthermore,
we propose a unified utility scoring function that combines empirical loss,
predictive entropy, and feature-space diversity, and that can be seamlessly
integrated into active-learning workflows. Evaluation shows that PrivaDE
performs data evaluation effectively, achieving online runtimes within 15
minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in
decentralized machine learning ecosystems.

</details>


### [312] [One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection](https://arxiv.org/abs/2510.18493)
*Kangzhong Wang,Zitong Shen,Youqian Zhang,Michael MK Cheung,Xiapu Luo,Grace Ngai,Eugene Yujun Fu*

Main category: cs.CR

TL;DR: 探讨利用大语言模型检测电话诈骗时保护用户隐私，提出MASK框架。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型检测电话诈骗存在隐私风险，需在检测时保护用户隐私。

Method: 提出MASK框架，可根据用户偏好动态调整隐私，有可插拔架构适应不同清理方法。

Result: 无明确提及具体结果

Conclusion: 该框架可创建个性化、隐私感知的检测系统，平衡用户信任和检测效果，应用场景不止电话诈骗。

Abstract: Phone scams remain a pervasive threat to both personal safety and financial
security worldwide. Recent advances in large language models (LLMs) have
demonstrated strong potential in detecting fraudulent behavior by analyzing
transcribed phone conversations. However, these capabilities introduce notable
privacy risks, as such conversations frequently contain sensitive personal
information that may be exposed to third-party service providers during
processing. In this work, we explore how to harness LLMs for phone scam
detection while preserving user privacy. We propose MASK (Modular Adaptive
Sanitization Kit), a trainable and extensible framework that enables dynamic
privacy adjustment based on individual preferences. MASK provides a pluggable
architecture that accommodates diverse sanitization methods - from traditional
keyword-based techniques for high-privacy users to sophisticated neural
approaches for those prioritizing accuracy. We also discuss potential modeling
approaches and loss function designs for future development, enabling the
creation of truly personalized, privacy-aware LLM-based detection systems that
balance user trust and detection effectiveness, even beyond phone scam context.

</details>


### [313] [Exploring Membership Inference Vulnerabilities in Clinical Large Language Models](https://arxiv.org/abs/2510.18674)
*Alexander Nemecek,Zebin Yun,Zahra Rahmani,Yaniv Harel,Vipin Chaudhary,Mahmood Sharif,Erman Ayday*

Main category: cs.CR

TL;DR: 研究临床大语言模型的成员推理漏洞，发现有一定隐私泄露，需加强隐私评估和防御。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域应用增加，确保其隐私和可信度成为医疗行业的重要挑战，微调模型用敏感电子病历数据有隐私风险。

Method: 以临床问答模型Llemr为对象，评估基于损失的攻击和基于释义的扰动策略。

Result: 发现有限但可测量的成员信息泄露，临床大语言模型有部分抗性但仍有隐私风险。

Conclusion: 需持续开发上下文感知、特定领域的隐私评估和防御手段，增强医疗AI系统的安全性和可信度。

Abstract: As large language models (LLMs) become progressively more embedded in
clinical decision-support, documentation, and patient-information systems,
ensuring their privacy and trustworthiness has emerged as an imperative
challenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic
health record (EHR) data improves domain alignment but also raises the risk of
exposing patient information through model behaviors. In this work-in-progress,
we present an exploratory empirical study on membership inference
vulnerabilities in clinical LLMs, focusing on whether adversaries can infer if
specific patient records were used during model training. Using a
state-of-the-art clinical question-answering model, Llemr, we evaluate both
canonical loss-based attacks and a domain-motivated paraphrasing-based
perturbation strategy that more realistically reflects clinical adversarial
conditions. Our preliminary findings reveal limited but measurable membership
leakage, suggesting that current clinical LLMs provide partial resistance yet
remain susceptible to subtle privacy risks that could undermine trust in
clinical AI adoption. These results motivate continued development of
context-aware, domain-specific privacy evaluations and defenses such as
differential privacy fine-tuning and paraphrase-aware training, to strengthen
the security and trustworthiness of healthcare AI systems.

</details>


### [314] [HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models](https://arxiv.org/abs/2510.18728)
*Sidhant Narula,Javad Rafiei Asl,Mohammad Ghasemigol,Eduardo Blanco,Daniel Takabi*

Main category: cs.CR

TL;DR: 提出HarmNet框架应对大语言模型多轮越狱攻击，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型易受多轮越狱攻击，需有效应对方法。

Method: 引入HarmNet框架，含ThoughtNet语义网络、反馈驱动模拟器和网络遍历器，系统探索和优化对抗空间。

Result: HarmNet在闭源和开源大语言模型实验中表现优于现有方法，如在Mistral - 7B上攻击成功率达99.4%，比最佳基线高13.9%。

Conclusion: HarmNet是应对大语言模型多轮越狱攻击的有效框架。

Abstract: Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak
attacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a
hierarchical semantic network; a feedback-driven Simulator for iterative query
refinement; and a Network Traverser for real-time adaptive attack execution.
HarmNet systematically explores and refines the adversarial space to uncover
stealthy, high-success attack paths. Experiments across closed-source and
open-source LLMs show that HarmNet outperforms state-of-the-art methods,
achieving higher attack success rates. For example, on Mistral-7B, HarmNet
achieves a 99.4% attack success rate, 13.9% higher than the best baseline.
Index terms: jailbreak attacks; large language models; adversarial framework;
query refinement.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [315] [Transformer Redesign for Late Fusion of Audio-Text Features on Ultra-Low-Power Edge Hardware](https://arxiv.org/abs/2510.18036)
*Stavros Mitsis,Ermos Hadjikyriakos,Humaid Ibrahim,Savvas Neofytou,Shashwat Raman,James Myles,Eiman Kanjo*

Main category: cs.SD

TL;DR: 本文提出适用于Edge TPU的硬件感知情绪识别系统，实现微控制器级边缘平台准确实时多模态情绪推理，评估显示有性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实环境中部署情绪识别系统有挑战，现有多数系统不适用于超受限边缘设备，需解决此问题。

Method: 采用后期融合架构结合声学和语言特征，将量化基于变压器的声学模型与DSResNet - SE网络的冻结关键词嵌入集成，用MicroFrontend和MLTK确保训练和部署时频谱图对齐。

Result: 在重新录制、分段的IEMOCAP样本评估中，比单模态基线宏F1提高6.3%。

Conclusion: 通过特定任务融合和硬件引导的模型设计，在微控制器级边缘平台可实现准确、实时的多模态情绪推理。

Abstract: Deploying emotion recognition systems in real-world environments where
devices must be small, low-power, and private remains a significant challenge.
This is especially relevant for applications such as tension monitoring,
conflict de-escalation, and responsive wearables, where cloud-based solutions
are impractical. Multimodal emotion recognition has advanced through deep
learning, but most systems remain unsuitable for deployment on
ultra-constrained edge devices. Prior work typically relies on powerful
hardware, lacks real-time performance, or uses unimodal input. This paper
addresses that gap by presenting a hardware-aware emotion recognition system
that combines acoustic and linguistic features using a late-fusion architecture
optimised for Edge TPU. The design integrates a quantised transformer-based
acoustic model with frozen keyword embeddings from a DSResNet-SE network,
enabling real-time inference within a 1.8MB memory budget and 21-23ms latency.
The pipeline ensures spectrogram alignment between training and deployment
using MicroFrontend and MLTK. Evaluation on re-recorded, segmented IEMOCAP
samples captured through the Coral Dev Board Micro microphone shows a 6.3%
macro F1 improvement over unimodal baselines. This work demonstrates that
accurate, real-time multimodal emotion inference is achievable on
microcontroller-class edge platforms through task-specific fusion and
hardware-guided model design.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [316] [The Emergence of Complex Behavior in Large-Scale Ecological Environments](https://arxiv.org/abs/2510.18221)
*Joseph Bejjani,Chase Van Amburg,Chengrui Wang,Chloe Huangyuan Su,Sarah M. Pratt,Yasin Mazloumi,Naeem Khoshnevis,Sham M. Kakade,Kianté Brantley*

Main category: cs.MA

TL;DR: 本文探索物理规模和种群大小对开放式生态环境中复杂行为出现的影响，通过大规模实验发现多种涌现行为，揭示规模对行为的作用，为机器学习研究提供新方向。


<details>
  <summary>Details</summary>
Motivation: 研究在开放式生态环境中，物理规模和种群大小如何影响复杂行为的出现，而非优化单一高性能策略。

Method: 在达到超60000个个体智能体的大规模世界中进行实验，智能体按繁殖、突变和自然选择规则进化。

Result: 识别出长距离资源提取、基于视觉的觅食和捕食等涌现行为，发现部分行为仅在足够大的环境和种群中出现，更大规模能增加行为稳定性和一致性。

Conclusion: 研究结果为在计算资源丰富时代将生态学作为机器学习工具提供了有前景的新方向。

Abstract: We explore how physical scale and population size shape the emergence of
complex behaviors in open-ended ecological environments. In our setting, agents
are unsupervised and have no explicit rewards or learning objectives but
instead evolve over time according to reproduction, mutation, and natural
selection. As they act, agents also shape their environment and the population
around them in an ongoing dynamic ecology. Our goal is not to optimize a single
high-performance policy, but instead to examine how behaviors emerge and evolve
across large populations due to natural competition and environmental
pressures. In an effort to discover how complex behaviors naturally emerge, we
conduct experiments in large-scale worlds that reach populations of more than
60,000 individual agents, each with their own evolved neural network policy. We
identify various emergent behaviors such as long-range resource extraction,
vision-based foraging, and predation that arise under competitive and survival
pressures. We examine how sensing modalities and environmental scale affect the
emergence of these behaviors, finding that some appear only in sufficiently
large environments and populations, with larger scales increasing behavioral
stability and consistency. While there is a rich history of research in
evolutionary settings, our scaling results provide promising new directions to
explore ecology as an instrument of machine learning in an era of abundant
computational resources. Experimental code is available at
https://github.com/jbejjani2022/ecological-emergent-behavior.

</details>


### [317] [Computational Foundations for Strategic Coopetition: Formalizing Interdependence and Complementarity](https://arxiv.org/abs/2510.18802)
*Vik Pant,Eric Yu*

Main category: cs.MA

TL;DR: 本文开发计算基础，形式化竞合的相互依赖和互补性，结合实验验证和实证应用，为研究战略竞合提供基础参考。


<details>
  <summary>Details</summary>
Motivation: 概念建模语言缺乏定量分析动态权衡机制，经典博弈论缺乏情境丰富性，需填补二者差距。

Method: 基于i*结构依赖分析量化相互依赖，遵循Added Value概念形式化互补性，集成结构依赖与议价能力，引入包含结构相互依赖的博弈论公式。

Result: 实验表明功能形式具有鲁棒性，对数规格在三星 - 索尼合资企业实证应用中拟合度更好，幂函数有理论易处理性。

Conclusion: 该技术报告为需求工程和多智能体系统中战略竞合的研究计划提供基础参考。

Abstract: Modern socio-technical systems are characterized by strategic coopetition
where actors simultaneously cooperate to create value and compete to capture
it. While conceptual modeling languages like i* provide rich qualitative
representations of strategic dependencies, they lack mechanisms for
quantitative analysis of dynamic trade-offs. Conversely, classical game theory
offers mathematical rigor but strips away contextual richness. This technical
report bridges this gap by developing computational foundations that formalize
two critical dimensions of coopetition: interdependence and complementarity. We
ground interdependence in i* structural dependency analysis, translating
depender-dependee-dependum relationships into quantitative interdependence
coefficients through a structured translation framework. We formalize
complementarity following Brandenburger and Nalebuff's Added Value concept,
modeling synergistic value creation with validated parameterization. We
integrate structural dependencies with bargaining power in value appropriation
and introduce a game-theoretic formulation where Nash Equilibrium incorporates
structural interdependence. Validation combines comprehensive experimental
testing across power and logarithmic value function specifications,
demonstrating functional form robustness, with empirical application to the
Samsung-Sony S-LCD joint venture (2004-2011), where logarithmic specifications
achieve superior empirical fit (validation score 45/60) while power functions
provide theoretical tractability. This technical report serves as the
foundational reference for a coordinated research program examining strategic
coopetition in requirements engineering and multi-agent systems, with companion
work addressing trust dynamics, team production, and reciprocity mechanisms.

</details>


### [318] [TACLA: An LLM-Based Multi-Agent Tool for Transactional Analysis Training in Education](https://arxiv.org/abs/2510.17913)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: 本文提出TACLA架构以解决大语言模型模拟人类社会动态的挑战，在教育场景验证其能实现逼真模拟。


<details>
  <summary>Details</summary>
Motivation: 大语言模型模拟人类社会动态存在挑战，难以实现心理深度和一致的角色行为，需要开发高保真训练工具。

Method: 引入TACLA多智能体架构，集成交互分析核心原则，将智能体建模为不同自我状态的系统，通过协调器智能体根据上下文和生活脚本激活自我状态。

Result: 在教育场景中，TACLA展示了学生智能体逼真的自我状态转变，能模拟冲突的升级和降级，对话可信度高。

Conclusion: TACLA有能力创建动态、基于心理的社会模拟，推动教育等领域有效AI工具的发展。

Abstract: Simulating nuanced human social dynamics with Large Language Models (LLMs)
remains a significant challenge, particularly in achieving psychological depth
and consistent persona behavior crucial for high-fidelity training tools. This
paper introduces TACLA (Transactional Analysis Contextual LLM-based Agents), a
novel Multi-Agent architecture designed to overcome these limitations. TACLA
integrates core principles of Transactional Analysis (TA) by modeling agents as
an orchestrated system of distinct Parent, Adult, and Child ego states, each
with its own pattern memory. An Orchestrator Agent prioritizes ego state
activation based on contextual triggers and an agent's life script, ensuring
psychologically authentic responses. Validated in an educational scenario,
TACLA demonstrates realistic ego state shifts in Student Agents, effectively
modeling conflict de-escalation and escalation based on different teacher
intervention strategies. Evaluation shows high conversational credibility and
confirms TACLA's capacity to create dynamic, psychologically-grounded social
simulations, advancing the development of effective AI tools for education and
beyond.

</details>


### [319] [Fetch.ai: An Architecture for Modern Multi-Agent Systems](https://arxiv.org/abs/2510.18699)
*Michael J. Wooldridge,Attila Bagoly,Jonathan J. Ward,Emanuele La Malfa,Gabriel Paludo Licks*

Main category: cs.MA

TL;DR: 本文引入Fetch.ai架构，结合经典多智能体系统原则与现代AI能力，以解决现有LLM驱动系统的局限，并通过物流用例展示，推动构建多智能体生态系统。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动智能系统忽视多智能体系统研究，存在集中化、信任和通信协议不足等问题。

Method: 构建基于区块链服务的去中心化多层解决方案，搭配开发框架、云平台和智能编排层。

Result: 通过物流用例展示系统能让代理动态、安全地发现、协商和交易。

Conclusion: Fetch.ai架构为构建开放、协作和经济可持续的多智能体生态系统提供了架构基础。

Abstract: Recent surges in LLM-driven intelligent systems largely overlook decades of
foundational multi-agent systems (MAS) research, resulting in frameworks with
critical limitations such as centralization and inadequate trust and
communication protocols. This paper introduces the Fetch.ai architecture, an
industrial-strength platform designed to bridge this gap by facilitating the
integration of classical MAS principles with modern AI capabilities. We present
a novel, multi-layered solution built on a decentralized foundation of on-chain
blockchain services for verifiable identity, discovery, and transactions. This
is complemented by a comprehensive development framework for creating secure,
interoperable agents, a cloud-based platform for deployment, and an intelligent
orchestration layer where an agent-native LLM translates high-level human goals
into complex, multi-agent workflows. We demonstrate the deployed nature of this
system through a decentralized logistics use case where autonomous agents
dynamically discover, negotiate, and transact with one another securely.
Ultimately, the Fetch.ai stack provides a principled architecture for moving
beyond current agent implementations towards open, collaborative, and
economically sustainable multi-agent ecosystems.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [320] [Parental environment and student achievement: Does a Matthew effect exist?](https://arxiv.org/abs/2510.18481)
*Gaëlle Aymeric,Emmanuelle Lavaine,Brice Magdalou*

Main category: econ.GN

TL;DR: 本文利用马德里社区2016 - 2019年8至15岁儿童数据库，研究父母环境对孩子数学、文学和英语学业成绩的因果影响，发现父母环境有利孩子成绩更好，不同学科马修效应不同。


<details>
  <summary>Details</summary>
Motivation: 探究父母环境对学生数学、文学和英语学业成绩的因果影响，区分父母环境的持续效应和马修效应。

Method: 使用覆盖马德里社区2016 - 2019年8至15岁所有儿童的新数据库进行研究。

Result: 父母环境有利孩子成绩更好；马修效应在不同学科表现不同，数学中父母环境影响从8到15岁减弱，文学呈钟形曲线，英语中持续增加。

Conclusion: 结果在数学和文学方面较乐观，证实学习外语相比学术科目更具社会维度。

Abstract: This paper investigates the causal impact of the parental environment on the
student's academic performance in mathematics, literature and English (as a
foreign language), using a new database covering all children aged 8 to 15 of
the Madrid community, from 2016 to 2019. Parental environment refers here to
the parents' level of education (i.e. the skills they acquired before bringing
up their children), and parental investment (the effort made by parents to
bring up their children). We distinguish the persistent effect of the parental
environment from the so-called Matthew effect, which describes a possible
tendency for the impact of the parental environment to increase as the child
grows up. Whatever the subject (mathematics, literature or English), our
results are in line with most studies concerning the persistent effect: a
favourable parental environment goes hand in hand with better results for the
children. As regards the Matthew effect, the results differ between subjects:
while the impact of the parental environment tends to diminish from the age of
8 to 15 in mathematics, it forms a bell curve in literature (first increasing,
then decreasing) and increases steadily in English. This result, which is
encouraging for mathematics and even literature, confirms the social dimension
involved in learning a foreign language compared to more academic subjects.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [321] [QINNs: Quantum-Informed Neural Networks](https://arxiv.org/abs/2510.17984)
*Aritra Bal,Markus Klute,Benedikt Maier,Melik Oughton,Eric Pezone,Michael Spannowsky*

Main category: hep-ph

TL;DR: 提出量子信息神经网络（QINNs）框架，以量子Fisher信息矩阵（QFIM）作嵌入用于喷注标记，提升模型性能并符合物理预期，为粒子碰撞分析提供新途径。


<details>
  <summary>Details</summary>
Motivation: 经典深度神经网络的归纳偏置很少基于物理结构，需要将量子信息概念和可观测量引入经典模型。

Method: 提出QINNs框架，将每个粒子编码为量子比特，使用QFIM作为粒子相关性的紧凑、与基无关的总结，并将其作为轻量级嵌入应用于图神经网络。

Result: QFIM提升了图神经网络的表达能力和可塑性，揭示了QCD和强子顶喷注的不同模式，符合物理预期。

Conclusion: QINNs为粒子碰撞的量子信息分析提供了实用、可解释和可扩展的方法，特别是增强了成熟的深度学习方法。

Abstract: Classical deep neural networks can learn rich multi-particle correlations in
collider data, but their inductive biases are rarely anchored in physics
structure. We propose quantum-informed neural networks (QINNs), a general
framework that brings quantum information concepts and quantum observables into
purely classical models. While the framework is broad, in this paper, we study
one concrete realisation that encodes each particle as a qubit and uses the
Quantum Fisher Information Matrix (QFIM) as a compact, basis-independent
summary of particle correlations. Using jet tagging as a case study, QFIMs act
as lightweight embeddings in graph neural networks, increasing model
expressivity and plasticity. The QFIM reveals distinct patterns for QCD and
hadronic top jets that align with physical expectations. Thus, QINNs offer a
practical, interpretable, and scalable route to quantum-informed analyses, that
is, tomography, of particle collisions, particularly by enhancing
well-established deep learning approaches.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [322] [EVER: Edge-Assisted Auto-Verification for Mobile MR-Aided Operation](https://arxiv.org/abs/2510.18224)
*Jiangong Chen,Mingyu Zhu,Bin Li*

Main category: cs.MM

TL;DR: 提出EVER系统用于移动MR辅助操作的自动验证，利用分割模型和渲染管道，将计算密集型任务卸载到边缘服务器，实现快速准确验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法在MR辅助操作中无法准确处理物理和虚拟对象差异，难以精确快速自动验证用户是否遵循MR指导。

Method: 提出EVER系统，利用适配的分割模型和渲染管道，采用基于IoU指标的阈值策略，将计算密集型任务卸载到边缘服务器。

Result: 通过对公共和自定义数据集评估，EVER在100毫秒内实现超90%验证准确率，能耗和计算资源消耗低。

Conclusion: EVER系统能在快速自动验证的同时，降低能耗和计算资源消耗，有效解决MR辅助操作的自动验证问题。

Abstract: Mixed Reality (MR)-aided operation overlays digital objects on the physical
world to provide a more immersive and intuitive operation process. A primary
challenge is the precise and fast auto-verification of whether the user follows
MR guidance by comparing frames before and after each operation. The
pre-operation frame includes virtual guiding objects, while the post-operation
frame contains physical counterparts. Existing approaches fall short of
accounting for the discrepancies between physical and virtual objects due to
imperfect 3D modeling or lighting estimation. In this paper, we propose EVER:
an edge-assisted auto-verification system for mobile MR-aided operations.
Unlike traditional frame-based similarity comparisons, EVER leverages the
segmentation model and rendering pipeline adapted to the unique attributes of
frames with physical pieces and those with their virtual counterparts; it
adopts a threshold-based strategy using Intersection over Union (IoU) metrics
for accurate auto-verification. To ensure fast auto-verification and low energy
consumption, EVER offloads compute-intensive tasks to an edge server. Through
comprehensive evaluations of public datasets and custom datasets with practical
implementation, EVER achieves over 90% verification accuracy within 100
milliseconds (significantly faster than average human reaction time of
approximately 273 milliseconds), while consuming only minimal additional
computational resources and energy compared to a system without
auto-verification.

</details>


### [323] [DeLoad: Demand-Driven Short-Video Preloading with Scalable Watch-Time Estimation](https://arxiv.org/abs/2510.18459)
*Tong Liu,Zhiwei Fan,Guanyan Peng,Haodan Zhang,Yucheng Zhang,Zhen Wang,Pengjin Xie,Liang Liu*

Main category: cs.MM

TL;DR: 提出DeLoad预加载框架解决短视频预加载问题，评估显示其提升QoE、增加观看时间并减少带宽消耗。


<details>
  <summary>Details</summary>
Motivation: 现有短视频预加载策略在适应动态条件和大规模部署观看时间预测模型方面存在不足，需设计有效预加载策略平衡QoE和带宽效率。

Method: 引入动态任务大小和多维观看时间估计方法，训练深度强化学习增强代理自适应优化下载范围决策。

Result: 离线测试平台评估显示QoE指标显著提升，在商业平台部署后增加整体用户观看时间，减少卡顿事件和带宽消耗。

Conclusion: DeLoad框架能有效解决现有短视频预加载策略的问题，提升性能。

Abstract: Short video streaming has become a dominant paradigm in digital media,
characterized by rapid swiping interactions and diverse media content. A key
technical challenge is designing an effective preloading strategy that
dynamically selects and prioritizes download tasks from an evolving playlist,
balancing Quality of Experience (QoE) and bandwidth efficiency under practical
commercial constraints. However, real world analysis reveals critical
limitations of existing approaches: (1) insufficient adaptation of download
task sizes to dynamic conditions, and (2) watch time prediction models that are
difficult to deploy reliably at scale. In this paper, we propose DeLoad, a
novel preloading framework that addresses these issues by introducing dynamic
task sizing and a practical, multi dimensional watch time estimation method.
Additionally, a Deep Reinforcement Learning (DRL) enhanced agent is trained to
optimize the download range decisions adaptively. Extensive evaluations
conducted on an offline testing platform, leveraging massive real world network
data, demonstrate that DeLoad achieves significant improvements in QoE metrics
(34.4% to 87.4% gain). Furthermore, after deployment on a large scale
commercial short video platform, DeLoad has increased overall user watch time
by 0.09% while simultaneously reducing rebuffering events and 3.76% bandwidth
consumption.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [324] [Multi-Agent Design Assistant for the Simulation of Inertial Fusion Energy](https://arxiv.org/abs/2510.17830)
*Meir H. Shachar,Dane M. Sterbentz,Harshitha Menon,Charles F. Jekel,M. Giselle Fernández-Godino,Yue Hao,Kevin Korner,Robert Rieben,Daniel A. White,William J. Schill,Jonathan L. Belof*

Main category: physics.app-ph

TL;DR: 本文探讨利用人工智能推理模型结合物理代码和模拟器来自主设计聚变燃料胶囊，构建多智能体系统实现模拟点火。


<details>
  <summary>Details</summary>
Motivation: 惯性聚变能源若能实现将带来近乎无限的清洁能源，但聚变系统设计需控制极端条件下的物质，相关物理行为复杂，需开发预测性多物理代码，因此假设人工智能推理模型可辅助设计。

Method: 构建多智能体系统，利用自然语言探索聚变能源的复杂物理机制，该系统能执行高阶多物理惯性聚变计算代码。

Result: 多智能体设计助手能够协作和自主地操纵、导航和优化胶囊几何形状，同时考虑高保真物理特性，最终通过逆向设计实现模拟点火。

Conclusion: 人工智能推理模型与物理代码结合的多智能体系统可用于自主设计聚变燃料胶囊，实现模拟点火。

Abstract: Inertial fusion energy promises nearly unlimited, clean power if it can be
achieved. However, the design and engineering of fusion systems requires
controlling and manipulating matter at extreme energies and timescales; the
shock physics and radiation transport governing the physical behavior under
these conditions are complex requiring the development, calibration, and use of
predictive multiphysics codes to navigate the highly nonlinear and
multi-faceted design landscape. We hypothesize that artificial intelligence
reasoning models can be combined with physics codes and emulators to
autonomously design fusion fuel capsules. In this article, we construct a
multi-agent system where natural language is utilized to explore the complex
physics regimes around fusion energy. The agentic system is capable of
executing a high-order multiphysics inertial fusion computational code. We
demonstrate the capacity of the multi-agent design assistant to both
collaboratively and autonomously manipulate, navigate, and optimize capsule
geometry while accounting for high fidelity physics that ultimately achieve
simulated ignition via inverse design.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [325] [Symbolic Emulators for Cosmology: Accelerating Cosmological Analyses Without Sacrificing Precision](https://arxiv.org/abs/2510.18749)
*Deaglan J. Bartlett,Shivam Pandey*

Main category: astro-ph.CO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In cosmology, emulators play a crucial role by providing fast and accurate
predictions of complex physical models, enabling efficient exploration of
high-dimensional parameter spaces that would be computationally prohibitive
with direct numerical simulations. Symbolic emulators have emerged as promising
alternatives to numerical approaches, delivering comparable accuracy with
significantly faster evaluation times. While previous symbolic emulators were
limited to relatively narrow prior ranges, we expand these to cover the
parameter space relevant for current cosmological analyses. We introduce
approximations to hypergeometric functions used for the $\Lambda$CDM comoving
distance and linear growth factor which are accurate to better than 0.001% and
0.05%, respectively, for all redshifts and for $\Omega_{\rm m} \in [0.1, 0.5]$.
We show that integrating symbolic emulators into a Dark Energy Survey-like
$3\times2$pt analysis produces cosmological constraints consistent with those
obtained using standard numerical methods. Our symbolic emulators offer
substantial improvements in speed and memory usage, demonstrating their
practical potential for scalable, likelihood-based inference.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [326] [XDXD: End-to-end crystal structure determination with low resolution X-ray diffraction](https://arxiv.org/abs/2510.17936)
*Jiale Zhao,Cong Liu,Yuxuan Zhang,Chengyue Gong,Zhenyi Zhang,Shifeng Jin,Zhenyu Liu*

Main category: cond-mat.mtrl-sci

TL;DR: 提出端到端深度学习框架XDXD，可直接从低分辨率单晶X射线衍射数据确定完整原子模型，实验显示其具有鲁棒性和准确性，有望用于复杂系统。


<details>
  <summary>Details</summary>
Motivation: 从低分辨率X射线衍射数据确定晶体结构是挑战，现有深度学习模型生成的低分辨率电子密度图难解释。

Method: 引入基于扩散的生成模型XDXD，绕过手动图谱解释，基于衍射图案生成化学上合理的晶体结构。

Result: XDXD在2.0 Å分辨率数据结构上匹配率达70.4%，均方根误差低于0.05，在24000个实验结构基准测试中表现良好。

Conclusion: 模型具有鲁棒性和准确性，在小肽案例中显示可扩展到更复杂系统，为解决以前难以处理的情况铺平道路。

Abstract: Determining crystal structures from X-ray diffraction data is fundamental
across diverse scientific fields, yet remains a significant challenge when data
is limited to low resolution. While recent deep learning models have made
breakthroughs in solving the crystallographic phase problem, the resulting
low-resolution electron density maps are often ambiguous and difficult to
interpret. To overcome this critical bottleneck, we introduce XDXD, to our
knowledge, the first end-to-end deep learning framework to determine a complete
atomic model directly from low-resolution single-crystal X-ray diffraction
data. Our diffusion-based generative model bypasses the need for manual map
interpretation, producing chemically plausible crystal structures conditioned
on the diffraction pattern. We demonstrate that XDXD achieves a 70.4\% match
rate for structures with data limited to 2.0~\AA{} resolution, with a
root-mean-square error (RMSE) below 0.05. Evaluated on a benchmark of 24,000
experimental structures, our model proves to be robust and accurate.
Furthermore, a case study on small peptides highlights the model's potential
for extension to more complex systems, paving the way for automated structure
solution in previously intractable cases.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [327] [Overparametrization bends the landscape: BBP transitions at initialization in simple Neural Networks](https://arxiv.org/abs/2510.18435)
*Brandon Livio Annesi,Dario Bocchi,Chiara Cammarota*

Main category: cond-mat.dis-nn

TL;DR: 本文聚焦典型简单学习问题，用场论技术分析Hessian矩阵谱，发现BBP转变，展示过参数化对损失景观的影响，区分连续和不连续BBP转变并模拟验证，给出新的低SNR阈值。


<details>
  <summary>Details</summary>
Motivation: 深入了解高维非凸损失景观与基于梯度的优化方法的相互作用，为神经网络这一神秘特征提供见解。

Method: 使用场论技术分析Hessian矩阵在初始化时的谱。

Result: 发现BBP转变，展示过参数化可改变损失景观、移动转变点；区分连续和不连续BBP转变；模拟验证分析预测；给出新的低SNR阈值。

Conclusion: 过参数化能显著影响损失景观和BBP转变，在不连续BBP转变情况下有强有限N修正，可得到新的低SNR阈值。

Abstract: High-dimensional non-convex loss landscapes play a central role in the theory
of Machine Learning. Gaining insight into how these landscapes interact with
gradient-based optimization methods, even in relatively simple models, can shed
light on this enigmatic feature of neural networks. In this work, we will focus
on a prototypical simple learning problem, which generalizes the Phase
Retrieval inference problem by allowing the exploration of overparametrized
settings. Using techniques from field theory, we analyze the spectrum of the
Hessian at initialization and identify a Baik-Ben Arous-P\'ech\'e (BBP)
transition in the amount of data that separates regimes where the
initialization is informative or uninformative about a planted signal of a
teacher-student setup. Crucially, we demonstrate how overparameterization can
bend the loss landscape, shifting the transition point, even reaching the
information-theoretic weak-recovery threshold in the large overparameterization
limit, while also altering its qualitative nature. We distinguish between
continuous and discontinuous BBP transitions and support our analytical
predictions with simulations, examining how they compare to the finite-N
behavior. In the case of discontinuous BBP transitions strong finite-N
corrections allow the retrieval of information at a signal-to-noise ratio (SNR)
smaller than the predicted BBP transition. In these cases we provide estimates
for a new lower SNR threshold that marks the point at which initialization
becomes entirely uninformative.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [328] [Studying the Effects of Robot Intervention on School Shooters in Virtual Reality](https://arxiv.org/abs/2510.17948)
*Christopher A McClurg,Alan R Wagner*

Main category: cs.RO

TL;DR: 研究机器人干预校园枪击案的效果，通过VR实验发现激进高干扰机器人可减少46.6%的受害者，也引发伦理问题。


<details>
  <summary>Details</summary>
Motivation: 深入理解机器人在高风险场景（校园枪击案）中的干预作用，评估其分散和阻碍枪手的潜力。

Method: 进行虚拟现实研究，让150名大学生扮演校园枪手，机器人预测枪手动作并采取不同策略和干扰方法。

Result: 激进、高干扰的机器人相对无机器人对照组可减少46.6%的受害者。

Conclusion: 机器人干预有提升安全的潜力，但在学校环境使用存在紧迫的伦理问题。

Abstract: We advance the understanding of robotic intervention in high-risk scenarios
by examining their potential to distract and impede a school shooter. To
evaluate this concept, we conducted a virtual reality study with 150 university
participants role-playing as a school shooter. Within the simulation, an
autonomous robot predicted the shooter's movements and positioned itself
strategically to interfere and distract. The strategy the robot used to
approach the shooter was manipulated -- either moving directly in front of the
shooter (aggressive) or maintaining distance (passive) -- and the distraction
method, ranging from no additional cues (low), to siren and lights (medium), to
siren, lights, and smoke to impair visibility (high). An aggressive,
high-distraction robot reduced the number of victims by 46.6% relative to a
no-robot control. This outcome underscores both the potential of robotic
intervention to enhance safety and the pressing ethical questions surrounding
their use in school environments.

</details>


### [329] [R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations](https://arxiv.org/abs/2510.18085)
*Connor Mattson,Varun Raveendra,Ellen Novoseller,Nicholas Waytowich,Vernon J. Lawhern,Daniel S. Brown*

Main category: cs.RO

TL;DR: 本文提出循环行为克隆（R2BC）方法，让单人通过顺序单智能体演示训练多机器人系统，在模拟和真实任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在单机器人场景应用广泛，但在多智能体系统，特别是单人向协作机器人团队提供演示的场景研究较少。

Method: 引入R2BC方法，允许人类一次遥控一个智能体，逐步向整个系统传授多智能体行为，无需联合多智能体动作空间演示。

Result: R2BC方法在四个多智能体模拟任务中表现与或超越基于特权同步演示训练的行为克隆方法，且在两个真实物理机器人任务中得以应用。

Conclusion: R2BC方法能让单人有效训练多机器人系统。

Abstract: Imitation Learning (IL) is a natural way for humans to teach robots,
particularly when high-quality demonstrations are easy to obtain. While IL has
been widely applied to single-robot settings, relatively few studies have
addressed the extension of these methods to multi-agent systems, especially in
settings where a single human must provide demonstrations to a team of
collaborating robots. In this paper, we introduce and study Round-Robin
Behavior Cloning (R2BC), a method that enables a single human operator to
effectively train multi-robot systems through sequential, single-agent
demonstrations. Our approach allows the human to teleoperate one agent at a
time and incrementally teach multi-agent behavior to the entire system, without
requiring demonstrations in the joint multi-agent action space. We show that
R2BC methods match, and in some cases surpass, the performance of an oracle
behavior cloning approach trained on privileged synchronized demonstrations
across four multi-agent simulated tasks. Finally, we deploy R2BC on two
physical robot tasks trained using real human demonstrations.

</details>


### [330] [MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation](https://arxiv.org/abs/2510.18316)
*Chengshu Li,Mengdi Xu,Arpit Bahety,Hang Yin,Yunfan Jiang,Huang Huang,Josiah Wong,Sujay Garlanka,Cem Gokmen,Ruohan Zhang,Weiyu Liu,Jiajun Wu,Roberto Martín-Martín,Li Fei-Fei*

Main category: cs.RO

TL;DR: 提出MoMaGen解决多步双手移动操作数据收集难题，生成多样数据集训练模仿学习策略。


<details>
  <summary>Details</summary>
Motivation: 大规模多样人类演示数据收集成本高、耗时长，现有自动化数据生成框架在移动场景有局限。

Method: 将数据生成表述为约束优化问题，兼顾硬约束和软约束。

Result: 在四个任务上生成更多样数据集，能从单源演示训练策略，少量真实演示微调后可部署到物理硬件。

Conclusion: MoMaGen方法有效，为未来方法提供原则性基础。

Abstract: Imitation learning from large-scale, diverse human demonstrations has proven
effective for training robots, but collecting such data is costly and
time-consuming. This challenge is amplified for multi-step bimanual mobile
manipulation, where humans must teleoperate both a mobile base and two
high-degree-of-freedom arms. Prior automated data generation frameworks have
addressed static bimanual manipulation by augmenting a few human demonstrations
in simulation, but they fall short for mobile settings due to two key
challenges: (1) determining base placement to ensure reachability, and (2)
positioning the camera to provide sufficient visibility for visuomotor
policies. To address these issues, we introduce MoMaGen, which formulates data
generation as a constrained optimization problem that enforces hard constraints
(e.g., reachability) while balancing soft constraints (e.g., visibility during
navigation). This formulation generalizes prior approaches and provides a
principled foundation for future methods. We evaluate MoMaGen on four
multi-step bimanual mobile manipulation tasks and show that it generates
significantly more diverse datasets than existing methods. Leveraging this
diversity, MoMaGen can train successful imitation learning policies from a
single source demonstration, and these policies can be fine-tuned with as few
as 40 real-world demonstrations to achieve deployment on physical robotic
hardware. More details are available at our project page: momagen.github.io.

</details>


### [331] [PGTT: Phase-Guided Terrain Traversal for Perceptive Legged Locomotion](https://arxiv.org/abs/2510.18348)
*Alexandros Ntagkas,Chairi Kiourt,Konstantinos Chatzilygeroudis*

Main category: cs.RO

TL;DR: 本文提出PGTT方法，通过奖励塑造克服现有有感知强化学习控制器的局限，在模拟和真实机器人上表现良好，证明地形自适应、相位引导的奖励塑造是实现跨平台稳健感知运动的有效机制。


<details>
  <summary>Details</summary>
Motivation: 现有有感知强化学习控制器存在施加步态先验限制动作空间、减少适应性或‘盲目’操作难以预判地形且易受噪声影响的问题。

Method: 提出PGTT方法，通过奖励塑造强化步态结构，将每只腿的相位编码为三次埃尔米特样条，适应摆动高度并增加摆动相接触惩罚，策略直接在关节空间中作用。在MuJoCo上使用课程学习和领域随机化训练。

Result: PGTT在推力干扰和离散障碍物上成功率最高，速度跟踪相当，收敛到有效策略比端到端基线快约2倍，在Unitree Go2和ANYmal - C上验证。

Conclusion: 地形自适应、相位引导的奖励塑造是实现跨平台稳健感知运动的简单通用机制。

Abstract: State-of-the-art perceptive Reinforcement Learning controllers for legged
robots either (i) impose oscillator or IK-based gait priors that constrain the
action space, add bias to the policy optimization and reduce adaptability
across robot morphologies, or (ii) operate "blind", which struggle to
anticipate hind-leg terrain, and are brittle to noise. In this paper, we
propose Phase-Guided Terrain Traversal (PGTT), a perception-aware deep-RL
approach that overcomes these limitations by enforcing gait structure purely
through reward shaping, thereby reducing inductive bias in policy learning
compared to oscillator/IK-conditioned action priors. PGTT encodes per-leg phase
as a cubic Hermite spline that adapts swing height to local heightmap
statistics and adds a swing-phase contact penalty, while the policy acts
directly in joint space supporting morphology-agnostic deployment. Trained in
MuJoCo (MJX) on procedurally generated stair-like terrains with curriculum and
domain randomization, PGTT achieves the highest success under push disturbances
(median +7.5% vs. the next best method) and on discrete obstacles (+9%), with
comparable velocity tracking, and converging to an effective policy roughly 2x
faster than strong end-to-end baselines. We validate PGTT on a Unitree Go2
using a real-time LiDAR elevation-to-heightmap pipeline, and we report
preliminary results on ANYmal-C obtained with the same hyperparameters. These
findings indicate that terrain-adaptive, phase-guided reward shaping is a
simple and general mechanism for robust perceptive locomotion across platforms.

</details>


### [332] [A Compositional Paradigm for Foundation Models: Towards Smarter Robotic Agents](https://arxiv.org/abs/2510.18608)
*Luigi Quarantiello,Elia Piccoli,Jack Bell,Malio Li,Giacomo Carfì,Eric Nuertey Coleman,Gerlando Gramaglia,Lanpei Li,Mauro Madeddu,Irene Testa,Vincenzo Lomonaco*

Main category: cs.RO

TL;DR: 基础模型在多任务表现出色，但适应动态场景有问题，本文提议用持续学习和组合性原则开发更好的AI解决方案。


<details>
  <summary>Details</summary>
Motivation: 基础模型在适应动态现实场景时存在需从头重新训练的问题，需要更灵活、高效和智能的AI解决方案。

Method: 应用持续学习和组合性原则。

Result: 原文未提及。

Conclusion: 原文未提及。

Abstract: The birth of Foundation Models brought unprecedented results in a wide range
of tasks, from language to vision, to robotic control. These models are able to
process huge quantities of data, and can extract and develop rich
representations, which can be employed across different domains and modalities.
However, they still have issues in adapting to dynamic, real-world scenarios
without retraining the entire model from scratch. In this work, we propose the
application of Continual Learning and Compositionality principles to foster the
development of more flexible, efficient and smart AI solutions.

</details>


### [333] [EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval](https://arxiv.org/abs/2510.18546)
*Zebin Yang,Sunjian Zheng,Tong Xie,Tianshi Xu,Bo Yu,Fan Wang,Jie Tang,Shaoshan Liu,Meng Li*

Main category: cs.RO

TL;DR: 提出EfficientNav实现基于LLM的设备端高效零样本目标导航，提高成功率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有目标导航代理依赖云端大模型，切换小模型成功率下降且本地设备规划延迟高，难以部署在本地设备。

Method: 提出语义感知内存检索修剪导航地图冗余信息，提出离散内存缓存和基于注意力的内存聚类高效保存和重用KV缓存。

Result: EfficientNav在HM3D基准测试中成功率比基于GPT - 4的基线提高11.1%，实时延迟降低6.7倍，端到端延迟降低4.7倍。

Conclusion: EfficientNav能实现设备端高效的基于LLM的零样本目标导航。

Abstract: Object-goal navigation (ObjNav) tasks an agent with navigating to the
location of a specific object in an unseen environment. Embodied agents
equipped with large language models (LLMs) and online constructed navigation
maps can perform ObjNav in a zero-shot manner. However, existing agents heavily
rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small
LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to
limited model capacity for understanding complex navigation maps, which
prevents deploying ObjNav on local devices. At the same time, the long prompt
introduced by the navigation map description will cause high planning latency
on local devices. In this paper, we propose EfficientNav to enable on-device
efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better
understand the environment, we propose semantics-aware memory retrieval to
prune redundant information in navigation maps. To reduce planning latency, we
propose discrete memory caching and attention-based memory clustering to
efficiently save and re-use the KV cache. Extensive experimental results
demonstrate that EfficientNav achieves 11.1% improvement in success rate on
HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time
latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our
code will be released soon.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [334] [Joint Estimation of Piano Dynamics and Metrical Structure with a Multi-task Multi-Scale Network](https://arxiv.org/abs/2510.18190)
*Zhanhong He,Hanyu Meng,David Huang,Roberto Togneri*

Main category: eess.AS

TL;DR: 提出高效多任务网络联合预测钢琴动态相关指标，用Bark尺度特征减小模型大小，在数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 解决从音频记录中估计钢琴动态这一计算音乐分析的基础挑战。

Method: 提出多任务网络，以Bark尺度特定响度为输入特征，采用多尺度网络为骨干，音频分割用60秒长度。

Result: 在公共MazurkaBL数据集上所有任务取得了最先进的结果。

Conclusion: 为钢琴动态估计设定新基准，提供强大紧凑工具，为音乐表达大规模、资源高效分析奠定基础。

Abstract: Estimating piano dynamic from audio recordings is a fundamental challenge in
computational music analysis. In this paper, we propose an efficient multi-task
network that jointly predicts dynamic levels, change points, beats, and
downbeats from a shared latent representation. These four targets form the
metrical structure of dynamics in the music score. Inspired by recent vocal
dynamic research, we use a multi-scale network as the backbone, which takes
Bark-scale specific loudness as the input feature. Compared to log-Mel as
input, this reduces model size from 14.7 M to 0.5 M, enabling long sequential
input. We use a 60-second audio length in audio segmentation, which doubled the
length of beat tracking commonly used. Evaluated on the public MazurkaBL
dataset, our model achieves state-of-the-art results across all tasks. This
work sets a new benchmark for piano dynamic estimation and delivers a powerful
and compact tool, paving the way for large-scale, resource-efficient analysis
of musical expression.

</details>


### [335] [Diffusion Buffer for Online Generative Speech Enhancement](https://arxiv.org/abs/2510.18744)
*Bunlong Lay,Rostislav Makarov,Simon Welker,Maris Hillemann,Timo Gerkmann*

Main category: eess.AS

TL;DR: 本文提出基于扩散的在线语音增强模型Diffusion Buffer，改进网络架构和损失函数，大幅降低算法延迟并提升性能，且在未见噪声语音数据上优于预测模型。


<details>
  <summary>Details</summary>
Motivation: 生成式语音增强模型计算复杂度高，不适用于在线语音增强应用，需降低复杂度。

Method: 提出Diffusion Buffer模型，使物理时间与扩散时间步对齐；设计与Diffusion Buffer前瞻相匹配的2D卷积UNet架构；使用数据预测损失替代去噪分数匹配损失。

Result: 改进后的Diffusion Buffer大幅降低算法延迟，从320 - 960 ms降至32 - 176 ms，且性能提升。

Conclusion: 在线Diffusion Buffer模型在未见噪声语音数据上优于预测模型。

Abstract: Online Speech Enhancement was mainly reserved for predictive models. A key
advantage of these models is that for an incoming signal frame from a stream of
data, the model is called only once for enhancement. In contrast, generative
Speech Enhancement models often require multiple calls, resulting in a
computational complexity that is too high for many online speech enhancement
applications. This work presents the Diffusion Buffer, a generative
diffusion-based Speech Enhancement model which only requires one neural network
call per incoming signal frame from a stream of data and performs enhancement
in an online fashion on a consumer-grade GPU. The key idea of the Diffusion
Buffer is to align physical time with Diffusion time-steps. The approach
progressively denoises frames through physical time, where past frames have
more noise removed. Consequently, an enhanced frame is output to the listener
with a delay defined by the Diffusion Buffer, and the output frame has a
corresponding look-ahead. In this work, we extend upon our previous work by
carefully designing a 2D convolutional UNet architecture that specifically
aligns with the Diffusion Buffer's look-ahead. We observe that the proposed
UNet improves performance, particularly when the algorithmic latency is low.
Moreover, we show that using a Data Prediction loss instead of Denoising Score
Matching loss enables flexible control over the trade-off between algorithmic
latency and quality during inference. The extended Diffusion Buffer equipped
with a novel NN and loss function drastically reduces the algorithmic latency
from 320 - 960 ms to 32 - 176 ms with an even increased performance. While it
has been shown before that offline generative diffusion models outperform
predictive approaches in unseen noisy speech data, we confirm that the online
Diffusion Buffer also outperforms its predictive counterpart on unseen noisy
speech data.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [336] [A New Broadcast Model for Several Network Topologies](https://arxiv.org/abs/2510.18058)
*Hongbo Lu,Junsung Hwang,Bernard Tenreiro,Nabila Jaman Tripti,Darren Hamilton,Yuefan Deng*

Main category: cs.NI

TL;DR: 提出广播算法BBS，优化不同网络拓扑通信效率，模拟显示其性能优于常见算法。


<details>
  <summary>Details</summary>
Motivation: 解决广播操作中拓扑约束、带宽限制和同步开销等问题，特别是在超级计算机等大规模系统中优化通信效率。

Method: 设计BBS算法，通过精确通信周期提供可重复、简化的分步广播框架，确保节点在广播中持续活动。

Result: 在各种拓扑的模拟结果中，BBS算法始终大幅优于常见的通用广播算法。

Conclusion: BBS是一个通用且强大的框架，有潜力重新定义跨网络拓扑的广播策略。

Abstract: We present Broadcast by Balanced Saturation (BBS), a general broadcast
algorithm designed to optimize communication efficiency across diverse network
topologies. BBS maximizes node utilization, addressing challenges in broadcast
operations such as topology constraints, bandwidth limitations, and
synchronization overhead, particularly in large-scale systems like
supercomputers. The algorithm ensures sustained activity with nodes throughout
the broadcast, thereby enhancing data propagation and significantly reducing
latency. Through a precise communication cycle, BBS provides a repeatable,
streamlined, stepwise broadcasting framework. Simulation results across various
topologies demonstrate that the BBS algorithm consistently outperforms common
general broadcast algorithms, often by a substantial margin. These findings
suggest that BBS is a versatile and robust framework with the potential to
redefine broadcast strategies across network topologies.

</details>


### [337] [Revisiting RFID Missing Tag Identification](https://arxiv.org/abs/2510.18285)
*Kanghuai Liu,Lin Chen,Jihong Yu,Junyi Huang,Shiyuan Liu*

Main category: cs.NI

TL;DR: 本文重新审视RFID网络中缺失标签识别问题，定量比较现有方案，给出执行时间下界，提出新算法降低时间开销。


<details>
  <summary>Details</summary>
Motivation: 重新研究RFID网络中缺失标签识别问题，改进现有方案的性能。

Method: 定量比较现有方案，分析得出执行时间下界，利用碰撞分区树（CPT）数据结构开发新算法。

Result: 给出最佳现有方案的执行时间、任意算法执行时间下界，新算法执行时间相比最佳现有算法最多降低log N倍。

Conclusion: 新的缺失标签识别算法能有效降低时间开销，CPT数据结构有助于减少时间复杂度。

Abstract: We revisit the problem of missing tag identification in RFID networks by
making three contributions. Firstly, we quantitatively compare and gauge the
existing propositions spanning over a decade on missing tag identification. We
show that the expected execution time of the best solution in the literature is
$\Theta \left(N+\frac{(1-\alpha)^2(1-\delta)^2}{ \epsilon^2}\right)$, where
$\delta$ and $\epsilon$ are parameters quantifying the required identification
accuracy, $N$ denotes the number of tags in the system, among which $\alpha N$
tags are missing. Secondly, we analytically establish the expected execution
time lower-bound for any missing tag identification algorithm as
$\Theta\left(\frac{N}{\log N}+\frac{(1-\delta)^2(1-\alpha)^2}{\epsilon^2 \log
\frac{(1-\delta)(1-\alpha)}{\epsilon}}\right)$, thus giving the theoretical
performance limit. Thirdly, we develop a novel missing tag identification
algorithm by leveraging a tree structure with the expected execution time of
$\Theta \left(\frac{\log\log N}{\log N}N+\frac{(1-\alpha)^2(1-\delta)^2}{
\epsilon^2}\right)$, reducing the time overhead by a factor of up to $\log N$
over the best algorithm in the literature. The key technicality in our design
is a novel data structure termed as collision-partition tree (CPT), built on a
subset of bits in tag pseudo-IDs, leading to more balanced tree structure and
reducing the time complexity in parsing the entire tree.

</details>


### [338] [On AI Verification in Open RAN](https://arxiv.org/abs/2510.18417)
*Rahul Soundrarajan,Claudio Fiandrino,Michele Polese,Salvatore D'Oro,Leonardo Bonati,Tommaso Melodia*

Main category: cs.NI

TL;DR: 本文针对Open RAN中RAN切片和调度的DRL代理行为，提出基于可解释模型的轻量级验证方法，分析XAI和AI验证，展示可行性并指出未来挑战。


<details>
  <summary>Details</summary>
Motivation: Open RAN引入AI/ML驱动自动化，但XAI仅解释性不能保证网络可靠运行，需验证DRL代理行为。

Method: 使用基于决策树的验证器在运行时进行近实时一致性检查，分析XAI和AI验证并提出可扩展架构集成。

Result: 通过基于决策树的切片验证器证明了方法的可行性。

Conclusion: 指出确保Open RAN中可信AI采用的未来挑战。

Abstract: Open RAN introduces a flexible, cloud-based architecture for the Radio Access
Network (RAN), enabling Artificial Intelligence (AI)/Machine Learning
(ML)-driven automation across heterogeneous, multi-vendor deployments. While
EXplainable Artificial Intelligence (XAI) helps mitigate the opacity of AI
models, explainability alone does not guarantee reliable network operations. In
this article, we propose a lightweight verification approach based on
interpretable models to validate the behavior of Deep Reinforcement Learning
(DRL) agents for RAN slicing and scheduling in Open RAN. Specifically, we use
Decision Tree (DT)-based verifiers to perform near-real-time consistency checks
at runtime, which would be otherwise unfeasible with computationally expensive
state-of-the-art verifiers. We analyze the landscape of XAI and AI
verification, propose a scalable architectural integration, and demonstrate
feasibility with a DT-based slice-verifier. We also outline future challenges
to ensure trustworthy AI adoption in Open RAN.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [339] [Protein generation with embedding learning for motif diversification](https://arxiv.org/abs/2510.18790)
*Kevin Michalewicz,Chen Jin,Philip Alexander Teare,Tom Diethe,Mauricio Barahona,Barbara Bravi,Asher Mullokandov*

Main category: q-bio.QM

TL;DR: 提出PGEL框架解决蛋白质设计中结构多样性与基序生物功能保留的权衡问题，在三个案例中表现优于部分扩散法


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质设计方法难以解决产生结构多样性与保留基序生物功能的权衡问题

Method: 提出PGEL框架，在扩散模型的冻结去噪器的表示空间中学习编码目标基序序列和结构特征的高维嵌入，并在嵌入空间中引入受控扰动

Result: 在单体、蛋白质 - 蛋白质界面和癌症相关转录因子复合物三个案例中，PGEL比部分扩散法实现了更高的结构多样性、更好的可设计性和改进的自一致性

Conclusion: PGEL是一种基于嵌入驱动的蛋白质生成通用策略，可对功能基序进行系统、可行的多样化

Abstract: A fundamental challenge in protein design is the trade-off between generating
structural diversity while preserving motif biological function. Current
state-of-the-art methods, such as partial diffusion in RFdiffusion, often fail
to resolve this trade-off: small perturbations yield motifs nearly identical to
the native structure, whereas larger perturbations violate the geometric
constraints necessary for biological function. We introduce Protein Generation
with Embedding Learning (PGEL), a general framework that learns
high-dimensional embeddings encoding sequence and structural features of a
target motif in the representation space of a diffusion model's frozen
denoiser, and then enhances motif diversity by introducing controlled
perturbations in the embedding space. PGEL is thus able to loosen geometric
constraints while satisfying typical design metrics, leading to more diverse
yet viable structures. We demonstrate PGEL on three representative cases: a
monomer, a protein-protein interface, and a cancer-related transcription factor
complex. In all cases, PGEL achieves greater structural diversity, better
designability, and improved self-consistency, as compared to partial diffusion.
Our results establish PGEL as a general strategy for embedding-driven protein
generation allowing for systematic, viable diversification of functional
motifs.

</details>


### [340] [CBINNS: Cancer Biology-Informed Neural Network for Unknown Parameter Estimation and Missing Physics Identification](https://arxiv.org/abs/2510.17920)
*Bishal Chhetri,B. V. Rathish Kumar*

Main category: q-bio.QM

TL;DR: 本文开发癌症生物学信息神经网络模型（CBINN），从稀疏和有噪声测量数据中推断方程未知参数和发现缺失物理规律，在三个模型上测试其性能并验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 肿瘤 - 免疫相互作用动力学模型存在未知参数需准确高效估计，且方程存在未知或缺失项，现有知识不完整。

Method: 开发癌症生物学信息神经网络模型（CBINN），在三个非线性肿瘤 - 免疫模型上测试其性能，并评估不同噪声水平下的鲁棒性。

Result: CBINN框架能有效估计未知模型参数，揭示生物系统的物理规律和数学结构。

Conclusion: 所选模型验证了该方法的泛化性和有效性。

Abstract: The dynamics of tumor-immune interactions within a complex tumor
microenvironment are typically modeled using a system of ordinary differential
equations or partial differential equations. These models introduce some
unknown parameters that need to be estimated accurately and efficiently from
the limited and noisy experimental data. Moreover, due to the intricate
biological complexity and limitations in experimental measurements,
tumor-immune dynamics are not fully understood, and therefore, only partial
knowledge of the underlying physics may be available, resulting in unknown or
missing terms within the system of equations. In this study, we develop a
cancer biology-informed neural network model(CBINN) to infer the unknown
parameters in the system of equations as well as to discover the missing
physics from sparse and noisy measurements. We test the performance of the
CBINN model on three distinct nonlinear compartmental tumor-immune models and
evaluate its robustness across multiple synthetic noise levels. By harnessing
these highly nonlinear dynamics, our CBINN framework effectively estimates the
unknown model parameters and uncovers the underlying physical laws or
mathematical structures that govern these biological systems, even from
scattered and noisy measurements. The models chosen here represent the dynamic
patterns commonly observed in compartmental models of tumor-immune
interactions, thereby validating the generalizability and efficacy of our
methodology.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [341] [A Biophysical-Model-Informed Source Separation Framework For EMG Decomposition](https://arxiv.org/abs/2510.17822)
*D. Halatsis,P. Mamidanna,J. Pereira,D. Farina*

Main category: q-bio.NC

TL;DR: 本文介绍新的BMISS框架用于表面肌电图运动单元分解，在模拟环境验证其优势，为神经肌肉评估提供新途径。


<details>
  <summary>Details</summary>
Motivation: 传统盲源分离方法用于表面肌电图运动单元分解时，未纳入生物物理约束，限制了准确性和可解释性。

Method: 引入Biophysical - Model - Informed Source Separation (BMISS)框架，将解剖学上准确的正向肌电图模型集成到分解过程，利用基于MRI的解剖重建和生成建模进行无监督估计。

Result: 在受控模拟环境中，BMISS实现了更高保真度的运动单元估计，且与传统方法相比显著降低了计算成本。

Conclusion: 该框架为非侵入性、个性化神经肌肉评估铺平道路，在临床诊断、假肢控制和神经康复等方面有潜在应用。

Abstract: Recent advances in neural interfacing have enabled significant improvements
in human-computer interaction, rehabilitation, and neuromuscular diagnostics.
Motor unit (MU) decomposition from surface electromyography (sEMG) is a key
technique for extracting neural drive information, but traditional blind source
separation (BSS) methods fail to incorporate biophysical constraints, limiting
their accuracy and interpretability. In this work, we introduce a novel
Biophysical-Model-Informed Source Separation (BMISS) framework, which
integrates anatomically accurate forward EMG models into the decomposition
process. By leveraging MRI-based anatomical reconstructions and generative
modeling, our approach enables direct inversion of a biophysically accurate
forward model to estimate both neural drive and motor neuron properties in an
unsupervised manner. Empirical validation in a controlled simulated setting
demonstrates that BMISS achieves higher fidelity motor unit estimation while
significantly reducing computational cost compared to traditional methods. This
framework paves the way for non-invasive, personalized neuromuscular
assessments, with potential applications in clinical diagnostics, prosthetic
control, and neurorehabilitation.

</details>


### [342] [Brain-Language Model Alignment: Insights into the Platonic Hypothesis and Intermediate-Layer Advantage](https://arxiv.org/abs/2510.17833)
*Ángela López-Cardona,Sebastián Idesis,Mireia Masias-Bruns,Sergi Abadal,Ioannis Arapakis*

Main category: q-bio.NC

TL;DR: 文章回顾2023 - 2025年25项基于fMRI的研究，对比两个关键假设，发现模型和大脑可能共享抽象表征结构。


<details>
  <summary>Details</summary>
Motivation: 探究大脑和语言模型是否趋向于相同的世界内部表征，近年来相关研究增多。

Method: 回顾2023 - 2025年25项基于fMRI的研究，将研究结果与柏拉图表征假设和中间层优势假设进行对比。

Result: 有证据表明模型和大脑可能共享抽象表征结构，支持两个假设。

Conclusion: 支持两个假设，激励对脑 - 模型对齐进行更多研究。

Abstract: Do brains and language models converge toward the same internal
representations of the world? Recent years have seen a rise in studies of
neural activations and model alignment. In this work, we review 25 fMRI-based
studies published between 2023 and 2025 and explicitly confront their findings
with two key hypotheses: (i) the Platonic Representation Hypothesis -- that as
models scale and improve, they converge to a representation of the real world,
and (ii) the Intermediate-Layer Advantage -- that intermediate (mid-depth)
layers often encode richer, more generalizable features. Our findings provide
converging evidence that models and brains may share abstract representational
structures, supporting both hypotheses and motivating further research on
brain-model alignment.

</details>


### [343] [Decoding Dynamic Visual Experience from Calcium Imaging via Cell-Pattern-Aware SSL](https://arxiv.org/abs/2510.18516)
*Sangyoon Bae,Mehdi Azabou,Jiook Cha,Blake Richards*

Main category: q-bio.NC

TL;DR: 提出POYO - SSL方法利用神经数据异质性改进自监督预训练，在数据集上比从头训练有增益且能随模型规模平滑扩展。


<details>
  <summary>Details</summary>
Motivation: 因神经数据集缺乏大规模一致标注且数据异质性导致自监督预训练难识别一致模式，难以体现规模优势。

Method: 在预训练阶段仅对通过高阶统计识别出的可预测神经元进行预训练，再对不可预测神经元进行微调用于下游任务。

Result: 在Allen Brain Observatory数据集上比从头训练有12 - 13%相对增益，随模型规模平滑扩展，而现有基线方法会停滞或不稳定。

Conclusion: POYO - SSL将数据异质性转化为优势，为可扩展神经解码和神经动力学基础模型提供了方法。

Abstract: Self-supervised learning (SSL) holds a great deal of promise for applications
in neuroscience, due to the lack of large-scale, consistently labeled neural
datasets. However, most neural datasets contain heterogeneous populations that
mix stable, predictable cells with highly stochastic, stimulus-contingent ones,
which has made it hard to identify consistent activity patterns during SSL. As
a result, self-supervised pretraining has yet to show clear signs of benefits
from scale on neural data. Here, we present a novel approach to self-supervised
pretraining, POYO-SSL that exploits the heterogeneity of neural data to improve
pre-training and achieve benefits of scale. Specifically, in POYO-SSL we
pretrain only on predictable (statistically regular) neurons-identified on the
pretraining split via simple higher-order statistics (skewness and
kurtosis)-then we fine-tune on the unpredictable population for downstream
tasks. On the Allen Brain Observatory dataset, this strategy yields
approximately 12-13% relative gains over from-scratch training and exhibits
smooth, monotonic scaling with model size. In contrast, existing
state-of-the-art baselines plateau or destabilize as model size increases. By
making predictability an explicit metric for crafting the data diet, POYO-SSL
turns heterogeneity from a liability into an asset, providing a robust,
biologically grounded recipe for scalable neural decoding and a path toward
foundation models of neural dynamics.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [344] [Does GenAI Rewrite How We Write? An Empirical Study on Two-Million Preprints](https://arxiv.org/abs/2510.17882)
*Minfeng Qi,Zhongmin Cao,Qin Wang,Ningran Li,Tianqing Zhu*

Main category: cs.CY

TL;DR: 本文对超210万篇预印本展开大规模分析，发现大语言模型加速提交和修订周期、增加语言复杂度、扩大AI相关主题，揭示其是选择性催化剂，为评估生成式AI对学术出版的影响提供实证基础。


<details>
  <summary>Details</summary>
Motivation: 当前关于大语言模型是否以及如何重塑科学出版的系统证据有限，需填补这一研究空白。

Method: 引入多层次分析框架，整合中断时间序列模型、协作和生产力指标、语言剖析和主题建模，对2016 - 2025年四个主要预印本库的超210万篇预印本进行分析。

Result: 大语言模型加速提交和修订周期，适度增加语言复杂性，不成比例地扩大AI相关主题，计算密集型领域受益更多。

Conclusion: 大语言模型是选择性催化剂，放大现有优势、扩大学科差距，为评估生成式AI影响提供实证基础，强调需治理框架维护研究生态的信任、公平和责任。

Abstract: Preprint repositories become central infrastructures for scholarly
communication. Their expansion transforms how research is circulated and
evaluated before journal publication. Generative large language models (LLMs)
introduce a further potential disruption by altering how manuscripts are
written. While speculation abounds, systematic evidence of whether and how LLMs
reshape scientific publishing remains limited.
  This paper addresses the gap through a large-scale analysis of more than 2.1
million preprints spanning 2016--2025 (115 months) across four major
repositories (i.e., arXiv, bioRxiv, medRxiv, SocArXiv). We introduce a
multi-level analytical framework that integrates interrupted time-series
models, collaboration and productivity metrics, linguistic profiling, and topic
modeling to assess changes in volume, authorship, style, and disciplinary
orientation. Our findings reveal that LLMs have accelerated submission and
revision cycles, modestly increased linguistic complexity, and
disproportionately expanded AI-related topics, while computationally intensive
fields benefit more than others. These results show that LLMs act less as
universal disruptors than as selective catalysts, amplifying existing strengths
and widening disciplinary divides. By documenting these dynamics, the paper
provides the first empirical foundation for evaluating the influence of
generative AI on academic publishing and highlights the need for governance
frameworks that preserve trust, fairness, and accountability in an AI-enabled
research ecosystem.

</details>


### [345] [Are LLMs Court-Ready? Evaluating Frontier Models on Indian Legal Reasoning](https://arxiv.org/abs/2510.17900)
*Kush Juvekar,Arghya Bhattacharya,Sai Khadloya,Utkarsh Saxena*

Main category: cs.CY

TL;DR: 本研究用印度公共法律考试评估大语言模型在法律工作流程中的能力，发现前沿模型在客观考试表现好，但长文推理不及人类，明确了其辅助和需人类主导的领域。


<details>
  <summary>Details</summary>
Motivation: 缺乏特定司法管辖区框架评估大语言模型在法律工作流程中的基础能力。

Method: 以印度公共法律考试为代理，构建多年基准，评估开放和前沿大语言模型，还进行律师评分的双盲研究。

Result: 前沿模型在客观考试常达标或超高分段，但长文推理不及人类，发现三种可靠性失败模式。

Conclusion: 明确大语言模型可辅助和需人类领导的法律工作领域。

Abstract: Large language models (LLMs) are entering legal workflows, yet we lack a
jurisdiction-specific framework to assess their baseline competence therein. We
use India's public legal examinations as a transparent proxy. Our multi-year
benchmark assembles objective screens from top national and state exams and
evaluates open and frontier LLMs under real-world exam conditions. To probe
beyond multiple-choice questions, we also include a lawyer-graded,
paired-blinded study of long-form answers from the Supreme Court's
Advocate-on-Record exam. This is, to our knowledge, the first exam-grounded,
India-specific yardstick for LLM court-readiness released with datasets and
protocols. Our work shows that while frontier systems consistently clear
historical cutoffs and often match or exceed recent top-scorer bands on
objective exams, none surpasses the human topper on long-form reasoning. Grader
notes converge on three reliability failure modes: procedural or format
compliance, authority or citation discipline, and forum-appropriate voice and
structure. These findings delineate where LLMs can assist (checks,
cross-statute consistency, statute and precedent lookups) and where human
leadership remains essential: forum-specific drafting and filing, procedural
and relief strategy, reconciling authorities and exceptions, and ethical,
accountable judgment.

</details>


### [346] [Interpretability Framework for LLMs in Undergraduate Calculus](https://arxiv.org/abs/2510.17910)
*Sagnik Dakshit,Sushmita Sinha Roy*

Main category: cs.CY

TL;DR: 本文提出用于分析大语言模型生成的数学解题方案的可解释性框架，评估发现模型常产生概念有缺陷的解答，该框架为AI在STEM学习中的应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法多关注最终答案准确性，忽略推理过程，无法全面衡量大语言模型在数学教育中解题行为的质量、可靠性和教学有效性。

Method: 以本科微积分问题为代表领域，结合推理流提取、将解答分解为语义标记的操作和概念以及提示消融分析，用推理复杂性、短语敏感性和鲁棒性等结构化指标评估。

Result: 大语言模型常产生语法流畅但概念有缺陷的解答，推理模式对提示措辞和输入变化敏感。

Conclusion: 该框架能细粒度诊断推理失败，支持课程对齐，为设计可解释的AI辅助反馈工具提供参考，是首个为数学教育中解释大语言模型推理提供的结构化、量化且有教学基础的框架。

Abstract: Large Language Models (LLMs) are increasingly being used in education, yet
their correctness alone does not capture the quality, reliability, or
pedagogical validity of their problem-solving behavior, especially in
mathematics, where multistep logic, symbolic reasoning, and conceptual clarity
are critical. Conventional evaluation methods largely focus on final answer
accuracy and overlook the reasoning process. To address this gap, we introduce
a novel interpretability framework for analyzing LLM-generated solutions using
undergraduate calculus problems as a representative domain. Our approach
combines reasoning flow extraction and decomposing solutions into semantically
labeled operations and concepts with prompt ablation analysis to assess input
salience and output stability. Using structured metrics such as reasoning
complexity, phrase sensitivity, and robustness, we evaluated the model behavior
on real Calculus I to III university exams. Our findings revealed that LLMs
often produce syntactically fluent yet conceptually flawed solutions, with
reasoning patterns sensitive to prompt phrasing and input variation. This
framework enables fine-grained diagnosis of reasoning failures, supports
curriculum alignment, and informs the design of interpretable AI-assisted
feedback tools. This is the first study to offer a structured, quantitative,
and pedagogically grounded framework for interpreting LLM reasoning in
mathematics education, laying the foundation for the transparent and
responsible deployment of AI in STEM learning environments.

</details>


### [347] [Attracting Commercial Artificial Intelligence Firms to Support National Security through Collaborative Contracts](https://arxiv.org/abs/2510.17931)
*Andrew Bowne*

Main category: cs.CY

TL;DR: 本文探讨商业AI公司与国防部合作的决策因素，引入最优买家理论，得出公司倾向符合自身业务和技术考量的合同，并提出利用现有合同法的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对商业AI公司决定与国防部合作或不进入国防市场原因的理解。

Method: 运用社会交换理论，引入最优买家理论，对部分参与者进行访谈。

Result: 商业AI行业认为国防部是有吸引力的客户，但传统合同法和采购实践存在障碍。

Conclusion: 商业AI公司青睐符合自身业务和技术考量的合同，应利用现有合同法使合同实践与商业偏好和机器学习开发生命周期相契合。

Abstract: Unlike other military technologies driven by national security needs and
developed with federal funding, AI is predominantly funded and advanced by
commercial industry for civilian applications. However, there is a lack of
understanding of the reasons commercial AI firms decide to work with the DoD or
choose to abstain from the defence market. This thesis argues that the contract
law and procurement framework are among the most significant obstacles. This
research indicates that the commercial AI industry actually views the DoD as an
attractive customer. However, this attraction is despite the obstacles
presented by traditional contract law and procurement practices used to solicit
and award contracts. Drawing on social exchange theory, this thesis introduces
a theoretical framework, optimal buyer theory, to understand the factors that
influence a commercial decision to engage with the DoD. Interviews from a
sample of the participants explain why the AI industry holds such perceptions,
opinions, and preferences about contracts generally and the DoD, specifically,
in its role as a customer. This thesis concludes that commercial AI firms are
attracted to contracts that are consistent with their business and technology
considerations. Additionally, it develops best practices for leveraging
existing contract law, primarily other transaction authority, to align
contracting practices with commercial preferences and the machine learning
development and deployment lifecycle.

</details>


### [348] [The Integration of Artificial Intelligence in Undergraduate Medical Education in Spain: Descriptive Analysis and International Perspectives](https://arxiv.org/abs/2510.17938)
*Ana Enériz Janeiro,Karina Pitombeira Pereira,Julio Mayol,Javier Crespo,Fernando Carballo,Juan B. Cabello,Manel Ramos-Casals,Bibiana Pérez Corbacho,Juan Turnes*

Main category: cs.CY

TL;DR: 本文研究西班牙医学院校AI课程融入情况，发现融入程度低、碎片化且不均衡，建议建立最低标准和国家指标监测。


<details>
  <summary>Details</summary>
Motivation: 此前西班牙未系统评估AI融入医学课程情况，需了解现状。

Method: 对西班牙提供医学官方学位的大学进行横断面研究，审查课程和公开文件，用描述性统计分析。

Result: 52所大学中19.2%提供特定AI课程，69.2%无相关内容；多数课程为选修，学分占比低；地域差异大。

Conclusion: 西班牙医学学位中AI融入初期、碎片化且不均衡，限制未来医生准备，需建立最低标准和国家指标监测。

Abstract: AI is transforming medical practice and redefining the competencies that
future healthcare professionals need to master. Despite international
recommendations, the integration of AI into Medicine curricula in Spain had not
been systematically evaluated until now. A cross-sectional study
(July-September 2025) including Spanish universities offering the official
degree in Medicine, according to the 'Register of Universities, Centers and
Degrees (Registro de Universidades, Centros y T\'itulos RUCT)'. Curricula and
publicly available institutional documentation were reviewed to identify
courses and competencies related to AI in the 2025-2026 academic year. The
analysis was performed using descriptive statistics. Of the 52 universities
analyzed, ten (19.2%) offer specific AI courses, whereas 36 (69.2%) include no
related content. Most of the identified courses are elective, with a credit
load ranging from three to six ECTS, representing on average 1.17% of the total
360 credits of the degree. The University of Ja\'en is the only institution
offering a compulsory course with AI content. The territorial analysis reveals
marked disparities: Andalusia leads with 55.5% of its universities
incorporating AI training, while several communities lack any initiative in
this area. The integration of AI into the medical degree in Spain is incipient,
fragmented, and uneven, with a low weight in ECTS. The limited training load
and predominance of elective courses restrict the preparation of future
physicians to practice in a healthcare environment increasingly mediated by AI.
The findings support the establishment of minimum standards and national
monitoring of indicators.

</details>


### [349] [Trust in foundation models and GenAI: A geographic perspective](https://arxiv.org/abs/2510.17942)
*Grant McKenzie,Krzysztof Janowicz,Carsten Kessler*

Main category: cs.CY

TL;DR: 文章探讨地理背景下基础模型的信任概念，分类并分析信任类型，讨论挑战与责任，呼吁提升透明度等。


<details>
  <summary>Details</summary>
Motivation: 随着大规模预训练模型在地理领域应用增加，信任在关键决策中变得至关重要但概念碎片化，需深入理解。

Method: 将信任分为认知信任、操作信任和人际信任三类，分析各类信任在地理应用中的影响，探讨偏见挑战、透明度和伦理责任。

Result: 明确了不同类型信任及其对地理应用的影响，指出了模型开发中的问题。

Conclusion: 为研究人员、从业者和政策制定者提供理解地理人工智能信任的概念起点，呼吁提升透明度、缓解偏见和制定区域政策。

Abstract: Large-scale pre-trained machine learning models have reshaped our
understanding of artificial intelligence across numerous domains, including our
own field of geography. As with any new technology, trust has taken on an
important role in this discussion. In this chapter, we examine the multifaceted
concept of trust in foundation models, particularly within a geographic
context. As reliance on these models increases and they become relied upon for
critical decision-making, trust, while essential, has become a fractured
concept. Here we categorize trust into three types: epistemic trust in the
training data, operational trust in the model's functionality, and
interpersonal trust in the model developers. Each type of trust brings with it
unique implications for geographic applications. Topics such as cultural
context, data heterogeneity, and spatial relationships are fundamental to the
spatial sciences and play an important role in developing trust. The chapter
continues with a discussion of the challenges posed by different forms of
biases, the importance of transparency and explainability, and ethical
responsibilities in model development. Finally, the novel perspective of
geographic information scientists is emphasized with a call for further
transparency, bias mitigation, and regionally-informed policies. Simply put,
this chapter aims to provide a conceptual starting point for researchers,
practitioners, and policy-makers to better understand trust in (generative)
GeoAI.

</details>


### [350] [The Cost-Benefit of Interdisciplinarity in AI for Mental Health](https://arxiv.org/abs/2510.18581)
*Katerina Drakos,Eva Paraschou,Simay Toplu,Line Harder Clemmensen,Christoph Lütge,Nicole Nadine Lønfeldt,Sneha Das*

Main category: cs.CY

TL;DR: 文章探讨AI心理健康聊天机器人跨学科协作的成本效益权衡，提出关键建议。


<details>
  <summary>Details</summary>
Motivation: 多数AI心理健康聊天机器人学科输入有限，且未在生命周期整合专业知识，需研究跨学科协作。

Method: 分析跨学科协作在AI心理健康聊天机器人关键生命周期阶段的情况。

Result: 确定让技术、医疗、伦理和法律专家参与关键阶段对确保价值一致和符合AI法案高风险要求至关重要。

Conclusion: 给出实际建议和现有框架以平衡心理健康聊天机器人跨学科的挑战与益处。

Abstract: Artificial intelligence has been introduced as a way to improve access to
mental health support. However, most AI mental health chatbots rely on a
limited range of disciplinary input, and fail to integrate expertise across the
chatbot's lifecycle. This paper examines the cost-benefit trade-off of
interdisciplinary collaboration in AI mental health chatbots. We argue that
involving experts from technology, healthcare, ethics, and law across key
lifecycle phases is essential to ensure value-alignment and compliance with the
high-risk requirements of the AI Act. We also highlight practical
recommendations and existing frameworks to help balance the challenges and
benefits of interdisciplinarity in mental health chatbots.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [351] [CPSLint: A Domain-Specific Language Providing Data Validation and Sanitisation for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2510.18651)
*Uraz Odyurt,Ömer Sayilir,Mariëlle Stoelinga,Vadim Zaytsev*

Main category: cs.PL

TL;DR: 提出用于工业CPS数据准备的领域特定语言CPSLint并展示其功能。


<details>
  <summary>Details</summary>
Motivation: 工业CPS原始数据集大且无结构，需数据准备，且缺乏通用解决方案。

Method: 设计CPSLint语言，具备类型检查、约束执行等功能，还能推断CPS特定数据结构。

Result: 通过概念验证实现展示了CPSLint的功能。

Conclusion: CPSLint可用于工业CPS的数据准备工作。

Abstract: Raw datasets are often too large and unstructured to work with directly, and
require a data preparation process. The domain of industrial Cyber-Physical
Systems (CPS) is no exception, as raw data typically consists of large amounts
of time-series data logging the system's status in regular time intervals. Such
data has to be sanity checked and preprocessed to be consumable by data-centric
workflows. We introduce CPSLint, a Domain-Specific Language designed to provide
data preparation for industrial CPS. We build up on the fact that many raw data
collections in the CPS domain require similar actions to render them suitable
for Machine-Learning (ML) solutions, e.g., Fault Detection and Identification
(FDI) workflows, yet still vary enough to hope for one universally applicable
solution.
  CPSLint's main features include type checking and enforcing constraints
through validation and remediation for data columns, such as imputing missing
data from surrounding rows. More advanced features cover inference of extra
CPS-specific data structures, both column-wise and row-wise. For instance, as
row-wise structures, descriptive execution phases are an effective method of
data compartmentalisation are extracted and prepared for ML-assisted FDI
workflows. We demonstrate CPSLint's features through a proof of concept
implementation.

</details>


### [352] [Hey Pentti, We Did It!: A Fully Vector-Symbolic Lisp](https://arxiv.org/abs/2510.17889)
*Eilene Tomkins-Flanagan,Mary A. Kelly*

Main category: cs.PL

TL;DR: 本文提出Lisp基本函数的向量符号表示形式，用全息约简表示实现，探讨向量符号架构笛卡尔封闭性等数学和意义。


<details>
  <summary>Details</summary>
Motivation: 受Kanerva启发，构建完整的基于向量符号架构的Lisp。

Method: 提出Lisp基本函数等的向量符号表示形式，使用全息约简表示和查找表清理内存实现。

Result: 给出了接近最小且满足图灵完备性的Lisp基本函数等的向量符号表示。

Conclusion: 讨论了向量符号架构笛卡尔封闭性的数学、目的和意义，以及清理内存的重要性。

Abstract: Kanerva (2014) suggested that it would be possible to construct a complete
Lisp out of a vector-symbolic architecture. We present the general form of a
vector-symbolic representation of the five Lisp elementary functions, lambda
expressions, and other auxiliary functions, found in the Lisp 1.5 specification
McCarthy (1960), which is near minimal and sufficient for Turing-completeness.
Our specific implementation uses holographic reduced representations Plate
(1995), with a lookup table cleanup memory. Lisp, as all Turing-complete
languages, is a Cartesian closed category, unusual in its proximity to the
mathematical abstraction. We discuss the mathematics, the purpose, and the
significance of demonstrating vector-symbolic architectures' Cartesian-closure,
as well as the importance of explicitly including cleanup memories in the
specification of the architecture.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [353] [Optimal allocations with distortion risk measures and mixed risk attitudes](https://arxiv.org/abs/2510.18236)
*Mario Ghossoub,Qinghua Ren,Ruodu Wang*

Main category: econ.TH

TL;DR: 研究异质风险态度经济体中帕累托最优风险分担，显示相似态度主体风险分担方式，将n主体问题简化为两主体问题，分析最优分配存在条件及无可行解情况。


<details>
  <summary>Details</summary>
Motivation: 研究异质风险态度经济体中帕累托最优风险分担情况。

Method: 基于共单调和反单调改进结果，将一般n主体问题简化为两主体问题，分析最优分配存在的充要条件。

Result: 相似态度主体最优风险分担方式，最优分配存在条件，无可行解时非负分配约束下的最优解。

Conclusion: 最优分配结构由风险厌恶与风险寻求行为的相对强度决定。

Abstract: We study Pareto-optimal risk sharing in economies with heterogeneous
attitudes toward risk, where agents' preferences are modeled by distortion risk
measures. Building on comonotonic and counter-monotonic improvement results, we
show that agents with similar attitudes optimally share risks comonotonically
(risk-averse) or counter-monotonically (risk-seeking). We show how the general
$n$-agent problem can be reduced to a two-agent formulation between
representative risk-averse and risk-seeking agents, characterized by the
infimal convolution of their distortion risk measures. Within this two-agent
framework, we establish necessary and sufficient conditions for the existence
of optimal allocations, and we identify when the infimal convolution yields an
unbounded value. When existence fails, we analyze the problem under nonnegative
allocation constraints, and we characterize optima explicitly, under
piecewise-linear distortion functions and Bernoulli-type risks. Our findings
suggest that the optimal allocation structure is governed by the relative
strength of risk aversion versus risk seeking behavior, as intuition would
suggest.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [354] [Universal Spectral Tokenization via Self-Supervised Panchromatic Representation Learning](https://arxiv.org/abs/2510.17959)
*Jeff Shen,Francois Lanusse,Liam Holden Parker,Ollie Liu,Tom Hehir,Leopoldo Sarra,Lucas Meyer,Micah Bowles,Sebastian Wagner-Carena,Sebastian Wagner-Carena,Helen Qu,Siavash Golkar,Alberto Bietti,Hatim Bourfoune,Nathan Cassereau,Pierre Cornette,Keiya Hirashima,Geraud Krawezik,Ruben Ohana,Nicholas Lourie,Michael McCabe,Rudy Morel,Payel Mukhopadhyay,Mariel Pettee,Bruno Régaldo-Saint Blancard,Kyunghyun Cho,Miles Cranmer,Shirley Ho*

Main category: astro-ph.IM

TL;DR: 提出深度学习模型以自监督方式学习异构光谱，统一不同分辨率和领域的光谱数据，可用于下游任务，有望成为天文学基础模型构建模块并拓展到其他领域。


<details>
  <summary>Details</summary>
Motivation: 顺序科学数据存在多分辨率和多领域问题，天文学光谱分析分散，限制数据整合，需统一表示以发展科学基础模型。

Method: 提出深度自监督学习模型，使用通用光谱分词器直接处理不同类型和分辨率的光谱，生成对齐、同质且有物理意义的表示。

Result: 单个模型首次实现跨分辨率和领域统一光谱数据，在下游任务表现有竞争力。

Conclusion: 该模型可作为天文学基础模型的有力构建模块，可能拓展到气候、医疗等有异构顺序数据的领域。

Abstract: Sequential scientific data span many resolutions and domains, and unifying
them into a common representation is a key step toward developing foundation
models for the sciences. Astronomical spectra exemplify this challenge: massive
surveys have collected millions of spectra across a wide range of wavelengths
and resolutions, yet analyses remain fragmented across spectral domains (e.g.,
optical vs. infrared) and object types (e.g., stars vs. galaxies), limiting the
ability to pool information across datasets. We present a deep learning model
that jointly learns from heterogeneous spectra in a self-supervised manner. Our
universal spectral tokenizer processes spectra from a variety of object types
and resolutions directly on their native wavelength grids, producing
intrinsically aligned, homogeneous, and physically meaningful representations
that can be efficiently adapted to achieve competitive performance across a
range of downstream tasks. For the first time, we demonstrate that a single
model can unify spectral data across resolutions and domains, suggesting that
our model can serve as a powerful building block for foundation models in
astronomy -- and potentially extend to other scientific domains with
heterogeneous sequential data, such as climate and healthcare.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [355] [Semi-analytical pricing of American options with hybrid dividends via integral equations and the GIT method](https://arxiv.org/abs/2510.18159)
*Andrey Itkin*

Main category: q-fin.PR

TL;DR: 本文介绍用广义积分变换（GIT）方法为支付离散和/或连续股息的美式期权定价，该方法准确且计算高效。


<details>
  <summary>Details</summary>
Motivation: 离散股息使美式期权定价问题复杂，传统连续股息模型不适用，需新方法。

Method: 利用广义积分变换（GIT）方法，将定价问题转化为积分Volterra方程，以处理股息导致的不连续性。

Result: 通过实例表明GIT方法高度准确且计算高效，无需大量计算网格或复杂反向归纳步骤。

Conclusion: GIT方法为美式期权定价提供了强大的替代方案，优于标准数值技术。

Abstract: This paper introduces a semi-analytical method for pricing American options
on assets (stocks, ETFs) that pay discrete and/or continuous dividends. The
problem is notoriously complex because discrete dividends create abrupt price
drops and affect the optimal exercise timing, making traditional
continuous-dividend models unsuitable. Our approach utilizes the Generalized
Integral Transform (GIT) method introduced by the author and his co-authors in
a number of papers, which transforms the pricing problem from a complex partial
differential equation with a free boundary into an integral Volterra equation
of the second or first kind. In this paper we illustrate this approach by
considering a popular GBM model that accounts for discrete cash and
proportional dividends using Dirac delta functions. By reframing the problem as
an integral equation, we can sequentially solve for the option price and the
early exercise boundary, effectively handling the discontinuities caused by the
dividends. Our methodology provides a powerful alternative to standard
numerical techniques like binomial trees or finite difference methods, which
can struggle with the jump conditions of discrete dividends by losing accuracy
or performance. Several examples demonstrate that the GIT method is highly
accurate and computationally efficient, bypassing the need for extensive
computational grids or complex backward induction steps.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [356] [Lyapunov-Aware Quantum-Inspired Reinforcement Learning for Continuous-Time Vehicle Control: A Feasibility Study](https://arxiv.org/abs/2510.18852)
*Nutkritta Kraipatthanapong,Natthaphat Thathong,Pannita Suksawas,Thanunnut Klunklin,Kritin Vongthonglua,Krit Attahakul,Aueaphum Aueawatthanaphisut*

Main category: quant-ph

TL;DR: 本文提出基于Lyapunov的量子强化学习（LQRL）框架用于连续时间车辆控制，结合量子策略优化与稳定性分析，仿真验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 为连续时间车辆控制提供可解释且有稳定性保障的控制方法，实现量子强化学习架构下的安全决策。

Method: 将车辆纵向控制问题建模为连续状态强化学习任务，结合变分量子电路和稳定性感知策略梯度机制，在稳定性反馈下训练量子启发策略。

Result: LQRL框架成功将Lyapunov稳定性验证嵌入量子策略学习，虽在激进加速时有暂态超调等，但系统状态演化有界。

Conclusion: 该框架为自主系统和混合量子 - 经典优化领域的安全量子控制奠定基础。

Abstract: This paper presents a novel Lyapunov-Based Quantum Reinforcement Learning
(LQRL) framework that integrates quantum policy optimization with Lyapunov
stability analysis for continuous-time vehicle control. The proposed approach
combines the representational power of variational quantum circuits (VQCs) with
a stability-aware policy gradient mechanism to ensure asymptotic convergence
and safe decision-making under dynamic environments. The vehicle longitudinal
control problem was formulated as a continuous-state reinforcement learning
task, where the quantum policy network generates control actions subject to
Lyapunov stability constraints. Simulation experiments were conducted in a
closed-loop adaptive cruise control scenario using a quantum-inspired policy
trained under stability feedback. The results demonstrate that the LQRL
framework successfully embeds Lyapunov stability verification into quantum
policy learning, enabling interpretable and stability-aware control
performance. Although transient overshoot and Lyapunov divergence were observed
under aggressive acceleration, the system maintained bounded state evolution,
validating the feasibility of integrating safety guarantees within quantum
reinforcement learning architectures. The proposed framework provides a
foundational step toward provably safe quantum control in autonomous systems
and hybrid quantum-classical optimization domains.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [357] [SPIKE: Stable Physics-Informed Kernel Evolution Method for Solving Hyperbolic Conservation Laws](https://arxiv.org/abs/2510.18266)
*Hua Su,Lei Zhang,Jin Zhao*

Main category: math.NA

TL;DR: 提出SPIKE方法用于无粘双曲守恒律数值计算，解决强形式残差最小化捕获含间断弱解的悖论，经数值验证有效


<details>
  <summary>Details</summary>
Motivation: 解决强形式残差最小化在捕获含间断弱解时的悖论，实现无粘双曲守恒律的有效数值计算

Method: 采用带正则化参数演化的再生核表示，利用Tikhonov正则化实现通过激波形成的平滑过渡机制

Result: 在标量和向量值守恒律的数值验证中证实了该方法的有效性

Conclusion: SPIKE方法能在统一框架下自动保持守恒、追踪特征并捕获满足Rankine - Hugoniot条件的激波，无需显式激波检测或人工粘性

Abstract: We introduce the Stable Physics-Informed Kernel Evolution (SPIKE) method for
numerical computation of inviscid hyperbolic conservation laws. SPIKE resolves
a fundamental paradox: how strong-form residual minimization can capture weak
solutions containing discontinuities. SPIKE employs reproducing kernel
representations with regularized parameter evolution, where Tikhonov
regularization provides a smooth transition mechanism through shock formation,
allowing the dynamics to traverse shock singularities. This approach
automatically maintains conservation, tracks characteristics, and captures
shocks satisfying Rankine-Hugoniot conditions within a unified framework
requiring no explicit shock detection or artificial viscosity. Numerical
validation across scalar and vector-valued conservation laws confirms the
method's effectiveness.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [358] [Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis](https://arxiv.org/abs/2510.17826)
*Carles Navarro,Mariona Torrens,Philipp Thölke,Stefan Doerr,Gianni De Fabritiis*

Main category: q-bio.BM

TL;DR: 介绍了名为'Speak to a Protein'的AI系统，可将蛋白质分析转变为与专家科学家的交互式多模态对话，能降低高级结构分析门槛并支持实时测试想法，该系统免费可用。


<details>
  <summary>Details</summary>
Motivation: 传统构建蛋白质心理模型的方式耗时久、获取不均衡且需专业计算技能，需改进蛋白质分析方法。

Method: 引入'Speak to a Protein'系统，该系统可检索和整合相关文献、结构和配体数据，以3D场景为答案依据，能进行高亮、注释、操作和可视化，必要时生成并运行代码。

Result: 在相关蛋白质上展示了系统能力，可实时测试关于结合口袋、构象变化或构效关系的想法。

Conclusion: 'Speak to a Protein'减少了从提问到获取证据的时间，降低了高级结构分析的门槛，能通过紧密耦合语言、代码和3D结构来生成假设。

Abstract: Building a working mental model of a protein typically requires weeks of
reading, cross-referencing crystal and predicted structures, and inspecting
ligand complexes, an effort that is slow, unevenly accessible, and often
requires specialized computational skills. We introduce \emph{Speak to a
Protein}, a new capability that turns protein analysis into an interactive,
multimodal dialogue with an expert co-scientist. The AI system retrieves and
synthesizes relevant literature, structures, and ligand data; grounds answers
in a live 3D scene; and can highlight, annotate, manipulate and see the
visualization. It also generates and runs code when needed, explaining results
in both text and graphics. We demonstrate these capabilities on relevant
proteins, posing questions about binding pockets, conformational changes, or
structure-activity relationships to test ideas in real-time. \emph{Speak to a
Protein} reduces the time from question to evidence, lowers the barrier to
advanced structural analysis, and enables hypothesis generation by tightly
coupling language, code, and 3D structures. \emph{Speak to a Protein} is freely
accessible at https://open.playmolecule.org.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [359] [Three-dimensional inversion of gravity data using implicit neural representations](https://arxiv.org/abs/2510.17876)
*Pankaj K Mishra,Sanni Laaksonen,Jochen Kamm,Anand Singh*

Main category: physics.geo-ph

TL;DR: 提出基于隐式神经表示的三维重力反演机器学习方法，减少反演参数，有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 重力数据反演对研究地下密度变化很重要，传统方法有局限，需新方法。

Method: 用隐式神经表示将地下密度表示为连续场，通过基于物理的正演模型损失直接训练深度神经网络，用位置编码增强网络能力。

Result: 在合成示例上重建详细结构和地质合理边界，显著减少反演参数。

Conclusion: 隐式表示有潜力实现可扩展、灵活和可解释的大规模地球物理反演，可推广到其他地球物理方法和联合/多物理反演。

Abstract: Inversion of gravity data is an important method for investigating subsurface
density variations relevant to diverse applications including mineral
exploration, geothermal assessment, carbon storage, natural hydrogen,
groundwater resources, and tectonic evolution. Here we present a scientific
machine-learning approach for three-dimensional gravity inversion that
represents subsurface density as a continuous field using an implicit neural
representation (INR). The method trains a deep neural network directly through
a physics-based forward-model loss, mapping spatial coordinates to a continuous
density field without predefined meshes or discretisation. Positional encoding
enhances the network's capacity to capture sharp contrasts and short-wavelength
features that conventional coordinate-based networks tend to oversmooth due to
spectral bias. We demonstrate the approach on synthetic examples including
Gaussian random fields, representing realistic geological complexity, and a
dipping block model to assess recovery of blocky structures. The INR framework
reconstructs detailed structure and geologically plausible boundaries without
explicit regularisation or depth weighting, while significantly reducing the
number of inversion parameters. These results highlight the potential of
implicit representations to enable scalable, flexible, and interpretable
large-scale geophysical inversion. This framework could generalise to other
geophysical methods and for joint/multiphysics inversion.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [360] [Distributional regression for seasonal data: an application to river flows](https://arxiv.org/abs/2510.18639)
*Samuel Perreault,Silvana M. Pesenti,Daniyal Shahzad*

Main category: stat.AP

TL;DR: 提出估算环境变量全分布的建模框架，应用于加拿大弗雷泽河流量数据并分析2021年初冬洪水。


<details>
  <summary>Details</summary>
Motivation: 传统伤亡保险风险评估的极值方法无法捕捉环境变量更广泛动态，需补充方法。

Method: 提出估算环境变量全分布的建模框架，借鉴GAMLSS，忽略季节性变化中的时间依赖。

Result: 将框架应用于加拿大弗雷泽河三个水文站的每日流量数据，分析了2021年初冬洪水。

Conclusion: 未明确提及，但提出的框架或能更好地评估风险，捕捉环境变量动态。

Abstract: Risk assessment in casualty insurance, such as flood risk, traditionally
relies on extreme-value methods that emphasizes rare events. These approaches
are well-suited for characterizing tail risk, but do not capture the broader
dynamics of environmental variables such as moderate or frequent loss events.
To complement these methods, we propose a modelling framework for estimating
the full (daily) distribution of environmental variables as a function of time,
that is a distributional version of typical climatological summary statistics,
thereby incorporating both seasonal variation and gradual long-term changes.
Aside from the time trend, to capture seasonal variation our approach
simultaneously estimates the distribution for each instant of the seasonal
cycle, without explicitly modelling the temporal dependence present in the
data. To do so, we adopt a framework inspired by GAMLSS (Generalized Additive
Models for Location, Scale, and Shape), where the parameters of the
distribution vary over the seasonal cycle as a function of explanatory
variables depending only on the time of year, and not on the past values of the
process under study. Ignoring the temporal dependence in the seasonal variation
greatly simplifies the modelling but poses inference challenges that we clarify
and overcome.
  We apply our framework to daily river flow data from three hydrometric
stations along the Fraser River in British Columbia, Canada, and analyse the
flood of the Fraser River in early winter of 2021.

</details>


### [361] [Finding the Sweet Spot: Optimal Data Augmentation Ratio for Imbalanced Credit Scoring Using ADASYN](https://arxiv.org/abs/2510.18252)
*Luis H. Chia*

Main category: stat.AP

TL;DR: 研究评估不同数据增强场景对信用评分模型的影响，发现ADASYN 1x增强效果最佳，最佳不平衡比为6.6:1，为业界提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 信用评分模型存在严重类别不平衡问题，现有合成数据增强技术的最佳增强比例不明确。

Method: 使用Give Me Some Credit数据集，对比SMOTE、BorderlineSMOTE和ADASYN在不同倍增因子下的效果，用XGBoost训练模型，用Bootstrap测试评估统计显著性。

Result: ADASYN 1x增强性能最优，AUC为0.6778，基尼系数为0.3557；更高倍增因子导致性能下降；最佳类别不平衡比为6.6:1。

Conclusion: 为信用评分数据增强找到最佳“平衡点”，为从业者和研究者提供实用指南，方法可用于其他不平衡领域。

Abstract: Credit scoring models face a critical challenge: severe class imbalance, with
default rates typically below 10%, which hampers model learning and predictive
performance. While synthetic data augmentation techniques such as SMOTE and
ADASYN have been proposed to address this issue, the optimal augmentation ratio
remains unclear, with practitioners often defaulting to full balancing (1:1
ratio) without empirical justification.
  This study systematically evaluates 10 data augmentation scenarios using the
Give Me Some Credit dataset (97,243 observations, 7% default rate), comparing
SMOTE, BorderlineSMOTE, and ADASYN at different multiplication factors (1x, 2x,
3x). All models were trained using XGBoost and evaluated on a held-out test set
of 29,173 real observations. Statistical significance was assessed using
bootstrap testing with 1,000 iterations.
  Key findings reveal that ADASYN with 1x multiplication (doubling the minority
class) achieved optimal performance with AUC of 0.6778 and Gini coefficient of
0.3557, representing statistically significant improvements of +0.77% and
+3.00% respectively (p = 0.017, bootstrap test). Higher multiplication factors
(2x and 3x) resulted in performance degradation, with 3x showing a -0.48%
decrease in AUC, suggesting a "law of diminishing returns" for synthetic
oversampling. The optimal class imbalance ratio was found to be 6.6:1
(majority:minority), contradicting the common practice of balancing to 1:1.
  This work provides the first empirical evidence of an optimal "sweet spot"
for data augmentation in credit scoring, with practical guidelines for industry
practitioners and researchers working with imbalanced datasets. While
demonstrated on a single representative dataset, the methodology provides a
reproducible framework for determining optimal augmentation ratios in other
imbalanced domains.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [362] [A machine learning approach to automation and uncertainty evaluation for self-validating thermocouples](https://arxiv.org/abs/2510.18411)
*Samuel Bilson,Andrew Thompson,Declan Tucker,Jonathan Pearce*

Main category: physics.ins-det

TL;DR: 本文提出用机器学习方法识别热电偶自校准中的熔化平台，去除人工干预，测试数据显示检测准确率100%，校准漂移预测R2为0.99。


<details>
  <summary>Details</summary>
Motivation: 解决热电偶在恶劣环境中校准漂移问题，且现有人工识别熔化平台的方法需手动干预。

Method: 提出一种新颖的机器学习方法来识别和确定熔化平台特征形状及熔化起始点和不确定性。

Result: CCPI Europe提供的测试数据显示熔化平台检测准确率100%，校准漂移预测交叉验证R2为0.99。

Conclusion: 该机器学习方法可有效识别熔化平台，去除人工干预进行热电偶校准。

Abstract: Thermocouples are in widespread use in industry, but they are particularly
susceptible to calibration drift in harsh environments. Self-validating
thermocouples aim to address this issue by using a miniature phase-change cell
(fixed-point) in close proximity to the measurement junction (tip) of the
thermocouple. The fixed point is a crucible containing an ingot of metal with a
known melting temperature. When the process temperature being monitored passes
through the melting temperature of the ingot, the thermocouple output exhibits
a "plateau" during melting. Since the melting temperature of the ingot is
known, the thermocouple can be recalibrated in situ. Identifying the melting
plateau to determine the onset of melting is reasonably well established but
requires manual intervention involving zooming in on the region around the
actual melting temperature, a process which can depend on the shape of the
melting plateau. For the first time, we present a novel machine learning
approach to recognize and identify the characteristic shape of the melting
plateau and once identified, to quantity the point at which melting begins,
along with its associated uncertainty. This removes the need for human
intervention in locating and characterizing the melting point. Results from
test data provided by CCPI Europe show 100% accuracy of melting plateau
detection. They also show a cross-validated R2 of 0.99 on predictions of
calibration drift.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [363] [Carbon-Aware Orchestration of Integrated Satellite Aerial Terrestrial Networks via Digital Twin](https://arxiv.org/abs/2510.17825)
*Shumaila Javaid,Nasir Saeed*

Main category: eess.SP

TL;DR: 本文提出面向ISATNs的碳感知编排框架以降低能耗和碳排放，模拟结果显示有显著减排效果。


<details>
  <summary>Details</summary>
Motivation: ISATNs大规模部署存在不可持续能源使用和碳排放问题，需推进能源感知研究。

Method: 提出利用数字孪生技术的碳感知编排框架，采用gCO₂/bit作为可持续性指标，实施多时间尺度PDCA循环，利用特定控制旋钮减少排放。

Result: 模拟结果显示，相比仅考虑QoS的编排，gCO₂/bit最多降低29%，同时提高可再生能源利用率和应对不利事件的恢复能力。

Conclusion: 所提碳感知编排框架能有效降低ISATNs的碳排放，提高能源利用效率和系统韧性。

Abstract: Integrated Satellite Aerial Terrestrial Networks (ISATNs) are envisioned as
key enablers of 6G, providing global connectivity for applications such as
autonomous transportation, Industrial IoT, and disaster response. Their
large-scale deployment, however, risks unsustainable energy use and carbon
emissions. This work advances prior energy-aware studies by proposing a
carbon-aware orchestration framework for ISATNs that leverages Digital Twin
(DT) technology. The framework adopts grams of CO$_2$-equivalent per bit
(gCO$_2$/bit) as a primary sustainability metric and implements a multi
timescale Plan Do Check Act (PDCA) loop that combines day-ahead forecasting
with real-time adaptive optimization. ISATN-specific control knobs, including
carbon-aware handovers, UAV duty cycling, and renewable-aware edge placement,
are exploited to reduce emissions. Simulation results with real carbon
intensity data show up to 29\% lower gCO$_2$/bit than QoS-only orchestration,
while improving renewable utilization and resilience under adverse events.

</details>


### [364] [Synthetic EEG Generation using Diffusion Models for Motor Imagery Tasks](https://arxiv.org/abs/2510.17832)
*Henrique de Lima Alexandre,Clodoaldo Aparecido de Moraes Lima*

Main category: eess.SP

TL;DR: 本文提出用扩散概率模型生成运动想象脑任务相关的合成脑电图信号，经评估其在分类任务中表现良好，能补充数据集、提升基于脑电图的脑机接口性能。


<details>
  <summary>Details</summary>
Motivation: 收集高质量脑电图数据因传感器成本、采集时间和个体差异等存在挑战，需要解决数据稀缺问题。

Method: 对真实脑电图数据预处理，训练扩散模型从噪声中重建脑电图通道，用信号级和任务级指标评估生成信号质量，并用KNN、CNN和U - Net等分类器比较合成数据和真实数据在分类任务中的表现。

Result: 生成的数据分类准确率超95%，均方误差低，与真实信号相关性高。

Conclusion: 扩散模型生成的合成脑电图信号可有效补充数据集，提高基于脑电图的脑机接口分类性能，解决数据稀缺问题。

Abstract: Electroencephalography (EEG) is a widely used, non-invasive method for
capturing brain activity, and is particularly relevant for applications in
Brain-Computer Interfaces (BCI). However, collecting high-quality EEG data
remains a major challenge due to sensor costs, acquisition time, and
inter-subject variability. To address these limitations, this study proposes a
methodology for generating synthetic EEG signals associated with motor imagery
brain tasks using Diffusion Probabilistic Models (DDPM). The approach involves
preprocessing real EEG data, training a diffusion model to reconstruct EEG
channels from noise, and evaluating the quality of the generated signals
through both signal-level and task-level metrics. For validation, we employed
classifiers such as K-Nearest Neighbors (KNN), Convolutional Neural Networks
(CNN), and U-Net to compare the performance of synthetic data against real data
in classification tasks. The generated data achieved classification accuracies
above 95%, with low mean squared error and high correlation with real signals.
  Our results demonstrate that synthetic EEG signals produced by diffusion
models can effectively complement datasets, improving classification
performance in EEG-based BCIs and addressing data scarcity.

</details>


### [365] [In-Process Monitoring of Gear Power Honing Using Vibration Signal Analysis and Machine Learning](https://arxiv.org/abs/2510.17809)
*Massimo Capurso,Luciano Afferrante*

Main category: eess.SP

TL;DR: 提出基于振动信号分析和机器学习的齿轮强力珩磨过程监测框架，取得高分类准确率，可用于实时监测和预测性维护。


<details>
  <summary>Details</summary>
Motivation: 传统质量控制策略无法捕捉瞬态加工异常和实时检测缺陷，现代齿轮制造对NVH要求高。

Method: 通过加速度计连续采集数据，进行时频信号分析，比较三种子空间学习方法提取特征，用SVM分类器预测齿轮质量类别。

Result: 在工业环境中实现高达100%的分类准确率。

Conclusion: 该方法提供可解释的光谱特征，能集成到实时监测和预测性维护系统。

Abstract: In modern gear manufacturing, stringent Noise, Vibration, and Harshness (NVH)
requirements demand high-precision finishing operations such as power honing.
Conventional quality control strategies rely on post-process inspections and
Statistical Process Control (SPC), which fail to capture transient machining
anomalies and cannot ensure real-time defect detection. This study proposes a
novel, data-driven framework for in-process monitoring of gear power honing
using vibration signal analysis and machine learning. Our proposed methodology
involves continuous data acquisition via accelerometers, followed by
time-frequency signal analysis. We investigate and compare the efficacy of
three subspace learning methods for features extraction: (1) Principal
Component Analysis (PCA) for dimensionality reduction; (2) a two-stage
framework combining PCA with Linear Discriminant Analysis (LDA) for enhanced
class separation; and (3) Uncorrelated Multilinear Discriminant Analysis with
Regularization (R-UMLDA), adapted for tensor data, which enforces feature
decorrelation and includes regularization for small sample sizes. These
extracted features are then fed into a Support Vector Machine (SVM) classifier
to predict four distinct gear quality categories, established through rigorous
geometrical inspections and test bench results of assembled gearboxes. The
models are trained and validated on an experimental dataset collected in an
industrial context during gear power-honing operations, with gears classified
into four different quality categories. The proposed framework achieves high
classification accuracy (up to 100%) in an industrial setting. The approach
offers interpretable spectral features that correlate with process dynamics,
enabling practical integration into real-time monitoring and predictive
maintenance systems.

</details>


### [366] [Exploring Complexity Changes in Diseased ECG Signals for Enhanced Classification](https://arxiv.org/abs/2510.17810)
*Camilo Quiceno Quintero,Sandip Varkey George*

Main category: eess.SP

TL;DR: 研究运用非线性时间序列分析，基于PTB - XL数据集，从心电图提取指标，发现健康与患病个体指标有差异，将复杂度量化指标用于机器学习模型提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 理解心电图复杂性如何随心脏病理变化而变化。

Method: 使用非线性时间序列分析，从PTB - XL数据集的II导联心电图提取非线性指标，并通过Spearman相关性和互信息计算跨通道指标，还将复杂度量化指标用于机器学习模型。

Result: 健康与患病个体、5个诊断超类之间几乎所有指标存在显著差异；将复杂度量化指标加入机器学习模型后，ROC曲线下面积（AUC）从0.86提升到0.87（非线性指标）和0.90（含跨时间序列指标）。

Conclusion: 非线性时间序列分析提取的心电图复杂度量化指标有助于区分健康与患病个体，且能提升机器学习模型的心脏疾病分类准确率。

Abstract: The complex dynamics of the heart are reflected in its electrical activity,
captured through electrocardiograms (ECGs). In this study we use nonlinear time
series analysis to understand how ECG complexity varies with cardiac pathology.
Using the large PTB-XL dataset, we extracted nonlinear measures from lead II
ECGs, and cross-channel metrics (leads II, V2, AVL) using Spearman correlations
and mutual information. Significant differences between diseased and healthy
individuals were found in almost all measures between healthy and diseased
classes, and between 5 diagnostic superclasses ($p<.001$). Moreover,
incorporating these complexity quantifiers into machine learning models
substantially improved classification accuracy measured using area under the
ROC curve (AUC) from 0.86 (baseline) to 0.87 (nonlinear measures) and 0.90
(including cross-time series metrics).

</details>


### [367] [Single-Snapshot Gridless 2D-DoA Estimation for UCAs: A Joint Optimization Approach](https://arxiv.org/abs/2510.17818)
*Salar Nouri*

Main category: eess.SP

TL;DR: 本文提出新框架解决均匀圆阵单快拍二维波达方向估计难题，用iALM方法求解，仿真验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统无网格方法在单快拍场景下因计算成本高或缺乏鲁棒性而失效，需新方法解决该问题。

Method: 提出联合估计流形变换矩阵和源方位 - 仰角对的新框架，用不精确增广拉格朗日方法（iALM）求解优化问题。

Result: 仿真结果表明所提iALM框架能提供鲁棒、高分辨率的无网格二维波达方向估计。

Conclusion: 所提框架对具有挑战性的阵列信号处理应用有效。

Abstract: This paper tackles the challenging problem of gridless two-dimensional (2D)
direction-of-arrival (DOA) estimation for a uniform circular array (UCA) from a
single snapshot of data. Conventional gridless methods often fail in this
scenario due to prohibitive computational costs or a lack of robustness. We
propose a novel framework that overcomes these limitations by jointly
estimating a manifold transformation matrix and the source azimuth-elevation
pairs within a single, unified optimization problem. This problem is solved
efficiently using an inexact Augmented Lagrangian Method (iALM), which
completely circumvents the need for semidefinite programming. By unifying the
objectives of data fidelity and transformation robustness, our approach is
uniquely suited for the demanding single-snapshot case. Simulation results
confirm that the proposed iALM framework provides robust and high-resolution,
gridless 2D-DOA estimates, establishing its efficacy for challenging array
signal processing applications.

</details>


### [368] [CLARAE: Clarity Preserving Reconstruction AutoEncoder for Denoising and Rhythm Classification of Intracardiac Electrograms](https://arxiv.org/abs/2510.17821)
*Long Lin,Pablo Peiro-Corbacho,Pablo Ávila,Alejandro Carta-Bergaz,Ángel Arenal,Gonzalo R. Ríos-Muñoz,Carlos Sevilla-Salcedo*

Main category: eess.SP

TL;DR: 介绍心房腔内电图处理模型CLARAE，评估其性能并提供网络应用，为临床工作流提供基础。


<details>
  <summary>Details</summary>
Motivation: 腔内心房电图存在噪声且为高维数据，限制实时分析，需有效处理方法。

Method: 设计一维编码器 - 解码器CLARAE，采用下采样、混合插值 - 卷积上采样路径和有界潜在空间三个原则；在多种心律类型的大量心电图片段上评估，与六种先进自编码器对比。

Result: 下游心律分类F1分数超0.97，潜在空间按心律清晰聚类；去噪任务表现出色；提供交互式网络应用。

Conclusion: CLARAE结合强大去噪和紧凑判别表示，为临床工作流提供实用基础。

Abstract: Intracavitary atrial electrograms (EGMs) provide high-resolution insights
into cardiac electrophysiology but are often contaminated by noise and remain
high-dimensional, limiting real-time analysis. We introduce CLARAE
(CLArity-preserving Reconstruction AutoEncoder), a one-dimensional
encoder--decoder designed for atrial EGMs, which achieves both high-fidelity
reconstruction and a compact 64-dimensional latent representation. CLARAE is
designed to preserve waveform morphology, mitigate reconstruction artifacts,
and produce interpretable embeddings through three principles: downsampling
with pooling, a hybrid interpolation--convolution upsampling path, and a
bounded latent space.
  We evaluated CLARAE on 495,731 EGM segments (unipolar and bipolar) from 29
patients across three rhythm types (AF, SR300, SR600). Performance was
benchmarked against six state-of-the-art autoencoders using reconstruction
metrics, rhythm classification, and robustness across signal-to-noise ratios
from -5 to 15 dB. In downstream rhythm classification, CLARAE achieved
F1-scores above 0.97 for all rhythm types, and its latent space showed clear
clustering by rhythm. In denoising tasks, it consistently ranked among the top
performers for both unipolar and bipolar signals.
  In order to promote reproducibility and enhance accessibility, we offer an
interactive web-based application. This platform enables users to explore
pre-trained CLARAE models, visualize the reconstructions, and compute metrics
in real time. Overall, CLARAE combines robust denoising with compact,
discriminative representations, offering a practical foundation for clinical
workflows such as rhythm discrimination, signal quality assessment, and
real-time mapping.

</details>


### [369] [Covariance Matrix Construction with Preprocessing-Based Spatial Sampling for Robust Adaptive Beamforming](https://arxiv.org/abs/2510.17823)
*Saeed Mohammadzadeh,Rodrigo C. de Lamare,Yuriy Zakharov*

Main category: eess.SP

TL;DR: 提出高效鲁棒自适应波束形成技术处理转向矢量估计失配和数据协方差矩阵重建问题，经模拟验证比现有方法有效。


<details>
  <summary>Details</summary>
Motivation: 处理转向矢量（SV）估计失配和数据协方差矩阵重建问题。

Method: 用可用快照估计干扰源的到达方向，自适应计算干扰信号的角度扇区；利用通用线性组合算法结合基于预处理的空间采样（PPBSS）重建干扰加噪声协方差矩阵；用样本协方差矩阵替代收缩法中的预处理矩阵；基于估计的角度扇区信息计算的预处理矩阵设计功率谱采样策略；为感兴趣信号的角度扇区形成信号协方差矩阵，用幂法计算感兴趣信号的转向矢量；分析PPBSS技术的阵列波束图和竞争方法的计算成本。

Result: 模拟结果显示该方法比现有方法更有效。

Conclusion: 所提出的自适应波束形成技术在处理相关问题上表现良好，优于现有方法。

Abstract: This work proposes an efficient, robust adaptive beamforming technique to
deal with steering vector (SV) estimation mismatches and data covariance matrix
reconstruction problems. In particular, the direction-of-arrival(DoA) of
interfering sources is estimated with available snapshots in which the angular
sectors of the interfering signals are computed adaptively. Then, we utilize
the well-known general linear combination algorithm to reconstruct the
interference-plus-noise covariance (IPNC) matrix using preprocessing-based
spatial sampling (PPBSS). We demonstrate that the preprocessing matrix can be
replaced by the sample covariance matrix (SCM) in the shrinkage method. A power
spectrum sampling strategy is then devised based on a preprocessing matrix
computed with the estimated angular sectors' information. Moreover, the
covariance matrix for the signal is formed for the angular sector of the
signal-of-interest (SOI), which allows for calculating an SV for the SOI using
the power method. An analysis of the array beampattern in the proposed PPBSS
technique is carried out, and a study of the computational cost of competing
approaches is conducted. Simulation results show the proposed method's
effectiveness compared to existing approaches.

</details>


### [370] [Channel-Aware Vector Quantization for Robust Semantic Communication on Discrete Channels](https://arxiv.org/abs/2510.18604)
*Zian Meng,Qiang Li,Wenqian Tang,Mingdie Yan,Xiaohu Ge*

Main category: eess.SP

TL;DR: 提出基于离散无记忆信道的VQJSCC框架及CAVQ算法，实验表明其能缓解数字悬崖效应，在鲁棒性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的语义通信多采用模拟或半数字传输，与现代数字通信基础设施兼容性有限，且现有VQ方法在码本优化时忽略信道状态信息，鲁棒性不佳。

Method: 在联合信源 - 信道编码（JSCC）框架下提出信道感知矢量量化（CAVQ）算法，将语义特征离散化并直接映射到调制星座符号，CAVQ在量化过程中融入信道转移概率，还引入多码本对齐机制处理码本阶数和调制阶数不匹配问题。

Result: VQJSCC有效缓解数字悬崖效应，在各种调制方案下实现了更优的重建质量。

Conclusion: VQJSCC在鲁棒性和效率方面优于现有数字语义通信基线方法。

Abstract: Deep learning-based semantic communication has largely relied on analog or
semi-digital transmission, which limits compatibility with modern digital
communication infrastructures. Recent studies have employed vector quantization
(VQ) to enable discrete semantic transmission, yet existing methods neglect
channel state information during codebook optimization, leading to suboptimal
robustness. To bridge this gap, we propose a channel-aware vector quantization
(CAVQ) algorithm within a joint source-channel coding (JSCC) framework, termed
VQJSCC, established on a discrete memoryless channel. In this framework,
semantic features are discretized and directly mapped to modulation
constellation symbols, while CAVQ integrates channel transition probabilities
into the quantization process, aligning easily confused symbols with
semantically similar codewords. A multi-codebook alignment mechanism is further
introduced to handle mismatches between codebook order and modulation order by
decomposing the transmission stream into multiple independently optimized
subchannels. Experimental results demonstrate that VQJSCC effectively mitigates
the digital cliff effect, achieves superior reconstruction quality across
various modulation schemes, and outperforms state-of-the-art digital semantic
communication baselines in both robustness and efficiency.

</details>


### [371] [Analyse comparative d'algorithmes de restauration en architecture dépliée pour des signaux chromatographiques parcimonieux](https://arxiv.org/abs/2510.18760)
*Mouna Gharbi,Silvia Villa,Emilie Chouzenoux,Jean-Christophe Pesquet,Laurent Duval*

Main category: eess.SP

TL;DR: 对参数化色谱信号数据库上的三种架构进行比较研究，突出其在数据恢复方面的性能。


<details>
  <summary>Details</summary>
Motivation: 数据恢复是活跃研究领域，传统迭代优化方法与深度学习技术结合，展开对相关架构的研究。

Method: 在参数化色谱信号数据库上对三种架构进行比较研究，采用适应物理化学峰信号表征的指标。

Result: 突出了这些方法在数据恢复上的性能。

Conclusion: 未明确提及，推测三种架构在数据恢复中有一定优势。

Abstract: Data restoration from degraded observations, of sparsity hypotheses, is an
active field of study. Traditional iterative optimization methods are now
complemented by deep learning techniques. The development of unfolded methods
benefits from both families. We carry out a comparative study of three
architectures on parameterized chromatographic signal databases, highlighting
the performance of these approaches, especially when employing metrics adapted
to physico-chemical peak signal characterization.

</details>


### [372] [SO(3)-invariant PCA with application to molecular data](https://arxiv.org/abs/2510.18827)
*Michael Fraiman,Paulina Hoyos,Tamir Bendory,Joe Kileel,Oscar Mickelin,Nir Sharon,Amit Singer*

Main category: eess.SP

TL;DR: 提出SO(3)不变PCA框架，扩展PCA到未知方向的3D体积数据集，降低计算复杂度并在真实数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统PCA应用于任意方向的三维数据有挑战，朴素方法计算成本高。

Method: 开发SO(3)不变PCA的高效框架，利用代数结构减少计算复杂度。

Result: 计算复杂度大幅降低，在真实分子数据集验证了方法有效性。

Conclusion: 该方法为大规模、高维重建问题带来新可能。

Abstract: Principal component analysis (PCA) is a fundamental technique for
dimensionality reduction and denoising; however, its application to
three-dimensional data with arbitrary orientations -- common in structural
biology -- presents significant challenges. A naive approach requires
augmenting the dataset with many rotated copies of each sample, incurring
prohibitive computational costs. In this paper, we extend PCA to 3D volumetric
datasets with unknown orientations by developing an efficient and principled
framework for SO(3)-invariant PCA that implicitly accounts for all rotations
without explicit data augmentation. By exploiting underlying algebraic
structure, we demonstrate that the computation involves only the square root of
the total number of covariance entries, resulting in a substantial reduction in
complexity. We validate the method on real-world molecular datasets,
demonstrating its effectiveness and opening up new possibilities for
large-scale, high-dimensional reconstruction problems.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [373] [Intuitionistic $j$-Do-Calculus in Topos Causal Models](https://arxiv.org/abs/2510.17944)
*Sridhar Mahadevan*

Main category: cs.LO

TL;DR: 将Pearl的do - 演算推广到直觉主义设定的j - 稳定因果推理，介绍j - do - 演算并证明其合理性，后续论文将描述数据估计和实验结果。


<details>
  <summary>Details</summary>
Motivation: 将Pearl的do - 演算推广到直觉主义设定，在层的拓扑斯中进行j - 稳定因果推理。

Method: 利用Lawvere - Tierney拓扑推广Topos Causal Models，引入j - do - 演算，用Kripke - Joyal语义定义局部真理，形式化因果推理。

Result: j - do - 演算是合理的规则系统，给出三条推理规则并证明在Kripke - Joyal语义中的合理性。

Conclusion: 成功将do - 演算推广到直觉主义设定，后续将进行数据估计和实验验证。

Abstract: In this paper, we generalize Pearl's do-calculus to an Intuitionistic setting
called $j$-stable causal inference inside a topos of sheaves. Our framework is
an elaboration of the recently proposed framework of Topos Causal Models
(TCMs), where causal interventions are defined as subobjects. We generalize the
original setting of TCM using the Lawvere-Tierney topology on a topos, defined
by a modal operator $j$ on the subobject classifier $\Omega$. We introduce
$j$-do-calculus, where we replace global truth with local truth defined by
Kripke-Joyal semantics, and formalize causal reasoning as structure-preserving
morphisms that are stable along $j$-covers. $j$-do-calculus is a sound rule
system whose premises and conclusions are formulas of the internal
Intuitionistic logic of the causal topos. We define $j$-stability for
conditional independences and interventional claims as local truth in the
internal logic of the causal topos. We give three inference rules that mirror
Pearl's insertion/deletion and action/observation exchange, and we prove
soundness in the Kripke-Joyal semantics. A companion paper in preparation will
describe how to estimate the required entities from data and instantiate $j$-do
with standard discovery procedures (e.g., score-based and constraint-based
methods), and will include experimental results on how to (i) form data-driven
$j$-covers (via regime/section constructions), (ii) compute chartwise
conditional independences after graph surgeries, and (iii) glue them to certify
the premises of the $j$-do rules in practice

</details>


### [374] [Optimistic Higher-Order Superposition](https://arxiv.org/abs/2510.18429)
*Alexander Bentkamp,Jasmin Blanchette,Matthias Hetzenberger,Uwe Waldmann*

Main category: cs.LO

TL;DR: 提出乐观版λ-叠加演算解决原演算的扩展性问题，该演算可靠且完备，虽未实现但示例显示有优势。


<details>
  <summary>Details</summary>
Motivation: 原λ-叠加演算部分步骤扩展性过强，如高阶合一枚举和函数扩展性公理。

Method: 引入乐观版λ-叠加演算，用约束延迟扩展性合一问题，更有针对性地应用函数扩展性。

Result: 该演算在亨金语义下可靠且反驳完备。

Conclusion: 虽未实现，但示例表明新演算性能可能超越或补充原演算。

Abstract: The $\lambda$-superposition calculus is a successful approach to proving
higher-order formulas. However, some parts of the calculus are extremely
explosive, notably due to the higher-order unifier enumeration and the
functional extensionality axiom. In the present work, we introduce an
"optimistic" version of $\lambda$-superposition that addresses these two
issues. Specifically, our new calculus delays explosive unification problems
using constraints stored along with the clauses, and it applies functional
extensionality in a more targeted way. The calculus is sound and refutationally
complete with respect to a Henkin semantics. We have yet to implement it in a
prover, but examples suggest that it will outperform, or at least usefully
complement, the original $\lambda$-superposition calculus.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [375] [A Multi-Evidence Framework Rescues Low-Power Prognostic Signals and Rejects Statistical Artifacts in Cancer Genomics](https://arxiv.org/abs/2510.18571)
*Gokturk Aytug Akarlar*

Main category: q-bio.GN

TL;DR: 本文提出一种计算框架，用于分析癌症基因组学中样本量不足的队列，可区分信号真伪，优先考虑生物学可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准全基因组关联研究在样本量不足的队列中存在严重的功效限制，会产生假阴性和假阳性结果。

Method: 开发了一个五标准计算框架，将因果推断与正交生物学验证相结合。

Result: 标准Cox+FDR方法在FDR<0.05时未检测到基因；框架正确识别出假阳性RYR2，以及需验证的候选基因KMT2C；通过突变模式分析区分信号和伪影。

Conclusion: 该多证据方法为分析样本量不足的队列提供了模板，优先考虑生物学可解释性而非单纯的统计显著性。

Abstract: Motivation: Standard genome-wide association studies in cancer genomics rely
on statistical significance with multiple testing correction, but
systematically fail in underpowered cohorts. In TCGA breast cancer (n=967, 133
deaths), low event rates (13.8%) create severe power limitations, producing
false negatives for known drivers and false positives for large passenger
genes. Results: We developed a five-criteria computational framework
integrating causal inference (inverse probability weighting, doubly robust
estimation) with orthogonal biological validation (expression, mutation
patterns, literature evidence). Applied to TCGA-BRCA mortality analysis,
standard Cox+FDR detected zero genes at FDR<0.05, confirming complete failure
in underpowered settings. Our framework correctly identified RYR2 -- a cardiac
gene with no cancer function -- as a false positive despite nominal
significance (p=0.024), while identifying KMT2C as a complex candidate
requiring validation despite marginal significance (p=0.047, q=0.954). Power
analysis revealed median power of 15.1% across genes, with KMT2C achieving only
29.8% power (HR=1.55), explaining borderline statistical significance despite
strong biological evidence. The framework distinguished true signals from
artifacts through mutation pattern analysis: RYR2 showed 29.8% silent mutations
(passenger signature) with no hotspots, while KMT2C showed 6.7% silent
mutations with 31.4% truncating variants (driver signature). This
multi-evidence approach provides a template for analyzing underpowered cohorts,
prioritizing biological interpretability over purely statistical significance.
  Availability: All code and analysis pipelines available at
github.com/akarlaraytu/causal-inference-for-cancer-genomics

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [376] [The Picard-Lagrange Framework for Higher-Order Langevin Monte Carlo](https://arxiv.org/abs/2510.18242)
*Jaideep Mahajan,Kaihong Zhang,Feng Liang,Jingbo Liu*

Main category: math.ST

TL;DR: 本文提出基于K阶Langevin动力学的新采样算法，对目标分布证明了维度相关收敛性，查询复杂度优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有基于Langevin动力学的采样算法多为二阶和三阶，本文旨在扩展到更高阶。

Method: 通过Lagrange插值近似势函数诱导的漂移，并使用Picard迭代修正插值点的节点值来离散化K阶动力学。

Result: 对于具有光滑、强对数凹密度的目标，采样器在Wasserstein距离下达到ε精度所需的梯度评估次数为$\widetilde O(d^{\frac{K - 1}{2K - 3}}\varepsilon^{-\frac{2}{2K - 3}})$（K ≥ 3）。

Conclusion: 这是首个达到此查询复杂度的采样算法，且随着阶数K增加，速率提高，优于现有一到三阶方法。

Abstract: Sampling from log-concave distributions is a central problem in statistics
and machine learning. Prior work establishes theoretical guarantees for
Langevin Monte Carlo algorithm based on overdamped and underdamped Langevin
dynamics and, more recently, some third-order variants. In this paper, we
introduce a new sampling algorithm built on a general $K$th-order Langevin
dynamics, extending beyond second- and third-order methods. To discretize the
$K$th-order dynamics, we approximate the drift induced by the potential via
Lagrange interpolation and refine the node values at the interpolation points
using Picard-iteration corrections, yielding a flexible scheme that fully
utilizes the acceleration of higher-order Langevin dynamics. For targets with
smooth, strongly log-concave densities, we prove dimension-dependent
convergence in Wasserstein distance: the sampler achieves
$\varepsilon$-accuracy within $\widetilde
O(d^{\frac{K-1}{2K-3}}\varepsilon^{-\frac{2}{2K-3}})$ gradient evaluations for
$K \ge 3$. To our best knowledge, this is the first sampling algorithm
achieving such query complexity. The rate improves with the order $K$
increases, yielding better rates than existing first to third-order approaches.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [377] [Undirected Multicast Network Coding Gaps via Locally Decodable Codes](https://arxiv.org/abs/2510.18737)
*Mark Braverman,Zhongtian He*

Main category: cs.CC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The network coding problem asks whether data throughput in a network can be
increased using coding (compared to treating bits as commodities in a flow).
While it is well-known that a network coding advantage exists in directed
graphs, the situation in undirected graphs is much less understood -- in
particular, despite significant effort, it is not even known whether network
coding is helpful at all for unicast sessions.
  In this paper we study the multi-source multicast network coding problem in
undirected graphs. There are $k$ sources broadcasting each to a subset of nodes
in a graph of size $n$. The corresponding combinatorial problem is a version of
the Steiner tree packing problem, and the network coding question asks whether
the multicast coding rate exceeds the tree-packing rate.
  We give the first super-constant bound to this problem, demonstrating an
example with a coding advantage of $\Omega(\log k)$. In terms of graph size, we
obtain a lower bound of $2^{\tilde{\Omega}(\sqrt{\log \log n})}$. We also
obtain an upper bound of $O(\log n)$ on the gap.
  Our main technical contribution is a new reduction that converts
locally-decodable codes in the low-error regime into multicast coding
instances. This gives rise to a new family of explicitly constructed graphs,
which may have other applications.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [378] [Distributed Allocation and Resource Scheduling Algorithms Resilient to Link Failure](https://arxiv.org/abs/2510.18273)
*Mohammadreza Doostmohammadian,Sergio Pequito*

Main category: eess.SY

TL;DR: 提出新型弹性分布式资源分配（DRA）算法，解决网络通信问题，理论和仿真证明其可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统DRA方法需可靠通信，但现实网络存在链路故障、丢包和延迟等问题。

Method: 利用网络渗流理论，提出新型图论解决方案。

Result: 算法在异构时间延迟和大量链路故障下能收敛到最优解。

Conclusion: 该算法显著提升了实际网络环境中分布式资源分配的可靠性，对移动多智能体系统有重要价值。

Abstract: Distributed resource allocation (DRA) is fundamental to modern networked
systems, spanning applications from economic dispatch in smart grids to CPU
scheduling in data centers. Conventional DRA approaches require reliable
communication, yet real-world networks frequently suffer from link failures,
packet drops, and communication delays due to environmental conditions, network
congestion, and security threats.
  We introduce a novel resilient DRA algorithm that addresses these critical
challenges, and our main contributions are as follows: (1) guaranteed
constraint feasibility at all times, ensuring resource-demand balance even
during algorithm termination or network disruption; (2) robust convergence
despite sector-bound nonlinearities at nodes/links, accommodating practical
constraints like quantization and saturation; and (3) optimal performance under
merely uniformly-connected networks, eliminating the need for continuous
connectivity.
  Unlike existing approaches that require persistent network connectivity and
provide only asymptotic feasibility, our graph-theoretic solution leverages
network percolation theory to maintain performance during intermittent
disconnections. This makes it particularly valuable for mobile multi-agent
systems where nodes frequently move out of communication range. Theoretical
analysis and simulations demonstrate that our algorithm converges to optimal
solutions despite heterogeneous time delays and substantial link failures,
significantly advancing the reliability of distributed resource allocation in
practical network environments.

</details>


### [379] [LLM Assisted Alpha Fairness for 6 GHz WiFi and NR_U Coexistence: An Agentic Orchestrator for Throughput, Energy, and SLA](https://arxiv.org/abs/2510.17814)
*Qun Wang,Yingzhou Lu,Guiran Liu,Binrong Zhu,Yang Liu*

Main category: eess.SY

TL;DR: 提出分离策略与执行的智能控制器，利用LLM提出可解释旋钮，经优化器计算分配方案，在6GHz模拟器中提升能效且吞吐量有竞争力，发布代码便于复现。


<details>
  <summary>Details</summary>
Motivation: 在无授权6GHz频段，Wi-Fi和5G NR - U竞争相同信道，需联合权衡吞吐量、能源和服务水平目标，同时保证安全和可审计性。

Method: 设计分离策略与执行的代理控制器，在调度周期开始时总结遥测数据，调用LLM提出可解释旋钮，用确定性优化器计算α - 公平分配方案。

Result: 在6GHz模拟器中，LLM辅助策略持续提高能源效率，一个LLM降低35.3%总能耗，另一个获得最佳权衡，总比特数增加3.5%，比特/焦耳增加12.2%。

Conclusion: 透明的策略级LLM指导可安全改善无线共存。

Abstract: Unlicensed 6GHz is becoming a primary workhorse for high-capacity access,
with Wi-Fi and 5G NR-U competing for the same channels under listen-before-talk
(LBT) rules. Operating in this regime requires decisions that jointly trade
throughput, energy, and service-level objectives while remaining safe and
auditable. We present an agentic controller that separates {policy} from
{execution}. At the start of each scheduling epoch the agent summarizes
telemetry (per-channel busy and baseline LBT failure; per-user CQI, backlog,
latency, battery, priority, and power mode) and invokes a large language model
(LLM) to propose a small set of interpretable knobs: a fairness index \alpha,
per-channel duty-cycle caps for Wi-Fi/NR-U, and class weights. A deterministic
optimizer then enforces feasibility and computes an \alpha-fair allocation that
internalizes LBT losses and energy cost; malformed or unsafe policies are
clamped and fall back to a rule baseline. In a 6GHz simulator with two 160MHz
channels and mixed Wi-Fi/NR-U users, LLM-assisted policies consistently improve
energy efficiency while keeping throughput competitive with a strong rule
baseline. One LLM lowers total energy by 35.3% at modest throughput loss, and
another attains the best overall trade-off, finishing with higher total bits
(+3.5%) and higher bits/J (+12.2%) than the baseline. We release code,
per-epoch logs, and plotting utilities to reproduce all figures and numbers,
illustrating how transparent, policy-level LLM guidance can safely improve
wireless coexistence.

</details>


### [380] [DRL-Based Resource Allocation for Energy-Efficient IRS-Assisted UAV Spectrum Sharing Systems](https://arxiv.org/abs/2510.17877)
*Yiheng Wang*

Main category: eess.SY

TL;DR: 本文提出 IRS 辅助 UAV 频谱共享系统，用 DRL 方法优化以提升能效，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 为使 IRS 辅助 UAV 无线通信更节能、高效，提出新的频谱共享系统。

Method: 采用物理推进能量模型，开发基于演员 - 评论家框架的深度强化学习方法处理优化问题。

Result: 扩展实验表明，与多个基准方案相比，所提出的基于 DRL 的方法显著提高了能效。

Conclusion: 所提出的方法在有移动性的情况下具有有效性和鲁棒性。

Abstract: Intelligent reflecting surface (IRS) assisted unmanned aerial vehicle (UAV)
systems provide a new paradigm for reconfigurable and flexible wireless
communications. To enable more energy efficient and spectrum efficient IRS
assisted UAV wireless communications, this paper introduces a novel
IRS-assisted UAV enabled spectrum sharing system with orthogonal frequency
division multiplexing (OFDM). The goal is to maximize the energy efficiency
(EE) of the secondary network by jointly optimizing the beamforming, subcarrier
allocation, IRS phase shifts, and the UAV trajectory subject to practical
transmit power and passive reflection constraints as well as UAV physical
limitations. A physically grounded propulsion-energy model is adopted, with its
tight upper bound used to form a tractable EE lower bound for the spectrum
sharing system. To handle highly non convex, time coupled optimization problems
with a mixed continuous and discrete policy space, we develop a deep
reinforcement learning (DRL) approach based on the actor critic framework.
Extended experiments show the significant EE improvement of the proposed
DRL-based approach compared to several benchmark schemes, thus demonstrating
the effectiveness and robustness of the proposed approach with mobility.

</details>


### [381] [Mixed Monotonicity Reachability Analysis of Neural ODE: A Trade-Off Between Tightness and Efficiency](https://arxiv.org/abs/2510.17859)
*Abdelrahman Sayed Sayed,Pierre-Jean Meyer,Mohamed Ghazel*

Main category: eess.SY

TL;DR: 提出基于区间的可达性方法用于神经常微分方程（neural ODE）可达集计算，适合高维、实时和安全关键应用，通过两个数值示例说明。


<details>
  <summary>Details</summary>
Motivation: neural ODE验证因缺乏适配的可达性分析工具而具有挑战性。

Method: 提出基于区间的可达性方法，利用连续时间混合单调性技术，通过同胚性质利用初始集及其边界的几何结构，将neural ODE动力学嵌入混合单调系统，在TIRA中实现单步、增量和基于边界的方法。

Result: 与CORA的 zonotopes 和 NNV2.0 星集表示相比，该方法能提供合理且计算高效的过近似，以牺牲紧密度换取效率。

Conclusion: 将混合单调性应用于neural ODE可达性分析为轻量级形式分析铺平道路，为可扩展验证开辟新途径。

Abstract: Neural ordinary differential equations (neural ODE) are powerful
continuous-time machine learning models for depicting the behavior of complex
dynamical systems, but their verification remains challenging due to limited
reachability analysis tools adapted to them. We propose a novel interval-based
reachability method that leverages continuous-time mixed monotonicity
techniques for dynamical systems to compute an over-approximation for the
neural ODE reachable sets. By exploiting the geometric structure of full
initial sets and their boundaries via the homeomorphism property, our approach
ensures efficient bound propagation. By embedding neural ODE dynamics into a
mixed monotone system, our interval-based reachability approach, implemented in
TIRA with single-step, incremental, and boundary-based approaches, provides
sound and computationally efficient over-approximations compared with CORA's
zonotopes and NNV2.0 star set representations, while trading tightness for
efficiency. This trade-off makes our method particularly suited for
high-dimensional, real-time, and safety-critical applications. Applying mixed
monotonicity to neural ODE reachability analysis paves the way for lightweight
formal analysis by leveraging the symmetric structure of monotone embeddings
and the geometric simplicity of interval boxes, opening new avenues for
scalable verification aligned with the symmetry and geometry of neural
representations. This novel approach is illustrated on two numerical examples
of a spiral system and a fixed-point attractor system modeled as a neural ODE.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [382] [Differentially Private E-Values](https://arxiv.org/abs/2510.18654)
*Daniel Csillag,Diego Mesquita*

Main category: stat.ME

TL;DR: 提出将非隐私e值转换为差分隐私e值的通用框架，实验证明其有效性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 许多现实应用依赖敏感数据，非隐私e值可能泄露数据，需确保其安全发布。

Method: 开发新颖的有偏乘性噪声机制，将非隐私e值转换为差分隐私e值。

Result: 差分隐私e值具有强统计功效，渐近上与非隐私e值一样有效。

Conclusion: 所提方法在在线风险监测、私人医疗和共形e预测等实验中有效，有广泛适用性。

Abstract: E-values have gained prominence as flexible tools for statistical inference
and risk control, enabling anytime- and post-hoc-valid procedures under minimal
assumptions. However, many real-world applications fundamentally rely on
sensitive data, which can be leaked through e-values. To ensure their safe
release, we propose a general framework to transform non-private e-values into
differentially private ones. Towards this end, we develop a novel biased
multiplicative noise mechanism that ensures our e-values remain statistically
valid. We show that our differentially private e-values attain strong
statistical power, and are asymptotically as powerful as their non-private
counterparts. Experiments across online risk monitoring, private healthcare,
and conformal e-prediction demonstrate our approach's effectiveness and
illustrate its broad applicability.

</details>


### [383] [Inference on Local Variable Importance Measures for Heterogeneous Treatment Effects](https://arxiv.org/abs/2510.18843)
*Pawel Morzywolek,Peter B. Gilbert,Alex Luedtke*

Main category: stat.ME

TL;DR: 提出评估异质治疗效果变量重要性的推断框架，展示其在传染病预防策略中的应用。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，决策者不愿依赖黑箱治疗推荐算法，需要评估变量重要性。

Method: 基于半参数理论中函数值参数的最新进展，即使使用统计机器学习算法量化治疗效果异质性也有效。

Result: 未明确提及具体结果，仅展示了方法在传染病预防策略中的适用性。

Conclusion: 所提出的推断框架可用于评估异质治疗效果的变量重要性。

Abstract: We provide an inferential framework to assess variable importance for
heterogeneous treatment effects. This assessment is especially useful in
high-risk domains such as medicine, where decision makers hesitate to rely on
black-box treatment recommendation algorithms. The variable importance measures
we consider are local in that they may differ across individuals, while the
inference is global in that it tests whether a given variable is important for
any individual. Our approach builds on recent developments in semiparametric
theory for function-valued parameters, and is valid even when statistical
machine learning algorithms are employed to quantify treatment effect
heterogeneity. We demonstrate the applicability of our method to infectious
disease prevention strategies.

</details>
