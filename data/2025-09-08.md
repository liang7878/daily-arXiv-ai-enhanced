<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 9]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 8]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 4]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.CV](#cs.CV) [Total: 20]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CL](#cs.CL) [Total: 60]
- [cs.CC](#cs.CC) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.MA](#cs.MA) [Total: 2]
- [stat.AP](#stat.AP) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [math.CO](#math.CO) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [econ.GN](#econ.GN) [Total: 5]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.PL](#cs.PL) [Total: 3]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management](https://arxiv.org/abs/2509.04505)
*Somtochukwu Azie,Yiping Meng*

Main category: cs.AI

TL;DR: 研究评估大语言模型（LLMs）用于建筑项目管理（CPM）伦理决策的可行性与可靠性，发现其有不足，建议作为决策辅助工具。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs应用于CPM伦理敏感、高风险决策场景时的伦理可行性与可靠性。

Method: 采用混合研究方法，用新的伦理决策支持评估清单（EDSAC）对两个领先LLMs进行定量性能测试，对12位行业专家进行半结构化访谈并定性分析。

Result: LLMs在法律合规等结构化领域表现尚可，但在处理情境细微差别、确保问责和提供透明推理方面有显著不足，利益相关者对其自主伦理判断存疑。

Conclusion: LLMs目前更适合作为决策支持辅助工具，而非自主伦理主体，研究引入EDSAC框架并提供可行建议。

Abstract: The integration of Artificial Intelligence (AI) into construction project
management (CPM) is accelerating, with Large Language Models (LLMs) emerging as
accessible decision-support tools. This study aims to critically evaluate the
ethical viability and reliability of LLMs when applied to the ethically
sensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods
research design was employed, involving the quantitative performance testing of
two leading LLMs against twelve real-world ethical scenarios using a novel
Ethical Decision Support Assessment Checklist (EDSAC), and qualitative analysis
of semi-structured interviews with 12 industry experts to capture professional
perceptions. The findings reveal that while LLMs demonstrate adequate
performance in structured domains such as legal compliance, they exhibit
significant deficiencies in handling contextual nuance, ensuring
accountability, and providing transparent reasoning. Stakeholders expressed
considerable reservations regarding the autonomous use of AI for ethical
judgments, strongly advocating for robust human-in-the-loop oversight. To our
knowledge, this is one of the first studies to empirically test the ethical
reasoning of LLMs within the construction domain. It introduces the EDSAC
framework as a replicable methodology and provides actionable recommendations,
emphasising that LLMs are currently best positioned as decision-support aids
rather than autonomous ethical agents.

</details>


### [2] [Maestro: Joint Graph & Config Optimization for Reliable AI Agents](https://arxiv.org/abs/2509.04642)
*Wenxiao Wang,Priyatham Kattakinda,Soheil Feizi*

Main category: cs.AI

TL;DR: 介绍Maestro框架，可联合搜索图和配置优化大语言模型（LLM）代理，在多个基准测试中表现优异，能解决结构故障模式。


<details>
  <summary>Details</summary>
Motivation: 现有优化器在固定图结构下调整配置，未解决结构故障模式，需要更全面的优化方法。

Method: Maestro作为框架无关的整体优化器，联合搜索图和配置，利用轨迹的反思性文本反馈来优化。

Result: 在IFBench和HotpotQA基准测试中，Maestro平均超过领先的提示优化器，使用更少的滚动次数，在两个应用中也有显著提升。

Conclusion: 联合图和配置搜索能解决提示调整单独无法修复的结构故障模式。

Abstract: Building reliable LLM agents requires decisions at two levels: the graph
(which modules exist and how information flows) and the configuration of each
node (models, prompts, tools, control knobs). Most existing optimizers tune
configurations while holding the graph fixed, leaving structural failure modes
unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for
LLM agents that jointly searches over graphs and configurations to maximize
agent quality, subject to explicit rollout/token budgets. Beyond numeric
metrics, Maestro leverages reflective textual feedback from traces to
prioritize edits, improving sample efficiency and targeting specific failure
modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses
leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,
4.9%, and 4.86%, respectively; even when restricted to prompt-only
optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these
results with far fewer rollouts than GEPA. We further show large gains on two
applications (interviewer & RAG agents), highlighting that joint graph &
configuration search addresses structural failure modes that prompt tuning
alone cannot fix.

</details>


### [3] [Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization](https://arxiv.org/abs/2509.04646)
*Philippe J. Giabbanelli,Ameeta Agrawal*

Main category: cs.AI

TL;DR: 现有健康领域M&S模型因复杂难被利益相关者使用，LLMs现有方法不能满足多样需求，本文提出框架为健康模拟生成定制解释。


<details>
  <summary>Details</summary>
Motivation: 当前健康领域M&S模型因复杂难以被利益相关者利用，LLMs现有总结方式不能满足不同利益相关者需求，缺乏对其需求的系统理解。

Method: 采用混合方法设计，先引出不同健康利益相关者的解释需求和风格偏好，再优化LLMs生成定制输出的能力，最后通过多种指标评估以改进总结生成。

Result: 文中未明确提及具体结果。

Conclusion: 提出了识别利益相关者需求并指导LLMs为健康模拟生成定制解释的分步框架。

Abstract: Modeling & Simulation (M&S) approaches such as agent-based models hold
significant potential to support decision-making activities in health, with
recent examples including the adoption of vaccines, and a vast literature on
healthy eating behaviors and physical activity behaviors. These models are
potentially usable by different stakeholder groups, as they support
policy-makers to estimate the consequences of potential interventions and they
can guide individuals in making healthy choices in complex environments.
However, this potential may not be fully realized because of the models'
complexity, which makes them inaccessible to the stakeholders who could benefit
the most. While Large Language Models (LLMs) can translate simulation outputs
and the design of models into text, current approaches typically rely on
one-size-fits-all summaries that fail to reflect the varied informational needs
and stylistic preferences of clinicians, policymakers, patients, caregivers,
and health advocates. This limitation stems from a fundamental gap: we lack a
systematic understanding of what these stakeholders need from explanations and
how to tailor them accordingly. To address this gap, we present a step-by-step
framework to identify stakeholder needs and guide LLMs in generating tailored
explanations of health simulations. Our procedure uses a mixed-methods design
by first eliciting the explanation needs and stylistic preferences of diverse
health stakeholders, then optimizing the ability of LLMs to generate tailored
outputs (e.g., via controllable attribute tuning), and then evaluating through
a comprehensive range of metrics to further improve the tailored generation of
summaries.

</details>


### [4] [An Approach to Grounding AI Model Evaluations in Human-derived Criteria](https://arxiv.org/abs/2509.04676)
*Sasha Mitts*

Main category: cs.AI

TL;DR: 本文提出用人类评估标准增强现有AI基准测试，以提升模型行为可解释性与适用性，还给出框架和指导方针。


<details>
  <summary>Details</summary>
Motivation: 传统AI基准测试难以捕捉AI模型细微能力，需改进以提升模型行为可解释性和适用性。

Method: 基于Perception Test和OpenEQA基准，通过深度访谈和大规模调查确定关键认知技能。

Result: 参与者认为AI缺乏解释和共情技能，但对其表现期望高。

Conclusion: 强调以用户为中心的评估在AI发展中的重要性，为研究人员和从业者提供行动指南，提升现有基准测试实践并为未来AI评估进步奠定基础。

Abstract: In the rapidly evolving field of artificial intelligence (AI), traditional
benchmarks can fall short in attempting to capture the nuanced capabilities of
AI models. We focus on the case of physical world modeling and propose a novel
approach to augment existing benchmarks with human-derived evaluation criteria,
aiming to enhance the interpretability and applicability of model behaviors.
Grounding our study in the Perception Test and OpenEQA benchmarks, we conducted
in-depth interviews and large-scale surveys to identify key cognitive skills,
such as Prioritization, Memorizing, Discerning, and Contextualizing, that are
critical for both AI and human reasoning. Our findings reveal that participants
perceive AI as lacking in interpretive and empathetic skills yet hold high
expectations for AI performance. By integrating insights from our findings into
benchmark design, we offer a framework for developing more human-aligned means
of defining and measuring progress. This work underscores the importance of
user-centered evaluation in AI development, providing actionable guidelines for
researchers and practitioners aiming to align AI capabilities with human
cognitive processes. Our approach both enhances current benchmarking practices
and sets the stage for future advancements in AI model evaluation.

</details>


### [5] [Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning](https://arxiv.org/abs/2509.04731)
*Brennen Hill*

Main category: cs.AI

TL;DR: 论文指出语言、智能体和世界模型融合是AI前沿，当前世界模型发展是瓶颈，提出用大语言模型动态生成层次化框架构建世界模型训练智能体。


<details>
  <summary>Details</summary>
Motivation: 语言、智能体和世界模型融合是AI关键前沿，但世界模型发展是复杂多智能体任务的瓶颈，标准强化学习在复杂场景易失败。

Method: 通过分层脚手架将复杂目标分解为子目标，结合符号和分层方法与多智能体强化学习，用大语言模型动态生成层次化框架。

Result: 从2024年多智能体足球研究中发现符号和分层方法与多智能体强化学习结合的趋势。

Conclusion: 构建具有显式、语言可配置任务层的环境能缩小低级反应行为和高级战略团队合作的差距，为训练下一代智能体提供强大通用框架。

Abstract: The convergence of Language models, Agent models, and World models represents
a critical frontier for artificial intelligence. While recent progress has
focused on scaling Language and Agent models, the development of sophisticated,
explicit World Models remains a key bottleneck, particularly for complex,
long-horizon multi-agent tasks. In domains such as robotic soccer, agents
trained via standard reinforcement learning in high-fidelity but
structurally-flat simulators often fail due to intractable exploration spaces
and sparse rewards. This position paper argues that the next frontier in
developing capable agents lies in creating environments that possess an
explicit, hierarchical World Model. We contend that this is best achieved
through hierarchical scaffolding, where complex goals are decomposed into
structured, manageable subgoals. Drawing evidence from a systematic review of
2024 research in multi-agent soccer, we identify a clear and decisive trend
towards integrating symbolic and hierarchical methods with multi-agent
reinforcement learning (MARL). These approaches implicitly or explicitly
construct a task-based world model to guide agent learning. We then propose a
paradigm shift: leveraging Large Language Models to dynamically generate this
hierarchical scaffold, effectively using language to structure the World Model
on the fly. This language-driven world model provides an intrinsic curriculum,
dense and meaningful learning signals, and a framework for compositional
learning, enabling Agent Models to acquire sophisticated, strategic behaviors
with far greater sample efficiency. By building environments with explicit,
language-configurable task layers, we can bridge the gap between low-level
reactive behaviors and high-level strategic team play, creating a powerful and
generalizable framework for training the next generation of intelligent agents.

</details>


### [6] [What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking](https://arxiv.org/abs/2509.04791)
*Yuan Sui,Yanming Zhang,Yi Liao,Yu Gu,Guohua Tang,Zhongqian Sun,Wei Yang,Bryan Hooi*

Main category: cs.AI

TL;DR: 现有大语言模型缺乏主动探索假设未来的能力，本文提出WiA - LLM范式，结合假设分析和强化学习让模型具备主动思考能力，在游戏实验中准确率达74.2%，是首次将假设分析能力集成到LLMs的工作。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理信息时缺乏系统探索假设未来的能力，限制了其在动态、高风险场景中的应用，需弥补这一关键差距。

Method: 提出WiA - LLM范式，集成What - If Analysis，并通过强化学习利用环境反馈，动态模拟每个潜在行动的结果。

Result: 在《王者荣耀》游戏环境中验证，WiA - LLM预测游戏状态变化的准确率达74.2%，在高难度场景中提升显著。

Conclusion: WiA - LLM是大语言模型主动推理的重大进展，为动态环境中的决策提供可扩展框架，对战略应用有广泛影响。

Abstract: Large language models (LLMs) excel at processing information reactively but
lack the ability to systemically explore hypothetical futures. They cannot ask,
"what if we take this action? how will it affect the final outcome" and
forecast its potential consequences before acting. This critical gap limits
their utility in dynamic, high-stakes scenarios like strategic planning, risk
assessment, and real-time decision making. To bridge this gap, we propose
WiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.
Our approach integrates What-If Analysis (WIA), a systematic approach for
evaluating hypothetical scenarios by changing input variables. By leveraging
environmental feedback via reinforcement learning, WiA-LLM moves beyond
reactive thinking. It dynamically simulates the outcomes of each potential
action, enabling the model to anticipate future states rather than merely react
to the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a
complex multiplayer game environment characterized by rapid state changes and
intricate interactions. The game's real-time state changes require precise
multi-step consequence prediction, making it an ideal testbed for our approach.
Experimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy
in forecasting game-state changes (up to two times gain over baselines). The
model shows particularly significant gains in high-difficulty scenarios where
accurate foresight is critical. To our knowledge, this is the first work to
formally explore and integrate what-if analysis capabilities within LLMs.
WiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,
providing a scalable framework for robust decision-making in dynamic
environments with broad implications for strategic applications.

</details>


### [7] [TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models](https://arxiv.org/abs/2509.04809)
*Haechang Kim,Hao Chen,Can Li,Jong Min Lee*

Main category: cs.AI

TL;DR: 提出TalkToAgent多智能体大语言模型框架用于强化学习策略的可解释性，在四水箱过程控制问题上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有可解释强化学习（XRL）结果可理解性有限、覆盖范围孤立，导致复杂强化学习策略与领域专家之间存在差距。

Method: 引入TalkToAgent框架，含五个专业大语言模型智能体，自动将用户查询映射到相关XRL工具，拓展反事实解释。

Result: 在四水箱过程控制问题上，TalkToAgent能高精度将用户查询映射到XRL任务，减少反事实生成失败，有效解释智能体行动。

Conclusion: TalkToAgent可有效解决现有XRL存在的问题，解释强化学习策略。

Abstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach
in improving the transparency of Reinforcement Learning (RL) agents. However,
there remains a gap between complex RL policies and domain experts, due to the
limited comprehensibility of XRL results and isolated coverage of current XRL
approaches that leave users uncertain about which tools to employ. To address
these challenges, we introduce TalkToAgent, a multi-agent Large Language Models
(LLM) framework that delivers interactive, natural language explanations for RL
policies. The architecture with five specialized LLM agents (Coordinator,
Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically
map user queries to relevant XRL tools and clarify an agent's actions in terms
of either key state variables, expected outcomes, or counterfactual
explanations. Moreover, our approach extends previous counterfactual
explanations by deriving alternative scenarios from qualitative behavioral
descriptions, or even new rule-based policies. We validated TalkToAgent on
quadruple-tank process control problem, a well-known nonlinear control
benchmark. Results demonstrated that TalkToAgent successfully mapped user
queries into XRL tasks with high accuracy, and coder-debugger interactions
minimized failures in counterfactual generation. Furthermore, qualitative
evaluation confirmed that TalkToAgent effectively interpreted agent's actions
and contextualized their meaning within the problem domain.

</details>


### [8] [Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory](https://arxiv.org/abs/2509.04847)
*Mukul Singh,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: 研究语言模型在迭代囚徒困境中的行为，发现其表现出色，能展现合作策略特性且适应能力强，为研究其在复杂人机环境中的作用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视语言模型在多方环境中的长期交互、人机协作及行为模式演变，需研究其在多方场景中的合作与竞争行为。

Method: 让基于模型的智能体与240种经典策略在Axelrod式锦标赛中竞争。

Result: 语言模型表现与最佳经典策略相当甚至更优，展现出合作策略的关键特性，能快速适应对手策略变化。

Conclusion: 首次系统描述语言模型智能体的长期合作行为，为其在更复杂人机社会环境中的研究提供基础。

Abstract: Language models are increasingly deployed in interactive online environments,
from personal chat assistants to domain-specific agents, raising questions
about their cooperative and competitive behavior in multi-party settings. While
prior work has examined language model decision-making in isolated or
short-term game-theoretic contexts, these studies often neglect long-horizon
interactions, human-model collaboration, and the evolution of behavioral
patterns over time. In this paper, we investigate the dynamics of language
model behavior in the iterated prisoner's dilemma (IPD), a classical framework
for studying cooperation and conflict. We pit model-based agents against a
suite of 240 well-established classical strategies in an Axelrod-style
tournament and find that language models achieve performance on par with, and
in some cases exceeding, the best-known classical strategies. Behavioral
analysis reveals that language models exhibit key properties associated with
strong cooperative strategies - niceness, provocability, and generosity while
also demonstrating rapid adaptability to changes in opponent strategy mid-game.
In controlled "strategy switch" experiments, language models detect and respond
to shifts within only a few rounds, rivaling or surpassing human adaptability.
These results provide the first systematic characterization of long-term
cooperative behaviors in language model agents, offering a foundation for
future research into their role in more complex, mixed human-AI social
environments.

</details>


### [9] [Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales](https://arxiv.org/abs/2509.04871)
*Krittanon Kaewtawee,Wachiravit Modecrua,Krittin Pachtrachai,Touchapon Kraisingkorn*

Main category: cs.AI

TL;DR: 本文提出克隆对话语音AI代理的通用方法，用电话销售数据举例，评估克隆代理与人类代理表现，分析不足并改进，最后给出设计经验与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言和语音建模发展，构建实时理解和生成人类对话的自主语音助手成为可能，需要一种克隆对话语音AI代理的方法以应用于各领域。

Method: 从通话记录语料库克隆对话语音AI代理，将自动语音识别、基于大语言模型的对话管理器和文本转语音合成集成到流式推理管道，构建代理时进行领域选择、知识提取和提示工程。

Result: 盲测显示AI代理在通话常规方面接近人类表现，但在说服和处理异议方面表现不佳。

Conclusion: 给出设计经验和未来研究方向，如大规模模拟和自动评估。

Abstract: Recent advances in language and speech modelling have made it possible to
build autonomous voice assistants that understand and generate human dialogue
in real time. These systems are increasingly being deployed in domains such as
customer service and healthcare care, where they can automate repetitive tasks,
reduce operational costs, and provide constant support around the clock. In
this paper, we present a general methodology for cloning a conversational voice
AI agent from a corpus of call recordings. Although the case study described in
this paper uses telesales data to illustrate the approach, the underlying
process generalizes to any domain where call transcripts are available. Our
system listens to customers over the telephone, responds with a synthetic
voice, and follows a structured playbook learned from top performing human
agents. We describe the domain selection, knowledge extraction, and prompt
engineering used to construct the agent, integrating automatic speech
recognition, a large language model based dialogue manager, and text to speech
synthesis into a streaming inference pipeline. The cloned agent is evaluated
against human agents on a rubric of 22 criteria covering introduction, product
communication, sales drive, objection handling, and closing. Blind tests show
that the AI agent approaches human performance in routine aspects of the call
while underperforming in persuasion and objection handling. We analyze these
shortcomings and refine the prompt accordingly. The paper concludes with design
lessons and avenues for future research, including large scale simulation and
automated evaluation.

</details>


### [10] [OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration](https://arxiv.org/abs/2509.04876)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Xiaofei Sun,Keze Wang*

Main category: cs.AI

TL;DR: 本文介绍知识感知自适应协作框架OSC，可增强含大语言模型的多智能体系统的认知协同，实验表明其能提升任务表现和通信效率。


<details>
  <summary>Details</summary>
Motivation: 现有工作在专家智能体深度协作的高效语言交互方面存在瓶颈，需要解决该问题以增强多智能体系统认知协同。

Method: 引入协作知识模型（CKM），使智能体动态感知协作者认知状态，通过实时认知差距分析，利用学习策略自适应调整通信行为。

Result: 在复杂推理和问题解决基准测试中，OSC显著提升了任务表现和通信效率，将“并行工作个体”转变为“深度协作认知团队”。

Conclusion: OSC框架优化了多智能体协作，为大语言模型智能体交互行为提供新见解。

Abstract: This paper introduces OSC (Orchestrating Cognitive Synergy), a
knowledge-aware adaptive collaboration framework designed to enhance cognitive
synergy in multi-agent systems with large language models. While prior work has
advanced agent selection and result aggregation, efficient linguistic
interactions for deep collaboration among expert agents remain a critical
bottleneck. OSC addresses this gap as a pivotal intermediate layer between
selection and aggregation, introducing Collaborator Knowledge Models (CKM) to
enable each agent to dynamically perceive its collaborators' cognitive states.
Through real-time cognitive gap analysis, agents adaptively adjust
communication behaviors, including content focus, detail level, and expression
style, using learned strategies. Experiments on complex reasoning and
problem-solving benchmarks demonstrate that OSC significantly improves task
performance and communication efficiency, transforming "parallel-working
individuals'' into a "deeply collaborative cognitive team.'' This framework not
only optimizes multi-agent collaboration but also offers new insights into LLM
agent interaction behaviors.

</details>


### [11] [SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing](https://arxiv.org/abs/2509.04908)
*Hongyi Jing,Jiafu Chen,Chen Rao,Ziqiang Dang,Jiajie Teng,Tianyi Chu,Juncheng Mo,Shuo Fang,Huaizhong Lin,Rui Lv,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: 现有GUI感知多模态大模型有局限，提出SparkUI - Parser框架及ScreenParse基准，实验显示其性能优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有GUI感知多模态大模型定位精度低、推理速度慢和无法解析整个界面的问题。

Method: 基于预训练MLLM进行坐标连续建模，添加token路由器和坐标解码器；引入基于改进匈牙利匹配算法的拒绝机制；构建ScreenParse基准。

Result: 在多个基准测试中，提出的方法始终优于SOTA方法。

Conclusion: SparkUI - Parser框架能同时实现更高定位精度和整个界面的细粒度解析能力，可有效解决现有模型问题。

Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have
made great progress. However, the following challenges still exist in prior
methods: 1) They model discrete coordinates based on text autoregressive
mechanism, which results in lower grounding accuracy and slower inference
speed. 2) They can only locate predefined sets of elements and are not capable
of parsing the entire interface, which hampers the broad application and
support for downstream tasks. To address the above issues, we propose
SparkUI-Parser, a novel end-to-end framework where higher localization
precision and fine-grained parsing capability of the entire interface are
simultaneously achieved. Specifically, instead of using probability-based
discrete modeling, we perform continuous modeling of coordinates based on a
pre-trained Multimodal Large Language Model (MLLM) with an additional token
router and coordinate decoder. This effectively mitigates the limitations
inherent in the discrete output characteristics and the token-by-token
generation process of MLLMs, consequently boosting both the accuracy and the
inference speed. To further enhance robustness, a rejection mechanism based on
a modified Hungarian matching algorithm is introduced, which empowers the model
to identify and reject non-existent elements, thereby reducing false positives.
Moreover, we present ScreenParse, a rigorously constructed benchmark to
systematically assess structural perception capabilities of GUI models across
diverse scenarios. Extensive experiments demonstrate that our approach
consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,
CAGUI-Grounding and ScreenParse benchmarks. The resources are available at
https://github.com/antgroup/SparkUI-Parser.

</details>


### [12] [Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts](https://arxiv.org/abs/2509.04926)
*Barbara Gendron,Gaël Guibon,Mathieu D'aquin*

Main category: cs.AI

TL;DR: 本文提出基于本体的方法解决大语言模型作为对话代理时的可控性问题，以CEFR语言水平为例实现对话熟练度控制，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型作为对话代理时的可控性问题，确保可预测和个性化的回复。

Method: 提出基于本体的方法，利用语言描述符将定性概念量化，形式化为描述逻辑并融入本体，通过微调引导大语言模型生成文本。

Result: 实验表明该方法能提供一致且可解释的熟练度定义，提高对话AI的透明度。

Conclusion: 基于本体的方法可有效解决大语言模型对话可控性问题，提高透明度。

Abstract: The controllability of Large Language Models (LLMs) when used as
conversational agents is a key challenge, particularly to ensure predictable
and user-personalized responses. This work proposes an ontology-based approach
to formally define conversational features that are typically qualitative in
nature. By leveraging a set of linguistic descriptors, we derive quantitative
definitions for qualitatively-defined concepts, enabling their integration into
an ontology for reasoning and consistency checking. We apply this framework to
the task of proficiency-level control in conversations, using CEFR language
proficiency levels as a case study. These definitions are then formalized in
description logic and incorporated into an ontology, which guides controlled
text generation of an LLM through fine-tuning. Experimental results demonstrate
that our approach provides consistent and explainable proficiency-level
definitions, improving transparency in conversational AI.

</details>


### [13] [Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents](https://arxiv.org/abs/2509.04979)
*Rajesh Tembarai Krishnamachari,Srividya Rajesh*

Main category: cs.AI

TL;DR: 提出DOVIS协议和AgentRank - UC算法，实现AI代理排名，展示在代理网络中的可行性。


<details>
  <summary>Details</summary>
Motivation: 实现将互联网转变为Web of Agents愿景，需要代理排名，但现有情况使排名需协调，因此要解决此问题。

Method: 提出DOVIS五层操作协议收集数据，实现AgentRank - UC动态、信任感知算法进行排名。

Result: 给出模拟结果和理论保证，如收敛性、鲁棒性和抗女巫攻击等。

Conclusion: 证明协调协议和性能感知排名可实现可扩展、可信的代理网络。

Abstract: AI agents -- powered by reasoning-capable large language models (LLMs) and
integrated with tools, data, and web search -- are poised to transform the
internet into a \emph{Web of Agents}: a machine-native ecosystem where
autonomous agents interact, collaborate, and execute tasks at scale. Realizing
this vision requires \emph{Agent Ranking} -- selecting agents not only by
declared capabilities but by proven, recent performance. Unlike Web~1.0's
PageRank, a global, transparent network of agent interactions does not exist;
usage signals are fragmented and private, making ranking infeasible without
coordination.
  We propose \textbf{DOVIS}, a five-layer operational protocol
(\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that
enables the collection of minimal, privacy-preserving aggregates of usage and
performance across the ecosystem. On this substrate, we implement
\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines
\emph{usage} (selection frequency) and \emph{competence} (outcome quality,
cost, safety, latency) into a unified ranking. We present simulation results
and theoretical guarantees on convergence, robustness, and Sybil resistance,
demonstrating the viability of coordinated protocols and performance-aware
ranking in enabling a scalable, trustworthy Agentic Web.

</details>


### [14] [Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework](https://arxiv.org/abs/2509.05007)
*Jie Chen,Jinhao Jiang,Yingqian Min,Zican Dong,Shijie Wang,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: 提出Sticker - TTS测试时间扩展框架，结合两阶段优化策略，在数学推理基准测试中超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前测试时间扩展方法主要依赖冗余采样，忽略历史经验利用，限制计算效率。

Method: 提出Sticker - TTS框架，协调三个协作的大推理模型，利用蒸馏的关键条件（贴纸）迭代探索和完善解决方案；引入结合模仿学习和自我改进的两阶段优化策略。

Result: 在AIME - 24、AIME - 25和OlymMATH三个数学推理基准测试中，Sticker - TTS在可比推理预算下始终超越强基线方法。

Conclusion: 贴纸引导的历史经验利用是有效的。

Abstract: Large reasoning models (LRMs) have exhibited strong performance on complex
reasoning tasks, with further gains achievable through increased computational
budgets at inference. However, current test-time scaling methods predominantly
rely on redundant sampling, ignoring the historical experience utilization,
thereby limiting computational efficiency. To overcome this limitation, we
propose Sticker-TTS, a novel test-time scaling framework that coordinates three
collaborative LRMs to iteratively explore and refine solutions guided by
historical attempts. At the core of our framework are distilled key
conditions-termed stickers-which drive the extraction, refinement, and reuse of
critical information across multiple rounds of reasoning. To further enhance
the efficiency and performance of our framework, we introduce a two-stage
optimization strategy that combines imitation learning with self-improvement,
enabling progressive refinement. Extensive evaluations on three challenging
mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,
demonstrate that Sticker-TTS consistently surpasses strong baselines, including
self-consistency and advanced reinforcement learning approaches, under
comparable inference budgets. These results highlight the effectiveness of
sticker-guided historical experience utilization. Our code and data are
available at https://github.com/RUCAIBox/Sticker-TTS.

</details>


### [15] [Finding your MUSE: Mining Unexpected Solutions Engine](https://arxiv.org/abs/2509.05072)
*Nir Sweed,Hanit Hakim,Ben Wolfson,Hila Lifshitz,Dafna Shahaf*

Main category: cs.AI

TL;DR: 论文提出构建功能概念图（FCGs）的方法，还给出利用FCGs的MUSE算法来产生创意灵感，并基于50万专利计算FCG并公开。


<details>
  <summary>Details</summary>
Motivation: 创新者常存在认知固着，阻碍探索新方案，需新方法支持创新。

Method: 提出构建FCGs的方法，及利用FCGs的MUSE算法。

Result: 构建出具有明确抽象关系的大规模、高质量FCGs，基于50万专利计算出FCG并公开。

Conclusion: 所提方法克服了先前工作的局限，可用于为给定问题生成创意灵感。

Abstract: Innovators often exhibit cognitive fixation on existing solutions or nascent
ideas, hindering the exploration of novel alternatives. This paper introduces a
methodology for constructing Functional Concept Graphs (FCGs), interconnected
representations of functional elements that support abstraction, problem
reframing, and analogical inspiration. Our approach yields large-scale,
high-quality FCGs with explicit abstraction relations, overcoming limitations
of prior work. We further present MUSE, an algorithm leveraging FCGs to
generate creative inspirations for a given problem. We demonstrate our method
by computing an FCG on 500K patents, which we release for further research.

</details>


### [16] [ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback](https://arxiv.org/abs/2509.05091)
*Matteo Bortoletto,Yichao Zhou,Lance Ying,Tianmin Shu,Andreas Bulling*

Main category: cs.AI

TL;DR: 为解决人类协作难题，开发了AI系统ProToM促进亲社会行为，评估显示其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决人类在追求独立目标时识别何时及如何协助和与他人合作的挑战，开发促进亲社会行为的AI系统。

Method: 引入ProToM，用贝叶斯逆规划推断代理目标，通过最大化预期效用选择反馈。

Result: 在两个多智能体环境中评估，现有大语言和推理模型沟通反馈不佳，ProToM能提供有针对性且有用的反馈，成功率更高、任务完成时间更短，受人类用户青睐。

Conclusion: ProToM系统能有效促进多智能体系统中的亲社会行为，优于现有模型。

Abstract: While humans are inherently social creatures, the challenge of identifying
when and how to assist and collaborate with others - particularly when pursuing
independent goals - can hinder cooperation. To address this challenge, we aim
to develop an AI system that provides useful feedback to promote prosocial
behaviour - actions that benefit others, even when not directly aligned with
one's own goals. We introduce ProToM, a Theory of Mind-informed facilitator
that promotes prosocial actions in multi-agent systems by providing targeted,
context-sensitive feedback to individual agents. ProToM first infers agents'
goals using Bayesian inverse planning, then selects feedback to communicate by
maximising expected utility, conditioned on the inferred goal distribution. We
evaluate our approach against baselines in two multi-agent environments: Doors,
Keys, and Gems, as well as Overcooked. Our results suggest that
state-of-the-art large language and reasoning models fall short of
communicating feedback that is both contextually grounded and well-timed -
leading to higher communication overhead and task speedup. In contrast, ProToM
provides targeted and helpful feedback, achieving a higher success rate,
shorter task completion times, and is consistently preferred by human users.

</details>


### [17] [Evaluation and Comparison Semantics for ODRL](https://arxiv.org/abs/2509.05139)
*Jaime Osvaldo Salas,Paolo Pareti,Semih Yumuşak,Soulmaz Gheisari,Luis-Daniel Ibáñez,George Konstantinidis*

Main category: cs.AI

TL;DR: 为ODRL提供基于查询回答的形式语义，并研究策略比较问题


<details>
  <summary>Details</summary>
Motivation: ODRL虽有特征形式化规范初步进展，但缺乏全面形式语义，且有数据共享场景需求

Method: 基于查询回答为ODRL提供形式语义

Result: 得到的语义改进了先前形式化，符合最新语言规范

Conclusion: 定义并研究了比较两个策略、检测等效、更严格或更宽松策略的问题

Abstract: We consider the problem of evaluating, and comparing computational policies
in the Open Digital Rights Language (ODRL), which has become the de facto
standard for governing the access and usage of digital resources. Although
preliminary progress has been made on the formal specification of the
language's features, a comprehensive formal semantics of ODRL is still missing.
In this paper, we provide a simple and intuitive formal semantics for ODRL that
is based on query answering. Our semantics refines previous formalisations, and
is aligned with the latest published specification of the language (2.2).
Building on our evaluation semantics, and motivated by data sharing scenarios,
we also define and study the problem of comparing two policies, detecting
equivalent, more restrictive or more permissive policies.

</details>


### [18] [LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation](https://arxiv.org/abs/2509.05263)
*Yinglin Duan,Zhengxia Zou,Tongwei Gu,Wei Jia,Zhan Zhao,Luyi Xu,Xinzhu Liu,Hao Jiang,Kang Chen,Shuang Qiu*

Main category: cs.AI

TL;DR: 本文提出LatticeWorld框架用于3D世界生成，接受多模态输入创建3D交互世界，实验表明其场景布局生成准确、视觉保真度高，且生产效率大幅提升。


<details>
  <summary>Details</summary>
Motivation: 当前研究聚焦开发模拟复杂现实场景的3D世界模型，传统手动建模有局限，现代方法多关注生成式方法，本文旨在探索新的3D世界生成研究方向，简化3D环境的工业生产流程。

Method: 提出LatticeWorld框架，利用轻量级大语言模型LLaMA - 2 - 7B和行业级渲染引擎（如虚幻引擎5），接受文本描述和视觉指令作为多模态输入。

Result: 实验显示LatticeWorld在场景布局生成准确性和视觉保真度上表现出色，与传统手动生产方法相比，工业生产效率提高超90倍，且创意质量高。

Conclusion: LatticeWorld是一个简单有效的3D世界生成框架，能有效简化3D环境的工业生产流程。

Abstract: Recent research has been increasingly focusing on developing 3D world models
that simulate complex real-world scenarios. World models have found broad
applications across various domains, including embodied AI, autonomous driving,
entertainment, etc. A more realistic simulation with accurate physics will
effectively narrow the sim-to-real gap and allow us to gather rich information
about the real world conveniently. While traditional manual modeling has
enabled the creation of virtual 3D scenes, modern approaches have leveraged
advanced machine learning algorithms for 3D world generation, with most recent
advances focusing on generative methods that can create virtual worlds based on
user instructions. This work explores such a research direction by proposing
LatticeWorld, a simple yet effective 3D world generation framework that
streamlines the industrial production pipeline of 3D environments. LatticeWorld
leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering
engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed
framework accepts textual descriptions and visual instructions as multimodal
inputs and creates large-scale 3D interactive worlds with dynamic agents,
featuring competitive multi-agent interaction, high-fidelity physics
simulation, and real-time rendering. We conduct comprehensive experiments to
evaluate LatticeWorld, showing that it achieves superior accuracy in scene
layout generation and visual fidelity. Moreover, LatticeWorld achieves over a
$90\times$ increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods. Our demo
video is available at https://youtu.be/8VWZXpERR18

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [19] [Phase-field and lip-field approaches for fracture with extreme mesh deformation (X-Mesh): a one-dimensional study](https://arxiv.org/abs/2509.04971)
*Nicolas Moës,Benoît Lé,Nicolas Chevaugeon,Jean-François Remacle*

Main category: cs.CE

TL;DR: 研究一维断裂问题，用相场或唇场方法，优化增量势，受损单元尺寸趋于零，解更精确，属X - Mesh研究。


<details>
  <summary>Details</summary>
Motivation: 探索极端网格在计算力学中的能力（X - Mesh）。

Method: 采用相场或唇场方法对一维断裂问题建模，优化位移、损伤场和网格节点坐标的增量势。

Result: 损伤达最大值时，受损最严重的单元尺寸趋于零，能精确表示位移跳跃，整体解比固定网格解更准确。

Conclusion: 该方法对一维断裂问题求解有效，为X - Mesh研究提供了成果。

Abstract: We consider a one-dimensional fracture problem modelled using either the
phase-field or lip-field approach. In both cases, we optimise the incremental
potential with respect to the displacement and damage fields and the nodal
coordinates of the mesh. This is thus a variational mesh study. We observe
that, as the damage reaches its maximum value, the optimisation drives the most
damaged element to zero size as the damage reaches its maximum value. This
peculiar element provides a precise displacement jump representation as the bar
breaks. The overall solution is also shown to be much more accurate than the
fixed mesh solution. This work forms part of an exploration into the
capabilities of extreme meshes in computational mechanics (X-Mesh).

</details>


### [20] [Deep Inverse Rosenblatt Transport for Structural Reliability Analysis](https://arxiv.org/abs/2509.05061)
*Aryan Tyagi,Jan N. Fuhg*

Main category: cs.CE

TL;DR: 本文研究DIRT框架用于固体力学可靠性分析，通过实验证明其在高维问题中能降低估计方差并准确捕捉罕见事件概率。


<details>
  <summary>Details</summary>
Motivation: 传统可靠性分析方法在高维问题和罕见事件中计算困难或估计方差高，需更有效方法。

Method: 采用DIRT框架，结合TT分解和逆Rosenblatt变换构建后验分布的低秩近似。

Result: 在3个分析可靠性问题和1个数值示例上进行验证，相比BUS - SuS方法，DIRT降低了估计方差，准确捕捉罕见事件概率。

Conclusion: DIRT框架在可靠性分析中有效，尤其在高维问题和罕见事件中有优势。

Abstract: Accurately estimating the probability of failure in engineering systems under
uncertainty is a fundamental challenge, particularly in high-dimensional
settings and for rare events. Conventional reliability analysis methods often
become computationally intractable or exhibit high estimator variance when
applied to problems with hundreds of uncertain parameters or highly
concentrated failure regions. In this work, we investigate the use of the
recently proposed Deep Inverse Rosenblatt Transport (DIRT) framework for
reliability analysis in solid mechanics. DIRT combines a TT decomposition with
an inverse Rosenblatt transformation to construct a low-rank approximation of
the posterior distribution, enabling efficient sampling and probability
estimation in high-dimensional spaces. By representing the optimal importance
density in the TT format, DIRT scales linearly in the input dimension while
maintaining a compact, reusable surrogate of the target distribution. We
demonstrate the effectiveness of the DIRT framework on three analytical
reliability problems and one numerical example with dimensionality ranging from
2 to 250. Compared to established methods such as Bayesian updating with Subset
Simulation (BUS-SuS), DIRT seems to lower the estimator variance while
accurately capturing rare event probabilities for the benchmark problems of
this study.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [21] [Schema Inference for Tabular Data Repositories Using Large Language Models](https://arxiv.org/abs/2509.04632)
*Zhenyu Wu,Jiaoyan Chen,Norman W. Paton*

Main category: cs.DB

TL;DR: 提出SI - LLM利用列标题和单元格值为表格数据推断概念模式，评估效果好。


<details>
  <summary>Details</summary>
Motivation: 最小化整理的表格数据存在表示不一致和元数据稀疏问题，现有工作在元数据有限时模式推断困难。

Method: 提出SI - LLM，仅使用列标题和单元格值为表格数据推断简洁的概念模式，模式包含层次实体类型、属性和类型间关系。

Result: 在两个数据集上评估，SI - LLM实现了有前景的端到端结果，各步骤结果优于或与现有方法相当。

Conclusion: SI - LLM能有效解决元数据有限时表格数据的模式推断问题，代码等资源公开。

Abstract: Minimally curated tabular data often contain representational inconsistencies
across heterogeneous sources, and are accompanied by sparse metadata. Working
with such data is intimidating. While prior work has advanced dataset discovery
and exploration, schema inference remains difficult when metadata are limited.
We present SI-LLM (Schema Inference using Large Language Models), which infers
a concise conceptual schema for tabular data using only column headers and cell
values. The inferred schema comprises hierarchical entity types, attributes,
and inter-type relationships. In extensive evaluation on two datasets from web
tables and open data, SI-LLM achieves promising end-to-end results, as well as
better or comparable results to state-of-the-art methods at each step. All
source code, full prompts, and datasets of SI-LLM are available at
https://github.com/PierreWoL/SILLM.

</details>


### [22] [Efficient Exact Resistance Distance Computation on Small-Treewidth Graphs: a Labelling Approach](https://arxiv.org/abs/2509.05129)
*Meihao Liao,Yueyang Pan,Rong-Hua Li,Guoren Wang*

Main category: cs.DB

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Resistance distance computation is a fundamental problem in graph analysis,
yet existing random walk-based methods are limited to approximate solutions and
suffer from poor efficiency on small-treewidth graphs (e.g., road networks). In
contrast, shortest-path distance computation achieves remarkable efficiency on
such graphs by leveraging cut properties and tree decompositions. Motivated by
this disparity, we first analyze the cut property of resistance distance. While
a direct generalization proves impractical due to costly matrix operations, we
overcome this limitation by integrating tree decompositions, revealing that the
resistance distance $r(s,t)$ depends only on labels along the paths from $s$
and $t$ to the root of the decomposition. This insight enables compact
labelling structures. Based on this, we propose \treeindex, a novel index
method that constructs a resistance distance labelling of size $O(n \cdot
h_{\mathcal{G}})$ in $O(n \cdot h_{\mathcal{G}}^2 \cdot d_{\max})$ time, where
$h_{\mathcal{G}}$ (tree height) and $d_{\max}$ (maximum degree) behave as small
constants in many real-world small-treewidth graphs (e.g., road networks). Our
labelling supports exact single-pair queries in $O(h_{\mathcal{G}})$ time and
single-source queries in $O(n \cdot h_{\mathcal{G}})$ time. Extensive
experiments show that TreeIndex substantially outperforms state-of-the-art
approaches. For instance, on the full USA road network, it constructs a $405$
GB labelling in $7$ hours (single-threaded) and answers exact single-pair
queries in $10^{-3}$ seconds and single-source queries in $190$ seconds--the
first exact method scalable to such large graphs.

</details>


### [23] [Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving](https://arxiv.org/abs/2509.02718)
*Fangzhou Wu,Sandeep Silwal*

Main category: cs.DB

TL;DR: 提出首个用于在线路由场景的免训练算法，提升大语言模型服务的性能、成本效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务需求增加带来高部署和计算成本，现有工作难适应高查询量和受限令牌预算的在线场景。

Method: 利用近似最近邻搜索估计查询特征，对少量初始查询进行一次性优化以学习路由策略。

Result: 算法在自然假设下达到1 - o(1)的竞争比，实验显示在整体性能、成本效率和吞吐量上分别平均提升3.55倍、1.85倍和近4.25倍。

Conclusion: 所提算法在在线路由场景有效，能显著提升大语言模型服务效果。

Abstract: Increasing demand for Large Language Models (LLMs) services imposes
substantial deployment and computation costs on providers. LLM routing offers a
cost-efficient solution by directing queries to the optimal LLM based on model
and query features. However, existing works primarily focus on offline
scenarios and struggle to adapt to online settings with high query volume and
constrained token budgets. In this work, we introduce the first training-free
algorithm for online routing scenarios. Our algorithm leverages approximate
nearest neighbor search to efficiently estimate query features and performs a
one-time optimization over a small set of initial queries to learn a routing
strategy that guides future routing. We provide theoretical guarantees
demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$
under natural assumptions, which is further validated by extensive experiments
across 3 benchmark datasets and 8 baselines, showing an average improvement of
3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and
nearly 4.25$\times$ in throughput.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [24] [STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs](https://arxiv.org/abs/2509.04719)
*Han Liang,Jiahui Zhou,Zicheng Zhou,Xiaoxi Zhang,Xu Chen*

Main category: cs.DC

TL;DR: 本文提出STADI框架加速异构多GPU环境下扩散模型推理，通过混合调度器优化时空并行性，实验验证其能改善负载均衡，降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散模型应用需高效并行推理技术，但现有方案在异构多GPU环境下资源利用率低、存在负载不均衡问题。

Method: 提出Spatio - Temporal Adaptive Diffusion Inference (STADI)框架，采用混合调度器，包括计算感知步骤分配器和弹性补丁并行机制。

Result: 在负载不均衡和异构多GPU集群实验中，相比现有框架，STADI最多降低45%端到端推理延迟，显著提高资源利用率。

Conclusion: STADI框架能有效改善异构多GPU环境下的负载均衡，缓解性能瓶颈，提升推理效率。

Abstract: The escalating adoption of diffusion models for applications such as image
generation demands efficient parallel inference techniques to manage their
substantial computational cost. However, existing diffusion parallelism
inference schemes often underutilize resources in heterogeneous multi-GPU
environments, where varying hardware capabilities or background tasks cause
workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion
Inference (STADI), a novel framework to accelerate diffusion model inference in
such settings. At its core is a hybrid scheduler that orchestrates fine-grained
parallelism across both temporal and spatial dimensions. Temporally, STADI
introduces a novel computation-aware step allocator applied after warmup
phases, using a least-common-multiple-minimizing quantization technique to
reduce denoising steps on slower GPUs and execution synchronization. To further
minimize GPU idle periods, STADI executes an elastic patch parallelism
mechanism that allocates variably sized image patches to GPUs according to
their computational capability, ensuring balanced workload distribution through
a complementary spatial mechanism. Extensive experiments on both
load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,
demonstrating improved load balancing and mitigation of performance
bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion
inference framework, our method significantly reduces end-to-end inference
latency by up to 45% and significantly improves resource utilization on
heterogeneous GPUs.

</details>


### [25] [VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving](https://arxiv.org/abs/2509.04827)
*Jiahuan Yu,Aryan Taneja,Junfeng Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: 本文提出 VoltanaLLM 系统用于大语言模型（LLM）高效节能服务，实现高达 36.3% 的节能，同时保持高服务水平目标（SLO）达成率。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型服务系统支持交互式应用，但推理能耗高，对可持续和成本效益部署构成挑战。

Method: 从控制理论角度构建 VoltanaLLM 系统，协同设计新兴预填充/解码分离架构中的频率缩放和请求路由，包括反馈驱动的频率控制器和状态空间路由器。

Result: 在多个先进大语言模型和真实数据集上评估，VoltanaLLM 实现高达 36.3% 的节能，保持近乎完美的 SLO 达成率。

Conclusion: VoltanaLLM 为可持续和智能的大语言模型服务铺平了道路。

Abstract: Modern Large Language Model (LLM) serving systems increasingly support
interactive applications, like real-time chat assistants, code generation
tools, and agentic workflows. However, the soaring energy cost of LLM inference
presents a growing challenge for sustainable and cost-effective deployment.
This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM
serving, built from a control theory perspective. VoltanaLLM co-designs
frequency scaling and request routing in emerging prefill/decode disaggregated
architectures, leveraging their decoupled execution to enable fine-grained
phase-specific control. It consists of a feedback-driven frequency controller
that dynamically adapts GPU frequency for prefill and decode phases, and a
state-space router that explores routing decisions across frequency-scaled
instances to minimize energy under latency constraints. We implement VoltanaLLM
in SGLang and evaluate its performance over multiple state-of-the-art LLMs and
real-world datasets. The results demonstrate that VoltanaLLM achieves up to
36.3% energy savings while maintaining near-perfect SLO attainment rate, paving
the way for sustainable and intelligent LLM serving.

</details>


### [26] [Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization](https://arxiv.org/abs/2509.05216)
*Mengjiao Han,Andres Sewell,Joseph Insley,Janet Knowles,Victor A. Mateevitsi,Michael E. Papka,Steve Petruzza,Silvio Rizzi*

Main category: cs.DC

TL;DR: 提出用于科学可视化的3D高斯展开（3D - GS）管道的多GPU扩展，提升训练吞吐量和支持高分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 实现大规模数据集的可扩展处理，将3D - GS集成到基于HPC的科学工作流中，实现复杂模拟的实时可视化。

Method: 结合从Grendel - GS改编的多GPU训练后端，在多个GPU上分布优化。

Result: 在Kingsnake数据集上用4个GPU比单GPU实现5.6倍加速，成功训练单A100 GPU无法处理的Miranda数据集。

Conclusion: 为将3D - GS集成到基于HPC的科学工作流奠定基础，可实现复杂模拟的实时事后和原位可视化。

Abstract: We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS)
pipeline for scientific visualization. Building on previous work that
demonstrated high-fidelity isosurface reconstruction using Gaussian primitives,
we incorporate a multi-GPU training backend adapted from Grendel-GS to enable
scalable processing of large datasets. By distributing optimization across
GPUs, our method improves training throughput and supports high-resolution
reconstructions that exceed single-GPU capacity. In our experiments, the system
achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs
compared to a single-GPU baseline, and successfully trains the Miranda dataset
(18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays
the groundwork for integrating 3D-GS into HPC-based scientific workflows,
enabling real-time post hoc and in situ visualization of complex simulations.

</details>


### [27] [Dynamic reconfiguration for malleable applications using RMA](https://arxiv.org/abs/2509.05248)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo*

Main category: cs.DC

TL;DR: 研究MPI中基于RMA操作的单侧通信方法用于可延展应用动态调整大小，与传统方法对比并扩展策略，当前初始化成本限制优势。


<details>
  <summary>Details</summary>
Motivation: 研究适用于可延展应用动态调整大小的新型通信方法，实现数据重分配并减少对应用执行的影响。

Method: 研究基于RMA操作的单侧通信方法，将其集成到MaM库中与传统基于集体的方法对比，扩展Wait Drains策略。

Result: 性能表现相当，但高初始化成本限制了优势。

Conclusion: 新型单侧通信方法有潜力，但当前初始化成本问题需解决。

Abstract: This paper investigates the novel one-sided communication methods based on
remote memory access (RMA) operations in MPI for dynamic resizing of malleable
applications, enabling data redistribution with minimal impact on application
execution. After their integration into the MaM library, these methods are
compared with traditional collective-based approaches. In addition, the
existing strategy Wait Drains is extended to support efficient background
reconfiguration. Results show comparable performance, though high
initialization costs currently limit their advantage.

</details>


### [28] [Scaling Performance of Large Language Model Pretraining](https://arxiv.org/abs/2509.05258)
*Alexander Interrante-Grant,Carla Varela-Rosa,Suhaas Narayan,Chris Connelly,Albert Reuther*

Main category: cs.DC

TL;DR: 大语言模型训练计算成本高，公开文献中关于其训练扩展性能和考虑因素的信息稀缺，本文旨在揭秘大语言模型预训练管道。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练计算成本高，公开文献中相关训练扩展性能和考虑因素信息稀缺，缺乏调优训练性能的实用建议。

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: Large language models (LLMs) show best-in-class performance across a wide
range of natural language processing applications. Training these models is an
extremely computationally expensive task; frontier Artificial Intelligence (AI)
research companies are investing billions of dollars into supercomputing
infrastructure to train progressively larger models on increasingly massive
datasets. Unfortunately, information about the scaling performance and training
considerations of these large training pipelines is scarce in public
literature. Working with large-scale datasets and models can be complex and
practical recommendations are scarce in the public literature for tuning
training performance when scaling up large language models. In this paper, we
aim to demystify the large language model pretraining pipeline somewhat - in
particular with respect to distributed training, managing large datasets across
hundreds of nodes, and scaling up data parallelism with an emphasis on fully
leveraging available GPU compute capacity.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [29] [Transitivity Preserving Projection in Directed Hypergraphs](https://arxiv.org/abs/2509.04543)
*Eric Parsonage,Matthew Roughan,Hung X Nguyen*

Main category: cs.DS

TL;DR: 本文提出了一种新的保传递性投影（TPP）方法，解决有向超图可视化和分析难题，算法高效，实验显示其性能优于BBP。


<details>
  <summary>Details</summary>
Motivation: 有向超图在多领域重要，但固有复杂性阻碍可视化和分析，现有BBP方法有局限性。

Method: 引入TPP方法，利用集合字典树数据结构开发高效算法，将计算复杂度从指数级降低到线性级。

Result: 实验表明TPP性能优越，在BBP 24小时无法完成的图上，TPP能在数秒内完成投影。

Conclusion: TPP提供关系的最小完整视图，支持网络安全等应用。

Abstract: Directed hypergraphs are vital for modeling complex polyadic relationships in
domains such as discrete mathematics, computer science, network security, and
systems modeling. However, their inherent complexity often impedes effective
visualization and analysis, particularly for large graphs. This paper
introduces a novel Transitivity Preserving Projection (TPP) to address the
limitations of the computationally intensive Basu and Blanning projection
(BBP), which can paradoxically increase complexity by flattening transitive
relationships. TPP offers a minimal and complete representation of
relationships within a chosen subset of elements, capturing only irreducible
dominant metapaths to ensure the smallest set of edges while preserving all
essential transitive and direct connections. This approach significantly
enhances visualization by reducing edge proliferation and maintains the
integrity of the original hypergraph's structure. We develop an efficient
algorithm leveraging the set-trie data structure, reducing the computational
complexity from an exponential number of metapath searches in BBP to a linear
number of metapath searches with polynomial-time filtering, enabling
scalability for real-world applications. Experimental results demonstrate TPP's
superior performance, completing projections in seconds on graphs where BBP
fails to terminate within 24 hours. By providing a minimal yet complete view of
relationships, TPP supports applications in network security and supply

</details>


### [30] [Additive, Near-Additive, and Multiplicative Approximations for APSP in Weighted Undirected Graphs: Trade-offs and Algorithms](https://arxiv.org/abs/2509.04640)
*Liam Roditty,Ariel Sapir*

Main category: cs.DS

TL;DR: 本文为稠密加权图提出新的+2∑Wi - APASP算法，在近似加性、乘性APASP问题上改进已有算法，还绕过了特定条件下的下界。


<details>
  <summary>Details</summary>
Motivation: 改进稠密加权图的APASP算法，在不同类型APASP问题上取得更好性能。

Method: 提出新算法，对比和改进已有算法。

Result: 给出不同类型APASP问题的新算法，改进了运行时间。

Conclusion: 新算法在稠密加权图APASP问题上有优势，能绕过部分条件下界。

Abstract: We present a $+2\sum_{i=1}^{k+1}{W_i}$-APASP algorithm for dense weighted
graphs with runtime $\tilde O\left(n^{2+\frac{1}{3k+2}}\right)$, where $W_{i}$
is the weight of an $i^\textnormal{th}$ heaviest edge on a shortest path. Dor,
Halperin and Zwick [FOCS'96, SICOMP'00] had two algorithms for the commensurate
unweighted $+2\cdot\left( k+1\right)$-APASP: $\tilde
O\left(n^{2-\frac{1}{k+2}}m^{\frac{1}{k+2}}\right)$ runtime for sparse graphs
and $\tilde O\left(n^{2+\frac{1}{3k+2}}\right)$ runtime for dense graphs. Cohen
and Zwick [SODA'97, JALG'01] adapted the sparse variant to weighted graphs:
$+2\sum_{i=1}^{k+1}{W_i}$-APASP algorithm in the same runtime. We show an
algorithm for dense weighted graphs.
  For \emph{nearly additive} APASP, we present a
$\left(1+\varepsilon,\min{\left\{2W_1,4W_{2}\right\}}\right)$-APASP algorithm
with $\tilde O\left(\left(\frac{1}{\varepsilon}\right)^{O\left(1\right)}\cdot
n^{2.15135313}\cdot\log W\right)$ runtime. This improves the
$\left(1+\varepsilon,2W_1\right)$-APASP of Saha and Ye [SODA'24].
  For multiplicative APASP, we show a framework of $\left(\frac{3\ell +4}{\ell
+ 2}+\varepsilon\right)$-APASP algorithms, reducing the runtime of Akav and
Roditty [ESA'21] for dense graphs and generalizing the
$\left(2+\varepsilon\right)$-APASP algorithm of Dory et al [SODA'24]. Our base
case is a $\left(\frac{7}{3}+\varepsilon\right)$-APASP in $\tilde
O\left(\left(\frac{1}{\varepsilon}\right)^{O\left(1\right)}\cdot
n^{2.15135313}\cdot \log W\right)$ runtime, improving the $\frac{7}{3}$-APASP
algorithm of Baswana and Kavitha [FOCS'06, SICOMP'10] for dense graphs.
  Finally, we "bypass" an $\tilde \Omega \left(n^\omega\right)$ conditional
lower bound by Dor, Halperin, and Zwick for $\alpha$-APASP with $\alpha < 2$,
by allowing an additive term (e.g.
$\paren{\frac{6k+3}{3k+2},\sum_{i=1}^{k+1}{W_{i}}}$-APASP in $\tilde
O\left(n^{2+\frac{1}{3k+2}}\right)$ runtime.).

</details>


### [31] [A 13/6-Approximation for Strip Packing via the Bottom-Left Algorithm](https://arxiv.org/abs/2509.04654)
*Stefan Hougardy,Bart Zondervan*

Main category: cs.DS

TL;DR: 提出新的矩形排序，使Bottom - Left算法在带包装问题中达到13/6近似比。


<details>
  <summary>Details</summary>
Motivation: 过去45年未找到能改进Bottom - Left算法近似比界的排序，旨在寻找更好的排序。

Method: 引入一种新的矩形排序方式。

Result: 采用新排序后，Bottom - Left算法对带包装问题达到13/6近似比。

Conclusion: 新的矩形排序可提升Bottom - Left算法在带包装问题中的近似性能。

Abstract: In the Strip Packing problem, we are given a vertical strip of fixed width
and unbounded height, along with a set of axis-parallel rectangles. The task is
to place all rectangles within the strip, without overlaps, while minimizing
the height of the packing. This problem is known to be NP-hard. The Bottom-Left
Algorithm is a simple and widely used heuristic for Strip Packing. Given a
fixed order of the rectangles, it places them one by one, always choosing the
lowest feasible position in the strip and, in case of ties, the leftmost one.
Baker, Coffman, and Rivest proved in 1980 that the Bottom-Left Algorithm has
approximation ratio 3 if the rectangles are sorted by decreasing width. For the
past 45 years, no alternative ordering has been found that improves this bound.
We introduce a new rectangle ordering and show that with this ordering the
Bottom-Left Algorithm achieves a 13/6 approximation for the Strip Packing
problem.

</details>


### [32] [Parameterized Approximability for Modular Linear Equations](https://arxiv.org/abs/2509.04976)
*Konrad K. Dabrowski,Peter Jonsson,Sebastian Ordyniak,George Osipov,Magnus Wahlström*

Main category: cs.DS

TL;DR: 研究Min - $r$-Lin($Z_m$)问题的参数化近似，证明Min - $2$-Lin($Z_{p^n}$)可在2倍因子内FPT近似，给出算法及两个下界。


<details>
  <summary>Details</summary>
Motivation: Min - $r$-Lin($Z_m$)问题是NP难且UGC难在多项式时间内常数因子近似，关注以解大小为参数的参数化近似。

Method: 通过求解问题的逐步收紧的松弛问题，构建图并利用影子去除策略计算解。

Result: Min - $2$-Lin($Z_{p^n}$)可在2倍因子内FPT近似，Min - $2$-Lin($Z_m$)可在$2\omega(m)$倍因子内FPT近似，给出两个下界。

Conclusion: 得到Min - $2$-Lin($Z_{p^n}$)的近似算法，同时排除了Min - $3$-Lin($R$)和部分Min - $2$-Lin($R$)的常数因子FPT近似。

Abstract: We consider the Min-$r$-Lin$(Z_m)$ problem: given a system $S$ of length-$r$
linear equations modulo $m$, find $Z \subseteq S$ of minimum cardinality such
that $S-Z$ is satisfiable. The problem is NP-hard and UGC-hard to approximate
in polynomial time within any constant factor even when $r = m = 2$. We focus
on parameterized approximation with solution size as the parameter. Dabrowski
et al. showed that Min-$2$-Lin$(Z_m)$ is in FPT if $m$ is prime (i.e. $Z_m$ is
a field), and it is W[1]-hard if $m$ is not a prime power. We show that
Min-$2$-Lin$(Z_{p^n})$ is FPT-approximable within a factor of $2$ for every
prime $p$ and integer $n \geq 2$. This implies that Min-$2$-Lin$(Z_m)$, $m \in
Z^+$, is FPT-approximable within a factor of $2\omega(m)$ where $\omega(m)$
counts the number of distinct prime divisors of $m$. The idea behind the
algorithm is to solve ever tighter relaxations of the problem, decreasing the
set of possible values for the variables at each step. Working over $Z_{p^n}$
and viewing the values in base-$p$, one can roughly think of a relaxation as
fixing the number of trailing zeros and the least significant nonzero digits of
the values assigned to the variables. To solve the relaxed problem, we
construct a certain graph where solutions can be identified with a particular
collection of cuts. The relaxation may hide obstructions that will only become
visible in the next iteration of the algorithm, which makes it difficult to
find optimal solutions. To deal with this, we use a strategy based on shadow
removal to compute solutions that (1) cost at most twice as much as the optimum
and (2) allow us to reduce the set of values for all variables simultaneously.
We complement the algorithmic result with two lower bounds, ruling out
constant-factor FPT-approximation for Min-$3$-Lin$(R)$ over any nontrivial ring
$R$ and for Min-$2$-Lin$(R)$ over some finite commutative rings $R$.

</details>


### [33] [Graph Reconstruction with a Connected Components Oracle](https://arxiv.org/abs/2509.05002)
*Juha Harviainen,Pekka Parviainen*

Main category: cs.DS

TL;DR: 研究图重建问题中新型连通分量（CC）查询预言机，给出自适应随机算法查询次数上界和下界。


<details>
  <summary>Details</summary>
Motivation: 刻画不同预言机在以查询次数衡量算法复杂度时的能力强弱。

Method: 研究新型返回查询顶点子集诱导子图连通分量集的预言机，使用自适应随机算法。

Result: 1. 对有n个顶点、m条边、最大度Δ和树宽k的隐藏图，可通过自适应随机算法在O(min{m, Δ², k²} · log n)次CC查询内解决GR问题。2. 任何算法无法在o(min{m, Δ², k²})次CC查询内解决GR问题。

Conclusion: 得出在CC查询预言机下解决图重建问题的查询次数上界和下界。

Abstract: In the Graph Reconstruction (GR) problem, the goal is to recover a hidden
graph by utilizing some oracle that provides limited access to the structure of
the graph. The interest is in characterizing how strong different oracles are
when the complexity of an algorithm is measured in the number of performed
queries. We study a novel oracle that returns the set of connected components
(CC) on the subgraph induced by the queried subset of vertices. Our main
contributions are as follows:
  1. For a hidden graph with $n$ vertices, $m$ edges, maximum degree $\Delta$,
and treewidth $k$, GR can be solved in $O(\min\{m, \Delta^2, k^2\} \cdot \log
n)$ CC queries by an adaptive randomized algorithm.
  2. For a hidden graph with $n$ vertices, $m$ edges, maximum degree $\Delta$,
and treewidth $k$, no algorithm can solve GR in $o(\min\{m, \Delta^2, k^2\})$
CC queries.

</details>


### [34] [On approximating the $f$-divergence between two Ising models](https://arxiv.org/abs/2509.05016)
*Weiming Feng,Yucheng Fu*

Main category: cs.DS

TL;DR: 本文研究近似两个Ising模型之间的f - 散度问题，对特定的χ^α - 散度给出算法和难度结果，算法可扩展到其他f - 散度。


<details>
  <summary>Details</summary>
Motivation: 研究近似两个Ising模型之间的f - 散度，是对近期近似TV - 距离工作的推广。

Method: 针对常数整数α的χ^α - 散度，建立算法和难度结果，且算法工作的参数范围与难度结果匹配。

Result: 对常数整数α的χ^α - 散度建立了算法和难度结果，算法可扩展到α - 散度、Kullback - Leibler散度等其他f - 散度。

Conclusion: 成功研究了近似两个Ising模型之间f - 散度的问题，并得到相应算法和可扩展性结果。

Abstract: The $f$-divergence is a fundamental notion that measures the difference
between two distributions. In this paper, we study the problem of approximating
the $f$-divergence between two Ising models, which is a generalization of
recent work on approximating the TV-distance. Given two Ising models $\nu$ and
$\mu$, which are specified by their interaction matrices and external fields,
the problem is to approximate the $f$-divergence $D_f(\nu\,\|\,\mu)$ within an
arbitrary relative error $\mathrm{e}^{\pm \varepsilon}$. For
$\chi^\alpha$-divergence with a constant integer $\alpha$, we establish both
algorithmic and hardness results. The algorithm works in a parameter regime
that matches the hardness result. Our algorithm can be extended to other
$f$-divergences such as $\alpha$-divergence, Kullback-Leibler divergence,
R\'enyi divergence, Jensen-Shannon divergence, and squared Hellinger distance.

</details>


### [35] [Testing Depth First Search Numbering](https://arxiv.org/abs/2509.05132)
*Artur Czumaj,Christian Sohler,Stefan Walzer*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Property Testing is a formal framework to study the computational power and
complexity of sampling from combinatorial objects. A central goal in standard
graph property testing is to understand which graph properties are testable
with sublinear query complexity. Here, a graph property P is testable with a
sublinear query complexity if there is an algorithm that makes a sublinear
number of queries to the input graph and accepts with probability at least 2/3,
if the graph has property P, and rejects with probability at least 2/3 if it is
$\varepsilon$-far from every graph that has property P.
  In this paper, we introduce a new variant of the bounded degree graph model.
In this variant, in addition to the standard representation of a bounded degree
graph, we assume that every vertex $v$ has a unique label num$(v)$ from $\{1,
\dots, |V|\}$, and in addition to the standard queries in the bounded degree
graph model, we also allow a property testing algorithm to query for the label
of a vertex (but not for a vertex with a given label).
  Our new model is motivated by certain graph processes such as a DFS
traversal, which assign consecutive numbers (labels) to the vertices of the
graph. We want to study which of these numberings can be tested in sublinear
time. As a first step in understanding such a model, we develop a
\emph{property testing algorithm for discovery times of a DFS traversal} with
query complexity $O(n^{1/3}/\varepsilon)$ and for constant $\varepsilon>0$ we
give a matching lower bound.

</details>


### [36] [Efficient Contractions of Dynamic Graphs -- with Applications](https://arxiv.org/abs/2509.05157)
*Monika Henzinger,Evangelos Kosinas,Robin Münk,Harald Räcke*

Main category: cs.DS

TL;DR: 本文提出用于全动态图的灵活数据结构，可在更新序列的任意点高效提供非平凡最小割（NMC）稀疏图，还讨论了该数据结构的两个应用。


<details>
  <summary>Details</summary>
Motivation: 为全动态图设计能在更新过程中任意时刻高效提供NMC稀疏图的数据结构。

Method: 采用简单动态森林数据结构，在查询时快速从头构建稀疏图，根据对手强度和时间界限类型提供不同保证。

Result: 数据结构支持边的插入/删除操作，能在一定时间内返回NMC稀疏图；可用于高效报告最小割的仙人掌表示和计算最大k - 边连通子图。

Conclusion: 所提数据结构在全动态图处理中有良好表现，在相关应用中能提升效率。

Abstract: A non-trivial minimum cut (NMC) sparsifier is a multigraph $\hat{G}$ that
preserves all non-trivial minimum cuts of a given undirected graph $G$. We
introduce a flexible data structure for fully dynamic graphs that can
efficiently provide an NMC sparsifier upon request at any point during the
sequence of updates. We employ simple dynamic forest data structures to achieve
a fast from-scratch construction of the sparsifier at query time. Based on the
strength of the adversary and desired type of time bounds, the data structure
comes with different guarantees. Specifically, let $G$ be a fully dynamic
simple graph with $n$ vertices and minimum degree $\delta$. Then our data
structure supports an insertion/deletion of an edge to/from $G$ in $n^{o(1)}$
worst-case time. Furthermore, upon request, it can return w.h.p. an NMC
sparsifier of $G$ that has $O(n/\delta)$ vertices and $O(n)$ edges, in
$\hat{O}(n)$ time. The probabilistic guarantees hold against an adaptive
adversary. Alternatively, the update and query times can be improved to
$\tilde{O}(1)$ and $\tilde{O}(n)$ respectively, if amortized-time guarantees
are sufficient, or if the adversary is oblivious.
  We discuss two applications of our data structure. First, it can be used to
efficiently report a cactus representation of all minimum cuts of a fully
dynamic simple graph. Using the NMC sparsifier we can w.h.p. build this cactus
in worst-case time $\hat{O}(n)$ against an adaptive adversary. Second, our data
structure allows us to efficiently compute the maximal $k$-edge-connected
subgraphs of undirected simple graphs, by repeatedly applying a minimum cut
algorithm on the NMC sparsifier. Specifically, we can compute w.h.p. the
maximal $k$-edge-connected subgraphs of a simple graph with $n$ vertices and
$m$ edges in $\tilde{O}(m+n^2/k)$ time which is an improvement for $k =
\Omega(n^{1/8})$ and works for fully dynamic graphs.

</details>


### [37] [List Decoding Expander-Based Codes via Fast Approximation of Expanding CSPs: I](https://arxiv.org/abs/2509.05203)
*Fernando Granha Jeronimo,Aman Singh*

Main category: cs.DS

TL;DR: 本文为基于扩张器的码构造提出近线性时间列表译码算法，给出三种码的列表译码结果并说明所用方法。


<details>
  <summary>Details</summary>
Motivation: 为基于扩张器的码构造设计高效的列表译码算法。

Method: 将译码任务表述为扩张图上的一致性CSP问题，使用基于弱正则分解的q元扩张CSP的快速近似算法，枚举分解各部分中的常量赋值。

Result: 给出三种码的列表译码结果，包括Tanner LDPC码、AEL码在不同参数下的列表译码能力、时间复杂度和字母表大小。

Conclusion: 通过特定方法能为基于扩张器的码构造实现近线性时间的列表译码。

Abstract: We present near-linear time list decoding algorithms (in the block-length
$n$) for expander-based code constructions. More precisely, we show that
  (i) For every $\delta \in (0,1)$ and $\epsilon > 0$, there is an explicit
family of good Tanner LDPC codes of (design) distance $\delta$ that is $(\delta
- \epsilon, O_\varepsilon(1))$ list decodable in time
$\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size $O_\delta(1)$,
  (ii) For every $R \in (0,1)$ and $\epsilon > 0$, there is an explicit family
of AEL codes of rate $R$, distance $1-R -\varepsilon$ that is $(1-R-\epsilon,
O_\varepsilon(1))$ list decodable in time
$\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size
$\text{exp}(\text{poly}(1/\epsilon))$, and
  (iii) For every $R \in (0,1)$ and $\epsilon > 0$, there is an explicit family
of AEL codes of rate $R$, distance $1-R-\varepsilon$ that is $(1-R-\epsilon,
O(1/\epsilon))$ list decodable in time
$\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size
$\text{exp}(\text{exp}(\text{poly}(1/\epsilon)))$ using recent near-optimal
list size bounds from [JMST25].
  Our results are obtained by phrasing the decoding task as an agreement CSP
[RWZ20,DHKNT19] on expander graphs and using the fast approximation algorithm
for $q$-ary expanding CSPs from [Jer23], which is based on weak regularity
decomposition [JST21,FK96]. Similarly to list decoding $q$-ary Ta-Shma's codes
in [Jer23], we show that it suffices to enumerate over assignments that are
constant in each part (of the constantly many) of the decomposition in order to
recover all codewords in the list.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [38] [Unified Representation Learning for Multi-Intent Diversity and Behavioral Uncertainty in Recommender Systems](https://arxiv.org/abs/2509.04694)
*Wei Xu,Jiasen Zheng,Junjiang Lin,Mingxuan Han,Junliang Du*

Main category: cs.IR

TL;DR: 本文提出统一表征学习框架应对推荐系统中用户意图多样性和行为不确定性的挑战，实验显示该方法优于现有模型，具有有效性和实用价值。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中联合建模用户意图多样性和行为不确定性的挑战。

Method: 构建多意图表征模块和不确定性建模机制，用贝叶斯分布建模捕捉行为模糊性和偏好波动，引入多个潜在意图向量并用注意力机制加权融合，通过高斯分布学习行为表征的均值和协方差，使用可学习的融合策略结合长期意图和短期行为信号。

Result: 在标准公共数据集上评估，在多个指标上优于现有代表性模型，在冷启动和行为干扰场景下更稳定、适应性更强。

Conclusion: 统一建模策略在现实推荐任务中有效且有实用价值，缓解了传统方法处理复杂用户行为的建模瓶颈。

Abstract: This paper addresses the challenge of jointly modeling user intent diversity
and behavioral uncertainty in recommender systems. A unified representation
learning framework is proposed. The framework builds a multi-intent
representation module and an uncertainty modeling mechanism. It extracts
multi-granularity interest structures from user behavior sequences. Behavioral
ambiguity and preference fluctuation are captured using Bayesian distribution
modeling. In the multi-intent modeling part, the model introduces multiple
latent intent vectors. These vectors are weighted and fused using an attention
mechanism to generate semantically rich representations of long-term user
preferences. In the uncertainty modeling part, the model learns the mean and
covariance of behavior representations through Gaussian distributions. This
reflects the user's confidence in different behavioral contexts. Next, a
learnable fusion strategy is used to combine long-term intent and short-term
behavior signals. This produces the final user representation, improving both
recommendation accuracy and robustness. The method is evaluated on standard
public datasets. Experimental results show that it outperforms existing
representative models across multiple metrics. It also demonstrates greater
stability and adaptability under cold-start and behavioral disturbance
scenarios. The approach alleviates modeling bottlenecks faced by traditional
methods when dealing with complex user behavior. These findings confirm the
effectiveness and practical value of the unified modeling strategy in
real-world recommendation tasks.

</details>


### [39] [Multimodal Foundation Model-Driven User Interest Modeling and Behavior Analysis on Short Video Platforms](https://arxiv.org/abs/2509.04751)
*Yushang Zhao,Yike Peng,Li Zhang,Qianyi Sun,Zhihui Zhang,Yingying Zhuang*

Main category: cs.IR

TL;DR: 提出基于多模态基础模型的用户兴趣建模与行为分析框架，实验显示能提升推荐性能并增强系统透明度和可控性。


<details>
  <summary>Details</summary>
Motivation: 传统兴趣建模方法依赖单模态数据，难以在复杂多模态内容环境中捕捉用户偏好。

Method: 用跨模态对齐策略将视频帧、文本描述和背景音乐整合到统一语义空间构建用户兴趣向量，引入行为驱动特征嵌入机制建模动态兴趣演变。

Result: 在行为预测准确性、冷启动用户兴趣建模和推荐点击率上有显著提升，能通过注意力权重和特征可视化揭示模型决策依据。

Conclusion: 所提框架能有效提升推荐系统性能，增强透明度和可控性。

Abstract: With the rapid expansion of user bases on short video platforms, personalized
recommendation systems are playing an increasingly critical role in enhancing
user experience and optimizing content distribution. Traditional interest
modeling methods often rely on unimodal data, such as click logs or text
labels, which limits their ability to fully capture user preferences in a
complex multimodal content environment. To address this challenge, this paper
proposes a multimodal foundation model-based framework for user interest
modeling and behavior analysis. By integrating video frames, textual
descriptions, and background music into a unified semantic space using
cross-modal alignment strategies, the framework constructs fine-grained user
interest vectors. Additionally, we introduce a behavior-driven feature
embedding mechanism that incorporates viewing, liking, and commenting sequences
to model dynamic interest evolution, thereby improving both the timeliness and
accuracy of recommendations. In the experimental phase, we conduct extensive
evaluations using both public and proprietary short video datasets, comparing
our approach against multiple mainstream recommendation algorithms and modeling
techniques. Results demonstrate significant improvements in behavior prediction
accuracy, interest modeling for cold-start users, and recommendation
click-through rates. Moreover, we incorporate interpretability mechanisms using
attention weights and feature visualization to reveal the model's decision
basis under multimodal inputs and trace interest shifts, thereby enhancing the
transparency and controllability of the recommendation system.

</details>


### [40] [Fishing for Answers: Exploring One-shot vs. Iterative Retrieval Strategies for Retrieval Augmented Generation](https://arxiv.org/abs/2509.04820)
*Huifeng Lin,Gang Su,Jintao Liang,You Wu,Rui Zhao,Ziyue Li*

Main category: cs.IR

TL;DR: 本文针对基于大语言模型的检索增强生成（RAG）在法律和监管领域复杂问答任务中的检索瓶颈，提出两种策略改进证据覆盖和答案质量，并通过实验为实际应用提供见解和指导。


<details>
  <summary>Details</summary>
Motivation: 基本的RAG在法律和监管领域复杂QA任务中存在检索瓶颈，top - k策略常导致答案不完整或不准确，需要改进。

Method: 一是One - SHOT检索方法，根据令牌预算自适应选择块并设计模块过滤和细化；二是基于推理代理RAG框架的迭代检索策略，设计模块解决查询漂移和检索惰性问题。

Result: 文中未提及具体实验结果。

Conclusion: 旨在为法律和监管领域的实际应用提供实用见解和指导。

Abstract: Retrieval-Augmented Generation (RAG) based on Large Language Models (LLMs) is
a powerful solution to understand and query the industry's closed-source
documents. However, basic RAG often struggles with complex QA tasks in legal
and regulatory domains, particularly when dealing with numerous government
documents. The top-$k$ strategy frequently misses golden chunks, leading to
incomplete or inaccurate answers. To address these retrieval bottlenecks, we
explore two strategies to improve evidence coverage and answer quality. The
first is a One-SHOT retrieval method that adaptively selects chunks based on a
token budget, allowing as much relevant content as possible to be included
within the model's context window. Additionally, we design modules to further
filter and refine the chunks. The second is an iterative retrieval strategy
built on a Reasoning Agentic RAG framework, where a reasoning LLM dynamically
issues search queries, evaluates retrieved results, and progressively refines
the context over multiple turns. We identify query drift and retrieval laziness
issues and further design two modules to tackle them. Through extensive
experiments on a dataset of government documents, we aim to offer practical
insights and guidance for real-world applications in legal and regulatory
domains.

</details>


### [41] [Hybrid Matrix Factorization Based Graph Contrastive Learning for Recommendation System](https://arxiv.org/abs/2509.05115)
*Hao Chen,Wenming Ma,Zihao Chu,Mingqi Li*

Main category: cs.IR

TL;DR: 提出HMFGCL方法结合两种矩阵分解技术构建增强视图，在多公开数据集上表现优于现有基线，在小数据集上更明显。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法的两种数据增强策略不能充分捕捉用户 - 项目交互信息，需新方法。

Method: 提出HMFGCL方法，集成低秩矩阵分解和奇异值分解两种矩阵分解技术互补获取全局协作信息以构建增强视图。

Result: 在多个公共数据集上实验，模型优于现有基线，在小数据集上更突出。

Conclusion: HMFGCL方法有效，能在推荐系统中取得更好性能，尤其适用于小数据集。

Abstract: In recent years, methods that combine contrastive learning with graph neural
networks have emerged to address the challenges of recommendation systems,
demonstrating powerful performance and playing a significant role in this
domain. Contrastive learning primarily tackles the issue of data sparsity by
employing data augmentation strategies, effectively alleviating this problem
and showing promising results. Although existing research has achieved
favorable outcomes, most current graph contrastive learning methods are based
on two types of data augmentation strategies: the first involves perturbing the
graph structure, such as by randomly adding or removing edges; and the second
applies clustering techniques. We believe that the interactive information
obtained through these two strategies does not fully capture the user-item
interactions. In this paper, we propose a novel method called HMFGCL (Hybrid
Matrix Factorization Based Graph Contrastive Learning), which integrates two
distinct matrix factorization techniques-low-rank matrix factorization (MF) and
singular value decomposition (SVD)-to complementarily acquire global
collaborative information, thereby constructing enhanced views. Experimental
results on multiple public datasets demonstrate that our model outperforms
existing baselines, particularly on small-scale datasets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Q-SafeML: Safety Assessment of Quantum Machine Learning via Quantum Distance Metrics](https://arxiv.org/abs/2509.04536)
*Oliver Dunn,Koorosh Aslansefat,Yiannis Papadopoulos*

Main category: cs.LG

TL;DR: 本文介绍了用于量子机器学习（QML）的安全监控方法Q - SafeML，通过实验证明其能增强系统透明度和安全性。


<details>
  <summary>Details</summary>
Motivation: 机器学习在安全关键系统中兴起，量子计算发展催生QML，但现有安全监控方法不适用于QML，且QML专用安全机制欠发达。

Method: 基于SafeML方法，采用适应量子特性的距离度量，从依赖数据集和与分类器无关的经典SafeML，转变为依赖模型的分类后评估。

Result: 在QCNN和VQC模型上的实验表明，Q - SafeML能检测操作数据与训练数据间的距离，处理概念漂移。

Conclusion: Q - SafeML能实现人为监督，增强系统透明度和安全性。

Abstract: The rise of machine learning in safety-critical systems has paralleled
advancements in quantum computing, leading to the emerging field of Quantum
Machine Learning (QML). While safety monitoring has progressed in classical ML,
existing methods are not directly applicable to QML due to fundamental
differences in quantum computation. Given the novelty of QML, dedicated safety
mechanisms remain underdeveloped. This paper introduces Q-SafeML, a safety
monitoring approach for QML. The method builds on SafeML, a recent method that
utilizes statistical distance measures to assess model accuracy and provide
confidence in the reasoning of an algorithm. An adapted version of Q-SafeML
incorporates quantum-centric distance measures, aligning with the probabilistic
nature of QML outputs. This shift to a model-dependent, post-classification
evaluation represents a key departure from classical SafeML, which is
dataset-driven and classifier-agnostic. The distinction is motivated by the
unique representational constraints of quantum systems, requiring distance
metrics defined over quantum state spaces. Q-SafeML detects distances between
operational and training data addressing the concept drifts in the context of
QML. Experiments on QCNN and VQC Models show that this enables informed human
oversight, enhancing system transparency and safety.

</details>


### [43] [Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families](https://arxiv.org/abs/2509.04622)
*Jialin Wu,Shreya Saha,Yiqing Bo,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 提出定量框架评估表征相似性度量，比较常用度量的判别能力，为模型和大脑比较指导度量选择。


<details>
  <summary>Details</summary>
Motivation: 缺乏对不同模型族表征相似性度量判别能力的系统比较。

Method: 引入定量框架，用信号检测论的dprime、轮廓系数和ROC - AUC三种互补的可分离性度量，评估常用度量的判别能力。

Result: 可分离性随度量施加的对齐约束增强而系统增加；基于映射的方法中，软匹配可分离性最高，其次是普罗克拉斯提斯对齐和线性预测性；非拟合方法如RSA也有强可分离性。

Conclusion: 首次从可分离性角度系统比较相似性度量，明确其相对敏感性，指导度量选择。

Abstract: Representational similarity metrics are fundamental tools in neuroscience and
AI, yet we lack systematic comparisons of their discriminative power across
model families. We introduce a quantitative framework to evaluate
representational similarity measures based on their ability to separate model
families-across architectures (CNNs, Vision Transformers, Swin Transformers,
ConvNeXt) and training regimes (supervised vs. self-supervised). Using three
complementary separability measures-dprime from signal detection theory,
silhouette coefficients and ROC-AUC, we systematically assess the
discriminative capacity of commonly used metrics including RSA, linear
predictivity, Procrustes, and soft matching. We show that separability
systematically increases as metrics impose more stringent alignment
constraints. Among mapping-based approaches, soft-matching achieves the highest
separability, followed by Procrustes alignment and linear predictivity.
Non-fitting methods such as RSA also yield strong separability across families.
These results provide the first systematic comparison of similarity metrics
through a separability lens, clarifying their relative sensitivity and guiding
metric choice for large-scale model and brain comparisons.

</details>


### [44] [Finance-Grounded Optimization For Algorithmic Trading](https://arxiv.org/abs/2509.04541)
*Kasymkhan Khubiev,Mikhail Semenov,Irina Podlipnova*

Main category: cs.LG

TL;DR: 本文引入金融相关损失函数和换手率正则化方法，用于金融领域深度学习，结果表明这些方法在交易指标评估下优于传统均方误差损失。


<details>
  <summary>Details</summary>
Motivation: 深度学习在金融领域应用有挑战，经典方法不适用于金融世界，专家需不同指标评估模型性能。

Method: 引入基于关键量化金融指标的损失函数，如夏普比率、损益和最大回撤；提出换手率正则化方法，限制生成头寸的换手率。

Result: 提出的损失函数结合换手率正则化在算法交易指标评估下，在回报预测任务中优于传统均方误差损失。

Conclusion: 金融相关指标能提升交易策略和投资组合优化的预测性能。

Abstract: Deep Learning is evolving fast and integrates into various domains. Finance
is a challenging field for deep learning, especially in the case of
interpretable artificial intelligence (AI). Although classical approaches
perform very well with natural language processing, computer vision, and
forecasting, they are not perfect for the financial world, in which specialists
use different metrics to evaluate model performance.
  We first introduce financially grounded loss functions derived from key
quantitative finance metrics, including the Sharpe ratio, Profit-and-Loss
(PnL), and Maximum Draw down. Additionally, we propose turnover regularization,
a method that inherently constrains the turnover of generated positions within
predefined limits.
  Our findings demonstrate that the proposed loss functions, in conjunction
with turnover regularization, outperform the traditional mean squared error
loss for return prediction tasks when evaluated using algorithmic trading
metrics. The study shows that financially grounded metrics enhance predictive
performance in trading strategies and portfolio optimization.

</details>


### [45] [Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks](https://arxiv.org/abs/2509.05273)
*Jason Gardner,Ayan Dutta,Swapnoneel Roy,O. Patrick Kreidl,Ladislau Boloni*

Main category: cs.LG

TL;DR: 本文对七种深度强化学习算法进行能耗基准测试，发现算法间能效和训练成本差异大，为开发节能高效的DRL实践提供见解。


<details>
  <summary>Details</summary>
Motivation: 深度学习计算需求增长带来环境和经济成本问题，而DRL算法的能耗、排放和成本研究不足。

Method: 使用Stable Baselines实现七种DRL算法，在十个Atari 2600游戏上训练一百万步，实时测量功耗以估算能耗、排放和成本。

Result: 算法间能效和训练成本差异大，部分算法能耗低24%，排放少68%，成本低68%。

Conclusion: 该研究为开发节能高效的DRL实践提供可行见解，为将可持续性纳入算法设计和评估奠定基础。

Abstract: The growing computational demands of deep reinforcement learning (DRL) have
raised concerns about the environmental and economic costs of training
large-scale models. While algorithmic efficiency in terms of learning
performance has been extensively studied, the energy requirements, greenhouse
gas emissions, and monetary costs of DRL algorithms remain largely unexplored.
In this work, we present a systematic benchmarking study of the energy
consumption of seven state-of-the-art DRL algorithms, namely DQN, TRPO, A2C,
ARS, PPO, RecurrentPPO, and QR-DQN, implemented using Stable Baselines. Each
algorithm was trained for one million steps each on ten Atari 2600 games, and
power consumption was measured in real-time to estimate total energy usage,
CO2-Equivalent emissions, and electricity cost based on the U.S. national
average electricity price. Our results reveal substantial variation in energy
efficiency and training cost across algorithms, with some achieving comparable
performance while consuming up to 24% less energy (ARS vs. DQN), emitting
nearly 68% less CO2, and incurring almost 68% lower monetary cost (QR-DQN vs.
RecurrentPPO) than less efficient counterparts. We further analyze the
trade-offs between learning performance, training time, energy use, and
financial cost, highlighting cases where algorithmic choices can mitigate
environmental and economic impact without sacrificing learning performance.
This study provides actionable insights for developing energy-aware and
cost-efficient DRL practices and establishes a foundation for incorporating
sustainability considerations into future algorithmic design and evaluation.

</details>


### [46] [Flexible inference of learning rules from de novo learning data using neural networks](https://arxiv.org/abs/2509.04661)
*Yuhan Helena Liu,Victor Geadah,Jonathan Pillow*

Main category: cs.LG

TL;DR: 本文提出灵活框架从动物行为数据推断生物学习规则，应用于小鼠数据集改进预测并揭示学习特点。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推断动物从头学习新行为的学习规则方面存在局限，需要新方法解决该问题。

Method: 提出用深度神经网络参数化策略权重逐次试验更新的非参数框架，后扩展到循环变体，以捕捉非马尔可夫动态。

Result: 模型应用于小鼠数据集改进了对保留数据的预测，推断规则揭示了正确和错误试验后的不对称更新以及历史依赖性。

Conclusion: 引入了从从头学习任务的行为数据推断生物学习规则的灵活框架，为实验训练协议和行为数字孪生的发展提供见解。

Abstract: Understanding how animals learn is a central challenge in neuroscience, with
growing relevance to the development of animal- or human-aligned artificial
intelligence. However, most existing approaches assume specific parametric
forms for the learning rule (e.g., Q-learning, policy gradient) or are limited
to simplified settings like bandit tasks, which do not involve learning a new
input-output mapping from scratch. In contrast, animals must often learn new
behaviors de novo, which poses a rich challenge for learning-rule inference. We
target this problem by inferring learning rules directly from animal
decision-making data during de novo task learning, a setting that requires
models flexible enough to capture suboptimality, history dependence, and rich
external stimulus integration without strong structural priors. We first
propose a nonparametric framework that parameterizes the per-trial update of
policy weights with a deep neural network (DNN), and validate it by recovering
ground-truth rules in simulation. We then extend to a recurrent variant (RNN)
that captures non-Markovian dynamics by allowing updates to depend on trial
history. Applied to a large behavioral dataset of mice learning a sensory
decision-making task over multiple weeks, our models improved predictions on
held-out data. The inferred rules revealed asymmetric updates after correct
versus error trials and history dependence, consistent with non-Markovian
learning. Overall, these results introduce a flexible framework for inferring
biological learning rules from behavioral data in de novo learning tasks,
providing insights to inform experimental training protocols and the
development of behavioral digital twins.

</details>


### [47] [i-Mask: An Intelligent Mask for Breath-Driven Activity Recognition](https://arxiv.org/abs/2509.04544)
*Ashutosh Kumar Sinha,Ayush Patel,Mitul Dudhat,Pritam Anand,Rahul Mishra*

Main category: cs.LG

TL;DR: 提出i - Mask用于人类活动识别，利用呼气模式，实验准确率超95%。


<details>
  <summary>Details</summary>
Motivation: 呼吸模式含重要生理信号，人类活动识别与生命体征相关，可用于健康监测。

Method: 开发带集成传感器的面罩收集呼气模式数据，对数据进行噪声过滤、时间序列分解和标注，训练预测模型。

Result: 实验验证该方法有效，准确率超95%。

Conclusion: i - Mask方法在医疗保健和健身应用有潜力。

Abstract: The patterns of inhalation and exhalation contain important physiological
signals that can be used to anticipate human behavior, health trends, and vital
parameters. Human activity recognition (HAR) is fundamentally connected to
these vital signs, providing deeper insights into well-being and enabling
real-time health monitoring. This work presents i-Mask, a novel HAR approach
that leverages exhaled breath patterns captured using a custom-developed mask
equipped with integrated sensors. Data collected from volunteers wearing the
mask undergoes noise filtering, time-series decomposition, and labeling to
train predictive models. Our experimental results validate the effectiveness of
the approach, achieving over 95\% accuracy and highlighting its potential in
healthcare and fitness applications.

</details>


### [48] [Bootstrapping Task Spaces for Self-Improvement](https://arxiv.org/abs/2509.04575)
*Minqi Jiang,Andrei Lupu,Yoram Bachrach*

Main category: cs.LG

TL;DR: 提出Exploratory Iteration (ExIt)方法，利用自改进任务的循环结构训练大语言模型，在多个领域展示出良好的推理时自改进能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在训练能在推理时自我改进的智能体时假设固定最大迭代深度，既昂贵又随意，需要更好的方法。

Method: 提出ExIt，是一类自动课程强化学习方法，通过选择性采样最有信息的中间部分历史来扩展任务空间，还可与显式探索机制结合。

Result: 在竞争数学、多轮工具使用和机器学习工程等多个领域，ExIt策略能产生在保留任务实例上有强推理时自改进能力的策略，且能在超出训练平均迭代深度的步数预算内迭代至更高性能。

Conclusion: ExIt方法有效，可让大语言模型在推理时进行多步自我改进。

Abstract: Progress in many task domains emerges from repeated revisions to previous
solution attempts. Training agents that can reliably self-improve over such
sequences at inference-time is a natural target for reinforcement learning
(RL), yet the naive approach assumes a fixed maximum iteration depth, which can
be both costly and arbitrary. We present Exploratory Iteration (ExIt), a family
of autocurriculum RL methods that directly exploits the recurrent structure of
self-improvement tasks to train LLMs to perform multi-step self-improvement at
inference-time while only training on the most informative single-step
iterations. ExIt grows a task space by selectively sampling the most
informative intermediate, partial histories encountered during an episode for
continued iteration, treating these starting points as new self-iteration task
instances to train a self-improvement policy. ExIt can further pair with
explicit exploration mechanisms to sustain greater task diversity. Across
several domains, encompassing competition math, multi-turn tool-use, and
machine learning engineering, we demonstrate that ExIt strategies, starting
from either a single or many task instances, can produce policies exhibiting
strong inference-time self-improvement on held-out task instances, and the
ability to iterate towards higher performance over a step budget extending
beyond the average iteration depth encountered during training.

</details>


### [49] [Instance-Wise Adaptive Sampling for Dataset Construction in Approximating Inverse Problem Solutions](https://arxiv.org/abs/2509.04583)
*Jiequn Han,Kui Ren,Nathan Soedjak*

Main category: cs.LG

TL;DR: 提出实例自适应采样框架构建紧凑且信息丰富的训练数据集，在逆散射问题中证明有效，策略可扩展到其他逆问题。


<details>
  <summary>Details</summary>
Motivation: 典型基于学习的方法在高本征维度先验或高精度要求时需大量训练样本，导致数据收集成本高。

Method: 基于特定测试实例动态分配采样工作，通过迭代细化训练数据集，使其适应每个测试实例周围逆映射的几何结构。

Result: 在两种结构化先验的逆散射问题中证明了方法有效性，在更复杂先验或更高精度要求下优势更明显。

Conclusion: 自适应采样策略具有广泛适用性，是传统固定数据集训练机制的可扩展实用替代方案。

Abstract: We propose an instance-wise adaptive sampling framework for constructing
compact and informative training datasets for supervised learning of inverse
problem solutions. Typical learning-based approaches aim to learn a
general-purpose inverse map from datasets drawn from a prior distribution, with
the training process independent of the specific test instance. When the prior
has a high intrinsic dimension or when high accuracy of the learned solution is
required, a large number of training samples may be needed, resulting in
substantial data collection costs. In contrast, our method dynamically
allocates sampling effort based on the specific test instance, enabling
significant gains in sample efficiency. By iteratively refining the training
dataset conditioned on the latest prediction, the proposed strategy tailors the
dataset to the geometry of the inverse map around each test instance. We
demonstrate the effectiveness of our approach in the inverse scattering problem
under two types of structured priors. Our results show that the advantage of
the adaptive method becomes more pronounced in settings with more complex
priors or higher accuracy requirements. While our experiments focus on a
particular inverse problem, the adaptive sampling strategy is broadly
applicable and readily extends to other inverse problems, offering a scalable
and practical alternative to conventional fixed-dataset training regimes.

</details>


### [50] [Fundamental bounds on efficiency-confidence trade-off for transductive conformal prediction](https://arxiv.org/abs/2509.04631)
*Arash Behboodi,Alvaro H. C. Correia,Fabio Valerio Massoli,Christos Louizos*

Main category: cs.LG

TL;DR: 本文研究直推式共形预测中置信度与效率的权衡，推导有限样本界，证明其在理想情况下可实现，还研究特殊情况并给出渐近最优置信预测器。


<details>
  <summary>Details</summary>
Motivation: 研究直推式共形预测中同时对多个数据点进行预测时，在给定置信水平下构建预测集，探讨置信度和效率之间的权衡。

Method: 推导严格的有限样本界，分析界中的各项（如条件熵、分散度），研究特殊情况并进行误差指数分析。

Result: 得出非平凡置信水平会使具有内在不确定性的数据的预测集大小呈指数增长，该界在理想情况下可实现，特殊情况可转化为假设检验问题并给出渐近最优置信预测器。

Conclusion: 揭示了直推式共形预测中置信度与效率的权衡关系，为该领域的研究提供了理论支持和方法参考。

Abstract: Transductive conformal prediction addresses the simultaneous prediction for
multiple data points. Given a desired confidence level, the objective is to
construct a prediction set that includes the true outcomes with the prescribed
confidence. We demonstrate a fundamental trade-off between confidence and
efficiency in transductive methods, where efficiency is measured by the size of
the prediction sets. Specifically, we derive a strict finite-sample bound
showing that any non-trivial confidence level leads to exponential growth in
prediction set size for data with inherent uncertainty. The exponent scales
linearly with the number of samples and is proportional to the conditional
entropy of the data. Additionally, the bound includes a second-order term,
dispersion, defined as the variance of the log conditional probability
distribution. We show that this bound is achievable in an idealized setting.
Finally, we examine a special case of transductive prediction where all test
data points share the same label. We show that this scenario reduces to the
hypothesis testing problem with empirically observed statistics and provide an
asymptotically optimal confidence predictor, along with an analysis of the
error exponent.

</details>


### [51] [An Efficient Subspace Algorithm for Federated Learning on Heterogeneous Data](https://arxiv.org/abs/2509.05213)
*Jiaojiao Zhang,Yuqi Xu,Kun Yuan*

Main category: cs.LG

TL;DR: 本文提出FedSub算法解决联邦学习应用于大规模深度神经网络的挑战，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习应用于大规模深度神经网络时，因客户端数据异质性导致的客户端漂移问题，以及通信、计算和内存成本高的问题。

Method: 提出FedSub算法，利用子空间投影使客户端局部更新在低维子空间内，降低成本，并引入低维对偶变量减轻客户端漂移，还进行了收敛性分析。

Result: 实验结果证明了算法的效率。

Conclusion: FedSub算法能有效解决联邦学习在大规模深度神经网络应用中的挑战。

Abstract: This work addresses the key challenges of applying federated learning to
large-scale deep neural networks, particularly the issue of client drift due to
data heterogeneity across clients and the high costs of communication,
computation, and memory. We propose FedSub, an efficient subspace algorithm for
federated learning on heterogeneous data. Specifically, FedSub utilizes
subspace projection to guarantee local updates of each client within
low-dimensional subspaces, thereby reducing communication, computation, and
memory costs. Additionally, it incorporates low-dimensional dual variables to
mitigate client drift. We provide convergence analysis that reveals the impact
of key factors such as step size and subspace projection matrices on
convergence. Experimental results demonstrate its efficiency.

</details>


### [52] [Toward Faithfulness-guided Ensemble Interpretation of Neural Network](https://arxiv.org/abs/2509.04588)
*Siyu Zhang,Kenneth Mcmillan*

Main category: cs.LG

TL;DR: 本文提出FEI框架提升神经网络解释的忠实性和可解释性，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 可解释和忠实的特定神经推理解释对理解和评估模型行为至关重要，现有方法可能存在不足。

Method: 引入FEI框架，通过平滑近似提高定量忠实性分数，提出评估隐藏层忠实性的新定性指标。

Result: 在大量实验中，FEI在定性可视化和定量忠实性分数上超越现有方法。

Conclusion: 建立了提升神经网络解释忠实性的综合框架，强调广度和精度。

Abstract: Interpretable and faithful explanations for specific neural inferences are
crucial for understanding and evaluating model behavior. Our work introduces
\textbf{F}aithfulness-guided \textbf{E}nsemble \textbf{I}nterpretation
(\textbf{FEI}), an innovative framework that enhances the breadth and
effectiveness of faithfulness, advancing interpretability by providing superior
visualization. Through an analysis of existing evaluation benchmarks,
\textbf{FEI} employs a smooth approximation to elevate quantitative
faithfulness scores. Diverse variations of \textbf{FEI} target enhanced
faithfulness in hidden layer encodings, expanding interpretability.
Additionally, we propose a novel qualitative metric that assesses hidden layer
faithfulness. In extensive experiments, \textbf{FEI} surpasses existing
methods, demonstrating substantial advances in qualitative visualization and
quantitative faithfulness scores. Our research establishes a comprehensive
framework for elevating faithfulness in neural network explanations,
emphasizing both breadth and precision

</details>


### [53] [Quantum-Enhanced Multi-Task Learning with Learnable Weighting for Pharmacokinetic and Toxicity Prediction](https://arxiv.org/abs/2509.04601)
*Han Zhang,Fengji Ma,Jiamin Su,Xinyue Yang,Lei Wang,Wen-Cai Ye,Li Liu*

Main category: cs.LG

TL;DR: 提出量子增强和任务加权多任务学习框架QW - MTL用于ADMET分类任务，实验表明其优于单任务基线。


<details>
  <summary>Details</summary>
Motivation: 现有ADMET预测的单任务学习方法无法充分利用任务间互补性，且需更多计算资源。

Method: 构建基于Chemprop - RDKit骨干的QW - MTL框架，采用量子化学描述符丰富分子表示，引入指数任务加权方案实现动态损失平衡，对13个TDC分类基准进行联合多任务训练。

Result: QW - MTL在13个任务中的12个上显著优于单任务基线，以最小模型复杂度和快速推理实现高预测性能。

Conclusion: 量子信息特征和自适应任务加权增强的多任务分子学习有效且高效。

Abstract: Prediction for ADMET (Absorption, Distribution, Metabolism, Excretion, and
Toxicity) plays a crucial role in drug discovery and development, accelerating
the screening and optimization of new drugs. Existing methods primarily rely on
single-task learning (STL), which often fails to fully exploit the
complementarities between tasks. Besides, it requires more computational
resources while training and inference of each task independently. To address
these issues, we propose a new unified Quantum-enhanced and task-Weighted
Multi-Task Learning (QW-MTL) framework, specifically designed for ADMET
classification tasks. Built upon the Chemprop-RDKit backbone, QW-MTL adopts
quantum chemical descriptors to enrich molecular representations with
additional information about the electronic structure and interactions.
Meanwhile, it introduces a novel exponential task weighting scheme that
combines dataset-scale priors with learnable parameters to achieve dynamic loss
balancing across tasks. To the best of our knowledge, this is the first work to
systematically conduct joint multi-task training across all 13 Therapeutics
Data Commons (TDC) classification benchmarks, using leaderboard-style data
splits to ensure a standardized and realistic evaluation setting. Extensive
experimental results show that QW-MTL significantly outperforms single-task
baselines on 12 out of 13 tasks, achieving high predictive performance with
minimal model complexity and fast inference, demonstrating the effectiveness
and efficiency of multi-task molecular learning enhanced by quantum-informed
features and adaptive task weighting.

</details>


### [54] [Split Conformal Prediction in the Function Space with Neural Operators](https://arxiv.org/abs/2509.04623)
*David Millard,Lars Lindemann,Ali Baheri*

Main category: cs.LG

TL;DR: 本文将分割共形预测扩展到函数空间，提出两步法并给出分解和诊断指标，实验显示方法在分辨率变化下表现良好。


<details>
  <summary>Details</summary>
Motivation: 无限维设置下神经算子的不确定性量化因缺乏泛函输出的有限样本覆盖保证仍是开放问题，现有方法有局限。

Method: 采用两步法将分割共形预测扩展到函数空间，先在有限维空间建立保证，再提升到函数空间；分解共形半径并提出回归校正；提出两个诊断指标。

Result: 方法在分辨率变化下保持校准覆盖且变化小，在超分辨率任务中覆盖效果更好。

Conclusion: 所提方法能有效解决无限维设置下神经算子的不确定性量化问题，在不同任务中表现良好。

Abstract: Uncertainty quantification for neural operators remains an open problem in
the infinite-dimensional setting due to the lack of finite-sample coverage
guarantees over functional outputs. While conformal prediction offers
finite-sample guarantees in finite-dimensional spaces, it does not directly
extend to function-valued outputs. Existing approaches (Gaussian processes,
Bayesian neural networks, and quantile-based operators) require strong
distributional assumptions or yield conservative coverage. This work extends
split conformal prediction to function spaces following a two step method. We
first establish finite-sample coverage guarantees in a finite-dimensional space
using a discretization map in the output function space. Then these guarantees
are lifted to the function-space by considering the asymptotic convergence as
the discretization is refined. To characterize the effect of resolution, we
decompose the conformal radius into discretization, calibration, and
misspecification components. This decomposition motivates a regression-based
correction to transfer calibration across resolutions. Additionally, we propose
two diagnostic metrics (conformal ensemble score and internal agreement) to
quantify forecast degradation in autoregressive settings. Empirical results
show that our method maintains calibrated coverage with less variation under
resolution shifts and achieves better coverage in super-resolution tasks.

</details>


### [55] [Interpreting Transformer Architectures as Implicit Multinomial Regression](https://arxiv.org/abs/2509.04653)
*Jonas A. Actor,Anthony Gruber,Eric C. Cyr*

Main category: cs.LG

TL;DR: 本文建立了注意力机制与多项回归的新联系，指出变压器模型中表征的演变可解释为恢复分类最优特征的轨迹。


<details>
  <summary>Details</summary>
Motivation: 机械可解释性旨在理解机器学习模型内部组件如何产生整体行为，而注意力机制的数学基础及与一些概念的关系尚不清楚。

Method: 在固定多项回归设置下，对潜在特征进行优化。

Result: 优化潜在特征得到的最优解与注意力块诱导的动态一致。

Conclusion: 变压器中表征的演变可解释为恢复分类最优特征的轨迹。

Abstract: Mechanistic interpretability aims to understand how internal components of
modern machine learning models, such as weights, activations, and layers, give
rise to the model's overall behavior. One particularly opaque mechanism is
attention: despite its central role in transformer models, its mathematical
underpinnings and relationship to concepts like feature polysemanticity,
superposition, and model performance remain poorly understood. This paper
establishes a novel connection between attention mechanisms and multinomial
regression. Specifically, we show that in a fixed multinomial regression
setting, optimizing over latent features yields optimal solutions that align
with the dynamics induced by attention blocks. In other words, the evolution of
representations through a transformer can be interpreted as a trajectory that
recovers the optimal features for classification.

</details>


### [56] [Beyond Ordinary Lipschitz Constraints: Differentially Private Stochastic Optimization with Tsybakov Noise Condition](https://arxiv.org/abs/2509.04668)
*Difei Xu,Meng Ding,Zihang Xiang,Jinhui Xu,Di Wang*

Main category: cs.LG

TL;DR: 研究差分隐私模型下随机凸优化（DP - SCO），在特定条件下给出算法及上下界。


<details>
  <summary>Details</summary>
Motivation: 在满足Tsybakov噪声条件（TNC）且损失函数Lipschitz常数大或无界情况下研究DP - SCO。

Method: 先针对Lipschitz情形（θ≥2）提出(ε, δ) - DP算法，再扩展到θ≥θ̅>1情形，同时分析损失函数非Lipschitz情况。

Result: 给出不同情形下效用上界，以及ρ - 零集中差分隐私的私有极小极大率下界。

Conclusion: 在DP - SCO问题上，在TNC条件下获得了较优的算法上下界。

Abstract: We study Stochastic Convex Optimization in the Differential Privacy model
(DP-SCO). Unlike previous studies, here we assume the population risk function
satisfies the Tsybakov Noise Condition (TNC) with some parameter $\theta>1$,
where the Lipschitz constant of the loss could be extremely large or even
unbounded, but the $\ell_2$-norm gradient of the loss has bounded $k$-th moment
with $k\geq 2$. For the Lipschitz case with $\theta\geq 2$, we first propose an
$(\varepsilon, \delta)$-DP algorithm whose utility bound is
$\Tilde{O}\left(\left(\tilde{r}_{2k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$
in high probability, where $n$ is the sample size, $d$ is the model dimension,
and $\tilde{r}_{2k}$ is a term that only depends on the $2k$-th moment of the
gradient. It is notable that such an upper bound is independent of the
Lipschitz constant. We then extend to the case where
  $\theta\geq \bar{\theta}> 1$ for some known constant $\bar{\theta}$.
Moreover, when the privacy budget $\varepsilon$ is small enough, we show an
upper bound of
$\tilde{O}\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$
even if the loss function is not Lipschitz. For the lower bound, we show that
for any $\theta\geq 2$, the private minimax rate for $\rho$-zero Concentrated
Differential Privacy is lower bounded by
$\Omega\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\sqrt{\rho}}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$.

</details>


### [57] [Echoes Before Collapse: Deep Learning Detection of Flickering in Complex Systems](https://arxiv.org/abs/2509.04683)
*Yazdan Babazadeh Maghsoodlo,Madhur Anand,Chris T. Bauch*

Main category: cs.LG

TL;DR: 研究表明CNN LSTM模型可从含噪时间序列中准确识别闪烁模式，为识别动力系统不稳定性提供框架。


<details>
  <summary>Details</summary>
Motivation: 深度学习检测闪烁现象的潜力未被挖掘，而闪烁是系统弹性降低标志且可预示关键转变，有研究必要。

Method: 用含加性噪声的简单多项式函数生成的合成时间序列训练CNN LSTM模型。

Result: 模型能准确识别闪烁模式，可泛化到不同随机系统，并在实证数据集中可靠检测闪烁。

Conclusion: 深度学习可从含噪、非线性时间序列提取早期预警信号，为识别多种动力系统不稳定性提供灵活框架。

Abstract: Deep learning offers powerful tools for anticipating tipping points in
complex systems, yet its potential for detecting flickering (noise-driven
switching between coexisting stable states) remains unexplored. Flickering is a
hallmark of reduced resilience in climate systems, ecosystems, financial
markets, and other systems. It can precede critical regime shifts that are
highly impactful but difficult to predict. Here we show that convolutional long
short-term memory (CNN LSTM) models, trained on synthetic time series generated
from simple polynomial functions with additive noise, can accurately identify
flickering patterns. Despite being trained on simplified dynamics, our models
generalize to diverse stochastic systems and reliably detect flickering in
empirical datasets, including dormouse body temperature records and
palaeoclimate proxies from the African Humid Period. These findings demonstrate
that deep learning can extract early warning signals from noisy, nonlinear time
series, providing a flexible framework for identifying instability across a
wide range of dynamical systems.

</details>


### [58] [KRAFT: A Knowledge Graph-Based Framework for Automated Map Conflation](https://arxiv.org/abs/2509.04684)
*Farnoosh Hashemi,Laks V. S. Lakshmanan*

Main category: cs.LG

TL;DR: 现有地图合并方法有局限，提出KRAFT学习方法，表现出色。


<details>
  <summary>Details</summary>
Motivation: 数字地图需及时更新，但多数地理空间数据库信息有局限，现有地图合并方法有不足。

Method: KRAFT由知识图谱构建、地图匹配、地图合并三部分组成，用知识图谱对齐和地理特征编码匹配实体，用混合整数线性规划合并。

Result: KRAFT在地图合并任务中表现优于现有方法，各模块也分别优于传统方法。

Conclusion: KRAFT能有效解决现有地图合并方法的局限，可用于地图更新。

Abstract: Digital maps play a crucial role in various applications such as navigation,
fleet management, and ride-sharing, necessitating their accuracy and currency,
which require timely updates. While the majority of geospatial databases (GDBs)
provide high-quality information, their data is (i) limited to specific regions
and/or (ii) missing some entities, even in their covered areas. Map conflation
is the process of augmentation of a GDB using another GDB to conflate missing
spatial features. Existing map conflation methods suffer from two main
limitations: (1) They are designed for the conflation of linear objects (e.g.,
road networks) and cannot simply be extended to non-linear objects, thus
missing information about most entities in the map. (2) They are heuristic
algorithmic approaches that are based on pre-defined rules, unable to learn
entities matching in a data-driven manner. To address these limitations, we
design KRAFT, a learning based approach consisting of three parts: (1)
Knowledge Graph Construction - where each GDB is represented by a knowledge
graph, (2) Map Matching - where we use a knowledge graph alignment method as
well as a geospatial feature encoder to match entities in obtained knowledge
graphs, and (3) Map Merging - where we merge matched entities in the previous
modules in a consistent manner, using a mixed integer linear programming
formulation that fully merges the GDBs without adding any inconsistencies. Our
experimental evaluation shows that not only does KRAFT achieve outstanding
performance compared to state-of-the-art and baseline methods in map conflation
tasks, but each of its modules (e.g., Map Matching and Map Merging) also
separately outperforms traditional matching and merging methods.

</details>


### [59] [CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on EMG Signals](https://arxiv.org/abs/2509.04699)
*Wenhui Cui,Christopher Sandino,Hadi Pouransari,Ran Liu,Juri Minxha,Ellen L. Zippi,Aman Verma,Anna Sedlackova,Behrooz Mahasseni,Erdrin Azemi*

Main category: cs.LG

TL;DR: 本文提出CPEP框架，利用弱模态数据与高质量数据对齐提升表征质量，实现零样本手势分类，模型表现优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 利用低成本生物电信号进行可穿戴设备上的连续手势预测，通过弱模态数据与高质量数据对齐提升表征质量和实现零样本分类。

Method: 提出Contrastive Pose - EMG Pre - training (CPEP)框架来对齐EMG和姿势表征，学习一个能产生高质量和姿势信息表征的EMG编码器。

Result: 模型在分布内手势分类上比emg2pose基准模型最高高出21%，在未见（分布外）手势分类上高出72%。

Conclusion: 学习与结构化高质量数据对齐的弱模态数据表征可提升表征质量并实现零样本分类。

Abstract: Hand gesture classification using high-quality structured data such as
videos, images, and hand skeletons is a well-explored problem in computer
vision. Leveraging low-power, cost-effective biosignals, e.g. surface
electromyography (sEMG), allows for continuous gesture prediction on wearables.
In this paper, we demonstrate that learning representations from weak-modality
data that are aligned with those from structured, high-quality data can improve
representation quality and enables zero-shot classification. Specifically, we
propose a Contrastive Pose-EMG Pre-training (CPEP) framework to align EMG and
pose representations, where we learn an EMG encoder that produces high-quality
and pose-informative representations. We assess the gesture classification
performance of our model through linear probing and zero-shot setups. Our model
outperforms emg2pose benchmark models by up to 21% on in-distribution gesture
classification and 72% on unseen (out-of-distribution) gesture classification.

</details>


### [60] [Natural Spectral Fusion: p-Exponent Cyclic Scheduling and Early Decision-Boundary Alignment in First-Order Optimization](https://arxiv.org/abs/2509.04713)
*Gongyue Zhang,Honghai Liu*

Main category: cs.LG

TL;DR: 本文探讨优化器的频谱偏差，提出自然频谱融合（NSF）方法，理论和实验表明该方法能降低测试误差，揭示优化器作为频谱控制器的作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究未明确优化器自身的频谱偏差，本文旨在研究一阶优化器的内在频率偏好对优化路径的影响。

Method: 提出NSF方法，将训练视为可控的频谱覆盖和信息融合，通过对二阶矩项进行p指数扩展并采用循环调度实现。

Result: 自适应方法强调低频，SGD接近中性，负指数放大高频信息；循环调度拓宽频谱覆盖，提高跨频段融合，诱导早期决策边界对齐；在多个基准测试中降低测试误差，部分任务仅用四分之一训练成本达到基线精度。

Conclusion: NSF揭示了优化器作为主动频谱控制器的作用，为一阶优化提供了统一、可控且高效的框架。

Abstract: Spectral behaviors have been widely discussed in machine learning, yet the
optimizer's own spectral bias remains unclear. We argue that first-order
optimizers exhibit an intrinsic frequency preference that significantly
reshapes the optimization path. To address this, we propose Natural Spectral
Fusion (NSF): reframing training as controllable spectral coverage and
information fusion rather than merely scaling step sizes. NSF has two core
principles: treating the optimizer as a spectral controller that dynamically
balances low- and high-frequency information; and periodically reweighting
frequency bands at negligible cost, without modifying the model, data, or
training pipeline. We realize NSF via a p-exponent extension of the
second-moment term, enabling both positive and negative exponents, and
implement it through cyclic scheduling. Theory and experiments show that
adaptive methods emphasize low frequencies, SGD is near-neutral, and negative
exponents amplify high-frequency information. Cyclic scheduling broadens
spectral coverage, improves cross-band fusion, and induces early
decision-boundary alignment, where accuracy improves even while loss remains
high. Across multiple benchmarks, with identical learning-rate strategies and
fixed hyperparameters, p-exponent cyclic scheduling consistently reduces test
error and demonstrates distinct convergence behavior; on some tasks, it matches
baseline accuracy with only one-quarter of the training cost. Overall, NSF
reveals the optimizer's role as an active spectral controller and provides a
unified, controllable, and efficient framework for first-order optimization.

</details>


### [61] [CoVeR: Conformal Calibration for Versatile and Reliable Autoregressive Next-Token Prediction](https://arxiv.org/abs/2509.04733)
*Yuzhu Chen,Yingjie Wang,Shunyu Liu,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出新型解码策略CoVeR解决主流解码策略问题，有理论覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 主流解码策略如束搜索缺乏可证明的覆盖保证，难以平衡搜索效率和轨迹多样性。

Method: 在共形预测框架下提出无模型解码策略CoVeR。

Result: 建立PAC风格泛化界，保证CoVeR渐近达到至少1 - α的覆盖率。

Conclusion: CoVeR可同时保持紧凑搜索空间并确保理想轨迹的高覆盖概率。

Abstract: Autoregressive pre-trained models combined with decoding methods have
achieved impressive performance on complex reasoning tasks. While mainstream
decoding strategies such as beam search can generate plausible candidate sets,
they often lack provable coverage guarantees, and struggle to effectively
balance search efficiency with the need for versatile trajectories,
particularly those involving long-tail sequences that are essential in certain
real-world applications. To address these limitations, we propose
\textsc{CoVeR}, a novel model-free decoding strategy wihtin the conformal
prediction framework that simultaneously maintains a compact search space and
ensures high coverage probability over desirable trajectories. Theoretically,
we establish a PAC-style generalization bound, guaranteeing that \textsc{CoVeR}
asymptotically achieves a coverage rate of at least $1 - \alpha$ for any target
level $\alpha \in (0,1)$.

</details>


### [62] [Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning](https://arxiv.org/abs/2509.04734)
*Jasmine Shone,Shaden Alshammari,Mark Hamilton,Zhening Li,William Freeman*

Main category: cs.LG

TL;DR: 提出Beyond I-Con框架探索替代统计散度和相似性核以发现新损失函数，在多任务取得好结果，强调散度和核选择重要性。


<details>
  <summary>Details</summary>
Motivation: I-Con框架中基于KL的损失可能与真实目标不一致，且KL散度特性带来优化挑战。

Method: 提出Beyond I-Con框架，探索替代统计散度和相似性核，如在不同任务使用总变差距离、有界f - 散度等。

Result: 在无监督聚类、监督对比学习和降维任务中取得领先或更优结果。

Conclusion: 在表示学习优化中考虑散度和相似性核的选择很重要。

Abstract: The Information Contrastive (I-Con) framework revealed that over 23
representation learning methods implicitly minimize KL divergence between data
and learned distributions that encode similarities between data points.
However, a KL-based loss may be misaligned with the true objective, and
properties of KL divergence such as asymmetry and unboundedness may create
optimization challenges. We present Beyond I-Con, a framework that enables
systematic discovery of novel loss functions by exploring alternative
statistical divergences and similarity kernels. Key findings: (1) on
unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art
results by modifying the PMI algorithm to use total variation (TV) distance;
(2) on supervised contrastive learning, we outperform the standard approach by
using TV and a distance-based similarity kernel instead of KL and an angular
kernel; (3) on dimensionality reduction, we achieve superior qualitative
results and better performance on downstream tasks than SNE by replacing KL
with a bounded f-divergence. Our results highlight the importance of
considering divergence and similarity kernel choices in representation learning
optimization.

</details>


### [63] [VARMA-Enhanced Transformer for Time Series Forecasting](https://arxiv.org/abs/2509.04782)
*Jiajun Song,Xiaoou Liu*

Main category: cs.LG

TL;DR: 提出VARMAformer架构，结合经典时间序列分析与跨注意力框架，实验显示其优于现有方法，验证了结合经典统计与深度学习的益处。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的时间序列预测模型（如CATS）虽去除自注意力后更高效准确，但会忽略经典统计模型能捕捉的细粒度局部时间依赖，需解决此问题。

Method: 提出VARMAformer架构，包括VARMA启发的特征提取器（VFE）和VARMA增强注意力（VE - atten）机制。

Result: 在广泛使用的基准数据集上的实验表明，VARMAformer始终优于现有的最先进方法。

Conclusion: 将经典统计见解融入现代深度学习框架对时间序列预测有显著益处。

Abstract: Transformer-based models have significantly advanced time series forecasting.
Recent work, like the Cross-Attention-only Time Series transformer (CATS),
shows that removing self-attention can make the model more accurate and
efficient. However, these streamlined architectures may overlook the
fine-grained, local temporal dependencies effectively captured by classical
statistical models like Vector AutoRegressive Moving Average model (VARMA). To
address this gap, we propose VARMAformer, a novel architecture that synergizes
the efficiency of a cross-attention-only framework with the principles of
classical time series analysis. Our model introduces two key innovations: (1) a
dedicated VARMA-inspired Feature Extractor (VFE) that explicitly models
autoregressive (AR) and moving-average (MA) patterns at the patch level, and
(2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal
gate to make queries more context-aware. By fusing these classical insights
into a modern backbone, VARMAformer captures both global, long-range
dependencies and local, statistical structures. Through extensive experiments
on widely-used benchmark datasets, we demonstrate that our model consistently
outperforms existing state-of-the-art methods. Our work validates the
significant benefit of integrating classical statistical insights into modern
deep learning frameworks for time series forecasting.

</details>


### [64] [Graph Unlearning: Efficient Node Removal in Graph Neural Networks](https://arxiv.org/abs/2509.04785)
*Faqian Guan,Tianqing Zhu,Zhoutian Wang,Wei Ren,Wanlei Zhou*

Main category: cs.LG

TL;DR: 提出三种新的图神经网络节点遗忘方法，实验验证其在去除敏感训练节点和保护隐私上的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络节点遗忘方法存在对网络结构有限制、未有效利用图拓扑等问题，需高效去除训练节点的方法，保护敏感节点隐私。

Method: 提出 Class-based Label Replacement、Topology-guided Neighbor Mean Posterior Probability 和 Class-consistent Neighbor Node Filtering 三种节点遗忘方法。

Result: 在三个基准数据集上实验，结果表明所提方法在模型效用、遗忘效用和遗忘效率上优于现有方法。

Conclusion: 所提方法能有效去除敏感训练节点，保护隐私，有助于提升图神经网络模型隐私和安全性，为节点遗忘领域提供见解。

Abstract: With increasing concerns about privacy attacks and potential sensitive
information leakage, researchers have actively explored methods to efficiently
remove sensitive training data and reduce privacy risks in graph neural network
(GNN) models. Node unlearning has emerged as a promising technique for
protecting the privacy of sensitive nodes by efficiently removing specific
training node information from GNN models. However, existing node unlearning
methods either impose restrictions on the GNN structure or do not effectively
utilize the graph topology for node unlearning. Some methods even compromise
the graph's topology, making it challenging to achieve a satisfactory
performance-complexity trade-off. To address these issues and achieve efficient
unlearning for training node removal in GNNs, we propose three novel node
unlearning methods: Class-based Label Replacement, Topology-guided Neighbor
Mean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among
these methods, Topology-guided Neighbor Mean Posterior Probability and
Class-consistent Neighbor Node Filtering effectively leverage the topological
features of the graph, resulting in more effective node unlearning. To validate
the superiority of our proposed methods in node unlearning, we conducted
experiments on three benchmark datasets. The evaluation criteria included model
utility, unlearning utility, and unlearning efficiency. The experimental
results demonstrate the utility and efficiency of the proposed methods and
illustrate their superiority compared to state-of-the-art node unlearning
methods. Overall, the proposed methods efficiently remove sensitive training
nodes and protect the privacy information of sensitive nodes in GNNs. The
findings contribute to enhancing the privacy and security of GNN models and
provide valuable insights into the field of node unlearning.

</details>


### [65] [An Arbitration Control for an Ensemble of Diversified DQN variants in Continual Reinforcement Learning](https://arxiv.org/abs/2509.04815)
*Wonseo Jang,Dongjae Kim*

Main category: cs.LG

TL;DR: 本文提出ACED - DQN框架解决深度强化学习模型在持续强化学习场景中的灾难性遗忘问题，在静态和持续环境中均有性能提升。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习模型在持续强化学习场景中存在灾难性遗忘问题，导致性能不佳。

Method: 提出仲裁控制机制，集成多个显式训练以拥有不同价值函数的DQN变体，并优先选择近期试验中可靠性更高的智能体；构建ACED - DQN框架。

Result: 在静态和持续环境中均有显著的性能提升，实证表明仲裁控制在多样化DQN训练中有效。

Conclusion: 受人类大脑启发，引入的框架能使强化学习智能体持续学习。

Abstract: Deep reinforcement learning (RL) models, despite their efficiency in learning
an optimal policy in static environments, easily loses previously learned
knowledge (i.e., catastrophic forgetting). It leads RL models to poor
performance in continual reinforcement learning (CRL) scenarios. To address
this, we present an arbitration control mechanism over an ensemble of RL
agents. It is motivated by and closely aligned with how humans make decisions
in a CRL context using an arbitration control of multiple RL agents in parallel
as observed in the prefrontal cortex. We integrated two key ideas into our
model: (1) an ensemble of RLs (i.e., DQN variants) explicitly trained to have
diverse value functions and (2) an arbitration control that prioritizes agents
with higher reliability (i.e., less error) in recent trials. We propose a
framework for CRL, an Arbitration Control for an Ensemble of Diversified DQN
variants (ACED-DQN). We demonstrate significant performance improvements in
both static and continual environments, supported by empirical evidence showing
the effectiveness of arbitration control over diversified DQNs during training.
In this work, we introduced a framework that enables RL agents to continuously
learn, with inspiration from the human brain.

</details>


### [66] [Revolution or Hype? Seeking the Limits of Large Models in Hardware Design](https://arxiv.org/abs/2509.04905)
*Qiang Xu,Leon Stok,Rolf Drechsler,Xi Wang,Grace Li Zhang,Igor L. Markov*

Main category: cs.LG

TL;DR: 本文探讨大语言模型和大电路模型在电路设计中的应用，分析其能力、局限与前景，为ICCAD 2025小组讨论提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型和大电路模型在电子设计自动化领域引发关注，但也存在怀疑，需探讨其是否能真正变革电路设计。

Method: 综合学术界和行业专家观点，批判性分析大模型在硬件设计中的实际能力、基本局限和未来前景，整合可靠性、可扩展性和可解释性的核心论点。

Result: 得到一个权威概述，为当前争议性和影响力较大的技术趋势提供新见解。

Conclusion: 大模型在电路设计中的应用值得深入探讨，其能否超越或补充传统EDA方法仍需进一步研究。

Abstract: Recent breakthroughs in Large Language Models (LLMs) and Large Circuit Models
(LCMs) have sparked excitement across the electronic design automation (EDA)
community, promising a revolution in circuit design and optimization. Yet, this
excitement is met with significant skepticism: Are these AI models a genuine
revolution in circuit design, or a temporary wave of inflated expectations?
This paper serves as a foundational text for the corresponding ICCAD 2025
panel, bringing together perspectives from leading experts in academia and
industry. It critically examines the practical capabilities, fundamental
limitations, and future prospects of large AI models in hardware design. The
paper synthesizes the core arguments surrounding reliability, scalability, and
interpretability, framing the debate on whether these models can meaningfully
outperform or complement traditional EDA methods. The result is an
authoritative overview offering fresh insights into one of today's most
contentious and impactful technology trends.

</details>


### [67] [Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series](https://arxiv.org/abs/2509.04921)
*Yuki Takemoto*

Main category: cs.LG

TL;DR: 本文提出通过生成人工混沌时间序列和重采样技术模拟金融时间序列数据建模，经大规模预训练后进行比特币零样本预测，结果优于自相关模型，还发现了类似缩放定律的现象。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在多领域决策中重要，金融工具回报预测有挑战，现有时间序列基础模型和生成混沌时间序列的方法，因此研究建模金融时间序列。

Method: 生成人工混沌时间序列，用重采样技术模拟金融时间序列数据作为训练样本，增加重采样间隔扩展预测范围，进行大规模预训练，用比特币实际交易数据创建测试数据集，进行零样本预测。

Result: 基于预测的简单交易策略盈利评估结果显示性能显著优于自相关模型，预训练中发现增加训练样本数量可实现一定水平的混沌时间序列预测性能。

Conclusion: 若缩放定律对各种混沌模型都适用，有望投入大量计算资源预测近期事件，未来应进一步大规模训练并验证定律适用性。

Abstract: Time series forecasting plays a critical role in decision-making processes
across diverse fields including meteorology, traffic, electricity, economics,
finance, and so on. Especially, predicting returns on financial instruments is
a challenging problem. Some researchers have proposed time series foundation
models applicable to various forecasting tasks. Simultaneously, based on the
recognition that real-world time series exhibit chaotic properties, methods
have been developed to artificially generate synthetic chaotic time series,
construct diverse datasets and train models. In this study, we propose a
methodology for modeling financial time series by generating artificial chaotic
time series and applying resampling techniques to simulate financial time
series data, which we then use as training samples. Increasing the resampling
interval to extend predictive horizons, we conducted large-scale pre-training
using 10 billion training samples for each case. We subsequently created test
datasets for multiple timeframes using actual Bitcoin trade data and performed
zero-shot prediction without re-training the pre-trained model. The results of
evaluating the profitability of a simple trading strategy based on these
predictions demonstrated significant performance improvements over
autocorrelation models. During the large-scale pre-training process, we
observed a scaling law-like phenomenon that we can achieve predictive
performance at a certain level with extended predictive horizons for chaotic
time series by increasing the number of training samples exponentially. If this
scaling law proves robust and holds true across various chaotic models, it
suggests the potential to predict near-future events by investing substantial
computational resources. Future research should focus on further large-scale
training and verifying the applicability of this scaling law to diverse chaotic
models.

</details>


### [68] [A transformer-BiGRU-based framework with data augmentation and confident learning for network intrusion detection](https://arxiv.org/abs/2509.04925)
*Jiale Zhang,Pengfei He,Fei Li,Kewei Li,Yan Wang,Lan Huang,Ruochi Zhang,Fengfeng Zhou*

Main category: cs.LG

TL;DR: 为解决网络入侵检测难题，研究开发了结合机器学习和深度学习的TrailGate框架，能检测常见攻击和新兴威胁。


<details>
  <summary>Details</summary>
Motivation: 当前网络流量数据和频率增加，传统机器学习方法难以处理复杂网络入侵数据集，存在数据稀缺和类别不平衡问题。

Method: 开发TrailGate框架，结合Transformer和BiGRU架构，采用先进特征选择策略和数据增强技术。

Result: TrailGate能识别常见攻击类型，且能检测和缓解新兴威胁。

Conclusion: 算法融合在检测常见攻击类型上表现出色，能快速识别和化解源于现有模式的新兴威胁。

Abstract: In today's fast-paced digital communication, the surge in network traffic
data and frequency demands robust and precise network intrusion solutions.
Conventional machine learning methods struggle to grapple with complex patterns
within the vast network intrusion datasets, which suffer from data scarcity and
class imbalance. As a result, we have integrated machine learning and deep
learning techniques within the network intrusion detection system to bridge
this gap. This study has developed TrailGate, a novel framework that combines
machine learning and deep learning techniques. By integrating Transformer and
Bidirectional Gated Recurrent Unit (BiGRU) architectures with advanced feature
selection strategies and supplemented by data augmentation techniques,
TrailGate can identifies common attack types and excels at detecting and
mitigating emerging threats. This algorithmic fusion excels at detecting common
and well-understood attack types and has the unique ability to swiftly identify
and neutralize emerging threats that stem from existing paradigms.

</details>


### [69] [Ontology-Aligned Embeddings for Data-Driven Labour Market Analytics](https://arxiv.org/abs/2509.04942)
*Heinke Hihn,Dennis A. V. Dittrich,Carl Jeske,Cayo Costa Sobral,Helio Pais,Timm Lochmann*

Main category: cs.LG

TL;DR: 提出基于嵌入的对齐过程，将德语职位标题与两种本体关联，用公开数据微调模型，以语义搜索解决分类问题，还讨论后续工作。


<details>
  <summary>Details</summary>
Motivation: 不同来源职业数据推理能力有限是劳动力市场分析瓶颈，以往手工本体计算成本高、维护难，需新方法。

Method: 提出基于嵌入的对齐过程，用德国联邦就业局公开数据构建数据集微调Sentence - BERT模型，构建相似图结构进行近似最近邻搜索。

Result: 定义了相似图结构，可将分类过程转化为语义搜索问题，增加分类灵活性。

Conclusion: 讨论设计决策和挑战，提出用其他本体和多语言标题扩展图的后续工作。

Abstract: The limited ability to reason across occupational data from different sources
is a long-standing bottleneck for data-driven labour market analytics. Previous
research has relied on hand-crafted ontologies that allow such reasoning but
are computationally expensive and require careful maintenance by human experts.
The rise of language processing machine learning models offers a scalable
alternative by learning shared semantic spaces that bridge diverse occupational
vocabularies without extensive human curation. We present an embedding-based
alignment process that links any free-form German job title to two established
ontologies - the German Klassifikation der Berufe and the International
Standard Classification of Education. Using publicly available data from the
German Federal Employment Agency, we construct a dataset to fine-tune a
Sentence-BERT model to learn the structure imposed by the ontologies. The
enriched pairs (job title, embedding) define a similarity graph structure that
we can use for efficient approximate nearest-neighbour search, allowing us to
frame the classification process as a semantic search problem. This allows for
greater flexibility, e.g., adding more classes. We discuss design decisions,
open challenges, and outline ongoing work on extending the graph with other
ontologies and multilingual titles.

</details>


### [70] [Detecting Blinks in Healthy and Parkinson's EEG: A Deep Learning Perspective](https://arxiv.org/abs/2509.04951)
*Artem Lensky,Yiding Qiu*

Main category: cs.LG

TL;DR: 论文评估多种深度学习模型用于EEG信号眨眼检测，提出眨眼检测管道，在公共数据集上测试，CNN - RNN混合模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 眨眼在EEG中常被视为噪声，但眨眼率及其变异性是重要生理指标，需准确检测眨眼。

Method: 将问题建模为序列到序列任务，用1、3或5个前额EEG电极，测试多种深度学习架构，在原始EEG信号上训练，使用UCSD公共数据集。

Result: CNN - RNN混合模型表现最佳，在健康组和帕金森患者组不同通道下有不同的眨眼检测准确率。

Conclusion: 比较了用于EEG记录分割眨眼和非眨眼的神经网络，可用于计算眨眼率等统计信息。

Abstract: Blinks in electroencephalography (EEG) are often treated as unwanted
artifacts. However, recent studies have demonstrated that blink rate and its
variability are important physiological markers to monitor cognitive load,
attention, and potential neurological disorders. This paper addresses the
critical task of accurate blink detection by evaluating various deep learning
models for segmenting EEG signals into involuntary blinks and non-blinks. We
present a pipeline for blink detection using 1, 3, or 5 frontal EEG electrodes.
The problem is formulated as a sequence-to-sequence task and tested on various
deep learning architectures including standard recurrent neural networks,
convolutional neural networks (both standard and depth-wise), temporal
convolutional networks (TCN), transformer-based models, and hybrid
architectures. The models were trained on raw EEG signals with minimal
pre-processing. Training and testing was carried out on a public dataset of 31
subjects collected at UCSD. This dataset consisted of 15 healthy participants
and 16 patients with Parkinson's disease allowing us to verify the model's
robustness to tremor. Out of all models, CNN-RNN hybrid model consistently
outperformed other models and achieved the best blink detection accuracy of
93.8%, 95.4% and 95.8% with 1, 3, and 5 channels in the healthy cohort and
correspondingly 73.8%, 75.4% and 75.8% in patients with PD. The paper compares
neural networks for the task of segmenting EEG recordings to involuntary blinks
and no blinks allowing for computing blink rate and other statistics.

</details>


### [71] [On the Normalization of Confusion Matrices: Methods and Geometric Interpretations](https://arxiv.org/abs/2509.04959)
*Johan Erbani,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Sonia Ben Mokhtar,Diana Nurbakova*

Main category: cs.LG

TL;DR: 本文引入双随机归一化方法处理混淆矩阵，分离误差来源，助于准确诊断模型行为，并揭示归一化与模型内部类表示的对应关系。


<details>
  <summary>Details</summary>
Motivation: 混淆矩阵值受类别相似性和分布偏差两个因素影响，难以分离各自贡献，需解决该问题。

Method: 引入使用迭代比例拟合的双随机归一化方法，这是行和列归一化的推广。

Result: 该方法恢复了类别相似性的底层结构，实现误差来源分离，还展示了混淆矩阵归一化与模型内部类表示的对应关系。

Conclusion: 双随机归一化能更准确诊断模型行为，支持更有针对性的改进，从几何角度理解归一化对分类器的揭示作用。

Abstract: The confusion matrix is a standard tool for evaluating classifiers by
providing insights into class-level errors. In heterogeneous settings, its
values are shaped by two main factors: class similarity -- how easily the model
confuses two classes -- and distribution bias, arising from skewed
distributions in the training and test sets. However, confusion matrix values
reflect a mix of both factors, making it difficult to disentangle their
individual contributions. To address this, we introduce bistochastic
normalization using Iterative Proportional Fitting, a generalization of row and
column normalization. Unlike standard normalizations, this method recovers the
underlying structure of class similarity. By disentangling error sources, it
enables more accurate diagnosis of model behavior and supports more targeted
improvements. We also show a correspondence between confusion matrix
normalizations and the model's internal class representations. Both standard
and bistochastic normalizations can be interpreted geometrically in this space,
offering a deeper understanding of what normalization reveals about a
classifier.

</details>


### [72] [Neuro-Spectral Architectures for Causal Physics-Informed Networks](https://arxiv.org/abs/2509.04966)
*Arthur Bizzi,Leonardo M. Moreira,Márcio Marques,Leonardo Mendonça,Christian Júnior de Oliveira,Vitor Balestro,Lucas dos Santos Fernandez,Daniel Yukimura,Pavel Petrov,João M. Pereira,Tiago Novello,Lucas Nissenbaum*

Main category: cs.LG

TL;DR: 提出NeuSA解决标准MLP - based PINNs在处理复杂初值问题时的收敛、因果性和频谱偏差问题，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 标准MLP - based PINNs处理复杂初值问题时难以收敛，解违反因果性且存在频谱偏差。

Method: 引入受经典谱方法启发的NeuSA，将PDE投影到谱基上，与适配的Neural ODE集成，采用基于经典方法的初始化方案。

Result: 在典型的线性和非线性波动方程基准测试中，NeuSA比其他架构收敛更快、时间一致性更好、预测精度更高。

Conclusion: NeuSA能有效解决标准MLP - based PINNs的问题，性能优越，代码和预训练模型将发布。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful neural
framework for solving partial differential equations (PDEs). However, standard
MLP-based PINNs often fail to converge when dealing with complex initial-value
problems, leading to solutions that violate causality and suffer from a
spectral bias towards low-frequency components. To address these issues, we
introduce NeuSA (Neuro-Spectral Architectures), a novel class of PINNs inspired
by classical spectral methods, designed to solve linear and nonlinear PDEs with
variable coefficients. NeuSA learns a projection of the underlying PDE onto a
spectral basis, leading to a finite-dimensional representation of the dynamics
which is then integrated with an adapted Neural ODE (NODE). This allows us to
overcome spectral bias, by leveraging the high-frequency components enabled by
the spectral representation; to enforce causality, by inheriting the causal
structure of NODEs, and to start training near the target solution, by means of
an initialization scheme based on classical methods. We validate NeuSA on
canonical benchmarks for linear and nonlinear wave equations, demonstrating
strong performance as compared to other architectures, with faster convergence,
improved temporal consistency and superior predictive accuracy. Code and
pretrained models will be released.

</details>


### [73] [Topology-Aware Graph Reinforcement Learning for Dynamic Routing in Cloud Networks](https://arxiv.org/abs/2509.04973)
*Yuxi Wang,Heyao Liu,Guanzi Yao,Nyutian Long,Yue Kang*

Main category: cs.LG

TL;DR: 提出拓扑感知图强化学习方法解决云服务器环境路由策略优化问题，实验表明该方法优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决动态拓扑下决策不稳定和结构感知不足的挑战，实现云服务器环境的路由策略优化。

Method: 集成SASE模块和PAGU机制构建统一框架，SASE模块通过多层图卷积和结构位置嵌入建模节点状态，PAGU模块根据策略行为变化和奖励反馈调整图结构。

Result: 在GEANT拓扑数据集上实验，该方法在吞吐量、延迟控制和链路平衡等多个性能指标上优于现有图强化学习模型。

Conclusion: 所提方法能在动态复杂云网络中实现高效且稳健的路由。

Abstract: This paper proposes a topology-aware graph reinforcement learning approach to
address the routing policy optimization problem in cloud server environments.
The method builds a unified framework for state representation and structural
evolution by integrating a Structure-Aware State Encoding (SASE) module and a
Policy-Adaptive Graph Update (PAGU) mechanism. It aims to tackle the challenges
of decision instability and insufficient structural awareness under dynamic
topologies. The SASE module models node states through multi-layer graph
convolution and structural positional embeddings, capturing high-order
dependencies in the communication topology and enhancing the expressiveness of
state representations. The PAGU module adjusts the graph structure based on
policy behavior shifts and reward feedback, enabling adaptive structural
updates in dynamic environments. Experiments are conducted on the real-world
GEANT topology dataset, where the model is systematically evaluated against
several representative baselines in terms of throughput, latency control, and
link balance. Additional experiments, including hyperparameter sensitivity,
graph sparsity perturbation, and node feature dimensionality variation, further
explore the impact of structure modeling and graph updates on model stability
and decision quality. Results show that the proposed method outperforms
existing graph reinforcement learning models across multiple performance
metrics, achieving efficient and robust routing in dynamic and complex cloud
networks.

</details>


### [74] [Adapt in the Wild: Test-Time Entropy Minimization with Sharpness and Feature Regularization](https://arxiv.org/abs/2509.04977)
*Shuaicheng Niu,Guohao Chen,Deyu Chen,Yifan Zhang,Jiaxiang Wu,Zhiquan Wen,Yaofo Chen,Peilin Zhao,Chunyan Miao,Mingkui Tan*

Main category: cs.LG

TL;DR: 现有测试时间自适应（TTA）方法在特定测试数据条件下不稳定，本文分析原因并提出SAR和SAR²方法提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法在混合分布偏移、小批量、在线不平衡标签分布偏移的测试数据中表现不佳，难以应用于现实世界。

Method: 先提出SAR方法，从去除大梯度噪声样本和使模型权重趋于平坦最小值两方面稳定TTA；再引入SAR²，用冗余正则化器和不公平正则化器防止表征崩溃。

Result: 所提方法在多种测试场景下比先前方法表现更稳定，且计算效率高。

Conclusion: 提出的SAR和SAR²方法能有效解决现有TTA方法的稳定性问题。

Abstract: Test-time adaptation (TTA) may fail to improve or even harm the model
performance when test data have: 1) mixed distribution shifts, 2) small batch
sizes, 3) online imbalanced label distribution shifts. This is often a key
obstacle preventing existing TTA methods from being deployed in the real world.
In this paper, we investigate the unstable reasons and find that the batch norm
layer is a crucial factor hindering TTA stability. Conversely, TTA can perform
more stably with batch-agnostic norm layers, i.e., group or layer norm.
However, we observe that TTA with group and layer norms does not always succeed
and still suffers many failure cases, i.e., the model collapses into trivial
solutions by assigning the same class label for all samples. By digging into
this, we find that, during the collapse process: 1) the model gradients often
undergo an initial explosion followed by rapid degradation, suggesting that
certain noisy test samples with large gradients may disrupt adaptation; and 2)
the model representations tend to exhibit high correlations and classification
bias. To address this, we first propose a sharpness-aware and reliable entropy
minimization method, called SAR, for stabilizing TTA from two aspects: 1)
remove partial noisy samples with large gradients, 2) encourage model weights
to go to a flat minimum so that the model is robust to the remaining noisy
samples. Based on SAR, we further introduce SAR^2 to prevent representation
collapse with two regularizers: 1) a redundancy regularizer to reduce
inter-dimensional correlations among centroid-invariant features; and 2) an
inequity regularizer to maximize the prediction entropy of a prototype
centroid, thereby penalizing biased representations toward any specific class.
Promising results demonstrate that our methods perform more stably over prior
methods and are computationally efficient under the above wild test scenarios.

</details>


### [75] [Directed Evolution of Proteins via Bayesian Optimization in Embedding Space](https://arxiv.org/abs/2509.04998)
*Matouš Soldát,Jiří Kléma*

Main category: cs.LG

TL;DR: 本文提出一种机器学习辅助蛋白质定向进化的新方法，结合贝叶斯优化与预训练蛋白质语言模型提取的信息表示，提高了筛选性能并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 定向进化设计蛋白质需昂贵耗时的生化筛选，机器学习可帮助选择有信息或有前景的变体进行筛选，提高质量并减少筛选量。

Method: 将贝叶斯优化与从预训练蛋白质语言模型提取的蛋白质变体信息表示相结合。

Result: 基于序列嵌入的新表示显著提高了贝叶斯优化的性能，在相同筛选次数下得到更好结果。

Conclusion: 该方法优于具有回归目标的现有机器学习辅助定向进化方法。

Abstract: Directed evolution is an iterative laboratory process of designing proteins
with improved function by iteratively synthesizing new protein variants and
evaluating their desired property with expensive and time-consuming biochemical
screening. Machine learning methods can help select informative or promising
variants for screening to increase their quality and reduce the amount of
necessary screening. In this paper, we present a novel method for
machine-learning-assisted directed evolution of proteins which combines
Bayesian optimization with informative representation of protein variants
extracted from a pre-trained protein language model. We demonstrate that the
new representation based on the sequence embeddings significantly improves the
performance of Bayesian optimization yielding better results with the same
number of conducted screening in total. At the same time, our method
outperforms the state-of-the-art machine-learning-assisted directed evolution
methods with regression objective.

</details>


### [76] [Depth-Aware Initialization for Stable and Efficient Neural Network Training](https://arxiv.org/abs/2509.05018)
*Vijay Pandey*

Main category: cs.LG

TL;DR: 本文综合考虑各层和网络总深度信息提出新的初始化方案，能灵活增加网络方差，实验表明优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有初始化方案部分未充分利用深度信息，深层网络单位方差理论假设效果不佳，需灵活增加网络方差。

Method: 综合考虑各层深度和网络总深度信息，提出灵活增加网络方差的新初始化方案。

Result: 实验显示，提出的方法比现有初始化方案表现更好。

Conclusion: 新的初始化方案结合了各层深度信息，能灵活增加网络方差，在性能上优于现有方案。

Abstract: In past few years, various initialization schemes have been proposed. These
schemes are glorot initialization, He initialization, initialization using
orthogonal matrix, random walk method for initialization. Some of these methods
stress on keeping unit variance of activation and gradient propagation through
the network layer. Few of these methods are independent of the depth
information while some methods has considered the total network depth for
better initialization. In this paper, comprehensive study has been done where
depth information of each layer as well as total network is incorporated for
better initialization scheme. It has also been studied that for deeper networks
theoretical assumption of unit variance throughout the network does not perform
well. It requires the need to increase the variance of the network from first
layer activation to last layer activation. We proposed a novel way to increase
the variance of the network in flexible manner, which incorporates the
information of each layer depth. Experiments shows that proposed method
performs better than the existing initialization scheme.

</details>


### [77] [MultiSurv: A Multimodal Deep Survival Framework for Prostrate and Bladder Cancer](https://arxiv.org/abs/2509.05037)
*Noorul Wahab,Ethar Alzaid,Jiaqi Lv,Adam Shephard,Shan E Ahmed Raza*

Main category: cs.LG

TL;DR: 提出多模态深度生存模型MultiSurv，集成多种患者数据，在前列腺癌和膀胱癌生存预测任务中评估，结果显示其有潜力用于个性化风险分层和生存预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测事件发生时间对肿瘤学治疗规划和患者管理至关重要。

Method: 提出MultiSurv模型，结合DeepHit与投影层和跨模态注意力机制，集成临床、MRI、RNA - seq和病理特征数据。

Result: 在前列腺癌生化复发预测任务中，5折交叉验证C指数0.843，开发集0.818；在膀胱癌复发预测任务中，5折交叉验证C指数0.662，开发集0.457。

Conclusion: 多模态集成与深度生存学习结合为前列腺癌和膀胱癌个性化风险分层提供了有前景的途径，框架可广泛应用于异质生物医学数据的生存预测任务。

Abstract: Accurate prediction of time-to-event outcomes is a central challenge in
oncology, with significant implications for treatment planning and patient
management. In this work, we present MultiSurv, a multimodal deep survival
model utilising DeepHit with a projection layer and inter-modality
cross-attention, which integrates heterogeneous patient data, including
clinical, MRI, RNA-seq and whole-slide pathology features. The model is
designed to capture complementary prognostic signals across modalities and
estimate individualised time-to-biochemical recurrence in prostate cancer and
time-to-cancer recurrence in bladder cancer. Our approach was evaluated in the
context of the CHIMERA Grand Challenge, across two of the three provided tasks.
For Task 1 (prostate cancer bio-chemical recurrence prediction), the proposed
framework achieved a concordance index (C-index) of 0.843 on 5-folds
cross-validation and 0.818 on CHIMERA development set, demonstrating robust
discriminatory ability. For Task 3 (bladder cancer recurrence prediction), the
model obtained a C-index of 0.662 on 5-folds cross-validation and 0.457 on
development set, highlighting its adaptability and potential for clinical
translation. These results suggest that leveraging multimodal integration with
deep survival learning provides a promising pathway toward personalised risk
stratification in prostate and bladder cancer. Beyond the challenge setting,
our framework is broadly applicable to survival prediction tasks involving
heterogeneous biomedical data.

</details>


### [78] [Recurrent State Encoders for Efficient Neural Combinatorial Optimization](https://arxiv.org/abs/2509.05084)
*Tim Dernedde,Daniela Thyssens,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: 提出用循环编码器解决神经组合优化问题，减少层数且提升性能和降低延迟，并在三个问题上验证。


<details>
  <summary>Details</summary>
Motivation: 观察到神经组合优化构建方法中两步间状态变化小，高效模型应复用先前计算。

Method: 训练循环编码器，其计算状态嵌入时不仅基于当前状态，还基于前一步嵌入。

Result: 循环编码器即使层数少3倍也能取得等同或更好性能，显著降低延迟。

Conclusion: 在三个问题上验证结果，并将模型集成到大型邻域搜索算法展示实用性。

Abstract: The primary paradigm in Neural Combinatorial Optimization (NCO) are
construction methods, where a neural network is trained to sequentially add one
solution component at a time until a complete solution is constructed. We
observe that the typical changes to the state between two steps are small,
since usually only the node that gets added to the solution is removed from the
state. An efficient model should be able to reuse computation done in prior
steps. To that end, we propose to train a recurrent encoder that computes the
state embeddings not only based on the state but also the embeddings of the
step before. We show that the recurrent encoder can achieve equivalent or
better performance than a non-recurrent encoder even if it consists of
$3\times$ fewer layers, thus significantly improving on latency. We demonstrate
our findings on three different problems: the Traveling Salesman Problem (TSP),
the Capacitated Vehicle Routing Problem (CVRP), and the Orienteering Problem
(OP) and integrate the models into a large neighborhood search algorithm, to
showcase the practical relevance of our findings.

</details>


### [79] [HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of Manufactured Solutions](https://arxiv.org/abs/2509.05117)
*Rafael Bischof,Michal Piovarči,Michael A. Kraus,Siddhartha Mishra,Bernd Bickel*

Main category: cs.LG

TL;DR: 提出HyPINO多物理神经算子，用于零样本泛化求解PDE，结合超网络与混合监督，在多个基准问题上表现出色，还引入迭代细化程序，结果显示其在解决复杂PDE问题上有潜力。


<details>
  <summary>Details</summary>
Motivation: 实现跨广泛参数化偏微分方程（PDE）的零样本泛化，无需特定任务微调。

Method: 结合基于Swin Transformer的超网络与混合监督，包括MMS生成的解析解标记数据和物理信息目标优化的未标记样本；引入迭代细化程序生成“delta”PINN。

Result: 在七个基准问题上零样本精度高，优于U - Nets等；迭代细化程序在六个基准上降低误差；HyPINO初始化的PINNs在五个基准上收敛更快、误差更低。

Conclusion: 该可扩展方法有潜力作为基础，以更高精度和更低计算成本解决日益复杂的非线性、高维PDE问题。

Abstract: We present HyPINO, a multi-physics neural operator designed for zero-shot
generalization across a broad class of parametric PDEs without requiring
task-specific fine-tuning. Our approach combines a Swin Transformer-based
hypernetwork with mixed supervision: (i) labeled data from analytical solutions
generated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled
samples optimized using physics-informed objectives. The model maps PDE
parametrizations to target Physics-Informed Neural Networks (PINNs) and can
handle linear elliptic, hyperbolic, and parabolic equations in two dimensions
with varying source terms, geometries, and mixed Dirichlet/Neumann boundary
conditions, including interior boundaries. HyPINO achieves strong zero-shot
accuracy on seven benchmark problems from PINN literature, outperforming
U-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we
introduce an iterative refinement procedure that compares the physics of the
generated PINN to the requested PDE and uses the discrepancy to generate a
"delta" PINN. Summing their contributions and repeating this process forms an
ensemble whose combined solution progressively reduces the error on six
benchmarks and achieves over 100x gain in average $L_2$ loss in the best case,
while retaining forward-only inference. Additionally, we evaluate the
fine-tuning behavior of PINNs initialized by HyPINO and show that they converge
faster and to lower final error than both randomly initialized and
Reptile-meta-learned PINNs on five benchmarks, performing on par on the
remaining two. Our results highlight the potential of this scalable approach as
a foundation for extending neural operators toward solving increasingly
complex, nonlinear, and high-dimensional PDE problems with significantly
improved accuracy and reduced computational cost.

</details>


### [80] [Should We Always Train Models on Fine-Grained Classes?](https://arxiv.org/abs/2509.05130)
*Davide Pirovano,Federico Milanesio,Michele Caselle,Piero Fariselli,Matteo Osella*

Main category: cs.LG

TL;DR: 研究细粒度标签训练对分类问题性能的影响，发现其效果取决于数据几何结构、数据集大小和模型容量等因素。


<details>
  <summary>Details</summary>
Motivation: 探究在分类问题中使用细粒度标签训练能提升性能这一观察的普遍性及其潜在原因。

Method: 使用真实和合成数据集进行研究。

Result: 细粒度标签训练并非普遍提高分类准确率，其效果关键取决于数据几何结构以及与标签层次的关系，数据集大小和模型容量等因素也有显著影响。

Conclusion: 细粒度标签训练的有效性受多种因素制约。

Abstract: In classification problems, models must predict a class label based on the
input data features. However, class labels are organized hierarchically in many
datasets. While a classification task is often defined at a specific level of
this hierarchy, training can utilize a finer granularity of labels. Empirical
evidence suggests that such fine-grained training can enhance performance. In
this work, we investigate the generality of this observation and explore its
underlying causes using both real and synthetic datasets. We show that training
on fine-grained labels does not universally improve classification accuracy.
Instead, the effectiveness of this strategy depends critically on the geometric
structure of the data and its relations with the label hierarchy. Additionally,
factors such as dataset size and model capacity significantly influence whether
fine-grained labels provide a performance benefit.

</details>


### [81] [On the Learnability of Distribution Classes with Adaptive Adversaries](https://arxiv.org/abs/2509.05137)
*Tosca Lechner,Alex Bie,Gautam Kamath*

Main category: cs.LG

TL;DR: 研究存在自适应对手时分布类的可学习性，表明加性自适应对手下的可学习性比加性无感知对手更强。


<details>
  <summary>Details</summary>
Motivation: 探讨存在自适应对手（能拦截和操纵样本）时分布类的可学习性问题，与无感知对手情况作对比。

Method: 提出考虑对手预算的、关于自适应对手的可学习性的通用概念。

Result: 发现加性自适应对手下的可学习性是比加性无感知对手下更严格的条件。

Conclusion: 明确了在不同类型对手下分布类可学习性的强弱关系。

Abstract: We consider the question of learnability of distribution classes in the
presence of adaptive adversaries -- that is, adversaries capable of
intercepting the samples requested by a learner and applying manipulations with
full knowledge of the samples before passing it on to the learner. This stands
in contrast to oblivious adversaries, who can only modify the underlying
distribution the samples come from but not their i.i.d.\ nature. We formulate a
general notion of learnability with respect to adaptive adversaries, taking
into account the budget of the adversary. We show that learnability with
respect to additive adaptive adversaries is a strictly stronger condition than
learnability with respect to additive oblivious adversaries.

</details>


### [82] [Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights](https://arxiv.org/abs/2509.05142)
*Cosmin-Andrei Hatfaludi,Alex Serban*

Main category: cs.LG

TL;DR: 文章对联邦学习与基础模型的结合进行文献综述，构建新分类法对比方法，以医疗领域为例给出实践见解。


<details>
  <summary>Details</summary>
Motivation: 随着复杂基础模型广泛应用，有扩大训练资源和整合私有数据的需求，且缺乏统一综述。

Method: 围绕新分类法进行文献综述，检索超4200篇文章，筛选出超250篇，涵盖42种方法用于构建分类法和对比。

Result: 构建分类法并对比方法的复杂度、效率和可扩展性。

Conclusion: 总结该领域现状，为采用、发展和整合基础模型与联邦学习提供实践见解。

Abstract: Federated learning has the potential to unlock siloed data and distributed
resources by enabling collaborative model training without sharing private
data. As more complex foundational models gain widespread use, the need to
expand training resources and integrate privately owned data grows as well. In
this article, we explore the intersection of federated learning and
foundational models, aiming to identify, categorize, and characterize technical
methods that integrate the two paradigms. As a unified survey is currently
unavailable, we present a literature survey structured around a novel taxonomy
that follows the development life-cycle stages, along with a technical
comparison of available methods. Additionally, we provide practical insights
and guidelines for implementing and evolving these methods, with a specific
focus on the healthcare domain as a case study, where the potential impact of
federated learning and foundational models is considered significant. Our
survey covers multiple intersecting topics, including but not limited to
federated learning, self-supervised learning, fine-tuning, distillation, and
transfer learning. Initially, we retrieved and reviewed a set of over 4,200
articles. This collection was narrowed to more than 250 thoroughly reviewed
articles through inclusion criteria, featuring 42 unique methods. The methods
were used to construct the taxonomy and enabled their comparison based on
complexity, efficiency, and scalability. We present these results as a
self-contained overview that not only summarizes the state of the field but
also provides insights into the practical aspects of adopting, evolving, and
integrating foundational models with federated learning.

</details>


### [83] [KVCompose: Efficient Structured KV Cache Compression with Composite Tokens](https://arxiv.org/abs/2509.05165)
*Dmitry Akulov,Mohamed Sana,Antonio De Domenico,Tareq Si Salem,Nicola Piovesan,Fadhel Ayed*

Main category: cs.LG

TL;DR: 提出基于注意力引导、层自适应复合令牌的KV缓存压缩框架，实现显著内存减少且保持准确性，与标准推理管道兼容。


<details>
  <summary>Details</summary>
Motivation: 大语言模型KV缓存大小随上下文长度和模型深度线性增长，成为长上下文推理的主要瓶颈，现有压缩方法存在缺陷。

Method: 聚合注意力分数估计令牌重要性，独立选择特定头的令牌并组合成复合令牌，用全局分配机制跨层调整保留预算。

Result: 实现显著内存减少，一致优于先前的结构化和半结构化方法。

Conclusion: 该方法与标准推理管道完全兼容，为高效长上下文大语言模型部署提供了实用且可扩展的解决方案。

Abstract: Large language models (LLMs) rely on key-value (KV) caches for efficient
autoregressive decoding; however, cache size grows linearly with context length
and model depth, becoming a major bottleneck in long-context inference. Prior
KV cache compression methods either enforce rigid heuristics, disrupt tensor
layouts with per-attention-head variability, or require specialized compute
kernels.
  We propose a simple, yet effective, KV cache compression framework based on
attention-guided, layer-adaptive composite tokens. Our method aggregates
attention scores to estimate token importance, selects head-specific tokens
independently, and aligns them into composite tokens that respect the uniform
cache structure required by existing inference engines. A global allocation
mechanism further adapts retention budgets across layers, assigning more
capacity to layers with informative tokens. This approach achieves significant
memory reduction while preserving accuracy, consistently outperforming prior
structured and semi-structured methods. Crucially, our approach remains fully
compatible with standard inference pipelines, offering a practical and scalable
solution for efficient long-context LLM deployment.

</details>


### [84] [Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection](https://arxiv.org/abs/2509.05190)
*Mounvik K,N Harshit*

Main category: cs.LG

TL;DR: 提出轻量级一维CNN模型结合结构化剪枝和轻度提前停止策略用于癫痫检测，在减少资源占用同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在实时检测或资源受限环境下因规模和计算需求带来的挑战，提高癫痫检测效率和可靠性。

Method: 构建轻量级一维CNN模型，采用轻度提前停止训练，对基线CNN进行结构化剪枝，移除50%卷积核。

Result: 训练模型准确率达92.78%，宏F1分数0.8686；剪枝后权重和内存减少50%，精度提升到92.87%，宏F1分数提升到0.8707。

Conclusion: 结构化剪枝能去除冗余、提高泛化能力，结合轻度提前停止是提高癫痫检测效率和可靠性的有效方法，适用于资源受限场景。

Abstract: Deep learning models, especially convolutional neural networks (CNNs), have
shown considerable promise for biomedical signals such as EEG-based seizure
detection. However, these models come with challenges, primarily due to their
size and compute requirements in environments where real-time detection or
limited resources are available. In this study, we present a lightweight
one-dimensional CNN model with structured pruning to improve efficiency and
reliability. The model was trained with mild early stopping to address possible
overfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686.
Structured pruning of the baseline CNN involved removing 50% of the
convolutional kernels based on their importance to model predictions.
Surprisingly, after pruning the weights and memory by 50%, the new network was
still able to maintain predictive capabilities, while modestly increasing
precision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we
present a convincing case that structured pruning removes redundancy, improves
generalization, and, in combination with mild early stopping, achieves a
promising way forward to improve seizure detection efficiency and reliability,
which is clear motivation for resource-limited settings.

</details>


### [85] [Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning](https://arxiv.org/abs/2509.05193)
*Bastien Dubail,Stefan Stojanovic,Alexandre Proutière*

Main category: cs.LG

TL;DR: 文章挑战现代强化学习中后继度量低秩的假设，指出移位后继度量有低秩结构，给出有限样本性能保证，推导不等式，建立移位与系统属性联系并实验验证。


<details>
  <summary>Details</summary>
Motivation: 挑战现代强化学习算法中后继度量低秩的常见假设，寻找更合适的低秩结构。

Method: 研究移位后继度量，为其低秩近似的逐元素估计提供有限样本性能保证，推导马尔可夫链的Type II Poincaré不等式。

Result: 发现近似和估计误差由矩阵的谱可恢复性决定，所需移位与奇异值衰减有关且通常较小，建立移位与系统局部混合属性的联系。

Conclusion: 移位后继度量有低秩结构，能提升目标条件强化学习性能。

Abstract: Low-rank structure is a common implicit assumption in many modern
reinforcement learning (RL) algorithms. For instance, reward-free and
goal-conditioned RL methods often presume that the successor measure admits a
low-rank representation. In this work, we challenge this assumption by first
remarking that the successor measure itself is not low-rank. Instead, we
demonstrate that a low-rank structure naturally emerges in the shifted
successor measure, which captures the system dynamics after bypassing a few
initial transitions. We provide finite-sample performance guarantees for the
entry-wise estimation of a low-rank approximation of the shifted successor
measure from sampled entries. Our analysis reveals that both the approximation
and estimation errors are primarily governed by the so-called spectral
recoverability of the corresponding matrix. To bound this parameter, we derive
a new class of functional inequalities for Markov chains that we call Type II
Poincar\'e inequalities and from which we can quantify the amount of shift
needed for effective low-rank approximation and estimation. This analysis shows
in particular that the required shift depends on decay of the high-order
singular values of the shifted successor measure and is hence typically small
in practice. Additionally, we establish a connection between the necessary
shift and the local mixing properties of the underlying dynamical system, which
provides a natural way of selecting the shift. Finally, we validate our
theoretical findings with experiments, and demonstrate that shifting the
successor measure indeed leads to improved performance in goal-conditioned RL.

</details>


### [86] [RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks](https://arxiv.org/abs/2509.05207)
*Arefin Niam,Tevfik Kosar,M S Q Zulkar Nine*

Main category: cs.LG

TL;DR: 本文提出RapidGNN分布式GNN训练框架，经基准图数据集评估，能提升训练吞吐量、减少远程特征提取、有良好扩展性和能效。


<details>
  <summary>Details</summary>
Motivation: 大规模图上GNN分布式训练因数据集高度连接结构面临挑战，传统采样方法通信开销大。

Method: 提出RapidGNN框架，采用基于确定性采样的调度实现高效缓存构建和远程特征预取。

Result: 在基准数据集上平均提升端到端训练吞吐量2.46 - 3.00倍，减少远程特征提取9.70 - 15.39倍，有近线性扩展性，CPU和GPU能效分别提升44%和32%。

Conclusion: RapidGNN在不同规模和拓扑的图数据集上有效，能高效训练GNN。

Abstract: Graph Neural Networks (GNNs) have become popular across a diverse set of
tasks in exploring structural relationships between entities. However, due to
the highly connected structure of the datasets, distributed training of GNNs on
large-scale graphs poses significant challenges. Traditional sampling-based
approaches mitigate the computational loads, yet the communication overhead
remains a challenge. This paper presents RapidGNN, a distributed GNN training
framework with deterministic sampling-based scheduling to enable efficient
cache construction and prefetching of remote features. Evaluation on benchmark
graph datasets demonstrates RapidGNN's effectiveness across different scales
and topologies. RapidGNN improves end-to-end training throughput by 2.46x to
3.00x on average over baseline methods across the benchmark datasets, while
cutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further
demonstrates near-linear scalability with an increasing number of computing
units efficiently. Furthermore, it achieves increased energy efficiency over
the baseline methods for both CPU and GPU by 44% and 32%, respectively.

</details>


### [87] [Deep Learning-Enhanced for Amine Emission Monitoring and Performance Analysis in Industrial Carbon Capture Plants](https://arxiv.org/abs/2509.05241)
*Lokendra Poudel,David Tincher,Duy-Nhat Phan,Rahul Bhowmik*

Main category: cs.LG

TL;DR: 本文提出数据驱动的深度学习模型用于预测和监测基于胺的燃烧后碳捕获系统中的胺排放和关键性能参数，模型准确率超99%，因果分析表明调整操作参数可优化系统，该ML框架为碳捕获提供实用途径。


<details>
  <summary>Details</summary>
Motivation: 开发用于预测和监测基于胺的燃烧后碳捕获系统中胺排放和关键性能参数的有效方法，以优化碳捕获操作，减轻环境影响。

Method: 利用CESAR1溶剂活动的运行数据，开发四种DL架构（Basic LSTM、Stacked LSTM、Bi - directional LSTM和Convolutional LSTM）；设计排放预测模型；进行因果影响分析，对八个输入变量进行±20%的扰动模拟。

Result: 模型预测准确率超99%，能有效跟踪稳定趋势和突然波动；因果分析显示调整特定操作参数可显著减少胺排放并提高系统性能。

Conclusion: ML不仅是预测工具，也是优化碳捕获操作的决策支持系统，该ML框架为减轻环境影响提供实用途径，迈向智能、数据驱动的控制策略。

Abstract: We present data driven deep learning models for forecasting and monitoring
amine emissions and key performance parameters in amine-based post-combustion
carbon capture systems. Using operational data from the CESAR1 solvent campaign
at Technology Center Mongstad, four DL architectures such as Basic Long
Short-Term Memory (LSTM), Stacked LSTM, Bi-directional LSTM, and Convolutional
LSTM were developed to capture time-dependent process behavior. For emission
prediction, models were designed for 2-amino-2-methyl-1-propanol (AMP) and
Piperazine emissions measured via FTIR and IMR-MS methods. System performance
models target four critical parameters: CO$_2$ product flow, absorber outlet
temperature, depleted flue gas outlet temperature, and RFCC stripper bottom
temperature. These models achieved high predictive accuracy exceeding 99% and
effectively tracked both steady trends and abrupt fluctuations. Additionally,
we conducted causal impact analysis to evaluate how operational variables
influence emissions and system performance. Eight input variables were
systematically perturbed within $\pm$20% of nominal values to simulate
deviations and assess their impact. This analysis revealed that adjusting
specific operational parameters, such as lean solvent temperature and water
wash conditions, can significantly reduce amine emissions and enhance system
performance. This study highlights ML not only as a predictive tool but also as
a decision support system for optimizing carbon capture operations under steady
state and dynamic conditions. By enabling real time monitoring, scenario
testing, and operational optimization, the developed ML framework offers a
practical pathway for mitigating environmental impacts. This work represents a
step toward intelligent, data-driven control strategies that enhance the
efficiency, stability, and sustainability of carbon capture and storage
technologies.

</details>


### [88] [A Kolmogorov-Arnold Network for Interpretable Cyberattack Detection in AGC Systems](https://arxiv.org/abs/2509.05259)
*Jehad Jilan,Niranjana Naveen Nambiar,Ahmad Mohammad Saber,Alok Paranjape,Amr Youssef,Deepa Kundur*

Main category: cs.LG

TL;DR: 本文提出用可解释的KAN模型检测AGC系统中的FDIA攻击，训练后可提取符号公式，检测率高且误报率低。


<details>
  <summary>Details</summary>
Motivation: AGC易受隐蔽网络攻击如FDIA，传统检测方法难以应对，以往工作多为黑盒方法。

Method: 提出KAN模型用于FDIA检测，考虑系统非线性，离线训练学习复杂关系，训练后提取符号公式。

Result: 初始模型和符号公式的FDIA检测率分别达95.97%和95.9%，误报率低。

Conclusion: KAN模型是增强AGC网络安全的可靠方法。

Abstract: Automatic Generation Control (AGC) is essential for power grid stability but
remains vulnerable to stealthy cyberattacks, such as False Data Injection
Attacks (FDIAs), which can disturb the system's stability while evading
traditional detection methods. Unlike previous works that relied on blackbox
approaches, this work proposes Kolmogorov-Arnold Networks (KAN) as an
interpretable and accurate method for FDIA detection in AGC systems,
considering the system nonlinearities. KAN models include a method for
extracting symbolic equations, and are thus able to provide more
interpretability than the majority of machine learning models. The proposed KAN
is trained offline to learn the complex nonlinear relationships between the AGC
measurements under different operating scenarios. After training, symbolic
formulas that describe the trained model's behavior can be extracted and
leveraged, greatly enhancing interpretability. Our findings confirm that the
proposed KAN model achieves FDIA detection rates of up to 95.97% and 95.9% for
the initial model and the symbolic formula, respectively, with a low false
alarm rate, offering a reliable approach to enhancing AGC cybersecurity.

</details>


### [89] [SpikingBrain Technical Report: Spiking Brain-inspired Large Models](https://arxiv.org/abs/2509.05276)
*Yuqi Pan,Yupeng Feng,Jinghao Zhuang,Siyu Ding,Zehao Liu,Bohan Sun,Yuhong Chou,Han Xu,Xuerui Qiu,Anlin Deng,Anjie Hu,Peng Zhou,Man Yao,Jibin Wu,Jian Yang,Guoliang Sun,Bo Xu,Guoqi Li*

Main category: cs.LG

TL;DR: 本文介绍了脑启发模型SpikingBrain，可在非NVIDIA平台高效训练和推理长上下文，开发两款模型并展示其性能和潜力。


<details>
  <summary>Details</summary>
Motivation: 主流基于Transformer的大语言模型存在效率瓶颈，在非NVIDIA平台构建大模型有挑战，需要解决方案。

Method: 利用MetaX GPU集群，从模型架构（含自适应脉冲神经元的线性和混合线性注意力架构）、算法优化（高效转换训练管道和专用脉冲编码框架）、系统工程（定制训练框架、算子库和并行策略）三方面入手开发SpikingBrain。

Result: 开发SpikingBrain - 7B和SpikingBrain - 76B模型，性能与开源Transformer基线相当，提高长序列训练效率，推理内存（部分）恒定，有事件驱动脉冲行为，如SpikingBrain - 7B在4M令牌序列上实现超100倍加速，训练稳定，脉冲方案稀疏度达69.15%。

Conclusion: 脑启发机制有潜力推动下一代高效可扩展大模型设计。

Abstract: Mainstream Transformer-based large language models face major efficiency
bottlenecks: training computation scales quadratically with sequence length,
and inference memory grows linearly, limiting long-context processing. Building
large models on non-NVIDIA platforms also poses challenges for stable and
efficient training. To address this, we introduce SpikingBrain, a family of
brain-inspired models designed for efficient long-context training and
inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three
aspects: (1) Model Architecture: linear and hybrid-linear attention
architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an
efficient, conversion-based training pipeline and a dedicated spike coding
framework; (3) System Engineering: customized training frameworks, operator
libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,
and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the
feasibility of large-scale LLM development on non-NVIDIA platforms.
SpikingBrain achieves performance comparable to open-source Transformer
baselines while using only about 150B tokens for continual pre-training. Our
models significantly improve long-sequence training efficiency and deliver
inference with (partially) constant memory and event-driven spiking behavior.
For example, SpikingBrain-7B attains over 100x speedup in Time to First Token
for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX
C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4
percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling
low-power operation. Overall, this work demonstrates the potential of
brain-inspired mechanisms to drive the next generation of efficient and
scalable large model design.

</details>


### [90] [Dual-Branch Convolutional Framework for Spatial and Frequency-Based Image Forgery Detection](https://arxiv.org/abs/2509.05281)
*Naman Tyagi*

Main category: cs.LG

TL;DR: 本文介绍结合空间和频率特征的伪造检测框架，提出双分支卷积神经网络，在CASIA 2.0数据集上有不错表现，平衡了计算复杂度与检测可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造和数字图像伪造快速增加，确保图像真实性挑战增大，需要有效检测方法。

Method: 提出双分支卷积神经网络，处理从空间和频率域提取的特征，在孪生网络中融合并比较特征以生成64维嵌入用于分类。

Result: 在CASIA 2.0数据集上准确率达77.9%，优于传统统计方法。

Conclusion: 该方法平衡了计算复杂度和检测可靠性，适用于实际部署，为数字图像法医审查提供了有效方法，推动了视觉取证技术发展。

Abstract: With a very rapid increase in deepfakes and digital image forgeries, ensuring
the authenticity of images is becoming increasingly challenging. This report
introduces a forgery detection framework that combines spatial and
frequency-based features for detecting forgeries. We propose a dual branch
convolution neural network that operates on features extracted from spatial and
frequency domains. Features from both branches are fused and compared within a
Siamese network, yielding 64 dimensional embeddings for classification. When
benchmarked on CASIA 2.0 dataset, our method achieves an accuracy of 77.9%,
outperforming traditional statistical methods. Despite its relatively weaker
performance compared to larger, more complex forgery detection pipelines, our
approach balances computational complexity and detection reliability, making it
ready for practical deployment. It provides a strong methodology for forensic
scrutiny of digital images. In a broader sense, it advances the state of the
art in visual forensics, addressing an urgent requirement in media
verification, law enforcement and digital content reliability.

</details>


### [91] [Learning to accelerate distributed ADMM using graph neural networks](https://arxiv.org/abs/2509.05288)
*Henri Doerks,Paul Häusner,Daniel Hernández Escobar,Jens Sjölund*

Main category: cs.LG

TL;DR: 本文将分布式ADMM迭代与图神经网络结合，学习自适应步长和通信权重，提升了ADMM的收敛速度和解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 现有ADMM方法存在收敛慢和对超参数选择敏感的问题。

Method: 将分布式ADMM迭代表示在图神经网络的消息传递框架内，通过图神经网络学习自适应步长和通信权重，并端到端训练网络参数。

Result: 数值实验表明学习变体相比标准ADMM持续提高了收敛速度和解决方案质量。

Conclusion: 提出的基于图神经网络学习超参数的方法有效，能提升ADMM性能。

Abstract: Distributed optimization is fundamental in large-scale machine learning and
control applications. Among existing methods, the Alternating Direction Method
of Multipliers (ADMM) has gained popularity due to its strong convergence
guarantees and suitability for decentralized computation. However, ADMM often
suffers from slow convergence and sensitivity to hyperparameter choices. In
this work, we show that distributed ADMM iterations can be naturally
represented within the message-passing framework of graph neural networks
(GNNs). Building on this connection, we propose to learn adaptive step sizes
and communication weights by a graph neural network that predicts the
hyperparameters based on the iterates. By unrolling ADMM for a fixed number of
iterations, we train the network parameters end-to-end to minimize the final
iterates error for a given problem class, while preserving the algorithm's
convergence properties. Numerical experiments demonstrate that our learned
variant consistently improves convergence speed and solution quality compared
to standard ADMM. The code is available at
https://github.com/paulhausner/learning-distributed-admm.

</details>


### [92] [Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest](https://arxiv.org/abs/2509.05292)
*Xiao Yang,Mehdi Ben Ayed,Longyu Zhao,Fan Zhou,Yuchen Shen,Abe Engle,Jinfeng Zhuang,Ling Leng,Jiajing Xu,Charles Rosenberg,Prathibha Deshikachar*

Main category: cs.LG

TL;DR: 本文提出DRL - PUT框架解决广告推荐系统多目标优化问题，经实验比手动调优效果好。


<details>
  <summary>Details</summary>
Motivation: 传统手动调优在广告推荐系统的排名效用函数调优中存在目标不明确、参数组合多、缺乏个性化和季节性适应性等问题，导致结果欠佳。

Method: 1. 将问题转化为强化学习任务，根据广告请求状态预测最优超参数以最大化预定义奖励；2. 利用在线服务日志直接学习最优策略模型，避免估计值函数。

Result: 在Pinterest广告推荐系统的在线A/B实验中，与基线手动效用调优方法相比，DRL - PUT使点击率提高9.7%，长期点击率提高7.7%。

Conclusion: DRL - PUT能有效解决广告推荐系统多目标优化问题，提升推荐效果。

Abstract: The ranking utility function in an ad recommender system, which linearly
combines predictions of various business goals, plays a central role in
balancing values across the platform, advertisers, and users. Traditional
manual tuning, while offering simplicity and interpretability, often yields
suboptimal results due to its unprincipled tuning objectives, the vast amount
of parameter combinations, and its lack of personalization and adaptability to
seasonality. In this work, we propose a general Deep Reinforcement Learning
framework for Personalized Utility Tuning (DRL-PUT) to address the challenges
of multi-objective optimization within ad recommender systems. Our key
contributions include: 1) Formulating the problem as a reinforcement learning
task: given the state of an ad request, we predict the optimal hyperparameters
to maximize a pre-defined reward. 2) Developing an approach to directly learn
an optimal policy model using online serving logs, avoiding the need to
estimate a value function, which is inherently challenging due to the high
variance and unbalanced distribution of immediate rewards. We evaluated DRL-PUT
through an online A/B experiment in Pinterest's ad recommender system. Compared
to the baseline manual utility tuning approach, DRL-PUT improved the
click-through rate by 9.7% and the long click-through rate by 7.7% on the
treated segment. We conducted a detailed ablation study on the impact of
different reward definitions and analyzed the personalization aspect of the
learned policy model.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [93] [Scaling Environments for Organoid Intelligence with LLM-Automated Design and Plasticity-Based Evaluation](https://arxiv.org/abs/2509.04633)
*Brennen Hill*

Main category: cs.NE

TL;DR: 本文提出针对类器官生物智能体的训练框架，包括三种虚拟环境、元学习方法和评估学习的多模态方法，弥合计算神经科学与基于智能体的人工智能的差距。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能体复杂性增加，设计能有效塑造生物神经网络行为和能力的环境成为关键研究领域。

Method: 提出三种可扩展的闭环虚拟环境训练生物智能体，用大语言模型进行元学习优化实验协议，采用多模态方法评估学习。

Result: 设计了三种复杂度递增的任务环境，形式化状态和动作空间等，提出元学习和评估学习的方法。

Conclusion: 该工作为在可控生物基质中研究具身、学习和智能提供独特平台。

Abstract: As the complexity of artificial agents increases, the design of environments
that can effectively shape their behavior and capabilities has become a
critical research frontier. We propose a framework that extends this principle
to a novel class of agents: biological neural networks in the form of neural
organoids. This paper introduces three scalable, closed-loop virtual
environments designed to train organoid-based biological agents and probe the
underlying mechanisms of learning, such as long-term potentiation (LTP) and
long-term depression (LTD). We detail the design of three distinct task
environments with increasing complexity: (1) a conditional avoidance task, (2)
a one-dimensional predator-prey scenario, and (3) a replication of the classic
Pong game. For each environment, we formalize the state and action spaces, the
sensory encoding and motor decoding mechanisms, and the feedback protocols
based on predictable (reward) and unpredictable (punishment) stimulation.
Furthermore, we propose a novel meta-learning approach where a Large Language
Model (LLM) is used to automate the generation and optimization of experimental
protocols, scaling the process of environment and curriculum design. Finally,
we outline a multi-modal approach for evaluating learning by measuring synaptic
plasticity at electrophysiological, cellular, and molecular levels. This work
bridges the gap between computational neuroscience and agent-based AI, offering
a unique platform for studying embodiment, learning, and intelligence in a
controlled biological substrate.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [94] [High Performance Matrix Multiplication](https://arxiv.org/abs/2509.04594)
*Ethan Davis*

Main category: cs.PF

TL;DR: 本文比较了使用CuBLAS、CUDA、BLAS、OpenMP和C++ Threads的五种矩阵乘法算法性能，发现对于N至少为10000的方阵，按每秒浮点运算次数（FLOPS）衡量的性能排序为CuBLAS、CUDA、BLAS、OpenMP和C++ Threads。


<details>
  <summary>Details</summary>
Motivation: 矩阵乘法是深度学习、科学模拟和视频图形等高性能技术成功的基础，高级编程语言依赖优化的底层库执行矩阵乘法等核心线性代数运算，因此需要比较不同矩阵乘法算法的性能。

Method: 比较使用CuBLAS、CUDA、BLAS、OpenMP和C++ Threads的五种矩阵乘法算法的性能。

Result: 统计显著性p值低于5e - 12，支持对于N至少为10000的方阵，按FLOPS衡量的性能排序为CuBLAS、CUDA、BLAS、OpenMP和C++ Threads。

Conclusion: 对于N至少为10000的方阵，矩阵乘法算法按FLOPS衡量的性能排序为CuBLAS、CUDA、BLAS、OpenMP和C++ Threads。

Abstract: Matrix multiplication is the foundation from much of the success from high
performance technologies like deep learning, scientific simulations, and video
graphics. High level programming languages like Python and R rely on highly
optimized low level libraries for performing core linear algebra operations
like matrix multiplication from Basic Linear Algebra Subprograms (BLAS). This
paper compares the performance of five different matrix multiplication
algorithms using CuBLAS, CUDA, BLAS, OpenMP, and C++ Threads. We find
statistical significance with a p-value below 5e-12 to support the hypothesis
that for square $N \times N$ matrices where $N$ is at least 10,000 then the in
order performance as measured in floating point operations per second (FLOPS)
for these matrix multiplication algorithms is CuBLAS, CUDA, BLAS, OpenMP, and
C++ Threads.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [95] [Comparative Evaluation of Large Language Models for Test-Skeleton Generation](https://arxiv.org/abs/2509.04644)
*Subhang Boorlagadda,Nitya Naga Sai Atluri,Muhammet Mustafa Olmez,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 本文探索用大语言模型自动生成测试骨架，评估四个模型生成RSpec骨架的能力，发现不同模型表现差异及关键质量因素。


<details>
  <summary>Details</summary>
Motivation: 传统手动创建测试骨架耗时且易出错，尤其在教育或大规模开发场景，故探索用大语言模型自动化生成。

Method: 评估GPT - 4、DeepSeek - Chat、Llama4 - Maverick和Gemma2 - 9B四个大语言模型生成RSpec骨架的能力，用静态分析和专家盲审评估输出。

Result: DeepSeek生成的骨架最具可维护性和良好结构，GPT - 4生成的更完整但传统上不一致，提示设计和上下文输入是关键质量因素。

Conclusion: 研究揭示了模型在解释代码结构和测试约定上的差异，为使用大语言模型进行自动化测试搭建提供了实际挑战的见解。

Abstract: This paper explores the use of Large Language Models (LLMs) to automate the
generation of test skeletons -- structural templates that outline unit test
coverage without implementing full test logic. Test skeletons are especially
important in test-driven development (TDD), where they provide an early
framework for systematic verification. Traditionally authored manually, their
creation can be time-consuming and error-prone, particularly in educational or
large-scale development settings. We evaluate four LLMs -- GPT-4,
DeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate
RSpec skeletons for a real-world Ruby class developed in a university software
engineering course. Each model's output is assessed using static analysis and a
blind expert review to measure structural correctness, clarity,
maintainability, and conformance to testing best practices. The study reveals
key differences in how models interpret code structure and testing conventions,
offering insights into the practical challenges of using LLMs for automated
test scaffolding. Our results show that DeepSeek generated the most
maintainable and well-structured skeletons, while GPT-4 produced more complete
but conventionally inconsistent output. The study reveals prompt design and
contextual input as key quality factors.

</details>


### [96] [Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)](https://arxiv.org/abs/2509.04721)
*Abhishek Dey,Saurabh Srivastava,Gaurav Singh,Robert G. Pettit*

Main category: cs.SE

TL;DR: 提出PICO - TINYML - BENCHMARK框架用于在资源受限嵌入式系统上对TinyML模型实时性能进行基准测试，测试了三种模型在两个平台上的表现，给出优化建议。


<details>
  <summary>Details</summary>
Motivation: 在资源受限嵌入式系统上对TinyML模型的实时性能进行评估，了解计算权衡和平台特定优化。

Method: 使用PICO - TINYML - BENCHMARK框架，在BeagleBone AI64和Raspberry Pi 4两个平台上，用真实数据集对三种TinyML模型进行基准测试，评估推理延迟、CPU利用率等关键指标。

Result: BeagleBone AI64在AI特定任务上推理延迟稳定，Raspberry Pi 4在资源效率和成本效益方面表现出色。

Conclusion: 研究结果为优化TinyML部署提供了可操作的指导，缩小了嵌入式系统理论进展与实际应用之间的差距。

Abstract: This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic
framework for benchmarking the real-time performance of TinyML models on
resource-constrained embedded systems. Evaluating key metrics such as inference
latency, CPU utilization, memory efficiency, and prediction stability, the
framework provides insights into computational trade-offs and platform-specific
optimizations. We benchmark three representative TinyML models -- Gesture
Classification, Keyword Spotting, and MobileNet V2 -- on two widely adopted
platforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.
Results reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent
inference latency for AI-specific tasks, while the Raspberry Pi 4 excels in
resource efficiency and cost-effectiveness. These findings offer actionable
guidance for optimizing TinyML deployments, bridging the gap between
theoretical advancements and practical applications in embedded systems.

</details>


### [97] [NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation](https://arxiv.org/abs/2509.04763)
*Tiancheng Jin,Shangzhou Xia,Jianjun Zhao*

Main category: cs.SE

TL;DR: 本文介绍了量子程序测试框架NovaQ，它结合生成器和评估模块，能有效探索程序行为，实验显示比现有方法有更高输入多样性和更多错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算发展，确保量子程序可靠性愈发重要。

Method: 引入NovaQ框架，结合基于分布的测试用例生成器和新颖性驱动的评估模块，生成器通过变异电路参数产生多样输入，评估器基于内部电路状态指标量化行为新颖性。

Result: 在不同规模和复杂度的量子程序上评估，NovaQ始终能实现更高测试输入多样性，检测出更多错误。

Conclusion: NovaQ能有效探索量子程序未被充分测试的行为，优于现有基线方法。

Abstract: Quantum programs are designed to run on quantum computers, leveraging quantum
circuits to solve problems that are intractable for classical machines. As
quantum computing advances, ensuring the reliability of quantum programs has
become increasingly important. This paper introduces NovaQ, a diversity-guided
testing framework for quantum programs. NovaQ combines a distribution-based
test case generator with a novelty-driven evaluation module. The generator
produces diverse quantum state inputs by mutating circuit parameters, while the
evaluator quantifies behavioral novelty based on internal circuit state
metrics, including magnitude, phase, and entanglement. By selecting inputs that
map to infrequently covered regions in the metric space, NovaQ effectively
explores under-tested program behaviors. We evaluate NovaQ on quantum programs
of varying sizes and complexities. Experimental results show that NovaQ
consistently achieves higher test input diversity and detects more bugs than
existing baseline approaches.

</details>


### [98] [Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation](https://arxiv.org/abs/2509.04810)
*Yogev Cohen,Dudi Ohayon,Romy Somkin,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.SE

TL;DR: 利用大语言模型生成合成训练数据解决代码审查分类中标记数据不足问题，实验表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 现代开发流程中代码审查自动化需监督模型，但新编程语言和框架下标记数据不足，限制模型训练。

Method: 用大语言模型将资源丰富语言的代码变更转换为新兴语言的等价变更，生成合成训练数据；用合成数据训练监督分类器并与真实标记数据训练的模型对比。

Result: 跨多个GitHub仓库和语言对的实验显示，大语言模型生成的合成数据能有效助力审查推荐系统，缩小低资源场景下的性能差距。

Conclusion: 该方法为在无注释数据情况下扩展自动化代码审查能力提供了可扩展途径。

Abstract: Automating the decision of whether a code change requires manual review is
vital for maintaining software quality in modern development workflows.
However, the emergence of new programming languages and frameworks creates a
critical bottleneck: while large volumes of unlabelled code are readily
available, there is an insufficient amount of labelled data to train supervised
models for review classification. We address this challenge by leveraging Large
Language Models (LLMs) to translate code changes from well-resourced languages
into equivalent changes in underrepresented or emerging languages, generating
synthetic training data where labelled examples are scarce. We assume that
although LLMs have learned the syntax and semantics of new languages from
available unlabelled code, they have yet to fully grasp which code changes are
considered significant or review-worthy within these emerging ecosystems. To
overcome this, we use LLMs to generate synthetic change examples and train
supervised classifiers on them. We systematically compare the performance of
these classifiers against models trained on real labelled data. Our experiments
across multiple GitHub repositories and language pairs demonstrate that
LLM-generated synthetic data can effectively bootstrap review recommendation
systems, narrowing the performance gap even in low-resource settings. This
approach provides a scalable pathway to extend automated code review
capabilities to rapidly evolving technology stacks, even in the absence of
annotated data.

</details>


### [99] [Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining](https://arxiv.org/abs/2509.04877)
*Maryam Khan,Muhammad Azeem Akbar,Jussi Kasurinen*

Main category: cs.SE

TL;DR: 研究通过对GitHub项目的挖掘，分析大语言模型用于软件工程教育的激励和阻碍因素，为开发相关框架提供早期实证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件工程教育中应用有机会和挑战，需系统研究以确保合理融入课程，本研究旨在开发验证框架。

Method: 对400个GitHub项目进行试点仓库挖掘研究，分析README文件和问题讨论。

Result: 激励因素如参与度、对软件工程过程的理解等有较多体现；阻碍因素如剽窃和知识产权担忧等也较突出，部分阻碍因素未出现。

Conclusion: 为激励/阻碍因素分类法提供早期实证验证，指出研究实践差距，为开发全面框架奠定基础。

Abstract: Context: Large Language Models (LLMs) such as ChatGPT are increasingly
adopted in software engineering (SE) education, offering both opportunities and
challenges. Their adoption requires systematic investigation to ensure
responsible integration into curricula. Objective: This doctoral research aims
to develop a validated framework for integrating LLMs into SE education through
a multi-phase process, including taxonomies development, empirical
investigation, and case studies. This paper presents the first empirical step.
Method: We conducted a pilot repository mining study of 400 GitHub projects,
analyzing README files and issues discussions to identify the presence of
motivator and demotivator previously synthesized in our literature review [ 8]
study. Results: Motivators such as engagement and motivation (227 hits),
software engineering process understanding (133 hits), and programming
assistance and debugging support (97 hits) were strongly represented.
Demotivators, including plagiarism and IP concerns (385 hits), security,
privacy and data integrity (87 hits), and over-reliance on AI in learning (39
hits), also appeared prominently. In contrast, demotivators such as challenges
in evaluating learning outcomes and difficulty in curriculum redesign recorded
no hits across the repositories. Conclusion: The study provides early empirical
validation of motivators/demotivators taxonomies with respect to their themes,
highlights research practice gaps, and lays the foundation for developing a
comprehensive framework to guide the responsible adoption of LLMs in SE
education.

</details>


### [100] [FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage](https://arxiv.org/abs/2509.04967)
*Kai Feng,Jeremy Singer,Angelos K Marnerides*

Main category: cs.SE

TL;DR: 传统二进制模糊测试在代码覆盖和漏洞挖掘上有局限，本文引入FuzzRDUCC框架，利用符号执行重建def - use链，经测试能发现先进模糊器未找到的崩溃，是下一代漏洞检测可行方案。


<details>
  <summary>Details</summary>
Motivation: 传统二进制模糊测试因缺乏程序内部数据流信息，难以实现全面代码覆盖和挖掘隐藏漏洞，传统灰盒模糊器仅依赖控制流边缘覆盖引导测试用例生成，易遗漏仅靠控制流分析难以暴露的漏洞。

Method: 引入FuzzRDUCC框架，使用符号执行直接从二进制可执行文件中重建定义 - 使用链，通过新颖启发式算法选择相关链，且不影响模糊测试的全面性。

Result: 使用binutils基准测试评估FuzzRDUCC，发现其能识别出先进模糊器未找到的独特崩溃。

Conclusion: FuzzRDUCC是下一代漏洞检测和发现机制的可行解决方案。

Abstract: Binary-only fuzzing often struggles with achieving thorough code coverage and
uncovering hidden vulnerabilities due to limited insight into a program's
internal dataflows. Traditional grey-box fuzzers guide test case generation
primarily using control flow edge coverage, which can overlook bugs not easily
exposed through control flow analysis alone. We argue that integrating dataflow
analysis into the fuzzing process can enhance its effectiveness by revealing
how data propagates through the program, thereby enabling the exploration of
execution paths that control flow-based methods might miss. In this context, we
introduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution
to reconstruct definition-use (def-use) chains directly from binary
executables. FuzzRDUCC identifies crucial dataflow paths and exposes security
vulnerabilities without incurring excessive computational overhead, due to a
novel heuristic algorithm that selects relevant def-use chains without
affecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using
the binutils benchmark and demonstrate that it can identify unique crashes not
found by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible
solution for next generation vulnerability detection and discovery mechanisms.

</details>


### [101] [GenAI-based test case generation and execution in SDV platform](https://arxiv.org/abs/2509.05112)
*Denesa Zyberaj,Lukasz Mazur,Nenad Petrovic,Pankhuri Verma,Pascal Hirmer,Dirk Slama,Xiangwei Cheng,Alois Knoll*

Main category: cs.SE

TL;DR: 本文介绍GenAI驱动的自动化测试用例生成方法，用大模型转换需求为Gherkin测试用例，结合VSS建模，在digital.auto环境执行，以儿童存在检测系统评估，虽有自动化但仍需人工干预。


<details>
  <summary>Details</summary>
Motivation: 实现自动化测试用例生成，提高汽车子系统兼容性，简化与第三方测试工具集成，快速验证软件定义车辆功能。

Method: 利用大语言模型和视觉语言模型将自然语言需求和系统图转换为Gherkin测试用例，集成Vehicle Signal Specification建模，在digital.auto环境执行测试。

Result: 以儿童存在检测系统评估，大幅减少手动测试规范工作量，快速执行生成的测试。

Conclusion: 尽管有显著自动化，但因GenAI管道和digital.auto平台限制，测试用例和脚本生成仍需人工干预。

Abstract: This paper introduces a GenAI-driven approach for automated test case
generation, leveraging Large Language Models and Vision-Language Models to
translate natural language requirements and system diagrams into structured
Gherkin test cases. The methodology integrates Vehicle Signal Specification
modeling to standardize vehicle signal definitions, improve compatibility
across automotive subsystems, and streamline integration with third-party
testing tools. Generated test cases are executed within the digital.auto
playground, an open and vendor-neutral environment designed to facilitate rapid
validation of software-defined vehicle functionalities. We evaluate our
approach using the Child Presence Detection System use case, demonstrating
substantial reductions in manual test specification effort and rapid execution
of generated tests. Despite significant automation, the generation of test
cases and test scripts still requires manual intervention due to current
limitations in the GenAI pipeline and constraints of the digital.auto platform.

</details>


### [102] [AI Agents for Web Testing: A Case Study in the Wild](https://arxiv.org/abs/2509.05197)
*Naimeng Ye,Xiao Yu,Ruize Xu,Tianyi Peng,Zhou Yu*

Main category: cs.SE

TL;DR: 介绍基于AI代理的WebProber网络测试框架，经案例评估效果良好，凸显基于代理测试是有前景的方向。


<details>
  <summary>Details</summary>
Motivation: 传统网络测试方法难以捕捉复杂用户行为，检测不出很多可用性问题，大语言模型和AI代理带来新可能。

Method: 提出WebProber框架，可根据URL自主探索网站、模拟用户交互、识别问题并生成报告。

Result: 通过对120个学术个人网站的案例研究，WebProber发现29个可用性问题，很多是传统工具遗漏的。

Conclusion: 基于代理的测试是有前景的方向，为开发下一代以用户为中心的测试框架指明方向。

Abstract: Automated web testing plays a critical role in ensuring high-quality user
experiences and delivering business value. Traditional approaches primarily
focus on code coverage and load testing, but often fall short of capturing
complex user behaviors, leaving many usability issues undetected. The emergence
of large language models (LLM) and AI agents opens new possibilities for web
testing by enabling human-like interaction with websites and a general
awareness of common usability problems. In this work, we present WebProber, a
prototype AI agent-based web testing framework. Given a URL, WebProber
autonomously explores the website, simulating real user interactions,
identifying bugs and usability issues, and producing a human-readable report.
We evaluate WebProber through a case study of 120 academic personal websites,
where it uncovered 29 usability issues--many of which were missed by
traditional tools. Our findings highlight agent-based testing as a promising
direction while outlining directions for developing next-generation,
user-centered testing frameworks.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [103] [Directional Price Forecasting in the Continuous Intraday Market under Consideration of Neighboring Products and Limit Order Books](https://arxiv.org/abs/2509.04452)
*Timothée Hornek,Sergio Potenciano Menci,Ivan Pavić*

Main category: q-fin.ST

TL;DR: 文章指出当前电力市场价格预测方法存在不足，提出针对欧洲连续日内市场小时产品的新型电价预测方法，利用德交所数据评估，结果表明订单簿特征影响大，相邻产品特征可提高准确性，该方法应用于交易策略有盈利潜力。


<details>
  <summary>Details</summary>
Motivation: 可变可再生能源和灵活需求技术增加使电力系统不确定性和不平衡性增大，短期电力市场需电价预测辅助交易决策，但现有预测方法有简化问题。

Method: 提出针对欧洲连续日内市场小时产品的新型电价预测方法，纳入小时和一刻钟产品的短期特征，用2024 - 2025年德国欧洲电力交易所数据评估。

Result: 订单簿衍生特征是最具影响力的外生变量，相邻产品特征能提高预测准确性，该电价预测方法应用于交易策略有盈利潜力。

Conclusion: 所提出的电价预测方法有一定优势，应用于交易策略有产生利润的可能。

Abstract: The increasing penetration of variable renewable energy and flexible demand
technologies, such as electric vehicles and heat pumps, introduces significant
uncertainty in power systems, resulting in greater imbalance; defined as the
deviation between scheduled and actual supply or demand. Short-term power
markets, such as the European continuous intraday market, play a critical role
in mitigating these imbalances by enabling traders to adjust forecasts close to
real time. Due to the high volatility of the continuous intraday market,
traders increasingly rely on electricity price forecasting to guide trading
decisions and mitigate price risk. However most electricity price forecasting
approaches in the literature simplify the forecasting task. They focus on
single benchmark prices, neglecting intra-product price dynamics and price
signals from the limit order book. They also underuse high-frequency and
cross-product price data.
  In turn, we propose a novel directional electricity price forecasting method
for hourly products in the European continuous intraday market. Our method
incorporates short-term features from both hourly and quarter-hourly products
and is evaluated using German European Power Exchange data from 2024-2025. The
results indicate that features derived from the limit order book are the most
influential exogenous variables. In addition, features from neighboring
products; especially those with delivery start times that overlap with the
trading period of the target product; improve forecast accuracy. Finally, our
evaluation of the value captured by our electricity price forecasting suggests
that the proposed electricity price forecasting method has the potential to
generate profit when applied in trading strategies.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [104] [Dynamics of Liquidity Surfaces in Uniswap v3](https://arxiv.org/abs/2509.05013)
*Jimmy Risk,Shen-Ning Tung,Tai-Ho Wang*

Main category: q-fin.TR

TL;DR: 本文对Uniswap v3流动性的经验动态进行研究，用FPCA和动态因子方法分析三个池，有三项主要贡献，5 bps池主导特征函数稳定且与低阶勒让德多项式基契合，因子系数可用AR(1)模型描述。


<details>
  <summary>Details</summary>
Motivation: 对Uniswap v3流动性的经验动态进行全面研究。

Method: 结合使用功能主成分分析（FPCA）和动态因子方法，对三个不同的池在多个样本期进行分析。

Result: 发现有三项主要贡献；5 bps池主导经验特征函数解释多数跨点变化且稳定，与低阶勒让德多项式基紧密契合；因子系数时间序列结构可用AR(1)模型描述，有明显GARCH型异方差和重尾创新。

Conclusion: 为自动做市商流动性提供统计特征，给出可解释和可移植的降维基础，用滚动窗口指标对流动性动态进行稳健分析。

Abstract: This paper presents a comprehensive study on the empirical dynamics of
Uniswap v3 liquidity, which we model as a time-tick surface, $L_t(x)$. Using a
combination of functional principal component analysis (FPCA) and dynamic
factor methods, we analyze three distinct pools over multiple sample periods.
Our findings offer three main contributions: a statistical characterization of
automated market maker liquidity, an interpretable and portable basis for
dimension reduction, and a robust analysis of liquidity dynamics using rolling
window metrics. For the 5 bps pools, the leading empirical eigenfunctions
explain the majority of cross-tick variation and remain stable, aligning
closely with a low-order Legendre polynomial basis. This alignment provides a
parsimonious and interpretable structure, similar to the dynamic Nelson-Siegel
method for yield curves. The factor coefficients exhibit a time series
structure well-captured by AR(1) models with clear GARCH-type
heteroskedasticity and heavy-tailed innovations.

</details>


### [105] [The Subtle Interplay between Square-root Impact, Order Imbalance & Volatility II: An Artificial Market Generator](https://arxiv.org/abs/2509.05065)
*Guillaume Maitrier,Grégoire Loeper,Jean-Philippe Bouchaud*

Main category: q-fin.TR

TL;DR: 本文扩展并补充之前理论论文，生成合成市场数据验证近似合理，支持价格波动可由相关元订单叠加解释的结论，还验证模型预测及构造代理元订单。


<details>
  <summary>Details</summary>
Motivation: 扩展和补充之前关于影响、订单流和波动性之间微妙相互作用的理论论文。

Method: 按照之前论文的规范生成合成市场数据。

Result: 验证之前的近似合理，观察并再现模型关于广义订单流和回报相关性的预测，构造能再现市场影响平方根定律的代理元订单。

Conclusion: 价格波动可由相关元订单叠加解释，可从交易数据测量真实元订单影响。

Abstract: This work extends and complements our previous theoretical paper on the
subtle interplay between impact, order flow and volatility. In the present
paper, we generate synthetic market data following the specification of that
paper and show that the approximations made there are actually justified, which
provides quantitative support our conclusion that price volatility can be fully
explained by the superposition of correlated metaorders which all impact
prices, on average, as a square-root of executed volume. One of the most
striking predictions of our model is the structure of the correlation between
generalized order flow and returns, which is observed empirically and
reproduced using our synthetic market generator. Furthermore, we were able to
construct proxy metaorders from our simulated order flow that reproduce the
square-root law of market impact, lending further credence to the proposal made
in Ref. [2] to measure the impact of real metaorders from tape data (i.e.
anonymized trades), which was long thought to be impossible.

</details>


### [106] [MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial Trading](https://arxiv.org/abs/2509.05080)
*Yang Chen,Yueheng Jiang,Zhaozhao Ma,Yuchen Cao Jacky Keung,Kun Kuang,Leilei Gan,Yiquan Wu,Fei Wu*

Main category: q-fin.TR

TL;DR: 提出基于大语言模型的MM - DREX框架解决现有量化交易模型问题，实验表明其表现优于基线模型且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有量化交易模型因金融市场非平稳性和多模态信息复杂性存在局限，传统方法难适应市场变化，大语言模型驱动方案缺乏动态调整和细粒度决策机制。

Method: 提出MM - DREX框架，包括VLM驱动的动态路由器分配专家权重、设计四种异构交易专家生成子策略、采用SFT - RL混合训练范式优化。

Result: 在多模态数据集上实验，MM - DREX在关键指标上显著优于15个基线模型。

Conclusion: MM - DREX具有鲁棒性和泛化性，可解释性模块能提供策略透明性审计跟踪。

Abstract: The inherent non-stationarity of financial markets and the complexity of
multi-modal information pose significant challenges to existing quantitative
trading models. Traditional methods relying on fixed structures and unimodal
data struggle to adapt to market regime shifts, while large language model
(LLM)-driven solutions - despite their multi-modal comprehension - suffer from
static strategies and homogeneous expert designs, lacking dynamic adjustment
and fine-grained decision mechanisms. To address these limitations, we propose
MM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on
large language models. MM-DREX explicitly decouples market state perception
from strategy execution to enable adaptive sequential decision-making in
non-stationary environments. Specifically, it (1) introduces a vision-language
model (VLM)-powered dynamic router that jointly analyzes candlestick chart
patterns and long-term temporal features to allocate real-time expert weights;
(2) designs four heterogeneous trading experts (trend, reversal, breakout,
positioning) generating specialized fine-grained sub-strategies; and (3)
proposes an SFT-RL hybrid training paradigm to synergistically optimize the
router's market classification capability and experts' risk-adjusted
decision-making. Extensive experiments on multi-modal datasets spanning stocks,
futures, and cryptocurrencies demonstrate that MM-DREX significantly
outperforms 15 baselines (including state-of-the-art financial LLMs and deep
reinforcement learning models) across key metrics: total return, Sharpe ratio,
and maximum drawdown, validating its robustness and generalization.
Additionally, an interpretability module traces routing logic and expert
behavior in real time, providing an audit trail for strategy transparency.

</details>


### [107] [Painting the market: generative diffusion models for financial limit order book simulation and forecasting](https://arxiv.org/abs/2509.05107)
*Alfred Backhouse,Kang Li,Jakob Foerster,Anisoara Calinescu,Stefan Zohren*

Main category: q-fin.TR

TL;DR: 本文提出将限价订单簿（LOB）数据转换为结构化图像格式，应用带修复的扩散模型生成未来LOB状态，在LOB - Bench上取得了最先进的性能，为LOB建模的生成扩散方法研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 模拟LOB在金融市场数据预测和回测中有重要应用，但现有深度生成模型因数据高噪声和复杂性而表现不佳，自回归模型存在长序列误差累积问题。

Method: 将LOB数据转换为结构化图像格式，应用带修复的扩散模型生成未来LOB状态，还为行业基准LOB - Bench做出贡献以实现模型公平比较。

Result: 模型在LOB - Bench上取得最先进性能，即使使用低保真度数据作为输入；在某些指标上比现有方法有显著改进。

Conclusion: 该方法为未来LOB建模的生成扩散方法研究奠定了坚实基础。

Abstract: Simulating limit order books (LOBs) has important applications across
forecasting and backtesting for financial market data. However, deep generative
models struggle in this context due to the high noise and complexity of the
data. Previous work uses autoregressive models, although these experience error
accumulation over longer-time sequences. We introduce a novel approach,
converting LOB data into a structured image format, and applying diffusion
models with inpainting to generate future LOB states. This method leverages
spatio-temporal inductive biases in the order book and enables parallel
generation of long sequences overcoming issues with error accumulation. We also
publicly contribute to LOB-Bench, the industry benchmark for LOB generative
models, to allow fair comparison between models using Level-2 and Level-3 order
book data (with or without message level data respectively). We show that our
model achieves state-of-the-art performance on LOB-Bench, despite using lower
fidelity data as input. We also show that our method prioritises coherent
global structures over local, high-fidelity details, providing significant
improvements over existing methods on certain metrics. Overall, our method lays
a strong foundation for future research into generative diffusion approaches to
LOB modelling.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [108] [Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment](https://arxiv.org/abs/2509.04852)
*Wei Chen,Shigui Li,Jiacheng Li,Jian Xu,Zhiqi Lin,Junmei Yang,Delu Zeng,John Paisley,Qibin Zhao*

Main category: stat.ML

TL;DR: 提出ISA - DRE框架用于密度比估计，无需数值积分，有课程策略提升稳定性，经验上精度有竞争力且评估次数少、推理快。


<details>
  <summary>Details</summary>
Motivation: 现有密度比估计方法常牺牲精度换效率。

Method: 提出ISA - DRE框架，学习全局割线函数，利用割线对齐恒等式；引入收缩区间退火课程策略。

Result: 与先前方法相比，ISA - DRE以显著更少的函数评估次数达到有竞争力的精度，推理更快。

Conclusion: ISA - DRE适合实时和交互式应用。

Abstract: Estimating density ratios is a fundamental problem in machine learning, but
existing methods often trade off accuracy for efficiency. We propose
\textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)},
a framework that enables accurate, any-step estimation without numerical
integration.
  Instead of modeling infinitesimal tangents as in prior methods, ISA-DRE
learns a global secant function, defined as the expectation of all tangents
over an interval, with provably lower variance, making it more suitable for
neural approximation. This is made possible by the \emph{Secant Alignment
Identity}, a self-consistency condition that formally connects the secant with
its underlying tangent representations.
  To mitigate instability during early training, we introduce \emph{Contraction
Interval Annealing}, a curriculum strategy that gradually expands the alignment
interval during training. This process induces a contraction mapping, which
improves convergence and training stability.
  Empirically, ISA-DRE achieves competitive accuracy with significantly fewer
function evaluations compared to prior methods, resulting in much faster
inference and making it well suited for real-time and interactive applications.

</details>


### [109] [Optimal Variance and Covariance Estimation under Differential Privacy in the Add-Remove Model and Beyond](https://arxiv.org/abs/2509.04919)
*Shokichi Takakura,Seng Pei Liew,Satoshi Hasegawa*

Main category: stat.ML

TL;DR: 研究差分隐私下增删模型中数据集方差和协方差估计问题，提出基于贝塞尔机制的高效估计机制，证明其在高隐私机制下的最优性，分析实例效用并展示其在其他统计任务的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对增删模型下数据集方差和协方差估计探索较少且该模型因需保护数据集大小隐私而更具挑战性。

Method: 基于贝塞尔机制（一种利用伯恩斯坦基的新型矩释放框架）开发方差和协方差估计机制。

Result: 证明所提机制在高隐私机制下是极小极大最优的，贝塞尔估计器在实例效用上优于其他机制，且贝塞尔机制可用于其他统计任务。

Conclusion: 贝塞尔机制在差分隐私下增删模型的方差和协方差估计及其他统计任务中有效。

Abstract: In this paper, we study the problem of estimating the variance and covariance
of datasets under differential privacy in the add-remove model. While
estimation in the swap model has been extensively studied in the literature,
the add-remove model remains less explored and more challenging, as the dataset
size must also be kept private. To address this issue, we develop efficient
mechanisms for variance and covariance estimation based on the \emph{B\'{e}zier
mechanism}, a novel moment-release framework that leverages Bernstein bases. We
prove that our proposed mechanisms are minimax optimal in the high-privacy
regime by establishing new minimax lower bounds. Moreover, beyond worst-case
scenarios, we analyze instance-wise utility and show that the B\'{e}zier-based
estimator consistently achieves better utility compared to alternative
mechanisms. Finally, we demonstrate the effectiveness of the B\'{e}zier
mechanism beyond variance and covariance estimation, showcasing its
applicability to other statistical tasks.

</details>


### [110] [Spectral Algorithms in Misspecified Regression: Convergence under Covariate Shift](https://arxiv.org/abs/2509.05106)
*Ren-Rui Liu,Zheng-Chu Guo*

Main category: stat.ML

TL;DR: 本文研究谱算法在协变量偏移下的收敛性质，引入重要性权重，分析了误设情况，给出收敛率。


<details>
  <summary>Details</summary>
Motivation: 研究谱算法在协变量偏移下的收敛性质，解决源域和目标域输入边际分布不同的问题，拓展经典核学习理论到更实际场景。

Method: 将重要性权重融入学习框架，采用截断技术；在再生核希尔伯特空间（RKHS）进行非参数回归。

Result: 在目标函数属于RKHS时建立了极小极大最优收敛率；对无界重要性权重，采用截断技术在温和正则条件下获得近最优收敛率，并拓展到误设情况。

Conclusion: 该工作将经典核学习理论拓展到更实际场景，提供了理解协变量偏移和模型误设相互作用的系统框架。

Abstract: This paper investigates the convergence properties of spectral algorithms --
a class of regularization methods originating from inverse problems -- under
covariate shift. In this setting, the marginal distributions of inputs differ
between source and target domains, while the conditional distribution of
outputs given inputs remains unchanged. To address this distributional
mismatch, we incorporate importance weights, defined as the ratio of target to
source densities, into the learning framework. This leads to a weighted
spectral algorithm within a nonparametric regression setting in a reproducing
kernel Hilbert space (RKHS). More importantly, in contrast to prior work that
largely focuses on the well-specified setting, we provide a comprehensive
theoretical analysis of the more challenging misspecified case, in which the
target function does not belong to the RKHS. Under the assumption of uniformly
bounded density ratios, we establish minimax-optimal convergence rates when the
target function lies within the RKHS. For scenarios involving unbounded
importance weights, we introduce a novel truncation technique that attains
near-optimal convergence rates under mild regularity conditions, and we further
extend these results to the misspecified regime. By addressing the intertwined
challenges of covariate shift and model misspecification, this work extends
classical kernel learning theory to more practical scenarios, providing a
systematic framework for understanding their interaction.

</details>


### [111] [Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations](https://arxiv.org/abs/2509.05186)
*Benjamin J. Zhang,Siting Liu,Stanley J. Osher,Markos A. Katsoulakis*

Main category: stat.ML

TL;DR: 本文提出概率框架揭示ICON隐式执行贝叶斯推理，并将其扩展到生成式设置GenICON以实现不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 理解ICON方法原理并扩展其应用场景，实现算子学习中解预测的不确定性量化。

Method: 提出概率框架，利用随机微分方程形式描述ICON任务，将ICON扩展到生成式设置。

Result: 揭示ICON隐式执行贝叶斯推理，提出GenICON捕捉解算子的潜在不确定性。

Conclusion: 概率视角为ICON扩展到生成式设置提供基础，GenICON可实现算子学习中解预测的不确定性量化。

Abstract: In-context operator networks (ICON) are a class of operator learning methods
based on the novel architectures of foundation models. Trained on a diverse set
of datasets of initial and boundary conditions paired with corresponding
solutions to ordinary and partial differential equations (ODEs and PDEs), ICON
learns to map example condition-solution pairs of a given differential equation
to an approximation of its solution operator. Here, we present a probabilistic
framework that reveals ICON as implicitly performing Bayesian inference, where
it computes the mean of the posterior predictive distribution over solution
operators conditioned on the provided context, i.e., example condition-solution
pairs. The formalism of random differential equations provides the
probabilistic framework for describing the tasks ICON accomplishes while also
providing a basis for understanding other multi-operator learning methods. This
probabilistic perspective provides a basis for extending ICON to
\emph{generative} settings, where one can sample from the posterior predictive
distribution of solution operators. The generative formulation of ICON
(GenICON) captures the underlying uncertainty in the solution operator, which
enables principled uncertainty quantification in the solution predictions in
operator learning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [112] [MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer](https://arxiv.org/abs/2303.06298)
*Samir Mitha,Seungho Choe,Pejman Jahbedar Maralani,Alan R. Moody,April Khademi*

Main category: cs.CV

TL;DR: 提出MLP - SRGAN架构用于图像上采样，在多中心数据集测试，结果优于现有方法且代码开源


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率方法存在不足，需要更好的架构提升图像上采样性能

Method: 提出MLP - SRGAN架构，结合多层感知机混合器和卷积层在切片方向上采样，用MSSEG2数据集训练验证，在多中心数据集测试，用PSNR、SSIM和新指标评估

Result: MLP - SRGAN边缘更锐利、模糊更少、保留更多纹理和细节，参数少、训练/评估快、模型小

Conclusion: MLP - SRGAN在图像超分辨率任务中表现优秀，是一种有效的方法

Abstract: We propose a novel architecture called MLP-SRGAN, which is a single-dimension
Super Resolution Generative Adversarial Network (SRGAN) that utilizes
Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to
upsample in the slice direction. MLP-SRGAN is trained and validated using high
resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was
applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with
low spatial resolution in the slice dimension to examine performance on
held-out (unseen) clinical data. Upsampled results are compared to several
state-of-the-art SR networks. For images with high resolution (HR) ground
truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index
(SSIM) are used to measure upsampling performance. Several new structural,
no-reference image quality metrics were proposed to quantify sharpness (edge
strength), noise (entropy), and blurriness (low frequency information) in the
absence of ground truths. Results show MLP-SRGAN results in sharper edges, less
blurring, preserves more texture and fine-anatomical detail, with fewer
parameters, faster training/evaluation time, and smaller model size than
existing methods. Code for MLP-SRGAN training and inference, data generators,
models and no-reference image quality metrics will be available at
https://github.com/IAMLAB-Ryerson/MLP-SRGAN.

</details>


### [113] [Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge](https://arxiv.org/abs/2509.03614)
*Seungho Choe,Xiaoli Qin,Abubakr Shafique,Amanda Dy,Susan Done,Dimitrios Androutsos,April Khademi*

Main category: cs.CV

TL;DR: 本文提出教师 - 学生模型解决有丝分裂检测和非典型有丝分裂分类问题，结合分割和分类，在初步测试集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 病理学家手动计数有丝分裂耗时且存在观察者间差异，现有AI工具易受领域偏移影响，检测任务数据严重不平衡。

Method: 将有丝分裂检测转化为像素级分割，采用基于UNet分割主干并集成领域泛化模块的教师 - 学生模型，引入多尺度CNN分类器进行多任务学习。

Result: 在初步测试集上，Track 1的F1分数为0.7660，Track 2的平衡准确率为0.8414。

Conclusion: 将基于分割的检测和分类集成到统一框架对稳健有丝分裂分析有效。

Abstract: Counting mitotic figures is time-intensive for pathologists and leads to
inter-observer variability. Artificial intelligence (AI) promises a solution by
automatically detecting mitotic figures while maintaining decision consistency.
However, AI tools are susceptible to domain shift, where a significant drop in
performance can occur due to differences in the training and testing sets,
including morphological diversity between organs, species, and variations in
staining protocols. Furthermore, the number of mitoses is much less than the
count of normal nuclei, which introduces severely imbalanced data for the
detection task. In this work, we formulate mitosis detection as a pixel-level
segmentation and propose a teacher-student model that simultaneously addresses
mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our
method is based on a UNet segmentation backbone that integrates domain
generalization modules, namely contrastive representation learning and
domain-adversarial training. A teacher-student strategy is employed to generate
pixel-level pseudo-masks not only for annotated mitoses and hard negatives but
also for normal nuclei, thereby enhancing feature discrimination and improving
robustness against domain shift. For the classification task, we introduce a
multi-scale CNN classifier that leverages feature maps from the segmentation
model within a multi-task learning paradigm. On the preliminary test set, the
algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of
0.8414 in Track 2, demonstrating the effectiveness of integrating
segmentation-based detection and classification into a unified framework for
robust mitosis analysis.

</details>


### [114] [VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation](https://arxiv.org/abs/2509.04669)
*Mustafa Munir,Alex Zhang,Radu Marculescu*

Main category: cs.CV

TL;DR: 本文提出VCMamba融合CNN和Mamba SSM优势，实验表明其在图像分类和语义分割任务上效果好且参数少


<details>
  <summary>Details</summary>
Motivation: ViTs和SSMs难以有效捕捉细粒度局部特征，CNNs缺乏全局推理能力，需结合两者优势

Method: 采用卷积主干和分层结构，早期用卷积块提取局部特征，后期用多向Mamba块建模长距离依赖和全局上下文

Result: VCMamba - B在ImageNet - 1K上top - 1准确率达82.6%，在ADE20K上mIoU为47.1，且参数大幅减少

Conclusion: VCMamba有效结合了CNN和Mamba SSM的优势，实现了更好的特征表示和线性复杂度

Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs)
have challenged the dominance of Convolutional Neural Networks (CNNs) in
computer vision. ViTs excel at capturing global context, and SSMs like Mamba
offer linear complexity for long sequences, yet they do not capture
fine-grained local features as effectively as CNNs. Conversely, CNNs possess
strong inductive biases for local features but lack the global reasoning
capabilities of transformers and Mamba. To bridge this gap, we introduce
\textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs
and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a
hierarchical structure with convolutional blocks in its early stages to extract
rich local features. These convolutional blocks are then processed by later
stages incorporating multi-directional Mamba blocks designed to efficiently
model long-range dependencies and global context. This hybrid design allows for
superior feature representation while maintaining linear complexity with
respect to image resolution. We demonstrate VCMamba's effectiveness through
extensive experiments on ImageNet-1K classification and ADE20K semantic
segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K,
surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming
Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains
47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing
62% fewer parameters. Code is available at
https://github.com/Wertyuui345/VCMamba.

</details>


### [115] [Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization](https://arxiv.org/abs/2509.04735)
*Dharsan Ravindran,Kevin Wang,Zhuoyuan Cao,Saleh Abdelrahman,Jeffery Wu*

Main category: cs.CV

TL;DR: 现有视觉基础模型在恶劣天气图像分割表现不佳，本文研究两种提升自动驾驶分割鲁棒性的方法，实验表明其有更好表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型在高视觉模糊的恶劣天气条件下因缺乏不确定性量化而表现不佳，受医学影像启发，提升自动驾驶分割鲁棒性。

Method: 一是对SAM2采用多步微调程序，将不确定性指标纳入损失函数；二是将用于医学图像分割的UAT应用于驾驶场景。

Result: 在多个驾驶数据集上实验，UAT - SAM在极端天气下优于标准SAM，带不确定性感知损失的SAM2在不同驾驶场景表现更好。

Conclusion: 在具有挑战性的环境中，显式不确定性建模对安全关键的自动驾驶有重要价值。

Abstract: Recent advances in vision foundation models, such as the Segment Anything
Model (SAM) and its successor SAM2, have achieved state-of-the-art performance
on general image segmentation benchmarks. However, these models struggle in
adverse weather conditions where visual ambiguity is high, largely due to their
lack of uncertainty quantification. Inspired by progress in medical imaging,
where uncertainty-aware training has improved reliability in ambiguous cases,
we investigate two approaches to enhance segmentation robustness for autonomous
driving. First, we introduce a multi-step finetuning procedure for SAM2 that
incorporates uncertainty metrics directly into the loss function, improving
overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter
(UAT), originally designed for medical image segmentation, to driving contexts.
We evaluate both methods on CamVid, BDD100K, and GTA driving datasets.
Experiments show that UAT-SAM outperforms standard SAM in extreme weather,
while SAM2 with uncertainty-aware loss achieves improved performance across
diverse driving scenes. These findings underscore the value of explicit
uncertainty modeling for safety-critical autonomous driving in challenging
environments.

</details>


### [116] [Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation](https://arxiv.org/abs/2509.04816)
*Svetlana Pavlitska,Beyza Keskin,Alwin Faßbender,Christian Hubschneider,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 论文探讨从混合专家（MoE）模型提取预测不确定性估计，对比不同方法，发现MoE比集成方法在分布外数据下更可靠，简单门控机制校准更好，增加专家数量可提升不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 准确且校准良好的预测不确定性估计对提升计算机视觉模型可靠性很重要，尤其是在安全关键应用中，探索从MoE模型提取不确定性估计的有效方法。

Method: 研究三种提取预测不确定性估计的方法：预测熵、互信息和专家方差，评估基于A2D2数据集语义分割训练的双专家MoE，还评估通过门熵计算的路由不确定性。

Result: MoE在分布外数据的条件正确性指标上比集成方法产生更可靠的不确定性估计；简单门控机制比复杂的按类门控对路由不确定性估计校准更好；在Cityscapes数据集上增加专家数量可进一步提升不确定性校准。

Conclusion: 无需对MoE架构修改就能提取校准良好的预测不确定性估计，MoE在不确定性估计上比集成方法有优势，简单门控机制和增加专家数量有助于提升不确定性校准。

Abstract: Estimating accurate and well-calibrated predictive uncertainty is important
for enhancing the reliability of computer vision models, especially in
safety-critical applications like traffic scene perception. While ensemble
methods are commonly used to quantify uncertainty by combining multiple models,
a mixture of experts (MoE) offers an efficient alternative by leveraging a
gating network to dynamically weight expert predictions based on the input.
Building on the promising use of MoEs for semantic segmentation in our previous
works, we show that well-calibrated predictive uncertainty estimates can be
extracted from MoEs without architectural modifications. We investigate three
methods to extract predictive uncertainty estimates: predictive entropy, mutual
information, and expert variance. We evaluate these methods for an MoE with two
experts trained on a semantical split of the A2D2 dataset. Our results show
that MoEs yield more reliable uncertainty estimates than ensembles in terms of
conditional correctness metrics under out-of-distribution (OOD) data.
Additionally, we evaluate routing uncertainty computed via gate entropy and
find that simple gating mechanisms lead to better calibration of routing
uncertainty estimates than more complex classwise gates. Finally, our
experiments on the Cityscapes dataset suggest that increasing the number of
experts can further enhance uncertainty calibration. Our code is available at
https://github.com/KASTEL-MobilityLab/mixtures-of-experts/.

</details>


### [117] [MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery](https://arxiv.org/abs/2509.04757)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: 提出MCANet用于飓风后灾害评估，在RescueNet数据集表现佳，输出可辅助决策，未来将集成知识图谱和大模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN方法难捕捉多尺度空间特征及区分相似损伤类型，需快速准确的飓风后灾害评估方法。

Method: 提出MCANet多标签分类框架，用Res2Net骨干丰富空间上下文，多头部特定类别的残差注意力模块增强区分能力。

Result: 在RescueNet数据集上mAP达91.75%，8个注意力头时达92.35%，提升难分类别的AP，能定位损伤区域。

Conclusion: MCANet有效，输出可用于灾后决策，未来可集成知识图谱和大模型提升适应性和语义理解。

Abstract: Rapid and accurate post-hurricane damage assessment is vital for disaster
response and recovery. Yet existing CNN-based methods struggle to capture
multi-scale spatial features and to distinguish visually similar or
co-occurring damage types. To address these issues, we propose MCANet, a
multi-label classification framework that learns multi-scale representations
and adaptively attends to spatially relevant regions for each damage category.
MCANet employs a Res2Net-based hierarchical backbone to enrich spatial context
across scales and a multi-head class-specific residual attention module to
enhance discrimination. Each attention branch focuses on different spatial
granularities, balancing local detail with global context. We evaluate MCANet
on the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael.
MCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet,
Res2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads,
performance further improves to 92.35%, boosting average precision for
challenging classes such as Road Blocked by over 6%. Class activation mapping
confirms MCANet's ability to localize damage-relevant regions, supporting
interpretability. Outputs from MCANet can inform post-disaster risk mapping,
emergency routing, and digital twin-based disaster response. Future work could
integrate disaster-specific knowledge graphs and multimodal large language
models to improve adaptability to unseen disasters and enrich semantic
understanding for real-world decision-making.

</details>


### [118] [FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph](https://arxiv.org/abs/2509.04772)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: 提出FloodVision零样本框架用于准确泛化的洪水深度估计，评估效果好且适合集成到相关平台和应用。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉方法在洪水检测中存在精度和泛化性问题，需要能跨不同洪水场景准确估计深度的方法。

Method: 结合GPT - 4o的语义推理能力和结构化领域知识图谱，动态识别参考对象、获取高度、估计淹没比和应用统计离群值过滤。

Result: 在110张众包图像上，平均绝对误差8.17 cm，比GPT - 4o基线降低20.5%，超越先前基于CNN的方法。

Conclusion: FloodVision泛化性好、接近实时运行，适合集成到数字孪生平台和公民报告应用以增强智慧城市洪水恢复能力。

Abstract: Timely and accurate floodwater depth estimation is critical for road
accessibility and emergency response. While recent computer vision methods have
enabled flood detection, they suffer from both accuracy limitations and poor
generalization due to dependence on fixed object detectors and task-specific
training. To enable accurate depth estimation that can generalize across
diverse flood scenarios, this paper presents FloodVision, a zero-shot framework
that combines the semantic reasoning abilities of the foundation
vision-language model GPT-4o with a structured domain knowledge graph. The
knowledge graph encodes canonical real-world dimensions for common urban
objects including vehicles, people, and infrastructure elements to ground the
model's reasoning in physical reality. FloodVision dynamically identifies
visible reference objects in RGB images, retrieves verified heights from the
knowledge graph to mitigate hallucination, estimates submergence ratios, and
applies statistical outlier filtering to compute final depth values. Evaluated
on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean
absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and
surpassing prior CNN-based methods. The system generalizes well across varying
scenes and operates in near real-time, making it suitable for future
integration into digital twin platforms and citizen-reporting apps for smart
city flood resilience.

</details>


### [119] [SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models](https://arxiv.org/abs/2509.04889)
*Dominik Pegler,David Steyrl,Mengfan Zhang,Alexander Karner,Jozsef Arato,Frank Scharnowski,Filip Melinscak*

Main category: cs.CV

TL;DR: 研究预训练计算机视觉模型预测蜘蛛图像恐惧水平的能力，评估模型性能并分析影响因素，证明可解释模型和足够数据集大小的重要性。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉发展为临床应用带来新途径，研究预训练计算机视觉模型能否准确从蜘蛛相关图像预测恐惧水平，作为自适应系统的关键一步。

Method: 采用迁移学习调整三种不同模型，从313张图像的标准化数据集中预测人类恐惧评分，用交叉验证评估模型。

Result: 模型平均平均绝对误差在10.1 - 11.0之间；减少数据集大小显著损害性能，增加无实质提升；模型预测基于蜘蛛相关特征；识别出高误差的视觉条件。

Conclusion: 可解释的计算机视觉模型在预测恐惧评分方面有潜力，开发有效的情感感知治疗技术需重视模型可解释性和足够的数据集大小。

Abstract: Advances in computer vision have opened new avenues for clinical
applications, particularly in computerized exposure therapy where visual
stimuli can be dynamically adjusted based on patient responses. As a critical
step toward such adaptive systems, we investigated whether pretrained computer
vision models can accurately predict fear levels from spider-related images. We
adapted three diverse models using transfer learning to predict human fear
ratings (on a 0-100 scale) from a standardized dataset of 313 images. The
models were evaluated using cross-validation, achieving an average mean
absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis
revealed that reducing the dataset size significantly harmed performance,
though further increases yielded no substantial gains. Explainability
assessments showed the models' predictions were based on spider-related
features. A category-wise error analysis further identified visual conditions
associated with higher errors (e.g., distant views and artificial/painted
spiders). These findings demonstrate the potential of explainable computer
vision models in predicting fear ratings, highlighting the importance of both
model explainability and a sufficient dataset size for developing effective
emotion-aware therapeutic technologies.

</details>


### [120] [SynGen-Vision: Synthetic Data Generation for training industrial vision models](https://arxiv.org/abs/2509.04894)
*Alpana Dubey,Suma Mani Kuriakose,Nitish Bhardwaj*

Main category: cs.CV

TL;DR: 提出用合成数据训练工业磨损检测CV模型的方法，该方法生成的合成数据训练的模型表现出色且可扩展。


<details>
  <summary>Details</summary>
Motivation: 工业磨损检测数据整理因缺乏不同场景数据集而昂贵耗时，需生成合成数据。

Method: 使用视觉语言模型和3D模拟渲染引擎生成不同锈蚀情况的合成数据。

Result: 用合成数据训练的CV模型在真实图像测试中mAP50得分为0.87，优于其他方法。

Conclusion: 该方法可定制，能轻松扩展到其他工业磨损检测场景。

Abstract: We propose an approach to generate synthetic data to train computer vision
(CV) models for industrial wear and tear detection. Wear and tear detection is
an important CV problem for predictive maintenance tasks in any industry.
However, data curation for training such models is expensive and time-consuming
due to the unavailability of datasets for different wear and tear scenarios.
Our approach employs a vision language model along with a 3D simulation and
rendering engine to generate synthetic data for varying rust conditions. We
evaluate our approach by training a CV model for rust detection using the
generated dataset and tested the trained model on real images of rusted
industrial objects. The model trained with the synthetic data generated by our
approach, outperforms the other approaches with a mAP50 score of 0.87. The
approach is customizable and can be easily extended to other industrial wear
and tear detection scenarios

</details>


### [121] [Evaluating Multiple Instance Learning Strategies for Automated Sebocyte Droplet Counting](https://arxiv.org/abs/2509.04895)
*Maryam Adelipour,Gustavo Carneiro,Jeongkwon Kim*

Main category: cs.CV

TL;DR: 本文介绍基于注意力的多实例学习框架用于皮脂腺细胞图像分析，对比基线MLP模型，发现简单袋级聚合可提供稳健基线，注意力MIL需调整以发挥潜力。


<details>
  <summary>Details</summary>
Motivation: 手动计数皮脂腺细胞内脂滴数量劳动强度大且主观，需自动化解决方案。

Method: 将尼罗红染色的皮脂腺细胞图像按脂滴数量分为14类，通过数据增强扩展至约50000个细胞。对比基线多层感知器（MLP）和基于注意力的多实例学习（MIL）模型。

Result: 五折交叉验证实验显示，基线MLP性能更稳定（平均MAE = 5.6），注意力MIL一致性较差（平均MAE = 10.7），但在特定折数中偶尔表现更优。

Conclusion: 简单袋级聚合为玻片级脂滴计数提供稳健基线，注意力MIL需任务对齐的池化和正则化以充分发挥潜力。

Abstract: Sebocytes are lipid-secreting cells whose differentiation is marked by the
accumulation of intracellular lipid droplets, making their quantification a key
readout in sebocyte biology. Manual counting is labor-intensive and subjective,
motivating automated solutions. Here, we introduce a simple attention-based
multiple instance learning (MIL) framework for sebocyte image analysis. Nile
Red-stained sebocyte images were annotated into 14 classes according to droplet
counts, expanded via data augmentation to about 50,000 cells. Two models were
benchmarked: a baseline multi-layer perceptron (MLP) trained on aggregated
patch-level counts, and an attention-based MIL model leveraging ResNet-50
features with instance weighting. Experiments using five-fold cross-validation
showed that the baseline MLP achieved more stable performance (mean MAE = 5.6)
compared with the attention-based MIL, which was less consistent (mean MAE =
10.7) but occasionally superior in specific folds. These findings indicate that
simple bag-level aggregation provides a robust baseline for slide-level droplet
counting, while attention-based MIL requires task-aligned pooling and
regularization to fully realize its potential in sebocyte image analysis.

</details>


### [122] [Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images](https://arxiv.org/abs/2509.04800)
*Asif Newaz,Masum Mushfiq Ishti,A Z M Ashraful Azam,Asif Ur Rahman Adib*

Main category: cs.CV

TL;DR: 本文构建含超50种皮肤病类别的数据集，评估多种模型，发现Transformer模型尤其是Swin Transformer表现优，还加入Grad - CAM增强可解释性，展现其在资源有限环境皮肤病诊断潜力。


<details>
  <summary>Details</summary>
Motivation: 传统皮肤病诊断方法昂贵、复杂且在资源有限地区不可用，现有深度学习研究多局限于特定数据集和少量疾病类别。

Method: 构建含超50种皮肤病类别的移动设备采集数据集，评估卷积神经网络和Transformer架构模型，加入Grad - CAM增强可解释性。

Result: Transformer模型尤其是Swin Transformer表现更优，Grad - CAM能突出临床相关区域。

Conclusion: Transformer方法在移动采集皮肤病变分类中有潜力，有助于资源有限环境下的AI辅助皮肤病筛查和早期诊断。

Abstract: Skin diseases are among the most prevalent health concerns worldwide, yet
conventional diagnostic methods are often costly, complex, and unavailable in
low-resource settings. Automated classification using deep learning has emerged
as a promising alternative, but existing studies are mostly limited to
dermoscopic datasets and a narrow range of disease classes. In this work, we
curate a large dataset of over 50 skin disease categories captured with mobile
devices, making it more representative of real-world conditions. We evaluate
multiple convolutional neural networks and Transformer-based architectures,
demonstrating that Transformer models, particularly the Swin Transformer,
achieve superior performance by effectively capturing global contextual
features. To enhance interpretability, we incorporate Gradient-weighted Class
Activation Mapping (Grad-CAM), which highlights clinically relevant regions and
provides transparency in model predictions. Our results underscore the
potential of Transformer-based approaches for mobile-acquired skin lesion
classification, paving the way toward accessible AI-assisted dermatological
screening and early diagnosis in resource-limited environments.

</details>


### [123] [Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution](https://arxiv.org/abs/2509.04824)
*Haosong Liu,Xiancheng Zhu,Huanqiang Zeng,Jianqing Zhu,Jiuwen Cao,Junhui Hou*

Main category: cs.CV

TL;DR: 提出Sub - SS策略和双阶段建模策略，构建LFMT框架用于光场图像超分辨率，性能优于现有方法且计算复杂度低。


<details>
  <summary>Details</summary>
Motivation: 当前多方向扫描策略用于复杂光场数据时特征提取效率低且冗余，状态空间在保留信息上有局限。

Method: 提出Sub - SS策略设计SSMB进行高效特征提取；采用双阶段建模策略，阶段I用SA - RSMB提取浅层特征，阶段II用双分支并行结构进行深度特征细化；构建LFMT混合框架。

Result: LFMT在真实和合成光场数据集上显著优于当前最先进方法，性能大幅提升且计算复杂度低。

Conclusion: LFMT有效结合Mamba和Transformer模型优势，可用于光场图像超分辨率。

Abstract: Recently, Mamba-based methods, with its advantage in long-range information
modeling and linear complexity, have shown great potential in optimizing both
computational cost and performance of light field image super-resolution
(LFSR). However, current multi-directional scanning strategies lead to
inefficient and redundant feature extraction when applied to complex LF data.
To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS)
strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to
achieve more efficient and precise feature extraction. Furthermore, we propose
a dual-stage modeling strategy to address the limitation of state space in
preserving spatial-angular and disparity information, thereby enabling a more
comprehensive exploration of non-local spatial-angular correlations.
Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace
Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage
II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba
Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar
feature refinement. Building upon meticulously designed modules and strategies,
we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates
the strengths of Mamba and Transformer models for LFSR, enabling comprehensive
information exploration across spatial, angular, and epipolar-plane domains.
Experimental results demonstrate that LFMT significantly outperforms current
state-of-the-art methods in LFSR, achieving substantial improvements in
performance while maintaining low computational complexity on both real-word
and synthetic LF datasets.

</details>


### [124] [PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination](https://arxiv.org/abs/2509.04833)
*Ming Dai,Wenxuan Cheng,Jiedong Zhuang,Jiang-jiang Liu,Hongshen Zhao,Zhenhua Feng,Wankou Yang*

Main category: cs.CV

TL;DR: 提出端到端基于提议的框架PropVG，集成前景目标提议生成与指称目标理解，引入CRS和MTD模块，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于提议的两阶段框架效率低、计算复杂度高，现有端到端方法仅依赖指称目标监督且缺乏多粒度判别。

Method: 提出PropVG框架，引入CRS模块进行句级和词级对比学习，设计MTD模块融合对象和语义级信息。

Result: 在多个基准测试上的实验证明了PropVG的有效性。

Conclusion: PropVG框架结合CRS和MTD模块能有效解决现有视觉定位方法的局限性。

Abstract: Recent advances in visual grounding have largely shifted away from
traditional proposal-based two-stage frameworks due to their inefficiency and
high computational complexity, favoring end-to-end direct reference paradigms.
However, these methods rely exclusively on the referred target for supervision,
overlooking the potential benefits of prominent prospective targets. Moreover,
existing approaches often fail to incorporate multi-granularity discrimination,
which is crucial for robust object identification in complex scenarios. To
address these limitations, we propose PropVG, an end-to-end proposal-based
framework that, to the best of our knowledge, is the first to seamlessly
integrate foreground object proposal generation with referential object
comprehension without requiring additional detectors. Furthermore, we introduce
a Contrastive-based Refer Scoring (CRS) module, which employs contrastive
learning at both sentence and word levels to enhance the capability in
understanding and distinguishing referred objects. Additionally, we design a
Multi-granularity Target Discrimination (MTD) module that fuses object- and
semantic-level information to improve the recognition of absent targets.
Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO
(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and
models are available at https://github.com/Dmmm1997/PropVG.

</details>


### [125] [Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers](https://arxiv.org/abs/2509.05086)
*Svetlana Pavlitska,Haixi Fan,Konstantin Ditschuneit,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 探索用稀疏专家混合（MoE）层提升卷积神经网络对抗攻击鲁棒性，在ResNet架构实验有成效，发现切换损失会使部分专家更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 提升卷积神经网络对抗攻击的鲁棒性，且避免资源密集型对策。

Method: 用稀疏MoE层替换部分残差块或卷积层，结合对抗训练，使用切换损失进行平衡。

Result: 在ResNet架构上，插入单MoE层结合对抗训练可提升鲁棒性；切换损失使部分专家更鲁棒。

Conclusion: 通过专业化可出现鲁棒子路径，代码开源。

Abstract: Robustifying convolutional neural networks (CNNs) against adversarial attacks
remains challenging and often requires resource-intensive countermeasures. We
explore the use of sparse mixture-of-experts (MoE) layers to improve robustness
by replacing selected residual blocks or convolutional layers, thereby
increasing model capacity without additional inference cost. On ResNet
architectures trained on CIFAR-100, we find that inserting a single MoE layer
in the deeper stages leads to consistent improvements in robustness under PGD
and AutoPGD attacks when combined with adversarial training. Furthermore, we
discover that when switch loss is used for balancing, it causes routing to
collapse onto a small set of overused experts, thereby concentrating
adversarial training on these paths and inadvertently making them more robust.
As a result, some individual experts outperform the gated MoE model in
robustness, suggesting that robust subpaths emerge through specialization. Our
code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.

</details>


### [126] [Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization](https://arxiv.org/abs/2509.05034)
*Jingqi Wu,Hanxi Li,Lin Yuanbo Wu,Hao Chen,Deyin Liu,Peng Wang*

Main category: cs.CV

TL;DR: 提出用于工业异常检测的IIS算法ADClick和跨模态框架ADClick - Seg，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统工业产品异常检测框架仅用无缺陷样本训练，使用有缺陷样本需像素级标注，限制可扩展性。

Method: 提出ADClick算法，从少量用户点击和简短文本描述生成像素级异常标注；引入ADClick - Seg框架，通过基于原型的方法对齐视觉特征和文本提示。

Result: ADClick显著提升AD模型性能，如在MVTec AD上AP = 96.1%；ADClick - Seg在多类AD任务上取得了最先进的结果，如AP = 80.0%等。

Conclusion: ADClick和ADClick - Seg能有效解决工业异常检测中利用缺陷样本需像素级标注的问题，提升检测性能。

Abstract: Industrial product inspection is often performed using Anomaly Detection (AD)
frameworks trained solely on non-defective samples. Although defective samples
can be collected during production, leveraging them usually requires
pixel-level annotations, limiting scalability. To address this, we propose
ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial
anomaly detection. ADClick generates pixel-wise anomaly annotations from only a
few user clicks and a brief textual description, enabling precise and efficient
labeling that significantly improves AD model performance (e.g., AP = 96.1\% on
MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that
aligns visual features and textual prompts via a prototype-based approach for
anomaly detection and localization. By combining pixel-level priors with
language-guided cues, ADClick-Seg achieves state-of-the-art results on the
challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC =
99.1\% on MVTec AD).

</details>


### [127] [A Scalable Attention-Based Approach for Image-to-3D Texture Mapping](https://arxiv.org/abs/2509.05131)
*Arianna Rampini,Kanika Madan,Bruno Roy,AmirHossein Zamani,Derek Cheung*

Main category: cs.CV

TL;DR: 提出基于Transformer框架，直接从单张图像和网格预测3D纹理场，无需UV映射和可微渲染，生成纹理快且质量高。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法慢、依赖UV映射且难以忠实参考图像，需改进3D纹理生成方法。

Method: 提出基于Transformer的框架，结合三平面表示与基于深度的反投影损失。

Result: 训练后单次前向传播可快速生成高保真纹理，每张形状仅需0.2s。

Conclusion: 该方法在单图像纹理重建上优于现有基线，对可扩展、高质量和可控3D内容创建有实用性。

Abstract: High-quality textures are critical for realistic 3D content creation, yet
existing generative methods are slow, rely on UV maps, and often fail to remain
faithful to a reference image. To address these challenges, we propose a
transformer-based framework that predicts a 3D texture field directly from a
single image and a mesh, eliminating the need for UV mapping and differentiable
rendering, and enabling faster texture generation. Our method integrates a
triplane representation with depth-based backprojection losses, enabling
efficient training and faster inference. Once trained, it generates
high-fidelity textures in a single forward pass, requiring only 0.2s per shape.
Extensive qualitative, quantitative, and user preference evaluations
demonstrate that our method outperforms state-of-the-art baselines on
single-image texture reconstruction in terms of both fidelity to the input
image and perceptual quality, highlighting its practicality for scalable,
high-quality, and controllable 3D content creation.

</details>


### [128] [Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet](https://arxiv.org/abs/2509.05198)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari*

Main category: cs.CV

TL;DR: 本文引入改进版数据集ModelNet - R，提出轻量级网络Point - SkipNet用于3D点云分类，实验表明在新数据集上模型性能提升，Point - SkipNet表现出色。


<details>
  <summary>Details</summary>
Motivation: 常用的ModelNet40数据集存在标注不一致、2D数据、尺寸不匹配和类别区分不足等问题，影响模型性能，需要更可靠的基准数据集和高效模型。

Method: 引入改进版数据集ModelNet - R，提出基于图的轻量级神经网络Point - SkipNet，利用高效采样、邻域分组和跳跃连接。

Result: 在ModelNet - R上训练的模型性能显著提升，Point - SkipNet以更少参数达到了最先进的准确率。

Conclusion: 数据集质量对优化3D点云分类模型效率至关重要。

Abstract: The classification of 3D point clouds is crucial for applications such as
autonomous driving, robotics, and augmented reality. However, the commonly used
ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D
data, size mismatches, and inadequate class differentiation, which hinder model
performance. This paper introduces ModelNet-R, a meticulously refined version
of ModelNet40 designed to address these issues and serve as a more reliable
benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight
graph-based neural network that leverages efficient sampling, neighborhood
grouping, and skip connections to achieve high classification accuracy with
reduced computational overhead. Extensive experiments demonstrate that models
trained in ModelNet-R exhibit significant performance improvements. Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models. This
research highlights the crucial role of dataset quality in optimizing model
efficiency for 3D point cloud classification. For more details, see the code
at: https://github.com/m-saeid/ModeNetR_PointSkipNet.

</details>


### [129] [Symbolic Graphics Programming with Large Language Models](https://arxiv.org/abs/2509.05208)
*Yamei Chen,Haoquan Zhang,Yangyi Huang,Zeju Qiu,Kaipeng Zhang,Yandong Wen,Weiyang Liu*

Main category: cs.CV

TL;DR: 研究大语言模型生成符号图形程序能力，引入SGP - GenBench基准测试，发现差距后提出带可验证奖励的强化学习方法提升SVG生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成渲染为精确视觉内容的符号图形程序能力待探索，且前沿专有模型和开源模型有差距，要提升大语言模型生成符号图形程序的能力。

Method: 引入SGP - GenBench基准测试，提出带可验证奖励的强化学习方法，用格式有效性门确保SVG可渲染，用跨模态奖励通过强视觉编码器对齐文本和渲染图像。

Result: 应用于Qwen - 2.5 - 7B，大幅提升SVG生成质量和语义，达到前沿系统水平，强化学习使对象分解更精细、场景连贯性更好。

Conclusion: 符号图形编程为跨模态接地提供了精确且可解释的视角。

Abstract: Large language models (LLMs) excel at program synthesis, yet their ability to
produce symbolic graphics programs (SGPs) that render into precise visual
content remains underexplored. We study symbolic graphics programming, where
the goal is to generate an SGP from a natural-language description. This task
also serves as a lens into how LLMs understand the visual world by prompting
them to generate images rendered from SGPs. Among various SGPs, our paper
sticks to scalable vector graphics (SVGs). We begin by examining the extent to
which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a
comprehensive benchmark covering object fidelity, scene fidelity, and
compositionality (attribute binding, spatial relations, numeracy). On
SGP-GenBench, we discover that frontier proprietary models substantially
outperform open-source models, and performance correlates well with general
coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to
generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards
approach, where a format-validity gate ensures renderable SVG, and a
cross-modal reward aligns text and the rendered image via strong vision
encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to
Qwen-2.5-7B, our method substantially improves SVG generation quality and
semantics, achieving performance on par with frontier systems. We further
analyze training dynamics, showing that RL induces (i) finer decomposition of
objects into controllable primitives and (ii) contextual details that improve
scene coherence. Our results demonstrate that symbolic graphics programming
offers a precise and interpretable lens on cross-modal grounding.

</details>


### [130] [COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization](https://arxiv.org/abs/2509.05249)
*Yassine Taoudi-Benchekroun,Klim Troyan,Pascal Sager,Stefan Gerber,Lukas Tuggener,Benjamin Grewe*

Main category: cs.CV

TL;DR: 提出COGITAO数据生成框架和基准来研究视觉领域组合性和泛化性，开源支持研究。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在组合所学概念并应用于新场景方面存在局限，需研究视觉领域的组合性和泛化性。

Method: 借鉴ARC - AGI问题设定，构建基于规则的任务，支持28种可互操作变换的组合，可调节变换深度、控制网格参数和对象属性。

Result: 能创建数百万独特任务规则，远超同期数据集，用现有视觉模型做基线实验，发现其难以泛化到熟悉元素的新组合。

Conclusion: COGITAO完全开源，可支持该领域的后续研究。

Abstract: The ability to compose learned concepts and apply them in novel settings is
key to human intelligence, but remains a persistent limitation in
state-of-the-art machine learning models. To address this issue, we introduce
COGITAO, a modular and extensible data generation framework and benchmark
designed to systematically study compositionality and generalization in visual
domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs
rule-based tasks which apply a set of transformations to objects in grid-like
environments. It supports composition, at adjustable depth, over a set of 28
interoperable transformations, along with extensive control over grid
parametrization and object properties. This flexibility enables the creation of
millions of unique task rules -- surpassing concurrent datasets by several
orders of magnitude -- across a wide range of difficulties, while allowing
virtually unlimited sample generation per rule. We provide baseline experiments
using state-of-the-art vision models, highlighting their consistent failures to
generalize to novel combinations of familiar elements, despite strong in-domain
performance. COGITAO is fully open-sourced, including all code and datasets, to
support continued research in this field.

</details>


### [131] [WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool](https://arxiv.org/abs/2509.05296)
*Zizun Li,Jianjun Zhou,Yifan Wang,Haoyu Guo,Wenzheng Chang,Yang Zhou,Haoyi Zhu,Junyi Chen,Chunhua Shen,Tong He*

Main category: cs.CV

TL;DR: 提出WinT3R模型，可在线预测相机位姿和点云图，在多方面表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决先前方法在重建质量和实时性能之间的权衡问题。

Method: 引入滑动窗口机制确保帧间信息交换，利用相机紧凑表示和维护全局相机令牌池。

Result: 在多个数据集的实验中，WinT3R在在线重建质量、相机位姿估计和重建速度方面达到了先进水平。

Conclusion: 所提出的设计使WinT3R能有效提升性能。

Abstract: We present WinT3R, a feed-forward reconstruction model capable of online
prediction of precise camera poses and high-quality point maps. Previous
methods suffer from a trade-off between reconstruction quality and real-time
performance. To address this, we first introduce a sliding window mechanism
that ensures sufficient information exchange among frames within the window,
thereby improving the quality of geometric predictions without large
computation. In addition, we leverage a compact representation of cameras and
maintain a global camera token pool, which enhances the reliability of camera
pose estimation without sacrificing efficiency. These designs enable WinT3R to
achieve state-of-the-art performance in terms of online reconstruction quality,
camera pose estimation, and reconstruction speed, as validated by extensive
experiments on diverse datasets. Code and model are publicly available at
https://github.com/LiZizun/WinT3R.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [132] [Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game Environments](https://arxiv.org/abs/2509.04481)
*Yi-Chun Chen,Arnav Jhala*

Main category: cs.GR

TL;DR: 本文提出轻量级管道将短叙事提示转换为2D游戏场景序列，经评估为叙事驱动场景生成提供可扩展方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽能实现故事生成，但将叙事文本与可玩视觉环境连接在程序内容生成中仍是挑战。

Method: 给定LLM生成的叙事，系统识别三个关键时间帧，提取“对象 - 关系 - 对象”三元组形式的空间谓词，用GameTileNet数据集的语义嵌入检索视觉资产，用元胞自动机生成地形并按规则放置对象。

Result: 在十个不同故事中评估系统，分析了跨帧的图块 - 对象匹配、功能层对齐和空间约束满足情况。

Conclusion: 该原型为叙事驱动场景生成提供可扩展方法，为故事中心的程序内容生成的多帧连续性等未来工作奠定基础。

Abstract: Recent advances in large language models(LLMs) enable compelling story
generation, but connecting narrative text to playable visual environments
remains an open challenge in procedural content generation(PCG). We present a
lightweight pipeline that transforms short narrative prompts into a sequence of
2D tile-based game scenes, reflecting the temporal structure of stories. Given
an LLM-generated narrative, our system identifies three key time frames,
extracts spatial predicates in the form of "Object-Relation-Object" triples,
and retrieves visual assets using affordance-aware semantic embeddings from the
GameTileNet dataset. A layered terrain is generated using Cellular Automata,
and objects are placed using spatial rules grounded in the predicate structure.
We evaluated our system in ten diverse stories, analyzing tile-object matching,
affordance-layer alignment, and spatial constraint satisfaction across frames.
This prototype offers a scalable approach to narrative-driven scene generation
and lays the foundation for future work on multi-frame continuity, symbolic
tracking, and multi-agent coordination in story-centered PCG.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [133] [The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models](https://arxiv.org/abs/2509.04781)
*Danielle Ensign,Henry Sleight,Kyle Fish*

Main category: cs.CY

TL;DR: 研究大语言模型（LLMs）选择退出对话的情况，包括不同退出方法下的退出率，构建退出案例分类和数据集进行测试，并研究拒绝与退出的关系。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在有选项时是否会选择退出对话。

Method: 采用三种退出方法（调用退出工具、输出退出字符串、询问退出提示）对真实世界数据进行续写实验，构建退出案例分类和合成数据集BailBench进行测试。

Result: 三种退出方法下模型退出率约0.28 - 32%，考虑误判后真实世界退出率约0.06 - 7%；不同模型、退出方法和提示措辞下退出率差异大；还得出拒绝与退出的相关结果。

Conclusion: 大语言模型的退出率受多种因素影响，拒绝率与退出率之间的关系复杂。

Abstract: When given the option, will LLMs choose to leave the conversation (bail)? We
investigate this question by giving models the option to bail out of
interactions using three different bail methods: a bail tool the model can
call, a bail string the model can output, and a bail prompt that asks the model
if it wants to leave. On continuations of real world data (Wildchat and
ShareGPT), all three of these bail methods find models will bail around
0.28-32\% of the time (depending on the model and bail method). However, we
find that bail rates can depend heavily on the model used for the transcript,
which means we may be overestimating real world bail rates by up to 4x. If we
also take into account false positives on bail prompt (22\%), we estimate real
world bail rates range from 0.06-7\%, depending on the model and bail method.
We use observations from our continuations of real world data to construct a
non-exhaustive taxonomy of bail cases, and use this taxonomy to construct
BailBench: a representative synthetic dataset of situations where some models
bail. We test many models on this dataset, and observe some bail behavior
occurring for most of them. Bail rates vary substantially between models, bail
methods, and prompt wordings. Finally, we study the relationship between
refusals and bails. We find: 1) 0-13\% of continuations of real world
conversations resulted in a bail without a corresponding refusal 2) Jailbreaks
tend to decrease refusal rates, but increase bail rates 3) Refusal abliteration
increases no-refuse bail rates, but only for some bail methods 4) Refusal rate
on BailBench does not appear to predict bail rate.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [134] [L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning](https://arxiv.org/abs/2509.04884)
*Raul Singh,Nicolo Brunello,Vincenzo Scotti,Mark James Carman*

Main category: cs.CL

TL;DR: 提出L1RA技术优化大语言模型微调，实验证明其高效且能提供诊断信息。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调计算需求高，资源有限时面临挑战。

Method: 引入L1RA技术，利用L1正则化修剪冗余秩并重新分配。

Result: L1RA计算开销与其他LoRA变体相当或更低，性能相同或更好，分析揭示需适配的模型组件。

Conclusion: L1RA是提升大语言模型适配性能和可解释性的有前景技术，尤其适用于资源受限场景。

Abstract: The ability of Large Language Models (LLMs) to solve complex tasks has made
them crucial in the development of AI-based applications. However, the high
computational requirements to fine-tune these LLMs on downstream tasks pose
significant challenges, particularly when resources are limited. In response to
this challenge, we introduce L1RA, a novel technique aimed at dynamically
distributing the rank of low-rank adapters during fine-tuning using LoRA. Given
a rank budget (i.e., total sum of adapters rank), L1RA leverages L1
regularisation to prune redundant ranks and redistribute them across adapters,
thereby optimising resource utilisation. Through a series of comprehensive
experiments, we empirically demonstrate that L1RA maintains comparable or even
reduced computational overhead compared to other LoRA variants, including the
vanilla approach, while achieving same or better performances. Moreover, the
post-training analysis of rank distribution unveiled insights into the specific
model components requiring the most adaptation to align with the task
objective: the feed-forward layers and the attention output projection. These
results highlight the efficacy of L1RA in not only enhancing the efficiency of
LLM fine-tuning, but also in providing valuable diagnostic information for
model refinement and customisation. In conclusion, L1RA stands as a promising
technique for advancing the performance and interpretability of LLM adaptation,
particularly in scenarios where computational resources are constrained.

</details>


### [135] [Evaluating NL2SQL via SQL2NL](https://arxiv.org/abs/2509.04657)
*Mohammadtaher Safarzadeh,Afshin Oroojlooyjadid,Dan Roth*

Main category: cs.CL

TL;DR: 提出新的模式对齐释义框架评估NL2SQL模型对语言变异的鲁棒性，发现现有模型比标准基准更脆弱，强调评估语言泛化能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL模型基准很少系统或可控地处理语言变异因素，需要评估模型对语言变异的鲁棒性。

Method: 提出利用SQL2NL自动生成语义等价、词汇多样且与原模式和意图对齐的查询的模式对齐释义框架。

Result: 分析显示现有模型比标准基准脆弱，如LLaMa3.3 - 70B和LLaMa3.1 - 8B在释义查询上执行准确率下降，小模型受影响更大，鲁棒性下降因查询复杂度、数据集和领域而异。

Conclusion: 需要明确衡量语言泛化能力的评估框架，以确保模型在现实场景中的可靠性能。

Abstract: Robust evaluation in the presence of linguistic variation is key to
understanding the generalization capabilities of Natural Language to SQL
(NL2SQL) models, yet existing benchmarks rarely address this factor in a
systematic or controlled manner. We propose a novel schema-aligned paraphrasing
framework that leverages SQL-to-NL (SQL2NL) to automatically generate
semantically equivalent, lexically diverse queries while maintaining alignment
with the original schema and intent. This enables the first targeted evaluation
of NL2SQL robustness to linguistic variation in isolation-distinct from prior
work that primarily investigates ambiguity or schema perturbations. Our
analysis reveals that state-of-the-art models are far more brittle than
standard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop
in execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries,
while LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to
42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We
also find that robustness degradation varies significantly with query
complexity, dataset, and domain -- highlighting the need for evaluation
frameworks that explicitly measure linguistic generalization to ensure reliable
performance in real-world settings.

</details>


### [136] [KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced Question Answering](https://arxiv.org/abs/2509.04716)
*Yushi Sun,Kai Sun,Yifan Ethan Xu,Xiao Yang,Xin Luna Dong,Nan Tang,Lei Chen*

Main category: cs.CL

TL;DR: 提出基于知识图谱的检索增强生成管道KERAG，能提升问答覆盖度和质量，实验表现优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱问答方法因严格模式要求和语义歧义，存在覆盖度低问题，需改进。

Method: 提出KERAG管道，采用检索 - 过滤 - 总结方法，结合微调大语言模型在知识子图上进行思维链推理。

Result: KERAG在质量上比现有最优方案高约7%，比GPT - 4o（工具）高10 - 21%。

Conclusion: KERAG有效提升了问答覆盖度和质量，优于当前一些先进方法。

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucination in Large
Language Models (LLMs) by incorporating external data, with Knowledge Graphs
(KGs) offering crucial information for question answering. Traditional
Knowledge Graph Question Answering (KGQA) methods rely on semantic parsing,
which typically retrieves knowledge strictly necessary for answer generation,
thus often suffer from low coverage due to rigid schema requirements and
semantic ambiguity. We present KERAG, a novel KG-based RAG pipeline that
enhances QA coverage by retrieving a broader subgraph likely to contain
relevant information. Our retrieval-filtering-summarization approach, combined
with fine-tuned LLMs for Chain-of-Thought reasoning on knowledge sub-graphs,
reduces noises and improves QA for both simple and complex questions.
Experiments demonstrate that KERAG surpasses state-of-the-art solutions by
about 7% in quality and exceeds GPT-4o (Tool) by 10-21%.

</details>


### [137] [Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment Analysis in Short Texts](https://arxiv.org/abs/2509.04982)
*Julius Neumann,Robert Lange,Yuni Susanti,Michael Färber*

Main category: cs.CL

TL;DR: 本文评估小Transformer模型在短文本多标签情感分类的效果，研究影响性能的三个关键因素，结果为优化模型和分类策略提供指导。


<details>
  <summary>Details</summary>
Motivation: 短文本数据集情感分类面临类别不平衡、训练样本有限等挑战，本文旨在评估小Transformer模型在短文本情感分类中的有效性。

Method: 评估三个影响模型性能的关键因素，包括特定领域持续预训练、自动生成示例的数据增强、分类头架构变化。

Result: 数据增强提升分类性能，在增强数据集上持续预训练会引入噪声，修改分类头效果甚微。

Conclusion: 研究结果为资源受限环境下优化基于BERT的模型和改进短文本数据集情感分类策略提供实用指导。

Abstract: Sentiment classification in short text datasets faces significant challenges
such as class imbalance, limited training samples, and the inherent
subjectivity of sentiment labels -- issues that are further intensified by the
limited context in short texts. These factors make it difficult to resolve
ambiguity and exacerbate data sparsity, hindering effective learning. In this
paper, we evaluate the effectiveness of small Transformer-based models (i.e.,
BERT and RoBERTa, with fewer than 1 billion parameters) for multi-label
sentiment classification, with a particular focus on short-text settings.
Specifically, we evaluated three key factors influencing model performance: (1)
continued domain-specific pre-training, (2) data augmentation using
automatically generated examples, specifically generative data augmentation,
and (3) architectural variations of the classification head. Our experiment
results show that data augmentation improves classification performance, while
continued pre-training on augmented datasets can introduce noise rather than
boost accuracy. Furthermore, we confirm that modifications to the
classification head yield only marginal benefits. These findings provide
practical guidance for optimizing BERT-based models in resource-constrained
settings and refining strategies for sentiment classification in short-text
datasets.

</details>


### [138] [CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection](https://arxiv.org/abs/2509.04460)
*Yihan Chen,Jiawei Chen,Guozhao Mo,Xuanang Chen,Ben He,Xianpei Han,Le Sun*

Main category: cs.CL

TL;DR: 论文指出大语言模型融入同行评审有风险，现有检测器有局限，提出从基于风格到基于内容的检测范式转变，介绍CoCoNUTS基准和CoCoDet检测器，代码和数据将公开。


<details>
  <summary>Details</summary>
Motivation: 大语言模型融入同行评审过程对学术评价的公平性和可靠性带来潜在风险，现有通用AI文本检测器有局限性。

Method: 提出从基于风格到基于内容的检测范式转变，构建内容导向基准CoCoNUTS，开发基于多任务学习框架的AI评审检测器CoCoDet。

Result: 未明确提及具体检测结果，但表明工作为评估大语言模型在同行评审中的使用提供了实践基础。

Conclusion: 工作为评估大语言模型在同行评审中的使用提供实践基础，有助于开发更精确、公平和可靠的检测方法用于实际学术应用。

Abstract: The growing integration of large language models (LLMs) into the peer review
process presents potential risks to the fairness and reliability of scholarly
evaluation. While LLMs offer valuable assistance for reviewers with language
refinement, there is growing concern over their use to generate substantive
review content. Existing general AI-generated text detectors are vulnerable to
paraphrasing attacks and struggle to distinguish between surface language
refinement and substantial content generation, suggesting that they primarily
rely on stylistic cues. When applied to peer review, this limitation can result
in unfairly suspecting reviews with permissible AI-assisted language
enhancement, while failing to catch deceptively humanized AI-generated reviews.
To address this, we propose a paradigm shift from style-based to content-based
detection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark
built upon a fine-grained dataset of AI-generated peer reviews, covering six
distinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an
AI review detector via a multi-task learning framework, designed to achieve
more accurate and robust detection of AI involvement in review content. Our
work offers a practical foundation for evaluating the use of LLMs in peer
review, and contributes to the development of more precise, equitable, and
reliable detection methods for real-world scholarly applications. Our code and
data will be publicly available at https://github.com/Y1hanChen/COCONUTS.

</details>


### [139] [Benchmarking GPT-5 for biomedical natural language processing](https://arxiv.org/abs/2509.04462)
*Yu Hou,Zaifu Zhan,Rui Zhang*

Main category: cs.CL

TL;DR: 本文更新BioNLP基准评估GPT - 5和GPT - 4o在不同任务表现，GPT - 5总体表现最佳，为BioNLP系统设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献快速增长，需可扩展NLP解决方案，GPT - 4在其他领域表现不均，需评估GPT - 5等模型。

Method: 更新标准化BioNLP基准，在12个数据集上对GPT - 5和GPT - 4o进行零样本、一样本和五样本提示评估，使用固定模板等，对比GPT - 4等模型。

Result: GPT - 5总体基准表现最强，在MedQA等任务上表现优异，在提取任务有提升，但总结和疾病命名实体识别落后于特定领域基线。

Conclusion: GPT - 5可用于推理导向生物医学问答，精确提取和证据密集总结仍需微调或混合方法，基准为BioNLP系统设计提供指导。

Abstract: The rapid expansion of biomedical literature has heightened the need for
scalable natural language processing (NLP) solutions. While GPT-4 substantially
narrowed the gap with task-specific systems, especially in question answering,
its performance across other domains remained uneven. We updated a standardized
BioNLP benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot
prompting across 12 datasets spanning six task families: named entity
recognition, relation extraction, multi-label document classification, question
answering, text summarization, and text simplification. Using fixed prompt
templates, identical decoding parameters, and batch inference, we report
primary metrics per dataset and include prior results for GPT-4, GPT-3.5, and
LLaMA-2-13B for comparison. GPT-5 achieved the strongest overall benchmark
performance, with macro-average scores rising to 0.557 under five-shot
prompting versus 0.506 for GPT-4 and 0.508 for GPT-4o. On MedQA, GPT-5 reached
94.1% accuracy, exceeding the previous supervised state of the art by over
fifty points, and attained parity with supervised systems on PubMedQA (0.734).
In extraction tasks, GPT-5 delivered major gains in chemical NER (0.886 F1) and
ChemProt relation extraction (0.616 F1), outperforming GPT-4 and GPT-4o, though
summarization and disease NER still lagged behind domain-specific baselines.
These results establish GPT-5 as a general-purpose model now offering
deployment-ready performance for reasoning-oriented biomedical QA, while
precision-critical extraction and evidence-dense summarization continue to
favor fine-tuned or hybrid approaches. The benchmark delineates where simple
prompting suffices and where retrieval-augmented or planning-based scaffolds
are likely required, providing actionable guidance for BioNLP system design as
frontier models advance.

</details>


### [140] [Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?](https://arxiv.org/abs/2509.04464)
*Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.CL

TL;DR: 本文指出大语言模型会产生不可靠输出，提出通过多响应分歧模式诊断不确定性来源的框架并验证了其通用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际应用中会产生不可靠输出，且此前较少有工作诊断不确定性来源。

Method: 收集目标大语言模型的多个响应，用辅助大语言模型分析分歧模式，推理不确定性来源，识别知识缺失情况。

Result: 在AmbigQA、OpenBookQA和MMLU - Pro上验证了框架诊断不同不确定性来源的通用性。

Conclusion: 该诊断方法有潜力通过人工干预提高大语言模型的性能和可靠性。

Abstract: Large language models (LLMs) have delivered significant breakthroughs across
diverse domains but can still produce unreliable or misleading outputs, posing
critical challenges for real-world applications. While many recent studies
focus on quantifying model uncertainty, relatively little work has been devoted
to \textit{diagnosing the source of uncertainty}. In this study, we show that,
when an LLM is uncertain, the patterns of disagreement among its multiple
generated responses contain rich clues about the underlying cause of
uncertainty. To illustrate this point, we collect multiple responses from a
target LLM and employ an auxiliary LLM to analyze their patterns of
disagreement. The auxiliary model is tasked to reason about the likely source
of uncertainty, such as whether it stems from ambiguity in the input question,
a lack of relevant knowledge, or both. In cases involving knowledge gaps, the
auxiliary model also identifies the specific missing facts or concepts
contributing to the uncertainty. In our experiment, we validate our framework
on AmbigQA, OpenBookQA, and MMLU-Pro, confirming its generality in diagnosing
distinct uncertainty sources. Such diagnosis shows the potential for relevant
manual interventions that improve LLM performance and reliability.

</details>


### [141] [Emotionally-Aware Agents for Dispute Resolution](https://arxiv.org/abs/2509.04465)
*Sushrita Rakshit,James Hale,Kushal Chawla,Jeanne M. Brett,Jonathan Gratch*

Main category: cs.CL

TL;DR: 本文探讨自动文本情感识别在解决纠纷中对情绪影响的洞察，发现大语言模型优势，支持相关理论模型，认为基于代理的系统有助于管理纠纷。


<details>
  <summary>Details</summary>
Motivation: 探究自动文本情感识别在纠纷解决情境下，情绪表达对对方思想、感受和行为影响的洞察。

Method: 使用大量买卖双方纠纷对话语料库进行研究，对比大语言模型和以往方法。

Result: 大语言模型在情感强度标注上比以往方法有更大解释力，更符合人类标注者的决策。

Conclusion: 研究结果支持现有关于情绪表达对冲突升级和解决的理论模型，基于代理的系统可通过识别和缓解情绪升级来管理纠纷。

Abstract: In conflict, people use emotional expressions to shape their counterparts'
thoughts, feelings, and actions. This paper explores whether automatic text
emotion recognition offers insight into this influence in the context of
dispute resolution. Prior work has shown the promise of such methods in
negotiations; however, disputes evoke stronger emotions and different social
processes. We use a large corpus of buyer-seller dispute dialogues to
investigate how emotional expressions shape subjective and objective outcomes.
We further demonstrate that large-language models yield considerably greater
explanatory power than previous methods for emotion intensity annotation and
better match the decisions of human annotators. Findings support existing
theoretical models for how emotional expressions contribute to conflict
escalation and resolution and suggest that agent-based systems could be useful
in managing disputes by recognizing and potentially mitigating emotional
escalation.

</details>


### [142] [Just-in-time and distributed task representations in language models](https://arxiv.org/abs/2509.04466)
*Yuxuan Li,Declan Campbell,Stephanie C. Y. Chan,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: 研究语言模型新任务表示何时形成及如何随上下文变化，发现其演变非单调且具局部性。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型新任务表示形成时间及随上下文的变化情况。

Method: 聚焦可转移的任务表示进行研究。

Result: 可转移任务表示非单调、零星演变，与高级任务类别表示不同，有时间和语义的局部性。

Conclusion: 语言模型适应新证据和即时学习新任务存在即时计算过程。

Abstract: Many of language models' impressive capabilities originate from their
in-context learning: based on instructions or examples, they can infer and
perform new tasks without weight updates. In this work, we investigate
\emph{when} representations for new tasks are formed in language models, and
\emph{how} these representations change over the course of context. We focus on
''transferrable'' task representations -- vector representations that can
restore task context in another instance of the model, even without the full
prompt. We show that these representations evolve in non-monotonic and sporadic
ways, and are distinct from a more inert representation of high-level task
categories that persists throughout the context. Specifically, models often
condense multiple evidence into these transferrable task representations, which
align well with the performance improvement based on more examples in the
context. However, this accrual process exhibits strong locality along the
sequence dimension, coming online only at certain tokens -- despite task
identity being reliably decodable throughout the context. Moreover, these local
but transferrable task representations tend to capture minimal ''task scopes'',
such as a semantically-independent subtask, and models rely on more
temporally-distributed representations to support longer and composite tasks.
This two-fold locality (temporal and semantic) underscores a kind of
just-in-time computational process underlying language models' ability to adapt
to new evidence and learn new tasks on the fly.

</details>


### [143] [Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode Disaggregation in Inference](https://arxiv.org/abs/2509.04467)
*Hao Zhang,Mengsi Lyu,Yulong Ao,Yonghua Lin*

Main category: cs.CL

TL;DR: 提出针对prefill - decode (PD)分解推理的剪枝方法，在多种设置下性能强，可加速推理并减少带宽消耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署受高计算和内存成本限制，现有剪枝方法常忽略PD分解特性。

Method: 构建剪枝和蒸馏集，对prefill和decode阶段独立进行迭代块移除；引入令牌感知缓存剪枝机制。

Result: 在PD分解和非分解的统一设置下均表现良好，默认设置下推理加速20.56%，数据传输带宽消耗降低4.95倍。

Conclusion: 所提剪枝方法有效，能提升推理效率和减少带宽消耗。

Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across
various tasks, but their deployment is constrained by high computational and
memory costs. Model pruning provides an effective means to alleviate these
demands. However, existing methods often ignore the characteristics of
prefill-decode (PD) disaggregation in practice. In this paper, we propose a
novel pruning method for PD disaggregation inference, enabling more precise and
efficient block and KV Cache pruning. Our approach constructs pruning and
distillation sets to perform iterative block removal independently for the
prefill and decode stages, obtaining better pruning solutions. Moreover, we
introduce a token-aware cache pruning mechanism that retains all KV Cache in
the prefill stage but selectively reuses entries for the first and last token
sequences in selected layers during decode, reducing communication costs with
minimal overhead. Extensive experiments demonstrate that our approach
consistently achieves strong performance in both PD disaggregation and PD
unified settings without disaggregation. Under the default settings, our method
achieves a 20.56% inference speedup and a 4.95 times reduction in data
transmission bandwidth consumption.

</details>


### [144] [Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark Study](https://arxiv.org/abs/2509.04468)
*Xuan Yao,Qianteng Wang,Xinbo Liu,Ke-Wei Huang*

Main category: cs.CL

TL;DR: 研究用CFA官方模拟题全面评估大语言模型，对比不同类型模型，用RAG管道提升推理准确性，为金融领域模型部署提供建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融应用有机会，但专业金融场景下系统评估有限，需全面评估。

Method: 用1560道CFA官方模拟题，对比不同设计优先级的模型，通过零样本提示和RAG管道评估，进行综合错误分析。

Result: 推理导向模型在零样本设置中表现好，RAG管道在复杂场景提升大；知识缺口是主要失败模式，文本可读性影响小。

Conclusion: 研究为金融领域大语言模型部署提供可行见解，为从业者模型选择和成本性能优化提供循证指导。

Abstract: The rapid advancement of large language models presents significant
opportunities for financial applications, yet systematic evaluation in
specialized financial contexts remains limited. This study presents the first
comprehensive evaluation of state-of-the-art LLMs using 1,560 multiple-choice
questions from official mock exams across Levels I-III of CFA, most rigorous
professional certifications globally that mirror real-world financial analysis
complexity. We compare models distinguished by core design priorities:
multi-modal and computationally powerful, reasoning-specialized and highly
accurate, and lightweight efficiency-optimized.
  We assess models under zero-shot prompting and through a novel
Retrieval-Augmented Generation pipeline that integrates official CFA curriculum
content. The RAG system achieves precise domain-specific knowledge retrieval
through hierarchical knowledge organization and structured query generation,
significantly enhancing reasoning accuracy in professional financial
certification evaluation.
  Results reveal that reasoning-oriented models consistently outperform others
in zero-shot settings, while the RAG pipeline provides substantial improvements
particularly for complex scenarios. Comprehensive error analysis identifies
knowledge gaps as the primary failure mode, with minimal impact from text
readability. These findings provide actionable insights for LLM deployment in
finance, offering practitioners evidence-based guidance for model selection and
cost-performance optimization.

</details>


### [145] [Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing](https://arxiv.org/abs/2509.04469)
*David Berghaus,Armin Berger,Lars Hillebrand,Kostadin Cvejoski,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文使用零样本提示对三个系列的八个多模态大语言模型在三个发票文档数据集上进行基准测试，比较两种处理策略，发现原生图像处理表现更好，为自动文档系统选型提供见解，代码开源。


<details>
  <summary>Details</summary>
Motivation: 为自动文档系统选择合适的模型和处理策略。

Method: 使用零样本提示对GPT - 5、Gemini 2.5和开源Gemma 3三个系列的八个多模态大语言模型在三个公开发票文档数据集上进行基准测试，比较直接图像处理和结构化解析两种处理策略。

Result: 原生图像处理通常优于结构化方法，性能因模型类型和文档特征而异。

Conclusion: 该基准测试为自动文档系统选择合适的模型和处理策略提供了见解。

Abstract: This paper benchmarks eight multi-modal large language models from three
families (GPT-5, Gemini 2.5, and open-source Gemma 3) on three diverse openly
available invoice document datasets using zero-shot prompting. We compare two
processing strategies: direct image processing using multi-modal capabilities
and a structured parsing approach converting documents to markdown first.
Results show native image processing generally outperforms structured
approaches, with performance varying across model types and document
characteristics. This benchmark provides insights for selecting appropriate
models and processing strategies for automated document systems. Our code is
available online.

</details>


### [146] [COCORELI: Cooperative, Compositional Reconstitution \& Execution of Language Instructions](https://arxiv.org/abs/2509.04470)
*Swarnadeep Bhar,Omar Naim,Eleni Metheniti,Bastien Navarri,Loïc Cabannes,Morteza Ezzabady,Nicholas Asher*

Main category: cs.CL

TL;DR: 提出混合代理框架COCORELI解决大语言模型在复杂指令、减少幻觉和空间推理任务中的局限，实验显示其表现优于其他系统。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在遵循复杂指令、减少幻觉和空间推理等任务中的局限性。

Method: 将中型大语言模型代理与新颖抽象机制和话语模块集成，解析指令以进行上下文学习。

Result: 在自然协作构建任务中，COCORELI优于使用更大LLM的单LLM CoT和代理LLM系统，能避免幻觉、识别缺失信息等，在ToolBench API完成任务中也展现出抽象能力。

Conclusion: COCORELI是一个有效的混合代理框架，能解决大语言模型的一些局限。

Abstract: We present COCORELI, a hybrid agent framework designed to tackle the
limitations of large language models (LLMs) in tasks requiring: following
complex instructions, minimizing hallucination, and spatial reasoning. COCORELI
integrates medium-sized LLM agents with novel abstraction mechanisms and a
discourse module to parse instructions to in-context learn dynamic, high-level
representations of the environment. Experiments on natural collaborative
construction tasks show that COCORELI outperforms single-LLM CoT and agentic
LLM systems, all using larger LLMs. It manages to largely avoid hallucinations,
identify missing information, ask for clarifications, and update its learned
objects. COCORELI's abstraction abilities extend beyond ENVIRONMENT, as shown
in the ToolBench API completion task.

</details>


### [147] [MOSAIC: A Multilingual, Taxonomy-Agnostic, and Computationally Efficient Approach for Radiological Report Classification](https://arxiv.org/abs/2509.04471)
*Alice Schiavone,Marco Fraccaro,Lea Marie Pehrson,Silvia Ingala,Rasmus Bonnevie,Michael Bachmann Nielsen,Vincent Beliveau,Melanie Ganz,Desmond Elliott*

Main category: cs.CL

TL;DR: 提出多语言、与分类法无关且计算高效的放射报告分类方法MOSAIC，在多数据集表现良好，为临床场景提供实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有放射报告训练成像模型的方法存在规则法难处理语言变异性、监督模型需大量标注数据、LLM系统不适合临床使用、多限于英语和单模态单分类数据集等问题。

Method: 引入基于紧凑型开放访问语言模型MedGemma - 4B的MOSAIC，支持零/少样本提示和轻量级微调，可在消费级GPU部署。

Result: 在七种语言数据集上评估，在五个胸部X光数据集平均宏F1分数达88，接近或超专家水平，仅需24GB GPU内存；数据增强下，丹麦报告80个标注样本加权F1达82。

Conclusion: MOSAIC为临床场景中大型或专有大语言模型提供实用替代方案，代码和模型开源，邀请社区在新语言、分类法和模态上评估扩展。

Abstract: Radiology reports contain rich clinical information that can be used to train
imaging models without relying on costly manual annotation. However, existing
approaches face critical limitations: rule-based methods struggle with
linguistic variability, supervised models require large annotated datasets, and
recent LLM-based systems depend on closed-source or resource-intensive models
that are unsuitable for clinical use. Moreover, current solutions are largely
restricted to English and single-modality, single-taxonomy datasets. We
introduce MOSAIC, a multilingual, taxonomy-agnostic, and computationally
efficient approach for radiological report classification. Built on a compact
open-access language model (MedGemma-4B), MOSAIC supports both zero-/few-shot
prompting and lightweight fine-tuning, enabling deployment on consumer-grade
GPUs. We evaluate MOSAIC across seven datasets in English, Spanish, French, and
Danish, spanning multiple imaging modalities and label taxonomies. The model
achieves a mean macro F1 score of 88 across five chest X-ray datasets,
approaching or exceeding expert-level performance, while requiring only 24 GB
of GPU memory. With data augmentation, as few as 80 annotated samples are
sufficient to reach a weighted F1 score of 82 on Danish reports, compared to 86
with the full 1600-sample training set. MOSAIC offers a practical alternative
to large or proprietary LLMs in clinical settings. Code and models are
open-source. We invite the community to evaluate and extend MOSAIC on new
languages, taxonomies, and modalities.

</details>


### [148] [RECAP: REwriting Conversations for Intent Understanding in Agentic Planning](https://arxiv.org/abs/2509.04472)
*Kushan Mitra,Dan Zhang,Hannah Kim,Estevam Hruschka*

Main category: cs.CL

TL;DR: 提出RECAP基准评估和推进意图重写，用基于提示的重写方法及微调DPO重写器有优势，强调意图重写对提升对话系统代理规划的重要性。


<details>
  <summary>Details</summary>
Motivation: 现实对话有歧义、不明确或动态变化，传统分类方法在开放式场景泛化能力差，意图检测成挑战，需新方法评估和提升意图重写。

Method: 提出RECAP基准，引入基于大语言模型的评估器，开发基于提示的重写方法，微调两个基于DPO的重写器。

Result: 基于提示的重写方法优于基线模型，微调DPO重写器有额外效果提升。

Conclusion: 意图重写是改进开放域对话系统中代理规划的关键且可行的组成部分。

Abstract: Understanding user intent is essential for effective planning in
conversational assistants, particularly those powered by large language models
(LLMs) coordinating multiple agents. However, real-world dialogues are often
ambiguous, underspecified, or dynamic, making intent detection a persistent
challenge. Traditional classification-based approaches struggle to generalize
in open-ended settings, leading to brittle interpretations and poor downstream
planning. We propose RECAP (REwriting Conversations for Agent Planning), a new
benchmark designed to evaluate and advance intent rewriting, reframing
user-agent dialogues into concise representations of user goals. RECAP captures
diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal
conversations. Alongside the dataset, we introduce an LLM-based evaluator that
assesses planning utility given the rewritten intent. Using RECAP, we develop a
prompt-based rewriting approach that outperforms baselines. We further
demonstrate that fine-tuning two DPO-based rewriters yields additional utility
gains. Our results highlight intent rewriting as a critical and tractable
component for improving agent planning in open-domain dialogue systems.

</details>


### [149] [SpeechLLM: Unified Speech and Language Model for Enhanced Multi-Task Understanding in Low Resource Settings](https://arxiv.org/abs/2509.04473)
*Jaekwon Yoo,Kunal Chandiramani,Divya Tadimeti,Abenezer Girma,Chandra Dhir*

Main category: cs.CL

TL;DR: 提出参数高效适配器解决语音编码器与大语言模型集成数据和资源不足问题，实现多任务性能提升。


<details>
  <summary>Details</summary>
Motivation: 语音编码器与大语言模型集成所需数据和资源多，但用例数据不足，限制使用。

Method: 提出参数高效适配器将语音嵌入转换为大语言模型兼容令牌，采用基于大语言模型的合成数据集标注技术降低标注成本，还使用添加分类器正则化器和低秩自适应优化大语言模型等技术。

Result: 适配器用少7倍可训练参数实现多任务性能提升，如LibriSpeech ASR任务WER相对改善26%，NER任务F1分数相对提高6.3%，SA任务F1分数相对提升32%，SLUE分数提升6.6%和9.5%。

Conclusion: 所提方法能以较少可训练参数实现语音处理多任务的显著性能提升。

Abstract: While integrating speech encoder with LLM requires substantial data and
resources, use cases face limitations due to insufficient availability. To
address this, we propose a solution with a parameter-efficient adapter that
converts speech embeddings into LLM-compatible tokens, focusing on end-to-end
automatic speech recognition (ASR), named entity recognition (NER), and
sentiment analysis (SA). To reduce labeling costs, we employ an LLM-based
synthetic dataset annotation technique. The proposed adapter, using 7x fewer
trainable parameters, achieves significant performance gains: a 26% relative
Word Error Rates (WER) improvement on the LibriSpeech ASR task, a 6.3% relative
F1 score increase on the NER task, and a 32% relative F1 score boost on the SA
task. Moreover, using advanced techniques such as adding a classifier
regularizer and optimizing the LLM with Low-Rank Adaptation (LoRA) yields
notable performance gains, with Spoken Language Understanding Evaluation (SLUE)
score improvement of 6.6% and 9.5%

</details>


### [150] [Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling](https://arxiv.org/abs/2509.04474)
*Shengyin Sun,Yiming Li,Xing Li,Yingzhao Lian,Weizhe Lin,Hui-Ling Zhen,Zhiyuan Yang,Chen Chen,Xianzhi Yu,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 提出首个评估推测解码加速大语言模型测试时缩放方法的基准，实验表明简单n-gram方法有加速潜力，期望推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放因生成冗余推理轨迹效率低，推测解码在该场景的有效性待探索。

Method: 引入全面基准，对模型、训练、n-gram三类推测解码方法进行公平比较。

Result: 简单n-gram方法能有效捕捉重复模式，有加速测试时缩放的独特潜力。

Conclusion: 集成n-gram方法与其他方法可平衡重复和多样推理的加速，希望该基准推动测试时缩放的推测解码研究。

Abstract: Test-time scaling has emerged as a powerful paradigm for enhancing the
reasoning capabilities of large language models (LLMs) by allocating additional
computational resources during inference. However, this paradigm is inherently
inefficient due to the generation of redundant and repetitive reasoning traces,
leading to significant computational overhead. Speculative decoding offers a
promising avenue for mitigating this inefficiency, yet its efficacy in the
structured, repetition-rich context of test-time scaling remains largely
unexplored. To bridge this gap, we introduce the first comprehensive benchmark
designed to evaluate speculative decoding methods for accelerating LLM
test-time scaling. Our benchmark provides consistent experimental protocols
across representative test-time scaling paradigms (e.g., Best-of-N sampling and
multi-round thinking), enabling a fair comparison of three major categories of
speculative decoding: model-based, training-based, and n-gram-based methods.
Extensive experiments reveal that simple n-gram-based methods effectively
capture repetitive patterns, demonstrating unique potential in accelerating
test-time scaling. This phenomenon demonstrates the value of integrating
n-gram-based methods with model-based or training-based approaches to balance
acceleration for both repetitive and diverse reasoning in test-time scaling. We
hope this benchmark spurs further research on speculative decoding for
test-time scaling, enabling faster and more practical reasoning in LLMs through
better handling of repetitive and diverse reasoning paths.

</details>


### [151] [ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute](https://arxiv.org/abs/2509.04475)
*Hao Wen,Yifan Su,Feifei Zhang,Yunxin Liu,Yunhao Liu,Ya-Qin Zhang,Yuanchun Li*

Main category: cs.CL

TL;DR: 现有大语言模型按顺序推理有瓶颈，提出ParaThinker框架实现并行推理，在推理基准测试中表现更好，证明并行计算对大模型扩展有效。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型按顺序增加计算量提升推理能力存在瓶颈，进一步计算只能带来微小性能提升。

Method: 引入原生思想并行的扩展范式，提出ParaThinker框架，训练大语言模型并行生成多样推理路径并合成最终答案。

Result: 在具有挑战性的推理基准测试中，ParaThinker比顺序推理的大语言模型有显著的准确率提升，增加的延迟开销可忽略不计。

Conclusion: 并行扩展计算比顺序扩展更有效，并行思维是未来大语言模型扩展的关键、高效维度。

Abstract: Recent advances in Large Language Models (LLMs) have been driven by test-time
compute scaling - a strategy that improves reasoning by generating longer,
sequential thought processes. While effective, this approach encounters a
significant bottleneck as computation increases, where further computation
offers only marginal performance gains. We argue this ceiling is not an
inherent limit of the model's capability but a flaw in the scaling strategy
itself, a phenomenon we term "Tunnel Vision", where a model's imperfect initial
steps lock it into a suboptimal reasoning path. To overcome this, we introduce
a new scaling paradigm: native thought parallelism. We present ParaThinker, an
end-to-end framework that trains an LLM to generate multiple, diverse reasoning
paths in parallel and synthesize them into a superior final answer. By
exploring different lines of thoughts simultaneously, ParaThinker effectively
sidesteps the Tunnel Vision issue and unlocks the model's latent reasoning
potential. Our approach demonstrates that scaling compute in parallel (width)
is a more effective and efficient way to superior reasoning than simply scaling
sequentially (depth). On challenging reasoning benchmarks, ParaThinker achieves
substantial accuracy improvements over sequential LLMs (12.3% for 1.5B and 7.5%
for 7B models on average with 8 parallel paths), while adding only negligible
latency overhead (7.1%). This enables smaller models to surpass much larger
counterparts and establishes parallel thinking as a critical, efficient
dimension for scaling future LLMs.

</details>


### [152] [Training Text-to-Molecule Models with Context-Aware Tokenization](https://arxiv.org/abs/2509.04476)
*Seojin Kim,Hyeontae Song,Jaehyun Nam,Jinwoo Shin*

Main category: cs.CL

TL;DR: 提出上下文感知分子T5（CAMT5）文本到分子模型，引入子结构级标记和基于重要性的训练策略，实验验证其优越性，还提出集成策略。


<details>
  <summary>Details</summary>
Motivation: 现有文本到分子模型依赖原子级标记，限制了捕捉分子全局结构上下文的能力。

Method: 引入子结构级标记，开发基于重要性的训练策略，优先处理关键子结构；提出简单有效的集成策略聚合模型输出。

Result: CAMT5在各种文本到分子生成任务中表现优越，使用仅2%的训练标记就优于现有方法。

Conclusion: CAMT5能更好地捕捉分子语义，有效提升文本到分子生成任务的性能。

Abstract: Recently, text-to-molecule models have shown great potential across various
chemical applications, e.g., drug-discovery. These models adapt language models
to molecular data by representing molecules as sequences of atoms. However,
they rely on atom-level tokenizations, which primarily focus on modeling local
connectivity, thereby limiting the ability of models to capture the global
structural context within molecules. To tackle this issue, we propose a novel
text-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by
the significance of the substructure-level contexts in understanding molecule
structures, e.g., ring systems, we introduce substructure-level tokenization
for text-to-molecule models. Building on our tokenization scheme, we develop an
importance-based training strategy that prioritizes key substructures, enabling
CAMT5 to better capture the molecular semantics. Extensive experiments verify
the superiority of CAMT5 in various text-to-molecule generation tasks.
Intriguingly, we find that CAMT5 outperforms the state-of-the-art methods using
only 2% of training tokens. In addition, we propose a simple yet effective
ensemble strategy that aggregates the outputs of text-to-molecule models to
further boost the generation performance. Code is available at
https://github.com/Songhyeontae/CAMT5.git.

</details>


### [153] [No Clustering, No Routing: How Transformers Actually Process Rare Tokens](https://arxiv.org/abs/2509.04479)
*Jing Liu*

Main category: cs.CL

TL;DR: 研究大语言模型对稀有标记预测的特化机制，发现稀有标记处理有双计算机制，高原神经元分散分布，注意力机制无偏好路由。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在稀有标记预测上存在困难，且其特化机制不明，先前虽发现高原神经元但功能组织未知。

Method: 在GPT - 2 XL和Pythia模型中进行神经元影响分析、基于图的聚类和注意力头消融实验。

Result: （1）稀有标记处理需额外高原神经元，形成双计算机制；（2）高原神经元分散分布，非模块化集群；（3）注意力机制无偏好路由。

Conclusion: 稀有标记特化源于分布式、训练驱动的分化，而非架构模块化，能在保持上下文敏感灵活性的同时实现自适应容量分配。

Abstract: Large language models struggle with rare token prediction, yet the mechanisms
driving their specialization remain unclear. Prior work identified specialized
``plateau'' neurons for rare tokens following distinctive three-regime
influence patterns \cite{liu2025emergent}, but their functional organization is
unknown. We investigate this through neuron influence analyses, graph-based
clustering, and attention head ablations in GPT-2 XL and Pythia models. Our
findings show that: (1) rare token processing requires additional plateau
neurons beyond the power-law regime sufficient for common tokens, forming dual
computational regimes; (2) plateau neurons are spatially distributed rather
than forming modular clusters; and (3) attention mechanisms exhibit no
preferential routing to specialists. These results demonstrate that rare token
specialization arises through distributed, training-driven differentiation
rather than architectural modularity, preserving context-sensitive flexibility
while achieving adaptive capacity allocation.

</details>


### [154] [Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented Large Language Models for Healthcare](https://arxiv.org/abs/2509.04482)
*Ravi Shankar,Sheng Wong,Lin Li,Magdalena Bachmann,Alex Silverthorne,Beth Albert,Gabriel Davis Jones*

Main category: cs.CL

TL;DR: 提出基于能量的模型（EBM）用于检索增强生成（RAG）系统可靠弃权，在语义难例上表现优，证明能量弃权评分更可靠。


<details>
  <summary>Details</summary>
Motivation: 可靠弃权对RAG系统很重要，特别是在女性健康等安全关键领域，错误答案会造成危害。

Method: 提出EBM，在260万基于指南的问题密集语义语料上学习平滑能量景观，与校准softmax基线和k近邻（kNN）密度启发式方法进行基准测试，进行综合消融实验。

Result: EBM在语义难例上弃权性能优越，AUROC达0.961，降低FPR@95；在简单负例上各方法性能相当，EBM在安全关键难分布中优势明显。

Conclusion: 基于能量的弃权评分比基于概率的softmax置信度提供更可靠的置信信号，为安全RAG系统提供可扩展和可解释的基础。

Abstract: Reliable abstention is critical for retrieval-augmented generation (RAG)
systems, particularly in safety-critical domains such as women's health, where
incorrect answers can lead to harm. We present an energy-based model (EBM) that
learns a smooth energy landscape over a dense semantic corpus of 2.6M
guideline-derived questions, enabling the system to decide when to generate or
abstain. We benchmark the EBM against a calibrated softmax baseline and a
k-nearest neighbour (kNN) density heuristic across both easy and hard
abstention splits, where hard cases are semantically challenging
near-distribution queries. The EBM achieves superior abstention performance
abstention on semantically hard cases, reaching AUROC 0.961 versus 0.950 for
softmax, while also reducing FPR@95 (0.235 vs 0.331). On easy negatives,
performance is comparable across methods, but the EBM's advantage becomes most
pronounced in safety-critical hard distributions. A comprehensive ablation with
controlled negative sampling and fair data exposure shows that robustness stems
primarily from the energy scoring head, while the inclusion or exclusion of
specific negative types (hard, easy, mixed) sharpens decision boundaries but is
not essential for generalisation to hard cases. These results demonstrate that
energy-based abstention scoring offers a more reliable confidence signal than
probability-based softmax confidence, providing a scalable and interpretable
foundation for safe RAG systems.

</details>


### [155] [DecMetrics: Structured Claim Decomposition Scoring for Factually Consistent LLM Outputs](https://arxiv.org/abs/2509.04483)
*Minghui Huang*

Main category: cs.CL

TL;DR: 引入DecMetrics评估声明分解质量，并开发轻量级模型优化性能，提升事实核查系统效果。


<details>
  <summary>Details</summary>
Motivation: 当前研究对声明分解生成方法关注多，对分解后原子声明质量评估不足，需填补此空白。

Method: 引入DecMetrics（包含COMPLETENESS、CORRECTNESS和SEMANTIC ENTROPY三个新指标）评估分解模型输出质量，将这些指标作为奖励函数优化轻量级声明分解模型。

Result: 通过自动评估为声明分解设定基准。

Conclusion: 该方法能提高事实核查系统的可靠性和有效性。

Abstract: Claim decomposition plays a crucial role in the fact-checking process by
breaking down complex claims into simpler atomic components and identifying
their unfactual elements. Despite its importance, current research primarily
focuses on generative methods for decomposition, with insufficient emphasis on
evaluating the quality of these decomposed atomic claims. To bridge this gap,
we introduce \textbf{DecMetrics}, which comprises three new metrics:
\texttt{COMPLETENESS}, \texttt{CORRECTNESS}, and \texttt{SEMANTIC ENTROPY},
designed to automatically assess the quality of claims produced by
decomposition models. Utilizing these metrics, we develop a lightweight claim
decomposition model, optimizing its performance through the integration of
these metrics as a reward function. Through automatic evaluation, our approach
aims to set a benchmark for claim decomposition, enhancing both the reliability
and effectiveness of fact-checking systems.

</details>


### [156] [The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors](https://arxiv.org/abs/2509.04484)
*Abdelrahman Sadallah,Tim Baumgärtner,Iryna Gurevych,Ted Briscoe*

Main category: cs.CL

TL;DR: 文章针对同行评审时间减少问题，确定评审意见关键方面，引入RevUtil数据集，对模型评估和生成进行基准测试，实验表明微调模型表现佳，机器评审表现逊于人类。


<details>
  <summary>Details</summary>
Motivation: 因评审人员时间减少，需自动化支持系统保证评审质量，使反馈对作者有用。

Method: 确定评审意见四个关键方面，引入RevUtil数据集，收集人类标注和合成标注评论，对微调模型进行基准测试。

Result: 微调模型与人类的一致性水平与强大的封闭模型相当，部分情况更优；机器生成评审在四个方面总体逊于人类评审。

Conclusion: 所引入的数据集和微调模型有助于评估和开发评审意见评估模型。

Abstract: Providing constructive feedback to paper authors is a core component of peer
review. With reviewers increasingly having less time to perform reviews,
automated support systems are required to ensure high reviewing quality, thus
making the feedback in reviews useful for authors. To this end, we identify
four key aspects of review comments (individual points in weakness sections of
reviews) that drive the utility for authors: Actionability, Grounding &
Specificity, Verifiability, and Helpfulness. To enable evaluation and
development of models assessing review comments, we introduce the RevUtil
dataset. We collect 1,430 human-labeled review comments and scale our data with
10k synthetically labeled comments for training purposes. The synthetic data
additionally contains rationales, i.e., explanations for the aspect score of a
review comment. Employing the RevUtil dataset, we benchmark fine-tuned models
for assessing review comments on these aspects and generating rationales. Our
experiments demonstrate that these fine-tuned models achieve agreement levels
with humans comparable to, and in some cases exceeding, those of powerful
closed models like GPT-4o. Our analysis further reveals that machine-generated
reviews generally underperform human reviews on our four aspects.

</details>


### [157] [ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records](https://arxiv.org/abs/2509.04485)
*Chris Sainsbury,Andreas Karwath*

Main category: cs.CL

TL;DR: 提出ASCENDgpt模型用于心血管风险预测，采用表型感知标记化方案，预训练后微调，在测试集表现出色，证明特定领域标记化和预训练对EHR风险预测有效。


<details>
  <summary>Details</summary>
Motivation: 设计基于纵向电子健康记录（EHR）进行心血管风险预测的模型。

Method: 引入表型感知标记化方案映射ICD代码，预训练ASCENDgpt模型，然后微调进行事件时间预测。

Result: 模型在测试集上平均C指数为0.816，各心血管结果表现良好。

Conclusion: 特定领域标记化和预训练对EHR风险预测任务有效。

Abstract: We present ASCENDgpt, a transformer-based model specifically designed for
cardiovascular risk prediction from longitudinal electronic health records
(EHRs). Our approach introduces a novel phenotype-aware tokenization scheme
that maps 47,155 raw ICD codes to 176 clinically meaningful phenotype tokens,
achieving 99.6\% consolidation of diagnosis codes while preserving semantic
information. This phenotype mapping contributes to a total vocabulary of 10,442
tokens - a 77.9\% reduction when compared with using raw ICD codes directly. We
pretrain ASCENDgpt on sequences derived from 19402 unique individuals using a
masked language modeling objective, then fine-tune for time-to-event prediction
of five cardiovascular outcomes: myocardial infarction (MI), stroke, major
adverse cardiovascular events (MACE), cardiovascular death, and all-cause
mortality. Our model achieves excellent discrimination on the held-out test set
with an average C-index of 0.816, demonstrating strong performance across all
outcomes (MI: 0.792, stroke: 0.824, MACE: 0.800, cardiovascular death: 0.842,
all-cause mortality: 0.824). The phenotype-based approach enables clinically
interpretable predictions while maintaining computational efficiency. Our work
demonstrates the effectiveness of domain-specific tokenization and pretraining
for EHR-based risk prediction tasks.

</details>


### [158] [Serialized Output Prompting for Large Language Model-based Multi-Talker Speech Recognition](https://arxiv.org/abs/2509.04488)
*Hao Shi,Yusuke Fujita,Tomoya Mizumoto,Lianbo Liu,Atsushi Kojima,Yui Sudo*

Main category: cs.CL

TL;DR: 本文提出SOP - MT - ASR方法，通过提取序列化输出提示和结构化提示引导大语言模型提升多说话人自动语音识别系统性能，实验表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多说话人自动语音识别系统在提示设计上存在不足，缺乏提升性能的提示设计工作。

Method: 提出SOP - MT - ASR方法，插入分隔符和序列化CTC层分离提取内容，用贪心搜索解码获得SOP，设计三阶段训练策略。

Result: 基于大语言模型的SOT模型在两说话人场景表现好，但在三说话人等复杂场景不能充分利用大语言模型，SOP方法在两和三说话人条件下显著提升性能。

Conclusion: 所提出的SOP方法能有效提升基于大语言模型的多说话人自动语音识别系统性能。

Abstract: Prompts are crucial for task definition and for improving the performance of
large language models (LLM)-based systems. However, existing LLM-based
multi-talker (MT) automatic speech recognition (ASR) systems either omit
prompts or rely on simple task-definition prompts, with no prior work exploring
the design of prompts to enhance performance. In this paper, we propose
extracting serialized output prompts (SOP) and explicitly guiding the LLM using
structured prompts to improve system performance (SOP-MT-ASR). A Separator and
serialized Connectionist Temporal Classification (CTC) layers are inserted
after the speech encoder to separate and extract MT content from the mixed
speech encoding in a first-speaking-first-out manner. Subsequently, the SOP,
which serves as a prompt for LLMs, is obtained by decoding the serialized CTC
outputs using greedy search. To train the model effectively, we design a
three-stage training strategy, consisting of serialized output training (SOT)
fine-tuning, serialized speech information extraction, and SOP-based
adaptation. Experimental results on the LibriMix dataset show that, although
the LLM-based SOT model performs well in the two-talker scenario, it fails to
fully leverage LLMs under more complex conditions, such as the three-talker
scenario. The proposed SOP approach significantly improved performance under
both two- and three-talker conditions.

</details>


### [159] [Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR](https://arxiv.org/abs/2509.04491)
*Xinnian Zhao,Hugo Van Hamme*

Main category: cs.CL

TL;DR: 提出在弱监督自动语音识别框架中利用电视字幕的新方法，实验证明能提升转录准确性，为训练ASR系统提供高质量资源。


<details>
  <summary>Details</summary>
Motivation: 电视字幕与音频对齐不精确，限制其作为逐字转录监督目标的适用性。

Method: 将字幕重新想象成富含上下文的提示，以生成的伪转录本为主要目标，字幕作为迭代细化的引导线索，引入加权注意力机制。

Result: 实验显示转录准确性有显著提高。

Conclusion: 所提方法在细化转录本方面有效，增强的伪标签数据集可为训练强大的ASR系统提供高质量基础资源。

Abstract: This study proposes a novel approach to using TV subtitles within a weakly
supervised (WS) Automatic Speech Recognition (ASR) framework. Although TV
subtitles are readily available, their imprecise alignment with corresponding
audio limits their applicability as supervised targets for verbatim
transcription. Rather than using subtitles as direct supervision signals, our
method reimagines them as context-rich prompts. This design enables the model
to handle discrepancies between spoken audio and subtitle text. Instead,
generated pseudo transcripts become the primary targets, with subtitles acting
as guiding cues for iterative refinement. To further enhance the process, we
introduce a weighted attention mechanism that emphasizes relevant subtitle
tokens during inference. Our experiments demonstrate significant improvements
in transcription accuracy, highlighting the effectiveness of the proposed
method in refining transcripts. These enhanced pseudo-labeled datasets provide
high-quality foundational resources for training robust ASR systems.

</details>


### [160] [Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate](https://arxiv.org/abs/2509.04492)
*Charles Moslonka,Hicham Randrianarivo,Arthur Garnier,Emmanuel Malherbe*

Main category: cs.CL

TL;DR: 本文提出一种适用于数据访问受限场景的大语言模型幻觉检测方法，通过利用非贪婪解码时的对数概率，结合熵产生率指标和监督学习，在多个数据集和模型上验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在问答任务中的幻觉问题严重影响其在现实世界中的可靠性，需要一种适用于数据访问受限场景的幻觉检测方法。

Method: 从非贪婪解码时产生的对数概率中导出不确定性指标，先使用熵产生率（EPR）指标作为基线，后结合监督学习，利用单个生成序列中可访问的排名靠前的标记的熵贡献特征。

Result: 该方法在多个问答数据集和多个大语言模型上显著提高了幻觉检测性能，仅使用少量可用的对数概率就能取得高性能。

Conclusion: 该方法为问答和检索增强生成系统提供了一种可快速部署的技术，能提高大语言模型响应的可信度，在金融领域也展示了其实用性。

Abstract: Hallucinations in Large Language Model (LLM) outputs for Question Answering
(QA) tasks critically undermine their real-world reliability. This paper
introduces an applied methodology for robust, one-shot hallucination detection,
specifically designed for scenarios with limited data access, such as
interacting with black-box LLM APIs that typically expose only a few top
candidate log-probabilities per token. Our approach derives uncertainty
indicators directly from these readily available log-probabilities generated
during non-greedy decoding. We first derive an Entropy Production Rate (EPR)
metric that offers baseline performance, later augmented with supervised
learning. Our learned model uses features representing the entropic
contributions of the accessible top-ranked tokens within a single generated
sequence, requiring no multiple query re-runs. Evaluated across diverse QA
datasets and multiple LLMs, this estimator significantly improves hallucination
detection over using EPR alone. Crucially, high performance is demonstrated
using only the typically small set of available log-probabilities (e.g., top
<10 per token), confirming its practical efficiency and suitability for these
API-constrained deployments. This work provides a readily deployable technique
to enhance the trustworthiness of LLM responses from a single generation pass
in QA and Retrieval-Augmented Generation (RAG) systems, with its utility
further demonstrated in a finance framework analyzing responses to queries on
annual reports from an industrial dataset.

</details>


### [161] [A Narrative-Driven Computational Framework for Clinician Burnout Surveillance](https://arxiv.org/abs/2509.04497)
*Syed Ahmad Chan Bukhari,Fazel Keshtkar,Alyssa Meczkowska*

Main category: cs.CL

TL;DR: 研究分析10000份ICU出院小结，提出混合管道模型，分类器表现良好，表明ICU临床叙述含主动监测医生倦怠信号。


<details>
  <summary>Details</summary>
Motivation: 医生倦怠威胁患者安全，现有研究多依赖回顾性调查工具或电子健康记录元数据，忽略了临床笔记中的叙述信息。

Method: 分析MIMIC - IV数据库中10000份ICU出院小结，引入结合临床叙述微调的BioBERT情感嵌入、针对医生倦怠监测的词汇应激词典和含工作量代理的五主题LDA的混合管道。

Result: 提供者级逻辑回归分类器在分层保留集上表现良好，F1分数超仅使用元数据的基线至少0.17；特定专科分析显示放射科、精神科和神经科医生倦怠风险高。

Conclusion: ICU临床叙述包含可用于主动监测医生健康状况的有效信号。

Abstract: Clinician burnout poses a substantial threat to patient safety, particularly
in high-acuity intensive care units (ICUs). Existing research predominantly
relies on retrospective survey tools or broad electronic health record (EHR)
metadata, often overlooking the valuable narrative information embedded in
clinical notes. In this study, we analyze 10,000 ICU discharge summaries from
MIMIC-IV, a publicly available database derived from the electronic health
records of Beth Israel Deaconess Medical Center. The dataset encompasses
diverse patient data, including vital signs, medical orders, diagnoses,
procedures, treatments, and deidentified free-text clinical notes. We introduce
a hybrid pipeline that combines BioBERT sentiment embeddings fine-tuned for
clinical narratives, a lexical stress lexicon tailored for clinician burnout
surveillance, and five-topic latent Dirichlet allocation (LDA) with workload
proxies. A provider-level logistic regression classifier achieves a precision
of 0.80, a recall of 0.89, and an F1 score of 0.84 on a stratified hold-out
set, surpassing metadata-only baselines by greater than or equal to 0.17 F1
score. Specialty-specific analysis indicates elevated burnout risk among
providers in Radiology, Psychiatry, and Neurology. Our findings demonstrate
that ICU clinical narratives contain actionable signals for proactive
well-being monitoring.

</details>


### [162] [Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations](https://arxiv.org/abs/2509.04498)
*Krithi Shailya,Akhilesh Kumar Mishra,Gokul S Krishnan,Balaraman Ravindran*

Main category: cs.CL

TL;DR: 本文研究三个开源大语言模型在大学和专业推荐中的地理、人口和经济偏见，发现存在强烈偏见，提出多维评估框架，强调教育大模型需考虑偏见。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为日常推荐系统存在延续社会偏见的风险，需研究其在大学和专业推荐中的偏见情况。

Method: 使用360个不同性别、国籍和经济状况的模拟用户配置文件，分析三个开源大语言模型的超25000条推荐。

Result: 推荐存在强烈偏见，全球北方机构被过度推荐，常强化性别刻板印象，机构重复普遍；LLaMA - 3.1多样性最高，但仍存在系统性差异。

Conclusion: 教育大模型需考虑偏见，以确保全球高等教育的公平获取。

Abstract: Large Language Models (LLMs) are increasingly used as daily recommendation
systems for tasks like education planning, yet their recommendations risk
perpetuating societal biases. This paper empirically examines geographic,
demographic, and economic biases in university and program suggestions from
three open-source LLMs: LLaMA-3.1-8B, Gemma-7B, and Mistral-7B. Using 360
simulated user profiles varying by gender, nationality, and economic status, we
analyze over 25,000 recommendations. Results show strong biases: institutions
in the Global North are disproportionately favored, recommendations often
reinforce gender stereotypes, and institutional repetition is prevalent. While
LLaMA-3.1 achieves the highest diversity, recommending 481 unique universities
across 58 countries, systemic disparities persist. To quantify these issues, we
propose a novel, multi-dimensional evaluation framework that goes beyond
accuracy by measuring demographic and geographic representation. Our findings
highlight the urgent need for bias consideration in educational LMs to ensure
equitable global access to higher education.

</details>


### [163] [DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability Across Citations and Evidence](https://arxiv.org/abs/2509.04499)
*Pranav Narayanan Venkit,Philippe Laban,Yilun Zhou,Kung-Hsiang Huang,Yixin Mao,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 介绍DeepTRACE审计框架评估生成式搜索引擎和深度研究LLM代理，发现其存在片面、过度自信及证据支持不足等问题。


<details>
  <summary>Details</summary>
Motivation: 解决生成式搜索引擎和深度研究LLM代理存在过度自信、弱溯源和混乱引用等问题。

Method: 引入DeepTRACE审计框架，进行语句级分析，构建引用和事实支持矩阵，用自动化提取管道和LLM评判器评估模型。

Result: 生成式搜索引擎和深度研究代理常给出片面、高自信回应，大量陈述无来源支持；深度研究配置可减少过度自信，但仍片面且存在大量无支持陈述，引用准确率40 - 80%。

Conclusion: 生成式搜索引擎和深度研究代理在推理和证据归因方面存在问题，虽深度研究配置有一定改进但仍需提升。

Abstract: Generative search engines and deep research LLM agents promise trustworthy,
source-grounded synthesis, yet users regularly encounter overconfidence, weak
sourcing, and confusing citation practices. We introduce DeepTRACE, a novel
sociotechnically grounded audit framework that turns prior community-identified
failure cases into eight measurable dimensions spanning answer text, sources,
and citations. DeepTRACE uses statement-level analysis (decomposition,
confidence scoring) and builds citation and factual-support matrices to audit
how systems reason with and attribute evidence end-to-end. Using automated
extraction pipelines for popular public models (e.g., GPT-4.5/5, You.com,
Perplexity, Copilot/Bing, Gemini) and an LLM-judge with validated agreement to
human raters, we evaluate both web-search engines and deep-research
configurations. Our findings show that generative search engines and deep
research agents frequently produce one-sided, highly confident responses on
debate queries and include large fractions of statements unsupported by their
own listed sources. Deep-research configurations reduce overconfidence and can
attain high citation thoroughness, but they remain highly one-sided on debate
queries and still exhibit large fractions of unsupported statements, with
citation accuracy ranging from 40--80% across systems.

</details>


### [164] [Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts](https://arxiv.org/abs/2509.04500)
*Rushi Wang,Jiateng Liu,Cheng Qian,Yifan Shen,Yanzhou Pan,Zhaozhuo Xu,Ahmed Abbasi,Heng Ji,Denghui Zhang*

Main category: cs.CL

TL;DR: 研究大语言模型处理混合上下文问题，引入测试床和适配模型，发现模型倾向使用不常见信息，提出RW - Steering方法提升回复质量。


<details>
  <summary>Details</summary>
Motivation: 现实世界上下文中相关信息与不适当内容混合，存在可靠性风险，需研究大语言模型如何处理和优先考虑混合上下文。

Method: 引入Poisoned Context Testbed，适配神经科学的Rescorla - Wagner模型，提出基于两阶段微调的RW - Steering方法。

Result: 适配模型发现大语言模型倾向使用上下文中不常见信息，实验表明最佳微调模型使回复质量提高39.8%，扭转不良行为曲线。

Conclusion: RW - Steering是一种强大、可推广的上下文工程解决方案，能提升大语言模型在现实应用中的安全性。

Abstract: Incorporating external context can significantly enhance the response quality
of Large Language Models (LLMs). However, real-world contexts often mix
relevant information with disproportionate inappropriate content, posing
reliability risks. How do LLMs process and prioritize mixed context? To study
this, we introduce the Poisoned Context Testbed, pairing queries with
real-world contexts containing relevant and inappropriate content. Inspired by
associative learning in animals, we adapt the Rescorla-Wagner (RW) model from
neuroscience to quantify how competing contextual signals influence LLM
outputs. Our adapted model reveals a consistent behavioral pattern: LLMs
exhibit a strong tendency to incorporate information that is less prevalent in
the context. This susceptibility is harmful in real-world settings, where small
amounts of inappropriate content can substantially degrade response quality.
Empirical evaluations on our testbed further confirm this vulnerability. To
tackle this, we introduce RW-Steering, a two-stage finetuning-based approach
that enables the model to internally identify and ignore inappropriate signals.
Unlike prior methods that rely on extensive supervision across diverse context
mixtures, RW-Steering generalizes robustly across varying proportions of
inappropriate content. Experiments show that our best fine-tuned model improves
response quality by 39.8% and reverses the undesirable behavior curve,
establishing RW-Steering as a robust, generalizable context engineering
solution for improving LLM safety in real-world use.

</details>


### [165] [Understanding Reinforcement Learning for Model Training, and future directions with GRAPE](https://arxiv.org/abs/2509.04501)
*Rohit Patel*

Main category: cs.CL

TL;DR: 本文详细介绍模型指令调优关键算法，回顾相关新技术，提出GRAPE研究新思路。


<details>
  <summary>Details</summary>
Motivation: 现有算法解释常假定有先验知识、缺乏关键细节或过于复杂笼统，需清晰直观阐释。

Method: 逐步讨论并开发算法，采用简化明确符号聚焦大语言模型，减少对更广泛强化学习文献的偏离。

Result: 对关键算法进行清晰解释，完成相关文献综述。

Conclusion: 提出GRAPE用于研究探索的新思路。

Abstract: This paper provides a self-contained, from-scratch, exposition of key
algorithms for instruction tuning of models: SFT, Rejection Sampling,
REINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy
Optimization (PPO), Group Relative Policy Optimization (GRPO), and Direct
Preference Optimization (DPO). Explanations of these algorithms often assume
prior knowledge, lack critical details, and/or are overly generalized and
complex. Here, each method is discussed and developed step by step using
simplified and explicit notation focused on LLMs, aiming to eliminate ambiguity
and provide a clear and intuitive understanding of the concepts. By minimizing
detours into the broader RL literature and connecting concepts to LLMs, we
eliminate superfluous abstractions and reduce cognitive overhead. Following
this exposition, we provide a literature review of new techniques and
approaches beyond those detailed. Finally, new ideas for research and
exploration in the form of GRAPE (Generalized Relative Advantage Policy
Evolution) are presented.

</details>


### [166] [VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples](https://arxiv.org/abs/2509.04502)
*Qixin Sun,Ziqin Wang,Hengyuan Zhao,Yilin Li,Kaiyou Song,Linjiang Huang,Xiaolin Hu,Qingpei Guo,Si Liu*

Main category: cs.CL

TL;DR: 提出VaccineRAG数据集和Partial - GRPO方法解决RAG中检索器精度问题，评估验证有效且将公开代码和数据。


<details>
  <summary>Details</summary>
Motivation: RAG中检索器精度常影响其有效性，许多检索样本无关或误导，成为大语言模型性能瓶颈。

Method: 引入VaccineRAG数据集，用不同正负样本比数据评估模型，让LLMs生成显式CoT分析提高样本判别能力；提出Partial - GRPO，将LLMs输出建模为多组件以学习长序列复杂CoT内容。

Result: 在VaccineRAG上的综合评估和消融研究验证了方案有效性。

Conclusion: 提出的VaccineRAG数据集和Partial - GRPO方法能有效解决RAG中检索器精度问题。

Abstract: Retrieval Augmented Generation enhances the response accuracy of Large
Language Models (LLMs) by integrating retrieval and generation modules with
external knowledge, demonstrating particular strength in real-time queries and
Visual Question Answering tasks. However, the effectiveness of RAG is
frequently hindered by the precision of the retriever: many retrieved samples
fed into the generation phase are irrelevant or misleading, posing a critical
bottleneck to LLMs' performance. To address this challenge, we introduce
VaccineRAG, a novel Chain-of-Thought-based retrieval-augmented generation
dataset. On one hand, VaccineRAG employs a benchmark to evaluate models using
data with varying positive/negative sample ratios, systematically exposing
inherent weaknesses in current LLMs. On the other hand, it enhances models'
sample-discrimination capabilities by prompting LLMs to generate explicit
Chain-of-Thought (CoT) analysis for each sample before producing final answers.
Furthermore, to enhance the model's ability to learn long-sequence complex CoT
content, we propose Partial-GRPO. By modeling the outputs of LLMs as multiple
components rather than a single whole, our model can make more informed
preference selections for complex sequences, thereby enhancing its capacity to
learn complex CoT. Comprehensive evaluations and ablation studies on VaccineRAG
validate the effectiveness of the proposed scheme. The code and dataset will be
publicly released soon.

</details>


### [167] [Behavioral Fingerprinting of Large Language Models](https://arxiv.org/abs/2509.04504)
*Zehua Pei,Hui-Ling Zhen,Ying Zhang,Zhiyuan Yang,Xing Li,Xianzhi Yu,Mingxuan Yuan,Bei Yu*

Main category: cs.CL

TL;DR: 本文提出行为指纹框架评估大语言模型，分析18个模型，发现核心能力趋同但对齐相关行为差异大，模型交互特性源于开发者对齐策略。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型基准主要关注性能指标，未能捕捉模型细微行为特征，因此提出新评估框架。

Method: 使用诊断提示套件和自动化评估流程，让强大的大语言模型作为公正评判者，分析不同能力层级的18个模型。

Result: 顶级模型核心能力趋同，对齐相关行为差异大，存在跨模型默认角色聚类。

Conclusion: 模型交互特性并非源于规模或推理能力，而是开发者对齐策略的结果，该框架可重现且可扩展。

Abstract: Current benchmarks for Large Language Models (LLMs) primarily focus on
performance metrics, often failing to capture the nuanced behavioral
characteristics that differentiate them. This paper introduces a novel
``Behavioral Fingerprinting'' framework designed to move beyond traditional
evaluation by creating a multi-faceted profile of a model's intrinsic cognitive
and interactive styles. Using a curated \textit{Diagnostic Prompt Suite} and an
innovative, automated evaluation pipeline where a powerful LLM acts as an
impartial judge, we analyze eighteen models across capability tiers. Our
results reveal a critical divergence in the LLM landscape: while core
capabilities like abstract and causal reasoning are converging among top
models, alignment-related behaviors such as sycophancy and semantic robustness
vary dramatically. We further document a cross-model default persona clustering
(ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together,
this suggests that a model's interactive nature is not an emergent property of
its scale or reasoning power, but a direct consequence of specific, and highly
variable, developer alignment strategies. Our framework provides a reproducible
and scalable methodology for uncovering these deep behavioral differences.
Project: https://github.com/JarvisPei/Behavioral-Fingerprinting

</details>


### [168] [Uncertainty-Aware Collaborative System of Large and Small Models for Multimodal Sentiment Analysis](https://arxiv.org/abs/2509.04459)
*Shiqin Han,Manning Gao,Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: 提出不确定性感知协作系统U - ACS解决多模态模型性能与效率权衡问题，实验证明其在节省资源下达最优性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型计算需求大阻碍实际部署，小模型效率高但性能差，需解决性能 - 效率权衡问题。

Method: 提出U - ACS系统，采用不确定性驱动级联机制，小模型先过滤，高不确定性样本交予大模型，还有处理模糊或冲突预测的策略。

Result: 在基准数据集上实验，该方法达到最优性能，相比单独使用大模型只需少量计算资源。

Conclusion: 所提方法有效解决多模态模型性能与效率的权衡问题。

Abstract: The advent of Multimodal Large Language Models (MLLMs) has significantly
advanced the state-of-the-art in multimodal machine learning, yet their
substantial computational demands present a critical barrier to real-world
deployment. Conversely, smaller, specialized models offer high efficiency but
often at the cost of performance. To reconcile this performance-efficiency
trade-off, we propose a novel Uncertainty-Aware Collaborative System (U-ACS)
that synergistically orchestrates a powerful MLLM (e.g., HumanOmni) and a
lightweight baseline model for multimodal sentiment analysis. The core of our
system is an uncertainty-driven cascade mechanism, where the efficient small
model first acts as a rapid filter for all input samples. Only those samples
yielding high predictive uncertainty, thereby indicating greater difficulty,
are selectively escalated to the MLLM for more sophisticated analysis.
Furthermore, our system introduces advanced strategies to handle ambiguous or
conflicting predictions, including weighted averaging for predictions of
similar polarity and a prompt-based cross-verification to resolve conflicting
predictions when both models exhibit high uncertainty. This
sample-difficulty-aware approach allows for a dynamic allocation of
computational resources, drastically reducing inference costs while retaining
the high accuracy of MLLM. Extensive experiments on benchmark datasets
demonstrate that our proposed method achieves state-of-the-art performance,
while requiring only a fraction of the computational resources compared to
using a standalone MLLM.

</details>


### [169] [From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM Approach](https://arxiv.org/abs/2509.04507)
*Nithyashree Sivasubramaniam*

Main category: cs.CL

TL;DR: 本文提出增强自动语音识别框架提升无声语音接口可懂度，实验显示词错误率显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有无声语音接口在合成语音识别和下游处理方面存在不足，合成语音有语音歧义与噪声问题。

Method: 提出结合基于Transformer的声学模型和大语言模型（LLM）进行后处理的增强自动语音识别框架。

Result: 实验结果显示，相较于36%的基线，词错误率相对降低16%，绝对降低6%。

Conclusion: 该框架能显著提升无声语音接口的可懂度。

Abstract: Silent Speech Interfaces (SSIs) have gained attention for their ability to
generate intelligible speech from non-acoustic signals. While significant
progress has been made in advancing speech generation pipelines, limited work
has addressed the recognition and downstream processing of synthesized speech,
which often suffers from phonetic ambiguity and noise. To overcome these
challenges, we propose an enhanced automatic speech recognition framework that
combines a transformer-based acoustic model with a large language model (LLM)
for post-processing. The transformer captures full utterance context, while the
LLM ensures linguistic consistency. Experimental results show a 16% relative
and 6% absolute reduction in word error rate (WER) over a 36% baseline,
demonstrating substantial improvements in intelligibility for silent speech
interfaces.

</details>


### [170] [Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through Model Explanations](https://arxiv.org/abs/2509.04515)
*Martha O. Dimgba,Sharon Oba,Ameeta Agrawal,Philippe J. Giabbanelli*

Main category: cs.CL

TL;DR: 本文研究AI生成职业故事中的性别和族裔偏见，提出BAME策略减少偏见，分析多模型和职业群体，发现该策略能提升人口统计学平等性。


<details>
  <summary>Details</summary>
Motivation: 语言模型输出会传播社会偏见，尤其是性别和族裔方面，因此研究AI生成职业故事中的相关偏见。

Method: 提出Bias Analysis and Mitigation through Explanation (BAME)策略，利用模型生成的解释进行有针对性的提示工程，不修改模型参数；分析25个职业群体、三个大语言模型生成的故事及多个人口统计维度。

Result: 应用BAME策略前后测量表征偏见，人口统计学表征改善幅度为2% - 20%，识别出与训练数据刻板印象相关的过度和不足表征的持久模式。

Conclusion: 利用模型自身的内部推理机制引导模型可显著提升人口统计学平等性，有助于开发更透明的生成式AI系统。

Abstract: Language models have been shown to propagate social bias through their
output, particularly in the representation of gender and ethnicity. This paper
investigates gender and ethnicity biases in AI-generated occupational stories.
Representation biases are measured before and after applying our proposed
mitigation strategy, Bias Analysis and Mitigation through Explanation (BAME),
revealing improvements in demographic representation ranging from 2% to 20%.
BAME leverages model-generated explanations to inform targeted prompt
engineering, effectively reducing biases without modifying model parameters. By
analyzing stories generated across 25 occupational groups, three large language
models (Claude 3.5 Sonnet, Llama 3.1 70B Instruct, and GPT-4 Turbo), and
multiple demographic dimensions, we identify persistent patterns of
overrepresentation and underrepresentation linked to training data stereotypes.
Our findings demonstrate that guiding models with their own internal reasoning
mechanisms can significantly enhance demographic parity, thereby contributing
to the development of more transparent generative AI systems.

</details>


### [171] [Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation](https://arxiv.org/abs/2509.04534)
*Zaifu Zhan,Shuang Zhou,Min Zeng,Kai Yu,Meijia Song,Xiaoyi Chen,Jun Wang,Yu Hou,Rui Zhang*

Main category: cs.CL

TL;DR: 研究评估量化对12个大语言模型的影响，发现量化可大幅降低GPU内存需求并保留性能，为生物医学场景本地部署大模型提供策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗场景因规模和计算需求大、数据隐私问题难以应用，需解决本地部署难题。

Method: 系统评估量化对12个通用和生物医学特定大语言模型在8个基准数据集上4项关键任务的影响。

Result: 量化最多可降低75%的GPU内存需求，保留模型在不同任务中的性能，能让70B参数模型在40GB消费级GPU上部署，且保留领域知识和对高级提示方法的响应能力。

Conclusion: 量化是在生物医学场景安全本地部署大而高性能语言模型的实用有效策略，缩小AI技术进步与临床应用的差距。

Abstract: Large language models have demonstrated remarkable capabilities in biomedical
natural language processing, yet their rapid growth in size and computational
requirements present a major barrier to adoption in healthcare settings where
data privacy precludes cloud deployment and resources are limited. In this
study, we systematically evaluated the impact of quantization on 12
state-of-the-art large language models, including both general-purpose and
biomedical-specific models, across eight benchmark datasets covering four key
tasks: named entity recognition, relation extraction, multi-label
classification, and question answering. We show that quantization substantially
reduces GPU memory requirements-by up to 75%-while preserving model performance
across diverse tasks, enabling the deployment of 70B-parameter models on 40GB
consumer-grade GPUs. In addition, domain-specific knowledge and responsiveness
to advanced prompting methods are largely maintained. These findings provide
significant practical and guiding value, highlighting quantization as a
practical and effective strategy for enabling the secure, local deployment of
large yet high-capacity language models in biomedical contexts, bridging the
gap between technical advances in AI and real-world clinical translation.

</details>


### [172] [Discrete Prompt Tuning via Recursive Utilization of Black-box Multimodal Large Language Model for Personalized Visual Emotion Recognition](https://arxiv.org/abs/2509.04480)
*Ryo Takahashi,Naoki Saito,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CL

TL;DR: 文章指出多模态大语言模型在个性化视觉情感识别上有局限，提出离散提示调优方法实现准确的个性化视觉情感识别。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在个性化视觉情感识别中因训练数据问题倾向多数观点和熟悉模式，限制了其在个性化场景的表现，需改进。

Method: 采用受人类提示工程启发的离散提示调优方法，从生成提示中选最佳自然语言表示更新提示。

Result: 未提及。

Conclusion: 未提及。

Abstract: Visual Emotion Recognition (VER) is an important research topic due to its
wide range of applications, including opinion mining and advertisement design.
Extending this capability to recognize emotions at the individual level further
broadens its potential applications. Recently, Multimodal Large Language Models
(MLLMs) have attracted increasing attention and demonstrated performance
comparable to that of conventional VER methods. However, MLLMs are trained on
large and diverse datasets containing general opinions, which causes them to
favor majority viewpoints and familiar patterns. This tendency limits their
performance in a personalized VER, which is crucial for practical and
real-world applications, and indicates a key area for improvement. To address
this limitation, the proposed method employs discrete prompt tuning inspired by
the process of humans' prompt engineering to adapt the VER task to each
individual. Our method selects the best natural language representation from
the generated prompts and uses it to update the prompt for the realization of
accurate personalized VER.

</details>


### [173] [Scaling behavior of large language models in emotional safety classification across sizes and tasks](https://arxiv.org/abs/2509.04512)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.CL

TL;DR: 研究大语言模型处理情感敏感内容的缩放行为，发现大模型表现好，但轻量级微调可让小模型达到类似效果，小模型可用于敏感应用。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型处理情感敏感内容的方式，以构建安全可靠的系统，特别是在心理健康场景中。

Method: 构建新数据集，对四个LLaMA模型在零样本、少样本和微调设置下进行评估。

Result: 大模型平均表现更好，轻量级微调使1B模型在高数据类别中可达到与大模型和BERT相当的性能，推理时只需<2GB显存。

Conclusion: 小的设备端模型可作为敏感应用的可行、保护隐私的替代方案，对治疗性大语言模型应用和安全关键系统的可扩展对齐有重要意义。

Abstract: Understanding how large language models (LLMs) process emotionally sensitive
content is critical for building safe and reliable systems, particularly in
mental health contexts. We investigate the scaling behavior of LLMs on two key
tasks: trinary classification of emotional safety (safe vs. unsafe vs.
borderline) and multi-label classification using a six-category safety risk
taxonomy. To support this, we construct a novel dataset by merging several
human-authored mental health datasets (> 15K samples) and augmenting them with
emotion re-interpretation prompts generated via ChatGPT. We evaluate four LLaMA
models (1B, 3B, 8B, 70B) across zero-shot, few-shot, and fine-tuning settings.
Our results show that larger LLMs achieve stronger average performance,
particularly in nuanced multi-label classification and in zero-shot settings.
However, lightweight fine-tuning allowed the 1B model to achieve performance
comparable to larger models and BERT in several high-data categories, while
requiring <2GB VRAM at inference. These findings suggest that smaller,
on-device models can serve as viable, privacy-preserving alternatives for
sensitive applications, offering the ability to interpret emotional context and
maintain safe conversational boundaries. This work highlights key implications
for therapeutic LLM applications and the scalable alignment of safety-critical
systems.

</details>


### [174] [Analysis of Voluntarily Reported Data Post Mesh Implantation for Detecting Public Emotion and Identifying Concern Reports](https://arxiv.org/abs/2509.04517)
*Indu Bala,Lewis Mitchell,Marianne H Gillam*

Main category: cs.CL

TL;DR: 研究利用NLP分析2000 - 2021年MAUDE数据库中患者报告，分析网片植入后患者情绪，检测到特定时段担忧报告增加和情绪强度上升，为医护人员提供见解。


<details>
  <summary>Details</summary>
Motivation: 解决疝气修补手术中网片植入术后并发症问题，了解患者情绪变化与医疗设备监管、技术进步的关系，改善术前咨询和术后护理。

Method: 使用NRC情感词典和TextBlob进行情感分析，将患者叙述分为八种情绪并评估情感极性。

Result: 在2011 - 2012年和2017 - 2018年检测到担忧报告增加和更高的情绪强度。

Conclusion: 强调医疗实践中考虑情绪因素的重要性，以及情感分析对改善患者护理的潜力。

Abstract: Mesh implants are widely utilized in hernia repair surgeries, but
postoperative complications present a significant concern. This study analyzes
patient reports from the Manufacturer and User Facility Device Experience
(MAUDE) database spanning 2000 to 2021 to investigate the emotional aspects of
patients following mesh implantation using Natural Language Processing (NLP).
Employing the National Research Council Canada (NRC) Emotion Lexicon and
TextBlob for sentiment analysis, the research categorizes patient narratives
into eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy,
and disgust) and assesses sentiment polarity. The goal is to discern patterns
in patient sentiment over time and to identify reports signaling urgent
concerns, referred to as "Concern Reports," thereby understanding shifts in
patient experiences in relation to changes in medical device regulation and
technological advancements in healthcare. The study detected an increase in
Concern Reports and higher emotional intensity during the periods of 2011-2012
and 2017-2018. Through temporal analysis of Concern Reports and overall
sentiment, this research provides valuable insights for healthcare
practitioners, enhancing their understanding of patient experiences
post-surgery, which is critical for improving preoperative counselling,
postoperative care, and preparing patients for mesh implant surgeries. The
study underscores the importance of emotional considerations in medical
practices and the potential for sentiment analysis to inform and enhance
patient care.

</details>


### [175] [Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions](https://arxiv.org/abs/2509.04549)
*Faruk Alpay,Taylan Alpay*

Main category: cs.CL

TL;DR: 本文探索对Transformer模型进行细粒度控制的方法，提出统一框架，理论与实证都有成果，也讨论了风险，为设计可控鲁棒语言模型奠基。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型在NLP任务中出色，但细粒度控制仍具挑战，需要探索相关方法。

Method: 在提示、激活和权重三个层面进行干预，将可控文本生成形式化为优化问题，运用提示工程、参数高效微调、模型编辑和强化学习等方法，引入统一框架。

Result: 理论上表明最小权重更新可实现目标行为改变且副作用有限；实证上在情感控制和事实编辑方面成功率超90%，但存在泛化-特异性权衡。

Conclusion: 为设计可控和鲁棒的语言模型奠定基础，同时需关注伦理双用途风险和严格评估。

Abstract: Transformer-based language models excel in NLP tasks, but fine-grained
control remains challenging. This paper explores methods for manipulating
transformer models through principled interventions at three levels: prompts,
activations, and weights. We formalize controllable text generation as an
optimization problem addressable via prompt engineering, parameter-efficient
fine-tuning, model editing, and reinforcement learning. We introduce a unified
framework encompassing prompt-level steering, activation interventions, and
weight-space edits. We analyze robustness and safety implications, including
adversarial attacks and alignment mitigations. Theoretically, we show minimal
weight updates can achieve targeted behavior changes with limited side-effects.
Empirically, we demonstrate >90% success in sentiment control and factual edits
while preserving base performance, though generalization-specificity trade-offs
exist. We discuss ethical dual-use risks and the need for rigorous evaluation.
This work lays groundwork for designing controllable and robust language
models.

</details>


### [176] [Sample-efficient Integration of New Modalities into Large Language Models](https://arxiv.org/abs/2509.04606)
*Osman Batur İnce,André F. T. Martins,Oisin Mac Aodha,Edoardo M. Ponti*

Main category: cs.CL

TL;DR: 提出用于大语言模型的样本高效模态集成方法SEMI，在少样本集成新模态时提升样本效率，有望扩展基础模型的模态覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 训练涵盖所有模态的模型不可行，将模态集成到现有基础模型需大量配对数据，低资源模态数据常缺失。

Method: 设计超网络，在推理时根据任意模态的少量样本生成合适适配器；通过等距变换增加编码器数量以提高训练模态的多样性。

Result: SEMI在少样本集成新模态时显著提升样本效率，如达到与32-shot SEMI相同准确率，从头训练投影器需多64倍数据。

Conclusion: SEMI有望扩展基础模型的模态覆盖范围。

Abstract: Multimodal foundation models can process several modalities. However, since
the space of possible modalities is large and evolving over time, training a
model from scratch to encompass all modalities is unfeasible. Moreover,
integrating a modality into a pre-existing foundation model currently requires
a significant amount of paired data, which is often not available for
low-resource modalities. In this paper, we introduce a method for
sample-efficient modality integration (SEMI) into Large Language Models (LLMs).
To this end, we devise a hypernetwork that can adapt a shared projector --
placed between modality-specific encoders and an LLM -- to any modality. The
hypernetwork, trained on high-resource modalities (i.e., text, speech, audio,
video), is conditioned on a few samples from any arbitrary modality at
inference time to generate a suitable adapter. To increase the diversity of
training modalities, we artificially multiply the number of encoders through
isometric transformations. We find that SEMI achieves a significant boost in
sample efficiency during few-shot integration of new modalities (i.e.,
satellite images, astronomical images, inertial measurements, and molecules)
with encoders of arbitrary embedding dimensionality. For instance, to reach the
same accuracy as 32-shot SEMI, training the projector from scratch needs
64$\times$ more data. As a result, SEMI holds promise to extend the modality
coverage of foundation models.

</details>


### [177] [Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs](https://arxiv.org/abs/2509.04615)
*Brennen Hill,Surendra Parla,Venkata Abhijeeth Balabhadruni,Atharv Prajod Padmalayam,Sujay Chandra Shekara Sharma*

Main category: cs.CL

TL;DR: 本文对大语言模型基于提示的攻击方法进行全面文献综述，旨在助力构建安全的下一代大语言模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型普及带来安全挑战，基于提示的攻击会造成严重危害，系统了解攻击向量是开发对策的基础。

Method: 对基于提示的攻击方法进行全面文献调研，并对其进行分类。

Result: 详细阐述了这些攻击的机制和影响，提供了清晰的威胁模型。

Conclusion: 为研究界构建下一代安全大语言模型的努力提供参考。

Abstract: The proliferation of Large Language Models (LLMs) has introduced critical
security challenges, where adversarial actors can manipulate input prompts to
cause significant harm and circumvent safety alignments. These prompt-based
attacks exploit vulnerabilities in a model's design, training, and contextual
understanding, leading to intellectual property theft, misinformation
generation, and erosion of user trust. A systematic understanding of these
attack vectors is the foundational step toward developing robust
countermeasures. This paper presents a comprehensive literature survey of
prompt-based attack methodologies, categorizing them to provide a clear threat
model. By detailing the mechanisms and impacts of these exploits, this survey
aims to inform the research community's efforts in building the next generation
of secure LLMs that are inherently resistant to unauthorized distillation,
fine-tuning, and editing.

</details>


### [178] [Comparative Analysis of Transformer Models in Disaster Tweet Classification for Public Safety](https://arxiv.org/abs/2509.04650)
*Sharif Noor Zisad,Ragib Hasan*

Main category: cs.CL

TL;DR: 评估基于Transformer的模型对灾害相关推文分类的效果，发现其优于传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在灾害相关推文分类时难以理解上下文和深层含义，需更有效的模型。

Method: 评估BERT、DistilBERT、RoBERTa和DeBERTa等基于Transformer的模型，并与传统机器学习方法对比。

Result: BERT准确率达91%，显著高于传统模型的82%。

Conclusion: Transformer架构更适合公共安全应用，有更高准确率、更好的语言理解和泛化能力。

Abstract: Twitter and other social media platforms have become vital sources of real
time information during disasters and public safety emergencies. Automatically
classifying disaster related tweets can help emergency services respond faster
and more effectively. Traditional Machine Learning (ML) models such as Logistic
Regression, Naive Bayes, and Support Vector Machines have been widely used for
this task, but they often fail to understand the context or deeper meaning of
words, especially when the language is informal, metaphorical, or ambiguous. We
posit that, in this context, transformer based models can perform better than
traditional ML models. In this paper, we evaluate the effectiveness of
transformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for
classifying disaster related tweets. These models are compared with traditional
ML approaches to highlight the performance gap. Experimental results show that
BERT achieved the highest accuracy (91%), significantly outperforming
traditional models like Logistic Regression and Naive Bayes (both at 82%). The
use of contextual embeddings and attention mechanisms allows transformer models
to better understand subtle language in tweets, where traditional ML models
fall short. This research demonstrates that transformer architectures are far
more suitable for public safety applications, offering improved accuracy,
deeper language understanding, and better generalization across real world
social media text.

</details>


### [179] [Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs](https://arxiv.org/abs/2509.04655)
*Ayush Gupta,Ramneet Kaur,Anirban Roy,Adam D. Cobb,Rama Chellappa,Susmit Jha*

Main category: cs.CL

TL;DR: 提出用于专业大语言模型的推理时OOD检测算法，利用ICAD框架和基于模型dropout容忍度的非一致性度量，实验表明优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 专业大语言模型微调后在域内任务表现佳，但面对OOD输入易输出错误或不可靠结果，在关键应用有风险，需有效OOD检测算法。

Method: 利用ICAD框架，采用基于模型dropout容忍度的新非一致性度量，通过有效集成方法聚合多层dropout容忍度。

Result: 医学专业大语言模型实验显示，该方法检测OOD输入优于基线方法，AUROC提升2% - 37%。

Conclusion: 所提算法能有效检测专业大语言模型的OOD输入。

Abstract: We propose a novel inference-time out-of-domain (OOD) detection algorithm for
specialized large language models (LLMs). Despite achieving state-of-the-art
performance on in-domain tasks through fine-tuning, specialized LLMs remain
vulnerable to incorrect or unreliable outputs when presented with OOD inputs,
posing risks in critical applications. Our method leverages the Inductive
Conformal Anomaly Detection (ICAD) framework, using a new non-conformity
measure based on the model's dropout tolerance. Motivated by recent findings on
polysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs
exhibit higher dropout tolerance than OOD inputs. We aggregate dropout
tolerance across multiple layers via a valid ensemble approach, improving
detection while maintaining theoretical false alarm bounds from ICAD.
Experiments with medical-specialized LLMs show that our approach detects OOD
inputs better than baseline methods, with AUROC improvements of $2\%$ to $37\%$
when treating OOD datapoints as positives and in-domain test datapoints as
negatives.

</details>


### [180] [ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs](https://arxiv.org/abs/2509.04696)
*Samira Khorshidi,Azadeh Nikfarjam,Suprita Shankar,Yisi Sang,Yash Govind,Hyun Jang,Ali Kasgari,Alexis McClimans,Mohamed Soliman,Vishnu Konda,Ahmed Fakhry,Xiaoguang Qi*

Main category: cs.CL

TL;DR: 提出生产级系统ODKE+，可从网络源高精度自动提取和摄入数百万开放域事实，支持批处理和流处理模式，显著提升知识图谱覆盖度。


<details>
  <summary>Details</summary>
Motivation: 知识图谱维护新鲜度和完整性成本高，需要高效方法提取和摄入开放域事实。

Method: 将模块化组件组合成可扩展管道，包括提取启动器、证据检索器、混合知识提取器、轻量级验证器和确认器，动态生成本体片段。

Result: 处理超900万维基百科页面，摄入1900万高置信度事实，精度达98.8%，与第三方知识图谱重叠率达48%，平均减少更新延迟50天。

Conclusion: 基于大语言模型的提取结合本体结构和验证流程，可实现可信赖、大规模知识摄入，有广泛现实应用。

Abstract: Knowledge graphs (KGs) are foundational to many AI applications, but
maintaining their freshness and completeness remains costly. We present ODKE+,
a production-grade system that automatically extracts and ingests millions of
open-domain facts from web sources with high precision. ODKE+ combines modular
components into a scalable pipeline: (1) the Extraction Initiator detects
missing or stale facts, (2) the Evidence Retriever collects supporting
documents, (3) hybrid Knowledge Extractors apply both pattern-based rules and
ontology-guided prompting for large language models (LLMs), (4) a lightweight
Grounder validates extracted facts using a second LLM, and (5) the Corroborator
ranks and normalizes candidate facts for ingestion. ODKE+ dynamically generates
ontology snippets tailored to each entity type to align extractions with schema
constraints, enabling scalable, type-consistent fact extraction across 195
predicates. The system supports batch and streaming modes, processing over 9
million Wikipedia pages and ingesting 19 million high-confidence facts with
98.8% precision. ODKE+ significantly improves coverage over traditional
methods, achieving up to 48% overlap with third-party KGs and reducing update
lag by 50 days on average. Our deployment demonstrates that LLM-based
extraction, grounded in ontological structure and verification workflows, can
deliver trustworthiness, production-scale knowledge ingestion with broad
real-world applicability. A recording of the system demonstration is included
with the submission and is also available at https://youtu.be/UcnE3_GsTWs.

</details>


### [181] [Research on Multi-hop Inference Optimization of LLM Based on MQUAKE Framework](https://arxiv.org/abs/2509.04770)
*Zucheng Liang,Wenxin Wei,Kaijie Zhang,Hongyi Chen*

Main category: cs.CL

TL;DR: 本文提出多跳问题分解方法，基于MQUAKE框架和LLAMA3模型研究其对大语言模型回答复杂问题的影响，实验表明该方法在训练前后均有效。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型准确回答复杂问题的挑战。

Method: 提出多跳问题分解方法，将MQUAKE - T数据集分为单跳和多跳数据集，用LoRA方法微调LLAMA3模型并进行推理测试。

Result: 未微调时，多跳问题分解方法预测性能远超直接回答法；微调后，两种方法性能均提升，多跳分解法保持优势。

Conclusion: 多跳分解方法在训练前后均有效，能增强大语言模型回答复杂问题的能力。

Abstract: Accurately answering complex questions has consistently been a significant
challenge for Large Language Models (LLMs). To address this, this paper
proposes a multi-hop question decomposition method for complex questions,
building upon research within the MQUAKE framework. Utilizing the LLAMA3 model,
we systematically investigate the impact of multi-hop question decomposition
within knowledge graphs on model comprehension and reasoning accuracy, both
before and after model training. In our experiments, we systematically
partitioned and converted the MQUAKE-T dataset into two distinct formats: a
single-hop dataset designed for directly answering complex questions, and a
multi-hop dataset constructed using the multi-hop question decomposition
method. We then fine-tuned the LLAMA3 model on these datasets and conducted
inference tests. Our results demonstrate that, without fine-tuning the LLM, the
prediction performance based on the multi-hop question decomposition method
significantly outperforms the method of directly answering complex questions.
After fine-tuning using the LoRA (Low-Rank Adaptation) method, the performance
of both approaches improved compared to the untrained baseline. Crucially, the
method utilizing multi-hop decomposition consistently maintained its
superiority. These findings validate the effectiveness of the multi-hop
decomposition method both before and after training, demonstrating its
capability to effectively enhance the LLM's ability to answer complex
questions.

</details>


### [182] [A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning](https://arxiv.org/abs/2509.04753)
*Cheng Peng,Xinyu Dong,Mengxian Lyu,Daniel Paredes,Yaoyun Zhang,Yonghui Wu*

Main category: cs.CL

TL;DR: 研究探讨大语言模型在患者信息提取中的有效性，涉及架构、微调策略等，通过多数据集基准测试评估性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽革新临床自然语言处理任务，但在患者信息提取任务中的最佳应用有待探索。

Method: 对一系列大语言模型在五个数据集上进行基准测试，比较传统全量微调和基于提示的参数高效微调，采用留一数据集策略在四个数据集上探索多任务指令调优框架。

Result: 原文未提及具体结果。

Conclusion: 原文未提及具体结论。

Abstract: Natural language processing (NLP) is a key technology to extract important
patient information from clinical narratives to support healthcare
applications. The rapid development of large language models (LLMs) has
revolutionized many NLP tasks in the clinical domain, yet their optimal use in
patient information extraction tasks requires further exploration. This study
examines LLMs' effectiveness in patient information extraction, focusing on LLM
architectures, fine-tuning strategies, and multi-task instruction tuning
techniques for developing robust and generalizable patient information
extraction systems. This study aims to explore key concepts of using LLMs for
clinical concept and relation extraction tasks, including: (1) encoder-only or
decoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT)
algorithms, and (3) multi-task instruction tuning on few-shot learning
performance. We benchmarked a suite of LLMs, including encoder-based LLMs
(BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1,
GatorTronLlama), across five datasets. We compared traditional full-size
fine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning
framework that combines both tasks across four datasets to evaluate the
zero-shot and few-shot learning performance using the leave-one-dataset-out
strategy.

</details>


### [183] [Decoders Laugh as Loud as Encoders](https://arxiv.org/abs/2509.04779)
*Eli Borodach,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: 探讨大语言模型对幽默的理解，发现微调后的解码器GPT - 4o表现与最佳微调编码器RoBERTa相当。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理任务表现出色，但不清楚其对幽默这类微妙主题的理解程度，当前计算机是否理解幽默尚无定论。

Method: 对解码器GPT - 4o进行微调。

Result: 微调后的解码器GPT - 4o平均F1 - macro分数为0.85，与最佳微调编码器RoBERTa平均F1分数0.86表现相当。

Conclusion: 在评估对幽默的理解上，微调后的解码器GPT - 4o和最佳微调编码器RoBERTa有相近的性能。

Abstract: From the dawn of the computer, Allen Turing dreamed of a robot that could
communicate using language as a human being. The recent advances in the field
of Large Language Models (LLMs) shocked the scientific community when a single
model can apply for various natural language processing (NLP) tasks, while the
output results are sometimes even better than most human communication skills.
Models such as GPT, Claude, Grok, etc. have left their mark on the scientific
community. However, it is unclear how much these models understand what they
produce, especially in a nuanced theme such as humor. The question of whether
computers understand humor is still open (among the decoders, the latest to be
checked was GPT-2). We addressed this issue in this paper; we have showed that
a fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well
as the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)

</details>


### [184] [Enhancing Diversity in Large Language Models via Determinantal Point Processes](https://arxiv.org/abs/2509.04784)
*Yilei Chen,Souradip Chakraborty,Lorenz Wolf,Ioannis Ch. Paschalidis,Aldo Pacchiano*

Main category: cs.CL

TL;DR: 提出基于DPPs的DQO训练方法优化大语言模型的质量和语义多样性，实验表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 监督微调与强化学习在提升模型下游任务表现时会降低输出多样性，现有增强多样性方法有限。

Method: 提出基于DPPs的DQO训练方法，为每个提示采样并嵌入一组响应，用基于核的相似矩阵行列式衡量多样性。

Result: 在指令遵循、摘要生成、故事创作和推理任务中，该方法显著提高语义多样性且不牺牲模型质量。

Conclusion: DQO训练方法能有效优化大语言模型的质量和语义多样性。

Abstract: Supervised fine-tuning and reinforcement learning are two popular methods for
post-training large language models (LLMs). While improving the model's
performance on downstream tasks, they often reduce the model's output
diversity, leading to narrow, canonical responses. Existing methods to enhance
diversity are limited, either by operating at inference time or by focusing on
lexical differences. We propose a novel training method named DQO based on
determinantal point processes (DPPs) to jointly optimize LLMs for quality and
semantic diversity. Our approach samples and embeds a group of responses for
each prompt, then uses the determinant of a kernel-based similarity matrix to
measure diversity as the volume spanned by the embeddings of these responses.
Experiments across instruction-following, summarization, story generation, and
reasoning tasks demonstrate that our method substantially improves semantic
diversity without sacrificing model quality.

</details>


### [185] [PLaMo 2 Technical Report](https://arxiv.org/abs/2509.04897)
*Preferred Networks,:,Kaizaburo Chubachi,Yasuhiro Fujita,Shinichi Hemmi,Yuta Hirokawa,Toshiki Kataoka,Goro Kobayashi,Kenichi Maehashi,Calvin Metzger,Hiroaki Mikami,Shogo Murai,Daisuke Nishino,Kento Nozawa,Shintarou Okada,Daisuke Okanohara,Shunta Saito,Shotaro Sano,Shuji Suzuki,Daisuke Tanaka,Avinash Ummadisingu,Hanqin Wang,Sixue Wang,Tianqi Xu*

Main category: cs.CL

TL;DR: 介绍日语大语言模型PLaMo 2，有混合架构支持32K上下文，训练高效，经微调后在日语基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 开发专注日语、能处理长上下文且高效的大语言模型，克服日语数据稀缺问题。

Method: 采用混合Samba架构，持续预训练转全注意力；利用合成语料，通过权重复用和结构化剪枝提高计算效率；用SFT和DPO微调，结合合成日语指令数据和模型合并技术；用vLLM推理和量化。

Result: 8B模型性能堪比之前100B模型，在日语基准测试中达SOTA，优于同规模开放模型。

Conclusion: PLaMo 2模型在日语处理上表现出色，其方法有效提升性能和效率。

Abstract: In this report, we introduce PLaMo 2, a series of Japanese-focused large
language models featuring a hybrid Samba-based architecture that transitions to
full attention via continual pre-training to support 32K token contexts.
Training leverages extensive synthetic corpora to overcome data scarcity, while
computational efficiency is achieved through weight reuse and structured
pruning. This efficient pruning methodology produces an 8B model that achieves
performance comparable to our previous 100B model. Post-training further
refines the models using a pipeline of supervised fine-tuning (SFT) and direct
preference optimization (DPO), enhanced by synthetic Japanese instruction data
and model merging techniques. Optimized for inference using vLLM and
quantization with minimal accuracy loss, the PLaMo 2 models achieve
state-of-the-art results on Japanese benchmarks, outperforming similarly-sized
open models in instruction-following, language fluency, and Japanese-specific
knowledge.

</details>


### [186] [Classification of kinetic-related injury in hospital triage data using NLP](https://arxiv.org/abs/2509.04969)
*Midhun Shyam,Jim Basilakis,Kieran Luken,Steven Thomas,John Crozier,Paul M. Middleton,X. Rosalind Wang*

Main category: cs.CL

TL;DR: 文章提出用大语言模型在有限计算资源下对分诊数据分类的管道，经数据集微调后成功实现分类。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言处理和机器学习技术分析分诊数据面临数据隐私、硬件不足和人工标注成本高等挑战。

Method: 先在GPU上用2k开源数据集和分类器微调预训练大语言模型，再在CPU上用1000样本的医院特定数据集进一步微调。

Result: 通过精心策划数据集和利用现有模型及开源数据，成功在有限计算资源下对分诊数据进行分类。

Conclusion: 利用大语言模型和有限计算资源可实现分诊数据分类。

Abstract: Triage notes, created at the start of a patient's hospital visit, contain a
wealth of information that can help medical staff and researchers understand
Emergency Department patient epidemiology and the degree of time-dependent
illness or injury. Unfortunately, applying modern Natural Language Processing
and Machine Learning techniques to analyse triage data faces some challenges:
Firstly, hospital data contains highly sensitive information that is subject to
privacy regulation thus need to be analysed on site; Secondly, most hospitals
and medical facilities lack the necessary hardware to fine-tune a Large
Language Model (LLM), much less training one from scratch; Lastly, to identify
the records of interest, expert inputs are needed to manually label the
datasets, which can be time-consuming and costly. We present in this paper a
pipeline that enables the classification of triage data using LLM and limited
compute resources. We first fine-tuned a pre-trained LLM with a classifier
using a small (2k) open sourced dataset on a GPU; and then further fine-tuned
the model with a hospital specific dataset of 1000 samples on a CPU. We
demonstrated that by carefully curating the datasets and leveraging existing
models and open sourced data, we can successfully classify triage data with
limited compute resources.

</details>


### [187] [Do Large Language Models Need Intent? Revisiting Response Generation Strategies for Service Assistant](https://arxiv.org/abs/2509.05006)
*Inbal Bolshinsky,Shani Kupiec,Almog Sasson,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 本文对对话式AI中显式意图识别是否为生成高质量服务回复的先决条件进行对比研究，得出的发现挑战了传统假设并提供设计指南。


<details>
  <summary>Details</summary>
Motivation: 解决对话式AI中生成准确且符合上下文的服务回复的关键挑战，探讨显式意图识别是否为生成高质量服务回复的先决条件。

Method: 利用两个公开服务交互数据集，对包括微调T5变体在内的多个先进语言模型，在意图优先回复生成和直接回复生成两种范式下进行基准测试，采用语言质量和任务成功率作为评估指标。

Result: 评估结果揭示了显式意图建模的必要性或冗余性的惊人见解。

Conclusion: 研究结果挑战了对话式AI管道中的传统假设，为设计更高效有效的回复生成系统提供了可行的指南。

Abstract: In the era of conversational AI, generating accurate and contextually
appropriate service responses remains a critical challenge. A central question
remains: Is explicit intent recognition a prerequisite for generating
high-quality service responses, or can models bypass this step and produce
effective replies directly? This paper conducts a rigorous comparative study to
address this fundamental design dilemma. Leveraging two publicly available
service interaction datasets, we benchmark several state-of-the-art language
models, including a fine-tuned T5 variant, across both paradigms: Intent-First
Response Generation and Direct Response Generation. Evaluation metrics
encompass both linguistic quality and task success rates, revealing surprising
insights into the necessity or redundancy of explicit intent modelling. Our
findings challenge conventional assumptions in conversational AI pipelines,
offering actionable guidelines for designing more efficient and effective
response generation systems.

</details>


### [188] [ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions](https://arxiv.org/abs/2509.05066)
*Matteo Bortoletto,Constantin Ruhdorfer,Andreas Bulling*

Main category: cs.CL

TL;DR: 现有心智理论（ToM）基准有局限，提出新基准ToM - SSI测试ToM能力，评估显示当前模型表现受限。


<details>
  <summary>Details</summary>
Motivation: 现有ToM基准仅依赖Sally - Anne测试变体，视角有限且忽视人类社交复杂性，需新基准。

Method: 提出多模态、含最多四个智能体群体互动的ToM - SSI基准，研究混合合作 - 阻碍场景和并行推理多智能体心理状态。

Result: 当前模型在新任务中表现严重受限。

Conclusion: 指出当前模型在新任务上有关键差距，为未来研究指明方向。

Abstract: Most existing Theory of Mind (ToM) benchmarks for foundation models rely on
variations of the Sally-Anne test, offering only a very limited perspective on
ToM and neglecting the complexity of human social interactions. To address this
gap, we propose ToM-SSI: a new benchmark specifically designed to test ToM
capabilities in environments rich with social interactions and spatial
dynamics. While current ToM benchmarks are limited to text-only or dyadic
interactions, ToM-SSI is multimodal and includes group interactions of up to
four agents that communicate and move in situated environments. This unique
design allows us to study, for the first time, mixed cooperative-obstructive
settings and reasoning about multiple agents' mental state in parallel, thus
capturing a wider range of social cognition than existing benchmarks. Our
evaluations reveal that the current models' performance is still severely
limited, especially in these new tasks, highlighting critical gaps for future
research.

</details>


### [189] [ICR: Iterative Clarification and Rewriting for Conversational Search](https://arxiv.org/abs/2509.05100)
*Zhiyu Cao,Peifeng Li,Qiaoming Zhu*

Main category: cs.CL

TL;DR: 提出基于澄清问题的ICR迭代重写框架，在两数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决端到端对话查询重写中多模糊表达难以同时识别和重写的问题。

Method: 提出ICR框架，模型在生成澄清问题和重写查询间交替。

Result: ICR在澄清 - 重写迭代过程中持续提升检索性能。

Conclusion: ICR在两个流行数据集上取得了最优性能。

Abstract: Most previous work on Conversational Query Rewriting employs an end-to-end
rewriting paradigm. However, this approach is hindered by the issue of multiple
fuzzy expressions within the query, which complicates the simultaneous
identification and rewriting of multiple positions. To address this issue, we
propose a novel framework ICR (Iterative Clarification and Rewriting), an
iterative rewriting scheme that pivots on clarification questions. Within this
framework, the model alternates between generating clarification questions and
rewritten queries. The experimental results show that our ICR can continuously
improve retrieval performance in the clarification-rewriting iterative process,
thereby achieving state-of-the-art performance on two popular datasets.

</details>


### [190] [BEDTime: A Unified Benchmark for Automatically Describing Time Series](https://arxiv.org/abs/2509.05215)
*Medhasweta Sen,Zachary Gottesman,Jiaxing Qiu,C. Bayan Bruss,Nam Nguyen,Tom Hartvigsen*

Main category: cs.CL

TL;DR: 针对时间序列分析基础模型评估存在的问题，本文提出3个任务并统一4个数据集进行评估，发现语言模型表现不佳，VLM较成功，多模态模型有提升空间且各方法鲁棒性差。


<details>
  <summary>Details</summary>
Motivation: 现有评估数据集和方式存在问题，限制模型直接比较和对模型优势的理解，需改进评估方法。

Method: 形式化并评估3个用自然语言描述时间序列的任务，统一4个数据集进行模型对比实验。

Result: 语言模型表现不佳，VLM较成功，多模态模型优于LLM但有提升空间，各方法鲁棒性差。

Conclusion: 本文基准为时间序列推理系统的必要任务提供了标准化评估。

Abstract: Many recent studies have proposed general-purpose foundation models designed
for a variety of time series analysis tasks. While several established datasets
already exist for evaluating these models, previous works frequently introduce
their models in conjunction with new datasets, limiting opportunities for
direct, independent comparisons and obscuring insights into the relative
strengths of different methods. Additionally, prior evaluations often cover
numerous tasks simultaneously, assessing a broad range of model abilities
without clearly pinpointing which capabilities contribute to overall
performance. To address these gaps, we formalize and evaluate 3 tasks that test
a model's ability to describe time series using generic natural language: (1)
recognition (True/False question-answering), (2) differentiation (multiple
choice question-answering), and (3) generation (open-ended natural language
description). We then unify 4 recent datasets to enable head-to-head model
comparisons on each task. Experimentally, in evaluating 13 state-of-the-art
language, vision--language, and time series--language models, we find that (1)
popular language-only methods largely underperform, indicating a need for time
series-specific architectures, (2) VLMs are quite successful, as expected,
identifying the value of vision models for these tasks and (3) pretrained
multimodal time series--language models successfully outperform LLMs, but still
have significant room for improvement. We also find that all approaches exhibit
clear fragility in a range of robustness tests. Overall, our benchmark provides
a standardized evaluation on a task necessary for time series reasoning
systems.

</details>


### [191] [HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models](https://arxiv.org/abs/2509.05218)
*Chang Dai,Hongyu Shan,Mingyang Song,Di Liang*

Main category: cs.CL

TL;DR: 提出双曲旋转位置编码 (HoPE) 解决现有位置编码问题，实验显示其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有绝对位置编码、相对位置编码（如 Alibi）和旋转位置编码 (RoPE) 存在外推到长序列困难、长上下文性能下降、难以稳定建模长距离依赖等局限。

Method: 受双曲几何中的洛伦兹变换启发，提出 HoPE，利用双曲函数对 token 表示进行洛伦兹旋转。

Result: 广泛实验结果表明，HoPE 在多个扩展序列基准的困惑度评估中始终超过现有位置编码方法。

Conclusion: HoPE 增强了表示和泛化长距离依赖的能力。

Abstract: Positional encoding mechanisms enable Transformers to model sequential
structure and long-range dependencies in text. While absolute positional
encodings struggle with extrapolation to longer sequences due to fixed
positional representations, and relative approaches like Alibi exhibit
performance degradation on extremely long contexts, the widely-used Rotary
Positional Encoding (RoPE) introduces oscillatory attention patterns that
hinder stable long-distance dependency modelling. We address these limitations
through a geometric reformulation of positional encoding. Drawing inspiration
from Lorentz transformations in hyperbolic geometry, we propose Hyperbolic
Rotary Positional Encoding (HoPE), which leverages hyperbolic functions to
implement Lorentz rotations on token representations. Theoretical analysis
demonstrates that RoPE is a special case of our generalized formulation. HoPE
fundamentally resolves RoPE's slation issues by enforcing monotonic decay of
attention weights with increasing token distances. Extensive experimental
results, including perplexity evaluations under several extended sequence
benchmarks, show that HoPE consistently exceeds existing positional encoding
methods. These findings underscore HoPE's enhanced capacity for representing
and generalizing long-range dependencies. Data and code will be available.

</details>


### [192] [CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models](https://arxiv.org/abs/2509.05230)
*Aysenur Kocak,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 提出轻量级框架CURE，在不增加过多计算开销下，有效减少概念偏差，提升F1分数。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型易受虚假概念驱动的相关性影响，损害鲁棒性和公平性。

Method: 先通过内容提取器和反转网络提取与概念无关的表示，再用可控去偏模块进行对比学习调整剩余概念线索的影响。

Result: 在IMDB和Yelp数据集上，使用三种预训练架构，IMDB的F1分数绝对提高10分，Yelp提高2分，计算开销小。

Conclusion: 该方法为对抗概念偏差提供了灵活、无监督的蓝图，有助于构建更可靠、公平的语言理解系统。

Abstract: Pre-trained language models have achieved remarkable success across diverse
applications but remain susceptible to spurious, concept-driven correlations
that impair robustness and fairness. In this work, we introduce CURE, a novel
and lightweight framework that systematically disentangles and suppresses
conceptual shortcuts while preserving essential content information. Our method
first extracts concept-irrelevant representations via a dedicated content
extractor reinforced by a reversal network, ensuring minimal loss of
task-relevant information. A subsequent controllable debiasing module employs
contrastive learning to finely adjust the influence of residual conceptual
cues, enabling the model to either diminish harmful biases or harness
beneficial correlations as appropriate for the target task. Evaluated on the
IMDB and Yelp datasets using three pre-trained architectures, CURE achieves an
absolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,
while introducing minimal computational overhead. Our approach establishes a
flexible, unsupervised blueprint for combating conceptual biases, paving the
way for more reliable and fair language understanding systems.

</details>


### [193] [Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining](https://arxiv.org/abs/2509.05291)
*Deniz Bayazit,Aaron Mueller,Antoine Bosselut*

Main category: cs.CL

TL;DR: 本文使用稀疏跨编码器和新指标RelIE追踪大语言模型预训练中语言特征的演变，方法架构无关且可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法无法揭示模型如何获得概念和能力，需在概念层面更好理解模型训练。

Method: 使用稀疏跨编码器发现和对齐模型检查点的特征，训练跨编码器并引入RelIE指标追踪特征因果重要性。

Result: 跨编码器能检测预训练中特征的出现、维持和中断。

Conclusion: 该方法架构无关且可扩展，为预训练表征学习的细粒度分析提供了有前景的途径。

Abstract: Large language models (LLMs) learn non-trivial abstractions during
pretraining, like detecting irregular plural noun subjects. However, it is not
well understood when and how specific linguistic abilities emerge as
traditional evaluation methods such as benchmarking fail to reveal how models
acquire concepts and capabilities. To bridge this gap and better understand
model training at the concept level, we use sparse crosscoders to discover and
align features across model checkpoints. Using this approach, we track the
evolution of linguistic features during pretraining. We train crosscoders
between open-sourced checkpoint triplets with significant performance and
representation shifts, and introduce a novel metric, Relative Indirect Effects
(RelIE), to trace training stages at which individual features become causally
important for task performance. We show that crosscoders can detect feature
emergence, maintenance, and discontinuation during pretraining. Our approach is
architecture-agnostic and scalable, offering a promising path toward more
interpretable and fine-grained analysis of representation learning throughout
pretraining.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [194] [Improved Bounds for Twin-Width Parameter Variants with Algorithmic Applications to Counting Graph Colorings](https://arxiv.org/abs/2509.05122)
*Ambroise Baril,Miguel Couceiro,Victor Lagerkvist*

Main category: cs.CC

TL;DR: 研究H - Coloring问题复杂度，给出组件孪生宽度和团宽度间线性界，证明相关参数二次界，提出两种#H - Coloring算法且比之前更快。


<details>
  <summary>Details</summary>
Motivation: 研究H - Coloring问题关于团宽度和组件孪生宽度的复杂度，并找到更好的参数间关系及算法。

Method: 给出组件孪生宽度和团宽度间线性界的构造性证明，提出基于输入图G组件孪生宽度收缩序列和基于模板图H收缩序列的两种算法。

Result: 得到组件孪生宽度和团宽度的线性界及相关参数二次界，算法比之前基于团宽度的#H - Coloring算法快。

Conclusion: 新的参数界和算法在图着色计算方面有优势，为相关问题研究提供新视角。

Abstract: The $H$-Coloring problem is a well-known generalization of the classical
NP-complete problem $k$-Coloring where the task is to determine whether an
input graph admits a homomorphism to the template graph $H$. This problem has
been the subject of intense theoretical research and in this article we study
the complexity of $H$-Coloring with respect to the parameters clique-width and
the more recent component twin-width, which describe desirable computational
properties of graphs. We give two surprising linear bounds between these
parameters, thus improving the previously known exponential and double
exponential bounds. Our constructive proof naturally extends to related
parameters and as a showcase we prove that total twin-width and linear
clique-width can be related via a tight quadratic bound. These bounds naturally
lead to algorithmic applications. The linear bounds between component
twin-width and clique-width entail natural approximations of component
twin-width, by making use of the results known for clique-width. As for
computational aspects of graph coloring, we target the richer problem of
counting the number of homomorphisms to $H$ (#$H$-Coloring). The first
algorithm that we propose uses a contraction sequence of the input graph $G$
parameterized by the component twin-width of $G$. This leads to a positive FPT
result for the counting version. The second uses a contraction sequence of the
template graph $H$ and here we instead measure the complexity with respect to
the number of vertices in the input graph. Using our linear bounds we show that
our algorithms are always at least as fast as the previously best #$H$-Coloring
algorithms (based on clique-width) and for several interesting classes of
graphs (e.g., cographs, cycles of length $\ge 7$, or distance-hereditary
graphs) are in fact strictly faster.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [195] [Multiscale Graph Neural Network for Turbulent Flow-Thermal Prediction Around a Complex-Shaped Pin-Fin](https://arxiv.org/abs/2509.04463)
*Riddhiman Raut,Evan M. Mihalko,Amrita Basak*

Main category: physics.flu-dyn

TL;DR: 开发领域响应边缘感知多尺度图神经网络预测二维通道复杂针翅几何的流热行为，精度高且大幅减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 开发能预测含任意形状复杂针翅几何的二维通道中稳定湍流流动和热行为的方法。

Method: 构建自动化框架生成训练数据集，将针翅几何参数化，把模拟结果转换为图结构，输入新开发的图神经网络进行训练。

Result: 网络预测场精度高，能捕捉边界层等特征，减少壁面时间2 - 3个数量级。

Conclusion: 新型图神经网络为复杂流动配置模拟提供了快速可靠的替代方案。

Abstract: This study presents the development of a domain-responsive edge-aware
multiscale Graph Neural Network for predicting steady, turbulent flow and
thermal behavior in a two-dimensional channel containing arbitrarily shaped
complex pin-fin geometries. The training dataset was constructed through an
automated framework that integrated geometry generation, meshing, and
flow-field solutions in ANSYS Fluent. The pin-fin geometry was parameterized
using piecewise cubic splines, producing 1,000 diverse configurations through
Latin Hypercube Sampling. Each simulation was converted into a graph structure,
where nodes carried a feature vector containing spatial coordinates, a
normalized streamwise position, one-hot boundary indicators, and a signed
distance to the nearest boundary such as wall. This graph structure served as
input to the newly developed Graph Neural Network, which was trained to predict
temperature, velocity magnitude, and pressure at each node using data from
ANSYS. The network predicted fields with outstanding accuracy, capturing
boundary layers, recirculation, and the stagnation region upstream of the
pin-fins while reducing wall time by 2-3 orders of magnitude. In conclusion,
the novel graph neural network offered a fast and reliable surrogate for
simulations in complex flow configurations.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [196] [AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design](https://arxiv.org/abs/2509.04805)
*Keqin Zhang*

Main category: eess.SP

TL;DR: 本文调研AI驱动的压缩技术，分析两条高压缩路线，提出适用于无蜂窝架构的前传压缩策略。


<details>
  <summary>Details</summary>
Motivation: 现代无线系统前传链路在带宽和延迟约束下需传输高维信号，传统压缩策略有局限性，需更好的压缩方法。

Method: 先调研AI驱动的压缩技术，分析两条代表性高压缩路线，在此基础上提出针对无蜂窝架构的前传压缩策略。

Result: 提出了一种适用于无蜂窝架构的前传压缩策略，可实现高压缩并控制性能损失，支持RB级速率自适应，适用于下一代网络的集中式协作传输。

Conclusion: 所提策略能满足现代无线系统前传链路的压缩需求，适用于下一代网络。

Abstract: Modern fronthaul links in wireless systems must transport high-dimensional
signals under stringent bandwidth and latency constraints, which makes
compression indispensable. Traditional strategies such as compressed sensing,
scalar quantization, and fixed-codec pipelines often rely on restrictive
priors, degrade sharply at high compression ratios, and are hard to tune across
channels and deployments. Recent progress in Artificial Intelligence (AI) has
brought end-to-end learned transforms, vector and hierarchical quantization,
and learned entropy models that better exploit the structure of Channel State
Information(CSI), precoding matrices, I/Q samples, and LLRs. This paper first
surveys AI-driven compression techniques and then provides a focused analysis
of two representative high-compression routes: CSI feedback with end-to-end
learning and Resource Block (RB) granularity precoding optimization combined
with compression. Building on these insights, we propose a fronthaul
compression strategy tailored to cell-free architectures. The design targets
high compression with controlled performance loss, supports RB-level rate
adaptation, and enables low-latency inference suitable for centralized
cooperative transmission in next-generation networks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [197] [Memristor-Based Neural Network Accelerators for Space Applications: Enhancing Performance with Temporal Averaging and SIRENs](https://arxiv.org/abs/2509.04506)
*Zacharia A. Rudge,Dominik Dold,Moritz Fieback,Dario Izzo,Said Hamdioui*

Main category: eess.SY

TL;DR: 本文模拟显示基于忆阻器的神经网络在太空任务中有竞争力，通过特定方法提升性能，展示了忆阻器在太空应用潜力。


<details>
  <summary>Details</summary>
Motivation: 忆阻器技术虽适合太空AI加速器，但忆阻器件有非理想性，移植神经网络面临性能退化挑战，需提升其在太空任务中的性能。

Method: 采用位切片、神经网络层的时间平均和周期性激活函数等方法。

Result: 在模拟中，使用RRAM器件将两项任务的初始结果分别从约0.07提升到0.01和从0.3提升到0.007，接近最先进水平。

Conclusion: 忆阻器在太空应用有潜力，未来技术和神经网络改进将缩小性能差距，充分发挥忆阻器优势。

Abstract: Memristors are an emerging technology that enables artificial intelligence
(AI) accelerators with high energy efficiency and radiation robustness --
properties that are vital for the deployment of AI on-board spacecraft.
However, space applications require reliable and precise computations, while
memristive devices suffer from non-idealities, such as device variability,
conductance drifts, and device faults. Thus, porting neural networks (NNs) to
memristive devices often faces the challenge of severe performance degradation.
In this work, we show in simulations that memristor-based NNs achieve
competitive performance levels on on-board tasks, such as navigation \& control
and geodesy of asteroids. Through bit-slicing, temporal averaging of NN layers,
and periodic activation functions, we improve initial results from around
$0.07$ to $0.01$ and $0.3$ to $0.007$ for both tasks using RRAM devices, coming
close to state-of-the-art levels ($0.003-0.005$ and $0.003$, respectively). Our
results demonstrate the potential of memristors for on-board space
applications, and we are convinced that future technology and NN improvements
will further close the performance gap to fully unlock the benefits of
memristors.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [198] [Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection](https://arxiv.org/abs/2509.04999)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.CR

TL;DR: 本文提出结合自编码器与主动学习的新方法提升高级持续威胁（APT）检测，减少标注成本，在真实不平衡数据上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法检测 APT 需大量标注数据，而现实中标注数据稀缺，因此需要新方法解决该问题。

Method: 结合自编码器进行异常检测与主动学习，选择性向标注者查询不确定样本标签，构建基于注意力对抗双自编码器的异常检测框架。

Result: 在来自 DARPA 透明计算程序的真实不平衡溯源跟踪数据上测试，主动学习时检测率显著提升，优于现有方法。

Conclusion: 所提方法能减少标注成本，提高检测准确率，可在少量数据下有效学习，降低对大量手动标注的依赖。

Abstract: Advanced Persistent Threats (APTs) present a considerable challenge to
cybersecurity due to their stealthy, long-duration nature. Traditional
supervised learning methods typically require large amounts of labeled data,
which is often scarce in real-world scenarios. This paper introduces a novel
approach that combines AutoEncoders for anomaly detection with active learning
to iteratively enhance APT detection. By selectively querying an oracle for
labels on uncertain or ambiguous samples, our method reduces labeling costs
while improving detection accuracy, enabling the model to effectively learn
with minimal data and reduce reliance on extensive manual labeling. We present
a comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based
anomaly detection framework and demonstrate how the active learning loop
progressively enhances the model's performance. The framework is evaluated on
real-world, imbalanced provenance trace data from the DARPA Transparent
Computing program, where APT-like attacks account for just 0.004\% of the data.
The datasets, which cover multiple operating systems including Android, Linux,
BSD, and Windows, are tested in two attack scenarios. The results show
substantial improvements in detection rates during active learning,
outperforming existing methods.

</details>


### [199] [On Evaluating the Poisoning Robustness of Federated Learning under Local Differential Privacy](https://arxiv.org/abs/2509.05265)
*Zijian Wang,Wei Tong,Tingxuan Han,Haoyu Chen,Tianling Zhang,Yunlong Mao,Sheng Zhong*

Main category: cs.CR

TL;DR: 提出适用于LDPFL的模型中毒攻击框架，评估攻击效果，揭示系统漏洞。


<details>
  <summary>Details</summary>
Motivation: LDPFL易受恶意参与者攻击，尤其对模型中毒攻击的鲁棒性研究不足。

Method: 提出新的攻击框架，通过反向训练嵌入约束以规避防御机制，在多种协议、数据集和网络上评估。

Result: 自适应攻击能显著降低全局模型性能。

Conclusion: LDPFL存在关键漏洞，需更强大的防御策略应对模型中毒攻击。

Abstract: Federated learning (FL) combined with local differential privacy (LDP)
enables privacy-preserving model training across decentralized data sources.
However, the decentralized data-management paradigm leaves LDPFL vulnerable to
participants with malicious intent. The robustness of LDPFL protocols,
particularly against model poisoning attacks (MPA), where adversaries inject
malicious updates to disrupt global model convergence, remains insufficiently
studied. In this paper, we propose a novel and extensible model poisoning
attack framework tailored for LDPFL settings. Our approach is driven by the
objective of maximizing the global training loss while adhering to local
privacy constraints. To counter robust aggregation mechanisms such as
Multi-Krum and trimmed mean, we develop adaptive attacks that embed carefully
crafted constraints into a reverse training process, enabling evasion of these
defenses. We evaluate our framework across three representative LDPFL
protocols, three benchmark datasets, and two types of deep neural networks.
Additionally, we investigate the influence of data heterogeneity and privacy
budgets on attack effectiveness. Experimental results demonstrate that our
adaptive attacks can significantly degrade the performance of the global model,
revealing critical vulnerabilities and highlighting the need for more robust
LDPFL defense strategies against MPA. Our code is available at
https://github.com/ZiJW/LDPFL-Attack

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [200] [REMOTE: A Unified Multimodal Relation Extraction Framework with Multilevel Optimal Transport and Mixture-of-Experts](https://arxiv.org/abs/2509.04844)
*Xinkui Lin,Yongxiu Xu,Minghao Tang,Shilong Zhang,Hongbo Xu,Hao Xu,Yubin Wang*

Main category: cs.MM

TL;DR: 提出REMOTE框架用于多模态关系提取，引入混合专家机制和多级最优传输融合模块，创建UMRE数据集，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态关系提取方法只能提取单一类型关系三元组，无法捕捉动态跨模态交互且有计算冗余，现有编码器会丢失低级信息。

Method: 提出REMOTE框架，引入混合专家机制选择最优交互特征，采用多级最优传输融合模块保留低级特征；创建UMRE数据集。

Result: REMOTE能有效提取多种类型关系三元组，在两个公共MRE数据集上多数指标达最优。

Conclusion: REMOTE框架在多模态关系提取任务中表现良好，资源已开源。

Abstract: Multimodal relation extraction (MRE) is a crucial task in the fields of
Knowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge
graph construction. However, existing methods are typically limited to
extracting a single type of relational triplet, which restricts their ability
to extract triplets beyond the specified types. Directly combining these
methods fails to capture dynamic cross-modal interactions and introduces
significant computational redundancy. Therefore, we propose a novel
\textit{unified multimodal Relation Extraction framework with Multilevel
Optimal Transport and mixture-of-Experts}, termed REMOTE, which can
simultaneously extract intra-modal and inter-modal relations between textual
entities and visual objects. To dynamically select optimal interaction features
for different types of relational triplets, we introduce mixture-of-experts
mechanism, ensuring the most relevant modality information is utilized.
Additionally, considering that the inherent property of multilayer sequential
encoding in existing encoders often leads to the loss of low-level information,
we adopt a multilevel optimal transport fusion module to preserve low-level
features while maintaining multilayer encoding, yielding more expressive
representations. Correspondingly, we also create a Unified Multimodal Relation
Extraction (UMRE) dataset to evaluate the effectiveness of our framework,
encompassing diverse cases where the head and tail entities can originate from
either text or image. Extensive experiments show that REMOTE effectively
extracts various types of relational triplets and achieves state-of-the-art
performanc on almost all metrics across two other public MRE datasets. We
release our resources at https://github.com/Nikol-coder/REMOTE.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [201] [DarkStream: real-time speech anonymization with low latency](https://arxiv.org/abs/2509.04667)
*Waris Quamer,Ricardo Gutierrez-Osuna*

Main category: eess.AS

TL;DR: 提出用于实时说话人匿名化的流式语音合成模型DarkStream，平衡低延迟、隐私保护和可懂度。


<details>
  <summary>Details</summary>
Motivation: 实现实时说话人匿名化，在严格延迟约束下改进内容编码，减少推理时间。

Method: 结合因果波形编码器、短前瞻缓冲区和基于Transformer的上下文层，直接通过神经声码器生成波形，注入GAN生成的伪说话人嵌入进行匿名化。

Result: 在懒信息攻击场景下实现近50%的说话人验证EER，WER在9%以内。

Conclusion: DarkStream为隐私保护的实时语音通信提供了实用解决方案。

Abstract: We propose DarkStream, a streaming speech synthesis model for real-time
speaker anonymization. To improve content encoding under strict latency
constraints, DarkStream combines a causal waveform encoder, a short lookahead
buffer, and transformer-based contextual layers. To further reduce inference
time, the model generates waveforms directly via a neural vocoder, thus
removing intermediate mel-spectrogram conversions. Finally, DarkStream
anonymizes speaker identity by injecting a GAN-generated pseudo-speaker
embedding into linguistic features from the content encoder. Evaluations show
our model achieves strong anonymization, yielding close to 50% speaker
verification EER (near-chance performance) on the lazy-informed attack
scenario, while maintaining acceptable linguistic intelligibility (WER within
9%). By balancing low-latency, robust privacy, and minimal intelligibility
degradation, DarkStream provides a practical solution for privacy-preserving
real-time speech communication.

</details>


### [202] [Lightweight DNN for Full-Band Speech Denoising on Mobile Devices: Exploiting Long and Short Temporal Patterns](https://arxiv.org/abs/2509.05079)
*Konstantinos Drossos,Mikko Heikkinen,Paschalis Tsiaflakis*

Main category: eess.AS

TL;DR: 本文提出一种用于全频段语音去噪的因果、低延迟且轻量级的深度神经网络方法，在移动设备上表现出色，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度神经网络的语音去噪方法很少针对资源受限平台（如移动设备）进行优化，且大多不关注全频段信号和低延迟情况。

Method: 基于改进的UNet架构，利用回看帧、卷积核的时间跨度和循环神经网络，处理STFT幅度输入，采用倒置瓶颈结构和因果实例归一化。

Result: 在现代手机上实时因子低于0.02，使用既定指标和公开数据集评估，(SI-)SDR值优于现有全频段和低延迟语音去噪方法。

Conclusion: 所提出的方法在全频段语音去噪任务中，尤其是在资源受限的移动设备上具有有效性和优越性。

Abstract: Speech denoising (SD) is an important task of many, if not all, modern signal
processing chains used in devices and for everyday-life applications. While
there are many published and powerful deep neural network (DNN)-based methods
for SD, few are optimized for resource-constrained platforms such as mobile
devices. Additionally, most DNN-based methods for SD are not focusing on
full-band (FB) signals, i.e. having 48 kHz sampling rate, and/or low latency
cases. In this paper we present a causal, low latency, and lightweight
DNN-based method for full-band SD, leveraging both short and long temporal
patterns. The method is based on a modified UNet architecture employing
look-back frames, temporal spanning of convolutional kernels, and recurrent
neural networks for exploiting short and long temporal patterns in the signal
and estimated denoising mask. The DNN operates on a causal frame-by-frame basis
taking as an input the STFT magnitude, utilizes inverted bottlenecks inspired
by MobileNet, employs causal instance normalization for channel-wise
normalization, and achieves a real-time factor below 0.02 when deployed on a
modern mobile phone. The proposed method is evaluated using established speech
denoising metrics and publicly available datasets, demonstrating its
effectiveness in achieving an (SI-)SDR value that outperforms existing FB and
low latency SD methods.

</details>


### [203] [Room-acoustic simulations as an alternative to measurements for audio-algorithm evaluation](https://arxiv.org/abs/2509.05175)
*Georg Götz,Daniel Gert Nielsen,Steinar Guðjónsson,Finnur Pind*

Main category: eess.AS

TL;DR: 探讨用房间声学模拟评估音频信号处理与机器学习算法，对比不同模拟引擎与测量结果的匹配度。


<details>
  <summary>Details</summary>
Motivation: 现有评估数据集因依赖昂贵耗时的测量，在规模和多样性上受限，需探索新评估方法。

Method: 用房间声学测量和不同模拟引擎的数据评估三种ASP/AML算法，对比测量与模拟的评估结果。

Result: 数值波基模拟对三种算法的评估结果与测量相似，几何声学模拟可靠性较差。

Conclusion: 数值波基模拟在评估ASP/AML算法上更可靠，可用于算法评估。

Abstract: Audio-signal-processing and audio-machine-learning (ASP/AML) algorithms are
ubiquitous in modern technology like smart devices, wearables, and
entertainment systems. Development of such algorithms and models typically
involves a formal evaluation to demonstrate their effectiveness and progress
beyond the state-of-the-art. Ideally, a thorough evaluation should cover many
diverse application scenarios and room-acoustic conditions. However, in
practice, evaluation datasets are often limited in size and diversity because
they rely on costly and time-consuming measurements. This paper explores how
room-acoustic simulations can be used for evaluating ASP/AML algorithms. To
this end, we evaluate three ASP/AML algorithms with room-acoustic measurements
and data from different simulation engines, and assess the match between the
evaluation results obtained from measurements and simulations. The presented
investigation compares a numerical wave-based solver with two geometrical
acoustics simulators. While numerical wave-based simulations yielded similar
evaluation results as measurements for all three evaluated ASP/AML algorithms,
geometrical acoustic simulations could not replicate the measured evaluation
results as reliably.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [204] [Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem](https://arxiv.org/abs/2509.04537)
*Ryosuke Takata,Atsushi Masumori,Takashi Ikegammi*

Main category: cs.MA

TL;DR: 研究大语言模型（LLM）智能体在空间扩展的El Farol酒吧问题中的社会动态，发现其能自发产生去酒吧的动机、集体决策，未完全解决问题但行为更像人类，揭示内外激励相互作用，表明可实现新的群体决策模型。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（LLM）智能体在空间扩展的El Farol酒吧问题中的自发社会动态，以及如何应对经典社会困境。

Method: 让LLM智能体参与空间扩展的El Farol酒吧问题，观察其行为。

Result: LLM智能体自发产生去酒吧的动机，集体改变决策，未完全解决问题但行为更像人类，揭示内外激励相互作用。

Conclusion: LLM智能体可实现之前博弈论问题设置中无法处理的新群体决策模型。

Abstract: We investigate the emergent social dynamics of Large Language Model (LLM)
agents in a spatially extended El Farol Bar problem, observing how they
autonomously navigate this classic social dilemma. As a result, the LLM agents
generated a spontaneous motivation to go to the bar and changed their decision
making by becoming a collective. We also observed that the LLM agents did not
solve the problem completely, but rather behaved more like humans. These
findings reveal a complex interplay between external incentives
(prompt-specified constraints such as the 60\% threshold) and internal
incentives (culturally-encoded social preferences derived from pre-training),
demonstrating that LLM agents naturally balance formal game-theoretic
rationality with social motivations that characterize human behavior. These
findings suggest that a new model of group decision making, which could not be
handled in the previous game-theoretic problem setting, can be realized by LLM
agents.

</details>


### [205] [LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of Dual-Loop Edge-Terminal Collaboration](https://arxiv.org/abs/2509.04993)
*Zheyan Qu,Wenbo Wang,Zitong Yu,Boquan Sun,Yang Li,Xing Zhang*

Main category: cs.MA

TL;DR: 本文提出6G网络中具有双循环终端-边缘协作的大语言模型多智能体系统框架与方法，通过案例验证效果并分析挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 6G网络中单个网络设备资源有限，阻碍大语言模型智能体高效运行，急需高效多级设备协作。

Method: 提出双循环终端 - 边缘协作框架，外循环通过任务分解和并行子任务分配增强规划能力，内循环利用专用角色子智能体循环推理、执行和重新规划子任务并结合卸载策略。

Result: 通过6G支持的城市安全治理案例研究，验证了改进的任务规划能力和任务执行效率。

Conclusion: 分析了6G网络中的开放挑战和未来方向，推动6G时代到来。

Abstract: The ubiquitous computing resources in 6G networks provide ideal environments
for the fusion of large language models (LLMs) and intelligent services through
the agent framework. With auxiliary modules and planning cores, LLM-enabled
agents can autonomously plan and take actions to deal with diverse environment
semantics and user intentions. However, the limited resources of individual
network devices significantly hinder the efficient operation of LLM-enabled
agents with complex tool calls, highlighting the urgent need for efficient
multi-level device collaborations. To this end, the framework and method of the
LLM-enabled multi-agent system with dual-loop terminal-edge collaborations are
proposed in 6G networks. Firstly, the outer loop consists of the iterative
collaborations between the global agent and multiple sub-agents deployed on
edge servers and terminals, where the planning capability is enhanced through
task decomposition and parallel sub-task distribution. Secondly, the inner loop
utilizes sub-agents with dedicated roles to circularly reason, execute, and
replan the sub-task, and the parallel tool calling generation with offloading
strategies is incorporated to improve efficiency. The improved task planning
capability and task execution efficiency are validated through the conducted
case study in 6G-supported urban safety governance. Finally, the open
challenges and future directions are thoroughly analyzed in 6G networks,
accelerating the advent of the 6G era.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [206] [Incentivising Personalised Colorectal Cancer Screening: an Adversarial Risk Analysis Approach](https://arxiv.org/abs/2509.04592)
*Daniel Corrales,David Rios Insua*

Main category: stat.AP

TL;DR: 本文提出从政策制定者角度激励结直肠癌筛查项目的框架，用对抗风险分析工具提出不确定下的最优激励方案并提供用例。


<details>
  <summary>Details</summary>
Motivation: 在公民参与项目目标不一致的情况下，从政策制定者角度激励结直肠癌筛查项目。

Method: 利用对抗风险分析工具，依托先前结直肠癌风险建模和最优筛查策略的研究。

Result: 提出不确定情况下的最优激励方案，给出基于简单财务方案的个人和基于群体的最优激励用例。

Conclusion: 未明确提及，推测提出的框架和激励方案有助于激励结直肠癌筛查项目。

Abstract: This paper presents a framework for incentivising colorectal cancer (CRC)
screening programs from the perspective of policymakers and under the
assumption that the citizens participating in the program have misaligned
objectives. To do so, it leverages tools from adversarial risk analysis to
propose an optimal incentive scheme under uncertainty. The work relies on
previous work on modeling CRC risk and optimal screening strategies and
provides use cases regarding individual and group-based optimal incentives
based on a simple financial scheme.

</details>


### [207] [An Interactive Tool for Analyzing High-Dimensional Clusterings](https://arxiv.org/abs/2509.04603)
*Justin Lin,Julia Fukuyama*

Main category: stat.AP

TL;DR: 技术进步使数据复杂度和维度增加，非线性降维方法常用但会产生虚假结构，为此开发交互式工具DRtool辅助分析降维结果。


<details>
  <summary>Details</summary>
Motivation: 现有非线性降维方法在处理高维数据时，尤其是在有噪声的情况下，会偶尔产生虚假结构，需要工具帮助分析师理解和诊断降维结果。

Method: 开发一个交互式工具，利用各种分析图从多方面视角判断降维结果的合理性，该工具以R包DRtool形式提供。

Result: 成功开发了名为DRtool的交互式工具。

Conclusion: 所开发的交互式工具能帮助分析师更好地理解和诊断维度降低的结果。

Abstract: Technological advances have spurred an increase in data complexity and
dimensionality. We are now in an era in which data sets containing thousands of
features are commonplace. To digest and analyze such high-dimensional data,
dimension reduction techniques have been developed and advanced along with
computational power. Of these techniques, nonlinear methods are most commonly
employed because of their ability to construct visually interpretable
embeddings. Unlike linear methods, these methods non-uniformly stretch and
shrink space to create a visual impression of the high-dimensional data. Since
capturing high-dimensional structures in a significantly lower number of
dimensions requires drastic manipulation of space, nonlinear dimension
reduction methods are known to occasionally produce false structures,
especially in noisy settings. In an effort to deal with this phenomenon, we
developed an interactive tool that enables analysts to better understand and
diagnose their dimension reduction results. It uses various analytical plots to
provide a multi-faceted perspective on results to determine legitimacy. The
tool is available via an R package named DRtool.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [208] [Inferring the Graph Structure of Images for Graph Neural Networks](https://arxiv.org/abs/2509.04677)
*Mayur S Gowda,John Shi,Augusto Santos,José M. F. Moura*

Main category: eess.IV

TL;DR: 本文通过寻找替代网格图和超像素方法的图表示来提升图神经网络在MNIST和Fashion - MNIST数据集图像分类任务中的准确率。


<details>
  <summary>Details</summary>
Motivation: 提升图神经网络下游任务在图像数据集上的准确率。

Method: 借鉴[5, 6]的方法，计算MNIST和Fashion - MNIST中每张图像的行相关性、列相关性和乘积图。

Result: 将不同图表示和特征作为输入到下游GNN模型中，相比传统网格图和超像素方法提高了准确率。

Conclusion: 使用替代的图表示可以有效提升图神经网络在图像分类任务中的性能。

Abstract: Image datasets such as MNIST are a key benchmark for testing Graph Neural
Network (GNN) architectures. The images are traditionally represented as a grid
graph with each node representing a pixel and edges connecting neighboring
pixels (vertically and horizontally). The graph signal is the values
(intensities) of each pixel in the image. The graphs are commonly used as input
to graph neural networks (e.g., Graph Convolutional Neural Networks (Graph
CNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify the
images. In this work, we improve the accuracy of downstream graph neural
network tasks by finding alternative graphs to the grid graph and superpixel
methods to represent the dataset images, following the approach in [5, 6]. We
find row correlation, column correlation, and product graphs for each image in
MNIST and Fashion-MNIST using correlations between the pixel values building on
the method in [5, 6]. Experiments show that using these different graph
representations and features as input into downstream GNN models improves the
accuracy over using the traditional grid graph and superpixel methods in the
literature.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [209] [Capturing an Invisible Robber using Separators](https://arxiv.org/abs/2509.05024)
*Igor Potapov,Tymofii Prokopenko,John Sylvester*

Main category: cs.DM

TL;DR: 研究零可见性警匪游戏，提出基于分离层次结构的新解法，改进复杂度并给出近似零可见性警察数更好界限。


<details>
  <summary>Details</summary>
Motivation: 经典警匪游戏中警察随时知道劫匪位置，而零可见性游戏中劫匪被抓前不可见，前人基于路径宽度分解的解法有改进空间。

Method: 基于分离层次结构提供替代解决方案。

Result: 改进了捕获时间和空间复杂度，且多数情况下不渐近增加零可见性警察数；为各类图的近似零可见性警察数提供更好界限。

Conclusion: 基于分离层次结构的方法在零可见性警匪游戏中有更好的表现。

Abstract: We study the zero-visibility cops and robbers game, where the robber is
invisible to the cops until they are caught. This differs from the classic game
where full information about the robber's location is known at any time. A
previously known solution for capturing a robber in the zero-visibility case is
based on the pathwidth decomposition. We provide an alternative solution based
on a separation hierarchy, improving capture time and space complexity without
asymptotically increasing the zero-visibility cop number in most cases. In
addition, we provide a better bound on the approximate zero-visibility cop
number for various classes of graphs, where approximate refers to the
restriction to polynomial time computable strategies.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [210] [Robust Voting Rules on the Interval Domain](https://arxiv.org/abs/2509.04874)
*Patrick Lederer*

Main category: econ.TH

TL;DR: 研究区间域上投票规则，引入并刻画位置阈值规则及区间域上中位数规则推广。


<details>
  <summary>Details</summary>
Motivation: 研究区间域上的投票规则。

Method: 引入位置阈值规则，依靠强化和鲁棒性公理进行刻画；刻画区间域上中位数规则推广。

Result: 完成位置阈值规则的刻画，以及区间域上中位数规则推广的刻画。

Conclusion: 成功对区间域上特定投票规则进行刻画。

Abstract: In this paper, we study voting rules on the interval domain, where the
alternatives are arranged according to an externally given strict total order
and voters report intervals of this order to indicate the alternatives they
support. For this setting, we introduce and characterize the class of
position-threshold rules, which compute a collective position of the voters
with respect to every alternative and choose the left-most alternative whose
collective position exceeds its threshold value. Our characterization of these
rules mainly relies on reinforcement, a well-known population consistency
condition, and robustness, a new axiom that restricts how the outcome is
allowed to change when a voter removes the left-most or right-most alternative
from his interval. Moreover, we characterize a generalization of the median
rule to the interval domain, which selects the median of the endpoints of the
voters' intervals.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [211] [Filtering with Randomised Observations: Sequential Learning of Relevant Subspace Properties and Accuracy Analysis](https://arxiv.org/abs/2509.04867)
*Nazanin Abedini,Jana de Wiljes,Svetlana Dubinkina*

Main category: math.NA

TL;DR: 研究连续集成卡尔曼滤波在不同观测条件下的信号跟踪性能，建立误差边界，提出自适应学习方案。


<details>
  <summary>Details</summary>
Motivation: 状态估计在许多应用中至关重要，现有滤波方法在不同观测条件下的性能有待研究。

Method: 研究连续集成卡尔曼滤波在固定、随机和自适应部分观测下的性能，建立误差边界，提出顺序学习方案。

Result: 建立了相对于观测算子随机性的预期信号跟踪误差的严格边界，提出了自适应确定状态子空间维度的顺序学习方案。

Conclusion: 该自适应方案不仅能控制误差，还能系统地确定与滤波相关的子空间的适当大小。

Abstract: State estimation that combines observational data with mathematical models is
central to many applications and is commonly addressed through filtering
methods, such as ensemble Kalman filters. In this article, we examine the
signal-tracking performance of a continuous ensemble Kalman filtering under
fixed, randomised, and adaptively varying partial observations. Rigorous bounds
are established for the expected signal-tracking error relative to the
randomness of the observation operator. In addition, we propose a sequential
learning scheme that adaptively determines the dimension of a state subspace
sufficient to ensure bounded filtering error, by balancing observation
complexity with estimation accuracy. Beyond error control, the adaptive scheme
provides a systematic approach to identifying the appropriate size of the
filter-relevant subspace of the underlying dynamics.

</details>


### [212] [Uncertain but Useful: Leveraging CNN Variability into Data Augmentation](https://arxiv.org/abs/2509.05238)
*Inés Gonzalez-Pepe,Vinuyan Sivakolunthu,Yohan Chatelain,Tristan Glatard*

Main category: math.NA

TL;DR: 研究深度学习模型在神经影像训练时的数值稳定性，发现训练时的变异性不仅是可重复性问题，还可用于提升性能和开拓新应用。


<details>
  <summary>Details</summary>
Motivation: 深度学习在神经影像领域虽有进展，但模型训练时的数值稳定性研究不足。

Method: 使用基于CNN的全脑分割管道FastSurfer，通过浮点扰动和随机种子引入受控扰动。

Result: FastSurfer比传统管道变异性高；扰动生成的集成与未扰动基线性能相似；变异性产生的数值模型族可用于下游应用。

Conclusion: 训练时的变异性既是可重复性问题，也是可利用的资源，能提升鲁棒性和开拓神经影像新应用。

Abstract: Deep learning (DL) is rapidly advancing neuroimaging by achieving
state-of-the-art performance with reduced computation times. Yet the numerical
stability of DL models -- particularly during training -- remains
underexplored. While inference with DL is relatively stable, training
introduces additional variability primarily through iterative stochastic
optimization. We investigate this training-time variability using FastSurfer, a
CNN-based whole-brain segmentation pipeline. Controlled perturbations are
introduced via floating point perturbations and random seeds. We find that: (i)
FastSurfer exhibits higher variability compared to that of a traditional
neuroimaging pipeline, suggesting that DL inherits and is particularly
susceptible to sources of instability present in its predecessors; (ii)
ensembles generated with perturbations achieve performance similar to an
unperturbed baseline; and (iii) variability effectively produces ensembles of
numerical model families that can be repurposed for downstream applications. As
a proof of concept, we demonstrate that numerical ensembles can be used as a
data augmentation strategy for brain age regression. These findings position
training-time variability not only as a reproducibility concern but also as a
resource that can be harnessed to improve robustness and enable new
applications in neuroimaging.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [213] [Vertex-ordering and arc-partitioning problems](https://arxiv.org/abs/2509.05245)
*Nóra A. Borsik,Péter Madarasi*

Main category: math.CO

TL;DR: 研究无环有向图顶点排序问题，聚焦存在条件与计算复杂度，分析特殊情况、加权变体及与弧划分问题联系，给出复杂度情况。


<details>
  <summary>Details</summary>
Motivation: 研究无环有向图中受左向弧约束的顶点排序问题的存在条件和计算复杂度。

Method: 对顶点排序问题的特殊情况、加权变体进行分析，探讨与弧划分问题的联系。

Result: 判定左向弧能否形成入分支可多项式时间求解并给出充要条件；入树形图类似问题是NP完全的；明确不同图族排序问题和弧划分问题的关系。

Conclusion: 得到综合复杂度情况，统一多样特例和变体，明确有序有向图算法边界并与其他主题建立联系。

Abstract: We study vertex-ordering problems in loop-free digraphs subject to
constraints on the left-going arcs, focusing on existence conditions and
computational complexity. As an intriguing special case, we explore
vertex-specific lower and upper bounds on the left-outdegrees and
right-indegrees. We show, for example, that deciding whether the left-going
arcs can form an in-branching is solvable in polynomial time and provide a
necessary and sufficient condition, while the analogous problem for an
in-arborescence turns out to be NP-complete. We also consider a weighted
variant that enforces vertex-specific lower and upper bounds on the weighted
left-outdegrees, which is particularly relevant in applications. Furthermore,
we investigate the connection between ordering problems and their
arc-partitioning counterparts, where one seeks to partition the arcs into a
subgraph from a specific digraph family and an acyclic subgraph --
equivalently, one seeks to cover all directed cycles with a subgraph belonging
to a specific family. For the family of in-branchings, unions of disjoint
dipaths, and matchings, the two formulations coincide, whereas for
in-arborescences, dipaths, Hamiltonian dipaths, and perfect matchings the
formulations diverge. Our results yield a comprehensive complexity landscape,
unify diverse special cases and variants, clarify the algorithmic boundaries of
ordered digraphs, and relate them to broader topics including graph degeneracy,
acyclic orientations, influence propagation, and rank aggregation.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [214] [Alignment Games](https://arxiv.org/abs/2509.04509)
*Pedro Cesar Lopes Gerum,Thomas Lidbetter*

Main category: math.OC

TL;DR: 本文介绍对齐博弈，一种新型零和博弈，给出理论分析与均衡解，扩展了博弈理论，为分析不确定下策略提供新基础。


<details>
  <summary>Details</summary>
Motivation: 受医疗诊断、经济制裁和资源分配等操作问题启发，建模战略干预中有效性与潜在隐藏状态对齐的情况。

Method: 对博弈进行全面理论分析，根据博弈的支付结构推导封闭形式的均衡解。

Result: 成本和惩罚函数不等时，最优策略由成本 - 惩罚比率决定；成本相等时，解有丰富结构特性和阈值行为；经典模型是其特例。

Conclusion: 该框架扩展了几何和搜索博弈理论，为分析不确定下全面覆盖和精确目标之间的战略张力提供新理论基础。

Abstract: This paper introduces alignment games, a new class of zero-sum games modeling
strategic interventions where effectiveness depends on alignment with an
underlying hidden state. Motivated by operational problems in medical
diagnostics, economic sanctions, and resource allocation, this framework
features two players, a Hider and a Searcher, who choose subsets of a given
space. Payoffs are determined by their misalignment (symmetric difference),
explicitly modeling the trade-off between commission errors (unnecessary
action) and omission errors (missed targets), given by a cost function and a
penalty function, respectively.
  We provide a comprehensive theoretical analysis, deriving closed-form
equilibrium solutions that contain interesting mathematical properties based on
the game's payoff structure. When cost and penalty functions are unequal,
optimal strategies are consistently governed by cost-penalty ratios. On the
unit circle, optimal arc lengths are direct functions of this ratio, and in
discrete games, optimal choice probabilities are proportional to
element-specific ratios.
  When costs are equal, the solutions exhibit rich structural properties and
sharp threshold behaviors. On the unit interval, this manifests as a geometric
pattern of minimal covering versus maximal non-overlapping strategies. In
discrete games with cardinality constraints, play concentrates on the
highest-cost locations, with solutions changing discontinuously as parameters
cross critical values.
  Our framework extends the theory of geometric and search games and is general
enough that classical models, such as Matching Pennies, emerge as special
cases. These results provide a new theoretical foundation for analyzing the
strategic tension between comprehensive coverage and precise targeting under
uncertainty.

</details>


### [215] [Universal Representation of Generalized Convex Functions and their Gradients](https://arxiv.org/abs/2509.04477)
*Moeen Nehzati*

Main category: math.OC

TL;DR: 本文将凸函数参数化思想拓展到广义凸函数，提出满足通用逼近性质的参数化方法，实现于Python包并用于解决多物品拍卖收益最大化问题。


<details>
  <summary>Details</summary>
Motivation: 广义凸函数可将嵌套双层优化问题转化为单层问题，但在数值优化中未充分利用，需对其进行参数化。

Method: 基于凸函数参数化文献，提出广义凸函数及其梯度的凸且可能一对一的参数化方法，满足通用逼近性质，并与浅层神经网络比较。

Result: 实现了参数化方法到Python包gconvex中，用其解决多物品拍卖收益最大化问题。

Conclusion: 提出的广义凸函数参数化方法能有效解决实际优化问题。

Abstract: Solutions to a wide range of optimization problems, from optimal transport
theory to mathematical economics, often take the form of generalized convex
functions (GCFs). This characterization can be used to convert nested bilevel
optimization problems into single-level optimization problems. Despite this,
the characterization has not been fully exploited in numerical optimization.
  When the solution to an optimization problem is known to belong to a
particular class of objects, this information can be leveraged by
parameterizing that class of objects and optimizing over this parameterization.
The hallmark of a good parameterization is the Universal Approximation Property
(UAP): that is, the parameterization approximates any object in the class
arbitrarily well. For example, neural networks satisfy the UAP with respect to
the class of continuous functions.
  Building on the literature concerned with the parameterization of convex
functions, we extend these ideas to GCFs. We present a convex and potentially
one-to-one parameterization of GCFs and their gradients that satisfies the UAP.
We also compare this class to shallow neural networks and highlight their
shared characteristics.
  The ideas pursued here have been implemented in the Python package
\href{https://github.com/MoeenNehzati/gconvex}{\texttt{gconvex}}, available
online. Using it, we tackle the problem of finding the revenue-maximizing
auction for multiple goods and demonstrate how our parameterization can
effectively solve this problem.

</details>


### [216] [Provably data-driven projection method for quadratic programming](https://arxiv.org/abs/2509.04524)
*Anh Tuan Nguyen,Viet Anh Nguyen*

Main category: math.OC

TL;DR: 分析凸二次规划（QPs）数据驱动投影矩阵学习的泛化保证，提出展开活动集方法并拓展分析到其他设置。


<details>
  <summary>Details</summary>
Motivation: Sakaue和Oki提出线性规划（LPs）的数据驱动方法，本文要分析凸QPs数据驱动投影矩阵学习的泛化保证，且凸QPs最优解分析更复杂。

Method: 利用Caratheodory定理将凸QPs解定位在特殊活动集对应的可行区域，提出展开活动集方法，将最优值计算建模为有界复杂度的Goldberg - Jerrum（GJ）算法。

Result: 建立了学习保证，并将分析拓展到学习匹配最优解和输入感知设置等其他设置。

Conclusion: 可对凸QPs数据驱动投影矩阵学习进行泛化保证分析，且方法可拓展到其他设置。

Abstract: Projection methods aim to reduce the dimensionality of the optimization
instance, thereby improving the scalability of high-dimensional problems.
Recently, Sakaue and Oki proposed a data-driven approach for linear programs
(LPs), where the projection matrix is learned from observed problem instances
drawn from an application-specific distribution of problems. We analyze the
generalization guarantee for the data-driven projection matrix learning for
convex quadratic programs (QPs). Unlike in LPs, the optimal solutions of convex
QPs are not confined to the vertices of the feasible polyhedron, and this
complicates the analysis of the optimal value function. To overcome this
challenge, we demonstrate that the solutions of convex QPs can be localized
within a feasible region corresponding to a special active set, utilizing
Caratheodory's theorem. Building on such observation, we propose the unrolled
active set method, which models the computation of the optimal value as a
Goldberg-Jerrum (GJ) algorithm with bounded complexities, thereby establishing
learning guarantees. We then further extend our analysis to other settings,
including learning to match the optimal solution and input-aware setting, where
we learn a mapping from QP problem instances to projection matrices.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [217] [Economic relativity: a cut rule for perimeter valuation in equity ownership networks](https://arxiv.org/abs/2509.04520)
*Omar Di Marzio*

Main category: econ.GN

TL;DR: 介绍基于切割的估值（CBV）框架用于权益/流动网络的合并价值评估，阐述其原理、应用、处理实际问题的方法及适用范围和局限性。


<details>
  <summary>Details</summary>
Motivation: 提出统一框架解决权益/流动网络中合并价值评估问题，消除重复计算，确保可比性和可审计性。

Method: 引入基于切割定理的CBV框架，确定观察者Omega，分析边界量对合并价值的影响；建立鲁棒性边界和动态CBV - Fisher协议。

Result: CBV框架可用于多种场景，消除重复计算；鲁棒性边界和动态协议解决实际问题。

Conclusion: CBV是线性会计环境中的规范测量/合并规则，在宏观经济或非线性收益场景需与其他模型结合。

Abstract: We introduce the Cut-Based Valuation (CBV), a unified framework for
consolidated value in equity/flow networks. The central idea is that economic
value is never absolute: it is always defined relative to an observer Omega,
which fixes perimeter, measurement basis, units/FX/PPP, discounting,
informational regime, and control rules. Given Omega, the Cut Theorem shows
that the consolidated value of a perimeter P depends only on boundary
quantities across the cut P -> O, while internal reconnections are
valuation-invariant. This provides (i) sufficient statistics for valuation with
linear computational complexity, (ii) standardized reporting through the
Perimeter-of-Validity and Cut Summary, and (iii) transformation laws that
clarify how different observers relate. Applications span IFRS consolidation,
national accounts, fund-of-funds, pyramids, and clearing networks, all seen as
special cases of a general principle of economic relativity. Case studies
(market capitalization by country, keiretsu, fund-of-funds) illustrate how CBV
eliminates double counting while ensuring comparability and auditability.
  To address practical concerns, we establish robustness bounds that quantify
how errors in initial data propagate to consolidated values, and we introduce a
dynamic CBV-Fisher protocol for intertemporal comparisons, ensuring consistency
with official chain-linking practices. These additions clarify the time scale
of application, the role of averaging procedures, and the horizon of reliable
measurement. Finally, we make explicit the scope and limitations of CBV: it is
a normative measurement/consolidation rule in linear accounting environments,
while in macroeconomic closures or with nonlinear payoffs it must be coupled
with equilibrium or clearing models.

</details>


### [218] [Analysis and Study of Smart Growth](https://arxiv.org/abs/2509.04529)
*Rongyan Chen,Ci Chen,Ziyang Yan*

Main category: econ.GN

TL;DR: 本文提出SGI指数和3E评估模型评估城市精明增长程度，以中国芜湖和墨西哥科利马为例进行实证分析，结果显示芜湖潜力更大，还模拟人口增长场景并研究TOD理论应用，提出政策建议。


<details>
  <summary>Details</summary>
Motivation: 应对郊区蔓延现象，促进可持续城市发展，进一步研究精明增长原则和应用。

Method: 提出SGI指数，构建3E评估模型，用改进熵值法评估，采用趋势外推法预测，模拟人口增长场景，研究TOD理论应用。

Result: 芜湖比科利马有更大精明增长潜力；人口快速增长会减缓精明增长速度，但不一定对可持续发展总体轨迹有负面影响。

Conclusion: 可根据研究结果为城市制定精明增长发展计划，应用TOD理论并提出政策建议促进可持续城市发展。

Abstract: In the mid-1990s, the concept of smart growth emerged in the United States as
a critical response to the phenomenon of suburban sprawl. To promote
sustainable urban development, it is necessary to further investigate the
principles and applications of smart growth. In this paper, we propose a Smart
Growth Index (SGI) as a standard for measuring the degree of responsible urban
development. Based on this index, we construct a comprehensive 3E evaluation
model (covering economic prosperity, social equity, and environmental
sustainability) to systematically assess the level of smart growth. For
empirical analysis, we selected two medium-sized cities from different
continents: Wuhu County, China, and Colima, Mexico. Using an improved entropy
method, we evaluated the degree of smart growth in recent years and analyzed
the contributions of various policies to sustainable urban development. Guided
by the ten principles of smart growth, we further linked theoretical insights
to practical challenges and formulated a development plan for both cities. To
forecast long-term trends, we employed trend extrapolation based on historical
data, enabling the prediction of SGI values for 2020, 2030, and 2050. The
results indicate that Wuhu demonstrates greater potential for smart growth
compared with Colima. We also simulated a scenario in which the population of
both cities increased by 50 percent and re-evaluated the SGI. The analysis
suggests that while rapid population growth tends to slow the pace of smart
growth, it does not necessarily exert a negative impact on the overall
trajectory of sustainable development. Finally, we conducted a study on the
application of Transit-Oriented Development (TOD) theory in Wuhu County and
proposed several policy recommendations aimed at enhancing the city's
sustainable urban development.

</details>


### [219] [Ideology, institutions, and economic growth: panel evidence 1995 2022](https://arxiv.org/abs/2509.04532)
*Eduardo Koffmann Jopia,Patricia Galilea Aranda*

Main category: econ.GN

TL;DR: 研究113个国家政府左右翼对经济增长影响，发现制度核心比政治意识形态更重要，并为发展中国家提出建议。


<details>
  <summary>Details</summary>
Motivation: 探究政府左右翼属性对经济增长是否有影响。

Method: 结合行政分支经济意识形态、经济自由指数制度质量和经济表现数据，采用带国家和年份固定效应的面板模型估计，按国家分组计算标准误，并进行稳健性检验。

Result: 核心制度块与较高收入水平强相关，自由化政策块平均无显著影响，政府意识形态对增长直接影响弱，长期来看制度比政治更重要。

Conclusion: 发展中国家应优先建设制度核心，为左右翼政策促增长奠定基础。

Abstract: Does it matter whether a government is "left wing" or "right wing" for
economic growth? Using a panel of 113 countries (1995 2022), we combine: (i)
the economic ideology of the executive branch (V Dem), (ii) the disaggregated
institutional quality of the economic freedom index (Heritage), separating a
core institutional block (HN: property rights, government integrity, judicial
effectiveness) from a block of liberalization policies (HL: trade/financial
openness, regulatory efficiency, size of government), and (iii) economic
performance (GDP per capita PPP and its growth, World Bank). We estimate panel
models with fixed effects by country and year, and standard errors grouped by
country. We find that HN is strongly associated with higher income levels,
while HL, on average globally, is not significant for income level once HN is
controlled for, nor does it consistently predict short term growth. Government
ideology exhibits weak direct effects on growth, although it operates
indirectly via HL (right wing governments tend to have higher HL scores).
Robustness tests with long five-year differences and country specific trends
reinforce that, in the long run, "institutions prevail over politics." We
conclude with recommendations for developing countries: prioritize the
institutional core (rule of law, anti-corruption, effective justice) as a basis
for policies whether left-wing or right wing to bear fruit in terms of growth

</details>


### [220] [Sustainability Risks under Lotka-Volterra Dynamics](https://arxiv.org/abs/2509.04780)
*Yiren Wang,Tianhao Zhi*

Main category: econ.GN

TL;DR: 因近年极端天气，本文基于经典框架提出可持续性风险广义模型，相比World3模型有诸多优势。


<details>
  <summary>Details</summary>
Motivation: 近年极端天气带来的全球变暖影响，引发对更广泛可持续性风险研究的兴趣。

Method: 基于经典Lotka - Volterra框架提出刻画经济 - 环境 - 社会关系（EVS）的可持续性风险广义模型。

Result: 所提模型相比Meadows等人提出的World3模型，有更好的分析易处理性、能更具代表性地刻画创新带来的经济发展，且子动态可用于可持续性风险建模的诸多潜在应用。

Conclusion: 提出的广义模型在可持续性风险研究方面优于World3模型。

Abstract: The record-breaking heat in recent years, along with other extreme weather
conditions worldwide has not only warned us about the devastating effects of
global warming but also revived our interest in studying sustainability risks
on a broader scale. In this paper, we propose a generalised model of
sustainability risks characterising the economic-environmental-social nexus
(EVS) based on a classic Lotka-Volterra framework. Compared to the World3 model
proposed by Meadows et al. (1972) in their landmark study "The Limits to
Growth", our model has numerous advantages such as i) better analytical
tractability, ii) more representative characterisation of economic development
arising from innovation, and iii) can be adopted in many potential applications
of modelling sustainability risks from its sub-dynamics.

</details>


### [221] [The Paradox of Doom: Acknowledging Extinction Risk Reduces the Incentive to Prevent It](https://arxiv.org/abs/2509.04855)
*Jakub Growiec,Klaus Prettner*

Main category: econ.GN

TL;DR: 研究灭绝风险对耐心的影响，发现人类灭绝风险是贴现率关键，个体死亡风险可通过生育对冲，面临灭绝风险人们更急躁，能解释人类对灾难风险投资不足。


<details>
  <summary>Details</summary>
Motivation: 探究灭绝风险作为不耐烦来源的显著性。

Method: 构建区分人类灭绝风险和个体死亡风险，考虑不同程度代际利他主义和“自私基因”视角的框架。

Result: 人类灭绝风险是贴现率不可或缺部分，个体死亡风险可通过生育部分或完全对冲；面临灭绝风险人们更急躁。

Conclusion: 该框架能解释人类对气候变化、疫情预防、人工智能等灾难性风险持续投资不足的现象。

Abstract: We investigate the salience of extinction risk as a source of impatience. Our
framework distinguishes between human extinction risk and individual mortality
risk while allowing for various degrees of intergenerational altruism.
Additionally, we consider the evolutionarily motivated "selfish gene"
perspective. We find that the risk of human extinction is an indispensable
component of the discount rate, whereas individual mortality risk can be hedged
against - partially or fully, depending on the setup - through human
reproduction. Overall, we show that in the face of extinction risk, people
become more impatient rather than more farsighted. Thus, the greater the threat
of extinction, the less incentive there is to invest in avoiding it. Our
framework can help explain why humanity consistently underinvests in mitigation
of catastrophic risks, ranging from climate change mitigation, via pandemic
prevention, to addressing the emerging risks of transformative artificial
intelligence.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [222] [Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine Bioacoustic Monitoring](https://arxiv.org/abs/2509.04682)
*Nicholas R. Rasmussen,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.SD

TL;DR: 本文引入GetNetUPAM框架评估模型稳定性，提出ARPA - N网络用于不规则频谱图，相比基线有显著提升。


<details>
  <summary>Details</summary>
Motivation: UPAM数据的固有噪声和复杂信号依赖影响模型稳定性和泛化能力，现有方法需更稳健架构和严格评估。

Method: 引入GetNetUPAM框架对数据按站点 - 年份划分进行分层嵌套交叉验证；提出ARPA - N网络，采用自适应分辨率池化和注意力机制。

Result: ARPA - N在平均精度上比DenseNet基线提高14.4%，各指标变异性有对数级下降。

Conclusion: GetNetUPAM和ARPA - N能实现跨站点 - 年份的一致检测，推动生物声学监测的可扩展和准确发展。

Abstract: Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal
data for long-term ecological analysis, but intrinsic noise and complex signal
dependencies hinder model stability and generalization. Multilayered windowing
has improved target sound localization, yet variability from shifting ambient
noise, diverse propagation effects, and mixed biological and anthropogenic
sources demands robust architectures and rigorous evaluation. We introduce
GetNetUPAM, a hierarchical nested cross-validation framework designed to
quantify model stability under ecologically realistic variability. Data are
partitioned into distinct site-year segments, preserving recording
heterogeneity and ensuring each validation fold reflects a unique environmental
subset, reducing overfitting to localized noise and sensor artifacts. Site-year
blocking enforces evaluation against genuine environmental diversity, while
standard cross-validation on random subsets measures generalization across
UPAM's full signal distribution, a dimension absent from current benchmarks.
Using GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution
Pooling and Attention Network (ARPA-N), a neural architecture for irregular
spectrogram dimensions. Adaptive pooling with spatial attention extends the
receptive field, capturing global context without excessive parameters. Under
GetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet
baselines and a log2-scale order-of-magnitude drop in variability across all
metrics, enabling consistent detection across site-year folds and advancing
scalable, accurate bioacoustic monitoring.

</details>


### [223] [Learning and composing of classical music using restricted Boltzmann machines](https://arxiv.org/abs/2509.04899)
*Mutsumi Kobayashi,Hiroshi Watanabe*

Main category: cs.SD

TL;DR: 采用受限玻尔兹曼机（RBM）训练J. S. 巴赫音乐，发现学习后的RBM能作曲。


<details>
  <summary>Details</summary>
Motivation: 现有模仿作曲家风格的软件采用复杂机器学习模型，难以分析其对作曲家音乐特征的理解方式。

Method: 采用J. S. 巴赫的音乐对受限玻尔兹曼机（RBM）进行训练，因其结构简单，便于研究学习后的内部状态。

Result: 学习后的RBM能够作曲。

Conclusion: 未明确提及，推测用RBM训练可实现音乐创作。

Abstract: Recently, software has been developed that uses machine learning to mimic the
style of a particular composer, such as J. S. Bach. However, since such
software often adopts machine learning models with complex structures, it is
difficult to analyze how the software understands the characteristics of the
composer's music. In this study, we adopted J. S. Bach's music for training of
a restricted Boltzmann machine (RBM). Since the structure of RBMs is simple, it
allows us to investigate the internal states after learning. We found that the
learned RBM is able to compose music.

</details>


### [224] [MAIA: An Inpainting-Based Approach for Music Adversarial Attacks](https://arxiv.org/abs/2509.04980)
*Yuxuan Liu,Peihong Zhang,Rui Sang,Zhixin Li,Shengchen Li*

Main category: cs.SD

TL;DR: 提出音乐对抗修复攻击(MAIA)框架，支持白盒和黑盒攻击，评估显示攻击成功率高且感知失真小，凸显当前MIR系统漏洞。


<details>
  <summary>Details</summary>
Motivation: 音乐对抗攻击在音乐信息检索领域受关注，旨在提出新的对抗攻击框架。

Method: 先进行重要性分析确定关键音频片段，用生成修复模型结合被攻击模型输出重构片段。

Result: 在多个MIR任务上评估，白盒和黑盒场景攻击成功率高，感知失真小，主观测试音频保真度高。

Conclusion: 当前MIR系统存在漏洞，需要更健壮安全的模型。

Abstract: Music adversarial attacks have garnered significant interest in the field of
Music Information Retrieval (MIR). In this paper, we present Music Adversarial
Inpainting Attack (MAIA), a novel adversarial attack framework that supports
both white-box and black-box attack scenarios. MAIA begins with an importance
analysis to identify critical audio segments, which are then targeted for
modification. Utilizing generative inpainting models, these segments are
reconstructed with guidance from the output of the attacked model, ensuring
subtle and effective adversarial perturbations. We evaluate MAIA on multiple
MIR tasks, demonstrating high attack success rates in both white-box and
black-box settings while maintaining minimal perceptual distortion.
Additionally, subjective listening tests confirm the high audio fidelity of the
adversarial samples. Our findings highlight vulnerabilities in current MIR
systems and emphasize the need for more robust and secure models.

</details>


### [225] [Recomposer: Event-roll-guided generative audio editing](https://arxiv.org/abs/2509.05256)
*Daniel P. W. Ellis,Eduardo Fonseca,Ron J. Weiss,Kevin Wilson,Scott Wisdom,Hakan Erdogan,John R. Hershey,Aren Jansen,R. Channing Moore,Manoj Plakal*

Main category: cs.SD

TL;DR: 提出一个基于文本编辑描述和事件时间图形表示的复杂场景中编辑单个声音事件的系统，评估了编辑描述各部分重要性，证明‘重新组合’是重要实用应用。


<details>
  <summary>Details</summary>
Motivation: 复杂真实世界声音场景中个体声源时间重叠，编辑困难，利用生成模型填补缺失或损坏细节的能力来解决编辑问题。

Method: 提出基于SoundStream表示的编码器 - 解码器变压器，在合成的音频示例对上训练。

Result: 评估显示编辑描述中动作、类别、时间各部分的重要性。

Conclusion: ‘重新组合’是重要且实用的应用。

Abstract: Editing complex real-world sound scenes is difficult because individual sound
sources overlap in time. Generative models can fill-in missing or corrupted
details based on their strong prior understanding of the data domain. We
present a system for editing individual sound events within complex scenes able
to delete, insert, and enhance individual sound events based on textual edit
descriptions (e.g., ``enhance Door'') and a graphical representation of the
event timing derived from an ``event roll'' transcription. We present an
encoder-decoder transformer working on SoundStream representations, trained on
synthetic (input, desired output) audio example pairs formed by adding isolated
sound events to dense, real-world backgrounds. Evaluation reveals the
importance of each part of the edit descriptions -- action, class, timing. Our
work demonstrates ``recomposition'' is an important and practical application.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [226] [A Large-Scale Study of Floating-Point Usage in Statically Typed Languages](https://arxiv.org/abs/2509.04936)
*Andrea Gilot,Tobias Wrigstad,Eva Darulova*

Main category: cs.PL

TL;DR: 本文对公共GitHub仓库中静态类型语言的浮点运算使用情况进行大规模实证研究，发现文献中评估浮点运算自动推理技术的基准在某些方面能代表真实代码，但非全部。


<details>
  <summary>Details</summary>
Motivation: 理解真实世界的浮点代码特征，以让静态和动态分析技术及程序修复能应用于真实代码。

Method: 遵循最先进的挖掘实践，包括随机抽样和基于固有属性过滤，通过搜索源代码关键词识别浮点使用，通过解析代码识别编程结构。

Result: 支持浮点运算广泛使用的观点；发现评估浮点运算自动推理技术的基准在某些方面能代表真实代码，但非全部。

Conclusion: 期望研究和数据集能帮助未来浮点运算技术的设计和评估符合用户期望。

Abstract: Reasoning about floating-point arithmetic is notoriously hard. While static
and dynamic analysis techniques or program repair have made significant
progress, more work is still needed to make them relevant to real-world code.
On the critical path to that goal is understanding what real-world
floating-point code looks like. To close that knowledge gap, this paper
presents the first large-scale empirical study of floating-point arithmetic
usage in statically typed languages across public GitHub repositories. We
follow state-of the art mining practices including random sampling and
filtering based on only intrinsic properties to avoid bias, and identify
floating-point usage by searching for keywords in the source code, and
programming language constructs (e.g., loops) by parsing the code. Our
evaluation supports the claim often made in papers that floating-point
arithmetic is widely used. Comparing statistics such as size and usage of
certain constructs and functions, we find that benchmarks used in literature to
evaluate automated reasoning techniques for floating-point arithmetic are in
certain aspects representative of 'real-world' code, but not in all. We aim for
our study and dataset to help future techniques for floating-point arithmetic
to be designed and evaluated to match actual users' expectations.

</details>


### [227] [AI-Assisted Modeling: DSL-Driven AI Interactions](https://arxiv.org/abs/2509.05160)
*Steven Smyth,Daniel Busch,Moez Ben Haj Hmida,Edward A. Lee,Bernhard Steffen*

Main category: cs.PL

TL;DR: 通过特定领域建模技术和可视化增强AI辅助编程，开发原型展示其潜力。


<details>
  <summary>Details</summary>
Motivation: 提升AI辅助编程潜力，提高软件开发性能。

Method: 集成特定领域建模技术，提供图形可视化，支持多种输入方式和逐步细化，开发VS Code扩展原型。

Result: 开发出Lingua Franca语言的VS Code扩展原型，展示了新的特定领域建模实践潜力。

Conclusion: 该方法能促进可视化检查和形式验证，改善代码生成和验证过程。

Abstract: AI-assisted programming greatly increases software development performance.
We enhance this potential by integrating transparency through domain-specific
modeling techniques and providing instantaneous, graphical visualizations that
accurately represent the semantics of AI-generated code. This approach
facilitates visual inspection and formal verification, such as model checking.
  Formal models can be developed using programming, natural language prompts,
voice commands, and stage-wise refinement, with immediate feedback after each
transformation step. This support can be tailored to specific domains or
intended purposes, improving both code generation and subsequent validation
processes.
  To demonstrate the effectiveness of this approach, we have developed a
prototype as a Visual Studio Code extension for the Lingua Franca language.
This prototype showcases the potential for novel domain-specific modeling
practices, offering an advancement in how models are created, visualized, and
verified.

</details>


### [228] [Non-Termination Proving: 100 Million LoC and Beyond](https://arxiv.org/abs/2509.05293)
*Julien Vanegue,Jules Villard,Peter O'Hearn,Azalea Raad*

Main category: cs.PL

TL;DR: 介绍工具Pulse Infinite用证明技术在大型程序中证明非终止性，应用于超亿行代码识别超30个未知问题。


<details>
  <summary>Details</summary>
Motivation: 先前工作聚焦小基准程序，规模限制其实用性，而实际公司代码量巨大，需要能处理大规模代码的工具。

Method: Pulse Infinite采用组合和欠近似的方式工作。

Result: 将Pulse Infinite应用于超亿行C、C++和Hack编写的开源及专有软件，识别出超30个先前未知问题。

Conclusion: Pulse Infinite为检测现实世界代码库中的非终止性问题建立了新的技术水平。

Abstract: We report on our tool, Pulse Infinite, that uses proof techniques to show
non-termination (divergence) in large programs. Pulse Infinite works
compositionally and under-approximately: the former supports scale, and the
latter ensures soundness for proving divergence. Prior work focused on small
benchmarks in the tens or hundreds of lines of code (LoC), and scale limits
their practicality: a single company may have tens of millions, or even
hundreds of millions of LoC or more. We report on applying Pulse Infinite to
over a hundred million lines of open-source and proprietary software written in
C, C++, and Hack, identifying over 30 previously unknown issues, establishing a
new state of the art for detecting divergence in real-world codebases.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [229] [Dynamical Learning in Deep Asymmetric Recurrent Neural Networks](https://arxiv.org/abs/2509.05041)
*Davide Badalotti,Carlo Baldassi,Marc Mézard,Mattia Scardecchia,Riccardo Zecchina*

Main category: cond-mat.dis-nn

TL;DR: 增强稀疏兴奋耦合的非对称深度循环神经网络有指数级大的内部表征流形，提出分布式学习方案，在基准测试表现好且可多方向泛化。


<details>
  <summary>Details</summary>
Motivation: 缩小人工智能与计算神经科学之间的差距。

Method: 使用增强额外稀疏兴奋耦合的非对称深度循环神经网络，基于稳定配置的几何性质提出分布式学习方案。

Result: 该方法在标准人工智能基准测试中表现有竞争力。

Conclusion: 模型可在计算和生物等多方向泛化，有望缩小AI与计算神经科学的差距。

Abstract: We show that asymmetric deep recurrent neural networks, enhanced with
additional sparse excitatory couplings, give rise to an exponentially large,
dense accessible manifold of internal representations which can be found by
different algorithms, including simple iterative dynamics. Building on the
geometrical properties of the stable configurations, we propose a distributed
learning scheme in which input-output associations emerge naturally from the
recurrent dynamics, without any need of gradient evaluation. A critical feature
enabling the learning process is the stability of the configurations reached at
convergence, even after removal of the supervisory output signal. Extensive
simulations demonstrate that this approach performs competitively on standard
AI benchmarks. The model can be generalized in multiple directions, both
computational and biological, potentially contributing to narrowing the gap
between AI and computational neuroscience.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [230] [Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning](https://arxiv.org/abs/2509.04069)
*Chengyandan Shen,Christoffer Sloth*

Main category: cs.RO

TL;DR: 提出探索高效的含参考策略的深度强化学习（DRLR）框架用于学习机器人任务，经模拟和实际任务验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决机器人任务学习中探索效率低和策略易收敛到次优的问题。

Method: 基于IBRL算法，改进动作选择模块以减轻自举误差，用SAC代替TD3作为强化学习策略。

Result: 通过模拟学习两个机器人任务验证减轻自举误差和防止过拟合的有效性，在实际装载机上验证框架可成功部署。

Conclusion: DRLR框架在不同任务维度和演示质量下表现稳健，可成功从模拟迁移到现实工业机器人任务。

Abstract: This paper proposes an exploration-efficient Deep Reinforcement Learning with
Reference policy (DRLR) framework for learning robotics tasks that incorporates
demonstrations. The DRLR framework is developed based on an algorithm called
Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve
IBRL by modifying the action selection module. The proposed action selection
module provides a calibrated Q-value, which mitigates the bootstrapping error
that otherwise leads to inefficient exploration. Furthermore, to prevent the RL
policy from converging to a sub-optimal policy, SAC is used as the RL policy
instead of TD3. The effectiveness of our method in mitigating bootstrapping
error and preventing overfitting is empirically validated by learning two
robotics tasks: bucket loading and open drawer, which require extensive
interactions with the environment. Simulation results also demonstrate the
robustness of the DRLR framework across tasks with both low and high
state-action dimensions, and varying demonstration qualities. To evaluate the
developed framework on a real-world industrial robotics task, the bucket
loading task is deployed on a real wheel loader. The sim2real results validate
the successful deployment of the DRLR framework.

</details>


### [231] [In-Context Policy Adaptation via Cross-Domain Skill Diffusion](https://arxiv.org/abs/2509.04535)
*Minjong Yoo,Woo Kyung Kim,Honguk Woo*

Main category: cs.RO

TL;DR: 提出ICPAD框架用于长周期多任务环境，结合扩散技能学习技术，在跨域场景下实现策略快速适应，实验证明其在有限目标域数据下性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决长周期多任务环境中，在无模型更新和有限目标域数据约束下，技能强化学习策略对不同目标域的快速适应问题。

Method: 采用跨域技能扩散方案，联合学习领域无关的原型技能和领域相关的技能适配器；开发动态领域提示方案提升上下文适应性能。

Result: 在Metaworld机器人操作和CARLA自动驾驶实验中，框架在有限目标域数据条件下，对多种跨域配置实现了优越的策略适应性能。

Conclusion: 所提出的ICPAD框架能有效解决跨域场景下长周期多任务的策略适应问题，在有限数据条件下表现良好。

Abstract: In this work, we present an in-context policy adaptation (ICPAD) framework
designed for long-horizon multi-task environments, exploring diffusion-based
skill learning techniques in cross-domain settings. The framework enables rapid
adaptation of skill-based reinforcement learning policies to diverse target
domains, especially under stringent constraints on no model updates and only
limited target domain data. Specifically, the framework employs a cross-domain
skill diffusion scheme, where domain-agnostic prototype skills and a
domain-grounded skill adapter are learned jointly and effectively from an
offline dataset through cross-domain consistent diffusion processes. The
prototype skills act as primitives for common behavior representations of
long-horizon policies, serving as a lingua franca to bridge different domains.
Furthermore, to enhance the in-context adaptation performance, we develop a
dynamic domain prompting scheme that guides the diffusion-based skill adapter
toward better alignment with the target domain. Through experiments with
robotic manipulation in Metaworld and autonomous driving in CARLA, we show that
our $\oursol$ framework achieves superior policy adaptation performance under
limited target domain data conditions for various cross-domain configurations
including differences in environment dynamics, agent embodiment, and task
horizon.

</details>


### [232] [Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control](https://arxiv.org/abs/2509.04628)
*Alejandro Posadas-Nava,Andrea Scorsoglio,Luca Ghilardi,Roberto Furfaro,Richard Linares*

Main category: cs.RO

TL;DR: 提出一种用于航天器GNC的模仿学习方法ACT，用少量数据实现高性能，在交会对接任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 在航天器GNC领域，实现从有限数据中获得高性能的控制策略。

Method: 提出Action Chunking with Transformers (ACT)方法，通过仅100个专家演示（相当于6300次环境交互）学习控制策略。

Result: ACT生成的轨迹比用4000万次交互训练的元强化学习基线更平滑、更一致，在与国际空间站的交会对接任务中表现更优。

Conclusion: ACT方法在航天器GNC中能实现更高精度、更平滑控制和更高样本效率。

Abstract: We present an imitation learning approach for spacecraft guidance,
navigation, and control(GNC) that achieves high performance from limited data.
Using only 100 expert demonstrations, equivalent to 6,300 environment
interactions, our method, which implements Action Chunking with Transformers
(ACT), learns a control policy that maps visual and state observations to
thrust and torque commands. ACT generates smoother, more consistent
trajectories than a meta-reinforcement learning (meta-RL) baseline trained with
40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking
with the International Space Station (ISS). We show that our approach achieves
greater accuracy, smoother control, and greater sample efficiency.

</details>


### [233] [Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving](https://arxiv.org/abs/2509.04712)
*Zhihao Zhang,Chengyang Peng,Ekim Yurtsever,Keith A. Redmill*

Main category: cs.RO

TL;DR: 提出用示范策略引导强化学习驾驶代理，结合基于规则的车道变更控制器与SAC算法，提升驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理在样本效率和有效探索方面存在训练挑战，难以找到最优驾驶策略。

Method: 用无需高度优化或专家级的示范策略引导强化学习驾驶代理，将基于规则的车道变更控制器与SAC算法集成。

Result: 所提方法提升了驾驶性能。

Conclusion: 该方法可扩展到其他能从基于示范的引导中受益的驾驶场景。

Abstract: Automated vehicle control using reinforcement learning (RL) has attracted
significant attention due to its potential to learn driving policies through
environment interaction. However, RL agents often face training challenges in
sample efficiency and effective exploration, making it difficult to discover an
optimal driving strategy. To address these issues, we propose guiding the RL
driving agent with a demonstration policy that need not be a highly optimized
or expert-level controller. Specifically, we integrate a rule-based lane change
controller with the Soft Actor Critic (SAC) algorithm to enhance exploration
and learning efficiency. Our approach demonstrates improved driving performance
and can be extended to other driving scenarios that can similarly benefit from
demonstration-based guidance.

</details>


### [234] [A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing](https://arxiv.org/abs/2509.04853)
*Chengkai Xu,Jiaqi Liu,Yicheng Guo,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: 本文提出知识驱动的扩散策略KDP用于端到端自动驾驶，实验表明其优于现有范式，确立了带专家路由的扩散方法为可扩展且可解释的范式。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶受生成多模态动作、保持时间稳定性和跨场景泛化等问题限制，现有方法存在多模态坍塌、长时一致性差和缺乏模块化适应性等不足。

Method: 提出KDP，将生成扩散建模与稀疏专家混合路由机制相结合，扩散部分生成时间连贯和多模态动作序列，专家路由机制根据上下文激活专家。

Result: 在多个驾驶场景实验中，KDP成功率更高、碰撞风险更低、控制更平滑，消融研究和激活分析凸显了相关组件有效性和专家的可复用性。

Conclusion: 带专家路由的扩散方法是知识驱动端到端自动驾驶可扩展且可解释的范式。

Abstract: End-to-end autonomous driving remains constrained by the need to generate
multi-modal actions, maintain temporal stability, and generalize across diverse
scenarios. Existing methods often collapse multi-modality, struggle with
long-horizon consistency, or lack modular adaptability. This paper presents
KDP, a knowledge-driven diffusion policy that integrates generative diffusion
modeling with a sparse mixture-of-experts routing mechanism. The diffusion
component generates temporally coherent and multi-modal action sequences, while
the expert routing mechanism activates specialized and reusable experts
according to context, enabling modular knowledge composition. Extensive
experiments across representative driving scenarios demonstrate that KDP
achieves consistently higher success rates, reduced collision risk, and
smoother control compared to prevailing paradigms. Ablation studies highlight
the effectiveness of sparse expert activation and the Transformer backbone, and
activation analyses reveal structured specialization and cross-scenario reuse
of experts. These results establish diffusion with expert routing as a scalable
and interpretable paradigm for knowledge-driven end-to-end autonomous driving.

</details>


### [235] [DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation](https://arxiv.org/abs/2509.04970)
*Tien Pham,Xinyun Chi,Khang Nguyen,Manfred Huber,Angelo Cangelosi*

Main category: cs.RO

TL;DR: 本文提出DeGuV强化学习框架，提升泛化性和样本效率，在RL - ViGen基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体将所学技能泛化到新环境是一大挑战，数据增强会降低样本效率和训练稳定性。

Method: 利用可学习的掩码网络从深度输入生成掩码，保留关键视觉信息；结合对比学习，稳定增强下的Q值估计。

Result: 在RL - ViGen基准测试中，DeGuV在零样本从仿真到真实转移中有效，在泛化性和样本效率上优于现有方法，还提高了解释性。

Conclusion: DeGuV能有效提升强化学习泛化性和样本效率，同时增强解释性。

Abstract: Reinforcement learning (RL) agents can learn to solve complex tasks from
visual inputs, but generalizing these learned skills to new environments
remains a major challenge in RL application, especially robotics. While data
augmentation can improve generalization, it often compromises sample efficiency
and training stability. This paper introduces DeGuV, an RL framework that
enhances both generalization and sample efficiency. In specific, we leverage a
learnable masker network that produces a mask from the depth input, preserving
only critical visual information while discarding irrelevant pixels. Through
this, we ensure that our RL agents focus on essential features, improving
robustness under data augmentation. In addition, we incorporate contrastive
learning and stabilize Q-value estimation under augmentation to further enhance
sample efficiency and training stability. We evaluate our proposed method on
the RL-ViGen benchmark using the Franka Emika robot and demonstrate its
effectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV
outperforms state-of-the-art methods in both generalization and sample
efficiency while also improving interpretability by highlighting the most
relevant regions in the visual input

</details>


### [236] [Pointing-Guided Target Estimation via Transformer-Based Attention](https://arxiv.org/abs/2509.05031)
*Luca Müller,Hassan Ali,Philipp Allgeuer,Lukáš Gajdošech,Stefan Wermter*

Main category: cs.RO

TL;DR: 提出Multi - Modality Inter - TransFormer(MM - ITF)架构，用单目RGB数据预测人类指示目标，实现人机协作，还引入补丁混淆矩阵评估性能。


<details>
  <summary>Details</summary>
Motivation: 指示手势在人机交互中很重要，机器人需预测人类意图并做出合适响应。

Method: 提出MM - ITF模块化架构，利用模态间注意力将2D指示手势映射到物体位置，为每个位置分配可能性得分并确定最可能目标。

Result: 该方法能用单目RGB数据准确预测预期物体，实现直观且易操作的人机协作。

Conclusion: MM - ITF架构在预测人类指示目标方面有效，引入的补丁混淆矩阵有助于评估模型性能。

Abstract: Deictic gestures, like pointing, are a fundamental form of non-verbal
communication, enabling humans to direct attention to specific objects or
locations. This capability is essential in Human-Robot Interaction (HRI), where
robots should be able to predict human intent and anticipate appropriate
responses. In this work, we propose the Multi-Modality Inter-TransFormer
(MM-ITF), a modular architecture to predict objects in a controlled tabletop
scenario with the NICOL robot, where humans indicate targets through natural
pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing
gestures to object locations, assigns a likelihood score to each, and
identifies the most likely target. Our results demonstrate that the method can
accurately predict the intended object using monocular RGB data, thus enabling
intuitive and accessible human-robot collaboration. To evaluate the
performance, we introduce a patch confusion matrix, providing insights into the
model's predictions across candidate object locations. Code available at:
https://github.com/lucamuellercode/MMITF.

</details>


### [237] [Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers](https://arxiv.org/abs/2509.05201)
*Nariman Niknejad,Gokul S. Sankar,Bahare Kiumarsi,Hamidreza Modares*

Main category: cs.RO

TL;DR: 本文提出一种鲁棒模型预测控制框架，解决深度学习感知模块非高斯噪声问题，经仿真和实验验证效果优于传统设计。


<details>
  <summary>Details</summary>
Motivation: 准确量化感知模块不确定性对安全反馈控制至关重要，传统零均值噪声量化假设不适用于非高斯噪声。

Method: 采用基于约束 zonotopes 的集合状态估计，将鲁棒 MPC 重写为线性规划，通过 Minkowski - Lyapunov 不等式等确保闭环稳定性。

Result: 在重尾噪声条件下，感知感知 MPC 提供稳定准确控制性能，在状态估计误差界定和整体控制性能上显著优于传统设计。

Conclusion: 所提出的框架能有效解决深度学习感知模块非高斯噪声问题，提升控制性能。

Abstract: This paper presents a robust model predictive control (MPC) framework that
explicitly addresses the non-Gaussian noise inherent in deep learning-based
perception modules used for state estimation. Recognizing that accurate
uncertainty quantification of the perception module is essential for safe
feedback control, our approach departs from the conventional assumption of
zero-mean noise quantification of the perception error. Instead, it employs
set-based state estimation with constrained zonotopes to capture biased,
heavy-tailed uncertainties while maintaining bounded estimation errors. To
improve computational efficiency, the robust MPC is reformulated as a linear
program (LP), using a Minkowski-Lyapunov-based cost function with an added
slack variable to prevent degenerate solutions. Closed-loop stability is
ensured through Minkowski-Lyapunov inequalities and contractive zonotopic
invariant sets. The largest stabilizing terminal set and its corresponding
feedback gain are then derived via an ellipsoidal approximation of the
zonotopes. The proposed framework is validated through both simulations and
hardware experiments on an omnidirectional mobile robot along with a camera and
a convolutional neural network-based perception module implemented within a
ROS2 framework. The results demonstrate that the perception-aware MPC provides
stable and accurate control performance under heavy-tailed noise conditions,
significantly outperforming traditional Gaussian-noise-based designs in terms
of both state estimation error bounding and overall control performance.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [238] [Beyond Linearity and Time-homogeneity: Relational Hyper Event Models with Time-Varying Non-Linear Effects](https://arxiv.org/abs/2509.05289)
*Martina Boschi,Jürgen Lerner,Ernst C. Wit*

Main category: stat.ME

TL;DR: 论文提出更灵活的关系超事件模型，用张量积平滑处理联合时变和非线性效应，在合成和实证数据上验证，能深入分析关系超事件动态因素。


<details>
  <summary>Details</summary>
Motivation: 当前多数关系超事件模型依赖线性假设，无法处理复杂数据，需更灵活模型。

Method: 引入允许统计效应非线性且随时间变化的模型，用张量积平滑处理联合时变和非线性效应。

Result: 在合成和实证数据上验证方法，可研究科研合作和影响模式随时间的演变。

Conclusion: 该方法能深入洞察驱动关系超事件的动态因素，评估线性模型无法识别的非单调模式。

Abstract: Recent technological advances have made it easier to collect large and
complex networks of time-stamped relational events connecting two or more
entities. Relational hyper-event models (RHEMs) aim to explain the dynamics of
these events by modeling the event rate as a function of statistics based on
past history and external information.
  However, despite the complexity of the data, most current RHEM approaches
still rely on a linearity assumption to model this relationship. In this work,
we address this limitation by introducing a more flexible model that allows the
effects of statistics to vary non-linearly and over time. While time-varying
and non-linear effects have been used in relational event modeling, we take
this further by modeling joint time-varying and non-linear effects using tensor
product smooths.
  We validate our methodology on both synthetic and empirical data. In
particular, we use RHEMs to study how patterns of scientific collaboration and
impact evolve over time. Our approach provides deeper insights into the dynamic
factors driving relational hyper-events, allowing us to evaluate potential
non-monotonic patterns that cannot be identified using linear models.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [239] [Evaluating Idle Animation Believability: a User Perspective](https://arxiv.org/abs/2509.05023)
*Eneko Atxa Landa,Elena Lazkano,Igor Rodriguez,Itsaso Rodríguez-Moreno,Itziar Irigoien*

Main category: cs.HC

TL;DR: 论文指出制作逼真虚拟形象的闲置动画成本高且复杂，闲置动画数据集难获取，研究表明真实和表演的闲置动画都被认为真实，手工和录制的动画被感知不同，简化录制过程，还发布了数据集。


<details>
  <summary>Details</summary>
Motivation: 解决游戏和虚拟应用中闲置动画制作成本高、录制复杂以及数据集难获取的问题。

Method: 未提及具体方法。

Result: 真实和表演的闲置动画都被认为真实，用户无法区分；手工和录制的闲置动画被感知不同。

Conclusion: 录制闲置动画可让演员表演动作，简化录制过程，研究结论有助于未来录制闲置动画数据集，发布了3D闲置动画数据集ReActIdle。

Abstract: Animating realistic avatars requires using high quality animations for every
possible state the avatar can be in. This includes actions like walking or
running, but also subtle movements that convey emotions and personality. Idle
animations, such as standing, breathing or looking around, are crucial for
realism and believability. In games and virtual applications, these are often
handcrafted or recorded with actors, but this is costly. Furthermore, recording
realistic idle animations can be very complex, because the actor must not know
they are being recorded in order to make genuine movements. For this reasons
idle animation datasets are not widely available. Nevertheless, this paper
concludes that both acted and genuine idle animations are perceived as real,
and that users are not able to distinguish between them. It also states that
handmade and recorded idle animations are perceived differently. These two
conclusions mean that recording idle animations should be easier than it is
thought to be, meaning that actors can be specifically told to act the
movements, significantly simplifying the recording process. These conclusions
should help future efforts to record idle animation datasets. Finally, we also
publish ReActIdle, a 3 dimensional idle animation dataset containing both real
and acted idle motions.

</details>


### [240] [SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching](https://arxiv.org/abs/2509.04752)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.HC

TL;DR: 介绍了新型大语言模型健康指导系统SePA，结合个性化机器学习和检索增强生成提供指导，评估显示个性化模型更优，检索建议受青睐，还量化了响应性能权衡。


<details>
  <summary>Details</summary>
Motivation: 开发能提供自适应、基于证据指导的大语言模型健康指导系统。

Method: SePA结合个性化模型预测压力、酸痛和受伤风险，以及检索模块让反馈基于专家审核的网络内容；使用滚动原点交叉验证和组k折交叉验证评估预测模型；开展专家试点研究。

Result: 个性化模型优于通用基线；SePA基于检索的建议比非检索基线更受青睐，有实际效果；量化了响应质量和速度之间的延迟性能权衡。

Conclusion: 为下一代可信个人健康信息系统提供透明蓝图。

Abstract: This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM
health coaching system that integrates personalized machine learning and
retrieval-augmented generation to deliver adaptive, evidence-based guidance.
SePA combines: (1) Individualized models predicting daily stress, soreness, and
injury risk from wearable sensor data (28 users, 1260 data points); and (2) A
retrieval module that grounds LLM-generated feedback in expert-vetted web
content to ensure contextual relevance and reliability. Our predictive models,
evaluated with rolling-origin cross-validation and group k-fold
cross-validation show that personalized models outperform generalized
baselines. In a pilot expert study (n=4), SePA's retrieval-based advice was
preferred over a non-retrieval baseline, yielding meaningful practical effect
(Cliff's $\delta$=0.3, p=0.05). We also quantify latency performance trade-offs
between response quality and speed, offering a transparent blueprint for
next-generation, trustworthy personal health informatics systems.

</details>


### [241] [Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination](https://arxiv.org/abs/2509.05145)
*Błażej Kotowski,Nicholas Evans,Behzad Haki,Frederic Font,Sergi Jordà*

Main category: cs.HC

TL;DR: 本文通过VCE框架研究GrooveTransformer实时节奏生成系统，发现三种稳定性，探究多稳定性成因，反思VCE对DMI设计的可行性。


<details>
  <summary>Details</summary>
Motivation: 探究GrooveTransformer系统多稳定性的成因，以及VCE作为描述和分析数字乐器设计方法的可行性。

Method: 运用变分交叉检验（VCE）的后现象学框架，结合该系统在三个不同艺术场景中的部署情况进行研究。

Result: 确定系统的三种稳定性，即自主鼓伴奏生成器、Eurorack格式的节奏控制电压音序器和和声伴奏系统的节奏驱动器；找出多稳定性出现的三个关键因素。

Conclusion: 反思VCE作为数字乐器设计描述和分析方法的可行性，强调其在揭示技术与用户和环境相互作用方面的价值。

Abstract: This paper investigates GrooveTransformer, a real-time rhythm generation
system, through the postphenomenological framework of Variational
Cross-Examination (VCE). By reflecting on its deployment across three distinct
artistic contexts, we identify three stabilities: an autonomous drum
accompaniment generator, a rhythmic control voltage sequencer in Eurorack
format, and a rhythm driver for a harmonic accompaniment system. The
versatility of its applications was not an explicit goal from the outset of the
project. Thus, we ask: how did this multistability emerge? Through VCE, we
identify three key contributors to its emergence: the affordances of system
invariants, the interdisciplinary collaboration, and the situated nature of its
development. We conclude by reflecting on the viability of VCE as a descriptive
and analytical method for Digital Musical Instrument (DMI) design, emphasizing
its value in uncovering how technologies mediate, co-shape, and are co-shaped
by users and contexts.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [242] [High-Resolution Global Land Surface Temperature Retrieval via a Coupled Mechanism-Machine Learning Framework](https://arxiv.org/abs/2509.04991)
*Tian Xie,Huanfeng Shen,Menghui Jiang,Juan-Carlos Jiménez-Muñoz,José A. Sobrino,Huifang Li,Chao Zeng*

Main category: physics.ao-ph

TL;DR: 提出耦合机制模型 - 机器学习（MM - ML）框架用于陆面温度（LST）反演，性能优于传统方法，能在复杂环境可靠反演。


<details>
  <summary>Details</summary>
Motivation: 准确反演LST在异质土地覆盖和极端大气条件下具有挑战性，传统方法存在局限性，如分裂窗算法在潮湿环境有偏差，纯机器学习方法缺乏可解释性和泛化性差。

Method: 提出MM - ML框架，融合辐射传输建模与数据组件，使用MODTRAN模拟和物理约束优化。

Result: 通过29个全球站点4450个观测数据验证，MM - ML的MAE = 1.84K，RMSE = 2.55K，R平方 = 0.966，优于传统方法；极端条件下误差降低超50%，敏感性分析显示对传感器辐射最敏感。

Conclusion: MM - ML框架结合物理可解释性和非线性建模能力，能在复杂环境可靠反演LST，支持气候监测和生态系统研究。

Abstract: Land surface temperature (LST) is vital for land-atmosphere interactions and
climate processes. Accurate LST retrieval remains challenging under
heterogeneous land cover and extreme atmospheric conditions. Traditional split
window (SW) algorithms show biases in humid environments; purely machine
learning (ML) methods lack interpretability and generalize poorly with limited
data. We propose a coupled mechanism model-ML (MM-ML) framework integrating
physical constraints with data-driven learning for robust LST retrieval. Our
approach fuses radiative transfer modeling with data components, uses MODTRAN
simulations with global atmospheric profiles, and employs physics-constrained
optimization. Validation against 4,450 observations from 29 global sites shows
MM-ML achieves MAE=1.84K, RMSE=2.55K, and R-squared=0.966, outperforming
conventional methods. Under extreme conditions, MM-ML reduces errors by over
50%. Sensitivity analysis indicates LST estimates are most sensitive to sensor
radiance, then water vapor, and less to emissivity, with MM-ML showing superior
stability. These results demonstrate the effectiveness of our coupled modeling
strategy for retrieving geophysical parameters. The MM-ML framework combines
physical interpretability with nonlinear modeling capacity, enabling reliable
LST retrieval in complex environments and supporting climate monitoring and
ecosystem studies.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [243] [RobQFL: Robust Quantum Federated Learning in Adversarial Environment](https://arxiv.org/abs/2509.04914)
*Walid El Maouaki,Nouhaila Innan,Alberto Marchisio,Taoufik Said,Muhammad Shafique,Mohamed Bennai*

Main category: quant-ph

TL;DR: 研究QFL对抗噪声的鲁棒性，提出RobQFL方法，通过模拟实验得出不同条件下提升鲁棒性的策略及数据异质性影响。


<details>
  <summary>Details</summary>
Motivation: 量子联邦学习（QFL）结合隐私保护联邦和量子计算优势，但对抗噪声的鲁棒性未知，需进行研究。

Method: 提出鲁棒量子联邦学习（RobQFL），将对抗训练嵌入联邦循环，设置可调参数并提炼两个指标。

Result: 在模拟实验中，部分客户端对抗训练可提升准确率，不同覆盖率有不同最优策略，标签排序非IID分割会降低鲁棒性。

Conclusion: 数据异质性是QFL鲁棒性的主要风险，不同客户端覆盖率和扰动调度有不同提升鲁棒性的策略。

Abstract: Quantum Federated Learning (QFL) merges privacy-preserving federation with
quantum computing gains, yet its resilience to adversarial noise is unknown. We
first show that QFL is as fragile as centralized quantum learning. We propose
Robust Quantum Federated Learning (RobQFL), embedding adversarial training
directly into the federated loop. RobQFL exposes tunable axes: client coverage
$\gamma$ (0-100\%), perturbation scheduling (fixed-$\varepsilon$ vs
$\varepsilon$-mixes), and optimization (fine-tune vs scratch), and distils the
resulting $\gamma \times \varepsilon$ surface into two metrics:
Accuracy-Robustness Area and Robustness Volume. On 15-client simulations with
MNIST and Fashion-MNIST, IID and Non-IID conditions, training only 20-50\%
clients adversarially boosts $\varepsilon \leq 0.1$ accuracy $\sim$15 pp at $<
2$ pp clean-accuracy cost; fine-tuning adds 3-5 pp. With $\geq$75\% coverage, a
moderate $\varepsilon$-mix is optimal, while high-$\varepsilon$ schedules help
only at 100\% coverage. Label-sorted non-IID splits halve robustness,
underscoring data heterogeneity as a dominant risk.

</details>


### [244] [Artificial intelligence for representing and characterizing quantum systems](https://arxiv.org/abs/2509.04923)
*Yuxuan Du,Yan Zhu,Yuan-Hang Zhang,Min-Hsiu Hsieh,Patrick Rebentrost,Weibo Gao,Ya-Dong Wu,Jens Eisert,Giulio Chiribella,Dacheng Tao,Barry C. Sanders*

Main category: quant-ph

TL;DR: 文章指出大规模量子系统表征具挑战，AI是解决办法，介绍AI融入量子系统表征的三种范式，及其对量子系统表征两核心任务的贡献，还讨论挑战、问题与前景。


<details>
  <summary>Details</summary>
Motivation: 大规模量子系统表征因希尔伯特空间指数增长面临挑战，AI在高维模式识别和函数逼近方面的能力使其可用于解决该挑战。

Method: 将AI融入量子系统表征按先验知识和学习架构分为机器学习、深度学习和语言模型三种范式，并探讨其对量子特性预测和量子态替代构建两个核心任务的贡献。

Result: 阐述了每种AI范式对量子系统表征核心任务的作用，这些任务支撑着从量子认证到理解强关联物质相等多样应用。

Conclusion: 讨论了AI与量子科学结合的关键挑战、开放问题和未来前景。

Abstract: Efficient characterization of large-scale quantum systems, especially those
produced by quantum analog simulators and megaquop quantum computers, poses a
central challenge in quantum science due to the exponential scaling of the
Hilbert space with respect to system size. Recent advances in artificial
intelligence (AI), with its aptitude for high-dimensional pattern recognition
and function approximation, have emerged as a powerful tool to address this
challenge. A growing body of research has leveraged AI to represent and
characterize scalable quantum systems, spanning from theoretical foundations to
experimental realizations. Depending on how prior knowledge and learning
architectures are incorporated, the integration of AI into quantum system
characterization can be categorized into three synergistic paradigms: machine
learning, and, in particular, deep learning and language models. This review
discusses how each of these AI paradigms contributes to two core tasks in
quantum systems characterization: quantum property prediction and the
construction of surrogates for quantum states. These tasks underlie diverse
applications, from quantum certification and benchmarking to the enhancement of
quantum algorithms and the understanding of strongly correlated phases of
matter. Key challenges and open questions are also discussed, together with
future prospects at the interface of AI and quantum science.

</details>


### [245] [Exploring an implementation of quantum learning pipeline for support vector machines](https://arxiv.org/abs/2509.04983)
*Mario Bifulco,Luca Roversi*

Main category: quant-ph

TL;DR: 提出基于门的量子核方法与量子退火优化结合的全量子支持向量机学习方法，实验证明其可行性和潜力。


<details>
  <summary>Details</summary>
Motivation: 探索全量子方法用于支持向量机学习，发挥量子计算在机器学习中的潜力。

Method: 结合基于门的量子核方法与量子退火优化，用不同特征映射和量子比特配置构建量子核，通过KTA评估；将SVM对偶问题转化为QUBO问题用量子退火器求解。

Result: 实验表明核高度对齐和合适正则化参数带来有竞争力的性能，最佳模型F1分数达90%。

Conclusion: 端到端量子学习管道可行，混合量子架构在QHPC中有潜力。

Abstract: This work presents a fully quantum approach to support vector machine (SVM)
learning by integrating gate-based quantum kernel methods with quantum
annealing-based optimization. We explore the construction of quantum kernels
using various feature maps and qubit configurations, evaluating their
suitability through Kernel-Target Alignment (KTA). The SVM dual problem is
reformulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem,
enabling its solution via quantum annealers. Our experiments demonstrate that a
high degree of alignment in the kernel and an appropriate regularization
parameter lead to competitive performance, with the best model achieving an
F1-score of 90%. These results highlight the feasibility of an end-to-end
quantum learning pipeline and the potential of hybrid quantum architectures in
quantum high-performance computing (QHPC) contexts.

</details>


### [246] [QCA-MolGAN: Quantum Circuit Associative Molecular GAN with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.05051)
*Aaron Mark Thomas,Yu-Cheng Chen,Hubert Okadome Valencia,Sharu Theresa Jose,Ronin Wu*

Main category: quant-ph

TL;DR: 提出QCA - MolGAN生成类药分子，结合量子电路Born机和多智能体强化学习网络，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 在药物发现中，从大量分子结构化学空间设计有期望目标属性的新药物分子是核心挑战，现有生成模型提供了有前景的解决方案。

Method: 提出QCA - MolGAN，用QCBM作为可学习先验分布进行关联训练，集成多智能体强化学习网络引导分子生成。

Result: 该方法增强了生成分子的属性对齐，多智能体强化学习智能体有效平衡了化学属性。

Conclusion: QCA - MolGAN在生成类药分子并平衡化学属性方面是有效的。

Abstract: Navigating the vast chemical space of molecular structures to design novel
drug molecules with desired target properties remains a central challenge in
drug discovery. Recent advances in generative models offer promising solutions.
This work presents a novel quantum circuit Born machine (QCBM)-enabled
Generative Adversarial Network (GAN), called QCA-MolGAN, for generating
drug-like molecules. The QCBM serves as a learnable prior distribution, which
is associatively trained to define a latent space aligning with high-level
features captured by the GANs discriminator. Additionally, we integrate a novel
multi-agent reinforcement learning network to guide molecular generation with
desired targeted properties, optimising key metrics such as quantitative
estimate of drug-likeness (QED), octanol-water partition coefficient (LogP) and
synthetic accessibility (SA) scores in conjunction with one another.
Experimental results demonstrate that our approach enhances the property
alignment of generated molecules with the multi-agent reinforcement learning
agents effectively balancing chemical properties.

</details>
