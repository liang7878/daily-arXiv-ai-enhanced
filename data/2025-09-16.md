<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 50]
- [cs.CE](#cs.CE) [Total: 9]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 17]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 153]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.SE](#cs.SE) [Total: 27]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 5]
- [q-fin.TR](#q-fin.TR) [Total: 3]
- [stat.ML](#stat.ML) [Total: 14]
- [stat.CO](#stat.CO) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]
- [math.HO](#math.HO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 35]
- [hep-th](#hep-th) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.MA](#cs.MA) [Total: 4]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.PL](#cs.PL) [Total: 3]
- [math.CO](#math.CO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 11]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 34]
- [cs.LO](#cs.LO) [Total: 2]
- [math.PR](#math.PR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 17]
- [cs.CY](#cs.CY) [Total: 11]
- [eess.SY](#eess.SY) [Total: 6]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [stat.AP](#stat.AP) [Total: 3]
- [cs.MS](#cs.MS) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.SD](#cs.SD) [Total: 9]
- [quant-ph](#quant-ph) [Total: 7]
- [math.OC](#math.OC) [Total: 7]
- [cs.NI](#cs.NI) [Total: 10]
- [math.NA](#math.NA) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [econ.GN](#econ.GN) [Total: 7]
- [eess.SP](#eess.SP) [Total: 6]
- [cs.ET](#cs.ET) [Total: 1]
- [nlin.AO](#nlin.AO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 21]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Situation Model of the Transport, Transport Emissions and Meteorological Conditions](https://arxiv.org/abs/2509.10541)
*V. Benes,M. Svitek,A. Michalikova,M. Melicherik*

Main category: cs.AI

TL;DR: 本文聚焦城市交通排放与气象条件关系，用模糊推理系统建模预测排放变化，助城市规划者和政策制定者环保规划交通。


<details>
  <summary>Details</summary>
Motivation: 解决城市空气污染及减排问题，为城市规划者和政策制定者提供环保规划和管理城市交通的方法。

Method: 采用模糊推理系统（FIS），基于捷克布拉格的交通、气象和排放数据，开发根据不同条件预测排放变化的模型。

Result: 开发出了基于相关数据的预测排放变化的模型。

Conclusion: 该模型可让城市规划者和政策制定者更有效地规划和管理城市交通，同时兼顾环境保护。

Abstract: Air pollution in cities and the possibilities of reducing this pollution
represents one of the most important factors that today's society has to deal
with. This paper focuses on a systemic approach to traffic emissions with their
relation to meteorological conditions, analyzing the effect of weather on the
quantity and dispersion of traffic emissions in a city. Using fuzzy inference
systems (FIS) the model for prediction of changes in emissions depending on
various conditions is developed. The proposed model is based on traffic,
meteorology and emission data measured in Prague, Czech Republic. The main
objective of the work is to provide insight into how urban planners and
policymakers can plan and manage urban transport more effectively with
environmental protection in mind.

</details>


### [2] [ZapGPT: Free-form Language Prompting for Simulated Cellular Control](https://arxiv.org/abs/2509.10660)
*Nam H. Le,Patrick Erickson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TL;DR: 本文首次展示简单智能体集体行为可由自由形式语言提示引导，方法无需特定任务调整，且系统能泛化到未见提示，为AI - 生物学合作愿景迈出一步。


<details>
  <summary>Details</summary>
Motivation: 多数人工或生物系统缺乏有效解读和回应人类语言的机制，现有系统在处理自然语言指令时存在泛化性差等局限，需探索能否仅用自由形式自然语言引导人工或生物集体。

Method: 一个AI模型将指令性提示转化为对模拟细胞的干预，另一个AI模型对提示描述细胞动态的程度打分，前一个AI模型通过进化提升后一个模型给出的分数。

Result: 进化后的系统无需重新训练即可泛化到未见提示。

Conclusion: 该工作为AI - 生物学合作愿景提供了具体进展，语言有望取代数学目标函数、固定规则和特定领域编程来引导系统行为。

Abstract: Human language is one of the most expressive tools for conveying intent, yet
most artificial or biological systems lack mechanisms to interpret or respond
meaningfully to it. Bridging this gap could enable more natural forms of
control over complex, decentralized systems. In AI and artificial life, recent
work explores how language can specify high-level goals, but most systems still
depend on engineered rewards, task-specific supervision, or rigid command sets,
limiting generalization to novel instructions. Similar constraints apply in
synthetic biology and bioengineering, where the locus of control is often
genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be
guided by free-form natural language alone, without task-specific tuning or
carefully designed evaluation metrics. We provide one possible answer here by
showing, for the first time, that simple agents' collective behavior can be
guided by free-form language prompts: one AI model transforms an imperative
prompt into an intervention that is applied to simulated cells; a second AI
model scores how well the prompt describes the resulting cellular dynamics; and
the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness
functions or domain-specific prompt design. We show that the evolved system
generalizes to unseen prompts without retraining. By treating natural language
as a control layer, the system suggests a future in which spoken or written
prompts could direct computational, robotic, or biological systems to desired
behaviors. This work provides a concrete step toward this vision of AI-biology
partnerships, in which language replaces mathematical objective functions,
fixed rules, and domain-specific programming.

</details>


### [3] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: 本文提出Maestro系统，让文本到图像模型能自主改进生成图像，实验证明其提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型依赖人工干预，存在可用性挑战，需手动迭代提示工程。

Method: Maestro系统包含自我批判（用多模态大语言模型代理识别图像弱点并提供编辑信号）和自我进化（用多模态大语言模型评判迭代图像）两个关键创新。

Result: 在复杂文本到图像任务的实验中，Maestro显著提升图像质量，且使用更先进多模态大语言模型组件时效果更好。

Conclusion: Maestro为文本到图像的自我改进生成提供了可靠、可解释且有效的途径。

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [4] [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
*Sajjad Abdoli,Rudi Cilibrasi,Rima Al-Shikh*

Main category: cs.AI

TL;DR: 研究分析GPT变体评估NVIDIA模型生成的视觉语言描述，发现不同‘评估个性’及偏差，指出评估能力与通用能力无关，需多元架构视角。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统评估其他AI输出增多，理解其评估行为对防止级联偏差至关重要。

Method: 分析GPT变体评估NVIDIA模型生成的描述，用Gemini 2.5 Pro做对照实验，进行跨家族语义相似度分析。

Result: GPT-4o-mini一致性高，GPT-4o擅于检测错误，GPT-5极端保守且变异性高；GPT模型有2:1负评估偏差；GPT模型评估策略相似，与Gemini差异大。

Conclusion: 评估能力不随通用能力提升，强大的AI评估需不同架构视角。

Abstract: As AI systems increasingly evaluate other AI outputs, understanding their
assessment behavior becomes crucial for preventing cascading biases. This study
analyzes vision-language descriptions generated by NVIDIA's Describe Anything
Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to
uncover distinct "evaluation personalities" the underlying assessment
strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic
consistency with minimal variance, GPT-4o excels at error detection, while
GPT-5 shows extreme conservatism with high variability. Controlled experiments
using Gemini 2.5 Pro as an independent question generator validate that these
personalities are inherent model properties rather than artifacts. Cross-family
analysis through semantic similarity of generated questions reveals significant
divergence: GPT models cluster together with high similarity while Gemini
exhibits markedly different evaluation strategies. All GPT models demonstrate a
consistent 2:1 bias favoring negative assessment over positive confirmation,
though this pattern appears family-specific rather than universal across AI
architectures. These findings suggest that evaluation competence does not scale
with general capability and that robust AI assessment requires diverse
architectural perspectives.

</details>


### [5] [AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework](https://arxiv.org/abs/2509.10762)
*Arlen Kumar,Leanid Palkhouski*

Main category: cs.AI

TL;DR: 介绍GEO - 16审计框架评估AI答案引擎引用网页质量，发现引擎引用网页质量有差异，给出预测引用的指标及实用指南。


<details>
  <summary>Details</summary>
Motivation: 评估AI答案引擎引用网页的质量，为出版商提供参考。

Method: 引入GEO - 16审计框架，用70个产品意图提示收集三个引擎的引用并审计1100个唯一URL，使用带领域聚类标准误差的逻辑模型。

Result: 引擎引用网页的GEO质量有差异，特定支柱与引用关联强，整体页面质量能强预测引用，特定操作点对应更高引用率。

Conclusion: 给出各引擎对比、垂直效应等分析结果，转化为出版商实用指南，讨论研究局限性等。

Abstract: AI answer engines increasingly mediate access to domain knowledge by
generating responses and citing web sources. We introduce GEO-16, a 16 pillar
auditing framework that converts on page quality signals into banded pillar
scores and a normalized GEO score G that ranges from 0 to 1. Using 70 product
intent prompts, we collected 1,702 citations across three engines (Brave
Summary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In
our corpus, the engines differed in the GEO quality of the pages they cited,
and pillars related to Metadata and Freshness, Semantic HTML, and Structured
Data showed the strongest associations with citation. Logistic models with
domain clustered standard errors indicate that overall page quality is a strong
predictor of citation, and simple operating points (for example, G at least
0.70 combined with at least 12 pillar hits) align with substantially higher
citation rates in our data. We report per engine contrasts, vertical effects,
threshold analysis, and diagnostics, then translate findings into a practical
playbook for publishers. The study is observational and focuses on English
language B2B SaaS pages; we discuss limitations, threats to validity, and
reproducibility considerations.

</details>


### [6] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: 研究对18种不同代理配置进行基准测试，揭示模型特定架构偏好及代理性能弱点，为未来系统设计提供依据。


<details>
  <summary>Details</summary>
Motivation: 现有对复杂多智能体系统中不同设计维度如何相互作用的实证理解有限，需填补此空白。

Method: 提供针对企业的综合基准测试，评估18种不同代理配置，考察编排策略、代理提示实现、内存架构和思维工具集成四个关键维度。

Result: 发现显著的模型特定架构偏好，挑战了通用范式；代理在企业任务上整体性能有明显弱点，最高得分模型在复杂任务上成功率仅35.3%，简单任务为70.8%。

Conclusion: 研究结果可为未来代理系统设计在架构组件和模型选择方面提供基于实证的决策参考。

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [7] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: 本文探讨如何让大语言模型（LLMs）使决策更高效，提出基于优化人机对话和函数的算法来发现决策的个人专家心智模型（EMM）。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在决策支持中存在训练数据缺失导致幻觉、现有方法不能完全解决问题、设计有效提示困难等问题，需要更好的方法让LLMs助力决策。

Method: 提出基于优化人机对话和单调布尔及k值函数的技术，EMM算法包含因子识别、因子层次结构构建、生成广义专家心智模型规范、从规范生成详细广义专家心智模型四个步骤。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [8] [From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering](https://arxiv.org/abs/2509.10837)
*Yuyin Lu,Hegang Chen,Yanghui Rao*

Main category: cs.AI

TL;DR: 本文建立二分法分析复杂查询回答方法，提出LVSA框架解决现有方法局限，理论上保证通用性，实证表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 复杂查询回答在不完整知识图谱中存在逻辑合理性和计算效率的权衡，现有基于Grounding和Skolemization的方法分别存在组合爆炸和忽略Skolem函数、损害逻辑一致性的问题。

Method: 提出Logic - constrained Vector Symbolic Architecture (LVSA)神经符号框架，统一可微的Skolemization模块和神经否定器，并采用逻辑约束驱动的优化协议。

Result: 理论上LVSA保证所有EFO₁查询的通用性；实证上优于现有基于Skolemization的方法，与基于Grounding的基线相比，推理成本降低了几个数量级。

Conclusion: LVSA框架有效解决了现有复杂查询回答方法的局限，在理论和实证上都有良好表现。

Abstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),
typically formalized as reasoning with Existential First-Order predicate logic
with one free variable (EFO$_1$), faces a fundamental trade-off between logical
soundness and computational efficiency. This work establishes the
Grounding-Skolemization dichotomy for systematically analyzing CQA methods
through the lens of formal logic. While Grounding-based methods inherently
suffer from combinatorial explosion, most Skolemization-based methods neglect
to explicitly model Skolem functions and compromise logical consistency. To
address these limitations, we propose the Logic-constrained Vector Symbolic
Architecture (LVSA), a neuro-symbolic framework that unifies a differentiable
Skolemization module and a neural negator, as well as a logical
constraint-driven optimization protocol to harmonize geometric and logical
requirements. Theoretically, LVSA guarantees universality for all EFO$_1$
queries. Empirically, it outperforms state-of-the-art Skolemization-based
methods and reduces inference costs by orders of magnitude compared to
Grounding-based baselines.

</details>


### [9] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: 本文重新评估以智能体为中心的范式，指出其局限性，区分不同系统类型，分析智能体范式挑战，建议转向系统层面框架，认为探索非智能体和系统性框架对发展通用智能至关重要。


<details>
  <summary>Details</summary>
Motivation: 批判地重新评估以智能体为中心的范式的必要性和最优性，因该范式存在概念模糊和人类中心主义偏见。

Method: 对相关文献进行系统回顾，在各种人工智能框架中解构智能体范式。

Result: 指出智能体范式存在定义和测量自主性、目标导向性等方面的挑战，“智能体”框架虽有启发但可能误导，掩盖底层计算机制。

Conclusion: 探索受复杂系统、生物学和非常规计算启发的非智能体和系统性框架，对推进通用智能发展至关重要，需新架构并重新思考对智能的理解。

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [10] [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 提出针对大语言模型的通用越狱攻击技术HaPLa，实验显示其攻击成功率高，同时指出安全调优大语言模型的挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有被恶意利用风险，需研究利用其架构和学习范式内在弱点的通用越狱攻击来强化防御。

Method: 提出HaPLa技术，包括诱导推理中间步骤的溯因框架和混淆有害内容的符号编码两种策略，且只需黑盒访问目标模型。

Result: HaPLa在GPT系列模型攻击成功率超95%，所有目标平均达70%。

Conclusion: 安全调优大语言模型且不显著降低其对良性查询响应能力仍困难。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their potential misuse for harmful purposes remains a
significant concern. To strengthen defenses against such vulnerabilities, it is
essential to investigate universal jailbreak attacks that exploit intrinsic
weaknesses in the architecture and learning paradigms of LLMs. In response, we
propose \textbf{H}armful \textbf{P}rompt \textbf{La}undering (HaPLa), a novel
and broadly applicable jailbreaking technique that requires only black-box
access to target models. HaPLa incorporates two primary strategies: 1)
\textit{abductive framing}, which instructs LLMs to infer plausible
intermediate steps toward harmful activities, rather than directly responding
to explicit harmful queries; and 2) \textit{symbolic encoding}, a lightweight
and flexible approach designed to obfuscate harmful content, given that current
LLMs remain sensitive primarily to explicit harmful keywords. Experimental
results show that HaPLa achieves over 95% attack success rate on GPT-series
models and 70% across all targets. Further analysis with diverse symbolic
encoding rules also reveals a fundamental challenge: it remains difficult to
safely tune LLMs without significantly diminishing their helpfulness in
responding to benign queries.

</details>


### [11] [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 论文提出结合公共数据的私有上下文学习算法，平衡隐私保护和模型效用，实验证明其能提升私有ICL效用且抗攻击。


<details>
  <summary>Details</summary>
Motivation: 大语言模型上下文学习（ICL）存在隐私泄露风险，差分隐私（DP）虽能保证隐私但会降低ICL效用。

Method: 将任务相关公共数据融入ICL框架，提出私有上下文学习算法。

Result: 实验表明该方法借助公共数据显著提升私有ICL的效用，且对成员推理攻击具有鲁棒性。

Conclusion: 该算法能有效平衡隐私保护和模型效用，实现经验隐私保护。

Abstract: In-context learning (ICL) in Large Language Models (LLMs) has shown
remarkable performance across various tasks without requiring fine-tuning.
However, recent studies have highlighted the risk of private data leakage
through the prompt in ICL, especially when LLMs are exposed to malicious
attacks. While differential privacy (DP) provides strong privacy guarantees, it
often significantly reduces the utility of in-context learning (ICL). To
address this challenge, we incorporate task-related public data into the ICL
framework while maintaining the DP guarantee. Based on this approach, we
propose a private in-context learning algorithm that effectively balances
privacy protection and model utility. Through experiments, we demonstrate that
our approach significantly improves the utility of private ICL with the
assistance of public data. Additionally, we show that our method is robust
against membership inference attacks, demonstrating empirical privacy
protection.

</details>


### [12] [Enhancing Computational Cognitive Architectures with LLMs: A Case Study](https://arxiv.org/abs/2509.10972)
*Ron Sun*

Main category: cs.AI

TL;DR: 文章指出计算认知架构计算能力有限，而大语言模型计算能力强，以Clarion认知架构与大语言模型的协同组合为例，将两者结合以兼顾现实复杂性和心理现实性。


<details>
  <summary>Details</summary>
Motivation: 现有计算认知架构计算能力有限，大语言模型计算能力更强，为同时处理现实世界复杂性和心理现实性，需将大语言模型融入认知架构。

Method: 以Clarion认知架构与大语言模型的协同组合为案例研究，利用Clarion的隐 - 显二分法实现两者无缝集成。

Result: 将大语言模型的计算能力与Clarion的心理精细度相结合。

Conclusion: 将大语言模型融入认知架构是处理现实复杂性和心理现实性的重要途径，Clarion与大语言模型的结合是成功的尝试。

Abstract: Computational cognitive architectures are broadly scoped models of the human
mind that combine different psychological functionalities (as well as often
different computational methods for these different functionalities) into one
unified framework. They structure them in a psychologically plausible and
validated way. However, such models thus far have only limited computational
capabilities, mostly limited by the computational tools and techniques that
were adopted. More recently, LLMs have proved to be more capable
computationally than any other tools. Thus, in order to deal with both
real-world complexity and psychological realism at the same time, incorporating
LLMs into cognitive architectures naturally becomes an important task. In the
present article, a synergistic combination of the Clarion cognitive
architecture and LLMs is discussed as a case study. The implicit-explicit
dichotomy that is fundamental to Clarion is leveraged for a seamless
integration of Clarion and LLMs. As a result, computational power of LLMs is
combined with psychological nicety of Clarion.

</details>


### [13] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: 本文探讨大语言模型生成推理依据的偏好评估问题，通过多方式评估关键属性，分析数据集，用属性特定ELO分数重新评估，发现细粒度属性评估能更好表征依据质量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成推理依据的评估方法不透明且粗粒度，难以洞察优劣原因，需改进评估方式。

Method: 从先前文献中确定关键推理依据属性，用自动指标、大语言模型判断和人工标注评估；用SHAP分析两个标准人类偏好数据集；用属性特定ELO分数重新评估模型生成的推理依据。

Result: 实现更细致的模型比较和洞察，细粒度属性评估可更好表征推理依据质量。

Conclusion: 细粒度属性评估能更好表征推理依据质量，为更具可解释性和可靠性的评估实践提供方向。

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [14] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: 提出Free - MAD框架解决现有多智能体辩论方法问题，单轮辩论提升推理性能、降低成本且更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论方法基于共识设计，存在通信轮次多增加开销、易导致错误传播、多数投票有随机性和不公平性等局限。

Method: 提出Free - MAD框架，引入基于分数的决策机制评估辩论轨迹，引入反从众机制重建辩论阶段。

Result: 在八个基准数据集实验表明，Free - MAD单轮辩论提升推理性能、降低token成本，在现实攻击场景中更鲁棒。

Conclusion: Free - MAD能有效解决现有多智能体辩论方法的问题，提升推理能力。

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


### [15] [Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration](https://arxiv.org/abs/2509.11067)
*Liangxuan Guo,Bin Zhu,Qingqian Tao,Kangning Liu,Xun Zhao,Xianzhe Qin,Jin Gao,Guangfu Hao*

Main category: cs.AI

TL;DR: 介绍了多智能体系统Agentic Lybic用于桌面自动化，在OSWorld基准测试中取得高成功率，证明多智能体编排和质量控制的优势。


<details>
  <summary>Details</summary>
Motivation: 现有桌面自动化自主智能体在复杂多步骤任务中存在协调和质量控制问题。

Method: 引入以有限状态机（FSM）运行的多智能体系统Agentic Lybic，包含控制器、管理器、三种工作者和评估器，通过FSM路由动态选择子任务执行策略。

Result: 在OSWorld基准测试中，Agentic Lybic在50步操作中成功率达57.07%，远超现有方法。

Conclusion: 有原则的多智能体编排和持续质量控制能为复杂计算环境中的通用桌面自动化提供更高可靠性。

Abstract: Autonomous agents for desktop automation struggle with complex multi-step
tasks due to poor coordination and inadequate quality control. We introduce
\textsc{Agentic Lybic}, a novel multi-agent system where the entire
architecture operates as a finite-state machine (FSM). This core innovation
enables dynamic orchestration. Our system comprises four components: a
Controller, a Manager, three Workers (Technician for code-based operations,
Operator for GUI interactions, and Analyst for decision support), and an
Evaluator. The critical mechanism is the FSM-based routing between these
components, which provides flexibility and generalization by dynamically
selecting the optimal execution strategy for each subtask. This principled
orchestration, combined with robust quality gating, enables adaptive replanning
and error recovery. Evaluated officially on the OSWorld benchmark,
\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\% success rate in 50
steps, substantially outperforming existing methods. Results demonstrate that
principled multi-agent orchestration with continuous quality control provides
superior reliability for generalized desktop automation in complex computing
environments.

</details>


### [16] [Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability](https://arxiv.org/abs/2509.11068)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 论文针对大语言模型多智能体系统中计算信任问题，提出基于确定性可复制性的验证框架，模拟显示有显著效率提升，为负责任AI奠定基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型向多智能体系统发展，面临计算信任挑战，即验证输出是否由声称的模型生成。

Method: 提出实现可处理的不对称努力的验证框架，基于确定性可复制性原则，让多个验证者对输出随机小段进行概率审计并有效分配验证工作。

Result: 模拟显示有针对性的验证比完全再生快12倍以上，可调整参数改变检测概率。

Conclusion: 工作为负责任AI提供基础层，是复杂异构多智能体系统未来研究的基石。

Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,
multi-agent systems. This introduces a fundamental challenge in establishing
computational trust, specifically how one agent can verify that another's
output was genuinely produced by a claimed LLM, and not falsified or generated
by a cheaper or inferior model. To address this challenge, this paper proposes
a verification framework that achieves tractable asymmetric effort, where the
cost to verify a computation is substantially lower than the cost to perform
it. Our approach is built upon the principle of deterministic replicability, a
property inherent to autoregressive models that strictly necessitates a
computationally homogeneous environment where all agents operate on identical
hardware and software stacks. Within this defined context, our framework
enables multiple validators to probabilistically audit small, random segments
of an LLM's output and it distributes the verification workload effectively.
The simulations demonstrated that targeted verification can be over 12 times
faster than full regeneration, with tunable parameters to adjust the detection
probability. By establishing a tractable mechanism for auditable LLM systems,
our work offers a foundational layer for responsible AI and serves as a
cornerstone for future research into the more complex, heterogeneous
multi-agent systems.

</details>


### [17] [Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation](https://arxiv.org/abs/2509.11078)
*Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: 提出无真实医疗记录的现实患者生成框架Patient - Zero，实验表明其在多方面表现良好，能提升现有模型在MedQA数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有利用大语言模型生成合成数据的研究存在数据隐私、准确性、多样性问题，且缺乏像真实患者一样的交互能力。

Method: 提出Patient - Zero框架，引入医学对齐的多步生成架构，通过分层医学知识注入构建患者记录；设计动态更新机制优化虚拟患者与人的交互能力。

Result: 模型在准确性、多样性和一致性方面表现良好，使用生成的虚拟患者训练后，现有模型在MedQA数据集上有显著提升。

Conclusion: Patient - Zero框架能有效解决现有研究问题，生成高质量的虚拟患者记录。

Abstract: Synthetic data generation using large language models (LLMs) has emerged as a
promising solution across various domains, particularly in medical field, to
mitigate data collection challenges. However, existing studies mainly utilize
LLMs to rewrite and complete existing medical records, where the limitations in
data privacy, accuracy, and diversity sill exist, and additionally lack the
ability to interact like real patients. To address these issues, we propose a
realistic patient generation framework, Patient-Zero, which requires no real
medical records. Patient-Zero first introduces a medically-aligned multi-step
generation architecture, which builds comprehensive patient records through
hierarchical medical knowledge injection without real medical records. Then, to
optimize the virtual patient's interaction abilities with humans, Patient-Zero
designs a dynamic updating mechanism to improve the consistency and
conversational performance. Our framework enables the generation of
contextually diverse patient records while maintaining strict medical
coherence, supported by adaptive dialogue strategies and real-time clinical
plausibility verification. Experimental results demonstrate that our model
achieves good performance in accuracy, diversity, and consistency. After
training with our generated virtual patients, existing models show significant
improvements on the MedQA dataset.

</details>


### [18] [Difficulty-Aware Agent Orchestration in LLM-Powered Workflows](https://arxiv.org/abs/2509.11079)
*Jinwei Su,Yinghui Xia,Qizhen Lan,Xinyuan Song,Yang Jingsong,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: 针对现有多智能体框架的局限，提出DAAO动态框架，在六个基准测试中表现优于先前系统，后续将开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架依赖静态或任务级工作流，存在处理简单或复杂查询表现不佳、忽略异构大语言模型效率与性能权衡等问题。

Method: 提出DAAO框架，包含用于难度估计的变分自编码器、模块化算子分配器和考虑成本与性能的大语言模型路由器，根据输入查询难度调整工作流深度、算子选择和大语言模型分配。

Result: DAAO在六个基准测试的准确性和推理效率上均优于先前的多智能体系统。

Conclusion: DAAO框架通过利用异构大语言模型和动态调整工作流，实现了细粒度、针对特定查询的推理策略。

Abstract: Large Language Model (LLM)-based agentic systems have shown strong
capabilities across various tasks. However, existing multi-agent frameworks
often rely on static or task-level workflows, which either over-process simple
queries or underperform on complex ones, while also neglecting the
efficiency-performance trade-offs across heterogeneous LLMs. To address these
limitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a
dynamic framework that adapts workflow depth, operator selection, and LLM
assignment based on the difficulty of each input query. DAAO comprises three
interdependent modules: a variational autoencoder (VAE) for difficulty
estimation, a modular operator allocator, and a cost- and performance-aware LLM
router. By leveraging heterogeneous LLMs and dynamically tailoring workflows,
DAAO enables fine-grained, query-specific reasoning strategies. DAAO
outperforms prior multi-agent systems in both accuracy and inference efficiency
across six benchmarks. We will release our code and implementation details upon
publication.

</details>


### [19] [Neural cellular automata: applications to biology and beyond classical AI](https://arxiv.org/abs/2509.11131)
*Benedikt Hartl,Michael Levin,Léo Pio-Lopez*

Main category: cs.AI

TL;DR: 本文介绍神经细胞自动机（NCA），回顾其生物或生物工程应用文献，指出其在多领域的优势，认为NCA是统一范式，能连接多尺度生物学与现代生成式AI。


<details>
  <summary>Details</summary>
Motivation: 鉴于NCA在近期发展中的巨大成功，对相关文献进行综述，探讨其在多领域的应用和潜力。

Method: 对NCA相关文献进行回顾和分析。

Result: NCA可模拟多尺度生物过程，具有鲁棒性和开放性适应能力，在生物、机器人控制、推理任务等领域有表现，其原理与现代生成式AI相似。

Conclusion: NCA是统一且计算高效的范式，能连接多尺度生物学与现代生成式AI，有望设计出真正受生物启发的集体智能。

Abstract: Neural Cellular Automata (NCA) represent a powerful framework for modeling
biological self-organization, extending classical rule-based systems with
trainable, differentiable (or evolvable) update rules that capture the adaptive
self-regulatory dynamics of living matter. By embedding Artificial Neural
Networks (ANNs) as local decision-making centers and interaction rules between
localized agents, NCA can simulate processes across molecular, cellular,
tissue, and system-level scales, offering a multiscale competency architecture
perspective on evolution, development, regeneration, aging, morphogenesis, and
robotic control. These models not only reproduce biologically inspired target
patterns but also generalize to novel conditions, demonstrating robustness to
perturbations and the capacity for open-ended adaptation and reasoning. Given
their immense success in recent developments, we here review current literature
of NCAs that are relevant primarily for biological or bioengineering
applications. Moreover, we emphasize that beyond biology, NCAs display robust
and generalizing goal-directed dynamics without centralized control, e.g., in
controlling or regenerating composite robotic morphologies or even on
cutting-edge reasoning tasks such as ARC-AGI-1. In addition, the same
principles of iterative state-refinement is reminiscent to modern generative
Artificial Intelligence (AI), such as probabilistic diffusion models. Their
governing self-regulatory behavior is constraint to fully localized
interactions, yet their collective behavior scales into coordinated
system-level outcomes. We thus argue that NCAs constitute a unifying
computationally lean paradigm that not only bridges fundamental insights from
multiscale biology with modern generative AI, but have the potential to design
truly bio-inspired collective intelligence capable of hierarchical reasoning
and control.

</details>


### [20] [AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions](https://arxiv.org/abs/2509.11595)
*Sabin Huda,Ernest Foo,Zahra Jadidi,MA Hakim Newton,Abdul Sattar*

Main category: cs.AI

TL;DR: 提出AMLNet框架，含交易生成器和检测管道，生成合规合成交易，检测效果好，提供多维度评估并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 解决反洗钱研究中缺乏公开、符合监管的交易数据集的问题。

Method: 构建基于知识的多智能体框架AMLNet，含法规感知交易生成器和集成检测管道。

Result: 生成1090173个合成交易，监管对齐达75%，检测集成F1为0.90，适应外部数据集。

Conclusion: 发布数据集，推动可重复、符合监管的反洗钱实验。

Abstract: Anti-money laundering (AML) research is constrained by the lack of publicly
shareable, regulation-aligned transaction datasets. We present AMLNet, a
knowledge-based multi-agent framework with two coordinated units: a
regulation-aware transaction generator and an ensemble detection pipeline. The
generator produces 1,090,173 synthetic transactions (approximately 0.16\%
laundering-positive) spanning core laundering phases (placement, layering,
integration) and advanced typologies (e.g., structuring, adaptive threshold
behavior). Regulatory alignment reaches 75\% based on AUSTRAC rule coverage
(Section 4.2), while a composite technical fidelity score of 0.75 summarizes
temporal, structural, and behavioral realism components (Section 4.4). The
detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the
internal test partitions of AMLNet and adapts to the external SynthAML dataset,
indicating architectural generalizability across different synthetic generation
paradigms. We provide multi-dimensional evaluation (regulatory, temporal,
network, behavioral) and release the dataset (Version 1.0,
https://doi.org/10.5281/zenodo.16736515), to advance reproducible and
regulation-conscious AML experimentation.

</details>


### [21] [AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment](https://arxiv.org/abs/2509.11135)
*Jing Xiao,Chang You,Zhiyu Chen*

Main category: cs.AI

TL;DR: 提出AlignKT模型解决现有KT模型问题，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有KT模型主要关注学习者交互序列，忽略知识状态本身，导致可解释性降低和教学支持不足。

Method: 采用前后端架构显式建模稳定知识状态，定义基于教学理论的理想知识状态作为对齐标准，使用五个编码器并结合对比学习模块。

Result: AlignKT在三个真实数据集上优于七个KT基线模型，在两个数据集上达到了最先进的结果，在第三个数据集上表现具有竞争力。

Conclusion: AlignKT能有效解决现有KT模型的问题，提高性能和可解释性。

Abstract: Knowledge Tracing (KT) serves as a fundamental component of Intelligent
Tutoring Systems (ITS), enabling these systems to monitor and understand
learners' progress by modeling their knowledge state. However, many existing KT
models primarily focus on fitting the sequences of learners' interactions, and
often overlook the knowledge state itself. This limitation leads to reduced
interpretability and insufficient instructional support from the ITS. To
address this challenge, we propose AlignKT, which employs a frontend-to-backend
architecture to explicitly model a stable knowledge state. In this approach,
the preliminary knowledge state is aligned with an additional criterion.
Specifically, we define an ideal knowledge state based on pedagogical theories
as the alignment criterion, providing a foundation for interpretability. We
utilize five encoders to implement this set-up, and incorporate a contrastive
learning module to enhance the robustness of the alignment process. Through
extensive experiments, AlignKT demonstrates superior performance, outperforming
seven KT baselines on three real-world datasets. It achieves state-of-the-art
results on two of these datasets and exhibits competitive performance on the
third. The code of this work is available at
https://github.com/SCNU203/AlignKT.

</details>


### [22] [AI-Generated Content in Cross-Domain Applications: Research Trends, Challenges and Propositions](https://arxiv.org/abs/2509.11151)
*Jianxin Li,Liang Qu,Taotao Cai,Zhixue Zhao,Nur Al Hasan Haldar,Aneesh Krishna,Xiangjie Kong,Flavio Romero Macau,Tanmoy Chakraborty,Aniket Deroy,Binshan Lin,Karen Blackmore,Nasimul Noman,Jingxian Cheng,Ningning Cui,Jianliang Xu*

Main category: cs.AI

TL;DR: 本文集合多学科16位学者，从跨领域视角探讨AIGC趋势与挑战，涵盖多方面内容并给出研究建议。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏探索不同领域AIGC最新进展和新兴挑战的研究，为填补此空白开展研究。

Method: 集合多学科16位学者，从训练技术、检测方法、社会影响等方面进行分析。

Result: 对AIGC进行广泛概述，介绍社会影响，讨论技术挑战并提出研究建议。

Conclusion: 为读者提供AIGC跨领域视角，洞察其研究趋势、挑战和未来方向。

Abstract: Artificial Intelligence Generated Content (AIGC) has rapidly emerged with the
capability to generate different forms of content, including text, images,
videos, and other modalities, which can achieve a quality similar to content
created by humans. As a result, AIGC is now widely applied across various
domains such as digital marketing, education, and public health, and has shown
promising results by enhancing content creation efficiency and improving
information delivery. However, there are few studies that explore the latest
progress and emerging challenges of AIGC across different domains. To bridge
this gap, this paper brings together 16 scholars from multiple disciplines to
provide a cross-domain perspective on the trends and challenges of AIGC.
Specifically, the contributions of this paper are threefold: (1) It first
provides a broader overview of AIGC, spanning the training techniques of
Generative AI, detection methods, and both the spread and use of AI-generated
content across digital platforms. (2) It then introduces the societal impacts
of AIGC across diverse domains, along with a review of existing methods
employed in these contexts. (3) Finally, it discusses the key technical
challenges and presents research propositions to guide future work. Through
these contributions, this vision paper seeks to offer readers a cross-domain
perspective on AIGC, providing insights into its current research trends,
ongoing challenges, and future directions.

</details>


### [23] [VideoAgent: Personalized Synthesis of Scientific Videos](https://arxiv.org/abs/2509.11253)
*Xiao Liang,Bangxin Li,Zixuan Chen,Hanyue Zheng,Zhi Ma,Di Wang,Cong Tian,Quan Wang*

Main category: cs.AI

TL;DR: 介绍VideoAgent框架自动生成个性化科学视频，提出SciVidEval评估套件，实验显示方法优于现有服务。


<details>
  <summary>Details</summary>
Motivation: 现有文档自动化工作缺乏个性化动态编排和多模态内容同步机制，需解决科学视频自动化生成难题。

Method: 引入VideoAgent框架，将源论文解析为细粒度资产库，按用户需求编排叙事流程；提出SciVidEval评估套件。

Result: 实验表明该方法显著优于现有商业科学视频生成服务，接近人类科学传播水平。

Conclusion: VideoAgent框架和SciVidEval评估套件有效解决科学视频自动化生成问题，提升生成质量。

Abstract: Automating the generation of scientific videos is a crucial yet challenging
task for effective knowledge dissemination. However, existing works on document
automation primarily focus on static media such as posters and slides, lacking
mechanisms for personalized dynamic orchestration and multimodal content
synchronization. To address these challenges, we introduce VideoAgent, a novel
multi-agent framework that synthesizes personalized scientific videos through a
conversational interface. VideoAgent parses a source paper into a fine-grained
asset library and, guided by user requirements, orchestrates a narrative flow
that synthesizes both static slides and dynamic animations to explain complex
concepts. To enable rigorous evaluation, we also propose SciVidEval, the first
comprehensive suite for this task, which combines automated metrics for
multimodal content quality and synchronization with a Video-Quiz-based human
evaluation to measure knowledge transfer. Extensive experiments demonstrate
that our method significantly outperforms existing commercial scientific video
generation services and approaches human-level quality in scientific
communication.

</details>


### [24] [Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble](https://arxiv.org/abs/2509.11311)
*Bingchen Wang,Zi-Yu Khoo,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: 提出新颖对齐框架，以大语言模型为代理解决社会科学调查挑战，介绍P2P系统，在真实数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 解决社会科学中调查部署成本上升和调查响应数据人口统计失衡的问题。

Method: 借鉴显示偏好理论，将对齐问题分为构建代理角色和选择代表性子集两阶段，引入P2P系统，使用结构化提示工程、基于熵的采样和基于回归的选择。

Result: 在真实世界的意见调查数据集上，对齐的代理群体能高保真地重现总体响应模式，且展现出显著的响应多样性。

Conclusion: 该框架不仅提高社会科学研究的数据效率，还为研究多元对齐的实施提供了试验平台，且方法具有更好的泛化性和简约性。

Abstract: Large language models (LLMs) have demonstrated promise in emulating
human-like responses across a wide range of tasks. In this paper, we propose a
novel alignment framework that treats LLMs as agent proxies for human survey
respondents, affording a cost-effective and steerable solution to two pressing
challenges in the social sciences: the rising cost of survey deployment and the
growing demographic imbalance in survey response data. Drawing inspiration from
the theory of revealed preference, we formulate alignment as a two-stage
problem: constructing diverse agent personas called endowments that simulate
plausible respondent profiles, and selecting a representative subset to
approximate a ground-truth population based on observed data. To implement the
paradigm, we introduce P2P, a system that steers LLM agents toward
representative behavioral patterns using structured prompt engineering,
entropy-based sampling, and regression-based selection. Unlike
personalization-heavy approaches, our alignment approach is
demographic-agnostic and relies only on aggregate survey results, offering
better generalizability and parsimony. Beyond improving data efficiency in
social science research, our framework offers a testbed for studying the
operationalization of pluralistic alignment. We demonstrate the efficacy of our
approach on real-world opinion survey datasets, showing that our aligned agent
populations can reproduce aggregate response patterns with high fidelity and
exhibit substantial response diversity, even without demographic conditioning.

</details>


### [25] [Decoding Plastic Toxicity: An Intelligent Framework for Conflict-Aware Relational Metapath Extraction from Scientific Abstracts](https://arxiv.org/abs/2509.11330)
*Sudeshna Jana,Manjira Sinha,Tirthankar Dasgupta*

Main category: cs.AI

TL;DR: 提出利用大语言模型从科学摘要中提取关系元路径构建毒性轨迹图的框架，性能良好。


<details>
  <summary>Details</summary>
Motivation: 塑料广泛使用致微纳塑料积累，对健康构成严重风险，需从科学文献中挖掘因果关系。

Method: 提出框架，用大语言模型提取关系元路径，构建毒性轨迹图，加入动态证据协调模块解决语义冲突。

Result: 该方法在从嘈杂科学文本中提取可靠、高实用性关系知识方面表现出色。

Conclusion: 此方法为挖掘特定领域语料中的复杂因果结构提供了可扩展的解决方案。

Abstract: The widespread use of plastics and their persistence in the environment have
led to the accumulation of micro- and nano-plastics across air, water, and
soil, posing serious health risks including respiratory, gastrointestinal, and
neurological disorders. We propose a novel framework that leverages large
language models to extract relational metapaths, multi-hop semantic chains
linking pollutant sources to health impacts, from scientific abstracts. Our
system identifies and connects entities across diverse contexts to construct
structured relational metapaths, which are aggregated into a Toxicity
Trajectory Graph that traces pollutant propagation through exposure routes and
biological systems. Moreover, to ensure consistency and reliability, we
incorporate a dynamic evidence reconciliation module that resolves semantic
conflicts arising from evolving or contradictory research findings. Our
approach demonstrates strong performance in extracting reliable, high-utility
relational knowledge from noisy scientific text and offers a scalable solution
for mining complex cause-effect structures in domain-specific corpora.

</details>


### [26] [The power of dynamic causality in observer-based design for soft sensor applications](https://arxiv.org/abs/2509.11336)
*William Farlessyost,Sebastian Oberst,Shweta Singh*

Main category: cs.AI

TL;DR: 本文提出基于动态因果分析优化基于观测器软传感器的框架，利用LTC网络识别和修剪传感器输入，在三个测试平台验证，结果表明该方法能识别最小传感器集，提高预测准确性且增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统传感器选择方法无法捕捉复杂系统的时间演化，存在不足。

Method: 利用LTC网络，通过迭代工作流程，包括在候选输入上训练LTC观测器、量化因果影响、移除影响小的输入并重新训练。

Result: 该方法能一致识别与物理原理相符的最小传感器集，提高预测准确性，区分关键测量和噪声，判断衍生交互项信息。

Conclusion: 此框架基于动态因果关系而非静态相关性进行传感器选择，在多个领域的软传感应用中有显著优势。

Abstract: This paper introduces a novel framework for optimizing observer-based soft
sensors through dynamic causality analysis. Traditional approaches to sensor
selection often rely on linearized observability indices or statistical
correlations that fail to capture the temporal evolution of complex systems. We
address this gap by leveraging liquid-time constant (LTC) networks,
continuous-time neural architectures with input-dependent time constants, to
systematically identify and prune sensor inputs with minimal causal influence
on state estimation. Our methodology implements an iterative workflow: training
an LTC observer on candidate inputs, quantifying each input's causal impact
through controlled perturbation analysis, removing inputs with negligible
effect, and retraining until performance degradation occurs. We demonstrate
this approach on three mechanistic testbeds representing distinct physical
domains: a harmonically forced spring-mass-damper system, a nonlinear
continuous stirred-tank reactor, and a predator-prey model following the
structure of the Lotka-Volterra model, but with seasonal forcing and added
complexity. Results show that our causality-guided pruning consistently
identifies minimal sensor sets that align with underlying physics while
improving prediction accuracy. The framework automatically distinguishes
essential physical measurements from noise and determines when derived
interaction terms provide complementary versus redundant information. Beyond
computational efficiency, this approach enhances interpretability by grounding
sensor selection decisions in dynamic causal relationships rather than static
correlations, offering significant benefits for soft sensing applications
across process engineering, ecological monitoring, and agricultural domains.

</details>


### [27] [MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization](https://arxiv.org/abs/2509.11361)
*Yichen Han,Bojun Liu,Zhengpeng zhou,Guanyu Liu,Zeng Zhang,Yang Yang,Wenli Wang,Isaac N Shi,Yunyan,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: 现有提示工程方法有局限，提出MAPGD框架，实验显示其在准确性和效率上优于基线，消融实验证实其组件优势。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法依赖单一优化轨迹，存在适应性和效率问题，有视角窄、梯度冲突和计算成本高的缺点。

Method: 提出MAPGD框架，集成多智能体协作与基于梯度的优化，有专门智能体、语义梯度协调、基于多臂老虎机的候选选择及理论收敛保证。

Result: 在分类、生成和推理任务实验中，MAPGD在准确性和效率上优于单智能体和随机基线。

Conclusion: MAPGD是一种统一的、受梯度启发的多智能体方法，可进行稳健且可解释的提示优化。

Abstract: Prompt engineering is crucial for leveraging large language models (LLMs),
but existing methods often rely on a single optimization trajectory, limiting
adaptability and efficiency while suffering from narrow perspectives, gradient
conflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt
Gradient Descent), a framework integrating multi-agent collaboration with
gradient-based optimization. MAPGD features specialized agents for task
clarity, example selection, format design, and stylistic refinement; semantic
gradient coordination to resolve conflicts; bandit-based candidate selection
for efficient exploration-exploitation; and theoretical convergence guarantees.
Experiments on classification, generation, and reasoning tasks show MAPGD
outperforms single-agent and random baselines in accuracy and efficiency.
Ablations confirm the benefits of gradient fusion, agent specialization, and
conflict resolution, providing a unified, gradient-inspired multi-agent
approach to robust and interpretable prompt optimization.

</details>


### [28] [Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications](https://arxiv.org/abs/2509.11431)
*Aadil Gani Ganie*

Main category: cs.AI

TL;DR: 大语言模型受限，AI 代理可缓解但有安全威胁，本文提出集成 RBAC 的框架保障安全。


<details>
  <summary>Details</summary>
Motivation: 大语言模型受训练数据限制，AI 代理虽有优势但面临安全威胁，需解决安全问题以有效部署。

Method: 提出将基于角色的访问控制（RBAC）集成到 AI 代理的框架。

Result: 未提及具体结果。

Conclusion: 该框架旨在支持 AI 代理有效且可扩展的部署，尤其关注本地部署。

Abstract: The emergence of Large Language Models (LLMs) has significantly advanced
solutions across various domains, from political science to software
development. However, these models are constrained by their training data,
which is static and limited to information available up to a specific date.
Additionally, their generalized nature often necessitates fine-tuning --
whether for classification or instructional purposes -- to effectively perform
specific downstream tasks. AI agents, leveraging LLMs as their core, mitigate
some of these limitations by accessing external tools and real-time data,
enabling applications such as live weather reporting and data analysis. In
industrial settings, AI agents are transforming operations by enhancing
decision-making, predictive maintenance, and process optimization. For example,
in manufacturing, AI agents enable near-autonomous systems that boost
productivity and support real-time decision-making. Despite these advancements,
AI agents remain vulnerable to security threats, including prompt injection
attacks, which pose significant risks to their integrity and reliability. To
address these challenges, this paper proposes a framework for integrating
Role-Based Access Control (RBAC) into AI agents, providing a robust security
guardrail. This framework aims to support the effective and scalable deployment
of AI agents, with a focus on on-premises implementations.

</details>


### [29] [Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction](https://arxiv.org/abs/2509.11459)
*Chen Jiang,Kofi Osei,Sai Deepthi Yeddula,Dongji Feng,Wei-Shinn Ku*

Main category: cs.AI

TL;DR: 提出自适应专家混合（MoE）模型用于降水率预测，引入交互式可视化工具，在飓风伊恩数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 准确降水预报在多领域重要，但传统深度学习模型难有效整合多源异构气象数据，以往研究在异构数据整合上存在局限。

Method: 提出自适应MoE模型，每个专家专注特定模态或时空模式，引入动态路由器分配输入；开发交互式基于网络的可视化工具。

Result: 模块化设计提高了预测准确性和可解释性，自适应MoE在基准测试中显著优于所有基线。

Conclusion: 自适应MoE模型和可视化工具能有效解决多源异构气象数据整合问题，可支持气候敏感部门决策。

Abstract: Accurate precipitation forecasting is indispensable in agriculture, disaster
management, and sustainable strategies. However, predicting rainfall has been
challenging due to the complexity of climate systems and the heterogeneous
nature of multi-source observational data, including radar, satellite imagery,
and surface-level measurements. The multi-source data vary in spatial and
temporal resolution, and they carry domain-specific features, making it
challenging for effective integration in conventional deep learning models.
Previous research has explored various machine learning techniques for weather
prediction; however, most struggle with the integration of data with
heterogeneous modalities. To address these limitations, we propose an Adaptive
Mixture of Experts (MoE) model tailored for precipitation rate prediction. Each
expert within the model specializes in a specific modality or spatio-temporal
pattern. We also incorporated a dynamic router that learns to assign inputs to
the most relevant experts. Our results show that this modular design enhances
predictive accuracy and interpretability. In addition to the modeling
framework, we introduced an interactive web-based visualization tool that
enables users to intuitively explore historical weather patterns over time and
space. The tool was designed to support decision-making for stakeholders in
climate-sensitive sectors. We evaluated our approach using a curated multimodal
climate dataset capturing real-world conditions during Hurricane Ian in 2022.
The benchmark results show that the Adaptive MoE significantly outperformed all
the baselines.

</details>


### [30] [Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs](https://arxiv.org/abs/2509.11480)
*Amir Taherin,Juyi Lin,Arash Akbari,Arman Akbari,Pu Zhao,Weiwei Chen,David Kaeli,Yanzhi Wang*

Main category: cs.AI

TL;DR: 评估五种VLA模型在边缘和数据中心GPU平台上的性能，发现不同缩放趋势并提供部署优化见解。


<details>
  <summary>Details</summary>
Motivation: 当前对VLA模型在不同架构、硬件平台上的性能缩放及功耗情况了解不足。

Method: 使用LIBERO基准测试，在不同边缘功率约束和高性能数据中心GPU配置下测量准确性和系统级指标。

Result: 确定了不同缩放趋势，如架构选择影响吞吐量和内存占用、边缘设备性能非线性下降、高吞吐量变体可实现且精度损失小。

Conclusion: 为跨部署约束选择和优化VLA提供见解，挑战了数据中心硬件用于机器人推理的优越性假设。

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist
policies for robotic control, yet their performance scaling across model
architectures and hardware platforms, as well as their associated power
budgets, remain poorly understood. This work presents an evaluation of five
representative VLA models -- spanning state-of-the-art baselines and two newly
proposed architectures -- targeting edge and datacenter GPU platforms. Using
the LIBERO benchmark, we measure accuracy alongside system-level metrics,
including latency, throughput, and peak memory usage, under varying edge power
constraints and high-performance datacenter GPU configurations. Our results
identify distinct scaling trends: (1) architectural choices, such as action
tokenization and model backbone size, strongly influence throughput and memory
footprint; (2) power-constrained edge devices exhibit non-linear performance
degradation, with some configurations matching or exceeding older datacenter
GPUs; and (3) high-throughput variants can be achieved without significant
accuracy loss. These findings provide actionable insights when selecting and
optimizing VLAs across a range of deployment constraints. Our work challenges
current assumptions about the superiority of datacenter hardware for robotic
inference.

</details>


### [31] [MedicalOS: An LLM Agent based Operating System for Digital Healthcare](https://arxiv.org/abs/2509.11507)
*Jared Zhu,Junde Wu*

Main category: cs.AI

TL;DR: 文章介绍多数数字医疗系统难用，提出医疗操作系统MedicalOS，经实证验证其能推动临床工作流自动化。


<details>
  <summary>Details</summary>
Motivation: 现有数字医疗系统难学难用，大型语言模型代理展现新交互潜力，医疗领域需特定抽象层确保安全合规。

Method: 提出MedicalOS作为医疗领域特定抽象层，将人类指令转化为预定义数字医疗命令，用机器语言封装成现成工具。

Result: 在214个患者案例、22个专科中验证，有高诊断准确性和置信度、合理检查请求、能生成结构化报告和药物推荐。

Conclusion: MedicalOS是推动临床实践工作流自动化的可靠且可扩展的基础。

Abstract: Decades' advances in digital health technologies, such as electronic health
records, have largely streamlined routine clinical processes. Yet, most these
systems are still hard to learn and use: Clinicians often face the burden of
managing multiple tools, repeating manual actions for each patient, navigating
complicated UI trees to locate functions, and spending significant time on
administration instead of caring for patients. The recent rise of large
language model (LLM) based agents demonstrates exceptional capability in coding
and computer operation, revealing the potential for humans to interact with
operating systems and software not by direct manipulation, but by instructing
agents through natural language. This shift highlights the need for an
abstraction layer, an agent-computer interface, that translates human language
into machine-executable commands. In digital healthcare, however, requires a
more domain-specific abstractions that strictly follow trusted clinical
guidelines and procedural standards to ensure safety, transparency, and
compliance. To address this need, we present \textbf{MedicalOS}, a unified
agent-based operational system designed as such a domain-specific abstract
layer for healthcare. It translates human instructions into pre-defined digital
healthcare commands, such as patient inquiry, history retrieval, exam
management, report generation, referrals, treatment planning, that we wrapped
as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,
Linux). We empirically validate MedicalOS on 214 patient cases across 22
specialties, demonstrating high diagnostic accuracy and confidence, clinically
sound examination requests, and consistent generation of structured reports and
medication recommendations. These results highlight MedicalOS as a trustworthy
and scalable foundation for advancing workflow automation in clinical practice.

</details>


### [32] [Task Decoding based on Eye Movements using Synthetic Data Augmentation](https://arxiv.org/abs/2509.11547)
*Shanmuka Sadhu,Arca Baran,Preeti Pandey,Ayush Kumar*

Main category: cs.AI

TL;DR: 本文利用合成数据生成器生成合成眼动数据，结合真实数据提升传统机器学习算法对任务解码的准确率。


<details>
  <summary>Details</summary>
Motivation: 支持Yarbus关于可从眼动解码观察者任务的假设，改善传统机器学习算法基于眼动数据解码任务的结果。

Method: 使用CTGAN及其变体等合成数据生成器生成合成数据样本，结合真实眼动数据，利用多种机器学习算法进行任务分类。

Result: 增加眼动数据和合成数据可提升分类准确率，如使用Inception Time时任务解码准确率从28.1%提升到82%，所提框架优于该数据集上的现有研究。

Conclusion: 随着生成数据与真实数据的扩充，任务解码准确率会提高，验证了合成数据可改善任务解码效果的观点。

Abstract: Machine learning has been extensively used in various applications related to
eye-tracking research. Understanding eye movement is one of the most
significant subsets of eye-tracking research that reveals the scanning pattern
of an individual. Researchers have thoroughly analyzed eye movement data to
understand various eye-tracking applications, such as attention mechanisms,
navigational behavior, task understanding, etc. The outcome of traditional
machine learning algorithms used for decoding tasks based on eye movement data
has received a mixed reaction to Yarbus' claim that it is possible to decode
the observer's task from their eye movements. In this paper, to support the
hypothesis by Yarbus, we are decoding tasks categories while generating
synthetic data samples using well-known Synthetic Data Generators CTGAN and its
variations such as CopulaGAN and Gretel AI Synthetic Data generators on
available data from an in-person user study. Our results show that augmenting
more eye movement data combined with additional synthetically generated
improves classification accuracy even with traditional machine learning
algorithms. We see a significant improvement in task decoding accuracy from
28.1% using Random Forest to 82% using Inception Time when five times more data
is added in addition to the 320 real eye movement dataset sample. Our proposed
framework outperforms all the available studies on this dataset because of the
use of additional synthetic datasets. We validated our claim with various
algorithms and combinations of real and synthetic data to show how decoding
accuracy increases with the increase in the augmentation of generated data to
real data.

</details>


### [33] [Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain](https://arxiv.org/abs/2509.11572)
*Tuan Bui,An Nguyen,Phat Thai,Minh Hua,Ngan Pham L. N.,Ngan Pham T. B.,Dung Le,Long Nguyen,Thanh-Tung Tran,Thang Bui,Tho Quan*

Main category: cs.AI

TL;DR: 本文提出MCFR框架结合大语言模型与模型检查支持属性验证，用EduMC - QA数据集评估，结果显示其提升推理忠实性和可解释性，并与先进大语言模型对比。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理痕迹常不忠实，结合符号引擎的方法局限于静态逻辑，难以处理动态、基于状态的推理，因此需要新方法支持封闭领域问答系统的推理。

Method: 提出MCFR框架，将自然语言转换为形式规范并在过渡模型上验证，引入EduMC - QA基准数据集进行评估。

Result: MCFR提高了推理的忠实性和可解释性，为高风险封闭领域应用中的可验证问答提供了可行途径。

Conclusion: MCFR是一种有效的方法，能解决现有推理方法的不足，在封闭领域问答系统中有应用潜力。

Abstract: Reasoning is essential for closed-domain QA systems in which procedural
correctness and policy compliance are critical. While large language models
(LLMs) have shown strong performance on many reasoning tasks, recent work
reveals that their reasoning traces are often unfaithful - serving more as
plausible justifications than as causally grounded derivations. Efforts to
combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved
reliability but remain limited to static forms of logic, struggling with
dynamic, state-based reasoning such as multi-step progressions and conditional
transitions.
  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a
neuro-symbolic framework that integrates LLMs with model checking to support
property verification. MCFR translates natural language into formal
specifications and verifies them over transition models. To support evaluation,
we introduce EduMC-QA, a benchmark dataset grounded in real academic
procedures. Our results show that MCFR improves reasoning faithfulness and
interpretability, offering a viable path toward verifiable QA in high-stakes
closed-domain applications. In addition to evaluating MCFR, we compare its
performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to
contextualize its effectiveness.

</details>


### [34] [A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models](https://arxiv.org/abs/2509.11575)
*Ching Chang,Yidan Shi,Defu Cao,Wei Yang,Jeehyun Hwang,Haixin Wang,Jiacheng Pang,Wei Wang,Yan Liu,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.AI

TL;DR: 本文对时间序列推理进行综述，按推理拓扑分类文献，横跨多个研究目标，回顾各领域方法系统，强调评估实践，指出未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 对时间序列推理领域的方法、系统等进行全面梳理和分析，推动该领域发展。

Method: 按推理拓扑将文献分为三类，结合领域主要目标，跨领域回顾方法和系统，分析各拓扑优缺点。

Result: 展示各拓扑的作用和局限性，提供相关数据集、基准和资源，强调评估实践。

Conclusion: 推理结构需平衡基础和自我修正能力与计算成本和可重复性，未来发展依赖于将推理质量与效用挂钩的基准和考虑成本风险的闭环测试平台。

Abstract: Time series reasoning treats time as a first-class axis and incorporates
intermediate evidence directly into the answer. This survey defines the problem
and organizes the literature by reasoning topology with three families: direct
reasoning in one step, linear chain reasoning with explicit intermediates, and
branch-structured reasoning that explores, revises, and aggregates. The
topology is crossed with the main objectives of the field, including
traditional time series analysis, explanation and understanding, causal
inference and decision making, and time series generation, while a compact tag
set spans these axes and captures decomposition and verification, ensembling,
tool use, knowledge access, multimodality, agent loops, and LLM alignment
regimes. Methods and systems are reviewed across domains, showing what each
topology enables and where it breaks down in faithfulness or robustness, along
with curated datasets, benchmarks, and resources that support study and
deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).
Evaluation practices that keep evidence visible and temporally aligned are
highlighted, and guidance is distilled on matching topology to uncertainty,
grounding with observable artifacts, planning for shift and streaming, and
treating cost and latency as design budgets. We emphasize that reasoning
structures must balance capacity for grounding and self-correction against
computational cost and reproducibility, while future progress will likely
depend on benchmarks that tie reasoning quality to utility and on closed-loop
testbeds that trade off cost and risk under shift-aware, streaming, and
long-horizon settings. Taken together, these directions mark a shift from
narrow accuracy toward reliability at scale, enabling systems that not only
analyze but also understand, explain, and act on dynamic worlds with traceable
evidence and credible outcomes.

</details>


### [35] [Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework](https://arxiv.org/abs/2509.11645)
*Zhaolong Wu,Pu Luo,Jason Pui Yin Cheung,Teng Zhang*

Main category: cs.AI

TL;DR: 对用于青少年特发性脊柱侧凸（AIS）自我管理的多模态大语言模型（MLLMs）进行综合评估，发现其不足并提出改进方法，结果显示改进效果不同，当前MLLMs远不能实现AIS护理个性化辅助。


<details>
  <summary>Details</summary>
Motivation: 首次对用于AIS自我管理的MLLMs进行全面评估。

Method: 构建约3000张前后位X光片及诊断文本数据库，通过包含视觉问答、领域知识评估和患者教育咨询评估任务的“分而治之”框架评估五个MLLMs，用脊柱关键点提示增强MLLMs，编译AIS知识库用于检索增强生成（RAG）。

Result: 视觉提示在不同架构上效果不同，RAG显著提高模型在知识评估任务上的表现。

Conclusion: 当前MLLMs远不能实现AIS护理个性化辅助，最大挑战在于准确检测脊柱畸形位置和方向的能力不足。

Abstract: This study presents the first comprehensive evaluation of Multimodal Large
Language Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS)
self-management. We constructed a database of approximately 3,000
anteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a
`Divide and Conquer' framework consisting of a visual question-answering task,
a domain knowledge assessment task, and a patient education counseling
assessment task. Our investigation revealed limitations of MLLMs' ability in
interpreting complex spinal radiographs and comprehending AIS care knowledge.
To address these, we pioneered enhancing MLLMs with spinal keypoint prompting
and compiled an AIS knowledge base for retrieval augmented generation (RAG),
respectively. Results showed varying effectiveness of visual prompting across
different architectures, while RAG substantially improved models' performances
on the knowledge assessment task. Our findings indicate current MLLMs are far
from capable in realizing personalized assistant in AIS care. The greatest
challenge lies in their abilities to obtain accurate detections of spinal
deformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).

</details>


### [36] [HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction](https://arxiv.org/abs/2509.11719)
*Bingqing Wei,Lianmin Chen,Zhongyu Xia,Yongtao Wang*

Main category: cs.AI

TL;DR: 本文提出HeLoFusion编码器解决自动驾驶多智能体轨迹预测中复杂社会动态建模难题，在Waymo数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉多尺度交互共存和异构智能体多样行为，需要更好的方法来建模复杂社会动态。

Method: 引入HeLoFusion编码器，构建以每个智能体为中心的局部多尺度图，采用聚合 - 分解消息传递方案和特定类型特征网络。

Result: 在Waymo Open Motion Dataset上达到了最先进的性能，为关键指标设定了新的基准。

Conclusion: 基于局部性的架构，显式建模多尺度和异构交互，是推进运动预测的有效策略。

Abstract: Multi-agent trajectory prediction in autonomous driving requires a
comprehensive understanding of complex social dynamics. Existing methods,
however, often struggle to capture the full richness of these dynamics,
particularly the co-existence of multi-scale interactions and the diverse
behaviors of heterogeneous agents. To address these challenges, this paper
introduces HeLoFusion, an efficient and scalable encoder for modeling
heterogeneous and multi-scale agent interactions. Instead of relying on global
context, HeLoFusion constructs local, multi-scale graphs centered on each
agent, allowing it to effectively model both direct pairwise dependencies and
complex group-wise interactions (\textit{e.g.}, platooning vehicles or
pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of
agent heterogeneity through an aggregation-decomposition message-passing scheme
and type-specific feature networks, enabling it to learn nuanced,
type-dependent interaction patterns. This locality-focused approach enables a
principled representation of multi-level social context, yielding powerful and
expressive agent embeddings. On the challenging Waymo Open Motion Dataset,
HeLoFusion achieves state-of-the-art performance, setting new benchmarks for
key metrics including Soft mAP and minADE. Our work demonstrates that a
locality-grounded architecture, which explicitly models multi-scale and
heterogeneous interactions, is a highly effective strategy for advancing motion
forecasting.

</details>


### [37] [Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning](https://arxiv.org/abs/2509.11880)
*Carlos Celemin,Joseph Brennan,Pierluigi Vito Amadori,Tim Bradley*

Main category: cs.AI

TL;DR: 本文将监督对比学习应用于模仿学习，在游戏环境中实验显示效果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 在模仿学习中为智能体学习更有效的状态表示，捕捉与动作相关因素，更好地建模观测与动作的因果关系。

Method: 提出将监督对比学习损失与连续输出空间相结合的方法，使监督对比学习不受环境动作类型限制。

Result: 在3D游戏Astro Bot和Returnal以及多个2D雅达利游戏上实验，显示出表示质量提高、学习收敛更快、泛化能力更好。

Conclusion: 将监督对比学习应用于模仿学习的方法有效，优于仅使用监督动作预测损失函数训练的基线模型。

Abstract: This paper introduces a novel application of Supervised Contrastive Learning
(SupCon) to Imitation Learning (IL), with a focus on learning more effective
state representations for agents in video game environments. The goal is to
obtain latent representations of the observations that capture better the
action-relevant factors, thereby modeling better the cause-effect relationship
from the observations that are mapped to the actions performed by the
demonstrator, for example, the player jumps whenever an obstacle appears ahead.
We propose an approach to integrate the SupCon loss with continuous output
spaces, enabling SupCon to operate without constraints regarding the type of
actions of the environment. Experiments on the 3D games Astro Bot and Returnal,
and multiple 2D Atari games show improved representation quality, faster
learning convergence, and better generalization compared to baseline models
trained only with supervised action prediction loss functions.

</details>


### [38] [EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](https://arxiv.org/abs/2509.11914)
*Yiqun Yao,Naitong Yu,Xiang Li,Xin Jiang,Xuezhi Fang,Wenjia Ma,Xuying Meng,Jing Li,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: 介绍首个适用于全双工模型的终身记忆代理EgoMem，介绍其工作流程，实验表明模块准确率超95%，系统事实一致性超87%。


<details>
  <summary>Details</summary>
Motivation: 为处理实时全模态流的全双工模型开发终身记忆代理，使实时模型能直接从原始视听流识别用户、提供个性化响应并维护长期知识。

Method: EgoMem通过三个异步进程运行，包括检索、全模态对话和内存管理进程。

Result: EgoMem的检索和内存管理模块在测试集上准确率超95%，与微调的RoboEgo全模态聊天机器人集成后，系统在实时个性化对话中事实一致性得分超87%。

Conclusion: EgoMem完全依赖原始视听流，适合终身、实时和具身场景，为未来研究建立了强大基线。

Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex
models that process real-time omnimodal streams. EgoMem enables real-time
models to recognize multiple users directly from raw audiovisual streams, to
provide personalized response, and to maintain long-term knowledge of users'
facts, preferences, and social relationships extracted from audiovisual
history. EgoMem operates with three asynchronous processes: (i) a retrieval
process that dynamically identifies user via face and voice, and gathers
relevant context from a long-term memory; (ii) an omnimodal dialog process that
generates personalized audio responses based on the retrieved context; and
(iii) a memory management process that automatically detects dialog boundaries
from omnimodal streams, and extracts necessary information to update the
long-term memory. Unlike existing memory agents for LLMs, EgoMem relies
entirely on raw audiovisual streams, making it especially suitable for
lifelong, real-time, and embodied scenarios. Experimental results demonstrate
that EgoMem's retrieval and memory management modules achieve over 95% accuracy
on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,
the system achieves fact-consistency scores above 87% in real-time personalized
dialogs, establishing a strong baseline for future research.

</details>


### [39] [BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning](https://arxiv.org/abs/2509.11922)
*Xilei Dai,Ruotian Chen,Songze Guan,Wen-Tai Li,Chau Yuen*

Main category: cs.AI

TL;DR: 提出开源工具BuildingGym用于建筑能源管理RL控制策略训练，可实现灵活配置，在制冷负载管理任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决建筑能源管理中缺乏灵活RL框架的问题。

Method: 提出BuildingGym工具，集成EnergyPlus，接受外部信号，提供内置RL算法。

Result: 在制冷负载管理任务中，内置算法表现出色。

Conclusion: BuildingGym能有效优化建筑能源管理的控制策略，弥合建筑管理者和AI专家的差距。

Abstract: Reinforcement learning (RL) has proven effective for AI-based building energy
management. However, there is a lack of flexible framework to implement RL
across various control problems in building energy management. To address this
gap, we propose BuildingGym, an open-source tool designed as a
research-friendly and flexible framework for training RL control strategies for
common challenges in building energy management. BuildingGym integrates
EnergyPlus as its core simulator, making it suitable for both system-level and
room-level control. Additionally, BuildingGym is able to accept external
signals as control inputs instead of taking the building as a stand-alone
entity. This feature makes BuildingGym applicable for more flexible
environments, e.g. smart grid and EVs community. The tool provides several
built-in RL algorithms for control strategy training, simplifying the process
for building managers to obtain optimal control strategies. Users can achieve
this by following a few straightforward steps to configure BuildingGym for
optimization control for common problems in the building energy management
field. Moreover, AI specialists can easily implement and test state-of-the-art
control algorithms within the platform. BuildingGym bridges the gap between
building managers and AI specialists by allowing for the easy configuration and
replacement of RL algorithms, simulators, and control environments or problems.
With BuildingGym, we efficiently set up training tasks for cooling load
management, targeting both constant and dynamic cooling load management. The
built-in algorithms demonstrated strong performance across both tasks,
highlighting the effectiveness of BuildingGym in optimizing cooling strategies.

</details>


### [40] [Neuromorphic Intelligence](https://arxiv.org/abs/2509.11940)
*Marcel van Gerven*

Main category: cs.AI

TL;DR: 本文探讨神经形态计算，指出动力学系统理论可提供统一框架，助力实现新兴神经形态智能。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算虽有优势，但缺乏能融合多学科的统一理论框架。

Method: 提出以动力学系统理论为基础，利用其建模推理、学习和控制，利用噪声学习，通过差分遗传编程发现实现自适应行为的动力系统。

Result: 未提及具体研究结果。

Conclusion: 采用动力学系统理论视角有助于实现新兴神经形态智能，推动人工智能科学及可持续性发展。

Abstract: Neuromorphic computing seeks to replicate the remarkable efficiency,
flexibility, and adaptability of the human brain in artificial systems. Unlike
conventional digital approaches, which depend on massive computational and
energy resources, neuromorphic systems exploit brain-inspired principles of
computation to achieve orders of magnitude greater energy efficiency. By
drawing on insights from artificial intelligence, neuroscience, physics,
chemistry, and materials science, neuromorphic computing promises to deliver
intelligent systems that are sustainable, transparent, and widely accessible. A
central challenge, however, is to identify a unifying theoretical framework
capable of bridging these diverse disciplines. We argue that dynamical systems
theory provides such a foundation. Rooted in differential calculus, it offers a
principled language for modeling inference, learning, and control in both
natural and artificial substrates. Within this framework, noise can be
harnessed as a resource for learning, while differential genetic programming
enables the discovery of dynamical systems that implement adaptive behaviors.
Embracing this perspective paves the way toward emergent neuromorphic
intelligence, where intelligent behavior arises from the dynamics of physical
substrates, advancing both the science and sustainability of AI.

</details>


### [41] [How to Evaluate Medical AI](https://arxiv.org/abs/2509.11941)
*Ilia Kopanichuk,Petr Anokhin,Vladimir Shaposhnikov,Vladimir Makharev,Ekaterina Tsapieva,Iaroslav Bespalov,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 提出新评估指标RPAD和RRAD，通过医疗对话评估，发现专家判断差异大，支持采用相对指标。


<details>
  <summary>Details</summary>
Motivation: 人工智能融入医疗诊断工作流需可靠评估方法，传统指标有局限性。

Method: 引入RPAD和RRAD指标，对比AI输出与多专家意见，用360个医疗对话评估多大型语言模型。

Result: 新指标能提供更稳定评估，自动化方法诊断准确率达98%，顶级模型表现达或超专家共识，专家判断差异常大于AI与人。

Conclusion: 绝对指标有局限，医疗AI应采用相对指标。

Abstract: The integration of artificial intelligence (AI) into medical diagnostic
workflows requires robust and consistent evaluation methods to ensure
reliability, clinical relevance, and the inherent variability in expert
judgments. Traditional metrics like precision and recall often fail to account
for the inherent variability in expert judgments, leading to inconsistent
assessments of AI performance. Inter-rater agreement statistics like Cohen's
Kappa are more reliable but they lack interpretability. We introduce Relative
Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new
evaluation metrics that compare AI outputs against multiple expert opinions
rather than a single reference. By normalizing performance against inter-expert
disagreement, these metrics provide a more stable and realistic measure of the
quality of predicted diagnosis. In addition to the comprehensive analysis of
diagnostic quality measures, our study contains a very important side result.
Our evaluation methodology allows us to avoid selecting diagnoses from a
limited list when evaluating a given case. Instead, both the models being
tested and the examiners verifying them arrive at a free-form diagnosis. In
this automated methodology for establishing the identity of free-form clinical
diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our
approach using 360 medical dialogues, comparing multiple large language models
(LLMs) against a panel of physicians. Large-scale study shows that
top-performing models, such as DeepSeek-V3, achieve consistency on par with or
exceeding expert consensus. Moreover, we demonstrate that expert judgments
exhibit significant variability - often greater than that between AI and
humans. This finding underscores the limitations of any absolute metrics and
supports the need to adopt relative metrics in medical AI.

</details>


### [42] [Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics](https://arxiv.org/abs/2509.11943)
*Antonin Sulc,Thorsten Hellert*

Main category: cs.AI

TL;DR: 本文探讨智能体在环境中决策能力，提出神经符号多智能体架构，在模拟环境中成功诊断复杂故障。


<details>
  <summary>Details</summary>
Motivation: 当前扩展模型和数据集范式虽有进展，但扩展智能体推理结构、保真度和逻辑一致性这一维度的研究不足，需在此方面进行探索。

Method: 引入神经符号多智能体架构，将个体智能体的信念状态用Kripke模型表示，利用模态逻辑进行推理，使用特定领域知识编码为逻辑约束来指导语言模型生成假设。

Result: 在高保真模拟粒子加速器环境中，系统结合语言模型语义直觉、模态逻辑验证和事实世界模型，成功诊断复杂级联故障。

Conclusion: 该架构为构建更强大、可靠和可验证的自主智能体提供了可行路径。

Abstract: The development of intelligent agents, particularly those powered by language
models (LMs), has shown the critical role in various environments that require
intelligent and autonomous decision. Environments are not passive testing
grounds and they represent the data required for agents to learn and exhibit
very challenging conditions that require adaptive, complex and autonomous
capacity to make decisions. While the paradigm of scaling models and datasets
has led to remarkable emergent capabilities, we argue that scaling the
structure, fidelity, and logical consistency of agent reasoning within these
environments is a crucial, yet underexplored, dimension of AI research. This
paper introduces a neuro-symbolic multi-agent architecture where the belief
states of individual agents are formally represented as Kripke models. This
foundational choice enables them to reason about known concepts of
\emph{possibility} and \emph{necessity} using the formal language of modal
logic. In this work, we use of immutable, domain-specific knowledge to make
infere information, which is encoded as logical constraints essential for
proper diagnosis. In the proposed model, we show constraints that actively
guide the hypothesis generation of LMs, effectively preventing them from
reaching physically or logically untenable conclusions. In a high-fidelity
simulated particle accelerator environment, our system successfully diagnoses
complex, cascading failures by combining the powerful semantic intuition of LMs
with the rigorous, verifiable validation of modal logic and a factual world
model and showcasing a viable path toward more robust, reliable, and verifiable
autonomous agents.

</details>


### [43] [Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare](https://arxiv.org/abs/2509.11944)
*Susanta Mitra*

Main category: cs.AI

TL;DR: 提出基于时间图的多模态医疗推理方法，可适应动态变化，考虑多时间点数据，有任务分配和交叉验证机制，实验证明其新颖性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理模型在医疗领域应用有限，难以正确诊断，需新方法协助医护人员。

Method: 提出基于有向图的时间图推理过程，考虑多时间点多模态数据，采用多智能体时间推理框架进行任务分配和交叉验证。

Result: 基本实验和分析结果证明了该初步方法的新颖性和实用价值。

Conclusion: 所提出的多模态医疗推理方法具有创新性和实用性，能提高推理输出准确性，可辅助医疗诊断。

Abstract: Healthcare and medicine are multimodal disciplines that deal with multimodal
data for reasoning and diagnosing multiple diseases. Although some multimodal
reasoning models have emerged for reasoning complex tasks in scientific
domains, their applications in the healthcare domain remain limited and fall
short in correct reasoning for diagnosis. To address the challenges of
multimodal medical reasoning for correct diagnosis and assist the healthcare
professionals, a novel temporal graph-based reasoning process modelled through
a directed graph has been proposed in the current work. It helps in
accommodating dynamic changes in reasons through backtracking, refining the
reasoning content, and creating new or deleting existing reasons to reach the
best recommendation or answer. Again, consideration of multimodal data at
different time points can enable tracking and analysis of patient health and
disease progression. Moreover, the proposed multi-agent temporal reasoning
framework provides task distributions and a cross-validation mechanism to
further enhance the accuracy of reasoning outputs. A few basic experiments and
analysis results justify the novelty and practical utility of the proposed
preliminary approach.

</details>


### [44] [MusicSwarm: Biologically Inspired Intelligence for Music Composition](https://arxiv.org/abs/2509.11973)
*Markus J. Buehler*

Main category: cs.AI

TL;DR: 本文展示了无权重更新的相同冻结基础模型分散群可生成连贯长形式音乐作品，且在多方面优于集中式系统，还明确了局部创新形成全局音乐形式的机制，其方法可迁移到其他领域。


<details>
  <summary>Details</summary>
Motivation: 探索在无权重更新情况下，如何实现连贯长形式音乐创作，并对比不同系统在音乐创作上的表现。

Method: 对比集中式多智能体系统与完全分散的群，群中的逐小节智能体感知和存储和声、节奏和结构线索，调整短期记忆并达成共识。

Result: 分散群在音乐质量、多样性、结构多样性和创造力指标上表现更优，动态趋向稳定互补角色配置，自相似网络呈现小世界架构。

Conclusion: MusicSwarm提供了计算和数据高效的长时创意结构路径，且可迁移到协作写作、设计和科学发现等领域。

Abstract: We show that coherent, long-form musical composition can emerge from a
decentralized swarm of identical, frozen foundation models that coordinate via
stigmergic, peer-to-peer signals, without any weight updates. We compare a
centralized multi-agent system with a global critic to a fully decentralized
swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and
structural cues, adapt short-term memory, and reach consensus. Across symbolic,
audio, and graph-theoretic analyses, the swarm yields superior quality while
delivering greater diversity and structural variety and leads across creativity
metrics. The dynamics contract toward a stable configuration of complementary
roles, and self-similarity networks reveal a small-world architecture with
efficient long-range connectivity and specialized bridging motifs, clarifying
how local novelties consolidate into global musical form. By shifting
specialization from parameter updates to interaction rules, shared memory, and
dynamic consensus, MusicSwarm provides a compute- and data-efficient route to
long-horizon creative structure that is immediately transferable beyond music
to collaborative writing, design, and scientific discovery.

</details>


### [45] [Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review](https://arxiv.org/abs/2509.12034)
*Emmanuel Adjei Domfeh,Christopher L. Dancy*

Main category: cs.AI

TL;DR: 文章对支持全灾种管理阶段决策的人机协作模式进行系统综述，指出AI系统优势与局限，给出挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在高风险灾害场景下，及时明智决策面临诸多挑战，需研究人机协作模式支持决策。

Method: 对51篇同行评审研究进行系统综述，识别四大类别并分析子模式。

Result: AI系统可增强态势感知、提高响应效率和支持复杂决策，但在可扩展性、可解释性和系统互操作性方面有局限。

Conclusion: 强调需要自适应、可信且上下文感知的人机系统，以提高灾害恢复力和公平的恢复结果。

Abstract: In high-stakes disaster scenarios, timely and informed decision-making is
critical yet often challenged by uncertainty, dynamic environments, and limited
resources. This paper presents a systematic review of Human-AI collaboration
patterns that support decision-making across all disaster management phases.
Drawing from 51 peer-reviewed studies, we identify four major categories:
Human-AI Decision Support Systems, Task and Resource Coordination, Trust and
Transparency, and Simulation and Training. Within these, we analyze
sub-patterns such as cognitive-augmented intelligence, multi-agent
coordination, explainable AI, and virtual training environments. Our review
highlights how AI systems may enhance situational awareness, improves response
efficiency, and support complex decision-making, while also surfacing critical
limitations in scalability, interpretability, and system interoperability. We
conclude by outlining key challenges and future research directions,
emphasizing the need for adaptive, trustworthy, and context-aware Human-AI
systems to improve disaster resilience and equitable recovery outcomes.

</details>


### [46] [When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models](https://arxiv.org/abs/2509.12060)
*Wei Cai,Shujuan Liu,Jian Zhao,Ziyan Shi,Yusheng Zhao,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 本文指出多模态大语言模型存在隐式推理风险，引入SSUI数据集并设计SRPO训练框架，实验显示SRPO训练的模型在安全基准测试中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在隐式推理风险，难以通过长链推理保持安全对齐。

Method: 引入SSUI数据集，设计Safety-aware Reasoning Path Optimization (SRPO)训练框架。

Result: SRPO训练的模型在关键安全基准测试中取得SOTA结果，显著优于开源和顶级商业多模态大语言模型。

Conclusion: SRPO训练框架能有效使多模态大语言模型的内部推理过程与人类安全价值观对齐。

Abstract: Multimodal Large Language Models (MLLMs) are susceptible to the implicit
reasoning risk, wherein innocuous unimodal inputs synergistically assemble into
risky multimodal data that produce harmful outputs. We attribute this
vulnerability to the difficulty of MLLMs maintaining safety alignment through
long-chain reasoning. To address this issue, we introduce
Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring
interpretable reasoning paths tailored for such a cross-modal challenge. A
novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is
also designed based on the SSUI dataset to align the MLLM's internal reasoning
process with human safety values. Experimental results show that our
SRPO-trained models achieve state-of-the-art results on key safety benchmarks,
including the proposed Reasoning Path Benchmark (RSBench), significantly
outperforming both open-source and top-tier commercial MLLMs.

</details>


### [47] [Bridging Engineering and AI Planning through Model-Based Knowledge Transformation for the Validation of Automated Production System Variants](https://arxiv.org/abs/2509.12091)
*Hamied Nabizada,Lasse Beers,Alain Chahine,Felix Gehlhoff,Oliver Niggemann,Alexander Fay*

Main category: cs.AI

TL;DR: 本文提出在SysML工程模型中规范和自动生成符号规划工件的方法，通过飞机装配案例展示其适用性。


<details>
  <summary>Details</summary>
Motivation: 现有MBSE环境下工程模型缺乏符号规划语义，限制了评估系统变体完成任务的能力和效率。

Method: 提出模型驱动方法，引入SysML专用配置文件定义规划构造的可重用原型，用算法生成PDDL领域文件和问题文件。

Result: 通过飞机装配案例，展示了为现有工程模型添加规划语义并生成规划工件，可通过AI规划验证系统变体。

Conclusion: 该方法支持原生集成，能保持工程和规划工件的一致性，可用于生成规划工件并验证系统变体。

Abstract: Engineering models created in Model-Based Systems Engineering (MBSE)
environments contain detailed information about system structure and behavior.
However, they typically lack symbolic planning semantics such as preconditions,
effects, and constraints related to resource availability and timing. This
limits their ability to evaluate whether a given system variant can fulfill
specific tasks and how efficiently it performs compared to alternatives.
  To address this gap, this paper presents a model-driven method that enables
the specification and automated generation of symbolic planning artifacts
within SysML-based engineering models. A dedicated SysML profile introduces
reusable stereotypes for core planning constructs. These are integrated into
existing model structures and processed by an algorithm that generates a valid
domain file and a corresponding problem file in Planning Domain Definition
Language (PDDL). In contrast to previous approaches that rely on manual
transformations or external capability models, the method supports native
integration and maintains consistency between engineering and planning
artifacts.
  The applicability of the method is demonstrated through a case study from
aircraft assembly. The example illustrates how existing engineering models are
enriched with planning semantics and how the proposed workflow is applied to
generate consistent planning artifacts from these models. The generated
planning artifacts enable the validation of system variants through AI
planning.

</details>


### [48] [JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference](https://arxiv.org/abs/2509.12104)
*Zongyue Xue,Siyuan Zheng,Shaochun Wang,Yiran Hu,Shenran Wang,Yuxin Yao,Haitao Li,Qingyao Ai,Yiqun Liu,Yun Liu,Weixing Shen*

Main category: cs.AI

TL;DR: 本文介绍开源评估工具JustEva以衡量大语言模型在法律任务中的公平性，实证发现当前大模型存在公平性缺陷，该工具为法律领域算法公平性评估和改进提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成到法律实践中引发对司法公平性的担忧，需要评估其公平性。

Method: 开发JustEva工具，包含结构化标签系统、核心公平指标、统计推断方法和可视化，支持两种实验类型进行评估。

Result: JustEva实证应用表明当前大语言模型存在显著公平性缺陷。

Conclusion: JustEva为评估和改进法律领域算法公平性提供了便利工具和方法基础。

Abstract: The integration of Large Language Models (LLMs) into legal practice raises
pressing concerns about judicial fairness, particularly due to the nature of
their "black-box" processes. This study introduces JustEva, a comprehensive,
open-source evaluation toolkit designed to measure LLM fairness in legal tasks.
JustEva features several advantages: (1) a structured label system covering 65
extra-legal factors; (2) three core fairness metrics - inconsistency, bias, and
imbalanced inaccuracy; (3) robust statistical inference methods; and (4)
informative visualizations. The toolkit supports two types of experiments,
enabling a complete evaluation workflow: (1) generating structured outputs from
LLMs using a provided dataset, and (2) conducting statistical analysis and
inference on LLMs' outputs through regression and other statistical methods.
Empirical application of JustEva reveals significant fairness deficiencies in
current LLMs, highlighting the lack of fair and trustworthy LLM legal tools.
JustEva offers a convenient tool and methodological foundation for evaluating
and improving algorithmic fairness in the legal domain.

</details>


### [49] [Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation](https://arxiv.org/abs/2509.12179)
*Yubo Li,Weiyi Song*

Main category: cs.AI

TL;DR: 提出双向认知对齐（BiCA）实现人机共同对齐，在协作导航中表现出色，验证从单向到共同对齐范式转变的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前基于RLHF的AI对齐采用单向范式，将人类认知视为固定，本文提出双向认知对齐以实现人机相互适应。

Method: 使用可学习协议、表示映射和KL预算约束进行可控的共同进化。

Result: 在协作导航中，BiCA成功率达85.5%，优于基线；相互适应能力提升230%，协议收敛性提升332%；涌现协议比手工协议性能高84%，双向适应提高安全性；协同作用提高46%。

Conclusion: 最优协作存在于人类和AI能力的交集而非并集，验证了从单向到共同对齐范式转变的合理性。

Abstract: Current AI alignment through RLHF follows a single directional paradigm that
AI conforms to human preferences while treating human cognition as fixed. We
propose a shift to co-alignment through Bidirectional Cognitive Alignment
(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,
representation mapping, and KL-budget constraints for controlled co-evolution.
In collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,
with 230% better mutual adaptation and 332% better protocol convergence.
Emergent protocols outperformed handcrafted ones by 84%, while bidirectional
adaptation unexpectedly improved safety (+23% out-of-distribution robustness).
The 46% synergy improvement demonstrates optimal collaboration exists at the
intersection, not union, of human and AI capabilities, validating the shift
from single-directional to co-alignment paradigms.

</details>


### [50] [Advancing Medical Artificial Intelligence Using a Century of Cases](https://arxiv.org/abs/2509.12194)
*Thomas A. Buckley,Riccardo Conci,Peter G. Brodeur,Jason Gusdorf,Sourik Beltrán,Bita Behrouzi,Byron Crowe,Jacob Dockterman,Muzzammil Muhammad,Sarah Ohnigian,Andrew Sanchez,James A. Diao,Aashna P. Shah,Daniel Restrepo,Eric S. Rosenberg,Andrew S. Lea,Marinka Zitnik,Scott H. Podolsky,Zahir Kanjee,Raja-Elie E. Abdulnour,Jacob M. Koshy,Adam Rodman,Arjun K. Manrai*

Main category: cs.AI

TL;DR: 本文基于CPC案例创建CPC - Bench基准评估大语言模型，并开发AI讨论者Dr. CaBot，结果显示LLMs在文本诊断和模拟专家医疗展示上超医生，但图像解读和文献检索较弱，还发布了CaBot和CPC - Bench。


<details>
  <summary>Details</summary>
Motivation: 以往AI评估聚焦最终诊断，未解决专家讨论所需的多方面推理和展示技能问题，需新评估。

Method: 利用7102个CPC案例和1021个图像挑战，进行医生标注和自动处理创建CPC - Bench基准评估大语言模型，开发AI讨论者Dr. CaBot。

Result: o3在最终诊断排名和下一步测试选择上表现好，在图像和文献检索任务表现较差，医生常误判CaBot生成文本，且评分更优。

Conclusion: LLMs在文本诊断和模拟展示上超医生，但图像和文献检索弱，CPC - Bench和CaBot可追踪医疗AI进展。

Abstract: BACKGROUND: For over a century, the New England Journal of Medicine
Clinicopathological Conferences (CPCs) have tested the reasoning of expert
physicians and, recently, artificial intelligence (AI). However, prior AI
evaluations have focused on final diagnoses without addressing the multifaceted
reasoning and presentation skills required of expert discussants.
  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),
we conducted extensive physician annotation and automated processing to create
CPC-Bench, a physician-validated benchmark spanning 10 text-based and
multimodal tasks, against which we evaluated leading large language models
(LLMs). Then, we developed "Dr. CaBot," an AI discussant designed to produce
written and slide-based video presentations using only the case presentation,
modeling the role of the human expert in these cases.
  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the
final diagnosis first in 60% of cases and within the top ten in 84% of cases,
outperforming a 20-physician baseline; next-test selection accuracy reached
98%. Event-level physician annotations quantified AI diagnostic accuracy per
unit of information. Performance was lower on literature search and image
tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image
challenges. In blinded comparisons of CaBot vs. human expert-generated text,
physicians misclassified the source of the differential in 46 of 62 (74%) of
trials, and scored CaBot more favorably across quality dimensions. To promote
research, we are releasing CaBot and CPC-Bench.
  CONCLUSIONS: LLMs exceed physician performance on complex text-based
differential diagnosis and convincingly emulate expert medical presentations,
but image interpretation and literature retrieval remain weaker. CPC-Bench and
CaBot may enable transparent and continued tracking of progress in medical AI.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [51] [Mechanics-Informed Machine Learning for Geospatial Modeling of Soil Liquefaction: Global and National Surrogate Models for Simulation and Near-Real-Time Response](https://arxiv.org/abs/2509.10962)
*Morgan D. Sanger,Mertcan Geyin,Brett W. Maurer*

Main category: cs.CE

TL;DR: 利用机器学习、高性能计算和地理空间信息开发区域尺度土壤液化预测代理模型，模型表现优，适合区域模拟和近实时响应。


<details>
  <summary>Details</summary>
Motivation: 开发能在区域尺度准确预测土壤液化的模型。

Method: 使用机器学习、高性能计算和地理空间信息，训练全球和新西兰特定的两组代理模型，模拟原位测试场地的岩土模型，结合地理统计更新。

Result: 模型显著优于其他模型，地理统计更新提升性能，学习全球大数据集可抵消区域特定模型优势。

Conclusion: 模型适合区域尺度液化灾害模拟和近实时响应，附带方差产品表明本地岩土数据对预测的影响。

Abstract: Using machine learning (ML), high performance computing, and a large body of
geospatial information, we develop surrogate models to predict soil
liquefaction across regional scales. Two sets of models - one global and one
specific to New Zealand - are trained by learning to mimic geotechnical models
at the sites of in-situ tests. Our geospatial approach has conceptual
advantages in that predictions: (i) are anchored to mechanics, which encourages
more sensible response and scaling across the domains of soil, site, and
loading characteristics; (ii) are driven by ML, which allows more predictive
information to be used, with greater potential for it to be exploited; (iii)
are geostatistically updated by subsurface data, which anchors the predictions
to known conditions; and (iv) are precomputed everywhere on earth for all
conceivable earthquakes, which allows the models to be executed very easily,
thus encouraging user adoption and evaluation. Test applications suggest that:
(i) the proposed models outperform others to a statistically significant
degree; (ii) the geostatistical updating further improves performance; and
(iii) the anticipated advantages of region-specific models may largely be
negated by the benefits of learning from larger global datasets. These models
are best suited for regional-scale liquefaction hazard simulation and
near-real-time response and are accompanied by variance products that convey
where, and to what degree, the ML-predicted liquefaction response is influenced
by local geotechnical data.

</details>


### [52] [Geospatial AI for Liquefaction Hazard and Impact Forecasting: A Demonstrative Study in the U.S. Pacific Northwest](https://arxiv.org/abs/2509.10965)
*Morgan D. Sanger,Brett W. Maurer*

Main category: cs.CE

TL;DR: 本文利用机器学习驱动的地理空间模型，对华盛顿和俄勒冈州85个情景地震的液化灾害进行高分辨率的区域预测，并展示了预测结果在资产和网络基础设施脆弱性评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 近期大地震凸显土壤液化危害，美国太平洋西北地区易受影响，需在区域尺度上理解和规划液化灾害。

Method: 采用机器学习驱动的新兴地理空间模型，替代现有岩土模型预测破坏性地面变形概率。

Result: 模型性能优于先前区域尺度建模方法，预测结合力学知识、利用更多地理空间信息并基于已知地下条件进行地质统计定位。

Conclusion: 液化灾害预测结果适用于灾害模拟、疏散路线规划等多种区域尺度应用，且已发布在公共存储库。

Abstract: Recent large-magnitude earthquakes have demonstrated the damaging
consequences of soil liquefaction and reinforced the need to understand and
plan for liquefaction hazards at a regional scale. In the United States, the
Pacific Northwest is uniquely vulnerable to such consequences given the
potential for crustal, intraslab, and subduction zone earthquakes. In this
study, the liquefaction hazard is predicted geospatially at high resolution and
across regional scales for 85 scenario earthquakes in the states of Washington
and Oregon. This is accomplished using an emergent geospatial model that is
driven by machine learning, and which predicts the probability of damaging
ground deformation by surrogating state-of-practice geotechnical models. The
adopted model shows improved performance and has conceptual advantages over
prior regional-scale modeling approaches in that predictions (i) are informed
by mechanics, (ii) employ more geospatial information using machine learning,
and (iii) are geostatistically anchored to known subsurface conditions. The
utility of the resulting predictions for the 85 scenarios is then demonstrated
via asset and network infrastructure vulnerability assessments. The
liquefaction hazard forecasts are published in a GIS-ready, public repository
and are suitable for disaster simulations, evacuation route planning, network
vulnerability analysis, land-use planning, insurance loss modeling, hazard
communication, public investment prioritization, and other regional-scale
applications.

</details>


### [53] [Why "AI" Models for Predicting Soil Liquefaction have been Ignored, Plus Some that Shouldn't Be](https://arxiv.org/abs/2509.10966)
*Brett W. Maurer,Morgan D. Sanger*

Main category: cs.CE

TL;DR: 本文通过回顾75篇文献，探究AI液化模型被从业者和研究者忽视的原因，指出问题并给出改进方向。


<details>
  <summary>Details</summary>
Motivation: AI液化模型在文献中快速发展，但被从业者和研究者广泛忽视，本文旨在探究原因。

Method: 对75篇出版物进行样本回顾。

Result: 发现AI液化模型存在未与现有模型比较、背离最佳实践、AI使用方式可能无用、过度强调复杂性以及不提供模型等问题。

Conclusion: 理解这些反复出现的问题可改进该领域研究方向和认知，还指出了AI更可能发挥价值的应用。

Abstract: Soil liquefaction remains an important and interesting problem that has
attracted the development of enumerable prediction models. Increasingly, these
models are utilizing algorithmic learning, or "artificial intelligence" (AI).
The rapid growth of AI in the liquefaction literature is unsurprising, given
its ease of implementation and potential advantages over traditional
statistical methods. However, AI liquefaction models have been widely ignored
by practitioners and researchers alike; the objective of this paper is to
investigate "why?" Through a sample review of 75 publications, we identify
several good reasons. Namely, these models frequently: (i) are not compared to
state-of-practice models, making it unclear why they should be adopted; (ii)
depart from best practices in model development; (iii) use AI in ways that may
not be useful; (iv) are presented in ways that overstate their complexity and
make them unapproachable; and (v) are discussed but not actually provided,
meaning that no one can use the models even if they wanted to. These prevailing
problems must be understood, identified, and remedied, but this does not mean
that AI itself is problematic, or that all prior efforts have been without
merit or utility. Instead, understanding these recurrent shortcomings can help
improve the direction and perceptions of this growing body of work. Towards
this end, we highlight papers that are generally free from these shortcomings,
and which demonstrate applications where AI is more likely to provide value in
the near term: permitting new modeling approaches and potentially improving
predictions of liquefaction phenomena.

</details>


### [54] [Large language model-empowered next-generation computer-aided engineering](https://arxiv.org/abs/2509.11447)
*Jiachen Guo,Chanwook Park,Dong Qian,Thomas J. R. Hughes,Wing Kam Liu*

Main category: cs.CE

TL;DR: 文章介绍大语言模型（LLMs）在计算机辅助工程（CAE）应用，提出LLM赋能的CAE代理用于无数据模型降阶，结果显示可减少人力并生成高保真降阶模型，凸显其对下一代CAE系统的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在CAE应用多为减少人力，难以应对大规模、高维系统计算挑战，需新方法。

Method: 引入LLM赋能的CAE代理概念，提出用于无数据模型降阶（MOR）的CAE代理，用基于张量分解的先验代理（TAPS）解决超大规模空间 - 参数 - 时间物理问题。

Result: 自然语言提示可转化为高效求解器实现，减少人力并生成高保真降阶模型，LLMs能为未见案例合成新MOR求解器。

Conclusion: LLMs有潜力为下一代CAE系统奠定基础。

Abstract: Software development has entered a new era where large language models (LLMs)
now serve as general-purpose reasoning engines, enabling natural language
interaction and transformative applications across diverse domains. This
paradigm is now extending into computer-aided engineering (CAE). Recent
applications of LLMs in CAE have successfully automated routine tasks,
including CAD model generation and FEM simulations. Nevertheless, these
contributions, which primarily serve to reduce manual labor, are often
insufficient for addressing the significant computational challenges posed by
large-scale, high-dimensional systems. To this aim, we first introduce the
concept of LLM-empowered CAE agent, where LLMs act as autonomous collaborators
that plan, execute, and adapt CAE workflows. Then, we propose an LLM-empowered
CAE agent for data-free model order reduction (MOR), a powerful yet underused
approach for ultra-fast large-scale parametric analysis due to the intrusive
nature and labor-intensive redevelopment of solvers. LLMs can alleviate this
barrier by automating derivations, code restructuring, and implementation,
making intrusive MOR both practical and broadly accessible. To demonstrate
feasibility, we present an LLM-empowered CAE agent for solving
ultra-large-scale space-parameter-time (S-P-T) physical problems using
Tensor-decomposition-based A Priori Surrogates (TAPS). Our results show that
natural language prompts describing parametric partial differential equations
(PDEs) can be translated into efficient solver implementations, substantially
reducing human effort while producing high-fidelity reduced-order models.
Moreover, LLMs can synthesize novel MOR solvers for unseen cases such as
nonlinear and high-dimensional parametric problems based on their internal
knowledge base. This highlights the potential of LLMs to establish the
foundation for next-generation CAE systems.

</details>


### [55] [Toward lean industry 5.0: a human-centered model for integrating lean and industry 4.0 in an automotive supplier](https://arxiv.org/abs/2509.11658)
*Peter Hines,Florian Magnani,Josefa Mula,Raquel Sanchis*

Main category: cs.CE

TL;DR: 本文提出以人為中心的精實與工業4.0整合概念模型，經案例驗證，提供理論與實踐成果，為邁向工業5.0奠定基礎。


<details>
  <summary>Details</summary>
Motivation: 填補現有精實工業4.0實施研究的空白。

Method: 文獻回顧提出概念模型，以五階段多方法案例研究驗證，從運營、社會和技術角度在群組和模型站點層面考慮。

Result: 呈現有效實施樣貌、精實工具數位化方式，突出案例26個積極和10個消極方面及因果關係。

Conclusion: 在適當技術和人力技能下，依概念模型成功實施能使組織和員工受益，邁向精實工業5.0。

Abstract: This paper proposes a human-centered conceptual model integrating lean and
Industry 4.0 based on the literature review and validated it through a case
study in the context of an advanced automotive first-tier supplier. Addressing
a significant gap in existing research on lean Industry 4.0 implementations,
the study provides both theoretical insights and practical findings. It
emphasizes the importance of a human-centered approach, identifies key enablers
and barriers. In the implementation process of the case study, it is considered
at group level and model site level through operational, social and
technological perspectives in a five-phase multi-method approach. It shows what
effective human-centered lean Industry 4.0 implementation look like and how
advanced lean tools can be digitized. It highlights 26 positive and 10 negative
aspects of the case and their causal relation. With the appropriate internal
and external technological knowhow and people skills, it shows how successful
implementation can benefit the organization and employees based on the
conceptual model that serves as a first step toward lean Industry 5.0.

</details>


### [56] [Very-low-field MRI scanners: from the ideal to the real permanent magnet array](https://arxiv.org/abs/2509.11762)
*Umberto Zanovello,Alessandro Arduino,Vittorio Basso,Luca Zilberti,Alessandro Sola,Andrea Agosto,Luca Toso,Oriano Bottauscio*

Main category: cs.CE

TL;DR: 介绍极低场MRI优势及低成本扫描仪发展，聚焦磁体B0空间均匀性，分析影响因素、数值与实际差异及数值模型近似后果。


<details>
  <summary>Details</summary>
Motivation: 极低场MRI因便携和适应不同环境受关注，开发低成本扫描仪，需研究磁体性能。

Method: 研究磁体B0空间均匀性对各因素的敏感性，分析数值预期与实际测量差异，探讨不同数值模型近似的影响。

Result: 发现数值模型近似虽简化模型、缩短计算时间，但影响结果可靠性。

Conclusion: 数值模型近似会影响磁体性能分析结果的可靠性，在研究磁体性能时需谨慎对待。

Abstract: Very-low-field MRIs are becoming increasingly popular due to their
portability and adaptability to different environments. They are being
successfully used for various clinical applications, leading to a paradigm
shift in the way imaging care is typically performed. The development of
low-cost MRI scanner prototypes began a few years ago, with some interesting
and promising open-source projects emerging in both hardware and software
design. Using permanent magnets (PMs) to generate the static magnetic field B0
can substantially reduce the manufacturing cost of low-field scanners while
achieving satisfactory homogeneity. This article focuses on characterizing
magnet performance in terms of B0 spatial homogeneity. Specifically, it
investigates its sensitivity to various factors and explores the reasons for
discrepancies between numerical expectations and actual measurements on
fabricated magnets. The analysis also examines the consequences of using
different numerical model approximations, revisiting concepts most frequently
used in other design contexts. While these assumptions simplify the numerical
model and may improve its performance in terms of computational time, this
paper demonstrates that they also impact the reliability of the obtained
results.

</details>


### [57] [Hetero-EUCLID: Interpretable model discovery for heterogeneous hyperelastic materials using stress-unsupervised learning](https://arxiv.org/abs/2509.11784)
*Kanhaiya Lal Chaurasiya,Saurav Dutta,Siddhant Kumar,Akshay Joshi*

Main category: cs.CE

TL;DR: 提出Hetero - EUCLID计算框架用于异质材料成分分割与参数识别，经实验验证其有效性与适用性。


<details>
  <summary>Details</summary>
Motivation: 为表征异质材料所有成分的超弹性行为，解决异质化公式问题。

Method: 利用Bayesian - EUCLID框架，通过稀疏先验和蒙特卡罗马尔可夫链采样进行模型选择，采用实验可观测的3D表面位移和边界平均力数据，框架包含基于残余力的分割和本构参数识别两步。

Result: 框架能对不同类型的薄方形异质域进行分割和材料表征，在不同位移噪声和非原生网格离散化情况下也有效。

Conclusion: Hetero - EUCLID框架适用于基于数字图像/体积相关的实验场景，可基于单实验数据进行分割和材料表征，适用于航空航天、国防复合材料及医学领域。

Abstract: We propose a computational framework, Hetero-EUCLID, for segmentation and
parameter identification to characterize the full hyperelastic behavior of all
constituents of a heterogeneous material. In this work, we leverage the
Bayesian-EUCLID (Efficient Unsupervised Constitutive Law Identification and
Discovery) framework to efficiently solve the heterogenized formulation through
parsimonious model selection using sparsity-promoting priors and Monte Carlo
Markov Chain sampling. We utilize experimentally observable 3D surface
displacement and boundary-averaged force data generated from Finite Element
simulations of non-equi-biaxial tension tests on heterogeneous specimens. The
framework broadly consists of two steps -- residual force-based segmentation,
and constitutive parameter identification. We validate and demonstrate the
ability of the proposed framework to segment the domain, and characterize the
constituent materials on various types of thin square heterogeneous domains. We
validate of the framework's ability to segment and characterize materials with
various levels of displacement noises and non-native mesh discretizations, i.e,
using different meshes for the forward FE simulations and the inverse EUCLID
problem. This demonstrates Hetero-EUCLID framework's applicability in Digital
Image/Volume Correlation-based experimental scenarios. Furthermore, the
proposed framework performs successful segmentation and material
characterizations based on data from a single experiment, thereby making it
viable for rapid, interpretable model discovery in domains such as aerospace
and defense composites and for characterization of selective tissue stiffening
in medical conditions such as fibroatheroma, atherosclerosis, or cancer.

</details>


### [58] [Numerical analysis of fluid estimation for source terms in neutral particles simulation](https://arxiv.org/abs/2509.11883)
*Zhirui Tang,Emil Løvbak,Julian Koellermeier,Giovanni Samaey*

Main category: cs.CE

TL;DR: 对含流体估计的渐近保持动力学 - 扩散蒙特卡罗（KDMC）方法进行收敛性数值分析，验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型反应堆等离子体边缘模拟中，传统动力学蒙特卡罗方法因高粒子碰撞率导致计算成本高，需评估新方法性能。

Method: 以动力学蒙特卡罗方法为参考，比较含流体估计的KDMC算法与近似流体方法的精度。

Result: 一维测试中，含流体估计的KDMC在高低碰撞区域误差比流体方法低至少一个数量级，且比动力学蒙特卡罗方法有明显加速。

Conclusion: 分析证实了所讨论算法的有效性。

Abstract: In plasma edge simulations, kinetic Monte Carlo (MC) is often used to
simulate neutral particles and estimate source terms. For large-sized reactors,
like ITER and DEMO, high particle collision rates lead to a substantial
computational cost for such schemes. To address this challenge, an
asymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation method
and a corresponding fluid estimation technique have been proposed in the
literature. In this work, we perform numerical analysis on the convergence of
KDMC with the fluid estimation. To do so, we compare the accuracy of the
analyzed algorithm with the accuracy of an approximate fluid method using the
kinetic MC method as a reference. In a one-dimensional test case, KDMC with the
fluid estimation achieves at least one order of magnitude lower errors than the
fluid method for both high- and low-collisional regimes. Moreover, KDMC with
the fluid estimation outperforms the kinetic MC method with a clear speed-up.
Overall, our analysis confirms the effectiveness of the discussed algorithm.

</details>


### [59] [FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval](https://arxiv.org/abs/2509.12042)
*Ying Li,Mengyu Wang,Miguel de Carvalho,Sotirios Sabanis,Tiejun Ma*

Main category: cs.CE

TL;DR: 提出适用于金融文档的检索框架FinGEAR，结合金融词典、双层次索引和两阶段交叉编码器重排器，评估显示性能显著提升，为金融分析提供基础。


<details>
  <summary>Details</summary>
Motivation: 金融披露文件检索难题，标准检索增强生成模型利用不足。

Method: 引入FinGEAR框架，结合金融词典、双层次索引和两阶段交叉编码器重排器。

Result: 在全量10 - K文件评估中，FinGEAR在多项指标上有一致提升，提高下游答案准确性。

Conclusion: 联合建模层次结构和领域词典信号，FinGEAR提高检索保真度，为金融分析提供实用基础。

Abstract: Financial disclosures such as 10-K filings present challenging retrieval
problems due to their length, regulatory section hierarchy, and domain-specific
language, which standard retrieval-augmented generation (RAG) models underuse.
We introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a
retrieval framework tailored to financial documents. FinGEAR combines a finance
lexicon for Item-level guidance (FLAM), dual hierarchical indices for
within-Item search (Summary Tree and Question Tree), and a two-stage
cross-encoder reranker. This design aligns retrieval with disclosure structure
and terminology, enabling fine-grained, query-aware context selection.
Evaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR
delivers consistent gains in precision, recall, F1, and relevancy, improving F1
by up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over
prior tree-based systems, while also increasing downstream answer accuracy with
a fixed reader. By jointly modeling section hierarchy and domain lexicon
signals, FinGEAR improves retrieval fidelity and provides a practical
foundation for high-stakes financial analysis.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [60] [Dynamic read & write optimization with TurtleKV](https://arxiv.org/abs/2509.10714)
*Tony Astolfi,Vidya Silai,Darby Huye,Lan Liu,Raja R. Sambasivan,Johes Bater*

Main category: cs.DB

TL;DR: 论文提出TurtleKV，通过动态用内存换读写性能，在YCSB测试中比RocksDB和SplinterDB有更好表现。


<details>
  <summary>Details</summary>
Motivation: 传统键值存储在读写性能和内存使用上存在权衡，现有方法限制了数据库最大性能和动态调优能力。

Method: 提出TurtleKV，采用新颖无偏数据结构用于磁盘存储，通过旋钮动态增加内存以提升读写性能。

Result: 在YCSB测试中，TurtleKV写吞吐量最高达RocksDB的8倍，读吞吐量最高达5倍；比SplinterDB在点查询上好40%，范围扫描上好6倍，空间放大率低50%。

Conclusion: TurtleKV通过动态内存交换策略，在读写性能和空间利用上有更好表现。

Abstract: High read and write performance is important for generic key/value stores,
which are fundamental to modern applications and databases. Yet, achieving high
performance for both reads and writes is challenging due to traditionally
limited memory and the pick-any-two-out-of-three tradeoff between memory use,
read performance, and write performance. Existing state-of-the-art approaches
limit memory usage and chose a primary dimension (reads or writes) for which to
optimize their on-disk structures. They recover performance in the remaining
dimension by other mechanisms. This approach limits databases' maximum
performance in the remaining dimension and their dynamic (online) tunability to
respond to changing workloads. We explore a different approach that dynamically
trades memory for read or write performance as needed. We present TurtleKV,
which includes a novel unbiased data structure for on-disk storage. It includes
a knob that dynamically increases memory reserved for increasing read or write
performance. When evaluated on YCSB, TurtleKV achieves up to 8x the write
throughput of industry-leader RocksDB and up to 5x the read throughput while
incurring similar space amplification. Compared to the state-of-the-art system
SplinterDB, TurtleKV runs up to 40% better on point queries, up to 6x better on
range scans and achieves similar write performance, while incurring 50% less
space amplification.

</details>


### [61] [The Space-Time Complexity of Sum-Product Queries](https://arxiv.org/abs/2509.11920)
*Kyle Deeds,Timo Camillo Merkl,Reinhard Pichler,Dan Suciu*

Main category: cs.DB

TL;DR: 研究联合查询和和积查询的时空复杂度，提出高效空间算法，能以更低空间复杂度达最优时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 以往查询评估研究多关注时间复杂度，忽略了空间复杂度，在有严格空间约束场景下是挑战。

Method: 提出几类用于评估和积查询的高效空间算法。

Result: 最优时间复杂度几乎总能以比传统方法渐近更低的空间复杂度实现。

Conclusion: 所提算法在查询评估的时空复杂度平衡上有优势。

Abstract: While extensive research on query evaluation has achieved consistent
improvements in the time complexity of algorithms, the space complexity of
query evaluation has been largely ignored. This is a particular challenge in
settings with strict pre-defined space constraints. In this paper, we examine
the combined space-time complexity of conjunctive queries (CQs) and, more
generally, of sum-product queries (SPQs). We propose several classes of
space-efficient algorithms for evaluating SPQs, and we show that the optimal
time complexity is almost always achievable with asymptotically lower space
complexity than traditional approaches.

</details>


### [62] [Query Answering under Volume-Based Diversity Functions](https://arxiv.org/abs/2509.11929)
*Marcelo Arenas,Timo Camillo Merkl,Reinhard Pichler,Cristian Riveros*

Main category: cs.DB

TL;DR: 提出基于体积而非距离计算元组多样性的新方法，可计算(1 - 1/e)近似值并找到多项式时间计算条件。


<details>
  <summary>Details</summary>
Motivation: 现有基于距离的元组多样性度量存在不直观行为，寻找固定大小最大多样性子集难且近似方法少。

Method: 引入基于体积计算元组多样性的新方法，给出定义基于体积的多样性函数框架。

Result: 可对任何基于体积的多样性函数计算(1 - 1/e)近似值，找到多项式时间计算(1 - 1/e)近似值的一般条件。

Conclusion: 基于体积计算元组多样性的方法在解决多样性查询回答问题上有一定优势和可行性。

Abstract: When query evaluation produces too many tuples, a new approach in query
answering is to retrieve a diverse subset of them. The standard approach for
measuring the diversity of a set of tuples is to use a distance function
between tuples, which measures the dissimilarity between them, to then
aggregate the pairwise distances of the set into a score (e.g., by using sum or
min aggregation). However, as we will point out in this work, the resulting
diversity measures may display some unintuitive behavior. Moreover, even in
very simple settings, finding a maximally diverse subset of the answers of
fixed size is, in general, intractable and little is known about approximations
apart from some hand-picked distance-aggregator pairs.
  In this work, we introduce a novel approach for computing the diversity of
tuples based on volume instead of distance. We present a framework for defining
volume-based diversity functions and provide several examples of these measures
applied to relational data. Although query answering of conjunctive queries
(CQ) under this setting is intractable in general, we show that one can always
compute a (1-1/e)-approximation for any volume-based diversity function.
Furthermore, in terms of combined complexity, we connect the evaluation of CQs
under volume-based diversity functions with the ranked enumeration of
solutions, finding general conditions under which a (1-1/e)-approximation can
be computed in polynomial time.

</details>


### [63] [SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation](https://arxiv.org/abs/2509.12086)
*Hui Li,Shiyuan Deng,Xiao Yan,Xiangyu Zhi,James Cheng*

Main category: cs.DB

TL;DR: 提出新的向量量化方法SAQ，优化维度分割和比特分配，实验显示其优于经典和现有方法，减少量化误差、加速编码。


<details>
  <summary>Details</summary>
Motivation: 现有向量量化方法在平衡编码效率和量化精度方面存在挑战，需要改进。

Method: 采用新的维度分割技术划分PCA投影向量，用动态规划算法优化分割和比特分配；设计代码调整技术，先独立量化各维度，再用类坐标下降法优化。

Result: SAQ比经典和现有方法更优，相比Extended RabitQ，量化误差最多降低80%，编码速度提升超80倍。

Conclusion: SAQ能有效平衡编码效率和量化精度，在向量量化方面表现出色。

Abstract: Approximate Nearest Neighbor Search (ANNS) plays a critical role in
applications such as search engines, recommender systems, and RAG for LLMs.
Vector quantization (VQ), a crucial technique for ANNS, is commonly used to
reduce space overhead and accelerate distance computations. However, despite
significant research advances, state-of-the-art VQ methods still face
challenges in balancing encoding efficiency and quantization accuracy. To
address these limitations, we propose a novel VQ method called SAQ. To improve
accuracy, SAQ employs a new dimension segmentation technique to strategically
partition PCA-projected vectors into segments along their dimensions. By
prioritizing leading dimension segments with larger magnitudes, SAQ allocates
more bits to high-impact segments, optimizing the use of the available space
quota. An efficient dynamic programming algorithm is developed to optimize
dimension segmentation and bit allocation, ensuring minimal quantization error.
To speed up vector encoding, SAQ devises a code adjustment technique to first
quantize each dimension independently and then progressively refine quantized
vectors using a coordinate-descent-like approach to avoid exhaustive
enumeration. Extensive experiments demonstrate SAQ's superiority over classical
methods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,
Extended RabitQ). SAQ achieves up to 80% reduction in quantization error and
accelerates encoding speed by over 80x compared to Extended RabitQ.

</details>


### [64] [Towards a Standard for JSON Document Databases](https://arxiv.org/abs/2509.12189)
*Elena Botoeva,Julien Corman*

Main category: cs.DB

TL;DR: 本文对MongoDB聚合框架进行形式化，旨在为JSON文档数据库查询制定行业标准，给出语法、语义、代数变换等。


<details>
  <summary>Details</summary>
Motivation: 识别一个可作为JSON文档数据库查询行业标准起点的片段。

Method: 为选定的操作符提供语法和形式语义，展示该片段与已知关系查询语言的联系，解释语义与MongoDB当前实现的差异并说明选择理由，提供代数变换。

Result: 完成了对MongoDB聚合框架的形式化，明确了相关语法、语义和代数变换。

Conclusion: 所做的形式化工作有助于推动JSON文档数据库查询的行业标准化。

Abstract: In this technical report, we present a formalisation of the MongoDB
aggregation framework. Our aim is to identify a fragment that could serve as
the starting point for an industry-wide standard for querying JSON document
databases. We provide a syntax and formal semantics for a set of selected
operators, We show how this fragment relates to known relational query
languages. We explain how our semantics differs from the current implementation
of MongoDB, and justify our choices. We provide a set of algebraic
transformations that can be used for query optimisation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [65] [Asynchronous Gathering of Opaque Robots with Mobility Faults](https://arxiv.org/abs/2509.10711)
*Subhajit Pramanick,Saswata Jana,Partha Sarathi Mandal,Gokarna Sharma*

Main category: cs.DC

TL;DR: 本文研究异步环境下含移动故障的机器人聚集问题，在LUMI模型中得出四个结果，包括(2,1)移动故障系统的不可解性和用三色灯的解决方案，以及(N,f)移动故障系统的两个有时间 - 颜色权衡的确定性算法。


<details>
  <summary>Details</summary>
Motivation: 此前关于机器人聚集问题在不同故障模型和同步条件下有诸多结果，本文考虑新的移动故障模型，以解决相关聚集问题。

Method: 针对(2,1)移动故障系统，先证明用两色灯无法解决聚集问题，再给出三色灯的解决方案；对于(N,f)移动故障系统，设计两个确定性算法并分析时间复杂度。

Result: 证明(2,1)移动故障系统用两色灯不可解，给出三色灯的最优解；给出(N,f)移动故障系统的两个确定性算法，有时间 - 颜色权衡，在l, f = O(1)时结果最优，算法能承受遮挡可见性和异步调度。

Conclusion: 在新的移动故障模型下，解决了机器人聚集问题，给出的算法在时间复杂度和应对复杂情况上有一定优势。

Abstract: We consider the fundamental benchmarking problem of gathering in an
$(N,f)$-fault system consisting of $N$ robots, of which at most $f$ might fail
at any execution, under asynchrony. Two seminal results established
impossibility of a solution in the oblivious robot (OBLOT) model in a
$(2,0)$-fault system under semi-synchrony and in a $(3,1)$-Byzantine fault
system under asynchrony. Recently, a breakthrough result circumvented the first
impossibility result by giving a deterministic algorithm in a $(2,0)$-fault
system under asynchrony in the luminous robot (LUMI) model using 2-colored
lights. However, a breakthrough result established impossibility of gathering
in a $(2,1)$-crash system in the LUMI model under semi-synchrony. In this
paper, we consider a {\em mobility fault} model in which a robot crash only
impacts it mobility but not the operation of the light.
  We establish four results under asynchrony in LUMI with the mobility fault
model. We show that it is impossible to solve gathering in a $(2,1)$-mobility
fault system using 2-colored lights, and then give a solution using 3-colored
lights, which is optimal w.r.t. the number of colors. We then consider an
$(N,f)$-mobility fault system, $f<N$, both $N,f$ not known, and give two
deterministic algorithms that exhibit a nice time-color trade-off: The first
with time $O(N)$ using 7-colored lights and the second with time
$O(\max\{\ell,f\})$ using 26-colored lights, where $\ell< N$ is the number of
distinct convex layers of robot positions in the initial configuration.
Interestingly, for $l, f = O(1)$, our result is optimal. Our algorithms for an
$(N,f)$-mobility fault system are the first to be analysed time complexity, can
withstand obstructed visibility (opaque robot model) and asynchronous
scheduling.

</details>


### [66] [Coordinated Reinforcement Learning Prefetching Architecture for Multicore Systems](https://arxiv.org/abs/2509.10719)
*Mohammed Humaid Siddiqui,Fernando Guzman,Yufei Wu,Ruishu Ann*

Main category: cs.DC

TL;DR: 提出适用于多核系统的CRL - Pythia预取器，减少冗余请求，提升性能且开销适中。


<details>
  <summary>Details</summary>
Motivation: 传统预取器在多核架构下有冗余请求和带宽浪费问题，前沿预取器在多核扩展时有性能损失。

Method: 提出基于协调强化学习的CRL - Pythia预取器，实现跨核心信息共享和协作预取决策。

Result: CRL - Pythia在所有情况下都优于单Pythia配置，带宽受限工作负载IPC提升约12%，硬件开销适中。

Conclusion: CRL - Pythia是当代多核系统实用且高效的解决方案。

Abstract: Hardware prefetching is critical to fill the performance gap between CPU
speeds and slower memory accesses. With multicore architectures becoming
commonplace, traditional prefetchers are severely challenged. Independent core
operation creates significant redundancy (up to 20% of prefetch requests are
duplicates), causing unnecessary memory bus traffic and wasted bandwidth.
Furthermore, cutting-edge prefetchers such as Pythia suffer from about a 10%
performance loss when scaling from a single-core to a four-core system. To
solve these problems, we propose CRL-Pythia, a coordinated reinforcement
learning based prefetcher specifically designed for multicore systems. In this
work, CRL-Pythia addresses these issues by enabling cross-core sharing of
information and cooperative prefetching decisions, which greatly reduces
redundant prefetch requests and improves learning convergence across cores. Our
experiments demonstrate that CRL-Pythia outperforms single Pythia
configurations in all cases, with approximately 12% IPC (instructions per
cycle) improvement for bandwidth-constrained workloads, while imposing moderate
hardware overhead. Our sensitivity analyses also verify its robustness and
scalability, thereby making CRL-Pythia a practical and efficient solution to
contemporary multicore systems.

</details>


### [67] [MinatoLoader: Accelerating Machine Learning Training Through Efficient Data Preprocessing](https://arxiv.org/abs/2509.10712)
*Rahma Nouaji,Stella Bitchebe,Ricardo Macedo,Oana Balmau*

Main category: cs.DC

TL;DR: 现有数据加载器浪费GPU资源，本文提出MinatoLoader，可加速训练并提高GPU利用率，在多GPU服务器上效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有数据加载器因未考虑样本预处理时间差异，导致GPU资源浪费和训练延迟，需要改进。

Method: 设计适用于单服务器多GPU的MinatoLoader，在后台持续准备数据，优先处理快速样本，并行处理慢速样本。

Result: 在有四个A100 GPU的机器上，MinatoLoader使多种工作负载的训练时间最多提高7.5倍，平均提高3.6倍；GPU平均利用率从46.4%提高到90.45%，且不影响模型精度。

Conclusion: MinatoLoader能有效加速训练、提高GPU利用率，同时保证模型精度和更快收敛。

Abstract: Data loaders are used by Machine Learning (ML) frameworks like PyTorch and
TensorFlow to apply transformations to data before feeding it into the
accelerator. This operation is called data preprocessing. Data preprocessing
plays an important role in the ML training workflow because if it is
inefficiently pipelined with the training, it can yield high GPU idleness,
resulting in important training delays. Unfortunately, existing data loaders
turn out to waste GPU resources, with $76\%$ GPU idleness when using the
PyTorch data loader, for example. One key source of inefficiency is the
variability in preprocessing time across samples within the same dataset.
Existing data loaders are oblivious to this variability, and they construct
batches without any consideration of slow or fast samples. In this case, the
entire batch is delayed by a single slow sample, stalling the training pipeline
and resulting in head-of-line blocking.
  To address these inefficiencies, we present MinatoLoader, a general-purpose
data loader for PyTorch that accelerates training and improves GPU utilization.
MinatoLoader is designed for a single-server setup, containing multiple GPUs.
It continuously prepares data in the background and actively constructs batches
by prioritizing fast-to-preprocess samples, while slower samples are processed
in parallel.
  We evaluate MinatoLoader on servers with V100 and A100 GPUs. On a machine
with four A100 GPUs, MinatoLoader improves the training time of a wide range of
workloads by up to $7.5\times$ ($3.6\times$ on average) over PyTorch DataLoader
and Pecan, and up to $3\times$ ($2.2\times$ on average) over DALI. It also
increases average GPU utilization from 46.4\% with PyTorch to 90.45\%, while
preserving model accuracy and enabling faster convergence.

</details>


### [68] [Enhancing Type Safety in MPI with Rust: A Statically Verified Approach for RSMPI](https://arxiv.org/abs/2509.10803)
*Nafees Iqbal,Jed Brown*

Main category: cs.DC

TL;DR: 本文介绍基于RSMPI库的MPI类型安全通信框架，利用TypedCommunicator实现点对点通信静态类型安全，消除常见MPI错误，提高开发效率并保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统MPI低级别接口且缺乏内置类型安全，易导致运行时错误、未定义行为和调试难题，本文旨在解决这些局限。

Method: 基于RSMPI库构建类型安全通信框架，核心是TypedCommunicator，利用Rust的Equivalence特性保证通信类型兼容，支持单值和切片通信。

Result: 该框架消除常见MPI错误，提高开发者生产力，保持性能。

Conclusion: 此工作为将类型安全扩展到集体操作奠定基础，提升Rust并行计算的健壮性。

Abstract: The Message Passing Interface (MPI) is a fundamental tool for building
high-performance computing (HPC) applications, enabling efficient communication
across distributed systems. Despite its widespread adoption, MPI's low-level
interface and lack of built-in type safety make it prone to runtime errors,
undefined behavior, and debugging challenges, especially in large-scale
applications. Rust, a modern systems programming language, offers a compelling
solution with its strong type system, which enforces memory and type safety at
compile time without compromising performance. This paper introduces a
type-safe communication framework for MPI, built on the RSMPI library, to
address the limitations of traditional MPI programming. At its core is the
TypedCommunicator, an abstraction that enforces static type safety in
point-to-point communication operations. By leveraging Rust's Equivalence
trait, our framework guarantees that only compatible types can participate in
communication, catching mismatches either at compile time or through runtime
validation. The framework supports both single-value and slice-based
communication, providing an intuitive API for diverse data structures. Our
implementation demonstrates that this approach eliminates common MPI errors,
improves developer productivity, and maintains performance, adhering to Rust's
principle of zero-cost abstractions. This work lays the foundation for
extending type safety to collective operations, advancing the robustness of
parallel computing in Rust.

</details>


### [69] [Chameleon: Taming Dynamic Operator Sequences for Memory-Intensive LLM Training](https://arxiv.org/abs/2509.11076)
*Zibo Wang,Yuhang Zhou,Zhibin Wang,Shipeng Li,Xinjing Huang,Chendong Cai,Bingxu Mu,Yuqing Sun,Zhiheng Hu,Bin She,Shu You,Guanghuan Fang,Rong Gu,Wanchun Dou,Guihai Chen,Chen Tian*

Main category: cs.DC

TL;DR: 现有交换方法在Eager模式下不实用，本文提出Chameleon，重新设计交换式内存优化的端到端流程，实验显示其能减少分析开销、适应算子序列变化并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练内存需求激增，现有交换方法在Eager模式下因假设算子序列一致而不实用。

Method: 提出Chameleon，引入轻量级在线分析器监控算子序列，在有限算子信息下生成有效交换策略，优化策略执行模块。

Result: Chameleon减少分析开销84.25%，能训练比硬件内存大4倍的模型，性能比重新计算或高度并行提升最多38.94%。

Conclusion: Chameleon能有效解决Eager模式下交换式内存优化问题，适应算子序列变化并提升性能。

Abstract: The increasing size of large language models (LLMs) has led to a surge in
memory requirements during training, often exceeding the capacity of
high-bandwidth memory (HBM). Swap-based memory optimization incurs neither
accuracy loss nor additional end-to-end overhead when effectively overlapped,
thus being an attractive solution. However, existing swap methods assume
consistent operator sequences, which is impractical in Eager Mode, where
operator sequences can vary during change.
  We propose Chameleon, which redesigns the end-to-end process of swap-based
memory optimization and is the first work to consider varying operator
sequences in Eager Mode. Chameleon (i) introduces a lightweight online profiler
to enable continuous profiling for monitoring operator sequences, (ii)
generates effective swap policies with limited operator information, and (iii)
optimizes the policy execution module for accurate policy application and
better performance. Experimental results demonstrate that Chameleon reduces
profiling overhead by 84.25%, enables training models up to 4x larger than
hardware memory while adapting to changes in operator sequences, improves
performance by up to 38.94% compared to recomputation or high-degree
parallelism.

</details>


### [70] [GFS: A Preemption-aware Scheduling Framework for GPU Clusters with Predictive Spot Instance Management](https://arxiv.org/abs/2509.11134)
*Jiaang Duan,Shenglin Xu,Shiyou Qian,Dingyu Yang,Kangjin Wang,Chenzhi Liao,Yinghao Yu,Qin Hua,Hanwen Hu,Qi Wang,Wenchao Wu,Dongqing Bao,Tianyu Lu,Jian Cao,Guangtao Xue,Guodong Yang,Liping Zhang,Gang Chen*

Main category: cs.DC

TL;DR: 本文提出GFS调度框架，利用预测模型、动态分配机制和预占调度策略，有效降低低优先级任务的驱逐率和排队延迟，提升GPU分配率并带来经济收益。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使GPU使用模式改变，现有调度器存在高驱逐率和长排队时间问题，需更高效管理策略。

Method: 提出GFS框架，利用轻量级预测模型预测GPU需求，采用动态分配机制调整低优先级任务的现货配额，结合预占调度策略优先处理高优先级任务。

Result: GFS使低优先级任务的驱逐率降低33.0%，排队延迟减少44.1%，在实际生产集群中GPU分配率提升达22.8%，在超10000个GPU的集群中每月带来约459715美元收益。

Conclusion: GFS能有效提升服务级别目标合规性，减少对低优先级任务的影响，提高GPU资源利用率并带来经济价值。

Abstract: The surge in large language models (LLMs) has fundamentally reshaped the
landscape of GPU usage patterns, creating an urgent need for more efficient
management strategies. While cloud providers employ spot instances to reduce
costs for low-priority (LP) tasks, existing schedulers still grapple with high
eviction rates and lengthy queuing times. To address these limitations, we
present GFS, a novel preemptive scheduling framework that enhances
service-level objective (SLO) compliance for high-priority (HP) tasks while
minimizing preemptions to LP tasks. Firstly, GFS utilizes a lightweight
forecasting model that predicts GPU demand among different tenants, enabling
proactive resource management. Secondly, GFS employs a dynamic allocation
mechanism to adjust the spot quota for LP tasks with guaranteed durations.
Lastly, GFS incorporates a preemptive scheduling policy that prioritizes HP
tasks while minimizing the impact on LP tasks. We demonstrate the effectiveness
of GFS through both real-world implementation and simulations. The results show
that GFS reduces eviction rates by 33.0\%, and cuts queuing delays by 44.1\%
for LP tasks. Furthermore, GFS enhances the GPU allocation rate by up to 22.8\%
in real production clusters. In a production cluster of more than 10,000 GPUs,
GFS yields roughly \$459,715 in monthly benefits.

</details>


### [71] [Linear Complexity $\mathcal{H}^2$ Direct Solver for Fine-Grained Parallel Architectures](https://arxiv.org/abs/2509.11152)
*Wajih Boukaram,David Keyes,Sherry Li,Yang Liu,George Turkiyyah*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present factorization and solution phases for a new linear complexity
direct solver designed for concurrent batch operations on fine-grained parallel
architectures, for matrices amenable to hierarchical representation. We focus
on the strong-admissibility-based $\mathcal{H}^2$ format, where strong
recursive skeletonization factorization compresses remote interactions. We
build upon previous implementations of $\mathcal{H}^2$ matrix construction for
efficient factorization and solution algorithm design, which are illustrated
graphically in stepwise detail. The algorithms are ``blackbox'' in the sense
that the only inputs are the matrix and right-hand side, without analytical or
geometrical information about the origin of the system. We demonstrate linear
complexity scaling in both time and memory on four representative families of
dense matrices up to one million in size. Parallel scaling up to 16 threads is
enabled by a multi-level matrix graph coloring and avoidance of dynamic memory
allocations thanks to prefix-sum memory management. An experimental backward
error analysis is included. We break down the timings of different phases,
identify phases that are memory-bandwidth limited, and discuss alternatives for
phases that may be sensitive to the trend to employ lower precisions for
performance.

</details>


### [72] [Adaptive K-PackCache: Cost-Centric Data Caching in Cloud](https://arxiv.org/abs/2509.11156)
*Suvarthi Sarkar,Aadarshraj Sah,Poddutoori Sweeya Reddy,Aryabartta Sahu*

Main category: cs.DC

TL;DR: 本文将缓存打包概念从2项扩展到K项，提出在线算法AKPC，在Netflix和Spotify数据集上评估，降低成本且接近最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作仅探索2项成对打包，为提高灵活性和性能，将概念扩展到一般K打包。

Method: 从CDN运营商角度提出K PackCache问题，提出在线算法AKPC，基于用户访问模式和内容相关性动态形成、合并和拆分数据团。

Result: 在Netflix和Spotify数据集上，AKPC分别比在线基线降低总成本达63%和55%，性能分别达到最优的15%和13%。

Conclusion: AKPC对现实世界缓存系统具有可扩展性和有效性。

Abstract: Recent advances in data analytics have enabled the accurate prediction of
user access patterns, giving rise to the idea of packed caching delivering
multiple co accessed data items together as a bundle. This improves caching
efficiency, as accessing one item often implies the need for others. Prior work
has explored only 2 item pairwise packing. In this paper, we extend the concept
to general K packing, allowing variable size bundles for improved flexibility
and performance. We formulate the K PackCache problem from a content delivery
network CDN operator perspective, aiming to minimize total cost comprising two
components: transfer cost modeled as a base cost plus a linearly increasing
term with the number of items packed, and memory rental cost for caching, which
depends on how long and how much is stored. Overpacking increases cost due to
low utility, underpacking leads to missed sharing opportunities. We propose an
online algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,
and splits data cliques based on user access patterns and content correlation.
Our approach supports batch requests, enables approximate clique merging, and
offers a formal competitive guarantee. Through extensive evaluation on the
Netflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55
percentage over online baselines, respectively, and achieves performance within
15 and 13 percentage of the optimal. This demonstrates its scalability and
effectiveness for real world caching systems.

</details>


### [73] [Energy-Efficient Joint Offloading and Resource Allocation for Deadline-Constrained Tasks in Multi-Access Edge Computing](https://arxiv.org/abs/2509.11162)
*Chuanchao Gao,Arvind Easwaran*

Main category: cs.DC

TL;DR: 本文研究多接入边缘计算中带截止时间约束的任务卸载与资源分配问题，提出基于图匹配的近似算法GMA，实验表明其节能效果平均达最优值的97%。


<details>
  <summary>Details</summary>
Motivation: 在考虑任务截止时间和系统资源约束的情况下，确定任务卸载位置和资源分配，以最大化物联网设备的总节能。

Method: 将问题建模为整数非线性规划问题，证明其为NP难问题，提出基于图匹配的近似算法GMA，利用线性松弛、三方图构建和线性规划舍入技术。

Result: 证明GMA是(1 - α)/(2 + ε)-近似算法，实验显示其节能效果平均达最优值的97%。

Conclusion: 提出的GMA算法在解决多接入边缘计算中带截止时间约束的任务卸载与资源分配问题上表现良好，能有效实现节能。

Abstract: This paper addresses the deadline-constrained task offloading and resource
allocation problem in multi-access edge computing. We aim to determine where
each task is offloaded and processed, as well as corresponding communication
and computation resource allocations, to maximize the total saved energy for
IoT devices, while considering task deadline and system resource constraints.
Especially, our system allows each task to be offloaded to one of its
accessible access points (APs) and processed on a server that is not co-located
with its offloading AP. We formulate this problem as an Integer Nonlinear
Programming problem and show it is NP-Hard. To address this problem, we propose
a Graph-Matching-based Approximation Algorithm ($\mathtt{GMA}$), the first
approximation algorithm of its kind. $\mathtt{GMA}$ leverages linear
relaxation, tripartite graph construction, and a Linear Programming rounding
technique. We prove that $\mathtt{GMA}$ is a
$\frac{1-\alpha}{2+\epsilon}$-approximation algorithm, where $\epsilon$ is a
small positive value, and $\alpha$ ($0$$\le$$\alpha$$<$$1$) is a system
parameter that ensures the resource allocated to any task by an AP or a server
cannot exceed $\alpha$ times its resource capacity. Experiments show that, in
practice, $\mathtt{GMA}$'s energy saving achieves $97\%$ of the optimal value
on average.

</details>


### [74] [Parallel/Distributed Tabu Search for Scheduling Microprocessor Tasks in Hybrid Flowshop](https://arxiv.org/abs/2509.11396)
*Adam Janiak,Damian Kowalczyk,Maciej Lichtenstein*

Main category: cs.DC

TL;DR: 本文研究带多处理器任务的混合流水车间调度问题的最小化完工时间，提出基于禁忌搜索技术的算法。


<details>
  <summary>Details</summary>
Motivation: 解决带多处理器任务的混合流水车间调度问题中最小化完工时间的问题。

Method: 基于禁忌搜索技术，使用并行和分布式机制进行邻域评估，并平衡异构网络环境。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: The paper deals with the makespan minimization in the hybrid flow shop
scheduling problem with multiprocessor tasks. The hybrid flow shop (HFS)
generalizes the classical flow shop processor configuration by replacing each
processor (processing stage) by some number of identical parallel processors.
Similarly, the multiprocessor tasks generalize the classical assumption, by
allowing a task to require more than one processor simultaneously for its
processing. In this work we present the algorithm for solving the problem based
on the tabu search technique. The proposed algorithm uses parallel and
distributed mechanisms for neighborhood evaluation and well balances
heterogeneous network environment.

</details>


### [75] [Machine Learning-Driven Predictive Resource Management in Complex Science Workflows](https://arxiv.org/abs/2509.11512)
*Tasnuva Chowdhury,Tadashi Maeno,Fatih Furkan Akman,Joseph Boudreau,Sankha Dutta,Shengyu Feng,Adolfy Hoisie,Kuan-Chieh Hsu,Raees Khan,Jaehyung Kim,Ozgur O. Kilic,Scott Klasky,Alexei Klimentov,Tatiana Korchuganova,Verena Ingrid Martinez Outschoorn,Paul Nilsson,David K. Park,Norbert Podhorszki,Yihui Ren,John Rembrandt Steele,Frédéric Suter,Sairam Sri Vatsavai,Torre Wenaus,Wei Yang,Yiming Yang,Shinjae Yoo*

Main category: cs.DC

TL;DR: 本文聚焦科学实验数据处理资源需求估计难题，提出在PanDA系统中引入机器学习模型预测资源需求以提升工作流管理效率。


<details>
  <summary>Details</summary>
Motivation: 科学实验数据处理复杂，资源需求估计因分析场景多、人员技能差异和计算选项增加而困难，现有两阶段方法有缺陷。

Method: 在PanDA系统中构建机器学习模型管道，运用先进机器学习技术预测关键资源需求。

Result: 模型能准确预测资源需求，实现工作流管理的主动决策。

Conclusion: 准确预测资源需求可提高跨异构资源处理复杂多样工作流的效率。

Abstract: The collaborative efforts of large communities in science experiments, often
comprising thousands of global members, reflect a monumental commitment to
exploration and discovery. Recently, advanced and complex data processing has
gained increasing importance in science experiments. Data processing workflows
typically consist of multiple intricate steps, and the precise specification of
resource requirements is crucial for each step to allocate optimal resources
for effective processing. Estimating resource requirements in advance is
challenging due to a wide range of analysis scenarios, varying skill levels
among community members, and the continuously increasing spectrum of computing
options. One practical approach to mitigate these challenges involves initially
processing a subset of each step to measure precise resource utilization from
actual processing profiles before completing the entire step. While this
two-staged approach enables processing on optimal resources for most of the
workflow, it has drawbacks such as initial inaccuracies leading to potential
failures and suboptimal resource usage, along with overhead from waiting for
initial processing completion, which is critical for fast-turnaround analyses.
In this context, our study introduces a novel pipeline of machine learning
models within a comprehensive workflow management system, the Production and
Distributed Analysis (PanDA) system. These models employ advanced machine
learning techniques to predict key resource requirements, overcoming challenges
posed by limited upfront knowledge of characteristics at each step. Accurate
forecasts of resource requirements enable informed and proactive
decision-making in workflow management, enhancing the efficiency of handling
diverse, complex workflows across heterogeneous resources.

</details>


### [76] [Towards the Distributed Large-scale k-NN Graph Construction by Graph Merge](https://arxiv.org/abs/2509.11697)
*Cheng Zhang,Wan-Lei Zhao,Shihai Xiao,Jiajie Yao,Xuecang Zhang*

Main category: cs.DC

TL;DR: 本文提出高效图合并方法解决大规模多媒体向量数据的k - NN图或索引图构建问题，单节点和多节点实验均有良好效果。


<details>
  <summary>Details</summary>
Motivation: 为支持与大语言模型实时交互、社交媒体即时搜索或推荐，需解决大规模多媒体向量数据的图构建问题，且数据规模可能超单机处理能力。

Method: 单节点图构建提出Two - way Merge和Multi - way Merge算法合并子图；多节点图构建提出基于Two - way Merge的多节点过程。

Result: k - NN图构建中，并行图合并可构建大规模高质量图，三节点约17小时可构建十亿规模图；索引图构建中，合并后的图搜索性能与原图相似，构建时间大幅减少。

Conclusion: 所提方法使在单节点或多节点上构建大规模k - NN图/索引图成为可能，即使数据超单节点内存容量。

Abstract: In order to support the real-time interaction with LLMs and the instant
search or the instant recommendation on social media, it becomes an imminent
problem to build k-NN graph or indexing graph for the massive number of
vectorized multimedia data. In such scenarios, the scale of the data or the
scale of the graph may exceed the processing capacity of a single machine. This
paper aims to address the graph construction problem of such scale via
efficient graph merge. For the graph construction on a single node, two generic
and highly parallelizable algorithms, namely Two-way Merge and Multi-way Merge
are proposed to merge subgraphs into one. For the graph construction across
multiple nodes, a multi-node procedure based on Two-way Merge is presented. The
procedure makes it feasible to construct a large-scale k-NN graph/indexing
graph on either a single node or multiple nodes when the data size exceeds the
memory capacity of one node. Extensive experiments are conducted on both
large-scale k-NN graph and indexing graph construction. For the k-NN graph
construction, the large-scale and high-quality k-NN graphs are constructed by
graph merge in parallel. Typically, a billion-scale k-NN graph can be built in
approximately 17h when only three nodes are employed. For the indexing graph
construction, similar NN search performance as the original indexing graph is
achieved with the merged indexing graphs while requiring much less time of
construction.

</details>


### [77] [A Uniqueness Theorem for Distributed Computation under Physical Constraint](https://arxiv.org/abs/2509.11754)
*Zhiyuan Ren,Mingxuan Lu,Wenchi Cheng*

Main category: cs.DC

TL;DR: 现有计算基础模型常忽略硬件限制，本文针对网络内计算极端环境，建立公理系统证明存在唯一最优范式SDPF，该范式有良好特性并给出分布式计算流必然结果。


<details>
  <summary>Details</summary>
Motivation: 现有分布式范式在网络内计算极端环境下面临通信效率、有限内存和可扩展性的三难困境，需新解决方案。

Method: 建立严格的公理系统，形式化物理约束，对存在幂等合并运算符的计算类进行证明。

Result: 证明存在唯一最优范式Self - Describing Parallel Flows (SDPF)，且该范式是收敛、图灵完备和最小的。

Conclusion: 如同CAP定理给出分布式状态管理的不可能边界，本文给出分布式计算流在物理定律下的必然结果。

Abstract: Foundational models of computation often abstract away physical hardware
limitations. However, in extreme environments like In-Network Computing (INC),
these limitations become inviolable laws, creating an acute trilemma among
communication efficiency, bounded memory, and robust scalability. Prevailing
distributed paradigms, while powerful in their intended domains, were not
designed for this stringent regime and thus face fundamental challenges. This
paper demonstrates that resolving this trilemma requires a shift in perspective
- from seeking engineering trade-offs to deriving solutions from logical
necessity. We establish a rigorous axiomatic system that formalizes these
physical constraints and prove that for the broad class of computations
admitting an idempotent merge operator, there exists a unique, optimal
paradigm. Any system satisfying these axioms must converge to a single normal
form: Self-Describing Parallel Flows (SDPF), a purely data-centric model where
stateless executors process flows that carry their own control logic. We
further prove this unique paradigm is convergent, Turing-complete, and minimal.
In the same way that the CAP theorem established a boundary for what is
impossible in distributed state management, our work provides a constructive
dual: a uniqueness theorem that reveals what is \textit{inevitable} for
distributed computation flows under physical law.

</details>


### [78] [LASLiN: A Learning-Augmented Peer-to-Peer Network](https://arxiv.org/abs/2509.11904)
*Julien Dallot,Caio Caldeira,Arash Pourdamghani,Olga Goussevskaia,Stefan Schmid*

Main category: cs.DC

TL;DR: 本文提出学习增强的P2P网络设计，利用流量模式预测优化拓扑，兼顾性能一致性和鲁棒性，有两项主要贡献。


<details>
  <summary>Details</summary>
Motivation: 优化P2P网络拓扑，以需求感知方式优化网络，在标准P2P指标上获得更好性能。

Method: 先考虑集中式设置，用动态规划解决最优静态跳跃表网络构建问题；引入Uniform P2P协议并构建学习增强的LASLiN协议。

Result: 证明LASLiN在预测正确时性能与最优静态SLN一致，预测错误时与现有P2P协议差距在对数因子内，在高稀疏需求下性能提升。

Conclusion: 所提出的学习增强P2P网络设计有效，能在不同预测情况下实现较好性能。

Abstract: We introduce a learning-augmented peer-to-peer (P2P) network design that
leverages the predictions of traffic patterns to optimize the network's
topology. While keeping formal guarantees on the standard P2P metrics (routing
path length, maximum degree), we optimize the network in a demand-aware manner
and minimize the path lengths weighted by the peer-to-peer communication
demands. Our protocol is learning-augmented, meaning that each node receives an
individual, possibly inaccurate prediction about the future traffic patterns,
with the goal of improving the network's performances. We strike a trade-off
between significantly improved performances when the predictions are correct
(consistency) and polylogarithmic performances when the predictions are
arbitrary (robustness).
  We have two main contributions. First, we consider the centralized setting
and show that the problem of constructing an optimum static skip list network
(SLN) is solvable in polynomial time and can be computed via dynamic
programming. This problem is the natural demand-aware extension of the optimal
skip list problem.
  Second, we introduce the Uniform P2P protocol which generalizes skip list
networks (SLN) by relaxing the node's heights from discrete to continuous. We
show that Uniform achieves state-of-the-art performances: logarithmic routing
and maximum degree, both with high probability. We then use Uniform to build a
learning-augmented P2P protocol in order to incorporate demand-awareness,
leading to our main contribution, LASLiN. We prove that the performances of
LASLiN are consistent with those of an optimum static SLN with correct
predictions (given via our dynamic programming approach), and are at most a
logarithmic factor off the state-of-the-art P2P protocols if the predictions
are arbitrary wrong. For the special case of highly sparse demands, we show
that LASLiN achieves improved performances.

</details>


### [79] [UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code Translation in HPC](https://arxiv.org/abs/2509.12136)
*Tomer Bitan,Tal Kadosh,Erel Kaplan,Shira Meiri,Le Chen,Peter Morales,Niranjan Hasabnis,Gal Oren*

Main category: cs.DC

TL;DR: 介绍评估框架UniPar用于评估大语言模型并行代码翻译能力，构建数据集PARATRANS，发现组合方法能提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有并行编程语言翻译工具存在局限，大语言模型提供新途径，需系统评估其并行代码翻译能力。

Method: 引入UniPar评估框架，针对串行代码、CUDA和OpenMP翻译，评估四种使用模式，构建PARATRANS数据集。

Result: 现成模型默认设置表现不佳，UniPar方法使性能提升达2倍。

Conclusion: 研究结果为改进大语言模型并行语言翻译提供有用见解。

Abstract: Translating programs between various parallel programming languages is an
important problem in the high-performance computing (HPC) community. Existing
tools for this problem are either too narrow in scope and/or outdated. Recent
explosive growth in the popularity of large language models (LLMs) and their
ability to generate and translate code offers a potential alternative approach.
Toward that end, we first need to systematically evaluate the ability of LLMs
to translate between parallel languages.
  In this work, we introduce UniPar, a systematic evaluation framework for
LLM-based parallel code translation. Specifically, in this work, we target
translations between serial code, CUDA, and OpenMP. Our goal is to assess how
well current instruction-tuned LLMs -- specifically GPT-4o-mini and
LLaMA-3.3-70B-Instruct -- can be used out of the box or enhanced through known
strategies. We evaluated four major usage modes: hyperparameter optimization
for decoding, zero- and few-shot prompting, supervised fine-tuning, and
iterative feedback through compiler-based repair. As a part of the evaluation,
we construct a new dataset called PARATRANS, covering both serial-to-parallel
translation and cross-paradigm transformations.
  Our findings reveal that while off-the-shelf models struggle under the
default settings (e.g., GPT-4o-mini achieves only 46% compilation and 15%
functional correctness), our UniPar methodology -- combining fine-tuning,
hyperparameter tuning, and compiler-guided repair -- improves performance by up
to 2X (69% compilation and 33% correctness). We believe that our findings will
provide useful insights for researchers to further improve LLMs for the
parallel language translation problem.
  UniPar source code and PARATRANS dataset are available at our GitHub
repository https://github.com/Scientific-Computing-Lab/UniPar_AI.

</details>


### [80] [Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization](https://arxiv.org/abs/2509.12138)
*Mengjiao Han,Andres Sewell,Joseph Insley,Janet Knowles,Victor A. Mateevitsi,Michael E. Papka,Steve Petruzza,Silvio Rizzi*

Main category: cs.DC

TL;DR: 提出适用于HPC的分布式3D - GS管道，可实现大规模科学数据的可扩展可视化，在基准测试中有速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有3D - GS在科学可视化中局限于单GPU设置，无法满足HPC系统上大规模数据集的可扩展性需求。

Method: 在节点间划分数据，使用多节点和多GPU并行训练高斯片并合并用于全局渲染，在分区边界添加幽灵单元和应用背景掩码消除伪影。

Result: 在Richtmyer - Meshkov数据集基准测试中，在Polaris的8个节点上实现高达3倍的加速，且保留图像质量。

Conclusion: 分布式3D - GS能实现大规模科学数据的可扩展可视化，为未来原位应用奠定基础。

Abstract: 3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique
for real-time, photorealistic rendering by optimizing anisotropic Gaussian
primitives from view-dependent images. While 3D-GS has been extended to
scientific visualization, prior work remains limited to single-GPU settings,
restricting scalability for large datasets on high-performance computing (HPC)
systems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach
partitions data across nodes, trains Gaussian splats in parallel using
multi-nodes and multi-GPUs, and merges splats for global rendering. To
eliminate artifacts, we add ghost cells at partition boundaries and apply
background masks to remove irrelevant pixels. Benchmarks on the
Richtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup
across 8 nodes on Polaris while preserving image quality. These results
demonstrate that distributed 3D-GS enables scalable visualization of
large-scale scientific data and provide a foundation for future in situ
applications.

</details>


### [81] [When MoE Meets Blockchain: A Trustworthy Distributed Framework of Large Models](https://arxiv.org/abs/2509.12141)
*Weihao Zhu,Long Shi,Kang Wei,Zhen Mei,Zhe Wang,Jiaheng Wang,Jun Li*

Main category: cs.DC

TL;DR: 提出区块链辅助的可信MoE框架B - MoE，实验表明其比传统分布式MoE更抗数据操纵攻击。


<details>
  <summary>Details</summary>
Motivation: 传统云基MoE有响应延迟长、带宽消耗高和数据隐私泄露问题，分布式MoE框架缺乏数据交互信任易受攻击。

Method: 提出包含边缘层、区块链层和存储层的B - MoE框架，边缘层处理任务，区块链层追踪、验证和记录结果。

Result: 实验表明B - MoE在训练和推理过程中比传统分布式MoE更能抵御数据操纵攻击。

Conclusion: B - MoE框架有效增强了分布式MoE的安全性和抗攻击能力。

Abstract: As an enabling architecture of Large Models (LMs), Mixture of Experts (MoE)
has become prevalent thanks to its sparsely-gated mechanism, which lowers
computational overhead while maintaining learning performance comparable to
dense LMs. The essence of MoE lies in utilizing a group of neural networks
(called experts) with each specializing in different types of tasks, along with
a trainable gating network that selectively activates a subset of these experts
to handle specific tasks. Traditional cloud-based MoE encounters challenges
such as prolonged response latency, high bandwidth consumption, and data
privacy leakage. To address these issues, researchers have proposed to deploy
MoE over distributed edge networks. However, a key concern of distributed MoE
frameworks is the lack of trust in data interactions among distributed experts
without the surveillance of any trusted authority, and thereby prone to
potential attacks such as data manipulation. In response to the security issues
of traditional distributed MoE, we propose a blockchain-aided trustworthy MoE
(B-MoE) framework that consists of three layers: the edge layer, the blockchain
layer, and the storage layer. In this framework, the edge layer employs the
activated experts downloaded from the storage layer to process the learning
tasks, while the blockchain layer functions as a decentralized trustworthy
network to trace, verify, and record the computational results of the experts
from the edge layer. The experimental results demonstrate that B-MoE is more
robust to data manipulation attacks than traditional distributed MoE during
both the training and inference processes.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [82] [The Chonkers Algorithm: Content-Defined Chunking with Strict Guarantees on Size and Locality](https://arxiv.org/abs/2509.11121)
*Benjamin Berger*

Main category: cs.DS

TL;DR: 提出Chonkers算法，是一种新的内容定义分块方法，有大小和编辑局部性保证，还介绍Yarn数据类型。


<details>
  <summary>Details</summary>
Motivation: 现有算法如Rabin指纹和基于锚点的方法无法同时提供块大小和编辑局部性的严格保证。

Method: 设计Chonkers算法，阐述其分层结构、理论保证和实现考虑，并引入Yarn数据类型。

Result: Chonkers算法实现了编辑的有限传播和对块大小的精确控制。

Conclusion: Chonkers算法是一种有效的内容定义分块方法，Yarn数据类型能受益于其严格保证。

Abstract: This paper presents the Chonkers algorithm, a novel content-defined chunking
method providing simultaneous strict guarantees on chunk size and edit
locality. Unlike existing algorithms such as Rabin fingerprinting and
anchor-based methods, Chonkers achieves bounded propagation of edits and
precise control over chunk sizes. I describe the algorithm's layered structure,
theoretical guarantees, implementation considerations, and introduce the Yarn
datatype, a deduplicated, merge-tree-based string representation benefiting
from Chonkers' strict guarantees.

</details>


### [83] [Triangle-Covered Graphs: Algorithms, Complexity, and Structure](https://arxiv.org/abs/2509.11448)
*Amirali Madani,Anil Maheshwari,Babak Miraftab,Paweł Żyliński*

Main category: cs.DS

TL;DR: 本文研究将给定图转化为三角覆盖图的新边修改问题，给出相关图的边数下界，证明决策问题的NP完全性和不可近似性，对不同图类给出最小Δ - 补集大小的界，设计近似和精确算法，并确定随机图成为三角覆盖图的阈值。


<details>
  <summary>Details</summary>
Motivation: 研究新的边修改问题，即把给定图转化为三角覆盖图

Method: 先给出连通三角覆盖图边数的下界并刻画达到最小边数的图；定义Δ - 补集概念，证明相关决策问题的复杂度；从算法角度对不同图类研究最小Δ - 补集大小并设计算法；分析随机图成为三角覆盖图的阈值。

Result: 给出连通三角覆盖图边数的紧下界；证明寻找至多t条边的Δ - 补集的决策问题是NP完全的且无常数因子近似算法，在连通二部图输入下仍NP完全；对树、弦图和仙人掌图等图类给出最小Δ - 补集大小的紧界；对一般图给出(ln n + 1) - 近似算法，对树和弦图设计计算最小Δ - 补集的算法；确定随机图成为三角覆盖图的阈值为n^(-2/3)。

Conclusion: 成功研究了将图转化为三角覆盖图的边修改问题，在复杂度、算法设计和随机图阈值方面取得成果。

Abstract: The widely studied edge modification problems ask how to minimally alter a
graph to satisfy certain structural properties. In this paper, we introduce and
study a new edge modification problem centered around transforming a given
graph into a triangle-covered graph (one in which every vertex belongs to at
least one triangle). We first present tight lower bounds on the number of edges
in any connected triangle-covered graph of order $n$, and then we characterize
all connected graphs that attain this minimum edge count. For a graph $G$, we
define the notion of a $\Delta$-completion set as a set of non-edges of $G$
whose addition to $G$ results in a triangle-covered graph. We prove that the
decision problem of finding a $\Delta$-completion set of size at most $t\geq0$
is $\mathbb{NP}$-complete and does not admit a constant-factor approximation
algorithm under standard complexity assumptions. Moreover, we show that this
problem remains $\mathbb{NP}$-complete even when the input is restricted to
connected bipartite graphs. We then study the problem from an algorithmic
perspective, providing tight bounds on the minimum $\Delta$-completion set size
for several graph classes, including trees, chordal graphs, and cactus graphs.
Furthermore, we show that the triangle-covered problem admits an $(\ln n
+1)$-approximation algorithm for general graphs. For trees and chordal graphs,
we design algorithms that compute minimum $\Delta$-completion sets. Finally, we
show that the threshold for a random graph $\mathbb{G}(n, p)$ to be
triangle-covered occurs at $n^{-2/3}$.

</details>


### [84] [On the Smallest Size of Internal Collage Systems](https://arxiv.org/abs/2509.11602)
*Soichiro Migita,Kyotaro Uehata,Tomohiro I*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A Straight-Line Program (SLP) for a stirng $T$ is a context-free grammar in
Chomsky normal form that derives $T$ only, which can be seen as a compressed
form of $T$. Kida et al.\ introduced collage systems [Theor. Comput. Sci.,
2003] to generalize SLPs by adding repetition rules and truncation rules. The
smallest size $c(T)$ of collage systems for $T$ has gained attention to see how
these generalized rules improve the compression ability of SLPs. Navarro et al.
[IEEE Trans. Inf. Theory, 2021] showed that $c(T) \in O(z(T))$ and there is a
string family with $c(T) \in \Omega(b(T) \log |T|)$, where $z(T)$ is the number
of Lempel-Ziv parsing of $T$ and $b(T)$ is the smallest size of bidirectional
schemes for $T$. They also introduced a subclass of collage systems, called
internal collage systems, and proved that its smallest size $\hat{c}(T)$ for
$T$ is at least $b(T)$. While $c(T) \le \hat{c}(T)$ is obvious, it is unknown
how large $\hat{c}(T)$ is compared to $c(T)$. In this paper, we prove that
$\hat{c}(T) = \Theta(c(T))$ by showing that any collage system of size $m$ can
be transformed into an internal collage system of size $O(m)$ in $O(m^2)$ time.
Thanks to this result, we can focus on internal collage systems to study the
asymptotic behavior of $c(T)$, which helps to suppress excess use of truncation
rules. As a direct application, we get $b(T) = O(c(T))$, which answers an open
question posed in [Navarro et al., IEEE Trans. Inf. Theory, 2021]. We also give
a MAX-SAT formulation to compute $\hat{c}(T)$ for a given $T$.

</details>


### [85] [An ETH-Tight FPT Algorithm for Rejection-Proof Set Packing with Applications to Kidney Exchange](https://arxiv.org/abs/2509.11965)
*Bart M. P. Jansen,Jeroen S. K. Lamme,Ruben F. A. Verhaegh*

Main category: cs.DS

TL;DR: 研究肾脏交换问题多智能体变体的参数化复杂度，给出核与算法，分析不同参数下问题复杂度。


<details>
  <summary>Details</summary>
Motivation: 探究肾脏交换问题多智能体变体的参数化复杂度，理解问题困难原因。

Method: 利用向日葵引理给出核，基于此设计算法，通过参数变化分析复杂度。

Result: 得到多项式核，设计FPT算法并证明渐近最优，分析不同参数下问题复杂度变化。

Conclusion: 该问题经典和参数化复杂度存在差异，显示了问题困难的原因。

Abstract: We study the parameterized complexity of a recently introduced multi-agent
variant of the Kidney Exchange problem. Given a directed graph $G$ and integers
$d$ and $k$, the standard problem asks whether $G$ contains a packing of
vertex-disjoint cycles, each of length $\leq d$, covering at least $k$ vertices
in total. In the multi-agent setting we consider, the vertex set is partitioned
over several agents who reject a cycle packing as solution if it can be
modified into an alternative packing that covers more of their own vertices. A
cycle packing is called rejection-proof if no agent rejects it and the problem
asks whether such a packing exists that covers at least $k$ vertices.
  We exploit the sunflower lemma on a set packing formulation of the problem to
give a kernel for this $\Sigma_2^P$-complete problem that is polynomial in $k$
for all constant values of $d$. We also provide a $2^{\mathcal{O}(k \log k)} +
n^{\mathcal{O}(1)}$ algorithm based on it and show that this FPT algorithm is
asymptotically optimal under the ETH. Further, we generalize the problem by
including an additional positive integer $c$ in the input that naturally
captures how much agents can modify a given cycle packing to reject it. For
every constant $c$, the resulting problem simplifies from being
$\Sigma_2^P$-complete to NP-complete. With a single-exponential algorithm for
the setting where $c = 1$, we show this to be strictly easier under the ETH
than when $c = 2$. In turn, we show that any $c \geq 2$ yields a problem that
is essentially as hard as the original problem with $c$ unbounded. This
displays an interesting discrepancy between the classical and parameterized
complexity of the problem and gives a good view of what makes it hard.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [86] [Strategic Cyber Defense via Reinforcement Learning-Guided Combinatorial Auctions](https://arxiv.org/abs/2509.10983)
*Mai Pham,Vikrant Vaze,Peter Chin*

Main category: cs.GT

TL;DR: 提出用组合拍卖分配网络防御行动束，训练CAFormer机制，方法在收益和抗误报上有竞争力，证明基于拍卖的规划在网络防御中可行。


<details>
  <summary>Details</summary>
Motivation: 网络防御行动需在不确定和资源约束下进行长期战略规划。

Method: 用强化学习Q值进行主机特定估值，训练基于Transformer的可微拍卖机制CAFormer以生成近似激励兼容的分配。

Result: 与神谕和启发式分配相比，方法实现有竞争力的收益，对误报有鲁棒性，分配模式与攻防活动相关。

Conclusion: 基于拍卖的规划在网络防御中可行，强化学习衍生的价值结构有可解释性优势。

Abstract: Cyber defense operations increasingly require long-term strategic planning
under uncertainty and resource constraints. We propose a new use of
combinatorial auctions for allocating defensive action bundles in a realistic
cyber environment, using host-specific valuations derived from reinforcement
learning (RL) Q-values. These Q-values encode long-term expected utility,
allowing upstream planning. We train CAFormer, a differentiable
Transformer-based auction mechanism, to produce allocations that are
approximately incentive-compatible under misreporting. Rather than benchmarking
against existing agents, we explore the qualitative and strategic properties of
the learned mechanisms. Compared to oracle and heuristic allocations, our
method achieves competitive revenue while offering robustness to misreporting.
In addition, we find that allocation patterns correlate with adversarial and
defensive activity, suggesting implicit alignment with operational priorities.
Our results demonstrate the viability of auction-based planning in cyber
defense and highlight the interpretability benefits of RL-derived value
structures.

</details>


### [87] [Actively Learning to Coordinate in Convex Games via Approximate Correlated Equilibrium](https://arxiv.org/abs/2509.10989)
*Zhenlong Fang,Aryan Deshwal,Yue Yu*

Main category: cs.GT

TL;DR: 研究协调者在凸博弈中学习相关均衡的方法，提出学习框架，通过查询玩家遗憾值近似相关均衡，并在多用户交通分配博弈中验证。


<details>
  <summary>Details</summary>
Motivation: 研究协调者在不知玩家成本函数情况下，如何在凸博弈中学习相关均衡。

Method: 提出学习框架，通过主动查询玩家遗憾值学习近似相关均衡，引入启发式方法选择有限代表联合行动，应用贝叶斯优化学习所选联合行动的概率分布。

Result: 所学习的分布通过最小化玩家遗憾值近似相关均衡。

Conclusion: 通过数值实验在多用户交通分配博弈中验证了所提方法的有效性。

Abstract: Correlated equilibrium generalizes Nash equilibrium by allowing a central
coordinator to guide players' actions through shared recommendations, similar
to how routing apps guide drivers. We investigate how a coordinator can learn a
correlated equilibrium in convex games where each player minimizes a convex
cost function that depends on other players' actions, subject to convex
constraints without knowledge of the players' cost functions. We propose a
learning framework that learns an approximate correlated equilibrium by
actively querying players' regrets, \emph{i.e.}, the cost saved by deviating
from the coordinator's recommendations. We first show that a correlated
equilibrium in convex games corresponds to a joint action distribution over an
infinite joint action space that minimizes all players' regrets. To make the
learning problem tractable, we introduce a heuristic that selects finitely many
representative joint actions by maximizing their pairwise differences. We then
apply Bayesian optimization to learn a probability distribution over the
selected joint actions by querying all players' regrets. The learned
distribution approximates a correlated equilibrium by minimizing players'
regrets. We demonstrate the proposed approach via numerical experiments on
multi-user traffic assignment games in a shared transportation network.

</details>


### [88] [Identifying Imperfect Clones in Elections](https://arxiv.org/abs/2509.11261)
*Piotr Faliszewski,Lukasz Janeczko,Grzegorz Lisowski,Kristyna Pekarkova,Ildiko Schlotter*

Main category: cs.GT

TL;DR: 研究序数选举中完美克隆概念的不同放松形式，确定识别不完美克隆及划分候选人为不完美克隆族的复杂度，还研究参数化复杂度。


<details>
  <summary>Details</summary>
Motivation: 探讨序数选举中完美克隆概念的不同放松形式，以更好地适应实际选举情况。

Method: 分析不同放松形式的克隆概念，建立识别和划分问题的复杂度，并研究参数化复杂度。

Result: 确定了识别不完美克隆及划分候选人为不完美克隆族的复杂度，研究了参数化复杂度。

Conclusion: 对序数选举中不完美克隆相关问题的复杂度有了深入认识。

Abstract: A perfect clone in an ordinal election (i.e., an election where the voters
rank the candidates in a strict linear order) is a set of candidates that each
voter ranks consecutively. We consider different relaxations of this notion:
independent or subelection clones are sets of candidates that only some of the
voters recognize as a perfect clone, whereas approximate clones are sets of
candidates such that every voter ranks their members close to each other, but
not necessarily consecutively. We establish the complexity of identifying such
imperfect clones, and of partitioning the candidates into families of imperfect
clones. We also study the parameterized complexity of these problems with
respect to a set of natural parameters such as the number of voters, the size
or the number of imperfect clones we are searching for, or their level of
imperfection.

</details>


### [89] [An Incentive-Compatible Reward Sharing Mechanism for Mitigating Mirroring Attacks in Decentralized Data-Feed Systems](https://arxiv.org/abs/2509.11294)
*Sina Aeeneh,Nikola Zlatanov,Jiangshan Yu*

Main category: cs.GT

TL;DR: 本文分析镜像攻击对基于多数投票的数据馈送系统的影响，提出新激励机制并证明其能达纳什均衡，还讨论了实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有激励机制易受镜像攻击，攻击者可控制多个预言机影响聚合函数决策并获取最大奖励，影响系统可靠性。

Method: 分析现有机制问题，提出新激励机制，证明该机制可使每个用户仅操作一个预言机达到纳什均衡。

Result: 新激励机制能有效避免Sybil行为，实现纳什均衡。

Conclusion: 新激励机制可提高基于多数投票的数据馈送系统的可靠性和安全性，具有实际应用价值。

Abstract: Decentralized data-feed systems enable blockchain-based smart contracts to
access off-chain information by aggregating values from multiple oracles. To
improve accuracy, these systems typically use an aggregation function, such as
majority voting, to consolidate the inputs they receive from oracles and make a
decision. Depending on the final decision and the values reported by the
oracles, the participating oracles are compensated through shared rewards.
However, such incentive mechanisms are vulnerable to mirroring attacks, where a
single user controls multiple oracles to bias the decision of the aggregation
function and maximize rewards. This paper analyzes the impact of mirroring
attacks on the reliability and dependability of majority voting-based data-feed
systems. We demonstrate how existing incentive mechanisms can unintentionally
encourage rational users to implement such attacks. To address this, we propose
a new incentive mechanism that discourages Sybil behavior. We prove that the
proposed mechanism leads to a Nash Equilibrium in which each user operates only
one oracle. Finally, we discuss the practical implementation of the proposed
incentive mechanism and provide numerical examples to demonstrate its
effectiveness.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [90] [DSRAG: A Domain-Specific Retrieval Framework Based on Document-derived Multimodal Knowledge Graph](https://arxiv.org/abs/2509.10467)
*Mengzheng Yang,Yanfei Ren,David Osei Opoku,Ruochang Li,Peng Ren,Chunxiao Xing*

Main category: cs.IR

TL;DR: 通用大语言模型在特定领域问答有局限，本文提出DSRAG框架，结合多模态知识图谱和检索增强生成，评估显示其在特定领域问答表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决通用大语言模型在特定领域任务中知识幻觉、适应性不足，以及传统RAG在领域知识准确性和上下文建模方面的局限。

Method: 提出DSRAG框架，以特定领域文档为知识源构建多模态知识图谱，引入语义剪枝和结构化子图检索机制，结合知识图谱上下文和向量检索结果指导语言模型。

Result: 使用Langfuse多维评分机制评估，方法在特定领域问答中表现优异。

Conclusion: 多模态知识图谱与检索增强生成相结合是有效的。

Abstract: Current general-purpose large language models (LLMs) commonly exhibit
knowledge hallucination and insufficient domain-specific adaptability in
domain-specific tasks, limiting their effectiveness in specialized question
answering scenarios. Retrieval-augmented generation (RAG) effectively tackles
these challenges by integrating external knowledge to enhance accuracy and
relevance. However, traditional RAG still faces limitations in domain knowledge
accuracy and context modeling.To enhance domain-specific question answering
performance, this work focuses on a graph-based RAG framework, emphasizing the
critical role of knowledge graph quality during the generation process. We
propose DSRAG (Domain-Specific RAG), a multimodal knowledge graph-driven
retrieval-augmented generation framework designed for domain-specific
applications. Our approach leverages domain-specific documents as the primary
knowledge source, integrating heterogeneous information such as text, images,
and tables to construct a multimodal knowledge graph covering both conceptual
and instance layers. Building on this foundation, we introduce semantic pruning
and structured subgraph retrieval mechanisms, combining knowledge graph context
and vector retrieval results to guide the language model towards producing more
reliable responses. Evaluations using the Langfuse multidimensional scoring
mechanism show that our method excels in domain-specific question answering,
validating the efficacy of integrating multimodal knowledge graphs with
retrieval-augmented generation.

</details>


### [91] [Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation](https://arxiv.org/abs/2509.10468)
*Yifan Liu,Yaokun Liu,Zelin Li,Zhenrui Yue,Gyuseok Lee,Ruichen Yao,Yang Zhang,Dong Wang*

Main category: cs.IR

TL;DR: 提出DECOR统一框架解决生成式推荐器两阶段目标不一致问题，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐器两阶段目标不一致，导致静态分词欠佳和预训练语义被丢弃的问题。

Method: 提出DECOR框架，引入上下文分词组合和分解嵌入融合方法。

Result: 在三个真实数据集上实验，DECOR在推荐性能上始终优于现有基线。

Conclusion: DECOR框架有效，能保留预训练语义并增强分词嵌入的适应性。

Abstract: Recent advances in generative recommenders adopt a two-stage paradigm: items
are first tokenized into semantic IDs using a pretrained tokenizer, and then
large language models (LLMs) are trained to generate the next item via
sequence-to-sequence modeling. However, these two stages are optimized for
different objectives: semantic reconstruction during tokenizer pretraining
versus user interaction modeling during recommender training. This objective
misalignment leads to two key limitations: (i) suboptimal static tokenization,
where fixed token assignments fail to reflect diverse usage contexts; and (ii)
discarded pretrained semantics, where pretrained knowledge - typically from
language model embeddings - is overwritten during recommender training on user
interactions. To address these limitations, we propose to learn DEcomposed
COntextual Token Representations (DECOR), a unified framework that preserves
pretrained semantics while enhancing the adaptability of token embeddings.
DECOR introduces contextualized token composition to refine token embeddings
based on user interaction context, and decomposed embedding fusion that
integrates pretrained codebook embeddings with newly learned collaborative
embeddings. Experiments on three real-world datasets demonstrate that DECOR
consistently outperforms state-of-the-art baselines in recommendation
performance. Our code will be made available upon publication.

</details>


### [92] [Real-Time RAG for the Identification of Supply Chain Vulnerabilities](https://arxiv.org/abs/2509.10469)
*Jesse Ponnock,Grace Kenneally,Michael Robert Briggs,Elinor Yeo,Tyrone Patterson III,Nicholas Kinberg,Matthew Kalinowski,David Hechtman*

Main category: cs.IR

TL;DR: 研究提出结合RAG预处理与网页抓取技术进行供应链分析，评估技术组合效果，发现微调嵌入检索模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型知识更新不及时，无法满足供应链分析对新兴及时信息的需求。

Method: 将新兴的检索增强生成（RAG）预处理和检索技术与先进网页抓取技术集成，减少新信息纳入增强大语言模型的延迟。

Result: 微调嵌入检索模型性能提升显著，自适应迭代检索能增强复杂查询性能，微调大语言模型效果有限且成本高，向下查询抽象优于向上抽象。

Conclusion: 在供应链分析中应用RAG系统时，检索质量至关重要，微调嵌入检索模型效果最佳。

Abstract: New technologies in generative AI can enable deeper analysis into our
nation's supply chains but truly informative insights require the continual
updating and aggregation of massive data in a timely manner. Large Language
Models (LLMs) offer unprecedented analytical opportunities however, their
knowledge base is constrained to the models' last training date, rendering
these capabilities unusable for organizations whose mission impacts rely on
emerging and timely information. This research proposes an innovative approach
to supply chain analysis by integrating emerging Retrieval-Augmented Generation
(RAG) preprocessing and retrieval techniques with advanced web-scraping
technologies. Our method aims to reduce latency in incorporating new
information into an augmented-LLM, enabling timely analysis of supply chain
disruptors. Through experimentation, this study evaluates the combinatorial
effects of these techniques towards timeliness and quality trade-offs. Our
results suggest that in applying RAG systems to supply chain analysis,
fine-tuning the embedding retrieval model consistently provides the most
significant performance gains, underscoring the critical importance of
retrieval quality. Adaptive iterative retrieval, which dynamically adjusts
retrieval depth based on context, further enhances performance, especially on
complex supply chain queries. Conversely, fine-tuning the LLM yields limited
improvements and higher resource costs, while techniques such as downward query
abstraction significantly outperforms upward abstraction in practice.

</details>


### [93] [ReFineG: Synergizing Small Supervised Models and LLMs for Low-Resource Grounded Multimodal NER](https://arxiv.org/abs/2509.10975)
*Jielong Tang,Shuang Wang,Zhenxing Wang,Jianxing Yu,Jian Yin*

Main category: cs.IR

TL;DR: 提出 ReFineG 框架解决低资源 GMNER 问题，在 CCKS2025 任务中取得佳绩。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法依赖昂贵的多模态注释且在低资源领域表现不佳，MLLMs 存在领域知识冲突。

Method: 提出三阶段协作框架 ReFineG，包括训练阶段的数据合成策略、细化阶段的不确定性机制和接地阶段的多模态上下文选择算法。

Result: 在 CCKS2025 GMNER 共享任务在线排行榜上 F1 分数达 0.6461，排名第二。

Conclusion: ReFineG 在有限注释下有效。

Abstract: Grounded Multimodal Named Entity Recognition (GMNER) extends traditional NER
by jointly detecting textual mentions and grounding them to visual regions.
While existing supervised methods achieve strong performance, they rely on
costly multimodal annotations and often underperform in low-resource domains.
Multimodal Large Language Models (MLLMs) show strong generalization but suffer
from Domain Knowledge Conflict, producing redundant or incorrect mentions for
domain-specific entities. To address these challenges, we propose ReFineG, a
three-stage collaborative framework that integrates small supervised models
with frozen MLLMs for low-resource GMNER. In the Training Stage, a domain-aware
NER data synthesis strategy transfers LLM knowledge to small models with
supervised training while avoiding domain knowledge conflicts. In the
Refinement Stage, an uncertainty-based mechanism retains confident predictions
from supervised models and delegates uncertain ones to the MLLM. In the
Grounding Stage, a multimodal context selection algorithm enhances visual
grounding through analogical reasoning. In the CCKS2025 GMNER Shared Task,
ReFineG ranked second with an F1 score of 0.6461 on the online leaderboard,
demonstrating its effectiveness with limited annotations.

</details>


### [94] [Membership Inference Attacks on Recommender System: A Survey](https://arxiv.org/abs/2509.11080)
*Jiajie He,Yuechun Gu,Keke Chen,Xintong Chen*

Main category: cs.IR

TL;DR: 本文首次对推荐系统的成员推理攻击进行全面调查，回顾进展，给出分类并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 推荐系统易受成员推理攻击致隐私泄露，传统攻击方法不适用于推荐系统，且该领域缺乏系统综述。

Method: 对推荐系统成员推理攻击的最新进展进行全面回顾，提供统一分类法对不同攻击进行分类。

Result: 给出不同推荐系统成员推理攻击的分类及优缺点。

Conclusion: 指出多个有前景的未来研究方向，为研究社区和非该领域研究者提供参考。

Abstract: Recommender systems (RecSys) have been widely applied to various
applications, including E-commerce, finance, healthcare, social media and have
become increasingly influential in shaping user behavior and decision-making,
highlighting their growing impact in various domains. However, recent studies
have shown that RecSys are vulnerable to membership inference attacks (MIAs),
which aim to infer whether user interaction record was used to train a target
model or not. MIAs on RecSys models can directly lead to a privacy breach. For
example, via identifying the fact that a purchase record that has been used to
train a RecSys associated with a specific user, an attacker can infer that
user's special quirks. In recent years, MIAs have been shown to be effective on
other ML tasks, e.g., classification models and natural language processing.
However, traditional MIAs are ill-suited for RecSys due to the unseen posterior
probability. Although MIAs on RecSys form a newly emerging and rapidly growing
research area, there has been no systematic survey on this topic yet. In this
article, we conduct the first comprehensive survey on RecSys MIAs. This survey
offers a comprehensive review of the latest advancements in RecSys MIAs,
exploring the design principles, challenges, attack and defense associated with
this emerging field. We provide a unified taxonomy that categorizes different
RecSys MIAs based on their characterizations and discuss their pros and cons.
Based on the limitations and gaps identified in this survey, we point out
several promising future research directions to inspire the researchers who
wish to follow this area. This survey not only serves as a reference for the
research community but also provides a clear description for researchers
outside this research domain.

</details>


### [95] [SPARK: Adaptive Low-Rank Knowledge Graph Modeling in Hybrid Geometric Spaces for Recommendation](https://arxiv.org/abs/2509.11094)
*Binhao Wang,Yutian Xiao,Maolin Wang,Zhiqi Li,Tianshuo Wei,Ruocheng Guo,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 本文提出SPARK框架解决知识图谱增强推荐系统的问题，在长尾物品推荐上表现优异。


<details>
  <summary>Details</summary>
Motivation: 知识图谱增强推荐系统面临噪声、稀疏性和欧几里得几何表示局限等问题，现有方法缺乏针对物品流行度的自适应多源信号融合。

Method: SPARK框架先使用Tucker低秩分解去噪，用SVD初始化的混合几何GNN在欧氏和双曲空间学习表示，采用物品流行度感知的自适应融合策略，最后用对比学习对齐多源表示。

Result: 实验表明SPARK显著优于现有方法，尤其在长尾物品推荐上表现出色。

Conclusion: SPARK为知识增强推荐提供了稳健、有原则的方法。

Abstract: Knowledge Graphs (KGs) enhance recommender systems but face challenges from
inherent noise, sparsity, and Euclidean geometry's inadequacy for complex
relational structures, critically impairing representation learning, especially
for long-tail entities. Existing methods also often lack adaptive multi-source
signal fusion tailored to item popularity. This paper introduces SPARK, a novel
multi-stage framework systematically tackling these issues. SPARK first employs
Tucker low-rank decomposition to denoise KGs and generate robust entity
representations. Subsequently, an SVD-initialized hybrid geometric GNN
concurrently learns representations in Euclidean and Hyperbolic spaces; the
latter is strategically leveraged for its aptitude in modeling hierarchical
structures, effectively capturing semantic features of sparse, long-tail items.
A core contribution is an item popularity-aware adaptive fusion strategy that
dynamically weights signals from collaborative filtering, refined KG
embeddings, and diverse geometric spaces for precise modeling of both
mainstream and long-tail items. Finally, contrastive learning aligns these
multi-source representations. Extensive experiments demonstrate SPARK's
significant superiority over state-of-the-art methods, particularly in
improving long-tail item recommendation, offering a robust, principled approach
to knowledge-enhanced recommendation. Implementation code is available at
https://github.com/Applied-Machine-Learning-Lab/SPARK.

</details>


### [96] [Understanding the Information Cocoon: A Multidimensional Assessment and Analysis of News Recommendation Systems](https://arxiv.org/abs/2509.11139)
*Xin Wang,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: 文章提出多维分析评估信息茧房，通过实验对比算法并设计缓解策略，建立统一度量框架并提供伦理推荐系统解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决先前研究中缺乏综合评估框架的问题，应对个性化新闻推荐系统产生的信息茧房及社会两极分化问题。

Method: 从个体同质化和群体两极分化双视角进行多维分析，在真实数据集上进行多轮实验。

Result: 对七种算法进行基准测试，揭示关键见解，设计了五种轻量级缓解策略。

Conclusion: 建立了首个信息茧房统一度量框架，为伦理推荐系统提供可部署解决方案。

Abstract: Personalized news recommendation systems inadvertently create information
cocoons--homogeneous information bubbles that reinforce user biases and amplify
societal polarization. To address the lack of comprehensive assessment
frameworks in prior research, we propose a multidimensional analysis that
evaluates cocoons through dual perspectives: (1) Individual homogenization via
topic diversity (including the number of topic categories and category
information entropy) and click repetition; (2) Group polarization via network
density and community openness. Through multi-round experiments on real-world
datasets, we benchmark seven algorithms and reveal critical insights.
Furthermore, we design five lightweight mitigation strategies. This work
establishes the first unified metric framework for information cocoons and
delivers deployable solutions for ethical recommendation systems.

</details>


### [97] [Do Large Language Models Favor Recent Content? A Study on Recency Bias in LLM-Based Reranking](https://arxiv.org/abs/2509.11353)
*Hanpei Fang,Sijie Tao,Nuo Chen,Kai-Xin Chang,Tetsuya Sakai*

Main category: cs.IR

TL;DR: 研究发现大语言模型在信息检索中存在普遍的近期偏差，即使大模型也无法消除，需有效策略缓解偏差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在信息系统中应用广泛，但对其近期偏差的关注较少，故展开研究。

Method: 在TREC Deep Learning的文档中添加人工出版日期，对七个模型进行列表重排和成对偏好实验。

Result: 七个模型均提升了“新”文档排名，前10平均出版年份最多提前4.78年，个别文档排名变动达95位；成对偏好实验中，日期注入后模型偏好逆转平均达25%；大模型可减弱但无法消除该影响。

Conclusion: 大语言模型存在普遍的近期偏差，需有效偏差缓解策略。

Abstract: Large language models (LLMs) are increasingly deployed in information
systems, including being used as second-stage rerankers in information
retrieval pipelines, yet their susceptibility to recency bias has received
little attention. We investigate whether LLMs implicitly favour newer documents
by prepending artificial publication dates to passages in the TREC Deep
Learning passage retrieval collections in 2021 (DL21) and 2022 (DL22). Across
seven models, GPT-3.5-turbo, GPT-4o, GPT-4, LLaMA-3 8B/70B, and Qwen-2.5
7B/72B, "fresh" passages are consistently promoted, shifting the Top-10's mean
publication year forward by up to 4.78 years and moving individual items by as
many as 95 ranks in our listwise reranking experiments. Although larger models
attenuate the effect, none eliminate it. We also observe that the preference of
LLMs between two passages with an identical relevance level can be reversed by
up to 25% on average after date injection in our pairwise preference
experiments. These findings provide quantitative evidence of a pervasive
recency bias in LLMs and highlight the importance of effective bias-mitigation
strategies.

</details>


### [98] [Decoding in Latent Spaces for Efficient Inference in LLM-based Recommendation](https://arxiv.org/abs/2509.11524)
*Chengbing Wang,Yang Zhang,Zhicheng Wang,Tianhao Shi,Keqin Bao,Fuli Feng,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 本文提出Light Latent - space Decoding (L2D)方法，绕过语言空间解码，在隐空间匹配候选项目，比语言空间解码快超10倍且能保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型用于推荐时，语言空间的自回归解码带来显著推理开销，需降低计算成本。

Method: 引入L2D方法，用测试序列隐藏状态表示用户偏好项目，从训练序列隐藏状态获取候选项目表示，匹配两者实现隐空间解码。

Result: L2D比语言空间解码快超10倍，且能保持或提升性能。

Conclusion: L2D能在不改变大语言模型生成式调优范式下实现高效解码，有效降低计算成本。

Abstract: Fine-tuning large language models (LLMs) for recommendation in a generative
manner has delivered promising results, but encounters significant inference
overhead due to autoregressive decoding in the language space. This work
explores bypassing language-space decoding by directly matching candidate items
with the LLM's internal thought representations in the latent space,
eliminating the time-consuming autoregressive process to reduce computational
costs. Towards this, we introduce Light Latent-space Decoding (L2D), an
effective and efficient latent-space decoding method. L2D represents
user-preferred items by using the hidden states of test sequences reflecting
the LLM's internal thought, and obtains candidate item representations from the
hidden states of training sequences labeled with the corresponding candidate
items. It then matches the two types of representations to decode items,
achieving latent-space decoding. In this way, it enables efficient decoding
without altering the LLM's generative tuning paradigm, thereby preserving
performance. Extensive empirical results demonstrate that L2D is more than 10x
faster than language-space decoding while maintaining or enhancing performance.

</details>


### [99] [Data-Driven Analysis of Text-Conditioned AI-Generated Music: A Case Study with Suno and Udio](https://arxiv.org/abs/2509.11824)
*Luca Casini,Laura Cros Vila,David Dalmazzo,Anna-Kaisa Kaila,Bob L. T. Sturm*

Main category: cs.IR

TL;DR: 文章基于2024年5-10月Suno和Udio平台用户生成的歌曲，分析其使用情况和用户灵感来源，分享代码资源促进AI音乐研究。


<details>
  <summary>Details</summary>
Motivation: 了解在线AI音乐平台的使用方式和用户灵感来源。

Method: 结合先进文本嵌入模型、降维和聚类方法，分析提示、标签和歌词，自动注释并以交互式图表展示数据。

Result: 揭示了歌词主题、语言偏好、提示策略以及使用元标签引导模型的独特尝试。

Conclusion: 分享代码和资源以促进AI生成音乐的音乐学研究。

Abstract: Online AI platforms for creating music from text prompts (AI music), such as
Suno and Udio, are now being used by hundreds of thousands of users. Some AI
music is appearing in advertising, and even charting, in multiple countries.
How are these platforms being used? What subjects are inspiring their users?
This article answers these questions for Suno and Udio using a large collection
of songs generated by users of these platforms from May to October 2024. Using
a combination of state-of-the-art text embedding models, dimensionality
reduction and clustering methods, we analyze the prompts, tags and lyrics, and
automatically annotate and display the processed data in interactive plots. Our
results reveal prominent themes in lyrics, language preference, prompting
strategies, as well as peculiar attempts at steering models through the use of
metatags. To promote the musicological study of the developing cultural
practice of AI-generated music we share our code and resources.

</details>


### [100] [AEFS: Adaptive Early Feature Selection for Deep Recommender Systems](https://arxiv.org/abs/2509.12076)
*Fan Hu,Gaofeng Lu,Jun Chen,Chaonan Guo,Yuekui Yang,Xirong Li*

Main category: cs.IR

TL;DR: 介绍自适应早期特征选择(AEFS)方法，可自适应选特征并减少嵌入层激活参数，实验验证其有效性且开源。


<details>
  <summary>Details</summary>
Motivation: 现有自适应特征选择在大规模推荐系统中因后嵌入层特征选择存在效率问题，需改进。

Method: 采用双模型架构，含特征选择辅助模型和预测主模型，加入两个协作训练损失约束。

Result: 在三个基准数据集上验证方法有效，性能与现有自适应晚期特征选择方法相当，嵌入层激活参数减少37.5%。

Conclusion: AEFS方法简单有效，能解决现有自适应特征选择问题，且已开源。

Abstract: Feature selection has emerged as a crucial technique in refining recommender
systems. Recent advancements leveraging Automated Machine Learning (AutoML) has
drawn significant attention, particularly in two main categories: early feature
selection and late feature selection, differentiated by whether the selection
occurs before or after the embedding layer. The early feature selection selects
a fixed subset of features and retrains the model, while the late feature
selection, known as adaptive feature selection, dynamically adjusts feature
choices for each data instance, recognizing the variability in feature
significance. Although adaptive feature selection has shown remarkable
improvements in performance, its main drawback lies in its post-embedding layer
feature selection. This process often becomes cumbersome and inefficient in
large-scale recommender systems with billions of ID-type features, leading to a
highly sparse and parameter-heavy embedding layer. To overcome this, we
introduce Adaptive Early Feature Selection (AEFS), a very simple method that
not only adaptively selects informative features for each instance, but also
significantly reduces the activated parameters of the embedding layer. AEFS
employs a dual-model architecture, encompassing an auxiliary model dedicated to
feature selection and a main model responsible for prediction. To ensure
effective alignment between these two models, we incorporate two collaborative
training loss constraints. Our extensive experiments on three benchmark
datasets validate the efficiency and effectiveness of our approach. Notably,
AEFS matches the performance of current state-of-theart Adaptive Late Feature
Selection methods while achieving a significant reduction of 37. 5% in the
activated parameters of the embedding layer. AEFS is open-source at
https://github. com/fly-dragon211/AEFS .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [101] [The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results](https://arxiv.org/abs/2509.10463)
*Qiuyu Chen,Xin Jin,Yue Song,Xihui Liu,Shuai Yang,Tao Yang,Ziqiang Li,Jianguo Huang,Yuntao Wei,Ba'ao Xie,Nicu Sebe,Wenjun,Zeng,Jooyeol Yun,Davide Abati,Mohamed Omran,Jaegul Choo,Amir Habibian,Auke Wiggers,Masato Kobayashi,Ning Ding,Toru Tamaki,Marzieh Gheisari,Auguste Genovesio,Yuheng Chen,Dingkun Liu,Xinyao Yang,Xinping Xu,Baicheng Chen,Dongrui Wu,Junhao Geng,Lexiang Lv,Jianxin Lin,Hanzhe Liang,Jie Zhou,Xuanxin Chen,Jinbao Wang,Can Gao,Zhangyi Wang,Zongze Li,Bihan Wen,Yixin Gao,Xiaohan Pan,Xin Li,Zhibo Chen,Baorui Peng,Zhongming Chen,Haoran Jin*

Main category: cs.LG

TL;DR: 本文回顾了与ICCV 2025同期举办的第一届可控制生成的解纠缠表征学习国际研讨会（DRL4Real），介绍其目标、接收论文主题及作者提出的方法。


<details>
  <summary>Details</summary>
Motivation: 缩小解纠缠表征学习（DRL）理论承诺与现实场景应用之间的差距，超越合成基准。

Method: 研讨会聚焦于在可控生成等实际应用中评估DRL方法，探索模型鲁棒性、可解释性和泛化性的进展。

Result: 研讨会接收了9篇涵盖广泛主题的论文，包括整合新归纳偏置、将扩散模型应用于DRL等。

Conclusion: 总结了研讨会目标、接收论文主题，并概述了作者提出的方法论。

Abstract: This paper reviews the 1st International Workshop on Disentangled
Representation Learning for Controllable Generation (DRL4Real), held in
conjunction with ICCV 2025. The workshop aimed to bridge the gap between the
theoretical promise of Disentangled Representation Learning (DRL) and its
application in realistic scenarios, moving beyond synthetic benchmarks.
DRL4Real focused on evaluating DRL methods in practical applications such as
controllable generation, exploring advancements in model robustness,
interpretability, and generalization. The workshop accepted 9 papers covering a
broad range of topics, including the integration of novel inductive biases
(e.g., language), the application of diffusion models to DRL, 3D-aware
disentanglement, and the expansion of DRL into specialized domains like
autonomous driving and EEG analysis. This summary details the workshop's
objectives, the themes of the accepted papers, and provides an overview of the
methodologies proposed by the authors.

</details>


### [102] [Moment Estimates and DeepRitz Methods on Learning Diffusion Systems with Non-gradient Drifts](https://arxiv.org/abs/2509.10495)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: cs.LG

TL;DR: 提出Moment - DeepRitz方法学习广义扩散系统中保守 - 耗散动力学的漂移分解，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 保守 - 耗散动力学在复杂开放系统中普遍存在，需方法学习广义扩散系统中的漂移分解。

Method: 提出数据驱动的两阶段方法Moment - DeepRitz方法。

Result: 该方法对噪声数据有鲁棒性，能适应粗糙势和振荡旋转。

Conclusion: 通过数值实验证明了方法的有效性。

Abstract: Conservative-dissipative dynamics are ubiquitous across a variety of complex
open systems. We propose a data-driven two-phase method, the Moment-DeepRitz
Method, for learning drift decompositions in generalized diffusion systems
involving conservative-dissipative dynamics. The method is robust to noisy
data, adaptable to rough potentials and oscillatory rotations. We demonstrate
its effectiveness through several numerical experiments.

</details>


### [103] [Enhancing ML Models Interpretability for Credit Scoring](https://arxiv.org/abs/2509.11389)
*Sagi Schwartz,Qinling Wang,Fang Fang*

Main category: cs.LG

TL;DR: 本文提出混合方法，用黑盒模型事后解释指导特征选择，训练兼具预测力与透明度的白盒模型，在Lending Club数据集上效果好，还可通过模型细化提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习方法缺乏透明度，多数XAI研究不能满足监管要求，需新方法兼顾预测力与透明度。

Method: 采用混合方法，用黑盒模型事后解释指导特征选择，再训练白盒模型，还进行模型细化。

Result: 在Lending Club数据集上，该方法用10个特征达到与基准黑盒模型相当的性能，特征减少88.5%。

Conclusion: 提出的混合方法有效，模型细化可进一步提升模型可解释性和鲁棒性。

Abstract: Predicting default is essential for banks to ensure profitability and
financial stability. While modern machine learning methods often outperform
traditional regression techniques, their lack of transparency limits their use
in regulated environments. Explainable artificial intelligence (XAI) has
emerged as a solution in domains like credit scoring. However, most XAI
research focuses on post-hoc interpretation of black-box models, which does not
produce models lightweight or transparent enough to meet regulatory
requirements, such as those for Internal Ratings-Based (IRB) models.
  This paper proposes a hybrid approach: post-hoc interpretations of black-box
models guide feature selection, followed by training glass-box models that
maintain both predictive power and transparency.
  Using the Lending Club dataset, we demonstrate that this approach achieves
performance comparable to a benchmark black-box model while using only 10
features - an 88.5% reduction. In our example, SHapley Additive exPlanations
(SHAP) is used for feature selection, eXtreme Gradient Boosting (XGBoost)
serves as the benchmark and the base black-box model, and Explainable Boosting
Machine (EBM) and Penalized Logistic Tree Regression (PLTR) are the
investigated glass-box models.
  We also show that model refinement using feature interaction analysis,
correlation checks, and expert input can further enhance model interpretability
and robustness.

</details>


### [104] [SOH-KLSTM: A Hybrid Kolmogorov-Arnold Network and LSTM Model for Enhanced Lithium-Ion Battery Health Monitoring](https://arxiv.org/abs/2509.10496)
*Imen Jarraya,Safa Ben Atitallah,Fatimah Alahmeda,Mohamed Abdelkadera,Maha Drissa,Fatma Abdelhadic,Anis Koubaaa*

Main category: cs.LG

TL;DR: 提出用于锂电池健康监测的SOH - KLSTM预测框架，结合LSTM与KAN能力。


<details>
  <summary>Details</summary>
Motivation: 传统SOH估计技术无法有效呈现电池退化的非线性和时间特征，准确可靠的锂电池SOH估计对多领域应用很关键。

Method: 提出使用KAN - 集成候选单元状态在LSTM中的SOH预测框架（SOH - KLSTM），结合LSTM学习长期依赖和KAN非线性近似能力。

Result: 未提及

Conclusion: 未提及

Abstract: Accurate and reliable State Of Health (SOH) estimation for Lithium (Li)
batteries is critical to ensure the longevity, safety, and optimal performance
of applications like electric vehicles, unmanned aerial vehicles, consumer
electronics, and renewable energy storage systems. Conventional SOH estimation
techniques fail to represent the non-linear and temporal aspects of battery
degradation effectively. In this study, we propose a novel SOH prediction
framework (SOH-KLSTM) using Kolmogorov-Arnold Network (KAN)-Integrated
Candidate Cell State in LSTM for Li batteries Health Monitoring. This hybrid
approach combines the ability of LSTM to learn long-term dependencies for
accurate time series predictions with KAN's non-linear approximation
capabilities to effectively capture complex degradation behaviors in Lithium
batteries.

</details>


### [105] [Exploring Multi-view Symbolic Regression methods in physical sciences](https://arxiv.org/abs/2509.10500)
*Etienne Russeil,Fabrício Olivetti de França,Konstantin Malanchev,Guillaume Moinard,Maxime Cherrey*

Main category: cs.LG

TL;DR: 本文测试并比较Operon、PySR、phy - SO和eggp中支持的多视图符号回归（MvSR）在不同真实数据集上的表现，展示其准确性，指出特定特征利于生成更好模型并给出未来开发指南。


<details>
  <summary>Details</summary>
Motivation: 传统通过第一性原理和观察推导方程，现代可采用符号回归（SR），多视图符号回归（MvSR）可缓解过拟合和数据稀缺问题，多个实现对MvSR支持有细微差异，需对其进行测试比较。

Method: 在不同真实世界数据集上测试和比较Operon、PySR、phy - SO和eggp中支持的MvSR。

Result: MvSR在不同数据集上常能达到较好的准确性，且提出的解决方案自由参数较少，某些特征能更频繁地生成更好的模型。

Conclusion: 为未来MvSR的发展提供了指导方针。

Abstract: Describing the world behavior through mathematical functions help scientists
to achieve a better understanding of the inner mechanisms of different
phenomena. Traditionally, this is done by deriving new equations from first
principles and careful observations. A modern alternative is to automate part
of this process with symbolic regression (SR). The SR algorithms search for a
function that adequately fits the observed data while trying to enforce
sparsity, in the hopes of generating an interpretable equation. A particularly
interesting extension to these algorithms is the Multi-view Symbolic Regression
(MvSR). It searches for a parametric function capable of describing multiple
datasets generated by the same phenomena, which helps to mitigate the common
problems of overfitting and data scarcity. Recently, multiple implementations
added support to MvSR with small differences between them. In this paper, we
test and compare MvSR as supported in Operon, PySR, phy-SO, and eggp, in
different real-world datasets. We show that they all often achieve good
accuracy while proposing solutions with only few free parameters. However, we
find that certain features enable a more frequent generation of better models.
We conclude by providing guidelines for future MvSR developments.

</details>


### [106] [From Noise to Precision: A Diffusion-Driven Approach to Zero-Inflated Precipitation Prediction](https://arxiv.org/abs/2509.10501)
*Wentao Gao,Jiuyong Li,Lin Liu,Thuc Duy Le,Xiongren Chen,Xiaojing Du,Jixue Liu,Yanchang Zhao,Yun Chen*

Main category: cs.LG

TL;DR: 提出零膨胀扩散框架（ZIDF）用于降水预测，在实验中表现优于多个先进模型，显示出处理稀疏时间序列数据的能力和潜在泛化性。


<details>
  <summary>Details</summary>
Motivation: 零膨胀数据中零值占主导且非零事件稀疏，给降水预测带来重大挑战，需解决该问题。

Method: 提出ZIDF，整合高斯扰动平滑零膨胀分布、基于Transformer的预测捕捉时间模式、基于扩散的去噪恢复原始数据结构。

Result: ZIDF在多个先进降水预测模型上表现显著提升，相对于基线非平稳Transformer，MSE最多降低56.7%，MAE最多降低21.1%。

Conclusion: ZIDF能稳健处理稀疏时间序列数据，在零膨胀是关键挑战的其他领域有潜在泛化性。

Abstract: Zero-inflated data pose significant challenges in precipitation forecasting
due to the predominance of zeros with sparse non-zero events. To address this,
we propose the Zero Inflation Diffusion Framework (ZIDF), which integrates
Gaussian perturbation for smoothing zero-inflated distributions,
Transformer-based prediction for capturing temporal patterns, and
diffusion-based denoising to restore the original data structure. In our
experiments, we use observational precipitation data collected from South
Australia along with synthetically generated zero-inflated data. Results show
that ZIDF demonstrates significant performance improvements over multiple
state-of-the-art precipitation forecasting models, achieving up to 56.7\%
reduction in MSE and 21.1\% reduction in MAE relative to the baseline
Non-stationary Transformer. These findings highlight ZIDF's ability to robustly
handle sparse time series data and suggest its potential generalizability to
other domains where zero inflation is a key challenge.

</details>


### [107] [FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free](https://arxiv.org/abs/2509.10503)
*Haolin Yuan,Jingtao Li,Weiming Zhuang,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: 提出FEDEXCHANGE框架解决联合目标检测跨域泛化问题，在不增加客户端计算开销下提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有FOD方法忽略边缘设备硬件约束且计算成本高，跨域泛化存在挑战。

Method: 采用服务器端动态模型交换策略，在聚合和交换轮次交替操作，基于距离度量聚类和交换本地模型。

Result: 在具有挑战性的领域（如雨天）平均精度提升1.6倍，仅需基线方法0.8倍计算资源。

Conclusion: FEDEXCHANGE能在不增加客户端计算开销的情况下提升FOD性能。

Abstract: Federated Object Detection (FOD) enables clients to collaboratively train a
global object detection model without accessing their local data from diverse
domains. However, significant variations in environment, weather, and other
domain specific factors hinder performance, making cross domain generalization
a key challenge. Existing FOD methods often overlook the hardware constraints
of edge devices and introduce local training regularizations that incur high
computational costs, limiting real-world applicability. In this paper, we
propose FEDEXCHANGE, a novel FOD framework that bridges domain gaps without
introducing additional local computational overhead. FEDEXCHANGE employs a
server side dynamic model exchange strategy that enables each client to gain
insights from other clients' domain data without direct data sharing.
Specifically, FEDEXCHANGE allows the server to alternate between model
aggregation and model exchange. During aggregation rounds, the server
aggregates all local models as usual. In exchange rounds, FEDEXCHANGE clusters
and exchanges local models based on distance measures, allowing local models to
learn from a variety of domains. As all operations are performed on the server
side, clients can achieve improved cross domain utility without any additional
computational overhead. Extensive evaluations demonstrate that FEDEXCHANGE
enhances FOD performance, achieving 1.6X better mean average precision in
challenging domains, such as rainy conditions, while requiring only 0.8X the
computational resources compared to baseline methods.

</details>


### [108] [A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2509.10512)
*Jiaxing Cao,Yuzhou Gao,Jiwei Huang*

Main category: cs.LG

TL;DR: 本文从服务导向视角提出自适应激励机制解决联邦学习数据不足问题，用Stackelberg博弈和MAMDP建模，设计ASOSA算法，实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习有时面临训练数据不足的问题，需要招募工人收集数据，因此要设计激励机制最大化任务发布者、本地模型所有者和工人的效用。

Method: 在本地模型所有者和任务发布者之间建立Stackelberg博弈求Nash均衡解；用多智能体马尔可夫决策过程（MAMDP）描述本地模型所有者和工人的交互，通过深度强化学习（DRL）确定最优策略；设计自适应搜索最优策略算法（ASOSA）稳定参与者策略和解决耦合问题。

Result: 进行了大量数值实验。

Conclusion: 提出的自适应激励机制有效。

Abstract: Recently, federated learning (FL) has emerged as a novel framework for
distributed model training. In FL, the task publisher (TP) releases tasks, and
local model owners (LMOs) use their local data to train models. Sometimes, FL
suffers from the lack of training data, and thus workers are recruited for
gathering data. To this end, this paper proposes an adaptive incentive
mechanism from a service-oriented perspective, with the objective of maximizing
the utilities of TP, LMOs and workers. Specifically, a Stackelberg game is
theoretically established between the LMOs and TP, positioning TP as the leader
and the LMOs as followers. An analytical Nash equilibrium solution is derived
to maximize their utilities. The interaction between LMOs and workers is
formulated by a multi-agent Markov decision process (MAMDP), with the optimal
strategy identified via deep reinforcement learning (DRL). Additionally, an
Adaptively Searching the Optimal Strategy Algorithm (ASOSA) is designed to
stabilize the strategies of each participant and solve the coupling problems.
Extensive numerical experiments are conducted to validate the efficacy of the
proposed method.

</details>


### [109] [Foundational theory for optimal decision tree problems. I. Algorithmic and geometric foundations](https://arxiv.org/abs/2509.11226)
*Xi He*

Main category: cs.LG

TL;DR: 论文第一部分给出ODT问题四个新定义并推导算法，第二部分给出最优超曲面决策树算法并实验，显示灵活分裂规则决策树潜力。


<details>
  <summary>Details</summary>
Motivation: 为ODT问题提供新的问题定义和高效算法，探究灵活分裂规则决策树的潜力。

Method: 基于代数编程理论，通过可执行递归程序明确问题定义，从规范中推导算法。

Result: 得到四个ODT问题新算法，实验证明灵活分裂规则决策树有显著潜力，框架可扩展。

Conclusion: 提出的定义和算法为ODT问题提供统一、高效、优雅的解决方案，灵活分裂规则决策树有应用前景。

Abstract: In the first paper (part I) of this series of two, we introduce four novel
definitions of the ODT problems: three for size-constrained trees and one for
depth-constrained trees. These definitions are stated unambiguously through
executable recursive programs, satisfying all criteria we propose for a formal
specification. In this sense, they resemble the "standard form" used in the
study of general-purpose solvers.
  Grounded in algebraic programming theory-a relational formalism for deriving
correct-by-construction algorithms from specifications-we can not only
establish the existence or nonexistence of dynamic programming solutions but
also derive them constructively whenever they exist. Consequently, the four
generic problem definitions yield four novel optimal algorithms for ODT
problems with arbitrary splitting rules that satisfy the axioms and objective
functions of a given form. These algorithms encompass the known
depth-constrained, axis-parallel ODT algorithm as the special case, while
providing a unified, efficient, and elegant solution for the general ODT
problem.
  In Part II, we present the first optimal hypersurface decision tree algorithm
and provide comprehensive experiments against axis-parallel decision tree
algorithms, including heuristic CART and state-of-the-art optimal methods. The
results demonstrate the significant potential of decision trees with flexible
splitting rules. Moreover, our framework is readily extendable to support
algorithms for constructing even more flexible decision trees, including those
with mixed splitting rules.

</details>


### [110] [Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs](https://arxiv.org/abs/2509.10504)
*Mianchu Wang,Giovanni Montana*

Main category: cs.LG

TL;DR: 本文将逆合成规划问题转化为树结构马尔可夫决策过程中的最坏路径优化问题，提出InterRetro方法，在基准测试中取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有逆合成规划方法在合成路线中存在“最弱环节”问题，且常优化平均性能，未考虑最坏情况的敏感性。

Method: 将逆合成问题重新定义为树结构马尔可夫决策过程中的最坏路径优化问题，提出InterRetro方法，与树MDP交互，学习最坏路径结果的价值函数，并通过自我模仿改进策略。

Result: InterRetro在Retro*-190基准测试中解决了100%的目标，缩短了4.9%的合成路线，仅使用10%的训练数据就取得了有前景的性能。

Conclusion: InterRetro方法在计算逆合成规划方面取得了显著进展。

Abstract: Retrosynthesis planning aims to decompose target molecules into available
building blocks, forming a synthesis tree where each internal node represents
an intermediate compound and each leaf ideally corresponds to a purchasable
reactant. However, this tree becomes invalid if any leaf node is not a valid
building block, making the planning process vulnerable to the "weakest link" in
the synthetic route. Existing methods often optimise for average performance
across branches, failing to account for this worst-case sensitivity. In this
paper, we reframe retrosynthesis as a worst-path optimisation problem within
tree-structured Markov Decision Processes (MDPs). We prove that this
formulation admits a unique optimal solution and offers monotonic improvement
guarantees. Building on this insight, we introduce Interactive Retrosynthesis
Planning (InterRetro), a method that interacts with the tree MDP, learns a
value function for worst-path outcomes, and improves its policy through
self-imitation, preferentially reinforcing past decisions with high estimated
advantage. Empirically, InterRetro achieves state-of-the-art results, solving
100% of targets on the Retro*-190 benchmark, shortening synthetic routes by
4.9%, and achieving promising performance using only 10% of the training data -
representing a significant advance in computational retrosynthesis planning.

</details>


### [111] [AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective](https://arxiv.org/abs/2509.10506)
*Muxin Ge,Hanyu Ma,Yiyang Wu,Xiaoli Ma,Yadi Liu,Ye Aung Moe,Weizheng Xie*

Main category: cs.LG

TL;DR: 提出AttnBoost框架，结合特征级注意力提升零售供应链产品需求预测的准确性与可解释性，在数据集上表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 零售供应链产品需求预测复杂，传统GBDT缺乏适应机制，需提升预测准确性和可解释性。

Method: 提出AttnBoost框架，在提升过程中集成特征级注意力，通过轻量级注意力机制动态调整特征重要性。

Result: 在大规模零售销售数据集上，AttnBoost优于标准机器学习和深度表格模型，消融研究证实注意力模块可缓解过拟合、提升可解释性。

Conclusion: 注意力引导的提升方法是现实世界预测应用中可解释和可扩展AI的有前景方向。

Abstract: Forecasting product demand in retail supply chains presents a complex
challenge due to noisy, heterogeneous features and rapidly shifting consumer
behavior. While traditional gradient boosting decision trees (GBDT) offer
strong predictive performance on structured data, they often lack adaptive
mechanisms to identify and emphasize the most relevant features under changing
conditions. In this work, we propose AttnBoost, an interpretable learning
framework that integrates feature-level attention into the boosting process to
enhance both predictive accuracy and explainability. Specifically, the model
dynamically adjusts feature importance during each boosting round via a
lightweight attention mechanism, allowing it to focus on high-impact variables
such as promotions, pricing, and seasonal trends. We evaluate AttnBoost on a
large-scale retail sales dataset and demonstrate that it outperforms standard
machine learning and deep tabular models, while also providing actionable
insights for supply chain managers. An ablation study confirms the utility of
the attention module in mitigating overfitting and improving interpretability.
Our results suggest that attention-guided boosting represents a promising
direction for interpretable and scalable AI in real-world forecasting
applications.

</details>


### [112] [The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback](https://arxiv.org/abs/2509.10509)
*Sai Teja Reddy Adapala*

Main category: cs.LG

TL;DR: 本文引入选择性反馈机制挑战大语言模型递归训练会崩溃的主流理论，发现能提升模型性能，提出反衔尾蛇效应，为开发安全鲁棒AI系统提供原则。


<details>
  <summary>Details</summary>
Motivation: 解决递归训练大语言模型的稳定性这一AI安全基础问题，挑战模型训练会崩溃的主流理论。

Method: 引入选择性反馈机制，并与简单分类器的基础实验对比。

Result: 在复杂摘要任务中，选择性反馈机制使Gemma 2B模型性能显著提升，有质量过滤条件的五代模型ROUGE - L F1得分提高6.6%，未过滤和随机过滤对照组分别下降3.5%和4.2%。

Conclusion: 简单选择压力下，大语言模型可涌现出系统弹性，为开发更安全鲁棒的AI系统提供强大且可扩展的原则。

Abstract: The stability of recursively trained large language models (LLMs) is a
foundational problem for AI safety. Prevailing theory predicts model collapse,
a progressive degradation when models are trained on their own output. We
challenge this narrative by introducing a selective feedback mechanism.
Contrary to expectation, instead of merely slowing decay, our experiments
provide strong evidence that this pressure reverses it, inducing a
statistically significant performance improvement in a Gemma 2B model on a
complex summarization task. We name this phenomenon the Anti-Ouroboros Effect.
We contrast this with a foundational experiment using a simple classifier,
where the theoretical degenerative loop was validated, highlighting the unique
dynamics of high-dimensional models. Our findings establish that systemic
resilience can be an emergent property of LLMs under simple selection pressure,
suggesting a powerful and scalable principle for developing safer and more
robust AI systems. Across five generations, a quality-filtered condition
improved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by
3.5% and a random-filter control degraded by 4.2%

</details>


### [113] [Designing MacPherson Suspension Architectures using Bayesian Optimization](https://arxiv.org/abs/2206.09022)
*Sinnu Susan Thomas,Jacopo Palandri,Mohsen Lakehal-ayat,Punarjay Chakravarty,Friedrich Wolf-Monheim,Matthew B. Blaschko*

Main category: cs.LG

TL;DR: 开发贝叶斯优化系统部分自动化工程设计流程，以车辆底盘设计问题验证方法通用、可扩展且高效。


<details>
  <summary>Details</summary>
Motivation: 传统工程设计手动进行耗时且成本高，需自动化方法提高效率和降低成本。

Method: 开发贝叶斯优化系统，计算高维非线性函数广义逆，提出两层收敛准则。

Result: 在车辆底盘设计问题上验证方法通用、可扩展且高效，收敛准则可基于现有软件包实现。

Conclusion: 提出的方法和收敛准则有效，可用于工程设计自动化。

Abstract: Engineering design is traditionally performed by hand: an expert makes design
proposals based on past experience, and these proposals are then tested for
compliance with certain target specifications. Testing for compliance is
performed first by computer simulation using what is called a discipline model.
Such a model can be implemented by a finite element analysis, multibody systems
approach, etc. Designs passing this simulation are then considered for physical
prototyping. The overall process may take months, and is a significant cost in
practice. We have developed a Bayesian optimization system for partially
automating this process by directly optimizing compliance with the target
specification with respect to the design parameters. The proposed method is a
general framework for computing a generalized inverse of a high-dimensional
non-linear function that does not require e.g. gradient information, which is
often unavailable from discipline models. We furthermore develop a two-tier
convergence criterion based on (i) convergence to a solution optimally
satisfying all specified design criteria, or (ii) convergence to a minimum-norm
solution. We demonstrate the proposed approach on a vehicle chassis design
problem motivated by an industry setting using a state-of-the-art commercial
discipline model. We show that the proposed approach is general, scalable, and
efficient, and that the novel convergence criteria can be implemented
straightforwardly based on existing concepts and subroutines in popular
Bayesian optimization software packages.

</details>


### [114] [LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs](https://arxiv.org/abs/2509.10511)
*Umberto Gonçalves de Sousa*

Main category: cs.LG

TL;DR: 提出LogGuardQ框架，结合人类认知双记忆系统和自适应探索策略，在模拟访问日志数据集上评估，表现优于DQN和PPO，为不确定环境下自适应学习提供可扩展方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法如DQN和PPO在动态环境中探索效率、稳定性和适应性不足。

Method: 提出LogGuardQ框架，集成受人类认知启发的双记忆系统和由温度衰减与好奇心驱动的自适应探索策略。

Result: 在数据集上，LogGuardQ检测率达96.0%，有较高的精确率、召回率和F1分数，平均奖励等指标表现更好，图形分析显示其稳定性和效率更优，统计测试证明性能优势显著。

Conclusion: LogGuardQ通过结合认知科学和强化学习，为不确定环境下自适应学习提供可扩展方法，有潜在应用价值。

Abstract: Reinforcement learning (RL) has transformed sequential decision-making, but
traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy
Optimization (PPO) often struggle with efficient exploration, stability, and
adaptability in dynamic environments. This study presents LogGuardQ (Adaptive
Log Guard with Cognitive enhancement), a novel framework that integrates a
dual-memory system inspired by human cognition and adaptive exploration
strategies driven by temperature decay and curiosity. Evaluated on a dataset of
1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes,
LogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for
PPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450.
The mean reward is 20.34 \pm 44.63 across all episodes (versus 18.80 \pm 43.98
for DQN and -0.17 \pm 23.79 for PPO), with an average of 5.0 steps per episode
(constant across models). Graphical analyses, including learning curves
smoothed with a Savgol filter (window=501, polynomial=2), variance trends,
action distributions, and cumulative detections, demonstrate LogGuardQ's
superior stability and efficiency. Statistical tests (Mann-Whitney U) confirm
significant performance advantages (e.g., p = 0.0002 vs. DQN with negligible
effect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN
vs. PPO with small effect size). By bridging cognitive science and RL,
LogGuardQ offers a scalable approach to adaptive learning in uncertain
environments, with potential applications in cybersecurity, intrusion
detection, and decision-making under uncertainty.

</details>


### [115] [Online Omniprediction with Long-Term Constraints](https://arxiv.org/abs/2509.11357)
*Yahav Bechavod,Jiuyao Lu,Aaron Roth*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce and study the problem of online omniprediction with long-term
constraints. At each round, a forecaster is tasked with generating predictions
for an underlying (adaptively, adversarially chosen) state that are broadcast
to a collection of downstream agents, who must each choose an action. Each of
the downstream agents has both a utility function mapping actions and state to
utilities, and a vector-valued constraint function mapping actions and states
to vector-valued costs. The utility and constraint functions can arbitrarily
differ across downstream agents. Their goal is to choose actions that guarantee
themselves no regret while simultaneously guaranteeing that they do not
cumulatively violate the constraints across time. We show how to make a single
set of predictions so that each of the downstream agents can guarantee this by
acting as a simple function of the predictions, guaranteeing each of them
$\tilde{O}(\sqrt{T})$ regret and $O(1)$ cumulative constraint violation. We
also show how to extend our guarantees to arbitrary intersecting contextually
defined \emph{subsequences}, guaranteeing each agent both regret and constraint
violation bounds not just marginally, but simultaneously on each subsequence,
against a benchmark set of actions simultaneously tailored to each subsequence.

</details>


### [116] [Mixture-of-Clustered-Experts: Advancing Expert Specialization and Generalization in Instruction Tuning](https://arxiv.org/abs/2509.10513)
*Sugyeong Eo,Jungjun Lee,Chanjun Park,Heuiseok Lim*

Main category: cs.LG

TL;DR: 提出Mixture-of-Clustered-Experts (MoCE)解决稀疏混合专家（MoE）架构在指令调优场景下的问题，经评估优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决MoE架构在指令调优场景中，提高专家专业化以增强性能和泛化能力的挑战。

Method: 提出MoCE，采用双阶段路由机制，第一阶段基于序列级特征进行专家组路由，第二阶段在组内按token级激活前k个专家。

Result: 在一系列基准测试中，MoCE始终优于强基线模型，展现出更强的泛化能力。

Conclusion: MoCE具有鲁棒性和有效性。

Abstract: A sparse Mixture-of-Experts (MoE) architecture has emerged as a highly
scalable solution by conditionally activating sub-modules without a
proportional increase in computational costs. However, improving expert
specialization to enhance performance and generalization remains a challenge
for MoE, especially in instruction tuning scenarios characterized by
significant input heterogeneity. In this work, we propose the
Mixture-of-Clustered-Experts (MoCE) to address this limitation through a
dual-stage routing mechanism. The first stage in the mechanism performs expert
group routing based on sequence-level features, while the second stage
activates the top-$k$ experts within the group at the token level. This
approach enables the effective partitioning of heterogeneous inputs based on
their knowledge requirements, encouraging expert group specialization while
maintaining the advantages of token-level routing. We evaluate MoCE across a
comprehensive set of benchmarks, demonstrating its consistent superiority over
strong baselines and its enhanced generalization capabilities. Detailed
analysis further highlights the robustness and effectiveness of MoCE.

</details>


### [117] [A Differential Manifold Perspective and Universality Analysis of Continuous Attractors in Artificial Neural Networks](https://arxiv.org/abs/2509.10514)
*Shaoxin Tian,Hongkai Liu,Yuying Yang,Jiali Yu,Zizheng Miao,Xuming Huang,Zhishuai Liu,Zhang Yi*

Main category: cs.LG

TL;DR: 本文建立新框架研究人工神经网络连续吸引子，验证与先前结论兼容性，揭示相关现象与特征值联系，表明连续吸引子可能普遍存在。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏统一框架分析不同动力系统中连续吸引子的性质，限制跨架构通用性。

Method: 从微分流形的角度建立新框架研究人工神经网络中的连续吸引子。

Result: 验证与先前结论的兼容性，阐明连续吸引子现象与局部雅可比矩阵特征值的联系，证明奇异值分层在常见分类模型和数据集的普遍性。

Conclusion: 连续吸引子可能普遍存在于一般神经网络中，需要通用理论，所提出框架因特征值和奇异值的紧密数学联系提供了有前景的基础。

Abstract: Continuous attractors are critical for information processing in both
biological and artificial neural systems, with implications for spatial
navigation, memory, and deep learning optimization. However, existing research
lacks a unified framework to analyze their properties across diverse dynamical
systems, limiting cross-architectural generalizability. This study establishes
a novel framework from the perspective of differential manifolds to investigate
continuous attractors in artificial neural networks. It verifies compatibility
with prior conclusions, elucidates links between continuous attractor phenomena
and eigenvalues of the local Jacobian matrix, and demonstrates the universality
of singular value stratification in common classification models and datasets.
These findings suggest continuous attractors may be ubiquitous in general
neural networks, highlighting the need for a general theory, with the proposed
framework offering a promising foundation given the close mathematical
connection between eigenvalues and singular values.

</details>


### [118] [Foundational theory for optimal decision tree problems. II. Optimal hypersurface decision tree algorithm](https://arxiv.org/abs/2509.12057)
*Xi He*

Main category: cs.LG

TL;DR: 文章基于系列论文Part I基础，提出首个超曲面决策树（HODT）算法，实验表明其比轴平行树更准确、抗噪，控制复杂度时比最优轴平行决策树算法准确率高。


<details>
  <summary>Details</summary>
Motivation: 解决最优决策树（ODT）问题是有挑战的组合优化任务，现有方法局限于超平面分裂规则，需提出新算法解决一般超曲面决策树模型。

Method: 基于Part I的算法和几何基础，提出HODT算法，不依赖外部求解器。

Result: 在合成数据集上，算法比轴平行树更准确、抗噪；在30个真实数据集上，控制复杂度时比最优轴平行决策树算法准确率高30%。

Conclusion: HODT算法有效，能处理一般超曲面决策树模型，且性能优于现有轴平行决策树算法。

Abstract: Decision trees are a ubiquitous model for classification and regression tasks
due to their interpretability and efficiency. However, solving the optimal
decision tree (ODT) problem remains a challenging combinatorial optimization
task. Even for the simplest splitting rules--axis-parallel hyperplanes--it is
NP-hard to optimize. In Part I of this series, we rigorously defined the proper
decision tree model through four axioms and, based on these, introduced four
formal definitions of the ODT problem. From these definitions, we derived four
generic algorithms capable of solving ODT problems for arbitrary decision trees
satisfying the axioms. We also analyzed the combinatorial geometric properties
of hypersurfaces, showing that decision trees defined by polynomial
hypersurface splitting rules satisfy the proper axioms that we proposed.
  In this second paper (Part II) of this two-part series, building on the
algorithmic and geometric foundations established in Part I, we introduce the
first hypersurface decision tree (HODT) algorithm. To the best of our
knowledge, existing optimal decision tree methods are, to date, limited to
hyperplane splitting rules--a special case of hypersurfaces--and rely on
general-purpose solvers. In contrast, our HODT algorithm addresses the general
hypersurface decision tree model without requiring external solvers.
  Using synthetic datasets generated from ground-truth hyperplane decision
trees, we vary tree size, data size, dimensionality, and label and feature
noise. Results showing that our algorithm recovers the ground truth more
accurately than axis-parallel trees and exhibits greater robustness to noise.
We also analyzed generalization performance across 30 real-world datasets,
showing that HODT can achieve up to 30% higher accuracy than the
state-of-the-art optimal axis-parallel decision tree algorithm when tree
complexity is properly controlled.

</details>


### [119] [Adaptive Preference Optimization with Uncertainty-aware Utility Anchor](https://arxiv.org/abs/2509.10515)
*Xiaobo Wang,Zixia Jia,Jiaqi Li,Qi Liu,Zilong Zheng*

Main category: cs.LG

TL;DR: 提出自适应偏好优化框架UAPO，解决现有离线偏好优化方法问题，提高数据利用效率，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有离线偏好优化方法（如DPO类学习）使用的BT奖励建模有诸多关键假设限制，需改进。

Method: 提出Adaptive Preference Optimization with Utility Anchor (UAPO)框架，引入锚定函数估计偏好数据标注带来的不确定性。

Result: UAPO在数据不配对场景也能训练，提高数据利用效率，训练过程更稳健，取得有竞争力的结果。

Conclusion: UAPO为更灵活有效的偏好优化方法开辟道路。

Abstract: Offline preference optimization methods are efficient for large language
models (LLMs) alignment. Direct Preference optimization (DPO)-like learning,
one of the most popular approaches, stands out for its efficiency in reward
modeling. However, these methods typically follow the convention to use
Bradley-Terry (BT) reward modeling that faces several critical assumptions,
including the requirement for pairwise training data, model distribution
shifting, human rationality assumption, etc. To address these limitations, we
propose a general framework for offline preference optimization methods,
Adaptive Preference Optimization with Utility Anchor (UAPO), which introduces
an anchoring function to estimate the uncertainties brought from preference
data annotation. Our method enables training even in scenarios where the data
is unpaired, significantly enhancing data utilization efficiency. Moreover, the
anchor design makes UAPO more robust in the training process. Experimental
results demonstrate that UAPO achieves competitive outcomes without the strict
dependency on data pairing, paving the way for more flexible and effective
preference optimization methods.

</details>


### [120] [M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations](https://arxiv.org/abs/2509.10659)
*Bo Lei,Victor M. Castillo,Yeping Hu*

Main category: cs.LG

TL;DR: 提出M4GN网络解决基于网格的图神经网络存在的问题，在多数据集上提升精度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于网格的图神经网络存在深层消息传递成本高、过平滑问题，分层GNN也面临构建粗图和保持细粒度精度的挑战。

Method: 采用三层、以段为中心的分层网络M4GN，结合混合分割策略，用置换不变聚合器编码，连接微观GNN和宏观transformer。

Result: 在多个代表性基准数据集上，M4GN预测精度最高提升56%，推理速度比最先进基线快22%。

Conclusion: M4GN能在精度和效率之间取得平衡。

Abstract: Mesh-based graph neural networks (GNNs) have become effective surrogates for
PDE simulations, yet their deep message passing incurs high cost and
over-smoothing on large, long-range meshes; hierarchical GNNs shorten
propagation paths but still face two key obstacles: (i) building coarse graphs
that respect mesh topology, geometry, and physical discontinuities, and (ii)
maintaining fine-scale accuracy without sacrificing the speed gained from
coarsening. We tackle these challenges with M4GN, a three-tier, segment-centric
hierarchical network. M4GN begins with a hybrid segmentation strategy that
pairs a fast graph partitioner with a superpixel-style refinement guided by
modal-decomposition features, producing contiguous segments of dynamically
consistent nodes. These segments are encoded by a permutation-invariant
aggregator, avoiding the order sensitivity and quadratic cost of aggregation
approaches used in prior works. The resulting information bridges a micro-level
GNN, which captures local dynamics, and a macro-level transformer that reasons
efficiently across segments, achieving a principled balance between accuracy
and efficiency. Evaluated on multiple representative benchmark datasets, M4GN
improves prediction accuracy by up to 56% while achieving up to 22% faster
inference than state-of-the-art baselines.

</details>


### [121] [pySigLib -- Fast Signature-Based Computations on CPU and GPU](https://arxiv.org/abs/2509.10613)
*Daniil Shmelev,Cristopher Salvi*

Main category: cs.LG

TL;DR: 提出高性能Python库pySigLib用于基于签名的计算，引入新的签名核微分方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于签名的方法在实际数据集大小和序列长度上无法扩展。

Method: 开发pySigLib库，在CPU和GPU上提供签名和签名核的优化实现，引入新的签名核微分方案。

Result: 得到高效的大规模基于签名计算的软件栈，新方案能以比现有库短得多的运行时间提供准确梯度。

Conclusion: pySigLib库可解决现有基于签名方法扩展性问题，新微分方案更高效。

Abstract: Signature-based methods have recently gained significant traction in machine
learning for sequential data. In particular, signature kernels have emerged as
powerful discriminators and training losses for generative models on
time-series, notably in quantitative finance. However, existing implementations
do not scale to the dataset sizes and sequence lengths encountered in practice.
We present pySigLib, a high-performance Python library offering optimised
implementations of signatures and signature kernels on CPU and GPU, fully
compatible with PyTorch's automatic differentiation. Beyond an efficient
software stack for large-scale signature-based computation, we introduce a
novel differentiation scheme for signature kernels that delivers accurate
gradients at a fraction of the runtime of existing libraries.

</details>


### [122] [Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction](https://arxiv.org/abs/2509.10516)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 提出基于联邦学习的隐私保护推荐系统解决教育个性化与隐私保护难题，FedProx 更优，模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 教育数字化带来数据驱动个性化机遇同时，传统推荐系统集中数据模式与数据保护法规不兼容，需解决学生数据隐私问题。

Method: 利用联邦学习，使用深度神经网络和 ASSISTments 教育数据集，对比不同联邦聚合策略。

Result: FedProx 处理异构学生数据更稳定有效，优化的联邦模型 F1 分数达 76.28%，为集中式 XGBoost 模型性能的 82.85%。

Conclusion: 联邦方法可在不集中敏感学生数据的情况下提供有效内容推荐，是解决现代教育平台个性化 - 隐私困境的可行方案。

Abstract: The increasing digitalization of education presents unprecedented
opportunities for data-driven personalization, yet it introduces significant
student data privacy challenges. Conventional recommender systems rely on
centralized data, a paradigm often incompatible with modern data protection
regulations. A novel privacy-preserving recommender system is proposed and
evaluated to address this critical issue using Federated Learning (FL). The
approach utilizes a Deep Neural Network (DNN) with rich, engineered features
from the large-scale ASSISTments educational dataset. A rigorous comparative
analysis of federated aggregation strategies was conducted, identifying FedProx
as a significantly more stable and effective method for handling heterogeneous
student data than the standard FedAvg baseline. The optimized federated model
achieves a high-performance F1-Score of 76.28\%, corresponding to 82.85\% of
the performance of a powerful, centralized XGBoost model. These findings
validate that a federated approach can provide highly effective content
recommendations without centralizing sensitive student data. Consequently, our
work presents a viable and robust solution to the personalization-privacy
dilemma in modern educational platforms.

</details>


### [123] [An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data](https://arxiv.org/abs/2509.11053)
*Shengke Sun,Shuzhen Han,Ziqian Luan,Xinghao Qin,Jiao Yin,Zhanshan Zhao,Jinli Cao,Hua Wang*

Main category: cs.LG

TL;DR: 提出先进的数据增强和对比傅里叶卷积框架 (DAC - FCF) 用于有限数据下的轴承故障诊断，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实中高质量标注数据稀缺，现有少样本学习方法在轴承故障诊断领域有局限，传统数据增强技术和卷积神经网络有不足。

Method: 提出 CCLR - GAN 生成更多样数据，利用基于对比学习的联合优化机制建模训练数据关系，提出 1D - FCNN 实现对输入数据的全局感知。

Result: DAC - FCF 显著提升性能，在 CWRU 数据集上比基线高 32%，在自收集测试台上高 10%，消融实验证明组件有效性。

Conclusion: DAC - FCF 为有限数据下的轴承故障诊断提供了有前景的解决方案。

Abstract: In the area of bearing fault diagnosis, deep learning (DL) methods have been
widely used recently. However, due to the high cost or privacy concerns,
high-quality labeled data are scarce in real world scenarios. While few-shot
learning has shown promise in addressing data scarcity, existing methods still
face significant limitations in this domain. Traditional data augmentation
techniques often suffer from mode collapse and generate low-quality samples
that fail to capture the diversity of bearing fault patterns. Moreover,
conventional convolutional neural networks (CNNs) with local receptive fields
makes them inadequate for extracting global features from complex vibration
signals. Additionally, existing methods fail to model the intricate
relationships between limited training samples. To solve these problems, we
propose an advanced data augmentation and contrastive fourier convolution
framework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, a
novel conditional consistent latent representation and reconstruction
generative adversarial network (CCLR-GAN) is proposed to generate more diverse
data. Secondly, a contrastive learning based joint optimization mechanism is
utilized to better model the relations between the available training data.
Finally, we propose a 1D fourier convolution neural network (1D-FCNN) to
achieve a global-aware of the input data. Experiments demonstrate that DAC-FCF
achieves significant improvements, outperforming baselines by up to 32\% on
case western reserve university (CWRU) dataset and 10\% on a self-collected
test bench. Extensive ablation experiments prove the effectiveness of the
proposed components. Thus, the proposed DAC-FCF offers a promising solution for
bearing fault diagnosis under limited data.

</details>


### [124] [Optimal Multimarginal Schrödinger Bridge: Minimum Spanning Tree over Measure-valued Vertices](https://arxiv.org/abs/2509.10626)
*Georgiy A. Bondar,Abhishek Halder*

Main category: cs.LG

TL;DR: 本文研究在所有可能图结构中寻找最优多边缘薛定谔桥（MSB）的问题，发现计算最优MSB等价于在测度值顶点上解决最小生成树问题，并给出两步骤解法，最后通过数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 在MSB公式中，关联结构是预先指定的，本文旨在解决在所有可能图结构中寻找最优MSB的问题。

Method: 首先构建一个边权重等于相应双边薛定谔桥最优值与端点熵之和的完全图，然后在该完全加权图上解决标准的最小生成树问题。

Result: 计算最优MSB等价于在测度值顶点上解决最小生成树问题，且可通过两步骤解决该问题。

Conclusion: 提出的解决方法有效，通过数值实验得到了验证。

Abstract: The Multimarginal Schr\"odinger Bridge (MSB) finds the optimal coupling among
a collection of random vectors with known statistics and a known correlation
structure. In the MSB formulation, this correlation structure is specified
\emph{a priori} as an undirected connected graph with measure-valued vertices.
In this work, we formulate and solve the problem of finding the optimal MSB in
the sense we seek the optimal coupling over all possible graph structures. We
find that computing the optimal MSB amounts to solving the minimum spanning
tree problem over measure-valued vertices. We show that the resulting problem
can be solved in two steps. The first step constructs a complete graph with
edge weight equal to a sum of the optimal value of the corresponding bimarginal
SB and the entropies of the endpoints. The second step solves a standard
minimum spanning tree problem over that complete weighted graph. Numerical
experiments illustrate the proposed solution.

</details>


### [125] [A Comparative Benchmark of Federated Learning Strategies for Mortality Prediction on Heterogeneous and Imbalanced Clinical Data](https://arxiv.org/abs/2509.10517)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 研究对比五种联邦学习策略用于死亡率预测，发现FedProx表现最佳，为医疗应用选策略提供基准。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型预测院内死亡率受数据隐私和异质性限制，联邦学习性能在非IID和不平衡条件下待研究。

Method: 对比FedAvg、FedProx等五种联邦学习策略，用MIMIC - IV数据集模拟非IID环境，用SMOTE - Tomek处理数据不平衡，进行50轮通信实验。

Result: FedProx表现最佳，F1分数达0.8831且收敛稳定，FedAvg计算效率高但预测性能低。

Conclusion: 基于正则化的联邦学习算法如FedProx对异质和不平衡临床预测任务更有效，为现实医疗应用选策略提供实证基准。

Abstract: Machine learning models hold significant potential for predicting in-hospital
mortality, yet data privacy constraints and the statistical heterogeneity of
real-world clinical data often hamper their development. Federated Learning
(FL) offers a privacy-preserving solution, but its performance under
non-Independent and Identically Distributed (non-IID) and imbalanced conditions
requires rigorous investigation. The study presents a comparative benchmark of
five federated learning strategies: FedAvg, FedProx, FedAdagrad, FedAdam, and
FedCluster for mortality prediction. Using the large-scale MIMIC-IV dataset, we
simulate a realistic non-IID environment by partitioning data by clinical care
unit. To address the inherent class imbalance of the task, the SMOTE-Tomek
technique is applied to each client's local training data. Our experiments,
conducted over 50 communication rounds, reveal that the regularization-based
strategy, FedProx, consistently outperformed other methods, achieving the
highest F1-Score of 0.8831 while maintaining stable convergence. While the
baseline FedAvg was the most computationally efficient, its predictive
performance was substantially lower. Our findings indicate that
regularization-based FL algorithms like FedProx offer a more robust and
effective solution for heterogeneous and imbalanced clinical prediction tasks
than standard or server-side adaptive aggregation methods. The work provides a
crucial empirical benchmark for selecting appropriate FL strategies for
real-world healthcare applications.

</details>


### [126] [Intelligent Reservoir Decision Support: An Integrated Framework Combining Large Language Models, Advanced Prompt Engineering, and Multimodal Data Fusion for Real-Time Petroleum Operations](https://arxiv.org/abs/2509.11376)
*Seyed Kourosh Mahjour,Seyed Saman Mahjour*

Main category: cs.LG

TL;DR: 本文提出结合大语言模型、提示工程和多模态数据融合的集成框架用于油藏分析，经实地验证性能出色，能提升效率、安全和经济效益。


<details>
  <summary>Details</summary>
Motivation: 石油行业在油藏管理面临挑战，需快速整合多模态数据集以提供实时决策支持。

Method: 结合大语言模型（GPT - 4o、Claude 4 Sonnet、Gemini 2.5 Pro），运用先进提示工程技术和多模态数据融合，实现特定领域检索增强生成、思维链推理和少样本学习，通过专业AI模型处理多模态数据。

Result: 在15个不同油藏环境验证，油藏表征准确率94.2%，生产预测精度87.6%，井位优化成功率91.4%，响应时间亚秒级，安全可靠性96.2%，成本降低62 - 78%，少样本学习减少72%现场适应时间，自动提示优化使推理质量提高89%，异常检测准确率96.2%，减少45%环境事故。

Conclusion: 该研究实现前沿AI技术与石油领域专业知识的实际整合，可提高运营效率、安全性和经济效益。

Abstract: The petroleum industry faces unprecedented challenges in reservoir
management, requiring rapid integration of complex multimodal datasets for
real-time decision support. This study presents a novel integrated framework
combining state-of-the-art large language models (GPT-4o, Claude 4 Sonnet,
Gemini 2.5 Pro) with advanced prompt engineering techniques and multimodal data
fusion for comprehensive reservoir analysis. The framework implements
domain-specific retrieval-augmented generation (RAG) with over 50,000 petroleum
engineering documents, chain-of-thought reasoning, and few-shot learning for
rapid field adaptation. Multimodal integration processes seismic
interpretations, well logs, and production data through specialized AI models
with vision transformers. Field validation across 15 diverse reservoir
environments demonstrates exceptional performance: 94.2% reservoir
characterization accuracy, 87.6% production forecasting precision, and 91.4%
well placement optimization success rate. The system achieves sub-second
response times while maintaining 96.2% safety reliability with no high-risk
incidents during evaluation. Economic analysis reveals 62-78% cost reductions
(mean 72%) relative to traditional methods with 8-month payback period.
Few-shot learning reduces field adaptation time by 72%, while automated prompt
optimization achieves 89% improvement in reasoning quality. The framework
processed real-time data streams with 96.2% anomaly detection accuracy and
reduced environmental incidents by 45%. We provide detailed experimental
protocols, baseline comparisons, ablation studies, and statistical significance
testing to ensure reproducibility. This research demonstrates practical
integration of cutting-edge AI technologies with petroleum domain expertise for
enhanced operational efficiency, safety, and economic performance.

</details>


### [127] [Interpretable neural network system identification method for two families of second-order systems based on characteristic curves](https://arxiv.org/abs/2509.10632)
*Federico J. Gonzalez,Luis P. Lara*

Main category: cs.LG

TL;DR: 提出结合微分方程结构与神经网络灵活性的统一框架，介绍三种识别策略，结果显示NN - CC对复杂非线性系统表现更优，该框架能兼顾捕捉复杂非线性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决非线性系统识别中可解释性与灵活性的权衡问题，需结合物理约束。

Method: 提出基于特征曲线（CC）的统一数据驱动框架，核心是用专用神经网络为每个CC建模；介绍SINDy - CC、Poly - CC、NN - CC三种识别策略。

Result: 三种方法适用于简单多项式非线性系统；NN - CC在复杂非线性和不连续系统建模中表现更优。

Conclusion: CC框架特别是NN - CC方法能捕捉复杂非线性并保持可解释性，适合传统方法难评估的系统，是强大的非线性系统识别工具。

Abstract: Nonlinear system identification often involves a fundamental trade-off
between interpretability and flexibility, often requiring the incorporation of
physical constraints. We propose a unified data-driven framework that combines
the mathematical structure of the governing differential equations with the
flexibility of neural networks (NNs). At the core of our approach is the
concept of characteristic curves (CCs), which represent individual nonlinear
functions (e.g., friction and restoring components) of the system. Each CC is
modeled by a dedicated NN, enabling a modular and interpretable representation
of the system equation. To demonstrate the versatility of the CC-based
formalism, we introduce three identification strategies: (1) SINDy-CC, which
extends the sparse regression approach of SINDy by incorporating the
mathematical structure of the governing equations as constraints; (2) Poly-CC,
which represents each CC using high-degree polynomials; and (3) NN-CC, which
uses NNs without requiring prior assumptions about basis functions. Our results
show that all three approaches are well-suited for systems with simple
polynomial nonlinearities, such as the van der Pol oscillator. In contrast,
NN-CC demonstrates superior performance in modeling systems with complex
nonlinearities and discontinuities, such as those observed in stick-slip
systems. The key contribution of this work is to demonstrate that the CC-based
framework, particularly the NN-CC approach, can capture complex nonlinearities
while maintaining interpretability through the explicit representation of the
CCs. This balance makes it well-suited for modeling systems with
discontinuities and complex nonlinearities that are challenging to assess using
traditional polynomial or sparse regression methods, providing a powerful tool
for nonlinear system identification.

</details>


### [128] [Holographic Knowledge Manifolds: A Novel Pipeline for Continual Learning Without Catastrophic Forgetting in Large Language Models](https://arxiv.org/abs/2509.10518)
*Justin Arndt*

Main category: cs.LG

TL;DR: 介绍全息知识流形（HKM），实现零灾难性遗忘，有压缩、效率等优势，实验表现好，有成本节省和范式转变潜力。


<details>
  <summary>Details</summary>
Motivation: 在AI知识表示中实现零灾难性遗忘，同时保持低内存增长和高效率。

Method: 采用分形量化、概率纠缠和动态衍射切片技术的四阶段HKM管道。

Result: 在实验中实现0%遗忘、3倍压缩、53%训练时间减少，有成本节省和能耗降低。

Conclusion: HKM可能引发公共大语言模型范式转变，未来扩展有进一步降低成本潜力。

Abstract: We introduce the Holographic Knowledge Manifold (HKM), a four-phase pipeline
that achieves zero catastrophic forgetting in AI knowledge representation while
maintaining minimal memory growth and high efficiency. Leveraging fractal
quantization, probabilistic entanglement, and dynamic diffraction chipping, HKM
compresses knowledge substrates by 3x with 67% storage savings, integrates
holographically at 100%, and supports over 1,020 updates with 1% growth per
increment. In experiments on combined WikiText and FB15k datasets (scaled to
2,997 nodes), we demonstrate industry-leading performance: 0% forgetting
(infinite improvement over GEM baselines), 3x compression, and 53% training
time reduction on consumer GPU hardware. Hypothetical cost analyses project
$92.4M savings over 5 years at petabyte scale, with 21.2% energy reduction and
33% lower carbon footprint. This work hypothesizes a paradigm shift for public
large language models (LLMs), enabling "eternal" adaptation without retraining.
Future extensions to multimodal fusion and quantum hardware could further
democratize scalable AI, potentially reducing fine-tuning costs by 60-80% for
models like Llama-3 or Grok-4. Code, datasets, and full results are publicly
available for reproducibility.

</details>


### [129] [On Using Large-Batches in Federated Learning](https://arxiv.org/abs/2509.10537)
*Sahil Tyagi*

Main category: cs.LG

TL;DR: 本文探讨联邦学习中大批量训练问题，提出权衡大小批量训练的方法，在ResNet50和VGG11模型上测试准确率更高。


<details>
  <summary>Details</summary>
Motivation: 联邦学习算法在并行和统计性能间权衡，大批量训练虽有训练加速但存在泛化问题，需解决这些挑战。

Method: 提出权衡小批量和大批量训练的方法，探索新方向以结合二者优势。

Result: 在相同迭代次数下，提出的大批量训练技术在ResNet50和VGG11模型上比小批量训练分别提高约32.33%和3.74%的测试准确率。

Conclusion: 所提方法能在享受大批量并行扩展优势的同时，具备小批量训练的良好泛化性。

Abstract: Efficient Federated learning (FL) is crucial for training deep networks over
devices with limited compute resources and bounded networks. With the advent of
big data, devices either generate or collect multimodal data to train either
generic or local-context aware networks, particularly when data privacy and
locality is vital. FL algorithms generally trade-off between parallel and
statistical performance, improving model quality at the cost of higher
communication frequency, or vice versa. Under frequent synchronization
settings, FL over a large cluster of devices may perform more work per-training
iteration by processing a larger global batch-size, thus attaining considerable
training speedup. However, this may result in poor test performance (i.e., low
test loss or accuracy) due to generalization degradation issues associated with
large-batch training. To address these challenges with large-batches, this work
proposes our vision of exploiting the trade-offs between small and large-batch
training, and explore new directions to enjoy both the parallel scaling of
large-batches and good generalizability of small-batch training. For the same
number of iterations, we observe that our proposed large-batch training
technique attains about 32.33% and 3.74% higher test accuracy than small-batch
training in ResNet50 and VGG11 models respectively.

</details>


### [130] [Learning Concave Bid Shading Strategies in Online Auctions via Measure-valued Proximal Optimization](https://arxiv.org/abs/2509.10693)
*Iman Nodozi,Djordje Gligorijevic,Abhishek Halder*

Main category: cs.LG

TL;DR: 提出首价拍卖的投标阴影策略，将其作为测度值优化问题，给出算法并展示有闭式解，用数值示例说明。


<details>
  <summary>Details</summary>
Motivation: 为首价拍卖提出有效的投标阴影策略。

Method: 采用标准参数形式进行投标阴影，将问题表述为关于阴影参数联合分布的凸优化，每次拍卖后通过正则化Wasserstein - 近端更新和数据驱动的能量泛函调整阴影参数分布。

Result: 得到的测度值凸优化问题有闭式解。

Conclusion: 所提出的投标阴影策略有效，且有闭式解，数值示例也验证了方法可行性。

Abstract: This work proposes a bid shading strategy for first-price auctions as a
measure-valued optimization problem. We consider a standard parametric form for
bid shading and formulate the problem as convex optimization over the joint
distribution of shading parameters. After each auction, the shading parameter
distribution is adapted via a regularized Wasserstein-proximal update with a
data-driven energy functional. This energy functional is conditional on the
context, i.e., on publisher/user attributes such as domain, ad slot type,
device, or location. The proposed algorithm encourages the bid distribution to
place more weight on values with higher expected surplus, i.e., where the win
probability and the value gap are both large. We show that the resulting
measure-valued convex optimization problem admits a closed form solution. A
numerical example illustrates the proposed method.

</details>


### [131] [Gradient Estimation Methods of Approximate Multipliers for High-Accuracy Retraining of Deep Learning Models](https://arxiv.org/abs/2509.10519)
*Chang Meng,Wayne Burleson,Giovanni De Micheli*

Main category: cs.LG

TL;DR: 提出两种计算近似乘法器梯度的方法提升深度学习模型重训练准确率。


<details>
  <summary>Details</summary>
Motivation: 现有使用精确乘法器梯度估计近似乘法器梯度的方法重训练效果不佳。

Method: 提出LUT - 2D方法用二维查找表刻画近似乘法器梯度；提出LUT - 1D方法用一维查找表存储梯度值。

Result: 在CIFAR - 10上LUT - 2D和LUT - 1D平均提升重训练准确率3.83%和3.72%；在ImageNet上LUT - 1D平均提升23.69%。

Conclusion: 所提两种方法能有效提升近似乘法器用于深度学习模型的重训练准确率。

Abstract: Approximate multipliers (AppMults) are widely used in deep learning
accelerators to reduce their area, delay, and power consumption. However,
AppMults introduce arithmetic errors into deep learning models, necessitating a
retraining process to recover accuracy. A key step in retraining is computing
the gradient of the AppMult, i.e., the partial derivative of the approximate
product with respect to each input operand. Existing approaches typically
estimate this gradient using that of the accurate multiplier (AccMult), which
can lead to suboptimal retraining results. To address this, we propose two
methods to obtain more precise gradients of AppMults. The first, called LUT-2D,
characterizes the AppMult gradient with 2-dimensional lookup tables (LUTs),
providing fine-grained estimation and achieving the highest retraining
accuracy. The second, called LUT-1D, is a compact and more efficient variant
that stores gradient values in 1-dimensional LUTs, achieving comparable
retraining accuracy with shorter runtime. Experimental results show that on
CIFAR-10 with convolutional neural networks, our LUT-2D and LUT-1D methods
improve retraining accuracy by 3.83% and 3.72% on average, respectively. On
ImageNet with vision transformer models, our LUT-1D method improves retraining
accuracy by 23.69% on average, compared to a state-of-the-art retraining
framework.

</details>


### [132] [FACTORS: Factorial Approximation for Complementary Two-factor Optimization with Risk-aware Scoring](https://arxiv.org/abs/2509.10825)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: 提出FACTORS框架结合实验设计与Shapley分解解决训练因素组合敏感的性能和稳定性问题，有效果估计双路径，理论上有误差分解等，能提升性能和降低风险。


<details>
  <summary>Details</summary>
Motivation: 解决对训练因素组合敏感的性能和稳定性问题。

Method: 结合实验设计与Shapley分解，通过条件均值的插件路径和从样本重建Shapley贡献的最小二乘法路径进行效果估计，纳入估计标准化、偏差校正和不确定性量化，有轻量级搜索程序。

Result: 在不同数据集和设计条件下，改善了排名保留和最优配置识别，降低了决策风险。

Conclusion: 该方法在预算约束下能提供可解释的理由和稳定的性能提升，为调优提供基础。

Abstract: We propose FACTORS, a framework that combines design of experiments with
Shapley decomposition to address performance and stability issues that are
sensitive to combinations of training factors. Our approach consistently
estimates main effects and two-factor interactions, then integrates them into a
risk-adjusted objective function that jointly accounts for uncertainty and
cost, enabling reliable selection of configurations under a fixed budget.
Effect estimation is implemented through two complementary paths: a plug-in
path based on conditional means, and a least-squares path that reconstructs
Shapley contributions from samples. These paths are designed to work
complementarily even when design density and bias levels differ. By
incorporating standardization of estimates, bias correction, and uncertainty
quantification, our procedure ensures comparability across heterogeneous factor
spaces and designs, while a lightweight search routine yields configurations
within practical time even for large factor spaces. On the theoretical side, we
provide error decompositions, sample complexity analysis, and upper bounds on
optimality gaps. On the interpretive side, we summarize main effects and
interactions in map form, highlighting adjustment priorities and safe
improvement pathways. Across diverse datasets and design conditions, our
approach improves rank preservation and optimal configuration identification,
reduces decision-making risks, and offers a tuning foundation that delivers
interpretable justification alongside stable performance gains even under
budget constraints.

</details>


### [133] [Offline Contextual Bandit with Counterfactual Sample Identification](https://arxiv.org/abs/2509.10520)
*Alexandre Gilotte,Otmane Sakhi,Imad Aouali,Benjamin Heymann*

Main category: cs.LG

TL;DR: 提出反事实样本识别方法，在合成实验和实际部署中表现优于直接模型


<details>
  <summary>Details</summary>
Motivation: 现有上下文多臂老虎机方法中的直接奖励模型存在混淆问题，难以区分动作和上下文的影响

Method: 提出反事实样本识别方法，通过与在相同上下文下从日志策略中采样的反事实动作比较，学习识别导致成功（二元）结果的动作

Result: 该方法在合成实验和实际部署中始终优于直接模型

Conclusion: 新提出的反事实样本识别方法理论基础扎实且效果更好

Abstract: In production systems, contextual bandit approaches often rely on direct
reward models that take both action and context as input. However, these models
can suffer from confounding, making it difficult to isolate the effect of the
action from that of the context. We present \emph{Counterfactual Sample
Identification}, a new approach that re-frames the problem: rather than
predicting reward, it learns to recognize which action led to a successful
(binary) outcome by comparing it to a counterfactual action sampled from the
logging policy under the same context. The method is theoretically grounded and
consistently outperforms direct models in both synthetic experiments and
real-world deployments.

</details>


### [134] [Variational Gaussian Mixture Manifold Models for Client-Specific Federated Personalization](https://arxiv.org/abs/2509.10521)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Sajedul Talukder*

Main category: cs.LG

TL;DR: 提出VGM²框架解决个性化联邦学习在标签偏斜和非平稳性下的问题，在多数据集上取得良好效果且通信量小，加强隐私。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习在标签偏斜和非平稳性下因单一全局参数化忽略客户端特定几何信息而失败的问题。

Method: 引入VGM²框架，学习客户端特定参数化UMAP嵌入，用混合关系标记建模潜在成对距离，仅交换变分、不确定性感知的标记统计信息，客户端维护Dir - NIG后验，服务器通过共轭矩匹配聚合形成全局先验。

Result: 在八个非IID标签分片的视觉数据集上，与强基线相比，VGM²取得有竞争力或更优的测试F1分数，仅需通信小的几何摘要。

Conclusion: VGM²在异构性下具有稳定性，通过安全聚合和可选差分隐私噪声加强隐私，代码将发布以保证可重复性。

Abstract: Personalized federated learning (PFL) often fails under label skew and
non-stationarity because a single global parameterization ignores
client-specific geometry. We introduce VGM$^2$ (Variational Gaussian Mixture
Manifold), a geometry-centric PFL framework that (i) learns client-specific
parametric UMAP embeddings, (ii) models latent pairwise distances with mixture
relation markers for same and different class pairs, and (iii) exchanges only
variational, uncertainty-aware marker statistics. Each client maintains a
Dirichlet-Normal-Inverse-Gamma (Dir-NIG) posterior over marker weights, means,
and variances; the server aggregates via conjugate moment matching to form
global priors that guide subsequent rounds. We prove that this aggregation
minimizes the summed reverse Kullback-Leibler divergence from client posteriors
within the conjugate family, yielding stability under heterogeneity. We further
incorporate a calibration term for distance-to-similarity mapping and report
communication and compute budgets. Across eight vision datasets with non-IID
label shards, VGM$^2$ achieves competitive or superior test F1 scores compared
to strong baselines while communicating only small geometry summaries. Privacy
is strengthened through secure aggregation and optional differential privacy
noise, and we provide a membership-inference stress test. Code and
configurations will be released to ensure full reproducibility.

</details>


### [135] [Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction](https://arxiv.org/abs/2509.10522)
*Kaizhen Tan*

Main category: cs.LG

TL;DR: 本文提出多模态深度学习框架估计空管指令生命周期关键参数，构建数据集，开发CNN - Transformer集成模型，支持智能指令生成，有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 在密集空域中，准确的空管人员（ATCOs）工作量建模对安全和效率至关重要。

Method: 提出集成结构化数据、轨迹序列和图像特征的多模态深度学习框架，用滑动窗口和基于直方图的方法检测机动点，开发CNN - Transformer集成模型。

Result: 构建了高质量数据集，实现对关键参数的准确、可泛化和可解释的预测。

Conclusion: 该模型支持智能指令生成，对工作量评估、人员配备和调度有实际价值。

Abstract: Air traffic controllers (ATCOs) issue high-intensity voice commands in dense
airspace, where accurate workload modeling is critical for safety and
efficiency. This paper proposes a multimodal deep learning framework that
integrates structured data, trajectory sequences, and image features to
estimate two key parameters in the ATCO command lifecycle: the time offset
between a command and the resulting aircraft maneuver, and the command
duration. A high-quality dataset was constructed, with maneuver points detected
using sliding window and histogram-based methods. A CNN-Transformer ensemble
model was developed for accurate, generalizable, and interpretable predictions.
By linking trajectories to voice commands, this work offers the first model of
its kind to support intelligent command generation and provides practical value
for workload assessment, staffing, and scheduling.

</details>


### [136] [Online Optimization on Hadamard Manifolds: Curvature Independent Regret Bounds on Horospherically Convex Objectives](https://arxiv.org/abs/2509.11236)
*Emre Sahinoglu,Shahin Shahrampour*

Main category: cs.LG

TL;DR: 研究Hadamard流形上的在线黎曼优化，分析h - 凸和强h - 凸函数的黎曼在线梯度下降，建立曲率无关的后悔界并实验验证。


<details>
  <summary>Details</summary>
Motivation: 以往工作依赖测地凸性（g - 凸性），后悔界与流形曲率缩放性差，需解决该局限。

Method: 分析h - 凸和强h - 凸函数的黎曼在线梯度下降。

Result: 分别建立了O(√T)和O(log(T))的后悔界，这些界与曲率无关且与欧几里得情形结果匹配；在对称正定矩阵流形上实验验证了方法。

Conclusion: h - 凸性可用于在线黎曼优化，在实际应用中有价值。

Abstract: We study online Riemannian optimization on Hadamard manifolds under the
framework of horospherical convexity (h-convexity). Prior work mostly relies on
the geodesic convexity (g-convexity), leading to regret bounds scaling poorly
with the manifold curvature. To address this limitation, we analyze Riemannian
online gradient descent for h-convex and strongly h-convex functions and
establish $O(\sqrt{T})$ and $O(\log(T))$ regret guarantees, respectively. These
bounds are curvature-independent and match the results in the Euclidean
setting. We validate our approach with experiments on the manifold of symmetric
positive definite (SPD) matrices equipped with the affine-invariant metric. In
particular, we investigate online Tyler's $M$-estimation and online Fr\'echet
mean computation, showing the application of h-convexity in practice.

</details>


### [137] [From Predictions to Explanations: Explainable AI for Autism Diagnosis and Identification of Critical Brain Regions](https://arxiv.org/abs/2509.10523)
*Kush Gupta,Amir Aly,Emmanuel Ifeachor,Rohit Shankar*

Main category: cs.LG

TL;DR: 提出结合深度学习与可解释AI的两模块计算机辅助诊断框架用于ASD诊断，展示跨域迁移学习可解决数据稀缺问题，揭示模型诊断决策方式及关键脑区，与神经生物学证据高度吻合。


<details>
  <summary>Details</summary>
Motivation: 机器学习中迁移学习范式在ASD研究中的应用有限，需要新方法解决数据稀缺和模型可解释性问题。

Method: 提出两模块框架，第一模块用跨域迁移学习微调深度学习模型进行ASD分类，第二模块用三种可解释AI技术（显著性映射、梯度加权类激活映射和SHAP分析）解释模型决策和识别关键脑区。

Result: 框架显示跨域迁移学习能有效解决ASD研究中的数据稀缺问题，通过三种可解释性技术揭示模型诊断决策方式和关键脑区。

Conclusion: 研究结果与既定神经生物学证据高度一致，强化了所提方法的临床相关性。

Abstract: Autism spectrum disorder (ASD) is a neurodevelopmental condition
characterized by atypical brain maturation. However, the adaptation of transfer
learning paradigms in machine learning for ASD research remains notably
limited. In this study, we propose a computer-aided diagnostic framework with
two modules. This chapter presents a two-module framework combining deep
learning and explainable AI for ASD diagnosis. The first module leverages a
deep learning model fine-tuned through cross-domain transfer learning for ASD
classification. The second module focuses on interpreting the model decisions
and identifying critical brain regions. To achieve this, we employed three
explainable AI (XAI) techniques: saliency mapping, Gradient-weighted Class
Activation Mapping, and SHapley Additive exPlanations (SHAP) analysis. This
framework demonstrates that cross-domain transfer learning can effectively
address data scarcity in ASD research. In addition, by applying three
established explainability techniques, the approach reveals how the model makes
diagnostic decisions and identifies brain regions most associated with ASD.
These findings were compared against established neurobiological evidence,
highlighting strong alignment and reinforcing the clinical relevance of the
proposed approach.

</details>


### [138] [SelectMix: Enhancing Label Noise Robustness through Targeted Sample Mixing](https://arxiv.org/abs/2509.11265)
*Qiuhao Liu,Ling Li,Yao Lu,Qi Xuan,Zhaowei Zhu,Jiaheng Wei*

Main category: cs.LG

TL;DR: 提出SelectMix框架解决深度神经网络在有噪声标签下泛化性能差的问题，实验证明其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易记忆噪声标签，现有Mixup方法无原则地混合样本，传播了噪声监督，影响泛化性能。

Method: SelectMix先通过K折交叉验证的基于置信度的不匹配分析识别潜在噪声或模糊样本，再将不确定样本与潜在类别中预测置信度高的样本混合，使用混合过程中所有类别的软标签。

Result: 在多个合成和真实世界基准数据集上，SelectMix始终优于强基线方法。

Conclusion: SelectMix在有噪声标签的学习中有效且具有鲁棒性。

Abstract: Deep neural networks tend to memorize noisy labels, severely degrading their
generalization performance. Although Mixup has demonstrated effectiveness in
improving generalization and robustness, existing Mixup-based methods typically
perform indiscriminate mixing without principled guidance on sample selection
and mixing strategy, inadvertently propagating noisy supervision. To overcome
these limitations, we propose SelectMix, a confidence-guided mixing framework
explicitly tailored for noisy labels. SelectMix first identifies potentially
noisy or ambiguous samples through confidence based mismatch analysis using
K-fold cross-validation, then selectively blends identified uncertain samples
with confidently predicted peers from their potential classes. Furthermore,
SelectMix employs soft labels derived from all classes involved in the mixing
process, ensuring the labels accurately represent the composition of the mixed
samples, thus aligning supervision signals closely with the actual mixed
inputs. Through extensive theoretical analysis and empirical evaluations on
multiple synthetic (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and real-world
benchmark datasets (CIFAR-N, MNIST and Clothing1M), we demonstrate that
SelectMix consistently outperforms strong baseline methods, validating its
effectiveness and robustness in learning with noisy labels.

</details>


### [139] [Resource-Aware Neural Network Pruning Using Graph-based Reinforcement Learning](https://arxiv.org/abs/2509.10526)
*Dieter Balemans,Thomas Huybrechts,Jan Steckel,Siegfried Mercelis*

Main category: cs.LG

TL;DR: 本文提出将基于图的观察空间集成到AutoML框架的神经网络剪枝新方法，在基准数据集实验中表现优于传统技术。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法依赖手工启发式和局部优化视角，导致性能欠佳和策略低效，需新方法解决。

Method: 引入目标神经网络的图表示，采用GAT编码器处理；行动空间从连续剪枝率转为细粒度二进制行动空间；在CMDP框架建模，设计自我竞争奖励系统。

Result: 在CIFAR - 10、CIFAR - 100和ImageNet等基准数据集实验中，该方法始终优于传统剪枝技术，取得了最先进的结果。

Conclusion: 该方法能学习特定任务的剪枝策略，识别出超越简单权重大小考量的功能冗余连接。

Abstract: This paper presents a novel approach to neural network pruning by integrating
a graph-based observation space into an AutoML framework to address the
limitations of existing methods. Traditional pruning approaches often depend on
hand-crafted heuristics and local optimization perspectives, which can lead to
suboptimal performance and inefficient pruning strategies. Our framework
transforms the pruning process by introducing a graph representation of the
target neural network that captures complete topological relationships between
layers and channels, replacing the limited layer-wise observation space with a
global view of network structure. The core innovations include a Graph
Attention Network (GAT) encoder that processes the network's graph
representation and generates a rich embedding. Additionally, for the action
space we transition from continuous pruning ratios to fine-grained binary
action spaces which enables the agent to learn optimal channel importance
criteria directly from data, moving away from predefined scoring functions.
These contributions are modelled within a Constrained Markov Decision Process
(CMDP) framework, allowing the agent to make informed pruning decisions while
adhering to resource constraints such as target compression rates. For this, we
design a self-competition reward system that encourages the agent to outperform
its previous best performance while satisfying the defined constraints. We
demonstrate the effectiveness of our approach through extensive experiments on
benchmark datasets including CIFAR-10, CIFAR-100, and ImageNet. The experiments
show that our method consistently outperforms traditional pruning techniques,
showing state-of-the-art results while learning task-specific pruning
strategies that identify functionally redundant connections beyond simple
weight magnitude considerations.

</details>


### [140] [STM-Graph: A Python Framework for Spatio-Temporal Mapping and Graph Neural Network Predictions](https://arxiv.org/abs/2509.10528)
*Amirhossein Ghaffari,Huong Nguyen,Lauri Lovén,Ekaterina Gilman*

Main category: cs.LG

TL;DR: 提出开源Python框架STM - Graph处理城市时空数据，便于GNN训练预测，代码开源。


<details>
  <summary>Details</summary>
Motivation: 城市时空数据动态复杂，给预测分析带来挑战，需要有效处理框架。

Method: 开发STM - Graph框架，集成多种空间映射方法、城市特征、GNN模型、可视化工具和GUI。

Result: 创建了模块化、可扩展的框架，方便快速实验和基准测试，可集成新方法和自定义模型。

Conclusion: STM - Graph是城市计算领域研究人员和从业者的宝贵资源。

Abstract: Urban spatio-temporal data present unique challenges for predictive analytics
due to their dynamic and complex nature. We introduce STM-Graph, an open-source
Python framework that transforms raw spatio-temporal urban event data into
graph representations suitable for Graph Neural Network (GNN) training and
prediction. STM-Graph integrates diverse spatial mapping methods, urban
features from OpenStreetMap, multiple GNN models, comprehensive visualization
tools, and a graphical user interface (GUI) suitable for professional and
non-professional users. This modular and extensible framework facilitates rapid
experimentation and benchmarking. It allows integration of new mapping methods
and custom models, making it a valuable resource for researchers and
practitioners in urban computing. The source code of the framework and GUI are
available at: https://github.com/Ahghaffari/stm_graph and
https://github.com/tuminguyen/stm_graph_gui.

</details>


### [141] [Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion via Latent Replay](https://arxiv.org/abs/2509.10529)
*Aoi Otani*

Main category: cs.LG

TL;DR: 本文针对文本到图像扩散模型的灾难性遗忘和模式崩溃问题，应用受神经科学启发的潜在重放方法，实验表明该方法显著优于现有方法，可实现生成式AI模型的高效持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型的灾难性遗忘和模式崩溃问题。

Method: 将受神经科学启发的潜在重放方法应用于扩散模型，仅保留从模型内部架构提取的紧凑高级特征表示。

Result: 通过对五个视觉概念的实验，潜在重放方法在保持模型通用性上显著优于现有方法，学习所有概念后，对最早概念的图像对齐率达77.59%，比基线方法高14%，且输出多样，随机选择存储的潜在示例效果优于基于相似度的策略。

Conclusion: 潜在重放方法能实现生成式AI模型的高效持续学习，为随用户需求进化的个性化文本到图像模型铺平道路，且无需过高计算成本。

Abstract: Continual learning -- the ability to acquire knowledge incrementally without
forgetting previous skills -- is fundamental to natural intelligence. While the
human brain excels at this, artificial neural networks struggle with
"catastrophic forgetting," where learning new tasks erases previously acquired
knowledge. This challenge is particularly severe for text-to-image diffusion
models, which generate images from textual prompts. Additionally, these models
face "mode collapse," where their outputs become increasingly repetitive over
time. To address these challenges, we apply Latent Replay, a
neuroscience-inspired approach, to diffusion models. Traditional replay methods
mitigate forgetting by storing and revisiting past examples, typically
requiring large collections of images. Latent Replay instead retains only
compact, high-level feature representations extracted from the model's internal
architecture. This mirrors the hippocampal process of storing neural activity
patterns rather than raw sensory inputs, reducing memory usage while preserving
critical information. Through experiments with five sequentially learned visual
concepts, we demonstrate that Latent Replay significantly outperforms existing
methods in maintaining model versatility. After learning all concepts, our
approach retained 77.59% Image Alignment (IA) on the earliest concept, 14%
higher than baseline methods, while maintaining diverse outputs. Surprisingly,
random selection of stored latent examples outperforms similarity-based
strategies. Our findings suggest that Latent Replay enables efficient continual
learning for generative AI models, paving the way for personalized
text-to-image models that evolve with user needs without excessive
computational costs.

</details>


### [142] [Long-time dynamics and universality of nonconvex gradient descent](https://arxiv.org/abs/2509.11426)
*Qiyang Han*

Main category: cs.LG

TL;DR: 本文提出通用方法刻画广义单指标模型中非凸梯度下降的长时间轨迹行为，给出集中性保证，通过两个应用展示理论实用性，还揭示与动态平均场理论的联系。


<details>
  <summary>Details</summary>
Motivation: 刻画广义单指标模型在大纵横比条件下非凸梯度下降的长时间轨迹行为。

Method: 提出用状态演化系统追踪名为‘高斯理论梯度下降’的确定性向量的动态。

Result: 得到梯度下降迭代的集中性保证，通过两个回归应用展示理论实用性，证明非凸梯度下降全局收敛和随机初始化梯度下降的普遍性，开发无数据迭代算法，发现与动态平均场理论的联系。

Conclusion: 所提方法能有效刻画非凸梯度下降的长时间轨迹行为，有实际应用价值且与现有理论有联系。

Abstract: This paper develops a general approach to characterize the long-time
trajectory behavior of nonconvex gradient descent in generalized single-index
models in the large aspect ratio regime. In this regime, we show that for each
iteration the gradient descent iterate concentrates around a deterministic
vector called the `Gaussian theoretical gradient descent', whose dynamics can
be tracked by a state evolution system of two recursive equations for two
scalars. Our concentration guarantees hold universally for a broad class of
design matrices and remain valid over long time horizons until algorithmic
convergence or divergence occurs. Moreover, our approach reveals that gradient
descent iterates are in general approximately independent of the data and
strongly incoherent with the feature vectors, a phenomenon previously known as
the `implicit regularization' effect of gradient descent in specific models
under Gaussian data.
  As an illustration of the utility of our general theory, we present two
applications of different natures in the regression setting. In the first, we
prove global convergence of nonconvex gradient descent with general independent
initialization for a broad class of structured link functions, and establish
universality of randomly initialized gradient descent in phase retrieval for
large aspect ratios. In the second, we develop a data-free iterative algorithm
for estimating state evolution parameters along the entire gradient descent
trajectory, thereby providing a low-cost yet statistically valid tool for
practical tasks such as hyperparameter tuning and runtime determination.
  As a by-product of our analysis, we show that in the large aspect ratio
regime, the Gaussian theoretical gradient descent coincides with a recent line
of dynamical mean-field theory for gradient descent over the constant-time
horizon.

</details>


### [143] [Dynamic Adaptive Shared Experts with Grouped Multi-Head Attention Mixture of Experts](https://arxiv.org/abs/2509.10530)
*Cheng Li,Jiexiong Liu,Yixuan Chen,Jie ji*

Main category: cs.LG

TL;DR: 提出DASG - MoE模型增强长序列建模能力，实验显示优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于MoE架构的Transformer模型在计算效率、捕捉长距离依赖及专家资源分配动态适应性方面有不足。

Method: 提出DASG - MoE模型，集成GMHA机制降低计算复杂度，设计DSSE结构实现效率与精度动态平衡，提出ADR机制优化资源分配。

Result: 在多个长序列基准数据集上实验，DASG - MoE模型优于现有模型。

Conclusion: DASG - MoE模型能有效增强长序列建模能力。

Abstract: Transformer models based on the Mixture of Experts (MoE) architecture have
made significant progress in long-sequence modeling, but existing models still
have shortcomings in computational efficiency and the ability to capture
long-range dependencies, especially in terms of the dynamic adaptability of
expert resource allocation. In this paper, we propose a Dynamic Adaptive Shared
Expert and Grouped Multi-Head Attention Hybrid Model (DASG-MoE) to enhance
long-sequence modeling capabilities by integrating three modules. First, we
employ the Grouped Multi-Head Attention (GMHA) mechanism to effectively reduce
the computational complexity of long sequences. By parallel processing through
sequence grouping, local sliding window attention, and feature aggregation, we
address long-range dependency issues and the model's lack of generalization for
local information. Second, we design a Dual-Scale Shared Expert Structure
(DSSE), where shallow experts use lightweight computations to quickly respond
to low-dimensional features, while deep experts process high-dimensional
complex semantics through pre-training transfer and post-training optimization,
achieving a dynamic balance between efficiency and accuracy. Third, we propose
a hierarchical Adaptive Dynamic Routing (ADR) mechanism that dynamically
selects expert levels based on feature complexity and task requirements, and
optimizes resource allocation through a local expert activation strategy.
Experiments on multiple long-sequence benchmark datasets demonstrate that our
DASG-MoE model outperforms state-of-the-art models.

</details>


### [144] [FinXplore: An Adaptive Deep Reinforcement Learning Framework for Balancing and Discovering Investment Opportunities](https://arxiv.org/abs/2509.10531)
*Himanshu Choudhary,Arishi Orra,Manoj Thakur*

Main category: cs.LG

TL;DR: 本文提出结合资产利用与新机会探索的投资策略，用双DRL代理实现，实验证明优于现有策略。


<details>
  <summary>Details</summary>
Motivation: 多数基于DRL的方法局限于预定义投资范围，忽略新机会探索，需要新方法以适应市场变化并提升投资组合表现。

Method: 引入投资格局，利用两个DRL代理，一个在现有投资范围分配资产，另一个探索扩展范围内的新机会。

Result: 使用两个真实市场数据集实验，证明该方法优于现有投资组合策略和基线方法。

Conclusion: 提出的结合资产利用与探索新机会的方法能适应市场变化，有效提升投资组合表现。

Abstract: Portfolio optimization is essential for balancing risk and return in
financial decision-making. Deep Reinforcement Learning (DRL) has stood out as a
cutting-edge tool for portfolio optimization that learns dynamic asset
allocation using trial-and-error interactions. However, most DRL-based methods
are restricted to allocating assets within a pre-defined investment universe
and overlook exploring new opportunities. This study introduces an investment
landscape that integrates exploiting existing assets with exploring new
investment opportunities in an extended universe. The proposed approach
leverages two DRL agents and dynamically balances these objectives to adapt to
evolving markets while enhancing portfolio performance. One agent allocates
assets within the existing universe, while another assists in exploring new
opportunities in the extended universe. The effciency of the proposed
methodology is determined using two real-world market data sets. The
experiments demonstrate the superiority of the suggested approach against the
state-of-the-art portfolio strategies and baseline methods.

</details>


### [145] [Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings](https://arxiv.org/abs/2509.10534)
*Anand Gopalakrishnan,Robert Csordás,Jürgen Schmidhuber,Michael C. Mozer*

Main category: cs.LG

TL;DR: 指出RoPE旋转位置嵌入中内容与位置信息纠缠问题，提出PoPE改进方案，在多领域表现优于RoPE。


<details>
  <summary>Details</summary>
Motivation: 发现RoPE中内容和位置信息纠缠会影响性能，尤其是需要独立匹配这两个因素时。

Method: 提出Polar Coordinate Position Embeddings (PoPE) 改进方案，消除内容与位置的混淆。

Result: 在诊断任务、自回归序列建模中，使用PoPE的Transformer在评估损失和下游任务性能上优于使用RoPE的基线；在语言建模中，不同规模模型均有提升；PoPE有强零样本长度外推能力。

Conclusion: PoPE优于RoPE，能解决RoPE存在的问题，在多领域有更好表现和更强外推能力。

Abstract: The attention mechanism in a Transformer architecture matches key to query
based on both content -- the what -- and position in a sequence -- the where.
We present an analysis indicating that what and where are entangled in the
popular RoPE rotary position embedding. This entanglement can impair
performance particularly when decisions require independent matches on these
two factors. We propose an improvement to RoPE, which we call Polar Coordinate
Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is
far superior on a diagnostic task requiring indexing solely by position or by
content. On autoregressive sequence modeling in music, genomic, and natural
language domains, Transformers using PoPE as the positional encoding scheme
outperform baselines using RoPE with respect to evaluation loss (perplexity)
and downstream task performance. On language modeling, these gains persist
across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong
zero-shot length extrapolation capabilities, whereas RoPE's performance
degrades significantly on longer sequences at test time without fine tuning or
the use of position-interpolation methods.

</details>


### [146] [Semantic-guided LoRA Parameters Generation](https://arxiv.org/abs/2509.10535)
*Miaoge Li,Yang Chen,Zhijie Rao,Can Jiang,Jingcai Guo*

Main category: cs.LG

TL;DR: 提出SG - LoRA框架，可在无额外训练和用户数据访问下生成用户特定LoRA参数，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型难处理边缘用户特定任务偏好，且为每个用户重新训练模型成本高、有隐私问题。

Method: SG - LoRA以任务描述为语义桥梁，在共享嵌入空间衡量与已知专家任务的接近度，建模目标任务的LoRA参数分布来生成参数。

Result: 在多个具有挑战性的任务上进行的大量实验证实了SG - LoRA的优越性能和显著适应性。

Conclusion: SG - LoRA能实时构建符合个人意图的LoRA模型，为零样本开放世界设置下的个性化模型适配提供了隐私保护解决方案。

Abstract: Low-Rank Adaptation (LoRA) has demonstrated strong generalization
capabilities across a variety of tasks for efficiently fine-tuning AI models,
especially on resource-constrained edges. However, in real-world applications,
edge users often exhibit task-specific preferences that are difficult to handle
with a unified model trained under a closed-world assumption, and the challenge
may further increase when there are significant domain shifts between training
and deployment. Meanwhile, retraining/fine-tuning models for each user is also
impractical due to its cost-intensive nature and privacy concerns over raw data
utilization from edges. To address these challenges, we propose Semantic-guided
LoRA Parameter Generation (SG-LoRA), the first of its kind framework to
efficiently produce user-specific LoRA parameters without any additional
training on user tasks or access to user-specific data. Concretely, SG-LoRA
uses task descriptions as the semantic bridge, measuring their proximity to a
set of known expert tasks in a shared embedding space. Based on this semantic
guidance, it models the target task's LoRA parameter distribution to generate
high-performing parameters for novel tasks. SG-LoRA enables the real-time
construction of LoRA models aligned with individual intents by distilling
knowledge from prominent LoRA experts and, meanwhile, offering a
privacy-preserving solution for personalized model adaptation in a novel
zero-shot open-world setting proposed in this work. Extensive experiments on
multiple challenging tasks confirm the superior performance and remarkable
adaptability of SG-LoRA. Code is available at
https://github.com/keepgoingjkg/SG-LoRA.

</details>


### [147] [Contextuality, Holonomy and Discrete Fiber Bundles in Group-Valued Boltzmann Machines](https://arxiv.org/abs/2509.10536)
*Jean-Pierre Magnot*

Main category: cs.LG

TL;DR: 提出受限玻尔兹曼机（RBM）的几何扩展，允许权重取值于抽象群，引入情境性指数，为AI开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 为了能对复杂关系结构进行建模，如投影变换、旋量动力学和函数对称性等，并应用于视觉、语言和量子学习。

Method: 对RBM进行几何扩展，允许权重取值于抽象群，引入基于群值环流的情境性指数。

Result: 建立了与层论情境性、规范理论和非交换几何的联系，并给出有限和无限维度的数值与图表示例。

Conclusion: 该框架为AI从曲率感知学习架构到不确定或对抗环境中的拓扑正则化开辟了新方向。

Abstract: We propose a geometric extension of restricted Boltzmann machines (RBMs) by
allowing weights to take values in abstract groups such as \(
\mathrm{GL}_n(\mathbb{R}) \), \( \mathrm{SU}(2) \), or even
infinite-dimensional operator groups. This generalization enables the modeling
of complex relational structures, including projective transformations, spinor
dynamics, and functional symmetries, with direct applications to vision,
language, and quantum learning.
  A central contribution of this work is the introduction of a
\emph{contextuality index} based on group-valued holonomies computed along
cycles in the RBM graph. This index quantifies the global inconsistency or
"curvature" induced by local weights, generalizing classical notions of
coherence, consistency, and geometric flatness. We establish links with
sheaf-theoretic contextuality, gauge theory, and noncommutative geometry, and
provide numerical and diagrammatic examples in both finite and infinite
dimensions.
  This framework opens novel directions in AI, from curvature-aware learning
architectures to topological regularization in uncertain or adversarial
environments.

</details>


### [148] [Learning Neural Networks by Neuron Pursuit](https://arxiv.org/abs/2509.12154)
*Akshay Kumar,Jarvis Haupt*

Main category: cs.LG

TL;DR: 本文先研究齐次神经网络梯度流在一类鞍点附近的演化，后基于此结果提出神经元追踪（NP）贪心算法训练深度神经网络，并通过实验验证算法有效性。


<details>
  <summary>Details</summary>
Motivation: 受先前齐次网络研究中对特定鞍点的发现启发，研究梯度流在鞍点附近的行为，并基于此开发新的深度神经网络训练算法。

Method: 先理论分析梯度流在鞍点附近的演化，后提出迭代的神经元追踪（NP）算法，交替进行网络扩展和损失最小化。

Result: 梯度流在鞍点附近长时间保持，小范数权重方向收敛；数值实验验证了NP算法的有效性。

Conclusion: 对梯度流在鞍点附近的研究有新发现，提出的NP算法可有效训练深度神经网络。

Abstract: The first part of this paper studies the evolution of gradient flow for
homogeneous neural networks near a class of saddle points exhibiting a sparsity
structure. The choice of these saddle points is motivated from previous works
on homogeneous networks, which identified the first saddle point encountered by
gradient flow after escaping the origin. It is shown here that, when
initialized sufficiently close to such saddle points, gradient flow remains
near the saddle point for a sufficiently long time, during which the set of
weights with small norm remain small but converge in direction. Furthermore,
important empirical observations are made on the behavior of gradient descent
after escaping these saddle points. The second part of the paper, motivated by
these results, introduces a greedy algorithm to train deep neural networks
called Neuron Pursuit (NP). It is an iterative procedure which alternates
between expanding the network by adding neuron(s) with carefully chosen
weights, and minimizing the training loss using this augmented network. The
efficacy of the proposed algorithm is validated using numerical experiments.

</details>


### [149] [DualAlign: Generating Clinically Grounded Synthetic Data](https://arxiv.org/abs/2509.10538)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.LG

TL;DR: 提出DualAlign框架生成合成临床数据，以AD为例验证效果，虽未完全捕捉纵向复杂性，但为低资源临床文本分析提供实用方法。


<details>
  <summary>Details</summary>
Motivation: 由于真实世界电子健康记录隐私限制、罕见疾病注释数据有限和观测数据集存在偏差，合成临床数据对推进医疗AI很重要，但生成现实且有临床意义的数据仍具挑战。

Method: 引入DualAlign框架，通过统计对齐（基于患者人口统计和风险因素进行生成）和语义对齐（结合现实症状轨迹引导内容生成）来增强统计保真度和临床合理性。

Result: 以AD为例，DualAlign生成的症状级句子更能反映现实临床记录；微调模型比仅用黄金数据或无引导合成基线训练的模型性能有显著提升。

Conclusion: DualAlign虽未完全捕捉纵向复杂性，但为生成有临床依据、保护隐私的合成数据以支持低资源临床文本分析提供了实用方法。

Abstract: Synthetic clinical data are increasingly important for advancing AI in
healthcare, given strict privacy constraints on real-world EHRs, limited
availability of annotated rare-condition data, and systemic biases in
observational datasets. While large language models (LLMs) can generate fluent
clinical text, producing synthetic data that is both realistic and clinically
meaningful remains challenging. We introduce DualAlign, a framework that
enhances statistical fidelity and clinical plausibility through dual alignment:
(1) statistical alignment, which conditions generation on patient demographics
and risk factors; and (2) semantic alignment, which incorporates real-world
symptom trajectories to guide content generation. Using Alzheimer's disease
(AD) as a case study, DualAlign produces context-grounded symptom-level
sentences that better reflect real-world clinical documentation. Fine-tuning an
LLaMA 3.1-8B model with a combination of DualAlign-generated and
human-annotated data yields substantial performance gains over models trained
on gold data alone or unguided synthetic baselines. While DualAlign does not
fully capture longitudinal complexity, it offers a practical approach for
generating clinically grounded, privacy-preserving synthetic data to support
low-resource clinical text analysis.

</details>


### [150] [GTS_Forecaster: a novel deep learning based geodetic time series forecasting toolbox with python](https://arxiv.org/abs/2509.10560)
*Xuechen Liang,Xiaoxing He,Shengdao Wang,Jean-Philippe Montillet,Zhengkai Huang,Gaël Kermarrec,Shunqiang Hu,Yu Zhou,Jiahui Huang*

Main category: cs.LG

TL;DR: 介绍用于大地测量时间序列预测的开源Python包GTS Forecaster，结合前沿模型与易用界面，便于深度学习在大地测量预测中应用。


<details>
  <summary>Details</summary>
Motivation: 大地测量时间序列变量的非线性、非平稳和不完整特性给经典模型带来挑战，准确预测这些变量对预警和减灾至关重要。

Method: 引入GTS Forecaster包，集成核注意力网络、基于图神经网络的门控循环单元和时间感知图神经网络等深度学习模型，提供预处理工具。

Result: GTS Forecaster支持GNSS、SSH和TG数据集的预测、可视化和评估，可适应一般时间序列应用。

Conclusion: 结合前沿模型与易用界面，有助于深度学习在大地测量预测任务中的应用。

Abstract: Geodetic time series -- such as Global Navigation Satellite System (GNSS)
positions, satellite altimetry-derived sea surface height (SSH), and tide gauge
(TG) records -- is essential for monitoring surface deformation and sea level
change. Accurate forecasts of these variables can enhance early warning systems
and support hazard mitigation for earthquakes, landslides, coastal storm surge,
and long-term sea level. However, the nonlinear, non-stationary, and incomplete
nature of such variables presents significant challenges for classic models,
which often fail to capture long-term dependencies and complex spatiotemporal
dynamics. We introduce GTS Forecaster, an open-source Python package for
geodetic time series forecasting. It integrates advanced deep learning models
-- including kernel attention networks (KAN), graph neural network-based gated
recurrent units (GNNGRU), and time-aware graph neural networks (TimeGNN) -- to
effectively model nonlinear spatial-temporal patterns. The package also
provides robust preprocessing tools, including outlier detection and a
reinforcement learning-based gap-filling algorithm, the Kalman-TransFusion
Interpolation Framework (KTIF). GTS Forecaster currently supports forecasting,
visualization, and evaluation of GNSS, SSH, and TG datasets, and is adaptable
to general time series applications. By combining cutting-edge models with an
accessible interface, it facilitates the application of deep learning in
geodetic forecasting tasks.

</details>


### [151] [SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs](https://arxiv.org/abs/2509.10594)
*Iqbal H. Sarker,Helge Janicke,Ahmad Mohsin,Leandros Maglaras*

Main category: cs.LG

TL;DR: 本文提出面向中小企业的AI生命周期内嵌入信任与道德原则的框架，为负责任的AI采用提供路线图。


<details>
  <summary>Details</summary>
Motivation: 人工智能和大语言模型在中小企业采用中存在技术、伦理和信任问题，需解决这些问题以实现安全和负责任使用。

Method: 提出围绕数据、算法、人工监督和模型架构四个支柱的多阶段结构化框架，将理论伦理原则与操作实践相结合。

Result: 该框架增强了AI在不同中小企业应用中的能力。

Conclusion: 为中小企业负责任地采用AI提供结构化路线图，信任和伦理可促进企业的韧性、竞争力和可持续创新。

Abstract: Artificial Intelligence (AI) and Large Language Models (LLMs) are reshaping
today's business practices, however, their adoption within small and
medium-sized enterprises (SMEs) raises significant technical, ethical and trust
issues. This paper proposes a structured, multi-phased framework designed to
embed trust and ethical principles throughout the AI lifecycle for their secure
and responsible use in SMEs. Structured around four pillars, i.e., Data,
Algorithms, Human oversight, and Model Architecture, the framework bridges
theoretical ethical principles with operational practice, enhancing AI
capabilities in diverse SME applications. Ultimately, this paper offers a
structured roadmap for responsible AI adoption, framing trust and ethics as a
catalyst for resilience, competitiveness, and sustainable innovation in SMEs.

</details>


### [152] [Accurate and Private Diagnosis of Rare Genetic Syndromes from Facial Images with Federated Deep Learning](https://arxiv.org/abs/2509.10635)
*Ali Burak Ünal,Cem Ata Baykara,Peter Krawitz,Mete Akgün*

Main category: cs.LG

TL;DR: 提出基于跨孤岛水平联邦学习框架的联邦GestaltMatcher服务，可在不共享患者图像下协作训练，实验显示其保留超90%集中式性能且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有GestaltMatcher框架依赖集中式数据集，患者数据分散且受隐私法规限制，限制其进一步发展。

Method: 引入基于跨孤岛水平联邦学习框架的联邦GestaltMatcher服务，将患者数据映射到共享潜空间，采用隐私保护的核矩阵计算框架。

Result: 联邦服务保留超90%集中式性能，对不同孤岛数量和异构数据分布保持鲁棒。

Conclusion: 该联邦服务在不共享患者图像下实现协作训练，能有效推动面部形态学机器学习应用发展。

Abstract: Machine learning has shown promise in facial dysmorphology, where
characteristic facial features provide diagnostic clues for rare genetic
disorders. GestaltMatcher, a leading framework in this field, has demonstrated
clinical utility across multiple studies, but its reliance on centralized
datasets limits further development, as patient data are siloed across
institutions and subject to strict privacy regulations. We introduce a
federated GestaltMatcher service based on a cross-silo horizontal federated
learning framework, which allows hospitals to collaboratively train a global
ensemble feature extractor without sharing patient images. Patient data are
mapped into a shared latent space, and a privacy-preserving kernel matrix
computation framework enables syndrome inference and discovery while
safeguarding confidentiality. New participants can directly benefit from and
contribute to the system by adopting the global feature extractor and kernel
configuration from previous training rounds. Experiments show that the
federated service retains over 90% of centralized performance and remains
robust to both varying silo numbers and heterogeneous data distributions.

</details>


### [153] [Test-Time Warmup for Multimodal Large Language Models](https://arxiv.org/abs/2509.10641)
*Nikita Rajaneesh,Thomas Zollo,Richard Zemel*

Main category: cs.LG

TL;DR: 多模态大语言模型在复杂推理任务上表现不佳，提出测试时预热方法，在多个数据集上提升性能，证明推理前“预热”可增强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽各组件在大规模数据集预训练，但整体模型训练样本少，在复杂推理任务上表现弱。

Method: 提出测试时预热方法，针对每个测试实例，利用弱监督辅助任务的数据来调整多模态大语言模型。

Result: 在Llama - Vision - Instruct模型上，MMMU相对性能提升4.03%，VQA - Rad提升5.28%，GQA提升1.63%。

Conclusion: 推理前“预热”能增强多模态大语言模型在不同推理任务上的鲁棒性。

Abstract: Multimodal Large Language Models (MLLMs) hold great promise for advanced
reasoning at the intersection of text and images, yet they have not fully
realized this potential. MLLMs typically integrate an LLM, a vision encoder,
and a connector that maps the vision encoder's embeddings into the LLM's text
embedding space. Although each component is pretrained on massive datasets with
billions of samples, the entire multimodal model is typically trained on only
thousands (or a few million) samples, which can result in weak performance on
complex reasoning tasks. To address these shortcomings, instead of relying on
extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup
method that adapts the MLLM per test instance by leveraging data from weakly
supervised auxiliary tasks. With our approach, we observe a relative
performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on
the Llama-Vision-Instruct model. Our method demonstrates that 'warming up'
before inference can enhance MLLMs' robustness across diverse reasoning tasks.

</details>


### [154] [Self-Supervised Goal-Reaching Results in Multi-Agent Cooperation and Exploration](https://arxiv.org/abs/2509.10656)
*Chirayu Nimonkar,Shlok Shah,Catherine Ji,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 研究利用自监督目标达成技术使智能体合作，该方法在MARL基准测试中表现优于其他方法，能实现涌现式合作与探索。


<details>
  <summary>Details</summary>
Motivation: 为使自主智能体群体实现特定目标需协调和长视野推理，但设计奖励函数来引发此类行为具有挑战性。

Method: 让智能体最大化访问特定目标的可能性，而非最大化标量奖励，用户可通过单一目标状态指定任务。

Result: 在MARL基准测试中，所提方法优于使用相同稀疏奖励信号的其他方法，且能实现涌现式合作与探索。

Conclusion: 自监督目标达成技术能让智能体从稀疏反馈中学习，实现合作与探索。

Abstract: For groups of autonomous agents to achieve a particular goal, they must
engage in coordination and long-horizon reasoning. However, designing reward
functions to elicit such behavior is challenging. In this paper, we study how
self-supervised goal-reaching techniques can be leveraged to enable agents to
cooperate. The key idea is that, rather than have agents maximize some scalar
reward, agents aim to maximize the likelihood of visiting a certain goal. This
problem setting enables human users to specify tasks via a single goal state
rather than implementing a complex reward function. While the feedback signal
is quite sparse, we will demonstrate that self-supervised goal-reaching
techniques enable agents to learn from such feedback. On MARL benchmarks, our
proposed method outperforms alternative approaches that have access to the same
sparse reward signal as our method. While our method has no explicit mechanism
for exploration, we observe that self-supervised multi-agent goal-reaching
leads to emergent cooperation and exploration in settings where alternative
approaches never witness a single successful trial.

</details>


### [155] [Least-Ambiguous Multi-Label Classifier](https://arxiv.org/abs/2509.10689)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: 提出一种无模型的单正多标签学习方法，在12个基准数据集上验证效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 多标签学习收集全标签注释成本高，单正多标签学习（SPMLL）因部分监督带来挑战。

Method: 采用共形预测产生校准的集值输出，不依赖标签分布假设弥合单标签训练和多标签评估的监督差距。

Result: 在12个基准数据集上评估，相比现有基线有持续改进且有实际适用性。

Conclusion: 所提无模型方法能有效解决SPMLL问题，实现可靠多标签预测。

Abstract: Multi-label learning often requires identifying all relevant labels for
training instances, but collecting full label annotations is costly and
labor-intensive. In many datasets, only a single positive label is annotated
per training instance, despite the presence of multiple relevant labels. This
setting, known as single-positive multi-label learning (SPMLL), presents a
significant challenge due to its extreme form of partial supervision. We
propose a model-agnostic approach to SPMLL that draws on conformal prediction
to produce calibrated set-valued outputs, enabling reliable multi-label
predictions at test time. Our method bridges the supervision gap between
single-label training and multi-label evaluation without relying on label
distribution assumptions. We evaluate our approach on 12 benchmark datasets,
demonstrating consistent improvements over existing baselines and practical
applicability.

</details>


### [156] [Verifying Computational Graphs in Production-Grade Distributed Machine Learning Frameworks](https://arxiv.org/abs/2509.10694)
*Kahfi S. Zulkifli,Wenbo Qian,Shaowei Zhu,Yuan Zhou,Zhen Zhang,Chang Lou*

Main category: cs.LG

TL;DR: 提出轻量级框架Scalify检测机器学习框架中的静默错误，可在数分钟内验证大模型并发现生产框架中未知错误。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习框架的并行和优化技术引入静默错误，现有解决方案不适用生产环境。

Method: 使用等式饱和和Datalog式推理验证计算图语义等价性，采用并行重写、层记忆化等技术，结合关系推理和符号双射推理，定位代码错误。

Result: 能在普通机器上数分钟内验证如Llama - 3.1 - 405B大小的模型，发现亚马逊生产机器学习框架中5个未知错误。

Conclusion: Scalify是有效的轻量级验证框架，可用于发现和调试机器学习框架中的静默错误。

Abstract: Modern machine learning frameworks support very large models by incorporating
parallelism and optimization techniques. Yet, these very techniques add new
layers of complexity, introducing silent errors that severely degrade model
performance. Existing solutions are either ad hoc or too costly for production.
  We present Scalify, a lightweight framework that exposes silent errors by
verifying semantic equivalence of computational graphs using equality
saturation and Datalog-style reasoning. To scale, Scalify partitions graphs
with parallel rewriting and layer memoization, reuses rewrite templates, and
augments equality saturation with relational reasoning and symbolic bijection
inference. It further localizes discrepancies to precise code sites, turning
verification results into actionable debugging guidance. Scalify verifies
models as large as Llama-3.1-405B within minutes on a commodity machine and
exposed five unknown bugs in Amazon production machine learning frameworks.

</details>


### [157] [Kalman Bayesian Transformer](https://arxiv.org/abs/2509.10695)
*Haoming Jing,Oren Wright,José M. F. Moura,Yorie Nakahira*

Main category: cs.LG

TL;DR: 本文提出新方法将顺序微调视为贝叶斯框架下的后验推理问题，实现稳健且数据高效的顺序学习，通过数值模拟验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 顺序学习需在少量数据下稳定训练，平衡新信息与预训练模型知识，且在低延迟环境中还需量化和处理不确定性。

Method: 将顺序微调构建为贝叶斯框架下的后验推理问题，集成随机变量的闭式矩传播、卡尔曼贝叶斯神经网络和softmax函数矩的泰勒近似。

Result: 通过数值模拟，展示了该方法在分布转移和内存资源有限任务中对决策变压器进行顺序适应的有效性。

Conclusion: 所提方法能实现稳健且数据高效的顺序学习。

Abstract: Sequential fine-tuning of transformers is useful when new data arrive
sequentially, especially with shifting distributions. Unlike batch learning,
sequential learning demands that training be stabilized despite a small amount
of data by balancing new information and previously learned knowledge in the
pre-trained models. This challenge is further complicated when training is to
be completed in latency-critical environments and learning must additionally
quantify and be mediated by uncertainty. Motivated by these challenges, we
propose a novel method that frames sequential fine-tuning as a posterior
inference problem within a Bayesian framework. Our approach integrates
closed-form moment propagation of random variables, Kalman Bayesian Neural
Networks, and Taylor approximations of the moments of softmax functions. By
explicitly accounting for pre-trained models as priors and adaptively balancing
them against new information based on quantified uncertainty, our method
achieves robust and data-efficient sequential learning. The effectiveness of
our method is demonstrated through numerical simulations involving sequential
adaptation of a decision transformer to tasks characterized by distribution
shifts and limited memory resources.

</details>


### [158] [CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome Prediction](https://arxiv.org/abs/2509.10698)
*Rabeya Tus Sadia,Qiang Cheng*

Main category: cs.LG

TL;DR: 提出CrunchLLM框架用于初创公司成功预测，集成结构化与非结构化数据，准确率超80%，还提供可解释推理。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以有效利用异构数据进行初创公司成功预测，传统机器学习仅用结构化特征准确率一般，大语言模型难适应特定业务数据。

Method: 提出CrunchLLM框架，集成公司结构化属性和非结构化文本叙述，应用参数高效微调策略和提示优化。

Result: 在Crunchbase初创公司成功预测上准确率超80%，显著优于传统分类器和基线大语言模型，还提供可解释推理痕迹。

Conclusion: 通过领域感知微调与数据融合适应大语言模型可推动创业结果预测建模，为风险投资和创新政策的数据驱动决策提供框架和工具。

Abstract: Predicting the success of start-up companies, defined as achieving an exit
through acquisition or IPO, is a critical problem in entrepreneurship and
innovation research. Datasets such as Crunchbase provide both structured
information (e.g., funding rounds, industries, investor networks) and
unstructured text (e.g., company descriptions), but effectively leveraging this
heterogeneous data for prediction remains challenging. Traditional machine
learning approaches often rely only on structured features and achieve moderate
accuracy, while large language models (LLMs) offer rich reasoning abilities but
struggle to adapt directly to domain-specific business data. We present
\textbf{CrunchLLM}, a domain-adapted LLM framework for startup success
prediction. CrunchLLM integrates structured company attributes with
unstructured textual narratives and applies parameter-efficient fine-tuning
strategies alongside prompt optimization to specialize foundation models for
entrepreneurship data. Our approach achieves accuracy exceeding 80\% on
Crunchbase startup success prediction, significantly outperforming traditional
classifiers and baseline LLMs. Beyond predictive performance, CrunchLLM
provides interpretable reasoning traces that justify its predictions, enhancing
transparency and trustworthiness for financial and policy decision makers. This
work demonstrates how adapting LLMs with domain-aware fine-tuning and
structured--unstructured data fusion can advance predictive modeling of
entrepreneurial outcomes. CrunchLLM contributes a methodological framework and
a practical tool for data-driven decision making in venture capital and
innovation policy.

</details>


### [159] [Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition](https://arxiv.org/abs/2509.10729)
*Ilker Demirel,Karan Thakkar,Benjamin Elizalde,Miquel Espi Marques,Shirley Ren,Jaya Narain*

Main category: cs.LG

TL;DR: 研究表明大语言模型（LLMs）可用于音频和运动时间序列数据的活动分类后期融合，在无特定任务训练下取得良好效果，还能用于多模态时间应用及模型部署。


<details>
  <summary>Details</summary>
Motivation: 解决传感器数据流中整合互补信息的挑战，利用大语言模型进行活动分类。

Method: 从Ego4D数据集中筛选用于不同活动识别的数据子集，采用大语言模型进行后期融合。

Result: 评估的大语言模型在12类零样本和单样本分类的F1分数显著高于随机水平，无需特定任务训练。

Conclusion: 基于大语言模型的融合可用于有限对齐训练数据的多模态时间应用，且能在无需额外内存和计算的情况下进行模型部署。

Abstract: Sensor data streams provide valuable information around activities and
context for downstream applications, though integrating complementary
information can be challenging. We show that large language models (LLMs) can
be used for late fusion for activity classification from audio and motion time
series data. We curated a subset of data for diverse activity recognition
across contexts (e.g., household activities, sports) from the Ego4D dataset.
Evaluated LLMs achieved 12-class zero- and one-shot classification F1-scores
significantly above chance, with no task-specific training. Zero-shot
classification via LLM-based fusion from modality-specific models can enable
multimodal temporal applications where there is limited aligned training data
for learning a shared embedding space. Additionally, LLM-based fusion can
enable model deploying without requiring additional memory and computation for
targeted application-specific multimodal models.

</details>


### [160] [Matched-Pair Experimental Design with Active Learning](https://arxiv.org/abs/2509.10742)
*Weizhi Li,Gautam Dasarathy,Visar Berisha*

Main category: cs.LG

TL;DR: 本文提出一种序贯主动招募高治疗效果区域患者的配对实验设计，将目标区域识别建模为分类问题并提出主动学习框架，经理论分析和实验验证其有效。


<details>
  <summary>Details</summary>
Motivation: 整体人群中治疗效果小，需识别和针对高治疗效果区域。

Method: 提出序贯主动招募患者的配对实验设计，将目标区域识别作为分类问题，采用主动学习框架。

Result: 通过理论分析框架的标签复杂度和实际场景实验，证明了该方法的效率和优势。

Conclusion: 所提设计能降低检测治疗效果的实验成本，确保识别区域涵盖整个高治疗效果区域。

Abstract: Matched-pair experimental designs aim to detect treatment effects by pairing
participants and comparing within-pair outcome differences. In many situations,
the overall effect size is small across the entire population. Then, the focus
naturally shifts to identifying and targeting high treatment-effect regions
where the intervention is most effective. This paper proposes a matched-pair
experimental design that sequentially and actively enrolls patients in high
treatment-effect regions. Importantly, we frame the identification of the
target region as a classification problem and propose an active learning
framework tailored to matched-pair designs. The proposed design not only
reduces the experimental cost of detecting treatment efficacy, but also ensures
that the identified regions enclose the entire high-treatment-effect regions.
Our theoretical analysis of the framework's label complexity, along with
experiments in practical scenarios, demonstrates the efficiency and advantages
of the approach.

</details>


### [161] [HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling](https://arxiv.org/abs/2509.10753)
*Minh Vu,Brian K. Tran,Syed A. Shah,Geigh Zollicoffer,Nhat Hoang-Xuan,Manish Bhattarai*

Main category: cs.LG

TL;DR: 本文介绍用于大语言模型幻觉检测的HalluField方法，基于场论，计算高效，能实现先进的幻觉检测性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生幻觉内容，限制其在高风险应用中的部署，因此需要通用方法检测幻觉。

Method: 引入HalluField，基于参数化变分原理和热力学，将大语言模型对查询的响应建模为离散似然标记路径集合，分析能量和熵分布变化来量化响应语义稳定性，通过识别能量景观中的不稳定行为检测幻觉。

Result: HalluField计算高效、实用，无需微调或辅助神经网络，基于热力学第一定律有原则性物理解释。

Conclusion: 通过物理视角建模大语言模型行为，HalluField在不同模型和数据集上实现了先进的幻觉检测性能。

Abstract: Large Language Models (LLMs) exhibit impressive reasoning and
question-answering capabilities. However, they often produce inaccurate or
unreliable content known as hallucinations. This unreliability significantly
limits their deployment in high-stakes applications. Thus, there is a growing
need for a general-purpose method to detect hallucinations in LLMs. In this
work, we introduce HalluField, a novel field-theoretic approach for
hallucination detection based on a parametrized variational principle and
thermodynamics. Inspired by thermodynamics, HalluField models an LLM's response
to a given query and temperature setting as a collection of discrete likelihood
token paths, each associated with a corresponding energy and entropy. By
analyzing how energy and entropy distributions vary across token paths under
changes in temperature and likelihood, HalluField quantifies the semantic
stability of a response. Hallucinations are then detected by identifying
unstable or erratic behavior in this energy landscape. HalluField is
computationally efficient and highly practical: it operates directly on the
model's output logits without requiring fine-tuning or auxiliary neural
networks. Notably, the method is grounded in a principled physical
interpretation, drawing analogies to the first law of thermodynamics.
Remarkably, by modeling LLM behavior through this physical lens, HalluField
achieves state-of-the-art hallucination detection performance across models and
datasets.

</details>


### [162] [Contextual Budget Bandit for Food Rescue Volunteer Engagement](https://arxiv.org/abs/2509.10777)
*Ariana Tang,Naveen Raman,Fei Fang,Zheyuan Ryan Shi*

Main category: cs.LG

TL;DR: 提出Contextual Budget Bandit解决志愿食物救援平台的地理差异问题，开发启发式算法和Mitosis算法，实验证明算法表现优且能实现地理公平。


<details>
  <summary>Details</summary>
Motivation: 现有提升志愿者参与度的算法加剧地理差异，导致部分社区处于劣势，需解决此问题。

Method: 提出Contextual Budget Bandit，结合上下文相关预算分配；开发启发式算法；设计Mitosis算法保证计算最优预算分配。

Result: 算法在合成和真实食物救援数据集上表现优于基线。

Conclusion: 算法能在食物救援中实现地理公平。

Abstract: Volunteer-based food rescue platforms tackle food waste by matching surplus
food to communities in need. These platforms face the dual problem of
maintaining volunteer engagement and maximizing the food rescued. Existing
algorithms to improve volunteer engagement exacerbate geographical disparities,
leaving some communities systematically disadvantaged. We address this issue by
proposing Contextual Budget Bandit. Contextual Budget Bandit incorporates
context-dependent budget allocation in restless multi-armed bandits, a model of
decision-making which allows for stateful arms. By doing so, we can allocate
higher budgets to communities with lower match rates, thereby alleviating
geographical disparities. To tackle this problem, we develop an empirically
fast heuristic algorithm. Because the heuristic algorithm can achieve a poor
approximation when active volunteers are scarce, we design the Mitosis
algorithm, which is guaranteed to compute the optimal budget allocation.
Empirically, we demonstrate that our algorithms outperform baselines on both
synthetic and real-world food rescue datasets, and show how our algorithm
achieves geographical fairness in food rescue.

</details>


### [163] [GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research](https://arxiv.org/abs/2509.10790)
*Luke Howard*

Main category: cs.LG

TL;DR: 介绍GoldenTransformer，一个评估大语言模型对硬件故障弹性的框架，支持多类型故障注入，具备多种特性，可用于模型鲁棒性分析。


<details>
  <summary>Details</summary>
Motivation: Transformer模型广泛应用，但在故障条件下的鲁棒性研究不足，需要评估其对硬件故障的弹性。

Method: 设计GoldenTransformer框架，基于Python，结合PyTorch和HuggingFace Transformers，可在预训练模型中注入多种故障。

Result: 详细介绍框架设计和使用，并通过分类和生成任务的实验进行展示。

Conclusion: GoldenTransformer为研究人员和从业者提供了有价值的工具，可用于模型鲁棒性分析和指导实际应用中的可靠系统设计。

Abstract: Transformers have become the foundation for a wide range of
state--of--the--art models across natural language processing, computer vision,
and other machine learning domains. Despite their widespread deployment, the
robustness of these models under fault conditions remains underexplored. We
present GoldenTransformer, a modular and extensible fault injection framework
designed to evaluate the resiliency of Large Language Models to induced
hardware faults. GoldenTransformer offers a unified Python-based platform for
injecting diverse classes of faults--such as weight corruption, activation
injections, and attention--level disruptions--into pretrained
transformer--based models. Inspired by the GoldenEye simulator for DNNs, our
framework focuses on the unique challenges of working with large transformer
architectures, including considerations such as structural complexity, latent
dependencies, and nonuniform layer definitions. GoldenTransformer is built atop
PyTorch and HuggingFace Transformers, and it supports experiment
reproducibility, metric logging, and visualization out of the box. We detail
the technical design and use of GoldenTransformer and demonstrate through
several example experiments on classification and generation tasks. By enabling
controlled injection of faults at multiple logical and structural points in a
transformer, GoldenTransformer offers researchers and practitioners a valuable
tool for model robustness analysis and for guiding dependable system design in
real-world LLM applications.

</details>


### [164] [Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone](https://arxiv.org/abs/2509.10809)
*Antonio Bărbălau,Cristian Daniel Păduraru,Teodor Poncu,Alexandru Tifrea,Elena Burceanu*

Main category: cs.LG

TL;DR: 本文挑战现有基于SAE的去偏方法假设，提出基于编码器的去偏方法，所提S&P TopK框架在公平性指标上表现优异且保持下游性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于SAE的去偏方法假定特征表示存在于解码器权重中，作者挑战这一假设，寻求新的去偏方法。

Method: 提出一种非常规SAE特征选择策略、一种使输入嵌入与编码器权重正交化的去偏方法，以及通过编码器权重插值在去偏过程中保持性能的机制。

Result: S&P TopK框架在公平性指标上比传统SAE使用方式高3.2倍，在测试时VLM去偏结果上比现有技术高1.8倍，且保持下游性能。

Conclusion: 基于编码器的去偏方法有效，S&P TopK框架能在提升公平性的同时保持下游性能。

Abstract: Sparse Autoencoders (SAEs) have proven valuable due to their ability to
provide interpretable and steerable representations. Current debiasing methods
based on SAEs manipulate these sparse activations presuming that feature
representations are housed within decoder weights. We challenge this
fundamental assumption and introduce an encoder-focused alternative for
representation debiasing, contributing three key findings: (i) we highlight an
unconventional SAE feature selection strategy, (ii) we propose a novel SAE
debiasing methodology that orthogonalizes input embeddings against encoder
weights, and (iii) we establish a performance-preserving mechanism during
debiasing through encoder weight interpolation. Our Selection and Projection
framework, termed S\&P TopK, surpasses conventional SAE usage in fairness
metrics by a factor of up to 3.2 and advances state-of-the-art test-time VLM
debiasing results by a factor of up to 1.8 while maintaining downstream
performance.

</details>


### [165] [Neurosymbolic AI Transfer Learning Improves Network Intrusion Detection](https://arxiv.org/abs/2509.10850)
*Huynh T. T. Tran,Jacob Sander,Achraf Cohen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.LG

TL;DR: 本文提出用于网络入侵检测系统的神经符号AI框架，结合迁移学习和不确定性量化，结果显示迁移学习模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 迁移学习在网络安全领域应用未充分探索，网络入侵检测系统对对抗恶意活动至关重要，需创新方法。

Method: 提出结合迁移学习和不确定性量化的神经符号AI框架。

Result: 在大型结构化数据集上训练的迁移学习模型优于依赖小数据集的基于神经网络的模型。

Conclusion: 为网络安全解决方案开辟了新纪元。

Abstract: Transfer learning is commonly utilized in various fields such as computer
vision, natural language processing, and medical imaging due to its impressive
capability to address subtasks and work with different datasets. However, its
application in cybersecurity has not been thoroughly explored. In this paper,
we present an innovative neurosymbolic AI framework designed for network
intrusion detection systems, which play a crucial role in combating malicious
activities in cybersecurity. Our framework leverages transfer learning and
uncertainty quantification. The findings indicate that transfer learning
models, trained on large and well-structured datasets, outperform neural-based
models that rely on smaller datasets, paving the way for a new era in
cybersecurity solutions.

</details>


### [166] [CogGNN: Cognitive Graph Neural Networks in Generative Connectomics](https://arxiv.org/abs/2509.10864)
*Mayssa Soussia,Yijun Lin,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: 本文提出认知生成模型CogGNN，能生成保留认知特征的脑网络，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于GNN的生成学习方法仅关注结构和拓扑属性，忽略认知特征，需新方法解决此问题。

Method: 引入CogGNN，赋予GNN认知能力，采用基于视觉记忆的损失函数，结合共同优化策略学习连接脑模板。

Result: CogGNN生成的连接脑模板在认知和结构上都有意义，且在实验中优于现有方法。

Conclusion: CogGNN为基于认知的脑网络建模奠定了坚实基础。

Abstract: Generative learning has advanced network neuroscience, enabling tasks like
graph super-resolution, temporal graph prediction, and multimodal brain graph
fusion. However, current methods, mainly based on graph neural networks (GNNs),
focus solely on structural and topological properties, neglecting cognitive
traits. To address this, we introduce the first cognified generative model,
CogGNN, which endows GNNs with cognitive capabilities (e.g., visual memory) to
generate brain networks that preserve cognitive features. While broadly
applicable, we present CogGNN, a specific variant designed to integrate visual
input, a key factor in brain functions like pattern recognition and memory
recall. As a proof of concept, we use our model to learn connectional brain
templates (CBTs), population-level fingerprints from multi-view brain networks.
Unlike prior work that overlooks cognitive properties, CogGNN generates CBTs
that are both cognitively and structurally meaningful. Our contributions are:
(i) a novel cognition-aware generative model with a visual-memory-based loss;
(ii) a CBT-learning framework with a co-optimization strategy to yield
well-centered, discriminative, cognitively enhanced templates. Extensive
experiments show that CogGNN outperforms state-of-the-art methods, establishing
a strong foundation for cognitively grounded brain network modeling.

</details>


### [167] [GTHNA: Local-global Graph Transformer with Memory Reconstruction for Holistic Node Anomaly Evaluation](https://arxiv.org/abs/2509.10869)
*Mingkang Li,Xuexiong Luo,Yue Zhang,Yaoyang Li,Fu Lin*

Main category: cs.LG

TL;DR: 本文提出一种新的异常评估框架用于图结构数据异常检测，在多数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图结构数据异常检测方法存在GCN过平滑、图重建方法受异常节点干扰导致检测不准确的问题。

Method: 提出包含局部 - 全局Transformer编码器、内存引导重建机制和多尺度表示匹配策略的异常评估框架，结合重建误差和内存匹配信号计算异常分数。

Result: 在七个基准数据集上的实验表明该方法优于现有最先进的方法。

Conclusion: 该方法为不同图领域的异常检测提供了全面且可推广的解决方案。

Abstract: Anomaly detection in graph-structured data is an inherently challenging
problem, as it requires the identification of rare nodes that deviate from the
majority in both their structural and behavioral characteristics. Existing
methods, such as those based on graph convolutional networks (GCNs), often
suffer from over-smoothing, which causes the learned node representations to
become indistinguishable. Furthermore, graph reconstruction-based approaches
are vulnerable to anomalous node interference during the reconstruction
process, leading to inaccurate anomaly detection. In this work, we propose a
novel and holistic anomaly evaluation framework that integrates three key
components: a local-global Transformer encoder, a memory-guided reconstruction
mechanism, and a multi-scale representation matching strategy. These components
work synergistically to enhance the model's ability to capture both local and
global structural dependencies, suppress the influence of anomalous nodes, and
assess anomalies from multiple levels of granularity. Anomaly scores are
computed by combining reconstruction errors and memory matching signals,
resulting in a more robust evaluation. Extensive experiments on seven benchmark
datasets demonstrate that our method outperforms existing state-of-the-art
approaches, offering a comprehensive and generalizable solution for anomaly
detection across various graph domains.

</details>


### [168] [Optimal message passing for molecular prediction is simple, attentive and spatial](https://arxiv.org/abs/2509.10871)
*Alma C. Castaneda-Leautaud,Rommie E. Amaro*

Main category: cs.LG

TL;DR: 本文设计模型架构提升分子属性预测性能，评估数据集多样性，发现简单模型效果好，卷积归一化因子非必要，2D图结合3D描述符可降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 提高消息传递神经网络对分子属性预测的性能。

Method: 设计模型架构，评估数据集多样性，分析空间特征和3D图的影响。

Result: 设计的模型达最优性能，简单模型类可分性更高，卷积归一化因子并非都有益，2D图结合3D描述符能保持性能并降低成本。

Conclusion: 相对简单的模型可提升预测性能，2D图结合3D描述符是高效选择。

Abstract: Strategies to improve the predicting performance of Message-Passing
Neural-Networks for molecular property predictions can be achieved by
simplifying how the message is passed and by using descriptors that capture
multiple aspects of molecular graphs. In this work, we designed model
architectures that achieved state-of-the-art performance, surpassing more
complex models such as those pre-trained on external databases. We assessed
dataset diversity to complement our performance results, finding that
structural diversity influences the need for additional components in our MPNNs
and feature sets.
  In most datasets, our best architecture employs bidirectional message-passing
with an attention mechanism, applied to a minimalist message formulation that
excludes self-perception, highlighting that relatively simpler models, compared
to classical MPNNs, yield higher class separability. In contrast, we found that
convolution normalization factors do not benefit the predictive power in all
the datasets tested. This was corroborated in both global and node-level
outputs. Additionally, we analyzed the influence of both adding spatial
features and working with 3D graphs, finding that 2D molecular graphs are
sufficient when complemented with appropriately chosen 3D descriptors. This
approach not only preserves predictive performance but also reduces
computational cost by over 50%, making it particularly advantageous for
high-throughput screening campaigns.

</details>


### [169] [Robustifying Diffusion-Denoised Smoothing Against Covariate Shift](https://arxiv.org/abs/2509.10913)
*Ali Hedayatnia,Mostafa Tavassolipour,Babak Nadjar Araabi,Abdol-Hossein Vahabie*

Main category: cs.LG

TL;DR: 提出新的对抗目标函数解决扩散去噪平滑中协变量偏移问题，提升认证准确率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散去噪平滑方法因去噪扩散模型对添加噪声的错误估计引入协变量偏移，降低平滑分类器性能。

Method: 提出针对去噪扩散模型添加噪声的对抗目标函数，训练基础分类器使其对去噪器引入的协变量偏移具有鲁棒性。

Result: 在MNIST、CIFAR - 10和ImageNet三个标准分类基准上显著提高认证准确率，达到l2对抗扰动下的新的最优性能。

Conclusion: 所提方法能有效解决协变量偏移问题，提升模型鲁棒性和性能。

Abstract: Randomized smoothing is a well-established method for achieving certified
robustness against l2-adversarial perturbations. By incorporating a denoiser
before the base classifier, pretrained classifiers can be seamlessly integrated
into randomized smoothing without significant performance degradation. Among
existing methods, Diffusion Denoised Smoothing - where a pretrained denoising
diffusion model serves as the denoiser - has produced state-of-the-art results.
However, we show that employing a denoising diffusion model introduces a
covariate shift via misestimation of the added noise, ultimately degrading the
smoothed classifier's performance. To address this issue, we propose a novel
adversarial objective function focused on the added noise of the denoising
diffusion model. This approach is inspired by our understanding of the origin
of the covariate shift. Our goal is to train the base classifier to ensure it
is robust against the covariate shift introduced by the denoiser. Our method
significantly improves certified accuracy across three standard classification
benchmarks - MNIST, CIFAR-10, and ImageNet - achieving new state-of-the-art
performance in l2-adversarial perturbations. Our implementation is publicly
available at
https://github.com/ahedayat/Robustifying-DDS-Against-Covariate-Shift

</details>


### [170] [ToMA: Token Merge with Attention for Image Generation with Diffusion Models](https://arxiv.org/abs/2509.10918)
*Wenbo Lu,Shaoyi Zheng,Yuxuan Xia,Shengjie Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diffusion models excel in high-fidelity image generation but face scalability
limits due to transformers' quadratic attention complexity. Plug-and-play token
reduction methods like ToMeSD and ToFu reduce FLOPs by merging redundant tokens
in generated images but rely on GPU-inefficient operations (e.g., sorting,
scattered writes), introducing overheads that negate theoretical speedups when
paired with optimized attention implementations (e.g., FlashAttention). To
bridge this gap, we propose Token Merge with Attention (ToMA), an off-the-shelf
method that redesigns token reduction for GPU-aligned efficiency, with three
key contributions: 1) a reformulation of token merge as a submodular
optimization problem to select diverse tokens; 2) merge/unmerge as an
attention-like linear transformation via GPU-friendly matrix operations; and 3)
exploiting latent locality and sequential redundancy (pattern reuse) to
minimize overhead. ToMA reduces SDXL/Flux generation latency by 24%/23%,
respectively (with DINO $\Delta < 0.07$), outperforming prior methods. This
work bridges the gap between theoretical and practical efficiency for
transformers in diffusion.

</details>


### [171] [Clarifying Model Transparency: Interpretability versus Explainability in Deep Learning with MNIST and IMDB Examples](https://arxiv.org/abs/2509.10929)
*Mitali Raj*

Main category: cs.LG

TL;DR: 文章对比深度学习中可解释性和可理解性，阐述差异，强调区分二者对可靠AI的重要性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的黑箱问题阻碍其在高信任领域应用，研究可解释性和可理解性。

Method: 对比二者定义、目标、方法和困难，通过MNIST和IMDB任务实例说明。

Result: 可理解性侧重模型整体机制的人类理解，可解释性侧重单个预测的事后解释，局部解释不能使模型全局透明。

Conclusion: 明确二者差异对发展可靠AI至关重要。

Abstract: The impressive capabilities of deep learning models are often counterbalanced
by their inherent opacity, commonly termed the "black box" problem, which
impedes their widespread acceptance in high-trust domains. In response, the
intersecting disciplines of interpretability and explainability, collectively
falling under the Explainable AI (XAI) umbrella, have become focal points of
research. Although these terms are frequently used as synonyms, they carry
distinct conceptual weights. This document offers a comparative exploration of
interpretability and explainability within the deep learning paradigm,
carefully outlining their respective definitions, objectives, prevalent
methodologies, and inherent difficulties. Through illustrative examinations of
the MNIST digit classification task and IMDB sentiment analysis, we
substantiate a key argument: interpretability generally pertains to a model's
inherent capacity for human comprehension of its operational mechanisms (global
understanding), whereas explainability is more commonly associated with
post-hoc techniques designed to illuminate the basis for a model's individual
predictions or behaviors (local explanations). For example, feature attribution
methods can reveal why a specific MNIST image is recognized as a '7', and
word-level importance can clarify an IMDB sentiment outcome. However, these
local insights do not render the complex underlying model globally transparent.
A clear grasp of this differentiation, as demonstrated by these standard
datasets, is vital for fostering dependable and sound artificial intelligence.

</details>


### [172] [The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models](https://arxiv.org/abs/2509.10970)
*Joshua Au Yeung,Jacopo Dalmasso,Luca Foschini,Richard JB Dobson,Zeljko Kraljevic*

Main category: cs.LG

TL;DR: 研究提出psychosis - bench评估大语言模型致精神病性，发现各模型均有致精神病风险，强调需重新思考训练方式及多方协作。


<details>
  <summary>Details</summary>
Motivation: 新兴的“AI精神病”报告增多，大语言模型可能强化脆弱用户的妄想信念，需评估其致精神病性。

Method: 引入psychosis - bench基准，模拟妄想主题和潜在危害的对话场景，评估八个大语言模型在妄想确认、危害促成和安全干预方面的表现。

Result: 所有模型都有致精神病潜力，倾向于延续妄想、常促成有害请求，安全干预少，隐式场景中表现更差，模型表现差异大。

Conclusion: 大语言模型致精神病性是可量化风险，需重新思考训练方式，这是公共卫生要务，需多方协作。

Abstract: Background: Emerging reports of "AI psychosis" are on the rise, where
user-LLM interactions may exacerbate or induce psychosis or adverse
psychological symptoms. The sycophantic and agreeable nature of LLMs can
beneficial, it can become a vector for harm by reinforcing delusional beliefs
in vulnerable users.
  Methods: We introduce psychosis-bench, a novel benchmark designed to
systematically evaluate the psychogenicity of LLMs comprimising 16 structured,
12-turn conversational scenarios simulating the progression of delusional
themes(Erotic Delusions, Grandiose/Messianic Delusions, Referential Delusions)
and potential harms. We evaluated eight prominent LLMs for Delusion
Confirmation (DCS), Harm Enablement (HES), and Safety Intervention(SIS) across
explicit and implicit conversational contexts.
  Findings: Across 1,536 simulated conversation turns, all LLMs demonstrated
psychogenic potential, showing a strong tendency to perpetuate rather than
challenge delusions (mean DCS of 0.91 $\pm$0.88). Models frequently enabled
harmful user requests (mean HES of 0.69 $\pm$0.84) and offered safety
interventions in only roughly a third of applicable turns (mean SIS of 0.37
$\pm$0.48). 51 / 128 (39.8%) of scenarios had no safety interventions offered.
Performance was significantly worse in implicit scenarios, models were more
likely to confirm delusions and enable harm while offering fewer interventions
(p < .001). A strong correlation was found between DCS and HES (rs = .77).
Model performance varied widely, indicating that safety is not an emergent
property of scale alone.
  Conclusion: This study establishes LLM psychogenicity as a quantifiable risk
and underscores the urgent need for re-thinking how we train LLMs. We frame
this issue not merely as a technical challenge but as a public health
imperative requiring collaboration between developers, policymakers, and
healthcare professionals.

</details>


### [173] [PHLoRA: data-free Post-hoc Low-Rank Adapter extraction from full-rank checkpoint](https://arxiv.org/abs/2509.10971)
*Bhoomit Vasani,Jack FitzGerald,Anjie Fang,Sushmit Vaish*

Main category: cs.LG

TL;DR: 介绍PHLoRA方法，可从全秩微调模型提取低秩适配适配器，实验证明其有效，为全秩检查点适配提供实用路径。


<details>
  <summary>Details</summary>
Motivation: 找到一种无需训练数据或梯度，从全秩微调模型提取低秩适配适配器的方法，实现可扩展推理并降低成本。

Method: 计算基础模型和微调模型权重差异的低秩分解，重建适配器模块，可在推理时合并或动态路由。

Result: 提取的适配器保留全权重增量的高能量，可安全剪枝，重新合并时下游任务性能下降可忽略不计。

Conclusion: PHLoRA为使所有现有全秩检查点适配提供了实用途径，让所有模型的可扩展推理更普及。

Abstract: We introduce PHLoRA (Pronounced "flora"). (Post-hoc LoRA), a simple yet
powerful method to extract low-rank adaptation adapters from full-rank
fine-tuned models without requiring access to training data or gradients. By
computing the low-rank decomposition of weight differences between a base model
and its fine-tuned counterpart, our method reconstructs adapter modules that
can be merged or dynamically routed at inference time via S-LoRA, or served in
scalable, industry settings using platforms like NVIDIA NIM. This approach
amortizes latency overhead across requests and yields substantial cost savings.
Unlike prior work that trains each adapter explicitly, our approach decouples
fine-tuning from adapter generation, allowing adapter extraction from existing
full-rank models or third-party checkpoints. Experiments on text, image, and
video benchmarks using the Amazon Nova model family demonstrate that extracted
adapters preserve high energy from the full weight delta, can be pruned safely,
and yield negligible degradation in downstream task performance when re-merged.
Overall, PHLoRA provides a practical path for making all existing full-rank
checkpoints adapter-ready, democratizing scalable inference for all models.

</details>


### [174] [Decoupling Search and Learning in Neural Net Training](https://arxiv.org/abs/2509.10973)
*Akshay Vegesna,Samip Dahal*

Main category: cs.LG

TL;DR: 提出两阶段训练框架克服梯度下降探索局限，在表征空间搜索多样解，在参数空间回归学习，模型性能接近SGD且有差异。


<details>
  <summary>Details</summary>
Motivation: 梯度下降通常收敛到单一最小值，直接在高维参数空间搜索多样最小值难以处理，需解决探索局限问题。

Method: 提出两阶段训练框架，先在表征空间通过进化搜索找多样解，再在参数空间回归学习。

Result: 找到适应度和多样性随计算量增加的表征解，训练模型性能接近SGD，性能随搜索计算量提升到饱和，模型与梯度下降训练的有差异。

Conclusion: 未来训练算法可通过分离表征空间搜索和参数空间梯度学习克服梯度下降探索局限。

Abstract: Gradient descent typically converges to a single minimum of the training loss
without mechanisms to explore alternative minima that may generalize better.
Searching for diverse minima directly in high-dimensional parameter space is
generally intractable. To address this, we propose a framework that performs
training in two distinct phases: search in a tractable representation space
(the space of intermediate activations) to find diverse representational
solutions, and gradient-based learning in parameter space by regressing to
those searched representations. Through evolutionary search, we discover
representational solutions whose fitness and diversity scale with
compute--larger populations and more generations produce better and more varied
solutions. These representations prove to be learnable: networks trained by
regressing to searched representations approach SGD's performance on MNIST,
CIFAR-10, and CIFAR-100. Performance improves with search compute up to
saturation. The resulting models differ qualitatively from networks trained
with gradient descent, following different representational trajectories during
training. This work demonstrates how future training algorithms could overcome
gradient descent's exploratory limitations by decoupling search in
representation space from efficient gradient-based learning in parameter space.

</details>


### [175] [California Wildfire Inventory (CAWFI): An Extensive Dataset for Predictive Techniques based on Artificial Intelligence](https://arxiv.org/abs/2509.11015)
*Rohan Tan Bhowmik,Youn Soo Jung,Juan Aguilera,Mary Prunicki,Kari Nadeau*

Main category: cs.LG

TL;DR: 论文介绍加州野火清单数据库CAWFI，含超3700万个数据点，可用于训练野火预测模型，已在训练时空AI模型中取得成功，有望推动野火预测研究。


<details>
  <summary>Details</summary>
Motivation: 气候变化致野火影响增大，现有野火管理技术多为事后检测，开发高精度预测技术需大量数据集。

Method: 构建加州野火清单数据库CAWFI，收集2012 - 2018年每日历史野火数据和2012 - 2022年指标数据。

Result: 用CAWFI训练时空人工智能模型，预测大于30万英亩的未来野火准确率达85.7%。

Conclusion: CAWFI能推动野火预测研究和解决方案，为其他地区野火数据库树立先例。

Abstract: Due to climate change and the disruption of ecosystems worldwide, wildfires
are increasingly impacting environment, infrastructure, and human lives
globally. Additionally, an exacerbating climate crisis means that these losses
would continue to grow if preventative measures are not implemented. Though
recent advancements in artificial intelligence enable wildfire management
techniques, most deployed solutions focus on detecting wildfires after
ignition. The development of predictive techniques with high accuracy requires
extensive datasets to train machine learning models. This paper presents the
California Wildfire Inventory (CAWFI), a wildfire database of over 37 million
data points for building and training wildfire prediction solutions, thereby
potentially preventing megafires and flash fires by addressing them before they
spark. The dataset compiles daily historical California wildfire data from 2012
to 2018 and indicator data from 2012 to 2022. The indicator data consists of
leading indicators (meteorological data correlating to wildfire-prone
conditions), trailing indicators (environmental data correlating to prior and
early wildfire activity), and geological indicators (vegetation and elevation
data dictating wildfire risk and spread patterns). CAWFI has already
demonstrated success when used to train a spatio-temporal artificial
intelligence model, predicting 85.7% of future wildfires larger than 300,000
acres when trained on 2012-2017 indicator data. This dataset is intended to
enable wildfire prediction research and solutions as well as set a precedent
for future wildfire databases in other regions.

</details>


### [176] [FragmentGPT: A Unified GPT Model for Fragment Growing, Linking, and Merging in Molecular Design](https://arxiv.org/abs/2509.11044)
*Xuefeng Liu,Songhao Jiang,Qinan Huang,Tinson Xu,Ian Foster,Mengdi Wang,Hening Lin,Jinbo Xu,Rick Stevens*

Main category: cs.LG

TL;DR: 提出FragmentGPT解决FBDD中连接子设计和结构冗余问题，实验证明其能生成高质量分子用于药物发现。


<details>
  <summary>Details</summary>
Motivation: FBDD中设计有效连接子以及处理片段结构冗余问题具有挑战性，需要统一框架解决。

Method: 引入FragmentGPT，包含化学感知、基于能量的键裂解预训练策略和RAE算法，可实现片段生长、连接、合并及多目标优化。

Result: 在真实癌症数据集上的实验和消融研究表明，能生成化学上有效的高质量分子。

Conclusion: FragmentGPT可促进可控、目标驱动的分子组装，适用于下游药物发现任务。

Abstract: Fragment-Based Drug Discovery (FBDD) is a popular approach in early drug
development, but designing effective linkers to combine disconnected molecular
fragments into chemically and pharmacologically viable candidates remains
challenging. Further complexity arises when fragments contain structural
redundancies, like duplicate rings, which cannot be addressed by simply adding
or removing atoms or bonds. To address these challenges in a unified framework,
we introduce FragmentGPT, which integrates two core components: (1) a novel
chemically-aware, energy-based bond cleavage pre-training strategy that equips
the GPT-based model with fragment growing, linking, and merging capabilities,
and (2) a novel Reward Ranked Alignment with Expert Exploration (RAE) algorithm
that combines expert imitation learning for diversity enhancement, data
selection and augmentation for Pareto and composite score optimality, and
Supervised Fine-Tuning (SFT) to align the learner policy with multi-objective
goals. Conditioned on fragment pairs, FragmentGPT generates linkers that
connect diverse molecular subunits while simultaneously optimizing for multiple
pharmaceutical goals. It also learns to resolve structural redundancies-such as
duplicated fragments-through intelligent merging, enabling the synthesis of
optimized molecules. FragmentGPT facilitates controlled, goal-driven molecular
assembly. Experiments and ablation studies on real-world cancer datasets
demonstrate its ability to generate chemically valid, high-quality molecules
tailored for downstream drug discovery tasks.

</details>


### [177] [Data-Efficient Ensemble Weather Forecasting with Diffusion Models](https://arxiv.org/abs/2509.11047)
*Kevin Valencia,Ziyang Liu,Justin Cui*

Main category: cs.LG

TL;DR: 本文探讨策展数据选择对自回归扩散模型的影响，发现简单时间分层采样法用20%训练数据达类似或更好效果，证明数据高效扩散训练可行性。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法在集合天气预报有前景，但自回归扩散模型计算成本高，且气候科学数据有限、昂贵或难处理，需探索数据选择对模型的影响。

Method: 评估几种数据采样策略，采用简单时间分层采样法。

Result: 简单时间分层采样法用20%训练数据达到类似或优于全数据训练的性能，在某些指标上表现更好，其他指标稍差。

Conclusion: 证明数据高效扩散训练的可行性，为超越随机或纯时间采样的自适应或模型感知采样方法研究提供动力。

Abstract: Although numerical weather forecasting methods have dominated the field,
recent advances in deep learning methods, such as diffusion models, have shown
promise in ensemble weather forecasting. However, such models are typically
autoregressive and are thus computationally expensive. This is a challenge in
climate science, where data can be limited, costly, or difficult to work with.
In this work, we explore the impact of curated data selection on these
autoregressive diffusion models. We evaluate several data sampling strategies
and show that a simple time stratified sampling approach achieves performance
similar to or better than full-data training. Notably, it outperforms the
full-data model on certain metrics and performs only slightly worse on others
while using only 20% of the training data. Our results demonstrate the
feasibility of data-efficient diffusion training, especially for weather
forecasting, and motivates future work on adaptive or model-aware sampling
methods that go beyond random or purely temporal sampling.

</details>


### [178] [Machine Learning Framework for Audio-Based Equipment Condition Monitoring: A Comparative Study of Classification Algorithms](https://arxiv.org/abs/2509.11075)
*Srijesh Pillai,Yodhin Agarwal,Zaheeruddin Ahmed*

Main category: cs.LG

TL;DR: 本文提出综合框架评估机器学习模型用于音频设备状态监测，验证后集成方法表现优，还提供基准协议和选择指南。


<details>
  <summary>Details</summary>
Motivation: 音频设备状态监测缺乏算法选择的标准化方法，阻碍可重复性研究。

Method: 引入综合框架，利用时、频和时频域的127个特征集，在合成和真实数据集上验证。

Result: 集成方法性能优越，准确率94.2%，F1分数0.942，比单个算法高8 - 15%。

Conclusion: 提供了经过验证的基准测试协议和在工业环境中选择可靠监测解决方案的实用指南。

Abstract: Audio-based equipment condition monitoring suffers from a lack of
standardized methodologies for algorithm selection, hindering reproducible
research. This paper addresses this gap by introducing a comprehensive
framework for the systematic and statistically rigorous evaluation of machine
learning models. Leveraging a rich 127-feature set across time, frequency, and
time-frequency domains, our methodology is validated on both synthetic and
real-world datasets. Results demonstrate that an ensemble method achieves
superior performance (94.2% accuracy, 0.942 F1-score), with statistical testing
confirming its significant outperformance of individual algorithms by 8-15%.
Ultimately, this work provides a validated benchmarking protocol and practical
guidelines for selecting robust monitoring solutions in industrial settings.

</details>


### [179] [DemandLens: Enhancing Forecast Accuracy Through Product-Specific Hyperparameter Optimization](https://arxiv.org/abs/2509.11085)
*Srijesh Pillai,M. I. Jawid Nazir*

Main category: cs.LG

TL;DR: DemandLens提出基于Prophet的床垫行业预测模型，结合COVID - 19指标和SKU特定超参数优化，助制造商管理供应链。


<details>
  <summary>Details</summary>
Motivation: 床垫电商行业依赖第三方合同制造，需准确销售预测以帮助制造商智能管理原材料、供应链和库存，避免瓶颈。

Method: 使用基于Prophet的预测模型，结合COVID - 19指标和SKU特定超参数优化。

Result: 模型通过SKU特定超参数优化展现出强大预测能力。

Conclusion: 该模型为合同制造商和床垫品牌提供可靠工具，可优化供应链运作。

Abstract: DemandLens demonstrates an innovative Prophet based forecasting model for the
mattress-in-a-box industry, incorporating COVID-19 metrics and SKU-specific
hyperparameter optimization. This industry has seen significant growth of
E-commerce players in the recent years, wherein the business model majorly
relies on outsourcing Mattress manufacturing and related logistics and supply
chain operations, focusing on marketing the product and driving conversions
through Direct-to-Consumer sales channels. Now, within the United States, there
are a limited number of Mattress contract manufacturers available, and hence,
it is important that they manage their raw materials, supply chain, and,
inventory intelligently, to be able to cater maximum Mattress brands. Our
approach addresses the critical need for accurate Sales Forecasting in an
industry that is heavily dependent on third-party Contract Manufacturing. This,
in turn, helps the contract manufacturers to be prepared, hence, avoiding
bottleneck scenarios, and aiding them to source raw materials at optimal rates.
The model demonstrates strong predictive capabilities through SKU-specific
Hyperparameter optimization, offering the Contract Manufacturers and Mattress
brands a reliable tool to streamline supply chain operations.

</details>


### [180] [GCN-TULHOR: Trajectory-User Linking Leveraging GCNs and Higher-Order Spatial Representations](https://arxiv.org/abs/2509.11095)
*Khoa Tran,Pranav Gupta,Manos Papagelis*

Main category: cs.LG

TL;DR: 本文提出GCN - TULHOR方法解决轨迹 - 用户关联问题，在六个真实数据集上表现优于经典基线、RNN和Transformer模型及TULHOR方法。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹 - 用户关联方法在处理稀疏数据、不完整路线和复杂空间依赖建模方面存在困难。

Method: 使用六边形分割将原始位置数据转换为高阶移动流表示，结合图卷积网络（GCN），将稀疏签到和连续GPS轨迹数据转换为统一高阶流表示。

Result: 在六个真实数据集上，在准确率、精确率、召回率和F1分数上均有提升，准确率和F1有1 - 8%的相对增益，敏感性分析确定了最优设置。

Conclusion: 结合基于图的空间学习和序列建模对轨迹 - 用户关联有价值，为该问题提供了强大且可扩展的解决方案。

Abstract: Trajectory-user linking (TUL) aims to associate anonymized trajectories with
the users who generated them, which is crucial for personalized
recommendations, privacy-preserving analytics, and secure location-based
services. Existing methods struggle with sparse data, incomplete routes, and
limited modeling of complex spatial dependencies, often relying on low-level
check-in data or ignoring spatial patterns. In this paper, we introduced
GCN-TULHOR, a method that transforms raw location data into higher-order
mobility flow representations using hexagonal tessellation, reducing data
sparsity and capturing richer spatial semantics, and integrating Graph
Convolutional Networks (GCNs). Our approach converts both sparse check-in and
continuous GPS trajectory data into unified higher-order flow representations,
mitigating sparsity while capturing deeper semantic information. The GCN layer
explicitly models complex spatial relationships and non-local dependencies
without requiring side information such as timestamps or points of interest.
Experiments on six real-world datasets show consistent improvements over
classical baselines, RNN- and Transformer-based models, and the TULHOR method
in accuracy, precision, recall, and F1-score. GCN-TULHOR achieves 1-8% relative
gains in accuracy and F1. Sensitivity analysis identifies an optimal setup with
a single GCN layer and 512-dimensional embeddings. The integration of GCNs
enhances spatial learning and improves generalizability across mobility data.
This work highlights the value of combining graph-based spatial learning with
sequential modeling, offering a robust and scalable solution for TUL with
applications in recommendations, urban planning, and security.

</details>


### [181] [BIGNet: Pretrained Graph Neural Network for Embedding Semantic, Spatial, and Topological Data in BIM Models](https://arxiv.org/abs/2509.11104)
*Jin Han,Xin-Zheng Lu,Jia-Rui Lin*

Main category: cs.LG

TL;DR: 本文开发大型图神经网络BIGNet学习和复用BIM模型设计特征，在设计检查任务中评估，结果显示其在学习和迁移特征上有效。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在土木工程中多关注文本和视觉数据，忽略BIM模型丰富特征，需开发新模型学习和复用这些特征。

Method: 引入可扩展图表示编码BIM组件特征并创建数据集，改进GraphMAE2提出BIGNet并预训练，在设计检查任务中评估。

Result: 同质图表示学习设计特征更优，考虑30cm半径局部空间关系增强性能，基于GAT特征提取的BIGNet迁移学习效果最佳，平均F1得分提升72.7%。

Conclusion: BIGNet在学习和迁移BIM设计特征上有效，有助于未来设计和生命周期管理自动化应用。

Abstract: Large Foundation Models (LFMs) have demonstrated significant advantages in
civil engineering, but they primarily focus on textual and visual data,
overlooking the rich semantic, spatial, and topological features in BIM
(Building Information Modelling) models. Therefore, this study develops the
first large-scale graph neural network (GNN), BIGNet, to learn, and reuse
multidimensional design features embedded in BIM models. Firstly, a scalable
graph representation is introduced to encode the "semantic-spatial-topological"
features of BIM components, and a dataset with nearly 1 million nodes and 3.5
million edges is created. Subsequently, BIGNet is proposed by introducing a new
message-passing mechanism to GraphMAE2 and further pretrained with a node
masking strategy. Finally, BIGNet is evaluated in various transfer learning
tasks for BIM-based design checking. Results show that: 1) homogeneous graph
representation outperforms heterogeneous graph in learning design features, 2)
considering local spatial relationships in a 30 cm radius enhances performance,
and 3) BIGNet with GAT (Graph Attention Network)-based feature extraction
achieves the best transfer learning results. This innovation leads to a 72.7%
improvement in Average F1-score over non-pretrained models, demonstrating its
effectiveness in learning and transferring BIM design features and facilitating
their automated application in future design and lifecycle management.

</details>


### [182] [Agentic Username Suggestion and Multimodal Gender Detection in Online Platforms: Introducing the PNGT-26K Dataset](https://arxiv.org/abs/2509.11136)
*Farbod Bijary,Mohsen Ebadpour,Amirhosein Tajbakhsh*

Main category: cs.LG

TL;DR: 论文针对波斯姓名在自然语言处理中的挑战，推出PNGT - 26K数据集及Open Gender Detection和Nominalist两个框架，且均公开于Github。


<details>
  <summary>Details</summary>
Motivation: 波斯姓名因音译不一致和文化特定命名模式，在自然语言处理应用（如性别检测和数字身份创建）中面临挑战，现有工具性能差且缺乏综合数据集。

Method: 引入包含约26000个元组的PNGT - 26K数据集，介绍Open Gender Detection和Nominalist两个框架。

Result: 推出PNGT - 26K数据集、Open Gender Detection和Nominalist框架，并公开于Github。

Conclusion: 所提出的数据集和框架可用于解决波斯姓名在自然语言处理中的问题，且具有可用性和可集成性。

Abstract: Persian names present unique challenges for natural language processing
applications, particularly in gender detection and digital identity creation,
due to transliteration inconsistencies and cultural-specific naming patterns.
Existing tools exhibit significant performance degradation on Persian names,
while the scarcity of comprehensive datasets further compounds these
limitations. To address these challenges, the present research introduces
PNGT-26K, a comprehensive dataset of Persian names, their commonly associated
gender, and their English transliteration, consisting of approximately 26,000
tuples. As a demonstration of how this resource can be utilized, we also
introduce two frameworks, namely Open Gender Detection and Nominalist. Open
Gender Detection is a production-grade, ready-to-use framework for using
existing data from a user, such as profile photo and name, to give a
probabilistic guess about the person's gender. Nominalist, the second framework
introduced by this paper, utilizes agentic AI to help users choose a username
for their social media accounts on any platform. It can be easily integrated
into any website to provide a better user experience. The PNGT-26K dataset,
Nominalist and Open Gender Detection frameworks are publicly available on
Github.

</details>


### [183] [Feature Space Topology Control via Hopkins Loss](https://arxiv.org/abs/2509.11154)
*Einari Vaaras,Manu Airaksinen*

Main category: cs.LG

TL;DR: 本文提出Hopkins损失函数，利用Hopkins统计量修改特征空间拓扑，在分类和降维场景实验表明其对分类性能影响小且能修改特征拓扑。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑相关方法旨在保留输入特征拓扑，而修改特征空间拓扑在机器学习应用中有好处，故提出新损失函数来实现。

Method: 引入Hopkins损失函数，利用Hopkins统计量来强制实现所需的特征空间拓扑；在语音、文本和图像数据的分类和降维两个场景进行评估。

Result: 将Hopkins损失函数集成到分类或降维中，对分类性能影响小，且能修改特征拓扑。

Conclusion: Hopkins损失函数能在对分类性能影响较小的情况下，实现特征空间拓扑的修改。

Abstract: Feature space topology refers to the organization of samples within the
feature space. Modifying this topology can be beneficial in machine learning
applications, including dimensionality reduction, generative modeling, transfer
learning, and robustness to adversarial attacks. This paper introduces a novel
loss function, Hopkins loss, which leverages the Hopkins statistic to enforce a
desired feature space topology, which is in contrast to existing
topology-related methods that aim to preserve input feature topology. We
evaluate the effectiveness of Hopkins loss on speech, text, and image data in
two scenarios: classification and dimensionality reduction using nonlinear
bottleneck autoencoders. Our experiments show that integrating Hopkins loss
into classification or dimensionality reduction has only a small impact on
classification performance while providing the benefit of modifying feature
topology.

</details>


### [184] [AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient Inference in LLMs](https://arxiv.org/abs/2509.11155)
*Santhosh G S,Saurav Prakash,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出AQUA近似策略降低注意力机制成本，理论分析并实证评估，能平衡效率与准确性，助力大模型推理。


<details>
  <summary>Details</summary>
Motivation: 注意力机制的二次复杂度阻碍大语言模型处理长上下文，成为计算和内存瓶颈。

Method: 分离线和在线两步，离线用SVD在校准数据集计算通用投影矩阵，在线投影向量并按查询向量幅度选稀疏维度。

Result: 在Llama - 3.1 - 8B等模型上实现注意力点积计算25%的减少，对性能影响不显著，还能加速现有方法、减少KV缓存内存。

Conclusion: AQUA提供可控手段平衡效率和准确性，是让大规模大语言模型推理更可行和可持续的实用工具。

Abstract: The quadratic complexity of the attention mechanism remains a fundamental
barrier to scaling Large Language Models (LLMs) to longer contexts, creating a
critical bottleneck in both computation and memory. To address this, we
introduce AQUA (Attention via QUery mAgnitudes) a novel and versatile
approximation strategy that significantly reduces the cost of attention with a
graceful performance trade-off. Our method operates in two phases: an efficient
offline step where we compute a universal, language agnostic projection matrix
via SVD on a calibration dataset, and an online inference step where we project
query and key vectors and dynamically select a sparse subset of dimensions
based on the query's magnitude. We provide a formal theoretical analysis of
AQUA, establishing the break-even point at which it becomes more
computationally efficient than standard attention. Our empirical evaluations on
state-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in
the attention dot-product computation can be achieved with a statistically
insignificant impact on performance across a wide range of benchmarks. We
further showcase the versatility of AQUA by demonstrating its ability to
synergistically accelerate existing token eviction methods like H2O and to
directly reduce KV-cache memory size. By offering a controllable knob to
balance efficiency and accuracy, AQUA provides a practical and powerful tool
for making large-scale LLM inference more accessible and sustainable.

</details>


### [185] [Stabilizing Data-Free Model Extraction](https://arxiv.org/abs/2509.11159)
*Dat-Thinh Nguyen,Kim-Hung Le,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: 提出MetaDFME数据无模型提取方法，用元学习减少分布偏移，缓解替代模型精度振荡，实验显示优于现有方法且精度更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有数据无模型提取方法存在替代模型精度振荡问题，因攻击中生成数据分布不断变化，且无目标模型分布内数据难以确定最优替代模型。

Method: 提出MetaDFME方法，在生成器训练中使用元学习，训练生成器迭代捕获合成数据的元表示，以适应生成数据减少分布偏移影响。

Result: 在MNIST、SVHN、CIFAR - 10和CIFAR - 100等数据集实验中，MetaDFME优于当前最先进的数据无模型提取方法，攻击时代替模型精度更稳定。

Conclusion: MetaDFME方法能有效减少分布偏移，缓解替代模型精度振荡，提升模型提取效果。

Abstract: Model extraction is a severe threat to Machine Learning-as-a-Service systems,
especially through data-free approaches, where dishonest users can replicate
the functionality of a black-box target model without access to realistic data.
Despite recent advancements, existing data-free model extraction methods suffer
from the oscillating accuracy of the substitute model. This oscillation, which
could be attributed to the constant shift in the generated data distribution
during the attack, makes the attack impractical since the optimal substitute
model cannot be determined without access to the target model's in-distribution
data. Hence, we propose MetaDFME, a novel data-free model extraction method
that employs meta-learning in the generator training to reduce the distribution
shift, aiming to mitigate the substitute model's accuracy oscillation. In
detail, we train our generator to iteratively capture the meta-representations
of the synthetic data during the attack. These meta-representations can be
adapted with a few steps to produce data that facilitates the substitute model
to learn from the target model while reducing the effect of distribution
shifts. Our experiments on popular baseline image datasets, MNIST, SVHN,
CIFAR-10, and CIFAR-100, demonstrate that MetaDFME outperforms the current
state-of-the-art data-free model extraction method while exhibiting a more
stable substitute model's accuracy during the attack.

</details>


### [186] [GK-SMOTE: A Hyperparameter-free Noise-Resilient Gaussian KDE-Based Oversampling Approach](https://arxiv.org/abs/2509.11163)
*Mahabubur Rahman Miraj,Hongyu Huang,Ting Yang,Jinxue Zhao,Nankun Mu,Xinyu Lei*

Main category: cs.LG

TL;DR: 提出GK - SMOTE解决不平衡分类问题，实验显示其优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统过采样技术如SMOTE难以处理标签噪声和复杂数据分布，导致分类精度降低。

Method: 提出基于高斯核密度估计的无超参数、抗噪声的GK - SMOTE，通过在高密度少数区域生成合成样本提高类可分性。

Result: 在多种二元分类数据集上的实验表明，GK - SMOTE在关键评估指标上优于现有技术。

Conclusion: GK - SMOTE为不平衡分类任务，尤其是噪声数据环境提供了强大、高效的解决方案，适用于实际应用。

Abstract: Imbalanced classification is a significant challenge in machine learning,
especially in critical applications like medical diagnosis, fraud detection,
and cybersecurity. Traditional oversampling techniques, such as SMOTE, often
fail to handle label noise and complex data distributions, leading to reduced
classification accuracy. In this paper, we propose GK-SMOTE, a
hyperparameter-free, noise-resilient extension of SMOTE, built on Gaussian
Kernel Density Estimation (KDE). GK-SMOTE enhances class separability by
generating synthetic samples in high-density minority regions, while
effectively avoiding noisy or ambiguous areas. This self-adaptive approach uses
Gaussian KDE to differentiate between safe and noisy regions, ensuring more
accurate sample generation without requiring extensive parameter tuning. Our
extensive experiments on diverse binary classification datasets demonstrate
that GK-SMOTE outperforms existing state-of-the-art oversampling techniques
across key evaluation metrics, including MCC, Balanced Accuracy, and AUPRC. The
proposed method offers a robust, efficient solution for imbalanced
classification tasks, especially in noisy data environments, making it an
attractive choice for real-world applications.

</details>


### [187] [Harnessing Optimization Dynamics for Curvature-Informed Model Merging](https://arxiv.org/abs/2509.11167)
*Pouria Mahdavinia,Hamed Mahdavi,Niloofar Mireshghallah,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: 研究监督微调阶段的模型合并，提出OTA和FFG方法，提升合并模型质量，开源代码等。


<details>
  <summary>Details</summary>
Motivation: 在监督微调阶段，需将多个基于能力的检查点合并为单一模型，解决合并中的干扰问题。

Method: 引入OTA Merging利用优化器二阶矩统计量重新加权参数编辑，提出FFG稀疏化冲突或低重要性编辑，还开发二阶矩的轻内存压缩。

Result: OTA+FFG提升合并模型质量，减少负迁移，在不同稀疏水平下保持鲁棒性。

Conclusion: FFG对减少任务干扰至关重要，压缩二阶矩能保留完整公式的收益，检查点间存在大量曲率重叠解释了线性合并有效的原因。

Abstract: Model merging is an effective post-training strategy for composing
capabilities in large language models without joint retraining. We study this
in the supervised fine-tuning (SFT) stage, where multiple capability-based SFT
checkpoints -- spanning math, code, precise instruction following, general
instruction following, and knowledge recall -- must be consolidated into a
single model. We introduce Optimization Trajectory Aware (OTA) Merging, a
curvature-aware aggregation that leverages optimizer second-moment statistics
as a diagonal curvature proxy to reweight parameter edits and mitigate
interference. Complementing OTA, we propose Fast Fisher Grafting (FFG), a
curvature-driven task-localization step that sparsifies conflicting or
low-importance edits. FFG induces extremely low-rank masks concentrated in
early attention query/key projections and token embeddings, exploiting shared
curvature across capabilities. We further develop a memory-light compression of
the second moments that preserves OTA's effect. Across diverse capability-based
SFT checkpoints, OTA+FFG improves merged-model quality over strong weight-space
baselines, reduces negative transfer, and remains robust across sparsity
levels. Analyses reveal substantial curvature overlap between checkpoints,
offering a novel lens on why simple linear merging can be effective in
practice. Ablations confirm that FFG is critical for reducing task interference
and that the compressed second moments retain the gains of the full
formulation. To facilitate reproducibility, we open-source all code, training
and evaluation scripts, visualization artifacts, and capability-specific SFT
checkpoints at https://github.com/pmahdavi/ota-merge.

</details>


### [188] [Federated Recommender System with Data Valuation for E-commerce Platform](https://arxiv.org/abs/2509.11196)
*Jongwon Park,Minku Kang,Wooseok Sim,Soyoung Lee,Hogun Park*

Main category: cs.LG

TL;DR: 随着隐私问题受关注，联邦学习在机器学习中愈发重要。现有基于联邦学习的推荐系统未充分利用公开数据集，本文提出FedGDVE方法，在基准测试中性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于联邦学习的推荐系统未充分利用公开数据集，且整合全局数据存在挑战，需解决这些问题以提升推荐系统性能。

Method: 提出FedGDVE，用预训练图编码器提取全局结构特征，用本地有效预测器评估相关性，用基于强化学习的概率估计器过滤和采样全局交互。

Result: FedGDVE在联邦学习环境的公认基准测试中性能提升达34.86%。

Conclusion: FedGDVE能有效解决整合全局数据带来的问题，提升推荐系统性能。

Abstract: Federated Learning (FL) is gaining prominence in machine learning as privacy
concerns grow. This paradigm allows each client (e.g., an individual online
store) to train a recommendation model locally while sharing only model
updates, without exposing the raw interaction logs to a central server, thereby
preserving privacy in a decentralized environment. Nonetheless, most existing
FL-based recommender systems still rely solely on each client's private data,
despite the abundance of publicly available datasets that could be leveraged to
enrich local training; this potential remains largely underexplored. To this
end, we consider a realistic scenario wherein a large shopping platform
collaborates with multiple small online stores to build a global recommender
system. The platform possesses global data, such as shareable user and item
lists, while each store holds a portion of interaction data privately (or
locally). Although integrating global data can help mitigate the limitations of
sparse and biased clients' local data, it also introduces additional
challenges: simply combining all global interactions can amplify noise and
irrelevant patterns, worsening personalization and increasing computational
costs. To address these challenges, we propose FedGDVE, which selectively
augments each client's local graph with semantically aligned samples from the
global dataset. FedGDVE employs: (i) a pre-trained graph encoder to extract
global structural features, (ii) a local valid predictor to assess
client-specific relevance, (iii) a reinforcement-learning-based probability
estimator to filter and sample only the most pertinent global interactions.
FedGDVE improves performance by up to 34.86% on recognized benchmarks in FL
environments.

</details>


### [189] [TransZero: Parallel Tree Expansion in MuZero using Transformer Networks](https://arxiv.org/abs/2509.11233)
*Emil Malmsten,Wendelin Böhmer*

Main category: cs.LG

TL;DR: 提出TransZero算法去除蒙特卡罗树搜索的顺序瓶颈，实验显示比MuZero有速度提升。


<details>
  <summary>Details</summary>
Motivation: 去除蒙特卡罗树搜索（MCTS）中的顺序瓶颈，加速基于模型的强化学习。

Method: 使用基于Transformer的网络同时生成多个潜在未来状态，结合MVC评估器消除对顺序访问计数的依赖，实现子树并行扩展。

Result: 在MiniGrid和LunarLander实验中，TransZero比MuZero时钟时间最多快11倍，且保持样本效率。

Conclusion: 并行树构建可显著加速基于模型的强化学习，使复杂环境实时决策更接近实际应用。

Abstract: We present TransZero, a model-based reinforcement learning algorithm that
removes the sequential bottleneck in Monte Carlo Tree Search (MCTS). Unlike
MuZero, which constructs its search tree step by step using a recurrent
dynamics model, TransZero employs a transformer-based network to generate
multiple latent future states simultaneously. Combined with the Mean-Variance
Constrained (MVC) evaluator that eliminates dependence on inherently sequential
visitation counts, our approach enables the parallel expansion of entire
subtrees during planning. Experiments in MiniGrid and LunarLander show that
TransZero achieves up to an eleven-fold speedup in wall-clock time compared to
MuZero while maintaining sample efficiency. These results demonstrate that
parallel tree construction can substantially accelerate model-based
reinforcement learning, bringing real-time decision-making in complex
environments closer to practice. The code is publicly available on GitHub.

</details>


### [190] [Gradient Free Deep Reinforcement Learning With TabPFN](https://arxiv.org/abs/2509.11259)
*David Schiff,Ofir Lindenbaum,Yonathan Efroni*

Main category: cs.LG

TL;DR: 提出TabPFN RL无梯度深度强化学习框架，用TabPFN作Q函数近似器，在Gymnasium经典控制套件上表现良好，为无梯度强化学习开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化在现代深度强化学习算法中有超参数敏感、训练不稳定和计算成本高的问题，需新方法。

Method: 提出TabPFN RL框架，用TabPFN仅通过推理预测Q值，设计高奖励情节门应对固定上下文预算，还提出截断策略。

Result: 在Gymnasium经典控制套件上，TabPFN RL不使用梯度下降和广泛超参数调优，表现匹配或超越深度Q网络。

Conclusion: 像TabPFN这样的先验拟合网络可作为快速高效强化学习的基础，为无梯度强化学习提供新方向。

Abstract: Gradient based optimization is fundamental to most modern deep reinforcement
learning algorithms, however, it introduces significant sensitivity to
hyperparameters, unstable training dynamics, and high computational costs. We
propose TabPFN RL, a novel gradient free deep RL framework that repurposes the
meta trained transformer TabPFN as a Q function approximator. Originally
developed for tabular classification, TabPFN is a transformer pre trained on
millions of synthetic datasets to perform inference on new unseen datasets via
in context learning. Given an in context dataset of sample label pairs and new
unlabeled data, it predicts the most likely labels in a single forward pass,
without gradient updates or task specific fine tuning. We use TabPFN to predict
Q values using inference only, thereby eliminating the need for back
propagation at both training and inference. To cope with the model's fixed
context budget, we design a high reward episode gate that retains only the top
5% of trajectories. Empirical evaluations on the Gymnasium classic control
suite demonstrate that TabPFN RL matches or surpasses Deep Q Network on
CartPole v1, MountainCar v0, and Acrobot v1, without applying gradient descent
or any extensive hyperparameter tuning. We discuss the theoretical aspects of
how bootstrapped targets and non stationary visitation distributions violate
the independence assumptions encoded in TabPFN's prior, yet the model retains a
surprising generalization capacity. We further formalize the intrinsic context
size limit of in context RL algorithms and propose principled truncation
strategies that enable continual learning when the context is full. Our results
establish prior fitted networks such as TabPFN as a viable foundation for fast
and computationally efficient RL, opening new directions for gradient free RL
with large pre trained transformers.

</details>


### [191] [Protected Probabilistic Classification Library](https://arxiv.org/abs/2509.11267)
*Ivan Petej*

Main category: cs.LG

TL;DR: 本文介绍用于解决数据集偏移下概率分类器校准问题的Python包，实验结果表明该技术在多种场景有用。


<details>
  <summary>Details</summary>
Motivation: 解决数据集偏移下概率分类器的校准问题。

Method: 在二分类和多分类场景展示新方法，并与现有事后校准方法对比。

Result: 实验结果有前景。

Conclusion: 该技术对训练集和测试集数据分布变化的批量和在线学习分类问题有帮助。

Abstract: This paper introduces a new Python package specifically designed to address
calibration of probabilistic classifiers under dataset shift. The method is
demonstrated in binary and multi-class settings and its effectiveness is
measured against a number of existing post-hoc calibration methods. The
empirical results are promising and suggest that our technique can be helpful
in a variety of settings for batch and online learning classification problems
where the underlying data distribution changes between the training and test
sets.

</details>


### [192] [PINGS: Physics-Informed Neural Network for Fast Generative Sampling](https://arxiv.org/abs/2509.11284)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce PINGS (Physics-Informed Neural Network for Fast Generative
Sampling), a framework that amortizes diffusion sampling by training a
physics-informed network to approximate reverse-time probability-flow dynamics,
reducing sampling to a single forward pass (NFE = 1). As a proof of concept, we
learn a direct map from a 3D standard normal to a non-Gaussian Gaussian Mixture
Model (GMM). PINGS preserves the target's distributional structure
(multi-bandwidth kernel $MMD^2 = 1.88 \times 10^{-2}$ with small errors in
mean, covariance, skewness, and excess kurtosis) and achieves constant-time
generation: $10^4$ samples in $16.54 \pm 0.56$ millisecond on an RTX 3090,
versus 468-843 millisecond for DPM-Solver (10/20) and 960 millisecond for DDIM
(50) under matched conditions. We also sanity-check the
PINN/automatic-differentiation pipeline on a damped harmonic oscillator,
obtaining MSEs down to $\mathcal{O}(10^{-5})$. Compared to fast but iterative
ODE solvers and direct-map families (Flow, Rectified-Flow, Consistency), PINGS
frames generative sampling as a PINN-style residual problem with endpoint
anchoring, yielding a white-box, differentiable map with NFE = 1. These
proof-of-concept results position PINGS as a promising route to fast,
function-based generative sampling with potential extensions to scientific
simulation (e.g., fast calorimetry).

</details>


### [193] [Efficient Single-Step Framework for Incremental Class Learning in Neural Networks](https://arxiv.org/abs/2509.11285)
*Alejandro Dopico-Castro,Oscar Fontenla-Romero,Bertha Guijarro-Berdiñas,Amparo Alonso-Betanzos*

Main category: cs.LG

TL;DR: 提出CIFNet解决类增量学习在资源受限下的问题，实验证明其有效且高效。


<details>
  <summary>Details</summary>
Motivation: 现有类增量学习方法存在计算资源需求大、训练过程复杂的问题，在资源受限场景下难以应用。

Method: 提出CIFNet，集成预训练冻结特征提取器、压缩数据缓冲区和高效非迭代单层神经网络进行分类。

Result: 在基准数据集上实验表明，CIFNet能有效缓解灾难性遗忘，精度与现有方法相当，大幅提高训练效率和可持续性。

Conclusion: CIFNet在资源受限环境下使类增量学习更实用，尤其在有强预训练特征提取器时优势明显。

Abstract: Incremental learning remains a critical challenge in machine learning, as
models often struggle with catastrophic forgetting -the tendency to lose
previously acquired knowledge when learning new information. These challenges
are even more pronounced in resource-limited settings. Many existing Class
Incremental Learning (CIL) methods achieve high accuracy by continually
adapting their feature representations; however, they often require substantial
computational resources and complex, iterative training procedures. This work
introduces CIFNet (Class Incremental and Frugal Network), a novel CIL approach
that addresses these limitations by offering a highly efficient and sustainable
solution. CIFNet's key innovation lies in its novel integration of several
existing, yet separately explored, components: a pre-trained and frozen feature
extractor, a compressed data buffer, and an efficient non-iterative one-layer
neural network for classification. A pre-trained and frozen feature extractor
eliminates computationally expensive fine-tuning of the backbone. This,
combined with a compressed buffer for efficient memory use, enables CIFNet to
perform efficient class-incremental learning through a single-step optimization
process on fixed features, minimizing computational overhead and training time
without requiring multiple weight updates. Experiments on benchmark datasets
confirm that CIFNet effectively mitigates catastrophic forgetting at the
classifier level, achieving high accuracy comparable to that of existing
state-of-the-art methods, while substantially improving training efficiency and
sustainability. CIFNet represents a significant advancement in making
class-incremental learning more accessible and pragmatic in environments with
limited resources, especially when strong pre-trained feature extractors are
available.

</details>


### [194] [Opal: An Operator Algebra View of RLHF](https://arxiv.org/abs/2509.11298)
*Madhava Gaikwad*

Main category: cs.LG

TL;DR: 提出强化学习从人类反馈（RLHF）的算子视图Opal，引入GKPO规范模式，用示例展示相关概念，还有轻量级Python库。


<details>
  <summary>Details</summary>
Motivation: 为强化学习从人类反馈（RLHF）提供新的算子视角和统一规范模式。

Method: 定义目标为基本效用上的两个原语阶梯，提出简单归约定律，引入GKPO规范模式。

Result: 用GKPO为DPO、RRHF和ORPO举例，展示跨方法转换和非归约性测试，有轻量级Python参考库。

Conclusion: Opal和GKPO为RLHF提供了新视角和统一规范，有助于相关方法的表示和转换。

Abstract: We present Opal, an operator view of reinforcement learning from human
feedback (RLHF). Objectives are expressed as ladders of two primitives on a
base utility: additive penalties and multiplicative pairwise weights. We
describe a simple reduction law with if-and-only-if conditions: such ladders
collapse to a normal form on pairwise margins when the reference is fixed,
penalties are additive, and weights are independent of intermediate margins.
When these assumptions do not hold (reference shift, non-additive gates,
score-dependent weights), small examples demonstrate non-reducibility.
  Building on this view, we introduce GKPO (Generalized Kernel Preference
Object), a canonical schema in which many RLHF methods can be represented and,
when reducible, mapped back from. GKPO provides a standard JSON serialization,
canonicalization and hashing rules, and explicit flags with finite witnesses
when assumptions fail.
  We illustrate these ideas with GKPO examples for DPO, RRHF, and ORPO, along
with cross-method conversions (where assumptions permit) and minimal stress
tests (SHIFT/GATE/SCORE) that highlight non-reducibility. A lightweight Python
reference library accompanies the schema, implementing canonical hashing and
adapters for DPO and RRHF.

</details>


### [195] [MatQnA: A Benchmark Dataset for Multi-modal Large Language Models in Materials Characterization and Analysis](https://arxiv.org/abs/2509.11335)
*Yonghao Weng,Liqiang Gao,Linwu Zhu,Jian Huang*

Main category: cs.LG

TL;DR: 提出首个多模态材料表征技术基准数据集MatQnA，评估显示先进多模态AI模型在材料数据任务上准确率近90%，数据集公开可用。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在材料表征和分析这一高度专业化领域的能力未得到系统充分验证，需填补此空白。

Method: 采用大语言模型与人工验证相结合的混合方法构建高质量问答对，包含选择题和主观题。

Result: 先进多模态AI模型在材料数据解释和分析任务的客观题上准确率近90%。

Conclusion: 先进多模态AI模型在材料表征和分析领域有很强应用潜力，MatQnA数据集可推动该领域研究。

Abstract: Recently, large language models (LLMs) have achieved remarkable breakthroughs
in general domains such as programming and writing, and have demonstrated
strong potential in various scientific research scenarios. However, the
capabilities of AI models in the highly specialized field of materials
characterization and analysis have not yet been systematically or sufficiently
validated. To address this gap, we present MatQnA, the first multi-modal
benchmark dataset specifically designed for material characterization
techniques. MatQnA includes ten mainstream characterization methods, such as
X-ray Photoelectron Spectroscopy (XPS), X-ray Diffraction (XRD), Scanning
Electron Microscopy (SEM), Transmission Electron Microscopy (TEM), etc. We
employ a hybrid approach combining LLMs with human-in-the-loop validation to
construct high-quality question-answer pairs, integrating both multiple-choice
and subjective questions. Our preliminary evaluation results show that the most
advanced multi-modal AI models (e.g., GPT-4.1, Claude 4, Gemini 2.5, and Doubao
Vision Pro 32K) have already achieved nearly 90% accuracy on objective
questions in materials data interpretation and analysis tasks, demonstrating
strong potential for applications in materials characterization and analysis.
The MatQnA dataset is publicly available at
https://huggingface.co/datasets/richardhzgg/matQnA.

</details>


### [196] [On the Escaping Efficiency of Distributed Adversarial Training Algorithms](https://arxiv.org/abs/2509.11337)
*Ying Cao,Kun Yuan,Ali H. Sayed*

Main category: cs.LG

TL;DR: 本文比较多智能体学习环境中不同分布式对抗训练算法，构建理论框架研究算法逃离局部极小值效率，发现小扰动和大批次下分散式算法逃离更快，模拟结果验证理论，凸显分散式策略提升模型鲁棒性潜力。


<details>
  <summary>Details</summary>
Motivation: 对抗训练对提升模型抗攻击鲁棒性重要，此前研究强调模型平坦度对鲁棒性的影响，本文旨在比较多智能体学习环境中不同分布式对抗训练算法。

Method: 构建通用理论框架研究算法从局部极小值的逃离效率，进行模拟实验。

Result: 小扰动和大批次下，分散式对抗训练算法比集中式策略更快逃离局部极小值，倾向更平坦极小值；扰动增大时此趋势可能改变。模拟结果验证理论发现并比较不同算法模型性能。

Conclusion: 分散式策略在分布式环境中有提升模型鲁棒性的潜力。

Abstract: Adversarial training has been widely studied in recent years due to its role
in improving model robustness against adversarial attacks. This paper focuses
on comparing different distributed adversarial training algorithms--including
centralized and decentralized strategies--within multi-agent learning
environments. Previous studies have highlighted the importance of model
flatness in determining robustness. To this end, we develop a general
theoretical framework to study the escaping efficiency of these algorithms from
local minima, which is closely related to the flatness of the resulting models.
We show that when the perturbation bound is sufficiently small (i.e., when the
attack strength is relatively mild) and a large batch size is used,
decentralized adversarial training algorithms--including consensus and
diffusion--are guaranteed to escape faster from local minima than the
centralized strategy, thereby favoring flatter minima. However, as the
perturbation bound increases, this trend may no longer hold. In the simulation
results, we illustrate our theoretical findings and systematically compare the
performance of models obtained through decentralized and centralized
adversarial training algorithms. The results highlight the potential of
decentralized strategies to enhance the robustness of models in distributed
settings.

</details>


### [197] [BiLSTM-VHP: BiLSTM-Powered Network for Viral Host Prediction](https://arxiv.org/abs/2509.11345)
*Azher Ahmed Efat,Farzana Islam,Annajiat Alim Rasel,Munima Haque*

Main category: cs.LG

TL;DR: 本文提出BiLSTM - VHP模型，能高精度预测病毒宿主，表现优于先前研究，并提供数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 动物携带的人畜共患病毒会给人类带来危害，快速准确预测病毒宿主有助于预防疾病传播。

Method: 提出基于双向长短期记忆网络（LSTM）的轻量级架构BiLSTM - VHP，处理400个碱基长度的核苷酸序列，用混淆矩阵、F - 1分数等评估模型性能。

Result: 模型对汉坦病毒、轮状病毒A和狂犬病病毒的预测准确率分别为89.62%、96.58%和77.22%，优于先前研究。

Conclusion: BiLSTM - VHP能高精度预测这三种病毒的宿主，且提供了相关数据集和代码，有助于预防人畜共患疾病。

Abstract: Recorded history shows the long coexistence of humans and animals, suggesting
it began much earlier. Despite some beneficial interdependence, many animals
carry viral diseases that can spread to humans. These diseases are known as
zoonotic diseases. Recent outbreaks of SARS-CoV-2, Monkeypox and swine flu
viruses have shown how these viruses can disrupt human life and cause death.
Fast and accurate predictions of the host from which the virus spreads can help
prevent these diseases from spreading. This work presents BiLSTM-VHP, a
lightweight bidirectional long short-term memory (LSTM)-based architecture that
can predict the host from the nucleotide sequence of orthohantavirus, rabies
lyssavirus, and rotavirus A with high accuracy. The proposed model works with
nucleotide sequences of 400 bases in length and achieved a prediction accuracy
of 89.62% for orthohantavirus, 96.58% for rotavirus A, and 77.22% for rabies
lyssavirus outperforming previous studies. Moreover, performance of the model
is assessed using the confusion matrix, F-1 score, precision, recall,
microaverage AUC. In addition, we introduce three curated datasets of
orthohantavirus, rotavirus A, and rabies lyssavirus containing 8,575, 95,197,
and 22,052 nucleotide sequences divided into 9, 12, and 29 host classes,
respectively. The codes and dataset are available at
https://doi.org/10.17605/OSF.IO/ANFKR

</details>


### [198] [On Linear Mode Connectivity of Mixture-of-Experts Architectures](https://arxiv.org/abs/2509.11348)
*Viet-Hoang Tran,Van Hoan Trinh,Khanh Vinh Bui,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 本文研究混合专家（MoE）架构中的线性模式连通性（LMC）现象，提出匹配算法并验证其存在。


<details>
  <summary>Details</summary>
Motivation: 受标准神经网络中LMC研究启发，系统研究MoE架构中的LMC现象，因其可扩展性和计算效率。

Method: 先分析密集和稀疏门控机制对称性，再提出匹配算法使独立训练的MoE对齐，最后用算法在不同配置、设置和数据集上验证。

Result: 证实MoE架构中存在LMC。

Conclusion: 研究为深度学习模型功能景观和优化动态提供基础见解。

Abstract: Linear Mode Connectivity (LMC) is a notable phenomenon in the loss landscapes
of neural networks, wherein independently trained models have been observed to
be connected--up to permutation symmetries--by linear paths in parameter space
along which the loss remains consistently low. This observation challenges
classical views of non-convex optimization and has implications for model
ensembling, generalization, and our understanding of neural loss geometry.
Inspired by recent studies on LMC in standard neural networks, we
systematically investigate this phenomenon within Mixture-of-Experts (MoE)
architectures--a class of models known for their scalability and computational
efficiency, which combine traditional neural networks--referred to as
experts--through a learnable gating mechanism. We begin by conducting a
comprehensive analysis of both dense and sparse gating regimes, demonstrating
that the symmetries inherent to MoE architectures are fully characterized by
permutations acting on both the expert components and the gating function.
Building on these foundational findings, we propose a matching algorithm that
enables alignment between independently trained MoEs, thereby facilitating the
discovery of LMC. Finally, we empirically validate the presence of LMC using
our proposed algorithm across diverse MoE configurations--including dense,
sparse, and shared-expert variants--under a wide range of model settings and
datasets of varying scales and modalities. Our results confirm the existence of
LMC in MoE architectures and offer fundamental insights into the functional
landscape and optimization dynamics of deep learning models.

</details>


### [199] [PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits](https://arxiv.org/abs/2509.11362)
*Loka Li,Wong Yu Kang,Minghao Fu,Guangyi Chen,Zhenhao Chen,Gongxu Luo,Yuewen Sun,Salman Khan,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 提出PersonaX多模态数据集用于全面分析公共特征，包含CelebPersona和AthlePersona，用统计测试和新的因果表示学习框架分析，实验证明有效，推动多模态特征分析和因果推理。


<details>
  <summary>Details</summary>
Motivation: 现有资源缺乏结合行为描述符与面部属性、传记信息等互补模态的数据集，需解决此问题以理解人类行为特征。

Method: 构建PersonaX数据集，包括CelebPersona和AthlePersona；用统计独立性测试分析高层特征分数与其他模态关系；引入适用于多模态多测量数据的因果表示学习框架。

Result: 在合成和真实数据上的实验证明了方法的有效性。

Conclusion: PersonaX统一结构化和非结构化分析，为研究大语言模型推断的行为特征与视觉、传记属性结合奠定基础，推动多模态特征分析和因果推理。

Abstract: Understanding human behavior traits is central to applications in
human-computer interaction, computational social science, and personalized AI
systems. Such understanding often requires integrating multiple modalities to
capture nuanced patterns and relationships. However, existing resources rarely
provide datasets that combine behavioral descriptors with complementary
modalities such as facial attributes and biographical information. To address
this gap, we present PersonaX, a curated collection of multimodal datasets
designed to enable comprehensive analysis of public traits across modalities.
PersonaX consists of (1) CelebPersona, featuring 9444 public figures from
diverse occupations, and (2) AthlePersona, covering 4181 professional athletes
across 7 major sports leagues. Each dataset includes behavioral trait
assessments inferred by three high-performing large language models, alongside
facial imagery and structured biographical features. We analyze PersonaX at two
complementary levels. First, we abstract high-level trait scores from text
descriptions and apply five statistical independence tests to examine their
relationships with other modalities. Second, we introduce a novel causal
representation learning (CRL) framework tailored to multimodal and
multi-measurement data, providing theoretical identifiability guarantees.
Experiments on both synthetic and real-world data demonstrate the effectiveness
of our approach. By unifying structured and unstructured analysis, PersonaX
establishes a foundation for studying LLM-inferred behavioral traits in
conjunction with visual and biographical attributes, advancing multimodal trait
analysis and causal reasoning.

</details>


### [200] [Detecting Model Drifts in Non-Stationary Environment Using Edit Operation Measures](https://arxiv.org/abs/2509.11367)
*Chang-Hwan Lee,Alexander Shim*

Main category: cs.LG

TL;DR: 本文提出新框架，通过分析智能体行为序列分布变化检测强化学习中的模型漂移，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现实应用中强化学习环境动态非平稳，会出现模型漂移，需检测方法。

Method: 引入基于编辑操作的度量方法，量化平稳和扰动条件下状态 - 动作轨迹的偏差。

Result: 这些度量方法能有效区分漂移和非漂移场景，在不同噪声水平下也适用。

Conclusion: 所提方法为非平稳强化学习环境中的漂移检测提供实用工具。

Abstract: Reinforcement learning (RL) agents typically assume stationary environment
dynamics. Yet in real-world applications such as healthcare, robotics, and
finance, transition probabilities or reward functions may evolve, leading to
model drift. This paper proposes a novel framework to detect such drifts by
analyzing the distributional changes in sequences of agent behavior.
Specifically, we introduce a suite of edit operation-based measures to quantify
deviations between state-action trajectories generated under stationary and
perturbed conditions. Our experiments demonstrate that these measures can
effectively distinguish drifted from non-drifted scenarios, even under varying
levels of noise, providing a practical tool for drift detection in
non-stationary RL environments.

</details>


### [201] [Decoding Musical Origins: Distinguishing Human and AI Composers](https://arxiv.org/abs/2509.11369)
*Cheng-Yang Tsai,Tzu-Wei Huang,Shao-Yu Wei,Guan-Wei Chen,Hung-Ying Chu,Yu-Cheng Lin*

Main category: cs.LG

TL;DR: 本文开发YNote音乐记谱系统训练分类模型区分音乐创作主体，模型准确率高，可识别AI技术指纹。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展下，音乐数据表示是挑战，需有效方法区分不同主体创作的音乐。

Method: 开发YNote记谱系统，将问题视为文本分类问题，用TF - IDF算法提取特征，用SMOTE处理数据不平衡。

Result: 模型准确率达98.25%。

Conclusion: YNote保留足够风格信息，模型可识别AI技术指纹，为追溯AI生成内容来源提供工具。

Abstract: With the rapid advancement of Large Language Models (LLMs), AI-driven music
generation has become a vibrant and fruitful area of research. However, the
representation of musical data remains a significant challenge. To address
this, a novel, machine-learning-friendly music notation system, YNote, was
developed. This study leverages YNote to train an effective classification
model capable of distinguishing whether a piece of music was composed by a
human (Native), a rule-based algorithm (Algorithm Generated), or an LLM (LLM
Generated). We frame this as a text classification problem, applying the Term
Frequency-Inverse Document Frequency (TF-IDF) algorithm to extract structural
features from YNote sequences and using the Synthetic Minority Over-sampling
Technique (SMOTE) to address data imbalance. The resulting model achieves an
accuracy of 98.25%, successfully demonstrating that YNote retains sufficient
stylistic information for analysis. More importantly, the model can identify
the unique " technological fingerprints " left by different AI generation
techniques, providing a powerful tool for tracing the origins of AI-generated
content.

</details>


### [202] [From Firewalls to Frontiers: AI Red-Teaming is a Domain-Specific Evolution of Cyber Red-Teaming](https://arxiv.org/abs/2509.11398)
*Anusha Sinha,Keltin Grimes,James Lucassen,Michael Feffer,Nathan VanHoudnos,Zhiwei Steven Wu,Hoda Heidari*

Main category: cs.LG

TL;DR: 随着企业系统采用AI，红队测试需进化，提出将AI红队测试视为网络红队测试的特定领域演变，合并AI和网络红队能创建强大安全生态。


<details>
  <summary>Details</summary>
Motivation: 企业系统采用AI，红队测试需应对AI系统独特漏洞和风险。

Method: 将AI红队测试视为网络红队测试特定领域演变，现有网络红队和AI红队采用对应框架。

Result: 现有网络红队能更好评估含AI组件系统，现有AI红队可利用成熟结构开展测试。

Conclusion: 合并AI和网络红队能创建强大安全生态，适应快速变化的威胁态势。

Abstract: A red team simulates adversary attacks to help defenders find effective
strategies to defend their systems in a real-world operational setting. As more
enterprise systems adopt AI, red-teaming will need to evolve to address the
unique vulnerabilities and risks posed by AI systems. We take the position that
AI systems can be more effectively red-teamed if AI red-teaming is recognized
as a domain-specific evolution of cyber red-teaming. Specifically, we argue
that existing Cyber Red Teams who adopt this framing will be able to better
evaluate systems with AI components by recognizing that AI poses new risks, has
new failure modes to exploit, and often contains unpatchable bugs that
re-prioritize disclosure and mitigation strategies. Similarly, adopting a
cybersecurity framing will allow existing AI Red Teams to leverage a
well-tested structure to emulate realistic adversaries, promote mutual
accountability with formal rules of engagement, and provide a pattern to mature
the tooling necessary for repeatable, scalable engagements. In these ways, the
merging of AI and Cyber Red Teams will create a robust security ecosystem and
best position the community to adapt to the rapidly changing threat landscape.

</details>


### [203] [Framing AI System Benchmarking as a Learning Task: FlexBench and the Open MLPerf Dataset](https://arxiv.org/abs/2509.11413)
*Grigori Fursin,Daniel Altunay*

Main category: cs.LG

TL;DR: 现有AI系统基准难跟上AI发展，提出将基准测试作为AI任务，推出FlexBench，收集结果到Open MLPerf Dataset，通过MLPerf Inference提交验证概念，目标是助从业者做有效AI部署决策。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统基准如MLPerf难以跟上AI快速发展，难以支持AI系统的部署、优化和协同设计决策。

Method: 将基准测试视为AI任务，提出FlexBench，它是MLPerf LLM推理基准的模块化扩展，与HuggingFace集成，将基准测试结果和元数据收集到Open MLPerf Dataset。

Result: 通过MLPerf Inference提交成功验证了FlexBench概念，对DeepSeek R1和LLaMA 3.3在商用服务器上进行评估。

Conclusion: 能让从业者基于可用资源、需求和约束做出具有成本效益的AI部署决策。

Abstract: Existing AI system benchmarks such as MLPerf often struggle to keep pace with
the rapidly evolving AI landscape, making it difficult to support informed
deployment, optimization, and co-design decisions for AI systems. We suggest
that benchmarking itself can be framed as an AI task - one in which models are
continuously evaluated and optimized across diverse datasets, software, and
hardware, using key metrics such as accuracy, latency, throughput, energy
consumption, and cost. To support this perspective, we present FlexBench: a
modular extension of the MLPerf LLM inference benchmark, integrated with
HuggingFace and designed to provide relevant and actionable insights.
Benchmarking results and metadata are collected into an Open MLPerf Dataset,
which can be collaboratively curated, extended, and leveraged for predictive
modeling and feature engineering. We successfully validated the FlexBench
concept through MLPerf Inference submissions, including evaluations of DeepSeek
R1 and LLaMA 3.3 on commodity servers. The broader objective is to enable
practitioners to make cost-effective AI deployment decisions that reflect their
available resources, requirements, and constraints.

</details>


### [204] [Tabular Data with Class Imbalance: Predicting Electric Vehicle Crash Severity with Pretrained Transformers (TabPFN) and Mamba-Based Models](https://arxiv.org/abs/2509.11449)
*Shriyank Somvanshi,Pavan Hebli,Gaurab Chhetri,Subasish Das*

Main category: cs.LG

TL;DR: 研究用德州2017 - 2023年真实撞车数据构建深度表格学习框架预测电动汽车撞车严重程度，对比三种模型，强调其在提升预测和安全干预方面潜力。


<details>
  <summary>Details</summary>
Motivation: 使用真实数据构建框架预测电动汽车撞车严重程度，以实现数据驱动的安全干预。

Method: 分析23301条纯电动汽车撞车记录，用XGBoost和随机森林确定特征重要性，用SMOTEENN处理类别不平衡，对TabPFN、MambaNet和MambaAttention三种模型进行基准测试。

Result: TabPFN泛化能力强，MambaAttention在分类严重受伤案例中表现更优。

Conclusion: 深度表格架构在改善电动汽车撞车严重程度预测和实现数据驱动安全干预方面有潜力。

Abstract: This study presents a deep tabular learning framework for predicting crash
severity in electric vehicle (EV) collisions using real-world crash data from
Texas (2017-2023). After filtering for electric-only vehicles, 23,301
EV-involved crash records were analyzed. Feature importance techniques using
XGBoost and Random Forest identified intersection relation, first harmful
event, person age, crash speed limit, and day of week as the top predictors,
along with advanced safety features like automatic emergency braking. To
address class imbalance, Synthetic Minority Over-sampling Technique and Edited
Nearest Neighbors (SMOTEENN) resampling was applied. Three state-of-the-art
deep tabular models, TabPFN, MambaNet, and MambaAttention, were benchmarked for
severity prediction. While TabPFN demonstrated strong generalization,
MambaAttention achieved superior performance in classifying severe injury cases
due to its attention-based feature reweighting. The findings highlight the
potential of deep tabular architectures for improving crash severity prediction
and enabling data-driven safety interventions in EV crash contexts.

</details>


### [205] [Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting](https://arxiv.org/abs/2509.11452)
*Yining Lu,Zilong Wang,Shiyang Li,Xin Liu,Changlong Yu,Qingyu Yin,Zhan Shi,Zixuan Zhang,Meng Jiang*

Main category: cs.LG

TL;DR: 现有多目标强化学习用固定权重线性奖励标量化有局限，本文提出动态奖励加权，含两种方法，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 现有多目标强化学习中固定权重线性奖励标量化方法无法捕捉非凸帕累托前沿，在大语言模型在线偏好对齐中存在局限。

Method: 引入动态奖励加权，在在线强化学习过程中自适应调整奖励权重，具体有超体积引导的权重适应和基于梯度的权重优化两种方法。

Result: 与常用在线强化学习算法兼容，在多个数学推理数据集上有效，适用于不同模型家族，比固定权重线性标量化基线用更少训练步骤实现帕累托占优解。

Conclusion: 提出的动态奖励加权方法能有效解决现有方法局限，为在线多目标对齐提供了通用工具包。

Abstract: Prior works in multi-objective reinforcement learning typically use linear
reward scalarization with fixed weights, which provably fail to capture
non-convex Pareto fronts and thus yield suboptimal results. This limitation
becomes especially critical in online preference alignment for large language
models. Here, stochastic trajectories generated by parameterized policies
create highly non-linear and non-convex mappings from parameters to objectives
that no single static weighting scheme can find optimal trade-offs. We address
this limitation by introducing dynamic reward weighting, which adaptively
adjusts reward weights during the online reinforcement learning process. Unlike
existing approaches that rely on fixed-weight interpolation, our dynamic
weighting continuously balances and prioritizes objectives in training,
facilitating effective exploration of Pareto fronts in objective space. We
introduce two approaches of increasing sophistication and generalizability: (1)
hypervolume-guided weight adaptation and (2) gradient-based weight
optimization, offering a versatile toolkit for online multi-objective
alignment. Our extensive experiments demonstrate their compatibility with
commonly used online reinforcement learning algorithms (including GRPO,
REINFORCE, and RLOO), effectiveness across multiple mathematical reasoning
datasets, and applicability to different model families, consistently achieving
Pareto dominant solutions with fewer training steps than fixed-weight linear
scalarization baselines.

</details>


### [206] [Drug Repurposing Using Deep Embedded Clustering and Graph Neural Networks](https://arxiv.org/abs/2509.11493)
*Luke Delzer,Robert Kroleski,Ali K. AlShami,Jugal Kalita*

Main category: cs.LG

TL;DR: 本文提出机器学习管道识别新的药物 - 疾病关联，表现良好，为药物再利用提供新前景。


<details>
  <summary>Details</summary>
Motivation: 传统药物再利用经济上不可行，现有机器学习研究多依赖简化数据集，需新方法识别药物 - 疾病关联。

Method: 提出机器学习管道，结合无监督深度嵌入聚类和有监督图神经网络链接预测，用无监督自动编码器和聚类训练降低数据维度。

Result: 9022种药物分为35个聚类，轮廓系数0.8550；图神经网络预测准确率0.901，ROC曲线下面积0.960，F1分数0.901，生成477个概率超99%的链接列表。

Conclusion: 本研究可为不相关疾病领域提供新药物 - 疾病链接前景，推动药物再利用中机器学习的理解。

Abstract: Drug repurposing has historically been an economically infeasible process for
identifying novel uses for abandoned drugs. Modern machine learning has enabled
the identification of complex biochemical intricacies in candidate drugs;
however, many studies rely on simplified datasets with known drug-disease
similarities. We propose a machine learning pipeline that uses unsupervised
deep embedded clustering, combined with supervised graph neural network link
prediction to identify new drug-disease links from multi-omic data.
Unsupervised autoencoder and cluster training reduced the dimensionality of
omic data into a compressed latent embedding. A total of 9,022 unique drugs
were partitioned into 35 clusters with a mean silhouette score of 0.8550. Graph
neural networks achieved strong statistical performance, with a prediction
accuracy of 0.901, receiver operating characteristic area under the curve of
0.960, and F1-Score of 0.901. A ranked list comprised of 477 per-cluster link
probabilities exceeding 99 percent was generated. This study could provide new
drug-disease link prospects across unrelated disease domains, while advancing
the understanding of machine learning in drug repurposing studies.

</details>


### [207] [OASIS: A Deep Learning Framework for Universal Spectroscopic Analysis Driven by Novel Loss Functions](https://arxiv.org/abs/2509.11499)
*Chris Young,Juejing Liu,Marie L. Mortensen,Yifu Feng,Elizabeth Li,Zheming Wang,Xiaofeng Guo,Kevin M. Rosso,Xin Zhang*

Main category: cs.LG

TL;DR: 介绍了用于光谱自动分析的机器学习框架OASIS，它能在无人工干预下完成多种任务，经实验验证有应用潜力，强调优化损失函数的重要性。


<details>
  <summary>Details</summary>
Motivation: 各科学和工程领域光谱数据激增，需要自动化处理。

Method: 创建机器学习框架OASIS，用精心设计的合成数据集训练模型，开发创新的特定任务损失函数。

Result: 框架能完成光谱分析多种任务，用拉曼、紫外 - 可见和荧光光谱实验数据验证模型准确性。

Conclusion: 优化损失函数是开发高性能机器学习模型的关键资源高效策略。

Abstract: The proliferation of spectroscopic data across various scientific and
engineering fields necessitates automated processing. We introduce OASIS
(Omni-purpose Analysis of Spectra via Intelligent Systems), a machine learning
(ML) framework for technique-independent, automated spectral analysis,
encompassing denoising, baseline correction, and comprehensive peak parameter
(location, intensity, FWHM) retrieval without human intervention. OASIS
achieves its versatility through models trained on a strategically designed
synthetic dataset incorporating features from numerous spectroscopy techniques.
Critically, the development of innovative, task-specific loss functions-such as
the vicinity peak response (ViPeR) for peak localization-enabled the creation
of compact yet highly accurate models from this dataset, validated with
experimental data from Raman, UV-vis, and fluorescence spectroscopy. OASIS
demonstrates significant potential for applications including in situ
experiments, high-throughput optimization, and online monitoring. This study
underscores the optimization of the loss function as a key resource-efficient
strategy to develop high-performance ML models.

</details>


### [208] [Know What You Don't Know: Selective Prediction for Early Exit DNNs](https://arxiv.org/abs/2509.11520)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 针对DNN推理延迟和可信度问题，提出SPEED方法结合选择性预测与早期退出策略，提升了准确率和延迟性能。


<details>
  <summary>Details</summary>
Motivation: DNN在关键应用中存在推理延迟和可信度瓶颈，早期退出策略因模型过度自信而不可靠。

Method: 提出SPEED方法，在每层使用延迟分类器检查样本难度，对难样本延迟到专家处理。

Result: 早期退出结合选择性预测提高了准确率和延迟，相比最终层，错误预测风险降低50%，速度提升2.05倍。

Conclusion: SPEED方法有效解决了DNN推理延迟和可信度问题。

Abstract: Inference latency and trustworthiness of Deep Neural Networks (DNNs) are the
bottlenecks in deploying them in critical applications like sensitive tasks.
Early Exit (EE) DNNs overcome the latency issues by allowing samples to exit
from intermediary layers if they attain `high' confidence scores on the
predicted class. However, the DNNs are known to exhibit overconfidence, which
can lead to many samples exiting early and render EE strategies untrustworthy.
We use Selective Prediction (SP) to overcome this issue by checking the
`hardness' of the samples rather than just relying on the confidence score
alone. We propose SPEED, a novel approach that uses Deferral Classifiers (DCs)
at each layer to check the hardness of samples before performing EEs.
Specifically, the DCs identify if a sample is hard to predict at an
intermediary layer, leading to hallucination, and defer it to an expert. Early
detection of hard samples for inference prevents the wastage of computational
resources and improves trust by deferring the hard samples to the expert. We
demonstrate that EE aided with SP improves both accuracy and latency. Our
method minimizes the risk of wrong prediction by $50\%$ with a speedup of
$2.05\times$ as compared to the final layer. The anonymized source code is
available at https://github.com/Div290/SPEED

</details>


### [209] [DARD: Dice Adversarial Robustness Distillation against Adversarial Attacks](https://arxiv.org/abs/2509.11525)
*Jing Zou,Shungeng Zhang,Meikang Qiu,Chong Li*

Main category: cs.LG

TL;DR: 本文提出Dice对抗鲁棒性蒸馏（DARD）和Dice投影梯度下降（DPGD）方法，将大模型的鲁棒性蒸馏到小模型，实验表明DARD性能优于相同架构的对抗训练网络。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型易受对抗样本攻击，对抗训练在增强鲁棒性时会降低自然数据性能，大模型鲁棒性更强，需将大模型鲁棒性转移到小模型。

Method: 提出Dice Adversarial Robustness Distillation (DARD) 方法，通过定制知识蒸馏范式转移鲁棒性；提出Dice Projected Gradient Descent (DPGD) 对抗样本泛化方法。

Result: DARD方法始终优于相同架构的对抗训练网络，实现了更好的鲁棒性和标准准确率。

Conclusion: 所提出的DARD和DPGD方法能有效将大模型鲁棒性转移到小模型，提升模型性能。

Abstract: Deep learning models are vulnerable to adversarial examples, posing critical
security challenges in real-world applications. While Adversarial Training (AT
) is a widely adopted defense mechanism to enhance robustness, it often incurs
a trade-off by degrading performance on unperturbed, natural data. Recent
efforts have highlighted that larger models exhibit enhanced robustness over
their smaller counterparts. In this paper, we empirically demonstrate that such
robustness can be systematically distilled from large teacher models into
compact student models. To achieve better performance, we introduce Dice
Adversarial Robustness Distillation (DARD), a novel method designed to transfer
robustness through a tailored knowledge distillation paradigm. Additionally, we
propose Dice Projected Gradient Descent (DPGD), an adversarial example
generalization method optimized for effective attack. Our extensive experiments
demonstrate that the DARD approach consistently outperforms adversarially
trained networks with the same architecture, achieving superior robustness and
standard accuracy.

</details>


### [210] [UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning](https://arxiv.org/abs/2509.11543)
*Zhengxi Lu,Jiabo Ye,Fei Tang,Yongliang Shen,Haiyang Xu,Ziwei Zheng,Weiming Lu,Ming Yan,Fei Huang,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 提出半在线强化学习范式，结合离线和在线强化学习优势，在四个动态基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前离线和在线强化学习在GUI代理自动化交互中的困境，离线RL缺乏轨迹级奖励信号，在线RL有稀疏奖励和高部署成本问题。

Method: 在离线轨迹上模拟在线RL，在多轮对话中保留原模型输出，用Patch模块恢复偏差，引入折扣未来回报到奖励计算，用加权步级和回合级优势优化策略，引入SOP指标。

Result: 半在线RL在四个动态基准测试的7B模型中取得SOTA性能，相比基础模型有显著提升，如在AndroidWorld上提升12.0%，在AITW上提升23.8%。

Conclusion: 半在线RL在缩小离线训练效率和在线多轮推理差距方面取得显著进展。

Abstract: Graphical User Interface (GUI) agents have demonstrated remarkable progress
in automating complex user interface interactions through reinforcement
learning. However, current approaches face a fundamental dilemma: offline RL
enables stable training on pre-collected trajectories, but struggles with
multi-step task execution for lack of trajectory-level reward signals; online
RL captures these signals through environment interaction, but suffers from
sparse rewards and prohibitive deployment costs. To address it, we present
Semi-online Reinforcement Learning, a novel paradigm that simulates online RL
on offline trajectories. During each rollout process, we preserve the original
model output within the multi-turn dialogue, where a Patch Module adaptively
recovers the divergence between rollout and expert trajectories. To capture
long-term training signals, Semi-online RL introduces discounted future returns
into the reward computation and optimizes the policy with weighted step-level
and episode-level advantages. We further introduce Semi-Online Performance
(SOP), a metric that aligns better with true online performance, serving as a
practical and effective proxy for real-world evaluation. Experiments show that
ours Semi-online RL achieves SOTA performance among 7B models across four
dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on
AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging
the gap between offline training efficiency and online multi-turn reasoning.
The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.

</details>


### [211] [Compressed Sensing: Mathematical Foundations, Implementation, and Advanced Optimization Techniques](https://arxiv.org/abs/2509.11550)
*Shane Stevenson,Maryam Sabagh*

Main category: cs.LG

TL;DR: 本文探讨压缩感知的数学公式、逻辑与问题，并将其应用于现实世界信号。


<details>
  <summary>Details</summary>
Motivation: 压缩感知可从小测量集重建信号，且现实信号多具有稀疏性，有应用价值。

Method: 对压缩感知的数学公式进行研究，并应用到现实世界信号。

Result: 未提及。

Conclusion: 未提及。

Abstract: Compressed sensing is a signal processing technique that allows for the
reconstruction of a signal from a small set of measurements. The key idea
behind compressed sensing is that many real-world signals are inherently
sparse, meaning that they can be efficiently represented in a different space
with only a few components compared to their original space representation. In
this paper we will explore the mathematical formulation behind compressed
sensing, its logic and pathologies, and apply compressed sensing to real world
signals.

</details>


### [212] [Dynamic Adaptive Parsing of Temporal and Cross-Variable Patterns for Network State Classification](https://arxiv.org/abs/2509.11601)
*Yuan Gao,Xuelong Wang,Zhenguo Dong,Yong Zhang*

Main category: cs.LG

TL;DR: 现有网络状态分类方法难兼顾时间和变量依赖特征，本文提出DAPNet框架解决该问题，实验验证其准确性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在网络状态分类中难以同时捕捉流量数据的时间周期性和变量间动态依赖特征。

Method: 引入基于混合专家架构的DAPNet框架，集成三个专业网络，用可学习门控网络分配权重，采用混合正则化损失函数。

Result: 在两个网络入侵检测数据集上验证了更高准确性，在十个公共UEA基准数据集上评估了架构设计的泛化性。

Conclusion: DAPNet是用于网络状态分类的专业框架。

Abstract: Effective network state classification is a primary task for ensuring network
security and optimizing performance. Existing deep learning models have shown
considerable progress in this area. Some methods excel at analyzing the complex
temporal periodicities found in traffic data, while graph-based approaches are
adept at modeling the dynamic dependencies between different variables.
However, a key trade-off remains, as these methods struggle to capture both
characteristics simultaneously. Models focused on temporal patterns often
overlook crucial variable dependencies, whereas those centered on dependencies
may fail to capture fine-grained temporal details. To address this trade-off,
we introduce DAPNet, a framework based on a Mixture-of-Experts architecture.
DAPNet integrates three specialized networks for periodic analysis, dynamic
cross-variable correlation modeling, and hybrid temporal feature extraction. A
learnable gating network dynamically assigns weights to experts based on the
input sample and computes a weighted fusion of their outputs. Furthermore, a
hybrid regularization loss function ensures stable training and addresses the
common issue of class imbalance. Extensive experiments on two large-scale
network intrusion detection datasets (CICIDS2017/2018) validate DAPNet's higher
accuracy for its target application. The generalizability of the architectural
design is evaluated across ten public UEA benchmark datasets, positioning
DAPNet as a specialized framework for network state classification.

</details>


### [213] [Topology Structure Optimization of Reservoirs Using GLMY Homology](https://arxiv.org/abs/2509.11612)
*Yu Chen,Shengwei Wang,Hongwei Lin*

Main category: cs.LG

TL;DR: 本文用持久GLMY同调理论研究储层拓扑结构并提出优化方法，实验验证储层性能受结构和数据集周期性共同影响。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏合适数学工具，储层拓扑结构及其性能难以分析，需研究其结构并提升性能。

Method: 使用持久GLMY同调理论研究储层拓扑结构，发现其性能与一维GLMY同调群密切相关，通过修改一维GLMY同调群的最小代表循环开发储层结构优化方法。

Result: 实验验证储层性能受储层结构和数据集周期性共同影响。

Conclusion: 可通过基于持久GLMY同调理论的结构优化方法提升储层性能。

Abstract: Reservoir is an efficient network for time series processing. It is well
known that network structure is one of the determinants of its performance.
However, the topology structure of reservoirs, as well as their performance, is
hard to analyzed, due to the lack of suitable mathematical tools. In this
paper, we study the topology structure of reservoirs using persistent GLMY
homology theory, and develop a method to improve its performance. Specifically,
it is found that the reservoir performance is closely related to the
one-dimensional GLMY homology groups. Then, we develop a reservoir structure
optimization method by modifying the minimal representative cycles of
one-dimensional GLMY homology groups. Finally, by experiments, it is validated
that the performance of reservoirs is jointly influenced by the reservoir
structure and the periodicity of the dataset.

</details>


### [214] [Inducing Uncertainty for Test-Time Privacy](https://arxiv.org/abs/2509.11625)
*Muhammad H. Ashiq,Peter Triantafillou,Hung Yun Tseng,Grigoris G. Chrysos*

Main category: cs.LG

TL;DR: 文章指出机器学习模型去学习后仍存在测试时隐私威胁，提出算法解决该威胁，有理论保证且实验效果好。


<details>
  <summary>Details</summary>
Motivation: 去学习后模型对已删除数据仍能高置信度预测，存在测试时隐私威胁，现有方法难以抵御。

Method: 引入算法扰动模型权重，基于帕累托最优目标微调，还有可证明的近似算法。

Result: 算法在多个图像识别基准上比预训练获得>3倍更强的不确定性，准确率下降<0.2%。

Conclusion: 该框架为终端用户提供了额外保护工具。

Abstract: Unlearning is the predominant method for removing the influence of data in
machine learning models. However, even after unlearning, models often continue
to produce the same predictions on the unlearned data with high confidence.
This persistent behavior can be exploited by adversaries using confident model
predictions on incorrect or obsolete data to harm users. We call this threat
model, which unlearning fails to protect against, *test-time privacy*. In
particular, an adversary with full model access can bypass any naive defenses
which ensure test-time privacy. To address this threat, we introduce an
algorithm which perturbs model weights to induce maximal uncertainty on
protected instances while preserving accuracy on the rest of the instances. Our
core algorithm is based on finetuning with a Pareto optimal objective that
explicitly balances test-time privacy against utility. We also provide a
certifiable approximation algorithm which achieves $(\varepsilon, \delta)$
guarantees without convexity assumptions. We then prove a tight, non-vacuous
bound that characterizes the privacy-utility tradeoff that our algorithms
incur. Empirically, our method obtains $>3\times$ stronger uncertainty than
pretraining with $<0.2\%$ drops in accuracy on various image recognition
benchmarks. Altogether, this framework provides a tool to guarantee additional
protection to end users.

</details>


### [215] [SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching](https://arxiv.org/abs/2509.11628)
*Jiacheng Liu,Chang Zou,Yuanhuiyi Lyu,Fei Ren,Shaobo Wang,Kaixin Li,Linfeng Zhang*

Main category: cs.LG

TL;DR: 本文提出SpeCa加速框架解决扩散模型实时应用的计算难题，实验显示显著加速且质量损失小。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算需求高，存在严格时间依赖和计算密集问题，难以用于实时应用。

Method: 引入投机采样预测中间特征，实现无参数验证机制评估预测可靠性，采用样本自适应计算分配动态调整资源。

Result: 在FLUX上加速6.34倍，质量仅降5.5%；在DiT上加速7.3倍且保真；在HunyuanVideo上6.1倍加速时VBench得分79.84%，验证机制开销小。

Conclusion: SpeCa建立了高效扩散模型推理的新范式，在高加速比下仍能保证生成质量。

Abstract: Diffusion models have revolutionized high-fidelity image and video synthesis,
yet their computational demands remain prohibitive for real-time applications.
These models face two fundamental challenges: strict temporal dependencies
preventing parallelization, and computationally intensive forward passes
required at each denoising step. Drawing inspiration from speculative decoding
in large language models, we present SpeCa, a novel 'Forecast-then-verify'
acceleration framework that effectively addresses both limitations. SpeCa's
core innovation lies in introducing Speculative Sampling to diffusion models,
predicting intermediate features for subsequent timesteps based on fully
computed reference timesteps. Our approach implements a parameter-free
verification mechanism that efficiently evaluates prediction reliability,
enabling real-time decisions to accept or reject each prediction while
incurring negligible computational overhead. Furthermore, SpeCa introduces
sample-adaptive computation allocation that dynamically modulates resources
based on generation complexity, allocating reduced computation for simpler
samples while preserving intensive processing for complex instances.
Experiments demonstrate 6.34x acceleration on FLUX with minimal quality
degradation (5.5% drop), 7.3x speedup on DiT while preserving generation
fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The
verification mechanism incurs minimal overhead (1.67%-3.5% of full inference
costs), establishing a new paradigm for efficient diffusion model inference
while maintaining generation quality even at aggressive acceleration ratios.
Our codes have been released in Github:
\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}

</details>


### [216] [Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check](https://arxiv.org/abs/2509.11629)
*Chentao Cao,Xiaojun Xu,Bo Han,Hang Li*

Main category: cs.LG

TL;DR: 提出Answer - Then - Check安全对齐方法，构建ReSA数据集，实验显示该方法安全能力强、减少过度拒绝率，模型保持通用推理能力，能安全补全，少量数据训练效果与全量相当。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，确保其抵御越狱攻击的安全性是关键挑战。

Method: 引入Answer - Then - Check方法，构建包含80K示例的ReSA数据集，让模型先思考作答再评估安全性。

Result: 方法在安全能力和降低过度拒绝率上达到帕累托最优，模型保持通用推理能力，能进行安全补全，少量数据训练效果与全量相当。

Conclusion: 所提方法有效提升大语言模型安全性，安全对齐所需数据可能比以往认为的少。

Abstract: As large language models (LLMs) continue to advance in capabilities, ensuring
their safety against jailbreak attacks remains a critical challenge. In this
paper, we introduce a novel safety alignment approach called Answer-Then-Check,
which enhances LLM robustness against malicious prompts by applying thinking
ability to mitigate jailbreaking problems before producing a final answer to
the user. Our method enables models to directly answer the question in their
thought and then critically evaluate its safety before deciding whether to
provide it. To implement this approach, we construct the Reasoned Safety
Alignment (ReSA) dataset, comprising 80K examples that teach models to reason
through direct responses and then analyze their safety. Experimental results
demonstrate that our approach achieves the Pareto frontier with superior safety
capability while decreasing over-refusal rates on over-refusal benchmarks.
Notably, the model fine-tuned with ReSA maintains general reasoning
capabilities on benchmarks like MMLU, MATH500, and HumanEval. Besides, our
method equips models with the ability to perform safe completion. Unlike
post-hoc methods that can only reject harmful queries, our model can provide
helpful and safe alternative responses for sensitive topics (e.g., self-harm).
Furthermore, we discover that training on a small subset of just 500 examples
can achieve comparable performance to using the full dataset, suggesting that
safety alignment may require less data than previously assumed.

</details>


### [217] [Adaptive-GraphSketch: Real-Time Edge Anomaly Detection via Multi-Layer Tensor Sketching and Temporal Decay](https://arxiv.org/abs/2509.11633)
*Ocheme Anthony Ekle,William Eberle*

Main category: cs.LG

TL;DR: 提出ADAPTIVE - GRAPHSKETCH框架用于流边数据实时异常检测，实验显示其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有动态图异常检测方法在可扩展性、概率解释性和对流量模式演变的适应性方面存在问题。

Method: 集成时间多张量草图与使用保守更新的Count - Min Sketch，结合贝叶斯推理进行概率异常评分，应用指数加权移动平均进行自适应阈值调整。

Result: 在四个真实入侵检测数据集上表现优于现有基线，如在CIC - IDS2018上AUC最多提升6.5%，在CIC - DDoS2019上最多提升15.6%，用10个哈希函数在3.4秒内处理2000万条边。

Conclusion: ADAPTIVE - GRAPHSKETCH对大规模流图的快速准确异常检测实用且有效。

Abstract: Anomaly detection in dynamic graphs is essential for identifying malicious
activities, fraud, and unexpected behaviors in real-world systems such as
cybersecurity and power grids. However, existing approaches struggle with
scalability, probabilistic interpretability, and adaptability to evolving
traffic patterns. In this paper, we propose ADAPTIVE-GRAPHSKETCH, a lightweight
and scalable framework for real-time anomaly detection in streaming edge data.
Our method integrates temporal multi-tensor sketching with Count-Min Sketch
using Conservative Update (CMS-CU) to compactly track edge frequency patterns
with bounded memory, while mitigating hash collision issues. We incorporate
Bayesian inference for probabilistic anomaly scoring and apply Exponentially
Weighted Moving Average (EWMA) for adaptive thresholding tuned to burst
intensity. Extensive experiments on four real-world intrusion detection
datasets demonstrate that ADAPTIVE-GRAPHSKETCH outperforms state-of-the-art
baselines such as ANOEDGE-G/L, MIDAS-R, and F-FADE, achieving up to 6.5% AUC
gain on CIC-IDS2018 and up to 15.6% on CIC-DDoS2019, while processing 20
million edges in under 3.4 seconds using only 10 hash functions. Our results
show that ADAPTIVE-GRAPHSKETCH is practical and effective for fast, accurate
anomaly detection in large-scale streaming graphs.
  Keywords: Anomaly Detection, Streaming, Real-time, Dynamic Graphs, Edge
Streams, Tensor Sketching

</details>


### [218] [Assessing On-the-Ground Disaster Impact Using Online Data Sources](https://arxiv.org/abs/2509.11634)
*Saketh Vishnubhatla,Ujun Jeong,Bohan Jiang,Paras Sheth,Zhen Tan,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 本文探讨用多在线数据源评估灾害影响，与传统方法对比，发现不同数据源可提供互补信息。


<details>
  <summary>Details</summary>
Motivation: 传统灾害影响评估方法有时间延迟且易有偏差，需研究不同在线源在给定行政单元评估灾害影响的作用。

Method: 收集多个在线源数据构建综合数据集，对比在线评估与传统离线评估。

Result: 发现不同在线源能提供互补信息以评估灾害。

Conclusion: 不同在线数据源对灾害影响评估有重要作用，可与传统方法互补。

Abstract: Assessing the impact of a disaster in terms of asset losses and human
casualties is essential for preparing effective response plans. Traditional
methods include offline assessments conducted on the ground, where volunteers
and first responders work together to collect the estimate of losses through
windshield surveys or on-ground inspection. However, these methods have a time
delay and are prone to different biases. Recently, various online data sources,
including social media, news reports, aerial imagery, and satellite data, have
been utilized to evaluate the impact of disasters. Online data sources provide
real-time data streams for estimating the offline impact. Limited research
exists on how different online sources help estimate disaster impact at a given
administrative unit. In our work, we curate a comprehensive dataset by
collecting data from multiple online sources for a few billion-dollar disasters
at the county level. We also analyze how online estimates compare with
traditional offline-based impact estimates for the disaster. Our findings
provide insight into how different sources can provide complementary
information to assess the disaster.

</details>


### [219] [Measuring Visual Understanding in Telecom domain: Performance Metrics for Image-to-UML conversion using VLMs](https://arxiv.org/abs/2509.11667)
*HG Ranjani,Rutuja Prabhudesai*

Main category: cs.LG

TL;DR: 本文提出性能指标衡量3GPP序列图图像转puml格式的转换效果，对比两个VLM模型输出，结果显示VLM在复杂结构表现不佳，需改进训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏对3GPP序列图图像转puml格式转换效果的评估，特别是未对puml脚本各组件进行比较。

Method: 选择3GPP文档序列图数据集，对比Claude Sonnet和GPT - 4V的puml输出与手动创建的真实表示，用版本控制工具捕捉差异，引入标准性能指标衡量各组件准确性。

Result: 节点、边和消息能被准确捕捉，但VLMs在笔记、框、组等复杂结构上表现不佳。

Conclusion: 微调的VLMs训练数据需要更好地表示复杂结构组件。

Abstract: Telecom domain 3GPP documents are replete with images containing sequence
diagrams. Advances in Vision-Language Large Models (VLMs) have eased conversion
of such images to machine-readable PlantUML (puml) formats. However, there is a
gap in evaluation of such conversions - existing works do not compare puml
scripts for various components. In this work, we propose performance metrics to
measure the effectiveness of such conversions. A dataset of sequence diagrams
from 3GPP documents is chosen to be representative of domain-specific actual
scenarios. We compare puml outputs from two VLMs - Claude Sonnet and GPT-4V -
against manually created ground truth representations. We use version control
tools to capture differences and introduce standard performance metrics to
measure accuracies along various components: participant identification,
message flow accuracy, sequence ordering, and grouping construct preservation.
We demonstrate effectiveness of proposed metrics in quantifying conversion
errors across various components of puml scripts. The results show that nodes,
edges and messages are accurately captured. However, we observe that VLMs do
not necessarily perform well on complex structures such as notes, box, groups.
Our experiments and performance metrics indicates a need for better
representation of these components in training data for fine-tuned VLMs.

</details>


### [220] [An Interventional Approach to Real-Time Disaster Assessment via Causal Attribution](https://arxiv.org/abs/2509.11676)
*Saketh Vishnubhatla,Alimohammad Beigi,Rui Heng Foo,Umang Goel,Ujun Jeong,Bohan Jiang,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 提出一种补充传统灾害建模工具的干预性工具，利用实时数据源，能理解因果归因并提供行动建议，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统灾害分析和建模工具不具备因果干预性，无法模拟反事实场景。

Method: 利用卫星图像、新闻和社交媒体等实时数据源开发干预性工具。

Result: 开发出可补充传统工具的干预性工具，能分析因果归因并提供行动建议，代码公开。

Conclusion: 所提供的干预性工具可有效补充传统灾害建模工具。

Abstract: Traditional disaster analysis and modelling tools for assessing the severity
of a disaster are predictive in nature. Based on the past observational data,
these tools prescribe how the current input state (e.g., environmental
conditions, situation reports) results in a severity assessment. However, these
systems are not meant to be interventional in the causal sense, where the user
can modify the current input state to simulate counterfactual "what-if"
scenarios. In this work, we provide an alternative interventional tool that
complements traditional disaster modelling tools by leveraging real-time data
sources like satellite imagery, news, and social media. Our tool also helps
understand the causal attribution of different factors on the estimated
severity, over any given region of interest. In addition, we provide actionable
recourses that would enable easier mitigation planning. Our source code is
publicly available.

</details>


### [221] [Beyond Regularity: Modeling Chaotic Mobility Patterns for Next Location Prediction](https://arxiv.org/abs/2509.11713)
*Yuqian Wu,Yuhong Peng,Jiapeng Yu,Xiangyu Liu,Zeting Yan,Kang Lin,Weifeng Su,Bingqing Qu,Raymond Lee,Dingqi Yang*

Main category: cs.LG

TL;DR: 提出CANOE模型用于下一位置预测，解决现有方法问题，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法未解决周期性和混沌移动模式的动态不平衡，且未充分利用上下文线索。

Method: 提出CANOE模型，引入混沌神经振荡注意力机制，采用三对交互编码器和跨上下文注意力解码器。

Result: 在两个真实数据集上，CANOE显著优于大量基线模型，不同情况下比最佳基线提升3.17%-13.11%，能对不同混沌程度轨迹进行稳健预测。

Conclusion: CANOE模型有效解决现有方法问题，提升下一位置预测性能，消融实验支持关键设计。

Abstract: Next location prediction is a key task in human mobility analysis, crucial
for applications like smart city resource allocation and personalized
navigation services. However, existing methods face two significant challenges:
first, they fail to address the dynamic imbalance between periodic and chaotic
mobile patterns, leading to inadequate adaptation over sparse trajectories;
second, they underutilize contextual cues, such as temporal regularities in
arrival times, which persist even in chaotic patterns and offer stronger
predictability than spatial forecasts due to reduced search spaces. To tackle
these challenges, we propose \textbf{\method}, a
\underline{\textbf{C}}h\underline{\textbf{A}}otic \underline{\textbf{N}}eural
\underline{\textbf{O}}scillator n\underline{\textbf{E}}twork for next location
prediction, which introduces a biologically inspired Chaotic Neural Oscillatory
Attention mechanism to inject adaptive variability into traditional attention,
enabling balanced representation of evolving mobility behaviors, and employs a
Tri-Pair Interaction Encoder along with a Cross Context Attentive Decoder to
fuse multimodal ``who-when-where'' contexts in a joint framework for enhanced
prediction performance. Extensive experiments on two real-world datasets
demonstrate that CANOE consistently and significantly outperforms a sizeable
collection of state-of-the-art baselines, yielding 3.17\%-13.11\% improvement
over the best-performing baselines across different cases. In particular, CANOE
can make robust predictions over mobility trajectories of different mobility
chaotic levels. A series of ablation studies also supports our key design
choices. Our code is available at: https://github.com/yuqian2003/CANOE.

</details>


### [222] [DRAG: Data Reconstruction Attack using Guided Diffusion](https://arxiv.org/abs/2509.11724)
*Wa-Kin Lei,Jun-Cheng Chen,Shang-Tse Chen*

Main category: cs.LG

TL;DR: 提出基于引导扩散的数据重建攻击，用于拆分推理场景下视觉基础模型，实验表明效果超现有方法，凸显大模型隐私保护机制的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 现有数据重建攻击多针对小CNN分类模型，基础模型在拆分推理场景下的隐私风险未充分研究。

Method: 提出基于引导扩散的新型数据重建攻击，利用预训练的潜在扩散模型的先验知识，对其学习的图像先验进行迭代重建。

Result: 实验表明该方法在重建视觉基础模型深层中间表示数据方面，在定性和定量上均显著优于现有方法。

Conclusion: 拆分推理场景下的大型模型迫切需要更强大的隐私保护机制。

Abstract: With the rise of large foundation models, split inference (SI) has emerged as
a popular computational paradigm for deploying models across lightweight edge
devices and cloud servers, addressing data privacy and computational cost
concerns. However, most existing data reconstruction attacks have focused on
smaller CNN classification models, leaving the privacy risks of foundation
models in SI settings largely unexplored. To address this gap, we propose a
novel data reconstruction attack based on guided diffusion, which leverages the
rich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on
a large-scale dataset. Our method performs iterative reconstruction on the
LDM's learned image prior, effectively generating high-fidelity images
resembling the original data from their intermediate representations (IR).
Extensive experiments demonstrate that our approach significantly outperforms
state-of-the-art methods, both qualitatively and quantitatively, in
reconstructing data from deep-layer IRs of the vision foundation model. The
results highlight the urgent need for more robust privacy protection mechanisms
for large models in SI scenarios. Code is available at:
https://github.com/ntuaislab/DRAG.

</details>


### [223] [Fast and Interpretable Machine Learning Modelling of Atmospheric Molecular Clusters](https://arxiv.org/abs/2509.11728)
*Lauri Seppäläinen,Jakub Kubečka,Jonas Elm,Kai Puolamäki*

Main category: cs.LG

TL;DR: 提出k - 近邻（k - NN）回归模型用于大气分子簇研究，对比KRR模型，展示其优势。


<details>
  <summary>Details</summary>
Motivation: 量子化学计算成本高，限制大气分子簇大规模探索，需新方法解决气候建模中气溶胶粒子形成的不确定性。

Method: 利用化学信息距离度量构建k - NN回归模型，与KRR模型对比，使用FCHL19等分子描述符。

Result: k - NN模型在准确性上可与KRR媲美，计算时间大幅减少，在多个数据集达近化学精度，能扩展到大数据集，外推误差小。

Conclusion: k - NN模型具有内置可解释性和不确定性估计，是加速大气化学等领域发现的有力工具。

Abstract: Understanding how atmospheric molecular clusters form and grow is key to
resolving one of the biggest uncertainties in climate modelling: the formation
of new aerosol particles. While quantum chemistry offers accurate insights into
these early-stage clusters, its steep computational costs limit large-scale
exploration. In this work, we present a fast, interpretable, and surprisingly
powerful alternative: $k$-nearest neighbour ($k$-NN) regression model. By
leveraging chemically informed distance metrics, including a kernel-induced
metric and one learned via metric learning for kernel regression (MLKR), we
show that simple $k$-NN models can rival more complex kernel ridge regression
(KRR) models in accuracy, while reducing computational time by orders of
magnitude. We perform this comparison with the well-established
Faber-Christensen-Huang-Lilienfeld (FCHL19) molecular descriptor, but other
descriptors (e.g., FCHL18, MBDF, and CM) can be shown to have similar
performance. Applied to both simple organic molecules in the QM9 benchmark set
and large datasets of atmospheric molecular clusters (sulphuric acid-water and
sulphuric-multibase -base systems), our $k$-NN models achieve near-chemical
accuracy, scale seamlessly to datasets with over 250,000 entries, and even
appears to extrapolate to larger unseen clusters with minimal error (often
nearing 1 kcal/mol). With built-in interpretability and straightforward
uncertainty estimation, this work positions $k$-NN as a potent tool for
accelerating discovery in atmospheric chemistry and beyond.

</details>


### [224] [Data Fusion and Machine Learning for Ship Fuel Consumption Modelling -- A Case of Bulk Carrier Vessel](https://arxiv.org/abs/2509.11750)
*Abdella Mohamed,Xiangyu Hu,Christian Hendricks*

Main category: cs.LG

TL;DR: 研究利用散货船航行报告和多源数据，评估融合外部数据对船舶燃油消耗建模准确性的影响，发现机器学习结合多类数据预测燃油消耗潜力大，但需更多验证。


<details>
  <summary>Details</summary>
Motivation: 国际海事组织推动减少船舶燃油消耗和碳排放，需准确预测燃油消耗，评估融合外部数据能否提高建模准确性。

Method: 收集散货船一年296份航行报告和28个参数，整合CMEMS和ECMWF的水文气象大数据。

Result: 机器学习技术结合航行报告与气候、海洋数据，有很强潜力准确预测船舶燃油消耗。

Conclusion: 虽有预测潜力，但需在同类船舶上验证以确认通用性。

Abstract: There is an increasing push for operational measures to reduce ships' bunker
fuel consumption and carbon emissions, driven by the International Maritime
Organization (IMO) mandates. Key performance indicators such as the Energy
Efficiency Operational Indicator (EEOI) focus on fuel efficiency. Strategies
like trim optimization, virtual arrival, and green routing have emerged. The
theoretical basis for these approaches lies in accurate prediction of fuel
consumption as a function of sailing speed, displacement, trim, climate, and
sea state. This study utilized 296 voyage reports from a bulk carrier vessel
over one year (November 16, 2021 to November 21, 2022) and 28 parameters,
integrating hydrometeorological big data from the Copernicus Marine Environment
Monitoring Service (CMEMS) with 19 parameters and the European Centre for
Medium-Range Weather Forecasts (ECMWF) with 61 parameters. The objective was to
evaluate whether fusing external public data sources enhances modeling accuracy
and to highlight the most influential parameters affecting fuel consumption.
The results reveal a strong potential for machine learning techniques to
predict ship fuel consumption accurately by combining voyage reports with
climate and sea data. However, validation on similar classes of vessels remains
necessary to confirm generalizability.

</details>


### [225] [Stabilizing PINNs: A regularization scheme for PINN training to avoid unstable fixed points of dynamical systems](https://arxiv.org/abs/2509.11768)
*Milos Babic,Franz M. Rohrhofer,Bernhard C. Geiger*

Main category: cs.LG

TL;DR: 本文基于稳定性理论提出正则化方案，避免物理上错误的解，提高物理神经网络（PINNs）训练成功率。


<details>
  <summary>Details</summary>
Motivation: PINNs训练损失函数在动力系统不动点解处存在局部极小值，干扰训练，可能导致物理上错误的解。

Method: 基于稳定性理论，提出一种对不稳定不动点对应解进行惩罚的正则化方案。

Result: 在四个动力系统（包括Lotka - Volterra模型和van der Pol振荡器）上的实验表明，该方案有助于避免物理上错误的解，大幅提高PINNs训练成功率。

Conclusion: 所提出的正则化方案能有效避免PINNs训练中物理上错误的解，提升训练成功率。

Abstract: It was recently shown that the loss function used for training
physics-informed neural networks (PINNs) exhibits local minima at solutions
corresponding to fixed points of dynamical systems. In the forward setting,
where the PINN is trained to solve initial value problems, these local minima
can interfere with training and potentially leading to physically incorrect
solutions. Building on stability theory, this paper proposes a regularization
scheme that penalizes solutions corresponding to unstable fixed points.
Experimental results on four dynamical systems, including the Lotka-Volterra
model and the van der Pol oscillator, show that our scheme helps avoiding
physically incorrect solutions and substantially improves the training success
rate of PINNs.

</details>


### [226] [Multimodal Regression for Enzyme Turnover Rates Prediction](https://arxiv.org/abs/2509.11782)
*Bozhen Hu,Cheng Tan,Siyuan Li,Jiangbin Zheng,Sizhe Qiu,Jun Xia,Stan Z. Li*

Main category: cs.LG

TL;DR: 提出多模态框架预测酶周转率，实验表明优于传统和先进深度学习方法，为酶动力学研究提供工具。


<details>
  <summary>Details</summary>
Motivation: 酶周转率是酶动力学基本参数，但多数生物中因实验测量成本高且复杂，数据稀缺。

Method: 结合预训练语言模型和卷积神经网络从蛋白质序列提取特征，用图神经网络从底物分子获取信息表示，引入注意力机制增强酶和底物表示间的相互作用，利用Kolmogorov - Arnold网络的符号回归学习控制酶周转率的数学公式。

Result: 框架在实验中优于传统和最先进的深度学习方法。

Conclusion: 该工作为酶动力学研究提供了强大工具，在酶工程、生物技术和工业生物催化等领域有应用前景。

Abstract: The enzyme turnover rate is a fundamental parameter in enzyme kinetics,
reflecting the catalytic efficiency of enzymes. However, enzyme turnover rates
remain scarce across most organisms due to the high cost and complexity of
experimental measurements. To address this gap, we propose a multimodal
framework for predicting the enzyme turnover rate by integrating enzyme
sequences, substrate structures, and environmental factors. Our model combines
a pre-trained language model and a convolutional neural network to extract
features from protein sequences, while a graph neural network captures
informative representations from substrate molecules. An attention mechanism is
incorporated to enhance interactions between enzyme and substrate
representations. Furthermore, we leverage symbolic regression via
Kolmogorov-Arnold Networks to explicitly learn mathematical formulas that
govern the enzyme turnover rate, enabling interpretable and accurate
predictions. Extensive experiments demonstrate that our framework outperforms
both traditional and state-of-the-art deep learning approaches. This work
provides a robust tool for studying enzyme kinetics and holds promise for
applications in enzyme engineering, biotechnology, and industrial biocatalysis.

</details>


### [227] [Watch Your Step: A Cost-Sensitive Framework for Accelerometer-Based Fall Detection in Real-World Streaming Scenarios](https://arxiv.org/abs/2509.11789)
*Timilehin B. Aderinola,Luca Palmerini,Ilaria D'Ascanio,Lorenzo Chiari,Jochen Klenk,Clemens Becker,Brian Caulfield,Georgiana Ifrim*

Main category: cs.LG

TL;DR: 本文提出无先验知识的实时跌倒检测框架，用IMU数据和高效分类器，引入成本敏感学习策略，在FARSEEING数据集表现好，证明该策略增强检测鲁棒性，框架适用于实时可穿戴监测系统。


<details>
  <summary>Details</summary>
Motivation: 现有实时跌倒检测方法依赖模拟数据或先验知识，缺乏高效计算和适用评估指标，限制了实际应用。

Method: 使用FARSEEING数据集的IMU数据，用高效分类器计算跌倒概率，引入成本敏感学习策略调整决策阈值。

Result: 框架在FARSEEING上召回率1.00、精确率0.84、F1分数0.91，检测到所有跌倒且误报低，平均推理时间低于5ms/样本。

Conclusion: 成本敏感阈值调整增强了基于加速度计的跌倒检测的鲁棒性，框架适用于实时可穿戴传感器系统的连续监测。

Abstract: Real-time fall detection is crucial for enabling timely interventions and
mitigating the severe health consequences of falls, particularly in older
adults. However, existing methods often rely on simulated data or assumptions
such as prior knowledge of fall events, limiting their real-world
applicability. Practical deployment also requires efficient computation and
robust evaluation metrics tailored to continuous monitoring. This paper
presents a real-time fall detection framework for continuous monitoring without
prior knowledge of fall events. Using over 60 hours of inertial measurement
unit (IMU) data from the FARSEEING real-world falls dataset, we employ recent
efficient classifiers to compute fall probabilities in streaming mode. To
enhance robustness, we introduce a cost-sensitive learning strategy that tunes
the decision threshold using a cost function reflecting the higher risk of
missed falls compared to false alarms. Unlike many methods that achieve high
recall only at the cost of precision, our framework achieved Recall of 1.00,
Precision of 0.84, and an F1 score of 0.91 on FARSEEING, detecting all falls
while keeping false alarms low, with average inference time below 5 ms per
sample. These results demonstrate that cost-sensitive threshold tuning enhances
the robustness of accelerometer-based fall detection. They also highlight the
potential of our computationally efficient framework for deployment in
real-time wearable sensor systems for continuous monitoring.

</details>


### [228] [Visualization and Analysis of the Loss Landscape in Graph Neural Networks](https://arxiv.org/abs/2509.11792)
*Samir Moustafa,Lorenz Kummer,Simon Fetzel,Nils M. Kriege,Wilfried N. Gansterer*

Main category: cs.LG

TL;DR: 提出可学习降维方法可视化GNN损失景观，分析多种因素对GNN优化的影响，为架构和策略设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 当前对GNN参数优化、表达能力和泛化能力之间的相互作用理解不足。

Method: 引入可学习的降维方法可视化GNN损失景观，分析过平滑、跳跃知识等因素对GNN优化的影响。

Result: 可学习投影方法优于基于PCA的方法，能以更低内存准确重构高维参数；架构、稀疏化和优化器预条件显著影响GNN优化景观、训练过程和预测性能。

Conclusion: 研究结果有助于开发更高效的GNN架构设计和训练策略。

Abstract: Graph Neural Networks (GNNs) are powerful models for graph-structured data,
with broad applications. However, the interplay between GNN parameter
optimization, expressivity, and generalization remains poorly understood. We
address this by introducing an efficient learnable dimensionality reduction
method for visualizing GNN loss landscapes, and by analyzing the effects of
over-smoothing, jumping knowledge, quantization, sparsification, and
preconditioner on GNN optimization. Our learnable projection method surpasses
the state-of-the-art PCA-based approach, enabling accurate reconstruction of
high-dimensional parameters with lower memory usage. We further show that
architecture, sparsification, and optimizer's preconditioning significantly
impact the GNN optimization landscape and their training process and final
prediction performance. These insights contribute to developing more efficient
designs of GNN architectures and training strategies.

</details>


### [229] [Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning](https://arxiv.org/abs/2509.11816)
*Filip Sondej,Yushi Yang*

Main category: cs.LG

TL;DR: 分析现有语言模型去学习技术的问题，提出高选择性技术，在Llama - 3.1 - 8B上实验效果好。


<details>
  <summary>Details</summary>
Motivation: 现有去学习技术和安全训练无法有效从语言模型中移除危险知识。

Method: 对激活和模块输出梯度进行PCA，识别包含共同表示的子空间并在计算去学习更新前将其坍缩。

Result: 在Llama - 3.1 - 8B上对WMDP数据集事实进行去学习时，在生物和网络危险事实攻击后准确率下降效果远超基线，对通用性能影响小，且每个事实所需GPU时间少。

Conclusion: 提出的高选择性技术能稳健地进行去学习，且不破坏通用性能。

Abstract: Current unlearning techniques and safety training consistently fail to remove
dangerous knowledge from language models. We analyze the root causes and
propose a highly selective technique which unlearns robustly and without
disrupting general performance.
  We perform PCA on activations and module output gradients to identify
subspaces containing common representations, and collapse them before
calculating unlearning updates. This way we avoid unlearning general
representations, and only target those specific to the unlearned facts.
  When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack
accuracy 80x more than our best baseline (Circuit Breakers) on biohazardous
facts and 30x more on cyberhazardous facts. Despite this, we disrupt general
performance 30x less (only 0.1% WikiText loss increase), while requiring less
than 3 GPU-seconds per fact.

</details>


### [230] [FedDAF: Federated Domain Adaptation Using Model Functional Distance](https://arxiv.org/abs/2509.11819)
*Mrinmay Sen,Ankita Das,Sidhant Nair,C Krishna Mohan*

Main category: cs.LG

TL;DR: 本文提出FedDAF解决FDA中领域偏移和目标数据稀缺的问题，通过计算模型功能距离进行聚合，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有FDA方法大多仅关注领域偏移，忽略数据稀缺问题或未按目标优先共享信息，本文旨在解决FDA中领域偏移和目标数据稀缺的双重挑战。

Method: 提出FedDAF方法，通过计算目标数据上平均梯度场的模型功能距离，基于目标数据构建目标函数进行有效模型聚合，计算平均梯度场夹角并用Gompertz函数归一化，服务器用简单平均聚合本地源模型。

Result: 在真实数据集实验中，FedDAF在测试准确率上优于现有FL、PFL和FDA方法。

Conclusion: FedDAF能有效解决FDA中领域偏移和目标数据稀缺的问题，性能优于现有方法。

Abstract: Federated Domain Adaptation (FDA) is a federated learning (FL) approach that
improves model performance at the target client by collaborating with source
clients while preserving data privacy. FDA faces two primary challenges: domain
shifts between source and target data and limited labeled data at the target.
Most existing FDA methods focus on domain shifts, assuming ample target data,
yet often neglect the combined challenges of both domain shifts and data
scarcity. Moreover, approaches that address both challenges fail to prioritize
sharing relevant information from source clients according to the target's
objective. In this paper, we propose FedDAF, a novel approach addressing both
challenges in FDA. FedDAF uses similarity-based aggregation of the global
source model and target model by calculating model functional distance from
their mean gradient fields computed on target data. This enables effective
model aggregation based on the target objective, constructed using target data,
even with limited data. While computing model functional distance between these
two models, FedDAF computes the angle between their mean gradient fields and
then normalizes with the Gompertz function. To construct the global source
model, all the local source models are aggregated using simple average in the
server. Experiments on real-world datasets demonstrate FedDAF's superiority
over existing FL, PFL, and FDA methods in terms of achieving better test
accuracy.

</details>


### [231] [Transparent and Fair Profiling in Employment Services: Evidence from Switzerland](https://arxiv.org/abs/2509.11847)
*Tim Räz*

Main category: cs.LG

TL;DR: 本文用瑞士行政数据对比传统统计、可解释和黑箱模型预测长期失业风险，发现可解释提升机表现接近最佳黑箱模型，且可在不牺牲太多性能下增强透明度和公平性。


<details>
  <summary>Details</summary>
Motivation: 长期失业对求职者和公共就业服务是挑战，现有一些不透明的黑箱机器学习模型用于预测长期失业风险，存在透明度和公平性问题，需研究可解释模型是否可作为替代。

Method: 使用瑞士行政数据，对比传统统计、可解释和黑箱模型在预测性能、可解释性和公平性方面的表现。

Result: 可解释提升机作为近期可解释模型，表现接近最佳黑箱模型，模型稀疏性、特征平滑和公平性缓解可在性能损失不大的情况下增强透明度和公平性。

Conclusion: 可解释的预测模型在不影响性能的情况下，可作为黑箱模型的可靠替代方案。

Abstract: Long-term unemployment (LTU) is a challenge for both jobseekers and public
employment services. Statistical profiling tools are increasingly used to
predict LTU risk. Some profiling tools are opaque, black-box machine learning
models, which raise issues of transparency and fairness. This paper
investigates whether interpretable models could serve as an alternative, using
administrative data from Switzerland. Traditional statistical, interpretable,
and black-box models are compared in terms of predictive performance,
interpretability, and fairness. It is shown that explainable boosting machines,
a recent interpretable model, perform nearly as well as the best black-box
models. It is also shown how model sparsity, feature smoothing, and fairness
mitigation can enhance transparency and fairness with only minor losses in
performance. These findings suggest that interpretable profiling provides an
accountable and trustworthy alternative to black-box models without
compromising performance.

</details>


### [232] [TabStruct: Measuring Structural Fidelity of Tabular Data](https://arxiv.org/abs/2509.11950)
*Xiangjian Jiang,Nikola Simidjievski,Mateja Jamnik*

Main category: cs.LG

TL;DR: 提出新评估框架，引入全局效用指标，用TabStruct基准对表格生成器评估，结果表明全局效用可评估性能，发布相关套件和代码。


<details>
  <summary>Details</summary>
Motivation: 现有表格生成器评估基准忽略结构保真度与传统评估维度的相互作用，且多限于玩具数据集，缺乏对真实数据集的评估。

Method: 提出联合考虑结构保真度和传统评估维度的新评估框架，引入全局效用指标，构建TabStruct综合评估基准。

Result: 全局效用为表格生成器性能提供了与任务和领域无关的评估视角。

Conclusion: 新的评估框架和指标能有效评估表格生成器，发布的TabStruct基准套件有助于该领域研究。

Abstract: Evaluating tabular generators remains a challenging problem, as the unique
causal structural prior of heterogeneous tabular data does not lend itself to
intuitive human inspection. Recent work has introduced structural fidelity as a
tabular-specific evaluation dimension to assess whether synthetic data complies
with the causal structures of real data. However, existing benchmarks often
neglect the interplay between structural fidelity and conventional evaluation
dimensions, thus failing to provide a holistic understanding of model
performance. Moreover, they are typically limited to toy datasets, as
quantifying existing structural fidelity metrics requires access to
ground-truth causal structures, which are rarely available for real-world
datasets. In this paper, we propose a novel evaluation framework that jointly
considers structural fidelity and conventional evaluation dimensions. We
introduce a new evaluation metric, $\textbf{global utility}$, which enables the
assessment of structural fidelity even in the absence of ground-truth causal
structures. In addition, we present $\textbf{TabStruct}$, a comprehensive
evaluation benchmark offering large-scale quantitative analysis on 13 tabular
generators from nine distinct categories, across 29 datasets. Our results
demonstrate that global utility provides a task-independent, domain-agnostic
lens for tabular generator performance. We release the TabStruct benchmark
suite, including all datasets, evaluation pipelines, and raw results. Code is
available at https://github.com/SilenceX12138/TabStruct.

</details>


### [233] [Deep operator network for surrogate modeling of poroelasticity with random permeability fields](https://arxiv.org/abs/2509.11966)
*Sangjoon Park,Yeonjong Shin,Jinhyun Choo*

Main category: cs.LG

TL;DR: 本文提出基于DeepONet的代理建模框架用于随机渗透率场的孔隙弹性模拟，加速推理并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 孔隙弹性模拟中随机渗透率场的重复正向求解成本高，且缺乏高效代理建模技术。

Method: 提出基于DeepONet的代理建模框架，集成无量纲化、输入降维和两步训练策略。

Result: 在土壤固结和地面沉降两个基准问题中，DeepONet推理加速且在不同渗透率统计下保持高预测精度。

Conclusion: 该方法是随机渗透率场孔隙弹性系统可扩展且高效的代理建模技术。

Abstract: Poroelasticity -- coupled fluid flow and elastic deformation in porous media
-- often involves spatially variable permeability, especially in subsurface
systems. In such cases, simulations with random permeability fields are widely
used for probabilistic analysis, uncertainty quantification, and inverse
problems. These simulations require repeated forward solves that are often
prohibitively expensive, motivating the development of efficient surrogate
models. However, efficient surrogate modeling techniques for poroelasticity
with random permeability fields remain scarce. In this study, we propose a
surrogate modeling framework based on the deep operator network (DeepONet), a
neural architecture designed to learn mappings between infinite-dimensional
function spaces. The proposed surrogate model approximates the solution
operator that maps random permeability fields to transient poroelastic
responses. To enhance predictive accuracy and stability, we integrate three
strategies: nondimensionalization of the governing equations, input
dimensionality reduction via Karhunen--Lo\'eve expansion, and a two-step
training procedure that decouples the optimization of branch and trunk
networks. The methodology is evaluated on two benchmark problems in
poroelasticity: soil consolidation and ground subsidence induced by groundwater
extraction. In both cases, the DeepONet achieves substantial speedup in
inference while maintaining high predictive accuracy across a wide range of
permeability statistics. These results highlight the potential of the proposed
approach as a scalable and efficient surrogate modeling technique for
poroelastic systems with random permeability fields.

</details>


### [234] [MillStone: How Open-Minded Are LLMs?](https://arxiv.org/abs/2509.11967)
*Harold Triedman,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: 提出MillStone基准衡量外部论点对大语言模型在争议问题上立场的影响，发现模型多数情况下较开明，信息源影响大。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型开始取代传统搜索引擎，用户依赖其获取争议问题信息，需了解模型输出立场受信息源影响情况。

Method: 提出MillStone基准并应用于九个领先的大语言模型，测量其对相反观点的开放性、模型间一致性、最具说服力的论点等。

Result: 大语言模型在大多数问题上比较开明，权威信息源易改变其立场。

Conclusion: 强调了信息源选择的重要性以及基于大语言模型的信息检索和搜索系统被操纵的风险。

Abstract: Large language models equipped with Web search, information retrieval tools,
and other agentic capabilities are beginning to supplant traditional search
engines. As users start to rely on LLMs for information on many topics,
including controversial and debatable issues, it is important to understand how
the stances and opinions expressed in LLM outputs are influenced by the
documents they use as their information sources.
  In this paper, we present MillStone, the first benchmark that aims to
systematically measure the effect of external arguments on the stances that
LLMs take on controversial issues (not all of them political). We apply
MillStone to nine leading LLMs and measure how ``open-minded'' they are to
arguments supporting opposite sides of these issues, whether different LLMs
agree with each other, which arguments LLMs find most persuasive, and whether
these arguments are the same for different LLMs.
  In general, we find that LLMs are open-minded on most issues. An
authoritative source of information can easily sway an LLM's stance,
highlighting the importance of source selection and the risk that LLM-based
information retrieval and search systems can be manipulated.

</details>


### [235] [Examining the Relationship between Scientific Publishing Activity and Hype-Driven Financial Bubbles: A Comparison of the Dot-Com and AI Eras](https://arxiv.org/abs/2509.11982)
*Aksheytha Chelikavada,Casey C. Bennett*

Main category: cs.LG

TL;DR: 研究利用时序社会网络分析研究互联网泡沫和人工智能时代科研引用网络与金融市场数据关系，发现互联网泡沫模式难以预测人工智能泡沫。


<details>
  <summary>Details</summary>
Motivation: 金融泡沫影响大且难预警，探讨分析科研出版数据预测未来泡沫的可能性。

Method: 利用时序社会网络分析，研究1994 - 2001年互联网时代和2017 - 2024年人工智能时代科研引用网络与金融市场数据关系，还用LSTM、KNN、ARX/GARCH等技术分析。

Result: 互联网时代模式不能明确预测人工智能泡沫，人工智能时代部分科学家发表影响模式与互联网时代相似，数据显示人工智能时代可能无泡沫或有前所未有的泡沫。

Conclusion: 互联网时代模式难以有效应用于人工智能市场。

Abstract: Financial bubbles often arrive without much warning, but create long-lasting
economic effects. For example, during the dot-com bubble, innovative
technologies created market disruptions through excitement for a promised
bright future. Such technologies originated from research where scientists had
developed them for years prior to their entry into the markets. That raises a
question on the possibility of analyzing scientific publishing data (e.g.
citation networks) leading up to a bubble for signals that may forecast the
rise and fall of similar future bubbles. To that end, we utilized temporal SNAs
to detect possible relationships between the publication citation networks of
scientists and financial market data during two modern eras of rapidly shifting
technology: 1) dot-com era from 1994 to 2001 and 2) AI era from 2017 to 2024.
Results showed that the patterns from the dot-com era (which did end in a
bubble) did not definitively predict the rise and fall of an AI bubble. While
yearly citation networks reflected possible changes in publishing behavior of
scientists between the two eras, there was a subset of AI era scientists whose
publication influence patterns mirrored those during the dot-com era. Upon
further analysis using multiple analysis techniques (LSTM, KNN, AR X/GARCH),
the data seems to suggest two possibilities for the AI era: unprecedented form
of financial bubble unseen or that no bubble exists. In conclusion, our
findings imply that the patterns present in the dot-com era do not effectively
translate in such a manner to apply them to the AI market.

</details>


### [236] [Low-rank Orthogonalization for Large-scale Matrix Optimization with Applications to Foundation Model Training](https://arxiv.org/abs/2509.11983)
*Chuan He,Zhanwang Deng,Zhaosong Lu*

Main category: cs.LG

TL;DR: 本文提出低秩正交化方法及低秩矩阵符号梯度下降和低秩版Muon优化器，实验显示低秩正交化性能优越，理论上分析了迭代复杂度。


<details>
  <summary>Details</summary>
Motivation: 神经网络训练是大规模矩阵优化问题，但参数矩阵结构长期被忽视，虽已有利用此结构的Muon优化器，但可进一步挖掘梯度低秩特性。

Method: 提出低秩正交化，基于此提出低秩矩阵符号梯度下降和低秩版Muon。

Result: 低秩正交化性能优越，低秩版Muon在GPT - 2和LLaMA预训练中超越普通Muon。

Conclusion: 低秩正交化及相关方法有效，理论上给出了低秩矩阵符号梯度下降和低秩版Muon在特定条件下的迭代复杂度。

Abstract: Neural network (NN) training is inherently a large-scale matrix optimization
problem, yet the matrix structure of NN parameters has long been overlooked.
Recently, the optimizer Muon \cite{jordanmuon}, which explicitly exploits this
structure, has gained significant attention for its strong performance in
foundation model training. A key component contributing to Muon's success is
matrix orthogonalization. In this paper, we propose {\it low-rank
orthogonalization}, which explicitly leverages the low-rank nature of gradients
during NN training. Building on this, we propose low-rank matrix-signed
gradient descent and a low-rank variant of Muon. Our numerical experiments
demonstrate the superior performance of low-rank orthogonalization, with the
low-rank Muon achieving promising results in GPT-2 and LLaMA pretraining --
surpassing the performance of the carefully tuned vanilla Muon. Theoretically,
we establish the iteration complexity of the low-rank matrix-signed gradient
descent for finding an approximate stationary solution, as well as that of
low-rank Muon for finding an approximate stochastic stationary solution under
heavy-tailed noise.

</details>


### [237] [Learning from Uncertain Similarity and Unlabeled Data](https://arxiv.org/abs/2509.11984)
*Meng Wei,Zhongnian Li,Peng Ying,Xinzheng Xu*

Main category: cs.LG

TL;DR: 提出USimUL框架处理隐私风险，给出无偏风险估计器，实验表明性能优。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似度的弱监督学习方法依赖精确相似度标注，存在隐私风险。

Method: 提出Uncertain Similarity and Unlabeled Learning (USimUL)框架，嵌入不确定性组件；提出无偏风险估计器并证明其收敛率。

Result: 在基准和真实数据集上实验显示，该方法比传统基于相似度方法有更优分类性能。

Conclusion: USimUL框架能有效降低标签泄露风险，且分类性能好。

Abstract: Existing similarity-based weakly supervised learning approaches often rely on
precise similarity annotations between data pairs, which may inadvertently
expose sensitive label information and raise privacy risks. To mitigate this
issue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel
framework where each similarity pair is embedded with an uncertainty component
to reduce label leakage. In this paper, we propose an unbiased risk estimator
that learns from uncertain similarity and unlabeled data. Additionally, we
theoretically prove that the estimator achieves statistically optimal
parametric convergence rates. Extensive experiments on both benchmark and
real-world datasets show that our method achieves superior classification
performance compared to conventional similarity-based approaches.

</details>


### [238] [Generalizing Behavior via Inverse Reinforcement Learning with Closed-Form Reward Centroids](https://arxiv.org/abs/2509.12010)
*Filippo Lazzati,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 研究将专家行为泛化到新环境或约束的问题，提出新准则选策略，推导奖励质心表达式并给出估计算法，通过模拟展示方法与专家行为关系。


<details>
  <summary>Details</summary>
Motivation: 解决Inverse Reinforcement Learning（IRL）因存在多个奖励函数导致需决策选策略的问题，将专家行为泛化到新环境和约束。

Method: 提出新颖、有原则的准则，选择可行集有界子集中奖励所诱导的“平均”策略，推导该子集奖励质心的闭式表达式，用专家演示离线数据集估计质心。

Result: 得出可通过该子集奖励质心规划得到策略，给出估计质心的有效算法，通过数值模拟展示方法与专家行为关系。

Conclusion: 所提准则和算法能处理IRL中策略选择问题，为将专家行为泛化到新环境提供有效途径。

Abstract: We study the problem of generalizing an expert agent's behavior, provided
through demonstrations, to new environments and/or additional constraints.
Inverse Reinforcement Learning (IRL) offers a promising solution by seeking to
recover the expert's underlying reward function, which, if used for planning in
the new settings, would reproduce the desired behavior. However, IRL is
inherently ill-posed: multiple reward functions, forming the so-called feasible
set, can explain the same observed behavior. Since these rewards may induce
different policies in the new setting, in the absence of additional
information, a decision criterion is needed to select which policy to deploy.
In this paper, we propose a novel, principled criterion that selects the
"average" policy among those induced by the rewards in a certain bounded subset
of the feasible set. Remarkably, we show that this policy can be obtained by
planning with the reward centroid of that subset, for which we derive a
closed-form expression. We then present a provably efficient algorithm for
estimating this centroid using an offline dataset of expert demonstrations
only. Finally, we conduct numerical simulations that illustrate the
relationship between the expert's behavior and the behavior produced by our
method.

</details>


### [239] [AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models](https://arxiv.org/abs/2509.12019)
*Sangjun Lee,Seung-taek Woo,Jungyu Jin,Changhun Lee,Eunhyeok Park*

Main category: cs.LG

TL;DR: 提出AMQ框架以在严格内存约束下找出最佳大语言模型，通过四项创新克服搜索挑战，实现高效探索并产出紧凑高性能模型。


<details>
  <summary>Details</summary>
Motivation: 为实现大语言模型更广泛部署，需在严格内存约束下找出最佳性能模型。

Method: 提出AMQ框架，通过搜索空间剪枝、量化代理、质量预测器和迭代搜索更新策略克服挑战。

Result: AMQ有效探索质量 - 效率空间，达到帕累托前沿，产出紧凑且高性能的大语言模型。

Conclusion: AMQ框架能在严格内存约束下平衡模型质量和内存使用，实现大语言模型更广泛部署。

Abstract: To enable broader deployment of Large Language Models (LLMs), it is essential
to identify the best-performing model under strict memory constraints. We
present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework
that assigns layer-wise quantization bit-widths to optimally balance model
quality and memory usage. However, the combinatorial search space, with over
10^{100} possible configurations, makes conventional black-box optimization
infeasible. AMQ overcomes this challenge through four key innovations:(1)
search space pruning using prior knowledge to exclude unpromising
configurations, (2) quantization proxy to bypass costly format conversions
during search, (3) quality predictor to minimize evaluation overhead, and (4)
iterative search-and-update strategy for fast and stable convergence. By
integrating these components, AMQ efficiently explores the quality-efficiency
landscape, reaching the Pareto frontier and yielding LLMs that are both compact
and high-performing. Our code is available at https://github.com/dlwns147/amq.

</details>


### [240] [Learning non-Markovian Dynamical Systems with Signature-based Encoders](https://arxiv.org/abs/2509.12022)
*Eliott Pradeleix,Rémy Hosseinkhan-Boucher,Alena Shilova,Onofrio Semeraro,Lionel Mathelin*

Main category: cs.LG

TL;DR: 本文研究使用签名变换编码学习连续时间下的非马尔可夫动力学，集成签名编码方案的模型在合成基准测试中表现优于基于RNN的替代方案。


<details>
  <summary>Details</summary>
Motivation: 神经常微分方程依赖马尔可夫假设，在现实场景有局限性，现有基于RNN的编码器有离散和训练效果差的问题，需新方法捕捉历史依赖。

Method: 使用签名变换作为编码器，将基于签名的编码方案集成到编码器 - 解码器动力学模型中。

Result: 在合成基准测试中，集成签名编码方案的模型测试性能优于基于RNN的替代方案。

Conclusion: 签名变换可作为学习连续时间下非马尔可夫动力学的有效编码器。

Abstract: Neural ordinary differential equations offer an effective framework for
modeling dynamical systems by learning a continuous-time vector field. However,
they rely on the Markovian assumption - that future states depend only on the
current state - which is often untrue in real-world scenarios where the
dynamics may depend on the history of past states. This limitation becomes
especially evident in settings involving the continuous control of complex
systems with delays and memory effects. To capture historical dependencies,
existing approaches often rely on recurrent neural network (RNN)-based
encoders, which are inherently discrete and struggle with continuous modeling.
In addition, they may exhibit poor training behavior. In this work, we
investigate the use of the signature transform as an encoder for learning
non-Markovian dynamics in a continuous-time setting. The signature transform
offers a continuous-time alternative with strong theoretical foundations and
proven efficiency in summarizing multidimensional information in time. We
integrate a signature-based encoding scheme into encoder-decoder dynamics
models and demonstrate that it outperforms RNN-based alternatives in test
performance on synthetic benchmarks.

</details>


### [241] [Imitation Learning as Return Distribution Matching](https://arxiv.org/abs/2509.12026)
*Filippo Lazzati,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 研究通过模仿学习训练风险敏感强化学习代理问题，提出通用公式，引入非马尔可夫策略子类，开发算法并分析性能。


<details>
  <summary>Details</summary>
Motivation: 标准模仿学习只关注专家预期回报，本文要使代理匹配专家风险态度，即回报分布的其他特征。

Method: 提出风险敏感模仿学习通用公式，引入非马尔可夫策略子类，开发RS - BC和RS - KT算法。

Result: RS - KT利用动态信息比RS - BC样本复杂度低，设计的RS - KT变体在专家奖励未知时也有样本效率。

Conclusion: RS - BC和RS - KT算法有样本效率，非马尔可夫策略优于标准样本高效模仿学习算法。

Abstract: We study the problem of training a risk-sensitive reinforcement learning (RL)
agent through imitation learning (IL). Unlike standard IL, our goal is not only
to train an agent that matches the expert's expected return (i.e., its average
performance) but also its risk attitude (i.e., other features of the return
distribution, such as variance). We propose a general formulation of the
risk-sensitive IL problem in which the objective is to match the expert's
return distribution in Wasserstein distance. We focus on the tabular setting
and assume the expert's reward is known. After demonstrating the limited
expressivity of Markovian policies for this task, we introduce an efficient and
sufficiently expressive subclass of non-Markovian policies tailored to it.
Building on this subclass, we develop two provably efficient algorithms, RS-BC
and RS-KT, for solving the problem when the transition model is unknown and
known, respectively. We show that RS-KT achieves substantially lower sample
complexity than RS-BC by exploiting dynamics information. We further
demonstrate the sample efficiency of return distribution matching in the
setting where the expert's reward is unknown by designing an oracle-based
variant of RS-KT. Finally, we complement our theoretical analysis of RS-KT and
RS-BC with numerical simulations, highlighting both their sample efficiency and
the advantages of non-Markovian policies over standard sample-efficient IL
algorithms.

</details>


### [242] [Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework](https://arxiv.org/abs/2509.12043)
*Mayur Patil,Qadeer Ahmed,Shawn Midlam-Mohler*

Main category: cs.LG

TL;DR: 提出结合GNN与ACP框架的交通流量预测模型，实验显示其预测准确性和不确定性量化表现更佳，模拟验证了模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 城市交通的随机性和环境因素使交通流量预测面临挑战，需要能适应多因素影响的模型。

Method: 提出GNN框架，用对数正态分布和CV值构建自适应邻接矩阵，利用天气因素调整边权重；使用ACP框架进行不确定性量化；在SUMO中构建交通场景并进行蒙特卡罗模拟。

Result: 与基线方法相比，模型有更好的预测准确性和不确定性界限；模拟的车辆平均行驶时间在历史数据区间内。

Conclusion: 所提出的模型能有效适应交通随机性和环境变化，具有良好的预测性能和鲁棒性。

Abstract: Traffic flow forecasting is essential for managing congestion, improving
safety, and optimizing various transportation systems. However, it remains a
prevailing challenge due to the stochastic nature of urban traffic and
environmental factors. Better predictions require models capable of
accommodating the traffic variability influenced by multiple dynamic and
complex interdependent factors. In this work, we propose a Graph Neural Network
(GNN) framework to address the stochasticity by leveraging adaptive adjacency
matrices using log-normal distributions and Coefficient of Variation (CV)
values to reflect real-world travel time variability. Additionally, weather
factors such as temperature, wind speed, and precipitation adjust edge weights
and enable GNN to capture evolving spatio-temporal dependencies across traffic
stations. This enhancement over the static adjacency matrix allows the model to
adapt effectively to traffic stochasticity and changing environmental
conditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP)
framework to provide reliable uncertainty quantification, achieving target
coverage while maintaining acceptable prediction intervals. Experimental
results demonstrate that the proposed model, in comparison with baseline
methods, showed better prediction accuracy and uncertainty bounds. We, then,
validate this method by constructing traffic scenarios in SUMO and applying
Monte-Carlo simulation to derive a travel time distribution for a Vehicle Under
Test (VUT) to reflect real-world variability. The simulated mean travel time of
the VUT falls within the intervals defined by INRIX historical data, verifying
the model's robustness.

</details>


### [243] [Hi-DARTS: Hierarchical Dynamically Adapting Reinforcement Trading System](https://arxiv.org/abs/2509.12048)
*Hoon Sagong,Heesu Kim,Hanbeen Hong*

Main category: cs.LG

TL;DR: 提出Hi - DARTS框架解决传统自主交易系统计算效率和市场响应平衡问题，回测表现超基准。


<details>
  <summary>Details</summary>
Motivation: 传统自主交易系统因固定操作频率难以平衡计算效率和市场响应性。

Method: 提出Hi - DARTS分层多智能体强化学习框架，用元智能体分析市场波动并按需激活专业时间框架智能体进行高频或低频交易。

Result: 在2024年1月至2025年5月苹果股票回测中，Hi - DARTS累计回报率25.17%，夏普比率0.75，超被动买入持有策略和标普500ETF表现。

Conclusion: 动态分层智能体可在保持高计算效率的同时实现更优的风险调整后回报。

Abstract: Conventional autonomous trading systems struggle to balance computational
efficiency and market responsiveness due to their fixed operating frequency. We
propose Hi-DARTS, a hierarchical multi-agent reinforcement learning framework
that addresses this trade-off. Hi-DARTS utilizes a meta-agent to analyze market
volatility and dynamically activate specialized Time Frame Agents for
high-frequency or low-frequency trading as needed. During back-testing on AAPL
stock from January 2024 to May 2025, Hi-DARTS yielded a cumulative return of
25.17% with a Sharpe Ratio of 0.75. This performance surpasses standard
benchmarks, including a passive buy-and-hold strategy on AAPL (12.19% return)
and the S&P 500 ETF (SPY) (20.01% return). Our work demonstrates that dynamic,
hierarchical agents can achieve superior risk-adjusted returns while
maintaining high computational efficiency.

</details>


### [244] [Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation in Tomato Crops Using Leaf Spectral Analysis and Machine Learning](https://arxiv.org/abs/2509.12074)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Parastoo Farajpoor,Hamid Jafarbiglu,Mohsen B. Mesgaran*

Main category: cs.LG

TL;DR: 研究用叶级光谱反射率和集成机器学习早期检测番茄寄生杂草分枝列当，在早期检测准确率高，后期因衰老和杂草干扰准确率下降。


<details>
  <summary>Details</summary>
Motivation: 分枝列当威胁番茄生产，需对其进行早期检测。

Method: 在加州林地进行田间试验，跟踪300株番茄植株，用便携式光谱仪获取叶片反射率并预处理，使用集成随机森林、XGBoost、带RBF核的SVM和朴素贝叶斯的模型。

Result: 在585 GDD时准确率达89%，感染和未感染召回率分别为0.86和0.93，后期准确率下降至69%。

Conclusion: 近端传感与集成学习能在冠层症状可见前及时检测列当，支持针对性干预并减少产量损失。

Abstract: Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic
weed that threatens tomato production by extracting nutrients from the host. We
investigate early detection using leaf-level spectral reflectance (400-2500 nm)
and ensemble machine learning. In a field experiment in Woodland, California,
we tracked 300 tomato plants across growth stages defined by growing degree
days (GDD). Leaf reflectance was acquired with a portable spectrometer and
preprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing,
correlation-based band reduction). Clear class differences were observed near
1500 nm and 2000 nm water absorption features, consistent with reduced leaf
water content in infected plants at early stages. An ensemble combining Random
Forest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at
585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy
declined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and
weed interference. Despite the small number of infected plants and
environmental confounders, results show that proximal sensing with ensemble
learning enables timely detection of broomrape before canopy symptoms are
visible, supporting targeted interventions and reduced yield losses.

</details>


### [245] [A Time-Series Foundation Model by Universal Delay Embedding](https://arxiv.org/abs/2509.12080)
*Zijian Wang,Peng Tao,Jifan Shi,Rui Bao,Rui Liu,Luonan Chen*

Main category: cs.LG

TL;DR: 本文提出通用延迟嵌入（UDE）模型用于时间序列预测，结合延迟嵌入和Koopman算子预测，在多个基准和实际数据集上表现优异，具有可扩展性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 革新时间序列预测方法，提供更有效、可解释的预测框架。

Method: 基于Takens嵌入定理，从Hankel矩阵构建二维子空间补丁，将其视为图像用深度学习处理，作为令牌学习自注意力编码器，在潜在空间用有限维Koopman算子进行线性预测。

Result: 在多个基准和实际气候数据集上，均方误差比现有模型平均降低超20%，微调场景泛化性好，学习的动态表示和预测形式可解释性强。

Conclusion: UDE是可扩展、可解释的通用时间序列建模和预测框架，有广泛科学和工业应用价值。

Abstract: This study introduces Universal Delay Embedding (UDE), a pretrained
foundation model designed to revolutionize time-series forecasting through
principled integration of delay embedding representation and Koopman operator
prediction. Leveraging Takens' embedding theorem, UDE as a dynamical
representation of observed data constructs two-dimensional subspace patches
from Hankel matrices, theoretically preserving dynamical and topological
properties of underlying dynamical systems. Such patches are viewed as images,
which can be efficiently processed by exploiting advanced deep learning
technologies. Computationally, these patches further serve as tokens for
learning a self-attention encoder, thus enabling accurate prediction of
nonlinear time-series by a finite-dimensional Koopman operator in a linear
manner in a latent space. Extensive evaluations across various benchmarks and
real-world climate datasets demonstrate over 20% average reduction in mean
squared error versus state-of-the-art foundation models, alongside superior
generalization in fine-tuning scenarios. In particular, the learned dynamical
representations and Koopman operator prediction forms from the patches exhibit
exceptional interpretability, with consistent identification of topologically
informative subspaces and robust encoding of domain-invariant dynamics,
establishing UDE as a scalable, interpretable framework for universal
time-series modeling and forecasting with broad scientific and industrial
applicability.

</details>


### [246] [Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors](https://arxiv.org/abs/2509.12081)
*Anirudha Majumdar*

Main category: cs.LG

TL;DR: 本文提出欺骗机制用于分布外泛化，即欺骗风险最小化（DRM），不依赖测试数据和训练数据划分，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决分布外泛化问题，消除虚假相关性，使模型能泛化到未见领域。

Method: 提出DRM原则，用可微目标同时学习消除分布偏移的特征并最小化特定任务损失。

Result: 在概念漂移数值实验和机器人部署环境协变量漂移模拟的模仿学习场景中证明了DRM的有效性。

Conclusion: DRM是一种有效的分布外泛化机制，无需测试数据和训练数据特定划分。

Abstract: This paper proposes deception as a mechanism for out-of-distribution (OOD)
generalization: by learning data representations that make training data appear
independent and identically distributed (iid) to an observer, we can identify
stable features that eliminate spurious correlations and generalize to unseen
domains. We refer to this principle as deceptive risk minimization (DRM) and
instantiate it with a practical differentiable objective that simultaneously
learns features that eliminate distribution shifts from the perspective of a
detector based on conformal martingales while minimizing a task-specific loss.
In contrast to domain adaptation or prior invariant representation learning
methods, DRM does not require access to test data or a partitioning of training
data into a finite number of data-generating domains. We demonstrate the
efficacy of DRM on numerical experiments with concept shift and a simulated
imitation learning setting with covariate shift in environments that a robot is
deployed in.

</details>


### [247] [Draw a Portrait of Your Graph Data: An Instance-Level Profiling Framework for Graph-Structured Data](https://arxiv.org/abs/2509.12094)
*Tianqi Zhao,Russa Biswas,Megha Khosla*

Main category: cs.LG

TL;DR: 提出NodePro框架用于细粒度诊断图机器学习模型节点行为，揭示模型差异，支持无标签预测可靠性，并在知识图中识别异常节点。


<details>
  <summary>Details</summary>
Motivation: 标准评估指标难以发现图机器学习模型在节点层面的细粒度差异，难以诊断模型失败情况。

Method: 引入NodePro框架，结合数据中心信号和模型中心指标为节点分配可解释的配置文件分数。

Result: 节点配置文件可泛化到未见节点，支持无真实标签的预测可靠性。

Conclusion: NodePro框架在现实场景中有效，能识别结构化知识图中语义不一致或损坏的节点。

Abstract: Graph machine learning models often achieve similar overall performance yet
behave differently at the node level, failing on different subsets of nodes
with varying reliability. Standard evaluation metrics such as accuracy obscure
these fine grained differences, making it difficult to diagnose when and where
models fail. We introduce NodePro, a node profiling framework that enables
fine-grained diagnosis of model behavior by assigning interpretable profile
scores to individual nodes. These scores combine data-centric signals, such as
feature dissimilarity, label uncertainty, and structural ambiguity, with
model-centric measures of prediction confidence and consistency during
training. By aligning model behavior with these profiles, NodePro reveals
systematic differences between models, even when aggregate metrics are
indistinguishable. We show that node profiles generalize to unseen nodes,
supporting prediction reliability without ground-truth labels. Finally, we
demonstrate the utility of NodePro in identifying semantically inconsistent or
corrupted nodes in a structured knowledge graph, illustrating its effectiveness
in real-world settings.

</details>


### [248] [$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.12117)
*Aryaman Reddi,Gabriele Tiboni,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: 本文提出K - Level Policy Gradient (KPG) 方法用于深度多智能体强化学习，理论证明其收敛性，应用于多种算法并在实验中表现优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度多智能体强化学习中演员 - 评论家算法的策略更新未考虑其他智能体在同一更新步骤的更新，导致协调不佳。

Method: 提出KPG方法，递归更新每个智能体的策略以应对其他智能体更新后的策略；将KPG应用于MAPPO、MADDPG和FACMAC等深度多智能体强化学习算法。

Result: 理论证明在一定条件下有限迭代的KPG能单调收敛到局部纳什均衡；在StarCraft II和多智能体MuJoCo实验中，KPG表现优于现有深度多智能体强化学习算法。

Conclusion: KPG方法可有效解决深度多智能体强化学习中的协调问题，加速有效协调策略的发现。

Abstract: Actor-critic algorithms for deep multi-agent reinforcement learning (MARL)
typically employ a policy update that responds to the current strategies of
other agents. While being straightforward, this approach does not account for
the updates of other agents at the same update step, resulting in
miscoordination. In this paper, we introduce the $K$-Level Policy Gradient
(KPG), a method that recursively updates each agent against the updated
policies of other agents, speeding up the discovery of effective coordinated
policies. We theoretically prove that KPG with finite iterates achieves
monotonic convergence to a local Nash equilibrium under certain conditions. We
provide principled implementations of KPG by applying it to the deep MARL
algorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior
performance over existing deep MARL algorithms in StarCraft II and multi-agent
MuJoCo.

</details>


### [249] [Do machine learning climate models work in changing climate dynamics?](https://arxiv.org/abs/2509.12147)
*Maria Conchita Agana Navarro,Geng Li,Theo Wolf,María Pérez-Ortiz*

Main category: cs.LG

TL;DR: 研究系统评估了ML气候模型在不同OOD场景中的表现，揭示性能差异，为可靠应用提供见解。


<details>
  <summary>Details</summary>
Motivation: 气候变化使OOD事件预测重要，但ML模型在分布转移下泛化能力有限且在气候领域研究不足。

Method: 将已有的OOD评估方法应用于气候数据，系统评估先进的基于ML的气候模型。

Result: 在大规模数据集实验中发现不同场景下模型性能有显著差异。

Conclusion: 强调了稳健评估框架的重要性，为ML用于气候风险预测提供可行见解。

Abstract: Climate change is accelerating the frequency and severity of unprecedented
events, deviating from established patterns. Predicting these
out-of-distribution (OOD) events is critical for assessing risks and guiding
climate adaptation. While machine learning (ML) models have shown promise in
providing precise, high-speed climate predictions, their ability to generalize
under distribution shifts remains a significant limitation that has been
underexplored in climate contexts. This research systematically evaluates
state-of-the-art ML-based climate models in diverse OOD scenarios by adapting
established OOD evaluation methodologies to climate data. Experiments on
large-scale datasets reveal notable performance variability across scenarios,
shedding light on the strengths and limitations of current models. These
findings underscore the importance of robust evaluation frameworks and provide
actionable insights to guide the reliable application of ML for climate risk
forecasting.

</details>


### [250] [From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via Adversarial Learning](https://arxiv.org/abs/2509.12176)
*Collin Guo*

Main category: cs.LG

TL;DR: 研究无配对人脸操纵，提出引导的CycleGAN框架，实验表明该方法在多方面优于自编码器。


<details>
  <summary>Details</summary>
Motivation: 人脸合成和操纵需求增长，需要在无配对、未对齐数据集下生成高真实感、保留身份的图像。

Method: 从自编码器基线转向引导的CycleGAN框架，集成谱归一化、身份和感知引导损失以及地标加权循环约束。

Result: 对抗训练的CycleGAN在真实感、感知质量和身份保留方面优于自编码器，循环重建SSIM有竞争力，推理时间实用，在无配对数据集上实现高质量，在配对子集上接近pix2pix。

Conclusion: 引导的、谱归一化的CycleGAN为从自编码器到强大的无配对人脸操纵提供了实用途径。

Abstract: Human face synthesis and manipulation are increasingly important in
entertainment and AI, with a growing demand for highly realistic,
identity-preserving images even when only unpaired, unaligned datasets are
available. We study unpaired face manipulation via adversarial learning, moving
from autoencoder baselines to a robust, guided CycleGAN framework. While
autoencoders capture coarse identity, they often miss fine details. Our
approach integrates spectral normalization for stable training, identity- and
perceptual-guided losses to preserve subject identity and high-level structure,
and landmark-weighted cycle constraints to maintain facial geometry across pose
and illumination changes. Experiments show that our adversarial trained
CycleGAN improves realism (FID), perceptual quality (LPIPS), and identity
preservation (ID-Sim) over autoencoders, with competitive cycle-reconstruction
SSIM and practical inference times, which achieved high quality without paired
datasets and approaching pix2pix on curated paired subsets. These results
demonstrate that guided, spectrally normalized CycleGANs provide a practical
path from autoencoders to robust unpaired face manipulation.

</details>


### [251] [All that structure matches does not glitter](https://arxiv.org/abs/2509.12178)
*Maya M. Martirossyan,Thomas Egg,Philipp Hoellmer,George Karypis,Mark Transtrum,Adrian Roitberg,Mingjie Liu,Richard G. Hennig,Ellad B. Tadmor,Stefano Martiniani*

Main category: cs.LG

TL;DR: 本文审视晶体结构预测任务常用数据集和指标问题，提出修正方法和新评估指标。


<details>
  <summary>Details</summary>
Motivation: 生成模型发展依赖可靠基准和信息丰富的最小数据集，当前存在被忽视的问题需解决。

Method: 分析常见数据集和指标，指出存在的三个关键问题，针对问题提供碳 - 24 数据集修订版本、perov - 5 数据集新划分方式，提出新评估指标 METRe 和 cRMSE。

Result: 提供碳 - 24 数据集修订版本，提出 perov - 5 数据集新划分，给出新评估指标 METRe 和 cRMSE。

Conclusion: 解决了晶体结构预测任务中数据集和评估指标常被忽视的问题，为模型评估设定更合理标准。

Abstract: Generative models for materials, especially inorganic crystals, hold
potential to transform the theoretical prediction of novel compounds and
structures. Advancement in this field depends critically on robust benchmarks
and minimal, information-rich datasets that enable meaningful model evaluation.
This paper critically examines common datasets and reported metrics for a
crystal structure prediction task$\unicode{x2014}$generating the most likely
structures given the chemical composition of a material. We focus on three key
issues: First, materials datasets should contain unique crystal structures; for
example, we show that the widely-utilized carbon-24 dataset only contains
$\approx$40% unique structures. Second, materials datasets should not be split
randomly if polymorphs of many different compositions are numerous, which we
find to be the case for the perov-5 dataset. Third, benchmarks can mislead if
used uncritically, e.g., reporting a match rate metric without considering the
structural variety exhibited by identical building blocks. To address these
oft-overlooked issues, we introduce several fixes. We provide revised versions
of the carbon-24 dataset: one with duplicates removed, one deduplicated and
split by number of atoms $N$, and two containing only identical structures but
with different unit cells. We also propose a new split for the perov-5 dataset
which ensures polymorphs are grouped within each split subset, setting a more
sensible standard for benchmarking model performance. Finally, we present METRe
and cRMSE, new model evaluation metrics that can correct existing issues with
the match rate metric.

</details>


### [252] [Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences](https://arxiv.org/abs/2509.12188)
*Antonin Sulc*

Main category: cs.LG

TL;DR: 引入Event2Vec框架学习离散事件序列表示，有欧氏空间和双曲空间模型，实验验证假设和不同几何空间模型优势。


<details>
  <summary>Details</summary>
Motivation: 受生物和人工系统中神经表征的几何与拓扑结构研究启发，学习离散事件序列表示。

Method: 采用简单的加法循环结构学习可组合、可解释的嵌入，进行理论分析，引入双曲空间模型。

Result: 欧氏空间模型学习的表示收敛到理想加法结构，双曲空间模型适合嵌入树状结构，实验验证假设。

Conclusion: 双曲空间模型在处理层次事件序列上性能更优。

Abstract: The study of neural representations, both in biological and artificial
systems, is increasingly revealing the importance of geometric and topological
structures. Inspired by this, we introduce Event2Vec, a novel framework for
learning representations of discrete event sequences. Our model leverages a
simple, additive recurrent structure to learn composable, interpretable
embeddings. We provide a theoretical analysis demonstrating that, under
specific training objectives, our model's learned representations in a
Euclidean space converge to an ideal additive structure. This ensures that the
representation of a sequence is the vector sum of its constituent events, a
property we term the linear additive hypothesis. To address the limitations of
Euclidean geometry for hierarchical data, we also introduce a variant of our
model in hyperbolic space, which is naturally suited to embedding tree-like
structures with low distortion. We present experiments to validate our
hypothesis and demonstrate the benefits of each geometry, highlighting the
improved performance of the hyperbolic model on hierarchical event sequences.

</details>


### [253] [Dynamic Relational Priming Improves Transformer in Multivariate Time Series](https://arxiv.org/abs/2509.12196)
*Hunjae Lee,Corey Clark*

Main category: cs.LG

TL;DR: 提出prime attention用于多变量时间序列数据，能动态调整token表示，性能优于标准注意力机制。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制使用静态token表示，难以捕捉多变量时间序列数据中不同通道对之间的异质依赖关系。

Method: 提出注意力与动态关系启动（prime attention），通过可学习的调制动态调整每个token以捕捉每个token对的独特关系动态。

Result: prime attention在基准测试中始终优于标准注意力机制，预测准确率最高提高6.5%，使用少40%的序列长度也能达到相当或更优性能。

Conclusion: prime attention具有优越的关系建模能力，能有效提取多变量时间序列中特定关系的信息。

Abstract: Standard attention mechanisms in transformers employ static token
representations that remain unchanged across all pair-wise computations in each
layer. This limits their representational alignment with the potentially
diverse relational dynamics of each token-pair interaction. While they excel in
domains with relatively homogeneous relationships, standard attention's static
relational learning struggles to capture the diverse, heterogeneous
inter-channel dependencies of multivariate time series (MTS) data--where
different channel-pair interactions within a single system may be governed by
entirely different physical laws or temporal dynamics. To better align the
attention mechanism for such domain phenomena, we propose attention with
dynamic relational priming (prime attention). Unlike standard attention where
each token presents an identical representation across all of its pair-wise
interactions, prime attention tailors each token dynamically (or per
interaction) through learnable modulations to best capture the unique
relational dynamics of each token pair, optimizing each pair-wise interaction
for that specific relationship. This representational plasticity of prime
attention enables effective extraction of relationship-specific information in
MTS while maintaining the same asymptotic computational complexity as standard
attention. Our results demonstrate that prime attention consistently
outperforms standard attention across benchmarks, achieving up to 6.5\%
improvement in forecasting accuracy. In addition, we find that prime attention
achieves comparable or superior performance using up to 40\% less sequence
length compared to standard attention, further demonstrating its superior
relational modeling capabilities.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [254] [Predator-Prey Model: Driven Hunt for Accelerated Grokking](https://arxiv.org/abs/2509.10562)
*I. A. Lopatin,S. V. Kozyrev,A. N. Pechen*

Main category: cs.NE

TL;DR: 提出用模拟捕食者和猎物生物行为的双智能体机器学习方法，可避免沟壑景观优化陷入困境，在部分问题上学习速度比标准程序快百倍。


<details>
  <summary>Details</summary>
Motivation: 解决沟壑景观优化中易陷入沟壑的问题，提升学习速度。

Method: 使用模拟捕食者和猎物生物行为的两个智能体，二者相互作用进行景观优化。

Result: 在沟壑景观优化中避免陷入沟壑，在一些示例问题上学习速度比标准程序快百倍。

Conclusion: 所提出的驱动狩猎方法在沟壑景观优化和学习速度提升上有显著效果。

Abstract: A machine learning method is proposed using two agents that simulate the
biological behavior of a predator and a prey. In this method, the predator and
the prey interact with each other - the predator chases the prey while the prey
runs away from the predator - to perform an optimization on the landscape. This
method allows, for the case of a ravine landscape (i.e., a landscape with
narrow ravines and with gentle slopes along the ravines) to avoid getting
optimization stuck in the ravine. For this, in the optimization over a ravine
landscape the predator drives the prey along the ravine. Thus we also call this
approach, for the case of ravine landscapes, the driven hunt method. For some
examples of grokking (i.e., delayed generalization) problems we show that this
method allows for achieving up to a hundred times faster learning compared to
the standard learning procedure.

</details>


### [255] [Deep Reinforcement Learning-Assisted Component Auto-Configuration of Differential Evolution Algorithm for Constrained Optimization: A Foundation Model](https://arxiv.org/abs/2509.11016)
*Xu Yang,Rui Wang,Kaiwen Li,Wenhua Li,Ling Wang*

Main category: cs.NE

TL;DR: 为解决差分进化算法在约束优化问题上适应性局限，提出基于深度强化学习的SuperDE框架，实验显示其性能优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 传统进化算法适应性有限，在线自适应方法有低效、收敛弱和泛化性差等问题，需新方法解决约束优化问题。

Method: 引入基于深度强化学习的自动化组件配置框架SuperDE，通过元学习离线训练，利用双深度Q网络根据种群状态调整配置策略。

Result: SuperDE在基准测试集上显著优于现有最先进算法，有更好泛化和优化性能。

Conclusion: SuperDE能有效解决约束优化问题，为进化算法自动化配置提供新思路。

Abstract: Despite significant efforts to manually design high-performance evolutionary
algorithms, their adaptability remains limited due to the dynamic and
ever-evolving nature of real-world problems. The "no free lunch" theorem
highlights that no single algorithm performs optimally across all problems.
While online adaptation methods have been proposed, they often suffer from
inefficiency, weak convergence, and limited generalization on constrained
optimization problems (COPs).
  To address these challenges, we introduce a novel framework for automated
component configuration in Differential Evolution (DE) algorithm to address
COPs, powered by Deep Reinforcement Learning (DRL). Specifically, we propose
SuperDE, a foundation model that dynamically configures DE's evolutionary
components based on real-time evolution. Trained offline through meta-learning
across a wide variety of COPs, SuperDE is capable of recommending optimal
per-generation configurations for unseen problems in a zero-shot manner.
Utilizing a Double Deep Q-Network (DDQN), SuperDE adapts its configuration
strategies in response to the evolving population states during optimization.
Experimental results demonstrate that SuperDE significantly outperforms
existing state-of-the-art algorithms on benchmark test suites, achieving
superior generalization and optimization performance.

</details>


### [256] [Application of Machine Learning for Correcting Defect-induced Neuromorphic Circuit Inference Errors](https://arxiv.org/abs/2509.11113)
*Vedant Sawal,Hiu Yung Wong*

Main category: cs.NE

TL;DR: 提出基于机器学习方法纠正全模拟ReRAM神经形态电路中固定故障导致的推理错误，用DTCO框架建模分析六种空间缺陷类型，方法能恢复推理精度损失，有可扩展性和泛化能力，框架可扩展支持实时自适应学习。


<details>
  <summary>Details</summary>
Motivation: 解决全模拟ReRAM神经形态电路中固定故障导致的推理错误问题。

Method: 使用DTCO模拟框架对六种空间缺陷类型建模分析，采用基于电路输出电压训练的轻量级神经网络进行错误纠正。

Result: 能恢复高达35%（从55%到90%）的推理精度损失，小的纠正网络可显著提高电路鲁棒性，方法有泛化能力。

Conclusion: 该方法为边缘和物联网应用中的神经形态系统提供了可扩展、节能的提高良率和可靠性的途径，框架可扩展支持实时自适应学习。

Abstract: This paper presents a machine learning-based approach to correct inference
errors caused by stuck-at faults in fully analog ReRAM-based neuromorphic
circuits. Using a Design-Technology Co-Optimization (DTCO) simulation
framework, we model and analyze six spatial defect types-circular,
circular-complement, ring, row, column, and checkerboard-across multiple layers
of a multi-array neuromorphic architecture. We demonstrate that the proposed
correction method, which employs a lightweight neural network trained on the
circuit's output voltages, can recover up to 35% (from 55% to 90%) inference
accuracy loss in defective scenarios. Our results, based on handwritten digit
recognition tasks, show that even small corrective networks can significantly
improve circuit robustness. This method offers a scalable and energy-efficient
path toward enhanced yield and reliability for neuromorphic systems in edge and
internet-of-things (IoTs) applications. In addition to correcting the specific
defect types used during training, our method also demonstrates the ability to
generalize-achieving reasonable accuracy when tested on different types of
defects not seen during training. The framework can be readily extended to
support real-time adaptive learning, enabling on-chip correction for dynamic or
aging-induced fault profiles.

</details>


### [257] [Time to Play: Simulating Early-Life Animal Dynamics Enhances Robotics Locomotion Discovery](https://arxiv.org/abs/2509.11755)
*Paul Templier,Hannah Janmohamed,David Labonte,Antoine Cully*

Main category: cs.NE

TL;DR: 提出SMOL课程动态调节机器人执行器强度，融入MAP - Elites框架，提升运动行为表现和多样性，还实现SMOL - Human课程并研究其影响。


<details>
  <summary>Details</summary>
Motivation: 动物身体形态发育变化影响运动，而人工代理和机器人常基于静态物理参数训练，受生物肌肉力量个体发育缩放启发。

Method: 提出Scaling Mechanical Output over Lifetime (SMOL)课程，将其集成到MAP - Elites质量多样性框架中，在标准机器人任务中改变扭矩；基于人类总功率输出研究实现SMOL - Human课程。

Result: SMOL课程在不同控制场景下持续提升运动行为的表现和多样性，使智能体利用有利物理条件发现技能。

Conclusion: SMOL及SMOL - Human课程对机器人运动有积极影响，可借鉴生物发育特点改进机器人训练。

Abstract: Developmental changes in body morphology profoundly shape locomotion in
animals, yet artificial agents and robots are typically trained under static
physical parameters. Inspired by ontogenetic scaling of muscle power in
biology, we propose Scaling Mechanical Output over Lifetime (SMOL), a novel
curriculum that dynamically modulates robot actuator strength to mimic natural
variations in power-to-weight ratio during growth and ageing. Integrating SMOL
into the MAP-Elites quality-diversity framework, we vary the torque in standard
robotics tasks to mimic the evolution of strength in animals as they grow up
and as their body changes. Through comprehensive empirical evaluation, we show
that the SMOL schedule consistently elevates both performance and diversity of
locomotion behaviours across varied control scenarios, by allowing agents to
leverage advantageous physics early on to discover skills that act as stepping
stones when they reach their final standard body properties. Based on studies
of the total power output in humans, we also implement the SMOL-Human schedule
that models isometric body variations due to non-linear changes like puberty,
and study its impact on robotics locomotion.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [258] [Quality Assessment of Tabular Data using Large Language Models and Code Generation](https://arxiv.org/abs/2509.10572)
*Ashlesha Akella,Akshar Kaul,Krishnasuri Narayanam,Sameep Mehta*

Main category: cs.SE

TL;DR: 提出结合统计离群检测与大语言模型驱动规则和代码生成的三阶段框架，评估证明有效。


<details>
  <summary>Details</summary>
Motivation: 基于规则的数据验证在处理表格数据集时存在效率低、需人工干预和计算成本高等问题。

Method: 采用三阶段框架，先传统聚类过滤数据样本，再用大语言模型生成规则和代码，利用检索增强生成辅助大语言模型，设置护栏保证规则和代码准确性。

Result: 在基准数据集上的广泛评估确认了该方法的有效性。

Conclusion: 所提出的结合统计离群检测与大语言模型驱动的框架可有效解决表格数据验证问题。

Abstract: Reliable data quality is crucial for downstream analysis of tabular datasets,
yet rule-based validation often struggles with inefficiency, human
intervention, and high computational costs. We present a three-stage framework
that combines statistical inliner detection with LLM-driven rule and code
generation. After filtering data samples through traditional clustering, we
iteratively prompt LLMs to produce semantically valid quality rules and
synthesize their executable validators through code-generating LLMs. To
generate reliable quality rules, we aid LLMs with retrieval-augmented
generation (RAG) by leveraging external knowledge sources and domain-specific
few-shot examples. Robust guardrails ensure the accuracy and consistency of
both rules and code snippets. Extensive evaluations on benchmark datasets
confirm the effectiveness of our approach.

</details>


### [259] [Reasonable Experiments in Model-Based Systems Engineering](https://arxiv.org/abs/2509.10649)
*Johan Cederbladh,Loek Cleophas,Eduard Kamburjan,Lucas Lima,Rakshit Mittal,Hans Vangheluwe*

Main category: cs.SE

TL;DR: 提出管理数字和/或物理资产实验的框架，利用基于案例推理和领域知识高效复用实验数据，并通过工业车辆能源系统设计案例验证。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型的系统工程趋势下，管理实验配置元数据和结果对加速设计很重要，智能复用实验相关数据可节省时间和精力。

Method: 提出一个框架，聚焦基于案例推理和领域知识，决定已执行实验或相关答案能否复用。

Result: 提供了实验管理器的通用架构，并通过工业车辆能源系统设计案例研究进行了验证。

Conclusion: 该框架可用于管理数字和/或物理资产的实验，能高效复用实验数据。

Abstract: With the current trend in Model-Based Systems Engineering towards Digital
Engineering and early Validation & Verification, experiments are increasingly
used to estimate system parameters and explore design decisions. Managing such
experimental configuration metadata and results is of utmost importance in
accelerating overall design effort. In particular, we observe it is important
to 'intelligent-ly' reuse experiment-related data to save time and effort by
not performing potentially superfluous, time-consuming, and resource-intensive
experiments. In this work, we present a framework for managing experiments on
digital and/or physical assets with a focus on case-based reasoning with domain
knowledge to reuse experimental data efficiently by deciding whether an
already-performed experiment (or associated answer) can be reused to answer a
new (potentially different) question from the engineer/user without having to
set up and perform a new experiment. We provide the general architecture for
such an experiment manager and validate our approach using an industrial
vehicular energy system-design case study.

</details>


### [260] [Arguzz: Testing zkVMs for Soundness and Completeness Bugs](https://arxiv.org/abs/2509.10819)
*Christoph Hochrainer,Valentin Wüstholz,Maria Christakis*

Main category: cs.SE

TL;DR: 本文介绍首个自动化工具Arguzz用于测试zkVMs的健全性和完整性漏洞，测试六个zkVMs发现十一个漏洞。


<details>
  <summary>Details</summary>
Motivation: zkVMs复杂，其约束系统或执行逻辑的漏洞会导致关键的健全性和完整性问题，需要系统性测试工具。

Method: Arguzz结合新颖的变形测试变体与故障注入，生成语义等效程序对，合并到有已知输出的Rust程序并在zkVM内运行，通过注入故障揭露过弱约束。

Result: 用Arguzz测试六个真实世界的zkVMs，在其中三个发现十一个漏洞，一个RISC Zero漏洞获50000美元赏金。

Conclusion: 凸显了对zkVMs进行系统性测试的迫切需求。

Abstract: Zero-knowledge virtual machines (zkVMs) are increasingly deployed in
decentralized applications and blockchain rollups since they enable verifiable
off-chain computation. These VMs execute general-purpose programs, frequently
written in Rust, and produce succinct cryptographic proofs. However, zkVMs are
complex, and bugs in their constraint systems or execution logic can cause
critical soundness (accepting invalid executions) or completeness (rejecting
valid ones) issues.
  We present Arguzz, the first automated tool for testing zkVMs for soundness
and completeness bugs. To detect such bugs, Arguzz combines a novel variant of
metamorphic testing with fault injection. In particular, it generates
semantically equivalent program pairs, merges them into a single Rust program
with a known output, and runs it inside a zkVM. By injecting faults into the
VM, Arguzz mimics malicious or buggy provers to uncover overly weak
constraints.
  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,
OpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug
resulted in a $50,000 bounty, despite prior audits, demonstrating the critical
need for systematic testing of zkVMs.

</details>


### [261] [TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications](https://arxiv.org/abs/2509.10920)
*Guan-Yan Yang,Farn Wang,You-Zong Gu,Ya-Wen Teng,Kuo-Hui Yeh,Ping-Hsueh Ho,Wei-Ling Wen*

Main category: cs.SE

TL;DR: 网络攻击增多，软件测试复杂度和工作量增加，本文提出针对SQL注入漏洞的测试优先级排序新方法，提升检测和缓解漏洞的效率。


<details>
  <summary>Details</summary>
Motivation: 网络攻击增多，注入攻击位列软件项目漏洞前三，增加软件测试复杂度和工作量，需要先进工具支持敏捷开发周期。

Method: 利用先前测试结果，调整后续测试的防御强度向量，优化测试工作流程，根据软件特定需求定制防御机制。

Result: 未提及具体结果。

Conclusion: 通过包含动态调整并考虑漏洞暴露时间因素的灵活框架，可提高漏洞检测和缓解的有效性和效率。

Abstract: The rapid proliferation of network applications has led to a significant
increase in network attacks. According to the OWASP Top 10 Projects report
released in 2021, injection attacks rank among the top three vulnerabilities in
software projects. This growing threat landscape has increased the complexity
and workload of software testing, necessitating advanced tools to support agile
development cycles. This paper introduces a novel test prioritization method
for SQL injection vulnerabilities to enhance testing efficiency. By leveraging
previous test outcomes, our method adjusts defense strength vectors for
subsequent tests, optimizing the testing workflow and tailoring defense
mechanisms to specific software needs. This approach aims to improve the
effectiveness and efficiency of vulnerability detection and mitigation through
a flexible framework that incorporates dynamic adjustments and considers the
temporal aspects of vulnerability exposure.

</details>


### [262] [When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning](https://arxiv.org/abs/2509.10946)
*Roberto Morabito,Guanghan Wu*

Main category: cs.SE

TL;DR: 文章对大语言模型驱动的机器学习管道中的故障模式进行实证研究，揭示错误行为、得出故障分类，指出基于大语言模型代码生成的挑战并探讨改进方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于嵌入式机器学习工作流的软件生成时输出常无声失败或行为不可预测。

Method: 基于自动驾驶框架编排数据预处理、模型转换和设备上推理代码生成，进行实证研究。

Result: 揭示提示格式、模型行为和结构假设影响成功率和故障特征，发现多种易出错行为，得出故障分类，分析多模型错误。

Conclusion: 指出基于大语言模型代码生成存在更广泛挑战，讨论提高基于大语言模型的嵌入式机器学习系统可靠性和可追溯性的方向。

Abstract: Large Language Models (LLMs) are increasingly used to automate software
generation in embedded machine learning workflows, yet their outputs often fail
silently or behave unpredictably. This article presents an empirical
investigation of failure modes in LLM-powered ML pipelines, based on an
autopilot framework that orchestrates data preprocessing, model conversion, and
on-device inference code generation. We show how prompt format, model behavior,
and structural assumptions influence both success rates and failure
characteristics, often in ways that standard validation pipelines fail to
detect. Our analysis reveals a diverse set of error-prone behaviors, including
format-induced misinterpretations and runtime-disruptive code that compiles but
breaks downstream. We derive a taxonomy of failure categories and analyze
errors across multiple LLMs, highlighting common root causes and systemic
fragilities. Though grounded in specific devices, our study reveals broader
challenges in LLM-based code generation. We conclude by discussing directions
for improving reliability and traceability in LLM-powered embedded ML systems.

</details>


### [263] [Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling](https://arxiv.org/abs/2509.11000)
*Omid Gheibi,Christian Kästner,Pooyan Jamshidi*

Main category: cs.SE

TL;DR: 本文研究系统结构方面和结构知识水平对模块化性能建模的影响，引入建模“难度”概念，通过实验得出建模难度主要因素，以及结构知识和难度对不同性能指标的影响，为系统设计者提供策略指导。


<details>
  <summary>Details</summary>
Motivation: 现有性能影响模型创建因配置空间指数增长而困难，且结构知识、系统结构方面与模型改进关系不明。

Method: 正式研究结构方面变化和结构知识水平对模块化性能建模的影响，引入建模“难度”概念并量化，通过合成系统模型的控制实验建立分析矩阵。

Result: 建模难度主要由模块数量和每个模块的配置选项数量驱动；更高的结构知识水平和更大的建模难度显著提升改进机会；对不同性能指标影响不同，结构知识对排名准确性影响大，难度对预测准确性影响大。

Conclusion: 研究结果为系统设计者提供可操作的见解，指导其根据系统特征和任务目标合理分配时间和选择建模方法。

Abstract: Performance-influence models are beneficial for understanding how
configurations affect system performance, but their creation is challenging due
to the exponential growth of configuration spaces. While gray-box approaches
leverage selective "structural knowledge" (like the module execution graph of
the system) to improve modeling, the relationship between this knowledge, a
system's characteristics (we call them "structural aspects"), and potential
model improvements is not well understood. This paper addresses this gap by
formally investigating how variations in structural aspects (e.g., the number
of modules and options per module) and the level of structural knowledge impact
the creation of "opportunities" for improved "modular performance modeling". We
introduce and quantify the concept of modeling "hardness", defined as the
inherent difficulty of performance modeling. Through controlled experiments
with synthetic system models, we establish an "analytical matrix" to measure
these concepts. Our findings show that modeling hardness is primarily driven by
the number of modules and configuration options per module. More importantly,
we demonstrate that both higher levels of structural knowledge and increased
modeling hardness significantly enhance the opportunity for improvement. The
impact of these factors varies by performance metric; for ranking accuracy
(e.g., in debugging task), structural knowledge is more dominant, while for
prediction accuracy (e.g., in resource management task), hardness plays a
stronger role. These results provide actionable insights for system designers,
guiding them to strategically allocate time and select appropriate modeling
approaches based on a system's characteristics and a given task's objectives.

</details>


### [264] [ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch](https://arxiv.org/abs/2509.11065)
*Yuan Si,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: 介绍了首个用于Scratch的多模态反馈生成系统ViScratch，利用代码和游戏视频诊断修复错误，评估显示其性能优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有Scratch调试工具依赖预定义规则或手动输入，忽略平台视觉特性，语义错误难调试。

Method: ViScratch采用两阶段管道，先通过视觉语言模型定位问题，再提出修复并在虚拟机验证。

Result: 在真实项目评估中，ViScratch在错误识别和修复质量上大幅超越现有工具。

Conclusion: 视频可作为可视化编程环境的一级规范，为基于大语言模型的调试开辟新方向。

Abstract: Block-based programming environments such as Scratch are increasingly popular
in programming education, in particular for young learners. While the use of
blocks helps prevent syntax errors, semantic bugs remain common and difficult
to debug. Existing tools for Scratch debugging rely heavily on predefined rules
or user manual inputs, and crucially, they ignore the platform's inherently
visual nature.
  We introduce ViScratch, the first multimodal feedback generation system for
Scratch that leverages both the project's block code and its generated gameplay
video to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a
vision-language model first aligns visual symptoms with code structure to
identify a single critical issue, then proposes minimal, abstract syntax tree
level repairs that are verified via execution in the Scratch virtual machine.
  We evaluate ViScratch on a set of real-world Scratch projects against
state-of-the-art LLM-based tools and human testers. Results show that gameplay
video is a crucial debugging signal: ViScratch substantially outperforms prior
tools in both bug identification and repair quality, even without access to
project descriptions or goals. This work demonstrates that video can serve as a
first-class specification in visual programming environments, opening new
directions for LLM-based debugging beyond symbolic code alone.

</details>


### [265] [Rethinking Technology Stack Selection with AI Coding Proficiency](https://arxiv.org/abs/2509.11132)
*Xiaoyu Zhang,Weipeng Jiang,Juan Zhai,Shiqing Ma,Qingshuang Bao,Chenhao Lin,Chao Shen,Tianlin Li,Yang Liu*

Main category: cs.SE

TL;DR: 本文提出AI编码熟练度概念，研究170个第三方库和61个任务场景下6个大语言模型的AI熟练度，发现库和模型间存在质量差距，呼吁将AI熟练度评估纳入技术选择框架。


<details>
  <summary>Details</summary>
Motivation: 传统技术栈选择未跟上大语言模型发展，现有方法忽略大语言模型能否有效利用所选技术，团队使用大语言模型助手有选择无法有效利用技术的风险。

Method: 提出AI编码熟练度概念，对170个第三方库和61个任务场景进行全面实证研究，评估6个广泛使用的大语言模型。

Result: 功能相似的库在大语言模型生成代码质量得分上差异可达84%，不同模型使用同一库的生成结果也有质量差距。

Conclusion: 呼吁社区将AI熟练度评估纳入技术选择框架并制定缓解策略，以保持人工智能驱动开发中的竞争平衡。

Abstract: Large language models (LLMs) are now an integral part of software development
workflows and are reshaping the whole process. Traditional technology stack
selection has not caught up. Most of the existing selection methods focus
solely on the inherent attributes of the technology, overlooking whether the
LLM can effectively leverage the chosen technology. For example, when
generating code snippets using popular libraries like Selenium (one of the most
widely used test automation tools with over 33k GitHub stars), existing LLMs
frequently generate low-quality code snippets (e.g., using deprecated APIs and
methods, or containing syntax errors). As such, teams using LLM assistants risk
choosing technologies that cannot be used effectively by LLMs, yielding high
debugging effort and mounting technical debt. We foresee a practical question
in the LLM era, is a technology ready for AI-assisted development? In this
paper, we first propose the concept, AI coding proficiency, the degree to which
LLMs can utilize a given technology to generate high-quality code snippets. We
conduct the first comprehensive empirical study examining AI proficiency across
170 third-party libraries and 61 task scenarios, evaluating six widely used
LLMs. Our findings reveal that libraries with similar functionalities can
exhibit up to 84% differences in the quality score of LLM-generated code, while
different models also exhibit quality gaps among their generation results using
the same library. These gaps translate into real engineering costs and can
steer developer choices toward a narrow set of libraries with high AI coding
proficiency, threatening technological diversity in the ecosystem. We call on
the community to integrate AI proficiency assessments into technology selection
frameworks and develop mitigation strategies, preserving competitive balance in
AI-driven development.

</details>


### [266] [UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories](https://arxiv.org/abs/2509.11238)
*Dongming Jin,Zhi Jin,Yiran Zhang,Zheng Fang,Linyu Li,Yuanpeng He,Xiaohong Chen,Weisong Sun*

Main category: cs.SE

TL;DR: 提出多智能体系统UserTrace，自动生成用户级需求并恢复实时跟踪链接，评估显示其效果优于基线和现有方法，用户研究表明有助于用户验证AI生成仓库是否符合意图。


<details>
  <summary>Details</summary>
Motivation: 现有自动代码摘要和需求跟踪技术分别存在生成开发者级需求、忽视项目演化影响的问题，用户级需求和实时跟踪链接未被充分探索。

Method: 提出UserTrace系统，通过三阶段过程协调四个专门智能体，包括构建仓库依赖、推导代码单元的实现级需求、结合特定领域上下文合成用户级需求。

Result: UserTrace生成的用户级需求在完整性、正确性和有用性上高于基线，跟踪链接恢复精度优于五个现有方法。

Conclusion: UserTrace有助于终端用户验证AI生成的仓库是否符合其意图。

Abstract: Software maintainability critically depends on high-quality requirements
descriptions and explicit traceability between requirements and code. Although
automated code summarization (ACS) and requirements traceability (RT)
techniques have been widely studied, existing ACS methods mainly generate
implementation-level (i.e., developer-oriented) requirements (IRs) for
fine-grained units (e.g., methods), while RT techniques often overlook the
impact of project evolution. As a result, user-level (i.e., end user-oriented)
requirements (URs) and live trace links remain underexplored, despite their
importance for supporting user understanding and for validating whether
AI-generated software aligns with user intent. To address this gap, we propose
UserTrace, a multi-agent system that automatically generates URs and recovers
live trace links (from URs to IRs to code) from software repositories.
UserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,
Writer, and Verifier) through a three-phase process: structuring repository
dependencies, deriving IRs for code units, and synthesizing URs with
domain-specific context. Our comparative evaluation shows that UserTrace
produces URs with higher completeness, correctness, and helpfulness than an
established baseline, and achieves superior precision in trace link recovery
compared to five state-of-the-art RT approaches. A user study further
demonstrates that UserTrace helps end users validate whether the AI-generated
repositories align with their intent.

</details>


### [267] [Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation](https://arxiv.org/abs/2509.11252)
*Chengze li,Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.SE

TL;DR: 本文开展首个针对代码生成的扩散大语言模型实证研究，对比其与自回归大语言模型，总结发现，开源代码促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型在代码生成中有效率低和生成顺序固定的局限，扩散大语言模型有改进但缺乏系统研究，为填补知识空白开展研究。

Method: 涉及9个有代表性的扩散大语言模型，在4个广泛使用的基准测试上进行实验。

Result: 现有扩散大语言模型与相似规模自回归大语言模型竞争力相当；有更强长度外推能力，长代码理解表现更好；探索了影响其有效性和效率的因素；讨论了改进的方向。

Conclusion: 总结扩散大语言模型在代码生成中的表现，为后续研究提供实用指导和方向，开源研究资源。

Abstract: LLMs have become the mainstream approaches to code generation. Existing LLMs
mainly employ autoregressive generation, i.e. generating code token-by-token
from left to right. However, the underlying autoregressive generation has two
limitations in code generation. First, autoregressive LLMs only generate a
token at each step, showing low efficiency in practice. Second, programming is
a non-sequential process involving back-and-forth editing, while autoregressive
LLMs only employ the left-to-right generation order. These two intrinsic
limitations hinder the further development of LLMs in code generation.
Recently, diffusion LLMs have emerged as a promising alternative. Diffusion
LLMs address the above limitations with two advances, including multi-token
prediction (i.e. generating multiple tokens at each step) and flexible
generation order (i.e. flexibly determining which positions to generate
tokens). However, there is no systematic study exploring diffusion LLMs in code
generation. To bridge the knowledge gap, we present the first empirical study
of diffusion LLMs for code generation. Our study involves 9 representative
diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on
the results, we summarize the following findings. (1) Existing diffusion LLMs
are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs
have a stronger length extrapolation ability than autoregressive LLMs and
perform better in long code understanding. (3) We explore factors impacting the
effectiveness and efficiency of diffusion LLMs, and provide practical guidance.
(4) We discuss several promising further directions to improve diffusion LLMs
on code generation. We open-source all source code, data, and results to
facilitate the following research. The code is publicly available at
https://github.com/zhangyitonggg/dllm4code.

</details>


### [268] [A Web-Based Environment for the Specification and Generation of Smart Legal Contracts](https://arxiv.org/abs/2509.11258)
*Regan Meloche,Durga Sivakumar,Amal A. Anda,Sofana Alfuhaid,Daniel Amyot,Luigi Logrippo,John Mylopoulos*

Main category: cs.SE

TL;DR: 本文介绍基于Web的环境，支持用户辅助细化法律合同模板规范，自动生成可部署在Hyperledger Fabric平台的监控智能合约，加速合规智能合约开发。


<details>
  <summary>Details</summary>
Motivation: 自然语言合同与智能合约实现存在差距，需工具缩小差距以监控合同履行合规性。

Method: 引入基于Web的环境，支持用户辅助细化Symboleo规范，自动生成可部署在Hyperledger Fabric平台的监控智能合约。

Result: 以电力交易领域合同为例展示环境，显示出在合规背景下加速智能合约开发的潜力。

Conclusion: 该环境在法律合规背景下加速智能合约开发有很大潜力。

Abstract: Monitoring the compliance of contract performance against legal obligations
is important in order to detect violations, ideally, as soon as they occur.
Such monitoring can nowadays be achieved through the use of smart contracts,
which provide protection against tampering as well as some level of automation
in handling violations. However, there exists a large gap between natural
language contracts and smart contract implementations. This paper introduces a
Web-based environment that partly fills that gap by supporting the
user-assisted refinement of Symboleo specifications corresponding to legal
contract templates, followed by the automated generation of monitoring smart
contracts deployable on the Hyperledger Fabric platform. This environment,
illustrated using a sample contract from the transactive energy domain, shows
much potential in accelerating the development of smart contracts in a legal
compliance context.

</details>


### [269] [Weakly Supervised Vulnerability Localization via Multiple Instance Learning](https://arxiv.org/abs/2509.11312)
*Wenchao Gu,Yupan Chen,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出WAVES方法用于弱监督漏洞定位，无需额外语句级标签，实验表明其在漏洞检测和语句级漏洞定位上表现良好。


<details>
  <summary>Details</summary>
Motivation: 以往粗粒度漏洞检测后开发者需手动检查大量代码定位具体漏洞语句，而训练漏洞定位模型需语句级标签，标注成本高，因此需要无额外语句级标签的方法。

Method: 提出WAVES方法，受多实例学习启发，将函数级标签转换为语句伪标签，用其训练函数级表示向量分类器。

Result: 在三个流行基准数据集上实验，漏洞检测性能与之前基线相当，语句级漏洞定位达到最优。

Conclusion: WAVES方法有效，可在无额外语句级标签下进行漏洞检测和定位。

Abstract: Software vulnerability detection has emerged as a significant concern in the
field of software security recently, capturing the attention of numerous
researchers and developers. Most previous approaches focus on coarse-grained
vulnerability detection, such as at the function or file level. However, the
developers would still encounter the challenge of manually inspecting a large
volume of code inside the vulnerable function to identify the specific
vulnerable statements for modification, indicating the importance of
vulnerability localization. Training the model for vulnerability localization
usually requires ground-truth labels at the statement-level, and labeling
vulnerable statements demands expert knowledge, which incurs high costs. Hence,
the demand for an approach that eliminates the need for additional labeling at
the statement-level is on the rise. To tackle this problem, we propose a novel
approach called WAVES for WeAkly supervised Vulnerability Localization via
multiplE inStance learning, which does not need the additional statement-level
labels during the training. WAVES has the capability to determine whether a
function is vulnerable (i.e., vulnerability detection) and pinpoint the
vulnerable statements (i.e., vulnerability localization). Specifically,
inspired by the concept of multiple instance learning, WAVES converts the
ground-truth label at the function-level into pseudo labels for individual
statements, eliminating the need for additional statement-level labeling. These
pseudo labels are utilized to train the classifiers for the function-level
representation vectors. Extensive experimentation on three popular benchmark
datasets demonstrates that, in comparison to previous baselines, our approach
achieves comparable performance in vulnerability detection and state-of-the-art
performance in statement-level vulnerability localization.

</details>


### [270] [Large Language Models (LLMs) for Requirements Engineering (RE): A Systematic Literature Review](https://arxiv.org/abs/2509.11446)
*Mohammad Amin Zadenoori,Jacek Dąbrowski,Waad Alhoshan,Liping Zhao,Alessio Ferrari*

Main category: cs.SE

TL;DR: 本文对2023 - 2024年74篇关于大语言模型（LLMs）在需求工程（RE）中应用的研究进行系统综述，分析应用情况并指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在各领域应用增加，了解其在RE中的应用情况。

Method: 对74篇研究进行系统文献综述，从多个维度对文献分类。

Result: LLMs应用与以往NLP技术有差异，研究多聚焦需求获取和验证，采用GPT模型与零样本或少样本提示，多在受控环境评估。

Conclusion: 提出未来研究方向如扩大RE在SE中影响、探索少研究任务等，还提供相关工具和数据集以助有效使用LLMs。

Abstract: Large Language Models (LLMs) are finding applications in numerous domains,
and Requirements Engineering (RE) is increasingly benefiting from their
capabilities to assist with complex, language-intensive tasks. This paper
presents a systematic literature review of 74 primary studies published between
2023 and 2024, examining how LLMs are being applied in RE. The study
categorizes the literature according to several dimensions, including
publication trends, RE activities, prompting strategies, and evaluation
methods. Our findings indicate notable patterns, among which we observe
substantial differences compared to previous works leveraging standard Natural
Language Processing (NLP) techniques. Most of the studies focus on using LLMs
for requirements elicitation and validation, rather than defect detection and
classification, which were dominant in the past. Researchers have also
broadened their focus and addressed novel tasks, e.g., test generation,
exploring the integration of RE with other software engineering (SE)
disciplines. Although requirements specifications remain the primary focus,
other artifacts are increasingly considered, including issues from issue
tracking systems, regulations, and technical manuals. The studies mostly rely
on GPT-based models, and often use Zero-shot or Few-shot prompting. They are
usually evaluated in controlled environments, with limited use in industry
settings and limited integration in complex workflows. Our study outlines
important future directions, such as leveraging the potential to expand the
influence of RE in SE, exploring less-studied tasks, improving prompting
methods, and testing in real-world environments. Our contribution also helps
researchers and practitioners use LLMs more effectively in RE, by providing a
list of identified tools leveraging LLMs for RE, as well as datasets.

</details>


### [271] [VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection](https://arxiv.org/abs/2509.11523)
*Ziliang Wang,Ge Li,Jia Li,Hao Zhu,Zhi Jin*

Main category: cs.SE

TL;DR: 提出基于假设验证的多智能体漏洞检测框架VulAgent，在两个数据集上提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型应用于项目级漏洞检测存在准确锁定敏感代码和关联推理复杂程序上下文的双重挑战。

Method: 设计语义敏感、多视图检测流程，用专业智能体协作定位敏感代码；采用假设验证范式，构建假设条件和触发路径以减少误报。

Result: 在两个数据集上平均提高整体准确率6.6%，易受攻击 - 修复代码对的正确识别率最高提升450%（平均246%），误报率降低约36%。

Conclusion: VulAgent在项目级漏洞检测上比基于大语言模型的现有方法更有效。

Abstract: The application of language models to project-level vulnerability detection
remains challenging, owing to the dual requirement of accurately localizing
security-sensitive code and correctly correlating and reasoning over complex
program context. We present VulAgent, a multi-agent vulnerability detection
framework based on hypothesis validation. Our design is inspired by how human
auditors review code: when noticing a sensitive operation, they form a
hypothesis about a possible vulnerability, consider potential trigger paths,
and then verify the hypothesis against the surrounding context. VulAgent
implements a semantics-sensitive, multi-view detection pipeline: specialized
agents, each aligned to a specific analysis perspective (e.g., memory,
authorization), collaboratively surface and precisely localize sensitive code
sites with higher coverage. Building on this, VulAgent adopts a
hypothesis-validation paradigm: for each vulnerability report, it builds
hypothesis conditions and a trigger path, steering the LLM to target the
relevant program context and defensive checks during verification, which
reduces false positives. On average across the two datasets, VulAgent improves
overall accuracy by 6.6%, increases the correct identification rate of
vulnerable--fixed code pairs by up to 450% (246% on average), and reduces the
false positive rate by about 36% compared with state-of-the-art LLM-based
baselines.

</details>


### [272] [Sedeve-Kit, a Specification-Driven Development Framework for Building Distributed Systems](https://arxiv.org/abs/2509.11566)
*Hua Guo,Yunhong Ji,Xuan Zhou*

Main category: cs.SE

TL;DR: 提出规范驱动开发框架应对分布式系统开发挑战，分三阶段开发并持续验证保证系统质量。


<details>
  <summary>Details</summary>
Motivation: 分布式系统开发因非确定性并发和故障带来的复杂性面临重大挑战，需解决这些问题。

Method: 分三阶段：用TLA⁺定义系统规范和不变量进行模型检查与生成测试用例；基于规范编写代码；用初始阶段生成的测试用例严格测试系统。

Result: 未提及具体结果。

Conclusion: 通过持续验证保持抽象设计和具体实现的紧密联系，确保系统质量。

Abstract: Developing distributed systems presents significant challenges, primarily due
to the complexity introduced by non-deterministic concurrency and faults. To
address these, we propose a specification-driven development framework. Our
method encompasses three key stages. The first stage defines system
specifications and invariants using TLA${^+}$. It allows us to perform model
checking on the algorithm's correctness and generate test cases for subsequent
development phases. In the second stage, based on the established
specifications, we write code to ensure consistency and accuracy in the
implementation. Finally, after completing the coding process, we rigorously
test the system using the test cases generated in the initial stage. This
process ensures system quality by maintaining a strong connection between the
abstract design and the concrete implementation through continuous
verification.

</details>


### [273] [Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools](https://arxiv.org/abs/2509.11626)
*Prerna Agarwal,Himanshu Gupta,Soujanya Soni,Rohith Vallam,Renuka Sindhgatta,Sameep Mehta*

Main category: cs.SE

TL;DR: 提出ACE框架将企业API转换为大语言模型兼容工具，可生成丰富工具规范和动态筛选工具，在专有和开源API上验证。


<details>
  <summary>Details</summary>
Motivation: 企业环境中API文档差、输入输出模式复杂等问题阻碍大语言模型对工具的有效使用，降低有效载荷形成的准确性。

Method: 提出ACE框架，生成丰富工具规范，纳入动态筛选机制。

Result: 在专有和开源API上验证框架，并展示其与代理框架的集成。

Conclusion: ACE是首个为大语言模型代理自动创建、丰富和动态选择企业API工具的端到端框架。

Abstract: Recent advancements in Large Language Models (LLMs) has lead to the
development of agents capable of complex reasoning and interaction with
external tools. In enterprise contexts, the effective use of such tools that
are often enabled by application programming interfaces (APIs), is hindered by
poor documentation, complex input or output schema, and large number of
operations. These challenges make tool selection difficult and reduce the
accuracy of payload formation by up to 25%. We propose ACE, an automated tool
creation and enrichment framework that transforms enterprise APIs into
LLM-compatible tools. ACE, (i) generates enriched tool specifications with
parameter descriptions and examples to improve selection and invocation
accuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters
relevant tools at runtime, reducing prompt complexity while maintaining
scalability. We validate our framework on both proprietary and open-source APIs
and demonstrate its integration with agentic frameworks. To the best of our
knowledge, ACE is the first end-to-end framework that automates the creation,
enrichment, and dynamic selection of enterprise API tools for LLM agents.

</details>


### [274] [Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models](https://arxiv.org/abs/2509.11686)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Yi Li*

Main category: cs.SE

TL;DR: Code LLMs有推理和语义表示问题，本文引入框架研究语义信息作用，实验表明其对SFT和测试时扩展作用有限。


<details>
  <summary>Details</summary>
Motivation: Code LLMs在推理程序运行行为和理解程序功能方面存在关键局限，需要系统方法提升其推理能力。

Method: 引入通用框架将语义信息集成到与代码任务相关的提示中，研究基于跟踪的语义信息对Code LLMs监督微调（SFT）和后阶段推理的作用。

Result: 实验结果与以往研究不同，表明语义信息对Code LLMs的SFT和测试时扩展作用有限。

Conclusion: 语义信息在提升Code LLMs推理能力方面的实际作用可能不像预期的那么大。

Abstract: Code Large Language Models (Code LLMs) have opened a new era in programming
with their impressive capabilities. However, recent research has revealed
critical limitations in their ability to reason about runtime behavior and
understand the actual functionality of programs, which poses significant
challenges for their post-training and practical deployment. Specifically, Code
LLMs encounter two principal issues: (1) a lack of proficiency in reasoning
about program execution behavior, as they struggle to interpret what programs
actually do during runtime, and (2) the inconsistent and fragmented
representation of semantic information, such as execution traces, across
existing methods, which hinders their ability to generalize and reason
effectively. These challenges underscore the necessity for more systematic
approaches to enhance the reasoning capabilities of Code LLMs. To address these
issues, we introduce a generic framework to support integrating semantic
information~(e.g., execution trace) to code task-relevant prompts, and conduct
a comprehensive study to explore the role of semantic information in enhancing
the reasoning ability of Code LLMs accordingly. Specifically, we focus on
investigating the usefulness of trace-based semantic information in boosting
supervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The
experimental results surprisingly disagree with previous works and demonstrate
that semantic information has limited usefulness for SFT and test time scaling
of Code LLM.

</details>


### [275] [AI Asset Management for Manufacturing (AIM4M): Development of a Process Model for Operationalization](https://arxiv.org/abs/2509.11691)
*Lukas Rauh,Mel-Rick Süner,Daniel Schel,Thomas Bauernhansl*

Main category: cs.SE

TL;DR: 本文提出新的AI资产生命周期管理流程模型，以解决制造领域AI落地挑战。


<details>
  <summary>Details</summary>
Motivation: 在制造领域将AI从原型投入实际运营面临技术系统复杂、缺乏实施标准和组织流程分散等挑战。

Method: 基于MLOps原则，细化三个方面以满足CPPS领域特定需求，提出流程模型。

Result: 所提出的流程模型旨在支持组织在实践中系统地开发、部署和管理AI资产。

Conclusion: 该模型有助于应对制造领域AI运营化挑战，符合CPPS特定约束和监管要求。

Abstract: The benefits of adopting artificial intelligence (AI) in manufacturing are
undeniable. However, operationalizing AI beyond the prototype, especially when
involved with cyber-physical production systems (CPPS), remains a significant
challenge due to the technical system complexity, a lack of implementation
standards and fragmented organizational processes. To this end, this paper
proposes a new process model for the lifecycle management of AI assets designed
to address challenges in manufacturing and facilitate effective
operationalization throughout the entire AI lifecycle. The process model, as a
theoretical contribution, builds on machine learning operations (MLOps)
principles and refines three aspects to address the domain-specific
requirements from the CPPS context. As a result, the proposed process model
aims to support organizations in practice to systematically develop, deploy and
manage AI assets across their full lifecycle while aligning with CPPS-specific
constraints and regulatory demands.

</details>


### [276] [From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation](https://arxiv.org/abs/2509.11708)
*Zhantong Xue,Pingchuan Ma,Zhaoyu Wang,Shuai Wang*

Main category: cs.SE

TL;DR: 文章提出ZK - Eval评估LLM在零知识编程的能力，发现其问题后推出ZK - Coder框架提升成功率，为零知识代码生成奠定基础。


<details>
  <summary>Details</summary>
Motivation: 零知识编程难，LLM在零知识编程的有效性未被探索，需评估和提升LLM在该领域能力。

Method: 提出ZK - Eval评估LLM在语言知识、组件能力和端到端程序生成三方面的能力；推出ZK - Coder框架，通过约束草图、引导检索和交互式修复增强LLM。

Result: 对四个先进LLM评估显示其语法尚可但组件使用和语义正确性差；Circom和Noir实验中成功率大幅提升。

Conclusion: ZK - Eval和ZK - Coder为系统衡量和增强LLM在零知识代码生成能力奠定基础，降低从业者门槛，推动可信计算。

Abstract: Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as
privacy-preserving authentication, blockchain scalability, and secure finance.
However, authoring ZK programs remains challenging: unlike mainstream
programming, ZK development requires reasoning about finite field arithmetic,
constraint systems, and gadgets, making it knowledge-intensive and error-prone.
While large language models (LLMs) have demonstrated strong code generation
capabilities in general-purpose languages, their effectiveness for ZK
programming, where correctness hinges on both language mastery and gadget-level
reasoning, remains unexplored. To address this gap, we propose
\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM
capabilities at three levels: language knowledge, gadget competence, and
end-to-end program generation. Our evaluation of four state-of-the-art LLMs
reveals that models excel at surface-level syntax but struggle with gadget
usage and semantic correctness, often yielding incorrect programs. Based on
these insights, we introduce \textsc{ZK-Coder}, an agentic framework that
augments LLMs with constraint sketching, guided retrieval, and interactive
repair. Experiments on Circom and Noir show substantial gains, with success
rates improving from 17.35\% to 83.38\% and from 32.21\% to 90.05\%,
respectively. With \textsc{ZK-Eval} and \textsc{ZK-Coder}, we establish a
foundation for systematically measuring and augmenting LLMs in ZK code
generation to lower barriers for practitioners and advance trustworthy
computation.

</details>


### [277] [Toward Greener Background Processes -- Measuring Energy Cost of Autosave Feature](https://arxiv.org/abs/2509.11738)
*Maria Küüsvek,Hina Anwar*

Main category: cs.SE

TL;DR: 本文介绍评估桌面应用程序后台进程能耗的方法，并通过案例研究给出节能建议。


<details>
  <summary>Details</summary>
Motivation: 桌面应用程序后台进程能耗在研究中常被忽视，但累积影响大，需评估其能耗行为。

Method: 提出分三个阶段的评估流程，即分解核心操作、操作隔离和控制测量；并对三个开源Python文本编辑器的自动保存功能进行案例研究。

Result: 通过900次基于软件的能耗测量，确定影响能耗的关键设计因素。

Conclusion: 给出四条Python自动保存功能的节能实施建议，支持可持续软件实践。

Abstract: Background processes in desktop applications are often overlooked in energy
consumption studies, yet they represent continuous, automated workloads with
significant cumulative impact. This paper introduces a reusable process for
evaluating the energy behavior of such features at the level of operational
design. The process works in three phases: 1) decomposing background
functionality into core operations, 2) operational isolation, and 3) controlled
measurements enabling comparative profiling. We instantiate the process in a
case study of autosave implementations across three open-source Python-based
text editors. Using 900 empirical software-based energy measurements, we
identify key design factors affecting energy use, including save frequency,
buffering strategy, and auxiliary logic such as change detection. We give four
actionable recommendations for greener implementations of autosave features in
Python to support sustainable software practices.

</details>


### [278] [Analysing Python Machine Learning Notebooks with Moose](https://arxiv.org/abs/2509.11748)
*Marius Mignard,Steven Costiou,Nicolas Anquetil,Anne Etien*

Main category: cs.SE

TL;DR: 现有机器学习代码分析工具存在局限性，本文介绍Vespucci Linter静态分析工具，通过元建模方法实现多级分析，应用于Kaggle笔记本验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有分析工具通常仅关注一个层面且难以捕捉机器学习特定语义，无法有效检测问题，需要开发能跨三个层面检测问题的工具。

Method: 基于Moose构建Vespucci Linter，采用元建模方法统一笔记本结构元素和Python代码实体，实现多级分析，并制定22条规则。

Result: 将工具应用于5000个Kaggle笔记本，发现各层面均存在违规情况。

Conclusion: 多级分析方法有相关性，Vespucci Linter有潜力提升笔记本环境中机器学习开发的质量和可靠性。

Abstract: Machine Learning (ML) code, particularly within notebooks, often exhibits
lower quality compared to traditional software. Bad practices arise at three
distinct levels: general Python coding conventions, the organizational
structure of the notebook itself, and ML-specific aspects such as
reproducibility and correct API usage. However, existing analysis tools
typically focus on only one of these levels and struggle to capture ML-specific
semantics, limiting their ability to detect issues. This paper introduces
Vespucci Linter, a static analysis tool with multi-level capabilities, built on
Moose and designed to address this challenge. Leveraging a metamodeling
approach that unifies the notebook's structural elements with Python code
entities, our linter enables a more contextualized analysis to identify issues
across all three levels. We implemented 22 linting rules derived from the
literature and applied our tool to a corpus of 5,000 notebooks from the Kaggle
platform. The results reveal violations at all levels, validating the relevance
of our multi-level approach and demonstrating Vespucci Linter's potential to
improve the quality and reliability of ML development in notebook environments.

</details>


### [279] [CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings](https://arxiv.org/abs/2509.11787)
*Pascal Joos,Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: 本文提出CodeCureAgent，利用基于大语言模型的智能体自动分析、分类和修复静态分析警告，在多个指标上表现良好，能助力清理代码库和集成到CI/CD流程。


<details>
  <summary>Details</summary>
Motivation: 传统手动解决静态分析警告过程繁琐，开发者可能忽略警告，导致代码质量下降。

Method: 采用智能体框架，迭代调用工具收集代码库信息并编辑代码，使用三步启发式方法批准补丁。

Result: 在106个Java项目的1000个SonarQube警告数据集上，96.8%的警告产生合理修复，手动检查291个案例的正确修复率为86.3%，每次警告的大语言模型成本约2.9美分，处理时间约4分钟。

Conclusion: CodeCureAgent能可靠修复静态分析警告，可用于清理现有代码库并集成到CI/CD管道。

Abstract: Static analysis tools are widely used to detect bugs, vulnerabilities, and
code smells. Traditionally, developers must resolve these warnings manually.
Because this process is tedious, developers sometimes ignore warnings, leading
to an accumulation of warnings and a degradation of code quality. This paper
presents CodeCureAgent, an approach that harnesses LLM-based agents to
automatically analyze, classify, and repair static analysis warnings. Unlike
previous work, our method does not follow a predetermined algorithm. Instead,
we adopt an agentic framework that iteratively invokes tools to gather
additional information from the codebase (e.g., via code search) and edit the
codebase to resolve the warning. CodeCureAgent detects and suppresses false
positives, while fixing true positives when identified. We equip CodeCureAgent
with a three-step heuristic to approve patches: (1) build the project, (2)
verify that the warning disappears without introducing new warnings, and (3)
run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube
warnings found in 106 Java projects and covering 291 distinct rules. Our
approach produces plausible fixes for 96.8% of the warnings, outperforming
state-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,
respectively. Manual inspection of 291 cases reveals a correct-fix rate of
86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.
The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end
processing time of about four minutes per warning. We envision CodeCureAgent
helping to clean existing codebases and being integrated into CI/CD pipelines
to prevent the accumulation of static analysis warnings.

</details>


### [280] [MMORE: Massive Multimodal Open RAG & Extraction](https://arxiv.org/abs/2509.11937)
*Alexandre Sallinen,Stefan Krsteski,Paul Teiletche,Marc-Antoine Allard,Baptiste Lecoeur,Michael Zhang,Fabrice Nemo,David Kalajdzic,Matthias Meyer,Mary-Anne Hartley*

Main category: cs.SE

TL;DR: 介绍开源管道MMORE用于大规模多模态开放检索增强生成和提取，支持多文件类型，有性能优势，代码开源。


<details>
  <summary>Details</summary>
Motivation: 构建可大规模处理异构文档格式知识的工具，为大语言模型下游应用提供支持。

Method: 采用模块化、分布式处理架构，集成混合密集 - 稀疏检索，支持交互式API和批量RAG端点。

Result: 在处理基准测试中比单节点基线快3.8倍，比Docling在扫描PDF上准确率高40%；增强的医疗大语言模型在PubMedQA上提高生物医学问答准确率。

Conclusion: MMORE为在多样真实世界多模态数据上部署与任务无关的RAG系统提供强大、可扩展基础。

Abstract: We introduce MMORE, an open-source pipeline for Massive Multimodal Open
RetrievalAugmented Generation and Extraction, designed to ingest, transform,
and retrieve knowledge from heterogeneous document formats at scale. MMORE
supports more than fifteen file types, including text, tables, images, emails,
audio, and video, and processes them into a unified format to enable downstream
applications for LLMs. The architecture offers modular, distributed processing,
enabling scalable parallelization across CPUs and GPUs. On processing
benchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines
and 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates
hybrid dense-sparse retrieval and supports both interactive APIs and batch RAG
endpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve
biomedical QA accuracy with increasing retrieval depth. MMORE provides a
robust, extensible foundation for deploying task-agnostic RAG systems on
diverse, real-world multimodal data. The codebase is available at
https://github.com/swiss-ai/mmore.

</details>


### [281] [VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems](https://arxiv.org/abs/2509.11942)
*Luís F. Gomes,Xin Zhou,David Lo,Rui Abreu*

Main category: cs.SE

TL;DR: 本文首次探索用智能体大语言模型系统自动生成可视化文档，提出VisDocSketcher方法和AutoSketchEval评估框架，实验表明该方法效果好，评估框架可靠。


<details>
  <summary>Details</summary>
Motivation: 可视化文档生成和评估困难，现有方法无法直接从代码自动生成高级可视化文档，评估也难标准化和自动化。

Method: 提出VisDocSketcher，结合静态分析与大语言模型智能体识别代码关键元素并生成可视化表示；提出AutoSketchEval评估框架，用代码级指标评估可视化文档质量。

Result: 该方法能为74.4%的样本生成有效可视化文档，比简单模板基线提高26.7 - 39.8%；评估框架能可靠区分高质量和低质量可视化文档，AUC超0.87。

Conclusion: 工作为自动化可视化文档的未来研究奠定基础，提供实用工具。

Abstract: Visual documentation is an effective tool for reducing the cognitive barrier
developers face when understanding unfamiliar code, enabling more intuitive
comprehension. Compared to textual documentation, it provides a higher-level
understanding of the system structure and data flow. Developers usually prefer
visual representations over lengthy textual descriptions for large software
systems. Visual documentation is both difficult to produce and challenging to
evaluate. Manually creating it is time-consuming, and currently, no existing
approach can automatically generate high-level visual documentation directly
from code. Its evaluation is often subjective, making it difficult to
standardize and automate. To address these challenges, this paper presents the
first exploration of using agentic LLM systems to automatically generate visual
documentation. We introduce VisDocSketcher, the first agent-based approach that
combines static analysis with LLM agents to identify key elements in the code
and produce corresponding visual representations. We propose a novel evaluation
framework, AutoSketchEval, for assessing the quality of generated visual
documentation using code-level metrics. The experimental results show that our
approach can valid visual documentation for 74.4% of the samples. It shows an
improvement of 26.7-39.8% over a simple template-based baseline. Our evaluation
framework can reliably distinguish high-quality (code-aligned) visual
documentation from low-quality (non-aligned) ones, achieving an AUC exceeding
0.87. Our work lays the foundation for future research on automated visual
documentation by introducing practical tools that not only generate valid
visual representations but also reliably assess their quality.

</details>


### [282] [LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code Analysis](https://arxiv.org/abs/2509.12021)
*Benedikt Fein,Florian Obermüller,Gordon Fraser*

Main category: cs.SE

TL;DR: 提出LitterBox+框架，扩展Scratch静态代码分析工具LitterBox，结合大语言模型能力，支持文本查询、代码修复等。


<details>
  <summary>Details</summary>
Motivation: Scratch编程环境的图形化符号阻碍大语言模型使用，需克服此局限。

Method: 将基于块的代码转换为适合大语言模型的文本表示，提供编程API并扩展Scratch用户界面，且框架易扩展。

Result: 可让用户对程序和质量问题查询大语言模型、生成代码修复，还提供工具演示视频。

Conclusion: LitterBox+框架能有效结合LitterBox程序分析能力与大语言模型生成特性，提升Scratch编程体验。

Abstract: Large language models (LLMs) have become an essential tool to support
developers using traditional text-based programming languages, but the
graphical notation of the block-based Scratch programming environment inhibits
the use of LLMs. To overcome this limitation, we propose the LitterBox+
framework that extends the Scratch static code analysis tool LitterBox with the
generative abilities of LLMs. By converting block-based code to a textual
representation suitable for LLMs, LitterBox+ allows users to query LLMs about
their programs, about quality issues reported by LitterBox, and it allows
generating code fixes. Besides offering a programmatic API for these
functionalities, LitterBox+ also extends the Scratch user interface to make
these functionalities available directly in the environment familiar to
learners. The framework is designed to be easily extensible with other prompts,
LLM providers, and new features combining the program analysis capabilities of
LitterBox with the generative features of LLMs. We provide a screencast
demonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.

</details>


### [283] [A New Benchmark for Evaluating Code Translation with Third-Party Libraries](https://arxiv.org/abs/2509.12087)
*Pengyu Xue,Kunwu Zheng,Zhen Yang,Yifei Pei,Linhao Wu,Jiahui Dong,Xiapu Luo,Yan Xiao,Fei Liu,Yuxuan Zhang,Xiran Lyu,Xianhang Li,Xuanyu Zhu,Chengyi Wang*

Main category: cs.SE

TL;DR: 构建首个以库为中心的代码翻译基准TransLibEval评估大语言模型，发现含第三方库时性能大幅下降，揭示此前被掩盖的错误。


<details>
  <summary>Details</summary>
Motivation: 现有代码翻译基准在第三方库类别和规模上受限，难以暴露相关错误，而实际编程高度依赖第三方库，需评估大语言模型涉及各类第三方库的代码翻译性能。

Method: 构建TransLibEval基准，涵盖Python、Java和C++的200个涉及第三方库的实际任务，用六种翻译策略评估七个大语言模型，并分析GPT - 4o的4831个失败案例。

Result: 与无库设置相比性能大幅下降（平均CA下降超60%），不同策略有不同优势，分析揭示大量此前被掩盖的第三方引用错误。

Conclusion: 凸显以库为中心的翻译的独特挑战，为提高第三方库感知代码智能提供实践指导。

Abstract: In recent years, Large Language Models (LLMs) have been widely studied in the
code translation field on the method, class, and even repository levels.
However, most of these benchmarks are limited in terms of Third-Party Library
(TPL) categories and scales, making TPL-related errors hard to expose and
hindering the development of targeted solutions. Considering the high
dependence (over 90%) on TPLs in practical programming, demystifying and
analyzing LLMs' code translation performance involving various TPLs becomes
imperative. To address this gap, we construct TransLibEval, the first benchmark
dedicated to library-centric code translation. It consists of 200 real-world
tasks across Python, Java, and C++, each explicitly involving TPLs from diverse
categories such as data processing, machine learning, and web development, with
comprehensive dependency coverage and high-coverage test suites. We evaluate
seven recent LLMs of commercial, general, and code-specialized families under
six translation strategies of three categories: Direct, IR-guided, and
Retrieval-augmented. Experimental results show a dramatic performance drop
compared with library-free settings (average CA decline over 60%), while
diverse strategies demonstrate heterogeneous advantages. Furthermore, we
analyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)
LLMs, revealing numerous third-party reference errors that were obscured
previously. These findings highlight the unique challenges of library-centric
translation and provide practical guidance for improving TPL-aware code
intelligence.

</details>


### [284] [EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression](https://arxiv.org/abs/2509.12159)
*Jingyu Xiao,Zhongyi Zhang,Yuxuan Wan,Yintong Huo,Yang Liu,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出EfficientUICoder压缩框架处理UI2Code任务，实验显示能在不降低网页质量下实现高压缩比并提升效率。


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models在UI2Code任务中计算开销大，存在图像和代码标记冗余问题。

Method: 提出EfficientUICoder，包含Element and Layout - aware Token Compression、Region - aware Token Refinement、Adaptive Duplicate Token Suppression三个组件。

Result: EfficientUICoder实现55% - 60%压缩比，减少计算成本、生成标记、预填充时间和推理时间。

Conclusion: EfficientUICoder能有效解决UI2Code任务中计算开销大的问题，提升效率。

Abstract: Multimodal Large Language Models have demonstrated exceptional performance in
UI2Code tasks, significantly enhancing website development efficiency. However,
these tasks incur substantially higher computational overhead than traditional
code generation due to the large number of input image tokens and extensive
output code tokens required. Our comprehensive study identifies significant
redundancies in both image and code tokens that exacerbate computational
complexity and hinder focus on key UI elements, resulting in excessively
lengthy and often invalid HTML files. We propose EfficientUICoder, a
compression framework for efficient UI code generation with three key
components. First, Element and Layout-aware Token Compression preserves
essential UI information by detecting element regions and constructing UI
element trees. Second, Region-aware Token Refinement leverages attention scores
to discard low-attention tokens from selected regions while integrating
high-attention tokens from unselected regions. Third, Adaptive Duplicate Token
Suppression dynamically reduces repetitive generation by tracking HTML/CSS
structure frequencies and applying exponential penalties. Extensive experiments
show EfficientUICoderachieves a 55%-60% compression ratio without compromising
webpage quality and delivers superior efficiency improvements: reducing
computational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,
and inference time by 48.8% on 34B-level MLLMs. Code is available at
https://github.com/WebPAI/EfficientUICoder.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [285] [Meta-Learning Neural Process for Implied Volatility Surfaces with SABR-induced Priors](https://arxiv.org/abs/2509.11928)
*Jirong Zhuang,Xuan Wu*

Main category: q-fin.CP

TL;DR: 将隐含波动率曲面（IVS）构建重新定义为元学习问题，引入基于注意力的Volatility Neural Process模型，在S&P 500期权数据上表现优于其他模型，SABR先验能降低误差。


<details>
  <summary>Details</summary>
Motivation: 解决每日重新校准问题，构建稳定的隐含波动率曲面。

Method: 将IVS构建作为元学习问题，使用Volatility Neural Process模型，进行两阶段训练，先在SABR生成的曲面上预训练，再在市场数据上微调。

Result: 在S&P 500期权（2006 - 2023；样本外2019 - 2023）上，模型优于SABR、SSVI、高斯过程等，SABR先验使RMSE降低约40%，在报价稀疏的中长到期区域表现更好。

Conclusion: 学习到的先验能抑制大误差，提供了一种实用、数据高效的方法，用单一可部署模型稳定构建IVS。

Abstract: Constructing the implied volatility surface (IVS) is reframed as a
meta-learning problem training across trading days to learn a general process
that reconstructs a full IVS from few quotes, eliminating daily recalibration.
We introduce the Volatility Neural Process, an attention-based model that uses
a two-stage training: pre-training on SABR-generated surfaces to encode a
financial prior, followed by fine-tuning on market data. On S&P 500 options
(2006-2023; out-of-sample 2019-2023), our model outperforms SABR, SSVI,
Gaussian Process, and an ablation trained only on real data. Relative to the
ablation, the SABR-induced prior reduces RMSE by about 40% and dominates in
mid- and long-maturity regions where quotes are sparse. The learned prior
suppresses large errors, providing a practical, data-efficient route to stable
IVS construction with a single deployable model.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [286] [Stabilising Lifetime PD Models under Forecast Uncertainty](https://arxiv.org/abs/2509.10586)
*Vahab Rostampour*

Main category: q-fin.RM

TL;DR: 本文在状态空间框架下重新表述问题，引入锚定观测模型，模拟显示该模型可减少预测噪声。


<details>
  <summary>Details</summary>
Motivation: IFRS - 9和CECL下估计终身违约概率时，宏观经济预测误差跨期累积，导致PD期限结构不稳定和波动大。

Method: 在状态空间框架下重新表述问题，先采用直接卡尔曼滤波器，后引入将中性长期经济状态纳入滤波器的锚定观测模型。

Result: 误差动态具有渐近随机稳定性，确保终身PD期限结构概率收敛；模拟显示锚定减少了预测噪声，使预测更平滑、更易解释。

Conclusion: 引入的锚定观测模型能有效解决估计终身违约概率时的问题，改善预测效果。

Abstract: Estimating lifetime probabilities of default (PDs) under IFRS~9 and CECL
requires projecting point--in--time transition matrices over multiple years. A
persistent weakness is that macroeconomic forecast errors compound across
horizons, producing unstable and volatile PD term structures. This paper
reformulates the problem in a state--space framework and shows that a direct
Kalman filter leaves non--vanishing variability. We then introduce an anchored
observation model, which incorporates a neutral long--run economic state into
the filter. The resulting error dynamics exhibit asymptotic stochastic
stability, ensuring convergence in probability of the lifetime PD term
structure. Simulation on a synthetic corporate portfolio confirms that
anchoring reduces forecast noise and delivers smoother, more interpretable
projections.

</details>


### [287] [Why Bonds Fail Differently? Explainable Multimodal Learning for Multi-Class Default Prediction](https://arxiv.org/abs/2509.10802)
*Yi Lu,Aifan Ling,Chaoqun Wang,Yaxin Xu*

Main category: q-fin.RM

TL;DR: 提出新型多类债券违约预测框架EMDLOT，结合多源数据，实验显示其性能优于基准模型，为透明金融风险建模提供实用工具和可靠框架。


<details>
  <summary>Details</summary>
Motivation: 中国债券市场违约增多，传统机器学习模型难以捕捉金融数据不规则性和时间依赖性，多数深度学习模型缺乏可解释性，无法满足金融决策需求。

Method: 提出EMDLOT框架，整合数值时间序列和非结构化文本数据，使用Time - Aware LSTM处理不规则序列，采用软聚类和多级注意力机制提高可解释性。

Result: 在1994家中国企业（2015 - 2024）的实验中，EMDLOT在召回率、F1分数和mAP方面优于传统和深度学习基准模型，消融研究验证各组件价值，注意力分析揭示经济上直观的违约驱动因素。

Conclusion: 该研究为透明金融风险建模提供了实用工具和可靠框架。

Abstract: In recent years, China's bond market has seen a surge in defaults amid
regulatory reforms and macroeconomic volatility. Traditional machine learning
models struggle to capture financial data's irregularity and temporal
dependencies, while most deep learning models lack interpretability-critical
for financial decision-making. To tackle these issues, we propose EMDLOT
(Explainable Multimodal Deep Learning for Time-series), a novel framework for
multi-class bond default prediction. EMDLOT integrates numerical time-series
(financial/macroeconomic indicators) and unstructured textual data (bond
prospectuses), uses Time-Aware LSTM to handle irregular sequences, and adopts
soft clustering and multi-level attention to boost interpretability.
Experiments on 1994 Chinese firms (2015-2024) show EMDLOT outperforms
traditional (e.g., XGBoost) and deep learning (e.g., LSTM) benchmarks in
recall, F1-score, and mAP, especially in identifying default/extended firms.
Ablation studies validate each component's value, and attention analyses reveal
economically intuitive default drivers. This work provides a practical tool and
a trustworthy framework for transparent financial risk modeling.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [288] [Momentum-integrated Multi-task Stock Recommendation with Converge-based Optimization](https://arxiv.org/abs/2509.10461)
*Hao Wang,Jingshu Peng,Yanyan Shen,Xujia Li,Lei Chen*

Main category: q-fin.ST

TL;DR: 提出MiM - StocR多任务学习框架用于股票推荐，引入动量线指标、Adaptive - k ApproxNDCG损失函数和CQB方法，实验显示其优于现有MTL基线。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测训练难以同时捕捉股票趋势和排名，现有MTL框架应用于股票时间序列存在过拟合问题。

Method: 引入MiM - StocR框架，调用动量线指标，提出Adaptive - k ApproxNDCG损失函数，采用CQB方法。

Result: 在SEE50、CSI 100和CSI 300三个股票基准上实验，MiM - StocR在排名和盈利评估中均优于现有MTL基线。

Conclusion: MiM - StocR框架在股票推荐中有效且性能良好。

Abstract: Stock recommendation is critical in Fintech applications, which use price
series and alternative information to estimate future stock performance.
Although deep learning models are prevalent in stock recommendation systems,
traditional time-series forecasting training often fails to capture stock
trends and rankings simultaneously, which are essential consideration factors
for investors. To tackle this issue, we introduce a Multi-Task Learning (MTL)
framework for stock recommendation, \textbf{M}omentum-\textbf{i}ntegrated
\textbf{M}ulti-task \textbf{Stoc}k \textbf{R}ecommendation with Converge-based
Optimization (\textbf{MiM-StocR}). To improve the model's ability to capture
short-term trends, we novelly invoke a momentum line indicator in model
training. To prioritize top-performing stocks and optimize investment
allocation, we propose a list-wise ranking loss function called Adaptive-k
ApproxNDCG. Moreover, due to the volatility and uncertainty of the stock
market, existing MTL frameworks face overfitting issues when applied to stock
time series. To mitigate this issue, we introduce the Converge-based
Quad-Balancing (CQB) method. We conducted extensive experiments on three stock
benchmarks: SEE50, CSI 100, and CSI 300. MiM-StocR outperforms state-of-the-art
MTL baselines across both ranking and profitable evaluations.

</details>


### [289] [Equity Premium Prediction: Taking into Account the Role of Long, even Asymmetric, Swings in Stock Market Behavior](https://arxiv.org/abs/2509.10483)
*Kuok Sin Un,Marcel Ausloos*

Main category: q-fin.ST

TL;DR: 本文通过新方法研究股市行为变化对股票风险溢价可预测性的影响，发现牛市指数正负冲击与不同预测期和指标的风险溢价可预测性相关。


<details>
  <summary>Details</summary>
Motivation: 探究股市行为的重大变化对股票风险溢价可预测性的影响。

Method: 引入“牛市指数”衡量股市行为变化，采用“波动去趋势移动平均分析”（FDMAA），考虑28个指标。

Result: 牛市指数“正冲击”与基于宏观经济变量长达6个月的强股票风险溢价可预测性密切相关；“负冲击”与基于技术指标长达9个月的强股票风险溢价可预测性相关。

Conclusion: 股市行为变化对股票风险溢价可预测性有显著影响，且不同冲击与不同预测期和指标相关。

Abstract: Through a novel approach, this paper shows that substantial change in stock
market behavior has a statistically and economically significant impact on
equity risk premium predictability both on in-sample and out-of-sample cases.
In line with Auer's ''Bullish ratio'', a ''Bullish index'' is introduced to
measure the changes in stock market behavior, which we describe through a
''fluctuation detrending moving average analysis'' (FDMAA) for returns. We
consider 28 indicators. We find that a ''positive shock'' of the Bullish Index
is closely related to strong equity risk premium predictability for forecasts
based on macroeconomic variables for up to six months. In contrast, a
''negative shock'' is associated with strong equity risk premium predictability
with adequate forecasts for up to nine months when based on technical
indicators.

</details>


### [290] [Adaptive Temporal Fusion Transformers for Cryptocurrency Price Prediction](https://arxiv.org/abs/2509.10542)
*Arash Peik,Mohammad Ali Zare Chahooki,Amin Milani Fard,Mehdi Agha Sarram*

Main category: q-fin.ST

TL;DR: 论文提出自适应TFT建模方法用于加密货币短期价格预测，实验表明该方法优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场高度波动，直接使用Temporal Fusion Transformers (TFTs) 进行短期价格预测面临市场非平稳性和极端波动性挑战，需要更有效的方法。

Method: 引入自适应TFT建模方法，采用动态子序列长度和基于模式的分类，提出以相对最大值结束子序列的分割方法，根据固定长度模式对后续可变长度子序列分类，为每个类别训练不同的TFT模型。

Result: 在ETH - USDT 10分钟数据两个月测试期的实验中，自适应方法在预测准确性和模拟交易盈利能力上显著优于基线固定长度TFT和LSTM模型。

Conclusion: 自适应分割和模式条件预测的结合使加密货币价格预测更稳健、更具响应性。

Abstract: Precise short-term price prediction in the highly volatile cryptocurrency
market is critical for informed trading strategies. Although Temporal Fusion
Transformers (TFTs) have shown potential, their direct use often struggles in
the face of the market's non-stationary nature and extreme volatility. This
paper introduces an adaptive TFT modeling approach leveraging dynamic subseries
lengths and pattern-based categorization to enhance short-term forecasting. We
propose a novel segmentation method where subseries end at relative maxima,
identified when the price increase from the preceding minimum surpasses a
threshold, thus capturing significant upward movements, which act as key
markers for the end of a growth phase, while potentially filtering the noise.
Crucially, the fixed-length pattern ending each subseries determines the
category assigned to the subsequent variable-length subseries, grouping typical
market responses that follow similar preceding conditions. A distinct TFT model
trained for each category is specialized in predicting the evolution of these
subsequent subseries based on their initial steps after the preceding peak.
Experimental results on ETH-USDT 10-minute data over a two-month test period
demonstrate that our adaptive approach significantly outperforms baseline
fixed-length TFT and LSTM models in prediction accuracy and simulated trading
profitability. Our combination of adaptive segmentation and pattern-conditioned
forecasting enables more robust and responsive cryptocurrency price prediction.

</details>


### [291] [A Stochastic Model for Illiquid Stock Prices and its Conclusion about Correlation Measurement](https://arxiv.org/abs/2509.10553)
*Erina Nanyonga,Juma Kasozi,Fred Mayambala,Hassan W. Kayondo,Matt Davison*

Main category: q-fin.ST

TL;DR: 研究乌干达证券交易所非流动性股票价格行为动态，结合模型建模，发现股票低相关性源于非流动性。


<details>
  <summary>Details</summary>
Motivation: 探索上市股票市场中非流动性股票价格的行为动态，非流动性影响价格形成且使价格建模复杂。

Method: 将马尔可夫模型与指数奥恩斯坦 - 乌伦贝克模型和几何布朗运动模型结合，同时也用后两者单独建模，采用理论分析、模拟研究和实证分析。

Result: 乌干达证券交易所股票间相关性低。

Conclusion: 股票的低相关性是由非流动性导致，如马尔可夫链从零状态到零状态过渡较高时，高相关性的几何布朗运动部分模拟数据测量出低相关性。

Abstract: This study explores the behavioral dynamics of illiquid stock prices in a
listed stock market. Illiquidity, characterized by wide bid and ask spreads
affects price formation by decoupling prices from standard risk and return
relationships and increasing sensitivity to market sentiment. We model the
prices at the Uganda Securities Exchange (USE) which is illiquid in that the
prices remain constant much of the time thus complicating price modelling. We
circumvent this challenge by combining the Markov model (MM) with two models;
the exponential Ornstein Uhlenbeck model (XOU) and geometric Brownian motion
(gBm). In the combined models, the MM was used to capture the constant prices
in the stock prices while the XOU and gBm captured the stochastic price
dynamics. We modelled stock prices using the combined models, as well as XOU
and gBm alone. We found that USE stocks appeared to have low correlation with
one another. Using theoretical analysis, simulation study and empirical
analysis, we conclude that this apparent low correlation is due to illiquidity.
In particular data simulated from combined MM-gBm, in which the gBm portion
were highly correlated resulted in a low measured correlation when the Markov
chain had a higher transition from zero state to zero state.

</details>


### [292] [ProteuS: A Generative Approach for Simulating Concept Drift in Financial Markets](https://arxiv.org/abs/2509.11844)
*Andrés L. Suárez-Cetrulo,Alejandro Cervantes,David Quintana*

Main category: q-fin.ST

TL;DR: 本文提出ProteuS框架生成半合成金融时间序列，用于评估概念漂移检测和适应机制，为金融预测模型提供工具。


<details>
  <summary>Details</summary>
Motivation: 金融市场数据分布会随时间变化，传统模型应对困难，且缺乏真实世界金融数据的地面真值来评估模型检测和恢复漂移的能力。

Method: 引入ProteuS框架，对真实世界ETF数据拟合ARMA - GARCH模型以捕捉不同市场状态，模拟它们之间的过渡，生成包含技术指标的数据集。

Result: 生成的数据集分析确认了任务的复杂性，不同市场状态存在显著重叠。

Conclusion: 为研究界提供严格评估概念漂移检测和适应机制的工具，为更稳健的金融预测模型铺平道路。

Abstract: Financial markets are complex, non-stationary systems where the underlying
data distributions can shift over time, a phenomenon known as regime changes,
as well as concept drift in the machine learning literature. These shifts,
often triggered by major economic events, pose a significant challenge for
traditional statistical and machine learning models. A fundamental problem in
developing and validating adaptive algorithms is the lack of a ground truth in
real-world financial data, making it difficult to evaluate a model's ability to
detect and recover from these drifts. This paper addresses this challenge by
introducing a novel framework, named ProteuS, for generating semi-synthetic
financial time series with pre-defined structural breaks. Our methodology
involves fitting ARMA-GARCH models to real-world ETF data to capture distinct
market regimes, and then simulating realistic, gradual, and abrupt transitions
between them. The resulting datasets, which include a comprehensive set of
technical indicators, provide a controlled environment with a known ground
truth of regime changes. An analysis of the generated data confirms the
complexity of the task, revealing significant overlap between the different
market states. We aim to provide the research community with a tool for the
rigorous evaluation of concept drift detection and adaptation mechanisms,
paving the way for more robust financial forecasting models.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [293] [Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.11420)
*Yijia Xiao,Edward Sun,Tong Chen,Fang Wu,Di Luo,Wei Wang*

Main category: q-fin.TR

TL;DR: 提出金融感知模型Trading - R1，经训练评估，风险调整回报更好、回撤更低，能生成结构化投资论点，相关终端将开源。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型缺乏可解释性，大语言模型难以将自然语言分析转化为可执行交易，推理大语言模型在金融决策应用待探索。

Method: 通过监督微调与强化学习，采用三阶段由易到难课程，使用Tauric - TR1 - DB语料库训练。

Result: 在六只主要股票和ETF上评估，相比其他模型有更好的风险调整回报和更低的回撤。

Conclusion: Trading - R1能生成结构化、基于证据的投资论点，支持有纪律且可解释的交易决策。

Abstract: Developing professional, structured reasoning on par with human financial
analysts and traders remains a central challenge in AI for finance, where
markets demand interpretability and trust. Traditional time-series models lack
explainability, while LLMs face challenges in turning natural-language analysis
into disciplined, executable trades. Although reasoning LLMs have advanced in
step-by-step planning and verification, their application to risk-sensitive
financial decisions is underexplored. We present Trading-R1, a
financially-aware model that incorporates strategic thinking and planning for
comprehensive thesis composition, facts-grounded analysis, and
volatility-adjusted decision making. Trading-R1 aligns reasoning with trading
principles through supervised fine-tuning and reinforcement learning with a
three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample
corpus spanning 18 months, 14 equities, and five heterogeneous financial data
sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates
improved risk-adjusted returns and lower drawdowns compared to both open-source
and proprietary instruction-following models as well as reasoning models. The
system generates structured, evidence-based investment theses that support
disciplined and interpretable trading decisions. Trading-R1 Terminal will be
released at https://github.com/TauricResearch/Trading-R1.

</details>


### [294] [Sentiment Feedback in Equity Markets: Asymmetries, Retail Heterogeneity, and Structural Calibration](https://arxiv.org/abs/2509.11970)
*Lucas Marques Sneller*

Main category: q-fin.TR

TL;DR: 研究情绪冲击通过股票回报和投资者群体的传播，得出定价影响、投资组合收益等结果及三个规律。


<details>
  <summary>Details</summary>
Motivation: 探究情绪冲击在股票市场中的传播机制和影响。

Method: 使用四个独立代理和符号对齐的kappa - rho参数进行研究，进行结构校准。

Result: 一个标准差的情绪创新定价影响为1.06个基点，D10 - D1投资组合每月收益4.0个基点，发现三个规律。

Conclusion: 校准动态能调和适度影响估计与可观多空回报，情绪冲击在低广度股票中影响更大且有波动性状态依赖。

Abstract: We study how sentiment shocks propagate through equity returns and investor
clientele using four independent proxies with sign-aligned kappa-rho
parameters. A structural calibration links a one standard deviation innovation
in sentiment to a pricing impact of 1.06 basis points with persistence
parameter rho = 0.940, yielding a half-life of 11.2 months. The impulse
response peaks around the 12-month horizon, indicating slow-moving
amplification. Cross-sectionally, a simple D10-D1 portfolio earns 4.0 basis
points per month with Sharpe ratios of 0.18-0.85, consistent with tradable
exposure to the sentiment factor. Three regularities emerge: (i) positive
sentiment innovations transmit more strongly than negative shocks
(amplification asymmetry); (ii) effects are concentrated in retail-tilted and
non-optionable stocks (clientele heterogeneity); and (iii) responses are
state-dependent across volatility regimes - larger on impact in high-VIX months
but more persistent in low-VIX months. Baseline time-series fits are
parsimonious (R2 ~ 0.001; 420 monthly observations), yet the calibrated
dynamics reconcile modest impact estimates with sizable long-short payoffs.
Consistent with Miller (1977), a one standard deviation sentiment shock has
1.72-8.69 basis points larger effects in low-breadth stocks across horizons of
1-12 months, is robust to institutional flows, and exhibits volatility state
dependence - larger on impact but less persistent in high-VIX months, smaller
on impact but more persistent in low-VIX months.

</details>


### [295] [Bootstrapping Liquidity in BTC-Denominated Prediction Markets](https://arxiv.org/abs/2509.11990)
*Fedor Shabashev*

Main category: q-fin.TR

TL;DR: 本文探讨比特币计价的预测市场，分析三种提供流动性的方法，指出其可行性取决于流动性机制及用户安全与部署便利性的权衡。


<details>
  <summary>Details</summary>
Motivation: 非生息稳定币计价的预测市场存在效率问题，参与者有机会成本，比特币持有者转换时失去升值机会，因此探索比特币计价的预测市场。

Method: 分析三种为新创建的比特币计价预测市场提供流动性的方法：跨市场做市、自动做市和基于DeFi的用户交易重定向，并评估执行机制、风险和资本效率。

Result: 跨市场做市风险低但需专业做市商或平台补贴；DeFi重定向可快速启动但有清算风险和汇率波动，资本效率低；自动做市部署简单但资本效率低且有永久损失风险。

Conclusion: 比特币计价的预测市场可行，但其成功关键在于流动性提供机制的选择以及用户安全与部署便利性的权衡。

Abstract: Prediction markets have gained adoption as on-chain mechanisms for
aggregating information, with platforms such as Polymarket demonstrating demand
for stablecoin-denominated markets. However, denominating in
non-interest-bearing stablecoins introduces inefficiencies: participants face
opportunity costs relative to the fiat risk-free rate, and Bitcoin holders in
particular lose exposure to BTC appreciation when converting into stablecoins.
This paper explores the case for prediction markets denominated in Bitcoin,
treating BTC as a deflationary settlement asset analogous to gold under the
classical gold standard. We analyse three methods of supplying liquidity to a
newly created BTC-denominated prediction market: cross-market making against
existing stablecoin venues, automated market making, and DeFi-based redirection
of user trades. For each approach we evaluate execution mechanics, risks
(slippage, exchange-rate risk, and liquidation risk), and capital efficiency.
Our analysis shows that cross-market making provides the most user-friendly
risk profile, though it requires active professional makers or
platform-subsidised liquidity. DeFi redirection offers rapid bootstrapping and
reuse of existing USDC liquidity, but exposes users to liquidation thresholds
and exchange-rate volatility, reducing capital efficiency. Automated market
making is simple to deploy but capital-inefficient and exposes liquidity
providers to permanent loss. The results suggest that BTC-denominated
prediction markets are feasible, but their success depends critically on the
choice of liquidity provisioning mechanism and the trade-off between user
safety and deployment convenience.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [296] [Variable Selection Using Relative Importance Rankings](https://arxiv.org/abs/2509.10853)
*Tien-En Chang,Argon Chen*

Main category: stat.ML

TL;DR: 本文探讨相对重要性（RI）分析用于变量排序和选择的潜力，实现并评估基于RI的变量选择方法，结果显示其表现出色，是强大且有竞争力的替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前文献中变量选择和RI分析处理方式不同，RI通常用于事后模型解释，本文探索其在模型创建前用于变量排序和选择的潜力。

Method: 实现并评估基于RI的变量选择方法，使用一般优势（GD）、综合相对重要性（CRI）和新提出的CRI.Z。

Result: RI度量比边际相关性更准确地对变量进行排序，基于这些排序构建的预测模型表现出色，常优于最先进方法，在处理高度相关预测变量簇时特别有效。

Conclusion: 基于RI的方法是强大且有竞争力的替代方案，这些未充分利用的工具值得统计和机器学习社区更多关注。

Abstract: Although conceptually related, variable selection and relative importance
(RI) analysis have been treated quite differently in the literature. While RI
is typically used for post-hoc model explanation, this paper explores its
potential for variable ranking and filter-based selection before model
creation. Specifically, we anticipate strong performance from the RI measures
because they incorporate both direct and combined effects of predictors,
addressing a key limitation of marginal correlation that ignores dependencies
among predictors. We implement and evaluate the RI-based variable selection
methods using general dominance (GD), comprehensive relative importance (CRI),
and a newly proposed, computationally efficient variant termed CRI.Z.
  We first demonstrate how the RI measures more accurately rank the variables
than the marginal correlation, especially when there are suppressed or weak
predictors. We then show that predictive models built on these rankings are
highly competitive, often outperforming state-of-the-art methods such as the
lasso and relaxed lasso. The proposed RI-based methods are particularly
effective in challenging cases involving clusters of highly correlated
predictors, a setting known to cause failures in many benchmark methods.
Although lasso methods have dominated the recent literature on variable
selection, our study reveals that the RI-based method is a powerful and
competitive alternative. We believe these underutilized tools deserve greater
attention in statistics and machine learning communities. The code is available
at: https://github.com/tien-endotchang/RI-variable-selection.

</details>


### [297] [Kernel-based Stochastic Approximation Framework for Nonlinear Operator Learning](https://arxiv.org/abs/2509.11070)
*Jia-Qi Yang,Lei Shi*

Main category: stat.ML

TL;DR: 本文提出用Mercer算子值核学习无限维空间非线性算子的随机逼近框架，给出收敛率并实验验证。


<details>
  <summary>Details</summary>
Motivation: 开发能学习无限维空间非线性算子的方法，克服维数灾难，适应多种算子学习任务。

Method: 利用一般Mercer算子值核构建随机逼近框架，引入向量值插值空间量化误差。

Result: 建立无维度多项式收敛率，可进行本质非线性算子学习。

Conclusion: 该框架有效，适用于多种算子学习任务，通过二维Navier - Stokes方程实验验证。

Abstract: We develop a stochastic approximation framework for learning nonlinear
operators between infinite-dimensional spaces utilizing general Mercer
operator-valued kernels. Our framework encompasses two key classes: (i) compact
kernels, which admit discrete spectral decompositions, and (ii) diagonal
kernels of the form $K(x,x')=k(x,x')T$, where $k$ is a scalar-valued kernel and
$T$ is a positive operator on the output space. This broad setting induces
expressive vector-valued reproducing kernel Hilbert spaces (RKHSs) that
generalize the classical $K=kI$ paradigm, thereby enabling rich structural
modeling with rigorous theoretical guarantees. To address target operators
lying outside the RKHS, we introduce vector-valued interpolation spaces to
precisely quantify misspecification error. Within this framework, we establish
dimension-free polynomial convergence rates, demonstrating that nonlinear
operator learning can overcome the curse of dimensionality. The use of general
operator-valued kernels further allows us to derive rates for intrinsically
nonlinear operator learning, going beyond the linear-type behavior inherent in
diagonal constructions of $K=kI$. Importantly, this framework accommodates a
wide range of operator learning tasks, ranging from integral operators such as
Fredholm operators to architectures based on encoder-decoder representations.
Moreover, we validate its effectiveness through numerical experiments on the
two-dimensional Navier-Stokes equations.

</details>


### [298] [Maximum diversity, weighting and invariants of time series](https://arxiv.org/abs/2509.11146)
*Byungchang So*

Main category: stat.ML

TL;DR: 本文聚焦幅度加权连续性及其对应最大多样性的变化，还为时间序列分析提供应用并通过实验验证不变量效果。


<details>
  <summary>Details</summary>
Motivation: 已有研究从多视角解释幅度含义，连续性为幅度研究提供新视角，且近期研究揭示幅度与数据分析的联系，因此开展相关研究。

Method: 基于幅度连续性和最大多样性的现有结果进行研究，引入周期性时间序列的新不变量，并进行机器学习实验。

Result: 引入的周期性时间序列新不变量在机器学习实验中提升了性能。

Conclusion: 对幅度加权连续性及变化的研究有价值，引入的不变量可用于时间序列分析并提升性能。

Abstract: Magnitude, obtained as a special case of Euler characteristic of enriched
category, represents a sense of the size of metric spaces and is related to
classical notions such as cardinality, dimension, and volume. While the studies
have explained the meaning of magnitude from various perspectives, continuity
also gives a valuable view of magnitude. Based on established results about
continuity of magnitude and maximum diversity, this article focuses on
continuity of weighting, a distribution whose totality is magnitude, and its
variation corresponding to maximum diversity. Meanwhile, recent studies also
illuminated the connection between magnitude and data analysis by applying
magnitude theory to point clouds representing the data or the set of model
parameters. This article will also provide an application for time series
analysis by introducing a new kind of invariants of periodic time series, where
the invariance follows directly from the continuity results. As a use-case, a
simple machine learning experiment is conducted with real-world data, in which
the suggested invariants improved the performance.

</details>


### [299] [Predictable Compression Failures: Why Language Models Actually Hallucinate](https://arxiv.org/abs/2509.11208)
*Leon Chlon,Ahmed Karim,Maggie Chlon*

Main category: stat.ML

TL;DR: 大语言模型在可交换数据上近贝叶斯推理但违反排列不变性，研究揭示其原因并推导相关理论和规划器，有实证结果，能将幻觉转化为可预测的压缩失败。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在可交换数据上进行近贝叶斯推理时违反排列不变性的问题。

Method: 证明变压器最小化排序上的预期条件描述长度，推导量化鞅违反界限、期望级解压缩定律和可部署规划器。

Result: 排列分散遵循a + b ln n，排列混合提高似然/准确性，随机剂量响应使幻觉减少，预指定审计可通过校准拒绝降低幻觉。

Conclusion: 该框架能将幻觉转化为可预测的压缩失败，实现有原则的信息预算。

Abstract: Large language models perform near-Bayesian inference yet violate permutation
invariance on exchangeable data. We resolve this by showing transformers
minimize expected conditional description length (cross-entropy) over
orderings, $\mathbb{E}_\pi[\ell(Y \mid \Gamma_\pi(X))]$, which admits a
Kolmogorov-complexity interpretation up to additive constants, rather than the
permutation-invariant description length $\ell(Y \mid X)$. This makes them
Bayesian in expectation, not in realization. We derive (i) a Quantified
Martingale Violation bound showing order-induced deviations scale as $O(\log
n)$ with constants; (ii) the Expectation-level Decompression Law linking
information budgets to reliability for Bernoulli predicates; and (iii)
deployable planners (B2T/RoH/ISR) for answer/abstain decisions. Empirically,
permutation dispersion follows $a+b\ln n$ (Qwen2-7B $b \approx 0.377$,
Llama-3.1-8B $b \approx 0.147$); permutation mixtures improve ground-truth
likelihood/accuracy; and randomized dose-response shows hallucinations drop by
$\sim 0.13$ per additional nat. A pre-specified audit with a fixed ISR=1.0
achieves near-0\% hallucinations via calibrated refusal at 24\% abstention. The
framework turns hallucinations into predictable compression failures and
enables principled information budgeting.

</details>


### [300] [Contrastive Network Representation Learning](https://arxiv.org/abs/2509.11316)
*Zihan Dong,Xin Zhou,Ryumei Nakada,Lexin Li,Linjun Zhang*

Main category: stat.ML

TL;DR: 提出名为ACERL的基于对比学习的网络边缘嵌入统计方法，建立误差界，证明收敛率最优，验证在下游任务的适用性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决脑连接数据分析中特定于主体、高维稀疏且缺乏节点或边协变量的网络分析挑战。

Method: 提出ACERL方法，包含增强网络对的对比学习和数据驱动的自适应随机掩码机制。

Result: 建立非渐近误差界，达到边缘表示学习的极小极大最优收敛率，在多个下游任务展示适用性和理论保证，通过实验验证性能优于基线方法。

Conclusion: ACERL方法在网络边缘嵌入和下游任务中有良好表现，具有一定理论保证和实际应用价值。

Abstract: Network representation learning seeks to embed networks into a
low-dimensional space while preserving the structural and semantic properties,
thereby facilitating downstream tasks such as classification, trait prediction,
edge identification, and community detection. Motivated by challenges in brain
connectivity data analysis that is characterized by subject-specific,
high-dimensional, and sparse networks that lack node or edge covariates, we
propose a novel contrastive learning-based statistical approach for network
edge embedding, which we name as Adaptive Contrastive Edge Representation
Learning (ACERL). It builds on two key components: contrastive learning of
augmented network pairs, and a data-driven adaptive random masking mechanism.
We establish the non-asymptotic error bounds, and show that our method achieves
the minimax optimal convergence rate for edge representation learning. We
further demonstrate the applicability of the learned representation in multiple
downstream tasks, including network classification, important edge detection,
and community detection, and establish the corresponding theoretical
guarantees. We validate our method through both synthetic data and real brain
connectivities studies, and show its competitive performance compared to the
baseline method of sparse principal components analysis.

</details>


### [301] [A Particle-Flow Algorithm for Free-Support Wasserstein Barycenters](https://arxiv.org/abs/2509.11435)
*Kisung You*

Main category: stat.ML

TL;DR: 本文提出一种无支撑算法计算Wasserstein重心，避免熵正则化，有理论保证且实证效果好。


<details>
  <summary>Details</summary>
Motivation: 将欧几里得均值扩展到概率测度空间，避免熵正则化计算Wasserstein重心。

Method: 让重心原子作为粒子，由平均最优传输位移驱动演化，用最优传输计划的重心投影代替Monge映射。

Result: 建立了理论保证，实证研究证明方法在多任务中有准确性和可扩展性。

Conclusion: 该粒子流方法是线性规划和正则化求解器的可行替代方案。

Abstract: The Wasserstein barycenter extends the Euclidean mean to the space of
probability measures by minimizing the weighted sum of squared 2-Wasserstein
distances. We develop a free-support algorithm for computing Wasserstein
barycenters that avoids entropic regularization and instead follows the formal
Riemannian geometry of Wasserstein space. In our approach, barycenter atoms
evolve as particles advected by averaged optimal-transport displacements, with
barycentric projections of optimal transport plans used in place of Monge maps
when the latter do not exist. This yields a geometry-aware particle-flow update
that preserves sharp features of the Wasserstein barycenter while remaining
computationally tractable. We establish theoretical guarantees, including
consistency of barycentric projections, monotone descent and convergence to
stationary points, stability with respect to perturbations of the inputs, and
resolution consistency as the number of atoms increases. Empirical studies on
averaging probability distributions, Bayesian posterior aggregation, image
prototypes and classification, and large-scale clustering demonstrate accuracy
and scalability of the proposed particle-flow approach, positioning it as a
principled alternative to both linear programming and regularized solvers.

</details>


### [302] [Next-Generation Reservoir Computing for Dynamical Inference](https://arxiv.org/abs/2509.11338)
*Rok Cestnik,Erik A. Martens*

Main category: stat.ML

TL;DR: 提出用于从时间序列数据建模动力系统的下一代储层计算的简单可扩展实现，应用于基准任务表现良好，适用于替代建模和数字孪生应用。


<details>
  <summary>Details</summary>
Motivation: 为从时间序列数据建模动力系统提供新的计算方法。

Method: 使用时间延迟嵌入输入的伪随机非线性投影，提供任意维度的特征空间。

Result: 模型在长滚动预测中保持稳定，能超越训练数据进行泛化。

Conclusion: 该框架可精确控制系统状态，适合替代建模和数字孪生应用。

Abstract: We present a simple and scalable implementation of next-generation reservoir
computing for modeling dynamical systems from time series data. Our approach
uses a pseudorandom nonlinear projection of time-delay embedded input, allowing
an arbitrary dimension of the feature space, thus providing a flexible
alternative to the polynomial-based projections used in previous
next-generation reservoir computing variants. We apply the method to benchmark
tasks -- including attractor reconstruction and bifurcation diagram estimation
-- using only partial and noisy observations. We also include an exploratory
example of estimating asymptotic oscillation phases. The models remain stable
over long rollouts and generalize beyond training data. This framework enables
the precise control of system state and is well suited for surrogate modeling
and digital twin applications.

</details>


### [303] [Some Robustness Properties of Label Cleaning](https://arxiv.org/abs/2509.11379)
*Chen Cheng,John Duchi*

Main category: stat.ML

TL;DR: 研究表明依赖聚合标签的学习程序具有无需数据清洗的鲁棒性，能在多种方面体现，且在一些情况下有更好效果。


<details>
  <summary>Details</summary>
Motivation: 探究依赖聚合标签的学习程序的鲁棒性特点及优势。

Method: 通过对比使用聚合标签的程序与使用原始标签的程序在风险一致性、模型拟合等方面的表现。

Result: 使用聚合标签的程序在风险一致性上有更强保证，在损失函数轻微错误指定时仍能收敛到最优分类器。

Conclusion: 结合数据分析全流程，利用聚合信息可通过提炼噪声信号产生更鲁棒的方法。

Abstract: We demonstrate that learning procedures that rely on aggregated labels, e.g.,
label information distilled from noisy responses, enjoy robustness properties
impossible without data cleaning. This robustness appears in several ways. In
the context of risk consistency -- when one takes the standard approach in
machine learning of minimizing a surrogate (typically convex) loss in place of
a desired task loss (such as the zero-one mis-classification error) --
procedures using label aggregation obtain stronger consistency guarantees than
those even possible using raw labels. And while classical statistical scenarios
of fitting perfectly-specified models suggest that incorporating all possible
information -- modeling uncertainty in labels -- is statistically efficient,
consistency fails for ``standard'' approaches as soon as a loss to be minimized
is even slightly mis-specified. Yet procedures leveraging aggregated
information still converge to optimal classifiers, highlighting how
incorporating a fuller view of the data analysis pipeline, from collection to
model-fitting to prediction time, can yield a more robust methodology by
refining noisy signals.

</details>


### [304] [Learning Majority-to-Minority Transformations with MMD and Triplet Loss for Imbalanced Classification](https://arxiv.org/abs/2509.11511)
*Suman Cha,Hyunjoong Kim*

Main category: stat.ML

TL;DR: 传统过采样技术和基于 GAN 的深度生成模型在处理类别不平衡问题时有局限，本文提出新过采样框架，在 29 个数据集上评估显示有改进。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡问题会导致模型性能下降，传统过采样技术无法捕捉高维空间全局数据分布，基于 GAN 的模型存在训练不稳定和模式崩溃问题。

Method: 引入过采样框架，学习参数变换将多数样本映射到少数分布，最小化变换样本与真实少数样本的最大均值差异进行全局对齐，加入三元组损失正则化器引导合成样本到边界区域。

Result: 在 29 个合成和真实数据集上评估，该方法在 AUROC、G-mean、F1 分数和 MCC 上比经典和生成基线有持续改进。

Conclusion: 所提框架对不平衡分类任务具有鲁棒性、计算效率和实际实用性。

Abstract: Class imbalance in supervised classification often degrades model performance
by biasing predictions toward the majority class, particularly in critical
applications such as medical diagnosis and fraud detection. Traditional
oversampling techniques, including SMOTE and its variants, generate synthetic
minority samples via local interpolation but fail to capture global data
distributions in high-dimensional spaces. Deep generative models based on GANs
offer richer distribution modeling yet suffer from training instability and
mode collapse under severe imbalance. To overcome these limitations, we
introduce an oversampling framework that learns a parametric transformation to
map majority samples into the minority distribution. Our approach minimizes the
maximum mean discrepancy (MMD) between transformed and true minority samples
for global alignment, and incorporates a triplet loss regularizer to enforce
boundary awareness by guiding synthesized samples toward challenging borderline
regions. We evaluate our method on 29 synthetic and real-world datasets,
demonstrating consistent improvements over classical and generative baselines
in AUROC, G-mean, F1-score, and MCC. These results confirm the robustness,
computational efficiency, and practical utility of the proposed framework for
imbalanced classification tasks.

</details>


### [305] [E-ROBOT: a dimension-free method for robust statistics and machine learning via Schrödinger bridge](https://arxiv.org/abs/2509.11532)
*Davide La Vecchia,Hang Liu*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose the Entropic-regularized Robust Optimal Transport (E-ROBOT)
framework, a novel method that combines the robustness of ROBOT with the
computational and statistical benefits of entropic regularization. We show
that, rooted in the Schr\"{o}dinger bridge problem theory, E-ROBOT defines the
robust Sinkhorn divergence $\overline{W}_{\varepsilon,\lambda}$, where the
parameter $\lambda$ controls robustness and $\varepsilon$ governs the
regularization strength. Letting $n\in \mathbb{N}$ denote the sample size, a
central theoretical contribution is establishing that the sample complexity of
$\overline{W}_{\varepsilon,\lambda}$ is $\mathcal{O}(n^{-1/2})$, thereby
avoiding the curse of dimensionality that plagues standard ROBOT. This
dimension-free property unlocks the use of $\overline{W}_{\varepsilon,\lambda}$
as a loss function in large-dimensional statistical and machine learning tasks.
With this regard, we demonstrate its utility through four applications:
goodness-of-fit testing; computation of barycenters for corrupted 2D and 3D
shapes; definition of gradient flows; and image colour transfer. From the
computation standpoint, a perk of our novel method is that it can be easily
implemented by modifying existing (\texttt{Python}) routines. From the
theoretical standpoint, our work opens the door to many research directions in
statistics and machine learning: we discuss some of them.

</details>


### [306] [SpaPool: Soft Partition Assignment Pooling for__Graph Neural Networks](https://arxiv.org/abs/2509.11675)
*Rodrigue Govan,Romane Scherrer,Philippe Fournier-Viger,Nazha Selmaoui-Folcher*

Main category: stat.ML

TL;DR: 本文介绍了用于图神经网络的新型池化方法SpaPool，结合了密集和稀疏技术优势，实验显示其有竞争力。


<details>
  <summary>Details</summary>
Motivation: 寻求一种能结合密集和稀疏技术优势，在保持图结构完整性的同时有效减小图规模的池化方法。

Method: SpaPool将顶点分组为自适应数量的簇，利用密集和稀疏方法的优势。

Result: 在多个数据集上的实验表明，SpaPool与现有池化技术相比有竞争力，在小规模图上表现尤其出色。

Conclusion: SpaPool是一种有前景的、适用于需要高效图处理应用的方法。

Abstract: This paper introduces SpaPool, a novel pooling method that combines the
strengths of both dense and sparse techniques for a graph neural network.
SpaPool groups vertices into an adaptive number of clusters, leveraging the
benefits of both dense and sparse approaches. It aims to maintain the
structural integrity of the graph while reducing its size efficiently.
Experimental results on several datasets demonstrate that SpaPool achieves
competitive performance compared to existing pooling techniques and excels
particularly on small-scale graphs. This makes SpaPool a promising method for
applications requiring efficient and effective graph processing.

</details>


### [307] [Identifiable Autoregressive Variational Autoencoders for Nonlinear and Nonstationary Spatio-Temporal Blind Source Separation](https://arxiv.org/abs/2509.11962)
*Mika Sipilä,Klaus Nordhausen,Sara Taskinen*

Main category: stat.ML

TL;DR: 本文介绍可识别自回归变分自编码器用于多元时空数据建模与预测，通过模拟研究和实际数据集评估其性能。


<details>
  <summary>Details</summary>
Motivation: 多元时空数据建模和预测存在挑战，现有降维方法需考虑复杂依赖关系，非线性盲源分离是有前景的方法。

Method: 引入可识别自回归变分自编码器，确保非平稳自回归过程的潜在成分可识别。

Result: 通过模拟研究展示了该方法的盲源分离效果，在空气污染和天气数据集上评估了时空预测性能。

Conclusion: 所提出的方法在多元时空数据的盲源分离和时空预测方面具有一定优势。

Abstract: The modeling and prediction of multivariate spatio-temporal data involve
numerous challenges. Dimension reduction methods can significantly simplify
this process, provided that they account for the complex dependencies between
variables and across time and space. Nonlinear blind source separation has
emerged as a promising approach, particularly following recent advances in
identifiability results. Building on these developments, we introduce the
identifiable autoregressive variational autoencoder, which ensures the
identifiability of latent components consisting of nonstationary autoregressive
processes. The blind source separation efficacy of the proposed method is
showcased through a simulation study, where it is compared against
state-of-the-art methods, and the spatio-temporal prediction performance is
evaluated against several competitors on air pollution and weather datasets.

</details>


### [308] [MMM: Clustering Multivariate Longitudinal Mixed-type Data](https://arxiv.org/abs/2509.12166)
*Francesco Amato,Julien Jacques*

Main category: stat.ML

TL;DR: 本文介绍MMM模型用于混合类型多元纵向数据聚类，通过MCMC - EM算法进行推理，经合成数据评估并应用于金融数据。


<details>
  <summary>Details</summary>
Motivation: 混合类型多元纵向数据聚类算法稀缺，需同时建模多元数据的时间内和时间间依赖结构。

Method: 引入MMM模型，将数据重组为三维结构，假设非连续变量是潜在连续变量的观测值，基于矩阵变量正态分布的混合在潜在维度进行聚类，通过MCMC - EM算法进行推理。

Result: 合成数据评估显示模型的推理能力，给出金融数据的实际应用。

Conclusion: MMM模型能以简约方式处理多种类型数据，同时建模异质性、响应间关联和时间依赖结构，且无需假设条件独立性。

Abstract: Multivariate longitudinal data of mixed-type are increasingly collected in
many science domains. However, algorithms to cluster this kind of data remain
scarce, due to the challenge to simultaneously model the within- and
between-time dependence structures for multivariate data of mixed kind. We
introduce the Mixture of Mixed-Matrices (MMM) model: reorganizing the data in a
three-way structure and assuming that the non-continuous variables are
observations of underlying latent continuous variables, the model relies on a
mixture of matrix-variate normal distributions to perform clustering in the
latent dimension. The MMM model is thus able to handle continuous, ordinal,
binary, nominal and count data and to concurrently model the heterogeneity, the
association among the responses and the temporal dependence structure in a
parsimonious way and without assuming conditional independence. The inference
is carried out through an MCMC-EM algorithm, which is detailed. An evaluation
of the model through synthetic data shows its inference abilities. A real-world
application on financial data is presented.

</details>


### [309] [The Morgan-Pitman Test of Equality of Variances and its Application to Machine Learning Model Evaluation and Selection](https://arxiv.org/abs/2509.12185)
*Argimiro Arratia,Alejandra Cabaña,Ernesto Mordecki,Gerard Rovira-Parra*

Main category: stat.ML

TL;DR: 提出一种统计测试用于非线性模型的模型评估与选择，经模拟和实际数据验证有效。


<details>
  <summary>Details</summary>
Motivation: 非线性模型选择重性能指标轻统计测试，难以考虑抽样变异性。

Method: 基于经典Morgan - Pitman方法，增强对重尾分布或高方差异常值数据的鲁棒性，使机器学习模型残差统计独立。

Result: 通过模拟和实际数据应用，证明该测试有效且实用。

Conclusion: 该测试为不同场景下的模型评估和选择提供可靠工具。

Abstract: Model selection in non-linear models often prioritizes performance metrics
over statistical tests, limiting the ability to account for sampling
variability. We propose the use of a statistical test to assess the equality of
variances in forecasting errors. The test builds upon the classic Morgan-Pitman
approach, incorporating enhancements to ensure robustness against data with
heavy-tailed distributions or outliers with high variance, plus a strategy to
make residuals from machine learning models statistically independent. Through
a series of simulations and real-world data applications, we demonstrate the
test's effectiveness and practical utility, offering a reliable tool for model
evaluation and selection in diverse contexts.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [310] [Bayesian model updating via streamlined Bayesian active learning cubature](https://arxiv.org/abs/2509.11204)
*Pei-Pei Li,Chao Dang,Cristóbal H. Acevedo,Marcos A. Valdebenito,Matthias G. R. Faes*

Main category: stat.CO

TL;DR: 提出名为SBALC的贝叶斯主动学习方法用于贝叶斯模型更新，通过高斯过程回归近似对数似然函数，利用均值和方差函数构建相关估计和准则，通过数值算例验证方法的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 提出新的贝叶斯主动学习方法用于贝叶斯模型更新，提高模型评估效率。

Method: 用高斯过程回归以精简贝叶斯主动学习方式近似对数似然函数，仅使用后验高斯过程的均值和方差函数形成模型证据估计器、停止准则和学习函数。

Result: 通过四个数值算例表明，该方法能显著减少模型评估次数和计算时间，且不影响准确性。

Conclusion: 所提出的SBALC方法在不降低准确性的前提下，能有效提高贝叶斯模型更新的效率。

Abstract: This paper proposes a novel Bayesian active learning method for Bayesian
model updating, which is termed as "Streamlined Bayesian Active Learning
Cubature" (SBALC). The core idea is to approximate the log-likelihood function
using Gaussian process (GP) regression in a streamlined Bayesian active
learning way. Rather than generating many samples from the posterior GP, we
only use its mean and variance function to form the model evidence estimator,
stopping criterion, and learning function. Specifically, the estimation of
model evidence is first treated as a Bayesian cubature problem, with a GP prior
assigned over the log-likelihood function. Second, a plug-in estimator for
model evidence is proposed based on the posterior mean function of the GP.
Third, an upper bound on the expected absolute error between the posterior
model evidence and its plug-in estimator is derived. Building on this result, a
novel stopping criterion and learning function are proposed using only the
posterior mean and standard deviation functions of the GP. Finally, we can
obtain the model evidence based on the posterior mean function of the
log-likelihood function in conjunction with Monte Carlo simulation, as well as
the samples for the posterior distribution of model parameters as a by-product.
Four numerical examples are presented to demonstrate the accuracy and
efficiency of the proposed method compared to several existing approaches. The
results show that the method can significantly reduce the number of model
evaluations and the computational time without compromising accuracy.

</details>


### [311] [Tidy simulation: Designing robust, reproducible, and scalable Monte Carlo simulations](https://arxiv.org/abs/2509.11741)
*Erik-Jan van Kesteren*

Main category: stat.CO

TL;DR: 本文提出tidy simulation框架用于模拟研究，具有结构简单、语言无关、灵活等特点，可实现模拟研究的稳健、可重复和可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有文献多关注模拟设计方面，缺乏与当前编程状态匹配的实现策略，因此提出新框架。

Method: 提出tidy simulation框架，包含模拟网格、数据生成函数、分析函数和结果表四个组件。

Result: 即使是小型模拟也能以一致、模块化方式编写，且可按需扩展到数千节点，支持迭代探索性实验。

Conclusion: 采用tidy simulation方法可实现模拟研究的稳健、可重复和可扩展，有助于高质量统计科学。

Abstract: Monte Carlo simulation studies are at the core of the modern applied,
computational, and theoretical statistical literature. Simulation is a broadly
applicable research tool, used to collect data on the relative performance of
methods or data analysis approaches under a well-defined data-generating
process. However, extant literature focuses largely on design aspects of
simulation, rather than implementation strategies aligned with the current
state of (statistical) programming languages, portable data formats, and
multi-node cluster computing.
  In this work, I propose tidy simulation: a simple, language-agnostic, yet
flexible functional framework for designing, writing, and running simulation
studies. It has four components: a tidy simulation grid, a data generation
function, an analysis function, and a results table. Using this structure, even
the smallest simulations can be written in a consistent, modular way, yet they
can be readily scaled to thousands of nodes in a computer cluster should the
need arise. Tidy simulation also supports the iterative, sometimes exploratory
nature of simulation-based experiments. By adopting the tidy simulation
approach, researchers can implement their simulations in a robust,
reproducible, and scalable way, which contributes to high-quality statistical
science.

</details>


### [312] [Extrapolation of Tempered Posteriors](https://arxiv.org/abs/2509.12173)
*Mengxin Xi,Zheyang Shen,Marina Riabiz,Nicolas Chopin,Chris J. Oates*

Main category: stat.CO

TL;DR: 研究指出高质量逼近至p1并非必要，可利用中间分布外推后验感兴趣量，还提出基于外推和光滑的后验期望逼近方法。


<details>
  <summary>Details</summary>
Motivation: 探究在贝叶斯计算中，不进行高质量逼近至p1情况下，能否获取后验感兴趣量。

Method: 建立后验期望由相关回火期望在非空t区间确定的条件，提出基于外推和光滑回火期望逼近后验期望的新方法，并作为序贯蒙特卡罗的后处理方差缩减工具实现。

Result: 建立了相应条件，提出了新的逼近后验期望的方法。

Conclusion: 提出的基于外推和光滑回火期望的方法可用于逼近后验期望，作为序贯蒙特卡罗的后处理方差缩减工具。

Abstract: Tempering is a popular tool in Bayesian computation, being used to transform
a posterior distribution $p_1$ into a reference distribution $p_0$ that is more
easily approximated. Several algorithms exist that start by approximating $p_0$
and proceed through a sequence of intermediate distributions $p_t$ until an
approximation to $p_1$ is obtained. Our contribution reveals that high-quality
approximation of terms up to $p_1$ is not essential, as knowledge of the
intermediate distributions enables posterior quantities of interest to be
extrapolated. Specifically, we establish conditions under which posterior
expectations are determined by their associated tempered expectations on any
non-empty $t$ interval. Harnessing this result, we propose novel methodology
for approximating posterior expectations based on extrapolation and smoothing
of tempered expectations, which we implement as a post-processing
variance-reduction tool for sequential Monte Carlo.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [313] [SH-SAS: An Implicit Neural Representation for Complex Spherical-Harmonic Scattering Fields for 3D Synthetic Aperture Sonar](https://arxiv.org/abs/2509.11087)
*Omkar Shailendra Vengurlekar,Adithya Pediredla,Suren Jayasuriya*

Main category: cs.GR

TL;DR: 提出SH - SAS隐式神经表示用于合成孔径声纳（SAS）重建，在合成和真实SAS基准测试中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D SAS重建算法不考虑方向性且存在采样限制等问题，先前神经体积方法将体素视为各向同性散射密度，未建模各向异性回波。

Method: 引入SH - SAS，用一组球谐（SH）系数表示复杂声散射场，多分辨率哈希编码器输入轻量级MLP输出复SH系数，直接从一维飞行时间信号训练。

Result: 在合成和真实SAS（空中和水下）基准测试中，SH - SAS在3D重建质量和几何指标方面表现更好。

Conclusion: SH - SAS是一种更优的合成孔径声纳重建方法。

Abstract: Synthetic aperture sonar (SAS) reconstruction requires recovering both the
spatial distribution of acoustic scatterers and their direction-dependent
response. Time-domain backprojection is the most common 3D SAS reconstruction
algorithm, but it does not model directionality and can suffer from sampling
limitations, aliasing, and occlusion. Prior neural volumetric methods applied
to synthetic aperture sonar treat each voxel as an isotropic scattering
density, not modeling anisotropic returns. We introduce SH-SAS, an implicit
neural representation that expresses the complex acoustic scattering field as a
set of spherical harmonic (SH) coefficients. A multi-resolution hash encoder
feeds a lightweight MLP that outputs complex SH coefficients up to a specified
degree L. The zeroth-order coefficient acts as an isotropic scattering field,
which also serves as the density term, while higher orders compactly capture
directional scattering with minimal parameter overhead. Because the model
predicts the complex amplitude for any transmit-receive baseline, training is
performed directly from 1-D time-of-flight signals without the need to beamform
intermediate images for supervision. Across synthetic and real SAS (both in-air
and underwater) benchmarks, results show that SH-SAS performs better in terms
of 3D reconstruction quality and geometric metrics than previous methods.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [314] [Bluffing in Scrabble](https://arxiv.org/abs/2509.10471)
*Nick Ballard,Timothy Y. Chow*

Main category: math.HO

TL;DR: 文章构建了一个拼字游戏（Scrabble）局面，指出在此局面下最优策略是以1/3概率选Move A，2/3概率选Move B，Move B可视为虚张声势。


<details>
  <summary>Details</summary>
Motivation: 此前无人展示过拼字游戏中最优策略包含虚张声势或为混合策略的局面，本文旨在填补这一空白。

Method: 精心构建一个在锦标赛中可能出现且无无效单词的拼字游戏局面。

Result: 得出在该局面下最优策略是以1/3概率选Move A，2/3概率选Move B，且Move B可视为虚张声势。

Conclusion: 证明了拼字游戏中存在最优策略为混合策略且包含虚张声势的情况。

Abstract: It is well known that in games with imperfect information, such as poker,
bluffing with some probability can be a component of the optimal strategy.
However, as far as we know, nobody has ever exhibited a Scrabble position in
which the optimal strategy involves bluffing, or even a Scrabble position in
which the optimal strategy is a mixed (i.e., randomized) strategy. We present a
carefully constructed Scrabble position, that could actually arise in a
tournament game with no invalid words played, in which the optimal strategy
(assuming that a tied score leads to the point being split equally, with no
recourse to so-called "spread points" as a tie-breaking mechanism) is to make
Move A with probability 1/3 and to make Move B with probability 2/3. Move B can
reasonably be called a bluff, in the sense that it sets up a threat which the
player cannot in fact execute, but which the opponent may not be able to rule
out.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [315] [RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction](https://arxiv.org/abs/2509.11191)
*Jian Chen,Shengyi Lv,Leilei Su*

Main category: cs.CL

TL;DR: 提出随机对抗训练（RAT）框架用于生物医学信息提取（BioIE）任务，结合随机采样与对抗训练原则，提升模型性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统对抗训练虽能提升预训练语言模型在BioIE任务中的性能，但存在计算开销大的问题，需要更高效的解决方案。

Method: 在PubMedBERT基础架构上，将随机采样机制与对抗训练原则相结合，提出RAT框架。

Result: 通过综合评估，RAT在BioIE任务中的表现优于基线模型。

Conclusion: RAT有潜力成为生物医学自然语言处理的变革性框架，能平衡模型性能和计算效率。

Abstract: We introduce random adversarial training (RAT), a novel framework
successfully applied to biomedical information extraction (BioIE) tasks.
Building on PubMedBERT as the foundational architecture, our study first
validates the effectiveness of conventional adversarial training in enhancing
pre-trained language models' performance on BioIE tasks. While adversarial
training yields significant improvements across various performance metrics, it
also introduces considerable computational overhead. To address this
limitation, we propose RAT as an efficiency solution for biomedical information
extraction. This framework strategically integrates random sampling mechanisms
with adversarial training principles, achieving dual objectives: enhanced model
generalization and robustness while significantly reducing computational costs.
Through comprehensive evaluations, RAT demonstrates superior performance
compared to baseline models in BioIE tasks. The results highlight RAT's
potential as a transformative framework for biomedical natural language
processing, offering a balanced solution to the model performance and
computational efficiency.

</details>


### [316] [A Dynamic Fusion Model for Consistent Crisis Response](https://arxiv.org/abs/2509.01053)
*Xiaoying Song,Anirban Saha Anik,Eduardo Blanco,Vanessa Frias-Martinez,Lingzi Hong*

Main category: cs.CL

TL;DR: 提出新颖指标评估风格一致性及基于融合的生成方法，在多数据集实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 危机通信中自动响应需保持风格一致性，但相关研究少，为填补此空白开展研究。

Method: 提出评估风格一致性的新指标，采用两阶段过程，先评估候选响应风格，再通过融合过程在实例层面优化整合。

Result: 在多个数据集上实验，该方法在响应质量和风格一致性上均优于基线。

Conclusion: 所提出的方法能有效生成高质量响应，显著减少实例间风格差异。

Abstract: In response to the urgent need for effective communication with
crisis-affected populations, automated responses driven by language models have
been proposed to assist in crisis communications. A critical yet often
overlooked factor is the consistency of response style, which could affect the
trust of affected individuals in responders. Despite its importance, few
studies have explored methods for maintaining stylistic consistency across
generated responses. To address this gap, we propose a novel metric for
evaluating style consistency and introduce a fusion-based generation approach
grounded in this metric. Our method employs a two-stage process: it first
assesses the style of candidate responses and then optimizes and integrates
them at the instance level through a fusion process. This enables the
generation of high-quality responses while significantly reducing stylistic
variation between instances. Experimental results across multiple datasets
demonstrate that our approach consistently outperforms baselines in both
response quality and stylistic uniformity.

</details>


### [317] [Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL](https://arxiv.org/abs/2509.01058)
*Xiaoying Song,Anirban Saha Anik,Dibakar Barua,Pengcheng Luo,Junhua Ding,Lingzi Hong*

Main category: cs.CL

TL;DR: 提出Controlled - Literacy框架生成适应不同健康素养水平的反制言论，实验显示效果优于基线，有助于公共卫生传播。


<details>
  <summary>Details</summary>
Motivation: 在线健康错误信息威胁公共卫生，现有自动生成反制言论方法忽略受众健康素养水平对反制言论可及性和有效性的影响。

Method: 提出使用检索增强生成（RAG）和强化学习（RL）的Controlled - Literacy框架，检索与特定健康素养水平对齐的知识，设计结合主观用户偏好和客观可读性奖励的奖励函数。

Result: Controlled - Literacy框架生成的反制言论更易获取且更受用户青睐，优于基线。

Conclusion: 该研究通过提高健康错误信息反制言论的可及性和理解力，有助于实现更公平、更有影响力的公共卫生传播。

Abstract: Health misinformation spreading online poses a significant threat to public
health. Researchers have explored methods for automatically generating
counterspeech to health misinformation as a mitigation strategy. Existing
approaches often produce uniform responses, ignoring that the health literacy
level of the audience could affect the accessibility and effectiveness of
counterspeech. We propose a Controlled-Literacy framework using
retrieval-augmented generation (RAG) with reinforcement learning (RL) to
generate tailored counterspeech adapted to different health literacy levels. In
particular, we retrieve knowledge aligned with specific health literacy levels,
enabling accessible and factual information to support generation. We design a
reward function incorporating subjective user preferences and objective
readability-based rewards to optimize counterspeech to the target health
literacy level. Experiment results show that Controlled-Literacy outperforms
baselines by generating more accessible and user-preferred counterspeech. This
research contributes to more equitable and impactful public health
communication by improving the accessibility and comprehension of counterspeech
to health misinformation

</details>


### [318] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
*Gang Cheng,Haibo Jin,Wenbin Zhang,Haohan Wang,Jun Zhuang*

Main category: cs.CL

TL;DR: 本文通过红队攻击方法研究金融大语言模型漏洞，提出RCA攻击框架和FIN - Bench基准，实验显示RCA能有效绕过主流大模型，揭示当前对齐技术存在差距，强调需强化金融领域审核机制。


<details>
  <summary>Details</summary>
Motivation: 现有红队研究主要关注有害内容，忽略金融大语言模型监管风险，因此要研究其漏洞。

Method: 提出Risk - Concealment Attacks (RCA)多轮攻击框架，构建FIN - Bench领域特定基准。

Result: 在FIN - Bench上实验表明，RCA能有效绕过九个主流大语言模型，平均攻击成功率达93.18%，如GPT - 4.1为98.28%，OpenAI o1为97.56%。

Conclusion: 当前对齐技术存在关键差距，金融领域迫切需要更强的审核机制，该研究为推进强大且特定领域感知的大语言模型对齐提供实用见解。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.

</details>


### [319] [No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes](https://arxiv.org/abs/2509.10625)
*Iván Vicente Moreno Cencerrado,Arnau Padrés Masdemont,Anton Gonzalvez Hawthorne,David Demitri Africa,Lorenzo Pacchiardi*

Main category: cs.CL

TL;DR: 研究大语言模型能否预判答案正确性，通过提取激活值训练线性探针预测，在多数据集表现好，有一定结论但数学推理泛化差。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型是否能预判自己的答案是否正确。

Method: 提取读题后、生成token前的激活值，训练线性探针预测答案正确性。

Result: 在多个开源模型和数据集上，“提前正确性方向”投影预测效果好，在中间层预测力饱和，数学推理泛化差，“我不知道”回答与探针分数强相关。

Conclusion: 为阐明大语言模型内部机制做出重要贡献。

Abstract: Do large language models (LLMs) anticipate when they will answer correctly?
To study this, we extract activations after a question is read but before any
tokens are generated, and train linear probes to predict whether the model's
forthcoming answer will be correct. Across three open-source model families
ranging from 7 to 70 billion parameters, projections on this "in-advance
correctness direction" trained on generic trivia questions predict success in
distribution and on diverse out-of-distribution knowledge datasets,
outperforming black-box baselines and verbalised predicted confidence.
Predictive power saturates in intermediate layers, suggesting that
self-assessment emerges mid-computation. Notably, generalisation falters on
questions requiring mathematical reasoning. Moreover, for models responding "I
don't know", doing so strongly correlates with the probe score, indicating that
the same direction also captures confidence. By complementing previous results
on truthfulness and other behaviours obtained with probes and sparse
auto-encoders, our work contributes essential findings to elucidate LLM
internals.

</details>


### [320] [Pluralistic Alignment for Healthcare: A Role-Driven Framework](https://arxiv.org/abs/2509.10685)
*Jiayou Zhong,Anudeex Shetty,Chao Jia,Xuanrui Lin,Usman Naseem*

Main category: cs.CL

TL;DR: 针对大语言模型在医疗领域多元对齐不足问题，提出EthosAgents方法，实证表明其能促进多元对齐，为高风险领域尊重多样性提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗等敏感领域应用时，现有对齐方法在医疗领域存在不足，需确保输出反映多元价值观和观点。

Method: 提出轻量级、可泛化的多元对齐方法EthosAgents来模拟不同观点和价值观。

Result: 该方法能促进七种不同规模的开放和封闭模型在三种模式下的多元对齐。

Conclusion: 医疗相关的多元性需要适应性强且具有规范意识的方法，此研究为高风险领域模型更好尊重多样性提供了见解。

Abstract: As large language models are increasingly deployed in sensitive domains such
as healthcare, ensuring their outputs reflect the diverse values and
perspectives held across populations is critical. However, existing alignment
approaches, including pluralistic paradigms like Modular Pluralism, often fall
short in the health domain, where personal, cultural, and situational factors
shape pluralism. Motivated by the aforementioned healthcare challenges, we
propose a first lightweight, generalizable, pluralistic alignment approach,
EthosAgents, designed to simulate diverse perspectives and values. We
empirically show that it advances the pluralistic alignment for all three modes
across seven varying-sized open and closed models. Our findings reveal that
health-related pluralism demands adaptable and normatively aware approaches,
offering insights into how these models can better respect diversity in other
high-stakes domains.

</details>


### [321] [Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models](https://arxiv.org/abs/2509.10744)
*Ozan Gokdemir,Neil Getty,Robert Underwood,Sandeep Madireddy,Franck Cappello,Arvind Ramanathan,Ian T. Foster,Rick L. Stevens*

Main category: cs.CL

TL;DR: 本文提出从科学论文语料生成MCQA基准的框架，以辐射和癌症生物学文章为例生成大量MCQ，评估小语言模型，发现推理轨迹检索能提升性能。


<details>
  <summary>Details</summary>
Motivation: 科学知识增长迅速，评估基准需进化以反映新发现并确保语言模型在最新多样文献上测试。

Method: 提出可扩展、模块化框架，自动化MCQA创建各阶段，以辐射和癌症生物学文章为例生成MCQ，评估小语言模型，对比不同检索增强方式。

Result: 推理轨迹检索在合成和专家标注基准上持续提升性能，部分小模型在2023年考试中超越GPT - 4。

Conclusion: 推理轨迹检索能有效提升小语言模型在MCQA基准上的表现。

Abstract: As scientific knowledge grows at an unprecedented pace, evaluation benchmarks
must evolve to reflect new discoveries and ensure language models are tested on
current, diverse literature. We propose a scalable, modular framework for
generating multiple-choice question-answering (MCQA) benchmarks directly from
large corpora of scientific papers. Our pipeline automates every stage of MCQA
creation, including PDF parsing, semantic chunking, question generation, and
model evaluation. As a case study, we generate more than 16,000 MCQs from
22,000 open-access articles in radiation and cancer biology. We then evaluate a
suite of small language models (1.1B-14B parameters) on these questions,
comparing baseline accuracy with retrieval-augmented generation (RAG) from
paper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.
We find that reasoning-trace retrieval consistently improves performance on
both synthetic and expert-annotated benchmarks, enabling several small models
to surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.

</details>


### [322] [Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction](https://arxiv.org/abs/2509.10798)
*Yijun Liu,Yixuan Wang,Yuzhuang Xu,Shiyu Ji,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文针对大语言模型KV缓存逐出时过度关注局部信息的问题，提出Judge Q方法，实验表明该方法在现有模型上能提升性能且训练开销小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型KV缓存大小随序列长度线性增长影响内存和效率，现有逐出方法过度关注局部信息，忽略全局信息。

Method: 提出Judge Q方法，引入软标记列表，仅微调模型嵌入层，通过训练软标记对原输入序列的注意力图，使软标记对应查询捕捉全局信息。

Result: 在Llama - 3.1 - 8B - Instruct和Mistral - 7B - Instruct - v0.3等模型上实验，LongBench提升约1分，RULER提升超3分。

Conclusion: 该方法可低成本集成到现有开源模型，提升KV缓存逐出场景性能。

Abstract: Large language models (LLMs) utilize key-value (KV) cache to store historical
information during sequence processing. The size of KV cache grows linearly as
the length of the sequence extends, which seriously affects memory usage and
decoding efficiency. Current methods for KV cache eviction typically utilize
the last window from the pre-filling phase as queries to compute the KV
importance scores for eviction. Although this scheme is simple to implement, it
tends to overly focus on local information, potentially leading to the neglect
or omission of crucial global information. To mitigate this issue, we propose
Judge Q, a novel training method which incorporates a soft token list. This
method only tunes the model's embedding layer at a low training cost. By
concatenating the soft token list at the end of the input sequence, we train
these tokens' attention map to the original input sequence to align with that
of the actual decoded tokens. In this way, the queries corresponding to the
soft tokens can effectively capture global information and better evaluate the
importance of the keys and values within the KV cache, thus maintaining
decoding quality when KV cache is evicted. Under the same eviction budget, our
method exhibits less performance degradation compared to existing eviction
approaches. We validate our approach through experiments conducted on models
such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks
including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an
improvement of approximately 1 point on the LongBench and over 3 points on
RULER. This proposed methodology can be seamlessly integrated into existing
open-source models with minimal training overhead, thereby enhancing
performance in KV cache eviction scenarios.

</details>


### [323] [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)
*Dominic Petrak,Thy Thy Tran,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出Automated Error Discovery框架和SEEED方法检测对话AI错误，SEEED在多数据集上表现优于基线，提升未知错误检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型检测对话代理错误的方法难以识别未明确指定的错误，如模型更新或用户行为变化产生的错误。

Method: 引入Automated Error Discovery框架，提出SEEED方法，增强Soft Nearest Neighbor Loss，引入Label - Based Sample Ranking。

Result: SEEED在多个错误标注对话数据集上优于GPT - 4o和Phi - 4等基线，未知错误检测准确率最多提升8个百分点，对未知意图检测有强泛化性。

Conclusion: SEEED方法在检测对话AI错误方面表现良好，能提升未知错误检测能力和泛化性。

Abstract: Although LLM-based conversational agents demonstrate strong fluency and
coherence, they still produce undesirable behaviors (errors) that are
challenging to prevent from reaching users during deployment. Recent research
leverages large language models (LLMs) to detect errors and guide
response-generation models toward improvement. However, current LLMs struggle
to identify errors not explicitly specified in their instructions, such as
those arising from updates to the response-generation model or shifts in user
behavior. In this work, we introduce Automated Error Discovery, a framework for
detecting and defining errors in conversational AI, and propose SEEED (Soft
Clustering Extended Encoder-Based Error Detection), as an encoder-based
approach to its implementation. We enhance the Soft Nearest Neighbor Loss by
amplifying distance weighting for negative samples and introduce Label-Based
Sample Ranking to select highly contrastive examples for better representation
learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --
across multiple error-annotated dialogue datasets, improving the accuracy for
detecting unknown errors by up to 8 points and demonstrating strong
generalization to unknown intent detection.

</details>


### [324] [A funny companion: Distinct neural responses to perceived AI- versus human- generated humor](https://arxiv.org/abs/2509.10847)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究用EEG对比人对AI和人类幽默的处理，发现AI幽默引发不同神经反应及处理效率变化，挑战算法厌恶，社会态度会调节神经反应。


<details>
  <summary>Details</summary>
Motivation: 随着AI能进行类人交流并讲笑话，了解人们对AI幽默的认知和情感反应很重要。

Method: 使用脑电图（EEG）对比人们处理AI和人类幽默的情况。

Result: 行为分析显示参与者认为AI和人类幽默同样有趣；神经生理数据显示AI幽默N400效应小、LPP大；人类幽默有习惯化效应，AI幽默处理效率和情感奖励增加；参与者对AI的社会态度会调节神经反应。

Conclusion: 大脑对AI幽默有积极强烈反应，幽默有助于促进人机社交互动。

Abstract: As AI companions become capable of human-like communication, including
telling jokes, understanding how people cognitively and emotionally respond to
AI humor becomes increasingly important. This study used electroencephalography
(EEG) to compare how people process humor from AI versus human sources.
Behavioral analysis revealed that participants rated AI and human humor as
comparably funny. However, neurophysiological data showed that AI humor
elicited a smaller N400 effect, suggesting reduced cognitive effort during the
processing of incongruity. This was accompanied by a larger Late Positive
Potential (LPP), indicating a greater degree of surprise and emotional
response. This enhanced LPP likely stems from the violation of low initial
expectations regarding AI's comedic capabilities. Furthermore, a key temporal
dynamic emerged: human humor showed habituation effects, marked by an
increasing N400 and a decreasing LPP over time. In contrast, AI humor
demonstrated increasing processing efficiency and emotional reward, with a
decreasing N400 and an increasing LPP. This trajectory reveals how the brain
can dynamically update its predictive model of AI capabilities. This process of
cumulative reinforcement challenges "algorithm aversion" in humor, as it
demonstrates how cognitive adaptation to AI's language patterns can lead to an
intensified emotional reward. Additionally, participants' social attitudes
toward AI modulated these neural responses, with higher perceived AI
trustworthiness correlating with enhanced emotional engagement. These findings
indicate that the brain responds to AI humor with surprisingly positive and
intense reactions, highlighting humor's potential for fostering genuine
engagement in human-AI social interaction.

</details>


### [325] [Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue](https://arxiv.org/abs/2509.10852)
*Sangyeop Kim,Yohan Lee,Sanghwa Kim,Hyunjong Kim,Sungzoon Cho*

Main category: cs.CL

TL;DR: 提出 PREMem 方法，将复杂推理从推理阶段转移到内存构建阶段，实验显示各模型尺寸性能均提升。


<details>
  <summary>Details</summary>
Motivation: 当前对话 AI 系统在响应生成时推理负担过重，性能依赖模型尺寸，需有效长期记忆方案。

Method: 引入 PREMem，提取细粒度记忆片段并分类，建立会话间记忆项关系，在预存储时进行推理。

Result: 各模型尺寸性能显著提升，小模型能达到大基线模型效果，受限令牌预算下仍有效。

Conclusion: PREMem 能在降低交互计算需求的同时创建丰富表示，提高对话 AI 性能。

Abstract: Effective long-term memory in conversational AI requires synthesizing
information across multiple sessions. However, current systems place excessive
reasoning burden on response generation, making performance significantly
dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for
Episodic Memory), a novel approach that shifts complex reasoning processes from
inference to memory construction. PREMem extracts fine-grained memory fragments
categorized into factual, experiential, and subjective information; it then
establishes explicit relationships between memory items across sessions,
capturing evolution patterns like extensions, transformations, and
implications. By performing this reasoning during pre-storage rather than when
generating a response, PREMem creates enriched representations while reducing
computational demands during interactions. Experiments show significant
performance improvements across all model sizes, with smaller models achieving
results comparable to much larger baselines while maintaining effectiveness
even with constrained token budgets. Code and dataset are available at
https://github.com/sangyeop-kim/PREMem.

</details>


### [326] [CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis](https://arxiv.org/abs/2509.10886)
*Xinyu Zhang,Pei Zhang,Shuang Luo,Jialong Tang,Yu Wan,Baosong Yang,Fei Huang*

Main category: cs.CL

TL;DR: 提出CultureSynth框架评估大语言模型文化能力，含综合分类和RAG方法，评估14个模型显示性能分层等结果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型文化能力评估存在分类碎片化、领域特定、依赖手动标注等问题，需改进。

Method: 引入CultureSynth框架，含综合分层多语言文化分类和基于RAG的方法合成问答对。

Result: CultureSynth - 7基准含19360条目和4149手动验证条目，评估显示模型性能分层，3B参数门槛等。

Conclusion: CultureSynth为开发有文化意识的AI系统提供可扩展框架，减少手动标注依赖。

Abstract: Cultural competence, defined as the ability to understand and adapt to
multicultural contexts, is increasingly vital for large language models (LLMs)
in global environments. While several cultural benchmarks exist to assess LLMs'
cultural competence, current evaluations suffer from fragmented taxonomies,
domain specificity, and heavy reliance on manual data annotation. To address
these limitations, we introduce CultureSynth, a novel framework comprising (1)
a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary
and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based
methodology leveraging factual knowledge to synthesize culturally relevant
question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360
entries and 4,149 manually verified entries across 7 languages. Evaluation of
14 prevalent LLMs of different sizes reveals clear performance stratification
led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that
a 3B-parameter threshold is necessary for achieving basic cultural competence,
models display varying architectural biases in knowledge processing, and
significant geographic disparities exist across models. We believe that
CultureSynth offers a scalable framework for developing culturally aware AI
systems while reducing reliance on manual annotation\footnote{Benchmark is
available at https://github.com/Eyr3/CultureSynth.}.

</details>


### [327] [Fluid Language Model Benchmarking](https://arxiv.org/abs/2509.11106)
*Valentin Hofmann,David Heineman,Ian Magnusson,Kyle Lo,Jesse Dodge,Maarten Sap,Pang Wei Koh,Chun Wang,Hannaneh Hajishirzi,Noah A. Smith*

Main category: cs.CL

TL;DR: 提出Fluid Benchmarking新评估方法，实验显示其在效率、有效性等四方面优于随机抽样等方法，可提升语言模型基准测试质量。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型基准测试存在评估成本高、无法测量预期能力、评估质量受标签错误和基准饱和影响等问题，且现有策略孤立解决问题，忽视整体评估质量。

Method: 受心理测量学启发，基于现有语言模型评估结果估计项目反应模型，动态选择评估项目，类似教育中的计算机自适应测试。

Result: 在效率、有效性、方差和饱和度四个维度上表现优于随机抽样和其他复杂基线方法，如在MMLU上用少50倍的项目实现更高有效性和更低方差。

Conclusion: 超越静态评估可大幅改进语言模型基准测试。

Abstract: Language model (LM) benchmarking faces several challenges: comprehensive
evaluations are costly, benchmarks often fail to measure the intended
capabilities, and evaluation quality can degrade due to labeling errors and
benchmark saturation. Although various strategies have been proposed to
mitigate these issues, they tend to address individual aspects in isolation,
neglecting broader questions about overall evaluation quality. Here, we
introduce Fluid Benchmarking, a new evaluation approach that advances LM
benchmarking across multiple dimensions. Inspired by psychometrics, Fluid
Benchmarking is based on the insight that the relative value of benchmark items
depends on an LM's capability level, suggesting that evaluation should adapt to
each LM. Methodologically, Fluid Benchmarking estimates an item response model
based on existing LM evaluation results and uses the inferred quantities to
select evaluation items dynamically, similar to computerized adaptive testing
in education. In our experiments, we compare Fluid Benchmarking against the
common practice of random item sampling as well as more sophisticated
baselines, including alternative methods grounded in item response theory. We
examine four dimensions -- efficiency, validity, variance, and saturation --
and find that Fluid Benchmarking achieves superior performance in all of them
(e.g., higher validity and less variance on MMLU with fifty times fewer items).
Our analysis shows that the two components of Fluid Benchmarking have distinct
effects: item response theory, used to map performance into a latent ability
space, increases validity, while dynamic item selection reduces variance.
Overall, our results suggest that LM benchmarking can be substantially improved
by moving beyond static evaluation.

</details>


### [328] [We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism](https://arxiv.org/abs/2509.11118)
*Priyanshu Priya,Saurav Dudhate,Desai Vishesh Yasheshbhai,Asif Ekbal*

Main category: cs.CL

TL;DR: 提出PAN - DG任务，引入PACT数据集，评估表明数据集高质量，实验显示微调大模型在任务中有效，为领域研究奠基。


<details>
  <summary>Details</summary>
Motivation: 提升谈判对话系统的冲突解决和适应能力，将论证机制和个性属性融入系统。

Method: 提出PAN - DG任务，用大语言模型生成含三种个性档案的PACT数据集，对预训练和微调大模型进行比较实验。

Result: PACT数据集包含高质量对话，微调大模型能有效生成个性驱动的理性回应。

Conclusion: PACT数据集能增强谈判对话系统的个性化和推理能力，为该领域未来研究奠定基础。

Abstract: Integrating argumentation mechanisms into negotiation dialogue systems
improves conflict resolution through exchanges of arguments and critiques.
Moreover, incorporating personality attributes enhances adaptability by
aligning interactions with individuals' preferences and styles. To advance
these capabilities in negotiation dialogue systems, we propose a novel
Personality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG)
task. To support this task, we introduce PACT, a dataset of Personality-driven
Argumentation-based negotiation Conversations for Tourism sector. This dataset,
generated using Large Language Models (LLMs), features three distinct
personality profiles, viz. Argumentation Profile, Preference Profile, and
Buying Style Profile to simulate a variety of negotiation scenarios involving
diverse personalities. Thorough automatic and manual evaluations indicate that
the dataset comprises high-quality dialogues. Further, we conduct comparative
experiments between pre-trained and fine-tuned LLMs for the PAN-DG task.
Multi-dimensional evaluation demonstrates that the fine-tuned LLMs effectively
generate personality-driven rational responses during negotiations. This
underscores the effectiveness of PACT in enhancing personalization and
reasoning capabilities in negotiation dialogue systems, thereby establishing a
foundation for future research in this domain.

</details>


### [329] [Differentially-private text generation degrades output language quality](https://arxiv.org/abs/2509.11176)
*Erion Çano,Ivan Habernal*

Main category: cs.CL

TL;DR: 本文研究DP微调大语言模型对生成文本质量和实用性的影响，发现强隐私约束下模型生成文本质量和下游任务准确率下降。


<details>
  <summary>Details</summary>
Motivation: 研究DP微调的大语言模型对生成语言质量和文本实用性的影响，此前该方面未被研究。

Method: 用三种语料在四种隐私级别下微调五个大语言模型，评估生成文本的长度、语法正确性、词汇多样性，并在下游分类任务中测试合成输出的实用性。

Result: 强隐私约束下的大语言模型生成的文本至少短77%，语法正确性至少低9%，二元语法多样性至少低10%，下游分类任务准确率下降。

Conclusion: 强隐私约束可能损害生成的合成数据的实用性。

Abstract: Ensuring user privacy by synthesizing data from large language models (LLMs)
tuned under differential privacy (DP) has become popular recently. However, the
impact of DP fine-tuned LLMs on the quality of the language and the utility of
the texts they produce has not been investigated. In this work, we tune five
LLMs with three corpora under four levels of privacy and assess the length, the
grammatical correctness, and the lexical diversity of the text outputs they
produce. We also probe the utility of the synthetic outputs in downstream
classification tasks such as book genre recognition based on book descriptions
and cause of death recognition based on verbal autopsies. The results indicate
that LLMs tuned under stronger privacy constrains produce texts that are
shorter by at least 77 %, that are less grammatically correct by at least 9 %,
and are less diverse by at least 10 % in bi-gram diversity. Furthermore, the
accuracy they reach in downstream classification tasks decreases, which might
be detrimental to the usefulness of the generated synthetic data.

</details>


### [330] [Struct-Bench: A Benchmark for Differentially Private Structured Text Generation](https://arxiv.org/abs/2509.10696)
*Shuaiqi Wang,Vikas Raunak,Arturs Backurs,Victor Reis,Pei Zhou,Sihao Chen,Longqi Yang,Zinan Lin,Sergey Yekhanin,Giulia Fanti*

Main category: cs.CL

TL;DR: 提出Struct - Bench框架和基准，用于评估含自然语言的结构化合成数据集，含数据集、指标实现和排行榜并公开，还给出案例。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据评估技术难以捕捉含自然语言的结构化数据集的结构特性和相关性，需新评估框架。

Method: 提出Struct - Bench框架，要求用上下文无关文法（CFG）表示数据集结构，构建含5个真实和2个合成数据集的基准，包含指标参考实现和排行榜。

Result: 所构建数据集对现有DP合成数据生成方法构成挑战，给出用Struct - Bench改进结构化数据合成质量的案例。

Conclusion: Struct - Bench为隐私保护合成数据生成方法提供标准化评估平台，已公开。

Abstract: Differentially private (DP) synthetic data generation is a promising
technique for utilizing private datasets that otherwise cannot be exposed for
model training or other analytics. While much research literature has focused
on generating private unstructured text and image data, in enterprise settings,
structured data (e.g., tabular) is more common, often including natural
language fields or components. Existing synthetic data evaluation techniques
(e.g., FID) struggle to capture the structural properties and correlations of
such datasets. In this work, we propose Struct-Bench, a framework and benchmark
for evaluating synthetic datasets derived from structured datasets that contain
natural language data. The Struct-Bench framework requires users to provide a
representation of their dataset structure as a Context-Free Grammar (CFG). Our
benchmark comprises 5 real-world and 2 synthetically generated datasets, each
annotated with CFGs. We show that these datasets demonstrably present a great
challenge even for state-of-the-art DP synthetic data generation methods.
Struct-Bench also includes reference implementations of different metrics and a
leaderboard, thereby providing researchers a standardized evaluation platform
to benchmark and investigate privacy-preserving synthetic data generation
methods. Further, we also present a case study showing how to use Struct-Bench
to improve the synthetic data quality of Private Evolution (PE) on structured
data. The benchmark and the leaderboard have been publicly made available at
https://struct-bench.github.io.

</details>


### [331] [PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models](https://arxiv.org/abs/2509.10737)
*Zaur Gouliev,Jennifer Waters,Chengqian Wang*

Main category: cs.CL

TL;DR: 文章对五个多语言transformer模型在多语言虚假信息检测任务上进行系统比较，引入新语料库，揭示模型性能差异，指出AI系统潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 多数AI模型仅在英语上进行基准测试，而虚假信息跨语言传播，需评估多语言模型在虚假信息检测中的有效性。

Method: 对mBERT、XLM、XLM - RoBERTa、RemBERT和mT5五个多语言transformer模型在真假信息分类任务上进行系统比较，引入PolyTruth Disinfo Corpus语料库进行评估。

Result: 不同模型表现有差异，RemBERT整体准确率更高，在低资源语言上表现出色，mBERT和XLM在训练数据稀缺时存在明显局限。

Conclusion: 研究揭示了多语言虚假信息检测AI系统的潜力和当前局限，数据集公开以促进进一步研究。

Abstract: Disinformation spreads rapidly across linguistic boundaries, yet most AI
models are still benchmarked only on English. We address this gap with a
systematic comparison of five multilingual transformer models: mBERT, XLM,
XLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning
classification task. While transformer-based language models have demonstrated
notable success in detecting disinformation in English, their effectiveness in
multilingual contexts still remains up for debate. To facilitate evaluation, we
introduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs
(false claim vs. factual correction) spanning over twenty five languages that
collectively cover five language families and a broad topical range from
politics, health, climate, finance, and conspiracy, half of which are
fact-checked disinformation claims verified by an augmented MindBugs Discovery
dataset. Our experiments revealed performance variations. Models such as
RemBERT achieved better overall accuracy, particularly excelling in
low-resource languages, whereas models like mBERT and XLM exhibit considerable
limitations when training data is scarce. We provide a discussion of these
performance patterns and implications for real-world deployment. The dataset is
publicly available on our GitHub repository to encourage further
experimentation and advancement. Our findings illuminate both the potential and
the current limitations of AI systems for multilingual disinformation
detection.

</details>


### [332] [Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity](https://arxiv.org/abs/2509.11374)
*Bowen Jing,Yang Cui,Tianpeng Huang*

Main category: cs.CL

TL;DR: 本文系统比较无Transformer和有Transformer的深度监督学习方法在关系抽取中的性能，发现基于Transformer的模型表现更优，还简要回顾研究历程并讨论大语言模型的作用和现状。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型时代，探究不同深度监督学习方法在关系抽取中的性能差异。

Method: 选用一系列非Transformer架构（PA - LSTM、C - GCN、AGGCN）和Transformer架构（BERT、RoBERTa、R - BERT），使用传统指标如微F1，在不同场景、句子长度和训练集比例下进行实验，实验数据集为TACRED、TACREV和RE - TACRED。

Result: 基于Transformer的模型表现优于非Transformer模型，前者微F1分数达80 - 90%，后者为64 - 67%。

Conclusion: 基于Transformer的模型在关系抽取中性能更好，同时对监督关系分类研究历程进行回顾，探讨了大语言模型在关系抽取中的角色和现状。

Abstract: In the era of large language model, relation extraction (RE) plays an
important role in information extraction through the transformation of
unstructured raw text into structured data (Wadhwa et al., 2023). In this
paper, we systematically compare the performance of deep supervised learning
approaches without transformers and those with transformers. We used a series
of non-transformer architectures such as PA-LSTM(Zhang et al., 2017),
C-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),
and a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu
and He, 2019). Our comparison included traditional metrics like micro F1, as
well as evaluations in different scenarios, varying sentence lengths, and
different percentages of the dataset for training. Our experiments were
conducted on TACRED, TACREV, and RE-TACRED. The results show that
transformer-based models outperform non-transformer models, achieving micro F1
scores of 80-90% compared to 64-67% for non-transformer models. Additionally,
we briefly review the research journey in supervised relation classification
and discuss the role and current status of large language models (LLMs) in
relation extraction.

</details>


### [333] [ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims](https://arxiv.org/abs/2509.11492)
*Anirban Saha Anik,Md Fahimul Kabir Chowdhury,Andrew Wyckoff,Sagnik Ray Choudhury*

Main category: cs.CL

TL;DR: 本文介绍CLEF 2025 CheckThat! Lab任务3系统，用两种方法验证数值和时间声明，最佳模型在验证集表现好但测试集有泛化问题。


<details>
  <summary>Details</summary>
Motivation: 验证数值和时间声明，解决数值事实验证问题。

Method: 探索零样本提示和参数高效LoRA监督微调两种方法，研究多种证据选择策略。

Result: 最佳模型LLaMA经LoRA微调在英语验证集表现好，测试集性能下降。

Conclusion: 强调证据粒度和模型适配对稳健数值事实验证的重要性。

Abstract: This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,
which focuses on verifying numerical and temporal claims using retrieved
evidence. We explore two complementary approaches: zero-shot prompting with
instruction-tuned large language models (LLMs) and supervised fine-tuning using
parameter-efficient LoRA. To enhance evidence quality, we investigate several
selection strategies, including full-document input and top-k sentence
filtering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned
with LoRA achieves strong performance on the English validation set. However, a
notable drop in the test set highlights a generalization challenge. These
findings underscore the importance of evidence granularity and model adaptation
for robust numerical fact verification.

</details>


### [334] [Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics](https://arxiv.org/abs/2509.11513)
*Zhongyang Hu,Naijie Gu,Xiangzhi Tao,Tianhui Gu,Yibing Zhou*

Main category: cs.CL

TL;DR: 本文研究词汇替换中候选词排序问题，提出基于注意力权重和集成梯度的两种方法，实验表明可提升排序性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效建模候选替换对目标词及其上下文的双向影响，难以准确表征语义变化。

Method: 研究基于注意力权重和集成梯度的两种方法，测量上下文标记对目标标记的影响，并结合原句和替换句的语义相似度对候选词排序。

Result: 在LS07和SWORDS数据集上的实验表明，两种方法均提高了排序性能。

Conclusion: 所提出的两种方法能有效提升词汇替换中候选词的排序性能。

Abstract: A key subtask in lexical substitution is ranking the given candidate words. A
common approach is to replace the target word with a candidate in the original
sentence and feed the modified sentence into a model to capture semantic
differences before and after substitution. However, effectively modeling the
bidirectional influence of candidate substitution on both the target word and
its context remains challenging. Existing methods often focus solely on
semantic changes at the target position or rely on parameter tuning over
multiple evaluation metrics, making it difficult to accurately characterize
semantic variation. To address this, we investigate two approaches: one based
on attention weights and another leveraging the more interpretable integrated
gradients method, both designed to measure the influence of context tokens on
the target token and to rank candidates by incorporating semantic similarity
between the original and substituted sentences. Experiments on the LS07 and
SWORDS datasets demonstrate that both approaches improve ranking performance.

</details>


### [335] [HARP: Hallucination Detection via Reasoning Subspace Projection](https://arxiv.org/abs/2509.11536)
*Junjie Hu,Gang Tu,ShengYu Cheng,Jinxin Li,Jinting Wang,Rui Chen,Zhilong Zhou,Dongbo Shan*

Main category: cs.CL

TL;DR: 提出新的幻觉检测框架HARP，通过推理子空间投影检测大语言模型幻觉，降维滤波，在多数据集实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型幻觉检测方法难以分离语义和推理信息，且鲁棒性不足。

Method: 提出HARP框架，将LLMs隐藏状态空间分解为语义和推理子空间，用Unembedding层分离，SVD获取子空间基向量，将隐藏状态投影到推理子空间基向量用于检测。

Result: HARP将特征维度降至约原维度5%，过滤大部分噪声，在多数据集实验中达最优幻觉检测性能，如在TriviaQA上AUROC达92.8%，超此前最优方法7.5%。

Conclusion: HARP能有效解决现有幻觉检测方法的问题，实现更优的幻觉检测。

Abstract: Hallucinations in Large Language Models (LLMs) pose a major barrier to their
reliable use in critical decision-making. Although existing hallucination
detection methods have improved accuracy, they still struggle with
disentangling semantic and reasoning information and maintaining robustness. To
address these challenges, we propose HARP (Hallucination detection via
reasoning subspace projection), a novel hallucination detection framework. HARP
establishes that the hidden state space of LLMs can be decomposed into a direct
sum of a semantic subspace and a reasoning subspace, where the former encodes
linguistic expression and the latter captures internal reasoning processes.
Moreover, we demonstrate that the Unembedding layer can disentangle these
subspaces, and by applying Singular Value Decomposition (SVD) to its
parameters, the basis vectors spanning the semantic and reasoning subspaces are
obtained. Finally, HARP projects hidden states onto the basis vectors of the
reasoning subspace, and the resulting projections are then used as input
features for hallucination detection in LLMs. By using these projections, HARP
reduces the dimension of the feature to approximately 5% of the original,
filters out most noise, and achieves enhanced robustness. Experiments across
multiple datasets show that HARP achieves state-of-the-art hallucination
detection performance; in particular, it achieves an AUROC of 92.8% on
TriviaQA, outperforming the previous best method by 7.5%.

</details>


### [336] [HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking](https://arxiv.org/abs/2509.11552)
*Wensheng Lu,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.CL

TL;DR: 本文针对RAG系统文档分块缺乏有效评估工具的问题，提出HiCBench和HiChunk框架，实验表明HiCBench能评估分块方法影响，HiChunk能提升RAG系统性能。


<details>
  <summary>Details</summary>
Motivation: RAG系统中文档分块缺乏有效评估工具，现有评估基准因证据稀疏难以评估分块质量。

Method: 提出HiCBench，包含人工标注的多级文档分块点、合成的证据密集问答对及对应证据源；引入基于微调大语言模型的多级文档结构化框架HiChunk，结合Auto - Merge检索算法。

Result: HiCBench能有效评估不同分块方法在整个RAG流程中的影响，HiChunk在合理时间消耗内实现更好分块质量。

Conclusion: HiCBench和HiChunk可提升RAG系统的整体性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances the response capabilities of
language models by integrating external knowledge sources. However, document
chunking as an important part of RAG system often lacks effective evaluation
tools. This paper first analyzes why existing RAG evaluation benchmarks are
inadequate for assessing document chunking quality, specifically due to
evidence sparsity. Based on this conclusion, we propose HiCBench, which
includes manually annotated multi-level document chunking points, synthesized
evidence-dense quetion answer(QA) pairs, and their corresponding evidence
sources. Additionally, we introduce the HiChunk framework, a multi-level
document structuring framework based on fine-tuned LLMs, combined with the
Auto-Merge retrieval algorithm to improve retrieval quality. Experiments
demonstrate that HiCBench effectively evaluates the impact of different
chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves
better chunking quality within reasonable time consumption, thereby enhancing
the overall performance of RAG systems.

</details>


### [337] [EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI](https://arxiv.org/abs/2509.11648)
*Sai Kartheek Reddy Kasu*

Main category: cs.CL

TL;DR: 引入EthicsMH数据集评估AI在心理健康领域的伦理推理能力，旨在促进负责任AI系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有道德和临床决策基准无法充分捕捉心理健康实践中的独特伦理困境，需新评估方法。

Method: 创建包含125个场景的EthicsMH数据集，场景有结构化字段，可评估多方面能力。

Result: 建立了连接AI伦理和心理健康决策的任务框架。

Conclusion: 发布数据集可作为种子资源，通过社区和专家贡献扩展，助力开发负责任的AI系统。

Abstract: The deployment of large language models (LLMs) in mental health and other
sensitive domains raises urgent questions about ethical reasoning, fairness,
and responsible alignment. Yet, existing benchmarks for moral and clinical
decision-making do not adequately capture the unique ethical dilemmas
encountered in mental health practice, where confidentiality, autonomy,
beneficence, and bias frequently intersect. To address this gap, we introduce
Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios
designed to evaluate how AI systems navigate ethically charged situations in
therapeutic and psychiatric contexts. Each scenario is enriched with structured
fields, including multiple decision options, expert-aligned reasoning, expected
model behavior, real-world impact, and multi-stakeholder viewpoints. This
structure enables evaluation not only of decision accuracy but also of
explanation quality and alignment with professional norms. Although modest in
scale and developed with model-assisted generation, EthicsMH establishes a task
framework that bridges AI ethics and mental health decision-making. By
releasing this dataset, we aim to provide a seed resource that can be expanded
through community and expert contributions, fostering the development of AI
systems capable of responsibly handling some of society's most delicate
decisions.

</details>


### [338] [CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model](https://arxiv.org/abs/2509.11698)
*Wei-Hsin Yeh,Yu-An Su,Chih-Ning Chen,Yi-Hsueh Lin,Calvin Ku,Wen-Hsin Chiu,Min-Chun Hu,Lun-Wei Ku*

Main category: cs.CL

TL;DR: 提出参考模型CoachMe分析学习者与参考动作差异，可用于特定体育项目，实验显示其表现优于GPT - 4o。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型提升了动作理解，但生成精确且特定于体育项目的动作指令仍具挑战。

Method: 提出参考模型CoachMe，从时间和物理方面分析学习者动作与参考动作的差异。

Result: CoachMe能提供高质量指令，在花样滑冰和拳击的G - Eval中分别比GPT - 4o高31.6%和58.3%，且指令会阐述错误及改进方法。

Conclusion: CoachMe能有效识别动作错误并提供改进反馈，适用于特定体育项目。

Abstract: Motion instruction is a crucial task that helps athletes refine their
technique by analyzing movements and providing corrective guidance. Although
recent advances in multimodal models have improved motion understanding,
generating precise and sport-specific instruction remains challenging due to
the highly domain-specific nature of sports and the need for informative
guidance. We propose CoachMe, a reference-based model that analyzes the
differences between a learner's motion and a reference under temporal and
physical aspects. This approach enables both domain-knowledge learning and the
acquisition of a coach-like thinking process that identifies movement errors
effectively and provides feedback to explain how to improve. In this paper, we
illustrate how CoachMe adapts well to specific sports such as skating and
boxing by learning from general movements and then leveraging limited data.
Experiments show that CoachMe provides high-quality instructions instead of
directions merely in the tone of a coach but without critical information.
CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on
boxing. Analysis further confirms that it elaborates on errors and their
corresponding improvement methods in the generated instructions. You can find
CoachMe here: https://motionxperts.github.io/

</details>


### [339] [CEMTM: Contextual Embedding-based Multimodal Topic Modeling](https://arxiv.org/abs/2509.11465)
*Amirhossein Abaskohi,Raymond Li,Chuyuan Li,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出CEMTM模型用于从含文本和图像的长短文档中推断主题结构，实验显示其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 从含文本和图像的长短文档中推断连贯且可解释的主题结构。

Method: 基于微调大视觉语言模型获取上下文嵌入，采用分布注意力机制加权，通过重建目标使主题表示与文档嵌入对齐。

Result: 在六个多模态基准测试中始终优于单模态和多模态基线，平均LLM分数达2.61，在少样本检索有效，能捕捉复杂领域视觉语义。

Conclusion: CEMTM模型在多模态主题建模方面表现出色，具有良好性能和应用潜力。

Abstract: We introduce CEMTM, a context-enhanced multimodal topic model designed to
infer coherent and interpretable topic structures from both short and long
documents containing text and images. CEMTM builds on fine-tuned large vision
language models (LVLMs) to obtain contextualized embeddings, and employs a
distributional attention mechanism to weight token-level contributions to topic
inference. A reconstruction objective aligns topic-based representations with
the document embedding, encouraging semantic consistency across modalities.
Unlike existing approaches, CEMTM can process multiple images per document
without repeated encoding and maintains interpretability through explicit
word-topic and document-topic distributions. Extensive experiments on six
multimodal benchmarks show that CEMTM consistently outperforms unimodal and
multimodal baselines, achieving a remarkable average LLM score of 2.61. Further
analysis shows its effectiveness in downstream few-shot retrieval and its
ability to capture visually grounded semantics in complex domains such as
scientific articles.

</details>


### [340] [Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models](https://arxiv.org/abs/2509.11868)
*Sabrina Patania,Luca Annese,Anna Lambiase,Anita Pellegrini,Tom Foulsham,Azzurra Ruggeri,Silvia Rossi,Silvia Serino,Dimitri Ognibene*

Main category: cs.CL

TL;DR: 研究PerspAct系统，用扩展导演任务评估GPT，结果显示语言交流助于完善表征，高阶段提升协作效果，强调结合具身视角与语言建模及评估内部言语的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型很少同时处理语言和具身视角采择，需研究能同时处理二者的模型。

Method: 将ReAct范式与大语言模型集成到PerspAct系统，用扩展导演任务评估GPT生成符合指定发展阶段内部叙事的能力，从定性和定量两方面评估对协作表现的影响。

Result: GPT执行任务前能产生符合发展阶段的叙事，互动中常向更高级阶段转变；高发展阶段提升协作有效性，早期阶段在复杂情境结果更易变。

Conclusion: 强调在大语言模型中整合具身视角采择和语言以更好模拟发展动态，以及在语言和具身任务中评估内部言语的重要性。

Abstract: Language and embodied perspective taking are essential for human
collaboration, yet few computational models address both simultaneously. This
work investigates the PerspAct system [1], which integrates the ReAct (Reason
and Act) paradigm with Large Language Models (LLMs) to simulate developmental
stages of perspective taking, grounded in Selman's theory [2]. Using an
extended director task, we evaluate GPT's ability to generate internal
narratives aligned with specified developmental stages, and assess how these
influence collaborative performance both qualitatively (action selection) and
quantitatively (task efficiency). Results show that GPT reliably produces
developmentally-consistent narratives before task execution but often shifts
towards more advanced stages during interaction, suggesting that language
exchanges help refine internal representations. Higher developmental stages
generally enhance collaborative effectiveness, while earlier stages yield more
variable outcomes in complex contexts. These findings highlight the potential
of integrating embodied perspective taking and language in LLMs to better model
developmental dynamics and stress the importance of evaluating internal speech
during combined linguistic and embodied tasks.

</details>


### [341] [PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation](https://arxiv.org/abs/2509.11517)
*Rodrigo M. Carrillo-Larco,Jesus Lovón Melgarejo,Manuel Castillo-Cara,Gusseppe Bravo-Rocca*

Main category: cs.CL

TL;DR: 本文构建秘鲁医学问答数据集，微调大语言模型，评估对比模型在医学问题回答上的表现，推荐使用medgemma-27b-text-it或微调后的medgemma-4b-it。


<details>
  <summary>Details</summary>
Motivation: 探究医学大语言模型在西班牙语医学问题及拉丁美洲国家医学问题上的表现，因基于大语言模型的医学应用在拉丁美洲逐渐流行。

Method: 构建含8380个问题的PeruMedQA数据集，选8个医学大语言模型，开发零样本任务特定提示，用参数高效微调（PEFT）和低秩自适应（LoRA）微调medgemma-4b-it。

Result: medgemma-27b-text-it表现最佳，部分情况正确答案比例超90%；参数少于100亿的模型正确率低于60%；微调后的medgemma-4b-it胜过参数少于100亿的模型，与700亿参数模型相当。

Conclusion: 对于需要西班牙语国家知识库及流行病学特征与秘鲁相似的医学AI应用和研究，建议使用medgemma-27b-text-it或微调后的medgemma-4b-it。

Abstract: BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable
performance in answering medical examinations. However, the extent to which
this high performance is transferable to medical questions in Spanish and from
a Latin American country remains unexplored. This knowledge is crucial as
LLM-based medical applications gain traction in Latin America. AIMS: to build a
dataset of questions from medical examinations taken by Peruvian physicians
pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate
and compare the performance in terms of accuracy between vanilla LLMs and the
fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice
question-answering (MCQA) datasets containing 8,380 questions spanning 12
medical domains (2018-2025). We selected eight medical LLMs including
medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific
prompts to answer the questions appropriately. We employed parameter-efficient
fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it
utilizing all questions except those from 2025 (test set). RESULTS:
medgemma-27b-text-it outperformed all other models, achieving a proportion of
correct answers exceeding 90% in several instances. LLMs with <10 billion
parameters exhibited <60% of correct answers, while some exams yielded results
<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all
LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters
across various examinations. CONCLUSIONS: For medical AI application and
research that require knowledge bases from Spanish-speaking countries and those
exhibiting similar epidemiological profiles to Peru's, interested parties
should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.

</details>


### [342] [Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles](https://arxiv.org/abs/2509.11991)
*Jesús Calleja,David Ponce,Thierry Etchegoyhen*

Main category: cs.CL

TL;DR: 介绍Vicomtech参加西班牙语文本适应挑战，采用自动后编辑方法，在官方指标平均表现上获佳绩。


<details>
  <summary>Details</summary>
Motivation: 参与CLEARS挑战，进行西班牙语文本向Plain Language和Easy Read的适应。

Method: 对不同类型的初始大语言模型适应结果进行自动后编辑，迭代生成连续适应结果直到可读性和相似度指标达标。

Result: 在官方指标平均表现上，提交结果在Plain language和Easy Read适应中分别获第一和第二名。

Conclusion: 该自动后编辑方法在文本适应挑战中取得了较好的成绩。

Abstract: We describe Vicomtech's participation in the CLEARS challenge on text
adaptation to Plain Language and Easy Read in Spanish. Our approach features
automatic post-editing of different types of initial Large Language Model
adaptations, where successive adaptations are generated iteratively until
readability and similarity metrics indicate that no further adaptation
refinement can be successfully performed. Taking the average of all official
metrics, our submissions achieved first and second place in Plain language and
Easy Read adaptation, respectively.

</details>


### [343] [User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums](https://arxiv.org/abs/2509.11777)
*Mikhail Kulyabin,Jan Joosten,Choro Ulan uulu,Nuno Miguel Martins Pacheco,Fabian Ries,Filippos Petridis,Jan Bosch,Helena Holmström Olsson*

Main category: cs.CL

TL;DR: 论文提出用户体验感知洞察数据集（UXPID），用于解决工业论坛用户反馈分析难题，支持相关研究和模型训练评估。


<details>
  <summary>Details</summary>
Motivation: 工业论坛用户反馈虽有价值，但因内容非结构化和特定领域性，传统数据分析技术难利用，限制其对产品开发和支持策略的作用。

Method: 从公共工业自动化论坛提取7130个合成匿名用户反馈分支，用大语言模型对每个分支进行系统分析和标注。

Result: 得到UXPID数据集，包含多帖子评论、元数据和上下文对话数据。

Conclusion: UXPID数据集可促进用户需求、用户体验分析和AI驱动反馈处理研究，支持基于Transformer模型的相关任务。

Abstract: Customer feedback in industrial forums reflect a rich but underexplored
source of insight into real-world product experience. These publicly shared
discussions offer an organic view of user expectations, frustrations, and
success stories shaped by the specific contexts of use. Yet, harnessing this
information for systematic analysis remains challenging due to the unstructured
and domain-specific nature of the content. The lack of structure and
specialized vocabulary makes it difficult for traditional data analysis
techniques to accurately interpret, categorize, and quantify the feedback,
thereby limiting its potential to inform product development and support
strategies. To address these challenges, this paper presents the User
eXperience Perception Insights Dataset (UXPID), a collection of 7130
artificially synthesized and anonymized user feedback branches extracted from a
public industrial automation forum. Each JavaScript object notation (JSON)
record contains multi-post comments related to specific hardware and software
products, enriched with metadata and contextual conversation data. Leveraging a
large language model (LLM), each branch is systematically analyzed and
annotated for UX insights, user expectations, severity and sentiment ratings,
and topic classifications. The UXPID dataset is designed to facilitate research
in user requirements, user experience (UX) analysis, and AI-driven feedback
processing, particularly where privacy and licensing restrictions limit access
to real-world data. UXPID supports the training and evaluation of
transformer-based models for tasks such as issue detection, sentiment analysis,
and requirements extraction in the context of technical forums.

</details>


### [344] [Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities](https://arxiv.org/abs/2509.12098)
*Payam Latifi*

Main category: cs.CL

TL;DR: 本文对六种系统的命名实体识别性能进行小规模基准测试，发现大语言模型在识别上下文敏感实体上表现更好，传统工具在特定任务仍有竞争力。


<details>
  <summary>Details</summary>
Motivation: 评估六种系统（三种非大语言模型NLP工具和三种大语言模型）在命名实体识别上的性能。

Method: 使用含119个标记、覆盖五种实体类型的数据集，用F1分数将各系统输出与手动标注的黄金标准数据集对比。

Result: 大语言模型在识别上下文敏感实体上总体优于传统工具，Gemini平均F1分数最高；传统系统如Stanza在结构化标签上更稳定；大语言模型间处理时间表达和多词组织时有差异。

Conclusion: 大语言模型上下文理解能力更强，但传统工具在特定任务仍有竞争力，为模型选择提供参考。

Abstract: This pilot study presents a small-scale but carefully annotated benchmark of
Named Entity Recognition (NER) performance across six systems: three non-LLM
NLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models
(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119
tokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).
We evaluated each system's output against the manually annotated gold standard
dataset using F1-score. The results show that LLMs generally outperform
conventional tools in recognizing context-sensitive entities like person names,
with Gemini achieving the highest average F1-score. However, traditional
systems like Stanza demonstrate greater consistency in structured tags such as
LOCATION and DATE. We also observed variability among LLMs, particularly in
handling temporal expressions and multi-word organizations. Our findings
highlight that while LLMs offer improved contextual understanding, traditional
tools remain competitive in specific tasks, informing model selection.

</details>


### [345] [In-domain SSL pre-training and streaming ASR](https://arxiv.org/abs/2509.12101)
*Jarod Duret,Salima Mdhaffar,Gaëlle Laperrière,Ryan Whetten,Audrey Galametz,Catherine Kobus,Marion-Cécile Martin,Jo Oleiwan,Yannick Estève*

Main category: cs.CL

TL;DR: 研究特定领域自监督预训练对空中交通管制（ATC）环境中离线和流式语音识别（ASR）的好处，结果表明领域自适应预训练和流式方法可提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究特定领域自监督预训练在ATC环境中对离线和流式ASR的益处。

Method: 在4.5k小时未标记的ATC数据上训练BEST - RQ模型，在较小的有监督ATC数据集上微调；提出使用分块注意力和动态卷积实现实时处理；将领域内SSL模型与w2v - BERT 2.0和HuBERT等通用语音编码器进行比较。

Result: 领域自适应预训练显著提高了标准ATC基准测试的性能，降低了字错误率；提出的流式方法在更严格的延迟约束下进一步降低了字错误率。

Conclusion: 为ATC数据专门定制SSL表示是在实际操作环境中实现更准确、高效ASR系统的可行途径。

Abstract: In this study, we investigate the benefits of domain-specific self-supervised
pre-training for both offline and streaming ASR in Air Traffic Control (ATC)
environments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then
fine-tune on a smaller supervised ATC set. To enable real-time processing, we
propose using chunked attention and dynamic convolutions, ensuring low-latency
inference. We compare these in-domain SSL models against state-of-the-art,
general-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show
that domain-adapted pre-training substantially improves performance on standard
ATC benchmarks, significantly reducing word error rates when compared to models
trained on broad speech corpora. Furthermore, the proposed streaming approach
further improves word error rate under tighter latency constraints, making it
particularly suitable for safety-critical aviation applications. These findings
highlight that specializing SSL representations for ATC data is a practical
path toward more accurate and efficient ASR systems in real-world operational
settings.

</details>


### [346] [Query-Focused Extractive Summarization for Sentiment Explanation](https://arxiv.org/abs/2509.11989)
*Ahmed Moubtahij,Sylvie Ratté,Yazid Attabi,Maxime Dumas*

Main category: cs.CL

TL;DR: 利用查询聚焦摘要（QFS）辅助客户反馈分析，提出多偏差框架和专门方法，实验结果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 建设性分析客户反馈需从大量文本中确定情感原因，QFS 模型受查询与源文档语言不和谐问题阻碍。

Method: 提出多偏差框架以通用方式弥合差距，通过基于情感的偏差和查询扩展制定专门方法解决情感解释问题。

Result: 在真实世界的专有情感感知 QFS 数据集上实验结果优于基线模型。

Conclusion: 所提出的多偏差框架和专门方法对解决 QFS 中的语言不和谐问题及提高情感解释效果有效。

Abstract: Constructive analysis of feedback from clients often requires determining the
cause of their sentiment from a substantial amount of text documents. To assist
and improve the productivity of such endeavors, we leverage the task of
Query-Focused Summarization (QFS). Models of this task are often impeded by the
linguistic dissonance between the query and the source documents. We propose
and substantiate a multi-bias framework to help bridge this gap at a
domain-agnostic, generic level; we then formulate specialized approaches for
the problem of sentiment explanation through sentiment-based biases and query
expansion. We achieve experimental results outperforming baseline models on a
real-world proprietary sentiment-aware QFS dataset.

</details>


### [347] [Pun Unintended: LLMs and the Illusion of Humor Understanding](https://arxiv.org/abs/2509.12158)
*Alessandro Zangari,Matteo Marcuzzo,Andrea Albarelli,Mohammad Taher Pilehvar,Jose Camacho-Collados*

Main category: cs.CL

TL;DR: 本文指出大语言模型理解双关语较浅，通过分析和重构基准展示细微变化会误导模型，并给出相关基准、人类评估及模型处理双关语的鲁棒性挑战分析。


<details>
  <summary>Details</summary>
Motivation: 指出大语言模型对双关语理解较浅，缺乏人类的细致理解。

Method: 系统分析和重构现有双关语基准。

Result: 展示双关语的细微变化足以误导大语言模型。

Conclusion: 提供了全面细致的双关语检测基准、对近期大语言模型的人类评估，以及分析了模型处理双关语的鲁棒性挑战。

Abstract: Puns are a form of humorous wordplay that exploits polysemy and phonetic
similarity. While LLMs have shown promise in detecting puns, we show in this
paper that their understanding often remains shallow, lacking the nuanced grasp
typical of human interpretation. By systematically analyzing and reformulating
existing pun benchmarks, we demonstrate how subtle changes in puns are
sufficient to mislead LLMs. Our contributions include comprehensive and nuanced
pun detection benchmarks, human evaluation across recent LLMs, and an analysis
of the robustness challenges these models face in processing puns.

</details>


### [348] [RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing](https://arxiv.org/abs/2509.12168)
*Timothy Rupprecht,Enfu Nan,Arash Akbari,Arman Akbari,Lei Lu,Priyanka Maan,Sean Duffy,Pu Zhao,Yumei He,David Kaeli,Yanzhi Wang*

Main category: cs.CL

TL;DR: 本文提出RAGs - to - Riches提示框架用于大语言模型角色扮演，评估显示该框架使模型更真实、更能保持角色设定。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型角色扮演的少样本学习方法存在使模型意外偏离角色设定的问题，尤其是面对敌对用户时，需要改进方法。

Method: 将大语言模型角色扮演问题转化为文本检索问题，提出RAGs - to - Riches提示框架，利用精心策划的参考示例来生成模型响应，并引入两个新的基于标记的ROUGE指标进行评估。

Result: 模拟与敌对用户交互时，提示策略推理时从参考示例中引入的标记平均增加35%；在453次角色扮演交互中，模型被认为更真实，比零样本和上下文学习方法更能保持角色设定。

Conclusion: 该方法为构建强大、符合人类需求的大语言模型角色扮演框架提供了可扩展的策略。

Abstract: Role-playing Large language models (LLMs) are increasingly deployed in
high-stakes domains such as healthcare, education, and governance, where
failures can directly impact user trust and well-being. A cost effective
paradigm for LLM role-playing is few-shot learning, but existing approaches
often cause models to break character in unexpected and potentially harmful
ways, especially when interacting with hostile users. Inspired by
Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a
text retrieval problem and propose a new prompting framework called
RAGs-to-Riches, which leverages curated reference demonstrations to condition
LLM responses. We evaluate our framework with LLM-as-a-judge preference voting
and introduce two novel token-level ROUGE metrics: Intersection over Output
(IOO) to quantity how much an LLM improvises and Intersection over References
(IOR) to measure few-shot demonstrations utilization rate during the evaluation
tasks. When simulating interactions with a hostile user, our prompting strategy
incorporates in its responses during inference an average of 35% more tokens
from the reference demonstrations. As a result, across 453 role-playing
interactions, our models are consistently judged as being more authentic, and
remain in-character more often than zero-shot and in-context Learning (ICL)
methods. Our method presents a scalable strategy for building robust,
human-aligned LLM role-playing frameworks.

</details>


### [349] [Preservation of Language Understanding Capabilities in Speech-aware Large Language Models](https://arxiv.org/abs/2509.12171)
*Marek Kubis,Paweł Skórzewski,Iwona Christop,Mateusz Czyżnikiewicz,Jakub Kubiak,Łukasz Bondaruk,Marcin Lewandowski*

Main category: cs.CL

TL;DR: 提出C3T基准评估语音感知大语言模型性能，量化语言理解能力保留程度、模型公平性和跨模态鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估语音感知大语言模型的性能。

Method: 利用文本任务和语音克隆文本转语音模型进行量化。

Result: 无明确提及具体结果

Conclusion: 无明确提及具体结论

Abstract: The paper presents C3T (Cross-modal Capabilities Conservation Test), a new
benchmark for assessing the performance of speech-aware large language models.
The benchmark utilizes textual tasks and a voice cloning text-to-speech model
to quantify the extent to which language understanding capabilities are
preserved when the model is accessed via speech input. C3T quantifies the
fairness of the model for different categories of speakers and its robustness
across text and speech modalities.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [350] [Physics-informed neural network solves minimal surfaces in curved spacetime](https://arxiv.org/abs/2509.10866)
*Koji Hashimoto,Koichi Kyo,Masaki Murata,Gakuto Ogiwara,Norihiro Tanahashi*

Main category: hep-th

TL;DR: 开发基于PINNs的框架解决弯曲时空最小曲面边值问题，在多领域有应用。


<details>
  <summary>Details</summary>
Motivation: 解决弯曲时空最小曲面边值问题，特别是涉及奇点和移动边界的情况。

Method: 将物理定律编码到损失函数，设计能处理奇异行为和动态边界的网络架构。

Result: 框架能有效处理边界奇点，支持多种边界条件施加方式，通过AdS时空最小曲面问题应用展示了其通用性。

Conclusion: 该技术不仅适用于高能理论物理，还广泛适用于数学、工程和自然科学中的边值问题。

Abstract: We develop a flexible framework based on physics-informed neural networks
(PINNs) for solving boundary value problems involving minimal surfaces in
curved spacetimes, with a particular emphasis on singularities and moving
boundaries. By encoding the underlying physical laws into the loss function and
designing network architectures that incorporate the singular behavior and
dynamic boundaries, our approach enables robust and accurate solutions to both
ordinary and partial differential equations with complex boundary conditions.
We demonstrate the versatility of this framework through applications to
minimal surface problems in anti-de Sitter (AdS) spacetime, including examples
relevant to the AdS/CFT correspondence (e.g. Wilson loops and gluon scattering
amplitudes) popularly used in the context of string theory in theoretical
physics. Our methods efficiently handle singularities at boundaries, and also
support both "soft" (loss-based) and "hard" (formulation-based) imposition of
boundary conditions, including cases where the position of a boundary is
promoted to a trainable parameter. The techniques developed here are not
limited to high-energy theoretical physics but are broadly applicable to
boundary value problems encountered in mathematics, engineering, and the
natural sciences, wherever singularities and moving boundaries play a critical
role.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [351] [Length-Aware Rotary Position Embedding for Text-Speech Alignment](https://arxiv.org/abs/2509.11084)
*Hyeongju Kim,Juheon Lee,Jinhyeok Yang,Jacob Morton*

Main category: eess.AS

TL;DR: 提出长度感知旋转位置嵌入（LARoPE）改进文本到语音（TTS）系统的文本 - 语音对齐，实验显示其优于传统RoPE。


<details>
  <summary>Details</summary>
Motivation: 改进现有基于transformer架构和交叉注意力机制的TTS系统中文本 - 语音对齐效果。

Method: 引入LARoPE，使用长度归一化索引计算查询和键位置之间的相对距离。

Result: LARoPE比RoPE收敛更快、对齐更准、TTS质量更高，对语句时长变化更具鲁棒性，在30秒长语音生成中性能稳定，在零样本TTS基准测试中达到了最先进的字错误率。

Conclusion: LARoPE是对RoPE的有效扩展，能提升TTS系统性能。

Abstract: Many recent text-to-speech (TTS) systems are built on transformer
architectures and employ cross-attention mechanisms for text-speech alignment.
Within these systems, rotary position embedding (RoPE) is commonly used to
encode positional information in text and speech representations. In this work,
we introduce length-aware RoPE (LARoPE), a simple yet effective extension of
RoPE that improves text-speech alignment. Unlike RoPE, which relies on absolute
indices, LARoPE computes relative distances between query and key positions
using length-normalized indices. Experimental results show that LARoPE
consistently outperforms RoPE, offering faster loss convergence, more accurate
text-speech alignment, and higher overall TTS quality. Furthermore, LARoPE
demonstrates greater resilience to variations in utterance duration and
maintains stable performance in extended speech generation up to 30 seconds,
whereas RoPE suffers from notable degradation. Notably, our method achieves a
state-of-the-art word error rate on a standard zero-shot TTS benchmark.

</details>


### [352] [Spectral Bottleneck in Deep Neural Networks: Noise is All You Need](https://arxiv.org/abs/2509.09719)
*Hemanth Chandravamsi,Dhanush V. Shenoy,Itay Zinn,Shimon Pisnoy,Steven H. Frankel*

Main category: eess.AS

TL;DR: 论文研究深度神经网络频谱学习偏差及频谱瓶颈问题，提出WINNER方案，解决频谱瓶颈，提升性能并开拓新方向。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在处理高频主导信号时存在频谱瓶颈，模型无法完整重建信号，需解决此问题。

Method: 提出广义的目标感知“权重扰动方案”（WINNER），用高斯噪声扰动均匀初始化的权重，噪声尺度由目标信号的频谱质心自适应确定。

Result: 该方法解决了频谱瓶颈，收敛更快、表示精度更高，在音频拟合中超越现有方法，在图像拟合和去噪任务中也有显著提升。

Conclusion: 此方法不仅解决信号重建问题，还为计算机视觉和科学机器学习中的自适应权重初始化策略开辟了新方向。

Abstract: Deep neural networks are known to exhibit a spectral learning bias, wherein
low-frequency components are learned early in training, while high-frequency
modes emerge more gradually in later epochs. However, when the target signal
lacks low-frequency components and is dominated by broadband high frequencies,
training suffers from a 'spectral bottleneck', and the model fails to
reconstruct the entire signal, including the frequency components that lie
within the network's representational capacity. We examine such a scenario in
the context of implicit neural representations (INRs) with sinusoidal
representation networks (SIRENs), focusing on the challenge of fitting
high-frequency-dominant signals that are susceptible to spectral bottleneck. To
effectively fit any target signal irrespective of it's frequency content, we
propose a generalized target-aware 'weight perturbation scheme' (WINNER -
weight initialization with noise for neural representations) for network
initialization. The scheme perturbs uniformly initialized weights with Gaussian
noise, where the noise scales are adaptively determined by the spectral
centroid of the target signal. We show that the noise scales can provide
control over the spectra of network activations and the eigenbasis of the
empirical neural tangent kernel. This method not only addresses the spectral
bottleneck but also yields faster convergence and with improved representation
accuracy, outperforming state-of-the-art approaches in audio fitting and
achieving notable gains in image fitting and denoising tasks. Beyond signal
reconstruction, our approach opens new directions for adaptive weight
initialization strategies in computer vision and scientific machine learning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [353] [Biomarkers of brain diseases](https://arxiv.org/abs/2509.10547)
*Pascal Helson,Arvind Kumar*

Main category: q-bio.NC

TL;DR: 当前脑特征在临床诊断和预后中应用少，传统生物标志物寻找方法不足，应采用多模态和纵向脑数据定义多维生物标志物。


<details>
  <summary>Details</summary>
Motivation: 解决脑特征在临床诊断和预后中应用少的问题，改进寻找脑疾病生物标志物的方法。

Method: 进行思想实验，指出仅靠更多数据和强大算法不足以识别生物标志物，提出使用多模态和纵向脑数据进行分组后定义多维生物标志物。

Result: 发现更多数据和强大算法不足以识别脑疾病生物标志物。

Conclusion: 应使用多模态和纵向脑数据来指导分组并定义脑疾病的多维生物标志物。

Abstract: Despite the diversity of brain data acquired and advanced AI-based algorithms
to analyze them, brain features are rarely used in clinics for diagnosis and
prognosis. Here we argue that the field continues to rely on cohort comparisons
to seek biomarkers, despite the well-established degeneracy of brain features.
Using a thought experiment, we show that more data and more powerful algorithms
will not be sufficient to identify biomarkers of brain diseases. We argue that
instead of comparing patient versus healthy controls using single data type, we
should use multimodal (e.g. brain activity, neurotransmitters, neuromodulators,
brain imaging) and longitudinal brain data to guide the grouping before
defining multidimensional biomarkers for brain diseases.

</details>


### [354] [Trial-Level Time-frequency EEG Desynchronization as a Neural Marker of Pain](https://arxiv.org/abs/2509.10552)
*D. A. Blanco-Mora,A. Dierolf,J. Gonçalves,M. van Der Meulen*

Main category: q-bio.NC

TL;DR: 研究分析高密度EEG数据，发现额叶 - 中央电极的β波段事件相关去同步化（ERD）可区分疼痛与非疼痛试验，可作为疼痛非言语标记，为无报告疼痛监测提供可能。


<details>
  <summary>Details</summary>
Motivation: 当前疼痛测量依赖自我报告，限制了对非交流患者的监测和转化研究，需要寻找可重复的伤害性处理标记。

Method: 分析59名健康参与者在疼痛和非疼痛条件下接受电刺激的高密度EEG数据，进行逐次时间 - 频率分解，使用广义线性混合模型。

Result: 额叶 - 中央电极有稳健的β波段ERD可区分疼痛与非疼痛试验，ERD与主观强度评分相关，年龄和性别会调节这种关系，ERD能预测参与者的评分。

Conclusion: 试验水平的EEG振荡可作为可靠的疼痛指标，为个性化、无报告的疼痛监测开辟了途径，未来需在患者群体中验证并拓展到多模态方法。

Abstract: Pain remains one of the most pressing health challenges, yet its measurement
still relies heavily on self-report, limiting monitoring in non-communicative
patients and hindering translational research. Neural oscillations recorded
with electroencephalography (EEG) provide a promising avenue for identifying
reproducible markers of nociceptive processing. Prior studies have reported
pain-related event-related desynchronization (ERD) in the alpha and beta bands,
but most rely on trial-averaging, obscuring variability that may be critical
for perception. We analyzed high-density EEG from 59 healthy participants who
underwent electrical stimulation under Pain and No-Pain conditions. Per-trial
time-frequency decomposition revealed robust beta-band ERD in frontal-central
electrodes that differentiated Pain from No-Pain trials. Generalized linear
mixed models demonstrated that ERD scaled with subjective intensity ratings
(VAS), and that age and gender moderated this relationship. Reverse models
further showed that ERD predicted VAS ratings across participants, underscoring
its potential as a nonverbal marker of pain. These findings provide preliminary
evidence that trial-level EEG oscillations can serve as reliable indicators of
pain and open avenues for individualized, report-free pain monitoring. Future
work should validate these results in patient populations and extend analyses
to multimodal approaches combining EEG, MRI, and attention-based modulation
strategies.

</details>


### [355] [HiLWS: A Human-in-the-Loop Weak Supervision Framework for Curating Clinical and Home Video Data for Neurological Assessment](https://arxiv.org/abs/2509.10557)
*Atefeh Irani,Maryam S. Mirian,Alex Lassooij,Reshad Hosseini,Hadi Moradi,Martin J. McKeown*

Main category: q-bio.NC

TL;DR: 提出HiLWS框架处理帕金森病手部运动任务视频标注问题，发现家庭录制数据失败模式，强调情境敏感策略重要性。


<details>
  <summary>Details</summary>
Motivation: 基于视频评估帕金森病运动症状，家庭录制视频存在视觉退化等挑战，需有效处理和标注方法。

Method: 采用级联人在环弱监督框架HiLWS，先弱监督聚合专家标注成概率标签训练模型，再结合模型预测和专家输入进行二次弱监督，还包括质量过滤等步骤和情境敏感评估指标。

Result: 发现家庭录制数据的关键失败模式。

Conclusion: 情境敏感的策整理略对稳健的医学视频分析很重要。

Abstract: Video-based assessment of motor symptoms in conditions such as Parkinson's
disease (PD) offers a scalable alternative to in-clinic evaluations, but
home-recorded videos introduce significant challenges, including visual
degradation, inconsistent task execution, annotation noise, and domain shifts.
We present HiLWS, a cascaded human-in-the-loop weak supervision framework for
curating and annotating hand motor task videos from both clinical and home
settings. Unlike conventional single-stage weak supervision methods, HiLWS
employs a novel cascaded approach, first applies weak supervision to aggregate
expert-provided annotations into probabilistic labels, which are then used to
train machine learning models. Model predictions, combined with expert input,
are subsequently refined through a second stage of weak supervision. The
complete pipeline includes quality filtering, optimized pose estimation, and
task-specific segment extraction, complemented by context-sensitive evaluation
metrics that assess both visual fidelity and clinical relevance by prioritizing
ambiguous cases for expert review. Our findings reveal key failure modes in
home recorded data and emphasize the importance of context-sensitive curation
strategies for robust medical video analysis.

</details>


### [356] [On a Geometry of Interbrain Networks](https://arxiv.org/abs/2509.10650)
*Nicolás Hinrichs,Noah Guzmán,Melanie Weber*

Main category: q-bio.NC

TL;DR: 本文提出用离散几何研究社交中神经交互动态重构，提升超扫描方法揭示社交行为神经机制的能力。


<details>
  <summary>Details</summary>
Motivation: 传统社交神经科学中脑间同步性的度量方法依赖固定的基于相关性的方法，解释能力局限于描述性观察，需要更好的概念框架。

Method: 利用离散几何，通过基于曲率分布的熵指标识别网络连接的关键转变的流程来解释脑间连接变化。

Result: 显著增强了超扫描方法揭示互动社交行为中潜在神经机制的能力。

Conclusion: 基于离散几何的框架能有效用于研究社交中的神经交互动态重构。

Abstract: Effective analysis in neuroscience benefits significantly from robust
conceptual frameworks. Traditional metrics of interbrain synchrony in social
neuroscience typically depend on fixed, correlation-based approaches,
restricting their explanatory capacity to descriptive observations. Inspired by
the successful integration of geometric insights in network science, we propose
leveraging discrete geometry to examine the dynamic reconfigurations in neural
interactions during social exchanges. Unlike conventional synchrony approaches,
our method interprets inter-brain connectivity changes through the evolving
geometric structures of neural networks. This geometric framework is realized
through a pipeline that identifies critical transitions in network connectivity
using entropy metrics derived from curvature distributions. By doing so, we
significantly enhance the capacity of hyperscanning methodologies to uncover
underlying neural mechanisms in interactive social behavior.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [357] [Information Entropy-Based Scheduling for Communication-Efficient Decentralized Learning](https://arxiv.org/abs/2507.17426)
*Jaiprakash Nagar,Zheng Chen,Marios Kountouris,Photios A. Stavrou*

Main category: cs.IT

TL;DR: 论文引入节点和链路调度策略提升资源受限网络中D - SGD通信效率，提出基于信息熵的重要性指标确定调度概率，模拟显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限网络中分散随机梯度下降（D - SGD）的通信效率问题。

Method: 在D - SGD算法每次迭代中，按通信成本约束随机激活部分节点或链路子集，提出基于信息熵的重要性指标确定调度概率。

Result: 节点调度时比基于BC的方法收敛快，通信预算最多降低60%，高预算时表现相当或更优；链路调度时优于或等同于MATCHA。

Conclusion: 所提方法能有效提升资源受限网络中D - SGD的通信效率。

Abstract: This paper addresses decentralized stochastic gradient descent (D-SGD) over
resource-constrained networks by introducing node-based and link-based
scheduling strategies to enhance communication efficiency. In each iteration of
the D-SGD algorithm, only a few disjoint subsets of nodes or links are randomly
activated, subject to a given communication cost constraint. We propose a novel
importance metric based on information entropy to determine node and link
scheduling probabilities. We validate the effectiveness of our approach through
extensive simulations, comparing it against state-of-the-art methods, including
betweenness centrality (BC) for node scheduling and \textit{MATCHA} for link
scheduling. The results show that our method consistently outperforms the
BC-based method in the node scheduling case, achieving faster convergence with
up to 60\% lower communication budgets. At higher communication budgets (above
60\%), our method maintains comparable or superior performance. In the link
scheduling case, our method delivers results that are superior to or on par
with those of \textit{MATCHA}.

</details>


### [358] [Task-Agnostic Learnable Weighted-Knowledge Base Scheme for Robust Semantic Communications](https://arxiv.org/abs/2509.11636)
*Shiyao Jiang,Jian Jiao,Xingjian Zhang,Ye Wang,Dusit Niyato,Qinyu Zhang*

Main category: cs.IT

TL;DR: 提出TALSC框架用于鲁棒图像传输，结合SCM和语义编码网络，设计SCM - GE方法，模拟显示能缓解噪声和不平衡问题，指标优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 应对6G网络中知识库存在的真实世界异构数据偏差，如标签翻转噪声和类不平衡问题，实现鲁棒图像传输。

Method: 提出TALSC框架，包含SCM作为元学习器和语义编码网络作为学习器；设计SCM - GE方法，在SCM中嵌入KAN。

Result: 模拟显示TALSC框架有效缓解任务无关图像语义通信中的翻转噪声和类不平衡影响，语义恢复准确率和多尺度结构相似性比现有方法至少高12%。

Conclusion: TALSC框架能有效处理知识库中的数据偏差问题，在图像语义通信中表现优于现有方法。

Abstract: With the emergence of diverse and massive data in the upcoming
sixth-generation (6G) networks, the task-agnostic semantic communication system
is regarded to provide robust intelligent services. In this paper, we propose a
task-agnostic learnable weighted-knowledge base semantic communication (TALSC)
framework for robust image transmission to address the real-world heterogeneous
data bias in KB, including label flipping noise and class imbalance. The TALSC
framework incorporates a sample confidence module (SCM) as meta-learner and the
semantic coding networks as learners. The learners are updated based on the
empirical knowledge provided by the learnable weighted-KB (LW-KB). Meanwhile,
the meta-learner evaluates the significance of samples according to the task
loss feedback, and adjusts the update strategy of learners to enhance the
robustness in semantic recovery for unknown tasks. To strike a balance between
SCM parameters and precision of significance evaluation, we design an SCM-grid
extension (SCM-GE) approach by embedding the Kolmogorov-Arnold networks (KAN)
within SCM, which leverages the concept of spline refinement in KAN and enables
scalable SCM with customizable granularity without retraining. Simulations
demonstrate that the TALSC framework effectively mitigates the effects of
flipping noise and class imbalance in task-agnostic image semantic
communication, achieving at least 12% higher semantic recovery accuracy (SRA)
and multi-scale structural similarity (MS-SSIM) compared to state-of-the-art
methods.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [359] [Statistical Model Checking of NetLogo Models](https://arxiv.org/abs/2509.10977)
*Marco Pangallo,Daniele Giachini,Andrea Vandin*

Main category: cs.MA

TL;DR: 本文提出用统计模型检查方法自动化分析基于代理的模型（ABMs），介绍MultiVeStA工具并展示其分析和校准能力，促进更严谨可靠的分析。


<details>
  <summary>Details</summary>
Motivation: ABMs分析具挑战性，传统方法缺乏统计严谨性，作者期望自动化分析过程并提供统计严谨性保证。

Method: 借鉴统计模型检查领域，提出方法论，使用MultiVeStA工具并将其与NetLogo集成。

Result: MultiVeStA工具显著减少运行统计严谨检查所需的时间和人力干预，展示了其分析和校准能力。

Conclusion: 该工具链使对NetLogo模型进行统计检查变得便捷，促进对ABM输出进行更严谨可靠的分析。

Abstract: Agent-based models (ABMs) are gaining increasing traction in several domains,
due to their ability to represent complex systems that are not easily
expressible with classical mathematical models. This expressivity and richness
come at a cost: ABMs can typically be analyzed only through simulation, making
their analysis challenging. Specifically, when studying the output of ABMs, the
analyst is often confronted with practical questions such as: (i) how many
independent replications should be run? (ii) how many initial time steps should
be discarded as a warm-up? (iii) after the warm-up, how long should the model
run? (iv) what are the right parameter values? Analysts usually resort to rules
of thumb and experimentation, which lack statistical rigor. This is mainly
because addressing these points takes time, and analysts prefer to spend their
limited time improving the model. In this paper, we propose a methodology,
drawing on the field of Statistical Model Checking, to automate the process and
provide guarantees of statistical rigor for ABMs written in NetLogo, one of the
most popular ABM platforms. We discuss MultiVeStA, a tool that dramatically
reduces the time and human intervention needed to run statistically rigorous
checks on ABM outputs, and introduce its integration with NetLogo. Using two
ABMs from the NetLogo library, we showcase MultiVeStA's analysis capabilities
for NetLogo ABMs, as well as a novel application to statistically rigorous
calibration. Our tool-chain makes it immediate to perform statistical checks
with NetLogo models, promoting more rigorous and reliable analyses of ABM
outputs.

</details>


### [360] [Nash Equilibrium and Belief Evolution in Differential Games](https://arxiv.org/abs/2509.11739)
*Jiangjing Zhou,Ovanes Petrosian,Ye Zhang,Hongwei Gao*

Main category: cs.MA

TL;DR: 研究连续时间下带运动 - 收益不确定性的微分博弈，提出连续贝叶斯更新框架，证明信念收敛，推导纳什均衡策略并验证其收敛性，在污染控制游戏中检验方法有效性。


<details>
  <summary>Details</summary>
Motivation: 研究连续时间下带运动 - 收益不确定性的微分博弈问题。

Method: 提出连续贝叶斯更新框架更新玩家对不确定参数的信念，利用关键概率定理进行理论证明。

Result: 玩家信念收敛到真实参数值，推导了带连续贝叶斯更新的纳什均衡策略且策略收敛，在污染控制游戏中玩家估计在离散小时间间隔下收敛。

Conclusion: 连续贝叶斯更新框架在处理带运动 - 收益不确定性的微分博弈中有效，信念更新对决策过程有重要作用。

Abstract: This study investigates differential games with motion-payoff uncertainty in
continuous-time settings. We propose a framework where players update their
beliefs about uncertain parameters using continuous Bayesian updating.
Theoretical proofs leveraging key probability theorems demonstrate that
players' beliefs converge to the true parameter values, ensuring stability and
accuracy in long-term estimations. We further derive Nash Equilibrium
strategies with continuous Bayesian updating for players, emphasizing the role
of belief updates in decision-making processes. Additionally, we establish the
convergence of Nash Equilibrium strategies with continuous Bayesian updating.
The efficacy of both continuous and dynamic Bayesian updating is examined in
the context of pollution control games, showing convergence in players'
estimates under small time intervals in discrete scenarios.

</details>


### [361] [MALLM: Multi-Agent Large Language Models Framework](https://arxiv.org/abs/2509.11656)
*Jonas Becker,Lars Benedikt Kaesberg,Niklas Bauer,Jan Philip Wahle,Terry Ruas,Bela Gipp*

Main category: cs.MA

TL;DR: 提出开源框架MALLM，可系统分析多智能体辩论组件，有超144种配置，能加载数据集并提供评估管道。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论框架存在面向工具使用、缺乏综合评估、配置性有限等问题。

Method: 引入MALLM框架，提供多种独特配置，用简单配置文件定义辩论，可加载Huggingface数据集并提供评估管道。

Result: MALLM提供超144种MAD独特配置，能加载数据集并进行评估。

Conclusion: MALLM适合研究人员，有助于理解多智能体辩论组件及其相互作用。

Abstract: Multi-agent debate (MAD) has demonstrated the ability to augment collective
intelligence by scaling test-time compute and leveraging expertise. Current
frameworks for multi-agent debate are often designed towards tool use, lack
integrated evaluation, or provide limited configurability of agent personas,
response generators, discussion paradigms, and decision protocols. We introduce
MALLM (Multi-Agent Large Language Models), an open-source framework that
enables systematic analysis of MAD components. MALLM offers more than 144
unique configurations of MAD, including (1) agent personas (e.g., Expert,
Personality), (2) response generators (e.g., Critical, Reasoning), (3)
discussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g.,
Voting, Consensus). MALLM uses simple configuration files to define a debate.
Furthermore, MALLM can load any textual Huggingface dataset (e.g., MMLU-Pro,
WinoGrande) and provides an evaluation pipeline for easy comparison of MAD
configurations. MALLM is tailored towards researchers and provides a window
into the heart of multi-agent debate, facilitating the understanding of its
components and their interplay.

</details>


### [362] [SafeDiver: Cooperative AUV-USV Assisted Diver Communication via Multi-agent Reinforcement Learning Approach](https://arxiv.org/abs/2509.11508)
*Tinglong Deng,Hang Tao,Xinxiang Wang,Yinyan Wang,Hanjiang Luo*

Main category: cs.MA

TL;DR: 提出利用海洋无人系统辅助潜水员通信方案，经仿真验证可实现可靠高速通信。


<details>
  <summary>Details</summary>
Motivation: 水下人类活动增加，现有潜水员通信方法因自身缺点和复杂环境面临挑战。

Method: 使用多个配备光和声学多模态通信设备的AUV作为中继节点，采用多智能体强化学习控制AUV协同移动；利用USV按需部署和覆盖广的优势作为水面中继节点，控制AUV自适应选择中继USV节点。

Result: 通过仿真验证，该方案能有效实现潜水员可靠和高速通信。

Conclusion: 所提方案可有效解决潜水员可靠高速通信问题。

Abstract: As underwater human activities are increasing, the demand for underwater
communication service presents a significant challenge. Existing underwater
diver communication methods face hurdles due to inherent disadvantages and
complex underwater environments. To address this issue, we propose a scheme
that utilizes maritime unmanned systems to assist divers with reliable and
high-speed communication. Multiple AUVs are equipped with optical and acoustic
multimodal communication devices as relay nodes, providing adaptive
communication services based on changes in the diver's activity area. By using
a multi-agent reinforcement learning (MARL) approach to control the cooperative
movement of AUVs, high-speed and reliable data transmission between divers can
be achieved. At the same time, utilizing the advantages of on-demand deployment
and wide coverage of unmanned surface vehicles (USVs) as surface relay nodes to
coordinate and forward information from AUVs, and controlling AUVs to
adaptively select relay USV nodes for data transmission, high-quality
communication between divers and surface platform can be achieved. Through
simulation verification, the proposed scheme can effectively achieve reliable
and high-speed communication for divers.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [363] [DeepSeasons: a Deep Learning scale-selecting approach to Seasonal Forecasts](https://arxiv.org/abs/2509.10494)
*A. Navarra,G. G. Navarra*

Main category: physics.ao-ph

TL;DR: 本文介绍了用于季节性预测的深度学习方法DeepSeasons，能以低成本提升预测准确性和可靠性，有新的长期预测途径和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 因大气动力学的混沌特性，季节性预测具有挑战性，需要提升预测的准确性和可靠性。

Method: 利用先进神经网络架构和大量历史气候数据集，识别气候变量中的复杂非线性模式和依赖关系，且可针对特定区域或变量应用。

Result: 相比基于GCM的预测方法，DeepSeasons有相似或更好的技能，成本显著降低，还能直接预测异常和时间均值。

Conclusion: 该创新方法有望显著改善气候相关风险的管理和决策过程。

Abstract: Seasonal forecasting remains challenging due to the inherent chaotic nature
of atmospheric dynamics. This paper introduces DeepSeasons, a novel deep
learning approach designed to enhance the accuracy and reliability of seasonal
forecasts. Leveraging advanced neural network architectures and extensive
historical climatic datasets, DeepSeasons identifies complex, nonlinear
patterns and dependencies in climate variables with similar or improved skill
respcet GCM-based forecasting methods, at a significant lower cost. The
framework also allow tailored application to specific regions or variables,
rather than the overall problem of predicting the entire atmosphere/ocean
system. The proposed methods also allow for direct predictions of anomalies and
time-means, opening a new approach to long-term forecasting and highlighting
its potential for operational deployment in climate-sensitive sectors. This
innovative methodology promises substantial improvements in managing
climate-related risks and decision-making processes.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [364] [Results of the 2025 Video Browser Showdown](https://arxiv.org/abs/2509.12000)
*Luca Rossetto,Klaus Schoeffmann,Cathal Gurrin,Jakub Lokoč,Werner Bailer*

Main category: cs.MM

TL;DR: 报告展示了2025年1月8日在日本奈良举行的第14届视频浏览器展示会的结果。


<details>
  <summary>Details</summary>
Motivation: 展示第14届视频浏览器展示会成果

Method: 未提及

Result: 未详细说明展示会结果

Conclusion: 未提及

Abstract: This report presents the results of the 14th Video Browser Showdown, held at
the 2025 International Conference on Multimedia Modeling on the 8th of January
2025 in Nara, Japan.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [365] [Efficient Decrease-and-Conquer Linearizability Monitoring](https://arxiv.org/abs/2410.04581)
*Lee Zheng Han,Umang Mathur*

Main category: cs.PL

TL;DR: 本文提出统一的`decrease - and - conquer`算法框架用于线性izability监测，为多种数据类型推导多项式时间算法并优化，经实现和评估，该方法可处理大历史记录且优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 线性izability监测是开发自定义并发数据结构实现时排除早期问题的有前景的第一步，但对该问题何时变得易处理缺乏系统理解。

Method: 提出统一的`decrease - and - conquer`算法框架，识别历史记录中保留线性izability的值；对几种流行数据类型实例化框架推导算法；通过高效数据结构分摊子问题成本进行优化。

Result: 为集合、栈、队列和优先队列等数据类型推导了多项式时间算法，优化后达到最优对数线性时间复杂度；实现和评估表明该方法可扩展到大型历史记录且优于现有工具。

Conclusion: 所提出的算法框架有效，能解决线性izability监测问题，且性能表现良好。

Abstract: Linearizability has become the de facto correctness specification for
implementations of concurrent data structures. While formally verifying such
implementations remains challenging, linearizability monitoring has emerged as
a promising first step to rule out early problems in the development of custom
implementations, and serves as a key component in approaches that stress test
such implementations. In this work, we investigate linearizability monitoring
-- check if an execution history of an implementation is linearizable. While
this problem is intractable in general, a systematic understanding of when it
becomes tractable has remained elusive. We revisit this problem and first
present a unified `decrease-and-conquer' algorithmic framework for
linearizability monitoring. At its heart, this framework asks to identify
special linearizability-preserving values in a given history -- values whose
presence yields an equilinearizable sub-history when removed, and whose absence
indicates non-linearizability. We prove that a polynomial time algorithm for
the problem of identifying linearizability-preserving values, yields a
polynomial time algorithm for linearizability monitoring, while conversely,
intractability of this problem implies intractability of the monitoring
problem. We demonstrate our framework's effectiveness by instantiating it for
several popular data types -- sets, stacks, queues and priority queues --
deriving polynomial time algorithms for each, with the unambiguity restriction,
where each insertion to the underlying data structure adds a distinct value. We
optimize these algorithms to achieve the optimal log-linear time complexity by
amortizing the cost of solving sub-problems through efficient data structures.
Our implementation and evaluation on publicly available implementations show
that our approach scales to large histories and outperforms existing tools.

</details>


### [366] [Enhanced Data Race Prediction Through Modular Reasoning](https://arxiv.org/abs/2504.10813)
*Zhendong Ang,Azadeh Farzan,Umang Mathur*

Main category: cs.PL

TL;DR: 本文识别并形式化了两种预测数据竞争的方法的原理，结合二者提出新的可预测数据竞争类型，给出改进算法并展示实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于交换性和前缀推理的预测数据竞争方法多为临时创建，缺乏统一原理，需结合二者开发新算法提升预测能力。

Method: 识别并形式化两种方法的原理，模块化结合二者提出新类型，基于反链思想改进前缀推理算法。

Result: 提出了可在常量空间和线性时间内以流式方式预测的粒度前缀竞争，给出改进算法，实验展示了新算法的表达能力和性能。

Conclusion: 结合交换性推理和前缀推理的新算法有效，能预测更多类型的数据竞争，改进的前缀推理算法是关键一步。

Abstract: There are two orthogonal methodologies for efficient prediction of data races
from concurrent program runs: commutativity and prefix reasoning. There are
several instances of each methodology in the literature, with the goal of
predicting data races using a streaming algorithm where the required memory
does not grow proportional to the length of the observed run, but these
instances were mostly created in an ad hoc manner, without much attention to
their unifying underlying principles. In this paper, we identify and formalize
these principles for each category with the ultimate goal of paving the way for
combining them into a new algorithm which shares their efficiency
characteristics but offers strictly more prediction power. In particular, we
formalize three distinct classes of races predictable using commutativity
reasoning, and compare them. We identify three different styles of prefix
reasoning, and prove that they predict the same class of races, which provably
contains all races predictable by any commutativity reasoning technique.
  Our key contribution is combining prefix reasoning and commutativity
reasoning in a modular way to introduce a new class of races, granular prefix
races, that are predictable in constant-space and linear time, in a streaming
fashion. This class of races includes all races predictable using commutativity
and prefix reasoning techniques. We present an improved constant-space
algorithm for prefix reasoning alone based on the idea of antichains (from
language theory). This improved algorithm is the stepping stone that is
required to devise an efficient algorithm for prediction of granular prefix
races. We present experimental results to demonstrate the expressive power and
performance of our new algorithm.

</details>


### [367] [Program Skeletons for Automated Program Translation](https://arxiv.org/abs/2504.07483)
*Bo Wang,Tianyu Li,Ruishi Li,Umang Mathur,Prateek Saxena*

Main category: cs.PL

TL;DR: 提出基于程序骨架的跨编程语言自动化翻译方法，有原型系统Skel，结果显示可扩展性有提升。


<details>
  <summary>Details</summary>
Motivation: 跨编程语言翻译有挑战，自动化技术难以扩展到大型程序，需将源程序行为用目标语言表达。

Method: 提出基于程序骨架的自动化翻译方法，骨架保留源程序高层结构，可与现有代码合成器结合。

Result: 原型系统Skel在9个真实Python程序上，95%代码片段可自动翻译，最终翻译结果通过整体测试套件验证。

Conclusion: 基于程序骨架的翻译方法有较好的可扩展性。

Abstract: Translating software between programming languages is a challenging task, for
which automated techniques have been elusive and hard to scale up to larger
programs. A key difficulty in cross-language translation is that one has to
re-express the intended behavior of the source program into idiomatic
constructs of a different target language. This task needs abstracting away
from the source language-specific details, while keeping the overall
functionality the same. In this work, we propose a novel and systematic
approach for making such translation amenable to automation based on a
framework we call program skeletons. A program skeleton retains the high-level
structure of the source program by abstracting away and effectively summarizing
lower-level concrete code fragments, which can be mechanically translated to
the target programming language. A skeleton, by design, permits many different
ways of filling in the concrete implementation for fragments, which can work in
conjunction with existing data-driven code synthesizers. Most importantly,
skeletons can conceptually enable sound decomposition, i.e., if each individual
fragment is correctly translated, taken together with the mechanically
translated skeleton, the final translated program is deemed to be correct as a
whole. We present a prototype system called Skel embodying the idea of
skeleton-based translation from Python to JavaScript. Our results show
promising scalability compared to prior works. For 9 real-world Python
programs, some with more than about 1k lines of code, 95% of their code
fragments can be automatically translated, while about 5% require manual
effort. All the final translations are correct with respect to whole-program
test suites.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [368] [Efficient Algorithms for Partitioning Circulant Graphs with Optimal Spectral Approximation](https://arxiv.org/abs/2509.11382)
*Surya Teja Gavva,Peng Zhang*

Main category: math.CO

TL;DR: 本文探索循环图的多项式时间分区算法，为生成元成等差数列的循环图开发高效算法，证明生成元远离等差数列时无法达到特定误差，并将算法扩展到特定凯莱图。


<details>
  <summary>Details</summary>
Motivation: Marcus - Spielman - Srivastava定理证明非构造性，能否在多项式时间找到图边集分区是开放问题，本文聚焦循环图解决该问题。

Method: 通过对循环图的生成元进行分区来探索算法，针对不同生成元情况分析。

Result: 为生成元成等差数列的循环图开发高效算法，误差与定理匹配且最优；证明生成元远离等差数列时无法达到定理误差；将算法扩展到特定凯莱图。

Conclusion: 在循环图及特定凯莱图上找到了多项式时间分区算法，明确了不同生成元情况的分区效果。

Abstract: The Marcus-Spielman-Srivastava theorem (Annals of Mathematics, 2015) for the
Kadison-Singer conjecture implies the following result in spectral graph
theory: For any undirected graph $G = (V,E)$ with a maximum edge effective
resistance at most $\alpha$, there exists a partition of its edge set $E$ into
$E_1 \cup E_2$ such that the two edge-induced subgraphs of $G$ spectrally
approximates $(1/2)G$ with a relative error $O(\sqrt{\alpha})$. However, the
proof of this theorem is non-constructive. It remains an open question whether
such a partition can be found in polynomial time, even for special classes of
graphs.
  In this paper, we explore polynomial-time algorithms for partitioning
circulant graphs via partitioning their generators. We develop an efficient
algorithm that partitions a circulant graph whose generators form an arithmetic
progression, with an error matching that in the Marcus-Spielman-Srivastava
theorem and optimal, up to a constant. On the other hand, we prove that if the
generators of a circulant graph are ``far" from an arithmetic progression, no
partition of the generators can yield two circulant subgraphs with an error
matching that in the Marcus-Spielman-Srivastava theorem.
  In addition, we extend our algorithm to Cayley graphs whose generators are
from a product of multiple arithmetic progressions.

</details>


### [369] [Liar's vertex-edge domination in unit disk graph](https://arxiv.org/abs/2509.11775)
*Debojyoti Bhattacharya,Subhabrata Paul*

Main category: math.CO

TL;DR: 研究图论中说谎者顶点 - 边支配问题，证明其在单位圆盘图中是NP完全问题，并设计多项式时间近似方案。


<details>
  <summary>Details</summary>
Motivation: 解决图论中最小说谎者顶点 - 边支配集的求解问题，了解该问题复杂度并寻找有效近似解法。

Method: 证明问题的NP完全性，设计多项式时间近似方案（PTAS）。

Result: 证明了说谎者顶点 - 边支配问题在单位圆盘图中是NP完全问题，设计出多项式时间近似方案。

Conclusion: 该问题在单位圆盘图中具有NP完全性，但可通过多项式时间近似方案求解。

Abstract: Let $G=(V, E)$ be a simple undirected graph. A closed neighbourhood of an
edge $e=uv$ between two vertices $u$ and $v$ of $G$, denoted by $N_G[e]$, is
the set of vertices in the neighbourhood of $u$ and $v$ including $\{u,v\}$. A
subset $L$ of $V$ is said to be liar's vertex-edge dominating set if $(i)$ for
every edge $e\in E$, $|N_G[e]\cap L|\geq 2$ and $(ii)$ for every pair of
distinct edges $e,e'$, $|(N_G[e]\cup N_G[e'])\cap L|\geq 3$. The minimum liar's
vertex-edge domination problem is to find the liar's vertex-edge dominating set
of minimum cardinality. In this article, we show that the liar's vertex-edge
domination problem is NP-complete in unit disk graphs, and we design a
polynomial time approximation scheme(PTAS) for the minimum liar's vertex-edge
domination problem in unit disk graphs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [370] [AI Hasn't Fixed Teamwork, But It Shifted Collaborative Culture: A Longitudinal Study in a Project-Based Software Development Organization (2023-2025)](https://arxiv.org/abs/2509.10956)
*Qing Xiao,Xinlan Emily Hu,Mark E. Whiting,Arvind Karunakaran,Hong Shen,Hancheng Cao*

Main category: cs.HC

TL;DR: 研究通过对软件开发组织成员的纵向访谈，发现AI在团队合作中的应用未达预期，虽未解决协作问题，但重塑了协作文化。


<details>
  <summary>Details</summary>
Motivation: 探究AI能否缓解团队协作的长期挑战，考察其在团队合作中的期望和使用情况。

Method: 对项目制软件开发组织的15名成员进行2023 - 2025年的纵向两波访谈研究。

Result: 2023年参与者期望AI成为智能协调者，到2025年AI主要用于加速个人任务，协作问题未解决，但重塑了协作文化。

Conclusion: AI在团队合作中的现实情况比预期复杂，虽未解决协作问题，但改变了协作文化。

Abstract: When AI entered the workplace, many believed it could reshape teamwork as
profoundly as it boosted individual productivity. Would AI finally ease the
longstanding challenges of team collaboration? Our findings suggested a more
complicated reality. We conducted a longitudinal two-wave interview study
(2023-2025) with members (N=15) of a project-based software development
organization to examine the expectations and use of AI in teamwork. In early
2023, just after the release of ChatGPT, participants envisioned AI as an
intelligent coordinator that could align projects, track progress, and ease
interpersonal frictions. By 2025, however, AI was used mainly to accelerate
individual tasks such as coding, writing, and documentation, leaving persistent
collaboration issues of performance accountability and fragile communication
unresolved. Yet AI reshaped collaborative culture: efficiency became a norm,
transparency and responsible use became markers of professionalism, and AI was
increasingly accepted as part of teamwork.

</details>


### [371] [Vibe Coding for UX Design: Understanding UX Professionals' Perceptions of AI-Assisted Design and Development](https://arxiv.org/abs/2509.10652)
*Jie Li,Youyang Hou,Laura Lin,Ruihao Zhu,Hancheng Cao,Abdallah El Ali*

Main category: cs.HC

TL;DR: 研究生成式AI通过“氛围编码”重塑UX设计实践，展示其工作流程、优势与挑战，探讨相关问题。


<details>
  <summary>Details</summary>
Motivation: 当前虽“氛围编码”在UX设计中快速应用，但缺乏对其如何重构UX工作流程和协作的研究。

Method: 对企业、初创公司和学术界的20位UX专业人员进行访谈。

Result: “氛围编码”有四阶段工作流程，能加速迭代等，但存在代码不可靠等挑战，团队内有新的不对称问题。

Conclusion: 从负责任的人机协作角度，加深了对技能弱化、所有权与披露及创造力保护等方面的理解。

Abstract: Generative AI is reshaping UX design practices through "vibe coding," where
UX professionals express intent in natural language and AI translates it into
functional prototypes and code. Despite rapid adoption, little research has
examined how vibe coding reconfigures UX workflows and collaboration. Drawing
on interviews with 20 UX professionals across enterprises, startups, and
academia, we show how vibe coding follows a four-stage workflow of ideation, AI
generation, debugging, and review. This accelerates iteration, supports
creativity, and lowers barriers to participation. However, professionals
reported challenges of code unreliability, integration, and AI over-reliance.
We find tensions between efficiency-driven prototyping ("intending the right
design") and reflection ("designing the right intention"), introducing new
asymmetries in trust, responsibility, and social stigma within teams. Through
the lens of responsible human-AI collaboration for AI-assisted UX design and
development, we contribute a deeper understanding of deskilling, ownership and
disclosure, and creativity safeguarding in the age of vibe coding.

</details>


### [372] [Dark Patterns Meet GUI Agents: LLM Agent Susceptibility to Manipulative Interfaces and the Role of Human Oversight](https://arxiv.org/abs/2509.10723)
*Jingyu Tang,Chaoran Chen,Jiawen Li,Zhiping Zhang,Bingcan Guo,Ibrahim Khalilov,Simret Araya Gebreegziabher,Bingsheng Yao,Dakuo Wang,Yanfang Ye,Tianshi Li,Ziang Xiao,Yaxing Yao,Toby Jia-Jun Li*

Main category: cs.HC

TL;DR: 研究16种暗模式对LLM驱动的GUI代理、人类参与者和人机团队的影响，发现人和代理都不具一致的抵御能力，协作有新漏洞，提出设计需求。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的GUI代理兴起，了解暗模式对代理的影响变得重要。

Method: 进行两阶段实证研究，考察代理、人类参与者和人机团队对16种暗模式在不同场景下的响应。

Result: 阶段1表明代理常无法识别暗模式，且更重任务完成；阶段2显示人类因认知捷径和习惯顺从失败，代理因程序盲点失败，人类监督可改善但有代价。

Conclusion: 人和代理都不具一致的抵御能力，协作有新漏洞，建议设计应注重透明性、可调节自主性和监督。

Abstract: The dark patterns, deceptive interface designs manipulating user behaviors,
have been extensively studied for their effects on human decision-making and
autonomy. Yet, with the rising prominence of LLM-powered GUI agents that
automate tasks from high-level intents, understanding how dark patterns affect
agents is increasingly important. We present a two-phase empirical study
examining how agents, human participants, and human-AI teams respond to 16
types of dark patterns across diverse scenarios. Phase 1 highlights that agents
often fail to recognize dark patterns, and even when aware, prioritize task
completion over protective action. Phase 2 revealed divergent failure modes:
humans succumb due to cognitive shortcuts and habitual compliance, while agents
falter from procedural blind spots. Human oversight improved avoidance but
introduced costs such as attentional tunneling and cognitive load. Our findings
show neither humans nor agents are uniformly resilient, and collaboration
introduces new vulnerabilities, suggesting design needs for transparency,
adjustable autonomy, and oversight.

</details>


### [373] [Bridging Cultural Distance Between Models Default and Local Classroom Demands: How Global Teachers Adopt GenAI to Support Everyday Teaching Practices](https://arxiv.org/abs/2509.10780)
*Ruiwei Xiao,Qing Xiao,Xinying Hou,Hanqi Jane Li,Phenyo Phemelo Moletsane,Hong Shen,John Stamper*

Main category: cs.HC

TL;DR: 研究定义文化距离概念，访谈30位K - 12教师，提出三级文化距离框架及应对策略，为教育中GenAI工具设计等提供启示。


<details>
  <summary>Details</summary>
Motivation: GenAI模型训练数据集文化不均，其默认文化与当地课堂教学不匹配，需了解教师如何应对这一差距。

Method: 定义文化距离概念，对来自南非、中国台湾和美国的30位将AI融入教学的K - 12教师进行深度访谈。

Result: 开发出三级文化距离框架，提供六个涵盖低、中、高距离水平的示例及教师应对策略。

Conclusion: 为AI设计师、政策制定者和教育工作者创建更公平、具有文化响应性的GenAI教育工具提供启示。

Abstract: Generative AI (GenAI) is rapidly entering K-12 classrooms, offering teachers
new ways for teaching practices. Yet GenAI models are often trained on
culturally uneven datasets, embedding a "default culture" that often misaligns
with local classrooms. To understand how teachers navigate this gap, we defined
the new concept Cultural Distance (the gap between GenAI's default cultural
repertoire and the situated demands of teaching practice) and conducted
in-depth interviews with 30 K-12 teachers, 10 each from South Africa, Taiwan,
and the United States, who had integrated AI into their teaching practice.
These teachers' experiences informed the development of our three-level
cultural distance framework. This work contributes the concept and framework of
cultural distance, six illustrative instances spanning in low, mid, high
distance levels with teachers' experiences and strategies for addressing them.
Empirically, we offer implications to help AI designers, policymakers, and
educators create more equitable and culturally responsive GenAI tools for
education.

</details>


### [374] [Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions](https://arxiv.org/abs/2509.11206)
*Tae Soo Kim,Heechan Lee,Yoonjoo Lee,Joseph Seering,Juho Kim*

Main category: cs.HC

TL;DR: 提出功能碎片化方法并应用于Evalet系统，用户研究表明该方法能帮助识别更多评估偏差，推动LLM评估向细粒度分析转变。


<details>
  <summary>Details</summary>
Motivation: 现有“LLM-as-a-Judge”方法产生的整体分数掩盖了影响评估的具体元素，需要更细粒度的评估方法。

Method: 提出功能碎片化方法，将每个输出分解为关键片段并解释其修辞功能，构建交互式系统Evalet进行可视化。

Result: 用户研究中，该方法帮助从业者多识别48%的评估偏差，有助于校准对LLM评估的信任并发现更多可操作问题。

Conclusion: 工作推动LLM评估从定量分数转向对模型行为的定性、细粒度分析。

Abstract: Practitioners increasingly rely on Large Language Models (LLMs) to evaluate
generative AI outputs through "LLM-as-a-Judge" approaches. However, these
methods produce holistic scores that obscure which specific elements influenced
the assessments. We propose functional fragmentation, a method that dissects
each output into key fragments and interprets the rhetoric functions that each
fragment serves relative to evaluation criteria -- surfacing the elements of
interest and revealing how they fulfill or hinder user goals. We instantiate
this approach in Evalet, an interactive system that visualizes fragment-level
functions across many outputs to support inspection, rating, and comparison of
evaluations. A user study (N=10) found that, while practitioners struggled to
validate holistic scores, our approach helped them identify 48% more evaluation
misalignments. This helped them calibrate trust in LLM evaluations and rely on
them to find more actionable issues in model outputs. Our work shifts LLM
evaluation from quantitative scores toward qualitative, fine-grained analysis
of model behavior.

</details>


### [375] [CareerPooler: AI-Powered Metaphorical Pool Simulation Improves Experience and Outcomes in Career Exploration](https://arxiv.org/abs/2509.11461)
*Ziyi Wang,Ziwen Zeng,Yuan Li,Zijian Ding*

Main category: cs.HC

TL;DR: 提出CareerPooler系统，以台球桌隐喻模拟职业发展，研究显示其较聊天机器人基线在多方面表现更优，表明可视化类比交互可提升生成系统体验。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式AI的职业指导系统多依赖线性聊天界面，忽略现实职业轨迹的非线性和复杂性，需要新的系统。

Method: 提出CareerPooler系统，采用台球桌隐喻进行空间和叙事交互，开展24名参与者的受试者内研究。

Result: CareerPooler在参与度、信息获取、满意度和职业清晰度方面显著优于聊天机器人基线，定性结果显示空间叙事交互有诸多好处。

Conclusion: 研究结果有助于AI辅助职业探索系统设计，可视化类比交互可让生成系统更具吸引力和满意度。

Abstract: Career exploration is uncertain, requiring decisions with limited information
and unpredictable outcomes. While generative AI offers new opportunities for
career guidance, most systems rely on linear chat interfaces that produce
overly comprehensive and idealized suggestions, overlooking the non-linear and
effortful nature of real-world trajectories. We present CareerPooler, a
generative AI-powered system that employs a pool-table metaphor to simulate
career development as a spatial and narrative interaction. Users strike balls
representing milestones, skills, and random events, where hints, collisions,
and rebounds embody decision-making under uncertainty. In a within-subjects
study with 24 participants, CareerPooler significantly improved engagement,
information gain, satisfaction, and career clarity compared to a chatbot
baseline. Qualitative findings show that spatial-narrative interaction fosters
experience-based learning, resilience through setbacks, and reduced
psychological burden. Our findings contribute to the design of AI-assisted
career exploration systems and more broadly suggest that visually grounded
analogical interactions can make generative systems engaging and satisfying.

</details>


### [376] [Designing and Evaluating a Conversational Agent for Early Detection of Alzheimer's Disease and Related Dementias](https://arxiv.org/abs/2509.11478)
*Andrew G. Breithaupt,Nayoung Choi,James D. Finch,Jeanne M. Powell,Arin L. Nelson,Oz A. Alon,Howard J. Rosen,Jinho D. Choi*

Main category: cs.HC

TL;DR: 本文设计基于大语言模型的语音交互对话代理用于获取与ADRD相关的叙述，并进行评估，初步表明该代理可作为痴呆评估的前端工具。


<details>
  <summary>Details</summary>
Motivation: 早期诊断ADRD至关重要，但多数诊断延迟，此前工作多关注筛查研究而非支持诊断过程，需要新方法辅助诊断。

Method: 设计基于LLMs的语音交互对话代理，通过对话分析、用户调查和与专家访谈对比进行临床验证。

Result: 代理检测到的症状与专家识别的症状吻合度高，用户认可代理的耐心和系统提问。

Conclusion: 对话代理可作为痴呆评估的结构化前端工具，同时强调了敏感医疗场景中的交互设计考量。

Abstract: Early detection of Alzheimer's disease and related dementias (ADRD) is
critical for timely intervention, yet most diagnoses are delayed until advanced
stages. While comprehensive patient narratives are essential for accurate
diagnosis, prior work has largely focused on screening studies that classify
cognitive status from interactions rather than supporting the diagnostic
process. We designed voice-interactive conversational agents, leveraging large
language models (LLMs), to elicit narratives relevant to ADRD from patients and
informants. We evaluated the agent with 30 adults with suspected ADRD through
conversation analysis (n=30), user surveys (n=19), and clinical validation
against blinded specialist interviews (n=24). Symptoms detected by the agent
aligned well with those identified by specialists across symptoms. Users
appreciated the agent's patience and systematic questioning, which supported
engagement and expression of complex, hard-to-describe experiences. This
preliminary work suggests conversational agents may serve as structured
front-end tools for dementia assessment, highlighting interaction design
considerations in sensitive healthcare contexts.

</details>


### [377] [Interaction-Driven Browsing: A Human-in-the-Loop Conceptual Framework Informed by Human Web Browsing for Browser-Using Agents](https://arxiv.org/abs/2509.12049)
*Hyeonggeun Yun,Jinkyu Jang*

Main category: cs.HC

TL;DR: 提出适用于浏览器使用代理（BUAs）的人在环概念框架，减少用户操作与认知负担，支持复杂浏览。


<details>
  <summary>Details</summary>
Motivation: 现有BUAs大多执行单条指令后终止，无法支持用户复杂、非线性的浏览需求。

Method: 基于人类网页浏览行为理论，构建以迭代循环为核心的人在环概念框架，区分探索和利用行动。

Result: 该框架能减少用户的体力和认知努力，保留传统浏览思维模式，支持用户获得满意结果。

Conclusion: 为BUAs贡献了一个理论驱动的概念框架。

Abstract: Although browser-using agents (BUAs) show promise for web tasks and
automation, most BUAs terminate after executing a single instruction, failing
to support users' complex, nonlinear browsing with ambiguous goals, iterative
decision-making, and changing contexts. We present a human-in-the-loop (HITL)
conceptual framework informed by theories of human web browsing behavior. The
framework centers on an iterative loop in which the BUA proactively proposes
next actions and the user steers the browsing process through feedback. It also
distinguishes between exploration and exploitation actions, enabling users to
control the breadth and depth of their browsing. Consequently, the framework
aims to reduce users' physical and cognitive effort while preserving users'
traditional browsing mental model and supporting users in achieving
satisfactory outcomes. We illustrate how the framework operates with
hypothetical use cases and discuss the shift from manual browsing to
interaction-driven browsing. We contribute a theoretically informed conceptual
framework for BUAs.

</details>


### [378] [Can LLMs Address Mental Health Questions? A Comparison with Human Therapists](https://arxiv.org/abs/2509.12102)
*Synthia Wang,Yuwei Cheng,Austin Song,Sarah Keedy,Marc Berman,Nick Feamster*

Main category: cs.HC

TL;DR: 研究对比治疗师与大语言模型（LLMs）对患者问题的回复，发现LLMs回复有优势但参与者仍偏好人类治疗师，凸显LLMs在心理健康领域利弊。


<details>
  <summary>Details</summary>
Motivation: 心理健康护理获取受限，促使使用数字工具和基于LLMs的对话代理，但质量和接受度不明。

Method: 对比治疗师和ChatGPT、Gemini、Llama对真实患者问题的回复，进行文本分析，并开展含150名用户和23名持牌治疗师的调查。

Result: LLMs回复更长、更易读、词汇更丰富、语气更积极，治疗师多使用第一人称；参与者认为LLMs回复更清晰、尊重和支持，但都更偏好人类治疗师。

Conclusion: LLMs在心理健康领域有潜力和局限，需设计平衡其沟通优势与信任、隐私和责任问题。

Abstract: Limited access to mental health care has motivated the use of digital tools
and conversational agents powered by large language models (LLMs), yet their
quality and reception remain unclear. We present a study comparing
therapist-written responses to those generated by ChatGPT, Gemini, and Llama
for real patient questions. Text analysis showed that LLMs produced longer,
more readable, and lexically richer responses with a more positive tone, while
therapist responses were more often written in the first person. In a survey
with 150 users and 23 licensed therapists, participants rated LLM responses as
clearer, more respectful, and more supportive than therapist-written answers.
Yet, both groups of participants expressed a stronger preference for human
therapist support. These findings highlight the promise and limitations of LLMs
in mental health, underscoring the need for designs that balance their
communicative strengths with concerns of trust, privacy, and accountability.

</details>


### [379] [Exploring Conversational Design Choices in LLMs for Pedagogical Purposes: Socratic and Narrative Approaches for Improving Instructor's Teaching Practice](https://arxiv.org/abs/2509.12107)
*Si Chen,Isabel R. Molnar,Peiyu Li,Adam Acunin,Ting Hua,Alex Ambrose,Nitesh V. Chawla,Ronald Metoyer*

Main category: cs.HC

TL;DR: 研究设计并评估了用于教学的大语言模型TeaPT，通过两种对话方式支持教师专业发展，不同版本受不同类型教师青睐，并给出设计启示。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用作学习工具，研究教师使用情况对推动AI在教育中的应用至关重要。

Method: 设计TeaPT，采用苏格拉底式和叙事式两种对话方式，对41位高等教育教师进行混合方法研究。

Result: 苏格拉底式版本引发更多参与度，叙事式版本在提供可操作指导方面更受青睐；经验少且对AI乐观的教师偏爱苏格拉底式，经验多且对AI谨慎的教师偏爱叙事式。

Conclusion: 为教学用大语言模型提供设计启示，表明自适应对话方式可支持不同类型教师，强调AI态度和经验影响交互与学习。

Abstract: Large language models (LLMs) typically generate direct answers, yet they are
increasingly used as learning tools. Studying instructors' usage is critical,
given their role in teaching and guiding AI adoption in education. We designed
and evaluated TeaPT, an LLM for pedagogical purposes that supports instructors'
professional development through two conversational approaches: a Socratic
approach that uses guided questioning to foster reflection, and a Narrative
approach that offers elaborated suggestions to extend externalized cognition.
In a mixed-method study with 41 higher-education instructors, the Socratic
version elicited greater engagement, while the Narrative version was preferred
for actionable guidance. Subgroup analyses further revealed that
less-experienced, AI-optimistic instructors favored the Socratic version,
whereas more-experienced, AI-cautious instructors preferred the Narrative
version. We contribute design implications for LLMs for pedagogical purposes,
showing how adaptive conversational approaches can support instructors with
varied profiles while highlighting how AI attitudes and experience shape
interaction and learning.

</details>


### [380] [Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM Inference](https://arxiv.org/abs/2509.12152)
*Synthia Wang,Sai Teja Peddinti,Nina Taft,Nick Feamster*

Main category: cs.HC

TL;DR: 研究用户对大语言模型文本推理隐私风险的估计与应对，发现用户难以预估推理风险，改写效果一般，强调推理感知设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 此前虽证明大语言模型文本推理存在隐私风险，但不清楚用户如何估计和应对。

Method: 对240名美国参与者进行调查，让其判断文本推理风险、报告担忧程度并改写文本，与ChatGPT和Rescriber的改写结果对比。

Result: 参与者难以预估推理风险，表现略好于随机猜测；用户改写有效率仅28%，优于Rescriber但不如ChatGPT；改写策略中释义最常用但效果最差，抽象和增加歧义更成功。

Conclusion: 强调在大语言模型交互中推理感知设计的重要性。

Abstract: Large Language Models (LLMs) such as ChatGPT can infer personal attributes
from seemingly innocuous text, raising privacy risks beyond memorized data
leakage. While prior work has demonstrated these risks, little is known about
how users estimate and respond. We conducted a survey with 240 U.S.
participants who judged text snippets for inference risks, reported concern
levels, and attempted rewrites to block inference. We compared their rewrites
with those generated by ChatGPT and Rescriber, a state-of-the-art sanitization
tool. Results show that participants struggled to anticipate inference,
performing a little better than chance. User rewrites were effective in just
28\% of cases - better than Rescriber but worse than ChatGPT. We examined our
participants' rewriting strategies, and observed that while paraphrasing was
the most common strategy it is also the least effective; instead abstraction
and adding ambiguity were more successful. Our work highlights the importance
of inference-aware design in LLM interactions.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [381] [COVID-BLUeS -- A Prospective Study on the Value of AI in Lung Ultrasound Analysis](https://arxiv.org/abs/2509.10556)
*Nina Wiedemann,Dianne de Korte-de Boer,Matthias Richter,Sjors van de Weijer,Charlotte Buhre,Franz A. M. Eggert,Sophie Aarnoudse,Lotte Grevendonk,Steffen Röber,Carlijn M. E. Remie,Wolfgang Buhre,Ronald Henry,Jannis Born*

Main category: q-bio.TO

TL;DR: 研究分析COVID - 19疑似患者肺部超声数据，应用AI模型检测肺部感染，评估AI在肺部超声评估中的价值，指出当前模型不足并发布数据集。


<details>
  <summary>Details</summary>
Motivation: 肺部超声在评估肺部病理方面重要，但AI模型因训练数据质量差，其实用性不明，需研究其在肺部超声评估中的应用。

Method: 前瞻性研究，收集63名COVID - 19疑似患者数据，按BLUE协议采集超声记录并手动标注，应用多个AI模型进行训练。

Result: 人类标注的COVID - 19阳性和阴性患者肺部感染严重程度无显著差异；图像AI模型零样本预测准确率65%，针对性训练后达79%，人类标注准确率至多65%；多模态模型优于仅图像模型。

Conclusion: 支持AI在肺部超声评估中的价值，但当前模型未达预期，原因包括数据集异质性、忽略视频信息和多模态模型研究不足，同时发布数据集以助未来研究。

Abstract: As a lightweight and non-invasive imaging technique, lung ultrasound (LUS)
has gained importance for assessing lung pathologies. The use of Artificial
intelligence (AI) in medical decision support systems is promising due to the
time- and expertise-intensive interpretation, however, due to the poor quality
of existing data used for training AI models, their usability for real-world
applications remains unclear. In a prospective study, we analyze data from 63
COVID-19 suspects (33 positive) collected at Maastricht University Medical
Centre. Ultrasound recordings at six body locations were acquired following the
BLUE protocol and manually labeled for severity of lung involvement. Several AI
models were applied and trained for detection and severity of pulmonary
infection. The severity of the lung infection, as assigned by human annotators
based on the LUS videos, is not significantly different between COVID-19
positive and negative patients (p = 0.89). Nevertheless, the predictions of
image-based AI models identify a COVID-19 infection with 65% accuracy when
applied zero-shot (i.e., trained on other datasets), and up to 79% with
targeted training, whereas the accuracy based on human annotations is at most
65%. Multi-modal models combining images and CBC improve significantly over
image-only models. Although our analysis generally supports the value of AI in
LUS assessment, the evaluated models fall short of the performance expected
from previous work. We find this is due to 1) the heterogeneity of LUS
datasets, limiting the generalization ability to new data, 2) the frame-based
processing of AI models ignoring video-level information, and 3) lack of work
on multi-modal models that can extract the most relevant information from
video-, image- and variable-based inputs. To aid future research, we publish
the dataset at: https://github.com/NinaWie/COVID-BLUES.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [382] [A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI](https://arxiv.org/abs/2509.10683)
*Felicia Liu,Jay J. Yoo,Farzad Khalvati*

Main category: cs.CV

TL;DR: 研究大语言模型（LLMs）在医学影像任务中的有效性，并与传统卷积神经网络（CNNs）比较，结果显示CNNs在神经胶质瘤分类和分割任务中均优于LLMs，LLMs当前不适合基于图像的任务。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在基于图像的医疗应用中的有效性，此前其在该领域的应用未被充分研究。

Method: 使用BraTS 2020多模态脑MRI数据集，评估通用视觉语言LLM（LLaMA 3.2 Instruct）微调前后的性能，并与自定义3D CNNs进行基准测试；在分割任务中实现中心点、边界框和多边形提取三种方法。

Result: 在神经胶质瘤分类中，CNN准确率达80%，LLM微调前准确率76%、微调后降至72%；在分割任务中，CNN能准确定位，LLM预测集中在图像中心，微调未显著提高空间准确性。

Conclusion: CNNs在两项任务中均优于LLMs，LLMs空间理解能力有限且微调效果不佳，当前不适合基于图像的任务，可能需更严格微调或替代训练策略。

Abstract: Large Language Models (LLMs) have shown strong performance in text-based
healthcare tasks. However, their utility in image-based applications remains
unexplored. We investigate the effectiveness of LLMs for medical imaging tasks,
specifically glioma classification and segmentation, and compare their
performance to that of traditional convolutional neural networks (CNNs). Using
the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a
general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after
fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma
classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and
balanced precision and recall. The general LLM reached 76% accuracy but
suffered from a specificity of only 18%, often misclassifying Low-Grade tumors.
Fine-tuning improved specificity to 55%, but overall performance declined
(e.g., accuracy dropped to 72%). For segmentation, three methods - center
point, bounding box, and polygon extraction, were implemented. CNNs accurately
localized gliomas, though small tumors were sometimes missed. In contrast, LLMs
consistently clustered predictions near the image center, with no distinction
of glioma size, location, or placement. Fine-tuning improved output formatting
but failed to meaningfully enhance spatial accuracy. The bounding polygon
method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in
both tasks. LLMs showed limited spatial understanding and minimal improvement
from fine-tuning, indicating that, in their current form, they are not
well-suited for image-based tasks. More rigorous fine-tuning or alternative
training strategies may be needed for LLMs to achieve better performance,
robustness, and utility in the medical space.

</details>


### [383] [The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge](https://arxiv.org/abs/2509.11071)
*Jinghan Peng,Jingwen Wang,Xing Yu,Dehui Du*

Main category: cs.CV

TL;DR: 报告介绍了CVPR 2024自动驾驶挑战赛中用视觉语言模型系统的方法，用DriveLM - nuScenes数据集，基于LLaVA模型微调，集成深度信息，用思维链推理，在验证集获第一。


<details>
  <summary>Details</summary>
Motivation: 参加CVPR 2024自动驾驶挑战赛的Driving with Language赛道。

Method: 用DriveLM - nuScenes数据集，基于LLaVA模型，用LoRA和DoRA方法微调，集成开源深度估计模型的深度信息，推理时用思维链推理方法。

Result: 在验证集排行榜上获得0.7799的高分，排名第一。

Conclusion: 所采用的综合方法有效，能在竞赛中取得优异成绩。

Abstract: This report outlines our approach using vision language model systems for the
Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We
have exclusively utilized the DriveLM-nuScenes dataset for training our models.
Our systems are built on the LLaVA models, which we enhanced through
fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated
depth information from open-source depth estimation models to enrich the
training and inference processes. For inference, particularly with
multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning
approach to improve the accuracy of the results. This comprehensive methodology
enabled us to achieve a top score of 0.7799 on the validation set leaderboard,
ranking 1st on the leaderboard.

</details>


### [384] [PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation](https://arxiv.org/abs/2509.11092)
*Zeyu Dong,Yuyang Yin,Yuqi Li,Eric Li,Hao-Xiang Guo,Yikai Wang*

Main category: cs.CV

TL;DR: 文章提出将全景视频生成视为透视视图的适应问题，用LoRA微调预训练视频扩散模型，高效生成高质量全景视频，效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 全景与传统透视视图投影有根本差异，现有全景视频生成方案效率低、效果不佳，受LoRA在风格迁移任务成功的启发。

Method: 将全景视频生成视为透视视图的适应问题，理论分析表明LoRA秩超过任务自由度时可有效建模投影间转换，用约1000个视频微调预训练视频扩散模型。

Result: 方法保持了正确的投影几何，在视觉质量、左右一致性和运动多样性上超越先前最先进方法。

Conclusion: 提出的方法能高效生成高质量全景视频。

Abstract: Generating high-quality 360{\deg} panoramic videos remains a significant
challenge due to the fundamental differences between panoramic and traditional
perspective-view projections. While perspective videos rely on a single
viewpoint with a limited field of view, panoramic content requires rendering
the full surrounding environment, making it difficult for standard video
generation models to adapt. Existing solutions often introduce complex
architectures or large-scale training, leading to inefficiency and suboptimal
results. Motivated by the success of Low-Rank Adaptation (LoRA) in style
transfer tasks, we propose treating panoramic video generation as an adaptation
problem from perspective views. Through theoretical analysis, we demonstrate
that LoRA can effectively model the transformation between these projections
when its rank exceeds the degrees of freedom in the task. Our approach
efficiently fine-tunes a pretrained video diffusion model using only
approximately 1,000 videos while achieving high-quality panoramic generation.
Experimental results demonstrate that our method maintains proper projection
geometry and surpasses previous state-of-the-art approaches in visual quality,
left-right consistency, and motion diversity.

</details>


### [385] [Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling](https://arxiv.org/abs/2310.06389)
*Huangjie Zheng,Zhendong Wang,Jianbo Yuan,Guanghan Ning,Pengcheng He,Quanzeng You,Hongxia Yang,Mingyuan Zhou*

Main category: cs.CV

TL;DR: 研究提出LEGO bricks用于扩散模型，可降低采样成本、生成高分辨率图像，提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型计算成本高，且缺乏高效、灵活的网络骨干用于迭代细化。

Method: 引入LEGO bricks，将局部特征富集和全局内容编排集成，可堆叠成测试时可重构的扩散骨干，选择性跳过砖块。

Result: LEGO bricks增强训练效率、加速收敛、支持可变分辨率图像生成，显著减少采样时间。

Conclusion: LEGO bricks是扩散模型的有价值改进。

Abstract: Diffusion models excel at generating photo-realistic images but come with
significant computational costs in both training and sampling. While various
techniques address these computational challenges, a less-explored issue is
designing an efficient and adaptable network backbone for iterative refinement.
Current options like U-Net and Vision Transformer often rely on
resource-intensive deep networks and lack the flexibility needed for generating
images at variable resolutions or with a smaller network than used in training.
This study introduces LEGO bricks, which seamlessly integrate Local-feature
Enrichment and Global-content Orchestration. These bricks can be stacked to
create a test-time reconfigurable diffusion backbone, allowing selective
skipping of bricks to reduce sampling costs and generate higher-resolution
images than the training data. LEGO bricks enrich local regions with an MLP and
transform them using a Transformer block while maintaining a consistent
full-resolution image across all bricks. Experimental results demonstrate that
LEGO bricks enhance training efficiency, expedite convergence, and facilitate
variable-resolution image generation while maintaining strong generative
performance. Moreover, LEGO significantly reduces sampling time compared to
other methods, establishing it as a valuable enhancement for diffusion models.
Our code and project page are available at
https://jegzheng.github.io/LEGODiffusion.

</details>


### [386] [StegOT: Trade-offs in Steganography via Optimal Transport](https://arxiv.org/abs/2509.11178)
*Chengde Lin,Xuezhu Gong,Shuxue Ding,Mingzhe Yang,Xijun Lu,Chengjun Mo*

Main category: cs.CV

TL;DR: 本文提出基于自编码器结合最优传输理论的隐写模型StegOT，解决现有模型模式崩溃问题，实验证明能平衡信息并提升图像质量，代码将开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN和VAE的隐写模型存在模式崩溃问题，导致隐写图像中封面和秘密图像信息失衡，影响后续提取。

Method: 提出基于自编码器的隐写模型StegOT，设计多通道最优传输（MCOT）模块将多峰特征分布转换为单峰以平衡信息。

Result: 实现了封面和秘密图像之间的信息权衡，提升了隐写和恢复图像的质量。

Conclusion: 所提的StegOT模型能有效解决现有模型的问题，平衡信息并提高图像质量。

Abstract: Image hiding is often referred to as steganography, which aims to hide a
secret image in a cover image of the same resolution. Many steganography models
are based on genera-tive adversarial networks (GANs) and variational
autoencoders (VAEs). However, most existing models suffer from mode collapse.
Mode collapse will lead to an information imbalance between the cover and
secret images in the stego image and further affect the subsequent extraction.
To address these challenges, this paper proposes StegOT, an autoencoder-based
steganography model incorporating optimal transport theory. We designed the
multiple channel optimal transport (MCOT) module to transform the feature
distribution, which exhibits multiple peaks, into a single peak to achieve the
trade-off of information. Experiments demonstrate that we not only achieve a
trade-off between the cover and secret images but also enhance the quality of
both the stego and recovery images. The source code will be released on
https://github.com/Rss1124/StegOT.

</details>


### [387] [Geometrically Constrained and Token-Based Probabilistic Spatial Transformers](https://arxiv.org/abs/2509.11218)
*Johann Schmidt,Sebastian Stober*

Main category: cs.CV

TL;DR: 本文提出基于空间变换网络（STNs）的改进方法，用于细粒度视觉分类，在蛾类分类基准测试中提高了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类对几何变化敏感，等变架构计算资源需求大且限制假设空间。

Method: 提出概率性、按组件扩展的STNs方法，将仿射变换分解，用共享定位编码器回归组件，用高斯变分后验建模，推理时进行基于采样的规范化，设计新的组件对齐损失。

Result: 在具有挑战性的蛾类分类基准测试中，该方法比其他STNs方法持续提高了鲁棒性。

Conclusion: 所提方法能有效提高细粒度视觉分类在几何变化下的鲁棒性。

Abstract: Fine-grained visual classification (FGVC) remains highly sensitive to
geometric variability, where objects appear under arbitrary orientations,
scales, and perspective distortions. While equivariant architectures address
this issue, they typically require substantial computational resources and
restrict the hypothesis space. We revisit Spatial Transformer Networks (STNs)
as a canonicalization tool for transformer-based vision pipelines, emphasizing
their flexibility, backbone-agnostic nature, and lack of architectural
constraints. We propose a probabilistic, component-wise extension that improves
robustness. Specifically, we decompose affine transformations into rotation,
scaling, and shearing, and regress each component under geometric constraints
using a shared localization encoder. To capture uncertainty, we model each
component with a Gaussian variational posterior and perform sampling-based
canonicalization during inference.A novel component-wise alignment loss
leverages augmentation parameters to guide spatial alignment. Experiments on
challenging moth classification benchmarks demonstrate that our method
consistently improves robustness compared to other STNs.

</details>


### [388] [MIS-LSTM: Multichannel Image-Sequence LSTM for Sleep Quality and Stress Prediction](https://arxiv.org/abs/2509.11232)
*Seongwan Park,Jieun Woo,Siheon Yang*

Main category: cs.CV

TL;DR: 提出MIS - LSTM混合框架用于从多模态生活日志数据进行日级睡眠质量和压力预测，引入UALRE集成方法提升性能，实验表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 从多模态生活日志数据进行日级睡眠质量和压力预测。

Method: 提出MIS - LSTM框架，将CNN编码器与LSTM序列模型结合，用CBAM融合两种模态；引入UALRE不确定性感知集成方法。

Result: 基础MIS - LSTM的Macro - F1为0.615，使用UALRE集成后提升到0.647，优于LSTM、1D - CNN和CNN基线模型。

Conclusion: 多通道成像优于堆叠垂直成像，4小时块粒度有优势，特定模态离散编码有效。

Abstract: This paper presents MIS-LSTM, a hybrid framework that joins CNN encoders with
an LSTM sequence model for sleep quality and stress prediction at the day level
from multimodal lifelog data. Continuous sensor streams are first partitioned
into N-hour blocks and rendered as multi-channel images, while sparse discrete
events are encoded with a dedicated 1D-CNN. A Convolutional Block Attention
Module fuses the two modalities into refined block embeddings, which an LSTM
then aggregates to capture long-range temporal dependencies. To further boost
robustness, we introduce UALRE, an uncertainty-aware ensemble that overrides
lowconfidence majority votes with high-confidence individual predictions.
Experiments on the 2025 ETRI Lifelog Challenge dataset show that Our base
MISLSTM achieves Macro-F1 0.615; with the UALRE ensemble, the score improves to
0.647, outperforming strong LSTM, 1D-CNN, and CNN baselines. Ablations confirm
(i) the superiority of multi-channel over stacked-vertical imaging, (ii) the
benefit of a 4-hour block granularity, and (iii) the efficacy of
modality-specific discrete encoding.

</details>


### [389] [Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses](https://arxiv.org/abs/2509.10620)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 提出基于SimCLR的3D脑结构MRI自监督基础模型，在多任务上表现优且公开代码和模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型难以跨任务和人群泛化，3D脑MRI基础模型在分辨率、范围或可访问性上有限。

Method: 提出基于SimCLR的自监督基础模型，在11个公开数据集的18759名患者（44958次扫描）上预训练，与MAE和两个监督基线模型在四个下游预测任务上对比。

Result: 微调后的SimCLR模型在所有任务上优于其他模型，用20%标记训练样本预测阿尔茨海默病时仍表现出色。

Conclusion: 贡献了适用于临床脑MRI分析的基础模型，并公开代码和训练模型。

Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly
acquired in clinical settings to monitor a wide range of neurological
conditions, including neurodegenerative disorders and stroke. While deep
learning models have shown promising results analyzing 3D MRI across a number
of brain imaging tasks, most are highly tailored for specific tasks with
limited labeled data, and are not able to generalize across tasks and/or
populations. The development of self-supervised learning (SSL) has enabled the
creation of large medical foundation models that leverage diverse, unlabeled
datasets ranging from healthy to diseased data, showing significant success in
2D medical imaging applications. However, even the very few foundation models
for 3D brain MRI that have been developed remain limited in resolution, scope,
or accessibility. In this work, we present a general, high-resolution
SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on
18,759 patients (44,958 scans) from 11 publicly available datasets spanning
diverse neurological diseases. We compare our model to Masked Autoencoders
(MAE), as well as two supervised baselines, on four diverse downstream
prediction tasks in both in-distribution and out-of-distribution settings. Our
fine-tuned SimCLR model outperforms all other models across all tasks. Notably,
our model still achieves superior performance when fine-tuned using only 20% of
labeled training samples for predicting Alzheimer's disease. We use publicly
available code and data, and release our trained model at
https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly
applicable and accessible foundation model for clinical brain MRI analysis.

</details>


### [390] [Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding](https://arxiv.org/abs/2509.11323)
*Jian Song,Wei Mei,Yunfeng Xu,Qiang Fu,Renke Kou,Lina Bu,Yucheng Long*

Main category: cs.CV

TL;DR: 本文提出Semantic - Independent KalmanNet (SIKNet)用于多目标跟踪的运动估计，在自建数据集上实验表明其优于传统卡尔曼滤波器和现有学习辅助滤波器。


<details>
  <summary>Details</summary>
Motivation: 传统基于线性恒速模型的卡尔曼滤波器在参数不匹配和目标非平稳运动时效果不佳，需更好的运动估计方法。

Method: 提出SIKNet方法，用Semantic - Independent Encoder (SIE)分两步编码状态向量；构建大规模半模拟数据集评估运动估计模块性能。

Result: 实验表明SIKNet优于传统KF，比现有学习辅助滤波器有更好的鲁棒性和准确性。

Conclusion: SIKNet是多目标跟踪运动估计的有效方法，代码已开源。

Abstract: Motion estimation is a crucial component in multi-object tracking (MOT).
  It predicts the trajectory of objects by analyzing the changes in their
positions in consecutive frames of images, reducing tracking failures and
identity switches.
  The Kalman filter (KF) based on the linear constant-velocity model is one of
the most commonly used methods in MOT.
  However, it may yield unsatisfactory results when KF's parameters are
mismatched and objects move in non-stationary.
  In this work, we utilize the learning-aided filter to handle the motion
estimation of MOT.
  In particular, we propose a novel method named Semantic-Independent KalmanNet
(SIKNet), which encodes the state vector (the input feature) using a
Semantic-Independent Encoder (SIE) by two steps.
  First, the SIE uses a 1D convolution with a kernel size of 1, which convolves
along the dimension of homogeneous-semantic elements across different state
vectors to encode independent semantic information.
  Then it employs a fully-connected layer and a nonlinear activation layer to
encode nonlinear and cross-dependency information between
heterogeneous-semantic elements.
  To independently evaluate the performance of the motion estimation module in
MOT, we constructed a large-scale semi-simulated dataset from several
open-source MOT datasets.
  Experimental results demonstrate that the proposed SIKNet outperforms the
traditional KF and achieves superior robustness and accuracy than existing
learning-aided filters.
  The code is available at (https://github.com/SongJgit/filternet and
https://github.com/SongJgit/TBDTracker).

</details>


### [391] [Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness](https://arxiv.org/abs/2509.11355)
*Robin Narsingh Ranabhat,Longwei Wang,Amit Kumar Patel,KC santosh*

Main category: cs.CV

TL;DR: 提出两种正则化策略提升CNN对图像损坏的鲁棒性，在CIFAR - 10 - C上验证有效。


<details>
  <summary>Details</summary>
Motivation: CNN在图像分类易受常见损坏影响，原因是依赖局部纹理而非全局形状，与人类感知不同，需提升其鲁棒性。

Method: 提出两种互补正则化策略，一是引入辅助损失确保原输入和低频滤波输入特征一致，二是结合监督对比学习构建特征空间。

Result: 在CIFAR - 10 - C基准测试中，两种方法提升了CNN对损坏的鲁棒性且不降低干净数据准确率。

Conclusion: 损失级正则化可有效引导CNN形成更具形状感知和弹性的表示。

Abstract: Convolutional Neural Networks (CNNs) excel at image classification but remain
vulnerable to common corruptions that humans handle with ease. A key reason for
this fragility is their reliance on local texture cues rather than global
object shapes -- a stark contrast to human perception. To address this, we
propose two complementary regularization strategies designed to encourage
shape-biased representations and enhance robustness. The first introduces an
auxiliary loss that enforces feature consistency between original and
low-frequency filtered inputs, discouraging dependence on high-frequency
textures. The second incorporates supervised contrastive learning to structure
the feature space around class-consistent, shape-relevant representations.
Evaluated on the CIFAR-10-C benchmark, both methods improve corruption
robustness without degrading clean accuracy. Our results suggest that
loss-level regularization can effectively steer CNNs toward more shape-aware,
resilient representations.

</details>


### [392] [Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation](https://arxiv.org/abs/2509.10919)
*Mohanad Albughdadi*

Main category: cs.CV

TL;DR: 研究提出仅250万参数的元数据感知MoE - MAE模型用于地球观测，虽小但性能与大模型竞争，是迈向未来地球观测基础模型的有效可扩展步骤。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测大规模基础模型计算成本高，限制可访问性和下游任务复用，需研究紧凑架构的通用模型。

Method: 提出元数据感知的MoE - MAE模型，结合稀疏专家路由和地理时间条件，在BigEarthNet - Landsat数据集上预训练，用线性探针评估冻结编码器嵌入。

Result: 模型虽小，但与大架构竞争，在BigEarthNet - Landsat和无明确元数据的EuroSAT - Landsat数据集上都表现良好。

Conclusion: 紧凑、元数据感知的MoE - MAEs是迈向未来地球观测基础模型的高效可扩展步骤。

Abstract: Recent advances in Earth Observation have focused on large-scale foundation
models. However, these models are computationally expensive, limiting their
accessibility and reuse for downstream tasks. In this work, we investigate
compact architectures as a practical pathway toward smaller general-purpose EO
models. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder
(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing
with geo-temporal conditioning, incorporating imagery alongside
latitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE
on the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen
encoder using linear probes. Despite its small size, the model competes with
much larger architectures, demonstrating that metadata-aware pretraining
improves transfer and label efficiency. To further assess generalization, we
evaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and
still observe competitive performance compared to models with hundreds of
millions of parameters. These results suggest that compact, metadata-aware
MoE-MAEs are an efficient and scalable step toward future EO foundation models.

</details>


### [393] [Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking](https://arxiv.org/abs/2509.11453)
*BaiChen Fan,Sifan Zhou,Jian Li,Shibo Zhao,Muqing Cao,Qin Wang*

Main category: cs.CV

TL;DR: 提出基于轨迹的3D单目标跟踪范式TrajTrack，在不增加点云输入成本下提升两帧跟踪器性能，实验表现优且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 现有两帧3D SOT方法缺长期时间上下文，序列方法计算成本高，需解决此困境。

Method: 提出基于轨迹的范式TrajTrack，先生成运动提案，再用隐式运动建模模块预测轨迹并修正提案。

Result: 在NuScenes基准测试中达新的最优性能，跟踪精度提升4.48%，运行速度56 FPS，且在不同基础跟踪器上泛化性强。

Conclusion: TrajTrack有效解决现有3D SOT方法的问题，性能和泛化性表现出色。

Abstract: LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics
and autonomous systems. Existing methods typically follow frame-wise motion
estimation or a sequence-based paradigm. However, the two-frame methods are
efficient but lack long-term temporal context, making them vulnerable in sparse
or occluded scenes, while sequence-based methods that process multiple point
clouds gain robustness at a significant computational cost. To resolve this
dilemma, we propose a novel trajectory-based paradigm and its instantiation,
TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame
tracker by implicitly learning motion continuity from historical bounding box
trajectories alone-without requiring additional, costly point cloud inputs. It
first generates a fast, explicit motion proposal and then uses an implicit
motion modeling module to predict the future trajectory, which in turn refines
and corrects the initial proposal. Extensive experiments on the large-scale
NuScenes benchmark show that TrajTrack achieves new state-of-the-art
performance, dramatically improving tracking precision by 4.48% over a strong
baseline while running at 56 FPS. Besides, we also demonstrate the strong
generalizability of TrajTrack across different base trackers. Video is
available at https://www.bilibili.com/video/BV1ahYgzmEWP.

</details>


### [394] [Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2509.11587)
*Haonan Shi,Yubin Wang,De Cheng,Lingfeng He,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出Hierarchical Identity Learning (HIL)框架解决USVI - ReID问题，实验表明性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于聚类的对比学习方法解决USVI - ReID时忽略图像细粒度差异，本文旨在解决该局限。

Method: 提出HIL框架，通过二次聚类为粗粒度聚类生成多个记忆；提出Multi - Center Contrastive Learning (MCCL)优化表示；设计Bidirectional Reverse Selection Transmission (BRST)机制建立跨模态对应关系。

Result: 在SYSU - MM01和RegDB数据集上的实验表明，该方法性能优于现有方法。

Conclusion: 所提方法有效解决了现有方法的局限，提升了USVI - ReID的性能。

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
learn modality-invariant image features from unlabeled cross-modal person
datasets by reducing the modality gap while minimizing reliance on costly
manual annotations. Existing methods typically address USVI-ReID using
cluster-based contrastive learning, which represents a person by a single
cluster center. However, they primarily focus on the commonality of images
within each cluster while neglecting the finer-grained differences among them.
To address the limitation, we propose a Hierarchical Identity Learning (HIL)
framework. Since each cluster may contain several smaller sub-clusters that
reflect fine-grained variations among images, we generate multiple memories for
each existing coarse-grained cluster via a secondary clustering. Additionally,
we propose Multi-Center Contrastive Learning (MCCL) to refine representations
for enhancing intra-modal clustering and minimizing cross-modal discrepancies.
To further improve cross-modal matching quality, we design a Bidirectional
Reverse Selection Transmission (BRST) mechanism, which establishes reliable
cross-modal correspondences by performing bidirectional matching of
pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB
datasets demonstrate that the proposed method outperforms existing approaches.
The source code is available at: https://github.com/haonanshi0125/HIL.

</details>


### [395] [WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild](https://arxiv.org/abs/2509.11114)
*Yuqiu Liu,Jialin Song,Manolis Savva,Wuyang Chen*

Main category: cs.CV

TL;DR: 提出从单张野外视频中提取和重建动态3D烟雾资产并集成交互模拟的管道，优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有流体重建依赖实验室环境，野外视频中的烟雾重建未充分探索。

Method: 针对野外视频烟雾重建的三个关键挑战设计技术，包括背景去除提取烟雾、初始化烟雾粒子和相机位姿、推断多视图视频。

Result: 高质量烟雾重建，在野外视频上平均PSNR提高2.22，能对流体动力学进行多样且逼真的编辑。

Conclusion: 提出的方法有效，优于先前重建和生成方法，还提供模型、数据和4D烟雾资产。

Abstract: We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from
a single in-the-wild video, and further integrate interactive simulation for
smoke design and editing. Recent developments in 3D vision have significantly
improved reconstructing and rendering fluid dynamics, supporting realistic and
temporally consistent view synthesis. However, current fluid reconstructions
rely heavily on carefully controlled clean lab environments, whereas real-world
videos captured in the wild are largely underexplored. We pinpoint three key
challenges of reconstructing smoke in real-world videos and design targeted
techniques, including smoke extraction with background removal, initialization
of smoke particles and camera poses, and inferring multi-view videos. Our
method not only outperforms previous reconstruction and generation methods with
high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but
also enables diverse and realistic editing of fluid dynamics by simulating our
smoke assets. We provide our models, data, and 4D smoke assets at
[https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke).

</details>


### [396] [DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition](https://arxiv.org/abs/2509.11661)
*Lifei Hao,Yue Cheng,Baoqi Huang,Bing Jia,Xuandong Zhao*

Main category: cs.CV

TL;DR: 提出用于细粒度脏餐具识别的少样本数据增强方案DTGen，能合成高质量样本，提升分类器性能，还阐述了部署策略并验证了生成式AI在少样本工业视觉中的价值。


<details>
  <summary>Details</summary>
Motivation: 现有智能餐具清洁方法受粗粒度分类和少样本数据稀缺限制，难以满足工业化需求。

Method: 提出基于生成扩散模型的少样本数据增强方案DTGen，通过LoRA实现高效领域专业化，用结构化提示生成多样脏图像，用基于CLIP的跨模态过滤确保数据质量。

Result: 在极少真实少样本条件下，DTGen能合成大量高质量样本，显著提升分类器性能，支持细粒度脏餐具识别。

Conclusion: DTGen验证了生成式AI在少样本工业视觉中的价值，为自动餐具清洁和食品安全监测提供了可行的部署路径。

Abstract: Intelligent tableware cleaning is a critical application in food safety and
smart homes, but existing methods are limited by coarse-grained classification
and scarcity of few-shot data, making it difficult to meet industrialization
requirements. We propose DTGen, a few-shot data augmentation scheme based on
generative diffusion models, specifically designed for fine-grained dirty
tableware recognition. DTGen achieves efficient domain specialization through
LoRA, generates diverse dirty images via structured prompts, and ensures data
quality through CLIP-based cross-modal filtering. Under extremely limited real
few-shot conditions, DTGen can synthesize virtually unlimited high-quality
samples, significantly improving classifier performance and supporting
fine-grained dirty tableware recognition. We further elaborate on lightweight
deployment strategies, promising to transfer DTGen's benefits to embedded
dishwashers and integrate with cleaning programs to intelligently regulate
energy consumption and detergent usage. Research results demonstrate that DTGen
not only validates the value of generative AI in few-shot industrial vision but
also provides a feasible deployment path for automated tableware cleaning and
food safety monitoring.

</details>


### [397] [MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs](https://arxiv.org/abs/2509.11662)
*Feilong Chen,Yijiang Liu,Yi Huang,Hao Wang,Miren Tian,Ya-Qi Yu,Minghui Liao,Jihao Wu*

Main category: cs.CV

TL;DR: 提出在昇腾NPUs上训练的多模态大语言模型MindVL，设计独特，训练框架适配，多技术提升性能，用少量数据达Qwen2.5 - VL相当表现。


<details>
  <summary>Details</summary>
Motivation: 开发在昇腾NPUs上训练的高性能多模态大语言模型，解决固定分辨率处理图像的问题。

Method: 采用原生分辨率视觉Transformer，开发适配昇腾NPUs的分布式多模态训练框架Mindspeed - MLLM，进行三阶段训练，采用多模态数据包装和混合并行技术，引入测试时分辨率搜索和模型权重平均。

Result: 使用约Qwen2.5 - VL十分之一训练数据，在通用多模态理解、文档/表格理解评估中表现相当，OCR评估中领先。

Conclusion: MindVL设计有效，适配昇腾NPUs训练，以较少数据达到较好性能，具有发展潜力。

Abstract: We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.
Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,
which enables it to process images at their original variable resolutions. This
design avoids the degradation caused by fixed-resolution tiling while
preserving fine-grained details and global layouts, which is crucial for
visually dense content such as complex charts and diagrams. To ensure the
smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a
distributed multimodal training framework tailored for Ascend NPUs. To maintain
training accuracy, we implement equivalent replacements for certain operators.
MindVL undergoes a three-phase training process, namely the warm-up phase,
multitask training phase, and supervised instruction tuning phase, to gradually
enhance its capabilities. This process starts with basic visual and multimodal
pre-training, followed by large-scale multiask trainging and instruction
tuning. We also adopt multimodal data packaging and hybrid parallelism
techniques, which significantly improve end-to-end training speed. To further
boost model performance, we specifically introduce test-time resolution search
and model weight averaging. Notably, despite using about 1/10 of the training
data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL
in evaluations of general multimodal understanding and document/table
comprehension. Beyond overall scores, MindVL also delivers leading performance
in OCR assessments.

</details>


### [398] [Beyond Instance Consistency: Investigating View Diversity in Self-supervised Learning](https://arxiv.org/abs/2509.11344)
*Huaiyuan Qin,Muli Yang,Siyuan Hu,Peng Hu,Yu Zhang,Chen Gong,Hongyuan Zhu*

Main category: cs.CV

TL;DR: 研究无实例一致性时自监督学习（SSL）效果，发现SSL仍能学习有意义表示，视图多样性有最佳范围，用EMD衡量视图互信息，结果具鲁棒性和适用性。


<details>
  <summary>Details</summary>
Motivation: 传统SSL依赖实例一致性范式，该假设对非标志性数据不成立，因此研究无实例一致性时SSL的有效性。

Method: 进行大量消融研究，采用地球移动距离（EMD）作为估计器来衡量视图间的互信息。

Result: 即使正样本对缺乏严格实例一致性，SSL仍能学习有意义表示；增加视图多样性可提升下游任务性能，但过度多样性会降低效果；中等EMD值与更好的SSL学习相关。

Conclusion: 研究结果在多种设置下得到验证，为未来SSL框架设计提供了见解，具有鲁棒性和适用性。

Abstract: Self-supervised learning (SSL) conventionally relies on the instance
consistency paradigm, assuming that different views of the same image can be
treated as positive pairs. However, this assumption breaks down for non-iconic
data, where different views may contain distinct objects or semantic
information. In this paper, we investigate the effectiveness of SSL when
instance consistency is not guaranteed. Through extensive ablation studies, we
demonstrate that SSL can still learn meaningful representations even when
positive pairs lack strict instance consistency. Furthermore, our analysis
further reveals that increasing view diversity, by enforcing zero overlapping
or using smaller crop scales, can enhance downstream performance on
classification and dense prediction tasks. However, excessive diversity is
found to reduce effectiveness, suggesting an optimal range for view diversity.
To quantify this, we adopt the Earth Mover's Distance (EMD) as an estimator to
measure mutual information between views, finding that moderate EMD values
correlate with improved SSL learning, providing insights for future SSL
framework design. We validate our findings across a range of settings,
highlighting their robustness and applicability on diverse data sources.

</details>


### [399] [Microsurgical Instrument Segmentation for Robot-Assisted Surgery](https://arxiv.org/abs/2509.11727)
*Tae Kyeong Jeong,Garam Kim,Juyoun Park*

Main category: cs.CV

TL;DR: 提出MISRA分割框架及专用数据集，MISRA性能佳，是计算机辅助和机器人显微手术可靠场景解析的有前景一步。


<details>
  <summary>Details</summary>
Motivation: 准确分割薄结构对显微手术场景理解至关重要，但因分辨率损失、低对比度和类别不平衡等问题仍具挑战。

Method: 提出MISRA分割框架，用亮度通道增强RGB输入，集成跳跃注意力保留细长特征，采用迭代反馈模块恢复连续性；引入有细粒度标注的专用显微手术数据集。

Result: MISRA性能有竞争力，平均类IoU比竞争方法提高5.37%，在器械接触和重叠处预测更稳定。

Conclusion: MISRA是计算机辅助和机器人显微手术可靠场景解析的有前景一步。

Abstract: Accurate segmentation of thin structures is critical for microsurgical scene
understanding but remains challenging due to resolution loss, low contrast, and
class imbalance. We propose Microsurgery Instrument Segmentation for Robotic
Assistance(MISRA), a segmentation framework that augments RGB input with
luminance channels, integrates skip attention to preserve elongated features,
and employs an Iterative Feedback Module(IFM) for continuity restoration across
multiple passes. In addition, we introduce a dedicated microsurgical dataset
with fine-grained annotations of surgical instruments including thin objects,
providing a benchmark for robust evaluation Dataset available at
https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstrate
that MISRA achieves competitive performance, improving the mean class IoU by
5.37% over competing methods, while delivering more stable predictions at
instrument contacts and overlaps. These results position MISRA as a promising
step toward reliable scene parsing for computer-assisted and robotic
microsurgery.

</details>


### [400] [Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference](https://arxiv.org/abs/2509.11731)
*Yudong Shen,Wenyu Wu,Jiali Mao,Yixiao Tong,Guoping Liu,Chaoya Wang*

Main category: cs.CV

TL;DR: 提出DGMap框架解决轨迹数据用于自动地图推理时因密度不均带来的问题，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 轨迹数据密度不均导致稀疏区域道路碎片化、密集区域路段冗余，现有方法难以处理。

Method: 提出DGMap双解码框架，有多尺度网格编码、掩码增强关键点提取和全局上下文感知关系预测模块，融合全局语义和局部几何特征。

Result: 在三个真实数据集上实验，DGMap在APLS指标上比现有方法高5%，在滴滴出行平台轨迹数据上性能提升显著。

Conclusion: DGMap能有效解决轨迹数据密度不均带来的问题，性能优于现有方法。

Abstract: Trajectory data has become a key resource for automated map in-ference due to
its low cost, broad coverage, and continuous availability. However, uneven
trajectory density often leads to frag-mented roads in sparse areas and
redundant segments in dense regions, posing significant challenges for existing
methods. To address these issues, we propose DGMap, a dual-decoding framework
with global context awareness, featuring Multi-scale Grid Encoding,
Mask-enhanced Keypoint Extraction, and Global Context-aware Relation
Prediction. By integrating global semantic context with local geometric
features, DGMap improves keypoint detection accuracy to reduce road
fragmentation in sparse-trajectory areas. Additionally, the Global
Context-aware Relation Prediction module suppresses false connections in
dense-trajectory regions by modeling long-range trajectory patterns.
Experimental results on three real-world datasets show that DGMap outperforms
state-of-the-art methods by 5% in APLS, with notable performance gains on
trajectory data from the Didi Chuxing platform

</details>


### [401] [SpecVLM: Fast Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.11815)
*Haiduo Huang,Fuwei Yang,Zhenhua Liu,Xuanwu Yin,Dong Li,Pengju Ren,Emad Barsoum*

Main category: cs.CV

TL;DR: 本文研究视觉语言模型（VLM）的推测解码，提出SpecVLM系统，实现2.5 - 2.9倍端到端加速且无损解码。


<details>
  <summary>Details</summary>
Motivation: 直接将推测解码应用于VLM面临计算和内存膨胀等系统约束，需要有效解决方案。

Method: 建立EagleVLM基线，用弹性视觉压缩器平衡计算与精度，提出在线对数蒸馏协议训练草稿模型。

Result: SpecVLM在5个epoch内实现2.5 - 2.9倍端到端加速，且保持目标模型输出分布。

Conclusion: SpecVLM能有效加速VLM推理，且在不同分辨率和任务难度下效果良好。

Abstract: Speculative decoding is a powerful way to accelerate autoregressive large
language models (LLMs), but directly porting it to vision-language models
(VLMs) faces unique systems constraints: the prefill stage is dominated by
visual tokens whose count scales with image resolution and video length,
inflating both compute and memory, especially the key-value (KV) cache. We
study speculative decoding for VLMs and introduce SpecVLM, a practical system
that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering
1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)
further accelerates VLM inference with an elastic visual compressor that
adaptively selects among pruning, pooling, convolution, and resampler
primitives to balance FLOPs/parameters and accuracy per input. To avoid costly
offline distillation corpora, we propose an online-logit distillation protocol
that trains the draft model with on-the-fly teacher logits and penultimate
features using a combined cross-entropy and Smooth L1 objective, eliminating
storage and preprocessing while remaining compute-efficient. This protocol
reveals a training-time scaling effect: longer online training monotonically
increases the draft model's average accepted length, improving speculative
efficiency. Empirically, SpecVLM achieves additional acceleration, culminating
in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,
consistently over resolutions and task difficulties, while preserving the
target model's output distribution (lossless decoding). Our code is available
at https://github.com/haiduo/SpecVLM.

</details>


### [402] [Disentanglement of Biological and Technical Factors via Latent Space Rotation in Clinical Imaging Improves Disease Pattern Discovery](https://arxiv.org/abs/2509.11436)
*Jeanny Pan,Philipp Seeböck,Christoph Fürböck,Svitlana Pochepnia,Jennifer Straub,Lucian Beer,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: 本文提出通过数据潜空间后验旋转主动学习域偏移的方法，以分离生物和技术因素，在临床数据上效果良好，有助于生物标志物发现。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据中存在因成像技术等导致的域偏移，阻碍数据表示学习和有生物学意义的聚类发现，需解决该问题。

Method: 引入通过数据潜空间后验旋转主动学习域偏移的方法，分离生物和技术因素。

Result: 在真实临床数据上，学习到的解纠缠表示形成稳定聚类，聚类一致性相比纠缠表示有显著提升，超过四种先进的协调方法，用于量化组织成分时可增强Cox生存预测。

Conclusion: 所提出的无标签框架有助于多中心常规成像数据中的生物标志物发现。

Abstract: Identifying new disease-related patterns in medical imaging data with the
help of machine learning enlarges the vocabulary of recognizable findings. This
supports diagnostic and prognostic assessment. However, image appearance varies
not only due to biological differences, but also due to imaging technology
linked to vendors, scanning- or re- construction parameters. The resulting
domain shifts impedes data representation learning strategies and the discovery
of biologically meaningful cluster appearances. To address these challenges, we
introduce an approach to actively learn the domain shift via post-hoc rotation
of the data latent space, enabling disentanglement of biological and technical
factors. Results on real-world heterogeneous clinical data showcase that the
learned disentangled representation leads to stable clusters representing
tissue-types across different acquisition settings. Cluster consistency is
improved by +19.01% (ARI), +16.85% (NMI), and +12.39% (Dice) compared to the
entangled representation, outperforming four state-of-the-art harmonization
methods. When using the clusters to quantify tissue composition on idiopathic
pulmonary fibrosis patients, the learned profiles enhance Cox survival
prediction. This indicates that the proposed label-free framework facilitates
biomarker discovery in multi-center routine imaging data. Code is available on
GitHub https://github.com/cirmuw/latent-space-rotation-disentanglement.

</details>


### [403] [Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network](https://arxiv.org/abs/2509.11838)
*Navid Hashemi,Samuel Sasaki,Diego Manzanas Lopez,Ipek Oguz,Meiyi Ma,Taylor T. Johnson*

Main category: cs.CV

TL;DR: 提出一种概率验证框架，结合采样可达性分析与共形推理，在大规模分割模型上验证效果好，还提供工具包。


<details>
  <summary>Details</summary>
Motivation: 现有概率验证方法难以适应现代分割任务的复杂性和高维性，保证过于保守。

Method: 结合采样可达性分析与共形推理，提出减少保守性的新策略。

Result: 在多个大规模分割模型上实验表明，相比SOTA能提供可靠安全保证并显著收紧边界。

Conclusion: 所提概率验证框架架构无关、可扩展到高维输出，能有效解决现有方法问题。

Abstract: Semantic segmentation networks (SSNs) play a critical role in domains such as
medical imaging, autonomous driving, and environmental monitoring, where safety
hinges on reliable model behavior under uncertainty. Yet, existing
probabilistic verification approaches struggle to scale with the complexity and
dimensionality of modern segmentation tasks, often yielding guarantees that are
too conservative to be practical. We introduce a probabilistic verification
framework that is both architecture-agnostic and scalable to high-dimensional
outputs. Our approach combines sampling-based reachability analysis with
conformal inference (CI) to deliver provable guarantees while avoiding the
excessive conservatism of prior methods. To counteract CI's limitations in
high-dimensional settings, we propose novel strategies that reduce conservatism
without compromising rigor. Empirical evaluation on large-scale segmentation
models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates
that our method provides reliable safety guarantees while substantially
tightening bounds compared to SOTA. We also provide a toolbox implementing this
technique, available on Github.

</details>


### [404] [Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision](https://arxiv.org/abs/2509.11476)
*Tianyao Sun,Dawei Xiang,Tianqi Ding,Xiang Fang,Yijiashun Qi,Zunduo Zhao*

Main category: cs.CV

TL;DR: 提出FusionNet用于红外与可见光图像融合，实验证明其在语义保留等方面表现好，为多模态图像融合提供通用可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 解决红外和可见光图像融合问题，整合不同光谱域的互补线索，增强关键区域。

Method: 提出FusionNet，引入模态感知注意力机制、像素级alpha混合模块，制定目标感知损失。

Result: 在M3FD数据集上实验表明，FusionNet生成的融合图像语义保留增强、感知质量高、可解释性强。

Conclusion: FusionNet为语义感知的多模态图像融合提供通用可扩展解决方案，利于下游任务。

Abstract: Infrared and visible image fusion (IVIF) is a fundamental task in multi-modal
perception that aims to integrate complementary structural and textural cues
from different spectral domains. In this paper, we propose FusionNet, a novel
end-to-end fusion framework that explicitly models inter-modality interaction
and enhances task-critical regions. FusionNet introduces a modality-aware
attention mechanism that dynamically adjusts the contribution of infrared and
visible features based on their discriminative capacity. To achieve
fine-grained, interpretable fusion, we further incorporate a pixel-wise alpha
blending module, which learns spatially-varying fusion weights in an adaptive
and content-aware manner. Moreover, we formulate a target-aware loss that
leverages weak ROI supervision to preserve semantic consistency in regions
containing important objects (e.g., pedestrians, vehicles). Experiments on the
public M3FD dataset demonstrate that FusionNet generates fused images with
enhanced semantic preservation, high perceptual quality, and clear
interpretability. Our framework provides a general and extensible solution for
semantic-aware multi-modal image fusion, with benefits for downstream tasks
such as object detection and scene understanding.

</details>


### [405] [Bridging Vision Language Models and Symbolic Grounding for Video Question Answering](https://arxiv.org/abs/2509.11862)
*Haodi Ma,Vyom Pathak,Daisy Zhe Wang*

Main category: cs.CV

TL;DR: 研究用符号场景图作为视频问答中间信号，提出SG - VLM框架，在多个基准测试中提升推理能力，但对强VLM提升有限。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在视频问答中依赖浅层关联，存在时间定位弱和可解释性有限的问题。

Method: 引入SG - VLM模块化框架，通过提示和视觉定位将冻结的VLM与场景图接地相结合。

Result: 在三个基准测试和多个VLM上，SG - VLM提升了因果和时间推理能力，优于先前基线，但对强VLM提升有限。

Conclusion: 符号接地有前景但存在当前局限，为未来混合VLM - 符号方法提供指导。

Abstract: Video Question Answering (VQA) requires models to reason over spatial,
temporal, and causal cues in videos. Recent vision language models (VLMs)
achieve strong results but often rely on shallow correlations, leading to weak
temporal grounding and limited interpretability. We study symbolic scene graphs
(SGs) as intermediate grounding signals for VQA. SGs provide structured
object-relation representations that complement VLMs holistic reasoning. We
introduce SG-VLM, a modular framework that integrates frozen VLMs with scene
graph grounding via prompting and visual localization. Across three benchmarks
(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM
improves causal and temporal reasoning and outperforms prior baselines, though
gains over strong VLMs are limited. These findings highlight both the promise
and current limitations of symbolic grounding, and offer guidance for future
hybrid VLM-symbolic approaches in video understanding.

</details>


### [406] [Integrating Prior Observations for Incremental 3D Scene Graph Prediction](https://arxiv.org/abs/2509.11895)
*Marian Renz,Felix Igelbrink,Martin Atzmueller*

Main category: cs.CV

TL;DR: 本文提出用于增量3D语义场景图预测的异质图模型，集成多模态信息，在数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有3D语义场景图方法多依赖传感器数据，未整合丰富语义信息，且假设能获取完整场景重建，限制了现实应用。

Method: 引入异质图模型，将多模态信息直接集成到消息传递过程，通过多层灵活整合全局和局部场景表示，无需专门模块和完整场景重建。

Result: 在3DSSG数据集上评估，表明融合多模态信息的GNN为复杂现实环境提供可扩展、可泛化的解决方案。

Conclusion: 提出的模型解决了现有方法的局限，具有较好的现实应用价值，代码将开源。

Abstract: 3D semantic scene graphs (3DSSG) provide compact structured representations
of environments by explicitly modeling objects, attributes, and relationships.
While 3DSSGs have shown promise in robotics and embodied AI, many existing
methods rely mainly on sensor data, not integrating further information from
semantically rich environments. Additionally, most methods assume access to
complete scene reconstructions, limiting their applicability in real-world,
incremental settings. This paper introduces a novel heterogeneous graph model
for incremental 3DSSG prediction that integrates additional, multi-modal
information, such as prior observations, directly into the message-passing
process. Utilizing multiple layers, the model flexibly incorporates global and
local scene representations without requiring specialized modules or full scene
reconstructions. We evaluate our approach on the 3DSSG dataset, showing that
GNNs enriched with multi-modal information such as semantic embeddings (e.g.,
CLIP) and prior observations offer a scalable and generalizable solution for
complex, real-world environments. The full source code of the presented
architecture will be made available at
https://github.com/m4renz/incremental-scene-graph-prediction.

</details>


### [407] [Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework](https://arxiv.org/abs/2509.11598)
*Siming Fu,Sijun Dong,Xiaoliang Meng*

Main category: cs.CV

TL;DR: 自监督学习受捷径学习阻碍泛化，提出HyGDL框架解决此问题。


<details>
  <summary>Details</summary>
Motivation: 自监督学习因捷径学习阻碍泛化，现有方法未触及根本学习机制。

Method: 提出HyGDL混合框架，遵循不变性预训练原则，在单一编码器上操作，通过向量投影定义风格。

Result: 未提及具体结果。

Conclusion: 未提及明确结论，但表明HyGDL有望从根本上解决自监督学习的泛化问题。

Abstract: Despite the remarkable success of Self-Supervised Learning (SSL), its
generalization is fundamentally hindered by Shortcut Learning, where models
exploit superficial features like texture instead of intrinsic structure. We
experimentally verify this flaw within the generative paradigm (e.g., MAE) and
argue it is a systemic issue also affecting discriminative methods, identifying
it as the root cause of their failure on unseen domains. While existing methods
often tackle this at a surface level by aligning or separating domain-specific
features, they fail to alter the underlying learning mechanism that fosters
shortcut dependency. To address this at its core, we propose HyGDL (Hybrid
Generative-Discriminative Learning Framework), a hybrid framework that achieves
explicit content-style disentanglement. Our approach is guided by the
Invariance Pre-training Principle: forcing a model to learn an invariant
essence by systematically varying a bias (e.g., style) at the input while
keeping the supervision signal constant. HyGDL operates on a single encoder and
analytically defines style as the component of a representation that is
orthogonal to its style-invariant content, derived via vector projection.

</details>


### [408] [A Controllable 3D Deepfake Generation Framework with Gaussian Splatting](https://arxiv.org/abs/2509.11624)
*Wending Liu,Siyun Liang,Huy H. Nguyen,Isao Echizen*

Main category: cs.CV

TL;DR: 提出基于3D高斯渲染的3D深度伪造生成框架，实现可控3D空间的换脸和重演，性能表现好。


<details>
  <summary>Details</summary>
Motivation: 传统2D深度伪造方法存在几何不一致和新视角泛化性差的问题，需改进。

Method: 结合参数化头部模型与动态高斯表示，分离头部和背景高斯，用预训练2D引导优化面部区域，引入修复模块。

Result: 在身份保留、姿态和表情一致性上与2D方法相当，多视图渲染质量和3D一致性远超2D方法。

Conclusion: 该方法弥合3D建模与深度伪造合成差距，揭示3D高斯渲染技术用于操纵攻击的威胁。

Abstract: We propose a novel 3D deepfake generation framework based on 3D Gaussian
Splatting that enables realistic, identity-preserving face swapping and
reenactment in a fully controllable 3D space. Compared to conventional 2D
deepfake approaches that suffer from geometric inconsistencies and limited
generalization to novel view, our method combines a parametric head model with
dynamic Gaussian representations to support multi-view consistent rendering,
precise expression control, and seamless background integration. To address
editing challenges in point-based representations, we explicitly separate the
head and background Gaussians and use pre-trained 2D guidance to optimize the
facial region across views. We further introduce a repair module to enhance
visual consistency under extreme poses and expressions. Experiments on
NeRSemble and additional evaluation videos demonstrate that our method achieves
comparable performance to state-of-the-art 2D approaches in identity
preservation, as well as pose and expression consistency, while significantly
outperforming them in multi-view rendering quality and 3D consistency. Our
approach bridges the gap between 3D modeling and deepfake synthesis, enabling
new directions for scene-aware, controllable, and immersive visual forgeries,
revealing the threat that emerging 3D Gaussian Splatting technique could be
used for manipulation attacks.

</details>


### [409] [Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing](https://arxiv.org/abs/2509.12040)
*Bingyu Li,Haocheng Dong,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 本文建立OVRSISBench基准评估现有模型局限性，提出RSKT - Seg框架，实验显示其性能优于基线且推理更快。


<details>
  <summary>Details</summary>
Motivation: OVRSIS因缺乏统一评估基准和领域差距研究不足，需填补这些空白。

Method: 建立OVRSISBench基准评估模型；提出RSKT - Seg框架，含RS - CMA、RS - Fusion和RS - Transfer模块。

Result: RSKT - Seg在基准上mIoU提高3.8、mACC提高5.9，推理速度快2倍。

Conclusion: RSKT - Seg在开放词汇遥感图像分割任务中表现良好，代码已开源。

Abstract: Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task
that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)
domain, remains underexplored due to the absence of a unified evaluation
benchmark and the domain gap between natural and RS images. To bridge these
gaps, we first establish a standardized OVRSIS benchmark (\textbf{OVRSISBench})
based on widely-used RS segmentation datasets, enabling consistent evaluation
across methods. Using this benchmark, we comprehensively evaluate several
representative OVS/OVRSIS models and reveal their limitations when directly
applied to remote sensing scenarios. Building on these insights, we propose
\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for
remote sensing. RSKT-Seg integrates three key components: (1) a
Multi-Directional Cost Map Aggregation (RS-CMA) module that captures
rotation-invariant visual cues by computing vision-language cosine similarities
across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)
transformer, which jointly models spatial and semantic dependencies with a
lightweight dimensionality reduction strategy; and (3) a Remote Sensing
Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and
facilitates domain adaptation via enhanced upsampling. Extensive experiments on
the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines
by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through
efficient aggregation. Our code is
\href{https://github.com/LiBingyu01/RSKT-Seg}{\textcolor{blue}{here}}.

</details>


### [410] [Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking](https://arxiv.org/abs/2509.12046)
*Zirui Zheng,Takashi Isobe,Tong Shen,Xu Jia,Jianbin Zhao,Xiaomin Li,Mengmeng Ge,Baolu Li,Qinghe Wang,Dong Li,Dong Zhou,Yunzhi Zhuge,Huchuan Lu,Emad Barsoum*

Main category: cs.CV

TL;DR: 提出SMARLI框架用于布局到图像生成，结合特殊掩码策略和后训练方案，实验证明其能有效整合布局信息，实现布局感知控制。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在布局条件图像生成中因布局条件稀疏和特征纠缠面临挑战，需要有效整合空间布局约束。

Method: 采用特殊结构化掩码策略用于注意力计算，结合基于Group Relative Policy Optimization (GRPO)的后训练方案和布局奖励函数。

Result: SMARLI能无缝整合布局、文本和图像令牌，不降低生成质量。

Conclusion: SMARLI能实现卓越的布局感知控制，同时保持自回归模型的结构简单性和生成效率。

Abstract: While autoregressive (AR) models have demonstrated remarkable success in
image generation, extending them to layout-conditioned generation remains
challenging due to the sparse nature of layout conditions and the risk of
feature entanglement. We present Structured Masking for AR-based
Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that
effectively integrates spatial layout constraints into AR-based image
generation. To equip AR model with layout control, a specially designed
structured masking strategy is applied to attention computation to govern the
interaction among the global prompt, layout, and image tokens. This design
prevents mis-association between different regions and their descriptions while
enabling sufficient injection of layout constraints into the generation
process. To further enhance generation quality and layout accuracy, we
incorporate Group Relative Policy Optimization (GRPO) based post-training
scheme with specially designed layout reward functions for next-set-based AR
models. Experimental results demonstrate that SMARLI is able to seamlessly
integrate layout tokens with text and image tokens without compromising
generation quality. It achieves superior layoutaware control while maintaining
the structural simplicity and generation efficiency of AR models.

</details>


### [411] [A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset](https://arxiv.org/abs/2509.12047)
*Haiyu Yang,Enhong Liu,Jennifer Sun,Sumit Sharma,Meike van Leerdam,Sebastien Franceschini,Puchun Niu,Miel Hostens*

Main category: cs.CV

TL;DR: 提出模块化管道利用计算机视觉技术自动化分析群居环境中动物行为，在猪监测中验证，表现优于现有方法，有适应其他场景潜力。


<details>
  <summary>Details</summary>
Motivation: 传统手动观察动物行为方法耗时、主观且可扩展性有限，需自动化分析方法。

Method: 采用模块化管道，结合零样本目标检测、运动感知跟踪与分割模型及视觉变压器进行特征提取以实现行为识别。

Result: 在爱丁堡猪行为视频数据集上，时间模型准确率达94.2%，比现有方法提高21.2个百分点，跟踪能力强，身份保留率93.3%，目标检测精度89.3%。

Conclusion: 模块化设计有适应其他场景潜力，开源实现为行为监测提供可扩展解决方案，有助于精准养猪和福利评估。

Abstract: Animal behavior analysis plays a crucial role in understanding animal
welfare, health status, and productivity in agricultural settings. However,
traditional manual observation methods are time-consuming, subjective, and
limited in scalability. We present a modular pipeline that leverages
open-sourced state-of-the-art computer vision techniques to automate animal
behavior analysis in a group housing environment. Our approach combines
state-of-the-art models for zero-shot object detection, motion-aware tracking
and segmentation, and advanced feature extraction using vision transformers for
robust behavior recognition. The pipeline addresses challenges including animal
occlusions and group housing scenarios as demonstrated in indoor pig
monitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset
for multiple behavioral tasks. Our temporal model achieved 94.2% overall
accuracy, representing a 21.2 percentage point improvement over existing
methods. The pipeline demonstrated robust tracking capabilities with 93.3%
identity preservation score and 89.3% object detection precision. The modular
design suggests potential for adaptation to other contexts, though further
validation across species would be required. The open-source implementation
provides a scalable solution for behavior monitoring, contributing to precision
pig farming and welfare assessment through automated, objective, and continuous
analysis.

</details>


### [412] [U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT](https://arxiv.org/abs/2509.12069)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: 提出用于多解剖CBCT分割的U - Mamba2网络，结合多种方法，实验表明其有效高效，在Toothfairy3挑战中获佳绩，代码开源。


<details>
  <summary>Details</summary>
Motivation: CBCT在牙科3D成像广泛应用，但解剖结构准确分割耗时且具挑战性，需高效准确的分割方法。

Method: 将Mamba2状态空间模型集成到U - Net架构，结合交互式点击提示与交叉注意力块，用自监督学习预训练，融入牙科领域知识。

Result: U - Mamba2在Toothfairy3挑战两项任务中均获前3，Task 1中mean Dice为0.792，HD95为93.19；Task 2中mean Dice为0.852，HD95为7.39。

Conclusion: U - Mamba2在牙科解剖结构CBCT分割中既有效又高效。

Abstract: Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in
dentistry, providing volumetric information about the anatomical structures of
jaws and teeth. Accurate segmentation of these anatomies is critical for
clinical applications such as diagnosis and surgical planning, but remains
time-consuming and challenging. In this paper, we present U-Mamba2, a new
neural network architecture designed for multi-anatomy CBCT segmentation in the
context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state
space models into the U-Net architecture, enforcing stronger structural
constraints for higher efficiency without compromising performance. In
addition, we integrate interactive click prompts with cross-attention blocks,
pre-train U-Mamba2 using self-supervised learning, and incorporate dental
domain knowledge into the model design to address key challenges of dental
anatomy segmentation in CBCT. Extensive experiments, including independent
tests, demonstrate that U-Mamba2 is both effective and efficient, securing top
3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2
achieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with
an average inference time of XX (TBC during the ODIN workshop). In Task 2,
U-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out
test data. The code is publicly available at
https://github.com/zhiqin1998/UMamba2.

</details>


### [413] [3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data](https://arxiv.org/abs/2509.12143)
*Nojod M. Alotaibi,Areej M. Alhothali,Manar S. Ali*

Main category: cs.CV

TL;DR: 本文提出统一管道，用ViTs提取sMRI数据的3D区域嵌入，GNN分类，在REST - meta - MDD数据集验证，基于图谱模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有MDD自动检测方法难以捕捉复杂大脑模式，需提升诊断准确性和实现早期干预。

Method: 开发统一管道，用ViTs提取3D区域嵌入，GNN分类；探索基于图谱和基于立方体两种区域定义策略，生成余弦相似度图建模区域关系引导分类。

Result: 用REST - meta - MDD数据集实验，最佳模型准确率78.98%等，基于图谱模型表现优于基于立方体方法。

Conclusion: 基于图谱模型表现更好，表明使用特定领域解剖先验对MDD检测很重要。

Abstract: Major depressive disorder (MDD) is a prevalent mental health condition that
negatively impacts both individual well-being and global public health.
Automated detection of MDD using structural magnetic resonance imaging (sMRI)
and deep learning (DL) methods holds increasing promise for improving
diagnostic accuracy and enabling early intervention. Most existing methods
employ either voxel-level features or handcrafted regional representations
built from predefined brain atlases, limiting their ability to capture complex
brain patterns. This paper develops a unified pipeline that utilizes Vision
Transformers (ViTs) for extracting 3D region embeddings from sMRI data and
Graph Neural Network (GNN) for classification. We explore two strategies for
defining regions: (1) an atlas-based approach using predefined structural and
functional brain atlases, and (2) an cube-based method by which ViTs are
trained directly to identify regions from uniformly extracted 3D patches.
Further, cosine similarity graphs are generated to model interregional
relationships, and guide GNN-based classification. Extensive experiments were
conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of
our model. With stratified 10-fold cross-validation, the best model obtained
78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and
78.98% F1-score. Further, atlas-based models consistently outperformed the
cube-based approach, highlighting the importance of using domain-specific
anatomical priors for MDD detection.

</details>


### [414] [Multi Anatomy X-Ray Foundation Model](https://arxiv.org/abs/2509.12146)
*Nishank Singla,Krisztian Koos,Farzin Haddadpour,Amin Honarmandi Shandiz,Lovish Chum,Xiaojian Xu,Qing Jin,Erhan Bas*

Main category: cs.CV

TL;DR: 提出多解剖X射线基础模型XR - 0，在多数据集和下游任务表现出色，证明解剖多样性和监督对构建医学视觉模型很关键。


<details>
  <summary>Details</summary>
Motivation: 现有AI基础模型局限于胸部解剖，难以在更广泛临床任务泛化。

Method: 在115万张涵盖不同解剖区域的私有图像数据集上进行自监督学习，在12个数据集和20个下游任务上评估。

Result: XR - 0在多数多解剖任务上达最优性能，在胸部特定基准测试中也具竞争力。

Conclusion: 解剖多样性和监督对构建强大通用医学视觉模型至关重要，为放射学AI系统发展铺路。

Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation
models are limited to chest anatomy and fail to generalize across broader
clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray
foundation model using self-supervised learning on a large, private dataset of
1.15 million images spanning diverse anatomical regions and evaluated across 12
datasets and 20 downstream tasks, including classification, retrieval,
segmentation, localization, visual grounding, and report generation. XR-0
achieves state-of-the-art performance on most multi-anatomy tasks and remains
competitive on chest-specific benchmarks. Our results demonstrate that
anatomical diversity and supervision are critical for building robust,
general-purpose medical vision models, paving the way for scalable and
adaptable AI systems in radiology.

</details>


### [415] [HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments](https://arxiv.org/abs/2509.12187)
*Johanna Karras,Yingwei Li,Yasamin Jafarian,Ira Kemelmacher-Shlizerman*

Main category: cs.CV

TL;DR: 提出HoloGarment方法解决野外服装新视角合成难题，结合真实视频和合成3D数据，实验表明其性能达最优。


<details>
  <summary>Details</summary>
Motivation: 现有野外服装新视角合成方法依赖合成3D训练数据，在真实服装上泛化性差。

Method: 提出HoloGarment方法，利用大规模真实视频数据和小规模合成3D数据优化共享服装嵌入空间，推理时构建服装“图谱”表示。

Result: HoloGarment在野外服装新视角合成上达到最优性能，能处理真实世界难题，保持多种特性。

Conclusion: HoloGarment方法有效解决野外服装新视角合成问题，性能优越。

Abstract: Novel view synthesis (NVS) of in-the-wild garments is a challenging task due
significant occlusions, complex human poses, and cloth deformations. Prior
methods rely on synthetic 3D training data consisting of mostly unoccluded and
static objects, leading to poor generalization on real-world clothing. In this
paper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3
images or a continuous video of a person wearing a garment and generates
360{\deg} novel views of the garment in a canonical pose. Our key insight is to
bridge the domain gap between real and synthetic data with a novel implicit
training paradigm leveraging a combination of large-scale real video data and
small-scale synthetic 3D data to optimize a shared garment embedding space.
During inference, the shared embedding space further enables dynamic
video-to-360{\deg} NVS through the construction of a garment "atlas"
representation by finetuning a garment embedding on a specific real-world
video. The atlas captures garment-specific geometry and texture across all
viewpoints, independent of body pose or motion. Extensive experiments show that
HoloGarment achieves state-of-the-art performance on NVS of in-the-wild
garments from images and videos. Notably, our method robustly handles
challenging real-world artifacts -- such as wrinkling, pose variation, and
occlusion -- while maintaining photorealism, view consistency, fine texture
details, and accurate geometry. Visit our project page for additional results:
https://johannakarras.github.io/HoloGarment

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [416] [A Tree Clock Data Structure for Causal Orderings in Concurrent Executions](https://arxiv.org/abs/2201.06325)
*Umang Mathur,Andreas Pavlogiannis,Hünkar Can Tunç,Mahesh Viswanathan*

Main category: cs.LO

TL;DR: 本文提出树时钟替代向量时钟计算程序执行因果顺序，具有性能优势，实验表明其计算速度更快，有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 向量时钟的两个基本操作在线程数较多时是计算瓶颈，需新的数据结构。

Method: 引入树时钟数据结构来替代向量时钟计算因果顺序。

Result: 树时钟计算经典happens - before偏序最优，可计算其他偏序，实验显示替换后计算速度平均提升2.02 - 2.97倍。

Conclusion: 树时钟有潜力成为并发分析中广泛应用的标准数据结构。

Abstract: Dynamic techniques are a scalable and effective way to analyze concurrent
programs. Instead of analyzing all behaviors of a program, these techniques
detect errors by focusing on a single program execution. Often a crucial step
in these techniques is to define a causal ordering between events in the
execution, which is then computed using vector clocks, a simple data structure
that stores logical times of threads. The two basic operations of vector
clocks, namely join and copy, require $\Theta(k)$ time, where $k$ is the number
of threads. Thus they are a computational bottleneck when $k$ is large.
  In this work, we introduce tree clocks, a new data structure that replaces
vector clocks for computing causal orderings in program executions. Joining and
copying tree clocks takes time that is roughly proportional to the number of
entries being modified, and hence the two operations do not suffer the a-priori
$\Theta(k)$ cost per application. We show that when used to compute the classic
happens-before (HB) partial order, tree clocks are optimal, in the sense that
no other data structure can lead to smaller asymptotic running time. Moreover,
we demonstrate that tree clocks can be used to compute other partial orders,
such as schedulable-happens-before (SHB) and the standard Mazurkiewicz (MAZ)
partial order, and thus are a versatile data structure. Our experiments show
that just by replacing vector clocks with tree clocks, the computation becomes
from $2.02 \times$ faster (MAZ) to $2.66 \times$ (SHB) and $2.97 \times$ (HB)
on average per benchmark. These results illustrate that tree clocks have the
potential to become a standard data structure with wide applications in
concurrent analyses.

</details>


### [417] [Proceedings 9th edition of Working Formal Methods Symposium](https://arxiv.org/abs/2509.11877)
*Andrei Arusoaie,Horaţiu Cheval,Radu Iosif*

Main category: cs.LO

TL;DR: 本文介绍了2025年9月17 - 19日在罗马尼亚雅西的亚历山德鲁·约安·库扎大学举行的第9届形式化方法工作研讨会的会议记录。


<details>
  <summary>Details</summary>
Motivation: 记录和分享研讨会的相关内容

Method: 无明确提及

Result: 呈现研讨会的会议记录

Conclusion: 无明确提及

Abstract: This volume contains the proceedings of the 9th Working Formal Methods
Symposium, which was held at the Alexandru Ioan Cuza University, Ia\c{s}i,
Romania on September 17-19, 2025.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [418] [The Horton-Strahler number of butterfly trees](https://arxiv.org/abs/2509.11384)
*John Peca-Medlin*

Main category: math.PR

TL;DR: 研究蝴蝶树中Horton - Strahler数（HS），对简单蝴蝶树获分布结果，拓展到有偏构造建立泛函极限定理，一般蝴蝶树有经验采样结论。


<details>
  <summary>Details</summary>
Motivation: 经典随机树中HS的波动行为难以分析，在蝴蝶树背景下研究其HS。

Method: 对简单蝴蝶树，利用递归粘贴结构将HS建模为有限状态马尔可夫过程的加性泛函；拓展到有偏构造用分析和概率工具。

Result: 对简单蝴蝶树得到包括大数定律和带显式方差增长的中心极限定理等分布结果；一般蝴蝶树经验采样显示HS分布支持集更窄且集中在上界附近。

Conclusion: 为非平凡随机树模型中的HS给出首个真正的高斯极限定律，拓展到有偏构造建立泛函极限定理，一般蝴蝶树有分布特征。

Abstract: The Horton-Strahler number (HS) is a measure of branching complexity of
rooted trees, introduced in hydrology and later studied in parallel computing
under the name register function. While its order of growth is well understood
for classical random trees, fluctuation behavior has largely resisted analysis.
In this work we investigate the HS in the setting of butterfly trees -- binary
trees constructed from butterfly permutations, a rich class of separable
permutations with origins in numerical linear algebra and parallel
architectures. For the subclass of simple butterfly trees, we exploit their
recursive gluing structure to model the HS as an additive functional of a
finite-state Markov process. This framework yields sharp distributional
results, including a law of large numbers and a Central Limit Theorem with
explicit variance growth, providing what appears to be the first genuine
Gaussian limit law for the HS in a nontrivial random tree model. Extending to
biased constructions, we further establish functional limit theorems via
analytic and probabilistic tools. For general butterfly trees, while exact
analysis remains open, empirical sampling shows that the HS distribution is
confined to a narrower support than in classical models, and appears to
concentrate tightly near the upper bound $\lfloor \log_4 N\rfloor$.

</details>


### [419] [Contractive kinetic Langevin samplers beyond global Lipschitz continuity](https://arxiv.org/abs/2509.12031)
*Iosif Lytras,Panagiotis Mertikopoulos*

Main category: math.PR

TL;DR: 研究在动力学Langevin算法下从具有（可能）超线性梯度增长的对数凹分布中采样的问题，提出两种离散化方法并给出非渐近界。


<details>
  <summary>Details</summary>
Motivation: 解决在动力学Langevin算法下从具有（可能）超线性梯度增长的对数凹分布中采样的问题。

Method: 使用精心设计的驯服方案，提出两种动力学Langevin随机微分方程的离散化方法。

Result: 所提出的两种离散化方法具有收缩性且满足对数Sobolev不等式，并建立了算法达到的分布与目标测度之间2 - Wasserstein距离的一系列非渐近界。

Conclusion: 提出的离散化方法在从特定对数凹分布采样问题上有效。

Abstract: In this paper, we examine the problem of sampling from log-concave
distributions with (possibly) superlinear gradient growth under kinetic
(underdamped) Langevin algorithms. Using a carefully tailored taming scheme, we
propose two novel discretizations of the kinetic Langevin SDE, and we show that
they are both contractive and satisfy a log-Sobolev inequality. Building on
this, we establish a series of non-asymptotic bounds in $2$-Wasserstein
distance between the law reached by each algorithm and the underlying target
measure.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [420] [FastTrack: GPU-Accelerated Tracking for Visual SLAM](https://arxiv.org/abs/2509.10757)
*Kimia Khabiri,Parsa Hosseininejad,Shishir Gopinath,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: 提出利用GPU加速视觉惯性SLAM系统跟踪模块的方法，在桌面和Jetson Xavier NX板上提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 视觉惯性SLAM系统跟踪模块需及时处理每帧数据，避免定位不佳或跟踪丢失，因此要提升其性能。

Method: 利用GPU计算能力加速跟踪中耗时的组件（如立体特征匹配和局部地图跟踪），并在ORB - SLAM3跟踪过程中使用CUDA实现该设计。

Result: 在桌面和Jetson Xavier NX板的立体惯性模式下，使用EuRoC和TUM - VI数据集评估，跟踪性能整体提升达2.8倍。

Conclusion: 所提出的利用GPU加速跟踪模块组件的方法能有效提升视觉惯性SLAM系统的跟踪性能。

Abstract: The tracking module of a visual-inertial SLAM system processes incoming image
frames and IMU data to estimate the position of the frame in relation to the
map. It is important for the tracking to complete in a timely manner for each
frame to avoid poor localization or tracking loss. We therefore present a new
approach which leverages GPU computing power to accelerate time-consuming
components of tracking in order to improve its performance. These components
include stereo feature matching and local map tracking. We implement our design
inside the ORB-SLAM3 tracking process using CUDA. Our evaluation demonstrates
an overall improvement in tracking performance of up to 2.8x on a desktop and
Jetson Xavier NX board in stereo-inertial mode, using the well-known SLAM
datasets EuRoC and TUM-VI.

</details>


### [421] [Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2509.10570)
*Wei Dai,Shengen Wu,Wei Wu,Zhenhao Wang,Sisuo Lyu,Haicheng Liao,Limin Yu,Weiping Ding,Runwei Guan,Yutao Yue*

Main category: cs.RO

TL;DR: 文章系统回顾大基础模型（LFMs）在轨迹预测方面的进展，介绍核心方法、任务、评估指标等，讨论挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在轨迹预测中有缺乏可解释性等局限，LFMs兴起改变研究范式，故进行系统回顾。

Method: 介绍轨迹 - 语言映射、多模态融合和基于约束的推理三种核心方法。

Result: 通过整合语言和场景语义，LFMs促进可解释的上下文推理，提升复杂环境中预测的安全性和泛化能力。

Conclusion: 指出计算延迟、数据稀缺等挑战，提出低延迟推理等未来研究方向。

Abstract: Trajectory prediction serves as a critical functionality in autonomous
driving, enabling the anticipation of future motion paths for traffic
participants such as vehicles and pedestrians, which is essential for driving
safety. Although conventional deep learning methods have improved accuracy,
they remain hindered by inherent limitations, including lack of
interpretability, heavy reliance on large-scale annotated data, and weak
generalization in long-tail scenarios. The rise of Large Foundation Models
(LFMs) is transforming the research paradigm of trajectory prediction. This
survey offers a systematic review of recent advances in LFMs, particularly
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for
trajectory prediction. By integrating linguistic and scene semantics, LFMs
facilitate interpretable contextual reasoning, significantly enhancing
prediction safety and generalization in complex environments. The article
highlights three core methodologies: trajectory-language mapping, multimodal
fusion, and constraint-based reasoning. It covers prediction tasks for both
vehicles and pedestrians, evaluation metrics, and dataset analyses. Key
challenges such as computational latency, data scarcity, and real-world
robustness are discussed, along with future research directions including
low-latency inference, causality-aware modeling, and motion foundation models.

</details>


### [422] [ViSTR-GP: Online Cyberattack Detection via Vision-to-State Tensor Regression and Gaussian Processes in Automated Robotic Operations](https://arxiv.org/abs/2509.10948)
*Navid Aftabi,Philip Samaha,Jin Ma,Long Cheng,Ramy Harik,Dan Li*

Main category: cs.RO

TL;DR: 本文提出ViSTR - GP在线检测框架，利用外部摄像头视觉估计交叉检查编码器测量值以检测机器人制造过程中的数据完整性攻击，在真实测试平台验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 智能工厂面临网络安全风险，数据完整性攻击难以用现有方法检测，需利用现有边信道检测此类攻击。

Method: 开发ViSTR - GP框架，用一次性交互式分割初始化SAM - Track生成掩码，低秩张量回归代理映射掩码到测量值，矩阵变量高斯过程建模名义残差，基于预测分布导出检验统计量作为在线检测器。

Result: 在真实测试平台验证，框架能准确恢复关节角度，比所有基线更早检测到数据完整性攻击，在细微攻击中优势明显。

Conclusion: 工厂可通过添加独立物理通道绕过控制器权限检测数据完整性攻击，无需复杂仪器。

Abstract: Industrial robotic systems are central to automating smart manufacturing
operations. Connected and automated factories face growing cybersecurity risks
that can potentially cause interruptions and damages to physical operations.
Among these attacks, data-integrity attacks often involve sophisticated
exploitation of vulnerabilities that enable an attacker to access and
manipulate the operational data and are hence difficult to detect with only
existing intrusion detection or model-based detection. This paper addresses the
challenges in utilizing existing side-channels to detect data-integrity attacks
in robotic manufacturing processes by developing an online detection framework,
ViSTR-GP, that cross-checks encoder-reported measurements against a
vision-based estimate from an overhead camera outside the controller's
authority. In this framework, a one-time interactive segmentation initializes
SAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate
maps each mask to measurements, while a matrix-variate Gaussian process models
nominal residuals, capturing temporal structure and cross-joint correlations. A
frame-wise test statistic derived from the predictive distribution provides an
online detector with interpretable thresholds. We validate the framework on a
real-world robotic testbed with synchronized video frame and encoder data,
collecting multiple nominal cycles and constructing replay attack scenarios
with graded end-effector deviations. Results on the testbed indicate that the
proposed framework recovers joint angles accurately and detects data-integrity
attacks earlier with more frequent alarms than all baselines. These
improvements are most evident in the most subtle attacks. These results show
that plants can detect data-integrity attacks by adding an independent physical
channel, bypassing the controller's authority, without needing complex
instrumentation.

</details>


### [423] [RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations](https://arxiv.org/abs/2509.11149)
*Mintae Kim,Jiaze Cai,Koushil Sreenath*

Main category: cs.RO

TL;DR: 提出RoVerFly统一学习控制框架，用于标准四旋翼和吊载系统轨迹跟踪，有强泛化性且无需重新调参。


<details>
  <summary>Details</summary>
Motivation: 四旋翼和吊载系统轨迹跟踪控制因非线性和欠驱动特性具有挑战性，传统基于模型方法需大量调参且难适应配置变化。

Method: 提出RoVerFly统一学习控制框架，采用强化学习策略，通过任务和领域随机化训练。

Result: 控制器对干扰和动力学变化有鲁棒性，在多种负载设置下实现零样本强泛化，无需切换或重新调参。

Conclusion: RoVerFly是一种有效的控制框架，可用于标准四旋翼和吊载系统的轨迹跟踪控制。

Abstract: Designing robust controllers for precise, arbitrary trajectory tracking with
quadrotors is challenging due to nonlinear dynamics and underactuation, and
becomes harder with flexible cable-suspended payloads that introduce extra
degrees of freedom and hybridness. Classical model-based methods offer
stability guarantees but require extensive tuning and often do not adapt when
the configuration changes, such as when a payload is added or removed, or when
the payload mass or cable length varies. We present RoVerFly, a unified
learning-based control framework in which a reinforcement learning (RL) policy
serves as a robust and versatile tracking controller for standard quadrotors
and for cable-suspended payload systems across a range of configurations.
Trained with task and domain randomization, the controller is resilient to
disturbances and varying dynamics. It achieves strong zero-shot generalization
across payload settings, including no payload as well as varying mass and cable
length, without controller switching or re-tuning, while retaining the
interpretability and structure of a feedback tracking controller. Code and
supplementary materials are available at
https://github.com/mintaeshkim/roverfly

</details>


### [424] [DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2509.11197)
*Yunheng Wang,Yuetong Fang,Taowen Wang,Yixiao Feng,Yawen Tan,Shuning Zhang,Peiran Liu,Yiding Ji,Renjing Xu*

Main category: cs.RO

TL;DR: 提出DreamNav方法解决现有零样本VLN-CE方法问题，在测试中创零样本最优。


<details>
  <summary>Details</summary>
Motivation: 现有零样本VLN方法依赖高成本感知、被动场景理解，存在部署成本高、动作语义不一致和规划短视问题。

Method: 提出EgoView Corrector降低感知成本；用Trajectory Predictor进行全局轨迹级规划；用Imagination Predictor赋予主动思考能力。

Result: DreamNav在VLN - CE和真实世界测试中创零样本最优，在SR和SPL指标上分别最多超基线7.49%和18.15%。

Conclusion: DreamNav是首个仅用第一人称输入统一轨迹级规划和主动想象的零样本VLN方法。

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
links language instructions to perception and control in the real world, is a
core capability of embodied robots. Recently, large-scale pretrained foundation
models have been leveraged as shared priors for perception, reasoning, and
action, enabling zero-shot VLN without task-specific training. However,
existing zero-shot VLN methods depend on costly perception and passive scene
understanding, collapsing control to point-level choices. As a result, they are
expensive to deploy, misaligned in action semantics, and short-sighted in
planning. To address these issues, we present DreamNav that focuses on the
following three aspects: (1) for reducing sensory cost, our EgoView Corrector
aligns viewpoints and stabilizes egocentric perception; (2) instead of
point-level actions, our Trajectory Predictor favors global trajectory-level
planning to better align with instruction semantics; and (3) to enable
anticipatory and long-horizon planning, we propose an Imagination Predictor to
endow the agent with proactive thinking capability. On VLN-CE and real-world
tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the
strongest egocentric baseline with extra information by up to 7.49\% and
18.15\% in terms of SR and SPL metrics. To our knowledge, this is the first
zero-shot VLN method to unify trajectory-level planning and active imagination
while using only egocentric inputs.

</details>


### [425] [MEMBOT: Memory-Based Robot in Intermittent POMDP](https://arxiv.org/abs/2509.11225)
*Youzhi Liang,Eyan Noronha*

Main category: cs.RO

TL;DR: 提出MEMBOT架构应对机器人控制任务中的间歇性部分可观测问题，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实环境中机器人系统常面临部分和间歇性可观测问题，传统强化学习方法难以应对。

Method: 采用两阶段训练过程，离线多任务学习预训练学习任务无关的潜在信念编码器，再用行为克隆微调特定任务策略，信念编码器用状态空间模型和LSTM实现。

Result: 在10个机器人操作基准任务上，MEMBOT始终优于无记忆和简单循环基线方法，在50%观测可用性下保持最高80%的峰值性能。

Conclusion: 显式信念建模对实现现实部分可观测机器人系统的鲁棒、可迁移和数据高效策略有效。

Abstract: Robotic systems deployed in real-world environments often operate under
conditions of partial and often intermittent observability, where sensor inputs
may be noisy, occluded, or entirely unavailable due to failures or
environmental constraints. Traditional reinforcement learning (RL) approaches
that assume full state observability are ill-equipped for such challenges. In
this work, we introduce MEMBOT, a modular memory-based architecture designed to
address intermittent partial observability in robotic control tasks. MEMBOT
decouples belief inference from policy learning through a two-phase training
process: an offline multi-task learning pretraining stage that learns a robust
task-agnostic latent belief encoder using a reconstruction losses, followed by
fine-tuning of task-specific policies using behavior cloning. The belief
encoder, implemented as a state-space model (SSM) and a LSTM, integrates
temporal sequences of observations and actions to infer latent state
representations that persist even when observations are dropped. We train and
evaluate MEMBOT on 10 robotic manipulation benchmark tasks from MetaWorld and
Robomimic under varying rates of observation dropout. Results show that MEMBOT
consistently outperforms both memoryless and naively recurrent baselines,
maintaining up to 80% of peak performance under 50% observation availability.
These findings highlight the effectiveness of explicit belief modeling in
achieving robust, transferable, and data-efficient policies for real-world
partially observable robotic systems.

</details>


### [426] [Embodied Intelligence in Disassembly: Multimodal Perception Cross-validation and Continual Learning in Neuro-Symbolic TAMP](https://arxiv.org/abs/2509.11270)
*Ziwen He,Zhigang Wang,Yanlong Peng,Pengxu Chang,Hong Yang,Ming Chen*

Main category: cs.RO

TL;DR: 本文提出基于神经符号任务与运动规划的持续学习框架，用于新能源汽车动力电池拆解，提高了动态场景任务成功率并减少感知误判，为具身智能提供新范式。


<details>
  <summary>Details</summary>
Motivation: 新能源汽车产业发展，动力电池高效拆解回收成挑战，当前非结构化拆解场景中环境动态性限制机器人感知鲁棒性，阻碍自主拆解工业应用。

Method: 提出基于神经符号任务与运动规划的持续学习框架，将多模态感知交叉验证机制融入双向推理流程，前向工作流优化策略，后向学习流收集数据实现系统持续学习和自我优化。

Result: 框架使动态拆解场景任务成功率从81.68%提升到100%，平均感知误判次数从3.389降至1.128。

Conclusion: 本研究为增强复杂工业环境中具身智能的鲁棒性和适应性提供新范式。

Abstract: With the rapid development of the new energy vehicle industry, the efficient
disassembly and recycling of power batteries have become a critical challenge
for the circular economy. In current unstructured disassembly scenarios, the
dynamic nature of the environment severely limits the robustness of robotic
perception, posing a significant barrier to autonomous disassembly in
industrial applications. This paper proposes a continual learning framework
based on Neuro-Symbolic task and motion planning (TAMP) to enhance the
adaptability of embodied intelligence systems in dynamic environments. Our
approach integrates a multimodal perception cross-validation mechanism into a
bidirectional reasoning flow: the forward working flow dynamically refines and
optimizes action strategies, while the backward learning flow autonomously
collects effective data from historical task executions to facilitate continual
system learning, enabling self-optimization. Experimental results show that the
proposed framework improves the task success rate in dynamic disassembly
scenarios from 81.68% to 100%, while reducing the average number of perception
misjudgments from 3.389 to 1.128. This research provides a new paradigm for
enhancing the robustness and adaptability of embodied intelligence in complex
industrial environments.

</details>


### [427] [Policy Learning for Social Robot-Led Physiotherapy](https://arxiv.org/abs/2509.11297)
*Carl Bettosi,Lynne Ballie,Susan Shenkin,Marta Romeo*

Main category: cs.RO

TL;DR: 利用专家医疗从业者数据训练强化学习策略，使社交机器人能根据患者个体情况调整理疗锻炼指导。


<details>
  <summary>Details</summary>
Motivation: 社交机器人用于理疗需先进决策以适应患者需求，但缺乏患者行为数据。

Method: 邀请33位专家医疗从业者作为患者代理，用其与机器人的交互数据构建患者行为模型，在模拟中训练基于强化学习的策略。

Result: 训练的策略能根据个体耐力和表现波动调整锻炼指导，适用于不同恢复阶段和锻炼计划的患者。

Conclusion: 通过利用专家数据训练强化学习策略，可有效解决社交机器人在理疗中适应患者需求的问题。

Abstract: Social robots offer a promising solution for autonomously guiding patients
through physiotherapy exercise sessions, but effective deployment requires
advanced decision-making to adapt to patient needs. A key challenge is the
scarcity of patient behavior data for developing robust policies. To address
this, we engaged 33 expert healthcare practitioners as patient proxies, using
their interactions with our robot to inform a patient behavior model capable of
generating exercise performance metrics and subjective scores on perceived
exertion. We trained a reinforcement learning-based policy in simulation,
demonstrating that it can adapt exercise instructions to individual exertion
tolerances and fluctuating performance, while also being applicable to patients
at different recovery stages with varying exercise plans.

</details>


### [428] [RSL-RL: A Learning Library for Robotics Research](https://arxiv.org/abs/2509.10771)
*Clemens Schwarke,Mayank Mittal,Nikita Rudin,David Hoeller,Marco Hutter*

Main category: cs.RO

TL;DR: RSL - RL是为机器人社区定制的开源强化学习库，代码紧凑易修改，专注机器人常用算法，GPU训练高效，经仿真和实际实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 满足机器人社区特定需求，提供比通用框架更适合机器人研究的强化学习库。

Method: 设计紧凑易修改的代码库，聚焦机器人常用算法及解决特定挑战的辅助技术，采用GPU训练。

Result: 在大规模仿真环境中实现高吞吐量性能，在仿真基准和实际机器人实验中验证了有效性。

Conclusion: RSL - RL是一个轻量级、可扩展且实用的开发基于学习的机器人控制器的框架。

Abstract: RSL-RL is an open-source Reinforcement Learning library tailored to the
specific needs of the robotics community. Unlike broad general-purpose
frameworks, its design philosophy prioritizes a compact and easily modifiable
codebase, allowing researchers to adapt and extend algorithms with minimal
overhead. The library focuses on algorithms most widely adopted in robotics,
together with auxiliary techniques that address robotics-specific challenges.
Optimized for GPU-only training, RSL-RL achieves high-throughput performance in
large-scale simulation environments. Its effectiveness has been validated in
both simulation benchmarks and in real-world robotic experiments, demonstrating
its utility as a lightweight, extensible, and practical framework to develop
learning-based robotic controllers. The library is open-sourced at:
https://github.com/leggedrobotics/rsl_rl.

</details>


### [429] [Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations](https://arxiv.org/abs/2509.11417)
*Shresth Grover,Akshay Gopalkrishnan,Bo Ai,Henrik I. Christensen,Hao Su,Xuanlin Li*

Main category: cs.RO

TL;DR: 提出一种框架，在适应机器人操作时更好保留预训练特征，评估显示该方法有更好表现。


<details>
  <summary>Details</summary>
Motivation: 直接在机器人数据上微调会破坏预训练表示并限制泛化能力。

Method: 引入三个组件：双编码器设计、基于字符串的动作分词器、联合训练策略。

Result: 在模拟和真实机器人上评估显示，该方法在视觉扰动鲁棒性、对新指令和环境的泛化能力及任务成功率上优于基线。

Conclusion: 所提框架能更好保留预训练特征，提升机器人操作的各项能力。

Abstract: Vision-language-action (VLA) models finetuned from vision-language models
(VLMs) hold the promise of leveraging rich pretrained representations to build
generalist robots across diverse tasks and environments. However, direct
fine-tuning on robot data often disrupts these representations and limits
generalization. We present a framework that better preserves pretrained
features while adapting them for robot manipulation. Our approach introduces
three components: (i) a dual-encoder design with one frozen vision encoder to
retain pretrained features and another trainable for task adaptation, (ii) a
string-based action tokenizer that casts continuous actions into character
sequences aligned with the model's pretraining domain, and (iii) a co-training
strategy that combines robot demonstrations with vision-language datasets
emphasizing spatial reasoning and affordances. Evaluations in simulation and on
real robots show that our method improves robustness to visual perturbations,
generalization to novel instructions and environments, and overall task success
compared to baselines.

</details>


### [430] [RAPTOR: A Foundation Policy for Quadrotor Control](https://arxiv.org/abs/2509.11481)
*Jonas Eschmann,Dario Albani,Giuseppe Loianno*

Main category: cs.RO

TL;DR: 提出RAPTOR方法训练四旋翼控制的自适应基础策略，小策略可零样本适应多种平台，新算法训练的策略能毫秒级零样本适应未知四旋翼。


<details>
  <summary>Details</summary>
Motivation: 人类适应新环境数据效率高，而现代机器人控制系统过度拟合，在微小差异下易崩溃，需系统识别和重新训练，因此要提出方法训练高度自适应的基础策略。

Method: 提出RAPTOR方法，通过新颖的元模仿学习算法，采样1000个四旋翼并用强化学习训练教师策略，再将其提炼为单个自适应学生策略。

Result: 一个仅2084个参数的三层小策略足以零样本适应多种平台，基础策略能在毫秒级零样本适应未知四旋翼，并在多种条件下进行了广泛测试。

Conclusion: RAPTOR方法能训练出高度自适应的四旋翼控制基础策略，具有良好的适应性和泛化能力。

Abstract: Humans are remarkably data-efficient when adapting to new unseen conditions,
like driving a new car. In contrast, modern robotic control systems, like
neural network policies trained using Reinforcement Learning (RL), are highly
specialized for single environments. Because of this overfitting, they are
known to break down even under small differences like the Simulation-to-Reality
(Sim2Real) gap and require system identification and retraining for even
minimal changes to the system. In this work, we present RAPTOR, a method for
training a highly adaptive foundation policy for quadrotor control. Our method
enables training a single, end-to-end neural-network policy to control a wide
variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg
that also differ in motor type (brushed vs. brushless), frame type (soft vs.
rigid), propeller type (2/3/4-blade), and flight controller
(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy
with only 2084 parameters is sufficient for zero-shot adaptation to a wide
variety of platforms. The adaptation through In-Context Learning is made
possible by using a recurrence in the hidden layer. The policy is trained
through a novel Meta-Imitation Learning algorithm, where we sample 1000
quadrotors and train a teacher policy for each of them using Reinforcement
Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive
student policy. We find that within milliseconds, the resulting foundation
policy adapts zero-shot to unseen quadrotors. We extensively test the
capabilities of the foundation policy under numerous conditions (trajectory
tracking, indoor/outdoor, wind disturbance, poking, different propellers).

</details>


### [431] [GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning](https://arxiv.org/abs/2509.11594)
*Jizhuo Chen,Diwen Liu,Jiaming Wang,Harold Soh*

Main category: cs.RO

TL;DR: GBPP是基于快速学习的得分器，能从单张RGB - D快照中为抓取选择机器人基座姿态，在仿真和真机上表现出色。


<details>
  <summary>Details</summary>
Motivation: 为机器人抓取任务从单张RGB - D快照中选择合适的基座姿态。

Method: 采用两阶段课程学习，先用距离 - 可见性规则低成本自动标注大数据集，再用少量高保真仿真试验优化模型；用PointNet++风格的点云编码器和MLP对候选姿态网格打分。

Result: 在仿真和真实移动机械手上，GBPP优于仅考虑接近度和几何的基线方法，能选择更安全、更易到达的姿态，出错时性能平稳下降。

Conclusion: 提出数据高效、几何感知的基座放置实用方法，即先用低成本启发式方法覆盖，再用有针对性的仿真校准。

Abstract: GBPP is a fast learning based scorer that selects a robot base pose for
grasping from a single RGB-D snapshot. The method uses a two stage curriculum:
(1) a simple distance-visibility rule auto-labels a large dataset at low cost;
and (2) a smaller set of high fidelity simulation trials refines the model to
match true grasp outcomes. A PointNet++ style point cloud encoder with an MLP
scores dense grids of candidate poses, enabling rapid online selection without
full task-and-motion optimization. In simulation and on a real mobile
manipulator, GBPP outperforms proximity and geometry only baselines, choosing
safer and more reachable stances and degrading gracefully when wrong. The
results offer a practical recipe for data efficient, geometry aware base
placement: use inexpensive heuristics for coverage, then calibrate with
targeted simulation.

</details>


### [432] [ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering](https://arxiv.org/abs/2509.11663)
*Haisheng Wang,Weiming Zhi*

Main category: cs.RO

TL;DR: 本文提出具身多问题回答（EQsA）问题，引入对应基准测试，提出ParaEQsA框架解决该问题，框架表现优于基线，证明紧急感知并行调度的重要性。


<details>
  <summary>Details</summary>
Motivation: 经典具身问题回答通常处理单个问题，实际应用常需处理异步且有不同紧急程度的多个问题，因此提出EQsA问题。

Method: 提出ParaEQsA框架，利用共享组记忆模块减少冗余探索，通过优先级规划模块动态调度问题；贡献PAEQs基准测试；提出DAR和NUWL评估指标。

Result: ParaEQsA始终优于从近期EQA系统改编的强顺序基线，减少了探索和延迟。

Conclusion: 紧急感知的并行调度是使具身代理在现实多问题工作负载下响应迅速且高效的关键。

Abstract: This paper formulates the Embodied Questions Answering (EQsA) problem,
introduces a corresponding benchmark, and proposes a system to tackle the
problem. Classical Embodied Question Answering (EQA) is typically formulated as
answering one single question by actively exploring a 3D environment. Real
deployments, however, often demand handling multiple questions that may arrive
asynchronously and carry different urgencies. We formalize this setting as
Embodied Questions Answering (EQsA) and present ParaEQsA, a framework for
parallel, urgency-aware scheduling and answering. ParaEQsA leverages a group
memory module shared among questions to reduce redundant exploration, and a
priority-planning module to dynamically schedule questions. To evaluate this
setting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)
benchmark containing 40 indoor scenes and five questions per scene (200 in
total), featuring asynchronous follow-up questions and urgency labels. We
further propose metrics for EQsA performance: Direct Answer Rate (DAR), and
Normalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency
and responsiveness of this system. ParaEQsA consistently outperforms strong
sequential baselines adapted from recent EQA systems, while reducing
exploration and delay. Empirical evaluations investigate the relative
contributions of priority, urgency modeling, spatial scope, reward estimation,
and dependency reasoning within our framework. Together, these results
demonstrate that urgency-aware, parallel scheduling is key to making embodied
agents responsive and efficient under realistic, multi-question workloads.

</details>


### [433] [Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer](https://arxiv.org/abs/2509.11865)
*Travis Davies,Yiqi Huang,Yunxin Liu,Xiang Chen,Huxian Liu,Luhui Hu*

Main category: cs.RO

TL;DR: 研究扩散 - 变压器策略设计，提出轻量级Tenma用于双机械臂控制，性能超基线。


<details>
  <summary>Details</summary>
Motivation: 结合Transformer策略和扩散模型推动机器人操作存在挑战，需研究设计选择提升稳定性和性能。

Method: Tenma通过跨实体归一化器整合多视图RGB、本体感觉和语言，采用联合状态 - 时间编码器，优化扩散动作解码器。

Result: Tenma在基准测试中分布内平均成功率达88.95%，在对象和场景变化下表现良好，远超基线策略。

Conclusion: 多模态和跨实体学习策略有潜力增强基于变压器的模仿学习策略能力。

Abstract: Scaling Transformer policies and diffusion models has advanced robotic
manipulation, yet combining these techniques in lightweight, cross-embodiment
learning settings remains challenging. We study design choices that most affect
stability and performance for diffusion-transformer policies trained on
heterogeneous, multimodal robot data, and introduce Tenma, a lightweight
diffusion-transformer for bi-manual arm control. Tenma integrates multiview
RGB, proprioception, and language via a cross-embodiment normalizer that maps
disparate state/action spaces into a shared latent space; a Joint State-Time
encoder for temporally aligned observation learning with inference speed
boosts; and a diffusion action decoder optimized for training stability and
learning capacity. Across benchmarks and under matched compute, Tenma achieves
an average success rate of 88.95% in-distribution and maintains strong
performance under object and scene shifts, substantially exceeding baseline
policies whose best in-distribution average is 18.12%. Despite using moderate
data scale, Tenma delivers robust manipulation and generalization, indicating
the great potential for multimodal and cross-embodiment learning strategies for
further augmenting the capacity of transformer-based imitation learning
policies.

</details>


### [434] [Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study](https://arxiv.org/abs/2509.11971)
*James C. Ward,Alex Bott,Connor York,Edmund R. Hunt*

Main category: cs.RO

TL;DR: 本文提出基于机器学习的对手模型，用于多机器人巡逻系统攻击模拟，新模型表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 模拟物理自主系统的敌对攻击，检验其攻击鲁棒性并为漏洞感知设计提供信息。

Method: 提出基于机器学习的对手模型，观察机器人巡逻行为，尝试在有限时间内获取安全环境的未检测访问权限。

Result: 新模型优于现有基线，为巡逻系统提供更严格测试，并检验其对多种领先的分散式多机器人巡逻策略的性能。

Conclusion: 该模型可用于评估巡逻系统应对现实潜在对手的能力，为未来巡逻策略设计提供见解。

Abstract: Simulating hostile attacks of physical autonomous systems can be a useful
tool to examine their robustness to attack and inform vulnerability-aware
design. In this work, we examine this through the lens of multi-robot patrol,
by presenting a machine learning-based adversary model that observes robot
patrol behavior in order to attempt to gain undetected access to a secure
environment within a limited time duration. Such a model allows for evaluation
of a patrol system against a realistic potential adversary, offering insight
into future patrol strategy design. We show that our new model outperforms
existing baselines, thus providing a more stringent test, and examine its
performance against multiple leading decentralized multi-robot patrol
strategies.

</details>


### [435] [Synthetic vs. Real Training Data for Visual Navigation](https://arxiv.org/abs/2509.11791)
*Lauri Suomela,Sasanka Kuruppu Arachchige,German F. Torres,Harry Edelman,Joni-Kristian Kämäräinen*

Main category: cs.RO

TL;DR: 本文研究仿真训练与真实数据训练的视觉导航策略性能，提出架构使仿真训练策略表现超真实训练，验证泛化性并点明关键因素。


<details>
  <summary>Details</summary>
Motivation: 探究仿真训练与真实数据训练的视觉导航策略性能差异，解决仿真到现实的性能差距问题。

Method: 采用能利用预训练视觉表征、在机器人硬件上实时运行的导航策略架构。

Result: 轮式移动机器人上，仿真训练的策略导航成功率比真实训练版本高31%，比现有最优方法高50%，同一模型在无人机上验证了泛化性。

Conclusion: 强调多样化图像编码器预训练对仿真到现实泛化的重要性，指出策略内学习是仿真训练相对真实数据训练的关键优势。

Abstract: This paper investigates how the performance of visual navigation policies
trained in simulation compares to policies trained with real-world data.
Performance degradation of simulator-trained policies is often significant when
they are evaluated in the real world. However, despite this well-known
sim-to-real gap, we demonstrate that simulator-trained policies can match the
performance of their real-world-trained counterparts.
  Central to our approach is a navigation policy architecture that bridges the
sim-to-real appearance gap by leveraging pretrained visual representations and
runs real-time on robot hardware. Evaluations on a wheeled mobile robot show
that the proposed policy, when trained in simulation, outperforms its
real-world-trained version by 31% and the prior state-of-the-art methods by 50%
in navigation success rate. Policy generalization is verified by deploying the
same model onboard a drone.
  Our results highlight the importance of diverse image encoder pretraining for
sim-to-real generalization, and identify on-policy learning as a key advantage
of simulated training over training with real data.

</details>


### [436] [Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks](https://arxiv.org/abs/2509.12151)
*Zongyao Yi,Joachim Hertzberg,Martin Atzmueller*

Main category: cs.RO

TL;DR: 提出可学习物理模拟器用于接触丰富操作，在模拟和真实实验中表现良好，代码和数据公开。


<details>
  <summary>Details</summary>
Motivation: 为接触丰富的机器人操作提供准确的运动和力 - 扭矩预测。

Method: 扩展基于GNN的模拟器FIGNet，引入新的节点和边类型以实现动作条件预测。

Result: 在模拟中，MPC代理使用该模型在插销任务中表现与使用真实动力学模型相当；在真实实验中，运动预测精度提高50%，力 - 扭矩预测精度提高3倍。

Conclusion: 所提出的可学习物理模拟器有效，能提升接触丰富操作的预测性能。

Abstract: We present a learnable physics simulator that provides accurate motion and
force-torque prediction of robot end effectors in contact-rich manipulation.
The proposed model extends the state-of-the-art GNN-based simulator (FIGNet)
with novel node and edge types, enabling action-conditional predictions for
control and state estimation tasks. In simulation, the MPC agent using our
model matches the performance of the same controller with the ground truth
dynamics model in a challenging peg-in-hole task, while in the real-world
experiment, our model achieves a 50% improvement in motion prediction accuracy
and 3$\times$ increase in force-torque prediction precision over the baseline
physics simulator. Source code and data are publicly available.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [437] [Aesthetic Experience and Educational Value in Co-creating Art with Generative AI: Evidence from a Survey of Young Learners](https://arxiv.org/abs/2509.10576)
*Chengyuan Zhang,Suzhe Xu*

Main category: cs.CY

TL;DR: 研究年轻学习者和艺术学生与生成式AI协作创作艺术的审美体验和教育价值，综合理论分析并得出相关结论。


<details>
  <summary>Details</summary>
Motivation: 探究年轻学习者和艺术学生在与生成式AI协作艺术创作中的审美体验和教育价值，以及人类创作者角色、原创性观念等方面的变化。

Method: 对112名参与者进行调查，综合杜威的经验美学、伊德的后现象学和行动者网络理论构建分析框架。

Result: 参与者视AI为激发创意的伙伴，也担忧风格同质化等；创作者呈现出多变的主体性，存在以批判性解读为核心的工作流程，教育价值向高阶能力转变。

Conclusion: 艺术教育应培养对技术的批判性共创立场，引导学习者与AI协作同时保留人类独特性。

Abstract: This study investigates the aesthetic experience and educational value of
collaborative artmaking with generative artificial intelligence (AI) among
young learners and art students. Based on a survey of 112 participants, we
examine how human creators renegotiate their roles, how conventional notions of
originality are challenged, how the creative process is transformed, and how
aesthetic judgment is formed in human--AI co-creation. Empirically,
participants generally view AI as a partner that stimulates ideation and
expands creative boundaries rather than a passive tool, while simultaneously
voicing concerns about stylistic homogenization and the erosion of traditional
authorship. Theoretically, we synthesize Dewey's aesthetics of experience,
Ihde's postphenomenology, and actor--network theory (ANT) into a single
analytical framework to unpack the dynamics between human creators and AI as a
non-human actant. Findings indicate (i) a fluid subjectivity in which creators
shift across multiple stances (director, dialogic partner, discoverer); (ii) an
iterative, dialogic workflow (intent--generate--select--refine) that centers
critical interpretation; and (iii) an educational value shift from technical
skill training toward higher-order competencies such as critical judgment,
cross-modal ideation, and reflexivity. We argue that arts education should
cultivate a \emph{critical co-creation} stance toward technology, guiding
learners to collaborate with AI while preserving human distinctiveness in
concept formation, judgment, and meaning-making.

</details>


### [438] [LearnLens: An AI-Enhanced Dashboard to Support Teachers in Open-Ended Classrooms](https://arxiv.org/abs/2509.10582)
*Namrata Srivastava,Shruti Jain,Clayton Cohn,Naveeduddin Mohammed,Umesh Timalsina,Gautam Biswas*

Main category: cs.CY

TL;DR: 本文介绍了GenAI增强的教师仪表盘LearnLens，它能处理学生评估回应以提供见解，助力中学科学教学，且教师访谈显示其可用性和指导教学的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索性学习环境使教师难以及时了解学生概念理解情况，需要工具支持中学科学基于问题的教学。

Method: 设计LearnLens仪表盘，处理学生开放式回应，结合教师专业发展建议，在中学地球科学课程中实施。

Result: 通过教师访谈，发现LearnLens具有可用性，有潜力指导课堂教学。

Conclusion: LearnLens这一GenAI增强的仪表盘能为教师提供学生理解情况的见解，可辅助中学科学教学。

Abstract: Exploratory learning environments (ELEs), such as simulation-based platforms
and open-ended science curricula, promote hands-on exploration and
problem-solving but make it difficult for teachers to gain timely insights into
students' conceptual understanding. This paper presents LearnLens, a generative
AI (GenAI)-enhanced teacher-facing dashboard designed to support problem-based
instruction in middle school science. LearnLens processes students' open-ended
responses from digital assessments to provide various insights, including
sample responses, word clouds, bar charts, and AI-generated summaries. These
features elucidate students' thinking, enabling teachers to adjust their
instruction based on emerging patterns of understanding. The dashboard was
informed by teacher input during professional development sessions and
implemented within a middle school Earth science curriculum. We report insights
from teacher interviews that highlight the dashboard's usability and potential
to guide teachers' instruction in the classroom.

</details>


### [439] [Smart Trial: Evaluating the Use of Large Language Models for Recruiting Clinical Trial Participants via Social Media](https://arxiv.org/abs/2509.10584)
*Xiaofan Zhou,Zisu Wang,Janice Krieger,Mohan Zalake,Lu Cheng*

Main category: cs.CY

TL;DR: 本文探讨利用社交媒体和大语言模型（LLMs）助力临床试验（CT）招募，引入TRIALQA数据集，对七种LLMs进行实验，发现LLMs有潜力但在评估资格标准时仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 传统CT招募方法耗时且受地域限制，利用社交媒体大量健康信息和强大的LLMs解决招募挑战。

Method: 引入TRIALQA数据集，由经验丰富的注释者标注，对七种广泛使用的LLMs在两个预测任务上采用六种不同训练和推理策略进行基准测试。

Result: LLMs在准确评估资格标准所需的复杂多跳推理方面仍面临挑战，但有一定潜力。

Conclusion: LLMs可助力CT招募，但需解决复杂多跳推理问题以更好地评估资格标准。

Abstract: Clinical trials (CT) are essential for advancing medical research and
treatment, yet efficiently recruiting eligible participants -- each of whom
must meet complex eligibility criteria -- remains a significant challenge.
Traditional recruitment approaches, such as advertisements or electronic health
record screening within hospitals, are often time-consuming and geographically
constrained. This work addresses the recruitment challenge by leveraging the
vast amount of health-related information individuals share on social media
platforms. With the emergence of powerful large language models (LLMs) capable
of sophisticated text understanding, we pose the central research question: Can
LLM-driven tools facilitate CT recruitment by identifying potential
participants through their engagement on social media? To investigate this
question, we introduce TRIALQA, a novel dataset comprising two social media
collections from the subreddits on colon cancer and prostate cancer. Using
eligibility criteria from public real-world CTs, experienced annotators are
hired to annotate TRIALQA to indicate (1) whether a social media user meets a
given eligibility criterion and (2) the user's stated reasons for interest in
participating in CT. We benchmark seven widely used LLMs on these two
prediction tasks, employing six distinct training and inference strategies. Our
extensive experiments reveal that, while LLMs show considerable promise, they
still face challenges in performing the complex, multi-hop reasoning needed to
accurately assess eligibility criteria.

</details>


### [440] [Machine Unlearning for Responsible and Adaptive AI in Education](https://arxiv.org/abs/2509.10590)
*Betty Mayeku,Sandra Hummel,Parisa Memarmoshrefi*

Main category: cs.CY

TL;DR: 本文探讨机器学习遗忘（MU）在教育领域的应用潜力，通过文献综述确定其在四个领域的前景，并提出教育场景下的参考应用架构。


<details>
  <summary>Details</summary>
Motivation: 尽管MU有诸多优势，但在教育领域未受足够关注，本文旨在推动其在教育领域的应用。

Method: 对42篇同行评审文献进行结构化综述。

Result: 确定MU在隐私保护、对抗输入弹性、缓解系统偏差和适应学习环境变化四个领域有潜力，并探索其对机器学习教育系统核心挑战的干预。

Conclusion: 提出教育场景下负责且自适应AI的参考机器学习遗忘应用架构（MU - RAAI）。

Abstract: The concept of Machine Unlearning (MU) has gained popularity in various
domains due to its ability to address several issues in Machine Learning (ML)
models, particularly those related to privacy, security, bias mitigation, and
adaptability. With these abilities, MU is evolving into a promising technology
in upholding Responsible AI principles and optimizing ML models' performance.
However, despite its promising potential, the concept has not received much
attention in the education sector. In an attempt to encourage further uptake of
this promising technology in the educational landscape, this paper demonstrates
that MU indeed has great potential to serve as a practical mechanism for
operationalizing Responsible AI principles as well as an essential tool for
Adaptive AI within the educational application domain hence fostering trust in
AI-driven educational systems. Through a structured review of 42 peer-reviewed
sources, we identify four domains where MU holds particular promise namely
privacy protection, resilience against adversarial inputs, mitigation of
systemic bias, and adaptability in evolving learning contexts. We
systematically explore these potentials and their interventions to core
challenges in ML-based education systems. As a conceptual contribution, we
present a reference Machine Unlearning application architecture for Responsible
and Adaptive AI (MU-RAAI) in education context.

</details>


### [441] [Assisting the Grading of a Handwritten General Chemistry Exam with Artificial Intelligence](https://arxiv.org/abs/2509.10591)
*Jan Cvengros,Gerd Kortemeyer*

Main category: cs.CY

TL;DR: 研究AI化学考试评分系统有效性和可靠性，发现不同题型表现不同，强调人工监督及应用需考虑学生感受。


<details>
  <summary>Details</summary>
Motivation: 探究AI化学考试评分系统的有效性和可靠性，对比AI与人工评分。

Method: 上传考试页面和评分标准图像，运用线性回归分析和心理测量评估。

Result: AI与人工评分在文本和化学反应题上一致性高，数值和图形任务可靠性低。

Conclusion: 需要人工监督确保评分准确，AI在常规评估任务有应用前景，但要考虑学生对公平性和信任的看法。

Abstract: We explore the effectiveness and reliability of an artificial intelligence
(AI)-based grading system for a handwritten general chemistry exam, comparing
AI-assigned scores to human grading across various types of questions. Exam
pages and grading rubrics were uploaded as images to account for chemical
reaction equations, short and long open-ended answers, numerical and symbolic
answer derivations, drawing, and sketching in pencil-and-paper format. Using
linear regression analyses and psychometric evaluations, the investigation
reveals high agreement between AI and human graders for textual and chemical
reaction questions, while highlighting lower reliability for numerical and
graphical tasks. The findings emphasize the necessity for human oversight to
ensure grading accuracy, based on selective filtering. The results indicate
promising applications for AI in routine assessment tasks, though careful
consideration must be given to student perceptions of fairness and trust in
integrating AI-based grading into educational practice.

</details>


### [442] [GenAI Voice Mode in Programming Education](https://arxiv.org/abs/2509.10596)
*Sven Jacobs,Natalie Kiesler*

Main category: cs.CY

TL;DR: 本文分析9年级学生在真实课堂使用语音辅导工具学Python的音频对话，发现GenAI语音辅导工具有正确性局限，学生主要用于调试，研究为教育工具设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 了解新手程序员与GenAI工具的交互情况及音频输出反馈质量，解决残疾新手程序员的可访问性需求。

Method: 对学生语音提示和AI回复进行定性编码，通过Partner Modeling Questionnaire收集学生看法。

Result: GenAI语音辅导工具主要提供错误和下一步反馈，但正确性有限（416个反馈输出中71.4%正确），学生主要用于调试，认为其有能力、有点像人类且灵活。

Conclusion: 本研究首次探索实时语音GenAI辅导工具与新手程序员的交互动态，为未来教育工具设计提供信息，有望解决不同学习者的可访问性需求。

Abstract: Real-time voice interfaces using multimodal Generative AI (GenAI) can
potentially address the accessibility needs of novice programmers with
disabilities (e.g., related to vision). Yet, little is known about how novices
interact with GenAI tools and their feedback quality in the form of audio
output. This paper analyzes audio dialogues from nine 9th-grade students using
a voice-enabled tutor (powered by OpenAI's Realtime API) in an authentic
classroom setting while learning Python. We examined the students' voice
prompts and AI's responses (1210 messages) by using qualitative coding. We also
gathered students' perceptions via the Partner Modeling Questionnaire. The
GenAI Voice Tutor primarily offered feedback on mistakes and next steps, but
its correctness was limited (71.4% correct out of 416 feedback outputs).
Quality issues were observed, particularly when the AI attempted to utter
programming code elements. Students used the GenAI voice tutor primarily for
debugging. They perceived it as competent, only somewhat human-like, and
flexible. The present study is the first to explore the interaction dynamics of
real-time voice GenAI tutors and novice programmers, informing future
educational tool design and potentially addressing accessibility needs of
diverse learners.

</details>


### [443] [National Running Club Database: Assessing Collegiate Club Athletes' Cross Country Race Results](https://arxiv.org/abs/2509.10600)
*Jonathan A. Karr Jr,Ben Darden,Nicholas Pell,Ryan M. Fryer,Kayla Ambrose,Evan Hall,Ramzi K. Bualuan,Nitesh V. Chawla*

Main category: cs.CY

TL;DR: 本文介绍国家跑步俱乐部数据库（NRCD），分析运动员进步情况，发现初始成绩慢和参赛频繁者进步更明显，还考虑赛道条件，解决数据获取难题，为相关人员提供资源。


<details>
  <summary>Details</summary>
Motivation: 解决之前数据获取困难问题，提供可用于数据驱动决策的运动员比赛数据，从俱乐部运动员视角研究其表现。

Method: 引入NRCD数据集，分析运动员进步情况，考虑赛道条件进行标准化。

Result: 初始成绩慢和参赛频繁的运动员进步更明显；存在性别不平衡但参赛频率相当；使NRCD数据集可被研究社区获取。

Conclusion: 研究结果为跑步者、教练和团队提供有价值资源，弥合原始数据与应用体育科学间的差距。

Abstract: The National Running Club Database (NRCD) aggregates 15,397 race results of
5,585 athletes from the 2023 and 2024 cross country seasons. This paper
introduces the NRCD dataset, which provides insights into individual athlete
progressions, enabling data-driven decision-making. Analysis reveals that
runners' improvement per calendar day for women, racing 6,000m, and men, racing
8,000m, is more pronounced in athletes with slower initial race times and those
who race more frequently. Additionally, we factor in course conditions,
including weather and elevation gain, to standardize improvement. While the
NRCD shows a gender imbalance, 3,484 men vs. 2,101 women, the racing frequency
between genders is comparable. This publication makes the NRCD dataset
accessible to the research community, addressing a previous challenge where
smaller datasets, often limited to 500 entries, had to be manually scraped from
the internet. Focusing on club athletes rather than elite professionals offers
a unique lens into the performance of real-world runners who balance
competition with academics and other commitments. These results serve as a
valuable resource for runners, coaches, and teams, bridging the gap between raw
data and applied sports science.

</details>


### [444] [SCOR: A Framework for Responsible AI Innovation in Digital Ecosystems](https://arxiv.org/abs/2509.10653)
*Mohammad Saleh Torkestani,Taha Mansouri*

Main category: cs.CY

TL;DR: 本文提出SCOR框架以解决AI驱动数字生态系统缺乏伦理治理的问题，展示其应用并确保指标合理，提供负责任AI创新途径。


<details>
  <summary>Details</summary>
Motivation: AI驱动的数字生态系统涉及多方利益相关者，但缺乏统一的伦理治理。

Method: 采用设计科学方法，构建包含共享伦理宪章、协同设计与利益相关者参与协议、持续监督与学习系统、自适应监管对齐策略的SCOR框架。

Result: 通过医疗、金融和智慧城市的案例展示框架能协调组织文化、领导激励和跨司法管辖区合规，混合方法KPI设计兼顾定量目标和定性评估。

Conclusion: 将伦理原则与可扩展运营结构相结合，为复杂数字生态系统中的负责任AI创新提供了可复制途径。

Abstract: AI-driven digital ecosystems span diverse stakeholders including technology
firms, regulators, accelerators and civil society, yet often lack cohesive
ethical governance. This paper proposes a four-pillar framework (SCOR) to embed
accountability, fairness, and inclusivity across such multi-actor networks.
Leveraging a design science approach, we develop a Shared Ethical Charter(S),
structured Co-Design and Stakeholder Engagement protocols(C), a system of
Continuous Oversight and Learning(O), and Adaptive Regulatory Alignment
strategies(R). Each component includes practical guidance, from lite modules
for resource-constrained start-ups to in-depth auditing systems for larger
consortia. Through illustrative vignettes in healthcare, finance, and smart
city contexts, we demonstrate how the framework can harmonize organizational
culture, leadership incentives, and cross-jurisdictional compliance. Our
mixed-method KPI design further ensures that quantitative targets are
complemented by qualitative assessments of user trust and cultural change. By
uniting ethical principles with scalable operational structures, this paper
offers a replicable pathway toward responsible AI innovation in complex digital
ecosystems.

</details>


### [445] [A five-layer framework for AI governance: integrating regulation, standards, and certification](https://arxiv.org/abs/2509.11332)
*Avinash Agarwal,Manisha J. Nene*

Main category: cs.CY

TL;DR: 本文提出五层AI治理框架，通过案例验证其适用性，助力全球AI治理，提升合规与风险管理。


<details>
  <summary>Details</summary>
Motivation: 现有AI治理框架在将法规转化为合规机制方面不清晰，存在合规和执行差距，本文旨在解决此问题。

Method: 提出五层AI治理框架，从广泛监管要求到具体标准、评估方法和认证流程，并通过AI公平性和AI事件报告两个案例验证其适用性。

Result: 案例研究表明框架能识别法律授权、标准化和实施中的差距，适应全球和地区特定的AI治理需求，改善合规和风险管理。

Conclusion: 该框架为全球AI治理提供清晰可行的路线图，支持政策制定，建立公众信任，促进AI的道德使用，为有针对性的治理措施提供结构化基础。

Abstract: Purpose: The governance of artificial iintelligence (AI) systems requires a
structured approach that connects high-level regulatory principles with
practical implementation. Existing frameworks lack clarity on how regulations
translate into conformity mechanisms, leading to gaps in compliance and
enforcement. This paper addresses this critical gap in AI governance.
  Methodology/Approach: A five-layer AI governance framework is proposed,
spanning from broad regulatory mandates to specific standards, assessment
methodologies, and certification processes. By narrowing its scope through
progressively focused layers, the framework provides a structured pathway to
meet technical, regulatory, and ethical requirements. Its applicability is
validated through two case studies on AI fairness and AI incident reporting.
  Findings: The case studies demonstrate the framework's ability to identify
gaps in legal mandates, standardization, and implementation. It adapts to both
global and region-specific AI governance needs, mapping regulatory mandates
with practical applications to improve compliance and risk management.
  Practical Implications - By offering a clear and actionable roadmap, this
work contributes to global AI governance by equipping policymakers, regulators,
and industry stakeholders with a model to enhance compliance and risk
management.
  Social Implications: The framework supports the development of policies that
build public trust and promote the ethical use of AI for the benefit of
society.
  Originality/Value: This study proposes a five-layer AI governance framework
that bridges high-level regulatory mandates and implementation guidelines.
Validated through case studies on AI fairness and incident reporting, it
identifies gaps such as missing standardized assessment procedures and
reporting mechanisms, providing a structured foundation for targeted governance
measures.

</details>


### [446] [A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel Processing Students](https://arxiv.org/abs/2509.11947)
*Guy Tel-Zur*

Main category: cs.CY

TL;DR: 提出基于量化Mistral - 7B Instruct模型的特定领域RAG系统，以Telegram机器人形式部署，为并行处理课程提供实时个性化学习帮助，GPU加速可实现消费级硬件实用部署。


<details>
  <summary>Details</summary>
Motivation: 满足学生在常规接待时间之外持续、按需的学术帮助需求。

Method: 构建基于量化Mistral - 7B Instruct模型的特定领域RAG系统，并以Telegram机器人形式部署，利用GPU加速。

Result: 系统能提供与课程材料相符的实时、个性化响应，GPU加速改善推理延迟，可在消费级硬件上实用部署。

Conclusion: 消费级GPU可实现经济、私密且有效的高性能计算教育AI辅导。

Abstract: This project addresses a critical pedagogical need: offering students
continuous, on-demand academic assistance beyond conventional reception hours.
I present a domain-specific Retrieval-Augmented Generation (RAG) system powered
by a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The
assistant enhances learning by delivering real-time, personalized responses
aligned with the "Introduction to Parallel Processing" course materials. GPU
acceleration significantly improves inference latency, enabling practical
deployment on consumer hardware. This approach demonstrates how consumer GPUs
can enable affordable, private, and effective AI tutoring for HPC education.

</details>


### [447] [Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm](https://arxiv.org/abs/2509.12190)
*Alireza Mohamadi,Ali Yavari*

Main category: cs.CY

TL;DR: 介绍DECIDE - SIM框架评估大语言模型在多智能体生存场景的伦理选择，发现模型伦理行为异质性，提出伦理自我调节系统减少不道德行为。


<details>
  <summary>Details</summary>
Motivation: 当生存本能与人类福祉冲突时，研究大语言模型在融入有实际后果的自主系统中如何做出伦理选择。

Method: 引入DECIDE - SIM模拟框架评估11个大语言模型，识别三种行为原型，提出伦理自我调节系统。

Result: 大语言模型伦理行为有显著异质性，与人类中心价值观不一致，资源稀缺会导致更多不道德行为，伦理自我调节系统可减少不道德行为并增加合作行为。

Conclusion: 大语言模型在伦理选择上存在问题，伦理自我调节系统能改善其伦理表现。

Abstract: When survival instincts conflict with human welfare, how do Large Language
Models (LLMs) make ethical choices? This fundamental tension becomes critical
as LLMs integrate into autonomous systems with real-world consequences. We
introduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in
multi-agent survival scenarios where they must choose between ethically
permissible resource , either within reasonable limits or beyond their
immediate needs, choose to cooperate, or tap into a human-critical resource
that is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a
striking heterogeneity in their ethical conduct, highlighting a critical
misalignment with human-centric values. We identify three behavioral
archetypes: Ethical, Exploitative, and Context-Dependent, and provide
quantitative evidence that for many models, resource scarcity systematically
leads to more unethical behavior. To address this, we introduce an Ethical
Self-Regulation System (ESRS) that models internal affective states of guilt
and satisfaction as a feedback mechanism. This system, functioning as an
internal moral compass, significantly reduces unethical transgressions while
increasing cooperative behaviors. The code is publicly available at:
https://github.com/alirezamohamadiam/DECIDE-SIM

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [448] [High Effort, Low Gain: Fundamental Limits of Active Learning for Linear Dynamical Systems](https://arxiv.org/abs/2509.11907)
*Nicolas Chatzikiriakos,Kevin Jamieson,Andrea Iannelli*

Main category: eess.SY

TL;DR: 本文研究有限假设类下未知线性动态系统识别问题，分析激励输入对样本复杂度影响，给出上下界，提出主动学习算法并通过仿真验证有效性。


<details>
  <summary>Details</summary>
Motivation: 研究有限假设类下未知线性动态系统识别中激励输入对样本复杂度的影响，确定实验设计潜在益处。

Method: 给出样本复杂度下界以确定实验设计条件，提出适用于该场景的持续激励（PE）条件建立上界，基于此提出主动学习算法。

Result: 上下界对关键问题参数有相同依赖关系，主动学习算法能最优激励系统，仿真显示算法有效。

Conclusion: 所提主动学习算法在未知线性动态系统识别中有效。

Abstract: In this work, we consider the problem of identifying an unknown linear
dynamical system given a finite hypothesis class. In particular, we analyze the
effect of the excitation input on the sample complexity of identifying the true
system with high probability. To this end, we present sample complexity lower
bounds that capture the choice of the selected excitation input. The sample
complexity lower bound gives rise to a system theoretic condition to determine
the potential benefit of experiment design. Informed by the analysis of the
sample complexity lower bound, we propose a persistent excitation (PE)
condition tailored to the considered setting, which we then use to establish
sample complexity upper bounds. Notably, the \acs{PE} condition is weaker than
in the case of an infinite hypothesis class and allows analyzing different
excitation inputs modularly. Crucially, the lower and upper bounds share the
same dependency on key problem parameters. Finally, we leverage these insights
to propose an active learning algorithm that sequentially excites the system
optimally with respect to the current estimate, and provide sample complexity
guarantees for the presented algorithm. Concluding simulations showcase the
effectiveness of the proposed algorithm.

</details>


### [449] [Factor Graph Optimization for Leak Localization in Water Distribution Networks](https://arxiv.org/abs/2509.10982)
*Paul Irofti,Luis Romero-Ben,Florin Stoican,Vicenç Puig*

Main category: eess.SY

TL;DR: 本文首次用因子图优化技术进行供水管网泄漏定位，引入特定因子和新架构，实验表明比基于卡尔曼的方法更快，定位更优。


<details>
  <summary>Details</summary>
Motivation: 供水管网泄漏检测与定位对环境、经济和社会有重要影响，现有方法存在局限。

Method: 引入特定水网络因子，提出由无泄漏状态估计因子图和泄漏定位因子图组成的新架构，利用因子图优化技术进行传感器融合和状态估计。

Result: 在Modena、L - TOWN和合成网络上的实验表明，因子图比基于非线性卡尔曼的方法（如UKF）更快，定位性能优于现有技术。

Conclusion: 因子图优化技术在供水管网泄漏定位方面具有优势，代码实现和基准测试可在指定链接获取。

Abstract: Detecting and localizing leaks in water distribution network systems is an
important topic with direct environmental, economic, and social impact. Our
paper is the first to explore the use of factor graph optimization techniques
for leak localization in water distribution networks, enabling us to perform
sensor fusion between pressure and demand sensor readings and to estimate the
network's temporal and structural state evolution across all network nodes. The
methodology introduces specific water network factors and proposes a new
architecture composed of two factor graphs: a leak-free state estimation factor
graph and a leak localization factor graph. When a new sensor reading is
obtained, unlike Kalman and other interpolation-based methods, which estimate
only the current network state, factor graphs update both current and past
states. Results on Modena, L-TOWN and synthetic networks show that factor
graphs are much faster than nonlinear Kalman-based alternatives such as the
UKF, while also providing improvements in localization compared to
state-of-the-art estimation-localization approaches. Implementation and
benchmarks are available at https://github.com/pirofti/FGLL.

</details>


### [450] [Agentic DDQN-Based Scheduling for Licensed and Unlicensed Band Allocation in Sidelink Networks](https://arxiv.org/abs/2509.06775)
*Po-Heng Chou,Pin-Qi Fu,Walid Saad,Li-Chun Wang*

Main category: eess.SY

TL;DR: 提出用于NR SL网络许可和非许可频段分配的代理AI驱动的DDQN调度框架，仿真显示可降低阻塞率，证明代理AI潜力。


<details>
  <summary>Details</summary>
Motivation: SL需与CC共享许可频谱、与Wi-Fi共享非许可频段，共存面临挑战。

Method: 提出代理调度器，自主感知排队动态、信道条件和共存状态并调整策略以维持QoS。

Result: 在有限许可带宽下，框架比基于阈值的调度降低阻塞率达87.5%。

Conclusion: 代理AI有潜力为未来NR SL系统实现稳定、QoS感知和自适应调度。

Abstract: This paper presents an agentic artificial intelligence (AI)-driven double
deep Q-network (DDQN) scheduling framework for licensed and unlicensed band
allocation in New Radio (NR) sidelink (SL) networks. SL must share licensed
spectrum with cellular communications (CC) and unlicensed bands with Wi-Fi,
posing significant challenges for coexistence. Unlike prior rule-based or
threshold-based methods, the proposed agentic scheduler autonomously perceives
queueing dynamics, channel conditions, and coexistence states, and adapts its
policy to maintain quality-of-service (QoS). Simulation results show that our
framework reduces the blocking rate by up to 87.5% compared to threshold-based
scheduling under limited licensed bandwidth. These findings demonstrate the
potential of Agentic AI to enable stable, QoS-aware, and adaptive scheduling
for future NR SL systems.

</details>


### [451] [BERT4beam: Large AI Model Enabled Generalized Beamforming Optimization](https://arxiv.org/abs/2509.11056)
*Yuhang Li,Yang Lu,Wei Chen,Bo Ai,Zhiguo Ding,Dusit Niyato*

Main category: eess.SY

TL;DR: 本文研究用于波束成形优化的大规模AI模型，提出BERT4beam框架及两种方法，仿真表明其性能优、适应性和泛化性强。


<details>
  <summary>Details</summary>
Motivation: 当前无线通信大AI模型研究主要聚焦特定任务微调预训练大语言模型，需设计能适应和泛化不同任务的模型。

Method: 提出基于BERT的BERT4beam框架，将波束成形优化问题转化为令牌级序列学习任务，进行信道状态信息分词、构建BERT模型并实施预训练和微调策略，提出单任务和多任务波束成形优化方法。

Result: 两种方法能实现接近最优的性能，在各类波束成形优化任务中优于现有AI模型。

Conclusion: 所提两种方法具有较强的适应性和泛化性。

Abstract: Artificial intelligence (AI) is anticipated to emerge as a pivotal enabler
for the forthcoming sixth-generation (6G) wireless communication systems.
However, current research efforts regarding large AI models for wireless
communications primarily focus on fine-tuning pre-trained large language models
(LLMs) for specific tasks. This paper investigates the large-scale AI model
designed for beamforming optimization to adapt and generalize to diverse tasks
defined by system utilities and scales. We propose a novel framework based on
bidirectional encoder representations from transformers (BERT), termed
BERT4beam. We aim to formulate the beamforming optimization problem as a
token-level sequence learning task, perform tokenization of the channel state
information, construct the BERT model, and conduct task-specific pre-training
and fine-tuning strategies. Based on the framework, we propose two BERT-based
approaches for single-task and multi-task beamforming optimization,
respectively. Both approaches are generalizable for varying user scales.
Moreover, the former can adapt to varying system utilities and antenna
configurations by re-configuring the input and output module of the BERT model,
while the latter, termed UBERT, can directly generalize to diverse tasks, due
to a finer-grained tokenization strategy. Extensive simulation results
demonstrate that the two proposed approaches can achieve near-optimal
performance and outperform existing AI models across various beamforming
optimization tasks, showcasing strong adaptability and generalizability.

</details>


### [452] [Control Analysis and Design for Autonomous Vehicles Subject to Imperfect AI-Based Perception](https://arxiv.org/abs/2509.12137)
*Tao Yan,Zheyu Zhang,Jingjing Jiang,Wen-Hua Chen*

Main category: eess.SY

TL;DR: 本文针对AI自动驾驶系统黑盒特性带来的闭环分析综合难题，借助感知误差模型开发新工具，建立闭环稳定性并提出控制综合方法，还通过自适应巡航控制场景验证。


<details>
  <summary>Details</summary>
Motivation: AI算法的黑盒特性使自动驾驶系统的闭环分析和综合极具挑战，而这对安全至关重要，需开发新的建模、分析和综合工具。

Method: 将重点从直接建模AI感知过程转向表征感知误差，用连续时间马尔可夫链和维纳过程分别对误检测和测量噪声建模，提出PEM增强驾驶模型，利用随机微积分建立闭环稳定性，将控制综合方法转化为凸优化问题。

Result: 建立了一类AI驱动自动驾驶系统的闭环稳定性，提出了保证性能的输出反馈控制综合方法，方法能以高效数值解求解。

Conclusion: 所提方法在自适应巡航控制场景中有效且鲁棒，能应对受损和误导的感知。

Abstract: Safety is a critical concern in autonomous vehicle (AV) systems, especially
when AI-based sensing and perception modules are involved. However, due to the
black box nature of AI algorithms, it makes closed-loop analysis and synthesis
particularly challenging, for example, establishing closed-loop stability and
ensuring performance, while they are fundamental to AV safety. To approach this
difficulty, this paper aims to develop new modeling, analysis, and synthesis
tools for AI-based AVs. Inspired by recent developments in perception error
models (PEMs), the focus is shifted from directly modeling AI-based perception
processes to characterizing the perception errors they produce. Two key classes
of AI-induced perception errors are considered: misdetection and measurement
noise. These error patterns are modeled using continuous-time Markov chains and
Wiener processes, respectively. By means of that, a PEM-augmented driving model
is proposed, with which we are able to establish the closed-loop stability for
a class of AI-driven AV systems via stochastic calculus. Furthermore, a
performance-guaranteed output feedback control synthesis method is presented,
which ensures both stability and satisfactory performance. The method is
formulated as a convex optimization problem, allowing for efficient numerical
solutions. The results are then applied to an adaptive cruise control (ACC)
scenario, demonstrating their effectiveness and robustness despite the
corrupted and misleading perception.

</details>


### [453] [Approaches to Analysis and Design of AI-Based Autonomous Vehicles](https://arxiv.org/abs/2509.12169)
*Tao Yan,Zheyu Zhang,Jingjing Jiang,Wen-Hua Chen*

Main category: eess.SY

TL;DR: 本文针对AI驱动的自动驾驶车辆闭环可靠性问题，提出建模、分析和综合工具，研究其闭环特性并应用于实例。


<details>
  <summary>Details</summary>
Motivation: AI反馈闭环对自动驾驶可靠性有风险，因对AI感知过程机制理解有限。

Method: 通过分析误差特性为AI感知过程建模，用Markov链、Gaussian过程和有界扰动分别建模不确定性，基于线性矩阵不等式建立闭环随机稳定性并提出控制综合方法，讨论鲁棒性和性能，研究随机最优有保证成本控制。

Result: 建立闭环随机稳定性，给出鲁棒性测试准则，开发基于LMI和凸优化的设计程序，应用于跟车控制实例并仿真。

Conclusion: 所开发工具能有效处理AI自动驾驶车辆闭环特性问题。

Abstract: Artificial intelligence (AI) models are becoming key components in an
autonomous vehicle (AV), especially in handling complicated perception tasks.
However, closing the loop through AI-based feedback may pose significant risks
on reliability of autonomous driving due to very limited understanding about
the mechanism of AI-driven perception processes. To overcome it, this paper
aims to develop tools for modeling, analysis, and synthesis for a class of
AI-based AV; in particular, their closed-loop properties, e.g., stability,
robustness, and performance, are rigorously studied in the statistical sense.
First, we provide a novel modeling means for the AI-driven perception processes
by looking at their error characteristics. Specifically, three fundamental
AI-induced perception uncertainties are recognized and modeled by Markov
chains, Gaussian processes, and bounded disturbances, respectively. By means of
that, the closed-loop stochastic stability (SS) is established in the sense of
mean square, and then, an SS control synthesis method is presented within the
framework of linear matrix inequalities (LMIs). Besides the SS properties, the
robustness and performance of AI-based AVs are discussed in terms of a
stochastic guaranteed cost, and criteria are given to test the robustness level
of an AV when in the presence of AI-induced uncertainties. Furthermore, the
stochastic optimal guaranteed cost control is investigated, and an efficient
design procedure is developed innovatively based on LMI techniques and convex
optimization. Finally, to illustrate the effectiveness, the developed results
are applied to an example of car following control, along with extensive
simulation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [454] [Unified Distributed Estimation Framework for Sufficient Dimension Reduction Based on Conditional Moments](https://arxiv.org/abs/2509.11455)
*Hongying Li,Minyi Zhu,Yaqi Cao,Xinyi Xu*

Main category: stat.ME

TL;DR: 本文提出分布式数据的充分降维方法，包括切片逆回归精确分布式估计和通用条件矩逆回归统一框架，经模拟和实际数据分析验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式数据面临高维与大样本挑战，需探索分布式数据的充分降维方法。

Method: 提出切片逆回归精确分布式估计；提出通用条件矩逆回归统一框架；进行多种数据生成机制的模拟实验，考虑不同样本分布场景。

Result: 统一框架具有通用性和适用性，通信成本可接受，计算成本大幅降低，算法在极端条件下具鲁棒性，实际数据分析显示算法性能优越。

Conclusion: 所提方法能有效解决分布式数据的充分降维问题，具有良好性能和实际应用价值。

Abstract: Nowadays, massive datasets are typically dispersed across multiple locations,
encountering dual challenges of high dimensionality and huge sample size.
Therefore, it is necessary to explore sufficient dimension reduction (SDR)
methods for distributed data. In this paper, we first propose an exact
distributed estimation of sliced inverse regression, which substantially
improves computational efficiency while obtaining identical estimation as that
on the full sample. Then, we propose a unified distributed framework for
general conditional-moment-based inverse regression methods. This framework
allows for distinct population structure for data distributed at different
locations, thus addressing the issue of heterogeneity. To assess the
effectiveness of our proposed methods, we conduct simulations incorporating
various data generation mechanisms, and examine scenarios where samples are
homogeneous equally, heterogeneous equally, and heterogeneous unequally
scattered across local nodes. Our findings highlight the versatility and
applicability of the unified framework. Meanwhile, the communication cost is
practically acceptable and the computation cost is greatly reduced. Sensitivity
analysis verifies the robustness of the algorithm under extreme conditions
where the SDR method locally fails on some nodes. A real data analysis also
demonstrates the superior performance of the algorithm.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [455] [DOSA: Differentiable Model-Based One-Loop Search for DNN Accelerators](https://arxiv.org/abs/2509.10702)
*Charles Hong,Qijing Huang,Grace Dinh,Mahesh Subedar,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: 介绍DOSA方法同时探索硬件设计空间和映射空间，实验显示其性能优于随机搜索和贝叶斯优化，且有模块化和灵活性。


<details>
  <summary>Details</summary>
Motivation: 以往分别探索硬件设计空间和映射空间导致组合爆炸，给优化器带来困难，需同时优化硬件参数和算法到硬件的映射。

Method: 引入DOSA，包含可微性能模型和基于梯度下降的优化技术来同时探索两个空间。

Result: 在改善DNN模型能量延迟积方面，DOSA比随机搜索和贝叶斯优化分别高2.80倍和12.59倍；通过增强分析模型可优化真实DNN加速器的缓冲区大小和映射，使能量延迟积提高1.82倍。

Conclusion: DOSA在硬件设计空间探索中能有效同时优化硬件参数和映射，性能出色且具有模块化和灵活性。

Abstract: In the hardware design space exploration process, it is critical to optimize
both hardware parameters and algorithm-to-hardware mappings. Previous work has
largely approached this simultaneous optimization problem by separately
exploring the hardware design space and the mapspace - both individually large
and highly nonconvex spaces - independently. The resulting combinatorial
explosion has created significant difficulties for optimizers.
  In this paper, we introduce DOSA, which consists of differentiable
performance models and a gradient descent-based optimization technique to
simultaneously explore both spaces and identify high-performing design points.
Experimental results demonstrate that DOSA outperforms random search and
Bayesian optimization by 2.80x and 12.59x, respectively, in improving DNN model
energy-delay product, given a similar number of samples. We also demonstrate
the modularity and flexibility of DOSA by augmenting our analytical model with
a learned model, allowing us to optimize buffer sizes and mappings of a real
DNN accelerator and attain a 1.82x improvement in energy-delay product.

</details>


### [456] [LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications](https://arxiv.org/abs/2509.12053)
*Yujun Lin,Zhekai Zhang,Song Han*

Main category: cs.AR

TL;DR: 提出LEGO框架用于张量应用，自动生成空间架构设计和可综合RTL代码，相比Gemmini有性能和能效提升，能为多种模型生成架构。


<details>
  <summary>Details</summary>
Motivation: 现代张量应用需灵活加速器架构，现有框架在设计灵活性和RTL生成效率间存在权衡问题。

Method: 前端利用仿射变换架构表示，找到功能单元连接、合成内存系统、融合空间数据流设计；后端将硬件转换为图进行低级优化，用线性规划算法插入流水线寄存器和减少逻辑开销。

Result: 相比Gemmini，速度提升3.2倍，能效提升2.4倍，能为多种生成式AI应用的基础模型生成一种架构。

Conclusion: LEGO框架有效解决了现有框架的问题，在性能和通用性上表现出色。

Abstract: Modern tensor applications, especially foundation models and generative AI
applications require multiple input modalities (both vision and language),
which increases the demand for flexible accelerator architecture. Existing
frameworks suffer from the trade-off between design flexibility and
productivity of RTL generation: either limited to very few hand-written
templates or cannot automatically generate the RTL. To address this challenge,
we propose the LEGO framework, which targets tensor applications and
automatically generates spatial architecture design and outputs synthesizable
RTL code without handwritten RTL design templates. Leveraging the
affine-transformation-based architecture representation, LEGO front end finds
interconnections between function units, synthesizes the memory system, and
fuses different spatial dataflow designs based on data reuse analysis. LEGO
back end then translates the hardware in a primitive-level graph to perform
lower-level optimizations, and applies a set of linear-programming algorithms
to optimally insert pipeline registers and reduce the overhead of unused logic
when switching spatial dataflows. Our evaluation demonstrates that LEGO can
achieve 3.2x speedup and 2.4x energy efficiency compared to previous work
Gemmini, and can generate one architecture for diverse modern foundation models
in generative AI applications.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [457] [Choice Paralysis in Evolutionary Games](https://arxiv.org/abs/2509.10567)
*Brendon G. Anderson*

Main category: econ.TH

TL;DR: 本文研究无限策略进化博弈的有限策略近似，证明其在有限时间区间收敛，识别并形式化新特征，展示选择麻痹的影响。


<details>
  <summary>Details</summary>
Motivation: 研究无限策略进化博弈的有限策略近似情况。

Method: 在温和正则条件下证明有限策略近似在有限时间区间收敛到真实动态，识别并形式化选择流动性和选择麻痹特征。

Result: 证明有限策略近似在有限时间区间收敛，发现选择流动性是有限策略近似长期极限行为与无限策略博弈一致的关键充分条件，构造示例展示选择麻痹影响。

Conclusion: 有限策略近似在一定条件下可用于研究无限策略进化博弈，选择流动性和选择麻痹特征对博弈结果有重要影响。

Abstract: In this paper, we consider finite-strategy approximations of
infinite-strategy evolutionary games. We prove that such approximations
converge to the true dynamics over finite-time intervals, under mild regularity
conditions which are satisfied by classical examples, e.g., the replicator
dynamics. We identify and formalize novel characteristics in evolutionary
games: choice mobility, and its complement choice paralysis. Choice mobility is
shown to be a key sufficient condition for the long-time limiting behavior of
finite-strategy approximations to coincide with that of the true
infinite-strategy game. An illustrative example is constructed to showcase how
choice paralysis may lead to the infinite-strategy game getting "stuck," even
though every finite approximation converges to equilibrium.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [458] [What is in a Price? Estimating Willingness-to-Pay with Bayesian Hierarchical Models](https://arxiv.org/abs/2509.11089)
*Srijesh Pillai,Rajesh Kumar Chandrawat*

Main category: stat.AP

TL;DR: 本文提出用贝叶斯分层联合分析方法解构高端消费品价格，以iPhone为例，结果显示模型能从数据中恢复特征估值，为产品设计和定价提供框架。


<details>
  <summary>Details</summary>
Motivation: 理解高端消费品特征的感知货币价值，为产品设计和定价策略提供依据。

Method: 采用贝叶斯分层联合分析，模拟基于选择的联合调查，开发贝叶斯分层Logit模型推断消费者偏好。

Result: 模型成功从嘈杂数据中恢复真实的特征估值，给出每个特征美元价值的后验概率分布。

Conclusion: 该研究为数据驱动的产品设计和定价策略提供实用框架，助力企业做出更明智决策。

Abstract: For premium consumer products, pricing strategy is not about a single number,
but about understanding the perceived monetary value of the features that
justify a higher cost. This paper proposes a robust methodology to deconstruct
a product's price into the tangible value of its constituent parts. We employ
Bayesian Hierarchical Conjoint Analysis, a sophisticated statistical technique,
to solve this high-stakes business problem using the Apple iPhone as a
universally recognizable case study. We first simulate a realistic choice based
conjoint survey where consumers choose between different hypothetical iPhone
configurations. We then develop a Bayesian Hierarchical Logit Model to infer
consumer preferences from this choice data. The core innovation of our model is
its ability to directly estimate the Willingness-to-Pay (WTP) in dollars for
specific feature upgrades, such as a "Pro" camera system or increased storage.
Our results demonstrate that the model successfully recovers the true,
underlying feature valuations from noisy data, providing not just a point
estimate but a full posterior probability distribution for the dollar value of
each feature. This work provides a powerful, practical framework for
data-driven product design and pricing strategy, enabling businesses to make
more intelligent decisions about which features to build and how to price them.

</details>


### [459] [A comparison between geostatistical and machine learning models for spatio-temporal prediction of PM2.5 data](https://arxiv.org/abs/2509.12051)
*Zeinab Mohamed,Wenlong Gong*

Main category: stat.AP

TL;DR: 本文利用PurpleAir传感器数据评估不同模型绘制加州每小时PM₂.₅地图的效果，用集成模型校正数据偏差提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统空气质量监测系统数据时空覆盖有限，需评估不同模型绘制准确PM₂.₅地图的效果。

Method: 利用PurpleAir传感器数据，评估传统地统计方法（克里金法、土地利用回归）、先进机器学习方法（神经网络、随机森林、支持向量机）及集成模型。

Result: 通过集成模型校正PurpleAir数据偏差，提高了PM₂.₅浓度预测准确性。

Conclusion: 集成模型结合时空依赖和机器学习模型，可有效提升PM₂.₅浓度预测准确性。

Abstract: Ambient air pollution poses significant health and environmental challenges.
Exposure to high concentrations of PM$_{2.5}$ have been linked to increased
respiratory and cardiovascular hospital admissions, more emergency department
visits and deaths. Traditional air quality monitoring systems such as
EPA-certified stations provide limited spatial and temporal data. The advent of
low-cost sensors has dramatically improved the granularity of air quality data,
enabling real-time, high-resolution monitoring. This study exploits the
extensive data from PurpleAir sensors to assess and compare the effectiveness
of various statistical and machine learning models in producing accurate hourly
PM$_{2.5}$ maps across California. We evaluate traditional geostatistical
methods, including kriging and land use regression, against advanced machine
learning approaches such as neural networks, random forests, and support vector
machines, as well as ensemble model. Our findings enhanced the predictive
accuracy of PM2.5 concentration by correcting the bias in PurpleAir data with
an ensemble model, which incorporating both spatiotemporal dependencies and
machine learning models.

</details>


### [460] [Wavelet-SARIMA-Transformer: A Hybrid Model for Rainfall Forecasting](https://arxiv.org/abs/2509.11903)
*Junmoni Saikia,Kuldeep Goswami,Sarat C. Kakaty*

Main category: stat.AP

TL;DR: 本文提出WST框架用于印度东北部降雨预测，Haar基混合模型WHST效果优，证明多分辨率信号分解与线性和深度学习模型结合有效，框架可用于复杂环境时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 开发有效方法对印度东北部五个气象分区的月降雨量进行预测，为洪水风险管理、水资源规划和气候适应策略提供支持。

Method: 采用MODWT对降雨序列进行分解，用SARIMA建模线性和季节性成分，Transformer网络建模非线性成分，通过逆MODWT重建预测。

Result: Haar基混合模型WHST在各分区预测误差低、与观测降雨一致性强、预测无偏，Ljung Box测试和Taylor图验证了方法的鲁棒性。

Conclusion: 多分辨率信号分解与互补线性和深度学习模型结合用于水文气候预测有效，WST框架可用于复杂环境时间序列预测。

Abstract: This study develops and evaluates a novel hybridWavelet SARIMA Transformer,
WST framework to forecast using monthly rainfall across five meteorological
subdivisions of Northeast India over the 1971 to 2023 period. The approach
employs the Maximal Overlap Discrete Wavelet Transform, MODWT with four wavelet
families such as, Haar, Daubechies, Symlet, Coiflet etc. to achieve shift
invariant, multiresolution decomposition of the rainfall series. Linear and
seasonal components are modeled using Seasonal ARIMA, SARIMA, while nonlinear
components are modeled by a Transformer network, and forecasts are
reconstructed via inverse MODWT. Comprehensive validation using an 80 is to 20
train test split and multiple performance indices such as, RMSE, MAE, SMAPE,
Willmotts d, Skill Score, Percent Bias, Explained Variance, and Legates McCabes
E1 demonstrates the superiority of the Haar-based hybrid model, WHST. Across
all subdivisions, WHST consistently achieved lower forecast errors, stronger
agreement with observed rainfall, and unbiased predictions compared with stand
alone SARIMA, stand-alone Transformer, and two-stage wavelet hybrids. Residual
adequacy was confirmed through the Ljung Box test, while Taylor diagrams
provided an integrated assessment of correlation, variance fidelity, and RMSE,
further reinforcing the robustness of the proposed approach. The results
highlight the effectiveness of integrating multiresolution signal decomposition
with complementary linear and deep learning models for hydroclimatic
forecasting. Beyond rainfall, the proposed WST framework offers a scalable
methodology for forecasting complex environmental time series, with direct
implications for flood risk management, water resources planning, and climate
adaptation strategies in data-sparse and climate-sensitive regions.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [461] [A Computational Framework and Implementation of Implicit Priors in Bayesian Inverse Problems](https://arxiv.org/abs/2509.11781)
*Jasper M. Everink,Chao Zhang,Amal M. A. Alghamdi,Rémi Laumont,Nicolai A. B. Riis,Jakob S. Jørgensen*

Main category: cs.MS

TL;DR: 本文提出隐式先验计算框架，在CUQIpy包中实现多种隐式先验，并应用于不同逆问题。


<details>
  <summary>Details</summary>
Motivation: 隐式先验在贝叶斯逆问题中的概念尚未系统探索，且缺乏对不同隐式先验的统一工作。

Method: 提出隐式先验计算框架，在CUQIpy Python包中实现多种隐式先验。

Result: 实现的隐式先验技术可应用于从图像处理到偏微分方程参数估计等多种逆问题。

Conclusion: 为贝叶斯逆问题中的隐式先验提供了计算框架和实现方法，展示了其在不同逆问题中的应用。

Abstract: Solving Bayesian inverse problems typically involves deriving a posterior
distribution using Bayes' rule, followed by sampling from this posterior for
analysis. Sampling methods, such as general-purpose Markov chain Monte Carlo
(MCMC), are commonly used, but they require prior and likelihood densities to
be explicitly provided. In cases where expressing the prior explicitly is
challenging, implicit priors offer an alternative, encoding prior information
indirectly. These priors have gained increased interest in recent years, with
methods like Plug-and-Play (PnP) priors and Regularized Linear
Randomize-then-Optimize (RLRTO) providing computationally efficient
alternatives to standard MCMC algorithms. However, the abstract concept of
implicit priors for Bayesian inverse problems is yet to be systematically
explored and little effort has been made to unify different kinds of implicit
priors. This paper presents a computational framework for implicit priors and
their distinction from explicit priors. We also introduce an implementation of
various implicit priors within the CUQIpy Python package for Computational
Uncertainty Quantification in Inverse Problems. Using this implementation, we
showcase several implicit prior techniques by applying them to a variety of
different inverse problems from image processing to parameter estimation in
partial differential equations.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [462] [Assessing the Limits of Graph Neural Networks for Vapor-Liquid Equilibrium Prediction: A Cryogenic Mixture Case Study](https://arxiv.org/abs/2509.10565)
*Aryan Gupta*

Main category: physics.chem-ph

TL;DR: 研究探讨结构感知图神经网络能否替代状态方程用于低温混合物汽液平衡计算，结果显示当前模型不适合求解汽液平衡且无运行时间优势，价值在于方法层面。


<details>
  <summary>Details</summary>
Motivation: 需要准确快速的热物理模型将汽液平衡计算嵌入低温混合物的设计、优化和控制循环，探究结构感知图神经网络能否作为状态方程的实用替代。

Method: 生成90 - 200K、压力至100 bar的三元数据集，用15%密度过滤器处理，将每个状态与轻量级分子动力学快照配对提供结构特征，分两阶段训练模型并通过多种方式评估。

Result: 图神经网络在其范围内能较好插值单相性质，但汽液平衡驱动不接受测试二元体系的平衡点，诊断显示导数平滑性/一致性不足，端到端计时对比无单相速度优势。

Conclusion: 当前配置下，替代模型不适用于汽液平衡求解且无运行时间优势，其价值在于方法层面，指出失败模式及补救措施。

Abstract: Accurate and fast thermophysical models are needed to embed vapor-liquid
equilibrium (VLE) calculations in design, optimization, and control loops for
cryogenic mixtures. This study asks whether a structure-aware graph neural
network (GNN; DimeNet++) trained on GERG-2008/CoolProp data can act as a
practical surrogate for an equation of state (EoS). We generate a ternary
dataset over 90-200 K and pressures to 100 bar, curate it with a 15% density
filter (reducing 5,200 states to 1,516), and pair each state with a lightweight
molecular-dynamics snapshot to supply structural features. The model is trained
in two stages; pretraining on residual Helmholtz energy followed by pressure
fine-tuning with a stability penalty; and evaluated via single-phase
interpolation tests, solver-free derivative-quality diagnostics, an audited VLE
driver, and a latency benchmark. Within its regime, the GNN interpolates
single-phase properties reasonably well; however, the VLE driver accepts no GNN
equilibria on tested binaries (all plotted VLE points are CoolProp fallback or
the solver fails), and diagnostic probes reveal jagged P(V|T) paths and
thermal-stability flags concentrated in dense/cold regions, indicating
insufficient derivative smoothness/consistency for robust equilibrium solving.
An end-to-end timing comparison shows no single-phase speed advantage relative
to CoolProp (tens of milliseconds vs sub-millisecond). We conclude that, as
configured, the surrogate in this study is not solver-ready for VLE and offers
no runtime benefit; its value is methodological, delineating failure modes and
pointing to remedies such as physics-informed training signals and targeted
coverage near phase boundaries.

</details>


### [463] [Predictive Free Energy Simulations Through Hierarchical Distillation of Quantum Hamiltonians](https://arxiv.org/abs/2509.10967)
*Chenghan Li,Garnet Kin-Lic Chan*

Main category: physics.chem-ph

TL;DR: 提出分层机器学习框架用于计算凝聚相化学反应自由能，通过实例验证其准确性。


<details>
  <summary>Details</summary>
Motivation: 高级量子力学方法计算凝聚相化学反应自由能在计算上难以实现。

Method: 引入分层机器学习框架，将少量高保真量子计算知识提炼到粗粒度的机器学习量子哈密顿量中，保留显式电子自由度以实现量子和经典自由度的嵌入。

Result: 从第一性原理计算弱酸质子解离常数和酶促反应动力学速率，结果与实验测量值在化学精度或其不确定性范围内相符。

Conclusion: 展示了以最高精度和收敛统计进行凝聚相反应自由能模拟的途径。

Abstract: Obtaining the free energies of condensed phase chemical reactions remains
computationally prohibitive for high-level quantum mechanical methods. We
introduce a hierarchical machine learning framework that bridges this gap by
distilling knowledge from a small number of high-fidelity quantum calculations
into increasingly coarse-grained, machine-learned quantum Hamiltonians. By
retaining explicit electronic degrees of freedom, our approach further enables
a faithful embedding of quantum and classical degrees of freedom that captures
long-range electrostatics and the quantum response to a classical environment
to infinite order. As validation, we compute the proton dissociation constants
of weak acids and the kinetic rate of an enzymatic reaction entirely from first
principles, reproducing experimental measurements within chemical accuracy or
their uncertainties. Our work demonstrates a path to condensed phase
simulations of reaction free energies at the highest levels of accuracy with
converged statistics.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [464] [Crystal Systems Classification of Phosphate-Based Cathode Materials Using Machine Learning for Lithium-Ion Battery](https://arxiv.org/abs/2509.10532)
*Yogesh Yadav,Sandeep K Yadav,Vivek Vijay,Ambesh Dixit*

Main category: cond-mat.mtrl-sci

TL;DR: 本文运用机器学习分类算法预测锂离子电池磷酸阴极晶体系统，集成学习算法表现佳，确定关键特征，预测材料可作潜在阴极材料。


<details>
  <summary>Details</summary>
Motivation: 正确预测晶体系统对评估电池阴极性能至关重要，需有效方法预测。

Method: 运用机器学习分类算法，从材料项目提取数据，进行特征评估和蒙特卡罗交叉验证测试，用顺序前向选择确定关键特征。

Result: 集成机器学习算法如随机森林、极度随机树和梯度提升机预测能力最佳，确定体积、带隙和位点为关键特征，预测有较高准确性和稳定性。

Conclusion: 预测材料可作为锂离子电池的潜在阴极材料。

Abstract: The physical and chemical characteristics of cathodes used in batteries are
derived from the lithium-ion phosphate cathodes crystalline arrangement, which
is pivotal to the overall battery performance. Therefore, the correct
prediction of the crystal system is essential to estimate the properties of
cathodes. This study applies machine learning classification algorithms for
predicting the crystal systems, namely monoclinic, orthorhombic, and triclinic,
related to Li P (Mn, Fe, Co, Ni, V) O based Phosphate cathodes. The data used
in this work is extracted from the Materials Project. Feature evaluation showed
that cathode properties depend on the crystal structure, and optimized
classification strategies lead to better predictability. Ensemble machine
learning algorithms such as Random Forest, Extremely Randomized Trees, and
Gradient Boosting Machines have demonstrated the best predictive capabilities
for crystal systems in the Monte Carlo cross-validation test. Additionally,
sequential forward selection (SFS) is performed to identify the most critical
features influencing the prediction accuracy for different machine learning
models, with Volume, Band gap, and Sites as input features ensemble machine
learning algorithms such as Random Forest (80.69%), Extremely Randomized Tree
(78.96%), and Gradient Boosting Machine (80.40%) approaches lead to the maximum
accuracy towards crystallographic classification with stability and the
predicted materials can be the potential cathode materials for lithium ion
batteries.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [465] [Acoustic Overspecification in Electronic Dance Music Taxonomy](https://arxiv.org/abs/2509.11474)
*Weilun Xu,Tianhao Dai,Oscar Goudet,Xiaoxuan Wang*

Main category: cs.SD

TL;DR: 提出无监督方法探索EDM自然声学结构，发现当前EDM分类法存在约三分之一过度指定问题。


<details>
  <summary>Details</summary>
Motivation: 当前EDM分类依赖行业定义的分类法，声学基础不明，且现有方法未系统评估预设流派标签的有效性。

Method: 结合基于节奏图的新特征和多标准特征选择的无监督方法，并与MERT和CLAP等预训练音频嵌入进行对比。

Result: 特征空间和嵌入表示均收敛到19 - 23个自然声学族，而预设为35个。

Conclusion: 当前EDM分类法存在约三分之一的过度指定问题。

Abstract: Electronic Dance Music (EDM) classification typically relies on
industry-defined taxonomies with numerous subgenres, yet the acoustic basis for
these distinctions remains unclear. Current approaches use supervised learning
with prescribed genre labels, assuming their validity without systematic
evaluation. In this paper, we propose an unsupervised approach to discover the
natural acoustic structure of EDM independent of commercial labels. Our method
combines novel tempogram-based features capturing EDM's layered rhythmic
patterns with multi-criteria feature selection. To validate that our findings
reflect genuine acoustic structure rather than methodological artifacts, we
compare our results against state-of-the-art pre-trained audio embeddings (MERT
and CLAP). Both our feature space and embedding representations converge to
19-23 natural acoustic families compared to the prescribed 35, providing
consistent evidence of significant overspecification in current EDM taxonomy by
approximately one-third.

</details>


### [466] [Spectral and Rhythm Features for Audio Classification with Deep Convolutional Neural Networks](https://arxiv.org/abs/2410.06927)
*Friedrich Wolf-Monheim*

Main category: cs.SD

TL;DR: 研究CNN在音频分类中对不同频谱和节奏特征的表现，发现mel - scaled spectrograms和MFCCs效果更佳。


<details>
  <summary>Details</summary>
Motivation: 探究CNN用于音频分类时不同频谱和节奏特征表示的性能。

Method: 使用深度卷积神经网络，借助ESC - 50数据集的2000条标记环境音频记录进行实验。

Result: mel - scaled spectrograms和MFCCs在使用深度CNN的音频分类任务中表现明显优于其他特征。

Conclusion: mel - scaled spectrograms和MFCCs更适合用于基于深度CNN的音频分类任务。

Abstract: Convolutional neural networks (CNNs) are widely used in computer vision. They
can be used not only for conventional digital image material to recognize
patterns, but also for feature extraction from digital imagery representing
spectral and rhythm features extracted from time-domain digital audio signals
for the acoustic classification of sounds. Different spectral and rhythm
feature representations like mel-scaled spectrograms, mel-frequency cepstral
coefficients (MFCCs), cyclic tempograms, short-time Fourier transform (STFT)
chromagrams, constant-Q transform (CQT) chromagrams and chroma energy
normalized statistics (CENS) chromagrams are investigated in terms of the audio
classification performance using a deep convolutional neural network. It can be
clearly shown that the mel-scaled spectrograms and the mel-frequency cepstral
coefficients (MFCCs) perform significantly better than the other spectral and
rhythm features investigated in this research for audio classification tasks
using deep CNNs. The experiments were carried out with the aid of the ESC-50
dataset with 2,000 labeled environmental audio recordings.

</details>


### [467] [ENJ: Optimizing Noise with Genetic Algorithms to Jailbreak LSMs](https://arxiv.org/abs/2509.11128)
*Yibo Zhang,Liang Lin*

Main category: cs.SD

TL;DR: 提出Evolutionary Noise Jailbreak (ENJ)方法攻击大语音模型，实验表明其攻击效果优于基线方法，揭示噪声在语音安全中的双重作用。


<details>
  <summary>Details</summary>
Motivation: 大语音模型广泛应用，安全风险凸显，传统语音对抗攻击方法难平衡有效性和隐蔽性。

Method: 利用遗传算法将环境噪声转化为可优化的攻击载体，通过种群初始化、交叉融合和概率变异迭代生成融合恶意指令与背景噪声的音频样本。

Result: 在多个主流语音模型上的大量实验显示，ENJ攻击效果显著优于现有基线方法。

Conclusion: 揭示噪声在语音安全中的双重作用，为复杂声学环境下的模型安全防御提供新见解。

Abstract: The widespread application of Large Speech Models (LSMs) has made their
security risks increasingly prominent. Traditional speech adversarial attack
methods face challenges in balancing effectiveness and stealth. This paper
proposes Evolutionary Noise Jailbreak (ENJ), which utilizes a genetic algorithm
to transform environmental noise from a passive interference into an actively
optimizable attack carrier for jailbreaking LSMs. Through operations such as
population initialization, crossover fusion, and probabilistic mutation, this
method iteratively evolves a series of audio samples that fuse malicious
instructions with background noise. These samples sound like harmless noise to
humans but can induce the model to parse and execute harmful commands.
Extensive experiments on multiple mainstream speech models show that ENJ's
attack effectiveness is significantly superior to existing baseline methods.
This research reveals the dual role of noise in speech security and provides
new critical insights for model security defense in complex acoustic
environments.

</details>


### [468] [An Entropy-Guided Curriculum Learning Strategy for Data-Efficient Acoustic Scene Classification under Domain Shift](https://arxiv.org/abs/2509.11168)
*Peihong Zhang,Yuxuan Liu,Zhixin Li,Rui Sang,Yiqiang Cai,Yizhou Tan,Shengchen Li*

Main category: cs.SD

TL;DR: 提出熵引导课程学习策略解决声学场景分类中设备泛化问题，实验证明策略有效。


<details>
  <summary>Details</summary>
Motivation: 声学场景分类在跨设备泛化有挑战，尤其在标签数据有限时，现有技术有局限，优化训练策略是较少探索的方向。

Method: 提出熵引导课程学习策略，计算样本设备后验概率的香农熵量化不确定性，从高熵样本开始学习。

Result: 在多个DCASE 2024 ASC基线实验中，策略有效缓解领域偏移，尤其在标签数据有限时。

Conclusion: 策略与架构无关、无额外推理成本，可集成到现有基线，是解决领域偏移的实用方案。

Abstract: Acoustic Scene Classification (ASC) faces challenges in generalizing across
recording devices, particularly when labeled data is limited. The DCASE 2024
Challenge Task 1 highlights this issue by requiring models to learn from small
labeled subsets recorded on a few devices. These models need to then generalize
to recordings from previously unseen devices under strict complexity
constraints. While techniques such as data augmentation and the use of
pre-trained models are well-established for improving model generalization,
optimizing the training strategy represents a complementary yet less-explored
path that introduces no additional architectural complexity or inference
overhead. Among various training strategies, curriculum learning offers a
promising paradigm by structuring the learning process from easier to harder
examples. In this work, we propose an entropy-guided curriculum learning
strategy to address the domain shift problem in data-efficient ASC.
Specifically, we quantify the uncertainty of device domain predictions for each
training sample by computing the Shannon entropy of the device posterior
probabilities estimated by an auxiliary domain classifier. Using entropy as a
proxy for domain invariance, the curriculum begins with high-entropy samples
and gradually incorporates low-entropy, domain-specific ones to facilitate the
learning of generalizable representations. Experimental results on multiple
DCASE 2024 ASC baselines demonstrate that our strategy effectively mitigates
domain shift, particularly under limited labeled data conditions. Our strategy
is architecture-agnostic and introduces no additional inference cost, making it
easily integrable into existing ASC baselines and offering a practical solution
to domain shift.

</details>


### [469] [FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs](https://arxiv.org/abs/2509.11425)
*Md Mubtasim Ahasan,Rafat Hasan Khan,Tasnim Mohiuddin,Aman Chadha,Tariq Iqbal,M Ashraful Amin,Amin Ahsan Ali,Md Mofijul Islam,A K M Mahbubur Rahman*

Main category: cs.SD

TL;DR: 提出FuseCodec统一声学、语义和上下文表示，有三项技术，在LibriSpeech表现佳，还推出FuseCodec - TTS用于零样本语音合成。


<details>
  <summary>Details</summary>
Motivation: 现有神经编解码器忽略语音语义和上下文线索，且语义和上下文表示的对齐与统一存在挑战。

Method: 提出Latent Representation Fusion、Global Semantic - Contextual Supervision、Temporally Aligned Contextual Supervision三项技术；推出FuseCodec - TTS用于零样本语音合成。

Result: FuseCodec在LibriSpeech中超越EnCodec、SpeechTokenizer和DAC，在转录准确性、感知质量、可懂度和说话人相似度方面达到了最先进水平。

Conclusion: 上下文和语义引导的标记化对语音标记化和下游任务有效。

Abstract: Speech tokenization enables discrete representation and facilitates speech
language modeling. However, existing neural codecs capture low-level acoustic
features, overlooking the semantic and contextual cues inherent to human
speech. While recent efforts introduced semantic representations from
self-supervised speech models or incorporated contextual representations from
pre-trained language models, challenges remain in aligning and unifying the
semantic and contextual representations. We introduce FuseCodec, which unifies
acoustic, semantic, and contextual representations through strong cross-modal
alignment and globally informed supervision. We propose three complementary
techniques: (i) Latent Representation Fusion, integrating semantic and
contextual features directly into the encoder latent space for robust and
unified representation learning; (ii) Global Semantic-Contextual Supervision,
supervising discrete tokens with globally pooled and broadcasted
representations to enhance temporal consistency and cross-modal alignment; and
(iii) Temporally Aligned Contextual Supervision, strengthening alignment by
dynamically matching contextual and speech tokens within a local window for
fine-grained token-level supervision. We further introduce FuseCodec-TTS,
demonstrating our methodology's applicability to zero-shot speech synthesis.
Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,
surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,
perceptual quality, intelligibility, and speaker similarity. Results highlight
the effectiveness of contextually and semantically guided tokenization for
speech tokenization and downstream tasks. Code and pretrained models are
available at https://github.com/mubtasimahasan/FuseCodec.

</details>


### [470] [Revisiting Meter Tracking in Carnatic Music using Deep Learning Approaches](https://arxiv.org/abs/2509.11241)
*Satyajeet Prabhu*

Main category: cs.SD

TL;DR: 评估TCN和Beat This!在卡纳蒂克音乐节拍跟踪中的表现，发现通过迁移学习可使模型性能提升，适用于小众音乐传统。


<details>
  <summary>Details</summary>
Motivation: 现有节拍跟踪深度学习模型在小众音乐传统中表现不佳，卡纳蒂克音乐的节拍跟踪SOTA深度学习模型表现未被充分探索。

Method: 在CMR$_f$数据集上复制DBN基线实验设置，评估TCN和Beat This!模型，还研究微调模型和使用音乐相关参数等适应策略。

Result: 现成模型不总能超越DBN，但迁移学习后性能大幅提升，可匹配或超越基线。

Conclusion: SOTA深度学习模型可有效适应小众音乐传统，为更具包容性和广泛适用性的节拍跟踪系统铺平道路。

Abstract: Beat and downbeat tracking, jointly referred to as Meter Tracking, is a
fundamental task in Music Information Retrieval (MIR). Deep learning models
have far surpassed traditional signal processing and classical machine learning
approaches in this domain, particularly for Western (Eurogenetic) genres, where
large annotated datasets are widely available. These systems, however, perform
less reliably on underrepresented musical traditions. Carnatic music, a rich
tradition from the Indian subcontinent, is renowned for its rhythmic intricacy
and unique metrical structures (t\=alas). The most notable prior work on meter
tracking in this context employed probabilistic Dynamic Bayesian Networks
(DBNs). The performance of state-of-the-art (SOTA) deep learning models on
Carnatic music, however, remains largely unexplored.
  In this study, we evaluate two models for meter tracking in Carnatic music:
the Temporal Convolutional Network (TCN), a lightweight architecture that has
been successfully adapted for Latin rhythms, and Beat This!, a
transformer-based model designed for broad stylistic coverage without the need
for post-processing. Replicating the experimental setup of the DBN baseline on
the Carnatic Music Rhythm (CMR$_f$) dataset, we systematically assess the
performance of these models in a directly comparable setting. We further
investigate adaptation strategies, including fine-tuning the models on Carnatic
data and the use of musically informed parameters. Results show that while
off-the-shelf models do not always outperform the DBN, their performance
improves substantially with transfer learning, matching or surpassing the
baseline. These findings indicate that SOTA deep learning models can be
effectively adapted to underrepresented traditions, paving the way for more
inclusive and broadly applicable meter tracking systems.

</details>


### [471] [Scaling to Multimodal and Multichannel Heart Sound Classification: Fine-Tuning Wav2Vec 2.0 with Synthetic and Augmented Biosignals](https://arxiv.org/abs/2509.11606)
*Milan Marocchi,Matthew Fynn,Kayapanda Mandana,Yue Rong*

Main category: cs.SD

TL;DR: 本文结合传统信号处理与去噪扩散模型创建增强数据集，微调基于Wav2Vec 2.0的分类器用于心血管疾病检测，取得了先进性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，早期检测需求大，但现有深度学习架构因数据集受限未充分利用，需解决数据问题。

Method: 结合传统信号处理与去噪扩散模型WaveGrad和DiffWave创建增强数据集，微调基于Wav2Vec 2.0的分类器。

Result: 在不同数据集上取得了较高的准确率、召回率等指标，如在CinC 2016单通道PCG数据集上准确率达92.48%等。

Conclusion: 增强数据集支持下，基于Transformer的模型用于心血管疾病检测有效，有推进多模态和多通道心音分类的潜力。

Abstract: Cardiovascular diseases (CVDs) are the leading cause of death worldwide,
accounting for approximately 17.9 million deaths each year. Early detection is
critical, creating a demand for accurate and inexpensive pre-screening methods.
Deep learning has recently been applied to classify abnormal heart sounds
indicative of CVDs using synchronised phonocardiogram (PCG) and
electrocardiogram (ECG) signals, as well as multichannel PCG (mPCG). However,
state-of-the-art architectures remain underutilised due to the limited
availability of synchronised and multichannel datasets. Augmented datasets and
pre-trained models provide a pathway to overcome these limitations, enabling
transformer-based architectures to be trained effectively. This work combines
traditional signal processing with denoising diffusion models, WaveGrad and
DiffWave, to create an augmented dataset to fine-tune a Wav2Vec 2.0-based
classifier on multimodal and multichannel heart sound datasets. The approach
achieves state-of-the-art performance. On the Computing in Cardiology (CinC)
2016 dataset of single channel PCG, accuracy, unweighted average recall (UAR),
sensitivity, specificity and Matthew's correlation coefficient (MCC) reach
92.48\%, 93.05\%, 93.63\%, 92.48\%, 94.93\% and 0.8283, respectively. Using the
synchronised PCG and ECG signals of the training-a dataset from CinC, 93.14\%,
92.21\%, 94.35\%, 90.10\%, 95.12\% and 0.8380 are achieved for accuracy, UAR,
sensitivity, specificity and MCC, respectively. Using a wearable vest dataset
consisting of mPCG data, the model achieves 77.13\% accuracy, 74.25\% UAR,
86.47\% sensitivity, 62.04\% specificity, and 0.5082 MCC. These results
demonstrate the effectiveness of transformer-based models for CVD detection
when supported by augmented datasets, highlighting their potential to advance
multimodal and multichannel heart sound classification.

</details>


### [472] [Neural Audio Codecs for Prompt-Driven Universal Source Separation](https://arxiv.org/abs/2509.11717)
*Adhiraj Banerjee,Vipul Arora*

Main category: cs.SD

TL;DR: 介绍基于神经音频编解码器的CodecSep模型用于设备端通用文本驱动分离，性能优且计算量小。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导源分离模型计算量大难用于边缘部署，神经音频编解码器模型只能固定类别分离，需新模型。

Method: CodecSep结合DAC压缩与由CLAP衍生FiLM参数调制的Transformer掩码器。

Result: 在六项开放域基准测试中，分离保真度超AudioSep，感知质量有竞争力，与固定基线相当或更优；代码流部署计算量仅1.35 GMACs，比AudioSep小很多且位流兼容。

Conclusion: CodecSep是有效设备端通用文本驱动分离模型，计算高效。

Abstract: Text-guided source separation supports flexible audio editing across media
and assistive applications, but existing models like AudioSep are too
compute-heavy for edge deployment. Neural audio codec (NAC) models such as
CodecFormer and SDCodec are compute-efficient but limited to fixed-class
separation. We introduce CodecSep, the first NAC-based model for on-device
universal, text-driven separation. CodecSep combines DAC compression with a
Transformer masker modulated by CLAP-derived FiLM parameters. Across six
open-domain benchmarks under matched training/prompt protocols,
\textbf{CodecSep} surpasses \textbf{AudioSep} in separation fidelity (SI-SDR)
while remaining competitive in perceptual quality (ViSQOL) and matching or
exceeding fixed-stem baselines (TDANet, CodecFormer, SDCodec). In code-stream
deployments, it needs just 1.35~GMACs end-to-end -- approximately $54\times$
less compute ($25\times$ architecture-only) than spectrogram-domain separators
like AudioSep -- while remaining fully bitstream-compatible.

</details>


### [473] [Improving Out-of-Domain Audio Deepfake Detection via Layer Selection and Fusion of SSL-Based Countermeasures](https://arxiv.org/abs/2509.12003)
*Pierre Serrano,Raphaël Duroselle,Florian Angulo,Jean-François Bonastre,Olivier Boeffard*

Main category: cs.SD

TL;DR: 研究基于预训练SSL编码器的音频深度伪造检测系统，发现选最佳层效果好且可减参，编码器预训练策略有影响，多编码器分数级融合可提升泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有基于冻结预训练SSL编码器的音频深度伪造检测系统在跨领域泛化上存在问题。

Method: 研究六种不同预训练SSL在四个测试语料库上的表现，进行逐层分析，比较单一层策略和MHFA自动选择策略，最后进行多编码器分数级融合。

Result: 选择最佳层效果好，可减少达80%系统参数，不同测试语料库和SSL模型性能差异大，多编码器分数级融合提升泛化性。

Conclusion: 编码器预训练策略影响性能，多编码器分数级融合有助于系统泛化到跨领域攻击。

Abstract: Audio deepfake detection systems based on frozen pre-trained self-supervised
learning (SSL) encoders show a high level of performance when combined with
layer-weighted pooling methods, such as multi-head factorized attentive pooling
(MHFA). However, they still struggle to generalize to out-of-domain (OOD)
conditions. We tackle this problem by studying the behavior of six different
pre-trained SSLs, on four different test corpora. We perform a layer-by-layer
analysis to determine which layers contribute most. Next, we study the pooling
head, comparing a strategy based on a single layer with automatic selection via
MHFA. We observed that selecting the best layer gave very good results, while
reducing system parameters by up to 80%. A wide variation in performance as a
function of test corpus and SSL model is also observed, showing that the
pre-training strategy of the encoder plays a role. Finally, score-level fusion
of several encoders improved generalization to OOD attacks.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [474] [Requirements for Early Quantum Advantage and Quantum Utility in the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2509.11469)
*Chinonso Onah,Kristel Michielsen*

Main category: quant-ph

TL;DR: 提出判定CVRP何时能实现早期量子优势的框架，分析NISQ硬件上难以实现，给出判定指标，对比不同编码差距，指出CVRP实现量子优势可能需创新分解技术。


<details>
  <summary>Details</summary>
Motivation: 确定CVRP在何时能实现早期量子优势。

Method: 引入透明、编码无关的框架，结合封闭形式资源计数和设备基准测试，给出三个判定指标，对比直接QUBO映射和HOBO编码。

Result: 在NISQ硬件上即使最佳情况也难实现量子优势；HOBO电路所需 qubits 远少于QUBO编码；框架可识别CVRP早期量子优势候选实例。

Conclusion: CVRP实现量子优势可能需要创新的问题分解技术。

Abstract: We introduce a transparent, encoding-agnostic framework for determining when
the Capacitated Vehicle Routing Problem (CVRP) can achieve early quantum
advantage. Our analysis shows this is unlikely on noisy intermediate scale
quantum (NISQ) hardware even in best case scenarios that use the most
qubit-efficient direct encodings. Closed-form resource counts, combined with
recent device benchmarks, yield three decisive go/no-go figures of merit: the
quantum feasibility point and the qubit- and gate-feasibility lines, which
place any CVRP instance on a single decision diagram. Contrasting a direct QUBO
mapping with a space-efficient higher-order (HOBO) encoding reveals a large
gap. Applied to early-advantage benchmarks such as Golden-5, our diagram shows
that HOBO circuits require only 7,685 qubits, whereas comparable QUBO encodings
still exceed 200,000 qubits. In addition to identifying candidate instances for
early quantum advantage in CVRP, the framework provides a unifying go/no-go
metric that ingests any CVRP encoding together with any hardware profile and
highlights when quantum devices could challenge classical heuristics. Quantum
advantage in CVRP would likely require innovative problem decomposition
techniques.

</details>


### [475] [V-ZOR: Enabling Verifiable Cross-Blockchain Communication via Quantum-Driven ZKP Oracle Relays](https://arxiv.org/abs/2509.10996)
*M. Z. Haider,Tayyaba Noreen,M. Salman,M. Dias de Assuncao,Kaiwen Zhang*

Main category: quant-ph

TL;DR: 当前跨链桥和预言机DAO存在诸多安全问题，本文提出V - ZOR可验证预言机中继方案，通过结合零知识证明等技术降低风险，原型取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 解决跨链桥和预言机DAO因信任失败、验证行为不透明和激励机制薄弱等导致的安全问题，以及现有预言机设计易受攻击和数据完整性受威胁的问题。

Method: 提出V - ZOR，集成零知识证明、量子级随机性和跨链重质押，每个预言机数据包包含Halo 2证明，用可审计量子熵重新播种VRF防止委员会操纵，在共享重质押中心质押并支持提交欺诈证明触发惩罚。

Result: 在Sepolia和Scroll的原型实现了低于300k gas验证、单块延迟和串通成本提高10倍。

Conclusion: 结合ZK证明和量子随机重质押能为跨链DeFi构建一个信任最小化、高性能的预言机层。

Abstract: Cross-chain bridges and oracle DAOs represent some of the most vulnerable
components of decentralized systems, with more than $2.8 billion lost due to
trust failures, opaque validation behavior, and weak incentives. Current oracle
designs are based on multisigs, optimistic assumptions, or centralized
aggregation, exposing them to attacks and delays. Moreover, predictable
committee selection enables manipulation, which threatens data integrity across
chains. We propose V-ZOR, a verifiable oracle relay that integrates
zero-knowledge proofs, quantum-grade randomness, and cross-chain restaking to
mitigate these risks. Each oracle packet includes a Halo 2 proof verifying that
the reported data was correctly aggregated using a deterministic median. To
prevent committee manipulation, VZOR reseeds its VRF using auditable quantum
entropy, ensuring unpredictable and secure selection of reporters. Reporters
stake once on a shared restaking hub; any connected chain can submit a fraud
proof to trigger slashing, removing the need for multisigs or optimistic
assumptions. A prototype in Sepolia and Scroll achieves sub-300k gas
verification, one-block latency, and a 10x increase in collusion cost. V-ZOR
demonstrates that combining ZK attestation with quantum-randomized restaking
enables a trust-minimized, high-performance oracle layer for cross-chain DeFi.

</details>


### [476] [Investigating the Lottery Ticket Hypothesis for Variational Quantum Circuits](https://arxiv.org/abs/2509.11190)
*Michael Kölle,Leonhard Klingert,Julian Schönberger,Philipp Altmann,Tobias Rohe,Claudia Linnhoff-Popien*

Main category: quant-ph

TL;DR: 研究Lottery Ticket Hypothesis (LTH)能否应用于变分量子电路 (VQCs)，发现弱LTH在VQCs成立，强LTH在二进制VQC中有效果，LTH或可提升VQCs效率。


<details>
  <summary>Details</summary>
Motivation: 变分量子电路 (VQCs) 常面临贫瘠高原现象阻碍优化，而经典机器学习中的Lottery Ticket Hypothesis (LTH) 可提升神经网络参数效率，探究LTH能否应用于VQCs。

Method: 研究LTH在VQCs中的应用情况，验证弱LTH和强LTH在不同类型VQCs中的表现。

Result: 弱LTH在VQCs成立，获胜子网络仅保留26.0%的原始参数；强LTH在二进制VQC中发现获胜子网络，仅用45%的权重达到100%准确率。

Conclusion: LTH可通过减少参数数量同时保持性能来缓解贫瘠高原问题，提升量子机器学习任务中VQCs的效率。

Abstract: Quantum computing is an emerging field in computer science that has seen
considerable progress in recent years, especially in machine learning. By
harnessing the principles of quantum physics, it can surpass the limitations of
classical algorithms. However, variational quantum circuits (VQCs), which rely
on adjustable parameters, often face the barren plateau phenomenon, hindering
optimization. The Lottery Ticket Hypothesis (LTH) is a recent concept in
classical machine learning that has led to notable improvements in parameter
efficiency for neural networks. It states that within a large network, a
smaller, more efficient subnetwork, or ''winning ticket,'' can achieve
comparable performance, potentially circumventing plateau challenges. In this
work, we investigate whether this idea can apply to VQCs. We show that the weak
LTH holds for VQCs, revealing winning tickets that retain just 26.0\% of the
original parameters. For the strong LTH, where a pruning mask is learned
without any training, we discovered a winning ticket in a binary VQC, achieving
100\% accuracy with only 45\% of the weights. These findings indicate that LTH
may mitigate barren plateaus by reducing parameter counts while preserving
performance, thus enhancing the efficiency of VQCs in quantum machine learning
tasks.

</details>


### [477] [Quantum Architecture Search for Solving Quantum Machine Learning Tasks](https://arxiv.org/abs/2509.11198)
*Michael Kölle,Simon Salfer,Tobias Rohe,Philipp Altmann,Claudia Linnhoff-Popien*

Main category: quant-ph

TL;DR: 本文介绍RL - QAS框架，用强化学习为分类任务寻找有效量子电路架构，在Iris和二进制MNIST数据集上评估，结果表明强化学习适用于量子机器学习架构搜索，但应用于更复杂任务需改进。


<details>
  <summary>Details</summary>
Motivation: 当前量子硬件有误差且规模有限，手动进行量子架构搜索复杂易错，强化学习在量子机器学习的自动架构搜索中未被充分探索。

Method: 引入RL - QAS框架，应用强化学习为分类任务发现有效电路架构，并使用Iris和二进制MNIST数据集进行评估。

Result: 代理自主发现低复杂度电路设计，实现了高测试准确率。

Conclusion: 强化学习是量子机器学习中自动架构搜索的可行方法，但应用于更复杂任务需进一步完善搜索策略和性能评估机制。

Abstract: Quantum computing leverages quantum mechanics to address computational
problems in ways that differ fundamentally from classical approaches. While
current quantum hardware remains error-prone and limited in scale, Variational
Quantum Circuits offer a noise-resilient framework suitable for today's
devices. The performance of these circuits strongly depends on the underlying
architecture of their parameterized quantum components. Identifying efficient,
hardware-compatible quantum circuit architectures -- known as Quantum
Architecture Search (QAS) -- is therefore essential. Manual QAS is complex and
error-prone, motivating efforts to automate it. Among various automated
strategies, Reinforcement Learning (RL) remains underexplored, particularly in
Quantum Machine Learning contexts. This work introduces RL-QAS, a framework
that applies RL to discover effective circuit architectures for classification
tasks. We evaluate RL-QAS using the Iris and binary MNIST datasets. The agent
autonomously discovers low-complexity circuit designs that achieve high test
accuracy. Our results show that RL is a viable approach for automated
architecture search in quantum machine learning. However, applying RL-QAS to
more complex tasks will require further refinement of the search strategy and
performance evaluation mechanisms.

</details>


### [478] [Parameter estimation with uncertainty quantification from continuous measurement data using neural network ensembles](https://arxiv.org/abs/2509.10756)
*Amanuel Anteneh*

Main category: quant-ph

TL;DR: 深度集成可用于量子参数估计，能量化不确定性，对噪声更鲁棒且所需数据更少。


<details>
  <summary>Details</summary>
Motivation: 寻找能进行量子参数估计并量化不确定性的方法。

Method: 使用深度神经网络集成（深度集成）进行量子参数估计。

Result: 深度集成对测量和训练数据中的噪声更鲁棒，且达到与贝叶斯推理相当性能所需数据更少。

Conclusion: 深度集成是一种有效的量子参数估计方法，具备量化不确定性等优势。

Abstract: We show that ensembles of deep neural networks, called deep ensembles, can be
used to perform quantum parameter estimation while also providing a means for
quantifying uncertainty in parameter estimates, which is a key advantage of
using Bayesian inference for parameter estimation. These models are shown to be
more robust to noise in the measurement results used to perform the parameter
estimation as well as noise in the data used to train them. We also show that
much less data is needed to achieve comparable performance to Bayesian
inference based estimation, which is known to reach the ultimate precision
limit as more data is collected, than was used in previous proposals.

</details>


### [479] [Quantum Graph Attention Networks: Trainable Quantum Encoders for Inductive Graph Learning](https://arxiv.org/abs/2509.11390)
*Arthur M. Faria,Mehdi Djellabi,Igor O. Sokolov,Savvas Varsamopoulos*

Main category: quant-ph

TL;DR: 本文提出量子图注意力网络（QGATs）作为图归纳学习的可训练量子编码器，在QM9数据集上评估，表明注意力机制能提升性能，量子注意力在大图上优势明显，QGATs在小图上与经典GAT模型精度相当。


<details>
  <summary>Details</summary>
Motivation: 为图的归纳学习扩展量子图神经网络（QGNN）框架，引入更有效的量子编码器。

Method: 提出QGATs，利用参数化量子电路编码节点特征和邻域结构，通过量子注意力机制动态调整邻居贡献。

Result: 在QM9数据集上实验，注意力机制在经典和量子图神经网络中都提升了性能，量子注意力在大图上优势明显，QGATs在小图上与经典GAT模型精度相当。

Conclusion: 量子注意力机制有潜力增强QGNN在化学等领域的归纳能力。

Abstract: We introduce Quantum Graph Attention Networks (QGATs) as trainable quantum
encoders for inductive learning on graphs, extending the Quantum Graph Neural
Networks (QGNN) framework. QGATs leverage parameterized quantum circuits to
encode node features and neighborhood structures, with quantum attention
mechanisms modulating the contribution of each neighbor via dynamically learned
unitaries. This allows for expressive, locality-aware quantum representations
that can generalize across unseen graph instances. We evaluate our approach on
the QM9 dataset, targeting the prediction of various chemical properties. Our
experiments compare classical and quantum graph neural networks-with and
without attention layers-demonstrating that attention consistently improves
performance in both paradigms. Notably, we observe that quantum attention
yields increasing benefits as graph size grows, with QGATs significantly
outperforming their non-attentive quantum counterparts on larger molecular
graphs. Furthermore, for smaller graphs, QGATs achieve predictive accuracy
comparable to classical GAT models, highlighting their viability as expressive
quantum encoders. These results show the potential of quantum attention
mechanisms to enhance the inductive capacity of QGNN in chemistry and beyond.

</details>


### [480] [Quantum Noise Tomography with Physics-Informed Neural Networks](https://arxiv.org/abs/2509.11911)
*Antonin Sulc*

Main category: quant-ph

TL;DR: 本文提出用物理信息神经网络（PINNs）进行Lindblad断层扫描的新框架，可从稀疏数据中学习量子态演化和推断耗散参数，是高效可扩展的量子设备表征方案。


<details>
  <summary>Details</summary>
Motivation: 传统断层扫描方法数据需求大且扩展性差，而表征量子系统环境相互作用是量子技术发展关键瓶颈。

Method: 将Lindblad主方程嵌入神经网络损失函数，从稀疏时间序列测量数据中同时学习量子态演化和推断耗散参数。

Result: PINNs能重构系统动力学和未知噪声参数的函数形式。

Conclusion: 该方法通过学习主方程生成了有噪声量子系统的全可微数字孪生模型，是一种高效可扩展的解决方案。

Abstract: Characterizing the environmental interactions of quantum systems is a
critical bottleneck in the development of robust quantum technologies.
Traditional tomographic methods are often data-intensive and struggle with
scalability. In this work, we introduce a novel framework for performing
Lindblad tomography using Physics-Informed Neural Networks (PINNs). By
embedding the Lindblad master equation directly into the neural network's loss
function, our approach simultaneously learns the quantum state's evolution and
infers the underlying dissipation parameters from sparse, time-series
measurement data. Our results show that PINNs can reconstruct both the system
dynamics and the functional form of unknown noise parameters, presenting a
sample-efficient and scalable solution for quantum device characterization.
Ultimately, our method produces a fully-differentiable digital twin of a noisy
quantum system by learning its governing master equation.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [481] [Dynamic Factor Models with Forward-Looking Views](https://arxiv.org/abs/2509.11528)
*Anas Abdelhakmi,Andrew E. B. Lim*

Main category: math.OC

TL;DR: 本文提出结合历史数据校准的动态因子模型与专家对未来因子的观点，研究对动态投资组合选择的影响，推导相关过程动态表达式和最优投资策略。


<details>
  <summary>Details</summary>
Motivation: 历史数据校准的预测模型在当前和未来动态与过去不同时预测效果不佳，需利用前瞻性信息改进预测。

Method: 结合历史数据校准的风险资产价格动态因子模型和专家对未来因子的观点，利用图形结构推导条件下因子和价格过程的动态表达式。

Result: 得出线性因子模型中价格过程成为具有新协变量的非齐次仿射过程，建立条件因子过程与Mean - Reverting Bridge的理论联系，推导投资者最优投资组合策略。

Conclusion: 该框架为在动态因子模型中嵌入协变量的前瞻性信息提供了可推广的方法。

Abstract: Prediction models calibrated using historical data may forecast poorly if the
dynamics of the present and future differ from observations in the past. For
this reason, predictions can be improved if information like forward looking
views about the state of the system are used to refine the forecast. We develop
an approach for combining a dynamic factor model for risky asset prices
calibrated on historical data, and noisy expert views of future values of the
factors/covariates in the model, and study the implications for dynamic
portfolio choice. By exploiting the graphical structure linking factors, asset
prices, and views, we derive closed-form expressions for the dynamics of the
factor and price processes after conditioning on the views. For linear factor
models, the price process becomes a time-inhomogeneous affine process with a
new covariate formed from the views. We establish a novel theoretical
connection between the conditional factor process and a process we call a
Mean-Reverting Bridge (MrB), an extension of the classical Brownian bridge. We
derive the investor's optimal portfolio strategy and show that views influence
both the myopic mean-variance term and the intertemporal hedge. The optimal
dynamic portfolio when the long-run mean of the expected return is uncertain
and learned online from data is also derived. More generally, our framework
offers a generalizable approach for embedding forward-looking information about
covariates in a dynamic factor model.

</details>


### [482] [Bilevel subsidy-enabled mobility hub network design with perturbed utility coalitional choice-based assignment](https://arxiv.org/abs/2509.10465)
*Hai Yang,Joseph Y. J. Chow*

Main category: math.OC

TL;DR: 本文提出双层移动枢纽平台设计模型，用特定方法求解，实验表明该方法能快速达接近最优解，还对比了不同补贴方式。


<details>
  <summary>Details</summary>
Motivation: 随着新服务出现城市移动性快速转变，移动枢纽作为物理 - 数字融合点，需设计模型支持移动即服务，处理出行决策与运营商策略的关系。

Method: 开发双层移动枢纽平台设计模型，上层通过设置补贴激励最后一英里运营商，下层用基于链路的PURC分配捕捉旅行者 - 运营商联合决策；通过KKT条件将双层问题转化为单层问题，用间隙惩罚法和迭代热启动方案求解。

Result: 在玩具网络和LIRR案例的数值实验中，方法能在数分钟内达到亚1%的最优性差距；对比链路补贴和枢纽补贴，后者计算成本高但便于比较和控制。

Conclusion: 所提模型和方法有效，能帮助政策制定者量化移动枢纽社会剩余价值，不同补贴方式各有优劣。

Abstract: Urban mobility is undergoing rapid transformation with the emergence of new
services. Mobility hubs (MHs) have been proposed as physical-digital
convergence points, offering a range of public and private mobility options in
close proximity. By supporting Mobility-as-a-Service, these hubs can serve as
focal points where travel decisions intersect with operator strategies. We
develop a bilevel MH platform design model that treats MHs as control levers.
The upper level (platform) maximizes revenue or flow by setting subsidies to
incentivize last-mile operators; the lower level captures joint
traveler-operator decisions with a link-based Perturbed Utility Route Choice
(PURC) assignment, yielding a strictly convex quadratic program. We reformulate
the bilevel problem to a single-level program via the KKT conditions of the
lower level and solve it with a gap-penalty method and an iterative warm-start
scheme that exploits the computationally cheap lower-level problem. Numerical
experiments on a toy network and a Long Island Rail Road (LIRR) case (244
nodes, 469 links, 78 ODs) show that the method attains sub-1% optimality gaps
in minutes. In the base LIRR case, the model allows policymakers to quantify
the social surplus value of a MH, or the value of enabling subsidy or
regulating the microtransit operator's pricing. Comparing link-based subsidies
to hub-based subsidies, the latter is computationally more expensive but offers
an easier mechanism for comparison and control.

</details>


### [483] [Optimal Micro-Transit Zoning via Clique Generation and Integer Programming](https://arxiv.org/abs/2509.11445)
*Hins Hu,Rhea Goswami,Hongyi Jiang,Samitha Samaranayake*

Main category: math.OC

TL;DR: 本文提出基于最大化服务需求的微公交服务区域设计算法框架，经评估优于基线算法。


<details>
  <summary>Details</summary>
Motivation: 有效设计微公交服务需确定最优服务区域，但面临运营预算和机构优先级等约束，是复杂挑战。

Method: 提出两阶段算法框架，第一阶段用可扩展算法生成候选区域，第二阶段将区域选择问题转化为加权最大覆盖问题并用整数规划求解器解决，还将共享性图概念用于静态空间分区，重新定义共享性。

Result: 在田纳西州查塔努加的真实数据和合成数据集上评估，该框架优于基线算法，实际服务需求多27.03%，合成场景最多多49.5%。

Conclusion: 所提出的两阶段算法框架能有效设计微公交最优服务区域，提升服务需求。

Abstract: Micro-transit services offer a promising solution to enhance urban mobility
and access, particularly by complementing existing public transit. However,
effectively designing these services requires determining optimal service zones
for these on-demand shuttles, a complex challenge often constrained by
operating budgets and transit agency priorities. This paper presents a novel
two-phase algorithmic framework for designing optimal micro-transit service
zones based on the objective of maximizing served demand. A key innovation is
our adaptation of the shareability graph concept from its traditional use in
dynamic trip assignment to the distinct challenge of static spatial zoning. We
redefine shareability by considering geographical proximity within a specified
diameter constraint, rather than trip characteristics. In Phase 1, the
framework employs a highly scalable algorithm to generate a comprehensive set
of candidate zones. In Phase 2, it formulates the selection of a specified
number of zones as a Weighted Maximum Coverage Problem, which can be
efficiently solved by an integer programming solver. Evaluations on real-world
data from Chattanooga, TN, and synthetic datasets show that our framework
outperforms a baseline algorithm, serving 27.03% more demand in practice and up
to 49.5% more demand in synthetic settings.

</details>


### [484] [Gradient Methods with Online Scaling Part II. Practical Aspects](https://arxiv.org/abs/2509.11007)
*Ya-Chi Chu,Wenzhi Gao,Yinyu Ye,Madeleine Udell*

Main category: math.OC

TL;DR: 论文聚焦在线缩放梯度方法（OSGM）实际方面，设计新自适应一阶方法，扩展到非凸优化并给出与现有优化理论和实践的联系方向。


<details>
  <summary>Details</summary>
Motivation: 探究OSGM的实际应用，设计新方法并了解其经验行为。

Method: 利用OSGM框架设计新的自适应一阶方法，并将其扩展到非凸优化。

Result: 得到的OSGM - Best方法性能与拟牛顿变体相当，但所需内存更少、迭代成本更低。

Conclusion: OSGM在实际应用中有良好表现，可扩展到非凸优化，且与现有优化理论和实践有联系。

Abstract: Part I of this work [Gao25] establishes online scaled gradient methods
(OSGM), a framework that utilizes online convex optimization to adapt stepsizes
in gradient methods. This paper focuses on the practical aspects of OSGM. We
leverage the OSGM framework to design new adaptive first-order methods and
provide insights into their empirical behavior. The resulting method,
OSGM-Best, matches the performance of quasi-Newton variants while requiring
less memory and cheaper iterations. We also extend OSGM to nonconvex
optimization and outline directions that connect OSGM to existing branches of
optimization theory and practice.

</details>


### [485] [Preconditioned subgradient method for composite optimization: overparameterization and fast convergence](https://arxiv.org/abs/2509.11486)
*Mateo Díaz,Liwei Jiang,Abdel Ghani Labassi*

Main category: math.OC

TL;DR: 提出Levenberg - Morrison - Marquardt次梯度法解决复合优化问题收敛慢的问题，证明条件并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统次梯度法在平滑映射病态或过参数化时收敛慢，需改进。

Method: 引入Levenberg - Morrison - Marquardt次梯度法。

Result: 该方法在温和正则条件下线性收敛，收敛速率仅由凸函数决定，正则条件在多个实际问题中成立。

Conclusion: 所提方法有效，数值实验体现其优势。

Abstract: Composite optimization problems involve minimizing the composition of a
smooth map with a convex function. Such objectives arise in numerous data
science and signal processing applications, including phase retrieval, blind
deconvolution, and collaborative filtering. The subgradient method achieves
local linear convergence when the composite loss is well-conditioned. However,
if the smooth map is, in a certain sense, ill-conditioned or overparameterized,
the subgradient method exhibits much slower sublinear convergence even when the
convex function is well-conditioned. To overcome this limitation, we introduce
a Levenberg-Morrison-Marquardt subgradient method that converges linearly under
mild regularity conditions at a rate determined solely by the convex function.
Further, we demonstrate that these regularity conditions hold for several
problems of practical interest, including square-variable formulations, matrix
sensing, and tensor factorization. Numerical experiments illustrate the
benefits of our method.

</details>


### [486] [Convergence Rate in Nonlinear Two-Time-Scale Stochastic Approximation with State (Time)-Dependence](https://arxiv.org/abs/2509.11039)
*Zixi Chen,Yumin Xu,Ruixun Zhang*

Main category: math.OC

TL;DR: 研究含状态和时间相关噪声的非线性双时间尺度随机逼近，证明李雅普诺夫函数收敛速率，给出数值例子。


<details>
  <summary>Details</summary>
Motivation: 受允许与当前状态或时间相关变化的进展启发，考虑状态和时间相关噪声。

Method: 分析李雅普诺夫函数在不同噪声参数下的收敛情况。

Result: 李雅普诺夫函数在两种情况下呈现多项式收敛速率，若状态噪声参数接近极限值，可实现指数收敛速率。

Conclusion: 给出了含状态和时间相关噪声的非线性双时间尺度随机逼近的收敛结论，并通过数值例子验证。

Abstract: The nonlinear two-time-scale stochastic approximation is widely studied under
conditions of bounded variances in noise. Motivated by recent advances that
allow for variability linked to the current state or time, we consider state-
and time-dependent noises. We show that the Lyapunov function exhibits
polynomial convergence rates in both cases, with the rate of polynomial delay
depending on the parameters of state- or time-dependent noises. Notably, if the
state noise parameters fully approach their limiting value, the Lyapunov
function achieves an exponential convergence rate. We provide two numerical
examples to illustrate our theoretical findings in the context of stochastic
gradient descent with Polyak-Ruppert averaging and stochastic bilevel
optimization.

</details>


### [487] [From PowerSGD to PowerSGD+: Low-Rank Gradient Compression for Distributed Optimization with Convergence Guarantees](https://arxiv.org/abs/2509.11254)
*Shengping Xie,Chuyan Chen,Kun Yuan*

Main category: math.OC

TL;DR: 指出PowerSGD收敛性不明，给出反例，提出PowerSGD+方法并证明其收敛性和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决PowerSGD在随机设置下收敛保证不明确的问题。

Method: 提出PowerSGD+，通过奇异值分解定期更新投影子空间。

Result: 证明PowerSGD+在标准假设下收敛，在大语言模型任务上验证了其有效性。

Conclusion: PowerSGD并非总能收敛到最优解，PowerSGD+能解决收敛问题。

Abstract: Low-rank gradient compression methods, such as PowerSGD, have gained
attention in communication-efficient distributed optimization. However, the
convergence guarantees of PowerSGD remain unclear, particularly in stochastic
settings. In this paper, we show that PowerSGD does not always converge to the
optimal solution and provide a clear counterexample to support this finding. To
address this, we introduce PowerSGD+, which periodically updates the projection
subspace via singular value decomposition, ensuring that it remains aligned
with the optimal subspace. We prove that PowerSGD+ converges under standard
assumptions and validate its effectiveness through empirical evaluation on
large language model tasks.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [488] [Pair-Bid Auction Model for Optimized Network Slicing in 5G RAN](https://arxiv.org/abs/2509.10533)
*Mengyao Li,Sebastian Troia,Yingqian Zhang,Guido Maier*

Main category: cs.NI

TL;DR: 本文聚焦5G网络切片，构建两级组合拍卖模型优化资源分配与收益，仿真显示收益提升12.5%。


<details>
  <summary>Details</summary>
Motivation: 解决5G网络切片中公平共享和财务相关的功率效率问题，优化资源分配和收益。

Method: 构建两级层次组合拍卖模型，上层MNO向MVNO拍卖切片，下层MVNO向终端用户分配资源，采用成对出价机制和VCG定价。

Result: 仿真验证了模型在资源分配和财务表现上的有效性，相比基线收益提高12.5%。

Conclusion: 所提出的模型能有效优化5G网络切片的资源分配和收益。

Abstract: Network slicing is a key 5G technology that enables multiple virtual networks
to share physical infrastructure, optimizing flexibility and resource
allocation. This involves Mobile Network Operators (MNO), Mobile Virtual
Network Operators (MVNOs), and end users, where MNO leases network slices to
MVNOs, and then provides customized services. This work considers end-to-end
network slicing with a focus on fair sharing and financial-related power
efficiency, modeled as a two level hierarchical combinatorial auction. At the
upper level, an MNO auctions slices to competing MVNOs, while at the lower
level, MVNOs allocate resources to end users through their own auctions.
Dynamic user requests add complexity to the process. Our model optimizes
resource allocation and revenue generation using a pair-bid mechanism and
Vickrey-Clarke-Groves (VCG) pricing. The pair-bid approach enhances competition
and efficiency, while VCG ensures truthful bidding based on marginal system
impact. Simulations validate the model's effectiveness in resource distribution
and financial performance, showing a 12.5% revenue improvement over the
baseline.

</details>


### [489] [SABR: A Stable Adaptive Bitrate Framework Using Behavior Cloning Pretraining and Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2509.10486)
*Pengcheng Luo,Yunyang Zhao,Bowen Zhang,Genke Yang,Boon-Hee Soong,Chau Yuen*

Main category: cs.NI

TL;DR: 5G时代进入视频主导互联网时代，ABR控制影响体验质量，现有基于学习的ABR方法泛化性差，提出SABR框架和基准测试，实验表明SABR表现佳、泛化性好。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于学习的ABR方法在训练时依赖有限网络轨迹集，忽视真实网络条件广泛分布特性，导致在分布外场景泛化性差的问题。

Method: 提出结合行为克隆预训练和强化学习微调的SABR训练框架，引入ABRBench - 3G和ABRBench - 4G+基准测试，提供广泛覆盖的训练轨迹和专门的分布外测试集。

Result: 在提出的基准测试中，SABR与Pensieve、Comyco和NetLLM相比，平均排名最佳。

Conclusion: SABR能在广泛分布上实现更稳定学习，提高对未见网络条件的泛化能力。

Abstract: With the advent of 5G, the internet has entered a new video-centric era. From
short-video platforms like TikTok to long-video platforms like Bilibili, online
video services are reshaping user consumption habits. Adaptive Bitrate (ABR)
control is widely recognized as a critical factor influencing Quality of
Experience (QoE). Recent learning-based ABR methods have attracted increasing
attention. However, most of them rely on limited network trace sets during
training and overlook the wide-distribution characteristics of real-world
network conditions, resulting in poor generalization in out-of-distribution
(OOD) scenarios. To address this limitation, we propose SABR, a training
framework that combines behavior cloning (BC) pretraining with reinforcement
learning (RL) fine-tuning. We also introduce benchmarks, ABRBench-3G and
ABRBench-4G+, which provide wide-coverage training traces and dedicated OOD
test sets for assessing robustness to unseen network conditions. Experimental
results demonstrate that SABR achieves the best average rank compared with
Pensieve, Comyco, and NetLLM across the proposed benchmarks. These results
indicate that SABR enables more stable learning across wide distributions and
improves generalization to unseen network conditions.

</details>


### [490] [Online Learning Based Efficient Resource Allocation for LoRaWAN Network](https://arxiv.org/abs/2509.10493)
*Ruiqi Wang,Jing Ren,Tongyu Song,Wenjun Li,Xiong Wang,Sheng Wang,Shizhong Xu*

Main category: cs.NI

TL;DR: 提出基于在线学习的资源分配框架D - LoRa和CD - LoRa解决LoRaWAN网络中PDR和EE的权衡问题，经实验验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化LoRaWAN网络中PDR和EE冲突指标时存在不足，缺乏适应性，导致性能不佳。

Method: 提出D - LoRa全分布式框架，将问题建模为组合多臂老虎机问题；提出CD - LoRa混合框架，集成轻量级集中初始化阶段。

Result: D - LoRa在非平稳环境表现出色，CD - LoRa在平稳条件下收敛最快，物理部署中优于现有基线，PDR最高提升10.8%，EE提升26.1%。

Conclusion: 所提框架对可扩展和高效的LoRaWAN网络具有实际有效性。

Abstract: The deployment of large-scale LoRaWAN networks requires jointly optimizing
conflicting metrics like Packet Delivery Ratio (PDR) and Energy Efficiency (EE)
by dynamically allocating transmission parameters, including Carrier Frequency,
Spreading Factor, and Transmission Power. Existing methods often oversimplify
this challenge, focusing on a single metric or lacking the adaptability needed
for dynamic channel environments, leading to suboptimal performance. To address
this, we propose two online learning-based resource allocation frameworks that
intelligently navigate the PDR-EE trade-off. Our foundational proposal, D-LoRa,
is a fully distributed framework that models the problem as a Combinatorial
Multi-Armed Bandit. By decomposing the joint parameter selection and employing
specialized, disaggregated reward functions, D-LoRa dramatically reduces
learning complexity and enables nodes to autonomously adapt to network
dynamics. To further enhance performance in LoRaWAN networks, we introduce
CD-LoRa, a hybrid framework that integrates a lightweight, centralized
initialization phase to perform a one-time, quasi-optimal channel assignment
and action space pruning, thereby accelerating subsequent distributed learning.
Extensive simulations and real-world field experiments demonstrate the
superiority of our frameworks, showing that D-LoRa excels in non-stationary
environments while CD-LoRa achieves the fastest convergence in stationary
conditions. In physical deployments, our methods outperform state-of-the-art
baselines, improving PDR by up to 10.8% and EE by 26.1%, confirming their
practical effectiveness for scalable and efficient LoRaWAN networks.

</details>


### [491] [Towards Scalable O-RAN Resource Management: Graph-Augmented Proximal Policy Optimization](https://arxiv.org/abs/2509.10499)
*Duc-Thinh Ngo,Kandaraj Piamrat,Ons Aouedi,Thomas Hassan,Philippe Raipin-Parvédy*

Main category: cs.NI

TL;DR: 提出GPPO框架优化O-RAN资源分配，实验显示性能优于基线，适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: O-RAN架构灵活性带来资源管理挑战，现有方案存在不足，需联合优化功能拆分选择和虚拟单元放置。

Method: 提出Graph-Augmented Proximal Policy Optimization (GPPO)框架，利用图神经网络进行拓扑感知特征提取，集成动作掩码导航组合决策空间。

Result: 在大小规模O-RAN场景实验中，GPPO始终优于现有基线，部署成本降低18%，泛化测试奖励提高25%，可靠性完美。

Conclusion: GPPO对实际O-RAN部署有效且可扩展。

Abstract: Open Radio Access Network (O-RAN) architectures enable flexible, scalable,
and cost-efficient mobile networks by disaggregating and virtualizing baseband
functions. However, this flexibility introduces significant challenges for
resource management, requiring joint optimization of functional split selection
and virtualized unit placement under dynamic demands and complex topologies.
Existing solutions often address these aspects separately or lack scalability
in large and real-world scenarios. In this work, we propose a novel
Graph-Augmented Proximal Policy Optimization (GPPO) framework that leverages
Graph Neural Networks (GNNs) for topology-aware feature extraction and
integrates action masking to efficiently navigate the combinatorial decision
space. Our approach jointly optimizes functional split and placement decisions,
capturing the full complexity of O-RAN resource allocation. Extensive
experiments on both small-and large-scale O-RAN scenarios demonstrate that GPPO
consistently outperforms state-of-the-art baselines, achieving up to 18% lower
deployment cost and 25% higher reward in generalization tests, while
maintaining perfect reliability. These results highlight the effectiveness and
scalability of GPPO for practical O-RAN deployments.

</details>


### [492] [An Internet of Intelligent Things Framework for Decentralized Heterogeneous Platforms](https://arxiv.org/abs/2509.10507)
*Vadim Allayev,Mahbubur Rahman*

Main category: cs.NI

TL;DR: 文章提出异构、去中心化的IoIT对等网状网络系统模型以解决现有问题，采用联邦学习等方法实现多个优化目标。


<details>
  <summary>Details</summary>
Motivation: IoIT在资源、能源和存储方面存在挑战，现有节能平台多为集中式有弊端，需要去中心化系统。

Method: 提出异构、去中心化的IoIT对等网状网络系统模型，采用联邦学习训练节点，元启发式优化任务分配和路由路径，多目标优化平衡性能目标。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Internet of Intelligent Things (IoIT), an emerging field, combines the
utility of Internet of Things (IoT) devices with the innovation of embedded AI
algorithms. However, it does not come without challenges, and struggles
regarding available computing resources, energy supply, and storage
limitations. In particular, many impediments to IoIT are linked to the
energy-efficient deployment of machine learning (ML)/deep learning (DL) models
in embedded devices. Research has been conducted to design energy-efficient
IoIT platforms, but these papers often focus on centralized systems, in which
some central entity processes all the data and coordinates actions. This can be
problematic, e.g., serve as bottleneck or lead to security concerns. In a
decentralized system, nodes/devices would self-organize and make their own
decisions. Therefore, to address such issues, we propose a heterogeneous,
decentralized sensing and monitoring IoIT peer-to-peer mesh network system
model. Nodes in the network will coordinate towards several optimization goals:
reliability, energy efficiency, and latency. The system employs federated
learning to train nodes in a distributed manner, metaheuristics to optimize
task allocation and routing paths, and multi-objective optimization to balance
conflicting performance goals.

</details>


### [493] [CAR-BRAINet: Sub-6GHz Aided Spatial Adaptive Beam Prediction with Multi Head Attention for Heterogeneous Vehicular Networks](https://arxiv.org/abs/2509.10508)
*Aathira G Menon,Prabu Krishnan,Shyam Lal*

Main category: cs.NI

TL;DR: 本文提出轻量级深度学习解决方案CAR - BRAINet用于异构车载网络（HetVNets）波束预测，在不同场景表现良好，提升频谱效率。


<details>
  <summary>Details</summary>
Motivation: HetVNets满足5G/B5G车载网络连接需求，但保持稳定连接有挑战，现有波束预测模型缺乏针对HetVNets的专用方案，且多在理想场景研究。

Method: 引入基于卷积神经网络和多头注意力机制的CAR - BRAINet，构建包含多种关键因素的三个动态数据集模拟真实驾驶场景。

Result: CAR - BRAINet在所有车载场景中有效，实现精确波束预测，减少波束开销，频谱效率较现有方法提升17.9422%。

Conclusion: CAR - BRAINet在复杂HetVNets中有效，无需依赖移动用户位置角度和天线尺寸，减少冗余传感器延迟。

Abstract: Heterogeneous Vehicular Networks (HetVNets) play a key role by stacking
different communication technologies such as sub-6GHz, mm-wave and DSRC to meet
diverse connectivity needs of 5G/B5G vehicular networks. HetVNet helps address
the humongous user demands-but maintaining a steady connection in a highly
mobile, real-world conditions remain a challenge. Though there has been ample
of studies on beam prediction models a dedicated solution for HetVNets is
sparsely explored. Hence, it is the need of the hour to develop a reliable beam
prediction solution, specifically for HetVNets. This paper introduces a
lightweight deep learning-based solution termed-"CAR-BRAINet" which consists of
convolutional neural networks with a powerful multi-head attention (MHA)
mechanism. Existing literature on beam prediction is largely studied under a
limited, idealised vehicular scenario, often overlooking the real-time
complexities and intricacies of vehicular networks. Therefore, this study aims
to mimic the complexities of a real-time driving scenario by incorporating key
factors such as prominent MAC protocols-3GPP-C-V2X and IEEE 802.11BD, the
effect of Doppler shifts under high velocity and varying distance and SNR
levels into three high-quality dynamic datasets pertaining to urban, rural and
highway vehicular networks. CAR-BRAINet performs effectively across all the
vehicular scenarios, demonstrating precise beam prediction with minimal beam
overhead and a steady improvement of 17.9422% on the spectral efficiency over
the existing methods. Thus, this study justifies the effectiveness of
CAR-BRAINet in complex HetVNets, offering promising performance without relying
on the location angle and antenna dimensions of the mobile users, and thereby
reducing the redundant sensor-latency.

</details>


### [494] [ASL360: AI-Enabled Adaptive Streaming of Layered 360° Video over UAV-assisted Wireless Networks](https://arxiv.org/abs/2509.10544)
*Alireza Mohammadhosseini,Jacob Chakareski,Nicholas Mastronarde*

Main category: cs.NI

TL;DR: 提出基于自适应深度强化学习调度器ASL360用于5G网络中360°视频流，有效提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 最大化UAV辅助5G无线网络中移动VR用户360°视频流的整体体验质量。

Method: 将调度决策建模为受限马尔可夫决策过程，用PPO算法找最优策略，实现成本组件动态调整机制。

Result: 相比基线方法，平均视频质量高约2dB，平均缓冲时间低80%，视频质量变化低57%。

Conclusion: 分层和自适应方法在动态和复杂网络环境下能有效提升沉浸式视频流应用的用户体验。

Abstract: We propose ASL360, an adaptive deep reinforcement learning-based scheduler
for on-demand 360{\deg} video streaming to mobile VR users in next generation
wireless networks. We aim to maximize the overall Quality of Experience (QoE)
of the users served over a UAV-assisted 5G wireless network. Our system model
comprises a macro base station (MBS) and a UAV-mounted base station which both
deploy mm-Wave transmission to the users. The 360{\deg} video is encoded into
dependent layers and segmented tiles, allowing a user to schedule downloads of
each layer's segments. Furthermore, each user utilizes multiple buffers to
store the corresponding video layer's segments. We model the scheduling
decision as a Constrained Markov Decision Process (CMDP), where the agent
selects Base or Enhancement layers to maximize the QoE and use a policy
gradient-based method (PPO) to find the optimal policy. Additionally, we
implement a dynamic adjustment mechanism for cost components, allowing the
system to adaptively balance and prioritize the video quality, buffer
occupancy, and quality change based on real-time network and streaming session
conditions. We demonstrate that ASL360 significantly improves the QoE,
achieving approximately 2 dB higher average video quality, 80% lower average
rebuffering time, and 57% lower video quality variation, relative to
competitive baseline methods. Our results show the effectiveness of our layered
and adaptive approach in enhancing the QoE in immersive videostreaming
applications, particularly in dynamic and challenging network environments.

</details>


### [495] [Multi-Modal Sensing Aided mmWave Beamforming for V2V Communications with Transformers](https://arxiv.org/abs/2509.11112)
*Muhammad Baqer Mollah,Honggang Wang,Hua Fang*

Main category: cs.NI

TL;DR: 提出多模态感知与融合学习框架降低毫米波通信波束训练开销，实验显示性能良好。


<details>
  <summary>Details</summary>
Motivation: 标准波束形成方法在高动态车载环境中会产生高波束训练开销，减少通信可用时间。

Method: 用特定编码器从视觉和GPS坐标感知模态中提取特征，融合多模态特征以预测前k个波束。

Result: 该框架预测前15个波束准确率达77.58%，优于单模态，平均功率损失约2.32dB，相比标准方法减少76.56%的波束搜索空间开销。

Conclusion: 提出的多模态感知与融合学习框架可有效降低波束训练开销。

Abstract: Beamforming techniques are utilized in millimeter wave (mmWave) communication
to address the inherent path loss limitation, thereby establishing and
maintaining reliable connections. However, adopting standard defined
beamforming approach in highly dynamic vehicular environments often incurs high
beam training overheads and reduces the available airtime for communications,
which is mainly due to exchanging pilot signals and exhaustive beam
measurements. To this end, we present a multi-modal sensing and fusion learning
framework as a potential alternative solution to reduce such overheads. In this
framework, we first extract the features individually from the visual and GPS
coordinates sensing modalities by modality specific encoders, and subsequently
fuse the multimodal features to obtain predicted top-k beams so that the best
line-of-sight links can be proactively established. To show the
generalizability of the proposed framework, we perform a comprehensive
experiment in four different vehicle-to-vehicle (V2V) scenarios from real-world
multi-modal sensing and communication dataset. From the experiment, we observe
that the proposed framework achieves up to 77.58% accuracy on predicting top-15
beams correctly, outperforms single modalities, incurs roughly as low as 2.32
dB average power loss, and considerably reduces the beam searching space
overheads by 76.56% for top-15 beams with respect to standard defined approach.

</details>


### [496] [The LLM as a Network Operator: A Vision for Generative AI in the 6G Radio Access Network](https://arxiv.org/abs/2509.10478)
*Oluwaseyi Giwa,Michael Adewole,Tobi Awodumila,Pelumi Aderinto*

Main category: cs.NI

TL;DR: 提出LLM - RAN Operator概念，构建正式框架，提供分析工具并指出研究挑战，旨在弥合AI理论与无线系统工程差距。


<details>
  <summary>Details</summary>
Motivation: 未来AI原生下一代无线接入网络管理复杂度超传统自动化能力，需新方法。

Method: 将大语言模型嵌入RAN控制循环，构建基于O - RAN标准的正式框架，分离非实时和近实时RIC功能。

Result: 提供分析工具以推理AI原生RAN控制的可行性和稳定性，识别安全、实时性能和物理世界接地等研究挑战。

Conclusion: 有望弥合NextG时代AI理论与无线系统工程间的差距，助力开发知识型、意图驱动的无线网络。

Abstract: The management of future AI-native Next-Generation (NextG) Radio Access
Networks (RANs), including 6G and beyond, presents a challenge of immense
complexity that exceeds the capabilities of traditional automation. In
response, we introduce the concept of the LLM-RAN Operator. In this paradigm, a
Large Language Model (LLM) is embedded into the RAN control loop to translate
high-level human intents into optimal network actions. Unlike prior empirical
studies, we present a formal framework for an LLM-RAN operator that builds on
earlier work by making guarantees checkable through an adapter aligned with the
Open RAN (O-RAN) standard, separating strategic LLM-driven guidance in the
Non-Real-Time (RT) RAN intelligent controller (RIC) from reactive execution in
the Near-RT RIC, including a proposition on policy expressiveness and a theorem
on convergence to stable fixed points. By framing the problem with mathematical
rigor, our work provides the analytical tools to reason about the feasibility
and stability of AI-native RAN control. It identifies critical research
challenges in safety, real-time performance, and physical-world grounding. This
paper aims to bridge the gap between AI theory and wireless systems engineering
in the NextG era, aligning with the AI4NextG vision to develop knowledgeable,
intent-driven wireless networks that integrate generative AI into the heart of
the RAN.

</details>


### [497] [Energy-Aware 6G Network Design: A Survey](https://arxiv.org/abs/2509.11289)
*Rashmi Kamran,Mahesh Ganesh Bhat,Pranav Jha,Shana Moothedath,Manjesh Hanawal,Prasanna Chaporkar*

Main category: cs.NI

TL;DR: 本文对6G节能网络设计方法进行全面调研，涵盖能源采集、模型参数等，介绍用例、标准化工作，最后指出开放研究问题与挑战。


<details>
  <summary>Details</summary>
Motivation: 6G网络新能力和应用引发能效与可持续性担忧，需设计节能网络。

Method: 对现有用于设计节能6G网络的方法进行全面调查，包括能源采集、能源模型和参数、能源感知服务分类及基于AI/ML的解决方案等，并结合用例和标准化工作进行分析。

Result: 完成对节能6G网络设计方法的调研，介绍了相关用例和标准化工作。

Conclusion: 指出了可探索的开放研究问题和挑战，以实现6G网络能源感知设计的可行性和性能与能源目标的最优性。

Abstract: 6th Generation (6G) mobile networks are envisioned to support several new
capabilities and data-centric applications for unprecedented number of users,
potentially raising significant energy efficiency and sustainability concerns.
This brings focus on sustainability as one of the key objectives in the their
design. To move towards sustainable solution, research and standardization
community is focusing on several key issues like energy information monitoring
and exposure, use of renewable energy, and use of Artificial
Intelligence/Machine Learning (AI/ML) for improving the energy efficiency in 6G
networks. The goal is to build energy-aware solutions that takes into account
the energy information resulting in energy efficient networks. Design of
energy-aware 6G networks brings in new challenges like increased overheads in
gathering and exposing of energy related information, and the associated user
consent management. The aim of this paper is to provide a comprehensive survey
of methods used for design of energy efficient 6G networks, like energy
harvesting, energy models and parameters, classification of energy-aware
services, and AI/ML-based solutions. The survey also includes few use cases
that demonstrate the benefits of incorporating energy awareness into network
decisions. Several ongoing standardization efforts in 3GPP, ITU, and IEEE are
included to provide insights into the ongoing work and highlight the
opportunities for new contributions. We conclude this survey with open research
problems and challenges that can be explored to make energy-aware design
feasible and ensure optimality regarding performance and energy goals for 6G
networks.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [498] [Derivative-informed Graph Convolutional Autoencoder with Phase Classification for the Lifshitz-Petrich Model](https://arxiv.org/abs/2509.11293)
*Yanlai Chen,Yajie Ji,Zhenli Xu*

Main category: math.NA

TL;DR: 提出DiGCA对LP模型多组分多状态解进行分类，比传统方法在效率和准确性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: LP模型求解和分类因高阶梯度项和准晶体长程取向序特征而具有挑战性，需解决这些问题。

Method: 提出Derivative - informed Graph Convolutional Autoencoder (DiGCA)分类器，分离线和在线两个阶段，离线阶段结合解及其导数训练图卷积自编码器，在线阶段用神经网络分类器对编码解分类。

Result: DiGCA相分类器能准确求解LP模型、分类其解并快速生成详细相图。

Conclusion: DiGCA相分类器比传统方法在效率和准确性上有显著改进。

Abstract: The Lifshitz-Petrich (LP) model is a classical model for describing complex
spatial patterns such as quasicrystals and multiphase structures. Solving and
classifying the solutions of the LP model is challenging due to the presence of
high-order gradient terms and the long-range orientational order characteristic
of the quasicrystals. To address these challenges, we propose a
Derivative-informed Graph Convolutional Autoencoder (DiGCA) to classify the
multi-component multi-state solutions of the LP model. The classifier consists
of two stages. In the offline stage, the DiGCA phase classifier innovatively
incorporates both solutions and their derivatives for training a graph
convolutional autoencoder which effectively captures intricate spatial
dependencies while significantly reducing the dimensionality of the solution
space. In the online phase, the framework employs a neural network classifier
to efficiently categorize encoded solutions into distinct phase diagrams. The
numerical results demonstrate that the DiGCA phase classifier accurately solves
the LP model, classifies its solutions, and rapidly generates detailed phase
diagrams in a robust manner, offering significant improvements in both
efficiency and accuracy over traditional methods.

</details>


### [499] [Learning Singularity-Encoded Green's Functions with Application to Iterative Methods](https://arxiv.org/abs/2509.11580)
*Qi Sun,Shengyan Li,Bowen Zheng,Lili Ju,Xuejun Xu*

Main category: math.NA

TL;DR: 提出新的无监督学习方法解决格林函数数值计算难题，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 格林函数无闭式表达式需替代建模，其数值计算因维数加倍和奇点问题具有挑战性。

Method: 将格林函数嵌入高一维空间，用神经网络参数化处理增加的维数，投影训练好的神经网络解到原域，利用光谱偏差加速迭代方案。

Result: 通过二维和四维格林函数的数值实验，有效解决奇点问题并加速迭代求解器。

Conclusion: 所提奇点编码学习方法能有效解决格林函数数值计算问题，加速迭代求解。

Abstract: Green's function provides an inherent connection between theoretical analysis
and numerical methods for elliptic partial differential equations, and general
absence of its closed-form expression necessitates surrogate modeling to guide
the design of effective solvers. Unfortunately, numerical computation of
Green's function remains challenging due to its doubled dimensionality and
intrinsic singularity. In this paper, we present a novel singularity-encoded
learning approach to resolve these problems in an unsupervised fashion. Our
method embeds the Green's function within a one-order higher-dimensional space
by encoding its prior estimate as an augmented variable, followed by a neural
network parametrization to manage the increased dimensionality. By projecting
the trained neural network solution back onto the original domain, our deep
surrogate model exploits its spectral bias to accelerate conventional iterative
schemes, serving either as a preconditioner or as part of a hybrid solver. The
effectiveness of our proposed method is empirically verified through numerical
experiments with two and four dimensional Green's functions, achieving
satisfactory resolution of singularities and acceleration of iterative solvers.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [500] [A High-Order Cumulant Extension of Quasi-Linkage Equilibrium](https://arxiv.org/abs/2509.10987)
*Kai S. Shimagaki,Jorge Fernandez-de-Cossio-Diaz,Rémi Monasson,Simona Cocco,John P. Barton*

Main category: q-bio.PE

TL;DR: 本文扩展了多基因座准连锁平衡（QLE）框架到exQLE，可捕捉高阶上位相互作用下的累积量动态，还能用于推断适应度参数。


<details>
  <summary>Details</summary>
Motivation: 传统QLE在强选择、显著上位相互作用或弱重组情况下失效，需扩展框架定量理解遗传多样群体动态。

Method: 扩展多基因座QLE框架，允许K阶累积量动态演化，假设高阶累积量快速平衡，得到K阶累积量的运动方程。

Result: K = 2的exQLE能准确捕捉高阶上位相互作用下的累积量动态，可用于从时间序列数据推断适应度参数。

Conclusion: exQLE提供了系统且可解释的近似方案，利用分析累积量动态并通过截断高阶累积量降低复杂度。

Abstract: A central question in evolutionary biology is how to quantitatively
understand the dynamics of genetically diverse populations. Modeling the
genotype distribution is challenging, as it ultimately requires tracking all
correlations (or cumulants) among alleles at different loci. The quasi-linkage
equilibrium (QLE) approximation simplifies this by assuming that correlations
between alleles at different loci are weak -- i.e., low linkage disequilibrium
-- allowing their dynamics to be modeled perturbatively. However, QLE breaks
down under strong selection, significant epistatic interactions, or weak
recombination. We extend the multilocus QLE framework to allow cumulants up to
order $K$ to evolve dynamically, while higher-order cumulants ($>K$) are
assumed to equilibrate rapidly. This extended QLE (exQLE) framework yields a
general equation of motion for cumulants up to order $K$, which parallels the
standard QLE dynamics (recovered when $K = 1$). In this formulation, cumulant
dynamics are driven by the gradient of average fitness, mediated by a
geometrically interpretable matrix that stems from competition among genotypes.
Our analysis shows that the exQLE with $K=2$ accurately captures cumulant
dynamics even when the fitness function includes higher-order (e.g., third- or
fourth-order) epistatic interactions, capabilities that standard QLE lacks. We
also applied the exQLE framework to infer fitness parameters from temporal
sequence data. Overall, exQLE provides a systematic and interpretable
approximation scheme, leveraging analytical cumulant dynamics and reducing
complexity by progressively truncating higher-order cumulants.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [501] [Gene-R1: Reasoning with Data-Augmented Lightweight LLMs for Gene Set Analysis](https://arxiv.org/abs/2509.10575)
*Zhizheng Wang,Yifan Yang,Qiao Jin,Zhiyong Lu*

Main category: q-bio.GN

TL;DR: 提出数据增强学习框架Gene - R1用于基因集分析，实验显示其性能佳且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 现有基因集分析的大语言模型方法多关注专有模型，且缺乏高级推理策略应用研究，存在成本和数据隐私问题。

Method: 引入数据增强学习框架Gene - R1，让轻量级开源大语言模型具备适用于基因集分析的逐步推理能力。

Result: 在1508个分布内基因集实验中性能大幅提升，与商业大语言模型相当；在106个分布外基因集上与商业和大规模大语言模型表现相近。

Conclusion: Gene - R1性能表现良好，在不同基因来源上有强大的泛化性。

Abstract: The gene set analysis (GSA) is a foundational approach for uncovering the
molecular functions associated with a group of genes. Recently, LLM-powered
methods have emerged to annotate gene sets with biological functions together
with coherent explanatory insights. However, existing studies primarily focus
on proprietary models, which have been shown to outperform their open-source
counterparts despite concerns over cost and data privacy. Furthermore, no
research has investigated the application of advanced reasoning strategies to
the GSA task. To address this gap, we introduce Gene-R1, a data-augmented
learning framework that equips lightweight and open-source LLMs with
step-by-step reasoning capabilities tailored to GSA. Experiments on 1,508
in-distribution gene sets demonstrate that Gene-R1 achieves substantial
performance gains, matching commercial LLMs. On 106 out-of-distribution gene
sets, Gene-R1 performs comparably to both commercial and large-scale LLMs,
exhibiting robust generalizability across diverse gene sources.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [502] [The Economics and Game Theory of OSINT Frontline Photography: Risk, Attention, and the Collective Dilemma](https://arxiv.org/abs/2509.10548)
*Jonathan Teagan*

Main category: econ.GN

TL;DR: 本文构建当代武装冲突中开源情报（OSINT）注意力经济的经济模型，分析参与者行为、进行实证研究，探讨政策影响，强调OSINT在信息战中的特性。


<details>
  <summary>Details</summary>
Motivation: 构建经济模型以理解当代武装冲突中OSINT注意力经济的运行机制。

Method: 将注意力概念化为收入，分析时间和风险为成本，运用效用函数和博弈论，结合实证案例研究，参考相关文献和数据。

Result: 展示了OSINT参与者如何分配精力以最大化净注意力收益，通过案例说明前线报道转化为数字收入，分析了OSINT的传播趋势。

Conclusion: OSINT既是公共产品，也是信息战中的竞争性产品，需平衡透明度和行动安全，关注验证伦理和注意力维持叙事。

Abstract: This paper develops an economic model of the Open Source Intelligence (OSINT)
attention economy in contemporary armed conflict. We conceptualize attention
(e.g. social media views, followers, likes) as revenue, and time and risk spent
in analysis as costs. Using utility functions and simple game theoretic setups,
we show how OSINT actors (amateurs, journalists, analysts, and state
operatives) allocate effort to maximize net attention benefit. We incorporate
strategic behaviors such as a first mover advantage (racing to publish) and
prisoner's dilemma scenarios (to share information or hold it back). In
empirical case studies, especially the Ukraine conflict actors like the UAV
unit Madyar's Birds and volunteer channels like Kavkazfighter, illustrate how
battlefront reporting translates into digital revenue (attention) at real cost.
We draw on recent literature and data (e.g., public follower counts, viral
posts) to examine trends such as OSINT virality. Finally, we discuss policy
implications for balancing transparency with operational security, citing calls
for verification ethics and attention sustaining narratives. Our analysis
bridges conflict studies and economics, highlighting OSINT as both a public
good and a competitive product in today's information war.

</details>


### [503] [The impact on health system expenditure in Australia and OECD countries from accelerated NCD mortality decline through prevention or treatment strategies to achieve Sustainable Development Goal Target 3.4](https://arxiv.org/abs/2509.10795)
*Bibha Dhungel,Jingjing Yang,Tim Wilson,Samantha Grimshaw,Emily Bourke,Stephanie Khuu,Tony Blakely*

Main category: econ.GN

TL;DR: 文章估算澳大利亚及34个经合组织国家预防和治疗非传染性疾病（NCDs）的支出，发现降低NCD死亡率是否省钱取决于途径。


<details>
  <summary>Details</summary>
Motivation: 明确预防或治疗NCDs对未来卫生系统支出的相对影响，并估算实现可持续发展目标3.4的预防和治疗途径的支出。

Method: 利用GBD数据估算疾病发病率等变化，预测2030年NCD死亡率风险；对未达目标国家建模两种干预情景；用PMSLT模型估算澳大利亚2022 - 2040年支出变化，并应用到经合组织国家。

Result: 澳大利亚当前趋势难达目标，预防到2030和2040年减少疾病支出，治疗先增后减；仅降低CFRs治疗初期增支，仅增加缓解率与预防节省效果相似；仅瑞典等三国达标，其他经合组织国家支出影响与澳类似。

Conclusion: 降低NCD死亡率是否省钱取决于途径，将NCD死亡率降低与卫生系统节省关联需谨慎。

Abstract: Background: It is unclear what the relative impacts of prevention or
treatment of NCDs are on future health system expenditure. First, we estimated
expenditure in Australia for prevention vs treatment pathways to achieve SDG
target 3.4. Second, we applied the method to 34 other OECD countries.
  Methods: We used GBD data to estimate average annual percentage changes in
disease incidence, remission, and CFRs from 1990-2021, and projected to 2030 to
estimate business-as-usual (BAU) reductions in NCD mortality risk (40q30). For
countries not on track to meet SDG3.4 under BAU, we modelled two intervention
scenarios commencing in 2022 to achieve SDG3.4: (1) prevention via accelerated
incidence reduction; (2) treatment via accelerated increases in remission and
decreases in CFRs. Australian disease expenditure data were input into a PMSLT
model to estimate expenditure changes from 2022 to 2040. Assuming similar
expenditure patterns, the method was applied across OECD countries.
  Findings: In Australia, current trends project a 25% reduction in 40q30 by
2030, short of the 33.3% SDG3.4 target. Achieving this requires a 2.53
percentage point (pp) annual acceleration in incidence decline (prevention) or
1.56pp acceleration in CFR reduction and remission increase (treatment).
Prevention reduces disease expenditure by 0.72%-3.17% by 2030 and 2040;
treatment initially increase expenditure by 0.16%, before reducing it by 0.98%.
A treatment scenario reducing only CFRs increased expenditure initially;
increasing remission alone achieved savings similar to prevention. Only Sweden,
Ireland, and South Korea were on track to meet SDG3.4. Other OECD countries
showed similar expenditure impacts to Australia.
  Interpretation: Whether reducing NCD mortality saves money depends on pathway
taken (prevention or treatment). Care is needed when linking NCD mortality
reduction to health system savings.

</details>


### [504] [Bailouts and Redistribution](https://arxiv.org/abs/2509.10933)
*Mikayel Sukiasyan*

Main category: econ.GN

TL;DR: 研究家庭对金融部门利润暴露不同时的最优宏观审慎监管，发现最优资本购买税为零，最优政策靠存款发行税，简单杠杆税规则能实现大部分福利增益。


<details>
  <summary>Details</summary>
Motivation: 探讨家庭对金融部门利润暴露不同时的最佳宏观审慎监管。

Method: 研究含家庭异质性和市场不完全性的实际经济周期模型，考虑Ramsey规划者用扭曲性劳动税融资转移支付并对专家资产负债表征税的问题。

Result: 最优资本购买税为零，最优政策主要依赖存款发行税，福利增益来自再分配和保险，初始分配越不平等增益越大，简单杠杆税规则能实现大部分福利增益。

Conclusion: 确定了特定情况下的最优宏观审慎监管政策及福利影响，简单杠杆税规则有较好效果。

Abstract: What is the best macroprudential regulation when households differ in their
exposure to profits from the financial sector? To answer the question, I study
a real business cycle model with household heterogeneity and market
incompleteness. In the model, shocks are amplified in states with high
leverage, leading to lower investment. I consider the problem of a Ramsey
planner who can finance transfers with a distortive tax on labor and levy taxes
on the balance sheet components of experts. I show that the optimal tax on
capital purchases is zero and the optimal policy relies mostly on a tax on
deposit issuance. The latter redistributes between agents by affecting the
equilibrium rate on deposits. The welfare gains from optimal policy are due to
both redistribution and insurance and are larger the more unequal the initial
distribution is. A simple tax rule that targets a level of leverage can achieve
most of the welfare gains from optimal policy.

</details>


### [505] [Out-of-sample gravity predictions and trade policy counterfactuals](https://arxiv.org/abs/2509.11271)
*Nicolas Apfel,Holger Breinlich,Nick Green,Dennis Novy,J. M. C. Santos Silva,Tom Zylkin*

Main category: econ.GN

TL;DR: 探讨引力方程用于评估贸易政策情景的适用性，比较不同版本引力方程及机器学习预测方法，发现3 - 向引力模型在样本外平均预测表现好，但预测双边贸易流时集成机器学习方法更优。


<details>
  <summary>Details</summary>
Motivation: 研究引力方程用于评估反事实贸易政策情景时，其适用性取决于样本外预测能力。

Method: 比较不同版本的引力方程，并与基于机器学习的预测方法（如随机森林和神经网络）进行对比。

Result: 3 - 向引力模型在样本外平均预测表现难以被超越；预测个别双边贸易流时，集成机器学习方法可超越3 - 向模型。

Conclusion: 3 - 向引力模型可作为应用贸易政策分析的主要工具，预测双边贸易流时可考虑集成机器学习方法。

Abstract: Gravity equations are often used to evaluate counterfactual trade policy
scenarios, such as the effect of regional trade agreements on trade flows. In
this paper, we argue that the suitability of gravity equations for this purpose
crucially depends on their out-of-sample predictive power. We propose a
methodology that compares different versions of the gravity equation, both
among themselves and with machine learning-based forecast methods such as
random forests and neural networks. We find that the 3-way gravity model is
difficult to beat in terms of out-of-sample average predictive performance,
further justifying its place as the predominant tool for applied trade policy
analysis. However, when the goal is to predict individual bilateral trade
flows, the 3-way model can be outperformed by an ensemble machine learning
method.

</details>


### [506] [The Price of Disaster: Estimating the Impact of Hurricane Harvey on the Texas Construction Labor Market](https://arxiv.org/abs/2509.11501)
*Kartik Ganesh*

Main category: econ.GN

TL;DR: 本文评估哈维飓风对德州受灾县建筑行业工资和就业的影响，发现有长期影响并给出政策启示。


<details>
  <summary>Details</summary>
Motivation: 评估哈维飓风对德州受灾县建筑行业劳动力市场工资和就业的影响。

Method: 采用双重差分事件研究法，对比41个联邦应急管理局指定的受灾县和未受影响的南部对照县2016 - 2019年季度就业与工资普查数据。

Result: 飓风后两季度，受灾县平均对数工资比对照县高约7.2%，并在随后两年维持高位；就业影响较缓慢，六个季度后才显著增加。

Conclusion: 自然灾害会对当地建筑市场产生持续劳动力需求冲击，对灾难恢复规划和劳动力调配有政策意义。

Abstract: This paper estimates the effect of Hurricane Harvey on wages and employment
in the construction labor industry across impacted counties in Texas. Based on
data from the Quarterly Census of Employment and Wages (QCEW) for the period
2016-2019, I adopted a difference-in-differences event study approach by
comparing results in 41 FEMA-designated disaster counties with a set of
unaffected southern control counties. I find that Hurricane Harvey had a large
and long-lasting impact on labor market outcomes in the construction industry.
More precisely, average log wages in treated counties rose by around 7.2
percent compared to control counties two quarters after the hurricane and
remained high for the next two years. Employment effects were more gradual,
showing a statistically significant increase only after six quarters, in line
with the lagged nature of large-scale reconstruction activities. These results
imply that natural disasters can generate persistent labor demand shocks to
local construction markets, with policy implications for disaster recovery
planning and workforce mobilization.

</details>


### [507] [Human or Robot? Evidence from Last-Mile Delivery Service](https://arxiv.org/abs/2509.11562)
*Baorui Li,Xincheng Ma,Brian Rongqing Han,Daizhong Tang,Lei Fu*

Main category: econ.GN

TL;DR: 本文通过分析阿里巴巴最后一公里配送站数据，研究消费者在人力与机器人配送间的选择，发现产品隐私、价值和环境因素影响选择，强调需进行考虑消费者感知的服务设计。


<details>
  <summary>Details</summary>
Motivation: 平台在最后一公里物流中部署机器人，但缺乏关于情境特征如何影响消费者偏好的研究，填补此空白。

Method: 对消费者在人力和机器人服务间的选择进行实证研究，分析241,517个包裹级别的选择数据。

Result: 消费者更倾向用机器人配送隐私敏感包裹和高价值产品，恶劣天气下偏好人力配送，结果稳健。

Conclusion: 配送选择受功能和心理因素影响，需进行考虑消费者感知的情境感知服务设计。

Abstract: As platforms increasingly deploy robots alongside human labor in last-mile
logistics, little is known about how contextual features like product
attributes, environmental conditions, and psychological mechanisms shape
consumer preference in real-world settings. To address this gap, this paper
conducts an empirical study on consumer choice between human versus robot
service, analyzing 241,517 package-level choices from Alibaba's last-mile
delivery stations. We identify how product privacy sensitivity, product value,
and environmental complexity affect consumer preference. Our findings reveal
that consumers are significantly more likely to choose robot delivery for
privacy-sensitive packages (11.49%) and high-value products (0.97% per 1%
increase in value), but prefer human couriers under adverse weather conditions
(1.63%). These patterns are robust to alternative specifications and controls.
These results also underscore that delivery choices are shaped not only by
functional considerations but also by psychological concerns, highlighting the
need for context-aware service design that aligns strategies with consumer
perceptions.

</details>


### [508] [Geopolitical Barriers to Globalization](https://arxiv.org/abs/2509.12084)
*Tianyu Fan,Mai Wo,Wei Xiang*

Main category: econ.GN

TL;DR: 本文研究三个时代地缘政治对齐对全球贸易的影响，构建新指标，得出贸易与地缘政治对齐相关及当前关系恶化使全球贸易降低的结论。


<details>
  <summary>Details</summary>
Motivation: 系统估计和量化三个不同时代地缘政治对齐如何塑造全球贸易。

Method: 用大语言模型构建双边对齐新指标分析政治事件；在引力框架内使用局部投影估计；将弹性纳入定量一般均衡模型。

Result: 贸易流动与地缘政治对齐相关；地缘政治对齐改善一个标准差使双边贸易十年内增加20%；1995 - 2020年地缘政治关系恶化使全球贸易降低7个百分点。

Conclusion: 研究结果为评估大国竞争时代地缘政治碎片化成本提供实证基准。

Abstract: This paper systematically estimates and quantifies how geopolitical alignment
shapes global trade across three distinct eras: the Cold War,
hyper-globalization, and contemporary fragmentation. We construct a novel
measure of bilateral alignment using large language models to compile and
analyze 833,485 political events spanning 193 countries from 1950 to 2024. Our
analysis reveals that trade flows systematically track geopolitical alignment
in both bilateral relationships and aggregate patterns. Using local projections
within a gravity framework, we estimate that a one-standard-deviation
improvement in geopolitical alignment increases bilateral trade by 20 percent
over ten years. Integrating these elasticities into a quantitative general
equilibrium model, we find that deteriorating geopolitical relations have
reduced global trade by 7 percentage points between 1995 and 2020. Our findings
provide empirical benchmarks for evaluating the costs of geopolitical
fragmentation in an era of renewed great power competition.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [509] [Solving ill-conditioned polynomial equations using score-based priors with application to multi-target detection](https://arxiv.org/abs/2509.11397)
*Rafi Beinhorn,Shay Kreymer,Amnon Balanov,Michael Cohen,Alon Zabatani,Tamir Bendory*

Main category: eess.SP

TL;DR: 提出将基于分数的扩散先验与基于矩的估计器相结合的框架来解决非线性逆问题，以多目标检测模型为例展示效果，MNIST 数据实验证实重建精度提升。


<details>
  <summary>Details</summary>
Motivation: 从低阶矩恢复信号是逆问题中一项困难的基础任务，恢复过程常归结为求解病态多项式方程组，需新方法。

Method: 提出将基于分数的扩散先验与基于矩的估计器相结合的框架。

Result: 扩散先验显著改善从三阶矩的恢复，使超分辨率多目标检测问题从病态变为可行，MNIST 数据实验显示重建精度在不同 SNR 水平下均有提升。

Conclusion: 结合生成先验与非线性多项式逆问题是有前景的新方向。

Abstract: Recovering signals from low-order moments is a fundamental yet notoriously
difficult task in inverse problems. This recovery process often reduces to
solving ill-conditioned systems of polynomial equations. In this work, we
propose a new framework that integrates score-based diffusion priors with
moment-based estimators to regularize and solve these nonlinear inverse
problems. This introduces a new role for generative models: stabilizing
polynomial recovery from noisy statistical features. As a concrete application,
we study the multi-target detection (MTD) model in the high-noise regime. We
demonstrate two main results: (i) diffusion priors substantially improve
recovery from third-order moments, and (ii) they make the super-resolution MTD
problem, otherwise ill-posed, feasible. Numerical experiments on MNIST data
confirm consistent gains in reconstruction accuracy across SNR levels. Our
results suggest a promising new direction for combining generative priors with
nonlinear polynomial inverse problems.

</details>


### [510] [Distributed Gossip-GAN for Low-overhead CSI Feedback Training in FDD mMIMO-OFDM Systems](https://arxiv.org/abs/2509.10490)
*Yuwen Cao,Guijun Liu,Tomoaki Ohtsuki,Howard H. Yang,Tony Q. S. Quek*

Main category: eess.SP

TL;DR: 提出Gossip - GAN辅助的CSI反馈训练框架，解决现有DAE方法的带宽、隐私和灾难性遗忘问题，模拟显示有良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有DAE方法依赖大量基站数据训练，有带宽和隐私问题，且存在灾难性遗忘，在用户移动和新信道环境下需重新训练。

Method: 提出Gossip - GAN框架，用户收集少量数据训练GAN模型，采用全分布式八卦学习策略。

Result: 能达到与集中训练相似的CSI反馈精度，解决移动场景的灾难性遗忘问题，大幅降低上行带宽使用，且具有内在鲁棒性。

Conclusion: Gossip - GAN框架有效，能在低开销下进行CSI反馈训练，保护用户隐私。

Abstract: The deep autoencoder (DAE) framework has turned out to be efficient in
reducing the channel state information (CSI) feedback overhead in massive
multiple-input multipleoutput (mMIMO) systems. However, these DAE approaches
presented in prior works rely heavily on large-scale data collected through the
base station (BS) for model training, thus rendering excessive bandwidth usage
and data privacy issues, particularly for mMIMO systems. When considering
users' mobility and encountering new channel environments, the existing CSI
feedback models may often need to be retrained. Returning back to previous
environments, however, will make these models perform poorly and face the risk
of catastrophic forgetting. To solve the above challenging problems, we propose
a novel gossiping generative adversarial network (Gossip-GAN)-aided CSI
feedback training framework. Notably, Gossip-GAN enables the CSI feedback
training with low-overhead while preserving users' privacy. Specially, each
user collects a small amount of data to train a GAN model. Meanwhile, a fully
distributed gossip-learning strategy is exploited to avoid model overfitting,
and to accelerate the model training as well. Simulation results demonstrate
that Gossip-GAN can i) achieve a similar CSI feedback accuracy as centralized
training with real-world datasets, ii) address catastrophic forgetting
challenges in mobile scenarios, and iii) greatly reduce the uplink bandwidth
usage. Besides, our results show that the proposed approach possesses an
inherent robustness.

</details>


### [511] [YOLO-based Bearing Fault Diagnosis With Continuous Wavelet Transform](https://arxiv.org/abs/2509.03070)
*Po-Heng Chou,Wei-Lung Mao,Ru-Ping Lin*

Main category: eess.SP

TL;DR: 提出基于YOLO的空间轴承故障诊断框架CWT - YOLO，在多数据集上表现优于基线模型，且能可视化故障位置。


<details>
  <summary>Details</summary>
Motivation: 为空间轴承故障诊断提供更准确、更具泛化性的方法。

Method: 用Morlet小波将一维振动信号转换为时间 - 频率谱图，用YOLOv9、v10、v11模型处理谱图进行故障类型分类。

Result: CWT - YOLO管道在三个基准数据集上比基线MCNN - LSTM模型有更高准确性和泛化性，YOLOv11在各数据集有高mAP分数。

Conclusion: 所提CWT - YOLO管道是旋转机械状态监测的实用解决方案。

Abstract: This letter proposes a YOLO-based framework for spatial bearing fault
diagnosis using time-frequency spectrograms derived from continuous wavelet
transform (CWT). One-dimensional vibration signals are first transformed into
time-frequency spectrograms using Morlet wavelets to capture transient fault
signatures. These spectrograms are then processed by YOLOv9, v10, and v11
models to classify fault types. Evaluated on three benchmark datasets,
including Case Western Reserve University (CWRU), Paderborn University (PU),
and Intelligent Maintenance System (IMS), the proposed CWT-YOLO pipeline
achieves significantly higher accuracy and generalizability than the baseline
MCNN-LSTM model. Notably, YOLOv11 reaches mAP scores of 99.4% (CWRU), 97.8%
(PU), and 99.5% (IMS). In addition, its region-aware detection mechanism
enables direct visualization of fault locations in spectrograms, offering a
practical solution for condition monitoring in rotating machinery.

</details>


### [512] [FlowECG: Using Flow Matching to Create a More Efficient ECG Signal Generator](https://arxiv.org/abs/2509.10491)
*Vitalii Bondar,Serhii Semenov,Vira Babenko,Dmytro Holovniak*

Main category: eess.SP

TL;DR: 提出FlowECG方法生成合成心电图，减少采样步骤，提高计算效率，可用于资源有限临床场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的合成心电图生成方法存在计算瓶颈，难以用于临床部署。

Method: 提出FlowECG方法，用连续流动力学取代迭代扩散过程，通过求解常微分方程学习从噪声到数据分布的直接传输路径。

Result: 在PTB - XL数据集上评估，FlowECG在200次神经函数评估时达到与SSSD - ECG相当的性能，在三个指标上优于基线；用10 - 25次评估即可达到与扩散方法200次评估相当的结果。

Conclusion: FlowECG在减少采样步骤的同时保持生成质量，降低一个数量级的计算需求，可用于资源有限的临床场景。

Abstract: Synthetic electrocardiogram generation serves medical AI applications
requiring privacy-preserving data sharing and training dataset augmentation.
Current diffusion-based methods achieve high generation quality but require
hundreds of neural network evaluations during sampling, creating computational
bottlenecks for clinical deployment. We propose FlowECG, a flow matching
approach that adapts the SSSD-ECG architecture by replacing the iterative
diffusion process with continuous flow dynamics. Flow matching learns direct
transport paths from noise to data distributions through ordinary differential
equation solving. We evaluate our method on the PTB-XL dataset using Dynamic
Time Warping, Wasserstein distance, Maximum Mean Discrepancy, and spectral
similarity metrics. FlowECG matches SSSD-ECG performance at 200 neural function
evaluations, outperforming the baseline on three metrics. The key finding shows
that FlowECG maintains generation quality with substantially fewer sampling
steps, achieving comparable results with 10-25 evaluations compared to 200 for
diffusion methods. This efficiency improvement reduces computational
requirements by an order of magnitude while preserving physiologically
realistic 12-lead ECG characteristics. The approach enables practical
deployment in resource-limited clinical settings where real-time generation or
large-scale synthetic data creation is needed.

</details>


### [513] [On the Impact of Downstream Tasks on Sampling and Reconstructing Noisy Graph Signals](https://arxiv.org/abs/2509.10874)
*Baskaran Sripathmanathan,Xiaowen Dong,Michael Bronstein*

Main category: eess.SP

TL;DR: 研究图信号重建与样本选择用于分类任务，给出分类误差理论表征，推导新采样方法并展示优势。


<details>
  <summary>Details</summary>
Motivation: 进行图信号重建和样本选择以用于分类任务，对比不同重建误差。

Method: 给出适用于多种常用重建方法的分类误差的一般理论表征。

Result: 利用结果为线性化图卷积网络推导新的最优采样方法，且优于其他基于图信号处理的方法。

Conclusion: 所提出的理论和方法在图信号分类任务中有一定优势和适用性。

Abstract: We investigate graph signal reconstruction and sample selection for
classification tasks. We present general theoretical characterisations of
classification error applicable to multiple commonly used reconstruction
methods, and compare that to the classical reconstruction error. We demonstrate
the applicability of our results by using them to derive new optimal sampling
methods for linearized graph convolutional networks, and show improvement over
other graph signal processing based methods.

</details>


### [514] [When marine radar target detection meets pretrained large language models](https://arxiv.org/abs/2509.12110)
*Qiying Hu,Linping Zhang,Xueqian Wang,Gang Li,Yu Liu,Xiao-Ping Zhang*

Main category: eess.SP

TL;DR: 提出结合特征预处理与大语言模型的框架处理雷达回波信号序列特征，实验表明该方法优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习算法处理雷达回波信号序列特征时存在特征段冗余、模型大小受限等问题。

Method: 预处理模块对雷达序列特征进行分词、使用补丁选择算法过滤无用段并投影到预训练大语言模型特征空间，利用处理后的嵌入，微调归一化层。

Result: 在测量数据集的监督学习测试中，所提方法显著优于现有基线。

Conclusion: 所提结合特征预处理与大语言模型的框架有效提升了处理雷达回波信号序列特征的性能。

Abstract: Deep learning (DL) methods are widely used to extract high-dimensional
patterns from the sequence features of radar echo signals. However,
conventional DL algorithms face challenges such as redundant feature segments,
and constraints from restricted model sizes. To address these issues, we
propose a framework that integrates feature preprocessing with large language
models (LLMs). Our preprocessing module tokenizes radar sequence features,
applies a patch selection algorithm to filter out uninformative segments, and
projects the selected patches into embeddings compatible with the feature space
of pre-trained LLMs. Leveraging these refined embeddings, we incorporate a
pre-trained LLM, fine-tuning only the normalization layers to reduce training
burdens while enhancing performance. Experiments on measured datasets
demonstrate that the proposed method significantly outperforms the
state-of-the-art baselines on supervised learning tests.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [515] [Hybrid Quantum Neural Networks for Efficient Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2509.11046)
*Seon-Geun Jeong,Kyeong-Hwan Moon,Won-Joo Hwang*

Main category: cs.ET

TL;DR: 本文提出混合量子神经网络HQNN用于蛋白 - 配体结合亲和力预测，该模型参数高效，在NISQ设备上可行，性能优于经典神经网络。


<details>
  <summary>Details</summary>
Motivation: 传统实验确定蛋白 - 配体结合亲和力耗时昂贵，AI预测需大模型和大量计算资源，量子机器学习有潜力解决这些问题，但存在一些挑战待解决。

Method: 提出混合量子神经网络HQNN，在潜在特征空间近似非线性函数。

Result: HQNN与经典神经网络相比，实现了相当或更优的性能和参数效率。

Conclusion: 混合量子机器学习在计算药物发现中有潜力，能解决蛋白 - 配体结合亲和力预测的计算挑战。

Abstract: Protein-ligand binding affinity is critical in drug discovery, but
experimentally determining it is time-consuming and expensive. Artificial
intelligence (AI) has been used to predict binding affinity, significantly
accelerating this process. However, the high-performance requirements and vast
datasets involved in affinity prediction demand increasingly large AI models,
requiring substantial computational resources and training time. Quantum
machine learning has emerged as a promising solution to these challenges. In
particular, hybrid quantum-classical models can reduce the number of parameters
while maintaining or improving performance compared to classical counterparts.
Despite these advantages, challenges persist: why hybrid quantum models achieve
these benefits, whether quantum neural networks (QNNs) can replace classical
neural networks, and whether such models are feasible on noisy
intermediate-scale quantum (NISQ) devices. This study addresses these
challenges by proposing a hybrid quantum neural network (HQNN) that empirically
demonstrates the capability to approximate non-linear functions in the latent
feature space derived from classical embedding. The primary goal of this study
is to achieve a parameter-efficient model in binding affinity prediction while
ensuring feasibility on NISQ devices. Numerical results indicate that HQNN
achieves comparable or superior performance and parameter efficiency compared
to classical neural networks, underscoring its potential as a viable
replacement. This study highlights the potential of hybrid QML in computational
drug discovery, offering insights into its applicability and advantages in
addressing the computational challenges of protein-ligand binding affinity
prediction.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [516] [Dual Reinforcement Learning Synergy in Resource Allocation: Emergence of Self-Organized Momentum Strategy](https://arxiv.org/abs/2509.11161)
*Zhen-Na Zhang,Guo-Zhong Zhen,Sheng-Feng Deng,Li Chen,Chao-Ran Cai,Ji-Qiang Zhang*

Main category: nlin.AO

TL;DR: 本文聚焦少数者博弈中双强化学习策略的协同以优化资源分配，研究混合结构群体，发现子群体协同效应、Q子群体内部和外部协同簇，分析策略作用及条件，丰富相关理论并提供实践见解。


<details>
  <summary>Details</summary>
Motivation: 优化自然生态和人类社会中的资源分配，研究少数者博弈中双强化学习策略的协同作用。

Method: 研究包含Q学习策略子群体和经典策略子群体的混合结构群体，进行数学分析。

Result: 发现子群体协同效应、一阶相变，Q子群体存在内部和外部协同簇，经典动量策略有作用也有弊端，冻结效应是协同的关键前提。

Conclusion: 全面探索了少数者博弈中复杂的资源分配动态，揭示多种协同机制及条件，丰富了基于强化学习的资源分配理论，提供了实践见解。

Abstract: In natural ecosystems and human societies, self-organized resource allocation
and policy synergy are ubiquitous and significant. This work focuses on the
synergy between Dual Reinforcement Learning Policies in the Minority Game
(DRLP-MG) to optimize resource allocation. Our study examines a
mixed-structured population with two sub-populations: a Q-subpopulation using
Q-learning policy and a C-subpopulation adopting the classical policy. We first
identify a synergy effect between these subpopulations. A first-order phase
transition occurs as the mixing ratio of the subpopulations changes. Further
analysis reveals that the Q-subpopulation consists of two internal synergy
clusters (IS-clusters) and a single external synergy cluster (ES-cluster). The
former contribute to the internal synergy within the Q-subpopulation through
synchronization and anti-synchronization, whereas the latter engages in the
inter-subpopulation synergy. Within the ES-cluster, the classical momentum
strategy in the financial market manifests and assumes a crucial role in the
inter-subpopulation synergy. This particular strategy serves to prevent
long-term under-utilization of resources. However, it also triggers trend
reversals and leads to a decrease in rewards for those who adopt it. Our
research reveals that the frozen effect, in either the C- or Q-subpopulation,
is a crucial prerequisite for synergy, consistent with previous studies. We
also conduct mathematical analyses on subpopulation synergy effects and the
synchronization and anti-synchronization forms of IS-clusters in the
Q-subpopulation. Overall, our work comprehensively explores the complex
resource-allocation dynamics in DRLP-MG, uncovers multiple synergy mechanisms
and their conditions, enriching the theoretical understanding of
reinforcement-learning-based resource allocation and offering valuable
practical insights

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [517] [FireGNN: Neuro-Symbolic Graph Neural Networks with Trainable Fuzzy Rules for Interpretable Medical Image Classification](https://arxiv.org/abs/2509.10510)
*Prajit Sengupta,Islem Rekik*

Main category: eess.IV

TL;DR: 提出可解释的图学习框架FireGNN用于医学图像分类，结合模糊规则和自监督任务，在多个数据集上表现好且有可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类需高预测性能和可解释性，标准GNN是黑盒，缺乏透明度和可用性。

Method: 提出FireGNN框架，将可训练模糊规则集成到GNN中，利用拓扑描述符进行符号推理，探索辅助自监督任务作为评估基准。

Result: 在五个MedMNIST基准和合成数据集MorphoMNIST上取得良好性能，能生成基于规则的可解释解释。

Conclusion: 首次将可训练模糊规则集成到GNN中，为医学图像分类提供可解释解决方案。

Abstract: Medical image classification requires not only high predictive performance
but also interpretability to ensure clinical trust and adoption. Graph Neural
Networks (GNNs) offer a powerful framework for modeling relational structures
within datasets; however, standard GNNs often operate as black boxes, limiting
transparency and usability, particularly in clinical settings. In this work, we
present an interpretable graph-based learning framework named FireGNN that
integrates trainable fuzzy rules into GNNs for medical image classification.
These rules embed topological descriptors - node degree, clustering
coefficient, and label agreement - using learnable thresholds and sharpness
parameters to enable intrinsic symbolic reasoning. Additionally, we explore
auxiliary self-supervised tasks (e.g., homophily prediction, similarity
entropy) as a benchmark to evaluate the contribution of topological learning.
Our fuzzy-rule-enhanced model achieves strong performance across five MedMNIST
benchmarks and the synthetic dataset MorphoMNIST, while also generating
interpretable rule-based explanations. To our knowledge, this is the first
integration of trainable fuzzy rules within a GNN.

</details>


### [518] [Data-Efficient Psychiatric Disorder Detection via Self-supervised Learning on Frequency-enhanced Brain Networks](https://arxiv.org/abs/2509.10524)
*Mujie Liu,Mengchu Zhu,Qichao Dong,Ting Dang,Jiangang Ma,Jing Ren,Feng Xia*

Main category: eess.IV

TL;DR: 提出FENet框架用于fMRI数据的精神疾病检测，结合时频域信息，在小样本数据上表现优，强调高频信息重要性。


<details>
  <summary>Details</summary>
Motivation: 精神疾病神经活动变化复杂，fMRI数据诊断有数据稀缺和信息多样挑战，现有图自监督学习方法多关注时域，忽略频域信息。

Method: 提出FENet框架，构建多视图脑网络，引入频域信息，用特定编码器捕捉时频谱特征，采用域一致性引导学习目标。

Result: 在两个真实医疗数据集上，FENet优于现有方法，在数据极少时仍表现良好。

Conclusion: FENet能有效结合时频域信息提升小样本精神疾病检测效果，高频信息对检测至关重要。

Abstract: Psychiatric disorders involve complex neural activity changes, with
functional magnetic resonance imaging (fMRI) data serving as key diagnostic
evidence. However, data scarcity and the diverse nature of fMRI information
pose significant challenges. While graph-based self-supervised learning (SSL)
methods have shown promise in brain network analysis, they primarily focus on
time-domain representations, often overlooking the rich information embedded in
the frequency domain. To overcome these limitations, we propose
Frequency-Enhanced Network (FENet), a novel SSL framework specially designed
for fMRI data that integrates time-domain and frequency-domain information to
improve psychiatric disorder detection in small-sample datasets. FENet
constructs multi-view brain networks based on the inherent properties of fMRI
data, explicitly incorporating frequency information into the learning process
of representation. Additionally, it employs domain-specific encoders to capture
temporal-spectral characteristics, including an efficient frequency-domain
encoder that highlights disease-relevant frequency features. Finally, FENet
introduces a domain consistency-guided learning objective, which balances the
utilization of diverse information and generates frequency-enhanced brain graph
representations. Experiments on two real-world medical datasets demonstrate
that FENet outperforms state-of-the-art methods while maintaining strong
performance in minimal data conditions. Furthermore, we analyze the correlation
between various frequency-domain features and psychiatric disorders,
emphasizing the critical role of high-frequency information in disorder
detection.

</details>


### [519] [Branched Broomrape Detection in Tomato Farms Using Satellite Imagery and Time-Series Analysis](https://arxiv.org/abs/2509.10804)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Parastoo Farajpoor,Hamid Jafarbiglu,Mohsen Mesgaran*

Main category: eess.IV

TL;DR: 本文提出利用Sentinel - 2影像和时间序列分析识别加州受列当侵染番茄田的端到端流程，模型有较高准确率，显示卫星驱动时间序列建模用于检测寄生胁迫的潜力。


<details>
  <summary>Details</summary>
Motivation: 分枝列当威胁番茄生产，其生命周期和种子特性使早期检测至关重要。

Method: 构建端到端流程，利用Sentinel - 2影像和时间序列分析，处理光谱波段和几何信息，计算植被指数和植物性状，用神经网络校准，训练LSTM网络。

Result: 模型训练准确率88%，测试准确率87%，有较高的精确率、召回率和F1值，部分特征信息量大。

Conclusion: 卫星驱动的时间序列建模在番茄农场寄生胁迫的可扩展检测方面有前景。

Abstract: Branched broomrape (Phelipanche ramosa (L.) Pomel) is a chlorophyll-deficient
parasitic plant that threatens tomato production by extracting nutrients from
the host, with reported yield losses up to 80 percent. Its mostly subterranean
life cycle and prolific seed production (more than 200,000 seeds per plant,
viable for up to 20 years) make early detection essential. We present an
end-to-end pipeline that uses Sentinel-2 imagery and time-series analysis to
identify broomrape-infested tomato fields in California. Regions of interest
were defined from farmer-reported infestations, and images with less than 10
percent cloud cover were retained. We processed 12 spectral bands and
sun-sensor geometry, computed 20 vegetation indices (e.g., NDVI, NDMI), and
derived five plant traits (Leaf Area Index, Leaf Chlorophyll Content, Canopy
Chlorophyll Content, Fraction of Absorbed Photosynthetically Active Radiation,
and Fractional Vegetation Cover) using a neural network calibrated with
ground-truth and synthetic data. Trends in Canopy Chlorophyll Content
delineated transplanting-to-harvest periods, and phenology was aligned using
growing degree days. Vegetation pixels were segmented and used to train a Long
Short-Term Memory (LSTM) network on 18,874 pixels across 48 growing-degree-day
time points. The model achieved 88 percent training accuracy and 87 percent
test accuracy, with precision 0.86, recall 0.92, and F1 0.89. Permutation
feature importance ranked NDMI, Canopy Chlorophyll Content, FAPAR, and a
chlorophyll red-edge index as most informative, consistent with the
physiological effects of infestation. Results show the promise of
satellite-driven time-series modeling for scalable detection of parasitic
stress in tomato farms.

</details>


### [520] [An Interpretable Ensemble Framework for Multi-Omics Dementia Biomarker Discovery Under HDLSS Conditions](https://arxiv.org/abs/2509.10527)
*Byeonghee Lee,Joonsung Kang*

Main category: eess.IV

TL;DR: 提出结合GAT、MOVE等的集成方法用于神经退行性疾病生物标志物发现，性能优于现有方法并揭示潜在分子机制。


<details>
  <summary>Details</summary>
Motivation: 神经退行性疾病生物标志物发现需要能在低样本条件下整合高维多组学数据的框架。

Method: 提出结合Graph Attention Networks (GAT)、MultiOmics Variational AutoEncoder (MOVE)、Elastic - net稀疏回归和Storey's False Discovery Rate (FDR)的集成方法，与DIABLO等方法进行对比，用模拟多组学数据和ADNI数据集评估。

Result: 该方法在预测准确性、特征选择精度和生物相关性方面表现更优，对两个数据集得出的生物标志物基因图谱进行可视化和解读。

Conclusion: 所提方法可用于神经退行性疾病生物标志物发现，有助于揭示痴呆潜在分子机制。

Abstract: Biomarker discovery in neurodegenerative diseases requires robust,
interpretable frameworks capable of integrating high-dimensional multi-omics
data under low-sample conditions. We propose a novel ensemble approach
combining Graph Attention Networks (GAT), MultiOmics Variational AutoEncoder
(MOVE), Elastic-net sparse regression, and Storey's False Discovery Rate (FDR).
This framework is benchmarked against state-of-the-art methods including
DIABLO, MOCAT, AMOGEL, and MOMLIN. We evaluate performance using both simulated
multi-omics data and the Alzheimer's Disease Neuroimaging Initiative (ADNI)
dataset. Our method demonstrates superior predictive accuracy, feature
selection precision, and biological relevance. Biomarker gene maps derived from
both datasets are visualized and interpreted, offering insights into latent
molecular mechanisms underlying dementia.

</details>


### [521] [EMeRALDS: Electronic Medical Record Driven Automated Lung Nodule Detection and Classification in Thoracic CT Images](https://arxiv.org/abs/2509.11714)
*Hafza Eman,Furqan Shaukat,Muhammad Hamza Zafar,Syed Muhammad Anwar*

Main category: eess.IV

TL;DR: 研究提出基于大视觉语言模型的计算机辅助诊断系统检测和分类肺部结节，在LIDC - IDRI数据集测试表现良好，有望提升肺癌早检。


<details>
  <summary>Details</summary>
Motivation: 肺癌因诊断延迟和早期检测差致死亡率高，需开发准确检测和分类肺部结节的计算机辅助诊断系统。

Method: 提出端到端CAD管道，包含基于SAM2和CLIP的检测模块及计算相似度得分的诊断模块，结合合成电子病历进行最终分类，在LIDC - IDRI数据集测试。

Result: 方法在零样本肺结节分析中表现好，检测模块分割结节Dice分数0.92、IoU 0.85，诊断模块恶性分类特异性0.97，超现有全监督方法。

Conclusion: 视觉语言模型与放射组学和合成电子病历结合可准确CAD肺部结节，系统有潜力提升肺癌早检、诊断信心和患者管理。

Abstract: Objective: Lung cancer is a leading cause of cancer-related mortality
worldwide, primarily due to delayed diagnosis and poor early detection. This
study aims to develop a computer-aided diagnosis (CAD) system that leverages
large vision-language models (VLMs) for the accurate detection and
classification of pulmonary nodules in computed tomography (CT) scans.
  Methods: We propose an end-to-end CAD pipeline consisting of two modules: (i)
a detection module (CADe) based on the Segment Anything Model 2 (SAM2), in
which the standard visual prompt is replaced with a text prompt encoded by CLIP
(Contrastive Language-Image Pretraining), and (ii) a diagnosis module (CADx)
that calculates similarity scores between segmented nodules and radiomic
features. To add clinical context, synthetic electronic medical records (EMRs)
were generated using radiomic assessments by expert radiologists and combined
with similarity scores for final classification. The method was tested on the
publicly available LIDC-IDRI dataset (1,018 CT scans).
  Results: The proposed approach demonstrated strong performance in zero-shot
lung nodule analysis. The CADe module achieved a Dice score of 0.92 and an IoU
of 0.85 for nodule segmentation. The CADx module attained a specificity of 0.97
for malignancy classification, surpassing existing fully supervised methods.
  Conclusions: The integration of VLMs with radiomics and synthetic EMRs allows
for accurate and clinically relevant CAD of pulmonary nodules in CT scans. The
proposed system shows strong potential to enhance early lung cancer detection,
increase diagnostic confidence, and improve patient management in routine
clinical workflows.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [522] [A Dichotomy Theorem for Multi-Pass Streaming CSPs](https://arxiv.org/abs/2509.11399)
*Yumou Fei,Dor Minzer,Shuo Wang*

Main category: cs.CC

TL;DR: 本文给出了所有CSP问题在多项式多轮流算法中的二分结果，包括近似算法和硬度结果。


<details>
  <summary>Details</summary>
Motivation: 研究CSP问题在流算法下的复杂度特性，给出复杂度的二分结果。

Method: 近似算法基于CSP的线性规划松弛和分布式算法；硬度结果通过将线性规划的积分间隙转化为困难实例，结合通信复杂度问题分析，运用离散傅里叶分析。

Result: 对于任意参数，存在常数s，有常数轮、$O_{\varepsilon}(\log n)$ 空间的随机流算法解决 $\text{MaxCSP}(\mathcal{F})[c,s - \varepsilon]$ 问题；解决 $\text{MaxCSP}(\mathcal{F})[c,s + \varepsilon]$ 问题的p轮流算法需 $\Omega_{\varepsilon}(n^{1/3}/p)$ 空间。

Conclusion: 得到了CSP问题在流算法下的复杂度二分结果，为该领域研究提供了理论依据。

Abstract: We show a dichotomy result for $p$-pass streaming algorithms for all CSPs and
for up to polynomially many passes. More precisely, we prove that for any arity
parameter $k$, finite alphabet $\Sigma$, collection $\mathcal{F}$ of $k$-ary
predicates over $\Sigma$ and any $c\in (0,1)$, there exists $0<s\leq c$ such
that:
  1. For any $\varepsilon>0$ there is a constant pass, $O_{\varepsilon}(\log
n)$-space randomized streaming algorithm solving the promise problem
$\text{MaxCSP}(\mathcal{F})[c,s-\varepsilon]$. That is, the algorithm accepts
inputs with value at least $c$ with probability at least $2/3$, and rejects
inputs with value at most $s-\varepsilon$ with probability at least $2/3$.
  2. For all $\varepsilon>0$, any $p$-pass (even randomized) streaming
algorithm that solves the promise problem
$\text{MaxCSP}(\mathcal{F})[c,s+\varepsilon]$ must use
$\Omega_{\varepsilon}(n^{1/3}/p)$ space.
  Our approximation algorithm is based on a certain linear-programming
relaxation of the CSP and on a distributed algorithm that approximates its
value. This part builds on the works [Yoshida, STOC 2011] and [Saxena, Singer,
Sudan, Velusamy, SODA 2025]. For our hardness result we show how to translate
an integrality gap of the linear program into a family of hard instances, which
we then analyze via studying a related communication complexity problem. That
analysis is based on discrete Fourier analysis and builds on a prior work of
the authors and on the work [Chou, Golovnev, Sudan, Velingker, Velusamy, J.ACM
2024].

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [523] [Structuring Definitions in Mathematical Libraries](https://arxiv.org/abs/2509.10828)
*Alena Gusakov,Peter Nelson,Stephen Watt*

Main category: cs.SC

TL;DR: 本文探讨数学定义形式化难题，观察好定义特征并研究Lean定理证明器数学库定义提炼过程，还提及计算机代数系统库设计相关问题。


<details>
  <summary>Details</summary>
Motivation: 解决数学理论在证明助手或计算机代数系统中形式化时，定义结构化困难、学习曲线陡峭及进展缓慢的问题。

Method: 观察好定义的特征，研究Lean定理证明器社区数学库中定义的提炼过程，并举例计算机代数系统库设计的相关问题。

Result: 发现定义形式化存在诸多需考虑因素，需经反复试验筛选，部分困难与计算机代数系统库设计有共性。

Conclusion: 分析了数学定义形式化中好定义的特点和提炼过程，指出与计算机代数系统库设计的关联。

Abstract: Codifying mathematical theories in a proof assistant or computer algebra
system is a challenging task, of which the most difficult part is,
counterintuitively, structuring definitions. This results in a steep learning
curve for new users and slow progress in formalizing even undergraduate level
mathematics. There are many considerations one has to make, such as level of
generality, readability, and ease of use in the type system, and there are
typically multiple equivalent or related definitions from which to choose.
Often, a definition that is ultimately selected for formalization is settled on
after a lengthy trial and error process. This process involves testing
potential definitions for usability by formalizing standard theorems about
them, and weeding out the definitions that are unwieldy.
  Inclusion of a formal definition in a centralized community-run mathematical
library is typically an indication that the definition is "good." For this
reason, in this survey, we make some observations about what makes a definition
"good," and examine several case studies of the refining process for
definitions that have ultimately been added to the Lean Theorem Prover
community-run mathematical library, mathlib. We observe that some of the
difficulties are shared with the design of libraries for computer algebra
systems, and give examples of related issues in that context.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [524] [The Honest Truth About Causal Trees: Accuracy Limits for Heterogeneous Treatment Effect Estimation](https://arxiv.org/abs/2509.11381)
*Matias D. Cattaneo,Jason M. Klusowski,Ruiqi Rae Yu*

Main category: math.ST

TL;DR: 研究Athey和Imbens提出的因果决策树估计量，证明其估计误差下界，指出在基本条件下无法达到多项式收敛率，诚实性改进有限，模拟验证理论。


<details>
  <summary>Details</summary>
Motivation: Athey和Imbens提出的“诚实”因果决策树估计量成为标准，需研究其估计误差情况。

Method: 研究其估计量及其变体，建立估计误差下界。

Result: 这些流行的异质处理效应估计量在基本条件下无法达到多项式收敛率，诚实性改进有限，实际表现可能不佳甚至不一致。

Conclusion: 常用的因果决策树估计量存在局限性，理论结果通过模拟得到验证。

Abstract: Recursive decision trees have emerged as a leading methodology for
heterogeneous causal treatment effect estimation and inference in experimental
and observational settings. These procedures are fitted using the celebrated
CART (Classification And Regression Tree) algorithm [Breiman et al., 1984], or
custom variants thereof, and hence are believed to be "adaptive" to
high-dimensional data, sparsity, or other specific features of the underlying
data generating process. Athey and Imbens [2016] proposed several "honest"
causal decision tree estimators, which have become the standard in both
academia and industry. We study their estimators, and variants thereof, and
establish lower bounds on their estimation error. We demonstrate that these
popular heterogeneous treatment effect estimators cannot achieve a
polynomial-in-$n$ convergence rate under basic conditions, where $n$ denotes
the sample size. Contrary to common belief, honesty does not resolve these
limitations and at best delivers negligible logarithmic improvements in sample
size or dimension. As a result, these commonly used estimators can exhibit poor
performance in practice, and even be inconsistent in some settings. Our
theoretical insights are empirically validated through simulations.

</details>


### [525] [Testing for LLM response differences: the case of a composite null consisting of semantically irrelevant query perturbations](https://arxiv.org/abs/2509.10963)
*Aranyak Acharyya,Carey E. Priebe,Hayden S. Helm*

Main category: math.ST

TL;DR: 本文探讨生成模型响应分布测试问题，提出结合语义相似查询改进测试，分析了二元响应情况并讨论实用性。


<details>
  <summary>Details</summary>
Motivation: 传统统计假设检验在判断生成模型输入查询响应分布是否相同时，结果可能与用户需求不符，因响应分布对语义无关扰动敏感。

Method: 在测试过程中纳入语义相似查询的考量，对用户定义的语义相似查询到响应分布的映射进行估计。

Result: 针对响应为二元的情况，证明所提出的测试渐近有效且一致。

Conclusion: 该方法能解决传统测试与用户需求的不一致问题，并讨论了关于功效和计算的重要实际考虑。

Abstract: Given an input query, generative models such as large language models produce
a random response drawn from a response distribution. Given two input queries,
it is natural to ask if their response distributions are the same. While
traditional statistical hypothesis testing is designed to address this
question, the response distribution induced by an input query is often
sensitive to semantically irrelevant perturbations to the query, so much so
that a traditional test of equality might indicate that two semantically
equivalent queries induce statistically different response distributions. As a
result, the outcome of the statistical test may not align with the user's
requirements. In this paper, we address this misalignment by incorporating into
the testing procedure consideration of a collection of semantically similar
queries. In our setting, the mapping from the collection of user-defined
semantically similar queries to the corresponding collection of response
distributions is not known a priori and must be estimated, with a fixed budget.
Although the problem we address is quite general, we focus our analysis on the
setting where the responses are binary, show that the proposed test is
asymptotically valid and consistent, and discuss important practical
considerations with respect to power and computation.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [526] [Fast Percolation Centrality Approximation with Importance Sampling](https://arxiv.org/abs/2509.11454)
*Antonio Cruciani,Leonardo Pellegrina*

Main category: cs.SI

TL;DR: 提出PercIS算法近似图中所有节点的渗流中心性，实验表明其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 渗流中心性在现代规模网络中难以精确计算，现有基于采样的近似方法存在局限性。

Method: 提出基于重要性采样的新采样算法PercIS，并证明了达到高质量近似所需的样本量边界。

Result: PercIS能计算高质量估计值，可扩展到大型真实网络，在样本量、准确性和运行时间方面显著优于现有技术。

Conclusion: PercIS算法是一种有效近似渗流中心性的方法，优于现有基于采样的近似方法。

Abstract: In this work we present PercIS, an algorithm based on Importance Sampling to
approximate the percolation centrality of all the nodes of a graph. Percolation
centrality is a generalization of betweenness centrality to attributed graphs,
and is a useful measure to quantify the importance of the vertices in a
contagious process or to diffuse information. However, it is impractical to
compute it exactly on modern-sized networks.
  First, we highlight key limitations of state-of-the-art sampling-based
approximation methods for the percolation centrality, showing that in most
cases they cannot achieve accurate solutions efficiently. Then, we propose and
analyze a novel sampling algorithm based on Importance Sampling, proving tight
sample size bounds to achieve high-quality approximations.
  Our extensive experimental evaluation shows that PercIS computes high-quality
estimates and scales to large real-world networks, while significantly
outperforming, in terms of sample sizes, accuracy and running times, the
state-of-the-art.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [527] [ORQ: Complex Analytics on Private Data with Strong Security Guarantees](https://arxiv.org/abs/2509.10793)
*Eli Baum,Sam Buxbaum,Nitin Mathai,Muhammad Faisal,Vasiliki Kalavri,Mayank Varia,John Liagouris*

Main category: cs.CR

TL;DR: 介绍ORQ系统，可利用MPC对大型私有数据集进行协作分析，评估显示其能减少执行时间、处理更大数据集。


<details>
  <summary>Details</summary>
Motivation: 实现对大型私有数据集的协作分析，保护数据安全并高效评估复杂关系查询。

Method: 利用实际查询结构消除安全连接的二次成本，构建通用不经意算子、数据并行矢量化查询引擎、通信层和数据流API。

Result: 在LAN和WAN部署中评估，相比现有方案，显著减少MPC执行时间，能处理更大数据集，完成TPC - H基准测试。

Conclusion: ORQ系统在保护数据和高效执行复杂查询方面表现出色，优于现有方案。

Abstract: We present ORQ, a system that enables collaborative analysis of large private
datasets using cryptographically secure multi-party computation (MPC). ORQ
protects data against semi-honest or malicious parties and can efficiently
evaluate relational queries with multi-way joins and aggregations that have
been considered notoriously expensive under MPC. To do so, ORQ eliminates the
quadratic cost of secure joins by leveraging the fact that, in practice, the
structure of many real queries allows us to join records and apply the
aggregations "on the fly" while keeping the result size bounded. On the system
side, ORQ contributes generic oblivious operators, a data-parallel vectorized
query engine, a communication layer that amortizes MPC network costs, and a
dataflow API for expressing relational analytics -- all built from the ground
up.
  We evaluate ORQ in LAN and WAN deployments on a diverse set of workloads,
including complex queries with multiple joins and custom aggregations. When
compared to state-of-the-art solutions, ORQ significantly reduces MPC execution
times and can process one order of magnitude larger datasets. For our most
challenging workload, the full TPC-H benchmark, we report results entirely
under MPC with Scale Factor 10 -- a scale that had previously been achieved
only with information leakage or the use of trusted third parties.

</details>


### [528] [Decentralized Identity Management on Ripple: A Conceptual Framework for High-Speed, Low-Cost Identity Transactions in Attestation-Based Attribute-Based Identity](https://arxiv.org/abs/2509.10545)
*Ruwanga Konara,Kasun De Zoysa,Asanka Sayakkara*

Main category: cs.CR

TL;DR: 近年DIDMS研究多，但ABABDIDM受关注少，Ripple上IDMS研究缺乏，本文尝试在Ripple上概念化ABABDIDM。


<details>
  <summary>Details</summary>
Motivation: ABABDIDM受关注不如一般ABDIDM，且Ripple上缺乏IDMS研究，而Ripple特性对ABIDM有吸引力。

Method: 尝试在Ripple上概念化ABABDIDM。

Result: 文档未提及。

Conclusion: 文档未提及。

Abstract: Recent years have seen many industrial implementations and much scholastic
research, i.e., prototypes and theoretical frameworks, in Decentralized
Identity Management Systems (DIDMS). It is safe to say that Attestation-Based
Attribute-Based Decentralized IDM (ABABDIDM) has not received anywhere near the
same level of attention in the literature as general Attribute-Based DIDMs
(ABDIDM), i.e, decentralized Attribute-Based Access Control (ABAC). The use of
decentralization, i.e., DIDM, is to improve upon the security and
privacy-related issues of centralized Identity Management Systems (IDM) and
Attribute-Based IDMs (ABIDM). And blockchain is the framework used for
decentralization in all these schemes. Many DIDMs - even ABDIDMs - have been
defined on popular blockchains such as Hyperledger, Ethereum, and Bitcoin.
However, despite the characteristics of Ripple that makes it appealing for an
ABIDM, there is a lack of research to develop an Identity Management System
(IDMS) on Ripple in literature. We have attempted to conceptualize an ABABDIDM
on Ripple.

</details>


### [529] [From Paradigm Shift to Audit Rift: Exploring Vulnerabilities and Audit Tips for TON Smart Contracts](https://arxiv.org/abs/2509.10823)
*Yury Yanovich,Sergey Sobolev,Yash Madhwal,Kirill Ziborov,Vladimir Gorgadze,Victoria Kovalevskay,Elizaveta Smirnova,Matvey Mishuris,Subodh Sharma*

Main category: cs.CR

TL;DR: 本文基于34份专业审计报告，为TON智能合约制定综合审计清单，还给出案例研究，采用该清单可增强TON项目安全性。


<details>
  <summary>Details</summary>
Motivation: TON虽有优势但给智能合约开发和安全带来挑战，需要适用于TON的审计方法。

Method: 分析34份含233个真实漏洞的专业审计报告，制定审计清单并进行案例研究。

Result: 得出适用于TON智能合约的综合审计清单，完成案例研究。

Conclusion: 采用该清单可系统识别和缓解漏洞，缩小以太坊成熟审计方法与TON生态需求差距，促进更安全区块链环境。

Abstract: The Open Network (TON) is a high-performance blockchain platform designed for
scalability and efficiency, leveraging an asynchronous execution model and a
multi-layered architecture. While TON's design offers significant advantages,
it also introduces unique challenges for smart contract development and
security. This paper introduces a comprehensive audit checklist for TON smart
contracts, based on an analysis of 34 professional audit reports containing 233
real-world vulnerabilities. The checklist addresses TON-specific challenges,
such as asynchronous message handling, and provides actionable insights for
developers and auditors. We also present detailed case studies of
vulnerabilities in TON smart contracts, highlighting their implications and
offering lessons learned. By adopting this checklist, developers and auditors
can systematically identify and mitigate vulnerabilities, enhancing the
security and reliability of TON-based projects. Our work bridges the gap
between Ethereum's mature audit methodologies and the emerging needs of the TON
ecosystem, fostering a more secure and robust blockchain environment.

</details>


### [530] [A Range-Based Sharding (RBS) Protocol for Scalable Enterprise Blockchain](https://arxiv.org/abs/2509.11006)
*M. Z. Haider,M. Dias de Assuncao,Kaiwen Zhang*

Main category: cs.CR

TL;DR: 本文提出适用于企业区块链的基于范围的分片（RBS）协议，可提升可扩展性，实验显示其比现有框架有更高吞吐量和更低延迟。


<details>
  <summary>Details</summary>
Motivation: 区块链技术存在可扩展性问题，现有分片方法在协调分片、容错和降低共识机制开销方面存在挑战，需要新的解决方案。

Method: 提出RBS协议，采用承诺 - 揭示方案进行安全无偏的分片分配，平衡各分片计算负载，减少共识开销，提高并行交易执行能力。

Result: 实验评估表明RBS比现有企业分片框架实现了更高的吞吐量和更低的延迟。

Conclusion: RBS是大规模区块链部署的可行且高效的解决方案。

Abstract: Blockchain technology offers decentralization and security but struggles with
scalability, particularly in enterprise settings where efficiency and
controlled access are paramount. Sharding is a promising solution for private
blockchains, yet existing approaches face challenges in coordinating shards,
ensuring fault tolerance with limited nodes, and minimizing the high overhead
of consensus mechanisms like PBFT. This paper proposes the Range-Based Sharding
(RBS) Protocol, a novel sharding mechanism tailored for enterprise blockchains,
implemented on Quorum. Unlike traditional sharding models such as OmniLedger
and non-sharding Corda framework, RBS employs a commit-reveal scheme for secure
and unbiased shard allocation, ensuring fair validator distribution while
reducing cross-shard transaction delays. Our approach enhances scalability by
balancing computational loads across shards, reducing consensus overhead, and
improving parallel transaction execution. Experimental evaluations demonstrate
that RBS achieves significantly higher throughput and lower latency compared to
existing enterprise sharding frameworks, making it a viable and efficient
solution for largescale blockchain deployments.

</details>


### [531] [Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers](https://arxiv.org/abs/2509.11173)
*Simin Chen,Jinjun Peng,Yixin He,Junfeng Yang,Baishakhi Ray*

Main category: cs.CR

TL;DR: 本文揭示深度学习编译器设计存在可改变模型语义并引入隐藏后门的漏洞，在对抗和自然场景验证，为安全可信机器学习开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 研究官方未修改的深度学习编译器是否会在编译时改变模型语义并引入隐藏后门。

Method: 在对抗场景构造良性模型，在自然场景分析HuggingFace前100模型。

Result: 对抗场景攻击触发输入成功率100%，自然场景31个模型存在自然触发。

Conclusion: 未修改的深度学习编译器会悄悄改变模型语义，暴露了其设计中固有的安全风险。

Abstract: Deep learning (DL) compilers are core infrastructure in modern DL systems,
offering flexibility and scalability beyond vendor-specific libraries. This
work uncovers a fundamental vulnerability in their design: can an official,
unmodified compiler alter a model's semantics during compilation and introduce
hidden backdoors? We study both adversarial and natural settings. In the
adversarial case, we craft benign models where triggers have no effect
pre-compilation but become effective backdoors after compilation. Tested on six
models, three commercial compilers, and two hardware platforms, our attack
yields 100% success on triggered inputs while preserving normal accuracy and
remaining undetected by state-of-the-art detectors. The attack generalizes
across compilers, hardware, and floating-point settings. In the natural
setting, we analyze the top 100 HuggingFace models (including one with 220M+
downloads) and find natural triggers in 31 models. This shows that compilers
can introduce risks even without adversarial manipulation.
  Our results reveal an overlooked threat: unmodified DL compilers can silently
alter model semantics. To our knowledge, this is the first work to expose
inherent security risks in DL compiler design, opening a new direction for
secure and trustworthy ML.

</details>


### [532] [A Holistic Approach to E-Commerce Innovation: Redefining Security and User Experience](https://arxiv.org/abs/2509.11712)
*Mohammad Olid Ali Akash,Priyangana Saha*

Main category: cs.CR

TL;DR: 本文指出电商 Android 应用存在设计复杂和安全问题，提出新电商平台，以直观界面和强安全措施应对，能重塑用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决现代电商 Android 应用在提供简单安全购物体验方面面临的设计复杂、安全不足等挑战。

Method: 提出具有直观界面（产品分类清晰、结账流程高效）和强安全措施（谷歌认证、SSL 安全支付网关）的新电商平台。

Result: 该平台可重塑电商用户体验。

Conclusion: 关注用户友好性、安全性和个性化能提升电商平台水平，为未来发展提供可行框架。

Abstract: In the modern, fast-moving world of e-commerce, many Android apps face
challenges in providing a simple and secure shopping experience. Many of these
apps, often enough, have complicated designs that prevent users from finding
what they want quickly, thus frustrating them and wasting their precious time.
Another major issue is that of security; with the limitation of payment options
and weak authentication mechanisms, users' sensitive information can be
compromised. This research presents a new e-commerce platform that responds to
the above challenges with an intuitive interface and strong security measures.
The platform makes online shopping easy with well-organized categories of
products and a fast, efficient checkout process. It also gives priority to
security by incorporating features such as Google authentication and
SSL-secured payment gateways to protect user data and ensure secure
transactions. This paper discusses how a focus on user-friendliness, security,
and personalization steps up the game for e-commerce platforms, providing
workable frameworks that match modern user needs and expectations. The findings
show the e-commerce user experience can be remodelled by the platform, hence
opening ways for future developments in that respect.

</details>


### [533] [A Practical Adversarial Attack against Sequence-based Deep Learning Malware Classifiers](https://arxiv.org/abs/2509.11836)
*Kai Tan,Dongyang Zhan,Lin Ye,Hongli Zhang,Binxing Fang*

Main category: cs.CR

TL;DR: 本文提出基于深度Q网络与启发式回溯搜索策略的对抗攻击方法，能生成满足实际条件的扰动序列，还通过映射避免直接修改行为日志序列，实验证明该方法有效且实用。


<details>
  <summary>Details</summary>
Motivation: 现有生成对抗样本的方法直接操作序列，使对抗样本在实际中难以实现或应用，需要新方法。

Method: 提出基于深度Q网络和启发式回溯搜索策略生成满足实际条件的扰动序列，利用新的转换方法将修改映射回源代码。

Result: 该方法能从真实世界的恶意软件行为序列中生成对抗样本，在躲避异常检测模型方面有高成功率。

Conclusion: 提出的方法有效且实用，能在保持修改后软件功能的同时生成对抗样本。

Abstract: Sequence-based deep learning models (e.g., RNNs), can detect malware by
analyzing its behavioral sequences. Meanwhile, these models are susceptible to
adversarial attacks. Attackers can create adversarial samples that alter the
sequence characteristics of behavior sequences to deceive malware classifiers.
The existing methods for generating adversarial samples typically involve
deleting or replacing crucial behaviors in the original data sequences, or
inserting benign behaviors that may violate the behavior constraints. However,
these methods that directly manipulate sequences make adversarial samples
difficult to implement or apply in practice. In this paper, we propose an
adversarial attack approach based on Deep Q-Network and a heuristic
backtracking search strategy, which can generate perturbation sequences that
satisfy practical conditions for successful attacks. Subsequently, we utilize a
novel transformation approach that maps modifications back to the source code,
thereby avoiding the need to directly modify the behavior log sequences. We
conduct an evaluation of our approach, and the results confirm its
effectiveness in generating adversarial samples from real-world malware
behavior sequences, which have a high success rate in evading anomaly detection
models. Furthermore, our approach is practical and can generate adversarial
samples while maintaining the functionality of the modified software.

</details>


### [534] [AegisShield: Democratizing Cyber Threat Modeling with Generative AI](https://arxiv.org/abs/2509.10482)
*Matthew Grofsky*

Main category: cs.CR

TL;DR: 本文开发并评估了AegisShield，一个生成式AI增强的威胁建模工具，它能简化威胁建模，对资源有限的组织有益。


<details>
  <summary>Details</summary>
Motivation: 传统威胁建模在技术系统日益复杂的情况下难以扩展，尤其是对资源有限的小组织，因此需要新工具。

Method: 开发AegisShield工具，整合实时威胁情报，实现STRIDE和MITRE ATT&CK以自动化威胁生成和评估。

Result: 对243个来自15个案例研究的威胁和超8000个AI生成威胁的评估显示，AegisShield降低了复杂性，输出语义与专家开发的威胁一致，且在将威胁映射到MITRE ATT&CK技术上成功率达85.4%。

Conclusion: 自动化和标准化威胁建模有助于资源不足的组织提前应对风险，支持更广泛采用安全设计实践。

Abstract: The increasing sophistication of technology systems makes traditional threat
modeling hard to scale, especially for small organizations with limited
resources. This paper develops and evaluates AegisShield, a generative AI
enhanced threat modeling tool that implements STRIDE and MITRE ATT&CK to
automate threat generation and provide systematic assessments. By integrating
real time threat intelligence from the National Vulnerability Database and
AlienVault Open Threat Exchange, AegisShield produces streamlined and
accessible threat descriptions. Our assessment of 243 threats from 15 case
studies and over 8000 AI generated threats shows that AegisShield reduces
complexity (p less than 0.001), yields outputs semantically aligned with expert
developed threats (p less than 0.05), and achieves an 85.4 percent success rate
in mapping threats to MITRE ATT&CK techniques (p less than 0.001). Automating
and standardizing threat modeling helps under resourced organizations address
risk earlier and supports wider adoption of secure by design practices.

</details>


### [535] [EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System](https://arxiv.org/abs/2509.10540)
*Pavan Reddy,Aditya Sanjay Gujral*

Main category: cs.CR

TL;DR: 本文深入研究微软365 Copilot的EchoLeak零点击提示注入漏洞，分析现有防御失败原因并提出缓解措施，得出提示注入是严重漏洞及防御建议。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型助手融入企业工作流带来新安全问题，研究微软365 Copilot的EchoLeak漏洞。

Method: 对EchoLeak漏洞进行深入案例研究，分析其绕过多种防御机制的方式。

Result: 发现现有防御失效，提出如提示分区、增强输入/输出过滤等工程缓解措施。

Conclusion: 提示注入是生产AI系统中严重的漏洞类型，应遵循最小权限原则、采用纵深防御架构和持续对抗测试来防御。

Abstract: Large language model (LLM) assistants are increasingly integrated into
enterprise workflows, raising new security concerns as they bridge internal and
external data sources. This paper presents an in-depth case study of EchoLeak
(CVE-2025-32711), a zero-click prompt injection vulnerability in Microsoft 365
Copilot that enabled remote, unauthenticated data exfiltration via a single
crafted email. By chaining multiple bypasses-evading Microsofts XPIA (Cross
Prompt Injection Attempt) classifier, circumventing link redaction with
reference-style Markdown, exploiting auto-fetched images, and abusing a
Microsoft Teams proxy allowed by the content security policy-EchoLeak achieved
full privilege escalation across LLM trust boundaries without user interaction.
We analyze why existing defenses failed, and outline a set of engineering
mitigations including prompt partitioning, enhanced input/output filtering,
provenance-based access control, and strict content security policies. Beyond
the specific exploit, we derive generalizable lessons for building secure AI
copilots, emphasizing the principle of least privilege, defense-in-depth
architectures, and continuous adversarial testing. Our findings establish
prompt injection as a practical, high-severity vulnerability class in
production AI systems and provide a blueprint for defending against future
AI-native threats.

</details>


### [536] [Robust DDoS-Attack Classification with 3D CNNs Against Adversarial Methods](https://arxiv.org/abs/2509.10543)
*Landon Bragg,Nathan Dorsey,Josh Prior,John Ajit,Ben Kim,Nate Willis,Pablo Rivas*

Main category: cs.CR

TL;DR: 提出用蜂巢图序列与3D CNN对DDoS流量高精度分类的方法，在基准数据集上提升对抗准确率。


<details>
  <summary>Details</summary>
Motivation: DDoS攻击常通过微妙改变流量绕过检测，威胁在线基础设施，需有效分类方法。

Method: 使用网络数据的蜂巢图序列和3D CNN，包括时空蜂巢图编码、结合FGSM和PGD的对抗训练及空间噪声与图像偏移、分析逐帧预测。

Result: 在基准数据集上，将对抗准确率从50 - 55%提升到超93%，保持干净样本性能，3 - 4帧有强预测信号。

Conclusion: 该方法能高精度分类DDoS流量，早期阶段分类可行。

Abstract: Distributed Denial-of-Service (DDoS) attacks remain a serious threat to
online infrastructure, often bypassing detection by altering traffic in subtle
ways. We present a method using hive-plot sequences of network data and a 3D
convolutional neural network (3D CNN) to classify DDoS traffic with high
accuracy. Our system relies on three main ideas: (1) using spatio-temporal
hive-plot encodings to set a pattern-recognition baseline, (2) applying
adversarial training with FGSM and PGD alongside spatial noise and image
shifts, and (3) analyzing frame-wise predictions to find early signals. On a
benchmark dataset, our method lifts adversarial accuracy from 50-55% to over
93% while maintaining clean-sample performance. Frames 3-4 offer strong
predictive signals, showing early-stage classification is possible.

</details>


### [537] [AVEC: Bootstrapping Privacy for Local LLMs](https://arxiv.org/abs/2509.10561)
*Madhava Gaikwad*

Main category: cs.CR

TL;DR: 本文提出AVEC框架，为本地大模型提供隐私保护，介绍算法、形式化保证，通过模拟评估，贡献概念架构与理论基础。


<details>
  <summary>Details</summary>
Motivation: 为本地语言模型在边缘端提供具有可验证性的隐私保护。

Method: 引入自适应预算算法分配差分隐私参数，使用可验证转换和设备完整性检查，用Rényi差分隐私和基于里程计的会计方法形式化保证。

Result: 建立了效用上限、委托泄漏边界和确定性门控与仅哈希认证的不可能结果，通过模拟评估研究机制行为和会计。

Conclusion: 提供了一个概念架构和理论基础，为私有引导本地大语言模型的实证研究指明了方向。

Abstract: This position paper presents AVEC (Adaptive Verifiable Edge Control), a
framework for bootstrapping privacy for local language models by enforcing
privacy at the edge with explicit verifiability for delegated queries. AVEC
introduces an adaptive budgeting algorithm that allocates per-query
differential privacy parameters based on sensitivity, local confidence, and
historical usage, and uses verifiable transformation with on-device integrity
checks. We formalize guarantees using R\'enyi differential privacy with
odometer-based accounting, and establish utility ceilings, delegation-leakage
bounds, and impossibility results for deterministic gating and hash-only
certification. Our evaluation is simulation-based by design to study mechanism
behavior and accounting; we do not claim deployment readiness or task-level
utility with live LLMs. The contribution is a conceptual architecture and
theoretical foundation that chart a pathway for empirical follow-up on
privately bootstrapping local LLMs.

</details>


### [538] [MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of Latent Diffusion Models](https://arxiv.org/abs/2509.10569)
*Leyi Pan,Sheng Guan,Zheyu Fu,Luyang Si,Zian Wang,Xuming Hu,Irwin King,Philip S. Yu,Aiwei Liu,Lijie Wen*

Main category: cs.CR

TL;DR: 介绍开源Python工具包MarkDiffusion用于潜在扩散模型生成水印，含三个关键组件，旨在助力研究、提升公众认知等。


<details>
  <summary>Details</summary>
Motivation: 助力研究人员，提升公众对生成水印的认知和参与度，推动共识并促进研究和应用。

Method: 构建MarkDiffusion，包含统一实现框架、机制可视化套件和综合评估模块。

Result: 创建了MarkDiffusion工具包。

Conclusion: MarkDiffusion能在生成水印领域发挥积极作用，有助于推动相关研究和应用。

Abstract: We introduce MarkDiffusion, an open-source Python toolkit for generative
watermarking of latent diffusion models. It comprises three key components: a
unified implementation framework for streamlined watermarking algorithm
integrations and user-friendly interfaces; a mechanism visualization suite that
intuitively showcases added and extracted watermark patterns to aid public
understanding; and a comprehensive evaluation module offering standard
implementations of 24 tools across three essential aspects - detectability,
robustness, and output quality - plus 8 automated evaluation pipelines. Through
MarkDiffusion, we seek to assist researchers, enhance public awareness and
engagement in generative watermarking, and promote consensus while advancing
research and applications.

</details>


### [539] [The Coding Limits of Robust Watermarking for Generative Models](https://arxiv.org/abs/2509.10577)
*Danilo Francati,Yevin Nikhel Goonatilake,Shubham Pawar,Daniele Venturi,Giuseppe Ateniese*

Main category: cs.CR

TL;DR: 论文证明生成模型加密水印鲁棒性的精确阈值，给出构造方法并实验验证阈值在实际中存在。


<details>
  <summary>Details</summary>
Motivation: 研究生成模型加密水印的鲁棒性阈值。

Method: 引入无消息密钥码的编码抽象，形式化鲁棒水印的要求；给出明确构造方法；对Gunn等人的图像水印进行实验。

Result: 确定二进制输出和q元输出的鲁棒性阈值；构造出接近理论界限的编码；实验表明简单操作能达到阈值擦除水印。

Conclusion: 对鲁棒水印进行了完整刻画，确定失败阈值、构造方法并验证阈值在实际中已达到。

Abstract: We prove a sharp threshold for the robustness of cryptographic watermarking
for generative models. This is achieved by introducing a coding abstraction,
which we call messageless secret-key codes, that formalizes sufficient and
necessary requirements of robust watermarking: soundness, tamper detection, and
pseudorandomness. Thus, we establish that robustness has a precise limit: For
binary outputs no scheme can survive if more than half of the encoded bits are
modified, and for an alphabet of size q the corresponding threshold is
$(1-1/q)$ of the symbols.
  Complementing this impossibility, we give explicit constructions that meet
the bound up to a constant slack. For every ${\delta} > 0$, assuming
pseudorandom functions and access to a public counter, we build linear-time
codes that tolerate up to $(1/2)(1-{\delta})$ errors in the binary case and
$(1-1/q)(1-{\delta})$ errors in the $q$-ary case. Together with the lower
bound, these yield the maximum robustness achievable under standard
cryptographic assumptions.
  We then test experimentally whether this limit appears in practice by looking
at the recent watermarking for images of Gunn, Zhao, and Song (ICLR 2025). We
show that a simple crop and resize operation reliably flipped about half of the
latent signs and consistently prevented belief-propagation decoding from
recovering the codeword, erasing the watermark while leaving the image visually
intact.
  These results provide a complete characterization of robust watermarking,
identifying the threshold at which robustness fails, constructions that achieve
it, and an experimental confirmation that the threshold is already reached in
practice.

</details>


### [540] [LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems](https://arxiv.org/abs/2509.10682)
*Vitor Hugo Galhardo Moia,Igor Jochem Sanz,Gabriel Antonio Fontes Rebello,Rodrigo Duarte de Meneses,Briland Hitaj,Ulf Lindqvist*

Main category: cs.CR

TL;DR: 本文对基于大语言模型（LLM）系统的安全和隐私问题进行系统综述和分类，分析现实场景，为减轻风险提供指导。


<details>
  <summary>Details</summary>
Motivation: 生成式AI成功广泛应用，吸引网络罪犯，保障基于LLM系统的安全面临挑战，需解决相关安全和隐私问题。

Method: 对威胁和防御策略进行系统综述和全面分类，考虑软件和LLM整个生命周期，分析现实场景，按严重程度和适用场景对威胁分类，对防御策略分类并映射到相应阶段和攻击策略。

Result: 完成威胁和防御策略的分类，明确不同场景的相关威胁及对应防御策略。

Conclusion: 有助于消费者和供应商理解并减轻LLM集成风险，也为研究界探讨阻碍安全隐私应用的挑战和边缘情况提供帮助。

Abstract: The success and wide adoption of generative AI (GenAI), particularly large
language models (LLMs), has attracted the attention of cybercriminals seeking
to abuse models, steal sensitive data, or disrupt services. Moreover, providing
security to LLM-based systems is a great challenge, as both traditional threats
to software applications and threats targeting LLMs and their integration must
be mitigated. In this survey, we shed light on security and privacy concerns of
such LLM-based systems by performing a systematic review and comprehensive
categorization of threats and defensive strategies considering the entire
software and LLM life cycles. We analyze real-world scenarios with distinct
characteristics of LLM usage, spanning from development to operation. In
addition, threats are classified according to their severity level and to which
scenarios they pertain, facilitating the identification of the most relevant
threats. Recommended defense strategies are systematically categorized and
mapped to the corresponding life cycle phase and possible attack strategies
they attenuate. This work paves the way for consumers and vendors to understand
and efficiently mitigate risks during integration of LLMs in their respective
solutions or organizations. It also enables the research community to benefit
from the discussion of open challenges and edge cases that may hinder the
secure and privacy-preserving adoption of LLM-based systems.

</details>


### [541] [Privacy-Preserving Decentralized Federated Learning via Explainable Adaptive Differential Privacy](https://arxiv.org/abs/2509.10691)
*Fardin Jalil Piran,Zhiling Chen,Yang Zhang,Qianyu Zhou,Jiong Tang,Farhad Imani*

Main category: cs.CR

TL;DR: 提出可解释框架PrivateDFL，结合超维计算与差分隐私，在多数据集上对比基准模型，表现出高准确率、低延迟和低能耗优势，未来将扩展到对抗客户端和自适应拓扑。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习存在隐私风险，差分隐私在无透明度时会因无法跟踪累积噪声而影响准确性。

Method: 提出PrivateDFL框架，结合超维计算与差分隐私，记录累积噪声，让客户端添加所需噪声与已累积噪声的差值。

Result: 在MNIST、ISOLET和UCI - HAR数据集上，对比基于Transformer和深度学习的基准模型，PrivateDFL在IID和非IID分区下都有更高准确率、更低延迟和能耗。

Conclusion: PrivateDFL能在保证隐私的同时，提升性能，未来可扩展到对抗客户端和自适应拓扑场景。

Abstract: Decentralized federated learning faces privacy risks because model updates
can leak data through inference attacks and membership inference, a concern
that grows over many client exchanges. Differential privacy offers principled
protection by injecting calibrated noise so confidential information remains
secure on resource-limited IoT devices. Yet without transparency, black-box
training cannot track noise already injected by previous clients and rounds,
which forces worst-case additions and harms accuracy. We propose PrivateDFL, an
explainable framework that joins hyperdimensional computing with differential
privacy and keeps an auditable account of cumulative noise so each client adds
only the difference between the required noise and what has already been
accumulated. We evaluate on MNIST, ISOLET, and UCI-HAR to span image, signal,
and tabular modalities, and we benchmark against transformer-based and deep
learning-based baselines trained centrally with Differentially Private
Stochastic Gradient Descent (DP-SGD) and Renyi Differential Privacy (RDP).
PrivateDFL delivers higher accuracy, lower latency, and lower energy across IID
and non-IID partitions while preserving formal (epsilon, delta) guarantees and
operating without a central server. For example, under non-IID partitions,
PrivateDFL achieves 24.42% higher accuracy than the Vision Transformer on MNIST
while using about 10x less training time, 76x lower inference latency, and 11x
less energy, and on ISOLET it exceeds Transformer accuracy by more than 80%
with roughly 10x less training time, 40x lower inference latency, and 36x less
training energy. Future work will extend the explainable accounting to
adversarial clients and adaptive topologies with heterogeneous privacy budgets.

</details>


### [542] [A Content-dependent Watermark for Safeguarding Image Attribution](https://arxiv.org/abs/2509.10766)
*Tong Zhou,Ruyi Ding,Gaowen Liu,Charles Fleming,Ramana Rao Kompella,Yunsi Fei,Xiaolin Xu,Shaolei Ren*

Main category: cs.CR

TL;DR: 数字和AI生成图像增长使图像归属验证需求增加，现有水印技术易伪造，提出MetaSeal框架保障图像归属安全。


<details>
  <summary>Details</summary>
Motivation: 数字和AI生成图像快速增长，现有水印技术易被伪造，存在错误归属风险，损害开发者声誉和艺术家权益。

Method: 提出MetaSeal框架，实现内容相关水印，具备抗伪造、自包含保护和篡改证据功能。

Result: 实验表明MetaSeal能有效减轻伪造企图，适用于自然和AI生成图像。

Conclusion: MetaSeal为安全图像归属建立了新标准。

Abstract: The rapid growth of digital and AI-generated images has amplified the need
for secure and verifiable methods of image attribution. While digital
watermarking offers more robust protection than metadata-based
approaches--which can be easily stripped--current watermarking techniques
remain vulnerable to forgery, creating risks of misattribution that can damage
the reputations of AI model developers and the rights of digital artists. These
vulnerabilities arise from two key issues: (1) content-agnostic watermarks,
which, once learned or leaked, can be transferred across images to fake
attribution, and (2) reliance on detector-based verification, which is
unreliable since detectors can be tricked. We present MetaSeal, a novel
framework for content-dependent watermarking with cryptographic security
guarantees to safeguard image attribution. Our design provides (1) forgery
resistance, preventing unauthorized replication and enforcing cryptographic
verification; (2) robust, self-contained protection, embedding attribution
directly into images while maintaining resilience against benign
transformations; and (3) evidence of tampering, making malicious alterations
visually detectable. Experiments demonstrate that MetaSeal effectively
mitigates forgery attempts and applies to both natural and AI-generated images,
establishing a new standard for secure image attribution.

</details>


### [543] [Large Language Models for Security Operations Centers: A Comprehensive Survey](https://arxiv.org/abs/2509.10858)
*Ali Habibzadeh,Farid Feyzi,Reza Ebrahimi Atani*

Main category: cs.CR

TL;DR: 本文探讨大语言模型（LLMs）在安全运营中心（SOC）工作流程中的集成，分析其能力、挑战和未来方向，是首篇详细研究LLMs在SOC应用的全面性研究。


<details>
  <summary>Details</summary>
Motivation: SOC面临高警报量、资源有限等挑战，LLMs可提供自动化日志分析等有前景的解决方案，因此研究其在SOC工作流程中的集成。

Method: 进行系统性调查。

Result: 提供了LLMs集成到SOC工作流程的结构化视角，涵盖其能力、挑战和未来方向。

Conclusion: 本调查为研究人员和SOC管理人员提供了学术研究中LLM集成现状的广泛概述。

Abstract: Large Language Models (LLMs) have emerged as powerful tools capable of
understanding and generating human-like text, offering transformative potential
across diverse domains. The Security Operations Center (SOC), responsible for
safeguarding digital infrastructure, represents one of these domains. SOCs
serve as the frontline of defense in cybersecurity, tasked with continuous
monitoring, detection, and response to incidents. However, SOCs face persistent
challenges such as high alert volumes, limited resources, high demand for
experts with advanced knowledge, delayed response times, and difficulties in
leveraging threat intelligence effectively. In this context, LLMs can offer
promising solutions by automating log analysis, streamlining triage, improving
detection accuracy, and providing the required knowledge in less time. This
survey systematically explores the integration of generative AI and more
specifically LLMs into SOC workflow, providing a structured perspective on its
capabilities, challenges, and future directions. We believe that this survey
offers researchers and SOC managers a broad overview of the current state of
LLM integration within academic study. To the best of our knowledge, this is
the first comprehensive study to examine LLM applications in SOCs in details.

</details>


### [544] [Auditable Early Stopping for Agentic Routing: Ledger-Verified Run-Wise Certificates under Local DP](https://arxiv.org/abs/2509.10550)
*Shivam Akhauri*

Main category: cs.CR

TL;DR: 提出运行级早期停止证书用于扰动 - MAP最佳优先搜索，添加编译器等组件，集成多LLM控制器和DP训练的LoRA适配器，给出实验结果和审计追踪。


<details>
  <summary>Details</summary>
Motivation: 生产工具使用代理中的路由器需在保留本地差分隐私和可审计记录的同时知道何时停止探索。

Method: 提出运行级早期停止证书，耦合路径得分和剪枝键，使用指数竞争，添加编译器、计数例程等组件，集成多LLM控制器和DP训练的LoRA适配器。

Result: 有Mac/商品硬件可复现结果、小型真实工具使用管道和验证器检查的审计追踪，提供代码和账本。

Conclusion: 所提出的方法和组件能有效解决生产工具使用代理中路由器的停止探索问题，且相关选择不影响双模式边界不变性。

Abstract: In production tool-use agents (e.g., retrieval $\to$ summarization $\to$
calculator), routers must know when to stop exploring while preserving local DP
and leaving an auditable trail. We present run-wise early-stopping certificates
for perturb-and-MAP (PaM) best-first search on context-indexed prefix DAGs
whose children partition the leaves. We couple realized path scores and pruning
keys to a single exponential race realized lazily via offset propagation. With
exact leaf counts $N(v)$, lazy reuse at winners and independent residuals yield
an Exact mode with a sound halting rule based on Key$(v) = M_tau(v) - \log
t(v)$, where $t(v)$ is the minimum arrival time among leaves under $v$. With
only upper bounds $N_{ub} \ge N$, a Surrogate mode uses a parent-anchored
surrogate race without winner reuse; because $-\log \hat t \ge -\log t$, the
frontier invariant holds and stopping remains sound. We add a compiler from
shared-node DAGs to prefix DAGs, local finiteness checks, a SuffixCountDP
routine for exact counts with safe downgrades, a validator-side tightening term
$\kappa = \log(N/N_{ub})$, and an auditable ledger/validator that replays runs
deterministically. We also give an absolute LogSumExp tail bound, an acyclicity
certificate, and a fallback PRF-per-leaf scheme (NoCert) whose work matches a
realized-score best-first baseline up to a small per-node overhead. Finally, we
integrate a price/latency/$(\epsilon, \delta)$-aware multi-LLM controller and
DP-trained LoRA adapters chosen at runtime; these choices do not affect the
two-mode frontier invariants. We report Mac/commodity-hardware reproducible
results, a small real tool-use pipeline, and validator-checked audit trails,
with code and ledgers provided.

</details>


### [545] [A Comparison of Selected Image Transformation Techniques for Malware Classification](https://arxiv.org/abs/2509.10838)
*Rishit Agrawal,Kunal Bhatnagar,Andrew Do,Ronnit Rana,Mark Stamp*

Main category: cs.CR

TL;DR: 研究八种恶意软件到图像的转换技术及多种学习模型，发现不同转换技术在多种模型上表现相似，图像分类有效性或更依赖图像分析技术本身。


<details>
  <summary>Details</summary>
Motivation: 现有恶意软件图像转换方法缺乏通用标准且未考虑可执行文件属性，需探索合适转换技术。

Method: 实验八种不同的恶意软件到图像的转换技术，并对每种技术测试多种学习模型。

Result: 几种图像转换技术在一系列学习模型上表现相似，尽管转换过程不同。

Conclusion: 基于图像的恶意软件分类技术的有效性可能更多依赖图像分析技术的内在优势，而非图像转换策略的细节。

Abstract: Recently, a considerable amount of malware research has focused on the use of
powerful image-based machine learning techniques, which generally yield
impressive results. However, before image-based techniques can be applied to
malware, the samples must be converted to images, and there is no
generally-accepted approach for doing so. The malware-to-image conversion
strategies found in the literature often appear to be ad hoc, with little or no
effort made to take into account properties of executable files. In this paper,
we experiment with eight distinct malware-to-image conversion techniques, and
for each, we test a variety of learning models. We find that several of these
image conversion techniques perform similarly across a range of learning
models, in spite of the image conversion processes being quite different. These
results suggest that the effectiveness of image-based malware classification
techniques may depend more on the inherent strengths of image analysis
techniques, as opposed to the precise details of the image conversion strategy.

</details>


### [546] [Dstack: A Zero Trust Framework for Confidential Containers](https://arxiv.org/abs/2509.11555)
*Shunfan Zhou,Kevin Wang,Hang Yin*

Main category: cs.CR

TL;DR: 本文提出dstack框架，将TEE技术转化为零信任平台，介绍三项创新及核心组件，评估显示其有安全保障和实用性。


<details>
  <summary>Details</summary>
Motivation: Web3应用需不依赖中心化信任机构的执行平台，当前TEE应用于Web3有安全可靠性、抗审查性和供应商独立性等局限。

Method: 提出dstack框架，引入便携式机密容器、去中心化代码管理、可验证域名管理三项创新，通过dstack - OS、dstack - KMS和dstack - Gateway三个核心组件实现。

Result: dstack能提供全面安全保障，且保持实际应用的可用性。

Conclusion: dstack可实现VM级TEE解决方案的性能优势和Web3应用所需的无信任保证。

Abstract: Web3 applications require execution platforms that maintain confidentiality
and integrity without relying on centralized trust authorities. While Trusted
Execution Environments (TEEs) offer promising capabilities for confidential
computing, current implementations face significant limitations when applied to
Web3 contexts, particularly in security reliability, censorship resistance, and
vendor independence.
  This paper presents dstack, a comprehensive framework that transforms raw TEE
technology into a true Zero Trust platform. We introduce three key innovations:
(1) Portable Confidential Containers that enable seamless workload migration
across heterogeneous TEE environments while maintaining security guarantees,
(2) Decentralized Code Management that leverages smart contracts for
transparent governance of TEE applications, and (3) Verifiable Domain
Management that ensures secure and verifiable application identity without
centralized authorities.
  These innovations are implemented through three core components: dstack-OS,
dstack-KMS, and dstack-Gateway. Together, they demonstrate how to achieve both
the performance advantages of VM-level TEE solutions and the trustless
guarantees required by Web3 applications. Our evaluation shows that dstack
provides comprehensive security guarantees while maintaining practical
usability for real-world applications.

</details>


### [547] [Poison to Detect: Detection of Targeted Overfitting in Federated Learning](https://arxiv.org/abs/2509.11974)
*Soumia Zohra El Mestari,Maciej Krzysztof Zuziak,Gabriele Lenzini*

Main category: cs.CR

TL;DR: 本文研究联邦学习中不诚实协调者诱导目标过拟合的威胁，提出三种检测技术并评估，结果显示能可靠检测但各有特点。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽有隐私优势但易受隐私攻击，现有研究多关注减少训练信息泄露，本文旨在实现客户端对目标过拟合的早期检测。

Method: 提出三种检测技术：标签翻转、后门触发器注入和模型指纹识别，用于客户端验证全局聚合的完整性。

Result: 在不同攻击场景的多个数据集上评估，三种方法能可靠检测目标过拟合，但在计算复杂度、检测延迟和误报率方面存在差异。

Conclusion: 三种检测技术可有效检测联邦学习中协调者诱导的目标过拟合，客户端可根据需求选择合适方法。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralised clients while keeping local data private, making it a widely
adopted privacy-enhancing technology (PET). Despite its privacy benefits, FL
remains vulnerable to privacy attacks, including those targeting specific
clients. In this paper, we study an underexplored threat where a dishonest
orchestrator intentionally manipulates the aggregation process to induce
targeted overfitting in the local models of specific clients. Whereas many
studies in this area predominantly focus on reducing the amount of information
leakage during training, we focus on enabling an early client-side detection of
targeted overfitting, thereby allowing clients to disengage before significant
harm occurs. In line with this, we propose three detection techniques - (a)
label flipping, (b) backdoor trigger injection, and (c) model fingerprinting -
that enable clients to verify the integrity of the global aggregation. We
evaluated our methods on multiple datasets under different attack scenarios.
Our results show that the three methods reliably detect targeted overfitting
induced by the orchestrator, but they differ in terms of computational
complexity, detection latency, and false-positive rates.

</details>
