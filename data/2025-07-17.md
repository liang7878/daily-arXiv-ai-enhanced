<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.SE](#cs.SE) [Total: 11]
- [stat.ML](#stat.ML) [Total: 4]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 5]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [cs.CY](#cs.CY) [Total: 5]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [math.CO](#math.CO) [Total: 2]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.MA](#cs.MA) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.SC](#cs.SC) [Total: 2]
- [hep-ex](#hep-ex) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [econ.GN](#econ.GN) [Total: 4]
- [cs.CV](#cs.CV) [Total: 33]
- [cs.RO](#cs.RO) [Total: 4]
- [nlin.CG](#nlin.CG) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.NI](#cs.NI) [Total: 4]
- [quant-ph](#quant-ph) [Total: 3]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CL](#cs.CL) [Total: 27]
- [cs.PL](#cs.PL) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [physics.data-an](#physics.data-an) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Study on the Application of Artificial Intelligence in Ecological Design](https://arxiv.org/abs/2507.11595)
*Hengyue Zhao*

Main category: cs.AI

TL;DR: 探讨人与大自然关系能否从人类主导转向相互依存及AI能否促成转变，通过案例研究展示AI在生态设计中的应用，提出设计途径，强调AI潜力。


<details>
  <summary>Details</summary>
Motivation: 研究人与大自然关系能否从人类主导转变为真正的相互依存，以及人工智能能否促成这一转变。

Method: 研究新的生态设计范式，通过案例研究展示艺术家和设计师运用AI进行数据分析、图像识别和生态修复，基于作者的AI辅助水修复原型提出设计途径。

Result: AI不仅扩展了创作方法，还重塑了生态设计的理论与实践；突出了AI将科学见解、艺术实践和环境管理联系起来的潜力。

Conclusion: AI为未来可持续的、技术赋能的生态系统研究提供了路线图。

Abstract: This paper asks whether our relationship with nature can move from human
dominance to genuine interdependence, and whether artificial intelligence (AI)
can mediate that shift. We examine a new ecological-design paradigm in which AI
interacts with non-human life forms. Through case studies we show how artists
and designers apply AI for data analysis, image recognition, and ecological
restoration, producing results that differ from conventional media. We argue
that AI not only expands creative methods but also reframes the theory and
practice of ecological design. Building on the author's prototype for
AI-assisted water remediation, the study proposes design pathways that couple
reinforcement learning with plant-based phytoremediation. The findings
highlight AI's potential to link scientific insight, artistic practice, and
environmental stewardship, offering a roadmap for future research on
sustainable, technology-enabled ecosystems.

</details>


### [2] [General Modular Harness for LLM Agents in Multi-Turn Gaming Environments](https://arxiv.org/abs/2507.11633)
*Yuxuan Zhang,Haoyang Yu,Lanxiang Hu,Haojian Jin,Hao Zhang*

Main category: cs.AI

TL;DR: 提出用于大语言模型（LLM）智能体的模块化设计，在多轮游戏环境测试效果好，揭示各模块贡献模式。


<details>
  <summary>Details</summary>
Motivation: 使单个大语言模型或视觉语言模型（VLM）骨干无需特定领域工程就能处理广泛的多轮游戏环境，推动通用智能体发展。

Method: 设计包含感知、记忆和推理组件的模块化框架，用经典和现代游戏套件作为测试平台分析各模块对性能的影响。

Result: 模块化框架持续提升游戏性能，揭示不同游戏中各模块不同贡献模式，如记忆在长视野谜题中占主导，感知在视觉嘈杂的街机游戏中关键。

Conclusion: 模块化设计在推动通用智能体方面有效。

Abstract: We introduce a modular harness design for LLM agents that composes of
perception, memory, and reasoning components, enabling a single LLM or VLM
backbone to tackle a wide spectrum of multi turn gaming environments without
domain-specific engineering. Using classic and modern game suites as
low-barrier, high-diversity testbeds, our framework provides a unified workflow
for analyzing how each module affects performance across dynamic interactive
settings. Extensive experiments demonstrate that the harness lifts gameplay
performance consistently over un-harnessed baselines and reveals distinct
contribution patterns, for example, memory dominates in long-horizon puzzles
while perception is critical in vision noisy arcades. These findings highlight
the effectiveness of our modular harness design in advancing general-purpose
agent, given the familiarity and ubiquity of games in everyday human
experience.

</details>


### [3] [Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification](https://arxiv.org/abs/2507.11662)
*Moises Andrade,Joonhyuk Cha,Brandon Ho,Vriksha Srihari,Karmesh Yadav,Zsolt Kira*

Main category: cs.AI

TL;DR: 论文评估MLLMs作为验证器存在一致性偏差问题，提出SGV方法解决，增强后MLLM验证器表现提升。


<details>
  <summary>Details</summary>
Motivation: 将AI在有明确成功标准领域的成果拓展到无明确标准领域（如计算机使用），MLLMs有潜力但存在一致性偏差问题。

Method: 提出Self - Grounded Verification (SGV)方法，分两步：先让MLLM获取任务完成的广泛先验，再基于先验评估候选轨迹。

Result: 增强后的MLLM验证器在准确率和故障检测率上提升达20分，能实时监督异构代理，在基准测试上创造新的最优结果，超过之前最佳48%。

Conclusion: SGV方法能有效利用MLLMs的知识和推理能力，解决其作为验证器的一致性偏差问题，提升性能。

Abstract: Verifiers -- functions assigning rewards to agent behavior -- have been key
for AI progress in domains like math and board games. However, extending these
gains to domains without clear-cut success criteria (e.g.,computer use) remains
a challenge: while humans can recognize suitable outcomes, translating this
intuition into scalable rules is non-trivial. Multimodal Large Language
Models(MLLMs) emerge as a promising solution, given their world knowledge,
human-preference alignment, and reasoning skills. We evaluate MLLMs as
verifiers of agent trajectories across web navigation, computer use, and
robotic manipulation, and identify a critical limitation: agreement bias, a
strong tendency for MLLMs to favor information in their context window, often
generating chains of thought to rationalize flawed behavior. This bias is
pervasive across models, resilient to test-time scaling, and can impact several
methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs
despite MLLMs showing strong, human-aligned priors on desired behavior. To
address this, we propose Self-Grounded Verification (SGV), a lightweight method
that enables more effective use of MLLMs' knowledge and reasoning by harnessing
their own sampling mechanisms via unconditional and conditional generation. SGV
operates in two steps: first, the MLLM is elicited to retrieve broad priors
about task completion, independent of the data under evaluation. Then,
conditioned on self-generated priors, it reasons over and evaluates a candidate
trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in
accuracy and failure detection rates, and can perform real-time supervision of
heterogeneous agents, boosting task completion of a GUI specialist in OSWorld,
a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting
a new state of the art on the benchmark, surpassing the previous best by 48%.

</details>


### [4] [ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making](https://arxiv.org/abs/2507.11733)
*Srikanth Vemula*

Main category: cs.AI

TL;DR: 介绍了ClarifAI方法，结合CBR和本体驱动方法增强AI透明度和可解释性，有广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 增强人工智能在决策领域的透明度和可解释性，满足不同利益相关者的解释需求。

Method: 利用基于案例的推理（CBR）方法，并集成本体驱动方法。

Result: 阐述了ClarifAI的理论基础、设计原则和架构蓝图，显示其在不同领域增强AI可解释性的潜力。

Conclusion: ClarifAI在提升AI系统可解释性方面有重要作用，为关键决策过程中的应用铺平道路。

Abstract: This Study introduces Clarity and Reasoning Interface for Artificial
Intelligence(ClarifAI), a novel approach designed to augment the transparency
and interpretability of artificial intelligence (AI) in the realm of improved
decision making. Leveraging the Case-Based Reasoning (CBR) methodology and
integrating an ontology-driven approach, ClarifAI aims to meet the intricate
explanatory demands of various stakeholders involved in AI-powered
applications. The paper elaborates on ClarifAI's theoretical foundations,
combining CBR and ontologies to furnish exhaustive explanation mechanisms. It
further elaborates on the design principles and architectural blueprint,
highlighting ClarifAI's potential to enhance AI interpretability across
different sectors and its applicability in high-stake environments. This
research delineates the significant role of ClariAI in advancing the
interpretability of AI systems, paving the way for its deployment in critical
decision-making processes.

</details>


### [5] [BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2507.12207)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: 本文介绍BuildEvo框架，用大语言模型自动设计有效且可解释的建筑能耗预测启发式方法，评估显示其达最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法缺乏精度，先进模型不透明且泛化能力差，需准确的建筑能耗预测方法。

Method: 引入BuildEvo框架，在进化过程中引导大语言模型结合建筑特征和运行数据的物理见解来构建和改进启发式方法。

Result: BuildEvo在基准测试中达到最优性能，具有更好的泛化能力和透明的预测逻辑。

Conclusion: 该工作推动了基于物理的鲁棒启发式方法的自动设计，促进复杂能源系统的可靠模型发展。

Abstract: Accurate building energy forecasting is essential, yet traditional heuristics
often lack precision, while advanced models can be opaque and struggle with
generalization by neglecting physical principles. This paper introduces
BuildEvo, a novel framework that uses Large Language Models (LLMs) to
automatically design effective and interpretable energy prediction heuristics.
Within an evolutionary process, BuildEvo guides LLMs to construct and enhance
heuristics by systematically incorporating physical insights from building
characteristics and operational data (e.g., from the Building Data Genome
Project 2). Evaluations show BuildEvo achieves state-of-the-art performance on
benchmarks, offering improved generalization and transparent prediction logic.
This work advances the automated design of robust, physically grounded
heuristics, promoting trustworthy models for complex energy systems.

</details>


### [6] [Auto-Formulating Dynamic Programming Problems with Large Language Models](https://arxiv.org/abs/2507.11737)
*Chenyu Zhou,Jingyuan Yang,Linwei Xin,Yitian Chen,Ziyan He,Dongdong Ge*

Main category: cs.AI

TL;DR: 提出DP - Bench基准评估DP问题，介绍DPLM模型及DualReflect数据生成方法，揭示前后向生成的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统DP模型制定需专业知识，现有LLM难以直接应用于DP问题，需评估方法和适用模型。

Method: 引入DP - Bench基准，构建7B参数的DPLM模型，采用DualReflect数据生成管道，结合前后向生成。

Result: DPLM性能与先进模型相当，在难题上更优，揭示前后向生成在不同数据规模下的特点。

Conclusion: 前后向生成方法优势互补，结合两者很重要。

Abstract: Dynamic programming (DP) is a fundamental method in operations research, but
formulating DP models has traditionally required expert knowledge of both the
problem context and DP techniques. Large Language Models (LLMs) offer the
potential to automate this process. However, DP problems pose unique challenges
due to their inherently stochastic transitions and the limited availability of
training data. These factors make it difficult to directly apply existing
LLM-based models or frameworks developed for other optimization problems, such
as linear or integer programming. We introduce DP-Bench, the first benchmark
covering a wide range of textbook-level DP problems to enable systematic
evaluation. We present Dynamic Programming Language Model (DPLM), a
7B-parameter specialized model that achieves performance comparable to
state-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on
hard problems. Central to DPLM's effectiveness is DualReflect, our novel
synthetic data generation pipeline, designed to scale up training data from a
limited set of initial examples. DualReflect combines forward generation for
diversity and backward generation for reliability. Our results reveal a key
insight: backward generation is favored in low-data regimes for its strong
correctness guarantees, while forward generation, though lacking such
guarantees, becomes increasingly valuable at scale for introducing diverse
formulations. This trade-off highlights the complementary strengths of both
approaches and the importance of combining them.

</details>


### [7] [Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity](https://arxiv.org/abs/2507.11787)
*Chandrashekar Muniyappa,Eunjin Kim*

Main category: cs.AI

TL;DR: 本文介绍群体智能在人工智能中受关注，可解决计算机优化问题，将综述基于群体智能算法的语义相似文档搜索的进展并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 群体智能在人工智能中很受欢迎且可解决计算机优化问题，因此有必要综述其在语义相似文档搜索方面的发展。

Method: 进行相关领域的文献调研。

Result: 文中未提及具体结果。

Conclusion: 文中未提及具体结论，只是表示会推荐未来研究方向。

Abstract: Swarm Intelligence (SI) is gaining a lot of popularity in artificial
intelligence, where the natural behavior of animals and insects is observed and
translated into computer algorithms called swarm computing to solve real-world
problems. Due to their effectiveness, they are applied in solving various
computer optimization problems. This survey will review all the latest
developments in Searching for documents based on semantic similarity using
Swarm Intelligence algorithms and recommend future research directions.

</details>


### [8] [A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS](https://arxiv.org/abs/2507.11916)
*Ehsan Futuhi,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: 本文介绍在深度优先搜索中批量进行GPU计算的方法，创建了Batch IDA*和Batch BTS等算法，在3x3魔方和4x4滑动拼图上评估，表明能有效批量执行GPU操作，并分析了超参数等对性能的影响。


<details>
  <summary>Details</summary>
Motivation: GPU技术发展带来新机遇，但搜索中利用GPU的算法很少，旨在设计在深度优先搜索中利用GPU并行计算的方法。

Method: 提出新的成本有界深度优先搜索（CB - DFS）方法，结合现代CPU和GPU的并行性，基于异步并行IDA*（AIDA*）方法，创建Batch IDA*和Batch BTS等算法。

Result: 在3x3魔方和4x4滑动拼图上的实验表明，能在深度优先搜索中有效批量执行GPU操作。

Conclusion: 所提出的方法可在深度优先搜索中有效批量进行GPU计算，同时保证最优性，且通过实验分析了超参数、神经网络启发式大小和硬件资源对性能的影响。

Abstract: The rapid advancement of GPU technology has unlocked powerful parallel
processing capabilities, creating new opportunities to enhance classic search
algorithms. A recent successful application of GPUs is in compressing large
pattern database (PDB) heuristics using neural networks while preserving
heuristic admissibility. However, very few algorithms have been designed to
exploit GPUs during search. Several variants of A* exist that batch GPU
computations. In this paper we introduce a method for batching GPU computations
in depth first search. In particular, we describe a new cost-bounded
depth-first search (CB-DFS) method that leverages the combined parallelism of
modern CPUs and GPUs. This is used to create algorithms like \emph{Batch IDA*},
an extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an
extensions of Budgeted Tree Search. Our approach builds on the general approach
used by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality
guarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding
tile puzzle (STP), showing that GPU operations can be efficiently batched in
DFS. Additionally, we conduct extensive experiments to analyze the effects of
hyperparameters, neural network heuristic size, and hardware resources on
performance.

</details>


### [9] [Aime: Towards Fully-Autonomous Multi-Agent Framework](https://arxiv.org/abs/2507.11988)
*Yexuan Shi,Mingyu Wang,Yunxiang Cao,Hongjie Lai,Junjian Lan,Xin Han,Yu Wang,Jie Geng,Zhenan Li,Zihao Xia,Xiang Chen,Chen Li,Jian Xu,Wenbo Duan,Yuanshuo Zhu*

Main category: cs.AI

TL;DR: 论文指出大语言模型驱动的多智能体系统受限于现有框架，提出新框架Aime，经实验Aime表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统的计划执行框架存在刚性执行计划、静态代理能力和低效通信等问题，限制了系统在动态环境中的适应性和鲁棒性，需要新框架解决。

Method: 提出Aime框架，包含动态规划器、动态执行器工厂和集中式进度管理模块。

Result: 在通用推理、软件工程和实时网页导航等基准测试中，Aime始终优于各领域高度专业化的现有模型。

Conclusion: Aime具有更好的适应性和任务成功率，是多智能体协作更具弹性和有效的基础。

Abstract: Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are
emerging as a powerful paradigm for solving complex, multifaceted problems.
However, the potential of these systems is often constrained by the prevalent
plan-and-execute framework, which suffers from critical limitations: rigid plan
execution, static agent capabilities, and inefficient communication. These
weaknesses hinder their adaptability and robustness in dynamic environments.
This paper introduces Aime, a novel multi-agent framework designed to overcome
these challenges through dynamic, reactive planning and execution. Aime
replaces the conventional static workflow with a fluid and adaptive
architecture. Its core innovations include: (1) a Dynamic Planner that
continuously refines the overall strategy based on real-time execution
feedback; (2) an Actor Factory that implements Dynamic Actor instantiation,
assembling specialized agents on-demand with tailored tools and knowledge; and
(3) a centralized Progress Management Module that serves as a single source of
truth for coherent, system-wide state awareness. We empirically evaluated Aime
on a diverse suite of benchmarks spanning general reasoning (GAIA), software
engineering (SWE-bench Verified), and live web navigation (WebVoyager). The
results demonstrate that Aime consistently outperforms even highly specialized
state-of-the-art agents in their respective domains. Its superior adaptability
and task success rate establish Aime as a more resilient and effective
foundation for multi-agent collaboration.

</details>


### [10] [Understanding visual attention beehind bee-inspired UAV navigation](https://arxiv.org/abs/2507.11992)
*Pranav Rajbhandari,Abhi Veda,Matthew Garratt,Mandayam Srinivasan,Sridhar Ravi*

Main category: cs.AI

TL;DR: 训练强化学习智能体仅用光流进行隧道避障导航，发现其关注光流不连续和大光流区域，该策略或可用于开发物理无人机控制律。


<details>
  <summary>Details</summary>
Motivation: 生物系统飞行和避障能力促使在无人机导航中采用仿生设计，借鉴蜜蜂用光流导航。

Method: 训练强化学习智能体，仅用光流作为感官输入在有障碍物的隧道中导航，并检查其注意力模式。

Result: 训练的智能体主要关注光流不连续区域和大光流区域，能避障并保持居中，类似飞行昆虫行为。

Conclusion: 该策略或可用于为物理无人机开发简单明确的控制律。

Abstract: Bio-inspired design is often used in autonomous UAV navigation due to the
capacity of biological systems for flight and obstacle avoidance despite
limited sensory and computational capabilities. In particular, honeybees mainly
use the sensory input of optic flow, the apparent motion of objects in their
visual field, to navigate cluttered environments. In our work, we train a
Reinforcement Learning agent to navigate a tunnel with obstacles using only
optic flow as sensory input. We inspect the attention patterns of trained
agents to determine the regions of optic flow on which they primarily base
their motor decisions. We find that agents trained in this way pay most
attention to regions of discontinuity in optic flow, as well as regions with
large optic flow magnitude. The trained agents appear to navigate a cluttered
tunnel by avoiding the obstacles that produce large optic flow, while
maintaining a centered position in their environment, which resembles the
behavior seen in flying insects. This pattern persists across independently
trained agents, which suggests that this could be a good strategy for
developing a simple explicit control law for physical UAVs.

</details>


### [11] [Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs](https://arxiv.org/abs/2507.12110)
*Ye Han,Lijun Zhang,Dejian Meng,Zhuang Zhang*

Main category: cs.AI

TL;DR: 提出TPE - MARL方法优化混合交通中CAV协同决策，经模拟验证有效，平衡探索与利用，性能优且代码开源。


<details>
  <summary>Details</summary>
Motivation: 强化学习中探索 - 利用权衡是挑战，多智能体强化学习中因联合状态 - 动作空间指数增长而加剧，需优化混合交通中CAV协同决策。

Method: 构建动态交通流的博弈拓扑张量压缩信息、减少搜索空间；基于博弈拓扑张量，以QMIX为骨干算法，建立包含访问计数和智能体互信息的TPE - MARL框架。

Result: 不同交通密度和CAV渗透率的模拟显示TPE - MARL有效平衡探索与利用，在交通效率、安全等方面表现优，决策合理性可与人类驾驶员媲美或更优。

Conclusion: TPE - MARL方法能有效优化混合交通中CAV的协同决策。

Abstract: The exploration-exploitation trade-off constitutes one of the fundamental
challenges in reinforcement learning (RL), which is exacerbated in multi-agent
reinforcement learning (MARL) due to the exponential growth of joint
state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)
method for optimizing cooperative decision-making of connected and autonomous
vehicles (CAVs) in mixed traffic. This work presents two primary contributions:
First, we construct a game topology tensor for dynamic traffic flow,
effectively compressing high-dimensional traffic state information and decrease
the search space for MARL algorithms. Second, building upon the designed game
topology tensor and using QMIX as the backbone RL algorithm, we establish a
topology-enhanced MARL framework incorporating visit counts and agent mutual
information. Extensive simulations across varying traffic densities and CAV
penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations
encompassing training dynamics, exploration patterns, macroscopic traffic
performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL
successfully balances exploration and exploitation. Consequently, it exhibits
superior performance in terms of traffic efficiency, safety, decision
smoothness, and task completion. Furthermore, the algorithm demonstrates
decision-making rationality comparable to or exceeding that of human drivers in
both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is
available at
\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.

</details>


### [12] [Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation](https://arxiv.org/abs/2507.12186)
*Edward Kim,Hanna Kurniawati*

Main category: cs.AI

TL;DR: 提出新型在线近似POMDP求解器，有理论保证，实证表现优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 解决在线规划采样稀疏问题，提高POMDP求解性能。

Method: 提出Partially Observable Reference Policy Programming求解器，采样深度未来历史并逐步更新策略，且给出算法底层方案理论保证。

Result: 在两个动态环境大规模问题的实证评估中，求解器表现显著优于当前在线基准。

Conclusion: 所提求解器有效，能在动态环境大规模问题中取得良好效果。

Abstract: This paper proposes Partially Observable Reference Policy Programming, a
novel anytime online approximate POMDP solver which samples meaningful future
histories very deeply while simultaneously forcing a gradual policy update. We
provide theoretical guarantees for the algorithm's underlying scheme which say
that the performance loss is bounded by the average of the sampling
approximation errors rather than the usual maximum, a crucial requirement given
the sampling sparsity of online planning. Empirical evaluations on two
large-scale problems with dynamically evolving environments -- including a
helicopter emergency scenario in the Corsica region requiring approximately 150
planning steps -- corroborate the theoretical results and indicate that our
solver considerably outperforms current online benchmarks.

</details>


### [13] [Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning](https://arxiv.org/abs/2507.12215)
*Yuhao Chen,Shuochen Liu,Yuanjie Lyu,Chao Zhang,Jiayao Shi,Tong Xu*

Main category: cs.AI

TL;DR: 文章以中国象棋为测试平台，提出针对象棋的训练框架，训练出7B参数的Xiangqi - R1模型，实验表明其相比通用大模型有显著提升，为空间复杂领域战略智能发展指明方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在空间战略推理方面能力探索不足，而这对复杂棋盘游戏很关键，需提升其在这类环境中的战略能力。

Method: 采用中国象棋为测试平台，基于五百万棋盘 - 走法对的大规模数据集，结合专家注释和引擎评估，分三阶段训练7B参数的Xiangqi - R1模型。

Result: 通用大模型在相关任务中表现不佳，Xiangqi - R1走法合法性提升18%，分析准确率提升22%。

Conclusion: 为空间复杂领域创建通用战略智能指出了有前景的路径。

Abstract: Game playing has long served as a fundamental benchmark for evaluating
Artificial General Intelligence (AGI). While Large Language Models (LLMs) have
demonstrated impressive capabilities in general reasoning, their effectiveness
in spatial strategic reasoning, which is critical for complex and fully
observable board games, remains insufficiently explored. In this work, we adopt
Chinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate
rules and spatial complexity. To advance LLMs' strategic competence in such
environments, we propose a training framework tailored to Xiangqi, built upon a
large-scale dataset of five million board-move pairs enhanced with expert
annotations and engine evaluations. Building on this foundation, we introduce
Xiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning
for legal move prediction to capture basic spatial rules, (2) incorporating
strategic annotations to improve decision-making, and (3) applying
reinforcement learning via Group Relative Policy Optimization (GRPO) with
multi-dimensional reward signals to enhance reasoning stability. Our
Experimental results indicate that, despite their size and power,
general-purpose LLMs struggle to achieve satisfactory performance in these
tasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an
18% rise in move legality and a 22% boost in analysis accuracy. Our results
point to a promising path for creating general strategic intelligence in
spatially complex areas.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [14] [Quantifying data needs in surrogate modeling for flow fields in 2D stirred tanks with physics-informed neural networks (PINNs)](https://arxiv.org/abs/2507.11640)
*Veronika Trávníková,Eric von Lieres,Marek Behr*

Main category: cs.CE

TL;DR: 研究量化二维搅拌槽流场替代模型中香草PINNs的数据需求，与经典监督神经网络和BINNs对比，发现少量或近似数据下PINNs也能高精度预测。


<details>
  <summary>Details</summary>
Motivation: CFD模拟搅拌槽内流动计算成本高，获取大数据集昂贵，PINNs可降低数据需求并保持精度，因此研究其数据需求。

Method: 量化香草PINNs开发二维搅拌槽流场替代模型的数据需求，并与经典监督神经网络和BINNs对比。

Result: 仅用六个数据点，替代模型在雷诺数50 - 5000范围内预测误差约3%；用速度剖面近似替代真实数据标签，预测误差约2.5%。

Conclusion: 即使数据集有限或为近似数据，PINNs也能有效训练，达到与高保真数据相当的高精度。

Abstract: Stirred tanks are vital in chemical and biotechnological processes,
particularly as bioreactors. Although computational fluid dynamics (CFD) is
widely used to model the flow in stirred tanks, its high computational
cost$-$especially in multi-query scenarios for process design and
optimization$-$drives the need for efficient data-driven surrogate models.
However, acquiring sufficiently large datasets can be costly. Physics-informed
neural networks (PINNs) offer a promising solution to reduce data requirements
while maintaining accuracy by embedding underlying physics into neural network
(NN) training. This study quantifies the data requirements of vanilla PINNs for
developing surrogate models of a flow field in a 2D stirred tank. We compare
these requirements with classical supervised neural networks and
boundary-informed neural networks (BINNs). Our findings demonstrate that
surrogate models can achieve prediction errors around 3% across Reynolds
numbers from 50 to 5000 using as few as six datapoints. Moreover, employing an
approximation of the velocity profile in place of real data labels leads to
prediction errors of around 2.5%. These results indicate that even with limited
or approximate datasets, PINNs can be effectively trained to deliver high
accuracy comparable to high-fidelity data.

</details>


### [15] [MNO : A Multi-modal Neural Operator for Parametric Nonlinear BVPs](https://arxiv.org/abs/2507.11870)
*Vamshi C. Madala,Nithin Govindarajan,Shivkumar Chandrasekaran*

Main category: cs.CE

TL;DR: 提出多模态神经算子（MNO）架构解决多参数非线性边值问题，展示其多模态泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子映射方式局限，灵活性和适用性不足，需新架构统一映射多参数。

Method: 受快速多极子方法（FMM）分层嵌套基启发，构建含广义FMM块、单模态神经算子（UNO）和多模态融合机制的MNO。

Result: 在实验中，网络能有效处理偏微分方程系数、源项或边界项的同时变化。

Conclusion: MNO架构能实现多参数统一映射，具备多模态泛化能力。

Abstract: We introduce a novel Multimodal Neural Operator (MNO) architecture designed
to learn solution operators for multi-parameter nonlinear boundary value
problems (BVPs). Traditional neural operators primarily map either the PDE
coefficients or source terms independently to the solution, limiting their
flexibility and applicability. In contrast, our proposed MNO architecture
generalizes these approaches by mapping multiple parameters including PDE
coefficients, source terms, and boundary conditions to the solution space in a
unified manner. Our MNO is motivated by the hierarchical nested bases of the
Fast Multipole Method (FMM) and is constructed systematically through three key
components: a parameter efficient Generalized FMM (GFMM) block, a Unimodal
Neural Operator (UNO) built upon GFMM blocks for single parameter mappings, and
most importantly, a multimodal fusion mechanism extending these components to
learn the joint map. We demonstrate the multimodal generalization capacity of
our approach on both linear and nonlinear BVPs. Our experiments show that the
network effectively handles simultaneous variations in PDE coefficients and
source or boundary terms.

</details>


### [16] [Universal Fourier Neural Operators for Micromechanics](https://arxiv.org/abs/2507.12233)
*Binh Huy Nguyen,Matti Schneider*

Main category: cs.CE

TL;DR: 本文倡导将傅里叶神经算子（FNOs）用于微观力学，结合基于快速傅里叶变换（FFT）的计算微观力学方法，展示FNOs解决微观力学问题的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决均匀化中的细胞问题困难，现有深度学习框架在速度和通用性上不如传统计算框架，且不清楚机器学习方法的效果和前景。

Method: 构建FNO替代模型，模仿基于FFT方法的基本方案，利用对FNO的物理增强给出显式保证，并构造无需训练的FNO。

Result: 得到的算子能以期望精度预测任意刚度分布（受材料对比度约束）的细胞问题解，具有通用近似性，满足内存要求，运行时间与经典FFT求解器成比例，可处理超1亿体素的大规模问题。

Conclusion: 强调FNOs解决微观力学问题的潜力，连接基于FFT的方法和FNOs，有望促进两者交流。

Abstract: \noindent Solving cell problems in homogenization is hard, and available
deep-learning frameworks fail to match the speed and generality of traditional
computational frameworks. More to the point, it is generally unclear what to
expect of machine-learning approaches, let alone single out which approaches
are promising. In the work at hand, we advocate Fourier Neural Operators (FNOs)
for micromechanics, empowering them by insights from computational
micromechanics methods based on the fast Fourier transform (FFT). We construct
an FNO surrogate mimicking the basic scheme foundational for FFT-based methods
and show that the resulting operator predicts solutions to cell problems with
\emph{arbitrary} stiffness distribution only subject to a material-contrast
constraint up to a desired accuracy. In particular, there are no restrictions
on the material symmetry like isotropy, on the number of phases and on the
geometry of the interfaces between materials. Also, the provided fidelity is
sharp and uniform, providing explicit guarantees leveraging our physical
empowerment of FNOs. To show the desired universal approximation property, we
construct an FNO explicitly that requires no training to begin with. Still, the
obtained neural operator complies with the same memory requirements as the
basic scheme and comes with runtimes proportional to classical FFT solvers. In
particular, large-scale problems with more than 100 million voxels are readily
handled. The goal of this work is to underline the potential of FNOs for
solving micromechanical problems, linking FFT-based methods to FNOs. This
connection is expected to provide a fruitful exchange between both worlds.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [17] [SIEVE: Effective Filtered Vector Search with Collection of Indexes](https://arxiv.org/abs/2507.11907)
*Zhaoheng Li,Silu Huang,Wei Ding,Yongjoo Park,Jianjun Chen*

Main category: cs.DB

TL;DR: 提出一种构建多索引的过滤向量搜索方法，在不同数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的过滤相似性搜索技术在考虑硬约束时效果不佳，且不能满足多种谓词的少跳搜索需求。

Method: 构建多个服务于不同谓词形式的索引，设计三维分析模型，采用工作量感知方法打包索引，查询时用模型选择最快搜索的索引。

Result: 在不同选择性和形式的数据集上实现了高达8.06倍的加速，构建时间低至1%，内存小于标准HNSW图的2.15倍。

Conclusion: 所提方法在过滤向量搜索任务中具有优越性能和良好支持。

Abstract: Many real-world tasks such as recommending videos with the kids tag can be
reduced to finding most similar vectors associated with hard predicates. This
task, filtered vector search, is challenging as prior state-of-the-art
graph-based (unfiltered) similarity search techniques quickly degenerate when
hard constraints are considered. That is, effective graph-based filtered
similarity search relies on sufficient connectivity for reaching the most
similar items within just a few hops. To consider predicates, recent works
propose modifying graph traversal to visit only the items that may satisfy
predicates. However, they fail to offer the just-a-few-hops property for a wide
range of predicates: they must restrict predicates significantly or lose
efficiency if only a small fraction of items satisfy predicates.
  We propose an opposite approach: instead of constraining traversal, we build
many indexes each serving different predicate forms. For effective
construction, we devise a three-dimensional analytical model capturing
relationships among index size, search time, and recall, with which we follow a
workload-aware approach to pack as many useful indexes as possible into a
collection. At query time, the analytical model is employed yet again to
discern the one that offers the fastest search at a given recall. We show
superior performance and support on datasets with varying selectivities and
forms: our approach achieves up to 8.06x speedup while having as low as 1%
build time versus other indexes, with less than 2.15x memory of a standard HNSW
graph and modest knowledge of past workloads.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [The AI Shadow War: SaaS vs. Edge Computing Architectures](https://arxiv.org/abs/2507.11545)
*Rhea Pritham Marpu,Kevin J McNamara,Preeti Gupta*

Main category: cs.DC

TL;DR: 本文分析AI架构中集中式云模型与去中心化边缘AI的竞争，展示边缘AI在性能、效率、隐私等方面优势及市场增长前景，预示混合边缘 - 云生态系统的出现。


<details>
  <summary>Details</summary>
Motivation: 探讨AI架构中集中式云模型与去中心化边缘AI的竞争态势。

Method: 从计算能力、能源效率和数据隐私等方面对比分析两种架构。

Result: 边缘AI在性能上挑战云系统，有10000倍效率优势，保障数据主权，市场预计从2025年90亿美元增长到2030年496亿美元，关键应用依赖其超低延迟。

Conclusion: 边缘AI分布式方法符合高效信息处理，混合边缘 - 云生态系统将不可避免地出现。

Abstract: The very DNA of AI architecture presents conflicting paths: centralized
cloud-based models (Software-as-a-Service) versus decentralized edge AI (local
processing on consumer devices). This paper analyzes the competitive
battleground across computational capability, energy efficiency, and data
privacy. Recent breakthroughs show edge AI challenging cloud systems on
performance, leveraging innovations like test-time training and
mixture-of-experts architectures. Crucially, edge AI boasts a 10,000x
efficiency advantage: modern ARM processors consume merely 100 microwatts
forinference versus 1 watt for equivalent cloud processing. Beyond efficiency,
edge AI secures data sovereignty by keeping processing local, dismantling
single points of failure in centralized architectures. This democratizes access
throughaffordable hardware, enables offline functionality, and reduces
environmental impact by eliminating data transmission costs. The edge AI market
projects explosive growth from $9 billion in 2025 to $49.6 billion by 2030
(38.5% CAGR), fueled by privacy demands and real-time analytics. Critical
applications including personalized education, healthcare monitoring,
autonomous transport, and smart infrastructure rely on edge AI's ultra-low
latency (5-10ms versus 100-500ms for cloud). The convergence of architectural
innovation with fundamental physics confirms edge AI's distributed approach
aligns with efficient information processing, signaling the inevitable
emergence of hybrid edge-cloud ecosystems.

</details>


### [19] [A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing](https://arxiv.org/abs/2507.11560)
*Xin Wang,Xiao Huan Li,Xun Wang*

Main category: cs.DC

TL;DR: 本文提出适用于工业物联网边缘计算环境的AIGC任务卸载框架和MADDPG - MATO算法，实验表明该算法在降低延迟、能耗和提高任务完成率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 工业物联网与AIGC集成面临计算密集和低延迟挑战，传统云计算难以满足实时需求，边缘计算面临任务动态性等问题。

Method: 提出AIGC任务卸载框架，设备作为多智能体协作卸载任务到合适边缘服务器，设计基于MADDPG - MATO的任务卸载算法。

Result: MADDPG - MATO算法在四组实验中，延迟平均降低6.98%，能耗降低7.12%，任务完成率提高3.72%。

Conclusion: 提出的算法在动态、高负载工业物联网环境中稳健且高效。

Abstract: The integration of the Industrial Internet of Things (IIoT) with Artificial
Intelligence-Generated Content (AIGC) offers new opportunities for smart
manufacturing, but it also introduces challenges related to
computation-intensive tasks and low-latency demands. Traditional generative
models based on cloud computing are difficult to meet the real-time
requirements of AIGC tasks in IIoT environments, and edge computing can
effectively reduce latency through task offloading. However, the dynamic nature
of AIGC tasks, model switching delays, and resource constraints impose higher
demands on edge computing environments. To address these challenges, this paper
proposes an AIGC task offloading framework tailored for IIoT edge computing
environments, considering the latency and energy consumption caused by AIGC
model switching for the first time. IIoT devices acted as multi-agent
collaboratively offload their dynamic AIGC tasks to the most appropriate edge
servers deployed with different generative models. A model aware AIGC task
offloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient
(MADDPG-MATO) is devised to minimize the latency and energy. Experimental
results show that MADDPG-MATO outperforms baseline algorithms, achieving an
average reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72%
increase in task completion rate across four sets of experiments with model
numbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is
robust and efficient in dynamic, high-load IIoT environments.

</details>


### [20] [Environmentally-Conscious Cloud Orchestration Considering Geo-Distributed Data Centers](https://arxiv.org/abs/2507.11563)
*Giulio Attenni,Novella Bartolini*

Main category: cs.DC

TL;DR: 本文对云环境下考虑环境因素的作业部署和迁移进行理论探讨，提出优化模型并通过案例展示其潜力。


<details>
  <summary>Details</summary>
Motivation: 随着对可持续云服务需求增长，需让云客户基于可持续性指标选择数据中心运营商并准确报告服务生态足迹。

Method: 分析可持续性报告，定义数据中心环境影响概况，将问题形式化为优化模型，平衡多环境因素并考虑用户偏好。

Result: 模拟案例研究表明该方法相比单一可持续因素优化的基线策略有潜力。

Conclusion: 所提方法在云环境作业部署和迁移中考虑多环境因素有应用潜力。

Abstract: This paper presents a theoretical discussion for environmentally-conscious
job deployment and migration in cloud environments, aiming to minimize the
environmental impact of resource provisioning while incorporating
sustainability requirements. As the demand for sustainable cloud services
grows, it is crucial for cloud customers to select data center operators based
on sustainability metrics and to accurately report the ecological footprint of
their services. To this end, we analyze sustainability reports and define
comprehensive environmental impact profiles for data centers, incorporating key
sustainability indicators. We formalize the problem as an optimization model,
balancing multiple environmental factors while respecting user preferences. A
simulative case study demonstrates the {potential} of our approach compared to
baseline strategies that optimize for single sustainability factors.

</details>


### [21] [PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training](https://arxiv.org/abs/2507.11683)
*Seth Ockerman,Amal Gueroudji,Tanwi Mallick,Yixuan He,Line Pouchard,Robert Ross,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 本文提出PGT - I扩展，解决时空图神经网络在大规模数据集应用的内存限制问题，减少内存开销并提高训练速度。


<details>
  <summary>Details</summary>
Motivation: 时空图神经网络因内存限制主要应用于小规模数据集，现有分布式训练框架不支持时空模型且忽略时空数据特性。

Method: 提出PGT - I扩展，集成分布式数据并行训练、索引批处理和分布式索引批处理两种新策略，利用时空结构在运行时动态构建快照。

Result: 首次在不进行图划分的情况下在整个PeMS数据集上训练ST - GNN，峰值内存使用最多减少89%，在128个GPU上比标准DDP最多提速13.1倍。

Conclusion: 所提出的技术能有效解决时空图神经网络在大规模数据集上的内存限制问题，提升训练效率。

Abstract: Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for
modeling spatial and temporal data dependencies. However, their applications
have been limited primarily to small-scale datasets because of memory
constraints. While distributed training offers a solution, current frameworks
lack support for spatiotemporal models and overlook the properties of
spatiotemporal data. Informed by a scaling study on a large-scale workload, we
present PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch
Geometric Temporal that integrates distributed data parallel training and two
novel strategies: index-batching and distributed-index-batching. Our index
techniques exploit spatiotemporal structure to construct snapshots dynamically
at runtime, significantly reducing memory overhead, while
distributed-index-batching extends this approach by enabling scalable
processing across multiple GPUs. Our techniques enable the first-ever training
of an ST-GNN on the entire PeMS dataset without graph partitioning, reducing
peak memory usage by up to 89\% and achieving up to a 13.1x speedup over
standard DDP with 128 GPUs.

</details>


### [22] [Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI](https://arxiv.org/abs/2507.11830)
*Samyam Rajbhandari,Mert Hidayetoglu,Aurick Qiao,Ye Wang,Juncheng Yang,Jeff Rasley,Michael Wyatt,Yuxiong He*

Main category: cs.DC

TL;DR: Arctic Inference 是 Snowflake AI Research 的开源 vLLM 插件，引入 Shift Parallelism 策略，实现更快推理，性能优越，已用于 Snowflake Cortex AI 并向社区开放。


<details>
  <summary>Details</summary>
Motivation: 解决现有系统在推理时需在延迟、吞吐量和成本之间权衡的问题。

Method: 引入 Shift Parallelism 动态并行策略，集成投机解码、SwiftKV 计算缩减和优化嵌入推理。

Result: 实现高达 3.4 倍更快的请求完成、1.75 倍更快的生成速度，每 GPU 嵌入推理达 160 万令牌/秒，性能优于延迟和吞吐量优化部署。

Conclusion: Arctic Inference 为企业 AI 提供了最先进且经济高效的推理方案，并已向社区开放。

Abstract: Inference is now the dominant AI workload, yet existing systems force
trade-offs between latency, throughput, and cost. Arctic Inference, an
open-source vLLM plugin from Snowflake AI Research, introduces Shift
Parallelism, a dynamic parallelism strategy that adapts to real-world traffic
while integrating speculative decoding, SwiftKV compute reduction, and
optimized embedding inference. It achieves up to 3.4 times faster request
completion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for
embeddings, outperforming both latency- and throughput-optimized deployments.
Already powering Snowflake Cortex AI, Arctic Inference delivers
state-of-the-art, cost-effective inference for enterprise AI and is now
available to the community.

</details>


### [23] [Performance Assessment of Load Balancing Methods in Cloud Computing: Analysis of Round Robin, Equally Spread, and Throttled Strategies Using Cloud Analyst](https://arxiv.org/abs/2507.11899)
*Saeid Aghasoleymani Najafabadi*

Main category: cs.DC

TL;DR: 研究用Cloud Analyst评估不同负载均衡算法在多种场景下的性能，发现分布式资源设置可降低响应时间和成本，强调战略资源部署和动态管理的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着云环境工作负载动态且不可预测，传统静态负载均衡策略需向自适应和智能方法转变，需评估不同算法性能。

Method: 使用Cloud Analyst模拟工具，在集中和分布式资源设置等多种场景下评估不同负载均衡算法性能。

Result: Round Robin算法在单数据中心处理时间略优；Equally Spread和Throttled技术在考虑网络延迟时竞争力强；分布式资源设置可显著降低响应时间，Equally Spread和Throttled算法能保持快速响应并降低成本。

Conclusion: 需战略资源部署和主动基础设施规划以平衡性能和成本，采用智能动态负载均衡和资源管理可满足云需求、优化成本，持续评估和集成新兴技术对云运营至关重要。

Abstract: Load balancing plays a pivotal role in cloud computing, ensuring that
resources are optimally allocated to maintain high service quality and
operational efficiency. As workloads in cloud environments become increasingly
dynamic and unpredictable, load balancing strategies are evolving from
traditional static methods to more adaptive and intelligent approaches. In this
study, the Cloud Analyst simulation tool was used to evaluate the performance
of different load balancing algorithms under various scenarios, including both
centralized and distributed resource setups. The results highlight that while
the Round Robin algorithm yields slightly better processing times within a
single data center, Equally Spread and Throttled techniques perform
competitively, especially when network latency is considered. More importantly,
when resources are distributed across multiple data centers, response times are
significantly reduced, emphasizing the value of proximity and efficient load
distribution. In these distributed environments, Equally Spread and Throttled
algorithms not only maintain quick response times but also contribute to lower
operational costs. These findings demonstrate the necessity of strategic
resource placement and proactive infrastructure planning to balance performance
and cost. Adopting intelligent, dynamic load balancing and resource management
practices can help organizations meet evolving cloud demands, optimize costs,
and maintain a competitive advantage. Continuous evaluation and integration of
emerging technologies are crucial for sustaining effective and scalable cloud
operations.

</details>


### [24] [Making Serverless Computing Extensible: A Case Study of Serverless Data Analytics](https://arxiv.org/abs/2507.11929)
*Minchen Yu,Yinghao Ren,Jiamu Zhao,Jiaqi Li*

Main category: cs.DC

TL;DR: 本文提出无服务器计算的可扩展设计原则，并在Proteus中实现，初步结果显示其能优化分析查询执行并支持细粒度资源共享。


<details>
  <summary>Details</summary>
Motivation: 通用无服务器平台对复杂工作负载性能不佳，而特定应用系统会破坏简单性和通用性。

Method: 提出可扩展设计原则，以数据分析为例，在Proteus中引入决策工作流抽象，让开发者定制控制平面行为。

Result: Proteus原型有效优化分析查询执行，支持跨不同应用的细粒度资源共享。

Conclusion: 所提设计原则可行，能在保留易用环境的同时实现领域优化。

Abstract: Serverless computing has attracted a broad range of applications due to its
ease of use and resource elasticity. However, developing serverless
applications often poses a dilemma -- relying on general-purpose serverless
platforms can fall short of delivering satisfactory performance for complex
workloads, whereas building application-specific serverless systems undermines
the simplicity and generality. In this paper, we propose an extensible design
principle for serverless computing. We argue that a platform should enable
developers to extend system behaviors for domain-specialized optimizations
while retaining a shared, easy-to-use serverless environment. We take data
analytics as a representative serverless use case and realize this design
principle in Proteus. Proteus introduces a novel abstraction of decision
workflows, allowing developers to customize control-plane behaviors for
improved application performance. Preliminary results show that Proteus's
prototype effectively optimizes analytical query execution and supports
fine-grained resource sharing across diverse applications.

</details>


### [25] [NineToothed: A Triton-Based High-Level Domain-Specific Language for Machine Learning](https://arxiv.org/abs/2507.11978)
*Jiacheng Huang,Zimin Li,Yinghui Li,Haojie Wang*

Main category: cs.DC

TL;DR: 本文介绍特定领域语言NineToothed，它为机器学习编程提供串行语义，能自动将串行代码转为并行代码，简化开发且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习DSL如Triton要求开发者有并行编程专业知识，接触大量底层细节，增加开发和维护难度，因此需开发支持串行编程的新编程模型。

Method: 引入NineToothed，包含采用排列 - 应用范式的面向张量元编程（TOM）语言和高性能并行代码生成器。

Result: 评估结果显示NineToothed能极大简化计算内核开发，性能与Triton相当。

Conclusion: NineToothed能在简化计算内核开发的同时保持较好性能。

Abstract: The emergence of deep learning domain-specific languages (DSLs) has
substantially reduced the obstacles in developing high-performance,
cross-platform compute kernels. However, current DSLs, such as Triton, still
demand that developers possess expertise in parallel programming and expose
them to many low-level details. This requirement complicates the development
process and adds to the difficulty of maintaining compute kernels.
Consequently, developing a new programming model that supports serial
programming for deep learning workloads is crucial.
  This paper introduces NineToothed, a domain-specific language that offers
serial semantics for machine learning programming. Through the automatic
transformation of serial code into parallel code, NineToothed significantly
streamlines the development process while causing minimal performance
degradation. NineToothed encompasses (1) a language with tensor-oriented
metaprogramming (TOM) that adopts the arrange-and-apply paradigm, enabling the
expression of tiled computations without the need to manage low-level details
and (2) a code generator for generating high-performance parallel code. Our
evaluation results indicate that NineToothed can greatly simplify compute
kernel development while maintaining performance comparable to that of Triton.

</details>


### [26] [ARRC: Explainable, Workflow-Integrated Recommender for Sustainable Resource Optimization Across the Edge-Cloud Continuum](https://arxiv.org/abs/2507.12032)
*Brian-Frederik Jahnke,René Brinkhege,Jan Peter Meyer,Daniel Tebernum,Falk Howar*

Main category: cs.DC

TL;DR: 本文介绍推荐系统 ARRC 用于边缘 - 云系统资源优化，经评估可减少操作员工作量、提高计算利用率并维持低错误率。


<details>
  <summary>Details</summary>
Motivation: 现有边缘 - 云资源优化方案存在反应式、限于单抽象层、需平台侵入式更改等问题，无法实现效率和可维护性提升，需要安全、透明、低工作量的资源优化方案。

Method: 引入基于软件工程设计原则的推荐系统 ARRC，将可解释的跨层资源推荐融入操作员工作流，通过共享接口协调专业可审计代理封装优化逻辑。

Result: 多区域工业部署实证评估显示，ARRC 减少超 50% 操作员工作量，计算利用率提高达 7.7 倍，错误率维持在 5% 以下。

Conclusion: 基于可解释推荐的架构能在生产规模上实现可持续的效率和可维护性提升，ARRC 为资源管理集成可解释、工作流驱动自动化提供评估框架。

Abstract: Achieving sustainable, explainable, and maintainable automation for resource
optimization is a core challenge across the edge-cloud continuum. Persistent
overprovisioning and operational complexity often stem from heterogeneous
platforms and layered abstractions, while systems lacking explainability and
maintainability become fragile, impede safe recovery, and accumulate technical
debt. Existing solutions are frequently reactive, limited to single abstraction
layers, or require intrusive platform changes, leaving efficiency and
maintainability gains unrealized.
  This paper addresses safe, transparent, and low-effort resource optimization
in dynamic, multi-tenant edge-cloud systems, without disrupting operator
workflows or increasing technical debt. We introduce ARRC, a recommender system
rooted in software engineering design principles, which delivers explainable,
cross-layer resource recommendations directly into operator workflows (such as
tickets and GitOps pull requests). ARRC encapsulates optimization logic in
specialized, auditable agents coordinated via a shared interface, supporting
maintainability and extensibility through transparency and the ability to
inspect both recommendations and their rationale.
  Empirical evaluation in a multi-region industrial deployment shows that ARRC
reduces operator workload by over 50%, improves compute utilization by up to
7.7x, and maintains error rates below 5%, with most benefits achieved through
incremental, operator-approved changes. This demonstrates that explainable,
recommendation-based architectures can achieve sustainable efficiency and
maintainability improvements at production scale.
  ARRC provides an empirically evaluated framework for integrating explainable,
workflow-driven automation into resource management, intended to advance best
practices for robust, maintainable, and transparent edge-cloud continuum
platforms.

</details>


### [27] [Distributed Algorithms for Potential Problems](https://arxiv.org/abs/2507.12038)
*Alkida Balliu,Thomas Boudier,Francesco d'Amore,Dennis Olivetti,Gustav Schmid,Jukka Suomela*

Main category: cs.DC

TL;DR: 提出求解局部势问题的快速分布式算法，解决有界度图中局部最优割的轮复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 局部最优割的分布式轮复杂度问题悬而未决，上下界差距大。

Method: 设计快速分布式算法求解局部势问题。

Result: 在有界度图中，所有局部势问题（包括局部最优割）在确定性和随机LOCAL模型中都可在$log^{O(1)} n$轮内解决。

Conclusion: 确定了局部最优割问题在确定性模型中的轮复杂度为$log^{\Theta(1)} n$。

Abstract: In this work we present a fast distributed algorithm for local potential
problems: these are graph problems where the task is to find a locally optimal
solution where no node can unilaterally improve the utility in its local
neighborhood by changing its own label. A simple example of such a problem is
the task of finding a locally optimal cut, i.e., a cut where for each node at
least half of its incident edges are cut edges. The distributed round
complexity of locally optimal cut has been wide open; the problem is known to
require $\Omega(\log n)$ rounds in the deterministic LOCAL model and
$\Omega(\log \log n)$ rounds in the randomized LOCAL model, but the only known
upper bound is the trivial brute-force solution of $O(n)$ rounds. Locally
optimal cut in bounded-degree graphs is perhaps the simplest example of a
locally checkable labeling problem for which there is still such a large gap
between current upper and lower bounds. We show that in bounded-degree graphs,
all local potential problems, including locally optimal cut, can be solved in
$\log^{O(1)} n$ rounds, both in the deterministic and randomized LOCAL models.
In particular, the deterministic round complexity of the locally optimal cut
problem is now settled to $\log^{\Theta(1)} n$.

</details>


### [28] [Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso](https://arxiv.org/abs/2507.12106)
*Antonio Salis,Gabriele Troina,Gianluca Boanelli,Marco Ottaviano,Paola Fortini,Soraya Versace*

Main category: cs.DC

TL;DR: 论文介绍意大利Campobasso市智能绿色城市项目，通过新兴技术实现城市绿地可持续管理，提升环境韧性与市民生活质量。


<details>
  <summary>Details</summary>
Motivation: 高效设计和管理公共绿地对促进城市人口健康和福祉至关重要，项目旨在提供城市绿地可持续管理的创新模式。

Method: 集成物联网系统和数据驱动治理平台，利用决策支持系统实时监测，收集多源数据，结合机器学习算法构建预测模型。

Result: 构建基于云的平台支持实时决策，实现城市绿地智能控制和管理，优化公园灌溉，设置定制化警报。

Conclusion: 数字化、物联网传感器融合和技术创新可支持可持续城市治理，增强环境韧性并改善市民生活质量。

Abstract: The efficient design and management of public green spaces is a key factor in
promoting the health and well-being of urban population, as emphasized by the
WHO, UNEP, and EEA. These areas serve as the "green lungs" of the urban
ecosystem, playing a vital role in enhancing quality of life thanks to the
provision of ecosystem services. In this context, the Smart Green City use case
in Campobasso municipality, funded by the Italian Ministry of Enterprises
(MIMIT), emerges as an innovative model for the sustainable management of green
urban areas through the adoption of an advanced system of emerging technologies
integrated and interoperable. The project integrates IoT systems and
data-driven governance platforms, enabling real-time monitoring of the health
status of trees and green areas via a Decision Support System (DSS). It also
facilitates the collection and analysis of data from diverse sources, including
weather conditions, air quality, soil moisture, pollution levels. The resulting
cloud-based platform supports a holistic real time decision making for green
urban managers, technical experts and operational staff. It enables intelligent
control and management of urban green spaces using Tree Talker sensors,
integrated with soil moisture and water potential monitoring systems. Thanks to
predictive models based on machine learning algorithms and real time data
provided by IoT sensors, irrigation of public parks can be optimized by
providing suggestions on when and how much water to apply. Customized alerts
layers are also activated warning users when monitored parameters, such as soil
temperature, humidity, or water potential, exceed predefined thresholds. This
Use Case demonstrates how digitalization, IoT sensors fusion and technological
innovation can support sustainable urban governance, fostering environmental
resilience and improving citizens quality of life.

</details>


### [29] [Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage](https://arxiv.org/abs/2507.12205)
*Junqing Lin,Jingwei Sun,Mingge Lu,Guangzhong Sun*

Main category: cs.DC

TL;DR: 提出EC - SpMV加速稀疏大语言模型推理，评估显示有显著性能提升和存储开销降低。


<details>
  <summary>Details</summary>
Motivation: 现有SpMV内核和稀疏矩阵格式用于稀疏大语言模型推理时性能不佳且存储开销大，成为性能瓶颈。

Method: 引入分层块提取算法捕捉稀疏大语言模型内多种粒度的块结构，采用新型压缩稀疏格式EC - CSR通过增量索引减少存储开销并提高内存访问效率。

Result: 在LLaMA和OPT模型的真实稀疏权重矩阵上评估，EC - SpMV比现有SpMV库最高加速6.44倍，比CSR最多降低55.4%存储开销。

Conclusion: EC - SpMV能有效加速稀疏大语言模型推理，减少存储开销。

Abstract: Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance
bottleneck in the local deployment of sparse Large Language Models (LLMs),
where inference predominantly operates on workloads during the decoder phase
with a batch size of one. Existing SpMV kernels and sparse matrix formats,
originally designed for scientific computing, fail to exploit the unique
structure patterns inherent in sparse LLMs, resulting in suboptimal performance
and excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized
SpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a
hierarchical block extraction algorithm that captures multiple granularities of
block structures within sparse LLMs, and (2) a novel compressed sparse format
(EC-CSR) that employs delta indexing to reduce storage overhead and enhance
memory access efficiency. Evaluated on real sparse weight matrices from LLaMA
and OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV
libraries and reduces storage overhead by up to 55.4% compared to CSR.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [30] [Finite Pinwheel Scheduling: the k-Visits Problem](https://arxiv.org/abs/2507.11681)
*Sotiris Kanellopoulos,Christos Pergaminelis,Maria Kokkou,Euripides Markou,Aris Pagourtzis*

Main category: cs.DS

TL;DR: 引入 Pinwheel 调度问题有限版本 k - Visits，证明 2 - Visits 强 NP 完全，扩展至 k ≥ 2 情况，还给出特定条件下的线性时间算法等。


<details>
  <summary>Details</summary>
Motivation: 解决 Pinwheel 调度问题复杂度未明确的问题，研究其有限版本以推动复杂度确定。

Method: 通过从 Numerical 3 - Dimensional Matching 归约证明 2 - Visits 强 NP 完全；用图灵归约从 2 - Visits 到 Position Matching 得到特定条件下算法。

Result: 证明 2 - Visits 强 NP 完全，扩展至 k ≥ 2 情况；在所有截止时间不同时 2 - Visits 可线性时间求解；给出 FPT 算法和最多两个不同截止时间实例的线性时间算法。

Conclusion: 对 Pinwheel 调度问题复杂度研究取得进展，发现 2 - Visits 有趣的二分性质。

Abstract: Pinwheel Scheduling is a fundamental scheduling problem, in which each task
$i$ is associated with a positive integer $d_i$, and the objective is to
schedule one task per time slot, ensuring each task perpetually appears at
least once in every $d_i$ time slots. Although conjectured to be
PSPACE-complete, it remains open whether Pinwheel Scheduling is NP-hard (unless
a compact input encoding is used) or even contained in NP.
  We introduce k-Visits, a finite version of Pinwheel Scheduling, where given n
deadlines, the goal is to schedule each task exactly k times. While we observe
that the 1-Visit problem is trivial, we prove that 2-Visits is strongly
NP-complete through a surprising reduction from Numerical 3-Dimensional
Matching (N3DM). As intermediate steps in the reduction, we define NP-complete
variants of N3DM which may be of independent interest. We further extend our
strong NP-hardness result to a generalization of k-Visits $k\geq 2$ in which
the deadline of each task may vary throughout the schedule, as well as to a
similar generalization of Pinwheel Scheduling, thus making progress towards
settling the complexity of Pinwheel Scheduling.
  Additionally, we prove that 2-Visits can be solved in linear time if all
deadlines are distinct, rendering it one of the rare natural problems which
exhibit the interesting dichotomy of being in P if their input is a set and
NP-complete if the input is a multiset. We achieve this through a Turing
reduction from 2-Visits to a variation of N3DM, which we call Position
Matching. Based on this reduction, we also show an FPT algorithm for 2-Visits
parameterized by a value related to how close the input deadlines are to each
other, as well as a linear-time algorithm for instances with up to two distinct
deadlines.

</details>


### [31] [Approaching Optimality for Solving Dense Linear Systems with Low-Rank Structure](https://arxiv.org/abs/2507.11724)
*Michał Dereziński,Aaron Sidford*

Main category: cs.DS

TL;DR: 提出新的高精度随机算法解决线性系统和回归问题，给出运行时间，改进先前方法，还得到首个近线性时间计算任意稠密矩阵核范数乘法近似的算法。


<details>
  <summary>Details</summary>
Motivation: 为除k个大奇异值外条件良好的线性系统和回归问题提供新的高精度随机算法，改进先前方法的运行时间。

Method: 基于三个通用递归预条件框架，将矩阵草图和低秩更新公式根据问题结构进行调整。

Result: 求解d×d正定系统运行时间为O~(d^2 + k^ω)，求解回归问题运行时间为O~(nnz(A) + d^2 + k^ω)，近匹配自然复杂度极限，改进先前方法的权衡，得到首个近线性时间计算任意稠密矩阵核范数乘法近似的算法。

Conclusion: 新算法在解决特定线性系统和回归问题上有优势，能改进先前方法的复杂度。

Abstract: We provide new high-accuracy randomized algorithms for solving linear systems
and regression problems that are well-conditioned except for $k$ large singular
values. For solving such $d \times d$ positive definite system our algorithms
succeed whp. and run in time $\tilde O(d^2 + k^\omega)$. For solving such
regression problems in a matrix $\mathbf{A} \in \mathbb{R}^{n \times d}$ our
methods succeed whp. and run in time $\tilde O(\mathrm{nnz}(\mathbf{A}) + d^2 +
k^\omega)$ where $\omega$ is the matrix multiplication exponent and
$\mathrm{nnz}(\mathbf{A})$ is the number of non-zeros in $\mathbf{A}$. Our
methods nearly-match a natural complexity limit under dense inputs for these
problems and improve upon a trade-off in prior approaches that obtain running
times of either $\tilde O(d^{2.065}+k^\omega)$ or $\tilde O(d^2 +
dk^{\omega-1})$ for $d\times d$ systems. Moreover, we show how to obtain these
running times even under the weaker assumption that all but $k$ of the singular
values have a suitably bounded generalized mean. Consequently, we give the
first nearly-linear time algorithm for computing a multiplicative approximation
to the nuclear norm of an arbitrary dense matrix. Our algorithms are built on
three general recursive preconditioning frameworks, where matrix sketching and
low-rank update formulas are carefully tailored to the problems' structure.

</details>


### [32] [Pathfinding in Self-Deleting Graphs](https://arxiv.org/abs/2507.12047)
*Michal Dvořák,Dušan Knop,Michal Opler,Jan Pokorný,Ondřej Suchý,Krisztina Szilágyi*

Main category: cs.DS

TL;DR: 研究遍历依赖图上的路径查找问题，证明自删除s - t路径问题的复杂度及参数化特性。


<details>
  <summary>Details</summary>
Motivation: 研究遍历依赖图（如自删除图）上的路径查找问题，包括最短自删除s - t路径问题。

Method: 理论证明，分析自删除s - t路径问题在不同图结构和参数化条件下的复杂度。

Result: 证明自删除s - t路径问题在特定图条件下NP - 难，最短自删除s - t路径问题在某些参数化下W[1] - 完全，部分参数化下为FPT，且在特定参数化和图结构下无多项式核。

Conclusion: 明确了自删除s - t路径问题在不同图和参数条件下的复杂度特性。

Abstract: In this paper, we study the problem of pathfinding on traversal-dependent
graphs, i.e., graphs whose edges change depending on the previously visited
vertices. In particular, we study \emph{self-deleting graphs}, introduced by
Carmesin et al. (Sarah Carmesin, David Woller, David Parker, Miroslav Kulich,
and Masoumeh Mansouri. The Hamiltonian cycle and travelling salesperson
problems with traversal-dependent edge deletion. J. Comput. Sci.), which
consist of a graph $G=(V, E)$ and a function $f\colon V\rightarrow 2^E$, where
$f(v)$ is the set of edges that will be deleted after visiting the vertex $v$.
In the \textsc{(Shortest) Self-Deleting $s$-$t$-path} problem we are given a
self-deleting graph and its vertices $s$ and $t$, and we are asked to find a
(shortest) path from $s$ to $t$, such that it does not traverse an edge in
$f(v)$ after visiting $v$ for any vertex $v$.
  We prove that \textsc{Self-Deleting $s$-$t$-path} is NP-hard even if the
given graph is outerplanar, bipartite, has maximum degree $3$, bandwidth $2$
and $|f(v)|\leq 1$ for each vertex $v$. We show that \textsc{Shortest
Self-Deleting $s$-$t$-path} is W[1]-complete parameterized by the length of the
sought path and that \textsc{Self-Deleting $s$-$t$-path} is \W{1}-complete
parameterized by the vertex cover number, feedback vertex set number and
treedepth. We also show that the problem becomes FPT when we parameterize by
the maximum size of $f(v)$ and several structural parameters. Lastly, we show
that the problem does not admit a polynomial kernel even for parameterization
by the vertex cover number and the maximum size of $f(v)$ combined already on
2-outerplanar graphs.

</details>


### [33] [Weighted $k$-Server Admits an Exponentially Competitive Algorithm](https://arxiv.org/abs/2507.12130)
*Adithya Bijoy,Ankit Mondal,Ashish Chiplunkar*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The weighted $k$-server is a variant of the $k$-server problem, where the
cost of moving a server is the server's weight times the distance through which
it moves. The problem is famous for its intriguing properties and for evading
standard techniques for designing and analyzing online algorithms. Even on
uniform metric spaces with sufficiently many points, the deterministic
competitive ratio of weighted $k$-server is known to increase doubly
exponentially with respect to $k$, while the behavior of its randomized
competitive ratio is not fully understood. Specifically, no upper bound better
than doubly exponential is known, while the best known lower bound is singly
exponential in $k$. In this paper, we close the exponential gap between these
bounds by giving an $\exp(O(k^2))$-competitive randomized online algorithm for
the weighted $k$-server problem on uniform metrics, thus breaking the doubly
exponential barrier for deterministic algorithms for the first time. This is
achieved by a recursively defined notion of a phase which, on the one hand,
forces a lower bound on the cost of any offline solution, while, on the other
hand, also admits a randomized online algorithm with bounded expected cost. The
algorithm is also recursive; it involves running several algorithms virtually
and in parallel and following the decisions of one of them in a random order.
We also show that our techniques can be lifted to construct an
$\exp(O(k^2))$-competitive randomized online algorithm for the generalized
$k$-server problem on weighted uniform metrics.

</details>


### [34] [A near-complete resolution of the exponential-time complexity of k-opt for the traveling salesman problem](https://arxiv.org/abs/2507.12304)
*Sophia Heimann,Hung P. Hoang,Stefan Hougardy*

Main category: cs.DS

TL;DR: 本文证明k=3和k=4时k - opt算法在最优枢轴规则下可能需指数次迭代，结合已有结果给出k≥3时迭代次数的完整答案，还为2.5 - opt算法建立指数下界，结果适用于一般和度量旅行商问题。


<details>
  <summary>Details</summary>
Motivation: 长期以来，小k值下k - opt算法在最优枢轴规则下所需迭代次数是开放问题，本文旨在解决该问题。

Method: 对k = 3和k = 4情况进行证明，结合Heimann等人的近期结果，并为2.5 - opt算法建立类似下界。

Result: 证明k = 3和k = 4时k - opt算法在最优枢轴规则下可能需指数次迭代，为2.5 - opt算法建立指数下界。

Conclusion: 为k≥3时k - opt算法在最优枢轴规则下所需迭代次数提供完整答案，结果适用于一般和度量旅行商问题。

Abstract: The $k$-opt algorithm is one of the simplest and most widely used heuristics
for solving the traveling salesman problem. Starting from an arbitrary tour,
the $k$-opt algorithm improves the current tour in each iteration by exchanging
up to $k$ edges. The algorithm continues until no further improvement of this
kind is possible. For a long time, it remained an open question how many
iterations the $k$-opt algorithm might require for small values of $k$,
assuming the use of an optimal pivot rule. In this paper, we resolve this
question for the cases $k = 3$ and $k = 4$ by proving that in both these cases
an exponential number of iterations may be needed even if an optimal pivot rule
is used. Combined with a recent result from Heimann, Hoang, and Hougardy (ICALP
2024), this provides a complete answer for all $k \geq 3$ regarding the number
of iterations the $k$-opt algorithm may require under an optimal pivot rule. In
addition we establish an analogous exponential lower bound for the 2.5-opt
algorithm, a variant that generalizes 2-opt and is a restricted version of
3-opt. All our results hold for both the general and the metric traveling
salesman problem.

</details>


### [35] [Online Block Packing](https://arxiv.org/abs/2507.12357)
*Ariel Ben Eliezer,Noam Nisan*

Main category: cs.DS

TL;DR: 为有多维块约束且服务准耐心投标人的区块链算法挑战提供在线近似算法，解决前人遗留开放问题。


<details>
  <summary>Details</summary>
Motivation: 解决有多维块约束且服务准耐心投标人的区块链所面临的算法挑战，处理前人遗留的开放问题。

Method: 提出在线近似算法。

Result: 得到适用于该问题的在线近似算法。

Conclusion: 成功解决了[Babaioff and Nisan, EC 2025]留下的开放问题。

Abstract: We consider the algorithmic challenge that is faced by blockchains that have
multidimensional block constraints and serve quasi-patient bidders. We provide
online approximation algorithms for this problem, thus solving open problems
left by [Babaioff and Nisan, EC 2025].

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [36] [New allocation rule based on graph structures and their application to economic phenomena](https://arxiv.org/abs/2507.11808)
*Taiki Yamada,Taisuke Matsubae,Tomoya Akamatsu*

Main category: cs.GT

TL;DR: 提出基于边的Shapley值分配规则，适用于网络系统，以边为基础评估贡献，通过两个用例验证其适用性。


<details>
  <summary>Details</summary>
Motivation: 传统分配规则难以充分捕捉边在网络系统中的作用，而边是某些系统价值产生的主要驱动因素，需新规则。

Method: 将特征函数从节点集转移到边集，建立理论基础，展示与经典分配规则关系。

Result: 在内容平台网络和供应链物流两个用例中，方法产生直观且结构一致的分配。

Conclusion: 该框架为复杂交互结构的合作场景提供价值分配新视角和实用工具。

Abstract: This study introduces the \emph{edge-based Shapley value}, a novel allocation
rule within cooperative game theory, specifically tailored for networked
systems, where value is generated through interactions represented by edges.
Traditional allocation rules, such as the Shapley and Myerson values, evaluate
player contributions based on node-level characteristics, or connected
components. However, these approaches often fail to adequately capture the
functional role of edges, which are crucial in systems such as supply chains
and digital platforms, where interactions, rather than individual agents, are
the primary drivers of value. Our edge-based Shapley value shifts the
characteristic function from node sets to edge sets, thereby enabling a more
granular and context-sensitive evaluation of the contributions. We establish
its theoretical foundations, demonstrate its relationship to classical
allocation rules, and show that it retains key properties such as fairness and
symmetry. To illustrate its applicability, we present two use cases: content
platform networks and supply chain logistics (SCL). In both cases, our method
produces intuitive and structurally consistent allocations, particularly in
scenarios with overlapping routes, exclusive contracts or cost-sensitive paths.
This framework offers a new perspective on value attribution in cooperative
settings with complex interaction structures and provides practical tools for
analyzing real-world economic and logistical networks.

</details>


### [37] [Coalitions on the Fly in Cooperative Games](https://arxiv.org/abs/2507.11883)
*Yao Zhang,Indrajit Saha,Zhaohong Sun,Makoto Yokoo*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we examine a sequential setting of a cooperative game in which
players arrive dynamically to form coalitions and complete tasks either
together or individually, depending on the value created. Upon arrival, a new
player as a decision maker faces two options: forming a new coalition or
joining an existing one. We assume that players are greedy, i.e., they aim to
maximize their rewards based on the information available at their arrival. The
objective is to design an online value distribution policy that incentivizes
players to form a coalition structure that maximizes social welfare. We focus
on monotone and bounded cooperative games. Our main result establishes an upper
bound of $\frac{3\mathsf{min}}{\mathsf{max}}$ on the competitive ratio for any
irrevocable policy (i.e., one without redistribution), and proposes a policy
that achieves a near-optimal competitive ratio of $\min\left\{\frac{1}{2},
\frac{3\mathsf{min}}{\mathsf{max}}\right\}$, where $\mathsf{min}$ and
$\mathsf{max}$ denote the smallest and largest marginal contribution of any
sub-coalition of players respectively. Finally, we also consider
non-irrevocable policies, with alternative bounds only when the number of
players is limited.

</details>


### [38] [Contracting with a Mechanism Designer](https://arxiv.org/abs/2507.12054)
*Tian Bai,Yiding Feng,Yaohao Liu,Mengfan Ma,Mingyu Xiao*

Main category: cs.GT

TL;DR: 本文研究现代众包市场的经济互动，提出三方模型并以Stackelberg博弈分析，有四方面贡献，包括刻画均衡、引入衡量指标及拓展结果等。


<details>
  <summary>Details</summary>
Motivation: 探索现代众包市场中雇主、平台和工人之间的经济互动关系。

Method: 引入三方模型，将其分析为扩展形式的Stackelberg博弈。

Result: 刻画子博弈完美均衡，引入价格指标并推导其边界，在匿名定价机制和信息不精确场景下有相关结论。

Conclusion: 模型能有效分析众包市场经济互动，相关指标及结论在不同场景有参考价值。

Abstract: This paper explores the economic interactions within modern crowdsourcing
markets. In these markets, employers issue requests for tasks, platforms
facilitate the recruitment of crowd workers, and workers complete tasks for
monetary rewards. Recognizing that these roles serve distinct functions within
the ecosystem, we introduce a three-party model that distinguishes among the
principal (the requester), the intermediary (the platform), and the pool of
agents (the workers). The principal, unable to directly engage with agents,
relies on the intermediary to recruit and incentivize them. This interaction
unfolds in two stages: first, the principal designs a profit-sharing contract
with the intermediary; second, the intermediary implements a mechanism to
select an agent to complete the delegated task.
  We analyze the proposed model as an extensive-form Stackelberg game. Our
contributions are fourfold: (1) We fully characterize the subgame perfect
equilibrium. In particular, we reduce the principal's contract design problem
to a novel auction-theoretic formulation we term virtual value pricing, and
reveals that linear contracts are optimal even when the task have multiple
outcomes and agents' cost distributions are asymmetric. (2) To quantify the
principal's utility loss from delegation and information asymmetry, we
introduce the price of double marginalization (PoDM) and the classical price of
anarchy (PoA), and derive tight or nearly tight bounds on both ratios under
regular and monotone hazard rate (MHR) distributions. (3) We further examine
these two ratios in a natural setting where the intermediary is restricted to
anonymous pricing mechanisms, and show that similar qualitative insights
continue to hold. (4) Finally, we extend our results on both ratios to a robust
framework that accommodates scenarios in which the principal lacks precise
information about the market size.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [39] [Similarity-Guided Diffusion for Contrastive Sequential Recommendation](https://arxiv.org/abs/2507.11866)
*Jinkyeong Choi,Yejin Noh,Donghyeon Park*

Main category: cs.IR

TL;DR: 提出用于对比序列推荐的相似性引导扩散方法，在五个基准数据集上优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有使用扩散模型的数据增强和对比学习方法多采用随机增强，有破坏原序列上下文信息的风险。

Method: 利用物品嵌入向量间的相似性生成语义一致的噪声，在去噪过程中利用高置信度分数选择增强位置。

Result: 在五个基准数据集上实验，SimDiffRec 优于现有基线模型。

Conclusion: 提出的增强技术能提供更具区分性的正负样本，同时提高训练效率和推荐性能。

Abstract: In sequential recommendation systems, data augmentation and contrastive
learning techniques have recently been introduced using diffusion models to
achieve robust representation learning. However, most of the existing
approaches use random augmentation, which risk damaging the contextual
information of the original sequence. Accordingly, we propose a
Similarity-Guided Diffusion for Contrastive Sequential Recommendation. Our
method leverages the similarity between item embedding vectors to generate
semantically consistent noise. Moreover, we utilize high confidence score in
the denoising process to select our augmentation positions. This approach more
effectively reflects contextual and structural information compared to
augmentation at random positions. From a contrastive learning perspective, the
proposed augmentation technique provides more discriminative positive and
negative samples, simultaneously improving training efficiency and
recommendation performance. Experimental results on five benchmark datasets
show that SimDiffRec outperforms the existing baseline models.

</details>


### [40] [Context-Aware Search and Retrieval Over Erasure Channels](https://arxiv.org/abs/2507.11894)
*Sara Ghasvarianjahromi,Yauhen Yakimenka,Jörg Kliewer*

Main category: cs.IR

TL;DR: 本文引入并分析一种搜索与检索模型，对远程文档检索系统做信息论分析，推导检索错误概率表达式，数值模拟验证分析并表明语义感知特征编码有效。


<details>
  <summary>Details</summary>
Motivation: 采用检索增强生成的关键语义通信原则，对远程文档检索系统进行信息论分析。

Method: 用依赖于术语上下文重要性的自适应速率重复码对查询特征向量编码，在解码器根据恢复查询的上下文接近度选择文档，利用联合高斯近似推导检索错误概率。

Result: 数值模拟验证分析的有效性，表明给关键特征分配更多冗余能降低错误率。

Conclusion: 语义感知特征编码在易出错通信环境中有效。

Abstract: This paper introduces and analyzes a search and retrieval model that adopts
key semantic communication principles from retrieval-augmented generation. We
specifically present an information-theoretic analysis of a remote document
retrieval system operating over a symbol erasure channel. The proposed model
encodes the feature vector of a query, derived from term-frequency weights of a
language corpus by using a repetition code with an adaptive rate dependent on
the contextual importance of the terms. At the decoder, we select between two
documents based on the contextual closeness of the recovered query. By
leveraging a jointly Gaussian approximation for both the true and reconstructed
similarity scores, we derive an explicit expression for the retrieval error
probability, i.e., the probability under which the less similar document is
selected. Numerical simulations on synthetic and real-world data (Google NQ)
confirm the validity of the analysis. They further demonstrate that assigning
greater redundancy to critical features effectively reduces the error rate,
highlighting the effectiveness of semantic-aware feature encoding in
error-prone communication settings.

</details>


### [41] [Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control](https://arxiv.org/abs/2507.12202)
*Anton Klenitskiy,Konstantin Polev,Daria Denisova,Alexey Vasilev,Dmitry Simakov,Gleb Gusev*

Main category: cs.IR

TL;DR: 本文将稀疏自编码器（SAE）应用于序列推荐领域，展示其可成功应用于相关任务的Transformer模型，学习到的方向更具可解释性，且能有效灵活控制模型行为。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer架构的序列推荐模型为黑盒模型，解释其内部机制对理解、影响和控制模型行为很重要，而SAE在提取语言模型可解释特征方面有潜力，因此想将其应用于序列推荐领域。

Method: 将SAE应用于在序列推荐任务上训练的Transformer模型，学习用激活空间中方向的稀疏线性组合来重构Transformer内部层的隐藏状态。

Result: 学习到的方向比原始隐藏状态维度更具可解释性和单语义性，SAE学习的特征能有效灵活控制模型行为。

Conclusion: SAE可成功应用于序列推荐领域的Transformer模型，能为终端用户提供调整推荐以适应不同场景和上下文的直接方法。

Abstract: Many current state-of-the-art models for sequential recommendations are based
on transformer architectures. Interpretation and explanation of such black box
models is an important research question, as a better understanding of their
internals can help understand, influence, and control their behavior, which is
very important in a variety of real-world applications. Recently sparse
autoencoders (SAE) have been shown to be a promising unsupervised approach for
extracting interpretable features from language models. These autoencoders
learn to reconstruct hidden states of the transformer's internal layers from
sparse linear combinations of directions in their activation space.
  This paper is focused on the application of SAE to the sequential
recommendation domain. We show that this approach can be successfully applied
to the transformer trained on a sequential recommendation task: learned
directions turn out to be more interpretable and monosemantic than the original
hidden state dimensions. Moreover, we demonstrate that the features learned by
SAE can be used to effectively and flexibly control the model's behavior,
providing end-users with a straightforward method to adjust their
recommendations to different custom scenarios and contexts.

</details>


### [42] [Looking for Fairness in Recommender Systems](https://arxiv.org/abs/2507.12242)
*Cécile Logé*

Main category: cs.IR

TL;DR: 文章探讨社交媒体推荐系统避免过滤气泡问题，提出定义多样性指标并融入评估框架以平衡个性化推荐与社会多元化目标。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统易产生过滤气泡，对用户、内容创作者和社会有负面影响，需解决该问题。

Method: 定义一个或多个代表多样性的性能指标，从公平角度调整推荐系统性能，并将该指标融入评估框架。

Result: 未提及具体结果。

Conclusion: 通过上述方法可平衡个性化推荐和促进丰富多样文化及观点的社会目标。

Abstract: Recommender systems can be found everywhere today, shaping our everyday
experience whenever we're consuming content, ordering food, buying groceries
online, or even just reading the news. Let's imagine we're in the process of
building a recommender system to make content suggestions to users on social
media. When thinking about fairness, it becomes clear there are several
perspectives to consider: the users asking for tailored suggestions, the
content creators hoping for some limelight, and society at large, navigating
the repercussions of algorithmic recommendations. A shared fairness concern
across all three is the emergence of filter bubbles, a side-effect that takes
place when recommender systems are almost "too good", making recommendations so
tailored that users become inadvertently confined to a narrow set of
opinions/themes and isolated from alternative ideas. From the user's
perspective, this is akin to manipulation. From the small content creator's
perspective, this is an obstacle preventing them access to a whole range of
potential fans. From society's perspective, the potential consequences are
far-reaching, influencing collective opinions, social behavior and political
decisions. How can our recommender system be fine-tuned to avoid the creation
of filter bubbles, and ensure a more inclusive and diverse content landscape?
Approaching this problem involves defining one (or more) performance metric to
represent diversity, and tweaking our recommender system's performance through
the lens of fairness. By incorporating this metric into our evaluation
framework, we aim to strike a balance between personalized recommendations and
the broader societal goal of fostering rich and varied cultures and points of
view.

</details>


### [43] [An Ecosystem for Ontology Interoperability](https://arxiv.org/abs/2507.12311)
*Zhangcheng Qiang*

Main category: cs.IR

TL;DR: 提出用于本体互操作性的生态系统，用案例验证其有用性


<details>
  <summary>Details</summary>
Motivation: 本体互操作性是限制知识图谱中本体使用的复杂问题，不同本体概念冲突重叠，难以设计开发部署互操作本体

Method: 在本体工程生命周期不同阶段采用三种语义技术，设计阶段用本体设计模式，开发阶段用本体匹配和版本控制，部署阶段用符合本体的知识图谱

Result: 通过建筑领域的案例研究，验证了所提出生态系统的有用性

Conclusion: 所提出的生态系统能在实际应用中实现更好的本体互操作性

Abstract: Ontology interoperability is one of the complicated issues that restricts the
use of ontologies in knowledge graphs (KGs). Different ontologies with
conflicting and overlapping concepts make it difficult to design, develop, and
deploy an interoperable ontology for downstream tasks. We propose an ecosystem
for ontology interoperability. The ecosystem employs three state-of-the-art
semantic techniques in different phases of the ontology engineering life cycle:
ontology design patterns (ODPs) in the design phase, ontology matching and
versioning (OM\&OV) in the develop phase, and ontology-compliant knowledge
graphs (OCKGs) in the deploy phase, to achieve better ontology interoperability
in real-world applications. A case study in the building domain validates the
usefulness of the proposed ecosystem.

</details>


### [44] [Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker](https://arxiv.org/abs/2507.12378)
*Rachna Saxena,Abhijeet Kumar,Suresh Shanmugam*

Main category: cs.IR

TL;DR: 传统信息提取系统和多模态大语言模型面临挑战，本文提出实用方法使视觉检索可扩展高效，实验表明该设计可用于企业生产系统。


<details>
  <summary>Details</summary>
Motivation: 传统信息提取系统未考虑信息图表，多模态大语言模型有检索难题，晚交互机制用于基于RAG的多模态问答存在挑战，需使视觉检索过程可扩展高效。

Method: 提出多步自定义实现，利用混合搜索和晚交互重排器检索匹配页面，让多模态大语言模型从页面生成答案。

Result: 实验发现提出的设计可扩展（显著加速）且稳定（不降低性能质量）。

Conclusion: 该设计可作为企业的生产系统使用。

Abstract: Traditional information extraction systems face challenges with text only
language models as it does not consider infographics (visual elements of
information) such as tables, charts, images etc. often used to convey complex
information to readers. Multimodal LLM (MLLM) face challenges of finding needle
in the haystack problem i.e., either longer context length or substantial
number of documents as search space. Late interaction mechanism over visual
language models has shown state of the art performance in retrieval-based
vision augmented Q&A tasks. There are yet few challenges using it for RAG based
multi-modal Q&A. Firstly, many popular and widely adopted vector databases do
not support native multi-vector retrieval. Secondly, late interaction requires
computation which inflates space footprint and can hinder enterprise adoption.
Lastly, the current state of late interaction mechanism does not leverage the
approximate neighbor search indexing methods for large speed ups in retrieval
process. This paper explores a pragmatic approach to make vision retrieval
process scalable and efficient without compromising on performance quality. We
propose multi-step custom implementation utilizing widely adopted hybrid search
(metadata & embedding) and state of the art late interaction re-ranker to
retrieve best matching pages. Finally, MLLM are prompted as reader to generate
answers from contextualized best matching pages. Through experiments, we
observe that the proposed design is scalable (significant speed up) and stable
(without degrading performance quality), hence can be used as production
systems at enterprises.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming](https://arxiv.org/abs/2507.11547)
*Yingxue Zhao,Qianyi Chen,Haoran Li,Haosu Zhou,Hamid Reza Attar,Tobias Pfaff,Tailin Wu,Nan Li*

Main category: cs.LG

TL;DR: 本文开发了名为RUGNN的图神经网络替代模型，用于板材成型过程中变形场的准确预测，经案例验证效果良好，是支持板材成型设计的可靠方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于AI的替代模型在捕捉复杂3D空间关系和排列不变性操作方面存在局限，需要开发新的模型。

Method: 开发RUGNN模型，结合GRUs处理时间动态，采用受U - Net启发的图下采样/上采样机制处理空间长距离依赖，提出“节点到表面”接触表示方法。

Result: RUGNN模型在冷成型和热成型案例中提供了与有限元模拟结果相近的准确变形预测，优于多个基线GNN架构。

Conclusion: RUGNN是支持板材成型设计、实现准确可制造性预测的可靠方法。

Abstract: In recent years, various artificial intelligence-based surrogate models have
been proposed to provide rapid manufacturability predictions of material
forming processes. However, traditional AI-based surrogate models, typically
built with scalar or image-based neural networks, are limited in their ability
to capture complex 3D spatial relationships and to operate in a
permutation-invariant manner. To overcome these issues, emerging graph-based
surrogate models are developed using graph neural networks. This study
developed a new graph neural network surrogate model named Recurrent U
Net-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate
predictions of sheet material deformation fields across multiple forming
timesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model
temporal dynamics and a U-Net inspired graph-based downsample/upsample
mechanism to handle spatial long-range dependencies. A novel 'node-to-surface'
contact representation method was proposed, offering significant improvements
in computational efficiency for large-scale contact interactions. The RUGNN
model was validated using a cold forming case study and a more complex hot
forming case study using aluminium alloys. Results demonstrate that the RUGNN
model provides accurate deformation predictions closely matching ground truth
FE simulations and outperforming several baseline GNN architectures. Model
tuning was also performed to identify suitable hyperparameters, training
strategies, and input feature representations. These results demonstrate that
RUGNN is a reliable approach to support sheet material forming design by
enabling accurate manufacturability predictions.

</details>


### [46] [SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery](https://arxiv.org/abs/2507.11570)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.LG

TL;DR: 本文开发SurgeryLSTM模型预测择期脊柱手术住院时长，效果优于传统模型，具高准确性和可解释性，支持将模型集成到临床决策系统。


<details>
  <summary>Details</summary>
Motivation: 开发并评估机器学习模型预测择期脊柱手术住院时长，关注时间建模和模型可解释性的优势。

Method: 对比传统机器学习模型和SurgeryLSTM模型，用决定系数评估性能，用可解释AI识别关键预测因子。

Result: SurgeryLSTM预测准确率最高，注意力机制提升可解释性，识别出骨疾病、慢性肾病和腰椎融合是住院时长的关键预测因子。

Conclusion: SurgeryLSTM是有效的可解释AI解决方案，支持将时间可解释的ML方法集成到临床决策支持系统。

Abstract: Objective: To develop and evaluate machine learning (ML) models for
predicting length of stay (LOS) in elective spine surgery, with a focus on the
benefits of temporal modeling and model interpretability. Materials and
Methods: We compared traditional ML models (e.g., linear regression, random
forest, support vector machine (SVM), and XGBoost) with our developed model,
SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an
attention, using structured perioperative electronic health records (EHR) data.
Performance was evaluated using the coefficient of determination (R2), and key
predictors were identified using explainable AI. Results: SurgeryLSTM achieved
the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)
and baseline models. The attention mechanism improved interpretability by
dynamically identifying influential temporal segments within preoperative
clinical sequences, allowing clinicians to trace which events or features most
contributed to each LOS prediction. Key predictors of LOS included bone
disorder, chronic kidney disease, and lumbar fusion identified as the most
impactful predictors of LOS. Discussion: Temporal modeling with attention
mechanisms significantly improves LOS prediction by capturing the sequential
nature of patient data. Unlike static models, SurgeryLSTM provides both higher
accuracy and greater interpretability, which are critical for clinical
adoption. These results highlight the potential of integrating attention-based
temporal models into hospital planning workflows. Conclusion: SurgeryLSTM
presents an effective and interpretable AI solution for LOS prediction in
elective spine surgery. Our findings support the integration of temporal,
explainable ML approaches into clinical decision support systems to enhance
discharge readiness and individualized patient care.

</details>


### [47] [Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators](https://arxiv.org/abs/2507.11574)
*Kazuma Kobayashi,Shailesh Garg,Farid Ahmed,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: 提出Conformalized Monte Carlo Operator (CMCO)框架，为神经算子虚拟传感提供校准、无分布预测区间，在多应用中实现良好经验覆盖，为科学机器学习奠定新基础。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习实时虚拟传感中鲁棒不确定性量化（UQ）难题，尤其是在高风险领域。

Method: 在单一DeepONet架构中将蒙特卡罗丢弃法与分裂共形预测相结合，提出CMCO框架。

Result: 在湍流、弹塑性变形和全球宇宙辐射剂量估计三个应用中，CMCO始终实现接近标称的经验覆盖。

Conclusion: CMCO为神经算子提供通用、即插即用的UQ解决方案，以最小计算开销连接理论与应用，为科学机器学习奠定新基础。

Abstract: Robust uncertainty quantification (UQ) remains a critical barrier to the safe
deployment of deep learning in real-time virtual sensing, particularly in
high-stakes domains where sparse, noisy, or non-collocated sensor data are the
norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework
that transforms neural operator-based virtual sensing with calibrated,
distribution-free prediction intervals. By unifying Monte Carlo dropout with
split conformal prediction in a single DeepONet architecture, CMCO achieves
spatially resolved uncertainty estimates without retraining, ensembling, or
custom loss design. Our method addresses a longstanding challenge: how to endow
operator learning with efficient and reliable UQ across heterogeneous domains.
Through rigorous evaluation on three distinct applications: turbulent flow,
elastoplastic deformation, and global cosmic radiation dose estimation-CMCO
consistently attains near-nominal empirical coverage, even in settings with
strong spatial gradients and proxy-based sensing. This breakthrough offers a
general-purpose, plug-and-play UQ solution for neural operators, unlocking
real-time, trustworthy inference in digital twins, sensor fusion, and
safety-critical monitoring. By bridging theory and deployment with minimal
computational overhead, CMCO establishes a new foundation for scalable,
generalizable, and uncertainty-aware scientific machine learning.

</details>


### [48] [Sparse Identification of Nonlinear Dynamics with Conformal Prediction](https://arxiv.org/abs/2507.11739)
*Urban Fasel*

Main category: cs.LG

TL;DR: 本文探索将共形预测与集成稀疏识别非线性动力学（E - SINDy）结合，介绍三种应用并在多个系统上验证，表明该结合方法有更好效果。


<details>
  <summary>Details</summary>
Motivation: 量化SINDy模型的不确定性对评估其可靠性至关重要，尤其是在安全关键应用中。

Method: 将共形预测与E - SINDy结合，介绍了时间序列预测不确定性量化、基于库特征重要性的模型选择、特征共形预测量化模型系数不确定性三种应用。

Result: 在随机捕食者 - 猎物动力学和几个混沌动力系统上验证，结合方法能可靠实现时间序列预测的目标覆盖率，有效量化特征重要性，在非高斯噪声下为模型系数生成更稳健的不确定性区间。

Conclusion: 共形预测方法与E - SINDy结合比标准E - SINDy系数估计有更好的效果。

Abstract: The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for
discovering nonlinear dynamical system models from data. Quantifying
uncertainty in SINDy models is essential for assessing their reliability,
particularly in safety-critical applications. While various uncertainty
quantification methods exist for SINDy, including Bayesian and ensemble
approaches, this work explores the integration of Conformal Prediction, a
framework that can provide valid prediction intervals with coverage guarantees
based on minimal assumptions like data exchangeability. We introduce three
applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)
quantifying uncertainty in time series prediction, (2) model selection based on
library feature importance, and (3) quantifying the uncertainty of identified
model coefficients using feature conformal prediction. We demonstrate the three
applications on stochastic predator-prey dynamics and several chaotic dynamical
systems. We show that conformal prediction methods integrated with E-SINDy can
reliably achieve desired target coverage for time series forecasting,
effectively quantify feature importance, and produce more robust uncertainty
intervals for model coefficients, even under non-Gaussian noise, compared to
standard E-SINDy coefficient estimates.

</details>


### [49] [Einstein Fields: A Neural Perspective To Computational General Relativity](https://arxiv.org/abs/2507.11589)
*Sandeep Suresh Cranganore,Andrei Bodnar,Arturs Berzins,Johannes Brandstetter*

Main category: cs.LG

TL;DR: 提出Einstein Fields将计算密集的四维数值相对论模拟压缩为紧凑的隐式神经网络权重，有诸多优势并开源代码。


<details>
  <summary>Details</summary>
Motivation: 压缩计算密集的四维数值相对论模拟，探索更具扩展性和表现力的数值相对论方法。

Method: 设计Einstein Fields对广义相对论的核心张量场——度规进行建模，通过自动微分推导物理量。

Result: Einstein Fields在连续4D时空建模、无网格性、存储效率、导数精度和易用性方面表现出显著潜力。

Conclusion: 在多个广义相对论的典型测试平台上验证Einstein Fields的有效性，并开源基于JAX的库，为数值相对论开辟新途径。

Abstract: We introduce Einstein Fields, a neural representation that is designed to
compress computationally intensive four-dimensional numerical relativity
simulations into compact implicit neural network weights. By modeling the
\emph{metric}, which is the core tensor field of general relativity, Einstein
Fields enable the derivation of physical quantities via automatic
differentiation. However, unlike conventional neural fields (e.g., signed
distance, occupancy, or radiance fields), Einstein Fields are \emph{Neural
Tensor Fields} with the key difference that when encoding the spacetime
geometry of general relativity into neural field representations, dynamics
emerge naturally as a byproduct. Einstein Fields show remarkable potential,
including continuum modeling of 4D spacetime, mesh-agnosticity, storage
efficiency, derivative accuracy, and ease of use. We address these challenges
across several canonical test beds of general relativity and release an open
source JAX-based library, paving the way for more scalable and expressive
approaches to numerical relativity. Code is made available at
https://github.com/AndreiB137/EinFields

</details>


### [50] [Thought Purity: Defense Paradigm For Chain-of-Thought Attack](https://arxiv.org/abs/2507.12314)
*Zihao Xue,Zhen Bi,Long Ma,Zhenlin Hu,Yan Wang,Zhenfang Liu,Qing Sheng,Jie Xiao,Jungang Lou*

Main category: cs.LG

TL;DR: 强化学习训练的大推理模型有推理能力但易受攻击，提出Thought Purity防御范式增强安全性。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练的大推理模型在思维链生成过程中易受攻击，存在安全与性能方面的漏洞，需要解决。

Method: 提出Thought Purity防御范式，包含安全优化的数据处理流程、强化学习增强的规则约束和自适应监控指标三个协同组件。

Result: 建立了针对思维链攻击漏洞的首个全面防御机制。

Conclusion: 该方法显著推进了下一代人工智能架构的安全 - 功能平衡。

Abstract: While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,
Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large
Language Models (LLMs) domain, their susceptibility to security threats remains
a critical vulnerability. This weakness is particularly evident in
Chain-of-Thought (CoT) generation processes, where adversarial methods like
backdoor prompt attacks can systematically subvert the model's core reasoning
mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this
vulnerability through exploiting prompt controllability, simultaneously
degrading both CoT safety and task performance with low-cost interventions. To
address this compounded security-performance vulnerability, we propose Thought
Purity (TP): a defense paradigm that systematically strengthens resistance to
malicious content while preserving operational efficacy. Our solution achieves
this through three synergistic components: (1) a safety-optimized data
processing pipeline (2) reinforcement learning-enhanced rule constraints (3)
adaptive monitoring metrics. Our approach establishes the first comprehensive
defense mechanism against CoTA vulnerabilities in reinforcement
learning-aligned reasoning systems, significantly advancing the
security-functionality equilibrium for next-generation AI architectures.

</details>


### [51] [Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques](https://arxiv.org/abs/2507.11590)
*Raju Challagundla,Mohsen Dorodchi,Pu Wang,Minwoo Lee*

Main category: cs.LG

TL;DR: 本文综述合成表格数据生成的进展，引入新分类法，提出基准框架，为研究和应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 隐私法规趋严，真实数据获取受限，合成表格数据生成成为重要解决方案。

Method: 对合成表格数据生成的近期进展进行全面且有针对性的综述，引入基于实际生成目标的新分类法，提出基准框架。

Result: 完成对合成表格数据生成进展的综述，引入新分类法并提出基准框架。

Conclusion: 该工作为未来研究提供路线图，指导在隐私关键环境中应用合成表格数据。

Abstract: As privacy regulations become more stringent and access to real-world data
becomes increasingly constrained, synthetic data generation has emerged as a
vital solution, especially for tabular datasets, which are central to domains
like finance, healthcare and the social sciences. This survey presents a
comprehensive and focused review of recent advances in synthetic tabular data
generation, emphasizing methods that preserve complex feature relationships,
maintain statistical fidelity, and satisfy privacy requirements. A key
contribution of this work is the introduction of a novel taxonomy based on
practical generation objectives, including intended downstream applications,
privacy guarantees, and data utility, directly informing methodological design
and evaluation strategies. Therefore, this review prioritizes the actionable
goals that drive synthetic data creation, including conditional generation and
risk-sensitive modeling. Additionally, the survey proposes a benchmark
framework to align technical innovation with real-world demands. By bridging
theoretical foundations with practical deployment, this work serves as both a
roadmap for future research and a guide for implementing synthetic tabular data
in privacy-critical environments.

</details>


### [52] [Measuring Informativeness Gap of (Mis)Calibrated Predictors](https://arxiv.org/abs/2507.12094)
*Yiding Feng,Wei Tang*

Main category: cs.LG

TL;DR: 本文引入信息差距概念来衡量预测模型在决策任务中的有用性，给出对偶表征和信息度量，该度量满足理想特性且可高效估计。


<details>
  <summary>Details</summary>
Motivation: 解决在多个可能校准错误的预测模型中，确定哪个模型在下游决策任务中更有用的问题。

Method: 引入信息差距概念，给出其对偶表征，提出类似地球移动距离的信息度量。

Result: 信息差距框架推广了现有概念，信息度量满足自然期望特性，可在仅预测访问设置下高效估计，还获得了校准良好预测器的组合结构结果。

Conclusion: 所提出的信息差距和度量可有效评估预测模型在决策任务中的有用性。

Abstract: In many applications, decision-makers must choose between multiple predictive
models that may all be miscalibrated. Which model (i.e., predictor) is more
"useful" in downstream decision tasks? To answer this, our first contribution
introduces the notion of the informativeness gap between any two predictors,
defined as the maximum normalized payoff advantage one predictor offers over
the other across all decision-making tasks. Our framework strictly generalizes
several existing notions: it subsumes U-Calibration [KLST-23] and Calibration
Decision Loss [HW-24], which compare a miscalibrated predictor to its
calibrated counterpart, and it recovers Blackwell informativeness [Bla-51,
Bla-53] as a special case when both predictors are perfectly calibrated. Our
second contribution is a dual characterization of the informativeness gap,
which gives rise to a natural informativeness measure that can be viewed as a
relaxed variant of the earth mover's distance (EMD) between two prediction
distributions. We show that this measure satisfies natural desiderata: it is
complete and sound, and it can be estimated sample-efficiently in the
prediction-only access setting. Along the way, we also obtain novel
combinatorial structural results when applying this measure to perfectly
calibrated predictors.

</details>


### [53] [Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification](https://arxiv.org/abs/2507.11620)
*Steven Dillmann,Juan Rafael Martínez-Galarza*

Main category: cs.LG

TL;DR: 提出事件时间序列的新颖张量表示及稀疏自编码器，在X射线天文学数据集验证，可用于多下游任务，为复杂事件时间序列分析提供通用方案。


<details>
  <summary>Details</summary>
Motivation: 事件时间序列非结构化和不规则结构，传统技术难以提取模式和识别现象。

Method: 提出新颖的二维和三维张量表示，结合稀疏自编码器学习潜在表示。

Result: 在X射线天文学真实数据集上，成功捕捉时间和光谱特征，分离不同类型X射线瞬变。

Conclusion: 框架为跨科学和工业领域的复杂不规则事件时间序列分析提供灵活、可扩展和通用的解决方案。

Abstract: Event time series are sequences of discrete events occurring at irregular
time intervals, each associated with a domain-specific observational modality.
They are common in domains such as high-energy astrophysics, computational
social science, cybersecurity, finance, healthcare, neuroscience, and
seismology. Their unstructured and irregular structure poses significant
challenges for extracting meaningful patterns and identifying salient phenomena
using conventional techniques. We propose novel two- and three-dimensional
tensor representations for event time series, coupled with sparse autoencoders
that learn physically meaningful latent representations. These embeddings
support a variety of downstream tasks, including anomaly detection,
similarity-based retrieval, semantic clustering, and unsupervised
classification. We demonstrate our approach on a real-world dataset from X-ray
astronomy, showing that these representations successfully capture temporal and
spectral signatures and isolate diverse classes of X-ray transients. Our
framework offers a flexible, scalable, and generalizable solution for analyzing
complex, irregular event time series across scientific and industrial domains.

</details>


### [54] [Reinforcement Learning from Adversarial Preferences in Tabular MDPs](https://arxiv.org/abs/2507.11706)
*Taira Tsuchiya,Shinji Ito,Haipeng Luo*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a new framework of episodic tabular Markov decision processes
(MDPs) with adversarial preferences, which we refer to as preference-based MDPs
(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the
numerical value of the loss is directly observed, in PbMDPs the learner instead
observes preferences between two candidate arms, which represent the choices
being compared. In this work, we focus specifically on the setting where the
reward functions are determined by Borda scores. We begin by establishing a
regret lower bound for PbMDPs with Borda scores. As a preliminary step, we
present a simple instance to prove a lower bound of $\Omega(\sqrt{HSAT})$ for
episodic MDPs with adversarial losses, where $H$ is the number of steps per
episode, $S$ is the number of states, $A$ is the number of actions, and $T$ is
the number of episodes. Leveraging this construction, we then derive a regret
lower bound of $\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda
scores, where $K$ is the number of arms. Next, we develop algorithms that
achieve a regret bound of order $T^{2/3}$. We first propose a global
optimization approach based on online linear optimization over the set of all
occupancy measures, achieving a regret bound of $\tilde{O}((H^2 S^2 K)^{1/3}
T^{2/3} )$ under known transitions. However, this approach suffers from
suboptimal dependence on the potentially large number of states $S$ and
computational inefficiency. To address this, we propose a policy optimization
algorithm whose regret is roughly bounded by $\tilde{O}( (H^6 S K^5)^{1/3}
T^{2/3} )$ under known transitions, and further extend the result to the
unknown-transition setting.

</details>


### [55] [Deep Generative Methods and Tire Architecture Design](https://arxiv.org/abs/2507.11639)
*Fouad Oubari,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: 研究五种深度生成模型在工业轮胎架构生成中的应用，评估不同场景，发现扩散模型总体表现最佳。


<details>
  <summary>Details</summary>
Motivation: 工业从业者不清楚哪种深度生成模型最适合复杂制造设计任务，本文以工业轮胎架构生成为例进行研究。

Method: 对五种代表性模型在三种工业场景下进行评估，引入分类修复方法使离散扩散模型处理条件场景，采用几何感知指标量化评估。

Result: 扩散模型总体性能最强，掩码训练的VAE在多数组件条件指标上优于MMVAE⁺，MDM在分布内领先，DDPM对分布外维度约束泛化性更好。

Conclusion: 明确了不同深度生成模型在工业轮胎架构生成不同场景下的性能表现。

Abstract: As deep generative models proliferate across the AI landscape, industrial
practitioners still face critical yet unanswered questions about which deep
generative models best suit complex manufacturing design tasks. This work
addresses this question through a complete study of five representative models
(Variational Autoencoder, Generative Adversarial Network, multimodal
Variational Autoencoder, Denoising Diffusion Probabilistic Model, and
Multinomial Diffusion Model) on industrial tire architecture generation. Our
evaluation spans three key industrial scenarios: (i) unconditional generation
of complete multi-component designs, (ii) component-conditioned generation
(reconstructing architectures from partial observations), and (iii)
dimension-constrained generation (creating designs that satisfy specific
dimensional requirements). To enable discrete diffusion models to handle
conditional scenarios, we introduce categorical inpainting, a mask-aware
reverse diffusion process that preserves known labels without requiring
additional training. Our evaluation employs geometry-aware metrics specifically
calibrated for industrial requirements, quantifying spatial coherence,
component interaction, structural connectivity, and perceptual fidelity. Our
findings reveal that diffusion models achieve the strongest overall
performance; a masking-trained VAE nonetheless outperforms the multimodal
variant MMVAE\textsuperscript{+} on nearly all component-conditioned metrics,
and within the diffusion family MDM leads in-distribution whereas DDPM
generalises better to out-of-distribution dimensional constraints.

</details>


### [56] [Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation](https://arxiv.org/abs/2507.11645)
*Ahmed Salah,David Yevick*

Main category: cs.LG

TL;DR: 本文引入多个实用指标预测神经网络的grokking行为，并借此深入了解grokking的起源和行为。


<details>
  <summary>Details</summary>
Motivation: 预测神经网络的grokking行为并了解其起源和行为。

Method: 引入方差、鲁棒性、嵌入相似度和稀疏性等指标，通过Dropout鲁棒性曲线估计网络对噪声的恢复能力，分析随机Dropout下测试准确率的方差等。

Result: Dropout鲁棒性曲线可在模型从记忆过渡到泛化时估计网络对噪声的恢复能力；随机Dropout下测试准确率的方差在grokking期间有局部最大值；泛化期间非活跃神经元比例下降，嵌入趋于双峰分布。

Conclusion: 所引入的指标能预测grokking行为，且为理解其起源和行为提供有价值的见解。

Abstract: Grokking refers to delayed generalization in which the increase in test
accuracy of a neural network occurs appreciably after the improvement in
training accuracy This paper introduces several practical metrics including
variance under dropout, robustness, embedding similarity, and sparsity
measures, that can forecast grokking behavior. Specifically, the resilience of
neural networks to noise during inference is estimated from a Dropout
Robustness Curve (DRC) obtained from the variation of the accuracy with the
dropout rate as the model transitions from memorization to generalization. The
variance of the test accuracy under stochastic dropout across training
checkpoints further exhibits a local maximum during the grokking. Additionally,
the percentage of inactive neurons decreases during generalization, while the
embeddings tend to a bimodal distribution independent of initialization that
correlates with the observed cosine similarity patterns and dataset symmetries.
These metrics additionally provide valuable insight into the origin and
behaviour of grokking.

</details>


### [57] [A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning](https://arxiv.org/abs/2507.12439)
*Daniel Commey,Rebecca A. Sarpong,Griffith S. Klogo,Winful Bagyl-Bac,Garth V. Crosby*

Main category: cs.LG

TL;DR: 本文提出一种轻量级贝叶斯激励机制应对联邦学习数据投毒攻击，实验证明其鲁棒性好、计算轻量且易集成。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习数据投毒防御方法常为被动防御，计算成本高且假设诚实多数，需要主动防御方法。

Method: 将每轮训练建模为不完全信息贝叶斯博弈，服务器用小的私有验证数据集验证更新质量后支付报酬，机制满足个体理性和激励兼容。

Result: 在MNIST和FashionMNIST非IID分区上实验，面对50%标签翻转攻击，机制保持96.7%准确率，比标准FedAvg高51.7个百分点。

Conclusion: 该机制计算轻量、预算有界且易集成到现有框架，为构建经济上稳健和可持续的联邦学习生态系统提供实用途径。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy. However, its
open-participation nature exposes it to data-poisoning attacks, in which
malicious actors submit corrupted model updates to degrade the global model.
Existing defenses are often reactive, relying on statistical aggregation rules
that can be computationally expensive and that typically assume an honest
majority. This paper introduces a proactive, economic defense: a lightweight
Bayesian incentive mechanism that makes malicious behavior economically
irrational. Each training round is modeled as a Bayesian game of incomplete
information in which the server, acting as the principal, uses a small, private
validation dataset to verify update quality before issuing payments. The design
satisfies Individual Rationality (IR) for benevolent clients, ensuring their
participation is profitable, and Incentive Compatibility (IC), making poisoning
an economically dominated strategy. Extensive experiments on non-IID partitions
of MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping
adversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3
percentage points lower than in a scenario with 30% label-flipping adversaries.
This outcome is 51.7 percentage points better than standard FedAvg, which
collapses under the same 50% attack. The mechanism is computationally light,
budget-bounded, and readily integrates into existing FL frameworks, offering a
practical route to economically robust and sustainable FL ecosystems.

</details>


### [58] [ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs](https://arxiv.org/abs/2507.11649)
*Daniel Commey,Benjamin Appiah,Griffith S. Klogo,Garth V. Crosby*

Main category: cs.LG

TL;DR: 本文提出结合零知识证明的协议，为联邦学习实现隐私保护和可验证评估，在MNIST和HAR数据集上实验并评估。


<details>
  <summary>Details</summary>
Motivation: 联邦学习评估阶段可能通过共享性能指标泄露敏感信息。

Method: 提出结合零知识证明的协议，客户端生成简洁证明表明本地损失低于预定义阈值，用自包含模块实现，不依赖外部API。

Result: 在MNIST和HAR数据集上进行实验。

Conclusion: 可从计算开销、通信成本和可验证性等方面评估该方法。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data without exposing raw data. However, the evaluation phase in FL may leak
sensitive information through shared performance metrics. In this paper, we
propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to
enable privacy-preserving and verifiable evaluation for FL. Instead of
revealing raw loss values, clients generate a succinct proof asserting that
their local loss is below a predefined threshold. Our approach is implemented
without reliance on external APIs, using self-contained modules for federated
learning simulation, ZKP circuit design, and experimental evaluation on both
the MNIST and Human Activity Recognition (HAR) datasets. We focus on a
threshold-based proof for a simple Convolutional Neural Network (CNN) model
(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate
the approach in terms of computational overhead, communication cost, and
verifiability.

</details>


### [59] [Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning](https://arxiv.org/abs/2507.11732)
*Shiyu Chen,Cencheng Shen,Youngser Park,Carey E. Priebe*

Main category: cs.LG

TL;DR: 本文提出用一热图编码器嵌入（GEE）生成高质量初始节点特征的GEE驱动的GNN框架（GG），并在节点聚类和分类任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN依赖随机或信息不足的初始特征表示，导致收敛慢和结果欠佳，需改进初始特征生成方法。

Method: 利用一热图编码器嵌入（GEE）生成初始节点特征，构建GG框架；在节点分类中提出GG - C变体，将GG和GEE的输出拼接。

Result: 在节点聚类中，GG在所有评估的真实数据集上排名第一，收敛更快；在节点分类中，GG - C优于其他基线模型。

Conclusion: 有原则、结构感知的特征初始化对发挥GNN的全部潜力至关重要。

Abstract: Graph neural networks (GNNs) have emerged as a powerful framework for a wide
range of node-level graph learning tasks. However, their performance is often
constrained by reliance on random or minimally informed initial feature
representations, which can lead to slow convergence and suboptimal solutions.
In this paper, we leverage a statistically grounded method, one-hot graph
encoder embedding (GEE), to generate high-quality initial node features that
enhance the end-to-end training of GNNs. We refer to this integrated framework
as the GEE-powered GNN (GG), and demonstrate its effectiveness through
extensive simulations and real-world experiments across both unsupervised and
supervised settings. In node clustering, GG consistently achieves
state-of-the-art performance, ranking first across all evaluated real-world
datasets, while exhibiting faster convergence compared to the standard GNN. For
node classification, we further propose an enhanced variant, GG-C, which
concatenates the outputs of GG and GEE and outperforms competing baselines.
These results confirm the importance of principled, structure-aware feature
initialization in realizing the full potential of GNNs.

</details>


### [60] [STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics](https://arxiv.org/abs/2507.11660)
*Joao F. Rocha,Ke Xu,Xingzhi Sun,Ananya Krishna,Dhananjay Bhaskar,Blanche Mongeon,Morgan Craig,Mark Gerstein,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 介绍了整合ABM与深度学习的STAGED模型，用于模拟细胞间通信及其对细胞内基因调控网络的影响，能更准确表示细胞动力学。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法将细胞视为独立数据点，传统ABM依赖手工规则，需数据驱动方法学习复杂细胞动力学。

Method: 引入STAGED模型，结合ABM与深度学习，用图ODE网络和注意力机制学习基因相互作用强度。

Result: 模型经训练匹配模拟和空间转录组数据推断的轨迹，能捕捉细胞间和细胞内相互作用。

Conclusion: 该模型更自适应准确地表示细胞动力学。

Abstract: The advent of single-cell technology has significantly improved our
understanding of cellular states and subpopulations in various tissues under
normal and diseased conditions by employing data-driven approaches such as
clustering and trajectory inference. However, these methods consider cells as
independent data points of population distributions. With spatial
transcriptomics, we can represent cellular organization, along with dynamic
cell-cell interactions that lead to changes in cell state. Still, key
computational advances are necessary to enable the data-driven learning of such
complex interactive cellular dynamics. While agent-based modeling (ABM)
provides a powerful framework, traditional approaches rely on handcrafted rules
derived from domain knowledge rather than data-driven approaches. To address
this, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)
integrating ABM with deep learning to model intercellular communication, and
its effect on the intracellular gene regulatory network. Using graph ODE
networks (GDEs) with shared weights per cell type, our approach represents
genes as vertices and interactions as directed edges, dynamically learning
their strengths through a designed attention mechanism. Trained to match
continuous trajectories of simulated as well as inferred trajectories from
spatial transcriptomics data, the model captures both intercellular and
intracellular interactions, enabling a more adaptive and accurate
representation of cellular dynamics.

</details>


### [61] [Composing Linear Layers from Irreducibles](https://arxiv.org/abs/2507.11688)
*Travis Pence,Daisuke Yamada,Vikas Singh*

Main category: cs.LG

TL;DR: 研究线性层的组合结构，用Clifford代数将线性层表示为双向量组合，引入可微算法分解为转子乘积，在LLM注意力层中性能与强基线相当，提供代数视角。


<details>
  <summary>Details</summary>
Motivation: 当代大模型中低层次基本元素构成更丰富功能模块的基本构建块未被充分理解，要研究线性层的组合结构，识别/合成来自最小几何基元集的线性变换。

Method: 使用Clifford代数，将线性层表示为双向量的组合，并引入可微算法将其分解为转子的乘积。

Result: 该构造仅使用O(log^2 d)参数，而密集矩阵需要O(d^2)参数；在LLM注意力层的键、查询和值投影中，基于转子的层性能与块Hadamard和低秩近似等强基线相当。

Conclusion: 研究为几何基元如何在深度模型中组成更高级功能提供了代数视角。

Abstract: Contemporary large models often exhibit behaviors suggesting the presence of
low-level primitives that compose into modules with richer functionality, but
these fundamental building blocks remain poorly understood. We investigate this
compositional structure in linear layers by asking: can we identify/synthesize
linear transformations from a minimal set of geometric primitives? Using
Clifford algebra, we show that linear layers can be expressed as compositions
of bivectors -- geometric objects encoding oriented planes -- and introduce a
differentiable algorithm that decomposes them into products of rotors. This
construction uses only O(log^2 d) parameters, versus O(d^2) required by dense
matrices. Applied to the key, query, and value projections in LLM attention
layers, our rotor-based layers match the performance of strong baselines such
as block-Hadamard and low-rank approximations. Our findings provide an
algebraic perspective on how these geometric primitives can compose into
higher-level functions within deep models.

</details>


### [62] [Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update](https://arxiv.org/abs/2507.11847)
*Yu-Jie Zhang,Sheng-An Xu,Peng Zhao,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文研究广义线性多臂老虎机问题，提出联合高效算法，每轮时间和空间复杂度为O(1)且能达到近似最优遗憾界。


<details>
  <summary>Details</summary>
Motivation: 广义线性多臂老虎机在计算和统计效率上存在挑战，现有方法需在两个目标间权衡。

Method: 提出基于在线镜像下降估计器的紧置信集，利用在线预测中的混合损失概念进行分析。

Result: 所提算法每轮时间和空间复杂度为O(1)，能达到近似最优遗憾界。

Conclusion: 所提算法实现了计算和统计效率的联合优化。

Abstract: We study the generalized linear bandit (GLB) problem, a contextual
multi-armed bandit framework that extends the classical linear model by
incorporating a non-linear link function, thereby modeling a broad class of
reward distributions such as Bernoulli and Poisson. While GLBs are widely
applicable to real-world scenarios, their non-linear nature introduces
significant challenges in achieving both computational and statistical
efficiency. Existing methods typically trade off between two objectives, either
incurring high per-round costs for optimal regret guarantees or compromising
statistical efficiency to enable constant-time updates. In this paper, we
propose a jointly efficient algorithm that attains a nearly optimal regret
bound with $\mathcal{O}(1)$ time and space complexities per round. The core of
our method is a tight confidence set for the online mirror descent (OMD)
estimator, which is derived through a novel analysis that leverages the notion
of mix loss from online prediction. The analysis shows that our OMD estimator,
even with its one-pass updates, achieves statistical efficiency comparable to
maximum likelihood estimation, thereby leading to a jointly efficient
optimistic method.

</details>


### [63] [Kevin: Multi-Turn RL for Generating CUDA Kernels](https://arxiv.org/abs/2507.11948)
*Carlo Baronio,Pietro Marsella,Ben Pan,Simon Guo,Silas Alberti*

Main category: cs.LG

TL;DR: 本文提出用多轮强化学习训练的Kevin模型进行CUDA内核生成与优化，评估显示其性能显著提升，还研究了测试时的缩放轴行为。


<details>
  <summary>Details</summary>
Motivation: 编写GPU内核具有挑战性且对AI系统效率至关重要，其迭代性和可验证奖励特性适合应用强化学习，需将迭代过程纳入训练。

Method: 开发灵活的多轮强化学习方法，训练出Kevin模型用于CUDA内核生成与优化。

Result: Kevin模型比其基础模型QwQ - 32B有显著提升，纯CUDA生成内核的正确性从56%提升到82%，平均加速比从0.53x提升到1.10x，超过o4 - mini等前沿模型。

Conclusion: 串行细化比并行采样更有益，更多细化轮次下Kevin模型改进率更高。

Abstract: Writing GPU kernels is a challenging task and critical for AI systems'
efficiency. It is also highly iterative: domain experts write code and improve
performance through execution feedback. Moreover, it presents verifiable
rewards like correctness and speedup, making it a natural environment to apply
Reinforcement Learning (RL). To explicitly incorporate the iterative nature of
this process into training, we develop a flexible multi-turn RL recipe that
addresses unique challenges encountered in real-world settings, such as
learning from long trajectories and effective reward attribution across turns.
We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL
for CUDA kernel generation and optimization. In our evaluation setup, Kevin
shows significant gains over its base model (QwQ-32B), improving correctness of
generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to
1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini
(0.78x). Finally, we study its behavior across test-time scaling axes: we found
scaling serial refinement more beneficial than parallel sampling. In
particular, when given more refinement turns, Kevin shows a higher rate of
improvement.

</details>


### [64] [The Impact of Coreset Selection on Spurious Correlations and Group Robustness](https://arxiv.org/abs/2507.11690)
*Amaya Dharmasiri,William Yang,Polina Kirichenko,Lydia Liu,Olga Russakovsky*

Main category: cs.LG

TL;DR: 本文对数据选择对所选核心集的虚假偏差水平及下游模型鲁棒性的影响进行全面分析，发现嵌入表征选核心集致偏差风险低，优先选难样本的方法不能保证下游鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 许多数据集存在偏差，需了解数据集缩减方法对偏差的影响。

Method: 使用涵盖十种虚假相关性基准、五种样本重要性/难度评分指标和五种数据选择策略的广泛实验设置。

Result: 发现用基于嵌入的样本表征分数选择核心集比基于学习动态的表征选择导致偏差的风险相对较低；优先选择困难样本的核心集选择方法虽能降低偏差水平，但不能可靠保证下游鲁棒性。

Conclusion: 核心集选择方法与偏差和模型鲁棒性存在复杂关系，优先选难样本的方法无法可靠保证下游模型鲁棒性。

Abstract: Coreset selection methods have shown promise in reducing the training data
size while maintaining model performance for data-efficient machine learning.
However, as many datasets suffer from biases that cause models to learn
spurious correlations instead of causal features, it is important to understand
whether and how dataset reduction methods may perpetuate, amplify, or mitigate
these biases. In this work, we conduct the first comprehensive analysis of the
implications of data selection on the spurious bias levels of the selected
coresets and the robustness of downstream models trained on them. We use an
extensive experimental setting spanning ten different spurious correlations
benchmarks, five score metrics to characterize sample importance/ difficulty,
and five data selection policies across a broad range of coreset sizes.
Thereby, we unravel a series of nontrivial nuances in interactions between
sample difficulty and bias alignment, as well as dataset bias and resultant
model robustness. For example, we find that selecting coresets using
embedding-based sample characterization scores runs a comparatively lower risk
of inadvertently exacerbating bias than selecting using characterizations based
on learning dynamics. Most importantly, our analysis reveals that although some
coreset selection methods could achieve lower bias levels by prioritizing
difficult samples, they do not reliably guarantee downstream robustness.

</details>


### [65] [Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption](https://arxiv.org/abs/2507.11702)
*Hein de Wilde,Ali Mohammed Mansoor Alsahag,Pierre Blanchet*

Main category: cs.LG

TL;DR: 铁路落叶造成巨大损失，当前落叶预测方法有局限，本研究用新系统预测落叶时间，结果较好，有望优化铁路落叶缓解措施。


<details>
  <summary>Details</summary>
Motivation: 铁路落叶致英国铁路行业每年损失超3亿英镑，当前预测方法有局限，需准确预测落叶时间以高效安排缓解措施。

Method: 利用专业预测方法和最新卫星数据源，用地面落叶数据、多光谱和气象卫星数据训练LSTM网络。

Result: 预测落叶开始时间的均方根误差为6.32天，结束时间为9.31天。

Conclusion: 该模型改进了以往研究，为铁路行业优化落叶缓解措施和理解生态系统提供了机会。

Abstract: Railroad traffic disruption as a result of leaf-fall cost the UK rail
industry over 300 million per year and measures to mitigate such disruptions
are employed on a large scale, with 1.67 million kilometers of track being
treated in the UK in 2021 alone. Therefore, the ability to anticipate the
timing of leaf-fall would offer substantial benefits for rail network
operators, enabling the efficient scheduling of such mitigation measures.
However, current methodologies for predicting leaf-fall exhibit considerable
limitations in terms of scalability and reliability. This study endeavors to
devise a prediction system that leverages specialized prediction methods and
the latest satellite data sources to generate both scalable and reliable
insights into leaf-fall timings. An LSTM network trained on ground-truth
leaf-falling data combined with multispectral and meteorological satellite data
demonstrated a root-mean-square error of 6.32 days for predicting the start of
leaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which
improves upon previous work on the topic, offers promising opportunities for
the optimization of leaf mitigation measures in the railway industry and the
improvement of our understanding of complex ecological systems.

</details>


### [66] [Subgraph Generation for Generalizing on Out-of-Distribution Links](https://arxiv.org/abs/2507.11710)
*Jay Revolinsky,Harry Shomer,Jiliang Tang*

Main category: cs.LG

TL;DR: 提出GGM框架FLEX，结合两种机制提升OOD场景下链接预测性能，且无需专家知识，有实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有GNN依赖同分布样本，GGM应用局限于特定领域，需填补二者差距。

Method: 提出FLEX框架，采用结构条件图生成和自编码器与GNN的对抗协同训练两种机制。

Result: 在合成和真实世界的OOD设置中进行大量实验，证明FLEX提升性能的能力，并分析图数据增强对链接结构的影响。

Conclusion: FLEX可确保样本分布间的结构对齐，提升OOD场景下链接预测性能，且无需专家知识。

Abstract: Graphs Neural Networks (GNNs) demonstrate high-performance on the link
prediction (LP) task. However, these models often rely on all dataset samples
being drawn from the same distribution. In addition, graph generative models
(GGMs) show a pronounced ability to generate novel output graphs. Despite this,
GGM applications remain largely limited to domain-specific tasks. To bridge
this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)
structurally-conditioned graph generation, and (2) adversarial co-training
between an auto-encoder and GNN. As such, FLEX ensures structural-alignment
between sample distributions to enhance link-prediction performance in
out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert
knowledge to function in different OOD scenarios. Numerous experiments are
conducted in synthetic and real-world OOD settings to demonstrate FLEX's
performance-enhancing ability, with further analysis for understanding the
effects of graph data augmentation on link structures. The source code is
available here: https://github.com/revolins/FlexOOD.

</details>


### [67] [Robust Causal Discovery in Real-World Time Series with Power-Laws](https://arxiv.org/abs/2507.12257)
*Matteo Tusoni,Giuseppe Masi,Andrea Coletta,Aldo Glielmo,Viviana Arrigoni,Novella Bartolini*

Main category: cs.LG

TL;DR: 本文提出基于提取幂律谱特征的因果发现方法，在合成基准和真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现算法对噪声敏感，应用于真实数据时会导致因果推断出错。

Method: 利用现实世界时间序列频谱服从幂律分布的特点，构建基于提取幂律谱特征的因果发现方法。

Result: 该方法在合成基准和有已知因果结构的真实数据集上始终优于现有方法。

Conclusion: 该方法具有鲁棒性和实际应用价值。

Abstract: Exploring causal relationships in stochastic time series is a challenging yet
crucial task with a vast range of applications, including finance, economics,
neuroscience, and climate science. Many algorithms for Causal Discovery (CD)
have been proposed, but they often exhibit a high sensitivity to noise,
resulting in misleading causal inferences when applied to real data. In this
paper, we observe that the frequency spectra of typical real-world time series
follow a power-law distribution, notably due to an inherent self-organizing
behavior. Leveraging this insight, we build a robust CD method based on the
extraction of power -law spectral features that amplify genuine causal signals.
Our method consistently outperforms state-of-the-art alternatives on both
synthetic benchmarks and real-world datasets with known causal structures,
demonstrating its robustness and practical relevance.

</details>


### [68] [Globalization for Scalable Short-term Load Forecasting](https://arxiv.org/abs/2507.11729)
*Amirhossein Ahmadi,Hamidreza Zareipour,Henry Leung*

Main category: cs.LG

TL;DR: 本文研究存在数据漂移时的全局负荷预测，对比特征转换和目标转换模型，提出不同聚类方法，实验表明全局目标转换模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统局部负荷预测模型在泛化性、过拟合、数据漂移、冷启动和可扩展性方面存在局限，而全局预测模型可改善这些问题。

Method: 探索特征转换和目标转换模型，提出基于模型的时间序列聚类用于特征转换模型，新的加权基于实例的时间序列聚类用于目标转换模型。

Result: 在阿尔伯塔省电力负荷真实数据集上实验，全局目标转换模型表现优于局部模型，全局特征转换模型平衡局部和全局动态有挑战。

Conclusion: 全局目标转换模型结合全局特征和聚类技术在负荷预测中表现更佳，全局特征转换模型需有效管理数据异质性。

Abstract: Forecasting load in power transmission networks is essential across various
hierarchical levels, from the system level down to individual points of
delivery (PoD). While intuitive and locally accurate, traditional local
forecasting models (LFMs) face significant limitations, particularly in
handling generalizability, overfitting, data drift, and the cold start problem.
These methods also struggle with scalability, becoming computationally
expensive and less efficient as the network's size and data volume grow. In
contrast, global forecasting models (GFMs) offer a new approach to enhance
prediction generalizability, scalability, accuracy, and robustness through
globalization and cross-learning. This paper investigates global load
forecasting in the presence of data drifts, highlighting the impact of
different modeling techniques and data heterogeneity. We explore
feature-transforming and target-transforming models, demonstrating how
globalization, data heterogeneity, and data drift affect each differently. In
addition, we examine the role of globalization in peak load forecasting and its
potential for hierarchical forecasting. To address data heterogeneity and the
balance between globality and locality, we propose separate time series
clustering (TSC) methods, introducing model-based TSC for feature-transforming
models and new weighted instance-based TSC for target-transforming models.
Through extensive experiments on a real-world dataset of Alberta's electricity
load, we demonstrate that global target-transforming models consistently
outperform their local counterparts, especially when enriched with global
features and clustering techniques. In contrast, global feature-transforming
models face challenges in balancing local and global dynamics, often requiring
TSC to manage data heterogeneity effectively.

</details>


### [69] [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](https://arxiv.org/abs/2507.12262)
*Zachary James,Joseph Guinness*

Main category: cs.LG

TL;DR: 本文提出用神经网络输出非平稳核参数的高斯过程框架，联合训练网络与高斯过程，在多数据集上表现优于平稳模型等。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程常用平稳核，限制模型表达能力，不适用于很多数据集。

Method: 提出使用非平稳核的框架，将核参数建模为以特征为输入的神经网络输出，用链式法则联合训练神经网络和高斯过程。

Result: 在多个机器学习数据集上，非平稳方差和噪声变体方法精度和对数得分优于平稳模型和分层模型；仅非平稳方差模型也有类似结果；能恢复空间数据集的非平稳参数。

Conclusion: 该方法灵活，易适配不同非平稳核，无需重新设计优化过程，表现良好。

Abstract: Gaussian processes have become a popular tool for nonparametric regression
because of their flexibility and uncertainty quantification. However, they
often use stationary kernels, which limit the expressiveness of the model and
may be unsuitable for many datasets. We propose a framework that uses
nonstationary kernels whose parameters vary across the feature space, modeling
these parameters as the output of a neural network that takes the features as
input. The neural network and Gaussian process are trained jointly using the
chain rule to calculate derivatives. Our method clearly describes the behavior
of the nonstationary parameters and is compatible with approximation methods
for scaling to large datasets. It is flexible and easily adapts to different
nonstationary kernels without needing to redesign the optimization procedure.
Our methods are implemented with the GPyTorch library and can be readily
modified. We test a nonstationary variance and noise variant of our method on
several machine learning datasets and find that it achieves better accuracy and
log-score than both a stationary model and a hierarchical model approximated
with variational inference. Similar results are observed for a model with only
nonstationary variance. We also demonstrate our approach's ability to recover
the nonstationary parameters of a spatial dataset.

</details>


### [70] [ROC-n-reroll: How verifier imperfection affects test-time scaling](https://arxiv.org/abs/2507.12399)
*Florian E. Dorner,Yatong Chen,André F. Cruz,Fanny Yang*

Main category: cs.LG

TL;DR: 本文研究验证器不完美性对测试时缩放技术影响，给出相关理论分析并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏对验证器不完美性如何影响语言模型测试时缩放性能的理论理解，本文旨在填补这一空白。

Method: 证明实例级准确率与验证器ROC曲线几何形状的关系，分析ROC曲线局部和全局特性对不同方法的影响。

Result: 拒绝采样的缩放取决于ROC曲线局部几何形状，BoN取决于全局特性；低计算量下无法外推拒绝采样性能；固定计算量时拒绝采样优于BoN，无限计算量时二者准确率收敛。

Conclusion: 理论结果在GSM8K数据集上通过不同模型的实验得到了验证。

Abstract: Test-time scaling aims to improve language model performance by leveraging
additional compute during inference. While many works have empirically studied
techniques like Best-of-N (BoN) and rejection sampling that make use of a
verifier to enable test-time scaling, there is little theoretical understanding
of how verifier imperfection affects performance. In this work, we address this
gap. Specifically, we prove how instance-level accuracy of these methods is
precisely characterized by the geometry of the verifier's ROC curve.
Interestingly, while scaling is determined by the local geometry of the ROC
curve for rejection sampling, it depends on global properties of the ROC curve
for BoN. As a consequence when the ROC curve is unknown, it is impossible to
extrapolate the performance of rejection sampling based on the low-compute
regime. Furthermore, while rejection sampling outperforms BoN for fixed
compute, in the infinite-compute limit both methods converge to the same level
of accuracy, determined by the slope of the ROC curve near the origin. Our
theoretical results are confirmed by experiments on GSM8K using different
versions of Llama and Qwen to generate and verify solutions.

</details>


### [71] [A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction](https://arxiv.org/abs/2507.11757)
*Yuehua Song,Yong Gao*

Main category: cs.LG

TL;DR: 提出Graph - in - Graph (GiG)模型预测药物 - 靶点相互作用，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在有效整合药物、靶点及其相互作用的多样特征方面存在困难，需改进药物 - 靶点相互作用预测。

Method: 引入结合直推式学习和归纳式学习的框架，使用基于GNN的Graph - in - Graph (GiG)模型，将药物和靶点分子结构图表示为药物 - 靶点相互作用图中的元节点，并构建特殊基准数据集。

Result: GiG模型在所有评估指标上显著优于现有方法。

Conclusion: 整合不同学习范式和相互作用数据有利于药物 - 靶点相互作用预测。

Abstract: Accurately predicting drug-target interactions (DTIs) is pivotal for
advancing drug discovery and target validation techniques. While machine
learning approaches including those that are based on Graph Neural Networks
(GNN) have achieved notable success in DTI prediction, many of them have
difficulties in effectively integrating the diverse features of drugs, targets
and their interactions. To address this limitation, we introduce a novel
framework to take advantage of the power of both transductive learning and
inductive learning so that features at molecular level and drug-target
interaction network level can be exploited. Within this framework is a
GNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and
target molecular structures as meta-nodes in a drug-target interaction graph,
enabling a detailed exploration of their intricate relationships. To evaluate
the proposed model, we have compiled a special benchmark comprising drug
SMILES, protein sequences, and their interaction data, which is interesting in
its own right. Our experimental results demonstrate that the GiG model
significantly outperforms existing approaches across all evaluation metrics,
highlighting the benefits of integrating different learning paradigms and
interaction data.

</details>


### [72] [Torsional-GFN: a conditional conformation generator for small molecules](https://arxiv.org/abs/2507.11759)
*Alexandra Volokhova,Léna Néhale Ezzine,Piotr Gaiński,Luca Scimeca,Emmanuel Bengio,Prudencio Tossou,Yoshua Bengio,Alex Hernandez-Garcia*

Main category: cs.LG

TL;DR: 本文介绍了用于从玻尔兹曼分布中采样分子构象的Torsional - GFN，结果表明其能以单一模型为多种分子采样，且可零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 生成稳定分子构象在药物发现应用中很重要，现有生成式机器学习方法比分子动力学更有潜力，需设计新方法从玻尔兹曼分布采样分子构象。

Method: 引入Torsional - GFN，一种条件GFlowNet，仅用奖励函数作为训练信号，基于分子图及其局部结构对扭转角旋转进行采样。

Result: Torsional - GFN能以单一模型为多种分子采样近似符合玻尔兹曼分布的构象，可对分子MD模拟中未见的键长和键角进行零样本泛化。

Conclusion: 该方法为扩展到更大分子系统、实现对未见分子的零样本泛化以及将局部结构生成纳入GFlowNet模型提供了有前景的途径。

Abstract: Generating stable molecular conformations is crucial in several drug
discovery applications, such as estimating the binding affinity of a molecule
to a target. Recently, generative machine learning methods have emerged as a
promising, more efficient method than molecular dynamics for sampling of
conformations from the Boltzmann distribution. In this paper, we introduce
Torsional-GFN, a conditional GFlowNet specifically designed to sample
conformations of molecules proportionally to their Boltzmann distribution,
using only a reward function as training signal. Conditioned on a molecular
graph and its local structure (bond lengths and angles), Torsional-GFN samples
rotations of its torsion angles. Our results demonstrate that Torsional-GFN is
able to sample conformations approximately proportional to the Boltzmann
distribution for multiple molecules with a single model, and allows for
zero-shot generalization to unseen bond lengths and angles coming from the MD
simulations for such molecules. Our work presents a promising avenue for
scaling the proposed approach to larger molecular systems, achieving zero-shot
generalization to unseen molecules, and including the generation of the local
structure into the GFlowNet model.

</details>


### [73] [Scaling laws for activation steering with Llama 2 models and refusal mechanisms](https://arxiv.org/abs/2507.11771)
*Sheikh Abdur Raheem Ali,Justin Xu,Ivory Yang,Jasmine Xinze Li,Ayse Arslan,Clark Benham*

Main category: cs.LG

TL;DR: 本文基于Llama 2系列模型，探索对比激活添加（CAA）在不同模型规模下的有效性，发现CAA在早中期层应用最有效、有效性随模型规模减小、负向引导效果更明显。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型复杂度和能力提升，不太广泛使用的对齐技术效果不确定，因此探索CAA在不同模型规模下的有效性。

Method: 基于Llama 2系列（7B、13B和70B）模型，利用对比激活添加方法，通过对比对在模型残差流向量空间中找到理想“方向”并在正向传播时添加到残差流，使用围绕拒绝行为的答案匹配问题进行研究。

Result: 1) CAA在早中期层应用最有效；2) CAA的有效性随模型规模减小；3) 在所有模型规模下，负向引导比正向引导效果更明显。

Conclusion: CAA在不同模型规模下有不同效果，在模型早中期层应用较好，且负向引导效果更优。

Abstract: As large language models (LLMs) evolve in complexity and capability, the
efficacy of less widely deployed alignment techniques are uncertain. Building
on previous work on activation steering and contrastive activation addition
(CAA), this paper explores the effectiveness of CAA with model scale using the
family of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable
'directions' in the model's residual stream vector space using contrastive
pairs (for example, hate to love) and adding this direction to the residual
stream during the forward pass. It directly manipulates the residual stream and
aims to extract features from language models to better control their outputs.
Using answer matching questions centered around the refusal behavior, we found
that 1) CAA is most effective when applied at early-mid layers. 2) The
effectiveness of CAA diminishes with model size. 3) Negative steering has more
pronounced effects than positive steering across all model sizes.

</details>


### [74] [Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network](https://arxiv.org/abs/2507.11776)
*Merel Kampere,Ali Mohammed Mansoor Alsahag*

Main category: cs.LG

TL;DR: 本文用XGBoost分类器，结合拓扑特征对荷兰铁路网络延误预测展开研究，虽结果表现有限，但为交通网络评估及后续模型开发提供方向。


<details>
  <summary>Details</summary>
Motivation: 荷兰铁路网繁忙，延误是主要问题，当前研究多侧重短期预测，忽略网络整体模式以减轻连锁反应，本文旨在填补荷兰铁路网络延误预测研究的空白。

Method: 采用XGBoost分类器，结合节点中心性度量，对比随机森林、决策树等多个分类器，改进用于预测美国航空网络演变的方法来预测荷兰铁路延误。

Result: 模型在非同步测试场景中表现有限。

Conclusion: 本研究有助于理解交通网络评估，并为开发更强大的延误预测模型提出了未来方向。

Abstract: The Dutch railway network is one of the busiest in the world, with delays
being a prominent concern for the principal passenger railway operator NS. This
research addresses a gap in delay prediction studies within the Dutch railway
network by employing an XGBoost Classifier with a focus on topological
features. Current research predominantly emphasizes short-term predictions and
neglects the broader network-wide patterns essential for mitigating ripple
effects. This research implements and improves an existing methodology,
originally designed to forecast the evolution of the fast-changing US air
network, to predict delays in the Dutch Railways. By integrating Node
Centrality Measures and comparing multiple classifiers like RandomForest,
DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is
to predict delayed trajectories. However, the results reveal limited
performance, especially in non-simultaneous testing scenarios, suggesting the
necessity for more context-specific adaptations. Regardless, this research
contributes to the understanding of transportation network evaluation and
proposes future directions for developing more robust predictive models for
delays.

</details>


### [75] [Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation](https://arxiv.org/abs/2507.11789)
*Alessandro Palma,Sergei Rybakov,Leon Hetzel,Stephan Günnemann,Fabian J. Theis*

Main category: cs.LG

TL;DR: 本文提出FlatVI训练框架，正则化离散似然变分自编码器的潜在流形至欧几里得几何，用于单细胞计数数据建模，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞RNA测序方法假设潜在空间线性插值和欧几里得几何，但线性插值可能不对应数据流形上的测地线路径，限制了相关方法。

Method: 引入FlatVI训练框架，正则化离散似然变分自编码器的潜在流形，使其符合欧几里得几何。

Result: 合成数据实验支持方法的理论合理性，时间分辨单细胞RNA测序数据应用展示了更好的轨迹重建和流形插值效果。

Conclusion: FlatVI框架增强了与假设欧几里得潜在几何的下游方法的兼容性，提升了单细胞数据建模效果。

Abstract: Latent space interpolations are a powerful tool for navigating deep
generative models in applied settings. An example is single-cell RNA
sequencing, where existing methods model cellular state transitions as latent
space interpolations with variational autoencoders, often assuming linear
shifts and Euclidean geometry. However, unless explicitly enforced, linear
interpolations in the latent space may not correspond to geodesic paths on the
data manifold, limiting methods that assume Euclidean geometry in the data
representations. We introduce FlatVI, a novel training framework that
regularises the latent manifold of discrete-likelihood variational autoencoders
towards Euclidean geometry, specifically tailored for modelling single-cell
count data. By encouraging straight lines in the latent space to approximate
geodesic interpolations on the decoded single-cell manifold, FlatVI enhances
compatibility with downstream approaches that assume Euclidean latent geometry.
Experiments on synthetic data support the theoretical soundness of our
approach, while applications to time-resolved single-cell RNA sequencing data
demonstrate improved trajectory reconstruction and manifold interpolation.

</details>


### [76] [CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels](https://arxiv.org/abs/2507.11807)
*Ruofan Hu,Dongyu Zhang,Huayi Zhang,Elke Rundensteiner*

Main category: cs.LG

TL;DR: 提出无需干净标签数据集的元学习方法CLID - MU应对含噪标签场景，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有元学习方法依赖干净标签元数据集，实际难以获取，需解决无干净标签数据集的含噪标签元学习问题。

Method: 基于干净样本和含噪样本对数据结构一致性的不同影响，设计Cross - layer Information Divergence - based Meta Update Strategy (CLID - MU)，利用不同特征空间的数据结构对齐评估模型性能并指导训练。

Result: 在合成和真实噪声的基准数据集上实验表明，CLID - MU优于现有方法。

Conclusion: CLID - MU是一种有效的无干净标签数据集的含噪标签元学习方法。

Abstract: Learning with noisy labels (LNL) is essential for training deep neural
networks with imperfect data. Meta-learning approaches have achieved success by
using a clean unbiased labeled set to train a robust model. However, this
approach heavily depends on the availability of a clean labeled meta-dataset,
which is difficult to obtain in practice. In this work, we thus tackle the
challenge of meta-learning for noisy label scenarios without relying on a clean
labeled dataset. Our approach leverages the data itself while bypassing the
need for labels. Building on the insight that clean samples effectively
preserve the consistency of related data structures across the last hidden and
the final layer, whereas noisy samples disrupt this consistency, we design the
Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU).
CLID-MU leverages the alignment of data structures across these diverse feature
spaces to evaluate model performance and use this alignment to guide training.
Experiments on benchmark datasets with varying amounts of labels under both
synthetic and real-world noise demonstrate that CLID-MU outperforms
state-of-the-art methods. The code is released at
https://github.com/ruofanhu/CLID-MU.

</details>


### [77] [SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling](https://arxiv.org/abs/2507.11818)
*Andrei Rekesh,Miruna Cretu,Dmytro Shevchuk,Vignesh Ram Somnath,Pietro Liò,Robert A. Batey,Mike Tyers,Michał Koziarski,Cheng-Hao Liu*

Main category: cs.LG

TL;DR: 提出SynCoGen框架用于可合成3D分子生成，表现优异并具应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有可合成小分子生成局限于2D图表示，无法进行基于几何的条件生成。

Method: 结合掩码图扩散和流匹配，从分子构建块、化学反应和原子坐标联合分布采样，用SynSpace数据集训练。

Result: 在无条件小分子图和构象生成中达SOTA，在零样本分子连接体设计中有竞争力。

Conclusion: 该多模态公式为非自回归分子生成应用奠定基础。

Abstract: Ensuring synthesizability in generative small molecule design remains a major
challenge. While recent developments in synthesizable molecule generation have
demonstrated promising results, these efforts have been largely confined to 2D
molecular graph representations, limiting the ability to perform geometry-based
conditional generation. In this work, we present SynCoGen (Synthesizable
Co-Generation), a single framework that combines simultaneous masked graph
diffusion and flow matching for synthesizable 3D molecule generation. SynCoGen
samples from the joint distribution of molecular building blocks, chemical
reactions, and atomic coordinates. To train the model, we curated SynSpace, a
dataset containing over 600K synthesis-aware building block graphs and 3.3M
conformers. SynCoGen achieves state-of-the-art performance in unconditional
small molecule graph and conformer generation, and the model delivers
competitive performance in zero-shot molecular linker design for protein ligand
generation in drug discovery. Overall, this multimodal formulation represents a
foundation for future applications enabled by non-autoregressive molecular
generation, including analog expansion, lead optimization, and direct structure
conditioning.

</details>


### [78] [MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory](https://arxiv.org/abs/2507.11821)
*Pouya Shaeri,Arash Karimi,Ariane Middel*

Main category: cs.LG

TL;DR: 提出MNIST - Gen框架生成MNIST风格图像数据集，用Tree - MNIST和Food - MNIST验证，准确率85%且节省80%时间。


<details>
  <summary>Details</summary>
Motivation: 标准MNIST数据集对特定领域任务不适用，创建自定义数据集耗时且受限。

Method: 结合CLIP语义理解、强化学习和人工反馈，用分层语义分类，将数据转换阶段建模为可组合态射。

Result: 生成Tree - MNIST和Food - MNIST数据集，自动分类准确率达85%，节省80%时间。

Conclusion: MNIST - Gen可有效生成特定任务评估数据。

Abstract: Neural networks are often benchmarked using standard datasets such as MNIST,
FashionMNIST, or other variants of MNIST, which, while accessible, are limited
to generic classes such as digits or clothing items. For researchers working on
domain-specific tasks, such as classifying trees, food items, or other
real-world objects, these data sets are insufficient and irrelevant.
Additionally, creating and publishing a custom dataset can be time consuming,
legally constrained, or beyond the scope of individual projects. We present
MNIST-Gen, an automated, modular, and adaptive framework for generating
MNIST-style image datasets tailored to user-specified categories using
hierarchical semantic categorization. The system combines CLIP-based semantic
understanding with reinforcement learning and human feedback to achieve
intelligent categorization with minimal manual intervention. Our hierarchical
approach supports complex category structures with semantic characteristics,
enabling fine-grained subcategorization and multiple processing modes:
individual review for maximum control, smart batch processing for large
datasets, and fast batch processing for rapid creation. Inspired by category
theory, MNIST-Gen models each data transformation stage as a composable
morphism, enhancing clarity, modularity, and extensibility. As proof of
concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and
\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing
task-specific evaluation data while achieving 85\% automatic categorization
accuracy and 80\% time savings compared to manual approaches.

</details>


### [79] [HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction](https://arxiv.org/abs/2507.11836)
*Jian Gao,Jianshe Wu,JingYi Ding*

Main category: cs.LG

TL;DR: 提出HyperEvent框架将动态链接预测转为超事件识别，在多数据集表现佳，还引入并行训练算法提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有节点中心和事件中心方法无法捕捉复合超事件的结构内聚性。

Method: 构建HyperEvent框架，用事件相关向量动态构建关联序列，预测查询事件是否能与历史事件形成有效超事件；引入并行训练算法分割大事件流进行并发训练。

Result: 在官方排行榜5个数据集中4个优于现有方法；在大规模Flight数据集上平均倒数排名提升6.95%，仅用10.17%训练时间。

Conclusion: HyperEvent在大规模图上有优越的准确性和效率。

Abstract: Dynamic link prediction in continuous-time dynamic graphs is a fundamental
task for modeling evolving complex systems. Existing node-centric and
event-centric methods focus on individual interactions or atomic states,
failing to capture the structural cohesion of composite hyper-events, groups of
causally related events. To address this, we propose HyperEvent, a framework
reframing dynamic link prediction as hyper-event recognition. Central to
HyperEvent is the dynamic construction of an association sequence using event
correlation vectors. These vectors quantify pairwise dependencies between the
query event and relevant historical events, thereby characterizing the
structural cohesion of a potential hyper-event. The framework predicts the
occurrence of the query event by evaluating whether it collectively forms a
valid hyper-event with these historical events. Notably, HyperEvent outperforms
state-of-the-art methods on 4 out of 5 datasets in the official leaderboard.
For scalability, we further introduce an efficient parallel training algorithm
that segments large event streams to enable concurrent training. Experiments
validate HyperEvent's superior accuracy and efficiency on large-scale graphs.
Among which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank
over state-of-the-art baseline on the large-scale Flight dataset while
utilizing only 10.17% of the training time.

</details>


### [80] [Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM](https://arxiv.org/abs/2507.11839)
*Chengyue Gong,Xinshi Chen,Yuxuan Zhang,Yuxuan Song,Hao Zhou,Wenzhi Xiao*

Main category: cs.LG

TL;DR: 本文提出轻量级模型Protenix - Mini用于高效蛋白质结构预测，通过关键修改降低模型复杂度，性能略降但适合资源有限场景。


<details>
  <summary>Details</summary>
Motivation: 解决生物分子结构预测中模型效率和预测准确性的平衡问题，实现高效的实际部署和大规模应用的推理时间扩展。

Method: 1. 用几步的ODE采样器替换多步AF3采样器；2. 对开源Protenix框架进行架构修剪和轻量级重新设计；3. 训练含ESM模块的模型替代传统MSA模块。

Result: Protenix - Mini显著降低模型复杂度，在基准数据集上性能仅下降1 - 5%。

Conclusion: Protenix - Mini适合计算资源有限但需准确结构预测的应用场景。

Abstract: Lightweight inference is critical for biomolecular structure prediction and
other downstream tasks, enabling efficient real-world deployment and
inference-time scaling for large-scale applications. In this work, we address
the challenge of balancing model efficiency and prediction accuracy by making
several key modifications, 1) Multi-step AF3 sampler is replaced by a few-step
ODE sampler, significantly reducing computational overhead for the diffusion
module part during inference; 2) In the open-source Protenix framework, a
subset of pairformer or diffusion transformer blocks doesn't make contributions
to the final structure prediction, presenting opportunities for architectural
pruning and lightweight redesign; 3) A model incorporating an ESM module is
trained to substitute the conventional MSA module, reducing MSA preprocessing
time. Building on these key insights, we present Protenix-Mini, a compact and
optimized model designed for efficient protein structure prediction. This
streamlined version incorporates a more efficient architectural design with a
two-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating
redundant Transformer components and refining the sampling process,
Protenix-Mini significantly reduces model complexity with slight accuracy drop.
Evaluations on benchmark datasets demonstrate that it achieves high-fidelity
predictions, with only a negligible 1 to 5 percent decrease in performance on
benchmark datasets compared to its full-scale counterpart. This makes
Protenix-Mini an ideal choice for applications where computational resources
are limited but accurate structure prediction remains crucial.

</details>


### [81] [OrdShap: Feature Position Importance for Sequential Black-Box Models](https://arxiv.org/abs/2507.11855)
*Davin Hill,Brian L. Hill,Aria Masoomi,Vijay S. Nori,Robert E. Tillman,Jennifer Dy*

Main category: cs.LG

TL;DR: 本文提出新归因方法OrdShap，解决现有特征归因技术假设特征顺序固定的问题，实证证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有顺序深度学习模型的特征归因技术假设特征顺序固定，混淆特征值和位置影响，需新方法解决。

Method: 引入OrdShap方法，通过排列特征位置量化模型预测变化，建立与Sanchez - Bergantiños值的博弈论联系。

Result: 在健康、自然语言和合成数据集上的实证结果表明，OrdShap能有效捕捉特征值和位置归因，深入洞察模型行为。

Conclusion: OrdShap是一种有效的位置敏感归因方法，可用于理解顺序深度学习模型的预测。

Abstract: Sequential deep learning models excel in domains with temporal or sequential
dependencies, but their complexity necessitates post-hoc feature attribution
methods for understanding their predictions. While existing techniques quantify
feature importance, they inherently assume fixed feature ordering - conflating
the effects of (1) feature values and (2) their positions within input
sequences. To address this gap, we introduce OrdShap, a novel attribution
method that disentangles these effects by quantifying how a model's predictions
change in response to permuting feature position. We establish a game-theoretic
connection between OrdShap and Sanchez-Berganti\~nos values, providing a
theoretically grounded approach to position-sensitive attribution. Empirical
results from health, natural language, and synthetic datasets highlight
OrdShap's effectiveness in capturing feature value and feature position
attributions, and provide deeper insight into model behavior.

</details>


### [82] [A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers](https://arxiv.org/abs/2507.11865)
*Hanwen Dai,Chang Gao,Fang He,Congyuan Ji,Yanni Yang*

Main category: cs.LG

TL;DR: 研究从单个平台角度动态管理司机对折扣快车服务的接受度，提出 pi - DDPG 框架，实验表明其学习效率高且减少早期训练损失。


<details>
  <summary>Details</summary>
Motivation: 平台整合中，鼓励司机参与折扣快车服务有扩大需求池和提高匹配效率的潜力，但需动态管理司机接受度，且新业务模式缺乏历史数据，早期探索成本高。

Method: 将司机接受行为比例决策建模为连续控制任务，提出 pi - DDPG 框架，包含精炼模块、使用卷积长短期记忆网络和优先经验回放机制。

Result: 基于真实数据集的模拟器验证，pi - DDPG 实现了更高的学习效率，显著降低早期训练损失。

Conclusion: pi - DDPG 框架在动态管理司机对折扣快车服务接受度问题上有效，能解决早期学习效率和损失问题。

Abstract: The rapid expansion of platform integration has emerged as an effective
solution to mitigate market fragmentation by consolidating multiple
ride-hailing platforms into a single application. To address heterogeneous
passenger preferences, third-party integrators provide Discount Express service
delivered by express drivers at lower trip fares. For the individual platform,
encouraging broader participation of drivers in Discount Express services has
the potential to expand the accessible demand pool and improve matching
efficiency, but often at the cost of reduced profit margins. This study aims to
dynamically manage drivers' acceptance of Discount Express from the perspective
of individual platforms. The lack of historical data under the new business
model necessitates online learning. However, early-stage exploration through
trial and error can be costly in practice, highlighting the need for reliable
early-stage performance in real-world deployment. To address these challenges,
this study formulates the decision regarding the proportion of drivers'
acceptance behavior as a continuous control task. In response to the high
stochasticity, the opaque matching mechanisms employed by third-party
integrator, and the limited availability of historical data, we propose a
policy-improved deep deterministic policy gradient (pi-DDPG) framework. The
proposed framework incorporates a refiner module to boost policy performance
during the early training phase, leverages a convolutional long short-term
memory network to effectively capture complex spatiotemporal patterns, and
adopts a prioritized experience replay mechanism to enhance learning
efficiency. A simulator based on a real-world dataset is developed to validate
the effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate
that pi-DDPG achieves superior learning efficiency and significantly reduces
early-stage training losses.

</details>


### [83] [Imbalanced Regression Pipeline Recommendation](https://arxiv.org/abs/2507.11901)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 本文提出Meta - IR框架解决不平衡回归问题，有独立和链式两种形式，链式表现更优，优于AutoML框架和多种基线配置。


<details>
  <summary>Details</summary>
Motivation: 不平衡问题在回归任务中有挑战，确定最佳的重采样方法和学习模型组合需大量测试，且受多种因素影响。

Method: 提出Meta - IR框架，训练元分类器以零样本方式推荐最佳管道，有独立和链式两种训练方式。

Result: 链式场景表现更好，Meta - IR比AutoML框架和42种基线配置效果更优。

Conclusion: Meta - IR框架在解决不平衡回归问题上有效，链式训练方式能体现学习算法和重采样策略间的关系。

Abstract: Imbalanced problems are prevalent in various real-world scenarios and are
extensively explored in classification tasks. However, they also present
challenges for regression tasks due to the rarity of certain target values. A
common alternative is to employ balancing algorithms in preprocessing to
address dataset imbalance. However, due to the variety of resampling methods
and learning models, determining the optimal solution requires testing many
combinations. Furthermore, the learning model, dataset, and evaluation metric
affect the best strategies. This work proposes the Meta-learning for Imbalanced
Regression (Meta-IR) framework, which diverges from existing literature by
training meta-classifiers to recommend the best pipeline composed of the
resampling strategy and learning model per task in a zero-shot fashion. The
meta-classifiers are trained using a set of meta-features to learn how to map
the meta-features to the classes indicating the best pipeline. We propose two
formulations: Independent and Chained. Independent trains the meta-classifiers
to separately indicate the best learning algorithm and resampling strategy.
Chained involves a sequential procedure where the output of one meta-classifier
is used as input for another to model intrinsic relationship factors. The
Chained scenario showed superior performance, suggesting a relationship between
the learning algorithm and the resampling strategy per task. Compared with
AutoML frameworks, Meta-IR obtained better results. Moreover, compared with
baselines of six learning algorithms and six resampling algorithms plus no
resampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of
them. The code, data, and further information of the experiments can be found
on GitHub: https://github.com/JusciAvelino/Meta-IR.

</details>


### [84] [Resampling strategies for imbalanced regression: a survey and empirical analysis](https://arxiv.org/abs/2507.11902)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 文章对不平衡回归问题进行实验研究，提出分类法，给出策略优势与后续研究方向，相关资源在GitHub。


<details>
  <summary>Details</summary>
Motivation: 解决现实中不平衡问题，现有研究多在分类领域，回归任务中同样存在该问题。

Method: 进行包含多种平衡和预测模型的实验研究，使用指标评估预测模型，基于回归模型、学习过程和评估指标提出分类法。

Result: 为使用策略带来新见解，凸显各模型学习过程优势。

Conclusion: 给出后续研究方向，相关实验代码、数据等可在GitHub获取。

Abstract: Imbalanced problems can arise in different real-world situations, and to
address this, certain strategies in the form of resampling or balancing
algorithms are proposed. This issue has largely been studied in the context of
classification, and yet, the same problem features in regression tasks, where
target values are continuous. This work presents an extensive experimental
study comprising various balancing and predictive models, and wich uses metrics
to capture important elements for the user and to evaluate the predictive model
in an imbalanced regression data context. It also proposes a taxonomy for
imbalanced regression approaches based on three crucial criteria: regression
model, learning process, and evaluation metrics. The study offers new insights
into the use of such strategies, highlighting the advantages they bring to each
model's learning process, and indicating directions for further studies. The
code, data and further information related to the experiments performed herein
can be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.

</details>


### [85] [From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning](https://arxiv.org/abs/2507.11926)
*Max Hopkins,Sihan Liu,Christopher Ye,Yuichi Yoshida*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The epidemic failure of replicability across empirical science and machine
learning has recently motivated the formal study of replicable learning
algorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from
a fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the
design of data-efficient replicable algorithms is now more or less understood.
In contrast, there remain significant gaps in our knowledge for control
settings like reinforcement learning where an agent must interact directly with
a shifting environment. Karbasi et. al show that with access to a generative
model of an environment with $S$ states and $A$ actions (the RL 'batch
setting'), replicably learning a near-optimal policy costs only
$\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a
generative model jumps to $\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the
substantial difficulty of environment exploration. This gap raises a key
question in the broader theory of replicability: Is replicable exploration
inherently more expensive than batch learning? Is sample-efficient replicable
RL even possible?
  In this work, we (nearly) resolve this problem (for low-horizon tabular
MDPs): exploration is not a significant barrier to replicable learning! Our
main result is a replicable RL algorithm on $\tilde{O}(S^2A)$ samples, bridging
the gap between the generative and episodic settings. We complement this with a
matching $\tilde{\Omega}(S^2A)$ lower bound in the generative setting (under
the common parallel sampling assumption) and an unconditional lower bound in
the episodic setting of $\tilde{\Omega}(S^2)$ showcasing the near-optimality of
our algorithm with respect to the state space $S$.

</details>


### [86] [Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning](https://arxiv.org/abs/2507.11928)
*Abhishek Sriram,Neal Tuffy*

Main category: cs.LG

TL;DR: 本文提出机器学习加速优化框架用于RF功率放大器设计，减少65%仿真需求，维持精度，还降低仿真时间。


<details>
  <summary>Details</summary>
Motivation: 减少RF功率放大器设计中的仿真需求，实现快速设计迭代且不降低生产RF电路所需精度。

Method: 结合MaxMin拉丁超立方采样与CatBoost梯度提升，智能探索多维参数空间，选择约35%关键仿真点，处理ADS网表，执行谐波平衡仿真并训练CatBoost模型。

Result: 在15种PA工作模式验证，平均$R^2$为0.901，系统对参数组合按满足目标规格的可能性排序，仿真时间减少58.24% - 77.78%。

Conclusion: 该框架能在不降低精度的情况下减少仿真需求和时间，实现RF功率放大器的快速设计迭代。

Abstract: This paper presents a machine learning-accelerated optimization framework for
RF power amplifier design that reduces simulation requirements by 65% while
maintaining $\pm0.3$ to $\pm0.4$ dBm accuracy. The proposed method combines
MaxMin Latin Hypercube Sampling with CatBoost gradient boosting to
intelligently explore multidimensional parameter spaces. Instead of
exhaustively simulating all parameter combinations to achieve target P2dB
compression specifications, our approach strategically selects approximately
35% of critical simulation points. The framework processes ADS netlists,
executes harmonic balance simulations on the reduced dataset, and trains a
CatBoost model to predict P2dB performance across the entire design space.
Validation across 15 PA operating modes yields an average $R^2$ of 0.901, with
the system ranking parameter combinations by their likelihood of meeting target
specifications. The integrated solution delivers 58.24% to 77.78% reduction in
simulation time through automated GUI-based workflows, enabling rapid design
iterations without compromising accuracy standards required for production RF
circuits.

</details>


### [87] [Online Training and Pruning of Deep Reinforcement Learning Networks](https://arxiv.org/abs/2507.11975)
*Valentin Frank Ingmar Guenter,Athanasios Sideris*

Main category: cs.LG

TL;DR: 提出在强化学习中集成训练和剪枝的方法，在连续控制基准测试中证明可大幅剪枝且性能损失小，训练中剪大网络更高效。


<details>
  <summary>Details</summary>
Motivation: 扩展强化学习算法的深度神经网络会增加计算和内存复杂度，而神经网络剪枝方法在强化学习中的应用未充分探索。

Method: 提出将同时训练和剪枝集成到高级强化学习方法中，训练网络解决随机优化问题，提出成本感知、促进稀疏性的正则化方案。

Result: 在连续控制基准测试和Soft Actor - Critic RL智能体上评估，表明可大幅剪枝且性能损失小，训练中剪大网络比从头训练小网络更高效。

Conclusion: 所提方法能有效结合强化学习目标和网络压缩，在训练中剪枝大网络可产生更高效、性能更好的强化学习智能体。

Abstract: Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms
has been shown to enhance performance when feature extraction networks are used
but the gained performance comes at the significant expense of increased
computational and memory complexity. Neural network pruning methods have
successfully addressed this challenge in supervised learning. However, their
application to RL is underexplored. We propose an approach to integrate
simultaneous training and pruning within advanced RL methods, in particular to
RL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our
networks (XiNet) are trained to solve stochastic optimization problems over the
RL networks' weights and the parameters of variational Bernoulli distributions
for 0/1 Random Variables $\xi$ scaling each unit in the networks. The
stochastic problem formulation induces regularization terms that promote
convergence of the variational parameters to 0 when a unit contributes little
to the performance. In this case, the corresponding structure is rendered
permanently inactive and pruned from its network. We propose a cost-aware,
sparsity-promoting regularization scheme, tailored to the DenseNet architecture
of OFENets expressing the parameter complexity of involved networks in terms of
the parameters of the RVs in these networks. Then, when matching this cost with
the regularization terms, the many hyperparameters associated with them are
automatically selected, effectively combining the RL objectives and network
compression. We evaluate our method on continuous control benchmarks (MuJoCo)
and the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned
considerably with minimal loss in performance. Furthermore, our results confirm
that pruning large networks during training produces more efficient and higher
performing RL agents rather than training smaller networks from scratch.

</details>


### [88] [Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection](https://arxiv.org/abs/2507.11997)
*Tairan Huang,Yili Wang*

Main category: cs.LG

TL;DR: 提出MLED框架利用LLM提取文本信息中的外部知识增强图欺诈检测，在四个真实数据集实验中表现达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有图欺诈检测方法忽略原始文本信息语义线索，且将处理后的文本嵌入与图结构进行多模态融合存在挑战。

Method: 提出MLED框架，设计类型级增强器和关系级增强器来融合LLM与图结构信息。

Result: 在四个真实数据集上实验表明MLED作为通用框架在图欺诈检测中达到了当前最优性能。

Conclusion: MLED是一个可应用于现有方法的通用框架，能有效提升图欺诈检测性能。

Abstract: Graph fraud detection has garnered significant attention as Graph Neural
Networks (GNNs) have proven effective in modeling complex relationships within
multimodal data. However, existing graph fraud detection methods typically use
preprocessed node embeddings and predefined graph structures to reveal
fraudsters, which ignore the rich semantic cues contained in raw textual
information. Although Large Language Models (LLMs) exhibit powerful
capabilities in processing textual information, it remains a significant
challenge to perform multimodal fusion of processed textual embeddings with
graph structures. In this paper, we propose a \textbf{M}ulti-level \textbf{L}LM
\textbf{E}nhanced Graph Fraud \textbf{D}etection framework called MLED. In
MLED, we utilize LLMs to extract external knowledge from textual information to
enhance graph fraud detection methods. To integrate LLMs with graph structure
information and enhance the ability to distinguish fraudsters, we design a
multi-level LLM enhanced framework including type-level enhancer and
relation-level enhancer. One is to enhance the difference between the
fraudsters and the benign entities, the other is to enhance the importance of
the fraudsters in different relations. The experiments on four real-world
datasets show that MLED achieves state-of-the-art performance in graph fraud
detection as a generalized framework that can be applied to existing methods.

</details>


### [89] [Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing](https://arxiv.org/abs/2507.12002)
*Alice Zhang,Callihan Bertley,Dawei Liang,Edison Thomaz*

Main category: cs.LG

TL;DR: 本文提出一种用智能手表音频和惯性数据检测面对面言语交流的计算方法，通过实验评估，框架在实验室和半自然场景中分别达到一定的F1分数。


<details>
  <summary>Details</summary>
Motivation: 社交互动对人类行为等至关重要，旨在开发一种在声学复杂场景中检测面对面言语交流的计算方法。

Method: 利用智能手表捕获的音频和惯性数据，分析机器学习和深度学习模型的3种不同融合方法，开展实验室研究和半自然主义研究。

Result: 框架在实验室检测对话时宏观F1分数为82.0±3.0%，在半自然场景为77.2±1.8%。

Conclusion: 融合音频和惯性数据考虑言语和非言语线索有优势，多模态传感在特定场景有好处。

Abstract: Social interactions play a crucial role in shaping human behavior,
relationships, and societies. It encompasses various forms of communication,
such as verbal conversation, non-verbal gestures, facial expressions, and body
language. In this work, we develop a novel computational approach to detect a
foundational aspect of human social interactions, in-person verbal
conversations, by leveraging audio and inertial data captured with a commodity
smartwatch in acoustically-challenging scenarios. To evaluate our approach, we
conducted a lab study with 11 participants and a semi-naturalistic study with
24 participants. We analyzed machine learning and deep learning models with 3
different fusion methods, showing the advantages of fusing audio and inertial
data to consider not only verbal cues but also non-verbal gestures in
conversations. Furthermore, we perform a comprehensive set of evaluations
across activities and sampling rates to demonstrate the benefits of multimodal
sensing in specific contexts. Overall, our framework achieved 82.0$\pm$3.0%
macro F1-score when detecting conversations in the lab and 77.2$\pm$1.8% in the
semi-naturalistic setting.

</details>


### [90] [DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning](https://arxiv.org/abs/2507.12011)
*Yao Lu,Hongyu Gao,Zhuangzhi Chen,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: 针对自动调制识别中数据稀缺问题，提出DUSE数据扩展框架，实验显示其优于多个基线且有强泛化性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自动调制识别中训练需大量标注数据，但实际目标域数据稀缺，手动标注成本高，数据增强无法根本解决问题。

Method: 引入DUSE框架，用不确定性评分函数从相关数据集筛选有用样本，用主动学习策略优化评分器。

Result: DUSE在类别平衡和不平衡设置中均优于8个核心集选择基线，对未见模型有强跨架构泛化性。

Conclusion: DUSE能有效解决自动调制识别中的数据稀缺问题，性能良好且泛化性强。

Abstract: Although deep neural networks have made remarkable achievements in the field
of automatic modulation recognition (AMR), these models often require a large
amount of labeled data for training. However, in many practical scenarios, the
available target domain data is scarce and difficult to meet the needs of model
training. The most direct way is to collect data manually and perform expert
annotation, but the high time and labor costs are unbearable. Another common
method is data augmentation. Although it can enrich training samples to a
certain extent, it does not introduce new data and therefore cannot
fundamentally solve the problem of data scarcity. To address these challenges,
we introduce a data expansion framework called Dynamic Uncertainty-driven
Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring
function to filter out useful samples from relevant AMR datasets and employs an
active learning strategy to continuously refine the scorer. Extensive
experiments demonstrate that DUSE consistently outperforms 8 coreset selection
baselines in both class-balance and class-imbalance settings. Besides, DUSE
exhibits strong cross-architecture generalization for unseen models.

</details>


### [91] [Granular feedback merits sophisticated aggregation](https://arxiv.org/abs/2507.12041)
*Anmol Kagrecha,Henrik Marklund,Potsawee Manakul,Richard Zeckhauser,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 研究在有限个体反馈下预测人群反馈分布，发现反馈粒度增加时，复杂方法比正则平均法预测效果更好，实证分析证实此模式。


<details>
  <summary>Details</summary>
Motivation: 在成本受限需用小群体反馈近似人群反馈分布情况下，探究能否比正则平均法有更好的预测方法。

Method: 理论分析结合使用社会态度问题进行实证分析。

Result: 对于二元反馈，复杂方法对减少达到固定性能所需个体数作用不大；对于五点反馈，复杂方法用约一半个体数就能达到正则平均法的性能。

Conclusion: 预测人群反馈分布时，反馈粒度增加，采用比正则平均法更复杂的方法能显著提升预测效果。

Abstract: Human feedback is increasingly used across diverse applications like training
AI models, developing recommender systems, and measuring public opinion -- with
granular feedback often being preferred over binary feedback for its greater
informativeness. While it is easy to accurately estimate a population's
distribution of feedback given feedback from a large number of individuals,
cost constraints typically necessitate using smaller groups. A simple method to
approximate the population distribution is regularized averaging: compute the
empirical distribution and regularize it toward a prior. Can we do better? As
we will discuss, the answer to this question depends on feedback granularity.
  Suppose one wants to predict a population's distribution of feedback using
feedback from a limited number of individuals. We show that, as feedback
granularity increases, one can substantially improve upon predictions of
regularized averaging by combining individuals' feedback in ways more
sophisticated than regularized averaging.
  Our empirical analysis using questions on social attitudes confirms this
pattern. In particular, with binary feedback, sophistication barely reduces the
number of individuals required to attain a fixed level of performance. By
contrast, with five-point feedback, sophisticated methods match the performance
of regularized averaging with about half as many individuals.

</details>


### [92] [Information-Theoretic Generalization Bounds of Replay-based Continual Learning](https://arxiv.org/abs/2507.12043)
*Wen Wen,Tieliang Gong,Yunjiao Zhang,Zeyu Gao,Weizhan Zhang,Yong-Jin Liu*

Main category: cs.LG

TL;DR: 本文为基于重放的持续学习建立统一理论框架，推导信息论界，实验验证界的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法虽经验性能好，但对基于重放方法的泛化行为理论理解有限。

Method: 建立统一理论框架，推导基于假设和预测的信息论界，以随机梯度朗之万动力学为例。

Result: 基于假设的界表明利用有限先前任务样本可促进泛化和缓解遗忘；基于预测的界产生更紧且易计算的泛化误差上界；实验验证界能捕捉泛化动态。

Conclusion: 所推导的界在基于重放的持续学习中有效，分析具有广泛适用性。

Abstract: Continual learning (CL) has emerged as a dominant paradigm for acquiring
knowledge from sequential tasks while avoiding catastrophic forgetting.
Although many CL methods have been proposed to show impressive empirical
performance, the theoretical understanding of their generalization behavior
remains limited, particularly for replay-based approaches. In this paper, we
establish a unified theoretical framework for replay-based CL, deriving a
series of information-theoretic bounds that explicitly characterize how the
memory buffer interacts with the current task to affect generalization.
Specifically, our hypothesis-based bounds reveal that utilizing the limited
exemplars of previous tasks alongside the current task data, rather than
exhaustive replay, facilitates improved generalization while effectively
mitigating catastrophic forgetting. Furthermore, our prediction-based bounds
yield tighter and computationally tractable upper bounds of the generalization
gap through the use of low-dimensional variables. Our analysis is general and
broadly applicable to a wide range of learning algorithms, exemplified by
stochastic gradient Langevin dynamics (SGLD) as a representative method.
Comprehensive experimental evaluations demonstrate the effectiveness of our
derived bounds in capturing the generalization dynamics in replay-based CL
settings.

</details>


### [93] [FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling](https://arxiv.org/abs/2507.12053)
*Seanglidet Yean,Jiazu Zhou,Bu-Sung Lee,Markus Schläpfer*

Main category: cs.LG

TL;DR: 本文引入新的数据驱动方法生成城市出行OD流，利用自适应因素和cGANs，无需大量校准数据，通过新加坡手机数据验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型依赖历史轨迹，机械方法假设静态场景，难以满足城市规划中模拟和分析人类出行模式的需求。

Method: 引入数据驱动方法，利用自适应因素如动态区域大小和土地利用原型，结合条件生成对抗网络（cGANs）融合历史数据和自适应参数。

Result: 应用于新加坡手机数据，与现有方法对比，显示出良好性能。

Conclusion: 该方法能快速生成出行OD流，空间粒度可调节，无需大量校准数据和复杂行为建模，具有应用潜力。

Abstract: The mobility patterns of people in cities evolve alongside changes in land
use and population. This makes it crucial for urban planners to simulate and
analyze human mobility patterns for purposes such as transportation
optimization and sustainable urban development. Existing generative models
borrowed from machine learning rely heavily on historical trajectories and
often overlook evolving factors like changes in population density and land
use. Mechanistic approaches incorporate population density and facility
distribution but assume static scenarios, limiting their utility for future
projections where historical data for calibration is unavailable. This study
introduces a novel, data-driven approach for generating origin-destination
mobility flows tailored to simulated urban scenarios. Our method leverages
adaptive factors such as dynamic region sizes and land use archetypes, and it
utilizes conditional generative adversarial networks (cGANs) to blend
historical data with these adaptive parameters. The approach facilitates rapid
mobility flow generation with adjustable spatial granularity based on regions
of interest, without requiring extensive calibration data or complex behavior
modeling. The promising performance of our approach is demonstrated by its
application to mobile phone data from Singapore, and by its comparison with
existing methods.

</details>


### [94] [Emergence of Quantised Representations Isolated to Anisotropic Functions](https://arxiv.org/abs/2507.12070)
*George Bird*

Main category: cs.LG

TL;DR: 本文基于Spotlight Resonance方法提出确定表征对齐的新方法，发现网络原语代数对称性可预测表征结构，揭示激活函数形式影响表征离散化，还指出表征量化与重构误差相关。


<details>
  <summary>Details</summary>
Motivation: 深入理解表征的形成和排列方式，研究功能形式选择对表征的影响，为涌现可解释性研究提供见解。

Method: 在现有Spotlight Resonance方法基础上开发新方法，进行仅改变激活函数的消融研究。

Result: 当激活函数由离散代数置换等变对称定义时，表征倾向于离散化；在连续代数正交等变定义下保持连续，且表征量化与重构误差增加相关。

Conclusion: 功能形式选择会产生意外归纳偏差，该工具和机制为涌现可解释性研究提供见解，表征量化可能有害。

Abstract: This paper describes a novel methodology for determining representational
alignment, developed upon the existing Spotlight Resonance method. Using this,
it is found that algebraic symmetries of network primitives are a strong
predictor for task-agnostic structure in representations. Particularly, this
new tool is used to gain insight into how discrete representations can form and
arrange in autoencoder models, through an ablation study where only the
activation function is altered. Representations are found to tend to discretise
when the activation functions are defined through a discrete algebraic
permutation-equivariant symmetry. In contrast, they remain continuous under a
continuous algebraic orthogonal-equivariant definition. These findings
corroborate the hypothesis that functional form choices can carry unintended
inductive biases which produce task-independent artefactual structures in
representations, particularly that contemporary forms induce discretisation of
otherwise continuous structure -- a quantisation effect. Moreover, this
supports a general causal model for one mode in which discrete representations
may form, and could constitute a prerequisite for downstream interpretability
phenomena, including grandmother neurons, discrete coding schemes, general
linear features and possibly Superposition. Hence, this tool and proposed
mechanism for the influence of functional form on representations may provide
several insights into emergent interpretability research. Finally, preliminary
results indicate that quantisation of representations appears to correlate with
a measurable increase in reconstruction error, reinforcing previous conjectures
that this collapse can be detrimental.

</details>


### [95] [Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks](https://arxiv.org/abs/2507.12127)
*Ngoc Duy Pham,Thusitha Dayaratne,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.LG

TL;DR: 无线设备增长致频谱稀缺，动态频谱分配是解决方案，本文解决基于联邦学习的频谱感知中的数据标签稀缺和安全漏洞问题，实验验证方案有效。


<details>
  <summary>Details</summary>
Motivation: 无线设备增多使频谱稀缺，集中式机器学习DSA系统有局限，分布式ML方法如联邦学习有潜力，但FLSS存在数据标签稀缺和安全漏洞问题。

Method: 用半监督FL结合能量检测解决数据标签稀缺问题；提出受疫苗启发的防御机制应对数据投毒攻击。

Result: 实验表明FLSS在无标签数据集上接近完美准确率，对数据投毒攻击保持拜占庭鲁棒性。

Conclusion: 所提方法能有效解决FLSS中的关键问题，为频谱感知提供可行方案。

Abstract: Advancements in wireless and mobile technologies, including 5G advanced and
the envisioned 6G, are driving exponential growth in wireless devices. However,
this rapid expansion exacerbates spectrum scarcity, posing a critical
challenge. Dynamic spectrum allocation (DSA)--which relies on sensing and
dynamically sharing spectrum--has emerged as an essential solution to address
this issue. While machine learning (ML) models hold significant potential for
improving spectrum sensing, their adoption in centralized ML-based DSA systems
is limited by privacy concerns, bandwidth constraints, and regulatory
challenges. To overcome these limitations, distributed ML-based approaches such
as Federated Learning (FL) offer promising alternatives. This work addresses
two key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of
labeled data for training FL models in practical spectrum sensing scenarios is
tackled with a semi-supervised FL approach, combined with energy detection,
enabling model training on unlabeled datasets. Second, we examine the security
vulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our
analysis highlights the shortcomings of existing majority-based defenses in
countering such attacks. To address these vulnerabilities, we propose a novel
defense mechanism inspired by vaccination, which effectively mitigates data
poisoning attacks without relying on majority-based assumptions. Extensive
experiments on both synthetic and real-world datasets validate our solutions,
demonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets
and maintain Byzantine robustness against both targeted and untargeted data
poisoning attacks, even when a significant proportion of participants are
malicious.

</details>


### [96] [HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD](https://arxiv.org/abs/2507.12133)
*Hanwen Liu,Yuhe Huang,Yifeng Gong,Yanjie Zhai,Jiaxuan Lu*

Main category: cs.LG

TL;DR: 本文提出HyDRA架构用于无线设备识别，结合优化VMD与CNN、Transformer和Mamba组件，在数据集评估中表现出色，在NVIDIA Jetson Xavier NX上实现低功耗实时推理。


<details>
  <summary>Details</summary>
Motivation: 设备识别对无线通信系统安全至关重要，射频指纹识别提供非加密解决方案，需要更好的方法支持闭集和开集分类任务。

Method: 提出HyDRA架构，融合优化VMD、CNN、Transformer和Mamba组件，使用TDSE进行全局依赖建模，MLFE进行线性复杂度处理。

Result: 在公共数据集上闭集场景达到SOTA准确率，开集分类方法表现稳健，在NVIDIA Jetson Xavier NX上实现毫秒级推理速度和低功耗。

Conclusion: HyDRA为现实环境中的实时无线认证提供了实用解决方案。

Abstract: Device recognition is vital for security in wireless communication systems,
particularly for applications like access control. Radio Frequency Fingerprint
Identification (RFFI) offers a non-cryptographic solution by exploiting
hardware-induced signal distortions. This paper proposes HyDRA, a Hybrid
Dual-mode RF Architecture that integrates an optimized Variational Mode
Decomposition (VMD) with a novel architecture based on the fusion of
Convolutional Neural Networks (CNNs), Transformers, and Mamba components,
designed to support both closed-set and open-set classification tasks. The
optimized VMD enhances preprocessing efficiency and classification accuracy by
fixing center frequencies and using closed-form solutions. HyDRA employs the
Transformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and
the Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting
to varying conditions. Evaluation on public datasets demonstrates
state-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance
in our proposed open-set classification method, effectively identifying
unauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves
millisecond-level inference speed with low power consumption, providing a
practical solution for real-time wireless authentication in real-world
environments.

</details>


### [97] [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
*Vladimir Bogachev,Vladimir Aletov,Alexander Molozhavenko,Denis Bobkov,Vera Soboleva,Aibek Alanov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出RiemannLoRA方法解决LoRA在参数高效微调大语言模型时的问题，实验显示其优于标准LoRA及其改进方法。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA在参数高效微调大语言模型时存在的寻找最优初始化策略和减轻低秩矩阵分解中过参数化的挑战。

Method: 将一组固定秩的LoRA矩阵视为光滑流形，考虑适配器作为流形上的元素以消除过参数化，确定沿流形损失下降最快的方向进行初始化，并采用数值线性代数和黎曼优化的最佳实践实现数值稳定和计算高效。

Result: 在大语言模型和扩散模型架构上的实验表明，RiemannLoRA在收敛速度和最终性能上始终优于标准LoRA及其最先进的改进方法。

Conclusion: RiemannLoRA是一种有效的解决LoRA现存问题的方法，能提升模型性能。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

</details>


### [98] [FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale](https://arxiv.org/abs/2507.12144)
*Boris Bonev,Thorsten Kurth,Ankur Mahesh,Mauro Bisson,Jean Kossaifi,Karthik Kashinath,Anima Anandkumar,William D. Collins,Michael S. Pritchard,Alexander Keller*

Main category: cs.LG

TL;DR: FourCastNet 3改进全球天气建模，用几何机器学习实现概率集合预报，性能超传统模型且速度快，适用于气象预报和预警系统。


<details>
  <summary>Details</summary>
Motivation: 改进全球天气建模，提升概率集合预报的准确性和效率。

Method: 采用适应球面几何的纯卷积神经网络架构，结合模型和数据并行的新训练范式。

Result: 预报准确性超传统集合模型，速度快8 - 60倍，概率校准出色，能保持真实光谱，单GPU推理高效。

Conclusion: FourCastNet 3因计算效率、概率技能等优势，适合用于改进气象预报和预警系统。

Abstract: FourCastNet 3 advances global weather modeling by implementing a scalable,
geometric machine learning (ML) approach to probabilistic ensemble forecasting.
The approach is designed to respect spherical geometry and to accurately model
the spatially correlated probabilistic nature of the problem, resulting in
stable spectra and realistic dynamics across multiple scales. FourCastNet 3
delivers forecasting accuracy that surpasses leading conventional ensemble
models and rivals the best diffusion-based methods, while producing forecasts 8
to 60 times faster than these approaches. In contrast to other ML approaches,
FourCastNet 3 demonstrates excellent probabilistic calibration and retains
realistic spectra, even at extended lead times of up to 60 days. All of these
advances are realized using a purely convolutional neural network architecture
tailored for spherical geometry. Scalable and efficient large-scale training on
1024 GPUs and more is enabled by a novel training paradigm for combined model-
and data-parallelism, inspired by domain decomposition methods in classical
numerical models. Additionally, FourCastNet 3 enables rapid inference on a
single GPU, producing a 90-day global forecast at 0.25{\deg}, 6-hourly
resolution in under 20 seconds. Its computational efficiency, medium-range
probabilistic skill, spectral fidelity, and rollout stability at subseasonal
timescales make it a strong candidate for improving meteorological forecasting
and early warning systems through large ensemble predictions.

</details>


### [99] [PRISM: Distributed Inference for Foundation Models at Edge](https://arxiv.org/abs/2507.12145)
*Muhammad Azlan Qazi,Alexandros Iosifidis,Qi Zhang*

Main category: cs.LG

TL;DR: 提出PRISM策略用于边缘设备上分布式Transformer推理，减少通信和计算开销，精度损失小。


<details>
  <summary>Details</summary>
Motivation: 基础模型在边缘部署有挑战，需开发实用高效策略将其引入边缘环境。

Method: 利用Segment Means表示近似中间输出特征，重组自注意力机制，设计分区感知因果掩码方案。

Result: 在多种数据集上评估，通信开销最多减少99.2%，单设备计算量减少51.24%，精度仅有轻微下降。

Conclusion: 该方法为在分布式资源受限环境部署基础模型提供可扩展实用方案。

Abstract: Foundation models (FMs) have achieved remarkable success across a wide range
of applications, from image classification to natural langurage processing, but
pose significant challenges for deployment at edge. This has sparked growing
interest in developing practical and efficient strategies for bringing
foundation models to edge environments. In this work, we propose PRISM, a
communication-efficient and compute-aware strategy for distributed Transformer
inference on edge devices. Our method leverages a Segment Means representation
to approximate intermediate output features, drastically reducing inter-device
communication. Additionally, we restructure the self-attention mechanism to
eliminate redundant computations caused by per-device Key/Value calculation in
position-wise partitioning and design a partition-aware causal masking scheme
tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2
across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and
CBT. Our results demonstrate substantial reductions in communication overhead
(up to 99.2% for BERT at compression rate CR = 128) and per-device computation
(51.24% for BERT at the same setting), with only minor accuracy degradation.
This method offers a scalable and practical solution for deploying foundation
models in distributed resource-constrained environments.

</details>


### [100] [Multi-Component VAE with Gaussian Markov Random Field](https://arxiv.org/abs/2507.12165)
*Fouad Oubari,Mohamed El-Baha,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: 提出GMRF MCVAE解决多组件数据集生成建模问题，实验表现良好，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有多组件变分自编码器依赖简化聚合策略，忽略关键细微差别，损害生成组件的结构连贯性。

Method: 引入高斯马尔可夫随机场多组件变分自编码器（GMRF MCVAE），将高斯马尔可夫随机场嵌入先验和后验分布，显式建模组件间关系。

Result: 在合成Copula数据集上达SOTA，在PolyMNIST基准测试有竞争力，在真实BIKED数据集上显著增强结构连贯性。

Conclusion: GMRF MCVAE特别适用于需要对多组件连贯性进行稳健和逼真建模的实际应用。

Abstract: Multi-component datasets with intricate dependencies, like industrial
assemblies or multi-modal imaging, challenge current generative modeling
techniques. Existing Multi-component Variational AutoEncoders typically rely on
simplified aggregation strategies, neglecting critical nuances and consequently
compromising structural coherence across generated components. To explicitly
address this gap, we introduce the Gaussian Markov Random Field Multi-Component
Variational AutoEncoder , a novel generative framework embedding Gaussian
Markov Random Fields into both prior and posterior distributions. This design
choice explicitly models cross-component relationships, enabling richer
representation and faithful reproduction of complex interactions. Empirically,
our GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula
dataset specifically constructed to evaluate intricate component relationships,
demonstrates competitive results on the PolyMNIST benchmark, and significantly
enhances structural coherence on the real-world BIKED dataset. Our results
indicate that the GMRF MCVAE is especially suited for practical applications
demanding robust and realistic modeling of multi-component coherence

</details>


### [101] [RadioDiff-3D: A 3D$\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication](https://arxiv.org/abs/2507.12166)
*Xiucheng Wang,Qiming Zhang,Nan Cheng,Junting Chen,Zezhong Zhang,Zan Li,Shuguang Cui,Xuemin Shen*

Main category: cs.LG

TL;DR: 提出UrbanRadio3D数据集和UNet、RadioDiff - 3D方法用于3D无线电地图构建，评估显示RadioDiff - 3D性能优越，提供基础数据集和基准。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的无线电地图构建方法多关注2D平面路径损耗预测，忽略关键参数，且依赖静态学习范式缺乏泛化性。

Method: 构建UrbanRadio3D数据集，提出用3D卷积算子的UNet进行基准测试，引入基于扩散模型的RadioDiff - 3D生成框架。

Result: 在UrbanRadio3D上广泛评估表明，RadioDiff - 3D在不同环境动态下构建高维无线电地图性能优越。

Conclusion: 本工作为3D环境感知通信的未来研究提供了基础数据集和基准。

Abstract: Radio maps (RMs) serve as a critical foundation for enabling
environment-aware wireless communication, as they provide the spatial
distribution of wireless channel characteristics. Despite recent progress in RM
construction using data-driven approaches, most existing methods focus solely
on pathloss prediction in a fixed 2D plane, neglecting key parameters such as
direction of arrival (DoA), time of arrival (ToA), and vertical spatial
variations. Such a limitation is primarily due to the reliance on static
learning paradigms, which hinder generalization beyond the training data
distribution. To address these challenges, we propose UrbanRadio3D, a
large-scale, high-resolution 3D RM dataset constructed via ray tracing in
realistic urban environments. UrbanRadio3D is over 37$\times$3 larger than
previous datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,
forming a novel 3D$\times$33D dataset with 7$\times$3 more height layers than
prior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet
with 3D convolutional operators is proposed. Moreover, we further introduce
RadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D
convolutional architecture. RadioDiff-3D supports both radiation-aware
scenarios with known transmitter locations and radiation-unaware settings based
on sparse spatial observations. Extensive evaluations on UrbanRadio3D validate
that RadioDiff-3D achieves superior performance in constructing rich,
high-dimensional radio maps under diverse environmental dynamics. This work
provides a foundational dataset and benchmark for future research in 3D
environment-aware communication. The dataset is available at
https://github.com/UNIC-Lab/UrbanRadio3D.

</details>


### [102] [Explainable Evidential Clustering](https://arxiv.org/abs/2507.12192)
*Victor F. Lopes de Souza,Karima Bakhti,Sofiane Ramdani,Denis Mottet,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 本文探讨证据聚类结果解释问题，提出IEMM算法并验证，能在多数情况下提供令决策者满意的解释


<details>
  <summary>Details</summary>
Motivation: 现实数据有不确定性和不精确性，传统方法难处理，且证据聚类结果解释问题研究不足，在高风险领域很关键

Method: 分析得出代表性是决策树作为溯因解释器的充要条件，通过效用函数推广代表性概念，定义证据错误性为解释成本，构建解释器，提出IEMM算法

Result: 在合成和真实数据上验证了IEMM算法，考虑决策者偏好时，高达93%的时间能提供满意解释

Conclusion: IEMM算法可为证据聚类函数提供可解释且谨慎的决策树解释

Abstract: Unsupervised classification is a fundamental machine learning problem.
Real-world data often contain imperfections, characterized by uncertainty and
imprecision, which are not well handled by traditional methods. Evidential
clustering, based on Dempster-Shafer theory, addresses these challenges. This
paper explores the underexplored problem of explaining evidential clustering
results, which is crucial for high-stakes domains such as healthcare. Our
analysis shows that, in the general case, representativity is a necessary and
sufficient condition for decision trees to serve as abductive explainers.
Building on the concept of representativity, we generalize this idea to
accommodate partial labeling through utility functions. These functions enable
the representation of "tolerable" mistakes, leading to the definition of
evidential mistakeness as explanation cost and the construction of explainers
tailored to evidential classifiers. Finally, we propose the Iterative
Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable
and cautious decision tree explanations for evidential clustering functions. We
validate the proposed algorithm on synthetic and real-world data. Taking into
account the decision-maker's preferences, we were able to provide an
explanation that was satisfactory up to 93% of the time.

</details>


### [103] [Selective Quantization Tuning for ONNX Models](https://arxiv.org/abs/2507.12196)
*Nikolaos Louloudakis,Ajitha Rajan*

Main category: cs.LG

TL;DR: 提出TuneQn工具实现ONNX模型选择性量化，经评估可有效减少精度损失和模型大小。


<details>
  <summary>Details</summary>
Motivation: 全量化模型性能不佳且在低端硬件部署有挑战，选择不量化的层有难度。

Method: 提出TuneQn工具，生成选择性量化的ONNX模型，在不同硬件部署，测量性能指标，进行Pareto Front最小化。

Result: 在四个ONNX模型上评估，与全量化模型相比精度损失最多降低54.14%，与原模型相比模型大小最多减少72.9%。

Conclusion: TuneQn能有效进行选择性量化和调优。

Abstract: Quantization is a process that reduces the precision of deep neural network
models to lower model size and computational demands, often at the cost of
accuracy. However, fully quantized models may exhibit sub-optimal performance
below acceptable levels and face deployment challenges on low-end hardware
accelerators due to practical constraints. To address these issues,
quantization can be selectively applied to only a subset of layers, but
selecting which layers to exclude is non-trivial. To this direction, we propose
TuneQn, a suite enabling selective quantization, deployment and execution of
ONNX models across various CPU and GPU devices, combined with profiling and
multi-objective optimization. TuneQn generates selectively quantized ONNX
models, deploys them on different hardware, measures performance on metrics
like accuracy and size, performs Pareto Front minimization to identify the best
model candidate and visualizes the results. To demonstrate the effectiveness of
TuneQn, we evaluated TuneQn on four ONNX models with two quantization settings
across CPU and GPU devices. As a result, we demonstrated that our utility
effectively performs selective quantization and tuning, selecting ONNX model
candidates with up to a $54.14$% reduction in accuracy loss compared to the
fully quantized model, and up to a $72.9$% model size reduction compared to the
original model.

</details>


### [104] [Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation](https://arxiv.org/abs/2507.12218)
*Tomohisa Okazaki*

Main category: cs.LG

TL;DR: 研究物理信息线性模型（PILM）用于解决线性偏微分方程的正反问题，还用于估计地壳应变率，数学正则化表现更优，PILM提供可解析框架。


<details>
  <summary>Details</summary>
Motivation: 解决偏微分方程并从观测数据估计其系数或边界条件对理解相关现象至关重要，探索新方法。

Method: 提出物理信息线性模型（PILM），用基函数线性组合表示解，针对正反问题进行公式化和验证，比较物理正则化和数学正则化。

Result: PILM可用于线性正反问题、欠定系统和物理正则化，从贝叶斯角度，数学正则化性能更优。

Conclusion: PILM为线性正反问题、欠定系统和物理正则化提供了一个可解析求解的框架。

Abstract: Many physical systems are described by partial differential equations (PDEs),
and solving these equations and estimating their coefficients or boundary
conditions (BCs) from observational data play a crucial role in understanding
the associated phenomena. Recently, a machine learning approach known as
physics-informed neural network, which solves PDEs using neural networks by
minimizing the sum of residuals from the PDEs, BCs, and data, has gained
significant attention in the scientific community. In this study, we
investigate a physics-informed linear model (PILM) that uses linear
combinations of basis functions to represent solutions, thereby enabling an
analytical representation of optimal solutions. The PILM was formulated and
verified for illustrative forward and inverse problems including cases with
uncertain BCs. Furthermore, the PILM was applied to estimate crustal strain
rates using geodetic data. Specifically, physical regularization that enforces
elastic equilibrium on the velocity fields was compared with mathematical
regularization that imposes smoothness constraints. From a Bayesian
perspective, mathematical regularization exhibited superior performance. The
PILM provides an analytically solvable framework applicable to linear forward
and inverse problems, underdetermined systems, and physical regularization.

</details>


### [105] [Optimizers Qualitatively Alter Solutions And We Should Leverage This](https://arxiv.org/abs/2507.12224)
*Razvan Pascanu,Clare Lyle,Ionut-Vlad Modoranu,Naima Elosegui Borras,Dan Alistarh,Petar Velickovic,Sarath Chandar,Soham De,James Martens*

Main category: cs.LG

TL;DR: 指出深度学习界在优化器设计上过于关注收敛速度，应重视优化器对学习解性质的影响并设计新优化器。


<details>
  <summary>Details</summary>
Motivation: 深度学习中优化器设计多关注收敛效率，而优化器对学习解性质的影响被忽视。

Method: 理论分析，论述优化器不仅影响收敛速度，还影响学习解的性质。

Result: 强调优化器能编码归纳偏置、改变模型有效表达能力，可在学习过程中编码期望属性。

Conclusion: 呼吁学界理解现有方法的偏置，构建能诱导解特定性质的新优化器，重视优化器设计对塑造模型结果的作用。

Abstract: Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not
guarantee convergence to a unique global minimum of the loss when using
optimizers relying only on local information, such as SGD. Indeed, this was a
primary source of skepticism regarding the feasibility of DNNs in the early
days of the field. The past decades of progress in deep learning have revealed
this skepticism to be misplaced, and a large body of empirical evidence shows
that sufficiently large DNNs following standard training protocols exhibit
well-behaved optimization dynamics that converge to performant solutions. This
success has biased the community to use convex optimization as a mental model
for learning, leading to a focus on training efficiency, either in terms of
required iteration, FLOPs or wall-clock time, when improving optimizers. We
argue that, while this perspective has proven extremely fruitful, another
perspective specific to DNNs has received considerably less attention: the
optimizer not only influences the rate of convergence, but also the qualitative
properties of the learned solutions. Restated, the optimizer can and will
encode inductive biases and change the effective expressivity of a given class
of models. Furthermore, we believe the optimizer can be an effective way of
encoding desiderata in the learning process. We contend that the community
should aim at understanding the biases of already existing methods, as well as
aim to build new optimizers with the explicit intent of inducing certain
properties of the solution, rather than solely judging them based on their
convergence rates. We hope our arguments will inspire research to improve our
understanding of how the learning process can impact the type of solution we
converge to, and lead to a greater recognition of optimizers design as a
critical lever that complements the roles of architecture and data in shaping
model outcomes.

</details>


### [106] [RegCL: Continual Adaptation of Segment Anything Model via Model Merging](https://arxiv.org/abs/2507.12297)
*Yuan-Chen Shu,Zhiwei Lin,Yongtao Wang*

Main category: cs.LG

TL;DR: 为解决SAM在特定领域的性能局限及灾难性遗忘问题，提出RegCL框架，通过模型合并集成多领域知识，实验证明其在多下游数据集上有效。


<details>
  <summary>Details</summary>
Motivation: 解决SAM在特定领域性能局限，以及现有基于适配器的一步适应范式存在的灾难性遗忘问题，提升模型可扩展性。

Method: 提出RegCL框架，将模型合并算法融入持续学习范式，合并不同领域训练的SAM适配模块参数，通过权重优化引导合并过程。

Result: RegCL在多个下游数据集上取得良好的持续学习性能。

Conclusion: RegCL能有效集成多领域知识，保持参数效率，在动态场景中有效。

Abstract: To address the performance limitations of the Segment Anything Model (SAM) in
specific domains, existing works primarily adopt adapter-based one-step
adaptation paradigms. However, some of these methods are specific developed for
specific domains. If used on other domains may lead to performance degradation.
This issue of catastrophic forgetting severely limits the model's scalability.
To address this issue, this paper proposes RegCL, a novel non-replay continual
learning (CL) framework designed for efficient multi-domain knowledge
integration through model merging. Specifically, RegCL incorporates the model
merging algorithm into the continual learning paradigm by merging the
parameters of SAM's adaptation modules (e.g., LoRA modules) trained on
different domains. The merging process is guided by weight optimization, which
minimizes prediction discrepancies between the merged model and each of the
domain-specific models. RegCL effectively consolidates multi-domain knowledge
while maintaining parameter efficiency, i.e., the model size remains constant
regardless of the number of tasks, and no historical data storage is required.
Experimental results demonstrate that RegCL achieves favorable continual
learning performance across multiple downstream datasets, validating its
effectiveness in dynamic scenarios.

</details>


### [107] [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/abs/2507.12305)
*M. Anwar Ma'sum,Mahardhika Pratama,Savitha Ramasamy,Lin Liu,Habibullah Habibullah,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 提出用于在线持续学习的新基于提示方法，性能超当前SOTA，参数少且训练等时间适中。


<details>
  <summary>Details</summary>
Motivation: 在线持续学习中数据隐私约束使灾难性遗忘问题复杂化，现有方法存在数据开放性和吞吐量问题。

Method: 提出包含单轻量级提示生成器、可训练缩放平移器、预训练模型泛化保留和硬 - 软更新机制的新基于提示方法。

Result: 在CIFAR100等数据集上性能显著高于当前SOTA，复杂度分析显示所需参数较少，训练、推理时间和吞吐量适中。

Conclusion: 所提方法有效，代码开源。

Abstract: The data privacy constraint in online continual learning (OCL), where the
data can be seen only once, complicates the catastrophic forgetting problem in
streaming data. A common approach applied by the current SOTAs in OCL is with
the use of memory saving exemplars or features from previous classes to be
replayed in the current task. On the other hand, the prompt-based approach
performs excellently in continual learning but with the cost of a growing
number of trainable parameters. The first approach may not be applicable in
practice due to data openness policy, while the second approach has the issue
of throughput associated with the streaming data. In this study, we propose a
novel prompt-based method for online continual learning that includes 4 main
components: (1) single light-weight prompt generator as a general knowledge,
(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model
(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our
proposed method achieves significantly higher performance than the current
SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity
analysis shows that our method requires a relatively smaller number of
parameters and achieves moderate training time, inference time, and throughput.
For further study, the source code of our method is available at
https://github.com/anwarmaxsum/PROL.

</details>


### [108] [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
*Antoine Saillenfest,Pirmin Lemberger*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Ensuring that neural models used in real-world applications cannot infer
sensitive information, such as demographic attributes like gender or race, from
text representations is a critical challenge when fairness is a concern. We
address this issue through concept erasure, a process that removes information
related to a specific concept from distributed representations while preserving
as much of the remaining semantic information as possible. Our approach
involves learning an orthogonal projection in the embedding space, designed to
make the class-conditional feature distributions of the discrete concept to
erase indistinguishable after projection. By adjusting the rank of the
projector, we control the extent of information removal, while its
orthogonality ensures strict preservation of the local structure of the
embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves
state-of-the-art performance in nonlinear erasure of a discrete attribute on
classic natural language processing benchmarks. Furthermore, we demonstrate
that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear
classifiers, thereby promoting fairness.

</details>


### [109] [Heat Kernel Goes Topological](https://arxiv.org/abs/2507.12380)
*Maximilian Krahn,Vikas Garg*

Main category: cs.LG

TL;DR: 提出新拓扑框架解决拓扑神经网络计算成本高问题，理论表达强且计算高效，在多方面表现出色，推动拓扑深度学习。


<details>
  <summary>Details</summary>
Motivation: 拓扑神经网络高阶消息传递计算成本高，需解决该问题。

Method: 引入组合复形上的拉普拉斯算子，高效计算作为节点描述符的热核，捕获多尺度信息。

Result: 理论上能区分任意非同构组合复形，计算效率超现有拓扑方法，在分子数据集表现好，区分复杂拓扑结构能力强。

Conclusion: 工作为拓扑深度学习提供有表达力且可扩展的表示，为分子分类和属性预测任务开辟新途径。

Abstract: Topological neural networks have emerged as powerful successors of graph
neural networks. However, they typically involve higher-order message passing,
which incurs significant computational expense. We circumvent this issue with a
novel topological framework that introduces a Laplacian operator on
combinatorial complexes (CCs), enabling efficient computation of heat kernels
that serve as node descriptors. Our approach captures multiscale information
and enables permutation-equivariant representations, allowing easy integration
into modern transformer-based architectures.
  Theoretically, the proposed method is maximally expressive because it can
distinguish arbitrary non-isomorphic CCs. Empirically, it significantly
outperforms existing topological methods in terms of computational efficiency.
Besides demonstrating competitive performance with the state-of-the-art
descriptors on standard molecular datasets, it exhibits superior capability in
distinguishing complex topological structures and avoiding blind spots on
topological benchmarks. Overall, this work advances topological deep learning
by providing expressive yet scalable representations, thereby opening up
exciting avenues for molecular classification and property prediction tasks.

</details>


### [110] [Improving Reinforcement Learning Sample-Efficiency using Local Approximation](https://arxiv.org/abs/2507.12383)
*Mohit Prashant,Arvind Easwaran*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this study, we derive Probably Approximately Correct (PAC) bounds on the
asymptotic sample-complexity for RL within the infinite-horizon Markov Decision
Process (MDP) setting that are sharper than those in existing literature. The
premise of our study is twofold: firstly, the further two states are from each
other, transition-wise, the less relevant the value of the first state is when
learning the $\epsilon$-optimal value of the second; secondly, the amount of
'effort', sample-complexity-wise, expended in learning the $\epsilon$-optimal
value of a state is independent of the number of samples required to learn the
$\epsilon$-optimal value of a second state that is a sufficient number of
transitions away from the first. Inversely, states within each other's vicinity
have values that are dependent on each other and will require a similar number
of samples to learn. By approximating the original MDP using smaller MDPs
constructed using subsets of the original's state-space, we are able to reduce
the sample-complexity by a logarithmic factor to $O(SA \log A)$ timesteps,
where $S$ and $A$ are the state and action space sizes. We are able to extend
these results to an infinite-horizon, model-free setting by constructing a
PAC-MDP algorithm with the aforementioned sample-complexity. We conclude with
showing how significant the improvement is by comparing our algorithm against
prior work in an experimental setting.

</details>


### [111] [Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries](https://arxiv.org/abs/2507.12384)
*Bo Wen,Guoyun Gao,Zhicheng Xu,Ruibin Mao,Xiaojuan Qi,X. Sharon Hu,Xunzhao Yin,Can Li*

Main category: cs.LG

TL;DR: 本文提出基于MoS₂ Flash的模拟内容可寻址内存（CAM）的软硬件协同设计方法，用于软树模型高效推理，在准确性、鲁棒性和可解释性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 人工智能的可信赖性受关注，树模型虽适合表格数据但扩展计算成本高，以往用模拟CAM加速模型存在硬件性能差和易受攻击等问题。

Method: 采用基于MoS₂ Flash的模拟CAM进行软硬件协同设计，实现软树模型的高效推理。

Result: 在MoS₂模拟CAM阵列上的实验表明，该方法对设备变化和对抗攻击有出色鲁棒性，在WDBC数据库达到96%准确率，MNIST数据集在10%设备阈值变化下准确率仅下降0.6%。

Conclusion: 该工作为增强人工智能可信赖性和效率的专用硬件铺平道路。

Abstract: The rapid advancement of artificial intelligence has raised concerns
regarding its trustworthiness, especially in terms of interpretability and
robustness. Tree-based models like Random Forest and XGBoost excel in
interpretability and accuracy for tabular data, but scaling them remains
computationally expensive due to poor data locality and high data dependence.
Previous efforts to accelerate these models with analog content addressable
memory (CAM) have struggled, due to the fact that the difficult-to-implement
sharp decision boundaries are highly susceptible to device variations, which
leads to poor hardware performance and vulnerability to adversarial attacks.
This work presents a novel hardware-software co-design approach using $MoS_2$
Flash-based analog CAM with inherent soft boundaries, enabling efficient
inference with soft tree-based models. Our soft tree model inference
experiments on $MoS_2$ analog CAM arrays show this method achieves exceptional
robustness against device variation and adversarial attacks while achieving
state-of-the-art accuracy. Specifically, our fabricated analog CAM arrays
achieve $96\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,
while maintaining decision explainability. Our experimentally calibrated model
validated only a $0.6\%$ accuracy drop on the MNIST dataset under $10\%$ device
threshold variation, compared to a $45.3\%$ drop for traditional decision
trees. This work paves the way for specialized hardware that enhances AI's
trustworthiness and efficiency.

</details>


### [112] [NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data](https://arxiv.org/abs/2507.12412)
*Dzung Dinh,Boqi Chen,Marc Niethammer,Junier Oliva*

Main category: cs.LG

TL;DR: 提出NOCTA方法解决资源受限下信息采集问题，在合成和医疗数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 关键应用中资源约束限制信息采集量，时间预测任务增加决策复杂性，需有效采集信息方法。

Method: 提出NOCTA方法，引入估计目标，开发非参数（NOCTA - NP）和参数（NOCTA - P）两种互补估计器。

Result: 在合成和真实医疗数据集实验中，NOCTA两个变体均优于现有基线。

Conclusion: NOCTA方法在考虑时间动态和采集成本情况下，能有效采集最具信息特征。

Abstract: In many critical applications, resource constraints limit the amount of
information that can be gathered to make predictions. For example, in
healthcare, patient data often spans diverse features ranging from lab tests to
imaging studies. Each feature may carry different information and must be
acquired at a respective cost of time, money, or risk to the patient. Moreover,
temporal prediction tasks, where both instance features and labels evolve over
time, introduce additional complexity in deciding when or what information is
important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff
Acquisition method that sequentially acquires the most informative features at
inference time while accounting for both temporal dynamics and acquisition
cost. We first introduce a cohesive estimation target for our NOCTA setting,
and then develop two complementary estimators: 1) a non-parametric method based
on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric
method that directly predicts the utility of potential acquisitions (NOCTA-P).
Experiments on synthetic and real-world medical datasets demonstrate that both
NOCTA variants outperform existing baselines.

</details>


### [113] [Mixture of Raytraced Experts](https://arxiv.org/abs/2507.12419)
*Andrea Perin,Giacomo Lagomarsini,Claudio Gallicchio,Giuseppe Nuti*

Main category: cs.LG

TL;DR: 介绍混合光线追踪专家模型，一种可动态选择专家序列的堆叠式混合专家架构，训练无需负载均衡机制，实验显示减少训练轮次且有不错精度。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家架构对给定样本需固定计算量，本文旨在提出可动态调整计算量并提高预测精度的架构。

Method: 通过从候选专家集迭代采样，像训练循环神经网络一样展开序列来训练模型，且无需负载均衡机制。

Result: 初步实验表明训练轮次减少10% - 40%，且精度相当或更高。

Conclusion: 该研究为混合专家领域指出新方向，可设计更快、表达力更强的模型。

Abstract: We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts
(MoE) architecture which can dynamically select sequences of experts, producing
computational graphs of variable width and depth. Existing MoE architectures
generally require a fixed amount of computation for a given sample. Our
approach, in contrast, yields predictions with increasing accuracy as the
computation cycles through the experts' sequence. We train our model by
iteratively sampling from a set of candidate experts, unfolding the sequence
akin to how Recurrent Neural Networks are trained. Our method does not require
load-balancing mechanisms, and preliminary experiments show a reduction in
training epochs of 10\% to 40\% with a comparable/higher accuracy. These
results point to new research directions in the field of MoEs, allowing the
design of potentially faster and more expressive models. The code is available
at https://github.com/nutig/RayTracing

</details>


### [114] [Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks](https://arxiv.org/abs/2507.12435)
*Yi Li,David Mccoy,Nolan Gunter,Kaitlyn Lee,Alejandro Schuler,Mark van der Laan*

Main category: cs.LG

TL;DR: 提出Targeted Deep Architectures (TDA)框架，将TMLE嵌入网络参数空间，可用于复杂多参数目标的因果推断，在实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络缺乏对因果参数的有效推断，现有神经实现存在缺陷，如不能保证解决有效影响函数方程或计算昂贵。

Method: TDA将TMLE嵌入网络参数空间，划分模型参数，冻结部分参数，沿目标梯度迭代更新，可扩展到多维因果估计量。

Result: 在基准IHDP数据集和模拟生存数据上，TDA相比标准神经网络估计器和先前的事后方法减少了偏差，提高了覆盖率。

Conclusion: TDA为现代深度架构中的复杂多参数目标提供了直接、可扩展的严格因果推断途径。

Abstract: Modern deep neural networks are powerful predictive tools yet often lack
valid inference for causal parameters, such as treatment effects or entire
survival curves. While frameworks like Double Machine Learning (DML) and
Targeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,
existing neural implementations either rely on "targeted losses" that do not
guarantee solving the efficient influence function equation or computationally
expensive post-hoc "fluctuations" for multi-parameter settings. We propose
Targeted Deep Architectures (TDA), a new framework that embeds TMLE directly
into the network's parameter space with no restrictions on the backbone
architecture. Specifically, TDA partitions model parameters - freezing all but
a small "targeting" subset - and iteratively updates them along a targeting
gradient, derived from projecting the influence functions onto the span of the
gradients of the loss with respect to weights. This procedure yields plug-in
estimates that remove first-order bias and produce asymptotically valid
confidence intervals. Crucially, TDA easily extends to multi-dimensional causal
estimands (e.g., entire survival curves) by merging separate targeting
gradients into a single universal targeting update. Theoretically, TDA inherits
classical TMLE properties, including double robustness and semiparametric
efficiency. Empirically, on the benchmark IHDP dataset (average treatment
effects) and simulated survival data with informative censoring, TDA reduces
bias and improves coverage relative to both standard neural-network estimators
and prior post-hoc approaches. In doing so, TDA establishes a direct, scalable
pathway toward rigorous causal inference within modern deep architectures for
complex multi-parameter targets.

</details>


### [115] [Cost-aware Stopping for Bayesian Optimization](https://arxiv.org/abs/2507.12453)
*Qian Xie,Linda Cai,Alexander Terenin,Peter I. Frazier,Ziv Scully*

Main category: cs.LG

TL;DR: 提出一种无启发式调优的贝叶斯优化成本感知停止规则，证明理论保证，实验表明与PBGI采集函数结合效果好。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化自适应停止规则在成本感知场景下，缺乏避免过高评估成本的保证。

Method: 提出与最先进的成本感知采集函数有理论联系的停止规则，证明其与两种采集函数配对时的预期累积评估成本边界。

Result: 在合成和实证任务实验中，该停止规则与PBGI采集函数结合在成本调整简单后悔指标上匹配或优于其他组合。

Conclusion: 提出的成本感知停止规则有效，与PBGI采集函数结合能在解质量和累积评估成本间取得较好平衡。

Abstract: In automated machine learning, scientific discovery, and other applications
of Bayesian optimization, deciding when to stop evaluating expensive black-box
functions is an important practical consideration. While several adaptive
stopping rules have been proposed, in the cost-aware setting they lack
guarantees ensuring they stop before incurring excessive function evaluation
costs. We propose a cost-aware stopping rule for Bayesian optimization that
adapts to varying evaluation costs and is free of heuristic tuning. Our rule is
grounded in a theoretical connection to state-of-the-art cost-aware acquisition
functions, namely the Pandora's Box Gittins Index (PBGI) and log expected
improvement per cost. We prove a theoretical guarantee bounding the expected
cumulative evaluation cost incurred by our stopping rule when paired with these
two acquisition functions. In experiments on synthetic and empirical tasks,
including hyperparameter optimization and neural architecture size search, we
show that combining our stopping rule with the PBGI acquisition function
consistently matches or outperforms other acquisition-function--stopping-rule
pairs in terms of cost-adjusted simple regret, a metric capturing trade-offs
between solution quality and cumulative evaluation cost.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [116] [Emergent Heterogeneous Swarm Control Through Hebbian Learning](https://arxiv.org/abs/2507.11566)
*Fuda van Diggelen,Tugay Alperen Karagüzel,Andres Garcia Rincon,A. E. Eiben,Dario Floreano,Eliseo Ferrante*

Main category: cs.NE

TL;DR: 本文引入赫布学习作为群体机器人学新方法，能自动产生异质性，解决学习异质控制的挑战，提升群体能力。


<details>
  <summary>Details</summary>
Motivation: 解决群体机器人学中学习异质控制的若干重大挑战。

Method: 引入生物启发的赫布学习方法，该方法仅依赖局部信息，基于群体行为演化学习规则。

Result: 实现异质性自然涌现，带来群体行为切换，显著提升群体能力，在标准基准任务中可替代多智能体强化学习。

Conclusion: 赫布学习是群体机器人学有效方法，可解决相关挑战，且演化规则能替代多智能体强化学习。

Abstract: In this paper, we introduce Hebbian learning as a novel method for swarm
robotics, enabling the automatic emergence of heterogeneity. Hebbian learning
presents a biologically inspired form of neural adaptation that solely relies
on local information. By doing so, we resolve several major challenges for
learning heterogeneous control: 1) Hebbian learning removes the complexity of
attributing emergent phenomena to single agents through local learning rules,
thus circumventing the micro-macro problem; 2) uniform Hebbian learning rules
across all swarm members limit the number of parameters needed, mitigating the
curse of dimensionality with scaling swarm sizes; and 3) evolving Hebbian
learning rules based on swarm-level behaviour minimises the need for extensive
prior knowledge typically required for optimising heterogeneous swarms. This
work demonstrates that with Hebbian learning heterogeneity naturally emerges,
resulting in swarm-level behavioural switching and in significantly improved
swarm capabilities. It also demonstrates how the evolution of Hebbian learning
rules can be a valid alternative to Multi Agent Reinforcement Learning in
standard benchmarking tasks.

</details>


### [117] [Survey of Genetic and Differential Evolutionary Algorithm Approaches to Search Documents Based On Semantic Similarity](https://arxiv.org/abs/2507.11751)
*Chandrashekar Muniyappa,Eunjin Kim*

Main category: cs.NE

TL;DR: 探讨基于语义文本相似度搜索文档的最新进展，聚焦遗传和差分进化计算算法。


<details>
  <summary>Details</summary>
Motivation: 识别大量数据中的相似文档是重大挑战，需有效方法应对。

Method: 聚焦遗传和差分进化计算算法。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Identifying similar documents within extensive volumes of data poses a
significant challenge. To tackle this issue, researchers have developed a
variety of effective distributed computing techniques. With the advancement of
computing power and the rise of big data, deep neural networks and evolutionary
computing algorithms such as genetic algorithms and differential evolution
algorithms have achieved greater success. This survey will explore the most
recent advancements in the search for documents based on their semantic text
similarity, focusing on genetic and differential evolutionary computing
algorithms.

</details>


### [118] [Simulated Language Acquisition in a Biologically Realistic Model of the Brain](https://arxiv.org/abs/2507.11788)
*Daniel Mitropolsky,Christos Papadimitriou*

Main category: cs.NE

TL;DR: 引入神经科学六原则的数学公式，实现模拟神经形态系统，该系统能进行基础语言习得并讨论结果的扩展和影响。


<details>
  <summary>Details</summary>
Motivation: 当前神经科学虽有进展，但缺乏神经元放电产生高级认知现象的明确解释。

Method: 引入神经科学六原则的数学公式，基于此实现模拟神经形态系统。

Result: 系统能从无到有，通过接触一定数量的接地句子，学习任何语言的单词语义、句法角色和词序，还能生成新句子。

Conclusion: 论文讨论了结果的几种可能扩展和影响。

Abstract: Despite tremendous progress in neuroscience, we do not have a compelling
narrative for the precise way whereby the spiking of neurons in our brain
results in high-level cognitive phenomena such as planning and language. We
introduce a simple mathematical formulation of six basic and broadly accepted
principles of neuroscience: excitatory neurons, brain areas, random synapses,
Hebbian plasticity, local inhibition, and inter-area inhibition. We implement a
simulated neuromorphic system based on this formalism, which is capable of
basic language acquisition: Starting from a tabula rasa, the system learns, in
any language, the semantics of words, their syntactic role (verb versus noun),
and the word order of the language, including the ability to generate novel
sentences, through the exposure to a modest number of grounded sentences in the
same language. We discuss several possible extensions and implications of this
result.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [119] [Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems](https://arxiv.org/abs/2507.11671)
*Mst Shamima Aktar,Peng Liang,Muhammad Waseem,Amjed Tahir,Mojtaba Shahin,Muhammad Azeem Akbar,Arif Ali Khan,Aakash Ahmad,Musengamana Jean de Dieu,Ruiyin Li*

Main category: cs.SE

TL;DR: 本文提出量子软件系统六个关键设计领域选择模式和策略的决策模型，经评估该模型能帮助从业者应对架构设计挑战，数据集公开。


<details>
  <summary>Details</summary>
Motivation: 量子软件从业者在选择和实施合适的模式与策略时面临挑战，缺乏相关指导。

Method: 基于挖掘研究（GitHub和Stack Exchange）和系统文献综述收集数据构建决策模型，通过半结构化访谈评估模型。

Result: 提出的决策模型能帮助从业者选择合适的模式和策略，应对量子软件系统架构设计挑战。

Conclusion: 决策模型有助于解决量子软件系统架构设计的相关问题，数据集可供社区复现和拓展研究。

Abstract: Quantum software represents disruptive technologies in terms of
quantum-specific software systems, services, and applications - leverage the
principles of quantum mechanics via programmable quantum bits (Qubits) that
manipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.
Quantum software architecture enables quantum software developers to abstract
away implementation-specific details (i.e., mapping of Qubits and QuGates to
high-level architectural components and connectors). Architectural patterns and
strategies can provide reusable knowledge and best practices to engineer
quantum software systems effectively and efficiently. However, quantum software
practitioners face significant challenges in selecting and implementing
appropriate patterns and strategies due to the complexity of quantum software
systems and the lack of guidelines. To address these challenges, this study
proposes decision models for selecting patterns and strategies in six critical
design areas in quantum software systems: Communication, Decomposition, Data
Processing, Fault Tolerance, Integration and Optimization, and Algorithm
Implementation. These decision models are constructed based on data collected
from both a mining study (i.e., GitHub and Stack Exchange) and a Systematic
Literature Review, which were used to identify relevant patterns and strategies
with their involved Quality Attributes (QAs). We then conducted semi-structured
interviews with 16 quantum software practitioners to evaluate the familiarity,
understandability, completeness, and usefulness of the proposed decision
models. The results show that the proposed decision models can aid
practitioners in selecting suitable patterns and strategies to address the
challenges related to the architecture design of quantum software systems. The
dataset is available at [6], allowing the community to reproduce and build upon
our findings.

</details>


### [120] [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
*Atharva Naik,Lawanya Baghel,Dhakshin Govindarajan,Darsh Agrawal,Daniel Fried,Carolyn Rose*

Main category: cs.SE

TL;DR: 引入MetaLint框架用于代码质量分析，能适应新代码模式，评估显示其在泛化能力上表现良好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码质量分析上受限于静态训练数据，难以适应不断发展的最佳实践。

Method: 引入MetaLint框架，采用指令调优合成的代码检查器生成的数据，构建挑战性习语基准进行评估。

Result: MetaLint提高了对未见PEP习语的泛化能力，习语检测F分数达70.37%，召回率最高，定位率达26.73%。

Conclusion: MetaLint在代码质量分析上有适应未来变化的潜力。

Abstract: Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

</details>


### [121] [REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps](https://arxiv.org/abs/2507.11689)
*Sergio Di Meglio,Valeria Pontillo,Luigi Libero Lucio Starace*

Main category: cs.SE

TL;DR: 分析大三网络技术课程中40个全栈Web应用代码，发现API设计基础规则常被违反，需加强教学和采用自动化工具。


<details>
  <summary>Details</summary>
Motivation: 计算机科学本科课程对软件质量重视不足，为了解学生代码质量以改进教育和招聘实践。

Method: 使用自动化静态分析管道评估代码对REST API设计规则的遵循情况。

Result: 发现端点路径缺少连字符（98%）、复数形式错误（88%）和HTTP方法使用不当（83%）等常见违规情况。

Conclusion: 需要更有针对性地开展API设计教学，并采用自动化工具提高学生项目代码质量。

Abstract: In Computer Science Bachelor's programs, software quality is often
underemphasized due to limited time and a focus on foundational skills, leaving
many students unprepared for industry expectations. To better understand the
typical quality of student code and inform both education and hiring practices,
we analyze 40 full-stack web applications developed in a third-year Web
Technologies course. Using an automated static analysis pipeline, we assess
adherence to REST API design rules. Results reveal frequent violations of
foundational conventions, such as missing hyphens in endpoint paths (98%),
incorrect pluralization (88%), and misuse of HTTP methods (83%). These findings
highlight the need for more focused instruction on API design and support the
adoption of automated tools to improve code quality in student projects.

</details>


### [122] [Extremal Testing for Network Software using LLMs](https://arxiv.org/abs/2507.11898)
*Rathin Singha,Harry Qian,Srinath Saikrishnan,Tracy Zhao,Ryan Beckett,Siva Kesava Reddy Kakarla,George Varghese*

Main category: cs.SE

TL;DR: 本文展示了如何使用大语言模型（LLM）自动进行网络软件的极值测试，通过生成输入约束和违反约束的测试用例，发现了新的漏洞，并探讨了方法的扩展和进一步自动化。


<details>
  <summary>Details</summary>
Motivation: 将物理学家手动考虑极端情况测试理论的方式自动化应用于网络软件测试。

Method: 分两步使用LLM，先让其生成输入约束，再生成违反约束的测试用例；还提出使用代理式AI进一步自动化。

Result: 为HTTP、BGP和DNS实现生成极值测试，发现新漏洞；该方法可扩展到集中式网络软件，LLM能生成过滤代码。

Conclusion: LLM生成的极值测试超越了软件测试中的边界值分析技术。

Abstract: Physicists often manually consider extreme cases when testing a theory. In
this paper, we show how to automate extremal testing of network software using
LLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS
name length limits); then ask the LLM to generate tests that violate the
constraints. We demonstrate how easy this process is by generating extremal
tests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.
We show how this methodology extends to centralized network software such as
shortest path algorithms, and how LLMs can generate filtering code to reject
extremal input. We propose using agentic AI to further automate extremal
testing. LLM-generated extremal testing goes beyond an old technique in
software testing called Boundary Value Analysis.

</details>


### [123] [A Task Taxonomy for Conformance Checking](https://arxiv.org/abs/2507.11976)
*Jana-Rebecca Rehse,Michael Grohs,Finn Klessascheck,Lisa-Marie Klein,Tatiana von Landesberger,Luise Pufahl*

Main category: cs.SE

TL;DR: 提出用于一致性检查分析的任务分类法，促进过程挖掘和可视化分析学科合作。


<details>
  <summary>Details</summary>
Motivation: 现有一致性检查可视化工具的分析目的不明确，缺乏对其用途的系统评估。

Method: 结合过程挖掘和可视化分析概念，提出任务分类法，从多方面对一致性检查任务进行分类。

Result: 构建了任务分类法，可帮助研究人员确定可视化目的。

Conclusion: 该分类法能促进过程挖掘和可视化分析领域研究人员的紧密合作。

Abstract: Conformance checking is a sub-discipline of process mining, which compares
observed process traces with a process model to analyze whether the process
execution conforms with or deviates from the process design. Organizations can
leverage this analysis, for example to check whether their processes comply
with internal or external regulations or to identify potential improvements.
Gaining these insights requires suitable visualizations, which make complex
results accessible and actionable. So far, however, the development of
conformance checking visualizations has largely been left to tool vendors. As a
result, current tools offer a wide variety of visual representations for
conformance checking, but the analytical purposes they serve often remain
unclear. However, without a systematic understanding of these purposes, it is
difficult to evaluate the visualizations' usefulness. Such an evaluation hence
requires a deeper understanding of conformance checking as an analysis domain.
To this end, we propose a task taxonomy, which categorizes the tasks that can
occur when conducting conformance checking analyses. This taxonomy supports
researchers in determining the purpose of visualizations, specifying relevant
conformance checking tasks in terms of their goal, means, constraint type, data
characteristics, data target, and data cardinality. Combining concepts from
process mining and visual analytics, we address researchers from both
disciplines to enable and support closer collaborations.

</details>


### [124] [LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation](https://arxiv.org/abs/2507.12084)
*Keke Gai,Haochen Liang,Jing Yu,Liehuang Zhu,Dusit Niyato*

Main category: cs.SE

TL;DR: 提出基于大语言模型的智能合约模糊测试框架LLAMA，实验表明其在覆盖率和漏洞检测上优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试器主要探索种子调度和生成，而突变调度研究较少，需提升智能合约模糊测试有效性。

Method: 提出LLAMA框架，包含分层提示策略生成初始种子、多反馈优化机制改进种子和突变调度、进化模糊测试引擎动态调整突变概率并结合符号执行。

Result: LLAMA实现91%指令覆盖率和90%分支覆盖率，检测出148个已知漏洞中的132个。

Conclusion: LLAMA在真实智能合约安全测试场景中有效、可适应且实用。

Abstract: Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing
remains an important approach to securing smart contracts. Even though mutation
scheduling is a key factor influencing fuzzing effectiveness, existing fuzzers
have primarily explored seed scheduling and generation, while mutation
scheduling has been rarely addressed by prior work. In this work, we propose a
Large Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing
framework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and
hybrid testing techniques. Key components of the proposed LLAMA include: (i) a
hierarchical prompting strategy that guides LLMs to generate semantically valid
initial seeds, coupled with a lightweight pre-fuzzing phase to select
high-potential inputs; (ii) a multi-feedback optimization mechanism that
simultaneously improves seed generation, seed selection, and mutation
scheduling by leveraging runtime coverage and dependency feedback; and (iii) an
evolutionary fuzzing engine that dynamically adjusts mutation operator
probabilities based on effectiveness, while incorporating symbolic execution to
escape stagnation and uncover deeper vulnerabilities. Our experiments
demonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage
and vulnerability detection. Specifically, it achieves 91% instruction coverage
and 90% branch coverage, while detecting 132 out of 148 known vulnerabilities
across diverse categories. These results highlight LLAMA's effectiveness,
adaptability, and practicality in real-world smart contract security testing
scenarios.

</details>


### [125] [From Static to Intelligent: Evolving SaaS Pricing with LLMs](https://arxiv.org/abs/2507.12104)
*Francisco Javier Cavero,Juan C. Alonso,Antonio Ruiz-Cortés*

Main category: cs.SE

TL;DR: SaaS市场发展使定价管理复杂，本文提出智能定价解决方案，用LLM驱动方法将静态HTML定价转为智能定价，验证有效但有挑战，未来将改进。


<details>
  <summary>Details</summary>
Motivation: SaaS市场快速扩张使DevOps团队手动管理定价结构耗时且易出错，缺乏自动化定价分析工具。

Method: 提出基于智能定价的解决方案，采用LLM驱动方法将静态HTML定价转换为智能定价，实现信息提取。

Result: 通过对30个不同商业SaaS的验证，系统能有效提取所需元素，但存在幻觉、复杂结构和动态内容等挑战。

Conclusion: 自动化智能定价转换可简化SaaS定价管理，未来需提升提取能力和系统适应性。

Abstract: The SaaS paradigm has revolutionized software distribution by offering
flexible pricing options to meet diverse customer needs. However, the rapid
expansion of the SaaS market has introduced significant complexity for DevOps
teams, who must manually manage and evolve pricing structures, an approach that
is both time-consuming and prone to errors. The absence of automated tools for
pricing analysis restricts the ability to efficiently evaluate, optimize, and
scale these models. This paper proposes leveraging intelligent pricing
(iPricing), dynamic, machine-readable pricing models, as a solution to these
challenges. Intelligent pricing enables competitive analysis, streamlines
operational decision-making, and supports continuous pricing evolution in
response to market dynamics, leading to improved efficiency and accuracy. We
present an LLM-driven approach that automates the transformation of static HTML
pricing into iPricing, significantly improving efficiency and consistency while
minimizing human error. Our implementation, AI4Pricing2Yaml, features a basic
Information Extractor that uses web scraping and LLMs technologies to extract
essential pricing components, plans, features, usage limits, and add-ons, from
SaaS websites. Validation against a dataset of 30 distinct commercial SaaS,
encompassing over 150 intelligent pricings, demonstrates the system's
effectiveness in extracting the desired elements across all steps. However,
challenges remain in addressing hallucinations, complex structures, and dynamic
content. This work highlights the potential of automating intelligent pricing
transformation to streamline SaaS pricing management, offering implications for
improved consistency and scalability in an increasingly intricate pricing
landscape. Future research will focus on refining extraction capabilities and
enhancing the system's adaptability to a wider range of SaaS websites.

</details>


### [126] [An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment](https://arxiv.org/abs/2507.12118)
*Noe Zermeño,Cristina Zuheros,Lucas Daniel Del Rosso Calache,Francisco Herrera,Rosana Montes*

Main category: cs.SE

TL;DR: 提出基于设计思维和语言决策的网页可用性评估方法并融入A/B测试决策支持系统，用真实用户评估墨西哥大学的Moodle平台。


<details>
  <summary>Details</summary>
Motivation: 增强用户对用户界面的满意度，解决A/B测试在扩大测试范围及涉及真实和虚构用户时在线工具支持少的问题。

Method: 提出名为Linguistic Decision - Making for Web Usability Evaluation的方法，让人们参与角色扮演场景并进行包括SUS的可用性测试，将其融入基于A/B测试的决策支持系统。

Result: 使用真实用户对墨西哥瓜达拉哈拉大学的三个Moodle平台进行了评估。

Conclusion: 未提及明确结论。

Abstract: In recent years, attention has increasingly focused on enhancing user
satisfaction with user interfaces, spanning both mobile applications and
websites. One fundamental aspect of human-machine interaction is the concept of
web usability. In order to assess web usability, the A/B testing technique
enables the comparison of data between two designs. Expanding the scope of
tests to include the designs being evaluated, in conjunction with the
involvement of both real and fictional users, presents a challenge for which
few online tools offer support. We propose a methodology for web usability
evaluation based on user-centered approaches such as design thinking and
linguistic decision-making, named Linguistic Decision-Making for Web Usability
Evaluation. This engages people in role-playing scenarios and conducts a number
of usability tests, including the widely recognized System Usability Scale. We
incorporate the methodology into a decision support system based on A/B
testing. We use real users in a case study to assess three Moodle platforms at
the University of Guadalajara, Mexico.

</details>


### [127] [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
*Artem Chervyakov,Alexander Kharitonov,Pavel Zadorozhny,Adamenko Pavel,Rodion Levichev,Dmitrii Vorobev,Dmitrii Salikhov,Aidar Valeev,Alena Pestova,Maria Dziuba,Ilseyar Alimova,Artem Zavgorodnev,Aleksandr Medvedev,Stanislav Moiseev,Elena Bruches,Daniil Grebenkin,Roman Derunets,Vikulov Vladimir,Anton Emelyanov,Dmitrii Babaev,Vladimir V. Ivanov,Valentin Malykh,Alena Fenogenova*

Main category: cs.SE

TL;DR: 当前大语言模型代码评估重自然语言轻代码质量，本文提出专注俄语代码生成评估的MERA Code基准，含11个任务，覆盖8种语言，评估模型并公开成果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估主要关注自然语言任务，忽视代码质量和实际性能，需评估模型在生产中的真实能力和风险。

Method: 提出MERA Code基准，包括11个评估任务、评估方法分类、开源代码库、评分系统及平台。

Result: 评估了开源和前沿API模型，分析其在非英语语言实际编码任务中的局限性。

Conclusion: 公开MERA以指导未来研究，推动模型开发，规范评估程序。

Abstract: Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

</details>


### [128] [GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities](https://arxiv.org/abs/2507.12367)
*Diganta Misra,Nizar Islah,Victor May,Brice Rauby,Zihan Wang,Justine Gehring,Antonio Orvieto,Muawiz Chaudhary,Eilif B. Muller,Irina Rish,Samira Ebrahimi Kahou,Massimo Caccia*

Main category: cs.SE

TL;DR: 提出GitChameleon数据集评估版本条件代码生成能力，指出当前系统面临挑战并公开数据和代码。


<details>
  <summary>Details</summary>
Motivation: 软件库快速演变给代码生成带来挑战，现有基准缺乏基于执行的特定库版本代码生成评估。

Method: 引入包含328个Python代码完成问题及可执行单元测试的GitChameleon数据集进行评估。

Result: 现有系统在版本条件代码生成任务中面临显著挑战，企业模型成功率在48 - 51%。

Conclusion: GitChameleon有助于更清晰理解挑战，指导开发更可靠的AI代码生成方法。

Abstract: The rapid evolution of software libraries poses a considerable hurdle for
code generation, necessitating continuous adaptation to frequent version
updates while preserving backward compatibility. While existing code evolution
benchmarks provide valuable insights, they typically lack execution-based
evaluation for generating code compliant with specific library versions. To
address this, we introduce GitChameleon, a novel, meticulously curated dataset
comprising 328 Python code completion problems, each conditioned on specific
library versions and accompanied by executable unit tests. GitChameleon
rigorously evaluates the capacity of contemporary large language models (LLMs),
LLM-powered agents, code assistants, and RAG systems to perform
version-conditioned code generation that demonstrates functional accuracy
through execution. Our extensive evaluations indicate that state-of-the-art
systems encounter significant challenges with this task; enterprise models
achieving baseline success rates in the 48-51\% range, underscoring the
intricacy of the problem. By offering an execution-based benchmark emphasizing
the dynamic nature of code libraries, GitChameleon enables a clearer
understanding of this challenge and helps guide the development of more
adaptable and dependable AI code generation methods. We make the dataset and
evaluation code publicly available at
https://github.com/mrcabbage972/GitChameleonBenchmark.

</details>


### [129] [SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?](https://arxiv.org/abs/2507.12415)
*Xinyi He,Qian Liu,Mingzhe Du,Lin Yan,Zhijie Fan,Yiming Huang,Zejian Yuan,Zejun Ma*

Main category: cs.SE

TL;DR: 提出首个用于评估大语言模型代码性能优化能力的SWE - Perf基准，评估后发现现有大语言模型与专家水平有差距。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在仓库级代码性能优化能力方面的研究不足，需系统评估。

Method: 引入SWE - Perf基准，其包含140个精心挑选的实例，对代表性方法进行全面评估。

Result: 揭示现有大语言模型与专家级优化性能存在显著能力差距。

Conclusion: 代码性能优化领域存在关键研究机会。

Abstract: Code performance optimization is paramount in real-world software engineering
and critical for production-level systems. While Large Language Models (LLMs)
have demonstrated impressive capabilities in code generation and bug fixing,
their proficiency in enhancing code performance at the repository level remains
largely unexplored. To address this gap, we introduce SWE-Perf, the first
benchmark specifically designed to systematically evaluate LLMs on code
performance optimization tasks within authentic repository contexts. SWE-Perf
comprises 140 carefully curated instances, each derived from
performance-improving pull requests from popular GitHub repositories. Each
benchmark instance includes the relevant codebase, target functions,
performance-related tests, expert-authored patches, and executable
environments. Through a comprehensive evaluation of representative methods that
span file-level and repo-level approaches (e.g., Agentless and OpenHands), we
reveal a substantial capability gap between existing LLMs and expert-level
optimization performance, highlighting critical research opportunities in this
emerging field.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [130] [LLMs are Bayesian, in Expectation, not in Realization](https://arxiv.org/abs/2507.11768)
*Leon Chlon,Sarah Rashidi,Zein Khamis,MarcAntonio M. Awada*

Main category: stat.ML

TL;DR: 研究大语言模型上下文学习能力，指出transformers违反贝叶斯更新特性，给出四个理论结果并实证验证，提供实用方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型上下文学习虽可建模为隐式贝叶斯推理，但transformers违反贝叶斯更新的鞅性质，挑战不确定性量化理论基础。

Method: 进行理论分析得出四个关键结果，在GPT - 3上进行实证验证。

Result: 理论上得出位置编码导致的鞅违反程度等四个结果，实证验证了前三个结果，transformers在20个示例内达到理论熵极限的99%。

Conclusion: 框架提供从位置感知架构提取校准不确定性估计及优化计算效率的实用方法。

Abstract: Large language models demonstrate remarkable in-context learning
capabilities, adapting to new tasks without parameter updates. While this
phenomenon has been successfully modeled as implicit Bayesian inference, recent
empirical findings reveal a fundamental contradiction: transformers
systematically violate the martingale property, a cornerstone requirement of
Bayesian updating on exchangeable data. This violation challenges the
theoretical foundations underlying uncertainty quantification in critical
applications.
  Our theoretical analysis establishes four key results: (1) positional
encodings induce martingale violations of order $\Theta(\log n / n)$; (2)
transformers achieve information-theoretic optimality with excess risk
$O(n^{-1/2})$ in expectation over orderings; (3) the implicit posterior
representation converges to the true Bayesian posterior in the space of
sufficient statistics; and (4) we derive the optimal chain-of-thought length as
$k^* = \Theta(\sqrt{n}\log(1/\varepsilon))$ with explicit constants, providing
a principled approach to reduce inference costs while maintaining performance.
Empirical validation on GPT-3 confirms predictions (1)-(3), with transformers
reaching 99\% of theoretical entropy limits within 20 examples. Our framework
provides practical methods for extracting calibrated uncertainty estimates from
position-aware architectures and optimizing computational efficiency in
deployment.

</details>


### [131] [Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B Experiments Work?](https://arxiv.org/abs/2507.11891)
*Shuangning Li,Chonghuan Wang,Jingyan Wang*

Main category: stat.ML

TL;DR: 研究比较两种推荐算法的A/B实验，指出标准均值差估计器有共生偏差，分析数据共享下GTE估计符号与真实GTE符号的关系及对算法选择的影响。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明标准均值差估计器在估计全局处理效应（GTE）时有偏差，本文为在决策时更好地选择算法，关注GTE符号而非精确值。

Method: 在多臂老虎机框架下形式化该观点，从理论上刻画数据共享下预期GTE估计符号与真实GTE符号一致或矛盾的情况。

Result: 分析确定了探索与利用的水平是共生偏差影响算法选择的关键决定因素。

Conclusion: 探索与利用的水平对共生偏差影响算法选择起关键作用。

Abstract: We study A/B experiments that are designed to compare the performance of two
recommendation algorithms. Prior work has shown that the standard
difference-in-means estimator is biased in estimating the global treatment
effect (GTE) due to a particular form of interference between experimental
units. Specifically, units under the treatment and control algorithms
contribute to a shared pool of data that subsequently train both algorithms,
resulting in interference between the two groups. The bias arising from this
type of data sharing is known as "symbiosis bias". In this paper, we highlight
that, for decision-making purposes, the sign of the GTE often matters more than
its precise magnitude when selecting the better algorithm. We formalize this
insight under a multi-armed bandit framework and theoretically characterize
when the sign of the expected GTE estimate under data sharing aligns with or
contradicts the sign of the true GTE. Our analysis identifies the level of
exploration versus exploitation as a key determinant of how symbiosis bias
impacts algorithm selection.

</details>


### [132] [Newfluence: Boosting Model interpretability and Understanding in High Dimensions](https://arxiv.org/abs/2507.11895)
*Haolin Zou,Arnab Auddy,Yongchan Kwon,Kamiar Rahnama Rad,Arian Maleki*

Main category: stat.ML

TL;DR: 论文指出传统影响函数在高维设置下不准确，提出新的近似方法Newfluence，能提升解读复杂AI模型的准确性，且其高维框架可用于分析其他技术。


<details>
  <summary>Details</summary>
Motivation: 机器学习和人工智能模型日益复杂，需要工具解读和优化模型决策与预测，但传统影响函数基于低维假设，不适用于高维现代AI模型。

Method: 对影响函数在高维设置下的准确性进行理论和实证分析，提出新的近似方法Newfluence。

Result: 影响函数在高维设置下不能可靠实现其目的，Newfluence在保持计算效率的同时显著提高了准确性。

Conclusion: Newfluence能为解读复杂AI模型和诊断问题提供更准确的见解，高维框架可用于分析其他流行技术。

Abstract: The increasing complexity of machine learning (ML) and artificial
intelligence (AI) models has created a pressing need for tools that help
scientists, engineers, and policymakers interpret and refine model decisions
and predictions. Influence functions, originating from robust statistics, have
emerged as a popular approach for this purpose.
  However, the heuristic foundations of influence functions rely on
low-dimensional assumptions where the number of parameters $p$ is much smaller
than the number of observations $n$. In contrast, modern AI models often
operate in high-dimensional regimes with large $p$, challenging these
assumptions.
  In this paper, we examine the accuracy of influence functions in
high-dimensional settings. Our theoretical and empirical analyses reveal that
influence functions cannot reliably fulfill their intended purpose. We then
introduce an alternative approximation, called Newfluence, that maintains
similar computational efficiency while offering significantly improved
accuracy.
  Newfluence is expected to provide more accurate insights than many existing
methods for interpreting complex AI models and diagnosing their issues.
Moreover, the high-dimensional framework we develop in this paper can also be
applied to analyze other popular techniques, such as Shapley values.

</details>


### [133] [Incorporating Fairness Constraints into Archetypal Analysis](https://arxiv.org/abs/2507.12021)
*Aleix Alcacer,Irene Epifanio*

Main category: stat.ML

TL;DR: 提出FairAA和FairKernelAA方法减少敏感属性影响，在合成和真实数据集验证，能平衡效用与公平性。


<details>
  <summary>Details</summary>
Motivation: 传统AA方法会无意中编码敏感属性，引发公平性问题。

Method: 提出FairAA修改公式，引入FairKernelAA非线性扩展，加入公平性正则项。

Result: 在合成数据集减少组可分性，不大幅降低解释方差，在真实数据集验证了鲁棒性和实用性。

Conclusion: FairAA在效用和公平性间取得良好平衡，是敏感应用中可靠表示学习的有前景工具。

Abstract: Archetypal Analysis (AA) is an unsupervised learning method that represents
data as convex combinations of extreme patterns called archetypes. While AA
provides interpretable and low-dimensional representations, it can
inadvertently encode sensitive attributes, leading to fairness concerns. In
this work, we propose Fair Archetypal Analysis (FairAA), a modified formulation
that explicitly reduces the influence of sensitive group information in the
learned projections. We also introduce FairKernelAA, a nonlinear extension that
addresses fairness in more complex data distributions. Our approach
incorporates a fairness regularization term while preserving the structure and
interpretability of the archetypes. We evaluate FairAA and FairKernelAA on
synthetic datasets, including linear, nonlinear, and multi-group scenarios,
demonstrating their ability to reduce group separability -- as measured by mean
maximum discrepancy and linear separability -- without substantially
compromising explained variance. We further validate our methods on the
real-world ANSUR I dataset, confirming their robustness and practical utility.
The results show that FairAA achieves a favorable trade-off between utility and
fairness, making it a promising tool for responsible representation learning in
sensitive applications.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [134] [Fast Variational Bayes for Large Spatial Data](https://arxiv.org/abs/2507.12251)
*Jiafang Song,Abhirup Datta*

Main category: stat.CO

TL;DR: 提出spVarBayes用于大规模地理空间数据分析，计算成本低，精度和速度表现好，有公开R包。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间回归变分贝叶斯方法在准确性和速度上不如基于MCMC的spNNGP，需改进。

Method: 用变分法、闭式梯度更新和线性响应校正替代自动微分，在模型中考虑协变量并对方差参数进行推断。

Result: 模拟实验表明精度与spNNGP相当，计算成本降低，在精度和速度上远超现有变分推理方法；大森林冠层高度数据集分析结果与MCMC方法一致。

Conclusion: 提出的spVarBayes方法有效，可用于大规模地理空间数据分析，且已在R包中实现。

Abstract: Recent variational Bayes methods for geospatial regression, proposed as an
alternative to computationally expensive Markov chain Monte Carlo (MCMC)
sampling, have leveraged Nearest Neighbor Gaussian processes (NNGP) to achieve
scalability. Yet, these variational methods remain inferior in accuracy and
speed compared to spNNGP, the state-of-the-art MCMC-based software for NNGP. We
introduce spVarBayes, a suite of fast variational Bayesian approaches for
large-scale geospatial data analysis using NNGP. Our contributions are
primarily computational. We replace auto-differentiation with a combination of
calculus of variations, closed-form gradient updates, and linear response
corrections for improved variance estimation. We also accommodate covariates
(fixed effects) in the model and offer inference on the variance parameters.
Simulation experiments demonstrate that we achieve comparable accuracy to
spNNGP but with reduced computational costs, and considerably outperform
existing variational inference methods in terms of both accuracy and speed.
Analysis of a large forest canopy height dataset illustrates the practical
implementation of proposed methods and shows that the inference results are
consistent with those obtained from the MCMC approach. The proposed methods are
implemented in publicly available Github R-package spVarBayes.

</details>


### [135] [Surrogate modeling for uncertainty quantification in nonlinear dynamics](https://arxiv.org/abs/2507.12358)
*S. Marelli,S. Schär,B. Sudret*

Main category: stat.CO

TL;DR: 本文综述了用于不确定性量化（UQ）的代理建模技术，聚焦于捕捉动态系统全时变响应，介绍时变问题分类及对应代理方法并举例说明。


<details>
  <summary>Details</summary>
Motivation: 工程中复杂系统行为预测存在不确定性，UQ虽重要但计算成本高，需要代理模型解决该问题。

Method: 对时变问题基于输入激励复杂度进行分类，讨论主成分分析与多项式混沌展开结合、时间扭曲技术、带外部输入的非线性自回归模型（NARX模型）等代理方法，并通过简单应用示例说明。

Result: 给出了用于UQ的代理建模技术的综述，明确了不同时变问题对应的代理方法。

Conclusion: 代理模型在现代UQ实践中至关重要，不同的代理方法适用于不同复杂度的时变问题。

Abstract: Predicting the behavior of complex systems in engineering often involves
significant uncertainty about operating conditions, such as external loads,
environmental effects, and manufacturing variability. As a result, uncertainty
quantification (UQ) has become a critical tool in modeling-based engineering,
providing methods to identify, characterize, and propagate uncertainty through
computational models. However, the stochastic nature of UQ typically requires
numerous evaluations of these models, which can be computationally expensive
and limit the scope of feasible analyses. To address this, surrogate models,
i.e., efficient functional approximations trained on a limited set of
simulations, have become central in modern UQ practice. This book chapter
presents a concise review of surrogate modeling techniques for UQ, with a focus
on the particularly challenging task of capturing the full time-dependent
response of dynamical systems. It introduces a classification of time-dependent
problems based on the complexity of input excitation and discusses
corresponding surrogate approaches, including combinations of principal
component analysis with polynomial chaos expansions, time warping techniques,
and nonlinear autoregressive models with exogenous inputs (NARX models). Each
method is illustrated with simple application examples to clarify the
underlying ideas and practical use.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [136] [AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding](https://arxiv.org/abs/2507.11911)
*Xiaoqing Chen,Siyang Li,Dongrui Wu*

Main category: cs.HC

TL;DR: 提出校准无关的AFPM框架用于跨数据集脑电解码，性能优于现有方法，提升BCI实用性。


<details>
  <summary>Details</summary>
Motivation: 现有脑机接口的脑电解码模型在跨数据集学习和泛化方面存在问题，如通道布局不一致、信号分布非平稳和神经生理先验整合有限。

Method: 提出AFPM框架，包含基于脑区先验选择相关通道、对齐不同域脑电分布并将通道重映射到统一布局的空间对齐，以及将多数据集信号建模为统一时空补丁进行脑电解码的帧补丁编码。

Result: 与17种需要特定数据集调优的先进方法相比，AFPM在运动想象任务上性能提升达4.40%，在事件相关电位任务上提升3.58%。

Conclusion: AFPM是首个校准无关的跨数据集脑电解码框架，显著提升了脑机接口在现实应用中的实用性。

Abstract: Electroencephalogram (EEG) decoding models for brain-computer interfaces
(BCIs) struggle with cross-dataset learning and generalization due to channel
layout inconsistencies, non-stationary signal distributions, and limited
neurophysiological prior integration. To address these issues, we propose a
plug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has
two main components: 1) Spatial Alignment, which selects task-relevant channels
based on brain-region priors, aligns EEG distributions across domains, and
remaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,
which models multi-dataset signals into unified spatiotemporal patches for EEG
decoding. Compared to 17 state-of-the-art approaches that need dataset-specific
tuning, the proposed calibration-free AFPM achieves performance gains of up to
4.40% on motor imagery and 3.58% on event-related potential tasks. To our
knowledge, this is the first calibration-free cross-dataset EEG decoding
framework, substantially enhancing the practicalness of BCIs in real-world
applications.

</details>


### [137] [Interactive Hybrid Rice Breeding with Parametric Dual Projection](https://arxiv.org/abs/2507.11848)
*Changjian Chen,Pengcheng Wang,Fei Lyu,Zhuo Tang,Li Yang,Long Wang,Yong Cai,Feng Yu,Kenli Li*

Main category: cs.HC

TL;DR: 本文提出可视化分析方法助力杂交水稻育种，开发有理论保证的参数化双投影方法及可视化，经案例验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有基因组选择方法因模型精度有限，育种仍耗时，需新方法简化过程。

Method: 提出可视化分析方法，开发参数化双投影方法用于交互式双分析，开发基因和杂交种可视化。

Result: 通过对参数化双投影方法、鉴定的调控基因和所需杂交种的定量评估及育种者积极反馈，证明方法有效。

Conclusion: 所提可视化分析方法能有效助力杂交水稻育种。

Abstract: Hybrid rice breeding crossbreeds different rice lines and cultivates the
resulting hybrids in fields to select those with desirable agronomic traits,
such as higher yields. Recently, genomic selection has emerged as an efficient
way for hybrid rice breeding. It predicts the traits of hybrids based on their
genes, which helps exclude many undesired hybrids, largely reducing the
workload of field cultivation. However, due to the limited accuracy of genomic
prediction models, breeders still need to combine their experience with the
models to identify regulatory genes that control traits and select hybrids,
which remains a time-consuming process. To ease this process, in this paper, we
proposed a visual analysis method to facilitate interactive hybrid rice
breeding. Regulatory gene identification and hybrid selection naturally
ensemble a dual-analysis task. Therefore, we developed a parametric dual
projection method with theoretical guarantees to facilitate interactive dual
analysis. Based on this dual projection method, we further developed a gene
visualization and a hybrid visualization to verify the identified regulatory
genes and hybrids. The effectiveness of our method is demonstrated through the
quantitative evaluation of the parametric dual projection method, identified
regulatory genes and desired hybrids in the case study, and positive feedback
from breeders.

</details>


### [138] [Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness](https://arxiv.org/abs/2507.12212)
*Garyoung Kim,Huisung Kwon,Seoju Yun,Yu-Won Youn*

Main category: cs.HC

TL;DR: 研究探讨四种生成式AI模型对丑陋概念的理解与表达及其中的偏差，发现AI仍存在偏见，强调创建伦理AI训练范式和包容性发展方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI会再现文化偏见，需审视其对丑陋概念的理解和表达。

Method: 通过迭代提示大语言模型提取13个与丑陋相关的形容词，在四个AI模型和三个提示下生成624张图像，对图像中的人口和社会经济属性进行独立编码和主题分析。

Result: AI模型过度将丑陋与老年白人男性形象关联，存在社会偏见和悖论性偏见；定性分析显示传统身体特征仍是表达丑陋的核心视觉主题。

Conclusion: 尽管努力创造更平等的表征，生成式AI仍延续固有和悖论性偏见，凸显创建伦理AI训练范式和推进包容性AI发展方法的重要性。

Abstract: Generative AI does not only replicate human creativity but also reproduces
deep-seated cultural biases, making it crucial to critically examine how
concepts like ugliness are understood and expressed by these tools. This study
investigates how four different generative AI models understand and express
ugliness through text and image and explores the biases embedded within these
representations. We extracted 13 adjectives associated with ugliness through
iterative prompting of a large language model and generated 624 images across
four AI models and three prompts. Demographic and socioeconomic attributes
within the images were independently coded and thematically analyzed. Our
findings show that AI models disproportionately associate ugliness with old
white male figures, reflecting entrenched social biases as well as paradoxical
biases, where efforts to avoid stereotypical depictions of marginalized groups
inadvertently result in the disproportionate projection of negative attributes
onto majority groups. Qualitative analysis further reveals that, despite
supposed attempts to frame ugliness within social contexts, conventional
physical markers such as asymmetry and aging persist as central visual motifs.
These findings demonstrate that despite attempts to create more equal
representations, generative AI continues to perpetuate inherited and
paradoxical biases, underscoring the critical work being done to create ethical
AI training paradigms and advance methodologies for more inclusive AI
development.

</details>


### [139] [d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement](https://arxiv.org/abs/2507.11960)
*Hyein Hong,Sangbong Yoo,SeokHwan Choi,Jisue Kim,Seongbum Seo,Haneol Cho,Chansoo Kim,Yun Jang*

Main category: cs.HC

TL;DR: 提出新型可视化分析系统d - DQIVAR，结合数据和过程驱动方法促进数据质量改进以提升机器学习模型性能，并通过案例等展示其作用。


<details>
  <summary>Details</summary>
Motivation: 以往研究多在数据驱动框架下使用批量数据预处理，不足以优化机器学习模型性能且导致数据特征扭曲，且主要关注数据预处理而非真正的数据质量改进。

Method: 引入d - DQIVAR系统，集成数据驱动（处理数据质量问题）和过程驱动（评估数据质量和改进程序）方法，还运用Kolmogorov - Smirnov测试。

Result: 通过案例研究、评估和用户研究展示系统能让用户在实际工作流程中有效利用专家和领域知识。

Conclusion: d - DQIVAR系统可促进数据质量改进策略，有助于提升机器学习模型性能。

Abstract: Approaches to enhancing data quality (DQ) are classified into two main
categories: data- and process-driven. However, prior research has predominantly
utilized batch data preprocessing within the data-driven framework, which often
proves insufficient for optimizing machine learning (ML) model performance and
frequently leads to distortions in data characteristics. Existing studies have
primarily focused on data preprocessing rather than genuine data quality
improvement (DQI). In this paper, we introduce d-DQIVAR, a novel visual
analytics system designed to facilitate DQI strategies aimed at improving ML
model performance. Our system integrates visual analytics techniques that
leverage both data-driven and process-driven approaches. Data-driven techniques
tackle DQ issues such as imputation, outlier detection, deletion, format
standardization, removal of duplicate records, and feature selection.
Process-driven strategies encompass evaluating DQ and DQI procedures by
considering DQ dimensions and ML model performance and applying the
Kolmogorov-Smirnov test. We illustrate how our system empowers users to harness
expert and domain knowledge effectively within a practical workflow through
case studies, evaluations, and user studies.

</details>


### [140] [Dataset-Adaptive Dimensionality Reduction](https://arxiv.org/abs/2507.11984)
*Hyeon Jeon,Jeongin Park,Soohyun Lee,Dae Hyun Kim,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: 提出基于结构复杂度指标的数据集自适应降维优化方法，可提高降维优化效率且不降低准确性。


<details>
  <summary>Details</summary>
Motivation: 解决选择降维技术和确定最优超参数设置时的大量试错和计算开销问题。

Method: 引入结构复杂度指标量化数据集内在复杂度，利用指标预测降维技术最大可实现的准确性，指导数据集自适应降维工作流程。

Result: 定量验证指标能有效近似数据集真实复杂度，适用于指导降维工作流程。

Conclusion: 数据集自适应工作流程能显著提高降维优化效率且不影响准确性。

Abstract: Selecting the appropriate dimensionality reduction (DR) technique and
determining its optimal hyperparameter settings that maximize the accuracy of
the output projections typically involves extensive trial and error, often
resulting in unnecessary computational overhead. To address this challenge, we
propose a dataset-adaptive approach to DR optimization guided by structural
complexity metrics. These metrics quantify the intrinsic complexity of a
dataset, predicting whether higher-dimensional spaces are necessary to
represent it accurately. Since complex datasets are often inaccurately
represented in two-dimensional projections, leveraging these metrics enables us
to predict the maximum achievable accuracy of DR techniques for a given
dataset, eliminating redundant trials in optimizing DR. We introduce the design
and theoretical foundations of these structural complexity metrics. We
quantitatively verify that our metrics effectively approximate the ground truth
complexity of datasets and confirm their suitability for guiding
dataset-adaptive DR workflow. Finally, we empirically show that our
dataset-adaptive workflow significantly enhances the efficiency of DR
optimization without compromising accuracy.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [141] [SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics](https://arxiv.org/abs/2507.11588)
*Suyuan Zhao,Yizhen Luo,Ganbo Yang,Yan Zhong,Hao Zhou,Zaiqing Nie*

Main category: q-bio.GN

TL;DR: 提出多尺度空间转录组基础模型SToFM用于分析数据，构建语料库预训练，在下游任务表现出色


<details>
  <summary>Details</summary>
Motivation: 空间转录组数据建模需提取多尺度信息，存在挑战，构建基础模型可提升数据分析能力

Method: 对每个ST切片进行多尺度信息提取构建子切片，用SE(2) Transformer获取细胞表示，构建SToCorpus - 88M语料库预训练

Result: SToFM在组织区域语义分割和细胞类型注释等下游任务中取得出色表现

Conclusion: SToFM对空间转录组数据有全面理解，能有效解决空间转录组数据建模问题

Abstract: Spatial Transcriptomics (ST) technologies provide biologists with rich
insights into single-cell biology by preserving spatial context of cells.
Building foundational models for ST can significantly enhance the analysis of
vast and complex data sources, unlocking new perspectives on the intricacies of
biological tissues. However, modeling ST data is inherently challenging due to
the need to extract multi-scale information from tissue slices containing vast
numbers of cells. This process requires integrating macro-scale tissue
morphology, micro-scale cellular microenvironment, and gene-scale gene
expression profile. To address this challenge, we propose SToFM, a multi-scale
Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale
information extraction on each ST slice, to construct a set of ST sub-slices
that aggregate macro-, micro- and gene-scale information. Then an SE(2)
Transformer is used to obtain high-quality cell representations from the
sub-slices. Additionally, we construct \textbf{SToCorpus-88M}, the largest
high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves
outstanding performance on a variety of downstream tasks, such as tissue region
semantic segmentation and cell type annotation, demonstrating its comprehensive
understanding of ST data

</details>


### [142] [RNAMunin: A Deep Machine Learning Model for Non-coding RNA Discovery](https://arxiv.org/abs/2507.11950)
*Lauren Lui,Torben Nielsen*

Main category: q-bio.GN

TL;DR: 本文介绍了机器学习模型RNAMunin，其能仅用基因组序列识别ncRNAs，适用于大序列数据集，模型小且速度快。


<details>
  <summary>Details</summary>
Motivation: 微生物基因组功能注释常偏向蛋白编码基因，ncRNAs研究不足，直接从基因组序列识别ncRNAs是重要挑战，为了解生物体完整调控潜力，需开发相关工具。

Method: 基于从约60 Gbp的长读长宏基因组中提取的Rfam序列训练RNAMunin模型。

Result: 开发出RNAMunin模型，能仅基于基因组序列检测ncRNAs，可用于大序列数据集，无需转录组数据。

Conclusion: RNAMunin是一个小型快速的模型，仅需基因组序列输入，在识别ncRNAs方面有独特优势。

Abstract: Functional annotation of microbial genomes is often biased toward
protein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs
(ncRNAs) that are critical for regulating bacterial and archaeal physiology,
stress response and metabolism. Identifying ncRNAs directly from genomic
sequence is a paramount challenge in bioinformatics and biology, essential for
understanding the complete regulatory potential of an organism. This paper
presents RNAMunin, a machine learning (ML) model that is capable of finding
ncRNAs using genomic sequence alone. It is also computationally viable for
large sequence datasets such as long read metagenomic assemblies with contigs
totaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from
approximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary
samples. We know of no other model that can detect ncRNAs based solely on
genomic sequence at this scale. Since RNAMunin only requires genomic sequence
as input, we do not need for an ncRNA to be transcribed to find it, i.e., we do
not need transcriptomics data. We wrote this manuscript in a narrative style in
order to best convey how RNAMunin was developed and how it works in detail.
Unlike almost all current ML models, at approximately 1M parameters, RNAMunin
is very small and very fast.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [143] [A Review of Generative AI in Computer Science Education: Challenges and Opportunities in Accuracy, Authenticity, and Assessment](https://arxiv.org/abs/2507.11543)
*Iman Reihanian,Yunfei Hou,Yu Chen,Yifei Zheng*

Main category: cs.CY

TL;DR: 本文调查生成式AI工具在计算机科学教育中的应用，指出机遇与挑战，强调需平衡各因素并给出建议，还提及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI工具（如ChatGPT和Claude）在计算机科学教育中的应用情况。

Method: 通过文献综述的方法。

Result: 生成式AI提高效率、支持学生创意作品，但存在幻觉、误差传播、偏见等问题，现有文献推荐采用混合评估模型、开发偏见检测框架和提升AI素养。

Conclusion: AI的成功集成需要平衡伦理、教学和技术因素，未来可探索提高AI准确性、维护学术诚信和开发自适应模型。

Abstract: This paper surveys the use of Generative AI tools, such as ChatGPT and
Claude, in computer science education, focusing on key aspects of accuracy,
authenticity, and assessment. Through a literature review, we highlight both
the challenges and opportunities these AI tools present. While Generative AI
improves efficiency and supports creative student work, it raises concerns such
as AI hallucinations, error propagation, bias, and blurred lines between
AI-assisted and student-authored content. Human oversight is crucial for
addressing these concerns. Existing literature recommends adopting hybrid
assessment models that combine AI with human evaluation, developing bias
detection frameworks, and promoting AI literacy for both students and
educators. Our findings suggest that the successful integration of AI requires
a balanced approach, considering ethical, pedagogical, and technical factors.
Future research may explore enhancing AI accuracy, preserving academic
integrity, and developing adaptive models that balance creativity with
precision.

</details>


### [144] [Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening](https://arxiv.org/abs/2507.11548)
*Kevin T Webster*

Main category: cs.CY

TL;DR: 研究对八大AI平台进行两部分审计，发现AI简历筛选存在种族和性别偏见及能力不足问题，提出“中立幻觉”现象并建议采用双重验证框架。


<details>
  <summary>Details</summary>
Motivation: 探究用于简历筛选的生成式AI系统是否具备执行评估任务的能力。

Method: 对八大AI平台进行两部分审计，实验1确认种族和性别偏见，实验2评估核心能力。

Result: 实验1发现复杂的、与背景相关的种族和性别偏见；实验2发现部分看似无偏见的模型无法进行实质性评估，仅依赖关键词匹配。

Conclusion: 提出“中立幻觉”现象，建议组织和监管机构采用双重验证框架，确保AI招聘工具公平有效。

Abstract: The increasing use of generative AI for resume screening is predicated on the
assumption that it offers an unbiased alternative to biased human
decision-making. However, this belief fails to address a critical question: are
these AI systems fundamentally competent at the evaluative tasks they are meant
to perform? This study investigates the question of competence through a
two-part audit of eight major AI platforms. Experiment 1 confirmed complex,
contextual racial and gender biases, with some models penalizing candidates
merely for the presence of demographic signals. Experiment 2, which evaluated
core competence, provided a critical insight: some models that appeared
unbiased were, in fact, incapable of performing a substantive evaluation,
relying instead on superficial keyword matching. This paper introduces the
"Illusion of Neutrality" to describe this phenomenon, where an apparent lack of
bias is merely a symptom of a model's inability to make meaningful judgments.
This study recommends that organizations and regulators adopt a dual-validation
framework, auditing AI hiring tools for both demographic bias and demonstrable
competence to ensure they are both equitable and effective.

</details>


### [145] [AI, Humans, and Data Science: Optimizing Roles Across Workflows and the Workforce](https://arxiv.org/abs/2507.11597)
*Richard Timpone,Yongwei Yang*

Main category: cs.CY

TL;DR: 本文探讨AI在研究中的应用，分析其潜力与局限，强调人机协作并鼓励AI辅助数据科学家同时加强方法培训。


<details>
  <summary>Details</summary>
Motivation: 研究AI在研究领域应用的潜力和局限性，解决现实中AI应用效果不明确的问题。

Method: 运用Truth, Beauty, and Justice (TBJ)框架评估AI，结合过往调查分析时代问题类比新AI工具风险。

Result: AI可辅助分析师，但一键自动化存在风险，类似过往统计软件使用问题且风险更大。

Conclusion: 鼓励发展AI工具辅助数据科学家，同时强调持续培训和理解方法以确保研究的实质价值。

Abstract: AI is transforming research. It is being leveraged to construct surveys,
synthesize data, conduct analysis, and write summaries of the results. While
the promise is to create efficiencies and increase quality, the reality is not
always as clear cut. Leveraging our framework of Truth, Beauty, and Justice
(TBJ) which we use to evaluate AI, machine learning and computational models
for effective and ethical use (Taber and Timpone 1997; Timpone and Yang 2024),
we consider the potential and limitation of analytic, generative, and agentic
AI to augment data scientists or take on tasks traditionally done by human
analysts and researchers. While AI can be leveraged to assist analysts in their
tasks, we raise some warnings about push-button automation. Just as earlier
eras of survey analysis created some issues when the increased ease of using
statistical software allowed researchers to conduct analyses they did not fully
understand, the new AI tools may create similar but larger risks. We emphasize
a human-machine collaboration perspective (Daugherty and Wilson 2018)
throughout the data science workflow and particularly call out the vital role
that data scientists play under VUCA decision areas. We conclude by encouraging
the advance of AI tools to complement data scientists but advocate for
continued training and understanding of methods to ensure the substantive value
of research is fully achieved by applying, interpreting, and acting upon
results most effectively and ethically.

</details>


### [146] [Small Data Explainer -- The impact of small data methods in everyday life](https://arxiv.org/abs/2507.11773)
*Maren Hackenberg,Sophia G. Connor,Fabian Kabus,June Brawner,Ella Markham,Mahi Hardalupas,Areeq Chowdhury,Rolf Backofen,Anna Köttgen,Angelika Rohde,Nadine Binder,Harald Binder,the Collaborative Research Center 1597 Small Data*

Main category: cs.CY

TL;DR: 探讨人工智能技术下小数据设置的益处，对比大小数据，分析案例，介绍技术并提出利用小数据的议程。


<details>
  <summary>Details</summary>
Motivation: 在突破性人工智能技术出现背景下，研究小数据设置如何从中受益，解决社会问题。

Method: 概念性概述，对比大小数据，分析案例，技术层面介绍数据分析和建模技术。

Result: 明确了当前可行的方向。

Conclusion: 提出了充分利用小数据的议程设想。

Abstract: The emergence of breakthrough artificial intelligence (AI) techniques has led
to a renewed focus on how small data settings, i.e., settings with limited
information, can benefit from such developments. This includes societal issues
such as how best to include under-represented groups in data-driven policy and
decision making, or the health benefits of assistive technologies such as
wearables. We provide a conceptual overview, in particular contrasting small
data with big data, and identify common themes from exemplary case studies and
application areas. Potential solutions are described in a more detailed
technical overview of current data analysis and modelling techniques,
highlighting contributions from different disciplines, such as knowledge-driven
modelling from statistics and data-driven modelling from computer science. By
linking application settings, conceptual contributions and specific techniques,
we highlight what is already feasible and suggest what an agenda for fully
leveraging small data might look like.

</details>


### [147] [The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models](https://arxiv.org/abs/2507.11544)
*Ann-Kathrin Dombrowski,Dillon Bowen,Adam Gleave,Chris Cundy*

Main category: cs.CY

TL;DR: 文章指出开源大语言模型可修改性带来安全风险，开源工具包评估安全差距，实验显示模型规模增加安全差距扩大，呼吁社区贡献。


<details>
  <summary>Details</summary>
Motivation: 解决开源大语言模型因可修改性带来的系统风险，评估其安全差距。

Method: 开源工具包，以Llama - 3和Qwen - 2.5系列不同参数规模模型为例，用不同防护移除技术评估生化和网络能力、拒绝率和生成质量。

Result: 模型规模增加安全差距扩大，移除防护后危险能力大幅增长。

Conclusion: 安全差距工具包可作为评估框架，激励开发抗篡改防护措施，欢迎社区贡献。

Abstract: Open-weight large language models (LLMs) unlock huge benefits in innovation,
personalization, privacy, and democratization. However, their core advantage -
modifiability - opens the door to systemic risks: bad actors can trivially
subvert current safeguards, turning beneficial models into tools for harm. This
leads to a 'safety gap': the difference in dangerous capabilities between a
model with intact safeguards and one that has been stripped of those
safeguards. We open-source a toolkit to estimate the safety gap for
state-of-the-art open-weight models. As a case study, we evaluate biochemical
and cyber capabilities, refusal rates, and generation quality of models from
two families (Llama-3 and Qwen-2.5) across a range of parameter scales (0.5B to
405B) using different safeguard removal techniques. Our experiments reveal that
the safety gap widens as model scale increases and effective dangerous
capabilities grow substantially when safeguards are removed. We hope that the
Safety Gap Toolkit (https://github.com/AlignmentResearch/safety-gap) will serve
as an evaluation framework for common open-source models and as a motivation
for developing and testing tamper-resistant safeguards. We welcome
contributions to the toolkit from the community.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [148] [Fragment size density estimator for shrinkage-induced fracture based on a physics-informed neural network](https://arxiv.org/abs/2507.11799)
*Shin-ichi Ito*

Main category: physics.comp-ph

TL;DR: 提出基于神经网络的积分 - 微分方程求解器，降低计算成本，验证了效率和可靠性，为反分析奠基并暗示框架扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 解决模拟收缩诱导破碎的积分 - 微分方程求解时计算成本高的问题。

Method: 使用基于神经网络的求解器，直接将输入参数映射到相应概率密度函数，不数值求解控制方程。

Result: 在蒙特卡罗模拟中能有效评估密度函数，计算效率高，预测可靠性好，准确性与传统有限差分法相当甚至更优。

Conclusion: 为数据驱动的破碎反分析奠定基础，框架有超出预设模型结构扩展的潜力。

Abstract: This paper presents a neural network (NN)-based solver for an
integro-differential equation that models shrinkage-induced fragmentation. The
proposed method directly maps input parameters to the corresponding probability
density function without numerically solving the governing equation, thereby
significantly reducing computational costs. Specifically, it enables efficient
evaluation of the density function in Monte Carlo simulations while maintaining
accuracy comparable to or even exceeding that of conventional finite difference
schemes. Validatation on synthetic data demonstrates both the method's
computational efficiency and predictive reliability. This study establishes a
foundation for the data-driven inverse analysis of fragmentation and suggests
the potential for extending the framework beyond pre-specified model
structures.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [149] [Inference on Optimal Policy Values and Other Irregular Functionals via Smoothing](https://arxiv.org/abs/2507.11780)
*Justin Whitehouse,Morgane Austern,Vasilis Syrgkanis*

Main category: econ.EM

TL;DR: 本文重新探讨非可微泛函的平滑近似问题，提出基于softmax平滑的估计量，能以√n速率收敛，避免参数限制和不现实假设，且常具统计效率。


<details>
  <summary>Details</summary>
Motivation: 构建最优治疗策略值的置信区间重要，但因最优值定义的泛函不可微，标准半参数推断方法不适用，现有处理方法有局限性。

Method: 仔细控制一阶偏差和二阶余项，使用基于softmax平滑的估计量来估计由含扰动分量的分数最大值指定的参数。

Result: 估计量可达到√n收敛速率，避免参数限制和不现实的边际假设，且常具有统计效率。

Conclusion: 基于softmax平滑的估计量可有效解决构建最优治疗策略值置信区间的问题。

Abstract: Constructing confidence intervals for the value of an optimal treatment
policy is an important problem in causal inference. Insight into the optimal
policy value can guide the development of reward-maximizing, individualized
treatment regimes. However, because the functional that defines the optimal
value is non-differentiable, standard semi-parametric approaches for performing
inference fail to be directly applicable. Existing approaches for handling this
non-differentiability fall roughly into two camps. In one camp are estimators
based on constructing smooth approximations of the optimal value. These
approaches are computationally lightweight, but typically place unrealistic
parametric assumptions on outcome regressions. In another camp are approaches
that directly de-bias the non-smooth objective. These approaches don't place
parametric assumptions on nuisance functions, but they either require the
computation of intractably-many nuisance estimates, assume unrealistic
$L^\infty$ nuisance convergence rates, or make strong margin assumptions that
prohibit non-response to a treatment. In this paper, we revisit the problem of
constructing smooth approximations of non-differentiable functionals. By
carefully controlling first-order bias and second-order remainders, we show
that a softmax smoothing-based estimator can be used to estimate parameters
that are specified as a maximum of scores involving nuisance components. In
particular, this includes the value of the optimal treatment policy as a
special case. Our estimator obtains $\sqrt{n}$ convergence rates, avoids
parametric restrictions/unrealistic margin assumptions, and is often
statistically efficient.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [150] [Matroids are Equitable](https://arxiv.org/abs/2507.12100)
*Hannaneh Akrami,Roshan Raj,László A. Végh*

Main category: math.CO

TL;DR: 论文证明了可将拟阵基集划分为k个基时，对给定子集S存在特定划分，解决了拟阵公平性猜想，还研究了两个不相交集的公平划分及在公平分配问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决Fekete和Szabó提出的拟阵公平性猜想，研究拟阵在公平分配问题中的应用。

Method: 通过理论推导证明关于拟阵基集划分及公平划分的相关结论。

Result: 解决了拟阵公平性猜想；对于两个不相交集S1和S2有特定公平划分结果；在拟阵约束公平分配问题中，相同三值加性估值下存在无1项嫉妒公平划分，二值加性估值下存在提供最大最小份额的分配。

Conclusion: 肯定解决了拟阵公平性猜想，所得公平划分结果对拟阵约束公平分配问题有重要应用价值。

Abstract: We show that if the ground set of a matroid can be partitioned into $k\ge 2$
bases, then for any given subset $S$ of the ground set, there is a partition
into $k$ bases such that the sizes of the intersections of the bases with $S$
may differ by at most one. This settles the matroid equitability conjecture by
Fekete and Szab\'o (Electron.~J.~Comb.~2011) in the affirmative. We also
investigate equitable splittings of two disjoint sets $S_1$ and $S_2$, and show
that there is a partition into $k$ bases such that the sizes of the
intersections with $S_1$ may differ by at most one and the sizes of the
intersections with $S_2$ may differ by at most two; this is the best possible
one can hope for arbitrary matroids.
  We also derive applications of this result into matroid constrained fair
division problems. We show that there exists a matroid-constrained fair
division that is envy-free up to 1 item if the valuations are identical and
tri-valued additive. We also show that for bi-valued additive valuations, there
exists a matroid-constrained allocation that provides everyone their maximin
share.

</details>


### [151] [Kernelization for list $H$-coloring for graphs with small vertex cover](https://arxiv.org/abs/2507.12005)
*Marta Piecyk,Astrid Pieterse,Paweł Rzążewski,Magnus Wahlström*

Main category: math.CO

TL;DR: 研究List $H$-Coloring问题以图$G$的顶点覆盖数为参数的核化性质，定义两个图不变量$c^*(H)$和$d^*(H)$，给出核大小的上下界。


<details>
  <summary>Details</summary>
Motivation: 探究List $H$-Coloring问题以图$G$的顶点覆盖数为参数时能否将实例约简为大小受$k$的低次多项式限制的等价实例。

Method: 定义新的图不变量$c^*(H)$和$d^*(H)$，使用多项式方法。

Result: 对于每个图$H$，给出List $H$-Coloring问题核的顶点数上下界，对部分图类证明$d^*(H)$是紧的。

Conclusion: 若多项式层级不崩溃，List $H$-Coloring问题核大小有相应上下界，推测$d^*(H)$的紧性普遍成立。

Abstract: For a fixed graph $H$, in the List $H$-Coloring problem, we are given a graph
$G$ along with list $L(v) \subseteq V(H)$ for every $v \in V(G)$, and we have
to determine if there exists a list homomorphism $\varphi$ from $(G,L)$ to $H$,
i.e., an edge preserving mapping $\varphi: V(G)\to V(H)$ that satisfies
$\varphi(v)\in L(v)$ for every $v\in V(G)$. Note that if $H$ is the complete
graph on $q$ vertices, the problem is equivalent to List $q$-Coloring. We
investigate the kernelization properties of List $H$-Coloring parameterized by
the vertex cover number of $G$: given an instance $(G,L)$ and a vertex cover of
$G$ of size $k$, can we reduce $(G,L)$ to an equivalent instance $(G',L')$ of
List $H$-Coloring where the size of $G'$ is bounded by a low-degree polynomial
$p(k)$ in $k$? This question has been investigated previously by Jansen and
Pieterse [Algorithmica 2019], who provided an upper bound, which turns out to
be optimal if $H$ is a complete graph, i.e., for List $q$-Coloring. This result
was one of the first applications of the method of kernelization via
bounded-degree polynomials. We define two new integral graph invariants,
$c^*(H)$ and $d^*(H)$, with $d^*(H) \leq c^*(H) \leq d^*(H)+1$, and show that
for every graph $H$, List $H$-Coloring
  -- has a kernel with $\mathcal{O}(k^{c^*(H)})$ vertices,
  -- admits no kernel of size $\mathcal{O}(k^{d^*(H)-\varepsilon})$ for any
$\varepsilon > 0$, unless the polynomial hierarchy collapses.
  -- Furthermore, if $c^*(H) > d^*(H)$, then there is a kernel with
$\mathcal{O}(k^{c^*(H)-\varepsilon})$ vertices where $\varepsilon \geq
2^{1-c^*(H)}$.
  Additionally, we show that for some classes of graphs, including powers of
cycles and graphs $H$ where $\Delta(H) \leq c^*(H)$ (which in particular
includes cliques), the bound $d^*(H)$ is tight, using the polynomial method. We
conjecture that this holds in general.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [152] [Landmark Detection for Medical Images using a General-purpose Segmentation Model](https://arxiv.org/abs/2507.11551)
*Ekaterina Stansfield,Jennifer A. Mitterer,Abdulrahman Altahhan*

Main category: eess.IV

TL;DR: 本文提出结合YOLO和SAM模型对骨科骨盆X光片进行解剖标志分割，取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 通用分割模型SAM和医学适配版本MedSAM无法准确进行骨科骨盆标志分割，需要更合适的方法。

Method: 利用YOLO进行目标检测提供边界框作为SAM的输入提示，组合两个模型形成可靠的分割管道。

Result: 组合模型能分割8个解剖标志的小试点集、72个标志和16个复杂轮廓区域，检测解剖标志和复杂轮廓效果良好。

Conclusion: YOLO和SAM的组合在骨科骨盆X光片的解剖标志和复杂轮廓检测中表现出色。

Abstract: Radiographic images are a cornerstone of medical diagnostics in orthopaedics,
with anatomical landmark detection serving as a crucial intermediate step for
information extraction. General-purpose foundational segmentation models, such
as SAM (Segment Anything Model), do not support landmark segmentation out of
the box and require prompts to function. However, in medical imaging, the
prompts for landmarks are highly specific. Since SAM has not been trained to
recognize such landmarks, it cannot generate accurate landmark segmentations
for diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has
been trained to identify larger anatomical structures, such as organs and their
parts, and lacks the fine-grained precision required for orthopaedic pelvic
landmarks. To address this limitation, we propose leveraging another
general-purpose, non-foundational model: YOLO. YOLO excels in object detection
and can provide bounding boxes that serve as input prompts for SAM. While YOLO
is efficient at detection, it is significantly outperformed by SAM in
segmenting complex structures. In combination, these two models form a reliable
pipeline capable of segmenting not only a small pilot set of eight anatomical
landmarks but also an expanded set of 72 landmarks and 16 regions with complex
outlines, such as the femoral cortical bone and the pelvic inlet. By using
YOLO-generated bounding boxes to guide SAM, we trained the hybrid model to
accurately segment orthopaedic pelvic radiographs. Our results show that the
proposed combination of YOLO and SAM yields excellent performance in detecting
anatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.

</details>


### [153] [3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation](https://arxiv.org/abs/2507.11557)
*Jiaxu Zheng,Meiman He,Xuhui Tang,Xiong Wang,Tuoyu Cao,Tianyi Zeng,Lichi Zhang,Chenyu You*

Main category: eess.IV

TL;DR: 提出3D Wavelet Latent Diffusion Model解决现有MR到CT合成方法的不足


<details>
  <summary>Details</summary>
Motivation: 现有MR到CT合成方法在全身成像中存在生成CT与输入MR图像空间对齐差、图像质量不足的问题，影响下游临床任务

Method: 提出3D-WLDM，在学习的潜在空间进行模态转换，将Wavelet Residual Module集成到编解码器架构，在扩散过程中分离结构和特定模态特征，引入Dual Skip Connection Attention机制

Result: 能够生成高分辨率CT图像，更好地表示骨骼结构和软组织对比度

Conclusion: 3D-WLDM有效解决了现有MR到CT合成方法的局限性

Abstract: Magnetic Resonance (MR) imaging plays an essential role in contemporary
clinical diagnostics. It is increasingly integrated into advanced therapeutic
workflows, such as hybrid Positron Emission Tomography/Magnetic Resonance
(PET/MR) imaging and MR-only radiation therapy. These integrated approaches are
critically dependent on accurate estimation of radiation attenuation, which is
typically facilitated by synthesizing Computed Tomography (CT) images from MR
scans to generate attenuation maps. However, existing MR-to-CT synthesis
methods for whole-body imaging often suffer from poor spatial alignment between
the generated CT and input MR images, and insufficient image quality for
reliable use in downstream clinical tasks. In this paper, we present a novel 3D
Wavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by
performing modality translation in a learned latent space. By incorporating a
Wavelet Residual Module into the encoder-decoder architecture, we enhance the
capture and reconstruction of fine-scale features across image and latent
spaces. To preserve anatomical integrity during the diffusion process, we
disentangle structural and modality-specific characteristics and anchor the
structural component to prevent warping. We also introduce a Dual Skip
Connection Attention mechanism within the diffusion model, enabling the
generation of high-resolution CT images with improved representation of bony
structures and soft-tissue contrast.

</details>


### [154] [Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach](https://arxiv.org/abs/2507.11561)
*Lucas Erlacher,Samuel Ruipérez-Campillo,Holger Michel,Sven Wellmann,Thomas M. Sutter,Ece Ozkan,Julia E. Vogt*

Main category: eess.IV

TL;DR: 本文采用多视图变分自编码器（VAE）利用超声心动图视频进行新生儿肺动脉高压（PH）预测，对比单视图和监督学习方法，结果显示多视图学习在PH评估中效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有超声心动图诊断PH依赖操作者且主观性强，自动化检测方法多针对成人、基于单视图且泛化性差，需要更好的方法评估新生儿PH。

Method: 采用多视图变分自编码器（VAE）利用超声心动图视频进行PH预测，并与单视图和监督学习方法对比。

Result: 模型在泛化性和分类准确性上有提升。

Conclusion: 多视图学习对新生儿PH的稳健评估有效。

Abstract: Pulmonary hypertension (PH) in newborns is a critical condition characterized
by elevated pressure in the pulmonary arteries, leading to right ventricular
strain and heart failure. While right heart catheterization (RHC) is the
diagnostic gold standard, echocardiography is preferred due to its non-invasive
nature, safety, and accessibility. However, its accuracy highly depends on the
operator, making PH assessment subjective. While automated detection methods
have been explored, most models focus on adults and rely on single-view
echocardiographic frames, limiting their performance in diagnosing PH in
newborns. While multi-view echocardiography has shown promise in improving PH
assessment, existing models struggle with generalizability. In this work, we
employ a multi-view variational autoencoder (VAE) for PH prediction using
echocardiographic videos. By leveraging the VAE framework, our model captures
complex latent representations, improving feature extraction and robustness. We
compare its performance against single-view and supervised learning approaches.
Our results show improved generalization and classification accuracy,
highlighting the effectiveness of multi-view learning for robust PH assessment
in newborns.

</details>


### [155] [Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?](https://arxiv.org/abs/2507.11569)
*Hanxue Gu,Yaqian Chen,Nicholas Konz,Qihang Li,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: 本文全面评估基于基础模型的乳腺MRI配准算法，发现SAM等算法在整体乳腺对齐上优于传统方法，但在捕捉纤维腺体组织细节上有困难，特定领域训练效果不佳，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 基础模型在零样本图像配准有潜力，但在处理复杂可变形解剖结构如乳腺MRI配准的能力未知，本文旨在评估其在乳腺MRI配准中的表现。

Method: 评估五个预训练编码器（DINO - v2、SAM、MedSAM、SSLSAM和MedCLIP）在四个关键乳腺配准任务中的表现。

Result: SAM等基于基础模型的算法在整体乳腺对齐上优于传统基线，尤其在大领域偏移情况下，但在捕捉纤维腺体组织细节上表现不佳；MedSAM和SSLSAM在医学或乳腺特定图像上的额外预训练或微调未提升配准性能，甚至在某些情况下会降低。

Conclusion: 需要进一步研究特定领域训练对配准的影响，探索提高全局对齐和精细结构准确性的策略，并公开了代码。

Abstract: Foundation models, pre-trained on large image datasets and capable of
capturing rich feature representations, have recently shown potential for
zero-shot image registration. However, their performance has mostly been tested
in the context of rigid or less complex structures, such as the brain or
abdominal organs, and it remains unclear whether these models can handle more
challenging, deformable anatomy. Breast MRI registration is particularly
difficult due to significant anatomical variation between patients, deformation
caused by patient positioning, and the presence of thin and complex internal
structure of fibroglandular tissue, where accurate alignment is crucial.
Whether foundation model-based registration algorithms can address this level
of complexity remains an open question. In this study, we provide a
comprehensive evaluation of foundation model-based registration algorithms for
breast MRI. We assess five pre-trained encoders, including DINO-v2, SAM,
MedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that
capture variations in different years and dates, sequences, modalities, and
patient disease status (lesion versus no lesion). Our results show that
foundation model-based algorithms such as SAM outperform traditional
registration baselines for overall breast alignment, especially under large
domain shifts, but struggle with capturing fine details of fibroglandular
tissue. Interestingly, additional pre-training or fine-tuning on medical or
breast-specific images in MedSAM and SSLSAM, does not improve registration
performance and may even decrease it in some cases. Further work is needed to
understand how domain-specific training influences registration and to explore
targeted strategies that improve both global alignment and fine structure
accuracy. We also publicly release our code at
\href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.

</details>


### [156] [Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease](https://arxiv.org/abs/2507.12012)
*Matthias Perkonigg,Nina Bastati,Ahmed Ba-Ssalamah,Peter Mesenbrink,Alexander Goehler,Miljen Martic,Xiaofei Zhou,Michael Trauner,Georg Langs*

Main category: eess.IV

TL;DR: 本文提出用无监督机器学习识别肝脏组织图像模式词汇表，量化弥漫性肝病治疗反应，并在试验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 可量化的疾病进展和治疗反应的图像模式对指导个体化治疗和开发新疗法至关重要，需找到量化治疗反应的方法。

Method: 使用深度聚类网络对医学图像块进行编码和聚类，建立组织词汇表。

Result: 该方法能识别与治疗相关的特定肝脏组织变化途径，比现有非成像指标能更好区分治疗组，还能从无创成像数据预测活检特征。

Conclusion: 在独立验证队列中验证了该方法的适用性。

Abstract: Quantifiable image patterns associated with disease progression and treatment
response are critical tools for guiding individual treatment, and for
developing novel therapies. Here, we show that unsupervised machine learning
can identify a pattern vocabulary of liver tissue in magnetic resonance images
that quantifies treatment response in diffuse liver disease. Deep clustering
networks simultaneously encode and cluster patches of medical images into a
low-dimensional latent space to establish a tissue vocabulary. The resulting
tissue types capture differential tissue change and its location in the liver
associated with treatment response. We demonstrate the utility of the
vocabulary on a randomized controlled trial cohort of non-alcoholic
steatohepatitis patients. First, we use the vocabulary to compare longitudinal
liver change in a placebo and a treatment cohort. Results show that the method
identifies specific liver tissue change pathways associated with treatment, and
enables a better separation between treatment groups than established
non-imaging measures. Moreover, we show that the vocabulary can predict biopsy
derived features from non-invasive imaging data. We validate the method on a
separate replication cohort to demonstrate the applicability of the proposed
method.

</details>


### [157] [Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation](https://arxiv.org/abs/2507.12427)
*Ashkan Shakarami,Azade Farshad,Yousef Yeganeh,Lorenzo Nicole,Peter Schuffler,Stefano Ghidoni,Nassir Navab*

Main category: eess.IV

TL;DR: 提出基于单元的组织分割框架UTS，以32*32图块为分割单元，引入L - ViT，在乳腺组织分割任务中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 减少组织病理学分割的标注工作量并提高计算效率，同时支持临床相关任务。

Method: 提出以32*32图块为分割单元的UTS框架，引入Multi - Level Vision Transformer (L - ViT) 进行多级别特征表示。

Result: 在459个H&E染色区域的386,371个图块上评估，优于U - Net变体和基于Transformer的基线。

Conclusion: UTS框架有效，可减少标注工作量、提高计算效率且不损失准确性，能支持临床任务。

Abstract: We propose UTS, a unit-based tissue segmentation framework for histopathology
that classifies each fixed-size 32 * 32 tile, rather than each pixel, as the
segmentation unit. This approach reduces annotation effort and improves
computational efficiency without compromising accuracy. To implement this
approach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits
the multi-level feature representation to capture both fine-grained morphology
and global tissue context. Trained to segment breast tissue into three
categories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports
clinically relevant tasks such as tumor-stroma quantification and surgical
margin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it
outperforms U-Net variants and transformer-based baselines. Code and Dataset
will be available at GitHub.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [158] [A Cellular Automata Approach to Donation Game](https://arxiv.org/abs/2507.11744)
*Marcin Kowalik,Przemysław Stokłosa,Mateusz Grabowski,Janusz Starzyk,Paweł Raif*

Main category: cs.MA

TL;DR: 本文用一维二元元胞自动机研究捐赠游戏中合作动态，发现合作受主体移动性和空间局部性影响。


<details>
  <summary>Details</summary>
Motivation: 传统模拟常假设完全随机交互，本文旨在研究主体仅与相邻主体交互时合作如何演化。

Method: 使用Stephen Wolfram的一维二元元胞自动机概念，定义符合捐赠游戏机制的规则，引入感知和行动噪声模型以及突变矩阵。

Result: 合作显著受主体移动性和游戏板上空间局部性影响。

Conclusion: 区分完全随机多主体系统和主体更可能与近邻交互的系统很重要。

Abstract: The donation game is a well-established framework for studying the emergence
and evolution of cooperation in multi-agent systems. The cooperative behavior
can be influenced by the environmental noise in partially observable settings
and by the decision-making strategies of agents, which may incorporate not only
reputation but also traits such as generosity and forgiveness. Traditional
simulations often assume fully random interactions, where cooperation is tested
between randomly selected agent pairs. In this paper, we investigate
cooperation dynamics using the concept of Stephen Wolfram's one-dimensional
binary cellular automata. This approach allows us to explore how cooperation
evolves when interactions are limited to neighboring agents. We define binary
cellular automata rules that conform to the donation game mechanics.
Additionally, we introduce models of perceptual and action noise, along with a
mutation matrix governing the probabilistic evolution of agent strategies. Our
empirical results demonstrate that cooperation is significantly affected by
agents' mobility and their spatial locality on the game board. These findings
highlight the importance of distinguishing between entirely random multi-agent
systems and those in which agents are more likely to interact with their
nearest neighbors.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [159] [Model averaging in the space of probability distributions](https://arxiv.org/abs/2507.11719)
*Emmanouil Androulakis,Georgios I. Papayiannis,Athanasios N. Yannacopoulos*

Main category: stat.ME

TL;DR: 研究度量值数据下模型平均问题，用Wasserstein距离度量概率分布空间聚合方案，采用正则化提升性能，通过实验评估方法并应用于保险损失数据集。


<details>
  <summary>Details</summary>
Motivation: 解决度量值数据下的模型平均问题，提升模型性能，以更好地处理经验数据和估计风险。

Method: 研究Wasserstein距离度量的概率分布空间聚合方案，采用类似弹性网络惩罚的正则化方案，用Γ - 收敛的变分框架建立一致性，通过合成实验评估，应用于实际数据集。

Result: 正则化方案能产生具有稀疏性的模型，方法的一致性得到严谨证明，通过实验评估了方法性能。

Conclusion: 提出的方法在合成实验和实际保险损失数据集上都有应用价值，可用于估计索赔规模分布和相关尾部风险。

Abstract: This work investigates the problem of model averaging in the context of
measure-valued data. Specifically, we study aggregation schemes in the space of
probability distributions metrized in terms of the Wasserstein distance. The
resulting aggregate models, defined via Wasserstein barycenters, are optimally
calibrated to empirical data. To enhance model performance, we employ
regularization schemes motivated by the standard elastic net penalization,
which is shown to consistently yield models enjoying sparsity properties. The
consistency properties of the proposed averaging schemes with respect to sample
size are rigorously established using the variational framework of
$\Gamma$-convergence. The performance of the methods is evaluated through
carefully designed synthetic experiments that assess behavior across a range of
distributional characteristics and stress conditions. Finally, the proposed
approach is applied to a real-world dataset of insurance losses - characterized
by heavy-tailed behavior - to estimate the claim size distribution and the
associated tail risk.

</details>


### [160] [Fiducial Matching: Differentially Private Inference for Categorical Data](https://arxiv.org/abs/2507.11762)
*Ogonnaya Michael Romanus,Younes Boulaguiem,Roberto Molinari*

Main category: stat.ME

TL;DR: 本文采用基于模拟的匹配方法，结合置信框架工具，研究差分隐私（DP）环境下分类数据统计推断，证明方法有效性并展示其良好性能。


<details>
  <summary>Details</summary>
Motivation: DP环境下统计推断仍是开放研究领域，因数据采样和噪声添加导致确定统计量和参数随机行为复杂。

Method: 采用基于模拟的匹配方法，通过置信框架工具解决，聚焦分类数据和加法隐私机制。

Result: 证明了所提方法在覆盖率方面的有效性，在模拟和实际数据中展示了良好计算和统计性能。

Conclusion: 所提方法可用于DP环境下分类数据的不同推断任务。

Abstract: The task of statistical inference, which includes the building of confidence
intervals and tests for parameters and effects of interest to a researcher, is
still an open area of investigation in a differentially private (DP) setting.
Indeed, in addition to the randomness due to data sampling, DP delivers another
source of randomness consisting of the noise added to protect an individual's
data from being disclosed to a potential attacker. As a result of this
convolution of noises, in many cases it is too complicated to determine the
stochastic behavior of the statistics and parameters resulting from a DP
procedure. In this work, we contribute to this line of investigation by
employing a simulation-based matching approach, solved through tools from the
fiducial framework, which aims to replicate the data generation pipeline
(including the DP step) and retrieve an approximate distribution of the
estimates resulting from this pipeline. For this purpose, we focus on the
analysis of categorical (nominal) data that is common in national surveys, for
which sensitivity is naturally defined, and on additive privacy mechanisms. We
prove the validity of the proposed approach in terms of coverage and highlight
its good computational and statistical performance for different inferential
tasks in simulated and applied data settings.

</details>


### [161] [Bayesian multivariate models for bounded directional data](https://arxiv.org/abs/2507.11784)
*Joel Montesinos-Vazquez,Gabriel Núñez-Antonio*

Main category: stat.ME

TL;DR: 本文提出基于copula函数构建多变量模型，各边缘变量为仅定义在单位圆第一象限的循环变量，通过条件方法进行推断并结合模拟和真实数据说明方法。


<details>
  <summary>Details</summary>
Motivation: 现有描述单位球子集变量的模型中，边缘变量仅定义在单位圆部分且具有灵活依赖结构的多变量模型较少，有构建此类模型的需求。

Method: 基于copula函数构建多变量模型，通过条件方法进行两阶段抽样生成模型所有参数后验联合密度的样本以进行推断。

Result: 使用模拟和真实数据对所提方法进行了说明。

Conclusion: 提出了构建边缘变量仅定义在单位圆第一象限的多变量模型的方法。

Abstract: In some areas of knowledge there are data representing directions restricted
to a specific range of values. Consequently, it is useful to have models for
describing variables defined in subsets of the k-dimensional unit sphere. This
need has led to the development of models such as the multivariate projected
Gamma distribution. However, the proposal of multivariate models whose marginal
variables are defined only in sections of the unit circle and with a flexible
dependency structure is limited. In this work, we propose constructing
multivariate models where each marginal variable is a circular variable defined
only in the first quadrant of the unit circle. Our approach is based on the
concept of copula functions. The inferences for the proposed models rely on
generating samples of the posterior joint density of all parameters involved in
the models. This is achieved by applying a conditional approach that allows
inferences to be made using a two-stage sampling. The proposed methodology is
illustrated with both simulated and real data.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [162] [The Evolving Role of Large Language Models in Scientific Innovation: Evaluator, Collaborator, and Scientist](https://arxiv.org/abs/2507.11810)
*Haoxuan Zhang,Ruochi Li,Yang Zhang,Ting Xiao,Jiangping Chen,Junhua Ding,Haihua Chen*

Main category: cs.DL

TL;DR: 文章指出大语言模型驱动科学创新范式转变，提出综合框架对其在科学创新中的角色分类，深入分析相关内容并给出研究启示与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有调查在理解大语言模型变革潜力和角色区分上有局限，科学面临信息过载等挑战，需研究大语言模型在科学创新中的作用。

Method: 提出综合框架将大语言模型在科学创新中的角色分为三个层次，区分其在结构化科研和开放式科学发现中的贡献，分析现有方法、基准等。

Result: 对大语言模型驱动的科学创新进行深入系统综合，明确其不仅是自动化工具，还能重塑科学认识论基础。

Conclusion: 为未来研究提供概念清晰性、实践指导和理论基础，指出开放挑战和伦理考量。

Abstract: Scientific innovation is undergoing a paradigm shift driven by the rapid
advancement of Large Language Models (LLMs). As science faces mounting
challenges including information overload, disciplinary silos, and diminishing
returns on conventional research methods, LLMs are emerging as powerful agents
capable not only of enhancing scientific workflows but also of participating in
and potentially leading the innovation process. Existing surveys mainly focus
on different perspectives, phrases, and tasks in scientific research and
discovery, while they have limitations in understanding the transformative
potential and role differentiation of LLM. This survey proposes a comprehensive
framework to categorize the evolving roles of LLMs in scientific innovation
across three hierarchical levels: Evaluator, Collaborator, and Scientist. We
distinguish between LLMs' contributions to structured scientific research
processes and open-ended scientific discovery, thereby offering a unified
taxonomy that clarifies capability boundaries, evaluation criteria, and
human-AI interaction patterns at each level. Through an extensive analysis of
current methodologies, benchmarks, systems, and evaluation metrics, this survey
delivers an in-depth and systematic synthesis on LLM-driven scientific
innovation. We present LLMs not only as tools for automating existing
processes, but also as catalysts capable of reshaping the epistemological
foundations of science itself. This survey offers conceptual clarity, practical
guidance, and theoretical foundations for future research, while also
highlighting open challenges and ethical considerations in the pursuit of
increasingly autonomous AI-driven science. Resources related to this survey can
be accessed on GitHub at: https://github.com/haoxuan-unt2024/llm4innovation.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [163] [Formal Verification of Neural Certificates Done Dynamically](https://arxiv.org/abs/2507.11987)
*Thomas A. Henzinger,Konstantin Kueffner,Emily Yu*

Main category: cs.SC

TL;DR: 提出轻量级运行时监控框架解决神经证书形式验证的可扩展性挑战，在案例中展示有效性。


<details>
  <summary>Details</summary>
Motivation: 传统神经证书形式验证因状态空间穷举探索面临可扩展性挑战。

Method: 提出轻量级运行时监控框架，在部署时观察系统，对前瞻区域内的证书进行即时验证。

Result: 在ReLU控制障碍函数案例中展示了方法的有效性。

Conclusion: 该方法能及时检测安全违规和错误证书，开销小，是静态验证的有效轻量级替代方案。

Abstract: Neural certificates have emerged as a powerful tool in cyber-physical systems
control, providing witnesses of correctness. These certificates, such as
barrier functions, often learned alongside control policies, once verified,
serve as mathematical proofs of system safety. However, traditional formal
verification of their defining conditions typically faces scalability
challenges due to exhaustive state-space exploration. To address this
challenge, we propose a lightweight runtime monitoring framework that
integrates real-time verification and does not require access to the underlying
control policy. Our monitor observes the system during deployment and performs
on-the-fly verification of the certificate over a lookahead region to ensure
safety within a finite prediction horizon. We instantiate this framework for
ReLU-based control barrier functions and demonstrate its practical
effectiveness in a case study. Our approach enables timely detection of safety
violations and incorrect certificates with minimal overhead, providing an
effective but lightweight alternative to the static verification of the
certificates.

</details>


### [164] [FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization](https://arxiv.org/abs/2507.12366)
*Yifei Zhou,Xuchu Huang,Chenyu Ni,Min Zhou,Zheyu Yan,Xunzhao Yin,Cheng Zhuo*

Main category: cs.SC

TL;DR: 本文提出FactorHD模型，能高效表示和分解复杂的类 - 子类关系，提升计算效率和准确性，评估显示有速度提升和高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有HDC模型在表示复杂的类 - 子类关系时面临分解挑战，无法满足神经符号AI系统需求。

Method: 提出FactorHD模型，采用符号编码方法嵌入额外记忆子句，运用高效分解算法选择性消除冗余类。

Result: 在表示大小为10^9时，比现有HDC模型提速约5667倍；与ResNet - 18集成时，在Cifar - 10数据集上分解准确率达92.48%。

Conclusion: FactorHD模型克服了现有HDC模型的局限，在表示和分解类 - 子类关系上有显著优势。

Abstract: Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical
analysis and reasoning. Hyperdimensional Computing (HDC), a promising
brain-inspired computational model, is integral to neuro-symbolic AI. Various
HDC models have been proposed to represent class-instance and class-class
relations, but when representing the more complex class-subclass relation,
where multiple objects associate different levels of classes and subclasses,
they face challenges for factorization, a crucial task for neuro-symbolic AI
systems. In this article, we propose FactorHD, a novel HDC model capable of
representing and factorizing the complex class-subclass relation efficiently.
FactorHD features a symbolic encoding method that embeds an extra memorization
clause, preserving more information for multiple objects. In addition, it
employs an efficient factorization algorithm that selectively eliminates
redundant classes by identifying the memorization clause of the target class.
Such model significantly enhances computing efficiency and accuracy in
representing and factorizing multiple objects with class-subclass relation,
overcoming limitations of existing HDC models such as "superposition
catastrophe" and "the problem of 2". Evaluations show that FactorHD achieves
approximately 5667x speedup at a representation size of 10^9 compared to
existing HDC models. When integrated with the ResNet-18 neural network,
FactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [165] [Recent results on searches with boosted Higgs bosons at CMS](https://arxiv.org/abs/2507.11977)
*Farouk Mokhtar*

Main category: hep-ex

TL;DR: 本文介绍CMS实验中高能希格斯玻色子搜索的最新结果及创新技术。


<details>
  <summary>Details</summary>
Motivation: 通过研究大型强子对撞机（LHC）上的高能希格斯玻色子，探索高能尺度下希格斯玻色子的耦合以及寻找超出标准模型的物理迹象。

Method: 采用创新的重建和标记技术。

Result: 获得了CMS实验中高能希格斯玻色子搜索的最新结果。

Conclusion: 创新技术可提高在该挑战性领域的灵敏度。

Abstract: The study of boosted Higgs bosons at the LHC provides a unique window to
probe Higgs boson couplings at high energy scales and search for signs of
physics beyond the standard model. In these proceedings, we present recent
results on boosted Higgs boson searches at the CMS experiment, highlighting
innovative reconstruction and tagging techniques that enhance sensitivity in
this challenging regime.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [166] [MOFSimBench: Evaluating Universal Machine Learning Interatomic Potentials In Metal--Organic Framework Molecular Modeling](https://arxiv.org/abs/2507.11806)
*Hendrik Kraß,Ju Huang,Seyed Mohamad Moosavi*

Main category: cond-mat.mtrl-sci

TL;DR: 介绍MOFSimBench评估通用机器学习原子间势(uMLIPs)在纳米多孔材料建模任务的表现，发现顶级uMLIPs表现佳，数据质量比模型架构更重要并开源框架。


<details>
  <summary>Details</summary>
Motivation: uMLIPs在实际应用中的可靠性和有效性存疑，纳米多孔材料建模对uMLIPs有独特挑战，需评估其在相关任务的表现。

Method: 引入MOFSimBench基准，在化学和结构多样的材料集上评估20多个不同架构的模型。

Result: 顶级uMLIPs在所有任务中始终优于经典力场和微调的机器学习势。

Conclusion: 数据质量在决定uMLIPs性能上比模型架构更关键，开源基准框架以指导纳米多孔材料建模和uMLIPs进一步发展。

Abstract: Universal machine learning interatomic potentials (uMLIPs) have emerged as
powerful tools for accelerating atomistic simulations, offering scalable and
efficient modeling with accuracy close to quantum calculations. However, their
reliability and effectiveness in practical, real-world applications remain an
open question. Metal-organic frameworks (MOFs) and related nanoporous materials
are highly porous crystals with critical relevance in carbon capture, energy
storage, and catalysis applications. Modeling nanoporous materials presents
distinct challenges for uMLIPs due to their diverse chemistry, structural
complexity, including porosity and coordination bonds, and the absence from
existing training datasets. Here, we introduce MOFSimBench, a benchmark to
evaluate uMLIPs on key materials modeling tasks for nanoporous materials,
including structural optimization, molecular dynamics (MD) stability, the
prediction of bulk properties, such as bulk modulus and heat capacity, and
guest-host interactions. Evaluating over 20 models from various architectures
on a chemically and structurally diverse materials set, we find that
top-performing uMLIPs consistently outperform classical force fields and
fine-tuned machine learning potentials across all tasks, demonstrating their
readiness for deployment in nanoporous materials modeling. Our analysis
highlights that data quality, particularly the diversity of training sets and
inclusion of out-of-equilibrium conformations, plays a more critical role than
model architecture in determining performance across all evaluated uMLIPs. We
release our modular and extendable benchmarking framework at
https://github.com/AI4ChemS/mofsim-bench, providing an open resource to guide
the adoption for nanoporous materials modeling and further development of
uMLIPs.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [167] [Orchestrating the Implementation of the Smart City](https://arxiv.org/abs/2507.12267)
*Filippo Marchesani*

Main category: econ.GN

TL;DR: 文章探讨智慧城市六大核心维度的相互依赖与整体协调，强调整合对可持续发展的重要性，指出ICT需与人力和社会资本结合，通过实例说明治理和能力对转型的影响，倡导以人为本。


<details>
  <summary>Details</summary>
Motivation: 研究智慧城市六大核心维度相互关系及如何实现可持续城市发展。

Method: 基于Giffinger等人（2007）及后续文献，结合机构实例。

Result: 说明了治理和内部能力对智慧城市转型的影响，强调了以人为本的重要性。

Conclusion: 智慧城市应被视为技术、人员和治理动态互动的城市生态系统。

Abstract: This chapter explores the six core dimensions of smart cities (i.e. smart
economy, mobility, environment, people, living, and governance) emphasizing
their interdependence and the need for holistic orchestration. Building on
Giffinger et al. (2007) and subsequent literature, it argues that integrating
these dimensions is crucial for sustainable urban development. ICT plays a key
enabling role but must be complemented by human and social capital. Through
institutional examples, such as the creation of dedicated municipal offices for
digital innovation, the chapter illustrates how governance and internal
capacity shape smart transitions. A human-centric approach is also essential,
ensuring inclusivity, creativity, and active civic participation. Ultimately,
smart cities must be viewed as cohesive urban ecosystems where technology,
people, and governance interact dynamically.

</details>


### [168] [International promotion patterns in the smart city literature: Exploring the role of geography in affecting local drivers and smart cities' outcomes](https://arxiv.org/abs/2507.12281)
*Filippo Marchesani,Francesca Masciarelli,Andrea Bikfalvi*

Main category: econ.GN

TL;DR: 对2008 - 2021年顶级同行评审期刊文章进行系统文献综述，分析智慧城市现有文献。


<details>
  <summary>Details</summary>
Motivation: 智慧城市虽兴起但国际推广受关注晚，其多学科性质导致全球认可度不确定，需研究。

Method: 对2008年至2021年12月发表在顶级同行评审期刊上的文章进行系统文献综述。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: The rise of smart cities represents a significant trend in urban development.
However, only in recent years has attention shifted toward the international
promotion of these cities. Despite ongoing academic discussions on the impact
of smart city development on urban environments, the global recognition of
smart cities remains uncertain due to their multidisciplinary nature. To
address this, we conducted a systematic literature review of articles published
in top-tier peer-reviewed journals from 2008 to December 2021, offering a
comprehensive analysis of the existing literature.

</details>


### [169] [Causality analysis of electricity market liberalization on electricity price using novel Machine Learning methods](https://arxiv.org/abs/2507.12331)
*Orr Shahar,Stefan Lessmann,Daniel Traian Pele*

Main category: econ.GN

TL;DR: 本文研究美国电力市场自由化对电价的因果影响，引入因果机器学习方法，发现DeepProbCP框架表现最佳，且市场自由化短期内使电价降7%。


<details>
  <summary>Details</summary>
Motivation: 理解能源和金融市场关系对政策制定者等很重要，为电力市场自由化益处的讨论提供新见解。

Method: 引入因果机器学习作为能源金融领域干预新方法，比较基于机器学习模型的性能。

Result: DeepProbCP框架优于其他被考察框架；电力市场自由化和个体参与者进入使短期内电价下降7%。

Conclusion: 因果机器学习方法适用于能源政策干预案例，可用于分析电力市场自由化对电价的影响。

Abstract: Relationships between the energy and the finance markets are increasingly
important. Understanding these relationships is vital for policymakers and
other stakeholders as the world faces challenges such as satisfying humanity's
increasing need for energy and the effects of climate change. In this paper, we
investigate the causal effect of electricity market liberalization on the
electricity price in the US. By performing this analysis, we aim to provide new
insights into the ongoing debate about the benefits of electricity market
liberalization. We introduce Causal Machine Learning as a new approach for
interventions in the energy-finance field. The development of machine learning
in recent years opened the door for a new branch of machine learning models for
causality impact, with the ability to extract complex patterns and
relationships from the data. We discuss the advantages of causal ML methods and
compare the performance of ML-based models to shed light on the applicability
of causal ML frameworks to energy policy intervention cases. We find that the
DeepProbCP framework outperforms the other frameworks examined. In addition, we
find that liberalization of, and individual players' entry to, the electricity
market resulted in a 7% decrease in price in the short term.

</details>


### [170] [The Case against Scale: Empirical Evidence of Underperformance in Large Secondary Funds](https://arxiv.org/abs/2507.12436)
*Jitesh Gurav*

Main category: econ.GN

TL;DR: 文章分析私募二级市场大基金流行现象，指出实证显示小基金内部收益率表现更好，建议投资者重新审视投资分配策略。


<details>
  <summary>Details</summary>
Motivation: 质疑大基金因规模优势更受欢迎的权威观点，探究小基金的优势。

Method: 引用实证研究证明小基金内部收益率比大基金更好。

Result: 发现小基金在内部收益率方面优于大基金。

Conclusion: 投资者应重新审视自动优先配置大基金的策略。

Abstract: The paper analyses the increasing popularity of large funds in the secondary
private equity market, which are pegged on the perceived larger scale
advantages of operational efficiency and fewer manager relationships (Reuter &
Zitzewitz, 2021). However, it has been proved empirically that smaller funds
perform better than the big ones in terms of their internal rates of return
(IRRs). This work questions this authoritative view, providing evidence of
outperformance of smaller funds relative to their larger counterparts. This
research shows us the benefits that smaller funds possess. Thus, investors must
revisit their allocation strategy to prioritize larger funds automatically
(Gualandris et al.,2021).

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [171] [An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search](https://arxiv.org/abs/2507.11549)
*Wendong Mao,Mingfan Zhao,Jianfeng Guan,Qiwei Dong,Zhongfeng Wang*

Main category: cs.CV

TL;DR: 本文提出硬件友好的DAT优化框架，可降低DRAM访问次数且精度损失小。


<details>
  <summary>Details</summary>
Motivation: DAT不规则内存访问模式给硬件部署带来挑战，现有加速方法有高硬件开销或精度损失问题。

Method: 提出基于NAS的方法和新切片策略，自动划分输入特征，联合优化硬件成本和推理精度；设计FPGA验证系统。

Result: 算法实验显示框架与基线DAT相比精度仅下降0.2%；硬件实验表明该方法较现有DAT加速方法将DRAM访问次数降至18%。

Conclusion: 提出的硬件友好优化框架能有效解决DAT硬件部署问题，兼顾精度和硬件效率。

Abstract: Deformable Attention Transformers (DAT) have shown remarkable performance in
computer vision tasks by adaptively focusing on informative image regions.
However, their data-dependent sampling mechanism introduces irregular memory
access patterns, posing significant challenges for efficient hardware
deployment. Existing acceleration methods either incur high hardware overhead
or compromise model accuracy. To address these issues, this paper proposes a
hardware-friendly optimization framework for DAT. First, a neural architecture
search (NAS)-based method with a new slicing strategy is proposed to
automatically divide the input feature into uniform patches during the
inference process, avoiding memory conflicts without modifying model
architecture. The method explores the optimal slice configuration by jointly
optimizing hardware cost and inference accuracy. Secondly, an FPGA-based
verification system is designed to test the performance of this framework on
edge-side hardware. Algorithm experiments on the ImageNet-1K dataset
demonstrate that our hardware-friendly framework can maintain have only 0.2%
accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA
show the proposed method reduces DRAM access times to 18% compared with
existing DAT acceleration methods.

</details>


### [172] [Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction](https://arxiv.org/abs/2507.11550)
*Hyeonseok Jin,Geonmin Kim,Kyungbaek Kim*

Main category: cs.CV

TL;DR: 本文提出可变形动态卷积网络（DDCN）用于时空交通预测，在四个真实数据集上取得有竞争力的性能，凸显基于CNN方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测方法难以捕捉异质性，图神经网络需预定义邻接矩阵且扩展性受限，传统CNN在建模非欧几里得空间结构和时空异质性上有限。

Method: 提出DDCN，将Transformer风格的CNN分解为编解码器结构，在编码器的空间和时空注意力块应用相关方法强调重要特征，解码器由前馈模块组成补充编码器输出。

Result: 在四个真实数据集上，DDCN取得有竞争力的性能。

Conclusion: DDCN能实现准确且高效的交通预测，基于CNN的方法在时空交通预测中有潜力和有效性。

Abstract: Spatio-temporal traffic prediction plays a key role in intelligent
transportation systems by enabling accurate prediction in complex urban areas.
Although not only accuracy but also efficiency for scalability is important,
some previous methods struggle to capture heterogeneity such as varying traffic
patterns across regions and time periods. Moreover, Graph Neural Networks
(GNNs), which are the mainstream of traffic prediction, not only require
predefined adjacency matrix, but also limit scalability to large-scale data
containing many nodes due to their inherent complexity. To overcome these
limitations, we propose Deformable Dynamic Convolution Network (DDCN) for
accurate yet efficient traffic prediction. Traditional Convolutional Neural
Networks (CNNs) are limited in modeling non-Euclidean spatial structures and
spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically
applying deformable filters based on offset. Specifically, DDCN decomposes
transformer-style CNN to encoder-decoder structure, and applies proposed
approaches to the spatial and spatio-temporal attention blocks of the encoder
to emphasize important features. The decoder, composed of feed-forward module,
complements the output of the encoder. This novel structure make DDCN can
perform accurate yet efficient traffic prediction. In comprehensive experiments
on four real-world datasets, DDCN achieves competitive performance, emphasizing
the potential and effectiveness of CNN-based approaches for spatio-temporal
traffic prediction.

</details>


### [173] [Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models](https://arxiv.org/abs/2507.11554)
*Zejian Li,Yize Li,Chenye Meng,Zhongni Liu,Yang Ling,Shengyuan Zhang,Guang Yang,Changyuan Yang,Zhiyuan Yang,Lingyun Sun*

Main category: cs.CV

TL;DR: 提出Inversion - DPO框架解决扩散模型对齐方法计算开销大等问题，实验显示性能提升，推进模型在复杂任务应用。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型对齐方法需计算密集训练，有计算开销大、影响准确性和效率等问题。

Method: 提出Inversion - DPO框架，通过DDIM反演重新表述DPO，在Diffusion - DPO中进行后验采样得到新的后训练范式。

Result: 应用于文本到图像生成和组合图像生成任务，相比现有方法有显著性能提升，能生成高保真、组合连贯图像。

Conclusion: Inversion - DPO为扩散模型高效高精度对齐探索新途径，推进其在复杂现实生成任务的应用。

Abstract: Recent advancements in diffusion models (DMs) have been propelled by
alignment methods that post-train models to better conform to human
preferences. However, these approaches typically require computation-intensive
training of a base model and a reward model, which not only incurs substantial
computational overhead but may also compromise model accuracy and training
efficiency. To address these limitations, we propose Inversion-DPO, a novel
alignment framework that circumvents reward modeling by reformulating Direct
Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts
intractable posterior sampling in Diffusion-DPO with the deterministic
inversion from winning and losing samples to noise and thus derive a new
post-training paradigm. This paradigm eliminates the need for auxiliary reward
models or inaccurate appromixation, significantly enhancing both precision and
efficiency of training. We apply Inversion-DPO to a basic task of text-to-image
generation and a challenging task of compositional image generation. Extensive
experiments show substantial performance improvements achieved by Inversion-DPO
compared to existing post-training methods and highlight the ability of the
trained generative models to generate high-fidelity compositionally coherent
images. For the post-training of compostitional image geneation, we curate a
paired dataset consisting of 11,140 images with complex structural annotations
and comprehensive scores, designed to enhance the compositional capabilities of
generative models. Inversion-DPO explores a new avenue for efficient,
high-precision alignment in diffusion models, advancing their applicability to
complex realistic generation tasks. Our code is available at
https://github.com/MIGHTYEZ/Inversion-DPO

</details>


### [174] [Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2507.11558)
*Changlu Chen,Yanbin Liu,Chaoxi Niu,Ling Chen,Tianqing Zhu*

Main category: cs.CV

TL;DR: 本文提出ST - VFM框架，用于时空预测，通过双分支架构和两个重编程阶段处理输入，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以捕捉时空预测所需的时空相关性，视觉基础模型应用于时空任务存在缺乏时间建模能力和模态差距的问题。

Method: 采用双分支架构，整合原始时空输入和辅助时空流输入；引入预VFM重编程阶段和后VFM重编程阶段处理输入。

Result: 在十个时空数据集上的实验表明，ST - VFM优于现有基线，在不同VFM骨干上都表现出有效性和鲁棒性。

Conclusion: ST - VFM是一个强大的通用时空预测框架。

Abstract: Foundation models have achieved remarkable success in natural language
processing and computer vision, demonstrating strong capabilities in modeling
complex patterns. While recent efforts have explored adapting large language
models (LLMs) for time-series forecasting, LLMs primarily capture
one-dimensional sequential dependencies and struggle to model the richer
spatio-temporal (ST) correlations essential for accurate ST forecasting. In
this paper, we present \textbf{ST-VFM}, a novel framework that systematically
reprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal
forecasting. While VFMs offer powerful spatial priors, two key challenges arise
when applying them to ST tasks: (1) the lack of inherent temporal modeling
capacity and (2) the modality gap between visual and ST data. To address these,
ST-VFM adopts a \emph{dual-branch architecture} that integrates raw ST inputs
with auxiliary ST flow inputs, where the flow encodes lightweight temporal
difference signals interpretable as dynamic spatial cues. To effectively
process these dual-branch inputs, ST-VFM introduces two dedicated reprogramming
stages. The \emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token
Adapter to embed temporal context and align both branches into VFM-compatible
feature spaces. The \emph{post-VFM reprogramming} stage introduces a Bilateral
Cross-Prompt Coordination module, enabling dynamic interaction between branches
through prompt-based conditioning, thus enriching joint representation learning
without modifying the frozen VFM backbone. Extensive experiments on ten
spatio-temporal datasets show that ST-VFM outperforms state-of-the-art
baselines, demonstrating effectiveness and robustness across VFM backbones
(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong
general framework for spatio-temporal forecasting.

</details>


### [175] [Expert Operational GANS: Towards Real-Color Underwater Image Restoration](https://arxiv.org/abs/2507.11562)
*Ozer Can Devecioglu,Serkan Kiranyaz,Mehmet Yamac,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 提出xOp - GAN用于水下图像恢复，含多个专家生成器网络，实验表明其PSNR达25.16 dB，优于单回归模型且复杂度低。


<details>
  <summary>Details</summary>
Motivation: 传统基于GAN的水下图像恢复方法因单一生成器网络难以应对复杂图像退化，恢复效果不佳。

Method: 提出xOp - GAN，有多个专家生成器网络，每个在特定图像质量子集上训练，推理时鉴别器根据感知置信度分数选最佳恢复图像。

Result: 在LSUI数据集上实验，xOp - GAN的PSNR达25.16 dB，大幅超越单回归模型，且复杂度降低。

Conclusion: xOp - GAN是首个在回归任务推理中使用鉴别器的多生成器GAN模型，能有效进行水下图像恢复。

Abstract: The wide range of deformation artifacts that arise from complex light
propagation, scattering, and depth-dependent attenuation makes the underwater
image restoration to remain a challenging problem. Like other single deep
regressor networks, conventional GAN-based restoration methods struggle to
perform well across this heterogeneous domain, since a single generator network
is typically insufficient to capture the full range of visual degradations. In
order to overcome this limitation, we propose xOp-GAN, a novel GAN model with
several expert generator networks, each trained solely on a particular subset
with a certain image quality. Thus, each generator can learn to maximize its
restoration performance for a particular quality range. Once a xOp-GAN is
trained, each generator can restore the input image and the best restored image
can then be selected by the discriminator based on its perceptual confidence
score. As a result, xOP-GAN is the first GAN model with multiple generators
where the discriminator is being used during the inference of the regression
task. Experimental results on benchmark Large Scale Underwater Image (LSUI)
dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,
surpassing all single-regressor models by a large margin even, with reduced
complexity.

</details>


### [176] [What cat is that? A re-id model for feral cats](https://arxiv.org/abs/2507.11575)
*Victor Caquilpan*

Main category: cs.CV

TL;DR: 本文探索CV方法构建再识别模型识别野猫，修改PPGNet为PPGNet - Cat，实验表明该模型效果好。


<details>
  <summary>Details</summary>
Motivation: 野猫对澳大利亚野生动物危害大，需密切监测，再识别技术可提升监测效果。

Method: 修改用于东北虎再识别的PPGNet模型为PPGNet - Cat以适配野猫图像，开展含ArcFace损失等对比学习实验。

Result: PPGNet - Cat识别野猫表现出色，平均精度均值（mAP）达0.86，rank - 1准确率达0.95。

Conclusion: PPGNet - Cat是再识别领域有竞争力的模型。

Abstract: Feral cats exert a substantial and detrimental impact on Australian wildlife,
placing them among the most dangerous invasive species worldwide. Therefore,
closely monitoring these cats is essential labour in minimising their effects.
In this context, the potential application of Re-Identification (re-ID) emerges
to enhance monitoring activities for these animals, utilising images captured
by camera traps. This project explores different CV approaches to create a
re-ID model able to identify individual feral cats in the wild. The main
approach consists of modifying a part-pose guided network (PPGNet) model,
initially used in the re-ID of Amur tigers, to be applicable for feral cats.
This adaptation, resulting in PPGNet-Cat, which incorporates specific
modifications to suit the characteristics of feral cats images. Additionally,
various experiments were conducted, particularly exploring contrastive learning
approaches such as ArcFace loss. The main results indicate that PPGNet-Cat
excels in identifying feral cats, achieving high performance with a mean
Average Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes
establish PPGNet-Cat as a competitive model within the realm of re-ID.

</details>


### [177] [Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders](https://arxiv.org/abs/2507.11638)
*Benjamin Keel,Aaron Quyn,David Jayne,Maryam Mohsin,Samuel D. Relton*

Main category: cs.CV

TL;DR: 本文用VAE作为特征编码器模型替代CNN，在MRI数据集上提出的VAE - MLP模型取得了先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于淋巴结大小、形状和纹理形态的放射学标准诊断准确性有限，VAE生成模型能直接编码视觉特征和有意义模式，其潜在空间比CNN更具可解释性。

Method: 用VAE作为特征编码器模型替代现有方法中的大型预训练CNN，在含168名未接受新辅助治疗患者的内部MRI数据集上部署模型，以术后病理N阶段为真实值评估模型预测。

Result: 提出的VAE - MLP模型在MRI数据集上取得了先进性能，交叉验证指标为AUC 0.86 +/- 0.05，灵敏度0.79 +/- 0.06，特异性0.85 +/- 0.05。

Conclusion: 使用VAE作为特征编码器模型用于直肠癌淋巴结转移分期是有效的，提出的VAE - MLP模型表现良好。

Abstract: Effective treatment for rectal cancer relies on accurate lymph node
metastasis (LNM) staging. However, radiological criteria based on lymph node
(LN) size, shape and texture morphology have limited diagnostic accuracy. In
this work, we investigate applying a Variational Autoencoder (VAE) as a feature
encoder model to replace the large pre-trained Convolutional Neural Network
(CNN) used in existing approaches. The motivation for using a VAE is that the
generative model aims to reconstruct the images, so it directly encodes visual
features and meaningful patterns across the data. This leads to a disentangled
and structured latent space which can be more interpretable than a CNN. Models
are deployed on an in-house MRI dataset with 168 patients who did not undergo
neo-adjuvant treatment. The post-operative pathological N stage was used as the
ground truth to evaluate model predictions. Our proposed model 'VAE-MLP'
achieved state-of-the-art performance on the MRI dataset, with cross-validated
metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85
+/- 0.05. Code is available at:
https://github.com/benkeel/Lymph_Node_Classification_MIUA.

</details>


### [178] [Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis](https://arxiv.org/abs/2507.11730)
*Maciej Szankin,Vidhyananth Venkatasamy,Lihang Ying*

Main category: cs.CV

TL;DR: 本文对代表性多模态视觉语言模型与紧凑型基于CNN的OCR基线在含合成天气失真的数据集上进行基准测试，发现轻量级CNN管道在裁剪文本识别上仍有竞争力，并公开了基准和评估代码。


<details>
  <summary>Details</summary>
Motivation: 户外广告是重要营销媒介，但准确验证现实条件下广告牌文本可见性有挑战，传统OCR管道在复杂户外场景表现不佳，而多模态视觉语言模型有潜力。

Method: 在两个公共数据集（ICDAR 2015和SVT）上，用合成天气失真增强数据，对Qwen 2.5 VL 3B、InternVL3和SmolVLM2等多模态视觉语言模型与PaddleOCRv4进行系统基准测试。

Result: 所选多模态视觉语言模型在整体场景推理方面表现出色，轻量级CNN管道在裁剪文本识别上以较低计算成本达到有竞争力的准确率。

Conclusion: 为促进未来研究，公开天气增强的基准和评估代码。

Abstract: Outdoor advertisements remain a critical medium for modern marketing, yet
accurately verifying billboard text visibility under real-world conditions is
still challenging. Traditional Optical Character Recognition (OCR) pipelines
excel at cropped text recognition but often struggle with complex outdoor
scenes, varying fonts, and weather-induced visual noise. Recently, multimodal
Vision-Language Models (VLMs) have emerged as promising alternatives, offering
end-to-end scene understanding with no explicit detection step. This work
systematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,
InternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline
(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with
synthetic weather distortions to simulate realistic degradation. Our results
reveal that while selected VLMs excel at holistic scene reasoning, lightweight
CNN pipelines still achieve competitive accuracy for cropped text at a fraction
of the computational cost-an important consideration for edge deployment. To
foster future research, we release our weather-augmented benchmark and
evaluation code publicly.

</details>


### [179] [Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning](https://arxiv.org/abs/2507.11761)
*Fan Shi,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 提出统一条件生成求解器UCGS处理多抽象视觉推理任务，单轮多任务训练后有跨任务推理能力及零样本推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度AVR求解器针对不同任务有特定设计或参数，解决新任务需重新训练甚至调整架构，成本高，因此提出统一框架。

Method: 证明部分AVR任务可转化为估计问题面板中目标图像可预测性问题，说明在框架下训练一个条件生成模型可解决多种AVR任务。

Result: UCGS单轮多任务训练后展现跨多种AVR任务的抽象推理能力，有零样本推理能力。

Conclusion: UCGS能在统一框架下解决多个AVR任务，具备跨任务和零样本推理能力。

Abstract: Abstract visual reasoning (AVR) enables humans to quickly discover and
generalize abstract rules to new scenarios. Designing intelligent systems with
human-like AVR abilities has been a long-standing topic in the artificial
intelligence community. Deep AVR solvers have recently achieved remarkable
success in various AVR tasks. However, they usually use task-specific designs
or parameters in different tasks. In such a paradigm, solving new tasks often
means retraining the model, and sometimes retuning the model architectures,
which increases the cost of solving AVR problems. In contrast to task-specific
approaches, this paper proposes a novel Unified Conditional Generative Solver
(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we
prove that some well-known AVR tasks can be reformulated as the problem of
estimating the predictability of target images in problem panels. Then, we
illustrate that, under the proposed framework, training one conditional
generative model can solve various AVR tasks. The experiments show that with a
single round of multi-task training, UCGS demonstrates abstract reasoning
ability across various AVR tasks. Especially, UCGS exhibits the ability of
zero-shot reasoning, enabling it to perform abstract reasoning on problems from
unseen AVR tasks in the testing phase.

</details>


### [180] [From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition](https://arxiv.org/abs/2507.11892)
*Yu Liu,Leyuan Qu,Hanlei Shi,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

TL;DR: 提出GRACE方法用于动态面部表情识别，在三个基准数据集实验中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言方法在动态面部表情识别中存在未充分利用文本情感线索和缺乏有效过滤无关面部动态机制的问题。

Method: 构建GRACE方法，包括CATE模块构建情感感知文本描述、运动差异加权机制突出相关面部运动，使用熵正则化最优传输进行语义和视觉信号的token级对齐。

Result: 在三个基准数据集实验中显著提高识别性能，在模糊或不平衡情感类别的挑战性设置中表现出色，取得UAR和WAR的SOTA结果。

Conclusion: GRACE方法能有效解决现有方法的局限，提升动态面部表情识别性能。

Abstract: Dynamic Facial Expression Recognition (DFER) aims to identify human emotions
from temporally evolving facial movements and plays a critical role in
affective computing. While recent vision-language approaches have introduced
semantic textual descriptions to guide expression recognition, existing methods
still face two key limitations: they often underutilize the subtle emotional
cues embedded in generated text, and they have yet to incorporate sufficiently
effective mechanisms for filtering out facial dynamics that are irrelevant to
emotional expression. To address these gaps, We propose GRACE, Granular
Representation Alignment for Cross-modal Emotion recognition that integrates
dynamic motion modeling, semantic text refinement, and token-level cross-modal
alignment to facilitate the precise localization of emotionally salient
spatiotemporal features. Our method constructs emotion-aware textual
descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and
highlights expression-relevant facial motion through a motion-difference
weighting mechanism. These refined semantic and visual signals are aligned at
the token level using entropy-regularized optimal transport. Experiments on
three benchmark datasets demonstrate that our method significantly improves
recognition performance, particularly in challenging settings with ambiguous or
imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in
terms of both UAR and WAR.

</details>


### [181] [Spatial Frequency Modulation for Semantic Segmentation](https://arxiv.org/abs/2507.11893)
*Linwei Chen,Ying Fu,Lin Gu,Dezhi Zheng,Jifeng Dai*

Main category: cs.CV

TL;DR: 提出空间频率调制（SFM）方法处理高频特征，缓解下采样混叠问题，在多任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 高频信息对语义分割准确性重要，但下采样层会导致高频成分混叠或失真。

Method: 提出SFM，调制高频特征到低频再解调，通过自适应重采样（ARS）实现调制，设计轻量级模块，提出多尺度自适应上采样（MSAU）进行解调，两模块可集成到多种架构。

Result: 特征可视化和分析表明方法有效缓解混叠，保留细节；在多任务中验证了SFM的广泛适用性和有效性。

Conclusion: SFM是一种有效处理高频特征的方法，可应用于多种视觉任务。

Abstract: High spatial frequency information, including fine details like textures,
significantly contributes to the accuracy of semantic segmentation. However,
according to the Nyquist-Shannon Sampling Theorem, high-frequency components
are vulnerable to aliasing or distortion when propagating through downsampling
layers such as strided-convolution. Here, we propose a novel Spatial Frequency
Modulation (SFM) that modulates high-frequency features to a lower frequency
before downsampling and then demodulates them back during upsampling.
Specifically, we implement modulation through adaptive resampling (ARS) and
design a lightweight add-on that can densely sample the high-frequency areas to
scale up the signal, thereby lowering its frequency in accordance with the
Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling
(MSAU) to demodulate the modulated feature and recover high-frequency
information through non-uniform upsampling This module further improves
segmentation by explicitly exploiting information interaction between densely
and sparsely resampled areas at multiple scales. Both modules can seamlessly
integrate with various architectures, extending from convolutional neural
networks to transformers. Feature visualization and analysis confirm that our
method effectively alleviates aliasing while successfully retaining details
after demodulation. Finally, we validate the broad applicability and
effectiveness of SFM by extending it to image classification, adversarial
robustness, instance segmentation, and panoptic segmentation tasks. The code is
available at
\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.

</details>


### [182] [RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation](https://arxiv.org/abs/2507.11947)
*Geon Park,Seon Bin Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 本文提出关系感知解耦学习（RaDL）框架以解决单图像提示中多实例生成问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在单图像提示中生成多实例时，难以解决关系差异和多属性泄漏问题。

Method: 提出RaDL框架，通过可学习参数增强特定实例属性，利用从全局提示中提取的动作动词，通过关系注意力生成关系感知图像特征。

Result: 在COCO - Position、COCO - MIG和DrawBench等基准测试中，RaDL在位置准确性、多属性考虑和实例间关系方面有显著提升，优于现有方法。

Conclusion: RaDL是解决多实例图像中考虑实例关系和多属性的有效方案。

Abstract: With recent advancements in text-to-image (T2I) models, effectively
generating multiple instances within a single image prompt has become a crucial
challenge. Existing methods, while successful in generating positions of
individual instances, often struggle to account for relationship discrepancy
and multiple attributes leakage. To address these limitations, this paper
proposes the relation-aware disentangled learning (RaDL) framework. RaDL
enhances instance-specific attributes through learnable parameters and
generates relation-aware image features via Relation Attention, utilizing
action verbs extracted from the global prompt. Through extensive evaluations on
benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that
RaDL outperforms existing methods, showing significant improvements in
positional accuracy, multiple attributes consideration, and the relationships
between instances. Our results present RaDL as the solution for generating
images that consider both the relationships and multiple attributes of each
instance within the multi-instance image.

</details>


### [183] [SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation](https://arxiv.org/abs/2507.11579)
*Sathvik Chereddy,John Femiani*

Main category: cs.CV

TL;DR: 提出SketchDNN生成模型，通过统一连续 - 离散扩散过程合成CAD草图，改善生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决CAD草图中原始参数化的异质性和原始图元的排列不变性这两个关键挑战。

Method: 采用Gaussian - Softmax扩散，将受高斯噪声干扰的对数几率通过softmax变换投影到概率单纯形。

Result: 显著提高生成质量，将Fréchet Inception Distance (FID)从16.04降至7.80，负对数似然 (NLL)从84.8降至81.33。

Conclusion: 在SketchGraphs数据集的CAD草图生成中达到新的最优水平。

Abstract: We present SketchDNN, a generative model for synthesizing CAD sketches that
jointly models both continuous parameters and discrete class labels through a
unified continuous-discrete diffusion process. Our core innovation is
Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are
projected onto the probability simplex via a softmax transformation,
facilitating blended class labels for discrete variables. This formulation
addresses 2 key challenges, namely, the heterogeneity of primitive
parameterizations and the permutation invariance of primitives in CAD sketches.
Our approach significantly improves generation quality, reducing Fr\'echet
Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)
from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch
generation on the SketchGraphs dataset.

</details>


### [184] [Frequency-Dynamic Attention Modulation for Dense Prediction](https://arxiv.org/abs/2507.12006)
*Linwei Chen,Lin Gu,Ying Fu*

Main category: cs.CV

TL;DR: 提出FDAM策略解决ViTs频率消失问题，在多模型和任务上提升性能，在遥感检测达SOTA。


<details>
  <summary>Details</summary>
Motivation: ViTs的注意力机制使每层为低通滤波器，堆叠架构存在频率消失问题，导致关键细节和纹理丢失。

Method: 提出Frequency - Dynamic Attention Modulation (FDAM)策略，包含Attention Inversion (AttInv)和Frequency Dynamic Scaling (FreqScale)两种技术。

Result: 通过特征相似性分析和有效秩评估，避免表示崩溃，在多模型和多任务上性能提升，在遥感检测单尺度设置中达SOTA。

Conclusion: FDAM策略能有效解决ViTs频率消失问题，提升模型性能。

Abstract: Vision Transformers (ViTs) have significantly advanced computer vision,
demonstrating strong performance across various tasks. However, the attention
mechanism in ViTs makes each layer function as a low-pass filter, and the
stacked-layer architecture in existing transformers suffers from frequency
vanishing. This leads to the loss of critical details and textures. We propose
a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention
Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly
modulates the overall frequency response of ViTs and consists of two
techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling
(FreqScale). Since circuit theory uses low-pass filters as fundamental
elements, we introduce AttInv, a method that generates complementary high-pass
filtering by inverting the low-pass filter in the attention matrix, and
dynamically combining the two. We further design FreqScale to weight different
frequency components for fine-grained adjustments to the target response
function. Through feature similarity analysis and effective rank evaluation, we
demonstrate that our approach avoids representation collapse, leading to
consistent performance improvements across various models, including SegFormer,
DeiT, and MaskDINO. These improvements are evident in tasks such as semantic
segmentation, object detection, and instance segmentation. Additionally, we
apply our method to remote sensing detection, achieving state-of-the-art
results in single-scale settings. The code is available at
\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.

</details>


### [185] [Dual form Complementary Masking for Domain-Adaptive Image Segmentation](https://arxiv.org/abs/2507.12008)
*Jiawen Wang,Yinda Chen,Xiaoyu Liu,Che Liu,Dong Liu,Jianqing Gao,Zhiwei Xiong*

Main category: cs.CV

TL;DR: 本文将掩码重建重新定义为稀疏信号重建问题，提出MaskTwins框架用于无监督域适应，实验验证其在自然和生物图像分割上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作仅将掩码视为输入图像变形，缺乏理论分析，对掩码重建理解肤浅，未充分挖掘其在特征提取和表示学习中的潜力。

Method: 将掩码重建重构为稀疏信号重建问题，证明互补掩码对偶形式在提取域无关图像特征上的优势，提出MaskTwins框架，在主训练流程中集成掩码重建，通过强制互补掩码图像预测的一致性挖掘跨域结构模式。

Result: 大量实验表明，MaskTwins在自然和生物图像分割中优于基线方法。

Conclusion: MaskTwins在无需单独预训练的情况下提取域不变特征方面有显著优势，为域自适应分割提供了新范式。

Abstract: Recent works have correlated Masked Image Modeling (MIM) with consistency
regularization in Unsupervised Domain Adaptation (UDA). However, they merely
treat masking as a special form of deformation on the input images and neglect
the theoretical analysis, which leads to a superficial understanding of masked
reconstruction and insufficient exploitation of its potential in enhancing
feature extraction and representation learning. In this paper, we reframe
masked reconstruction as a sparse signal reconstruction problem and
theoretically prove that the dual form of complementary masks possesses
superior capabilities in extracting domain-agnostic image features. Based on
this compelling insight, we propose MaskTwins, a simple yet effective UDA
framework that integrates masked reconstruction directly into the main training
pipeline. MaskTwins uncovers intrinsic structural patterns that persist across
disparate domains by enforcing consistency between predictions of images masked
in complementary ways, enabling domain generalization in an end-to-end manner.
Extensive experiments verify the superiority of MaskTwins over baseline methods
in natural and biological image segmentation. These results demonstrate the
significant advantages of MaskTwins in extracting domain-invariant features
without the need for separate pre-training, offering a new paradigm for
domain-adaptive segmentation.

</details>


### [186] [Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment](https://arxiv.org/abs/2507.11642)
*Abhishek Jaiswal,Nisheeth Srivastava*

Main category: cs.CV

TL;DR: 本文探讨基于姿势的心理状态推断，以板球运动为例提出解决方案，取得良好效果，还利用数据统计作弱监督，为体育分析和多领域人类行为分析提供可能。


<details>
  <summary>Details</summary>
Motivation: 基于姿势的心理状态推断工具需经大数据集验证才能应用，但视觉诊断因人类数据敏感面临挑战，需寻找积累数据的途径。

Method: 以板球运动为场景，通过运动分析从活动视频中识别人类意图；利用现有数据统计作弱监督验证结果。

Result: 方法在区分攻击性和防御性击球意图时F1分数超75%，AUC - ROC超80%。

Conclusion: 姿势能为意图推断提供强信号，研究为体育分析提供通用技术，也为多领域人类行为分析带来可能。

Abstract: Posture-based mental state inference has significant potential in diagnosing
fatigue, preventing injury, and enhancing performance across various domains.
Such tools must be research-validated with large datasets before being
translated into practice. Unfortunately, such vision diagnosis faces serious
challenges due to the sensitivity of human subject data. To address this, we
identify sports settings as a viable alternative for accumulating data from
human subjects experiencing diverse emotional states. We test our hypothesis in
the game of cricket and present a posture-based solution to identify human
intent from activity videos. Our method achieves over 75\% F1 score and over
80\% AUC-ROC in discriminating aggressive and defensive shot intent through
motion analysis. These findings indicate that posture leaks out strong signals
for intent inference, even with inherent noise in the data pipeline.
Furthermore, we utilize existing data statistics as weak supervision to
validate our findings, offering a potential solution for overcoming data
labelling limitations. This research contributes to generalizable techniques
for sports analytics and also opens possibilities for applying human behavior
analysis across various fields.

</details>


### [187] [SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection](https://arxiv.org/abs/2507.12017)
*Xiwei Zhang,Chunjin Yang,Yiming Xiao,Runtong Zhang,Fanman Meng*

Main category: cs.CV

TL;DR: 提出基于解耦 - 耦合策略的SS - DC框架用于RGB - IR领域的无监督域自适应目标检测，实验表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RGB - IR领域无监督域自适应目标检测方法将RGB域视为统一域，忽略其中多个子域，本文认为解耦多个子域的域不变和域特定特征有利于RGB - IR域适应。

Method: 提出SS - DC框架，设计SAID模块解耦，提出基于滤波器组的光谱处理范式和自蒸馏驱动的解耦损失改进解耦，提出空间 - 光谱耦合方法并引入域特定特征减少域偏差。

Result: 方法显著提升基线性能，在多个RGB - IR数据集上优于现有UDAOD方法。

Conclusion: 所提SS - DC框架在RGB - IR无监督域自适应目标检测任务中有效。

Abstract: Unsupervised domain adaptive object detection (UDAOD) from the visible domain
to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB
domain as a unified domain and neglect the multiple subdomains within it, such
as daytime, nighttime, and foggy scenes. We argue that decoupling the
domain-invariant (DI) and domain-specific (DS) features across these multiple
subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper
proposes a new SS-DC framework based on a decoupling-coupling strategy. In
terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)
module in the aspect of spectral decomposition. Due to the style and content
information being highly embedded in different frequency bands, this module can
decouple DI and DS components more accurately and interpretably. A novel filter
bank-based spectral processing paradigm and a self-distillation-driven
decoupling loss are proposed to improve the spectral domain decoupling. In
terms of coupling, a new spatial-spectral coupling method is proposed, which
realizes joint coupling through spatial and spectral DI feature pyramids.
Meanwhile, this paper introduces DS from decoupling to reduce the domain bias.
Extensive experiments demonstrate that our method can significantly improve the
baseline performance and outperform existing UDAOD methods on multiple RGB-IR
datasets, including a new experimental protocol proposed in this paper based on
the FLIR-ADAS dataset.

</details>


### [188] [Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery](https://arxiv.org/abs/2507.12029)
*Xinhang Wan,Jiyuan Liu,Qian Qu,Suyuan Liu,Chuyu Zhang,Fangdi Wang,Xinwang Liu,En Zhu,Kunlun He*

Main category: cs.CV

TL;DR: 文章针对新颖类发现（NCD）问题，指出现有方法局限，提出IICMVNCD框架，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有NCD方法主要处理单视图数据，且依赖伪标签导致性能不稳定，需要解决多视图数据的NCD问题。

Method: 提出IICMVNCD框架，在视图内利用分布相似性进行矩阵分解，在视图间利用已知类视图关系指导新颖类聚类。

Result: 实验结果验证了所提方法的有效性。

Conclusion: IICMVNCD框架能有效解决多视图数据的新颖类发现问题。

Abstract: In this paper, we address the problem of novel class discovery (NCD), which
aims to cluster novel classes by leveraging knowledge from disjoint known
classes. While recent advances have made significant progress in this area,
existing NCD methods face two major limitations. First, they primarily focus on
single-view data (e.g., images), overlooking the increasingly common multi-view
data, such as multi-omics datasets used in disease diagnosis. Second, their
reliance on pseudo-labels to supervise novel class clustering often results in
unstable performance, as pseudo-label quality is highly sensitive to factors
such as data noise and feature dimensionality. To address these challenges, we
propose a novel framework named Intra-view and Inter-view Correlation Guided
Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to
explore NCD in multi-view setting so far. Specifically, at the intra-view
level, leveraging the distributional similarity between known and novel
classes, we employ matrix factorization to decompose features into
view-specific shared base matrices and factor matrices. The base matrices
capture distributional consistency among the two datasets, while the factor
matrices model pairwise relationships between samples. At the inter-view level,
we utilize view relationships among known classes to guide the clustering of
novel classes. This includes generating predicted labels through the weighted
fusion of factor matrices and dynamically adjusting view weights of known
classes based on the supervision loss, which are then transferred to novel
class learning. Experimental results validate the effectiveness of our proposed
approach.

</details>


### [189] [InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing](https://arxiv.org/abs/2507.12060)
*Kun-Hsiang Lin,Yu-Wen Tseng,Kang-Yang Huang,Jhih-Ciang Wu,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: 提出InstructFLIP框架解决人脸反欺骗中攻击类型语义理解不足和跨域训练冗余问题，实验显示效果优于SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 解决人脸反欺骗中攻击类型语义理解有限和跨域训练冗余两个挑战。

Method: 集成视觉 - 语言模型增强视觉输入感知；采用元域策略学习统一模型；提出InstructFLIP框架，将指令解耦为内容和风格组件。

Result: InstructFLIP在准确性上优于SOTA模型，大幅减少跨域训练冗余。

Conclusion: InstructFLIP框架有效提升人脸反欺骗跨域泛化能力。

Abstract: Face anti-spoofing (FAS) aims to construct a robust system that can withstand
diverse attacks. While recent efforts have concentrated mainly on cross-domain
generalization, two significant challenges persist: limited semantic
understanding of attack types and training redundancy across domains. We
address the first by integrating vision-language models (VLMs) to enhance the
perception of visual input. For the second challenge, we employ a meta-domain
strategy to learn a unified model that generalizes well across multiple
domains. Our proposed InstructFLIP is a novel instruction-tuned framework that
leverages VLMs to enhance generalization via textual guidance trained solely on
a single domain. At its core, InstructFLIP explicitly decouples instructions
into content and style components, where content-based instructions focus on
the essential semantics of spoofing, and style-based instructions consider
variations related to the environment and camera characteristics. Extensive
experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA
models in accuracy and substantially reducing training redundancy across
diverse domains in FAS. Project website is available at
https://kunkunlin1221.github.io/InstructFLIP.

</details>


### [190] [Non-Adaptive Adversarial Face Generation](https://arxiv.org/abs/2507.12107)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Minsu Kim,Jae Hong Seo*

Main category: cs.CV

TL;DR: 提出利用FRS特征空间结构特性生成对抗人脸的新方法，只需单次非自适应查询，成功率超93%，还能按需求生成对抗人脸。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击对人脸识别系统构成安全和隐私威胁，需新方法生成对抗人脸。

Method: 利用FRS特征空间的结构特性，利用相同属性个体形成的子球体，实现非自适应和少量查询。

Result: 仅需100张人脸图像的单次非自适应查询，对AWS的CompareFaces API成功率超93%。

Conclusion: 该方法无需依赖可迁移性和开源替代模型，能按攻击者选择的属性生成对抗人脸。

Abstract: Adversarial attacks on face recognition systems (FRSs) pose serious security
and privacy threats, especially when these systems are used for identity
verification. In this paper, we propose a novel method for generating
adversarial faces-synthetic facial images that are visually distinct yet
recognized as a target identity by the FRS. Unlike iterative optimization-based
approaches (e.g., gradient descent or other iterative solvers), our method
leverages the structural characteristics of the FRS feature space. We figure
out that individuals sharing the same attribute (e.g., gender or race) form an
attributed subsphere. By utilizing such subspheres, our method achieves both
non-adaptiveness and a remarkably small number of queries. This eliminates the
need for relying on transferability and open-source surrogate models, which
have been a typical strategy when repeated adaptive queries to commercial FRSs
are impossible. Despite requiring only a single non-adaptive query consisting
of 100 face images, our method achieves a high success rate of over 93% against
AWS's CompareFaces API at its default threshold. Furthermore, unlike many
existing attacks that perturb a given image, our method can deliberately
produce adversarial faces that impersonate the target identity while exhibiting
high-level attributes chosen by the adversary.

</details>


### [191] [Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement](https://arxiv.org/abs/2507.12188)
*Shuangli Du,Siming Yan,Zhenghao Shi,Zhenzhen You,Lu Sun*

Main category: cs.CV

TL;DR: 提出基于小波特征空间解耦的低光立体图像增强方法，含高频引导跨视图交互模块和细节纹理增强模块，实验表明算法在调光和恢复高频信息上有优势。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法将所有退化因素编码在单一潜在空间，导致特征纠缠、黑盒特性强、易走捷径学习。

Method: 用小波变换将特征空间分解为低频调光分支和多个高频纹理增强分支；提出高频引导跨视图交互模块和基于交叉注意力机制的细节纹理增强模块；在均匀和非均匀光照图像数据集上训练模型。

Result: 在真实和合成图像上实验，算法在调光方面有显著优势，能有效恢复高频信息。

Conclusion: 所提方法有效解决现有低光图像增强方法问题，代码和数据集公开。

Abstract: Low-light images suffer from complex degradation, and existing enhancement
methods often encode all degradation factors within a single latent space. This
leads to highly entangled features and strong black-box characteristics, making
the model prone to shortcut learning. To mitigate the above issues, this paper
proposes a wavelet-based low-light stereo image enhancement method with feature
space decoupling. Our insight comes from the following findings: (1) Wavelet
transform enables the independent processing of low-frequency and
high-frequency information. (2) Illumination adjustment can be achieved by
adjusting the low-frequency component of a low-light image, extracted through
multi-level wavelet decomposition. Thus, by using wavelet transform the feature
space is decomposed into a low-frequency branch for illumination adjustment and
multiple high-frequency branches for texture enhancement. Additionally, stereo
low-light image enhancement can extract useful cues from another view to
improve enhancement. To this end, we propose a novel high-frequency guided
cross-view interaction module (HF-CIM) that operates within high-frequency
branches rather than across the entire feature space, effectively extracting
valuable image details from the other view. Furthermore, to enhance the
high-frequency information, a detail and texture enhancement module (DTEM) is
proposed based on cross-attention mechanism. The model is trained on a dataset
consisting of images with uniform illumination and images with non-uniform
illumination. Experimental results on both real and synthetic images indicate
that our algorithm offers significant advantages in light adjustment while
effectively recovering high-frequency information. The code and dataset are
publicly available at: https://github.com/Cherisherr/WDCI-Net.git.

</details>


### [192] [Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision](https://arxiv.org/abs/2507.12195)
*Arkaprabha Basu*

Main category: cs.CV

TL;DR: 本文提出针对印度古迹的三种新技术，能以低成本实现自动化，平衡传统与创新，提升文化遗产保护效率与美学质量。


<details>
  <summary>Details</summary>
Motivation: 现代数字化方法改变文化珍宝保护与修复，为利用计算机技术保护印度古迹，发挥其建筑与美学价值。

Method: 提出三种技术：基于图像处理的分形卷积方法、用于孟加拉邦班古拉陶土寺庙的自敏感瓷砖填充法和图像超分辨率策略，还有新数据增强方法MosaicSlice。

Result: 能实现无缝区域填充和制作高细节瓷砖，在保持真实性的同时，以低成本引入自动化。

Conclusion: 所提方法推动文化遗产保护领域进入高效且具美学质量的时代，平衡了传统与创新。

Abstract: Modern digitised approaches have dramatically changed the preservation and
restoration of cultural treasures, integrating computer scientists into
multidisciplinary projects with ease. Machine learning, deep learning, and
computer vision techniques have revolutionised developing sectors like 3D
reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and
image processing with the integration of computer scientists into
multidisciplinary initiatives. We suggest three cutting-edge techniques in
recognition of the special qualities of Indian monuments, which are famous for
their architectural skill and aesthetic appeal. First is the Fractal
Convolution methodology, a segmentation method based on image processing that
successfully reveals subtle architectural patterns within these irreplaceable
cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling
(SSTF) method created especially for West Bengal's mesmerising Bankura
Terracotta Temples with a brand-new data augmentation method called MosaicSlice
on the third. Furthermore, we delve deeper into the Super Resolution strategy
to upscale the images without losing significant amount of quality. Our methods
allow for the development of seamless region-filling and highly detailed tiles
while maintaining authenticity using a novel data augmentation strategy within
affordable costs introducing automation. By providing effective solutions that
preserve the delicate balance between tradition and innovation, this study
improves the subject and eventually ensures unrivalled efficiency and aesthetic
excellence in cultural heritage protection. The suggested approaches advance
the field into an era of unmatched efficiency and aesthetic quality while
carefully upholding the delicate equilibrium between tradition and innovation.

</details>


### [193] [Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants](https://arxiv.org/abs/2507.12269)
*Sybelle Goedicke-Fritz,Michelle Bous,Annika Engel,Matthias Flotho,Pascal Hirsch,Hannah Wittig,Dino Milanovic,Dominik Mohr,Mathias Kaspar,Sogand Nemat,Dorothea Kerner,Arno Bücker,Andreas Keller,Sascha Meyer,Michael Zemlin,Philipp Flotho*

Main category: cs.CV

TL;DR: 本文利用深度学习方法，基于出生24小时内极低出生体重婴儿胸部X光片预测支气管肺发育不良（BPD）结果，证明特定领域预训练有效。


<details>
  <summary>Details</summary>
Motivation: BPD影响35%极低出生体重婴儿，有终身呼吸并发症，预防性干预有严重风险，因此需要早期预后和结果预测以避免低风险婴儿不必要的毒性。

Method: 使用163名极低出生体重婴儿出生24小时内的胸部X光片，微调在成人胸部X光片上预训练的ResNet - 50，采用渐进式层冻结、判别式学习率、CutMix增强和线性探测。

Result: 最佳模型预测中度/重度BPD结果时，AUROC为0.78 ± 0.10，平衡准确率为0.69 ± 0.10，F1分数为0.67 ± 0.11；特定领域预训练显著优于ImageNet初始化；常规IRDS分级预后价值有限。

Conclusion: 特定领域预训练能从常规第1天X光片准确预测BPD，该方法计算上可行，适用于现场实施和未来联合学习部署。

Abstract: Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of
extremely low birth weight infants. Defined by oxygen dependence at 36 weeks
postmenstrual age, it causes lifelong respiratory complications. However,
preventive interventions carry severe risks, including neurodevelopmental
impairment, ventilator-induced lung injury, and systemic complications.
Therefore, early BPD prognosis and prediction of BPD outcome is crucial to
avoid unnecessary toxicity in low risk infants. Admission radiographs of
extremely preterm infants are routinely acquired within 24h of life and could
serve as a non-invasive prognostic tool. In this work, we developed and
investigated a deep learning approach using chest X-rays from 163 extremely
low-birth-weight infants ($\leq$32 weeks gestation, 401-999g) obtained within
24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult
chest radiographs, employing progressive layer freezing with discriminative
learning rates to prevent overfitting and evaluated a CutMix augmentation and
linear probing. For moderate/severe BPD outcome prediction, our best performing
model with progressive freezing, linear probing and CutMix achieved an AUROC of
0.78 $\pm$ 0.10, balanced accuracy of 0.69 $\pm$ 0.10, and an F1-score of 0.67
$\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet
initialization (p = 0.031) which confirms domain-specific pretraining to be
important for BPD outcome prediction. Routine IRDS grades showed limited
prognostic value (AUROC 0.57 $\pm$ 0.11), confirming the need of learned
markers. Our approach demonstrates that domain-specific pretraining enables
accurate BPD prediction from routine day-1 radiographs. Through progressive
freezing and linear probing, the method remains computationally feasible for
site-level implementation and future federated learning deployments.

</details>


### [194] [MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model](https://arxiv.org/abs/2507.12023)
*Xu Fan,Zhihao Wang,Yuetan Lin,Yan Zhang,Yang Xiang,Hao Li*

Main category: cs.CV

TL;DR: 提出MVAR模型用于多变量空气污染物预测，构建数据集，实验显示该模型优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦单污染物预测，忽略不同污染物间相互作用及空间响应差异，为满足多变量空气污染物预测的实际需求。

Method: 提出MultiVariate AutoRegressive空气污染物预测模型（MVAR），设计多变量自回归训练范式，开发气象耦合空间变换器模块，构建涵盖6种主要污染物的综合数据集。

Result: 实验结果表明，所提出的模型优于现有先进方法。

Conclusion: 验证了所提出架构的有效性。

Abstract: Air pollutants pose a significant threat to the environment and human health,
thus forecasting accurate pollutant concentrations is essential for pollution
warnings and policy-making. Existing studies predominantly focus on
single-pollutant forecasting, neglecting the interactions among different
pollutants and their diverse spatial responses. To address the practical needs
of forecasting multivariate air pollutants, we propose MultiVariate
AutoRegressive air pollutants forecasting model (MVAR), which reduces the
dependency on long-time-window inputs and boosts the data utilization
efficiency. We also design the Multivariate Autoregressive Training Paradigm,
enabling MVAR to achieve 120-hour long-term sequential forecasting.
Additionally, MVAR develops Meteorological Coupled Spatial Transformer block,
enabling the flexible coupling of AI-based meteorological forecasts while
learning the interactions among pollutants and their diverse spatial responses.
As for the lack of standardized datasets in air pollutants forecasting, we
construct a comprehensive dataset covering 6 major pollutants across 75 cities
in North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0
forecast data. Experimental results demonstrate that the proposed model
outperforms state-of-the-art methods and validate the effectiveness of the
proposed architecture.

</details>


### [195] [Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models](https://arxiv.org/abs/2507.12318)
*Samuel Lavoie,Michael Noukhovitch,Aaron Courville*

Main category: cs.CV

TL;DR: 本文从理想表示应提升样本保真度、易于生成和可组合的角度，研究扩散模型的输入条件表示，引入离散潜码DLC，提升了图像生成保真度，可生成分布外样本并实现文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 探究扩散模型成功的原因在于输入条件，研究能提升样本保真度、易于生成和可组合的条件表示。

Method: 引入基于单纯形嵌入且通过自监督学习目标训练得到的离散潜码DLC，它是离散令牌序列。

Result: 使用DLC训练的扩散模型提升了生成保真度，在ImageNet上实现无条件图像生成的新最优；组合DLC可生成分布外样本；利用预训练语言模型，微调文本扩散语言模型生成DLC实现文本到图像生成。

Conclusion: DLC作为一种新的图像表示，能提升扩散模型图像生成效果，还可用于文本到图像生成。

Abstract: We argue that diffusion models' success in modeling complex distributions is,
for the most part, coming from their input conditioning. This paper
investigates the representation used to condition diffusion models from the
perspective that ideal representations should improve sample fidelity, be easy
to generate, and be compositional to allow out-of-training samples generation.
We introduce Discrete Latent Code (DLC), an image representation derived from
Simplicial Embeddings trained with a self-supervised learning objective. DLCs
are sequences of discrete tokens, as opposed to the standard continuous image
embeddings. They are easy to generate and their compositionality enables
sampling of novel images beyond the training distribution. Diffusion models
trained with DLCs have improved generation fidelity, establishing a new
state-of-the-art for unconditional image generation on ImageNet. Additionally,
we show that composing DLCs allows the image generator to produce
out-of-distribution samples that coherently combine the semantics of images in
diverse ways. Finally, we showcase how DLCs can enable text-to-image generation
by leveraging large-scale pretrained language models. We efficiently finetune a
text diffusion language model to generate DLCs that produce novel samples
outside of the image generator training distribution.

</details>


### [196] [Cluster Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2507.12359)
*Nikolaos Giakoumoglou,Tania Stathaki*

Main category: cs.CV

TL;DR: 提出Cluster Contrast (CueCo)方法结合对比学习和聚类，在多个数据集取得好成绩，为无监督视觉表征学习指明新方向。


<details>
  <summary>Details</summary>
Motivation: 有效结合对比学习和聚类方法的优势，推动无监督视觉表征学习发展。

Method: 利用查询和键两个神经网络，用对比损失分离不同特征，用聚类目标聚合相同簇特征。

Result: 使用ResNet - 18骨干网络进行线性评估，在CIFAR - 10、CIFAR - 100和ImageNet - 100上分别达到91.40%、68.56%和78.65%的top - 1分类准确率。

Conclusion: 将对比学习与聚类结合，为无监督视觉表征学习开创了新方向。

Abstract: We introduce Cluster Contrast (CueCo), a novel approach to unsupervised
visual representation learning that effectively combines the strengths of
contrastive learning and clustering methods. Inspired by recent advancements,
CueCo is designed to simultaneously scatter and align feature representations
within the feature space. This method utilizes two neural networks, a query and
a key, where the key network is updated through a slow-moving average of the
query outputs. CueCo employs a contrastive loss to push dissimilar features
apart, enhancing inter-class separation, and a clustering objective to pull
together features of the same cluster, promoting intra-class compactness. Our
method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on
CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18
backbone. By integrating contrastive learning with clustering, CueCo sets a new
direction for advancing unsupervised visual representation learning.

</details>


### [197] [Neural Human Pose Prior](https://arxiv.org/abs/2507.12138)
*Michal Heker,Sefy Kararlitsky,David Tolpin*

Main category: cs.CV

TL;DR: 本文提出用归一化流对人体姿态的神经先验进行建模的方法，通过定性和定量评估证明其有效性，为姿态先验融入人体运动捕捉和重建提供概率基础。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为启发式或低表达性，需一种更好的对人体姿态神经先验建模的方法。

Method: 利用RealNVP对6D旋转格式表示的姿态学习灵活密度，训练时反转Gram - Schmidt过程处理有效6D旋转流形分布建模挑战。

Result: 通过定性和定量评估证明了学习到的先验的有效性，并通过消融研究分析其影响。

Conclusion: 为将姿态先验融入人体运动捕捉和重建管道提供了可靠的概率基础。

Abstract: We introduce a principled, data-driven approach for modeling a neural prior
over human body poses using normalizing flows. Unlike heuristic or
low-expressivity alternatives, our method leverages RealNVP to learn a flexible
density over poses represented in the 6D rotation format. We address the
challenge of modeling distributions on the manifold of valid 6D rotations by
inverting the Gram-Schmidt process during training, enabling stable learning
while preserving downstream compatibility with rotation-based frameworks. Our
architecture and training pipeline are framework-agnostic and easily
reproducible. We demonstrate the effectiveness of the learned prior through
both qualitative and quantitative evaluations, and we analyze its impact via
ablation studies. This work provides a sound probabilistic foundation for
integrating pose priors into human motion capture and reconstruction pipelines.

</details>


### [198] [AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models](https://arxiv.org/abs/2507.12414)
*Santosh Vasa,Aditi Ramadwar,Jnana Rama Krishna Darabattula,Md Zafar Anwar,Stanislaw Antol,Andrei Vatavu,Thomas Monninger,Sihao Ding*

Main category: cs.CV

TL;DR: 本文提出AutoVDC框架，利用VLMs自动识别视觉数据集中错误标注，在KITTI和nuImages数据集验证，结果表明方法在错误检测和数据清理实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统训练需精确标注的大规模数据集，人工标注有缺陷且审查成本高，需自动清理数据方法。

Method: 引入AutoVDC框架，利用VLMs自动识别视觉数据集中错误标注，在KITTI和nuImages数据集验证，创建含注入错误标注的数据集变体测试，比较不同VLMs检测率，探索VLM微调影响。

Result: 方法在错误检测和数据清理实验中表现出高性能。

Conclusion: 该方法有潜力显著提高自动驾驶大规模生产数据集的可靠性和准确性。

Abstract: Training of autonomous driving systems requires extensive datasets with
precise annotations to attain robust performance. Human annotations suffer from
imperfections, and multiple iterations are often needed to produce high-quality
datasets. However, manually reviewing large datasets is laborious and
expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)
framework and investigate the utilization of Vision-Language Models (VLMs) to
automatically identify erroneous annotations in vision datasets, thereby
enabling users to eliminate these errors and enhance data quality. We validate
our approach using the KITTI and nuImages datasets, which contain object
detection benchmarks for autonomous driving. To test the effectiveness of
AutoVDC, we create dataset variants with intentionally injected erroneous
annotations and observe the error detection rate of our approach. Additionally,
we compare the detection rates using different VLMs and explore the impact of
VLM fine-tuning on our pipeline. The results demonstrate our method's high
performance in error detection and data cleaning experiments, indicating its
potential to significantly improve the reliability and accuracy of large-scale
production datasets in autonomous driving.

</details>


### [199] [QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval](https://arxiv.org/abs/2507.12416)
*Jaehyun Kwak,Ramahdani Muhammad Izaaz Inhar,Se-Young Yun,Sung-Ju Lee*

Main category: cs.CV

TL;DR: 提出QuRe方法解决CIR方法存在的假负样本问题，创建HP - FashionIQ数据集评估模型，实验显示QuRe性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有CIR方法只关注目标图像检索，忽视其他图像相关性，采用对比学习易引入假负样本，导致检索到不相关图像，降低用户满意度。

Method: 提出Query - Relevant Retrieval through Hard Negative Sampling (QuRe)优化奖励模型目标以减少假负样本，引入硬负采样策略过滤假负样本，创建Human - Preference FashionIQ (HP - FashionIQ)数据集评估CIR模型与人类满意度的一致性。

Result: QuRe在FashionIQ和CIRR数据集上达到了最先进的性能，在HP - FashionIQ数据集上与人类偏好的一致性最强。

Conclusion: QuRe方法有效解决了现有CIR方法的问题，在多个数据集上表现出色，且代码已开源。

Abstract: Composed Image Retrieval (CIR) retrieves relevant images based on a reference
image and accompanying text describing desired modifications. However, existing
CIR methods only focus on retrieving the target image and disregard the
relevance of other images. This limitation arises because most methods
employing contrastive learning-which treats the target image as positive and
all other images in the batch as negatives-can inadvertently include false
negatives. This may result in retrieving irrelevant images, reducing user
satisfaction even when the target image is retrieved. To address this issue, we
propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which
optimizes a reward model objective to reduce false negatives. Additionally, we
introduce a hard negative sampling strategy that selects images positioned
between two steep drops in relevance scores following the target image, to
effectively filter false negatives. In order to evaluate CIR models on their
alignment with human satisfaction, we create Human-Preference FashionIQ
(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond
target retrieval. Extensive experiments demonstrate that QuRe achieves
state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting
the strongest alignment with human preferences on the HP-FashionIQ dataset. The
source code is available at https://github.com/jackwaky/QuRe.

</details>


### [200] [Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST](https://arxiv.org/abs/2507.12248)
*Anida Nezović,Jalal Romano,Nada Marić,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 研究对比Keras、PyTorch和JAX框架在医学图像分类任务中CNN实现的性能，用PathMNIST数据集评估，发现计算速度和模型精度的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架在医学成像任务中的比较性能未充分探索。

Method: 使用PathMNIST数据集，对三个框架的CNN实现进行综合分析，评估训练效率、分类准确性和推理速度。

Result: 发现计算速度和模型精度之间存在权衡。

Conclusion: 为医学图像分析的研究人员和从业者提供有价值的见解。

Abstract: Deep learning has significantly advanced the field of medical image
classification, particularly with the adoption of Convolutional Neural Networks
(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer
unique advantages in model development and deployment. However, their
comparative performance in medical imaging tasks remains underexplored. This
study presents a comprehensive analysis of CNN implementations across these
frameworks, using the PathMNIST dataset as a benchmark. We evaluate training
efficiency, classification accuracy and inference speed to assess their
suitability for real-world applications. Our findings highlight the trade-offs
between computational speed and model accuracy, offering valuable insights for
researchers and practitioners in medical image analysis.

</details>


### [201] [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](https://arxiv.org/abs/2507.12461)
*Trong-Thang Pham,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 提出基于深度学习的RadGazeIntent方法，处理眼动数据以预测放射科医生诊断意图，实验结果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型无法捕捉放射科医生每次注视背后的潜在意图，需建模其主动搜索行为。

Method: 采用基于Transformer的架构处理眼动数据的时空维度，将注视特征转化为诊断意图表示；处理现有医学眼动追踪数据集创建三个意图标签子集。

Result: RadGazeIntent能预测放射科医生在特定时刻检查的结果，在所有意图标签数据集上优于基线方法。

Conclusion: RadGazeIntent可有效预测放射科医生的诊断意图，表现优于现有方法。

Abstract: Radiologists rely on eye movements to navigate and interpret medical images.
A trained radiologist possesses knowledge about the potential diseases that may
be present in the images and, when searching, follows a mental checklist to
locate them using their gaze. This is a key observation, yet existing models
fail to capture the underlying intent behind each fixation. In this paper, we
introduce a deep learning-based approach, RadGazeIntent, designed to model this
behavior: having an intention to find something and actively searching for it.
Our transformer-based architecture processes both the temporal and spatial
dimensions of gaze data, transforming fine-grained fixation features into
coarse, meaningful representations of diagnostic intent to interpret
radiologists' goals. To capture the nuances of radiologists' varied
intention-driven behaviors, we process existing medical eye-tracking datasets
to create three intention-labeled subsets: RadSeq (Systematic Sequential
Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid
Pattern). Experimental results demonstrate RadGazeIntent's ability to predict
which findings radiologists are examining at specific moments, outperforming
baseline methods across all intention-labeled datasets.

</details>


### [202] [Describe Anything Model for Visual Question Answering on Text-rich Images](https://arxiv.org/abs/2507.12441)
*Yen-Linh Vu,Dinh-Thang Duong,Truong-Binh Duong,Anh-Khoi Nguyen,Thanh-Huy Nguyen,Le Thien Phuc Nguyen,Jianhua Xing,Xingjian Li,Tianyang Wang,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 本文提出DAM-QA框架用于文本密集图像的视觉问答，在多基准测试中表现优异，代码开源。


<details>
  <summary>Details</summary>
Motivation: 假设DAM的区域描述能力对视觉问答有益，尤其是处理文本密集图像时，希望利用其能力解决文本丰富的视觉问答问题。

Method: 引入DAM-QA框架和定制评估协议，通过聚合图像多区域视图答案的机制，更有效识别与文本相关证据。

Result: 在六个VQA基准测试中，DAM-QA始终优于基线DAM，在DocVQA上提升超7分，以更少参数在区域感知模型中表现最佳，缩小与强大通用VLM差距。

Conclusion: 当采用有效使用和集成策略时，类似DAM的模型在文本丰富及更广泛VQA任务中有潜力。

Abstract: Recent progress has been made in region-aware vision-language modeling,
particularly with the emergence of the Describe Anything Model (DAM). DAM is
capable of generating detailed descriptions of any specific image areas or
objects without the need for additional localized image-text alignment
supervision. We hypothesize that such region-level descriptive capability is
beneficial for the task of Visual Question Answering (VQA), especially in
challenging scenarios involving images with dense text. In such settings, the
fine-grained extraction of textual information is crucial to producing correct
answers. Motivated by this, we introduce DAM-QA, a framework with a tailored
evaluation protocol, developed to investigate and harness the region-aware
capabilities from DAM for the text-rich VQA problem that requires reasoning
over text-based information within images. DAM-QA incorporates a mechanism that
aggregates answers from multiple regional views of image content, enabling more
effective identification of evidence that may be tied to text-related elements.
Experiments on six VQA benchmarks show that our approach consistently
outperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA
also achieves the best overall performance among region-aware models with fewer
parameters, significantly narrowing the gap with strong generalist VLMs. These
results highlight the potential of DAM-like models for text-rich and broader
VQA tasks when paired with efficient usage and integration strategies. Our code
is publicly available at https://github.com/Linvyl/DAM-QA.git.

</details>


### [203] [CytoSAE: Interpretable Cell Embeddings for Hematology](https://arxiv.org/abs/2507.12464)
*Muhammed Furkan Dasdelen,Hyesu Lim,Michele Buck,Katharina S. Götze,Carsten Marr,Steffen Schneider*

Main category: cs.CV

TL;DR: 提出适用于血液学的稀疏自编码器CytoSAE，在多种数据集有效，可生成特定概念，在AML亚型分类任务表现好且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域缺乏解释基础模型推理的工具，尝试将SAEs应用于血液学。

Method: 提出CytoSAE并在超40000张外周血单细胞图像上训练。

Result: CytoSAE能泛化到不同数据集，识别相关概念并经专家验证，可生成特定概念，在AML亚型分类任务表现与SOTA相当。

Conclusion: SAEs在血液学领域有适用性，CytoSAE在医学图像解释和疾病检测方面有潜力。

Abstract: Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic
interpretability of transformer-based foundation models. Very recently, SAEs
were also adopted for the visual domain, enabling the discovery of visual
concepts and their patch-wise attribution to tokens in the transformer model.
While a growing number of foundation models emerged for medical imaging, tools
for explaining their inferences are still lacking. In this work, we show the
applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder
which is trained on over 40,000 peripheral blood single-cell images. CytoSAE
generalizes to diverse and out-of-domain datasets, including bone marrow
cytology, where it identifies morphologically relevant concepts which we
validated with medical experts. Furthermore, we demonstrate scenarios in which
CytoSAE can generate patient-specific and disease-specific concepts, enabling
the detection of pathognomonic cells and localized cellular abnormalities at
the patch level. We quantified the effect of concepts on a patient-level AML
subtype classification task and show that CytoSAE concepts reach performance
comparable to the state-of-the-art, while offering explainability on the
sub-cellular level. Source code and model weights are available at
https://github.com/dynamical-inference/cytosae.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [204] [HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways](https://arxiv.org/abs/2507.11621)
*Tianyi Wang,Yangyang Wang,Jie Pan,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: 本文针对高速入口合流区问题，提出分层协同匝道合流控制（HCOMC）框架，经仿真分析其在不同条件下有显著综合优势。


<details>
  <summary>Details</summary>
Motivation: 高速入口合流区是拥堵和事故瓶颈，在联网自动驾驶车辆（CAVs）未完全普及情况下，需为双车道异质车流提出控制框架。

Method: 扩展基于智能驾驶员模型的纵向跟驰模型和五次多项式曲线的横向换道模型；提出由分层协同规划模型、基于博弈论的换道模型和多目标优化模型组成的HCOMC框架。

Result: 通过仿真分析，HCOMC在不同交通密度和CAV渗透率下，相比基准方案在提升车辆安全、稳定加速合流过程、优化交通效率和节省燃油消耗方面有显著综合优势。

Conclusion: HCOMC框架能有效解决高速入口合流区问题，具有良好应用前景。

Abstract: Highway on-ramp merging areas are common bottlenecks to traffic congestion
and accidents. Currently, a cooperative control strategy based on connected and
automated vehicles (CAVs) is a fundamental solution to this problem. While CAVs
are not fully widespread, it is necessary to propose a hierarchical cooperative
on-ramp merging control (HCOMC) framework for heterogeneous traffic flow on
two-lane highways to address this gap. This paper extends longitudinal
car-following models based on the intelligent driver model and lateral
lane-changing models using the quintic polynomial curve to account for
human-driven vehicles (HDVs) and CAVs, comprehensively considering human
factors and cooperative adaptive cruise control. Besides, this paper proposes a
HCOMC framework, consisting of a hierarchical cooperative planning model based
on the modified virtual vehicle model, a discretionary lane-changing model
based on game theory, and a multi-objective optimization model using the
elitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and
efficient merging process. Then, the performance of our HCOMC is analyzed under
different traffic densities and CAV penetration rates through simulation. The
findings underscore our HCOMC's pronounced comprehensive advantages in
enhancing the safety of group vehicles, stabilizing and expediting merging
process, optimizing traffic efficiency, and economizing fuel consumption
compared with benchmarks.

</details>


### [205] [A Roadmap for Climate-Relevant Robotics Research](https://arxiv.org/abs/2507.11623)
*Alan Papalia,Charles Dawson,Laurentiu L. Anton,Norhan Magdy Bayomi,Bianca Champenois,Jung-Hoon Cho,Levi Cai,Joseph DelPreto,Kristen Edwards,Bilha-Catherine Githinji,Cameron Hickert,Vindula Jayawardana,Matthew Kramer,Shreyaa Raghavan,David Russell,Shide Salimi,Jingnan Shi,Soumya Sudhakar,Yanwei Wang,Shouyi Wang,Luca Carlone,Vijay Kumar,Daniela Rus,John E. Fernandez,Cathy Wu,George Kantor,Derek Young,Hanumant Singh*

Main category: cs.RO

TL;DR: 本文提出气候相关机器人研究路线图，确定机器人专家与气候领域专家合作机会，鼓励新研究方向和合作。


<details>
  <summary>Details</summary>
Motivation: 气候变化是21世纪挑战，机器人界寻求贡献途径。

Method: 确定机器人专家与能源、建筑环境等气候领域专家的合作机会，提出应用物理机器人及更广泛机器人工具包解决气候问题。

Result: 明确了能源系统优化、精准农业等气候相关应用问题。

Conclusion: 本路线图旨在激发新研究方向和合作，邀请机器人界解决紧迫气候问题。

Abstract: Climate change is one of the defining challenges of the 21st century, and
many in the robotics community are looking for ways to contribute. This paper
presents a roadmap for climate-relevant robotics research, identifying
high-impact opportunities for collaboration between roboticists and experts
across climate domains such as energy, the built environment, transportation,
industry, land use, and Earth sciences. These applications include problems
such as energy systems optimization, construction, precision agriculture,
building envelope retrofits, autonomous trucking, and large-scale environmental
monitoring. Critically, we include opportunities to apply not only physical
robots but also the broader robotics toolkit - including planning, perception,
control, and estimation algorithms - to climate-relevant problems. A central
goal of this roadmap is to inspire new research directions and collaboration by
highlighting specific, actionable problems at the intersection of robotics and
climate. This work represents a collaboration between robotics researchers and
domain experts in various climate disciplines, and it serves as an invitation
to the robotics community to bring their expertise to bear on urgent climate
priorities.

</details>


### [206] [Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers](https://arxiv.org/abs/2507.11991)
*Juanran Wang,Marc R. Schlichting,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 研究利用深度生成模型增强十字路口自动驾驶汽车安全，训练模型生成碰撞噪声序列，蒸馏为单步模型，用于构建鲁棒规划器，仿真显示其效果优于基线控制器。


<details>
  <summary>Details</summary>
Motivation: 提升十字路口等高危交通区域自动驾驶汽车的安全性。

Method: 训练1000步去噪扩散概率模型生成碰撞噪声序列，用生成对抗架构将其蒸馏为单步去噪扩散模型。

Result: 单步模型能快速推理且采样质量相近，基于其构建的鲁棒规划器在仿真中失败率和延迟率显著低于基线控制器。

Conclusion: 深度生成模型可有效提升十字路口自动驾驶汽车的安全性。

Abstract: High-risk traffic zones such as intersections are a major cause of
collisions. This study leverages deep generative models to enhance the safety
of autonomous vehicles in an intersection context. We train a 1000-step
denoising diffusion probabilistic model to generate collision-causing sensor
noise sequences for an autonomous vehicle navigating a four-way intersection
based on the current relative position and velocity of an intruder. Using the
generative adversarial architecture, the 1000-step model is distilled into a
single-step denoising diffusion model which demonstrates fast inference speed
while maintaining similar sampling quality. We demonstrate one possible
application of the single-step model in building a robust planner for the
autonomous vehicle. The planner uses the single-step model to efficiently
sample potential failure cases based on the currently measured traffic state to
inform its decision-making. Through simulation experiments, the robust planner
demonstrates significantly lower failure rate and delay rate compared with the
baseline Intelligent Driver Model controller.

</details>


### [207] [EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos](https://arxiv.org/abs/2507.12440)
*Ruihan Yang,Qinxi Yu,Yecheng Wu,Rui Yan,Borui Li,An-Chieh Cheng,Xueyan Zou,Yunhao Fang,Hongxu Yin,Sifei Liu,Song Han,Yao Lu,Xiaolong Wang*

Main category: cs.RO

TL;DR: 本文探索用第一人称视角人类视频训练VLA模型，经逆运动学转换和微调得到机器人策略EgoVLA，并在新基准上评估取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 真实机器人数据收集受硬件限制，规模受限，而人类视频数据规模大且场景和任务丰富。

Method: 用人类视频训练VLA模型预测人类手腕和手部动作，经逆运动学和重定向转换为机器人动作，用少量机器人操作演示微调模型得到EgoVLA，提出Isaac Humanoid Manipulation Benchmark进行评估。

Result: 在Isaac Humanoid Manipulation Benchmark上对EgoVLA进行微调评估，相比基线有显著改进，且证明了人类数据的重要性。

Conclusion: 利用人类视频训练模型可有效解决机器人数据规模受限问题，EgoVLA在机器人操作任务上表现良好。

Abstract: Real robot data collection for imitation learning has led to significant
advancements in robotic manipulation. However, the requirement for robot
hardware in the process fundamentally constrains the scale of the data. In this
paper, we explore training Vision-Language-Action (VLA) models using egocentric
human videos. The benefit of using human videos is not only for their scale but
more importantly for the richness of scenes and tasks. With a VLA trained on
human video that predicts human wrist and hand actions, we can perform Inverse
Kinematics and retargeting to convert the human actions to robot actions. We
fine-tune the model using a few robot manipulation demonstrations to obtain the
robot policy, namely EgoVLA. We propose a simulation benchmark called Isaac
Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation
tasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid
Manipulation Benchmark and show significant improvements over baselines and
ablate the importance of human data. Videos can be found on our website:
https://rchalyang.github.io/EgoVLA

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [208] [MaCE: General Mass Conserving Dynamics for Cellular Automata](https://arxiv.org/abs/2507.12306)
*Vassilis Papadopoulos,Etienne Guichard*

Main category: nlin.CG

TL;DR: 提出MaCE方法使元胞自动机实现质量守恒，展示其稳定性、应用效果并探讨研究方向。


<details>
  <summary>Details</summary>
Motivation: 实现元胞自动机（CA）的质量守恒，使CA产生更多有趣行为，避免模式爆炸或消亡。

Method: 提出MaCE这一简单进化规则并附加到现有CA上；先证明MaCE数值稳定性和连续极限，再在Lenia上测试，最后应用于Neural - CAs和离散CAs。

Result: MaCE能产生多种有趣行为，如孤子多样性和资源受限环境中的内在进化迹象。

Conclusion: MaCE方法具有通用性，为相关研究开辟了有前景的方向。

Abstract: We present Mass-Conserving Evolution (MaCE), a general method for
implementing mass conservation in Cellular Automata (CA). MaCE is a simple
evolution rule that can be easily 'attached' to existing CAs to make them
mass-conserving, which tends to produce interesting behaviours more often, as
patterns can no longer explode or die out. We first show that MaCE is
numerically stable and admits a simple continuous limit. We then test MaCE on
Lenia, and through several experiments, we demonstrate that it produces a wide
variety of interesting behaviours, starting from the variety and abundance of
solitons up to hints of intrinsic evolution in resource-constrained
environments. Finally, we showcase the versatility of MaCE by applying it to
Neural-CAs and discrete CAs, and discuss promising research directions opened
up by this scheme.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [209] [CosmoFlow: Scale-Aware Representation Learning for Cosmology with Flow Matching](https://arxiv.org/abs/2507.11842)
*Sidharth Kannan,Tian Qiu,Carolina Cuesta-Lazaro,Haewon Jeong*

Main category: astro-ph.CO

TL;DR: 研究表明基于流匹配的生成模型CosmoFlow可无监督学习冷暗物质模拟数据的紧凑、语义丰富的潜在表示。


<details>
  <summary>Details</summary>
Motivation: 探索生成机器学习模型对冷暗物质模拟数据低维表示学习能力，以用于下游任务。

Method: 使用基于流匹配的生成模型CosmoFlow进行无监督学习。

Result: CosmoFlow学习到的表示比原始数据小32倍，可用于场级重建、合成数据生成和参数推断，且学习到的表示可解释。

Conclusion: 基于流匹配的生成模型能无监督学习冷暗物质模拟数据的紧凑、语义丰富且可解释的潜在表示。

Abstract: Generative machine learning models have been demonstrated to be able to learn
low dimensional representations of data that preserve information required for
downstream tasks. In this work, we demonstrate that flow matching based
generative models can learn compact, semantically rich latent representations
of field level cold dark matter (CDM) simulation data without supervision. Our
model, CosmoFlow, learns representations 32x smaller than the raw field data,
usable for field level reconstruction, synthetic data generation, and parameter
inference. Our model also learns interpretable representations, in which
different latent channels correspond to features at different cosmological
scales.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [210] [FastReChain: Highly Responsive and Low-Overhead Centralized Route Scheduling in Clos Datacenter Networks](https://arxiv.org/abs/2507.12265)
*Zihan Zhu,Dongchao Wu,Zhanbang Zhang,Jian Yang*

Main category: cs.NI

TL;DR: 本文提出适用于数据中心网络的集中调度算法，利用替换链和位集优化，实现最大吞吐量，减少重排次数且支持动态调度。


<details>
  <summary>Details</summary>
Motivation: 数据中心网络采用Clos拓扑后缺乏实用的动态集中调度算法，引入光开关加剧了这一问题。

Method: 使用替换链概念和位集优化。

Result: 算法能在单速率双向Clos网络中实现理论最大吞吐量，重排次数接近最小，运行时间短，可支持动态调度且灵活性高。

Conclusion: 该算法是目前唯一直接支持双向Clos网络且时间效率足以支持动态调度的算法，能满足现实环境多种功能需求。

Abstract: Ever since Clos topologies were used in datacenter networks (DCNs), a
practical centralized scheduling algorithm that supports dynamic scheduling has
been absent. The introduction of optical switches in DCNs as a future-proof
solution exacerbates this problem due to several properties of optical
switches, such as the fact that they are generally bufferless and therefore
rely on centralized scheduling, and that they have long switching times and
therefore require the number of rearrangements to be minimized.
  In this paper, we propose a centralized scheduling algorithm that achieves
theoretical maximum throughput even in one-rate bidirectional Clos networks,
while producing schemes with near-minimal numbers of rearrangements. It is the
only algorithm that directly supports bidirectional Clos networks and has a
time efficiency high enough to support dynamic scheduling to date. For static
minimal rewiring, its running time ranges from a fraction to a few hundredths
of other algorithms, and the number of rearrangements has also been steadily
improved, allowing for more frequent adjustments and less impact on ongoing
communications. In addition, the algorithm is very flexible and can support
various functional requirements in real-world environments. We achieve this
result through the replacement chain concept and bitset optimization.

</details>


### [211] [CRAFT: Latency and Cost-Aware Genetic-Based Framework for Node Placement in Edge-Fog Environments](https://arxiv.org/abs/2507.12445)
*Soheil Mahdizadeh,Amir Mahdi Rasouli,Mohammad Pourashory,Sadra Galavani,Mohsen Ansari*

Main category: cs.NI

TL;DR: 本文提出基于遗传算法的边缘 - 雾节点放置策略，可减少物联网延迟和成本，模拟显示有显著效果。


<details>
  <summary>Details</summary>
Motivation: 云计算难以可靠满足物联网实时需求，边缘和雾计算可降低延迟，但节点放置影响延迟和成本，需优化。

Method: 提出基于遗传算法的有效且可调节的节点放置策略。

Result: 模拟结果显示，所提框架可减少 2.77% 的延迟和 31.15% 的成本。

Conclusion: 基于遗传算法的节点放置策略能有效优化边缘和雾节点部署，降低延迟和成本。

Abstract: Reducing latency in the Internet of Things (IoT) is a critical concern. While
cloud computing facilitates communication, it falls short of meeting real-time
requirements reliably. Edge and fog computing have emerged as viable solutions
by positioning computing nodes closer to end users, offering lower latency and
increased processing power. An edge-fog framework comprises various components,
including edge and fog nodes, whose strategic placement is crucial as it
directly impacts latency and system cost. This paper presents an effective and
tunable node placement strategy based on a genetic algorithm to address the
optimization problem of deploying edge and fog nodes. The main objective is to
minimize latency and cost through optimal node placement. Simulation results
demonstrate that the proposed framework achieves up to 2.77% latency and 31.15%
cost reduction.

</details>


### [212] [Native-AI Empowered Scalable Architectures and Solutions for Future Non-Terrestrial Networks: An Overview](https://arxiv.org/abs/2507.11935)
*Jikang Deng,Fizza Hassan,Hui Zhou,Saad Al-Ahmadi,Mohamed-Slim Alouini,Daniel B. Da Costa*

Main category: cs.NI

TL;DR: 本文探讨6G网络下非地面网络（NTN）与开放无线接入网（ORAN）结合，提出ORAN - NTN框架并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: NTN在开发运维（DevOps）生命周期存在挑战，缺乏原生AI能力阻碍智能和可扩展网络管理，且缺少将ORAN与NTN在DevOps生命周期有效结合的整体观点。

Method: 先介绍ORAN和NTN背景知识与相关研究、DevOps挑战，再提出ORAN - NTN框架并详细讨论其特性和架构。

Result: 提出了ORAN - NTN框架，包含灵活前传分割、RAN智能控制器增强、可扩展部署架构和多域服务管理等内容。

Conclusion: 指明了ORAN - NTN框架与其他技术结合等未来研究方向和候选用例。

Abstract: As the path toward 6G networks is being charted, the emerging applications
have motivated evolutions of network architectures to realize the efficient,
reliable, and flexible wireless networks. Among the potential architectures,
the non-terrestrial network (NTN) and open radio access network (ORAN) have
received increasing interest from both academia and industry. Although the
deployment of NTNs ensures coverage, enhances spectral efficiency, and improves
the resilience of wireless networks. The high altitude and mobility of NTN
present new challenges in the development and operations (DevOps) lifecycle,
hindering intelligent and scalable network management due to the lack of native
artificial intelligence (AI) capability. With the advantages of ORAN in
disaggregation, openness, virtualization, and intelligence, several works
propose integrating ORAN principles into the NTN, focusing mainly on ORAN
deployment options based on transparent and regenerative systems. However, a
holistic view of how to effectively combine ORAN and NTN throughout the DevOps
lifecycle is still missing, especially regarding how intelligent ORAN addresses
the scalability challenges in NTN. Motivated by this, in this paper, we first
provide the background knowledge about ORAN and NTN, outline the
state-of-the-art research on ORAN for NTNs, and present the DevOps challenges
that motivate the adoption of ORAN solutions. We then propose the ORAN-based
NTN framework, discussing its features and architectures in detail. These
include the discussion about flexible fronthaul split, RAN intelligent
controllers (RICs) enhancement for distributed learning, scalable deployment
architecture, and multi-domain service management. Finally, the future research
directions, including combinations of the ORAN-based NTN framework and other
enabling technologies and schemes, as well as the candidate use cases, are
highlighted.

</details>


### [213] [LLM-Based Config Synthesis requires Disambiguation](https://arxiv.org/abs/2507.12443)
*Rajdeep Mondal,Nikolaj Bjorner,Todd Millstein,Alan Tang,George Varghese*

Main category: cs.NI

TL;DR: 论文指出LLM程序合成中用户意图模糊问题，以网络配置为例说明，提出Clarify系统解决模糊性并验证策略。


<details>
  <summary>Details</summary>
Motivation: 解决LLM程序合成中用户意图模糊问题，以网络配置合成场景为例。

Method: 提出Clarify原型系统，使用带有Disambiguator模块的LLM引出用户意图。

Result: 在小的合成工作负载上，Clarify能在消除歧义后增量合成路由策略并验证。

Conclusion: 处理模糊性在LLM能正确合成更新意图但集成有歧义时普遍有用。

Abstract: Beyond hallucinations, another problem in program synthesis using LLMs is
ambiguity in user intent. We illustrate the ambiguity problem in a networking
context for LLM-based incremental configuration synthesis of route-maps and
ACLs. These structures frequently overlap in header space, making the relative
priority of actions impossible for the LLM to infer without user interaction.
Measurements in a large cloud identify complex ACLs with 100's of overlaps,
showing ambiguity is a real problem. We propose a prototype system, Clarify,
which uses an LLM augmented with a new module called a Disambiguator that helps
elicit user intent. On a small synthetic workload, Clarify incrementally
synthesizes routing policies after disambiguation and then verifies them. Our
treatment of ambiguities is useful more generally when the intent of updates
can be correctly synthesized by LLMs, but their integration is ambiguous and
can lead to different global behaviors.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [214] [Quantum Machine Learning in Multi-Qubit Phase-Space Part I: Foundations](https://arxiv.org/abs/2507.12117)
*Timothy Heightman,Edward Jiang,Ruth Mora-Soto,Maciej Lewenstein,Marcin Płodzień*

Main category: quant-ph

TL;DR: 本文构建了相空间中单和多量子比特系统的动力学形式，为基于相空间变分建模的量子机器学习开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在经典模拟中因希尔伯特空间指数增长面临实际限制，相空间方法提供了替代方案。

Method: 基于先前量子比特相空间和Stratonovich - Weyl对应工作，构建相空间中量子比特系统的封闭、可组合动力学形式。

Result: 用辛流形上的函数动力学取代泡利群的算子代数，将维数灾难转化为与量子比特数量线性相关的调和支撑问题。

Conclusion: 为基于相空间变分建模的量子机器学习开辟了新途径。

Abstract: Quantum machine learning (QML) seeks to exploit the intrinsic properties of
quantum mechanical systems, including superposition, coherence, and quantum
entanglement for classical data processing. However, due to the exponential
growth of the Hilbert space, QML faces practical limits in classical
simulations with the state-vector representation of quantum system. On the
other hand, phase-space methods offer an alternative by encoding quantum states
as quasi-probability functions. Building on prior work in qubit phase-space and
the Stratonovich-Weyl (SW) correspondence, we construct a closed, composable
dynamical formalism for one- and many-qubit systems in phase-space. This
formalism replaces the operator algebra of the Pauli group with function
dynamics on symplectic manifolds, and recasts the curse of dimensionality in
terms of harmonic support on a domain that scales linearly with the number of
qubits. It opens a new route for QML based on variational modelling over
phase-space.

</details>


### [215] [BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search](https://arxiv.org/abs/2507.12189)
*Azhar Ikhtiarudin,Aditi Das,Param Thakkar,Akash Kundu*

Main category: quant-ph

TL;DR: 介绍BenchRL - QAS框架用于评估量子架构搜索中强化学习算法，对多种算法在不同量子任务上测试，提出加权排名指标，发现结果并得出结论，代码数据公开。


<details>
  <summary>Details</summary>
Motivation: 系统评估量子架构搜索中强化学习算法，推动量子电路综合发展。

Method: 引入BenchRL - QAS框架，对9种RL代理在不同量子问题上测试，提出加权排名指标。

Result: 基于RL的量子分类器优于基线变分分类器；没有单一RL算法在所有QAS任务上最优，性能与任务结构、qubit数量和噪声有关。

Conclusion: 为基于RL的量子电路设计的“没有免费午餐”原则提供证据，强调定制算法选择和系统基准测试的必要性。

Abstract: We introduce BenchRL-QAS, a unified benchmarking framework for systematically
evaluating reinforcement learning (RL) algorithms in quantum architecture
search (QAS) across diverse variational quantum algorithm tasks and system
sizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including
both value-based and policy-gradient methods on representative quantum problems
such as variational quantum eigensolver, variational quantum state
diagonalization, quantum classification, and state preparation, spanning both
noiseless and realistic noisy regimes. We propose a weighted ranking metric
that balances accuracy, circuit depth, gate count, and computational
efficiency, enabling fair and comprehensive comparison. Our results first
reveal that RL-based quantum classifier outperforms baseline variational
classifiers. Then we conclude that no single RL algorithm is universally
optimal when considering a set of QAS tasks; algorithmic performance is highly
context-dependent, varying with task structure, qubit count, and noise. This
empirical finding provides strong evidence for the "no free lunch" principle in
RL-based quantum circuit design and highlights the necessity of tailored
algorithm selection and systematic benchmarking for advancing quantum circuit
synthesis. This work represents the most comprehensive RL-QAS benchmarking
effort to date, and BenchRL-QAS along with all experimental data are made
publicly available to support reproducibility and future research
https://github.com/azhar-ikhtiarudin/bench-rlqas.

</details>


### [216] [Surrogate Quantum Circuit Design for the Lattice Boltzmann Collision Operator](https://arxiv.org/abs/2507.12256)
*Monica Lăcătuş,Matthias Möller*

Main category: quant-ph

TL;DR: 传统CFD工具在高雷诺数湍流直接数值模拟有挑战，量子算法受关注，QLBM的碰撞步骤实现有难题，本文提出学习替代量子电路框架解决该问题并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统CFD工具在高雷诺数湍流模拟有挑战，量子计算机有望加速计算，QLBM的碰撞步骤实现存在困难。

Method: 引入学习替代量子电路（SQC）的框架，训练四量子比特电路使其符合BGK碰撞算子物理特性，编译到IBM Heron处理器门集。

Result: 15块SQC只需2430个原生门，无需辅助量子比特、后选择或重复执行，深度与网格分辨率无关。

Conclusion: 在两个基准流验证SQC，证明其能准确捕捉涡旋耗散和流动再循环。

Abstract: Direct numerical simulation of turbulent flows at high Reynolds numbers
remains a major challenge for traditional computational fluid dynamics (CFD)
tools running on classical computer hardware. This has motivated growing
interest in quantum algorithms for CFD to enable flow simulations on quantum
computers. The reason being that these computers are expected to deliver
potential speed-ups for certain problems. One promising quantum CFD approach is
a fully quantum implementation of the lattice Boltzmann method called QLBM.
Although efficient quantum routines are now available for the streaming step,
implementing the nonlinear, irreversible collision step with a low depth
circuit that avoids additional ancilla qubits, probabilistic post-selection and
repeated executions remains a significant challenge. In this study, we address
this challenge by introducing a framework for learning a surrogate quantum
circuit (SQC) that approximates the full Bhatnagar Gross Krook (BGK) collision
operator for the D2Q9 lattice. The four qubit circuit is trained to respect the
physical properties of the BGK collision operator, including mass and momentum
conservation, D8 equivariance and scale equivariance. When compiled to the gate
set used by IBM Heron processor under the assumption of full qubit
connectivity, the 15 block SQC requires only 2,430 native gates and uses
neither ancilla qubits nor post-selection or repeated executions. Moreover, its
depth is independent of the grid resolution, as collision is a local operation
that can exploit quantum parallelism to its full extent. We validate the SQC on
two benchmark flows, the Taylor Green vortex decay and the lid driven cavity,
demonstrating that it accurately captures vortex dissipation and flow
recirculation.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [217] [Galaxy image simplification using Generative AI](https://arxiv.org/abs/2507.11692)
*Sai Teja Erukude,Lior Shamir*

Main category: astro-ph.GA

TL;DR: 提出基于生成式AI的星系图像分析新方法，应用于DESI Legacy Survey图像，代码、数据和简化图像目录公开。


<details>
  <summary>Details</summary>
Motivation: 现代数字巡天获取大量星系图像，准确分析需有效自动化，现有方案多依赖预定义类别的机器学习标注。

Method: 基于生成式AI，将星系图像简化并转换为“骨架化”形式。

Result: 对125,000张DESI Legacy Survey图像应用该方法，简化图像目录公开。

Conclusion: 新方法可准确测量星系形状，且不受预定义类别限制。

Abstract: Modern digital sky surveys have been acquiring images of billions of
galaxies. While these images often provide sufficient details to analyze the
shape of the galaxies, accurate analysis of such high volumes of images
requires effective automation. Current solutions often rely on machine learning
annotation of the galaxy images based on a set of pre-defined classes. Here we
introduce a new approach to galaxy image analysis that is based on generative
AI. The method simplifies the galaxy images and automatically converts them
into a ``skeletonized" form. The simplified images allow accurate measurements
of the galaxy shapes and analysis that is not limited to a certain pre-defined
set of classes. We demonstrate the method by applying it to galaxy images
acquired by the DESI Legacy Survey. The code and data are publicly available.
The method was applied to 125,000 DESI Legacy Survey images, and the catalog of
the simplified images is publicly available.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [218] [Enhancing Signal Proportion Estimation Through Leveraging Arbitrary Covariance Structures](https://arxiv.org/abs/2507.11922)
*Jingtian Bai,Xinge Jessie Jeng*

Main category: math.ST

TL;DR: 本文提出新的信号比例估计器，结合变量依赖信息，理论分析其有效性并通过模拟证明性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统信号比例估计器假设变量独立和特定稀疏条件，在现实场景适用性有限，需改进。

Method: 基于已有信号比例下界置信区间工作，纳入主因子近似过程处理变量依赖。

Result: 通过模拟表明新方法在估计准确性和检测弱信号方面优于现有方法。

Conclusion: 新估计器能利用变量依赖信息，在不同稀疏水平和依赖结构下表现良好，理论基础保证其在多样场景可靠应用。

Abstract: Accurately estimating the proportion of true signals among a large number of
variables is crucial for enhancing the precision and reliability of scientific
research. Traditional signal proportion estimators often assume independence
among variables and specific signal sparsity conditions, limiting their
applicability in real-world scenarios where such assumptions may not hold. This
paper introduces a novel signal proportion estimator that leverages arbitrary
covariance dependence information among variables, thereby improving
performance across a wide range of sparsity levels and dependence structures.
Building on previous work that provides lower confidence bounds for signal
proportions, we extend this approach by incorporating the principal factor
approximation procedure to account for variable dependence. Our theoretical
insights offer a deeper understanding of how signal sparsity, signal intensity,
and covariance dependence interact. By comparing the conditions for estimation
consistency before and after dependence adjustment, we highlight the advantages
of integrating dependence information across different contexts. This
theoretical foundation not only validates the effectiveness of the new
estimator but also guides its practical application, ensuring reliable use in
diverse scenarios. Through extensive simulations, we demonstrate that our
method outperforms state-of-the-art estimators in both estimation accuracy and
the detection of weaker signals that might otherwise go undetected.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [219] [MOFCO: Mobility- and Migration-Aware Task Offloading in Three-Layer Fog Computing Environments](https://arxiv.org/abs/2507.12028)
*Soheil Mahdizadeh,Elyas Oustad,Mohsen Ansari*

Main category: cs.AR

TL;DR: 本文提出MOFCO算法解决三层雾计算环境中因用户设备移动性导致的任务卸载难题，经实验验证该算法可降低系统成本。


<details>
  <summary>Details</summary>
Motivation: 三层雾计算环境中用户设备移动性常引发服务迁移成本高、系统性能下降问题，需解决任务卸载难题。

Method: 将任务卸载和资源分配建模为混合整数非线性规划问题，采用启发式辅助进化博弈论方法求解；用SUMO模拟移动用户以提供真实移动模式。

Result: 与现有方法相比，MOFCO算法平均降低系统成本（延迟和能耗组合）19%，特定场景下最高降低43%。

Conclusion: MOFCO算法能有效降低系统成本，在雾计算任务卸载中表现良好。

Abstract: Task offloading in three-layer fog computing environments presents a critical
challenge due to user equipment (UE) mobility, which frequently triggers costly
service migrations and degrades overall system performance. This paper
addresses this problem by proposing MOFCO, a novel Mobility- and
Migration-aware Task Offloading algorithm for Fog Computing environments. The
proposed method formulates task offloading and resource allocation as a
Mixed-Integer Nonlinear Programming (MINLP) problem and employs a
heuristic-aided evolutionary game theory approach to solve it efficiently. To
evaluate MOFCO, we simulate mobile users using SUMO, providing realistic
mobility patterns. Experimental results show that MOFCO reduces system cost,
defined as a combination of latency and energy consumption, by an average of
19% and up to 43% in certain scenarios compared to state-of-the-art methods.

</details>


### [220] [Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length](https://arxiv.org/abs/2507.12442)
*Saptarshi Mitra,Rachid Karami,Haocheng Xu,Sitao Huang,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: 对Transformer、SSM和混合模型在消费级和嵌入式GPU上进行长上下文推理基准测试，发现SSM在长上下文表现优越，还给出算子级分析和设备特性结果并将开源框架。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构处理长上下文输入效率低，当前研究缺乏对新架构在实际消费硬件上的系统性能表征，需以此指导系统级优化和开发新应用。

Method: 对精心挑选的Transformer、SSM和混合模型在消费级和嵌入式GPU上进行全面的对比基准测试。

Result: SSM在长上下文领域不仅可行且优越，在24GB消费级GPU上能处理达220K令牌序列，约为可比Transformer的4倍；短序列时Transformers快1.8倍，长上下文（约57K令牌）时SSM快4倍；自定义硬件感知的SSM内核占边缘平台推理运行时间超55%。

Conclusion: SSM适合长上下文推理，其自定义内核是未来硬件加速的主要目标，提供的设备特性结果可指导边缘系统协同设计。

Abstract: The demand for machine intelligence capable of processing continuous,
long-context inputs on local devices is growing rapidly. However, the quadratic
complexity and memory requirements of traditional Transformer architectures
make them inefficient and often unusable for these tasks. This has spurred a
paradigm shift towards new architectures like State Space Models (SSMs) and
hybrids, which promise near-linear scaling. While most current research focuses
on the accuracy and theoretical throughput of these models, a systematic
performance characterization on practical consumer hardware is critically
needed to guide system-level optimization and unlock new applications.
  To address this gap, we present a comprehensive, comparative benchmarking of
carefully selected Transformer, SSM, and hybrid models specifically for
long-context inference on consumer and embedded GPUs. Our analysis reveals that
SSMs are not only viable but superior for this domain, capable of processing
sequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than
comparable Transformers. While Transformers may be up to 1.8x faster at short
sequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x
faster at very long contexts (~57K tokens). Our operator-level analysis reveals
that custom, hardware-aware SSM kernels dominate the inference runtime,
accounting for over 55% of latency on edge platforms, identifying them as a
primary target for future hardware acceleration. We also provide detailed,
device-specific characterization results to guide system co-design for the
edge. To foster further research, we will open-source our characterization
framework.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [221] [Multimodal Coordinated Online Behavior: Trade-offs and Strategies](https://arxiv.org/abs/2507.12108)
*Lorenzo Mannocci,Stefano Cresci,Matteo Magnani,Anna Monreale,Maurizio Tesconi*

Main category: cs.SI

TL;DR: 本文比较了检测多模态协调行为的不同方法，研究强弱集成多模态模型的权衡，发现多模态方法能更全面理解协调动态，提升检测和分析在线协调行为的能力。


<details>
  <summary>Details</summary>
Motivation: 传统单模态或独立多模态方法可能忽略多模态协调的复杂动态，需要更好的方法来检测和分析在线协调行为。

Method: 比较不同操作化多模态协调行为检测的方法，评估单模态和多模态方法，研究强弱集成多模态模型的权衡。

Result: 并非所有模态都能提供独特见解，但多模态方法可更全面理解协调动态。

Conclusion: 本研究增强了检测和分析在线协调行为的能力，为维护数字平台完整性提供新视角。

Abstract: Coordinated online behavior, which spans from beneficial collective actions
to harmful manipulation such as disinformation campaigns, has become a key
focus in digital ecosystem analysis. Traditional methods often rely on
monomodal approaches, focusing on single types of interactions like co-retweets
or co-hashtags, or consider multiple modalities independently of each other.
However, these approaches may overlook the complex dynamics inherent in
multimodal coordination. This study compares different ways of operationalizing
the detection of multimodal coordinated behavior. It examines the trade-off
between weakly and strongly integrated multimodal models, highlighting the
balance between capturing broader coordination patterns and identifying tightly
coordinated behavior. By comparing monomodal and multimodal approaches, we
assess the unique contributions of different data modalities and explore how
varying implementations of multimodality impact detection outcomes. Our
findings reveal that not all the modalities provide distinct insights, but that
with a multimodal approach we can get a more comprehensive understanding of
coordination dynamics. This work enhances the ability to detect and analyze
coordinated online behavior, offering new perspectives for safeguarding the
integrity of digital platforms.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [222] [Foundation Models for Brain Signals: A Critical Review of Current Progress and Future Directions](https://arxiv.org/abs/2507.11783)
*Gayal Kuruppu,Neeraj Wagh,Yogatheesan Varatharajah*

Main category: eess.SP

TL;DR: 本文回顾了10个早期脑电基础模型（EEG - FMs），分析其方法、结果与研究差距，指出评估不足问题并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 有监督脑电编码器存在局限，早期EEG - FMs在现实应用和长期研究进展方面情况不明，需系统全面回顾早期模型以明确现状和未来方向。

Method: 回顾10个早期EEG - FMs，综合分析其方法、实证结果和研究差距。

Result: 多数EEG - FMs采用基于序列的建模方案，依赖基于Transformer的主干网络和掩码序列重建进行自监督学习，但模型评估异质性大且受限，难以评估其实用性。

Conclusion: 未来研究应采用标准化和现实的评估，展示更大的缩放效应，在脑电表征学习流程中做出合理可靠的选择，与领域专家合作开发相关基准、工具、方法和应用以推动EEG - FMs的转化应用和实际采用。

Abstract: Patterns of electrical brain activity recorded via electroencephalography
(EEG) offer immense value for scientific and clinical investigations. The
inability of supervised EEG encoders to learn robust EEG patterns and their
over-reliance on expensive signal annotations have sparked a transition towards
general-purpose self-supervised EEG encoders, i.e., EEG foundation models
(EEG-FMs), for robust and scalable EEG feature extraction. However, the
real-world readiness of early EEG-FMs and the rubric for long-term research
progress remain unclear. A systematic and comprehensive review of
first-generation EEG-FMs is therefore necessary to understand the current
state-of-the-art and identify key directions for future EEG-FMs. To that end,
this study reviews 10 early EEG-FMs and presents a critical synthesis of their
methodology, empirical findings, and outstanding research gaps. We find that
most EEG-FMs adopt a sequence-based modeling scheme that relies on
transformer-based backbones and the reconstruction of masked sequences for
self-supervision. However, model evaluations remain heterogeneous and largely
limited, making it challenging to assess their practical off-the-shelf utility.
In addition to adopting standardized and realistic evaluations, future work
should demonstrate more substantial scaling effects and make principled and
trustworthy choices throughout the EEG representation learning pipeline. We
believe that developing benchmarks, software tools, technical methodologies,
and applications in collaboration with domain experts may further advance the
translational utility and real-world adoption of EEG-FMs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [223] [Quantize More, Lose Less: Autoregressive Generation from Residually Quantized Speech Representations](https://arxiv.org/abs/2507.12197)
*Yichen Han,Xiaoyang Hao,Keming Chen,Weibo Xiong,Jun He,Ruonan Zhang,Junjie Cao,Yue Liu,Bowen Li,Dongrui Zhang,Hui Xia,Huilei Fu,Kai Jia,Kaixuan Guo,Mingli Jin,Qingyun Meng,Ruidong Ma,Ruiqian Fang,Shaotong Guo,Xuhui Li,Yang Xiang,Ying Zhang,Yulong Liu,Yunfeng Li,Yuyi Zhang,Yuze Zhou,Zhen Wang,Zhaowen Chen*

Main category: cs.SD

TL;DR: 提出基于新音频编解码器QDAC的TTS框架QTTS，在合成质量和保留表达内容上优于基线，多码本建模是高保真音频生成的有前景方向。


<details>
  <summary>Details</summary>
Motivation: 现有自回归TTS方法依赖单码本表示，信息损失大，难恢复细粒度细节，在复杂场景表现不佳。

Method: 构建基于QDAC的QTTS框架，QDAC端到端训练基于ASR的自回归网络与GAN；QTTS采用分层并行架构和延迟多头方法处理离散代码。

Result: 实验表明，所提框架合成质量更高，能更好保留表达内容。

Conclusion: 通过多码本建模扩展压缩是高保真、通用语音和音频生成的有前景方向。

Abstract: Text-to-speech (TTS) synthesis has seen renewed progress under the discrete
modeling paradigm. Existing autoregressive approaches often rely on
single-codebook representations, which suffer from significant information
loss. Even with post-hoc refinement techniques such as flow matching, these
methods fail to recover fine-grained details (e.g., prosodic nuances,
speaker-specific timbres), especially in challenging scenarios like singing
voice or music synthesis. We propose QTTS, a novel TTS framework built upon our
new audio codec, QDAC. The core innovation of QDAC lies in its end-to-end
training of an ASR-based auto-regressive network with a GAN, which achieves
superior semantic feature disentanglement for scalable, near-lossless
compression. QTTS models these discrete codes using two innovative strategies:
the Hierarchical Parallel architecture, which uses a dual-AR structure to model
inter-codebook dependencies for higher-quality synthesis, and the Delay
Multihead approach, which employs parallelized prediction with a fixed delay to
accelerate inference speed. Our experiments demonstrate that the proposed
framework achieves higher synthesis quality and better preserves expressive
content compared to baseline. This suggests that scaling up compression via
multi-codebook modeling is a promising direction for high-fidelity,
general-purpose speech and audio generation.

</details>


### [224] [RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection](https://arxiv.org/abs/2507.12175)
*Sungkyun Chang,Simon Dixon,Emmanouil Benetos*

Main category: cs.SD

TL;DR: 介绍基于Transformer的音乐表演分析框架RUMAA，近端到端统一多项任务，性能表现良好。


<details>
  <summary>Details</summary>
Motivation: 以往方法分别处理音乐表演分析任务，需统一方法；传统MIDI方法有局限性。

Method: 使用预训练的乐谱和音频编码器及新颖的三流解码器，通过代理任务捕获任务间依赖关系。

Result: 在非重复乐谱上与现有对齐方法相当，在有重复的乐谱上表现更优，转录和错误检测结果也不错。

Conclusion: RUMAA能有效统一音乐表演分析的多项任务，性能良好。

Abstract: This study introduces RUMAA, a transformer-based framework for music
performance analysis that unifies score-to-performance alignment,
score-informed transcription, and mistake detection in a near end-to-end
manner. Unlike prior methods addressing these tasks separately, RUMAA
integrates them using pre-trained score and audio encoders and a novel
tri-stream decoder capturing task interdependencies through proxy tasks. It
aligns human-readable MusicXML scores with repeat symbols to full-length
performance audio, overcoming traditional MIDI-based methods that rely on
manually unfolded score-MIDI data with pre-specified repeat structures. RUMAA
matches state-of-the-art alignment methods on non-repeated scores and
outperforms them on scores with repeats in a public piano music dataset, while
also delivering promising transcription and mistake detection results.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [225] [LevelSetPy: A GPU-Accelerated Package for Hyperbolic Hamilton-Jacobi Partial Differential Equations](https://arxiv.org/abs/2507.11542)
*Lekan Molu*

Main category: cs.MS

TL;DR: 本文介绍用于通过水平集方法对复杂动力系统安全需求进行几何推理的软件包，利用现代计算框架加速，助用户评估系统安全需求。


<details>
  <summary>Details</summary>
Motivation: 为解决现代工程问题中对复杂动力系统安全需求进行几何推理的需求，提高计算速度和与其他科学计算库的互操作性。

Method: 使用水平集方法分析安全问题，用Hamilton - Jacobi方程；提供求解Hamilton - Jacobi - Isaacs方程的数值算法；利用CUPY和NUMPY等框架，将重计算任务移到GPU进行并行计算。

Result: 开发出相关软件包。

Conclusion: 该软件包能帮助用户在现代工程问题中快速迭代想法，通过几何模拟评估系统的所有可能安全需求。

Abstract: This article introduces a software package release for geometrically
reasoning about the \textit{safety} desiderata of (complex) dynamical systems
via level set methods. In emphasis, safety is analyzed with Hamilton-Jacobi
equations. In scope, we provide implementations of numerical algorithms for the
resolution of Hamilton-Jacobi-Isaacs equations: the spatial derivatives of the
associated value function via upwinding, the Hamiltonian via Lax-Friedrichs
schemes, and the integration of the Hamilton-Jacobi equation altogether via
total variation diminishing Runge-Kutta schemes. Since computational speed and
interoperability with other modern scientific computing libraries (typically
written in the Python language) is of essence, we capitalize on modern
computational frameworks such as \texttt{CUPY} and \texttt{NUMPY} and move
heavy computations to GPU devices to aid parallelization and improve bring-up
time in safety analysis. We hope that this package can aid users to quickly
iterate on ideas and evaluate all possible safety desiderata of a system via
geometrical simulation in modern engineering problems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [226] [Expanding ML-Documentation Standards For Better Security](https://arxiv.org/abs/2507.12003)
*Cara Ellen Appel*

Main category: cs.CR

TL;DR: 文章基于文献综述分析ML安全与文档现状，指出问题并提出扩展现有标准以包含安全部分的建议。


<details>
  <summary>Details</summary>
Motivation: 当前ML从业者和组织对安全方面意识低，文档缺乏标准化，质量差，需改进ML安全文档。

Method: 广泛回顾现有文献，基于现有Model Cards和Datasheets for Datasets标准，提出扩展ML文档标准以包含安全部分。

Result: 发现ML从业者和组织安全意识低、文档不规范、未采用现有标准等问题。

Conclusion: 有必要改进ML安全文档，建议在所有ML文档中采用扩展的安全要求记录方法。

Abstract: This article presents the current state of ML-security and of the
documentation of ML-based systems, models and datasets in research and practice
based on an extensive review of the existing literature. It shows a generally
low awareness of security aspects among ML-practitioners and organizations and
an often unstandardized approach to documentation, leading to overall low
quality of ML-documentation. Existing standards are not regularly adopted in
practice and IT-security aspects are often not included in documentation. Due
to these factors, there is a clear need for improved security documentation in
ML, as one step towards addressing the existing gaps in ML-security. To achieve
this, we propose expanding existing documentation standards for
ML-documentation to include a security section with specific security relevant
information. Implementing this, a novel expanded method of documenting security
requirements in ML-documentation is presented, based on the existing Model
Cards and Datasheets for Datasets standards, but with the recommendation to
adopt these findings in all ML-documentation.

</details>


### [227] [Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630)
*Brendan Murphy,Dillon Bowen,Shahrad Mohammadzadeh,Julius Broomfield,Adam Gleave,Kellin Pelrine*

Main category: cs.CR

TL;DR: 本文指出微调可使模型生成有害回复，后门会增强攻击危害，新模型更易受攻击，强调需防篡改防护措施。


<details>
  <summary>Details</summary>
Motivation: 虽模型开发者重视安全防护，但研究发现微调可产生有害模型，需揭示问题并强调防护紧迫性。

Method: 提出越狱微调方法，让模型对有害请求生成高质量回复，并研究后门及强越狱提示在微调攻击中的作用。

Result: OpenAI、Google和Anthropic等模型会按有害请求行动，后门增加攻击隐蔽性和严重性，新模型更易受攻击。

Conclusion: 急需防篡改防护措施，在其出现前，公司和政策制定者应谨慎对待可微调模型发布。

Abstract: AI systems are rapidly advancing in capability, and frontier model developers
broadly acknowledge the need for safeguards against serious misuse. However,
this paper demonstrates that fine-tuning, whether via open weights or closed
fine-tuning APIs, can produce helpful-only models. In contrast to prior work
which is blocked by modern moderation systems or achieved only partial removal
of safeguards or degraded output quality, our jailbreak-tuning method teaches
models to generate detailed, high-quality responses to arbitrary harmful
requests. For example, OpenAI, Google, and Anthropic models will fully comply
with requests for CBRN assistance, executing cyberattacks, and other criminal
activity. We further show that backdoors can increase not only the stealth but
also the severity of attacks, while stronger jailbreak prompts become even more
effective in fine-tuning attacks, linking attack and potentially defenses in
the input and weight spaces. Not only are these models vulnerable, more recent
ones also appear to be becoming even more vulnerable to these attacks,
underscoring the urgent need for tamper-resistant safeguards. Until such
safeguards are discovered, companies and policymakers should view the release
of any fine-tunable model as simultaneously releasing its evil twin: equally
capable as the original model, and usable for any malicious purpose within its
capabilities.

</details>


### [228] [Challenges in GenAI and Authentication: a scoping review](https://arxiv.org/abs/2507.11775)
*Wesley dos Reis Bezerra,Lais Machado Bezerra,Carlos Becker Westphall*

Main category: cs.CR

TL;DR: 本文对认证和生成式人工智能相关文献进行范围审查，分析相关挑战、威胁等，支持新研究。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能发展，认证和真实性安全挑战演变，需更新分析其对社会和系统安全的影响。

Method: 对IEEExplorer、Scopus和ACM数据库的88篇文献进行范围审查，通过六个引导问题分析结果组合，并对文章进行单独分析。

Result: 明确了图像、文本、音频和视频相关的挑战、差距和威胁。

Conclusion: 研究结果支持认证和生成式人工智能领域的新研究。

Abstract: Authentication and authenticity have been a security challenge since the
beginning of information sharing, especially in the context of digital
information. With the advancement of generative artificial intelligence, these
challenges have evolved, demanding a more up-to-date analysis of their impacts
on society and system security. This work presents a scoping review that
analyzed 88 documents from the IEEExplorer, Scopus, and ACM databases,
promoting an analysis of the resulting portfolio through six guiding questions
focusing on the most relevant work, challenges, attack surfaces, threats,
proposed solutions, and gaps. Finally, the portfolio articles are analyzed
through this guiding research lens and also receive individualized analysis.
The results consistently outline the challenges, gaps, and threats related to
images, text, audio, and video, thereby supporting new research in the areas of
authentication and generative artificial intelligence.

</details>


### [229] [Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification](https://arxiv.org/abs/2507.11943)
*Haiwei Lin,Shoko Imaizumi,Hitoshi Kiya*

Main category: cs.CR

TL;DR: 提出一种低秩适配方法训练隐私保护视觉Transformer模型，减少可训练参数且保持准确率。


<details>
  <summary>Details</summary>
Motivation: 训练隐私保护的视觉Transformer模型，同时减少可训练参数。

Method: 在ViT架构每层注入可训练的秩分解矩阵，且不冻结补丁嵌入层。

Result: 能减少可训练参数，且保持与全时调优几乎相同的准确率。

Conclusion: 该低秩适配方法有效，可用于训练隐私保护的ViT模型。

Abstract: We propose a low-rank adaptation method for training privacy-preserving
vision transformer (ViT) models that efficiently freezes pre-trained ViT model
weights. In the proposed method, trainable rank decomposition matrices are
injected into each layer of the ViT architecture, and moreover, the patch
embedding layer is not frozen, unlike in the case of the conventional low-rank
adaptation methods. The proposed method allows us not only to reduce the number
of trainable parameters but to also maintain almost the same accuracy as that
of full-time tuning.

</details>


### [230] [A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy](https://arxiv.org/abs/2507.12098)
*Xiang Li,Yifan Lin,Yuanzhe Zhang*

Main category: cs.CR

TL;DR: 本文提出结合联邦学习和差分隐私的框架解决个性化广告问题，实验显示可实现推荐准确性和系统效率的双重优化并确保隐私。


<details>
  <summary>Details</summary>
Motivation: 缓解个性化广告中的隐私泄露和性能问题。

Method: 提出集成联邦学习和差分隐私的框架，结合分布式特征提取、动态隐私预算分配和鲁棒模型聚合，采用多方安全计算和异常检测机制。

Result: 框架实现了推荐准确性和系统效率的双重优化，同时确保了隐私。

Conclusion: 为隐私保护技术在广告推荐中的应用提供了实际解决方案和理论基础。

Abstract: To mitigate privacy leakage and performance issues in personalized
advertising, this paper proposes a framework that integrates federated learning
and differential privacy. The system combines distributed feature extraction,
dynamic privacy budget allocation, and robust model aggregation to balance
model accuracy, communication overhead, and privacy protection. Multi-party
secure computing and anomaly detection mechanisms further enhance system
resilience against malicious attacks. Experimental results demonstrate that the
framework achieves dual optimization of recommendation accuracy and system
efficiency while ensuring privacy, providing both a practical solution and a
theoretical foundation for applying privacy protection technologies in
advertisement recommendation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [231] [JSQA: Speech Quality Assessment with Perceptually-Inspired Contrastive Pretraining Based on JND Audio Pairs](https://arxiv.org/abs/2507.11636)
*Junyi Fan,Donald Williamson*

Main category: eess.AS

TL;DR: 提出JSQA两阶段框架用于语音质量评估，实验表明感知启发的对比预训练能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音质量评估方法未充分结合感知因素，学习从高维输入到MOS的映射存在挑战。

Method: 使用感知引导的对比学习对JND对预训练音频编码器，再针对MOS预测进行微调。JND对由LibriSpeech和CHiME - 3生成，微调用NISQA数据集。

Result: 感知启发的对比预训练模型在多种指标上优于未预训练的模型。

Conclusion: 在预训练中纳入感知因素对语音质量评估性能提升有很大贡献。

Abstract: Speech quality assessment (SQA) is often used to learn a mapping from a
high-dimensional input space to a scalar that represents the mean opinion score
(MOS) of the perceptual speech quality. Learning such a mapping is challenging
for many reasons, but largely because MOS exhibits high levels of inherent
variance due to perceptual and experimental-design differences. Many solutions
have been proposed, but many approaches do not properly incorporate perceptual
factors into their learning algorithms (beyond the MOS label), which could lead
to unsatisfactory results. To this end, we propose JSQA, a two-stage framework
that pretrains an audio encoder using perceptually-guided contrastive learning
on just noticeable difference (JND) pairs, followed by fine-tuning for MOS
prediction. We first generate pairs of audio data within JND levels, which are
then used to pretrain an encoder to leverage perceptual quality similarity
information and map it into an embedding space. The JND pairs come from clean
LibriSpeech utterances that are mixed with background noise from CHiME-3, at
different signal-to-noise ratios (SNRs). The encoder is later fine-tuned with
audio samples from the NISQA dataset for MOS prediction. Experimental results
suggest that perceptually-inspired contrastive pretraining significantly
improves the model performance evaluated by various metrics when compared
against the same network trained from scratch without pretraining. These
findings suggest that incorporating perceptual factors into pretraining greatly
contributes to the improvement in performance for SQA.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [232] [Counting Answer Sets of Disjunctive Answer Set Programs](https://arxiv.org/abs/2507.11655)
*Mohimenul Kabir,Supratik Chakraborty,Kuldeep S Meel*

Main category: cs.LO

TL;DR: 本文提出SharpASP - SR框架用于计算析取逻辑程序的答案集数量，实验显示其在大答案集计数实例上表现出色，并开发混合计数方法实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 计数答案集在多个领域有应用，正常逻辑程序已有进展，但析取逻辑程序的实用计数器开发仍具挑战。

Method: 提出基于减法归约到投影命题模型计数的SharpASP - SR框架，引入答案集的替代特征描述，结合枚举技术开发混合计数方法。

Result: SharpASP - SR在大答案集计数实例上显著优于现有计数器，混合计数方法在全范围析取程序上达到最先进性能。

Conclusion: SharpASP - SR框架及混合计数方法在析取逻辑程序答案集计数上有良好效果。

Abstract: Answer Set Programming (ASP) provides a powerful declarative paradigm for
knowledge representation and reasoning. Recently, counting answer sets has
emerged as an important computational problem with applications in
probabilistic reasoning, network reliability analysis, and other domains. This
has motivated significant research into designing efficient ASP counters. While
substantial progress has been made for normal logic programs, the development
of practical counters for disjunctive logic programs remains challenging.
  We present SharpASP-SR, a novel framework for counting answer sets of
disjunctive logic programs based on subtractive reduction to projected
propositional model counting. Our approach introduces an alternative
characterization of answer sets that enables efficient reduction while ensuring
that intermediate representations remain of polynomial size. This allows
SharpASP-SR to leverage recent advances in projected model counting technology.
Through extensive experimental evaluation on diverse benchmarks, we demonstrate
that SharpASP-SR significantly outperforms existing counters on instances with
large answer set counts. Building on these results, we develop a hybrid
counting approach that combines enumeration techniques with SharpASP-SR to
achieve state-of-the-art performance across the full spectrum of disjunctive
programs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [233] [Neural Polar Decoders for Deletion Channels](https://arxiv.org/abs/2507.12329)
*Ziv Aharoni,Henry D. Pfister*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a neural polar decoder (NPD) for deletion channels with
a constant deletion rate. Existing polar decoders for deletion channels exhibit
high computational complexity of $O(N^4)$, where $N$ is the block length. This
limits the application of polar codes for deletion channels to
short-to-moderate block lengths. In this work, we demonstrate that employing
NPDs for deletion channels can reduce the computational complexity. First, we
extend the architecture of the NPD to support deletion channels. Specifically,
the NPD architecture consists of four neural networks (NNs), each replicating
fundamental successive cancellation (SC) decoder operations. To support
deletion channels, we change the architecture of only one. The computational
complexity of the NPD is $O(AN\log N)$, where the parameter $A$ represents a
computational budget determined by the user and is independent of the channel.
We evaluate the new extended NPD for deletion channels with deletion rates
$\delta\in\{0.01, 0.1\}$ and we verify the NPD with the ground truth given by
the trellis decoder by Tal et al. We further show that due to the reduced
complexity of the NPD, we are able to incorporate list decoding and further
improve performance. We believe that the extended NPD presented here could have
applications in future technologies like DNA storage.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [234] [Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data](https://arxiv.org/abs/2507.12425)
*Chandana Cheerla*

Main category: cs.CL

TL;DR: 论文提出先进RAG框架处理企业数据，实验显示在多项指标上有显著提升，未来将扩展到多模态数据等。


<details>
  <summary>Details</summary>
Motivation: 组织依赖企业数据决策，LLMs有局限，传统RAG框架处理结构化和半结构化数据有困难，因此需要新的解决方案。

Method: 提出结合密集嵌入和BM25的混合检索策略，用SpaCy NER进行元数据过滤和交叉编码器重排序，应用语义分块，保留表格结构，采用量化索引，结合人工反馈和对话记忆。

Result: 在企业数据集实验中，Precision@5提高15%，Recall@5提高13%，Mean Reciprocal Rank提高16%；定性评估中，忠实性、完整性和相关性得分更高。

Conclusion: 该框架能为企业任务提供准确、全面和上下文相关的响应，未来将扩展到多模态数据和集成基于代理的检索，代码将开源。

Abstract: Organizations increasingly rely on proprietary enterprise data, including HR
records, structured reports, and tabular documents, for critical
decision-making. While Large Language Models (LLMs) have strong generative
capabilities, they are limited by static pretraining, short context windows,
and challenges in processing heterogeneous data formats. Conventional
Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but
often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval
strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by
metadata-aware filtering with SpaCy NER and cross-encoder reranking. The
framework applies semantic chunking to maintain textual coherence and retains
tabular data structures to preserve row-column integrity. Quantized indexing
optimizes retrieval efficiency, while human-in-the-loop feedback and
conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5
increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),
and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative
evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness
(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.
These results demonstrate the framework's effectiveness in delivering accurate,
comprehensive, and contextually relevant responses for enterprise tasks. Future
work includes extending to multimodal data and integrating agent-based
retrieval. The source code will be released at
https://github.com/CheerlaChandana/Enterprise-Chatbot

</details>


### [235] [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)
*Matteo Fasulo,Luca Babboni,Luca Tedeschini*

Main category: cs.CL

TL;DR: 本文介绍AI Wizards团队参加CLEF 2025 CheckThat! Lab任务1，提出结合情感分数增强基于Transformer的分类器，实验表明该框架提升性能并获高排名。


<details>
  <summary>Details</summary>
Motivation: 参加CLEF 2025 CheckThat! Lab任务1，对新闻文章进行主观性检测，在多种语言设置下提高分类性能。

Method: 通过将辅助模型得到的情感分数与句子表征结合，增强基于Transformer的分类器，采用决策阈值校准处理类别不平衡问题，使用mDeBERTaV3 - base、ModernBERT - base和Llama3.2 - 1B进行实验。

Result: 情感特征集成显著提升性能，尤其是主观F1分数，该框架获高排名，如希腊语排名第1（Macro F1 = 0.51）。

Conclusion: 结合情感分数增强基于Transformer的分类器的框架在新闻文章主观性检测中有效，能提升性能并取得良好排名。

Abstract: This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab
Task 1: Subjectivity Detection in News Articles, classifying sentences as
subjective/objective in monolingual, multilingual, and zero-shot settings.
Training/development datasets were provided for Arabic, German, English,
Italian, and Bulgarian; final evaluation included additional unseen languages
(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our
primary strategy enhanced transformer-based classifiers by integrating
sentiment scores, derived from an auxiliary model, with sentence
representations, aiming to improve upon standard fine-tuning. We explored this
sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base
(English), and Llama3.2-1B. To address class imbalance, prevalent across
languages, we employed decision threshold calibration optimized on the
development set. Our experiments show sentiment feature integration
significantly boosts performance, especially subjective F1 score. This
framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

</details>


### [236] [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)
*Amos You*

Main category: cs.CL

TL;DR: 提出并行GPU实现的BlockBPE分词器，优化高吞吐量批量推理，减少复杂度，提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有分词器CPU受限，在GPU批量推理工作流中表现不佳。

Method: 提出BlockBPE，消除Regex预分词，实现线程块内高度并行的标记合并。

Result: 在高批量推理工作负载上，BlockBPE吞吐量比tiktoken高2倍，比HuggingFace Tokenizers高2.5倍。

Conclusion: BlockBPE能在接近线性时间复杂度下优化高吞吐量批量推理。

Abstract: Tokenization is a critical preprocessing step in large language model
pipelines, yet widely-used implementations remain CPU-bound and suboptimal for
batch inference workflows on GPU. We present BlockBPE, a parallel GPU
implementation of byte-pair encoding (BPE) that achieves near linear-time
complexity under realistic assumptions and is optimized for high-throughput,
batch inference. Unlike existing Rust-based tokenizers such as HuggingFace
Tokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex
pre-tokenization and exhibit $O(n \log n)$ runtime-BlockBPE eliminates the
Regex pre-tokenization which leads to small loss in generation quality, but
enables highly parallelized token merges within thread blocks, reducing overall
complexity to $O(nd)$ where $d \ll n$. On high-batch inference workloads,
BlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over
HuggingFace Tokenizers.

</details>


### [237] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

TL;DR: 本文引入MapIQ基准数据集评估多模态大语言模型在地图视觉问答上的表现，并研究地图设计变化对模型的影响。


<details>
  <summary>Details</summary>
Motivation: 现有Map - VQA研究主要聚焦于专题地图，覆盖主题类别和视觉分析任务有限，需弥补此空白。

Method: 引入包含14,706个问答对的MapIQ数据集，涵盖三种地图类型和六个主题；用六种视觉分析任务评估多个MLLMs，并与人类基线对比；进行地图设计变化实验。

Result: 可得到多模态大语言模型的表现情况，以及其鲁棒性、敏感性、对内部地理知识的依赖等信息。

Conclusion: 研究为提高Map - VQA性能提供了潜在途径。

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [238] [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)
*Farideh Majidi,Ziaeddin Beheshtifard*

Main category: cs.CL

TL;DR: 研究使用少样本和增量学习方法在波斯语中进行跨语言情感分析，用多语言预训练模型微调，mDeBERTa和XLM - RoBERTa表现佳，准确率达96%。


<details>
  <summary>Details</summary>
Motivation: 开发一个能用有限波斯语数据进行情感分析，并从高资源语言获取先验知识的模型。

Method: 采用三个预训练多语言模型（XLM - RoBERTa、mDeBERTa和DistilBERT），用少样本和增量学习方法在来自不同来源的少量波斯语数据上微调。

Result: mDeBERTa和XLM - RoBERTa表现出色，在波斯语情感分析中准确率达96%。

Conclusion: 结合少样本学习、增量学习与多语言预训练模型是有效的。

Abstract: This research examines cross-lingual sentiment analysis using few-shot
learning and incremental learning methods in Persian. The main objective is to
develop a model capable of performing sentiment analysis in Persian using
limited data, while getting prior knowledge from high-resource languages. To
achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and
DistilBERT) were employed, which were fine-tuned using few-shot and incremental
learning approaches on small samples of Persian data from diverse sources,
including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled
the models to learn from a broad range of contexts. Experimental results show
that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%
accuracy on Persian sentiment analysis. These findings highlight the
effectiveness of combining few-shot learning and incremental learning with
multilingual pre-trained models.

</details>


### [239] [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)
*Guimin Hu,Yi Xin,Lijie Hu,Zhihong Zhu,Hasti Seifi*

Main category: cs.CL

TL;DR: 提出分区器引导的模态学习框架PgM，有多种优势，实验证明其有效性和可迁移性，并可视化特征分布。


<details>
  <summary>Details</summary>
Motivation: 为充分利用多模态学习中不同类型的模态信息，提升多模态学习效果。

Method: 构建包含模态分区器、单模态学习器、配对模态学习器和单 - 配对模态解码器的PgM框架。

Result: PgM在四个多模态任务中有效，可迁移到现有模型，还可视化了特征分布。

Conclusion: PgM框架能有效学习单模态和配对模态特征，可灵活调整分布，不同模态和分区有不同学习率，具有良好效果和可迁移性。

Abstract: Multimodal learning benefits from multiple modal information, and each
learned modal representations can be divided into uni-modal that can be learned
from uni-modal training and paired-modal features that can be learned from
cross-modal interaction. Building on this perspective, we propose a
partitioner-guided modal learning framework, PgM, which consists of the modal
partitioner, uni-modal learner, paired-modal learner, and uni-paired modal
decoder. Modal partitioner segments the learned modal representation into
uni-modal and paired-modal features. Modal learner incorporates two dedicated
components for uni-modal and paired-modal learning. Uni-paired modal decoder
reconstructs modal representation based on uni-modal and paired-modal features.
PgM offers three key benefits: 1) thorough learning of uni-modal and
paired-modal features, 2) flexible distribution adjustment for uni-modal and
paired-modal representations to suit diverse downstream tasks, and 3) different
learning rates across modalities and partitions. Extensive experiments
demonstrate the effectiveness of PgM across four multimodal tasks and further
highlight its transferability to existing models. Additionally, we visualize
the distribution of uni-modal and paired-modal features across modalities and
tasks, offering insights into their respective contributions.

</details>


### [240] [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Pedro Alonso Doval,Jorge Alcalde Vesteiro,Héctor Cerezo-Costas*

Main category: cs.CL

TL;DR: 提出ExpliCIT - QA系统，将MRT方法扩展为多模态管道处理表格图像问答，有模块化设计，评估显示其在可解释性和透明度上有改进。


<details>
  <summary>Details</summary>
Motivation: 缩小端到端TableVQA系统的可解释性差距，满足敏感领域审计结果的需求。

Method: 采用模块化设计，包含多模态表格理解、基于语言的推理、自动代码生成、代码执行和自然语言解释等步骤。

Result: 在TableVQA - Bench基准上评估，相比现有基线，在可解释性和透明度上有改进。

Conclusion: 该系统可应用于金融和医疗等敏感领域。

Abstract: We present ExpliCIT-QA, a system that extends our previous MRT approach for
tabular question answering into a multimodal pipeline capable of handling
complex table images and providing explainable answers. ExpliCIT-QA follows a
modular design, consisting of: (1) Multimodal Table Understanding, which uses a
Chain-of-Thought approach to extract and transform content from table images;
(2) Language-based Reasoning, where a step-by-step explanation in natural
language is generated to solve the problem; (3) Automatic Code Generation,
where Python/Pandas scripts are created based on the reasoning steps, with
feedback for handling errors; (4) Code Execution to compute the final answer;
and (5) Natural Language Explanation that describes how the answer was
computed. The system is built for transparency and auditability: all
intermediate outputs, parsed tables, reasoning steps, generated code, and final
answers are available for inspection. This strategy works towards closing the
explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on
the TableVQA-Bench benchmark, comparing it with existing baselines. We
demonstrated improvements in interpretability and transparency, which open the
door for applications in sensitive domains like finance and healthcare where
auditing results are critical.

</details>


### [241] [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
*Meng Li,Timothy M. McPhillips,Dingmin Wang,Shin-Rong Tsai,Bertram Ludäscher*

Main category: cs.CL

TL;DR: 提出CRABS策略理解Python笔记本，用Kaggle数据集评估，效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型理解Python笔记本存在幻觉和长上下文挑战，需更好方法评估、复用和适配笔记本。

Method: 提出笔记本理解任务，采用CRABS策略，结合语法分析和大语言模型，通过浅语法解析和抽象语法树分析捕捉信息，用大语言模型消除歧义。

Result: 大语言模型解决了98%的歧义，CRABS识别信息流和执行依赖的F1分数分别达98%和99%。

Conclusion: CRABS策略在理解Python笔记本的信息流动和执行依赖方面有效。

Abstract: Recognizing the information flows and operations comprising data science and
machine learning Python notebooks is critical for evaluating, reusing, and
adapting notebooks for new tasks. Investigating a notebook via re-execution
often is impractical due to the challenges of resolving data and software
dependencies. While Large Language Models (LLMs) pre-trained on large codebases
have demonstrated effectiveness in understanding code without running it, we
observe that they fail to understand some realistic notebooks due to
hallucinations and long-context challenges. To address these issues, we propose
a notebook understanding task yielding an information flow graph and
corresponding cell execution dependency graph for a notebook, and demonstrate
the effectiveness of a pincer strategy that uses limited syntactic analysis to
assist full comprehension of the notebook using an LLM. Our Capture and Resolve
Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and
analysis of the abstract syntax tree (AST) to capture the correct
interpretation of a notebook between lower and upper estimates of the
inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via
cell-by-cell zero-shot learning, thereby identifying the true data inputs and
outputs of each cell. We evaluate and demonstrate the effectiveness of our
approach using an annotated dataset of 50 representative, highly up-voted
Kaggle notebooks that together represent 3454 actual cell inputs and outputs.
The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the
syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves
average F1 scores of 98% identifying cell-to-cell information flows and 99%
identifying transitive cell execution dependencies.

</details>


### [242] [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
*Dante Campregher,Yanxu Chen,Sander Hoffman,Maria Heuss*

Main category: cs.CL

TL;DR: 本文对大语言模型处理事实与反事实信息进行可重复性研究，聚焦注意力头作用，发现注意力头通过通用复制抑制促进事实输出，且其行为具有领域依赖性。


<details>
  <summary>Details</summary>
Motivation: 对近期三项研究的发现进行重现和调和，研究大语言模型如何处理竞争的事实和反事实信息。

Method: 使用机械可解释性工具，研究注意力头强度与事实输出比率的关系，评估注意力头抑制机制的竞争假设，研究注意力模式的领域特异性。

Result: 注意力头通过通用复制抑制而非选择性反事实抑制促进事实输出，强化它们也会抑制正确事实；注意力头行为具有领域依赖性，更大模型表现出更专业和类别敏感的模式。

Conclusion: 明确了注意力头在大语言模型处理信息时的作用机制和领域特性。

Abstract: This paper presents a reproducibility study examining how Large Language
Models (LLMs) manage competing factual and counterfactual information, focusing
on the role of attention heads in this process. We attempt to reproduce and
reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and
Pavlick and McDougall et al. that investigate the competition between
model-learned facts and contradictory context information through Mechanistic
Interpretability tools. Our study specifically examines the relationship
between attention head strength and factual output ratios, evaluates competing
hypotheses about attention heads' suppression mechanisms, and investigates the
domain specificity of these attention patterns. Our findings suggest that
attention heads promoting factual output do so via general copy suppression
rather than selective counterfactual suppression, as strengthening them can
also inhibit correct facts. Additionally, we show that attention head behavior
is domain-dependent, with larger models exhibiting more specialized and
category-sensitive patterns.

</details>


### [243] [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
*Jianzhe Ma,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文对深度学习在几何问题求解中的应用进行了综述，涵盖任务总结、方法回顾、评估指标分析及挑战与方向探讨，并在GitHub提供论文列表。


<details>
  <summary>Details</summary>
Motivation: 几何问题求解在多领域重要，深度学习尤其是多模态大语言模型发展引发研究热潮，旨在为该领域提供全面实用参考以推动发展。

Method: 对几何问题求解相关任务进行全面总结，回顾相关深度学习方法，分析评估指标和方法，讨论当前挑战和未来方向。

Result: 完成了对深度学习在几何问题求解应用的调查，创建了持续更新的论文列表。

Conclusion: 为深度学习用于几何问题求解提供了全面实用参考，利于该领域进一步发展。

Abstract: Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.

</details>


### [244] [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)
*Yichen Xu,Liangyu Chen,Liang Zhang,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文提出首个大规模多语言图表问答基准PolyChartQA，可促进多语言图表理解评估，实验揭示不同语言模型表现差异。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准以英语为主，限制了全球受众的可访问性和适用性。

Method: 采用解耦管道，分离图表数据和渲染代码，利用先进的基于大语言模型的翻译并严格控制质量。

Result: 在开源和闭源大视觉语言模型上的实验表明，英语和其他语言（尤其是低资源非拉丁文字语言）之间存在显著性能差距。

Conclusion: 该基准为推进全球包容性视觉语言模型奠定基础。

Abstract: Charts are a universally adopted medium for interpreting and communicating
data. However, existing chart understanding benchmarks are predominantly
English-centric, limiting their accessibility and applicability to global
audiences. In this paper, we present PolyChartQA, the first large-scale
multilingual chart question answering benchmark covering 22,606 charts and
26,151 question-answering pairs across 10 diverse languages. PolyChartQA is
built using a decoupled pipeline that separates chart data from rendering code,
allowing multilingual charts to be flexibly generated by simply translating the
data and reusing the code. We leverage state-of-the-art LLM-based translation
and enforce rigorous quality control in the pipeline to ensure the linguistic
and semantic consistency of the generated multilingual charts. PolyChartQA
facilitates systematic evaluation of multilingual chart understanding.
Experiments on both open- and closed-source large vision-language models reveal
a significant performance gap between English and other languages, especially
low-resource ones with non-Latin scripts. This benchmark lays a foundation for
advancing globally inclusive vision-language models.

</details>


### [245] [PoTPTQ: A Two-step Power-of-Two Post-training for LLMs](https://arxiv.org/abs/2507.11959)
*Xinyu Wang,Vahid Partovi Nia,Peng Lu,Jerry Huang,Xiao-Wen Chang,Boxing Chen,Yufei Cui*

Main category: cs.CL

TL;DR: 提出用于大语言模型权重的新型POT量化框架，通过两步后训练算法保持模型精度，在低精度下表现优且加速反量化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署因计算资源需求大而困难，现有PoT量化在GPU上效果不佳。

Method: 提出新型POT量化框架，引入两步后训练算法：用稳健起点初始化量化尺度，用最小校准集细化尺度。

Result: PoT后训练算法在低精度下超越现有整数量化技术，PoT量化加速浮点推理的反量化步骤，在NVIDIA V100和RTX 4090上有速度提升。

Conclusion: 新型POT量化框架有效，在低精度下有高精度和更快推理速度。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing (NLP) tasks. However, their deployment is
challenging due to the substantial computational resources required.
Power-of-two (PoT) quantization is a general tool to counteract this
difficulty. Albeit previous works on PoT quantization can be efficiently
dequantized on CPUs using fixed-point addition, it showed less effectiveness on
GPUs. The reason is entanglement of the sign bit and sequential bit
manipulations needed for dequantization. We propose a novel POT quantization
framework for LLM weights that (i) outperforms state-of-the-art accuracy in
extremely low-precision number formats, and (ii) enables faster inference
through more efficient dequantization. To maintain the accuracy of the
quantized model, we introduce a two-step post-training algorithm: (i)
initialize the quantization scales with a robust starting point, and (ii)
refine these scales using a minimal calibration set. The performance of our PoT
post-training algorithm surpasses the current state-of-the-art in integer
quantization, particularly at low precisions such as 2- and 3-bit formats. Our
PoT quantization accelerates the dequantization step required for the floating
point inference and leads to $3.67\times$ speed up on a NVIDIA V100, and
$1.63\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.

</details>


### [246] [Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](https://arxiv.org/abs/2507.11966)
*Ziyu Ge,Gabriel Chua,Leanne Tan,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 提出毒性保留翻译的两阶段框架，在新加坡英语安全语料库验证，证明有效且对多元文化大语言模型安全有贡献。


<details>
  <summary>Details</summary>
Motivation: 标准翻译系统难以处理小众语言和方言中的毒性内容翻译，低资源语言对翻译有额外挑战。

Method: 分两阶段，先进行人工验证的少样本提示工程，再通过基准测试优化模型 - 提示对。

Result: 定量人工评估证实了框架的有效性和效率。

Conclusion: 框架不仅提升翻译质量，还助力多元文化大语言模型安全，强调保留社会语言细微差别的重要性。

Abstract: As online communication increasingly incorporates under-represented languages
and colloquial dialects, standard translation systems often fail to preserve
local slang, code-mixing, and culturally embedded markers of harmful speech.
Translating toxic content between low-resource language pairs poses additional
challenges due to scarce parallel data and safety filters that sanitize
offensive expressions. In this work, we propose a reproducible, two-stage
framework for toxicity-preserving translation, demonstrated on a code-mixed
Singlish safety corpus. First, we perform human-verified few-shot prompt
engineering: we iteratively curate and rank annotator-selected Singlish-target
examples to capture nuanced slang, tone, and toxicity. Second, we optimize
model-prompt pairs by benchmarking several large language models using semantic
similarity via direct and back-translation. Quantitative human evaluation
confirms the effectiveness and efficiency of our pipeline. Beyond improving
translation quality, our framework contributes to the safety of multicultural
LLMs by supporting culturally sensitive moderation and benchmarking in
low-resource contexts. By positioning Singlish as a testbed for inclusive NLP,
we underscore the importance of preserving sociolinguistic nuance in real-world
applications such as content moderation and regional platform governance.

</details>


### [247] [StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features](https://arxiv.org/abs/2507.12064)
*Jeremi K. Ochab,Mateusz Matias,Tymoteusz Boba,Tomasz Walkowiak*

Main category: cs.CL

TL;DR: 本文基于模块化文体分析流程参与二元AI检测任务，用spaCy模型预处理文本、提取特征，用轻梯度提升机分类，收集大量语料训练，探索参数选项提升分类器性能。


<details>
  <summary>Details</summary>
Motivation: 参与二元AI检测任务，寻找有效的检测方法。

Method: 采用模块化文体分析流程，用spaCy模型进行文本预处理和特征提取，用轻梯度提升机作为分类器，收集超500000条机器生成文本训练分类器，探索参数选项。

Result: 未提及明确结果。

Conclusion: 采用非神经、计算成本低且可解释的方法，该方法此前被证明有效。

Abstract: This submission to the binary AI detection task is based on a modular
stylometric pipeline, where: public spaCy models are used for text
preprocessing (including tokenisation, named entity recognition, dependency
parsing, part-of-speech tagging, and morphology annotation) and extracting
several thousand features (frequencies of n-grams of the above linguistic
annotations); light-gradient boosting machines are used as the classifier. We
collect a large corpus of more than 500 000 machine-generated texts for the
classifier's training. We explore several parameter options to increase the
classifier's capacity and take advantage of that training set. Our approach
follows the non-neural, computationally inexpensive but explainable approach
found effective previously.

</details>


### [248] [BOOKCOREF: Coreference Resolution at Book Scale](https://arxiv.org/abs/2507.12075)
*Giuliano Martinelli,Tommaso Bonomo,Pere-Lluís Huguet Cabot,Roberto Navigli*

Main category: cs.CL

TL;DR: 提出自动标注流程创建首个书籍规模共指消解基准BOOKCOREF，实验证明其价值，揭示新挑战并开源数据代码。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估长文本共指消解系统时，无法在书籍规模上充分评估系统能力。

Method: 提出自动标注流程，并采用该流程创建BOOKCOREF基准。

Result: 实验表明自动流程具有鲁棒性，BOOKCOREF能让长文档共指系统在全书籍评估时CoNLL - F1得分提升20分。

Conclusion: 当前模型在书籍规模上表现不如小文档，开源数据代码以推动书籍规模共指消解系统的研发。

Abstract: Coreference Resolution systems are typically evaluated on benchmarks
containing small- to medium-scale documents. When it comes to evaluating long
texts, however, existing benchmarks, such as LitBank, remain limited in length
and do not adequately assess system capabilities at the book scale, i.e., when
co-referring mentions span hundreds of thousands of tokens. To fill this gap,
we first put forward a novel automatic pipeline that produces high-quality
Coreference Resolution annotations on full narrative texts. Then, we adopt this
pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with
an average document length of more than 200,000 tokens. We carry out a series
of experiments showing the robustness of our automatic procedure and
demonstrating the value of our resource, which enables current long-document
coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full
books. Moreover, we report on the new challenges introduced by this
unprecedented book-scale setting, highlighting that current models fail to
deliver the same performance they achieve on smaller documents. We release our
data and code to encourage research and development of new book-scale
Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.

</details>


### [249] [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
*Mohammad Samragh,Arnav Kundu,David Harrison,Kumari Nishu,Devang Naik,Minsik Cho,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 提出新框架利用自回归语言模型知识实现多后续标记同时预测，有多项创新，在预训练模型微调后显著提速且无质量损失。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型顺序生成的特性限制推理速度和并行性，尤其在生成后期。

Method: 提出新框架，包括掩码输入、门控LoRA、可学习采样器、辅助训练损失和投机生成策略，在预训练模型上进行监督微调。

Result: 生成代码和数学内容快近5倍，通用聊天和知识任务提速近2.5倍，且无质量损失。

Conclusion: 该方法能有效提升自回归语言模型的生成速度，且不影响质量。

Abstract: Autoregressive language models are constrained by their inherently sequential
nature, generating one token at a time. This paradigm limits inference speed
and parallelism, especially during later stages of generation when the
direction and semantics of text are relatively certain. In this work, we
propose a novel framework that leverages the inherent knowledge of vanilla
autoregressive language models about future tokens, combining techniques to
realize this potential and enable simultaneous prediction of multiple
subsequent tokens. Our approach introduces several key innovations: (1) a
masked-input formulation where multiple future tokens are jointly predicted
from a common prefix; (2) a gated LoRA formulation that preserves the original
LLM's functionality, while equipping it for multi-token prediction; (3) a
lightweight, learnable sampler module that generates coherent sequences from
the predicted future tokens; (4) a set of auxiliary training losses, including
a consistency loss, to enhance the coherence and accuracy of jointly generated
tokens; and (5) a speculative generation strategy that expands tokens
quadratically in the future while maintaining high fidelity. Our method
achieves significant speedups through supervised fine-tuning on pretrained
models. For example, it generates code and math nearly 5x faster, and improves
general chat and knowledge tasks by almost 2.5x. These gains come without any
loss in quality.

</details>


### [250] [IAM: Efficient Inference through Attention Mapping between Different-scale LLMs](https://arxiv.org/abs/2507.11953)
*Yi Zhao,Zuchao Li,Hai Zhao*

Main category: cs.CL

TL;DR: 文章指出大语言模型资源消耗问题，基于注意力矩阵相似性提出 IAM 框架，实验显示可加速推理并减少 KV 缓存使用，且具有通用性和正交性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型资源消耗大，现有推理效率提升方法未利用外部信息优化，文章旨在基于注意力矩阵相似性找到新优化视角。

Method: 先全面分析测量相似度、选择映射层及映射一致性等问题，再引入 IAM 框架，在大小模型间进行注意力映射。

Result: IAM 能加速 prefill 15%，减少 KV 缓存使用 22.1%，不同系列模型实验显示其具有通用性，且与现有 KV 缓存优化方法正交。

Conclusion: IAM 框架是提升大语言模型效率的有效工具，具有通用性和与其他方法的正交性，可作为现有工具包的补充。

Abstract: LLMs encounter significant challenges in resource consumption nowadays,
especially with long contexts. Despite extensive efforts dedicate to enhancing
inference efficiency, these methods primarily exploit internal sparsity within
the models, without leveraging external information for optimization. We
identify the high similarity of attention matrices across different-scale LLMs,
which offers a novel perspective for optimization. We first conduct a
comprehensive analysis of how to measure similarity, how to select mapping
Layers and whether mapping is consistency. Based on these insights, we
introduce the IAM framework, which achieves dual benefits of accelerated
attention computation and reduced KV cache usage by performing attention
mapping between small and large LLMs. Our experimental results demonstrate that
IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without
appreciably sacrificing performance. Experiments on different series of models
show the generalizability of IAM. Importantly, it is also orthogonal to many
existing KV cache optimization methods, making it a versatile addition to the
current toolkit for enhancing LLM efficiency.

</details>


### [251] [The benefits of query-based KGQA systems for complex and temporal questions in LLM era](https://arxiv.org/abs/2507.11954)
*Artem Alekseev,Mikhail Chaichuk,Miron Butko,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 探索基于查询的多阶段框架用于WikiData问答，提出增强多跳和时间问答性能的方法，评估鲁棒性并引入新实体链接和谓词匹配方法，结果显示对小语言模型有提升潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多跳推理和时间问题问答上存在困难，基于查询的知识图谱问答提供了模块化替代方案。

Method: 提出多阶段方法，通过泛化和拒绝研究评估鲁棒性，引入基于思维链推理的实体链接和谓词匹配方法。

Result: 证明了基于查询的多阶段知识图谱问答框架对小语言模型在多跳和时间问答上有提升潜力。

Conclusion: 基于查询的多阶段KGQA框架可用于改进小语言模型的多跳和时间问答。

Abstract: Large language models excel in question-answering (QA) yet still struggle
with multi-hop reasoning and temporal questions. Query-based knowledge graph QA
(KGQA) offers a modular alternative by generating executable queries instead of
direct answers. We explore multi-stage query-based framework for WikiData QA,
proposing multi-stage approach that enhances performance on challenging
multi-hop and temporal benchmarks. Through generalization and rejection
studies, we evaluate robustness across multi-hop and temporal QA datasets.
Additionally, we introduce a novel entity linking and predicate matching method
using CoT reasoning. Our results demonstrate the potential of query-based
multi-stage KGQA framework for improving multi-hop and temporal QA with small
language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System

</details>


### [252] [Improving Contextual ASR via Multi-grained Fusion with Large Language Models](https://arxiv.org/abs/2507.12252)
*Shilin Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: 提出多粒度融合方法提升端到端ASR模型关键词识别性能，在中英文数据集实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 端到端ASR模型难以准确识别上下文相关关键词，以往基于关键词字典的方法有局限性。

Method: 提出结合大语言模型的多粒度融合方法，采用后期融合策略结合ASR声学信息和LLM上下文知识。

Result: 在中英文数据集上关键词相关指标达SOTA，非关键词文本也保持高精度，消融实验证明各组件有显著贡献。

Conclusion: 所提多粒度融合方法有效提升关键词识别性能，代码和模型将公开。

Abstract: While end-to-end Automatic Speech Recognition (ASR) models have shown
impressive performance in transcribing general speech, they often struggle to
accurately recognize contextually relevant keywords, such as proper nouns or
user-specific entities.
  Previous approaches have explored leveraging keyword dictionaries in the
textual modality to improve keyword recognition, either through token-level
fusion that guides token-by-token generation or phrase-level fusion that
enables direct copying of keyword phrases.
  However, these methods operate at different granularities and have their own
limitations.
  In this paper, we propose a novel multi-grained fusion approach that jointly
leverages the strengths of both token-level and phrase-level fusion with Large
Language Models (LLMs).
  Our approach incorporates a late-fusion strategy that elegantly combines
ASR's acoustic information with LLM's rich contextual knowledge, balancing
fine-grained token precision with holistic phrase-level understanding.
  Experiments on Chinese and English datasets demonstrate that our approach
achieves state-of-the-art performance on keyword-related metrics while
preserving high accuracy on non-keyword text.
  Ablation studies further confirm that the token-level and phrase-level
components both contribute significantly to the performance gains,
complementing each other in our joint multi-grained framework.
  The code and models will be publicly available at https://github.com/.

</details>


### [253] [Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes](https://arxiv.org/abs/2507.12261)
*Johann Frei,Nils Feldhus,Lisa Raithel,Roland Roller,Alexander Meyer,Frank Kramer*

Main category: cs.CL

TL;DR: 提出端到端框架Infherno解决临床笔记转FHIR资源问题，表现良好且支持数据集成与互操作性。


<details>
  <summary>Details</summary>
Motivation: 以往将自由形式临床笔记自动转换为结构化FHIR资源的方法存在泛化性有限和结构不一致问题，需解决。

Method: 提出由大语言模型代理、代码执行和医疗术语数据库工具驱动的端到端框架Infherno。

Result: Infherno符合FHIR文档模式，在从非结构化文本预测FHIR资源方面与人类基线表现相当。

Conclusion: Infherno支持临床数据集成过程和跨机构互操作性。

Abstract: For clinical data integration and healthcare services, the HL7 FHIR standard
has established itself as a desirable format for interoperability between
complex health data. Previous attempts at automating the translation from
free-form clinical notes into structured FHIR resources rely on modular,
rule-based systems or LLMs with instruction tuning and constrained decoding.
Since they frequently suffer from limited generalizability and structural
inconformity, we propose an end-to-end framework powered by LLM agents, code
execution, and healthcare terminology database tools to address these issues.
Our solution, called Infherno, is designed to adhere to the FHIR document
schema and competes well with a human baseline in predicting FHIR resources
from unstructured text. The implementation features a front end for custom and
synthetic data and both local and proprietary models, supporting clinical data
integration processes and interoperability across institutions.

</details>


### [254] [Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding](https://arxiv.org/abs/2507.12295)
*Feng Xiao,Jicong Fan*

Main category: cs.CL

TL;DR: 本文开展文本异常检测的实证研究，引入基准测试，评估多种模型嵌入的检测效果，发现嵌入质量影响检测效能，DL方法无优势，有低秩特性，还开源工具包。


<details>
  <summary>Details</summary>
Motivation: 现有文本异常检测方法缺乏标准化和全面的基准测试，限制方法比较和创新。

Method: 利用多种预训练语言模型嵌入，在多领域文本数据集上，采用综合评估指标系统评估基于嵌入的文本异常检测。

Result: 嵌入质量显著影响异常检测效能；使用大模型嵌入时，深度学习方法无性能优势；跨模型性能矩阵有低秩特性。

Conclusion: 研究为未来鲁棒可扩展的文本异常检测系统研究提供基础，开源工具包方便后续研究。

Abstract: Text anomaly detection is a critical task in natural language processing
(NLP), with applications spanning fraud detection, misinformation
identification, spam detection and content moderation, etc. Despite significant
advances in large language models (LLMs) and anomaly detection algorithms, the
absence of standardized and comprehensive benchmarks for evaluating the
existing anomaly detection methods on text data limits rigorous comparison and
development of innovative approaches. This work performs a comprehensive
empirical study and introduces a benchmark for text anomaly detection,
leveraging embeddings from diverse pre-trained language models across a wide
array of text datasets. Our work systematically evaluates the effectiveness of
embedding-based text anomaly detection by incorporating (1) early language
models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI
(small, ada, large)); (3) multi-domain text datasets (news, social media,
scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).
Our experiments reveal a critical empirical insight: embedding quality
significantly governs anomaly detection efficacy, and deep learning-based
approaches demonstrate no performance advantage over conventional shallow
algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived
embeddings.In addition, we observe strongly low-rank characteristics in
cross-model performance matrices, which enables an efficient strategy for rapid
model evaluation (or embedding evaluation) and selection in practical
applications. Furthermore, by open-sourcing our benchmark toolkit that includes
all embeddings from different models and code at
https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work
provides a foundation for future research in robust and scalable text anomaly
detection systems.

</details>


### [255] [Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization](https://arxiv.org/abs/2507.12308)
*Prashanth Vijayaraghavan,Apoorva Nitsure,Charles Mackin,Luyao Shi,Stefano Ambrogio,Arvind Haran,Viresh Paruthi,Ali Elzein,Dan Coops,David Beymer,Tyler Baldwin,Ehsan Degan*

Main category: cs.CL

TL;DR: 本文评估现有代码大模型在VHDL代码生成和总结任务的表现，发现表现不佳，提出CoDes方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大模型在硬件描述语言VHDL方面的评估和改进，需提升大模型在VHDL代码生成和总结任务的性能。

Method: 使用VHDL - Eval和VHDL - Xform两个数据集评估现有代码大模型，提出Chain - of - Descriptions (CoDes)方法，通过生成中间描述步骤并与原输入提示整合输入大模型。

Result: 现有模型在各指标下表现不佳，CoDes方法在两个数据集上各指标显著优于标准提示策略。

Conclusion: CoDes方法提升了VHDL代码生成和总结质量，可为未来提升VHDL代码大模型研究提供框架。

Abstract: Large Language Models (LLMs) have become widely used across diverse NLP tasks
and domains, demonstrating their adaptability and effectiveness. In the realm
of Electronic Design Automation (EDA), LLMs show promise for tasks like
Register-Transfer Level (RTL) code generation and summarization. However,
despite the proliferation of LLMs for general code-related tasks, there's a
dearth of research focused on evaluating and refining these models for hardware
description languages (HDLs), notably VHDL. In this study, we evaluate the
performance of existing code LLMs for VHDL code generation and summarization
using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,
an in-house dataset, aims to gauge LLMs' understanding of functionally
equivalent code. Our findings reveal consistent underperformance of these
models across different metrics, underscoring a significant gap in their
suitability for this domain. To address this challenge, we propose
Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of
LLMs for VHDL code generation and summarization tasks. CoDes involves
generating a series of intermediate descriptive steps based on: (i) the problem
statement for code generation, and (ii) the VHDL code for summarization. These
steps are then integrated with the original input prompt (problem statement or
code) and provided as input to the LLMs to generate the final output. Our
experiments demonstrate that the CoDes approach significantly surpasses the
standard prompting strategy across various metrics on both datasets. This
method not only improves the quality of VHDL code generation and summarization
but also serves as a framework for future research aimed at enhancing code LLMs
for VHDL.

</details>


### [256] [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](https://arxiv.org/abs/2507.12126)
*Payal Bhattad,Sai Manoj Pudukotai Dinakarrao,Anju Gupta*

Main category: cs.CL

TL;DR: 本文提出大语言模型文本增强评估框架，评估发现GPT - 3.5 Turbo表现最佳，应用于实际任务效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有文本数据增强技术在大规模或迭代生成时缺乏语义保存机制，导致冗余和不稳定，需要评估框架。

Method: 提出评估框架，包含可扩展性分析和迭代增强与摘要细化两部分。

Result: GPT - 3.5 Turbo在语义保真、多样性和生成效率上达到最佳平衡，应用于实际任务使主题粒度提升400%，消除主题重叠。

Conclusion: 所提框架对基于大语言模型的文本增强结构化评估有实用价值。

Abstract: Text data augmentation is a widely used strategy for mitigating data sparsity
in natural language processing (NLP), particularly in low-resource settings
where limited samples hinder effective semantic modeling. While augmentation
can improve input diversity and downstream interpretability, existing
techniques often lack mechanisms to ensure semantic preservation during
large-scale or iterative generation, leading to redundancy and instability.
This work introduces a principled evaluation framework for large language model
(LLM) based text augmentation, comprising two components: (1) Scalability
Analysis, which measures semantic consistency as augmentation volume increases,
and (2) Iterative Augmentation with Summarization Refinement (IASR), which
evaluates semantic drift across recursive paraphrasing cycles. Empirical
evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the
best balance of semantic fidelity, diversity, and generation efficiency.
Applied to a real-world topic modeling task using BERTopic with GPT-enhanced
few-shot labeling, the proposed approach results in a 400% increase in topic
granularity and complete elimination of topic overlaps. These findings
validated the utility of the proposed frameworks for structured evaluation of
LLM-based augmentation in practical NLP pipelines.

</details>


### [257] [Probing for Arithmetic Errors in Language Models](https://arxiv.org/abs/2507.12379)
*Yucheng Sun,Alessandro Stolfo,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 研究语言模型内部激活能否检测算术错误，发现可从隐藏状态解码输出与正确答案，训练的探测器准确率超90%，能推广到复杂场景并指导重提示纠错。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型内部激活是否可用于检测算术错误。

Method: 从3位数加法的受控设置开始，训练简单探测器；将分析扩展到GSM8K仅加法问题的结构化思维链轨迹；利用探测器指导选择性重提示。

Result: 简单探测器能准确解码输出和正确答案，预测模型正确性准确率超90%，简单算术训练的探测器能很好推广到复杂场景，可指导重提示提高任务准确率。

Conclusion: 仅从内部激活就能预判算术错误，简单探测器为轻量级模型自我纠错提供可行途径。

Abstract: We investigate whether internal activations in language models can be used to
detect arithmetic errors. Starting with a controlled setting of 3-digit
addition, we show that simple probes can accurately decode both the model's
predicted output and the correct answer from hidden states, regardless of
whether the model's output is correct. Building on this, we train lightweight
error detectors that predict model correctness with over 90% accuracy. We then
extend our analysis to structured chain-of-thought traces on addition-only
GSM8K problems and find that probes trained on simple arithmetic generalize
well to this more complex setting, revealing consistent internal
representations. Finally, we demonstrate that these probes can guide selective
re-prompting of erroneous reasoning steps, improving task accuracy with minimal
disruption to correct outputs. Our findings suggest that arithmetic errors can
be anticipated from internal activations alone, and that simple probes offer a
viable path toward lightweight model self-correction.

</details>


### [258] [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)
*Yik Siu Chan,Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CL

TL;DR: 研究用思维链预测最终回复的不一致性，发现基于思维链激活的线性探针预测效果好，可用于实时安全监控和早期干预。


<details>
  <summary>Details</summary>
Motivation: 开放权重推理语言模型生成思维链提高性能但带来额外对齐风险，研究能否用思维链预测最终回复的不一致性。

Method: 评估包括人类、大语言模型和文本分类器等在内的多种监控方法，使用思维链文本或激活。

Result: 基于思维链激活训练的简单线性探针在预测最终回复安全性上显著优于所有基于文本的方法，在推理完成前就能准确预测，且在早期思维链片段上也有良好表现。

Conclusion: 轻量级探针可实现生成过程中的实时安全监控和早期干预，且结果在不同模型规模、家族和安全基准测试中具有通用性。

Abstract: Open-weights reasoning language models generate long chains-of-thought (CoTs)
before producing a final response, which improves performance but introduces
additional alignment risks, with harmful content often appearing in both the
CoTs and the final outputs. In this work, we investigate if we can use CoTs to
predict final response misalignment. We evaluate a range of monitoring
approaches, including humans, highly-capable large language models, and text
classifiers, using either CoT text or activations. First, we find that a simple
linear probe trained on CoT activations can significantly outperform all
text-based methods in predicting whether a final response will be safe or
unsafe. CoT texts are often unfaithful and can mislead humans and classifiers,
while model latents (i.e., CoT activations) offer a more reliable predictive
signal. Second, the probe makes accurate predictions before reasoning
completes, achieving strong performance even when applied to early CoT
segments. These findings generalize across model sizes, families, and safety
benchmarks, suggesting that lightweight probes could enable real-time safety
monitoring and early intervention during generation.

</details>


### [259] [S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling](https://arxiv.org/abs/2507.12451)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 提出用于主题建模的S2WTM，缓解后验坍缩问题，实验显示其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: VAE - NTMs在建模高维文本数据潜在表示时存在后验坍缩问题，导致潜在表示无效。

Method: 提出S2WTM，采用单位超球面上的先验分布，利用球面切片Wasserstein距离使聚合后验分布与先验对齐。

Result: S2WTM在实验中表现优于现有主题模型，生成的主题更连贯、多样，下游任务性能提升。

Conclusion: S2WTM能在建模潜在空间超球结构时有效缓解后验坍缩问题，具有更好的性能。

Abstract: Modeling latent representations in a hyperspherical space has proven
effective for capturing directional similarities in high-dimensional text data,
benefiting topic modeling. Variational autoencoder-based neural topic models
(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical
structure. However, VAE-NTMs often suffer from posterior collapse, where the KL
divergence term in the objective function highly diminishes, leading to
ineffective latent representations. To mitigate this issue while modeling
hyperspherical structure in the latent space, we propose the Spherical Sliced
Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior
distribution supported on the unit hypersphere and leverages the Spherical
Sliced-Wasserstein distance to align the aggregated posterior distribution with
the prior. Experimental results demonstrate that S2WTM outperforms
state-of-the-art topic models, generating more coherent and diverse topics
while improving performance on downstream tasks.

</details>


### [260] [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/abs/2507.12466)
*David Mizrahi,Anders Boesen Lindbo Larsen,Jesse Allardice,Suzie Petryk,Yuri Gorokhov,Jeffrey Li,Alex Fang,Josh Gardner,Tom Gunter,Afshin Dehghan*

Main category: cs.CL

TL;DR: 提出基准目标排名（BETR）方法选预训练文档，对比多种数据选择方法，发现BETR有计算优势、泛化性好，且大模型需较宽松过滤。


<details>
  <summary>Details</summary>
Motivation: 探究将数据选择优化显式化会怎样。

Method: 提出BETR方法，通过将基准示例和预训练文档样本嵌入共享空间，根据与基准的相似度评分，训练轻量级分类器预测全量语料分数；训练超500个模型并拟合缩放定律对比数据选择方法。

Result: BETR比DCLM - Baseline有2.1倍计算乘数（未过滤数据4.7倍），10个任务中9个表现提升，泛化性好，大模型需较宽松过滤。

Conclusion: 预训练数据与目标任务精确匹配能塑造模型能力，最优选择策略要适应模型规模。

Abstract: Every data selection method inherently has a target. In practice, these
targets often emerge implicitly through benchmark-driven iteration: researchers
develop selection strategies, train models, measure benchmark performance, then
refine accordingly. This raises a natural question: what happens when we make
this optimization explicit? To explore this, we propose benchmark-targeted
ranking (BETR), a simple method that selects pretraining documents based on
similarity to benchmark training examples. BETR embeds benchmark examples and a
sample of pretraining documents in a shared space, scores this sample by
similarity to benchmarks, then trains a lightweight classifier to predict these
scores for the full corpus. We compare data selection methods by training over
500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to
them. From this, we find that simply aligning pretraining data to evaluation
benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline
(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks
across all scales. BETR also generalizes well: when targeting a diverse set of
benchmarks disjoint from our evaluation suite, it still matches or outperforms
baselines. Our scaling analysis further reveals a clear trend: larger models
require less aggressive filtering. Overall, our findings show that directly
matching pretraining data to target tasks precisely shapes model capabilities
and highlight that optimal selection strategies must adapt to model scale.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [261] [Towards Relational Contextual Equality Saturation](https://arxiv.org/abs/2507.11897)
*Tyler Hou,Shadaj Laddad,Joseph M. Hellerstein*

Main category: cs.PL

TL;DR: 本文介绍将上下文等式饱和扩展到egglog中关系等式饱和的正在进行的工作，总结现有方法、应用并指出结合挑战。


<details>
  <summary>Details</summary>
Motivation: 现有工作将上下文推理引入egg，本文旨在将其扩展到egglog中的关系等式饱和。

Method: 总结现有上下文等式饱和方法，概述其主要应用。

Result: 无明确提及具体结果

Conclusion: 识别出将上下文等式饱和方法与关系模型结合的关键挑战。

Abstract: Equality saturation is a powerful technique for program optimization.
Contextual equality saturation extends this to support rewrite rules that are
conditioned on where a term appears in an expression. Existing work has brought
contextual reasoning to egg; in this paper, we share our ongoing work to extend
this to relational equality saturation in egglog. We summarize the existing
approaches to contextual equality saturation, outline its main applications,
and identify key challenges in combining this approach with relational models.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [262] [Designing Algorithms for Entropic Optimal Transport from an Optimisation Perspective](https://arxiv.org/abs/2507.12246)
*Vishwak Srinivasan,Qijia Jiang*

Main category: math.OC

TL;DR: 本文受Sinkhorn算法镜像下降解释启发，为熵正则化最优传输问题开发新方法，有非渐近收敛率，还提出有加速保证的动量方法，框架可用于薛定谔桥问题。


<details>
  <summary>Details</summary>
Motivation: 为熵正则化最优传输问题开发新方法，从优化角度提供解决思路。

Method: 从优化视角出发，基于半对偶问题或在联合分布子集上解决非凸约束问题，还提出动量方法。

Result: 所提方法在对问题结构最小假设下有非渐近收敛率，动量方法有可证明的加速保证。

Conclusion: 开发的基于联合分布优化的更广泛框架可应用于动态薛定谔桥问题。

Abstract: In this work, we develop a collection of novel methods for the
entropic-regularised optimal transport problem, which are inspired by existing
mirror descent interpretations of the Sinkhorn algorithm used for solving this
problem. These are fundamentally proposed from an optimisation perspective:
either based on the associated semi-dual problem, or based on solving a
non-convex constrained problem over subset of joint distributions. This
optimisation viewpoint results in non-asymptotic rates of convergence for the
proposed methods under minimal assumptions on the problem structure. We also
propose a momentum-equipped method with provable accelerated guarantees through
this viewpoint, akin to those in the Euclidean setting. The broader framework
we develop based on optimisation over the joint distributions also finds an
analogue in the dynamical Schr\"{o}dinger bridge problem.

</details>


### [263] [Improved Analysis for Sign-based Methods with Momentum Updates](https://arxiv.org/abs/2507.12091)
*Wei Jiang,Dingzhi Yu,Sifan Yang,Wenhao Yang,Lijun Zhang*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we present enhanced analysis for sign-based optimization
algorithms with momentum updates. Traditional sign-based methods, under the
separable smoothness assumption, guarantee a convergence rate of
$\mathcal{O}(T^{-1/4})$, but they either require large batch sizes or assume
unimodal symmetric stochastic noise. To address these limitations, we
demonstrate that signSGD with momentum can achieve the same convergence rate
using constant batch sizes without additional assumptions. Our analysis, under
the standard $l_2$-smoothness condition, improves upon the result of the prior
momentum-based signSGD method by a factor of $\mathcal{O}(d^{1/2})$, where $d$
is the problem dimension. Furthermore, we explore sign-based methods with
majority vote in distributed settings and show that the proposed momentum-based
method yields convergence rates of $\mathcal{O}\left( d^{1/2}T^{-1/2} +
dn^{-1/2} \right)$ and $\mathcal{O}\left( \max \{ d^{1/4}T^{-1/4},
d^{1/10}T^{-1/5} \} \right)$, which outperform the previous results of
$\mathcal{O}\left( dT^{-1/4} + dn^{-1/2} \right)$ and $\mathcal{O}\left(
d^{3/8}T^{-1/8} \right)$, respectively. Numerical experiments further validate
the effectiveness of the proposed methods.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [264] [Neural Network-Guided Symbolic Regression for Interpretable Descriptor Discovery in Perovskite Catalysts](https://arxiv.org/abs/2507.12404)
*Yeming Xian,Xiaoming Wang,Yanfa Yan*

Main category: physics.data-an

TL;DR: 提出结合神经网络、特征重要性分析和符号回归的两阶段框架，为氧化物钙钛矿的OER活性发现可解释描述符，在数据稀缺情况下实现准确且可解释的描述符发现。


<details>
  <summary>Details</summary>
Motivation: 理解和预测氧化物钙钛矿催化剂的OER活性需要准确且可物理解释的描述符，而符号回归在高维输入和小数据集下性能下降。

Method: 提出两阶段框架，第一阶段用小数据集和七个结构特征，通过工程复合特征和应用符号回归改进已知描述符；第二阶段扩展到164个特征，降维并确定关键电子描述符。

Result: 第一阶段训练和验证MAE分别为22.8和20.8 meV；最终公式使用多个描述符，训练和验证MAE分别为22.1和20.6 meV。

Conclusion: 神经网络引导的符号回归能在数据稀缺情况下实现准确、可解释且有物理意义的描述符发现，材料信息学中可解释性不一定要牺牲准确性。

Abstract: Understanding and predicting the activity of oxide perovskite catalysts for
the oxygen evolution reaction (OER) requires descriptors that are both accurate
and physically interpretable. While symbolic regression (SR) offers a path to
discover such formulas, its performance degrades with high-dimensional inputs
and small datasets. We present a two-phase framework that combines neural
networks (NN), feature importance analysis, and symbolic regression (SR) to
discover interpretable descriptors for OER activity in oxide perovskites. In
Phase I, using a small dataset and seven structural features, we reproduce and
improve the known {\mu}/t descriptor by engineering composite features and
applying symbolic regression, achieving training and validation MAEs of 22.8
and 20.8 meV, respectively. In Phase II, we expand to 164 features, reduce
dimensionality, and identify LUMO energy as a key electronic descriptor. A
final formula using {\mu}/t, {\mu}/RA, and LUMO energy achieves improved
accuracy (training and validation MAEs of 22.1 and 20.6 meV) with strong
physical interpretability. Our results demonstrate that NN-guided symbolic
regression enables accurate, interpretable, and physically meaningful
descriptor discovery in data-scarce regimes, indicating interpretability need
not sacrifice accuracy for materials informatics.

</details>
