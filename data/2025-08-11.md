<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 28]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 31]
- [cs.LG](#cs.LG) [Total: 50]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 12]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.CO](#stat.CO) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [cs.DM](#cs.DM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 28]
- [cs.CY](#cs.CY) [Total: 3]
- [math.PR](#math.PR) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.HC](#cs.HC) [Total: 7]
- [cs.SC](#cs.SC) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [nlin.CG](#nlin.CG) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.DL](#cs.DL) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [cs.CL](#cs.CL) [Total: 19]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.CR](#cs.CR) [Total: 14]
- [econ.GN](#econ.GN) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: 提出AEPO框架解决多模态大语言模型在GUI上自然语言指令语义对齐的探索问题，训练的模型在多个基准测试中取得新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的方法在多模态大语言模型自然语言指令语义对齐上存在低效探索瓶颈，影响模型学习困难语义关联。

Method: 提出自适应探索策略优化（AEPO）框架，采用多答案生成策略进行更广泛探索，并由基于效率原则推导的自适应探索奖励（AER）函数引导。

Result: AEPO训练的InfiGUI - G1 - 3B和InfiGUI - G1 - 7B模型在多个GUI基准测试中建立新的最优结果，相对朴素RLVR基线有显著提升。

Conclusion: AEPO框架有效解决了多模态大语言模型在GUI上语义对齐的探索问题，提升了模型性能。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


### [2] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: 本文结合主动推理原则和大语言模型提出开发安全AGI的新框架，介绍架构、安全机制并提出研究议程。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全方法有局限性，需新途径开发安全AGI。

Method: 结合主动推理原则和大语言模型，构建多智能体系统，采用自然语言表示信念，提出具体安全机制。

Result: 提出了新的AGI开发框架及具体安全机制。

Conclusion: 该方法为AGI开发提供本质安全的路径，围绕ARC基准提出验证框架安全特性的研究议程。

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [3] [Whither symbols in the era of advanced neural networks?](https://arxiv.org/abs/2508.05776)
*Thomas L. Griffiths,Brenden M. Lake,R. Thomas McCoy,Ellie Pavlick,Taylor W. Webb*

Main category: cs.AI

TL;DR: 现代神经网络有类似人类思维组合、创新和快速学习能力，挑战人类思维是符号系统观点并提出新研究议程


<details>
  <summary>Details</summary>
Motivation: 探讨人类思维是否基于符号系统

Method: 对比现代神经网络与人类思维能力

Result: 现代神经网络有类似人类思维能力，削弱人类思维是符号系统观点

Conclusion: 提出关于人类思维符号基础研究的新议程

Abstract: Some of the strongest evidence that human minds should be thought about in
terms of symbolic systems has been the way they combine ideas, produce novelty,
and learn quickly. We argue that modern neural networks -- and the artificial
intelligence systems built upon them -- exhibit similar abilities. This
undermines the argument that the cognitive processes and representations used
by human minds are symbolic, although the fact that these neural networks are
typically trained on data generated by symbolic systems illustrates that such
systems play an important role in characterizing the abstract problems that
human minds have to solve. This argument leads us to offer a new agenda for
research on the symbolic basis of human thought.

</details>


### [4] [Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making](https://arxiv.org/abs/2508.05792)
*Kausik Lakkaraju,Siva Likitha Valluru,Biplav Srivastava*

Main category: cs.AI

TL;DR: 介绍了Holistic - XAI (H - XAI)框架，结合因果评级和传统XAI方法，通过案例展示其通用性，弥补现有XAI方法不足。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法主要服务开发者，多关注模型输出合理性，不能满足不同利益相关者需求，Evaluative AI也主要关注运营组织。

Method: 引入H - XAI框架，集成因果评级方法与传统XAI方法，支持交互式、多方法的解释过程，结合实例级和全局解释。

Result: 通过两个案例研究，涵盖六个场景（二元信用风险分类和金融时间序列预测）展示了方法的通用性。

Conclusion: H - XAI结合因果评级和事后解释，能在个体决策和整体模型层面回答特定利益相关者的问题，填补了现有XAI方法的关键空白。

Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing
on justifying model outputs rather than supporting diverse stakeholder needs. A
recent shift toward Evaluative AI reframes explanation as a tool for hypothesis
testing, but still focuses primarily on operational organizations. We introduce
Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods
with traditional XAI methods to support explanation as an interactive,
multi-method process. H-XAI allows stakeholders to ask a series of questions,
test hypotheses, and compare model behavior against automatically constructed
random and biased baselines. It combines instance-level and global
explanations, adapting to each stakeholder's goals, whether understanding
individual decisions, assessing group-level bias, or evaluating robustness
under perturbations. We demonstrate the generality of our approach through two
case studies spanning six scenarios: binary credit risk classification and
financial time-series forecasting. H-XAI fills critical gaps left by existing
XAI methods by combining causal ratings and post-hoc explanations to answer
stakeholder-specific questions at both the individual decision level and the
overall model level.

</details>


### [5] [Safety of Embodied Navigation: A Survey](https://arxiv.org/abs/2508.05855)
*Zixia Wang,Jia Hu,Ronghui Mu*

Main category: cs.AI

TL;DR: 本文全面分析具身导航安全性，探讨现存挑战、缓解技术、评估方法等，指出未解决问题和未来方向，为开发更安全可靠系统提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推动下具身导航发展加速，但集成到关键应用有安全隐患，确保系统安全至关重要。

Method: 从攻击策略、防御机制和评估方法等多视角综合分析。

Result: 对现有安全挑战、缓解技术、数据集和指标进行全面考察，探索未解决问题和未来研究方向。

Conclusion: 研究为开发更安全可靠具身导航系统提供有价值见解，对提升社会安全和产业效率有广泛影响。

Abstract: As large language models (LLMs) continue to advance and gain influence, the
development of embodied AI has accelerated, drawing significant attention,
particularly in navigation scenarios. Embodied navigation requires an agent to
perceive, interact with, and adapt to its environment while moving toward a
specified target in unfamiliar settings. However, the integration of embodied
navigation into critical applications raises substantial safety concerns. Given
their deployment in dynamic, real-world environments, ensuring the safety of
such systems is critical. This survey provides a comprehensive analysis of
safety in embodied navigation from multiple perspectives, encompassing attack
strategies, defense mechanisms, and evaluation methodologies. Beyond conducting
a comprehensive examination of existing safety challenges, mitigation
technologies, and various datasets and metrics that assess effectiveness and
robustness, we explore unresolved issues and future research directions in
embodied navigation safety. These include potential attack methods, mitigation
strategies, more reliable evaluation techniques, and the implementation of
verification frameworks. By addressing these critical gaps, this survey aims to
provide valuable insights that can guide future research toward the development
of safer and more reliable embodied navigation systems. Furthermore, the
findings of this study have broader implications for enhancing societal safety
and increasing industrial efficiency.

</details>


### [6] [Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning](https://arxiv.org/abs/2508.05888)
*Sahil Bansal,Sai Shruthi Sistla,Aarti Arikatala,Sebastian Schreiber*

Main category: cs.AI

TL;DR: 提出基于知识图谱的工具检索框架，提升多步任务工具检索准确率，实验效果优于非KG基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有工具检索方法在处理多步用户请求时检索准确率低，该方面研究不足。

Method: 提出基于知识图谱的工具检索框架，利用1 - hop自我工具图的集合来建模工具间的直接和间接连接。

Result: 工具图方法在微观平均完全召回指标上工具覆盖率达91.85%，优于非KG基线方法。

Conclusion: 知识图谱中的结构信息能为纯相似度匹配提供补充信号，尤其适用于需要顺序组合工具的查询。

Abstract: Effective tool retrieval is essential for AI agents to select from a vast
array of tools when identifying and planning actions in the context of complex
user queries. Despite its central role in planning, this aspect remains
underexplored in the literature. Traditional approaches rely primarily on
similarities between user queries and tool descriptions, which significantly
limits retrieval accuracy, specifically when handling multi-step user requests.
To address these limitations, we propose a Knowledge Graph (KG)-based tool
retrieval framework that captures the semantic relationships between tools and
their functional dependencies. Our retrieval algorithm leverages ensembles of
1-hop ego tool graphs to model direct and indirect connections between tools,
enabling more comprehensive and contextual tool selection for multi-step tasks.
We evaluate our approach on a synthetically generated internal dataset across
six defined user classes, extending previous work on coherent dialogue
synthesis and too retrieval benchmarks. Results demonstrate that our tool
graph-based method achieves 91.85% tool coverage on the micro-average Complete
Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid
retrieval, the strongest non-KG baseline in our experiments. These findings
support our hypothesis that the structural information in the KG provides
complementary signals to pure similarity matching, particularly for queries
requiring sequential tool composition.

</details>


### [7] [Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making](https://arxiv.org/abs/2508.05996)
*Kaitao Chen,Mianxin Liu,Daoming Zong,Chaoyue Ding,Shaohao Rui,Yankai Jiang,Mu Zhou,Xiaosong Wang*

Main category: cs.AI

TL;DR: 本文提出MedOrch框架用于医疗多模态决策，利用多VLM代理协作，在五个基准测试中验证其性能，代码将开源。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体研究主要关注纯语言任务，扩展到多模态场景有挑战，VLM在指令遵循和自我反思上不如LLM，限制其在协作流程中的能力。

Method: 提出MedOrch框架，使用基于LLM的中介代理让多个基于VLM的专家代理交换和反思输出以实现协作，利用多个开源VLM。

Result: 不同基于VLM的代理之间的协作能超越任何单个代理，在五个医疗视觉问答基准测试中展示了无需模型训练的卓越协作性能。

Conclusion: 中介引导的多智能体协作对推进医疗多模态智能有价值。

Abstract: Complex medical decision-making involves cooperative workflows operated by
different clinicians. Designing AI multi-agent systems can expedite and augment
human-level clinical decision-making. Existing multi-agent researches primarily
focus on language-only tasks, yet their extension to multimodal scenarios
remains challenging. A blind combination of diverse vision-language models
(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are
less capable in instruction following and importantly self-reflection, compared
to large language models (LLMs) of comparable sizes. This disparity largely
constrains VLMs' ability in cooperative workflows. In this study, we propose
MedOrch, a mediator-guided multi-agent collaboration framework for medical
multimodal decision-making. MedOrch employs an LLM-based mediator agent that
enables multiple VLM-based expert agents to exchange and reflect on their
outputs towards collaboration. We utilize multiple open-source general-purpose
and domain-specific VLMs instead of costly GPT-series models, revealing the
strength of heterogeneous models. We show that the collaboration within
distinct VLM-based agents can surpass the capabilities of any individual agent.
We validate our approach on five medical vision question answering benchmarks,
demonstrating superior collaboration performance without model training. Our
findings underscore the value of mediator-guided multi-agent collaboration in
advancing medical multimodal intelligence. Our code will be made publicly
available.

</details>


### [8] [Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning](https://arxiv.org/abs/2508.06042)
*Daechul Ahn,San Kim,Jonghyun Choi*

Main category: cs.AI

TL;DR: 提出分层模仿多智能体框架HIMA和测试平台TEXTSCII - ALL，HIMA在战略清晰度、适应性和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实时战略游戏等动态、长视野任务中表现不佳，现有基于大语言模型的方法难以应对星际争霸II这类游戏的挑战。

Method: 提出分层多智能体框架HIMA，利用专业模仿学习智能体和元控制器战略规划器（SP），并构建涵盖所有种族组合的测试平台TEXTSCII - ALL。

Result: HIMA在战略清晰度、适应性和计算效率上优于现有方法。

Conclusion: 结合专业模仿模块和元级编排有潜力开发更强大、通用的人工智能智能体。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.

</details>


### [9] [LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences](https://arxiv.org/abs/2508.06060)
*Sankarshan Damle,Boi Faltings*

Main category: cs.AI

TL;DR: 本文提出利用参与式预算（PB）作为框架，评估大语言模型（LLMs）资源分配能力和推理能力，测试不同提示策略，结果显示提示设计重要且LLMs在非结构化输入机制设计有潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理复杂决策任务时，结构化资源分配能力未充分研究，且现有基准评估推理能力存在数据污染和静态问题。

Method: 提出基于PB的双用途框架，使用贪婪选择、直接优化和爬山启发式细化三种提示策略让LLMs进行项目子集选择，与效用最大化的oracle对比，还测试LLMs从自然语言输入中推断偏好的能力。

Result: 突出了提示设计的作用，表明LLMs在非结构化输入的机制设计方面有前景。

Conclusion: 提示设计对LLMs资源分配和推理评估很关键，LLMs在处理非结构化输入的机制设计中有潜力。

Abstract: Large Language Models (LLMs) are increasingly expected to handle complex
decision-making tasks, yet their ability to perform structured resource
allocation remains underexplored. Evaluating their reasoning is also difficult
due to data contamination and the static nature of existing benchmarks. We
present a dual-purpose framework leveraging Participatory Budgeting (PB) both
as (i) a practical setting for LLM-based resource allocation and (ii) an
adaptive benchmark for evaluating their reasoning capabilities. We task LLMs
with selecting project subsets under feasibility (e.g., budget) constraints via
three prompting strategies: greedy selection, direct optimization, and a
hill-climbing-inspired refinement. We benchmark LLMs' allocations against a
utility-maximizing oracle. Interestingly, we also test whether LLMs can infer
structured preferences from natural-language voter input or metadata, without
explicit votes. By comparing allocations based on inferred preferences to those
from ground-truth votes, we evaluate LLMs' ability to extract preferences from
open-ended input. Our results underscore the role of prompt design and show
that LLMs hold promise for mechanism design with unstructured inputs.

</details>


### [10] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: 本文提出“Fair Game”机制解决公平机器学习在动态社会环境中的公平性问题，可灵活适应不同阶段。


<details>
  <summary>Details</summary>
Motivation: 现有公平机器学习中偏差定义存在冲突，且需已知地面真值或算法部署后才能应用，与动态社会环境需求存在差距。

Method: 提出“Fair Game”机制，将审计器和去偏算法通过强化学习置于机器学习算法周围形成循环。

Result: “Fair Game”能通过修改审计器和量化的偏差来适应公平性目标的变化。

Conclusion: “Fair Game”可构建灵活且能随时间自适应的公平机器学习系统，适用于部署前后。

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [11] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: 论文指出认知想象在人类思维中作用关键但在AI中被低估，呼吁重视它，并提出语义模型来模拟认知想象。


<details>
  <summary>Details</summary>
Motivation: 当前认知想象在AI中的作用被低估，导致诸多问题并限制了AI能力。

Method: 提出语义模型这一基于概率因果关系、可学习的新数学模型方法来模拟认知想象。

Result: 语义模型能确保想象语境的一致性，可将语境作为由因果关系连接的事实整体系统进行操作。

Conclusion: 应更加关注认知想象，将其作为人工智能下一个有前景的突破点。

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [12] [What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting](https://arxiv.org/abs/2508.06454)
*Joshua Caiata,Ben Armstrong,Kate Larson*

Main category: cs.AI

TL;DR: 提出数据驱动框架评估多获胜者投票规则违反公理的频率，分析规则与公理表现关系，表明神经网络投票规则更优，支持数据驱动研究。


<details>
  <summary>Details</summary>
Motivation: 社会选择研究社区关注不同多获胜者投票规则满足的属性，现有研究多为最坏情况分析，需从数据驱动视角评估规则违反公理的频率。

Method: 提出数据驱动框架，分析多获胜者投票规则在不同偏好分布下与公理表现的关系。

Result: 神经网络作为投票规则在最小化公理违反方面优于传统规则。

Conclusion: 数据驱动的社会选择方法可为新投票系统设计提供信息，支持社会选择领域的数据驱动研究继续开展。

Abstract: Committee-selection problems arise in many contexts and applications, and
there has been increasing interest within the social choice research community
on identifying which properties are satisfied by different multi-winner voting
rules. In this work, we propose a data-driven framework to evaluate how
frequently voting rules violate axioms across diverse preference distributions
in practice, shifting away from the binary perspective of axiom satisfaction
given by worst-case analysis. Using this framework, we analyze the relationship
between multi-winner voting rules and their axiomatic performance under several
preference distributions. We then show that neural networks, acting as voting
rules, can outperform traditional rules in minimizing axiom violations. Our
results suggest that data-driven approaches to social choice can inform the
design of new voting systems and support the continuation of data-driven
research in social choice.

</details>


### [13] [A Generic Complete Anytime Beam Search for Optimal Decision Tree](https://arxiv.org/abs/2508.06064)
*Harold Silvère Kiossou,Siegfried Nijssen,Pierre Schaus*

Main category: cs.AI

TL;DR: 本文提出CA - DL8.5算法，统一现有策略，结合高效剪枝和缓存，经实验对比，基于LDS的CA - DL8.5有最佳任意时间性能。


<details>
  <summary>Details</summary>
Motivation: 寻找最优决策树是NP难问题，现有精确算法任意时间行为差，已有任意时间扩展方法未系统比较，难以评估效果。

Method: 提出CA - DL8.5算法，扩展DL8.5框架，允许集成多种启发式和松弛机制，采用基于重启的束搜索逐步放宽剪枝标准。

Result: 在标准分类基准上实验，基于LDS的CA - DL8.5始终有最佳任意时间性能，优于其他CA - DL8.5变体和Blossom算法。

Conclusion: CA - DL8.5算法为精确和任意时间决策树学习提供新通用框架，基于LDS的变体有最佳性能且保证完整性和最优性。

Abstract: Finding an optimal decision tree that minimizes classification error is known
to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic
programming guarantee optimality, they often suffer from poor anytime behavior
-- meaning they struggle to find high-quality decision trees quickly when the
search is stopped before completion -- due to unbalanced search space
exploration. To address this, several anytime extensions of exact methods have
been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not
been systematically compared, making it difficult to assess their relative
effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and
anytime beam search algorithm that extends the DL8.5 framework and unifies some
existing anytime strategies. In particular, CA-DL8.5 generalizes previous
approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various
heuristics and relaxation mechanisms through a modular design. The algorithm
reuses DL8.5's efficient branch-and-bound pruning and trie-based caching,
combined with a restart-based beam search that gradually relaxes pruning
criteria to improve solution quality over time. Our contributions are twofold:
(1) We introduce this new generic framework for exact and anytime decision tree
learning, enabling the incorporation of diverse heuristics and search
strategies; (2) We conduct a rigorous empirical comparison of several
instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k
heuristics -- using an anytime evaluation metric called the primal gap
integral. Experimental results on standard classification benchmarks show that
CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime
performance, outperforming both other CA-DL8.5 variants and the Blossom
algorithm while maintaining completeness and optimality guarantees.

</details>


### [14] [ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception](https://arxiv.org/abs/2508.06074)
*Siyi Lu,Run Liu,Dongsheng Yang,Lei He*

Main category: cs.AI

TL;DR: 本文提出基于深度强化学习（DRL）的自动驾驶新方法，结合鸟瞰图（BEV）感知，介绍Mamba - BEV模型和ME³ - BEV框架，实验表明该方法性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统模块化方法有误差传播和协调问题，端到端学习系统有计算瓶颈，需新方法解决自动驾驶感知环境和实时决策挑战。

Method: 引入Mamba - BEV模型进行时空特征提取，结合BEV感知与Mamba框架；提出ME³ - BEV框架，将Mamba - BEV模型作为端到端DRL的特征输入；通过语义分割可视化高维特征增强模型可解释性。

Result: 在CARLA模拟器上的大量实验表明，ME³ - BEV在碰撞率和轨迹精度等多个指标上优于现有模型。

Conclusion: ME³ - BEV为实时自动驾驶提供了有前景的解决方案。

Abstract: Autonomous driving systems face significant challenges in perceiving complex
environments and making real-time decisions. Traditional modular approaches,
while offering interpretability, suffer from error propagation and coordination
issues, whereas end-to-end learning systems can simplify the design but face
computational bottlenecks. This paper presents a novel approach to autonomous
driving using deep reinforcement learning (DRL) that integrates bird's-eye view
(BEV) perception for enhanced real-time decision-making. We introduce the
\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction
network that combines BEV-based perception with the Mamba framework for
temporal feature modeling. This integration allows the system to encode vehicle
surroundings and road features in a unified coordinate system and accurately
model long-range dependencies. Building on this, we propose the
\texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a
feature input for end-to-end DRL, achieving superior performance in dynamic
urban driving scenarios. We further enhance the interpretability of the model
by visualizing high-dimensional features through semantic segmentation,
providing insight into the learned representations. Extensive experiments on
the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing
models across multiple metrics, including collision rate and trajectory
accuracy, offering a promising solution for real-time autonomous driving.

</details>


### [15] [Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2](https://arxiv.org/abs/2508.06091)
*Stan P Hauke,Przemysław Andrzej Wałęga*

Main category: cs.AI

TL;DR: 本文解决了聚合 - 组合 - 读出图神经网络（GNN）的逻辑表达能力是否由完整C2逻辑刻画的开放问题，证明其表达能力严格超过C2。


<details>
  <summary>Details</summary>
Motivation: Barceló等人留下了完整C2是否能刻画聚合 - 组合 - 读出GNN逻辑表达能力的开放问题，该问题虽经多次尝试仍未解决，这促使作者开展研究。

Method: 通过证明得出聚合 - 组合 - 读出GNN的逻辑表达能力严格超过C2的结论。

Result: 聚合 - 组合 - 读出GNN的逻辑表达能力严格超过C2，此结果在无向图和有向图上均成立。

Conclusion: 解决了开放问题，除对GNN有影响外，还带来了关于无穷逻辑表达能力的纯逻辑见解。

Abstract: In recent years, there has been growing interest in understanding the
expressive power of graph neural networks (GNNs) by relating them to logical
languages. This research has been been initialised by an influential result of
Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded
fragment of the logic C2), characterises the logical expressiveness of
aggregate-combine GNNs. As a ``challenging open problem'' they left the
question whether full C2 characterises the logical expressiveness of
aggregate-combine-readout GNNs. This question has remained unresolved despite
several attempts. In this paper, we solve the above open problem by proving
that the logical expressiveness of aggregate-combine-readout GNNs strictly
exceeds that of C2. This result holds over both undirected and directed graphs.
Beyond its implications for GNNs, our work also leads to purely logical
insights on the expressive power of infinitary logics.

</details>


### [16] [PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion](https://arxiv.org/abs/2508.06110)
*Yiran Rex Ma*

Main category: cs.AI

TL;DR: 提出PanelTR框架解决表格推理问题，实验显示其性能优且无需训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有表格推理依赖标注数据或复杂数据增强，灵活性和泛化性受限，大语言模型表现不如简单监督模型。

Method: 引入PanelTR框架，通过LLM代理科学家以结构化科学方法进行表格推理，包含个体调查、自我审查和协作同行评审讨论。

Result: 在四个基准测试中，PanelTR性能优于普通大语言模型，与全监督模型相当，且不依赖训练数据。

Conclusion: 结构化科学方法能在零样本情况下以灵活语义理解处理表格推理外的复杂任务。

Abstract: Table reasoning, including tabular QA and fact verification, often depends on
annotated data or complex data augmentation, limiting flexibility and
generalization. LLMs, despite their versatility, often underperform compared to
simple supervised models. To approach these issues, we introduce PanelTR, a
framework utilizing LLM agent scientists for robust table reasoning through a
structured scientific approach. PanelTR's workflow involves agent scientists
conducting individual investigations, engaging in self-review, and
participating in collaborative peer-review discussions. This process, driven by
five scientist personas, enables semantic-level transfer without relying on
data augmentation or parametric optimization. Experiments across four
benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully
supervised models, all while remaining independent of training data. Our
findings indicate that structured scientific methodology can effectively handle
complex tasks beyond table reasoning with flexible semantic understanding in a
zero-shot context.

</details>


### [17] [SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges](https://arxiv.org/abs/2508.06111)
*Dewi S. W. Gould,Bruno Mlodozeniec,Samuel F. Brown*

Main category: cs.AI

TL;DR: 提出SKATE评估框架，让大语言模型相互生成和解决可验证任务进行评估，通过代码输出预测挑战验证，评估六个前沿大语言模型，为通用可扩展评估框架迈进重要一步。


<details>
  <summary>Details</summary>
Motivation: 当前评估基础模型能力和风险的方法需大量领域专业知识，难以随模型快速发展扩展，需要新的评估方法。

Method: 引入SKATE评估框架，让大语言模型作为任务设定者和解决者，使用可验证任务，采用TrueSkill排名系统。

Result: 较弱模型能可靠区分和评分较强模型；基于大语言模型的系统有自我偏好行为；SKATE能自动发现模型间细粒度能力差异。

Conclusion: 研究成果是迈向能跟上大语言模型发展的通用可扩展评估框架的重要一步。

Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet
current methods demand extensive domain expertise, hindering their scalability
as these models rapidly evolve. We introduce SKATE: a novel evaluation
framework in which large language models (LLMs) compete by generating and
solving verifiable tasks for one another. Our core insight is to treat
evaluation as a game: models act as both task-setters and solvers, incentivized
to create questions which highlight their own strengths while exposing others'
weaknesses. SKATE offers several key advantages, balancing scalability,
open-endedness, and objectivity. It is fully automated, data-free, and
scalable, requiring no human input or domain expertise. By using verifiable
tasks rather than LLM judges, scoring is objective. Unlike domain-limited
programmatically-generated benchmarks (e.g. chess-playing or spatial
reasoning), having LLMs creatively pose challenges enables open-ended and
scalable evaluation. As a proof of concept, we introduce LLM-set
code-output-prediction (COP) challenges as a verifiable and extensible
framework in which to test our approach. Using a TrueSkill-based ranking
system, we evaluate six frontier LLMs and find that: (1) weaker models can
reliably differentiate and score stronger ones, (2) LLM-based systems are
capable of self-preferencing behavior, generating questions that align with
their own capabilities, and (3) SKATE automatically surfaces fine-grained
capability differences between models. Our findings are an important step
towards general, scalable evaluation frameworks which can keep pace with LLM
progress.

</details>


### [18] [Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem](https://arxiv.org/abs/2508.06129)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux*

Main category: cs.AI

TL;DR: 研究通过多分类器模型对VRP解质量进行敏感性分析，借助可解释AI理解模型决策，发现特定特征是强预测因子并提出统一框架，凸显特征重要性分析对VRP元启发式算法指导机制开发的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统元启发式算法靠人工设计，机器学习可利用组合优化解的结构特性，本研究旨在扩展此前研究，通过多分类器模型对VRP解质量进行敏感性分析。

Method: 使用多个能预测VRP解质量的分类器模型进行敏感性分析，借助可解释AI理解模型决策。

Result: 发现特征重要性有差异，但特定特征始终是强预测因子，提出能在不同场景对特征影响进行排名的统一框架。

Conclusion: 特征重要性分析有潜力作为开发解决VRP的元启发式算法指导机制的基础。

Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with
numerous real-world applications, mostly solved using metaheuristic algorithms
due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely
on human-crafted designs developed through empirical studies. However, recent
research shows that machine learning methods can be used the structural
characteristics of solutions in combinatorial optimization, thereby aiding in
designing more efficient algorithms, particularly for solving VRP. Building on
this advancement, this study extends the previous research by conducting a
sensitivity analysis using multiple classifier models that are capable of
predicting the quality of VRP solutions. Hence, by leveraging explainable AI,
this research is able to extend the understanding of how these models make
decisions. Finally, our findings indicate that while feature importance varies,
certain features consistently emerge as strong predictors. Furthermore, we
propose a unified framework able of ranking feature impact across different
scenarios to illustrate this finding. These insights highlight the potential of
feature importance analysis as a foundation for developing a guidance mechanism
of metaheuristic algorithms for solving the VRP.

</details>


### [19] [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](https://arxiv.org/abs/2508.06145)
*Byeonghun Bang,Jongsuk Yoon,Dong-Jin Chang,Seho Park,Yong Oh Lee*

Main category: cs.AI

TL;DR: 研究通过RAG管道增强大语言模型处理药物禁忌问题的能力，显著提升了模型准确率，能为用药决策提供更可靠信息。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗保健领域，尤其是药物禁忌方面应用存在挑战，需要更准确可靠信息。

Method: 采用Retrieval Augmented Generation (RAG)管道，以OpenAI的GPT - 4o - mini为基础模型，text - embedding - 3 - small模型进行嵌入，结合Langchain编排混合检索系统并重新排序，利用公共数据库的DUR数据。

Result: 集成RAG管道后，模型在年龄组、怀孕和联合用药禁忌方面的准确率分别达到0.94、0.87和0.89，相比基线模型有显著提升。

Conclusion: 用RAG框架增强大语言模型能减少处方和用药决策的不确定性，提供更精确可靠的药物禁忌信息。

Abstract: The versatility of large language models (LLMs) has been explored across
various sectors, but their application in healthcare poses challenges,
particularly in the domain of pharmaceutical contraindications where accurate
and reliable information is required. This study enhances the capability of
LLMs to address contraindications effectively by implementing a Retrieval
Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base
model, and the text-embedding-3-small model for embeddings, our approach
integrates Langchain to orchestrate a hybrid retrieval system with re-ranking.
This system leverages Drug Utilization Review (DUR) data from public databases,
focusing on contraindications for specific age groups, pregnancy, and
concomitant drug use. The dataset includes 300 question-answer pairs across
three categories, with baseline model accuracy ranging from 0.49 to 0.57.
Post-integration of the RAG pipeline, we observed a significant improvement in
model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications
related to age groups, pregnancy, and concomitant drug use, respectively. The
results indicate that augmenting LLMs with a RAG framework can substantially
reduce uncertainty in prescription and drug intake decisions by providing more
precise and reliable drug contraindication information.

</details>


### [20] [Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution](https://arxiv.org/abs/2508.06225)
*Zailong Tian,Zhuoheng Han,Yanzhe Chen,Haozhe Xu,Xi Yang,richeng xuan,Hongfeng Wang,Lizi Liao*

Main category: cs.AI

TL;DR: 本文倡导将大语言模型作为评判系统从以准确率为中心转向以置信度驱动、考虑风险的评估方式，识别了过度自信现象，引入TH - Score指标，提出LLM - as - Fuser框架，实验表明该方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型作为评判系统的方法主要关注准确率，忽略了校准良好的置信度，而这对自适应和可靠的评估流程至关重要。

Method: 识别当前大语言模型评判系统中的过度自信现象，引入TH - Score指标量化该现象，提出LLM - as - Fuser集成框架。

Result: 所提方法显著改善了校准，并实现了自适应、以置信度驱动的评估流程，相比现有基线在可靠性和准确性上表现更优。

Conclusion: 应从以准确率为中心的评估转向以置信度驱动、考虑风险的大语言模型评判系统，所提方法可行有效。

Abstract: Large Language Models (LLMs) are widely used as automated judges, where
practical value depends on both accuracy and trustworthy, risk-aware judgments.
Existing approaches predominantly focus on accuracy, overlooking the necessity
of well-calibrated confidence, which is vital for adaptive and reliable
evaluation pipelines. In this work, we advocate a shift from accuracy-centric
evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing
the necessity of well-calibrated confidence for trustworthy and adaptive
evaluation. We systematically identify the **Overconfidence Phenomenon** in
current LLM-as-a-Judges, where predicted confidence significantly overstates
actual correctness, undermining reliability in practical deployment. To
quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring
confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an
ensemble framework that transforms LLMs into reliable, risk-aware evaluators.
Extensive experiments demonstrate that our approach substantially improves
calibration and enables adaptive, confidence-driven evaluation pipelines,
achieving superior reliability and accuracy compared to existing baselines.

</details>


### [21] [GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines](https://arxiv.org/abs/2508.06226)
*Yumeng Fu,Jiayin Zhu,Lingling Zhang,Bo Zhao,Shaoxuan Ma,Yushun Zhang,Yanrui Wu,Wenjun Wu*

Main category: cs.AI

TL;DR: 提出GeoLaux基准测试评估MLLMs几何长步骤推理能力，对13个模型实验有重要发现并指导能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有评估MLLM几何技能的基准忽略辅助线构建和细粒度过程评估，不足以评估长步骤推理能力。

Method: 构建包含2186个几何问题的GeoLaux基准，设计五维评估策略。

Result: 1. 模型在长推理步骤中性能大幅下降；2. 解决证明题时倾向走捷径；3. 缺乏辅助线意识，增强该能力有益。

Conclusion: GeoLaux可作为评估含辅助线的长步骤几何推理的基准，也能指导能力提升。

Abstract: Geometry problem solving (GPS) requires models to master diagram
comprehension, logical reasoning, knowledge application, numerical computation,
and auxiliary line construction. This presents a significant challenge for
Multimodal Large Language Models (MLLMs). However, existing benchmarks for
evaluating MLLM geometry skills overlook auxiliary line construction and lack
fine-grained process evaluation, making them insufficient for assessing MLLMs'
long-step reasoning abilities. To bridge these gaps, we present the GeoLaux
benchmark, comprising 2,186 geometry problems, incorporating both calculation
and proving questions. Notably, the problems require an average of 6.51
reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary
line construction. Building on the dataset, we design a novel five-dimensional
evaluation strategy assessing answer correctness, process correctness, process
quality, auxiliary line impact, and error causes. Extensive experiments on 13
leading MLLMs (including thinking models and non-thinking models) yield three
pivotal findings: First, models exhibit substantial performance degradation in
extended reasoning steps (nine models demonstrate over 50% performance drop).
Second, compared to calculation problems, MLLMs tend to take shortcuts when
solving proving problems. Third, models lack auxiliary line awareness, and
enhancing this capability proves particularly beneficial for overall geometry
reasoning improvement. These findings establish GeoLaux as both a benchmark for
evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a
guide for capability advancement. Our dataset and code are included in
supplementary materials and will be released.

</details>


### [22] [Learning Logical Rules using Minimum Message Length](https://arxiv.org/abs/2508.06230)
*Ruben Sharma,Sebastijan Dumančić,Ross D. King,Andrew Cropper*

Main category: cs.AI

TL;DR: 提出贝叶斯归纳逻辑编程方法，从噪声数据学习最小消息长度程序，实验表明优于先前方法且数据高效。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中概率和逻辑学习统一的关键挑战。

Method: 引入贝叶斯归纳逻辑编程方法，通过先验平衡假设复杂度和数据拟合。

Result: 在多个领域实验中显著优于先前方法，数据高效且对示例平衡不敏感。

Conclusion: 该方法有效解决概率和逻辑学习统一问题，具有优越性和实用性。

Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We
introduce a Bayesian inductive logic programming approach that learns minimum
message length programs from noisy data. Our approach balances hypothesis
complexity and data fit through priors, which explicitly favour more general
programs, and a likelihood that favours accurate programs. Our experiments on
several domains, including game playing and drug design, show that our method
significantly outperforms previous methods, notably those that learn minimum
description length programs. Our results also show that our approach is
data-efficient and insensitive to example balance, including the ability to
learn from exclusively positive examples.

</details>


### [23] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti Järvisalo*

Main category: cs.AI

TL;DR: 本文提出破对称方法解决归纳逻辑编程中搜索假设空间的挑战，实验显示显著减少求解时间。


<details>
  <summary>Details</summary>
Motivation: 归纳逻辑编程需搜索假设空间，存在大量逻辑等价假设，搜索难度大。

Method: 引入破对称方法并在答案集编程中实现。

Result: 在多个领域实验中，将求解时间从超1小时降至17秒。

Conclusion: 所提破对称方法能有效减少归纳逻辑编程的求解时间。

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [24] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peigné - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: 介绍PRISM Eval的BET工具用于LLM红队测试，有高成功率，提出细粒度指标和原始级漏洞分析，展示分布式评估途径。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型（LLM）的鲁棒性。

Method: 使用BET工具进行动态对抗优化的自动化红队测试，提出细粒度鲁棒性指标和原始级漏洞分析。

Result: 对41个模型中的37个实现100%攻击成功率，发现模型攻击难度差异大，确定不同危害类别有效的越狱技术。

Conclusion: 展示了社区分布式鲁棒性评估的实用途径。

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


### [25] [A "good regulator theorem" for embodied agents](https://arxiv.org/abs/2508.06326)
*Nathaniel Virgo,Martin Biehl,Manuel Baltieri,Matteo Capucci*

Main category: cs.AI

TL;DR: 研究指出当智能体执行调节任务时，观察者可将其解读为有关于环境的‘信念’并更新，此信念更新概念提供了比Conant和Ashby更复杂、更广泛适用的模型概念。


<details>
  <summary>Details</summary>
Motivation: Conant和Ashby的定理难以在受限设置外推广，需以不同方式阐述类似直觉。

Method: 分析智能体执行调节任务时观察者对其的解读。

Result: 得到了比Conant和Ashby更复杂、更广泛适用的模型概念及定理，且观察者在理论中起关键作用。

Conclusion: 定理适用于经典控制理论设置或调节自身内部状态的系统，模型可能平凡，可解决明显反例问题。

Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a
system must be a model of that system." Artificial Life has produced many
examples of systems that perform tasks with apparently no model in sight; these
suggest Conant and Ashby's theorem doesn't easily generalise beyond its
restricted setup. Nevertheless, here we show that a similar intuition can be
fleshed out in a different way: whenever an agent is able to perform a
regulation task, it is possible for an observer to interpret it as having
"beliefs" about its environment, which it "updates" in response to sensory
input. This notion of belief updating provides a notion of model that is more
sophisticated than Conant and Ashby's, as well as a theorem that is more
broadly applicable. However, it necessitates a change in perspective, in that
the observer plays an essential role in the theory: models are not a mere
property of the system but are imposed on it from outside. Our theorem holds
regardless of whether the system is regulating its environment in a classic
control theory setup, or whether it's regulating its own internal state; the
model is of its environment either way. The model might be trivial, however,
and this is how the apparent counterexamples are resolved.

</details>


### [26] [AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games](https://arxiv.org/abs/2508.06348)
*Mille Mei Zhen Loo,Gert Luzkov,Paolo Burelli*

Main category: cs.AI

TL;DR: 本文提出基于Transformer的AntiCheatPT_256模型检测CS2作弊行为，公开数据集CS2CD，模型在测试集取得较好效果，为作弊检测研究提供基线。


<details>
  <summary>Details</summary>
Motivation: 在线游戏作弊破坏游戏体验，现有反作弊系统难以跟上作弊手段演变且避免对用户系统造成侵扰。

Method: 提出AntiCheatPT_256模型，创建并公开CS2CD数据集，创建上下文窗口并进行数据增强以解决类别不平衡问题，用这些窗口训练模型。

Result: 模型在未增强测试集上准确率达89.17%，AUC为93.36%。

Conclusion: 该方法强调可重复性和实际应用，为数据驱动的作弊检测未来研究提供了可靠基线。

Abstract: Cheating in online video games compromises the integrity of gaming
experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face
significant challenges in keeping pace with evolving cheating methods without
imposing invasive measures on users' systems. This paper presents
AntiCheatPT\_256, a transformer-based machine learning model designed to detect
cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we
introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using
this dataset, 90,707 context windows were created and subsequently augmented to
address class imbalance. The transformer model, trained on these windows,
achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test
set. This approach emphasizes reproducibility and real-world applicability,
offering a robust baseline for future research in data-driven cheat detection.

</details>


### [27] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: 本文提出‘解释性AI’范式，开发概念模型并实证验证，强调以用户为中心的AI解释方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法重算法透明度，难以让终端用户理解，需新范式支持人类决策。

Method: 开发八维概念模型区分解释性AI，用快速情境设计方法进行实证验证。

Result: 用户更偏好上下文敏感、多模态解释，而非技术透明度。

Conclusion: AI系统应注重人类理解，需推进以用户为中心的AI解释方法研究。

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


### [28] [Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned](https://arxiv.org/abs/2508.06368)
*Claudia dAmato,Giuseppe Rubini,Francesco Didio,Donato Francioso,Fatima Zahra Amara,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 文章介绍构建针对女性暴力法律案件的知识图谱，采用两种方法，结果经验证，此图谱有多种用途。


<details>
  <summary>Details</summary>
Motivation: 法律决策需全面知识和信息，法律知识图谱有价值但法律领域较少，为填补空白。

Method: 提出两种自动构建法律知识图谱的互补方法，包括定制的自底向上方法和利用大语言模型的新方案，整合数据提取、本体开发和语义丰富。

Result: 开发出针对女性暴力法律案件的知识图谱，通过能力问题验证。

Conclusion: 开发的知识图谱可提高法律信息可及性，支持复杂查询，可为机器学习工具提供知识组件。

Abstract: Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [29] [Rethinking the Sioux Falls Network: Insights from Path-Driven Higher-Order Network Analysis](https://arxiv.org/abs/2508.06234)
*Chen Zhang,Timothy LaRock,Alben Rome Bagabaldo,Jürgen Hackl*

Main category: cs.CE

TL;DR: 本文引入基于高阶网络模型的数学框架评估基准网络代表性，以苏福尔斯场景为例，结果显示经典网络存在局限，高阶模型可弥合模拟与现实差距。


<details>
  <summary>Details</summary>
Motivation: 现有交通研究中基准场景结构和行为保真度未量化，模拟结果外部有效性存疑。

Method: 引入基于高阶网络模型的数学框架，将经验和模拟轨迹数据编码为有记忆的网络表示，以量化移动行为的顺序依赖关系。

Result: 经典苏福尔斯网络路径多样性有限、高阶时结构快速碎片化、与经验路由行为对齐弱。

Conclusion: 高阶网络模型可弥合模拟与现实移动分析的差距，为交通研究提供更准确和可推广的见解。

Abstract: Benchmark scenarios are widely used in transportation research to evaluate
routing algorithms, simulate infrastructure interventions, and test new
technologies under controlled conditions. However, the structural and
behavioral fidelity of these benchmarks remains largely unquantified, raising
concerns about the external validity of simulation results. In this study, we
introduce a mathematical framework based on higher-order network models to
evaluate the representativeness of benchmark networks, focusing on the widely
used Sioux Falls scenario. Higher-order network models encode empirical and
simulated trajectory data into memory-aware network representations, which we
use to quantify sequential dependencies in mobility behavior and assess how
well benchmark networks capture real-world structural and functional patterns.
Applying this framework to the Sioux Falls network, as well as real-world
trajectory data, we quantify structural complexity, optimal memory length, link
prediction accuracy, and centrality alignment. Our results show and
statistically quantify that the classical Sioux Falls network exhibits limited
path diversity, rapid structural fragmentation at higher orders, and weak
alignment with empirical routing behavior. These results illustrate the
potential of higher-order network models to bridge the gap between
simulation-based and real-world mobility analysis, providing a robust
foundation for more accurate and generalizable insights in transportation
research.

</details>


### [30] [Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha Mining in Quantitative Trading](https://arxiv.org/abs/2508.06312)
*Lang Cao,Zekun Xi,Long Liao,Ziwei Yang,Zheng Cao*

Main category: cs.CE

TL;DR: 提出Chain - of - Alpha框架用于全自动公式化阿尔法因子挖掘，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的阿尔法挖掘方法在自动化、通用性和效率方面存在局限，需要更好的方法。

Method: 提出Chain - of - Alpha框架，采用双链架构（因子生成链和因子优化链），利用市场数据迭代生成、评估和优化候选阿尔法因子。

Result: 在真实A股基准上的大量实验表明，Chain - of - Alpha在多个指标上优于现有基线。

Conclusion: Chain - of - Alpha为大语言模型驱动的量化研究提供了有前景的方向。

Abstract: Alpha factor mining is a fundamental task in quantitative trading, aimed at
discovering interpretable signals that can predict asset returns beyond
systematic market risk. While traditional methods rely on manual formula design
or heuristic search with machine learning, recent advances have leveraged Large
Language Models (LLMs) for automated factor discovery. However, existing
LLM-based alpha mining approaches remain limited in terms of automation,
generality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel,
simple, yet effective and efficient LLM-based framework for fully automated
formulaic alpha mining. Our method features a dual-chain architecture,
consisting of a Factor Generation Chain and a Factor Optimization Chain, which
iteratively generate, evaluate, and refine candidate alpha factors using only
market data, while leveraging backtest feedback and prior optimization
knowledge. The two chains work synergistically to enable high-quality alpha
discovery without human intervention and offer strong scalability. Extensive
experiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha
outperforms existing baselines across multiple metrics, presenting a promising
direction for LLM-driven quantitative research.

</details>


### [31] [Bridging Farm Economics and Landscape Ecology for Global Sustainability through Hierarchical and Bayesian Optimization](https://arxiv.org/abs/2508.06386)
*Kevin Bradley Dsouza,Graham Alexander Watt,Yuri Leonenko,Juan Moreno-Cruz*

Main category: cs.CE

TL;DR: 本文提出分层优化框架，连接农场经济决策与景观规划以解决农业景观问题，应用于加拿大农业景观证明有效。


<details>
  <summary>Details</summary>
Motivation: 农业景观面临维持粮食生产和扭转生物多样性丧失的双重挑战，现有农业环境政策在提供生态功能方面不足，原因是农场经济决策与景观规划脱节。

Method: 提出分层优化框架，先通过生态强化模型确定农场层面经济最优的土地分配，再用生态连通性模型进行景观层面空间布局，最后用贝叶斯优化方法将空间结果转化为政策工具。

Result: 将框架应用于加拿大农业景观，证明能在现实经济约束下增强连通性。

Conclusion: 该方法为使农场激励与生物多样性目标一致提供了全球适用工具，推动了经济可行且生态有效的农业环境政策发展。

Abstract: Agricultural landscapes face the dual challenge of sustaining food production
while reversing biodiversity loss. Agri-environmental policies often fall short
of delivering ecological functions such as landscape connectivity, in part due
to a persistent disconnect between farm-level economic decisions and
landscape-scale spatial planning. We introduce a novel hierarchical
optimization framework that bridges this gap. First, an Ecological
Intensification (EI) model determines the economically optimal allocation of
land to margin and habitat interventions at the individual farm level. These
farm-specific intervention levels are then passed to an Ecological Connectivity
(EC) model, which spatially arranges them across the landscape to maximize
connectivity while preserving farm-level profitability. Finally, we introduce a
Bayesian Optimization (BO) approach that translates these spatial outcomes into
simple, cost effective, and scalable policy instruments, such as subsidies and
eco-premiums, using non-spatial, farm-level policy parameters. Applying the
framework to a Canadian agricultural landscape, we demonstrate how it enhances
connectivity under real-world economic constraints. Our approach provides a
globally relevant tool for aligning farm incentives with biodiversity goals,
advancing the development of agri-environmental policies that are economically
viable and ecologically effective.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [32] [A Cross-Perspective Annotated Dataset for Dynamic Object-Level Attention Modeling in Cloud Gaming](https://arxiv.org/abs/2508.06077)
*Hongqin Lei,Haowei Tang,Zhe Zhang*

Main category: cs.DB

TL;DR: 提出基于《侠盗猎车手5》的游戏数据集，分析影响玩家兴趣的因素，数据集公开。


<details>
  <summary>Details</summary>
Motivation: 云游戏传输高分辨率和超低延迟游戏帧是保证玩家体验的关键，现有DL方法受数据集影响，而现有数据集忽略语义关系和独特特征。

Method: 收集《侠盗猎车手5》的游戏片段，标注玩家感兴趣的对象，分析影响玩家兴趣的因素。

Result: 确定玩家游戏内速度、对象大小和对象速度是影响玩家兴趣的主要因素。

Conclusion: 提供了新的游戏数据集，有助于解决云游戏DL方法中数据集的问题。

Abstract: Cloud gaming has gained popularity as it provides high-quality gaming
experiences on thin hardware, such as phones and tablets. Transmitting gameplay
frames at high resolutions and ultra-low latency is the key to guaranteeing
players' quality of experience (QoE). Numerous studies have explored deep
learning (DL) techniques to address this challenge. The efficiency of these
DL-based approaches is highly affected by the dataset. However, existing
datasets usually focus on the positions of objects while ignoring semantic
relationships with other objects and their unique features. In this paper, we
present a game dataset by collecting gameplay clips from Grand Theft Auto (GTA)
V, and annotating the player's interested objects during the gameplay. Based on
the collected data, we analyze several factors that have an impact on player's
interest and identify that the player's in-game speed, object's size, and
object's speed are the main factors. The dataset is available at
https://drive.google.com/drive/folders/1idH251a2K-hGGd3pKjX-3Gx5o_rUqLC4?usp=sharing

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [33] [Accelerating Data Chunking in Deduplication Systems using Vector Instructions](https://arxiv.org/abs/2508.05797)
*Sreeharsha Udayashankar,Abdelrahman Baba,Samer Al-Kiswany*

Main category: cs.DC

TL;DR: 提出VectorCDC方法加速无哈希CDC算法，在多类型CPU上有效且提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有内容定义分块（CDC）算法扫描文件慢，是数据去重性能瓶颈。

Method: 使用向量CPU指令（如SSE / AVX）加速无哈希CDC算法的VectorCDC方法。

Result: VectorCDC在Intel、AMD、ARM和IBM CPU上有效，吞吐量比现有向量加速技术高8.35 - 26.2倍，不影响去重空间节省。

Conclusion: VectorCDC方法能有效加速无哈希CDC算法，提升系统性能。

Abstract: Content-defined Chunking (CDC) algorithms dictate the overall space savings
that deduplication systems achieve. However, due to their need to scan each
file in its entirety, they are slow and often the main performance bottleneck
within data deduplication. We present VectorCDC, a method to accelerate
hashless CDC algorithms using vector CPU instructions, such as SSE / AVX. Our
evaluation shows that VectorCDC is effective on Intel, AMD, ARM, and IBM CPUs,
achieving 8.35x - 26.2x higher throughput than existing vector-accelerated
techniques without affecting the deduplication space savings.

</details>


### [34] [A Dynamic Approach to Load Balancing in Cloud Infrastructure: Enhancing Energy Efficiency and Resource Utilization](https://arxiv.org/abs/2508.05821)
*Shadman Sakib,Ajay Katangur,Rahul Dubey*

Main category: cs.DC

TL;DR: 本文提出基于实时性能指标分配工作负载的SBDLB，通过CloudSim 7G平台测试，性能优于节流负载均衡策略，提升响应时间、降低处理时间和运营成本。


<details>
  <summary>Details</summary>
Motivation: 云计算发展使负载均衡成为关键，但管理服务器资源和保持工作负载平衡仍是云环境重大挑战，旨在提升资源利用率和系统整体效率。

Method: 引入基于实时性能指标将工作负载分配给虚拟机的Score - Based Dynamic Load Balancer (SBDLB)，用CloudSim 7G平台测试，与节流负载均衡策略对比。

Result: SBDLB能动态适应工作负载波动，优化资源使用，在不同场景下平均响应时间分别提升34%和37%，数据中心处理时间平均降低13%，24小时模拟中运营成本降低15%。

Conclusion: SBDLB方法性能优于节流策略，可通过降低能耗促进更节能和可持续的云基础设施。

Abstract: Cloud computing has grown rapidly in recent years, mainly due to the sharp
increase in data transferred over the internet. This growth makes load
balancing a key part of cloud systems, as it helps distribute user requests
across servers to maintain performance, prevent overload, and ensure a smooth
user experience. Despite its importance, managing server resources and keeping
workloads balanced over time remains a major challenge in cloud environments.
This paper introduces a novel Score-Based Dynamic Load Balancer (SBDLB) that
allocates workloads to virtual machines based on real-time performance metrics.
The objective is to enhance resource utilization and overall system efficiency.
The method was thoroughly tested using the CloudSim 7G platform, comparing its
performance against the throttled load balancing strategy. Evaluations were
conducted across a variety of workloads and scenarios, demonstrating the
SBDLB's ability to adapt dynamically to workload fluctuations while optimizing
resource usage. The proposed method outperformed the throttled strategy,
improving average response times by 34% and 37% in different scenarios. It also
reduced data center processing times by an average of 13%. Over a 24-hour
simulation, the method decreased operational costs by 15%, promoting a more
energy-efficient and sustainable cloud infrastructure through reduced energy
consumption.

</details>


### [35] [Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML Next To Your Data](https://arxiv.org/abs/2508.05904)
*Brandon Baker,Elliott Brossard,Chenwei Xie,Zihao Ye,Deen Liu,Yijun Xie,Arthur Zwiegincew,Nitya Kumar Sharma,Gaurav Jain,Eugene Retunsky,Mike Halcrow,Derek Denny-Brown,Istvan Cseri,Tyler Akidau,Yuxiong He*

Main category: cs.DC

TL;DR: 本文介绍Snowpark设计目标、架构、创新点及通过案例展示其在大规模数据工程与AI/ML任务中的效率和有效性。


<details>
  <summary>Details</summary>
Motivation: 基于Snowflake弹性架构基础，推进其AI数据云愿景，介绍支持数据工程和AI/ML工作负载的Snowpark。

Method: 详细阐述Snowpark架构，包括利用控制平面进行分布式计算、用安全沙箱隔离工作负载等，并提出核心创新点。

Result: 展示了Snowpark在大规模数据工程和AI/ML任务中的效率和有效性。

Conclusion: Snowpark具有高性能、强安全性和易使用性，能满足大规模数据工程和AI/ML任务需求。

Abstract: Snowflake revolutionized data analytics with an elastic architecture that
decouples compute and storage, enabling scalable solutions supporting data
architectures like data lake, data warehouse, data lakehouse, and data mesh.
Building on this foundation, Snowflake has advanced its AI Data Cloud vision by
introducing Snowpark, a managed turnkey solution that supports data engineering
and AI and ML workloads using Python and other programming languages.
  This paper outlines Snowpark's design objectives towards high performance,
strong security and governance, and ease of use. We detail the architecture of
Snowpark, highlighting its elastic scalability and seamless integration with
Snowflake core compute infrastructure. This includes leveraging Snowflake
control plane for distributed computing and employing a secure sandbox for
isolating Snowflake SQL workloads from Snowpark executions. Additionally, we
present core innovations in Snowpark that drive further performance
enhancements, such as query initialization latency reduction through Python
package caching, improved workload scheduling for customized workloads, and
data skew management via efficient row redistribution. Finally, we showcase
real-world case studies that illustrate Snowpark's efficiency and effectiveness
for large-scale data engineering and AI and ML tasks.

</details>


### [36] [KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training](https://arxiv.org/abs/2508.06001)
*Kai Zhang,Peng Wang,Sai Bi,Jianming Zhang,Yuanjun Xiong*

Main category: cs.DC

TL;DR: 提出KnapFormer框架结合负载均衡和序列并行用于Diffusion Transformers分布式训练，实现低通信开销和低负载差异，加速训练并开源。


<details>
  <summary>Details</summary>
Motivation: 解决Diffusion Transformers分布式训练中因可变长度文本输入和不同视觉标记计数导致的各进程间显著的标记不平衡问题。

Method: 通过在平衡组中收集所有进程的序列长度元数据并解决全局背包问题来重新分配标记，在负载均衡决策过程中集成基于DeepSpeed - Ulysees的序列并行，并使用半经验工作负载模型。

Result: 在实际训练工作负载中实现最小通信开销和小于1%的工作负载差异，消除拖后腿效应，在训练如FLUX等扩散模型时实现2 - 3倍加速。

Conclusion: KnapFormer是一个高效且通用的框架，可有效结合负载均衡和序列并行进行Diffusion Transformers分布式训练。

Abstract: We present KnapFormer, an efficient and versatile framework to combine
workload balancing and sequence parallelism in distributed training of
Diffusion Transformers (DiT). KnapFormer builds on the insight that strong
synergy exists between sequence parallelism and the need to address the
significant token imbalance across ranks. This imbalance arises from
variable-length text inputs and varying visual token counts in mixed-resolution
and image-video joint training. KnapFormer redistributes tokens by first
gathering sequence length metadata across all ranks in a balancing group and
solving a global knapsack problem. The solver aims to minimize the variances of
total workload per-GPU, while accounting for the effect of sequence
parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the
load-balancing decision process and utilizing a simple semi-empirical workload
model, KnapFormers achieves minimal communication overhead and less than 1%
workload discrepancy in real-world training workloads with sequence length
varying from a few hundred to tens of thousands. It eliminates straggler
effects and achieves 2x to 3x speedup when training state-of-the-art diffusion
models like FLUX on mixed-resolution and image-video joint data corpora. We
open-source the KnapFormer implementation at
https://github.com/Kai-46/KnapFormer/

</details>


### [37] [EC2MoE: Adaptive End-Cloud Pipeline Collaboration Enabling Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2508.06024)
*Zheming Yang,Yunqing Hu,Sheng Sun,Wen Ji*

Main category: cs.DC

TL;DR: 提出EC2MoE框架用于端云管道协作的可扩展MoE推理，实验显示其能提升吞吐量、降低延迟且保持高准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决在异构端云环境中部署MoE模型在专家调度、通信开销和资源异构性方面的新挑战。

Method: 设计硬件感知的轻量级组门控网络，增强专家选择和计算效率；开发基于端云协作的管道优化机制，包括低秩压缩的编解码器结构和路由感知的启发式管道调度算法。

Result: 与现有方法相比，EC2MoE可将吞吐量提高2.2倍至5.1倍，端到端延迟降低53%至67%，并在动态负载和网络环境下保持良好的可扩展性。

Conclusion: EC2MoE是一种有效的自适应框架，能在端云环境中实现可扩展的MoE推理。

Abstract: The Mixture-of-Experts (MoE) paradigm has emerged as a promising solution to
scale up model capacity while maintaining inference efficiency. However,
deploying MoE models across heterogeneous end-cloud environments poses new
challenges in expert scheduling, communication overhead, and resource
heterogeneity. In this paper, we propose EC2MoE, an adaptive framework for
scalable MoE inference via end-cloud pipeline collaboration. First, we design a
hardware-aware lightweight group gate network that enhances expert selection
and computational efficiency. By incorporating a hardware-aware local expert
selection mechanism, the system adaptively filters candidate experts based on
real-time device profiles. A lightweight group gate module then integrates
local and global gating outputs to achieve high-quality expert routing with
minimal overhead. Second, we develop a pipeline optimization mechanism based on
endcloud collaboration to accelerate MoE inference. This includes an
encoder-decoder structure based on low-rank compression, which reduces
transmission and computation costs. And a route-aware heuristic pipeline
scheduling algorithm that dynamically allocates inference stages across devices
according to workload and network topology. Extensive experiments show that
EC2MoE can increase throughput by 2.2x to 5.1x and reduce end-to-end latency by
53% to 67% while maintaining high accuracy compared to state-of-the-art
methods. It also maintains good scalability under dynamic load and network
environments.

</details>


### [38] [KV Cache Compression for Inference Efficiency in LLMs: A Review](https://arxiv.org/abs/2508.06297)
*Yanyu Liu,Jingying Fu,Sixiang Liu,Yitian Zou,You Fu,Jiehan Zhou,Shouhua Zhang*

Main category: cs.DC

TL;DR: 随着大语言模型发展，KV缓存需求增长致内存瓶颈，文章综述KV缓存优化技术，评估效果并指出局限，给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理时KV缓存需求增长造成内存瓶颈，限制推理效率和可扩展性，需优化KV缓存。

Method: 系统研究当前KV缓存优化技术，评估其有效性、权衡和应用场景。

Result: 对现有方法在内存使用和推理速度方面的影响进行全面分析，找出局限性和挑战。

Conclusion: 提出混合优化技术、自适应动态策略和软硬件协同设计等未来研究方向，以提高推理效率并推动大语言模型实际应用。

Abstract: Withtherapid advancement of large language models (LLMs), the context length
for inference has been continuously increasing, leading to an exponential
growth in the demand for Key-Value (KV) caching. This has resulted in a
significant memory bottleneck, limiting the inference efficiency and
scalability of the models. Therefore, optimizing the KV cache during inference
is crucial for enhancing performance and efficiency. This review systematically
examines current KV cache optimization techniques, including compression
strategies such as selective token strategies, quantization, and attention
compression. We evaluate the effectiveness, trade-offs, and application
scenarios of these methods, providing a comprehensive analysis of their impact
on memory usage and inference speed. We focus on identifying the limitations
and challenges of existing methods, such as compatibility issues with different
models and tasks. Additionally, this review highlights future research
directions, including hybrid optimization techniques, adaptive dynamic
strategies, and software-hardware co-design. These approaches aim to improve
inference efficiency and promote the practical application of large language
models.

</details>


### [39] [Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision](https://arxiv.org/abs/2508.06339)
*Evelyne Ringoot,Rabab Alomairy,Valentin Churavy,Alan Edelman*

Main category: cs.DC

TL;DR: 本文介绍了基于Julia实现的便携式、GPU加速的基于QR的奇异值计算算法，该算法支持多种GPU架构和数据类型，性能表现出色。


<details>
  <summary>Details</summary>
Motivation: 奇异值分解在科学计算和机器学习中是基础工具，在大规模机器学习管道中重要性日益增加，需要实现GPU加速且具有良好可移植性的算法。

Method: 基于经典的两阶段QR约简，利用Julia的多重调度和元编程能力，集成GPUArrays和KernelAbstractions框架。

Result: 统一函数在矩阵尺寸大于1024x1024时优于多数线性代数库，对于大矩阵能达到cuSOLVER性能的80%-90%。

Conclusion: 实现的可移植性算法无需牺牲性能。

Abstract: This paper presents a portable, GPU-accelerated implementation of a QR-based
singular value computation algorithm in Julia. The singular value ecomposition
(SVD) is a fundamental numerical tool in scientific computing and machine
learning, providing optimal low-rank matrix approximations. Its importance has
increased even more in large-scale machine learning pipelines, including large
language models (LLMs), where it enables low-rank adaptation (LoRA). The
implemented algorithm is based on the classic two-stage QR reduction,
consisting of successive matrix reduction to band form and bidiagonal form. Our
implementation leverages Julia's multiple dispatch and metaprogramming
capabilities, integrating with the GPUArrays and KernelAbstractions frameworks
to provide a unified type and hardware-agnostic function. It supports diverse
GPU architectures and data types, and is, to our knowledge, the first
GPU-accelerated singular value implementation to support Apple Metal GPUs and
half precision. Performance results on multiple GPU backends and data types
demonstrate that portability does not require sacrificing performance: the
unified function outperforms most linear algebra libraries (MAGMA, SLATE,
rocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%
of the performance of cuSOLVER for large matrices.

</details>


### [40] [Blockchain-Enabled Federated Learning](https://arxiv.org/abs/2508.06406)
*Murtaza Rangwala,Venugopal K R,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文对区块链赋能的联邦学习（BCFL）系统进行全面架构分析，通过案例和实际部署验证其可行性与优势。


<details>
  <summary>Details</summary>
Motivation: 解决协作式AI系统中信任、隐私和协调的基本挑战。

Method: 采用四维分类法分析BCFL系统，研究设计模式、共识机制、存储架构，进行技术案例研究和实际部署分析。

Result: 案例展示了BCFL系统能在高非IID数据分布的物联网设备上实现有效协作学习，实际部署验证其性能与集中式方法相当且安全性更高。

Conclusion: BCFL系统具有实际可行性，能提供增强的安全保障，实现无信任协作智能。

Abstract: Blockchain-enabled federated learning (BCFL) addresses fundamental challenges
of trust, privacy, and coordination in collaborative AI systems. This chapter
provides comprehensive architectural analysis of BCFL systems through a
systematic four-dimensional taxonomy examining coordination structures,
consensus mechanisms, storage architectures, and trust models. We analyze
design patterns from blockchain-verified centralized coordination to fully
decentralized peer-to-peer networks, evaluating trade-offs in scalability,
security, and performance. Through detailed examination of consensus mechanisms
designed for federated learning contexts, including Proof of Quality and Proof
of Federated Learning, we demonstrate how computational work can be repurposed
from arbitrary cryptographic puzzles to productive machine learning tasks. The
chapter addresses critical storage challenges by examining multi-tier
architectures that balance blockchain's transaction constraints with neural
networks' large parameter requirements while maintaining cryptographic
integrity. A technical case study of the TrustMesh framework illustrates
practical implementation considerations in BCFL systems through distributed
image classification training, demonstrating effective collaborative learning
across IoT devices with highly non-IID data distributions while maintaining
complete transparency and fault tolerance. Analysis of real-world deployments
across healthcare consortiums, financial services, and IoT security
applications validates the practical viability of BCFL systems, achieving
performance comparable to centralized approaches while providing enhanced
security guarantees and enabling new models of trustless collaborative
intelligence.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [41] [Debiasing Polynomial and Fourier Regression](https://arxiv.org/abs/2508.05920)
*Chris Camaño,Raphael A. Meyer,Kevin Shu*

Main category: cs.DS

TL;DR: 提出去偏方法，用最少函数评估次数近似未知函数，该方法无偏、样本复杂度接近最优，还能对现有周期函数近似方法去偏。


<details>
  <summary>Details</summary>
Motivation: 现有随机算法在恢复(1 + ε)-最优多项式时样本复杂度接近最优，但对最佳多项式近似的估计有偏差，这是不理想的。

Method: 基于多项式回归和随机矩阵理论的联系，提出去偏方法，评估合适设计的随机复矩阵（针对分布μ）的特征值对应的函数值。

Result: 所提估计器无偏，样本复杂度接近最优，实验表现优于独立同分布杠杆得分抽样。

Conclusion: 所提方法有效，还能对现有用截断傅里叶级数近似周期函数的方法进行去偏，且样本复杂度接近最优。

Abstract: We study the problem of approximating an unknown function
$f:\mathbb{R}\to\mathbb{R}$ by a degree-$d$ polynomial using as few function
evaluations as possible, where error is measured with respect to a probability
distribution $\mu$. Existing randomized algorithms achieve near-optimal sample
complexities to recover a $ (1+\varepsilon) $-optimal polynomial but produce
biased estimates of the best polynomial approximation, which is undesirable.
  We propose a simple debiasing method based on a connection between polynomial
regression and random matrix theory. Our method involves evaluating
$f(\lambda_1),\ldots,f(\lambda_{d+1})$ where $\lambda_1,\ldots,\lambda_{d+1}$
are the eigenvalues of a suitably designed random complex matrix tailored to
the distribution $\mu$. Our estimator is unbiased, has near-optimal sample
complexity, and experimentally outperforms iid leverage score sampling.
  Additionally, our techniques enable us to debias existing methods for
approximating a periodic function with a truncated Fourier series with
near-optimal sample complexity.

</details>


### [42] [A Structural Linear-Time Algorithm for Computing the Tutte Decomposition](https://arxiv.org/abs/2508.06212)
*Romain Bourneuf,Tim Planken*

Main category: cs.DS

TL;DR: 提出基于结构特征的线性时间计算Tutte - 分解的算法，同时得到关于全嵌套2 - 分离结构的新结果。


<details>
  <summary>Details</summary>
Motivation: 在Hopcroft和Tarjan、Cunningham和Edmonds工作基础上，提出概念更简单的线性时间计算Tutte - 分解的算法。

Method: 先计算所有全嵌套2 - 分离，再据此构建Tutte - 分解，还使用了稳定性的新概念推导结构结果。

Result: 得到了计算Tutte - 分解的线性时间算法和关于全嵌套2 - 分离结构的新结果。

Conclusion: 提出的算法概念简单且能在线性时间内计算Tutte - 分解，新的结构结果有独立研究价值。

Abstract: The block-cut tree decomposes a connected graph along its cutvertices,
displaying its 2-connected components. The Tutte-decomposition extends this
idea to 2-separators in 2-connected graphs, yielding a canonical
tree-decomposition that decomposes the graph into its triconnected components.
In 1973, Hopcroft and Tarjan introduced a linear-time algorithm to compute the
Tutte-decomposition. Cunningham and Edmonds later established a structural
characterization of the Tutte-decomposition via totally-nested 2-separations.
We present a conceptually simple algorithm based on this characterization,
which computes the Tutte-decomposition in linear time. Our algorithm first
computes all totally-nested 2-separations and then builds the
Tutte-decomposition from them.
  Along the way, we derive new structural results on the structure of
totally-nested 2-separations in 2-connected graphs using a novel notion of
stability, which may be of independent interest.

</details>


### [43] [The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations](https://arxiv.org/abs/2508.06316)
*Theresa Pollinger,Masado Ishii,Jens Domke*

Main category: cs.DS

TL;DR: 本文提出omnitrees作为八叉树等结构的各向异性推广，可提升各向异性问题AMR方案收敛性，在三维和四维问题验证其优势。


<details>
  <summary>Details</summary>
Motivation: 现有八叉树在各向异性问题中效率低，花费大量分辨率却收获信息少。

Method: 提出omnitrees，允许仅细化局部最重要维度。

Result: 在三维问题中，omnitrees使平均收敛率提高1.5倍，存储需求少，信息密度提升快；四维问题也有优势。

Conclusion: omnitree离散化可提高现有AMR方法效率，为高维应用带来新可能。

Abstract: Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees
and octrees, underpins a wide range of applications including databases,
computer graphics, physics simulations, and machine learning. However, octrees
enforce isotropic refinement in regions of interest, which can be especially
inefficient for problems that are intrinsically anisotropic--much resolution is
spent where little information is gained. This paper presents omnitrees as an
anisotropic generalization of octrees and related data structures. Omnitrees
allow to refine only the locally most important dimensions, providing tree
structures that are less deep than bintrees and less wide than octrees. As a
result, the convergence of the AMR schemes can be increased by up to a factor
of the dimensionality d for very anisotropic problems, quickly offsetting their
modest increase in storage overhead. We validate this finding on the problem of
binary shape representation across 4,166 three-dimensional objects: Omnitrees
increase the mean convergence rate by 1.5x, require less storage to achieve
equivalent error bounds, and maximize the information density of the stored
function faster than octrees. These advantages are projected to be even
stronger for higher-dimensional problems. We provide a first validation by
introducing a time-dependent rotation to create four-dimensional
representations, and discuss the properties of their 4-d octree and omnitree
approximations. Overall, omnitree discretizations can make existing AMR
approaches more efficient, and open up new possibilities for high-dimensional
applications.

</details>


### [44] [A Simple PTAS for Weighted $k$-means and Sensor Coverage](https://arxiv.org/abs/2508.06460)
*Akash Pareek,Supratim Shit*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Clustering is a fundamental technique in data analysis, with the $k$-means
being one of the widely studied objectives due to its simplicity and broad
applicability. In many practical scenarios, data points come with associated
weights that reflect their importance, frequency, or confidence. Given a
weighted point set $P \subset R^d$, where each point $p \in P$ has a positive
weight $w_p$, the goal is to compute a set of $k$ centers $C = \{ c_1, c_2,
\ldots, c_k \} \subset R^d$ that minimizes the weighted clustering cost:
$\Delta_w(P,C) = \sum_{p \in P} w_p \cdot d(p,C)^2$, where $d(p,C)$ denotes the
Euclidean distance from $p$ to its nearest center in $C$. Although most
existing coreset-based algorithms for $k$-means extend naturally to the
weighted setting and provide a PTAS, no prior work has offered a simple,
coreset-free PTAS designed specifically for the weighted $k$-means problem.
  In this paper, we present a simple PTAS for weighted $k$-means that does not
rely on coresets. Building upon the framework of Jaiswal, Kumar, and Sen (2012)
for the unweighted case, we extend the result to the weighted setting by using
the weighted $D^2$-sampling technique. Our algorithm runs in time $n d \cdot
2^{O\left(\frac{k^2}{\epsilon}\right)}$ and outputs a set of $k$ centers whose
total clustering cost is within a $(1 + \epsilon)$-factor of the optimal cost.
As a key application of the weighted $k$-means, we obtain a PTAS for the sensor
coverage problem, which can also be viewed as a continuous locational
optimization problem. For this problem, the best-known result prior to our work
was an $O(\log k)$-approximation by Deshpande (2014), whereas our algorithm
guarantees a $(1 + \epsilon)$-approximation to the optimal coverage cost even
before applying refinement steps like Lloyd desent.

</details>


### [45] [On the Parallel Complexity of Identifying Groups and Quasigroups via Decompositions](https://arxiv.org/abs/2508.06478)
*Dan Johnson,Michael Levet,Petr Vojtěchovský,Brett Widholm*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we investigate the computational complexity of isomorphism
testing for finite groups and quasigroups, given by their multiplication
tables. We crucially take advantage of their various decompositions to show the
following:
  - We first consider the class $\mathcal{C}$ of groups that admit direct
product decompositions, where each indecompsable factor is $O(1)$-generated,
and either perfect or centerless. We show any group in $\mathcal{C}$ is
identified by the $O(1)$-dimensional count-free Weisfeiler--Leman (WL)
algorithm with $O(\log \log n)$ rounds, and the $O(1)$-dimensional counting WL
algorithm with $O(1)$ rounds. Consequently, the isomorphism problem for
$\mathcal{C}$ is in $\textsf{L}$. The previous upper bound for this class was
$\textsf{TC}^{1}$, using $O(\log n)$ rounds of the $O(1)$-dimensional counting
WL (Grochow and Levet, FCT 2023).
  - We next consider more generally, the class of groups where each
indecomposable factor is $O(1)$-generated. We exhibit an $\textsf{AC}^{3}$
canonical labeling procedure for this class. Here, we accomplish this by
showing that in the multiplication table model, the direct product
decomposition can be computed in $\textsf{AC}^{3}$, parallelizing the work of
Kayal and Nezhmetdinov (ICALP 2009).
  - Isomorphism testing between a central quasigroup $G$ and an arbitrary
quasigroup $H$ is in $\textsf{NC}$. Here, we take advantage of the fact that
central quasigroups admit an affine decomposition in terms of an underlying
Abelian group. Only the trivial bound of $n^{\log(n)+O(1)}$-time was previously
known for isomorphism testing of central quasigroups.

</details>


### [46] [Does block size matter in randomized block Krylov low-rank approximation?](https://arxiv.org/abs/2508.06486)
*Tyler Chen,Ethan N. Epperly,Raphael A. Meyer,Christopher Musco,Akash Rao*

Main category: cs.DS

TL;DR: 本文研究用随机块Krylov迭代计算矩阵的秩k近似问题，解决了理论与实践差距，证明任意块大小下只需O(k/√ε)次矩阵向量积。


<details>
  <summary>Details</summary>
Motivation: 此前研究在块大小介于1和k之间时，矩阵向量积次数界可能达O(k²)，而实践中常选1 ≪ b ≪ k优化性能，存在理论与实践差距。

Method: 证明随机块Krylov迭代的性质，依赖随机块Krylov矩阵最小奇异值的新界。

Result: 证明了对于任意块大小1≤b≤k，随机块Krylov迭代能用O(k/√ε)次矩阵向量积产生(1 + ε)因子的近似秩k近似。

Conclusion: 解决了随机块Krylov迭代计算矩阵秩k近似问题的理论与实践差距，新的界可能有独立价值。

Abstract: We study the problem of computing a rank-$k$ approximation of a matrix using
randomized block Krylov iteration. Prior work has shown that, for block size $b
= 1$ or $b = k$, a $(1 + \varepsilon)$-factor approximation to the best
rank-$k$ approximation can be obtained after $\tilde O(k/\sqrt{\varepsilon})$
matrix-vector products with the target matrix. On the other hand, when $b$ is
between $1$ and $k$, the best known bound on the number of matrix-vector
products scales with $b(k-b)$, which could be as large as $O(k^2)$.
Nevertheless, in practice, the performance of block Krylov methods is often
optimized by choosing a block size $1 \ll b \ll k$. We resolve this
theory-practice gap by proving that randomized block Krylov iteration produces
a $(1 + \varepsilon)$-factor approximate rank-$k$ approximation using $\tilde
O(k/\sqrt{\varepsilon})$ matrix-vector products for any block size $1\le b\le
k$. Our analysis relies on new bounds for the minimum singular value of a
random block Krylov matrix, which may be of independent interest. Similar
bounds are central to recent breakthroughs on faster algorithms for sparse
linear systems [Peng & Vempala, SODA 2021; Nie, STOC 2022].

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [47] [Stochastic Bandits for Crowdsourcing and Multi-Platform Autobidding](https://arxiv.org/abs/2508.05844)
*François Bachoc,Nicolò Cesa-Bianchi,Tommaso Cesari,Roberto Colomboni*

Main category: cs.GT

TL;DR: 本文针对众包和自动竞价场景定义随机多臂老虎机模型，设计算法并给出期望遗憾上界和匹配下界，在特定条件下有改进界。


<details>
  <summary>Details</summary>
Motivation: 受众包中资金分配和自动竞价中预算分配应用场景的启发，定义新的随机多臂老虎机模型。

Method: 设计了一种算法，并对其期望遗憾进行分析，证明匹配的下界。

Result: 算法的期望遗憾在T步后为K√T（含对数因子），在满足额外条件时改进为K(log T)^2。

Conclusion: 所设计算法在不同条件下能达到相应的遗憾界。

Abstract: Motivated by applications in crowdsourcing, where a fixed sum of money is
split among $K$ workers, and autobidding, where a fixed budget is used to bid
in $K$ simultaneous auctions, we define a stochastic bandit model where arms
belong to the $K$-dimensional probability simplex and represent the fraction of
budget allocated to each task/auction. The reward in each round is the sum of
$K$ stochastic rewards, where each of these rewards is unlocked with a
probability that varies with the fraction of the budget allocated to that
task/auction. We design an algorithm whose expected regret after $T$ steps is
of order $K\sqrt{T}$ (up to log factors) and prove a matching lower bound.
Improved bounds of order $K (\log T)^2$ are shown when the function mapping
budget to probability of unlocking the reward (i.e., terminating the task or
winning the auction) satisfies additional diminishing-returns conditions.

</details>


### [48] [An Overlapping Coalition Game Approach for Collaborative Block Mining and Edge Task Offloading in MEC-assisted Blockchain Networks](https://arxiv.org/abs/2508.06031)
*Licheng Ye,Zehui Xiong,Lin Gao,Dusit Niyato*

Main category: cs.GT

TL;DR: 本文探索MEC辅助的区块链网络多联盟协作模式，提出两阶段Stackelberg博弈分析，给出ERC博弈均衡解，提出OCF算法和近最优定价策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单联盟协作模式，本文探索更全面的多联盟协作模式。

Method: 提出两阶段Stackelberg博弈，第一阶段ECP确定资源价格，第二阶段MU决定加入联盟形成OCF博弈，联盟决定购买资源形成ERC博弈。推导ERC博弈的Nash均衡解，提出OCF交替算法和ECP资源定价的近最优策略。

Result: 得到ERC博弈的闭式Nash均衡解，通过OCF算法实现OCF博弈的稳定联盟结构。

Conclusion: 所提方法能为MEC辅助的协作区块链网络在多联盟协作模式下实现稳定联盟结构和近最优资源定价。

Abstract: Mobile edge computing (MEC) is a promising technology that enhances the
efficiency of mobile blockchain networks, by enabling miners, often acted by
mobile users (MUs) with limited computing resources, to offload
resource-intensive mining tasks to nearby edge computing servers. Collaborative
block mining can further boost mining efficiency by allowing multiple miners to
form coalitions, pooling their computing resources and transaction data
together to mine new blocks collaboratively. Therefore, an MEC-assisted
collaborative blockchain network can leverage the strengths of both
technologies, offering improved efficiency, security, and scalability for
blockchain systems. While existing research in this area has mainly focused on
the single-coalition collaboration mode, where each miner can only join one
coalition, this work explores a more comprehensive multi-coalition
collaboration mode, which allows each miner to join multiple coalitions. To
analyze the behavior of miners and the edge computing service provider (ECP) in
this scenario, we propose a novel two-stage Stackelberg game. In Stage I, the
ECP, as the leader, determines the prices of computing resources for all MUs.
In Stage II, each MU decides the coalitions to join, resulting in an
overlapping coalition formation (OCF) game; Subsequently, each coalition
decides how many edge computing resources to purchase from the ECP, leading to
an edge resource competition (ERC) game. We derive the closed-form Nash
equilibrium for the ERC game, based on which we further propose an OCF-based
alternating algorithm to achieve a stable coalition structure for the OCF game
and develop a near-optimal pricing strategy for the ECP's resource pricing
problem.

</details>


### [49] [Social Welfare in Battery Charging Games](https://arxiv.org/abs/2508.06320)
*Simon Krogmann,Pascal Lenzner,Alexander Skopalik,Tobias Sträubig*

Main category: cs.GT

TL;DR: 文章从博弈论角度分析可再生能源电网中家用电池充放电策略，研究不同定价策略下均衡的存在性和质量，指出需更复杂市场模型和定价机制。


<details>
  <summary>Details</summary>
Motivation: 可再生能源分布式供应及家用电池增加带来电网供需平衡挑战，且电池所有者决策基于私利，需从博弈论角度分析。

Method: 考虑博弈论场景，研究类似Stackelberg市场模型，分析不同定价策略下均衡情况。

Result: 均衡的存在性关键取决于定价，社会福利差异大。

Conclusion: 需要更复杂的市场模型和定价机制，为可再生能源网络激励的算法博弈论研究开辟了新领域。

Abstract: The recent rise of renewable energy produced by many decentralized sources
yields interesting market design challenges for electrical grids. Balancing
supply and demand in such networks is both a temporal and spatial challenge due
to capacity constraints. The recent surge in the number of household-owned
batteries, especially in regions with rooftop solar adoption, offers mitigation
potential but often acts misaligned with grid-level objectives. In fact, the
decision to charge or discharge a household-owned battery is a strategic choice
by each battery owner governed by selfish incentives. This calls for an
analysis from a game-theoretic point of view.
  We initiate this timely research direction by considering a game-theoretic
setting where selfish agents strategically charge or discharge their batteries
to increase their profit. In particular, we study a Stackelberg-like market
model where a third party introduces price incentives, aiming to optimize
renewable energy utilization while preserving grid feasibility. For this, we
study the existence and the quality of equilibria under various pricing
strategies. We find that the existence of equilibria crucially depends on the
chosen pricing and that the obtained social welfare varies widely. This calls
for more sophisticated market models and pricing mechanisms and opens up a rich
field for future research in Algorithmic Game Theory on incentives in renewable
energy networks.

</details>


### [50] [A Geometric Analysis of Gains from Trade](https://arxiv.org/abs/2508.06469)
*Jason Hartline,Kangning Wang*

Main category: cs.GT

TL;DR: 给出随机提议者机制在双边交换中对最优贸易收益的近似比的几何证明，并优化分析得到当前最优近似比3.15


<details>
  <summary>Details</summary>
Motivation: 研究随机提议者机制在双边交换中对最优贸易收益的近似情况

Method: 采用几何证明方法，先证明是4 - 近似，后优化分析

Result: 得到随机提议者机制是4 - 近似，后将近似比优化到3.15

Conclusion: 通过几何证明和优化分析，得出随机提议者机制在双边交换中对最优贸易收益的较好近似比

Abstract: We provide a geometric proof that the random proposer mechanism is a
$4$-approximation to the first-best gains from trade in bilateral exchange. We
then refine this geometric analysis to recover the state-of-the-art
approximation ratio of $3.15$.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [51] [Request-Only Optimization for Recommendation Systems](https://arxiv.org/abs/2508.05640)
*Liang Guo,Wei Li,Lucy Liao,Huihui Cheng,Rui Zhang,Yu Shi,Yueming Wang,Yanzun Huang,Keke Zhai,Pengchao Wang,Timothy Shi,Xuan Cao,Shengzhi Wang,Renqin Cai,Zhaojie Gong,Omkar Vichare,Rui Jian,Leon Gao,Shiyan Deng,Xingyu Liu,Xiong Zhang,Fu Li,Wenlei Xie,Bin Wen,Rui Li,Xing Liu,Jiaqi Zhai*

Main category: cs.IR

TL;DR: 本文提出Request - Only Optimizations (ROO)训练和建模范式，可提升推荐系统的存储、训练效率和模型质量。


<details>
  <summary>Details</summary>
Motivation: DLRMs规模大、训练数据多，需要新的存储和训练算法来提升复杂推荐系统的质量。

Method: 通过共同设计数据（请求式数据）、基础设施（基于请求的数据处理管道）和模型架构（请求式神经架构），将用户请求作为训练数据单元。

Result: 实现了数据记录中的原生特征去重，节省数据存储；去重请求中多个展示的计算和通信，使大规模神经网络架构能更好捕捉用户兴趣信号。

Conclusion: ROO训练和建模范式能同时提升推荐系统的存储、训练效率以及模型质量。

Abstract: Deep Learning Recommendation Models (DLRMs) represent one of the largest
machine learning applications on the planet. Industry-scale DLRMs are trained
with petabytes of recommendation data to serve billions of users every day. To
utilize the rich user signals in the long user history, DLRMs have been scaled
up to unprecedented complexity, up to trillions of floating-point operations
(TFLOPs) per example. This scale, coupled with the huge amount of training
data, necessitates new storage and training algorithms to efficiently improve
the quality of these complex recommendation systems. In this paper, we present
a Request-Only Optimizations (ROO) training and modeling paradigm. ROO
simultaneously improves the storage and training efficiency as well as the
model quality of recommendation systems. We holistically approach this
challenge through co-designing data (i.e., request-only data), infrastructure
(i.e., request-only based data processing pipeline), and model architecture
(i.e., request-only neural architectures). Our ROO training and modeling
paradigm treats a user request as a unit of the training data. Compared with
the established practice of treating a user impression as a unit, our new
design achieves native feature deduplication in data logging, consequently
saving data storage. Second, by de-duplicating computations and communications
across multiple impressions in a request, this new paradigm enables highly
scaled-up neural network architectures to better capture user interest signals,
such as Generative Recommenders (GRs) and other request-only friendly
architectures.

</details>


### [52] [Query-Aware Graph Neural Networks for Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05647)
*Vibhor Agrawal,Fay Wang,Rishi Puri*

Main category: cs.IR

TL;DR: 提出用于检索增强生成的新型图神经网络架构，在复杂问答任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 提升复杂多跳问题的检索准确性，改进传统密集检索方法。

Method: 构建每轮知识图，引入带查询引导池化的增强图注意力网络。

Result: 在复杂问答任务上显著优于标准密集检索器。

Conclusion: 该架构可利用PyTorch Geometric高效处理图结构数据，适用于生产检索系统可扩展部署。

Abstract: We present a novel graph neural network (GNN) architecture for
retrieval-augmented generation (RAG) that leverages query-aware attention
mechanisms and learned scoring heads to improve retrieval accuracy on complex,
multi-hop questions. Unlike traditional dense retrieval methods that treat
documents as independent entities, our approach constructs per-episode
knowledge graphs that capture both sequential and semantic relationships
between text chunks. We introduce an Enhanced Graph Attention Network with
query-guided pooling that dynamically focuses on relevant parts of the graph
based on user queries. Experimental results demonstrate that our approach
significantly outperforms standard dense retrievers on complex question
answering tasks, particularly for questions requiring multi-document reasoning.
Our implementation leverages PyTorch Geometric for efficient processing of
graph-structured data, enabling scalable deployment in production retrieval
systems

</details>


### [53] [AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups](https://arxiv.org/abs/2508.05648)
*Chandler Campbell,Bernie Boscoe,Tuan Do*

Main category: cs.IR

TL;DR: 介绍轻量级模块化RAG系统AquiLLM，解决研究小组知识获取问题。


<details>
  <summary>Details</summary>
Motivation: 研究小组在捕获、存储和检索分散在成员间的知识面临挑战，现有RAG - LLM系统忽视内部研究材料隐私问题。

Method: 引入AquiLLM系统，支持多种文档类型和可配置隐私设置。

Result: AquiLLM可实现对学术小组正式和非正式知识更有效访问。

Conclusion: AquiLLM能满足研究小组对知识获取的需求。

Abstract: Research groups face persistent challenges in capturing, storing, and
retrieving knowledge that is distributed across team members. Although
structured data intended for analysis and publication is often well managed,
much of a group's collective knowledge remains informal, fragmented, or
undocumented--often passed down orally through meetings, mentoring, and
day-to-day collaboration. This includes private resources such as emails,
meeting notes, training materials, and ad hoc documentation. Together, these
reflect the group's tacit knowledge--the informal, experience-based expertise
that underlies much of their work. Accessing this knowledge can be difficult,
requiring significant time and insider understanding. Retrieval-augmented
generation (RAG) systems offer promising solutions by enabling users to query
and generate responses grounded in relevant source material. However, most
current RAG-LLM systems are oriented toward public documents and overlook the
privacy concerns of internal research materials. We introduce AquiLLM
(pronounced ah-quill-em), a lightweight, modular RAG system designed to meet
the needs of research groups. AquiLLM supports varied document types and
configurable privacy settings, enabling more effective access to both formal
and informal knowledge within scholarly groups.

</details>


### [54] [AI Guided Accelerator For Search Experience](https://arxiv.org/abs/2508.05649)
*Jayanth Yetukuri,Mehran Elyasi,Samarth Agrawal,Aritra Mandal,Rui Kong,Harish Vempati,Ishita Khan*

Main category: cs.IR

TL;DR: 提出新框架建模过渡查询，结合LLMs扩展查询，在电商场景提升转化和参与度。


<details>
  <summary>Details</summary>
Motivation: 传统查询重写方法难以捕捉用户行为的顺序和过渡动态，需有效方法缩小用户搜索行为与商品识别的差距。

Method: 从eBay用户交互日志挖掘结构化查询轨迹，重建查询序列；引入生成式大语言模型生成多样化替代查询。

Result: 与现有相关搜索模块相比，在转化和参与度指标上有可衡量的提升。

Conclusion: 提出的方法在实际电商环境中有效。

Abstract: Effective query reformulation is pivotal in narrowing the gap between a
user's exploratory search behavior and the identification of relevant products
in e-commerce environments. While traditional approaches predominantly model
query rewrites as isolated pairs, they often fail to capture the sequential and
transitional dynamics inherent in real-world user behavior. In this work, we
propose a novel framework that explicitly models transitional
queries--intermediate reformulations occurring during the user's journey toward
their final purchase intent. By mining structured query trajectories from
eBay's large-scale user interaction logs, we reconstruct query sequences that
reflect shifts in intent while preserving semantic coherence. This approach
allows us to model a user's shopping funnel, where mid-journey transitions
reflect exploratory behavior and intent refinement. Furthermore, we incorporate
generative Large Language Models (LLMs) to produce semantically diverse and
intent-preserving alternative queries, extending beyond what can be derived
through collaborative filtering alone. These reformulations can be leveraged to
populate Related Searches or to power intent-clustered carousels on the search
results page, enhancing both discovery and engagement. Our contributions
include (i) the formal identification and modeling of transitional queries,
(ii) the introduction of a structured query sequence mining pipeline for intent
flow understanding, and (iii) the application of LLMs for scalable,
intent-aware query expansion. Empirical evaluation demonstrates measurable
gains in conversion and engagement metrics compared to the existing Related
Searches module, validating the effectiveness of our approach in real-world
e-commerce settings.

</details>


### [55] [OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools](https://arxiv.org/abs/2508.05650)
*Jiaxuan Liang,Shide Zhou,Kailong Wang*

Main category: cs.IR

TL;DR: 现有RAG评估方法存在不足，本文提出OmniBench RAG平台进行多领域评估，揭示RAG有效性差异。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估方法缺乏领域覆盖、指标粗糙、无标准化框架，难以可复现和可解释地评估RAG性能。

Method: 引入OmniBench RAG平台，量化准确性和效率维度的性能提升，引入两个标准化指标，具备动态测试生成、模块化评估管道和自动知识库构建等功能。

Result: 评估揭示RAG有效性存在显著差异，如文化领域有显著提升，数学领域有下降。

Conclusion: 系统的、领域感知的评估对RAG至关重要。

Abstract: While Retrieval Augmented Generation (RAG) is now widely adopted to enhance
LLMs, evaluating its true performance benefits in a reproducible and
interpretable way remains a major hurdle. Existing methods often fall short:
they lack domain coverage, employ coarse metrics that miss sub document
precision, and fail to capture computational trade offs. Most critically, they
provide no standardized framework for comparing RAG effectiveness across
different models and domains.
  We introduce OmniBench RAG, a novel automated platform for multi domain
evaluation of RAG systems. The platform quantifies performance gains across
accuracy and efficiency dimensions, spanning nine knowledge fields including
culture, geography, and health. We introduce two standardized metrics:
Improvements (accuracy gains) and Transformation (efficiency differences
between pre RAG and post RAG models), enabling reproducible comparisons across
models and tasks. The platform features dynamic test generation, modular
evaluation pipelines, and automated knowledge base construction. Our evaluation
reveals striking variability in RAG effectiveness, from significant gains in
culture to declines in mathematics, highlighting the critical importance of
systematic, domain aware assessment. A demonstration video is available at:
https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:
https://github.com/Garnett-Liang/Omnibench-RAG.

</details>


### [56] [Lessons from A Large Language Model-based Outdoor Trail Recommendation Chatbot with Retrieval Augmented Generation](https://arxiv.org/abs/2508.05652)
*Julia Ann Mathew,Suining He*

Main category: cs.IR

TL;DR: 本文介绍基于大语言模型与检索增强生成技术开发的户外步道推荐聊天机器人Judy，通过美国康涅狄格州案例研究，验证其准确性、有效性和可用性。


<details>
  <summary>Details</summary>
Motivation: 户外休闲活动流行，需对话式AI系统提供户外步道信息和个性化建议，但面临准确提供信息和实现高效推荐服务的挑战。

Method: 以美国康涅狄格州户外步道为案例，进行基于网络的数据收集、户外步道数据管理和基于RAG的推荐的大语言模型性能研究。

Result: 实验结果表明，基于大语言模型与RAG的Judy在推荐户外步道方面具有准确性、有效性和可用性。

Conclusion: 开发基于大语言模型与RAG的户外步道推荐聊天机器人Judy是可行且有效的。

Abstract: The increasing popularity of outdoor recreational activities (such as hiking
and biking) has boosted the demand for a conversational AI system to provide
informative and personalized suggestion on outdoor trails. Challenges arise in
response to (1) how to provide accurate outdoor trail information via
conversational AI; and (2) how to enable usable and efficient recommendation
services. To address above, this paper discusses the preliminary and practical
lessons learned from developing Judy, an outdoor trail recommendation chatbot
based on the large language model (LLM) with retrieval augmented generation
(RAG). To gain concrete system insights, we have performed case studies with
the outdoor trails in Connecticut (CT), US. We have conducted web-based data
collection, outdoor trail data management, and LLM model performance studies on
the RAG-based recommendation. Our experimental results have demonstrated the
accuracy, effectiveness, and usability of Judy in recommending outdoor trails
based on the LLM with RAG.

</details>


### [57] [Comparison of Information Retrieval Techniques Applied to IT Support Tickets](https://arxiv.org/abs/2508.05654)
*Leonardo Santiago Benitez Pereira,Robinson Pizzio,Samir Bonho*

Main category: cs.IR

TL;DR: 本文比较11种信息检索技术处理IT支持工单，Sentence - BERT多语言变体效果最佳，还开源数据和代码，实现原型并提出新评估指标。


<details>
  <summary>Details</summary>
Motivation: 不同机器学习模型在不同数据集上性能不同，为方便IT支持分析师工作。

Method: 在IT支持工单数据集上比较11种信息检索技术，实现最小可行原型。

Result: Sentence - BERT多语言变体distilluse - base - multilingual - cased - v1效果最佳，78.7%推荐相关，TF - IDF等也有不错结果，开源数据和代码。

Conclusion: 提出新评估指标以反映IT分析师对检索质量的看法，证明支持工单恢复系统的实用性。

Abstract: Institutions dependent on IT services and resources acknowledge the crucial
significance of an IT help desk system, that act as a centralized hub
connecting IT staff and users for service requests. Employing various Machine
Learning models, these IT help desk systems allow access to corrective actions
used in the past, but each model has different performance when applied to
different datasets. This work compares eleven Information Retrieval techniques
in a dataset of IT support tickets, with the goal of implementing a software
that facilitates the work of Information Technology support analysts. The best
results were obtained with the Sentence-BERT technique, in its multi-language
variation distilluse-base-multilingual-cased-v1, where 78.7% of the
recommendations made by the model were considered relevant. TF-IDF (69.0%),
Word2vec (68.7%) and LDA (66.3%) techniques also had consistent results.
Furthermore, the used datasets and essential parts of coding have been
published and made open source. It also demonstrated the practicality of a
support ticket recovery system by implementing a minimal viable prototype, and
described in detail the implementation of the system. Finally, this work
proposed a novel metric for comparing the techniques, whose aim is to closely
reflect the perception of the IT analysts about the retrieval quality.

</details>


### [58] [Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation](https://arxiv.org/abs/2508.05657)
*Haozhe Xu,Xiaohua Wang,Changze Lv,Xiaoqing Zheng*

Main category: cs.IR

TL;DR: 本文提出一种数据增强框架和两阶段训练策略，解决对话推荐系统假阴性问题，实验证明能提升性能。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统存在假阴性问题，数据增强面临语义相关性和协作信息平衡挑战。

Method: 提出基于大语言模型的语义检索器识别相关项目，用相关性评分器过滤噪声候选，引入两阶段训练策略。

Result: 在两个基准数据集和用户模拟器上实验，各推荐器性能显著且持续提升。

Conclusion: 所提方法能有效提升对话推荐系统性能。

Abstract: Conversational recommender systems (CRSs) enhance recommendation quality by
engaging users in multi-turn dialogues, capturing nuanced preferences through
natural language interactions. However, these systems often face the false
negative issue, where items that a user might like are incorrectly labeled as
negative during training, leading to suboptimal recommendations.Expanding the
label set through data augmentation presents an intuitive solution but faces
the challenge of balancing two key aspects: ensuring semantic relevance and
preserving the collaborative information inherent in CRS datasets. To address
these issues, we propose a novel data augmentation framework that first
leverages an LLM-based semantic retriever to identify diverse and semantically
relevant items, which are then filtered by a relevance scorer to remove noisy
candidates. Building on this, we introduce a two-stage training strategy
balancing semantic relevance and collaborative information. Extensive
experiments on two benchmark datasets and user simulators demonstrate
significant and consistent performance improvements across various
recommenders, highlighting the effectiveness of our approach in advancing CRS
performance.

</details>


### [59] [Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review](https://arxiv.org/abs/2508.05660)
*Aditya Nagori,Ricardo Accorsi Casonatto,Ayush Gautam,Abhinav Manikantha Sai Cheruvu,Rishikesan Kamaleswaran*

Main category: cs.IR

TL;DR: 传统文献综述方法难应对科学出版物激增，本文提出代理方法封装混合RAG管道，经实验在多指标上取得增益，为科学发现建立可扩展框架。


<details>
  <summary>Details</summary>
Motivation: 传统综述方法难以应对科学出版物激增，现有混合RAG系统存在静态、依赖专有工具和缺乏不确定性估计等问题。

Method: 将混合RAG管道封装在自主代理中，动态选择GraphRAG和VectorRAG，实时调整指令调优生成，量化推理不确定性；摄入文献数据构建知识图和向量存储，用Llama - 3.3 - 70B代理选择检索方式，指令调优和自举评估。

Result: 在模拟现实查询的合成基准测试中，指令调优代理在多个指标上优于基线，如VS上下文召回率提升0.63，整体上下文精度提升0.56等。

Conclusion: 该系统提高了对异构源的推理能力，为自主的科学发现建立了可扩展框架。

Abstract: The surge in scientific publications challenges traditional review methods,
demanding tools that integrate structured metadata with full-text analysis.
Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries
with vector search offer promise but are typically static, rely on proprietary
tools, and lack uncertainty estimates. We present an agentic approach that
encapsulates the hybrid RAG pipeline within an autonomous agent capable of (1)
dynamically selecting between GraphRAG and VectorRAG for each query, (2)
adapting instruction-tuned generation in real time to researcher needs, and (3)
quantifying uncertainty during inference. This dynamic orchestration improves
relevance, reduces hallucinations, and promotes reproducibility.
  Our pipeline ingests bibliometric open-access data from PubMed, arXiv, and
Google Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and
embeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2
model. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher
for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking).
Instruction tuning refines domain-specific generation, and bootstrapped
evaluation yields standard deviation for evaluation metrics.
  On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned
Agent with Direct Preference Optimization (DPO) outperforms the baseline,
achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall
Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in
both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score,
0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall
Precision. These results highlight the system's improved reasoning over
heterogeneous sources and establish a scalable framework for autonomous,
agentic scientific discovery.

</details>


### [60] [Zero-Shot Retrieval for Scalable Visual Search in a Two-Sided Marketplace](https://arxiv.org/abs/2508.05661)
*Andre Rusli,Shoma Ishimoto,Sho Akiyama,Aman Kumar Singh*

Main category: cs.IR

TL;DR: 本文介绍Mercari C2C市场的可扩展视觉搜索系统，评估零样本图像检索模型，离线和在线测试显示多语言SigLIP模型表现佳，表明零样本模型可作实用基线。


<details>
  <summary>Details</summary>
Motivation: 在C2C市场中提供直观的视觉搜索方式，探索适用的视觉搜索系统和模型。

Method: 评估近期的视觉 - 语言模型进行零样本图像检索，并与现有微调基线对比；系统集成实时推理和后台索引工作流，通过降维优化统一嵌入管道；进行离线评估和在线A/B测试。

Result: 多语言SigLIP模型在多个检索指标上优于其他模型，nDCG@5比基线提高13.3%；在线A/B测试中处理组在参与度和转化率上有显著提升，图像搜索交易率最高提升40.9%。

Conclusion: 近期的零样本模型可作为生产使用的强大实用基线，能以最小开销部署有效视觉搜索系统，并可根据未来数据或特定领域需求进行微调。

Abstract: Visual search offers an intuitive way for customers to explore diverse
product catalogs, particularly in consumer-to-consumer (C2C) marketplaces where
listings are often unstructured and visually driven. This paper presents a
scalable visual search system deployed in Mercari's C2C marketplace, where
end-users act as buyers and sellers. We evaluate recent vision-language models
for zero-shot image retrieval and compare their performance with an existing
fine-tuned baseline. The system integrates real-time inference and background
indexing workflows, supported by a unified embedding pipeline optimized through
dimensionality reduction. Offline evaluation using user interaction logs shows
that the multilingual SigLIP model outperforms other models across multiple
retrieval metrics, achieving a 13.3% increase in nDCG@5 over the baseline. A
one-week online A/B test in production further confirms real-world impact, with
the treatment group showing substantial gains in engagement and conversion, up
to a 40.9% increase in transaction rate via image search. Our findings
highlight that recent zero-shot models can serve as a strong and practical
baseline for production use, which enables teams to deploy effective visual
search systems with minimal overhead, while retaining the flexibility to
fine-tune based on future data or domain-specific needs.

</details>


### [61] [From Static to Dynamic: A Streaming RAG Approach to Real-time Knowledge Base](https://arxiv.org/abs/2508.05662)
*Yuzhou Zhu*

Main category: cs.IR

TL;DR: 提出Streaming RAG解决动态流数据对静态RAG框架的挑战，实验显示多方面性能提升并建立新的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 动态流数据对静态RAG框架构成挑战，如全量索引内存成本高、周期性重建有延迟、简单采样牺牲语义覆盖。

Method: 提出Streaming RAG统一管道，结合多向量余弦筛选、小批量聚类和基于计数器的重击中过滤器，维护紧凑原型集，还有增量索引更新机制，证明近似边界。

Result: 在八个实时流实验中Recall@10提升、端到端延迟低、吞吐量高；在开放域问答和摘要任务中有性能提升。

Conclusion: Streaming RAG为检索增强建立了新的帕累托前沿。

Abstract: Dynamic streams from news feeds, social media, sensor networks, and financial
markets challenge static RAG frameworks. Full-scale indices incur high memory
costs; periodic rebuilds introduce latency that undermines data freshness;
naive sampling sacrifices semantic coverage. We present Streaming RAG, a
unified pipeline that combines multi-vector cosine screening, mini-batch
clustering, and a counter-based heavy-hitter filter to maintain a compact
prototype set. We further prove an approximation bound \$E\[R(K\_t)] \ge R^\* -
L \Delta\$ linking retrieval quality to clustering variance. An incremental
index upsert mechanism refreshes prototypes without interrupting queries.
Experiments on eight real-time streams show statistically significant gains in
Recall\@10 (up to 3 points, p < 0.01), end-to-end latency below 15 ms, and
throughput above 900 documents per second under a 150 MB budget. Hyperparameter
sensitivity analysis over cluster count, admission probability, relevance
threshold, and counter capacity validates default settings. In open-domain
question answering with GPT-3.5 Turbo, we record 3.2-point gain in Exact Match
and 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L
improvements. Streaming RAG establishes a new Pareto frontier for retrieval
augmentation.

</details>


### [62] [Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support](https://arxiv.org/abs/2508.05664)
*Hei Yu Chan,Kuok Tou Ho,Chenglong Ma,Yujing Si,Hok Lai Lin,Sa Lei Lam*

Main category: cs.IR

TL;DR: 本文评估多种技术构建电力领域客服系统，选图基RAG框架，结合部分技术构建最终系统，准确率超基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有AI客服系统处理模糊、多意图或特定细节查询能力不足，需构建电力领域强大客服系统。

Method: 评估查询重写、RAG融合、关键词增强、意图识别和上下文重排序等技术，比较向量存储和图基RAG框架，结合意图识别、RAG融合和重排序构建最终系统。

Result: 图基RAG处理复杂查询性能优；查询重写、RAG融合、重排序和意图识别有积极效果，关键词增强有负面影响；最终系统在两个数据集上准确率分别达97.9%和89.6%。

Conclusion: 结合意图识别、RAG融合和重排序的最终系统能有效处理消歧和多源查询，性能显著优于基线RAG模型。

Abstract: Many AI customer service systems use standard NLP pipelines or finetuned
language models, which often fall short on ambiguous, multi-intent, or
detail-specific queries. This case study evaluates recent techniques: query
rewriting, RAG Fusion, keyword augmentation, intent recognition, and context
reranking, for building a robust customer support system in the electric power
domain. We compare vector-store and graph-based RAG frameworks, ultimately
selecting the graph-based RAG for its superior performance in handling complex
queries. We find that query rewriting improves retrieval for queries using
non-standard terminology or requiring precise detail. RAG Fusion boosts
performance on vague or multifaceted queries by merging multiple retrievals.
Reranking reduces hallucinations by filtering irrelevant contexts. Intent
recognition supports the decomposition of complex questions into more targeted
sub-queries, increasing both relevance and efficiency. In contrast, keyword
augmentation negatively impacts results due to biased keyword selection. Our
final system combines intent recognition, RAG Fusion, and reranking to handle
disambiguation and multi-source queries. Evaluated on both a GPT-4-generated
dataset and a real-world electricity provider FAQ dataset, it achieves 97.9%
and 89.6% accuracy respectively, substantially outperforming baseline RAG
models.

</details>


### [63] [HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis](https://arxiv.org/abs/2508.05666)
*Alejandro Godinez*

Main category: cs.IR

TL;DR: 本文提出HySemRAG框架结合ETL与RAG进行大规模文献综合与方法研究差距识别，经评估效果良好且有广泛应用。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG架构的局限性，实现大规模文献综合与研究差距识别。

Method: 采用多层方法，包括混合检索、代理自我纠正框架、事后引用验证；通过八个集成阶段处理学术文献，创建双数据产品。

Result: 结构化字段提取语义相似度得分比PDF分块方法高35.1%；代理质量保证机制单次成功率68.3%，引用准确率99.0%。

Conclusion: HySemRAG框架在科学领域有广泛适用性，可加速证据综合与发现。

Abstract: We present HySemRAG, a framework that combines Extract, Transform, Load (ETL)
pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale
literature synthesis and identify methodological research gaps. The system
addresses limitations in existing RAG architectures through a multi-layered
approach: hybrid retrieval combining semantic search, keyword filtering, and
knowledge graph traversal; an agentic self-correction framework with iterative
quality assurance; and post-hoc citation verification ensuring complete
traceability. Our implementation processes scholarly literature through eight
integrated stages: multi-source metadata acquisition, asynchronous PDF
retrieval, custom document layout analysis using modified Docling architecture,
bibliographic management, LLM-based field extraction, topic modeling, semantic
unification, and knowledge graph construction. The system creates dual data
products - a Neo4j knowledge graph enabling complex relationship queries and
Qdrant vector collections supporting semantic search - serving as foundational
infrastructure for verifiable information synthesis. Evaluation across 643
observations from 60 testing sessions demonstrates structured field extraction
achieving 35.1% higher semantic similarity scores (0.655 $\pm$ 0.178) compared
to PDF chunking approaches (0.485 $\pm$ 0.204, p < 0.000001). The agentic
quality assurance mechanism achieves 68.3% single-pass success rates with 99.0%
citation accuracy in validated responses. Applied to geospatial epidemiology
literature on ozone exposure and cardiovascular disease, the system identifies
methodological trends and research gaps, demonstrating broad applicability
across scientific domains for accelerating evidence synthesis and discovery.

</details>


### [64] [ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations](https://arxiv.org/abs/2508.05667)
*Zekun Liu,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: 本文构建了指令调优数据集ITDR以提升大语言模型在推荐系统中的性能，实验证明其有效，还分析了相关因素影响并给出资源链接。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推荐系统中因数据结构差异及对任务理解不足，性能受限，需解决该问题。

Method: 构建包含7个子任务、整合13个公开数据集、约200000实例的指令调优数据集ITDR。

Result: ITDR显著提升了GLM - 4等主流开源大语言模型在推荐任务上的性能，还进行了任务相关性分析和对比实验。

Conclusion: 构建的ITDR数据集能有效提升大语言模型在推荐系统中的性能，相关资源可在指定链接获取。

Abstract: Large language models (LLMs) have demonstrated outstanding performance in
natural language processing tasks. However, in the field of recommendation
systems, due to the structural differences between user behavior data and
natural language, LLMs struggle to effectively model the associations between
user preferences and items. Although prompt-based methods can generate
recommendation results, their inadequate understanding of recommendation tasks
leads to constrained performance. To address this gap, in this work, we
construct a sufficient instruction tuning dataset, ITDR, which encompasses 7
subtasks across two core root tasks--user-item interaction and user-item
understanding. The dataset integrates data from 13 public recommendation
datasets and is built using manually crafted standardized templates, comprising
approximately 200,000 instances. Experimental results demonstrate that ITDR
significantly enhances the performance of mainstream open-source LLMs such as
GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks.
Furthermore, we analyze the correlations between tasks and explore the impact
of task descriptions and data scale on instruction tuning effectiveness.
Finally, we perform comparative experiments against closed-source LLMs with
substantial parameters. Our tuning dataset ITDR and the fine-tuned large
recommendation models can be accessed at https://github.com/hellolzk/ITDR.

</details>


### [65] [A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges](https://arxiv.org/abs/2508.05668)
*Yunjia Xi,Jianghao Lin,Yongzhao Xiao,Zheli Zhou,Rong Shan,Te Gao,Jiachen Zhu,Weiwen Liu,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: 本文对基于大语言模型的搜索代理进行首次系统分析，对现有工作分类，指出关键挑战和未来研究方向，并提供相关代码库。


<details>
  <summary>Details</summary>
Motivation: 大语言模型带来网络搜索变革，基于大语言模型的搜索代理出现，需要对其进行系统分析。

Method: 从架构、优化、应用和评估等角度对现有工作进行全面分析和分类。

Result: 对搜索代理进行了系统分析和分类。

Conclusion: 确定了该领域关键开放挑战，给出了有前景的未来研究方向。

Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized
web search. The emergence of LLM-based Search Agents marks a pivotal shift
towards deeper, dynamic, autonomous information seeking. These agents can
comprehend user intentions and environmental context and execute multi-turn
retrieval with dynamic planning, extending search capabilities far beyond the
web. Leading examples like OpenAI's Deep Research highlight their potential for
deep information mining and real-world applications. This survey provides the
first systematic analysis of search agents. We comprehensively analyze and
categorize existing works from the perspectives of architecture, optimization,
application, and evaluation, ultimately identifying critical open challenges
and outlining promising future research directions in this rapidly evolving
field. Our repository is available on
https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.

</details>


### [66] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: 研究提出基于Qwen2.5-VL-7B的微调视觉语言模型，将马来西亚审计财务报告表格转为Markdown格式，表现超多种模型，证明特定领域微调有效。


<details>
  <summary>Details</summary>
Motivation: 准确提取和表示金融文档中表格数据结构是文档理解关键挑战，解决将马来西亚审计财务报告表格转为Markdown格式的复杂任务。

Method: 提出基于Qwen2.5-VL-7B的微调视觉语言模型，采用含2152个图像 - 文本对的数据集和LoRA监督微调策略，用双框架评估。

Result: 模型在基于标准评估中准确率92.20%，Markdown TEDS得分96.53%，超多种模型，还显著减少推理时间。

Conclusion: 特定领域微调是连接非结构化金融文档和下游自动化的有效高效方法，可媲美大型通用模型且无计算开销。

Abstract: Accurately extracting and representing the structure of tabular data from
financial documents remains a critical challenge in document understanding,
particularly for regulatory and analytical use cases. This study addresses the
complexity of converting financial tables from Malaysian audited financial
reports into Markdown format, a task complicated by rotated layouts,
multi-level headers, and implicit structural cues. We propose a fine-tuned
vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for
high-fidelity Markdown generation from document images. Our approach includes a
curated dataset of 2,152 image-text pairs with augmentations and a supervised
fine-tuning strategy using LoRA. To assess performance, we evaluated our model
on 100 out-of-sample tables using a dual framework: a criteria-based
LLM-as-a-judge for fine-grained accuracy and our novel Markdown
Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural
fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based
assessment and a 96.53% Markdown TEDS score. This performance significantly
surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized
reasoning-enabled models. Compared to these self-hosted alternatives, it also
significantly reduces inference time. Furthermore, its accuracy exceeds that of
widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.
These results demonstrate that domain-specific fine-tuning provides an
effective and efficient method to bridge the gap between unstructured financial
documents and downstream automation, rivalling much larger and more general
models without their computational overhead.

</details>


### [67] [LMAR: Language Model Augmented Retriever for Domain-specific Knowledge Indexing](https://arxiv.org/abs/2508.05672)
*Yao Zhao,Yantian Ding,Zhiyue Zhang,Dapeng Yao,Yanxun Xu*

Main category: cs.IR

TL;DR: 本文提出LMAR框架解决RAG系统在特定领域知识处理上的问题，实验显示其优于基线模型，是实用且经济的解决方案。


<details>
  <summary>Details</summary>
Motivation: RAG系统在处理特定领域知识时，因预训练嵌入性能下降和基于大语言模型的检索器计算成本高而面临挑战，且微调数据增强嵌入模型效果受限。

Method: 提出LMAR框架，结合大语言模型引导的数据合成、对比嵌入适配和高效文本聚类，采用两阶段流水线，在第一阶段用大语言模型作为标注器和验证器。

Result: 在多个特定领域基准数据集上的实验表明，LMAR优于多个基线模型，硬件要求适中、延迟低，且能与新兴RAG架构和文本嵌入模型无缝集成。

Conclusion: LMAR是可扩展的特定领域适配的实用且经济的解决方案。

Abstract: Retrieval Augmented Generation (RAG) systems often struggle with
domain-specific knowledge due to performance deterioration of pre-trained
embeddings and prohibitive computational costs of large language model
(LLM)-based retrievers. While fine-tuning data augmentation embedding models
offers a promising direction, its effectiveness is limited by the need for
high-quality training data and reliable chunking strategies that preserve
contextual integrity. We propose LMAR (Language Model Augmented Retriever), a
model-agnostic framework that addresses these challenges by combining
LLM-guided data synthesis with contrastive embedding adaptation and efficient
text clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling
and synthetic data augmentation, where LLMs act as both labeler and validator
to ensure high-fidelity supervision throughout the pipeline. Experimental
results across multiple domain-specific benchmark datasets demonstrate that
LMAR outperforms multiple baseline models, while maintaining moderate hardware
requirements and low latency. Its model-agnostic nature further enables
seamless integration with emerging RAG architectures and text embedding models,
ensuring continual improvements without redesigning the pipeline. These results
highlight LMAR as a practical and cost-effective solution for scalable
domain-specific adaptation.

</details>


### [68] [Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems](https://arxiv.org/abs/2508.05673)
*Weiqin Yang,Jiawei Chen,Shengjia Zhang,Peng Wu,Yuegang Sun,Yan Feng,Chun Chen,Can Wang*

Main category: cs.IR

TL;DR: 提出用于优化NDCG@K的SoftmaxLoss@$K$ (SL@$K$)损失函数，实验表明其优于现有损失函数。


<details>
  <summary>Details</summary>
Motivation: 现有优化NDCG@$K$的方法存在忽略Top - K截断、计算成本高和训练不稳定等问题。

Method: 提出SL@$K$损失函数，集成分位数技术处理Top - K截断，推导平滑上界解决不连续性。

Result: 在四个真实数据集和三个推荐骨干模型上实验，SL@$K$平均提升6.03%，优于现有损失函数。

Conclusion: SL@$K$具有理论保证、易实现、计算高效、梯度稳定和抗噪等优点，是优化NDCG@$K$的有效损失函数。

Abstract: In the realm of recommender systems (RS), Top-$K$ ranking metrics such as
NDCG@$K$ are the gold standard for evaluating recommendation performance.
However, during the training of recommendation models, optimizing NDCG@$K$
poses significant challenges due to its inherent discontinuous nature and the
intricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either
overlooked the Top-$K$ truncation or suffered from high computational costs and
training instability. To overcome these limitations, we propose SoftmaxLoss@$K$
(SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization.
Specifically, we integrate the quantile technique to handle Top-$K$ truncation
and derive a smooth upper bound for optimizing NDCG@$K$ to address
discontinuity. The resulting SL@$K$ loss has several desirable properties,
including theoretical guarantees, ease of implementation, computational
efficiency, gradient stability, and noise robustness. Extensive experiments on
four real-world datasets and three recommendation backbones demonstrate that
SL@$K$ outperforms existing losses with a notable average improvement of 6.03%.
The code is available at https://github.com/Tiny-Snow/IR-Benchmark.

</details>


### [69] [Domain-Specific Fine-Tuning and Prompt-Based Learning: A Comparative Study for developing Natural Language-Based BIM Information Retrieval Systems](https://arxiv.org/abs/2508.05676)
*Han Gao,Timo Hartmann,Botao Zhong,Kai Lia,Hanbin Luo*

Main category: cs.IR

TL;DR: 本文对比分析基于自然语言接口（NLI）的建筑信息模型（BIM）信息检索系统的两种方法，构建数据集评估，得出混合配置方法并测试，为设计智能BIM系统提供见解。


<details>
  <summary>Details</summary>
Motivation: 自然语言查询准确提取BIM相关数据存在挑战，需研究有效开发NLI的BIM信息检索系统的方法。

Method: 对比分析领域特定微调与基于提示学习两种方法，实施由意图识别和基于表格问答组成的两阶段框架，构建BIM特定数据集评估，通过案例研究测试混合配置方法。

Result: 领域特定微调在意图识别任务中表现更好，基于提示学习（尤其是GPT - 4o）在基于表格问答中表现出色，混合配置方法在各任务中表现更平衡稳健。

Conclusion: 对两种方法的优缺点进行系统分析，讨论NLI在现实BIM场景的适用性，为研究者和从业者设计智能语言驱动的BIM系统提供见解。

Abstract: Building Information Modeling (BIM) is essential for managing building data
across the entire lifecycle, supporting tasks from design to maintenance.
Natural Language Interface (NLI) systems are increasingly explored as
user-friendly tools for information retrieval in Building Information Modeling
(BIM) environments. Despite their potential, accurately extracting BIM-related
data through natural language queries remains a persistent challenge due to the
complexity use queries and specificity of domain knowledge. This study presents
a comparative analysis of two prominent approaches for developing NLI-based BIM
information retrieval systems: domain-specific fine-tuning and prompt-based
learning using large language models (LLMs). A two-stage framework consisting
of intent recognition and table-based question answering is implemented to
evaluate the effectiveness of both approaches. To support this evaluation, a
BIM-specific dataset of 1,740 annotated queries of varying types across 69
models is constructed. Experimental results show that domain-specific
fine-tuning delivers superior performance in intent recognition tasks, while
prompt-based learning, particularly with GPT-4o, shows strength in table-based
question answering. Based on these findings, this study identify a hybrid
configuration that combines fine-tuning for intent recognition with
prompt-based learning for question answering, achieving more balanced and
robust performance across tasks. This integrated approach is further tested
through case studies involving BIM models of varying complexity. This study
provides a systematic analysis of the strengths and limitations of each
approach and discusses the applicability of the NLI to real-world BIM
scenarios. The findings offer insights for researchers and practitioners in
designing intelligent, language-driven BIM systems.

</details>


### [70] [Are All Genders Equal in the Eyes of Algorithms? -- Analysing Search and Retrieval Algorithms for Algorithmic Gender Fairness](https://arxiv.org/abs/2508.05680)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Ludwig Bothmann,Christian Heumann,Stephanie Thiemichen*

Main category: cs.IR

TL;DR: 本文研究算法系统的性别公平性，用德国高校学术资料分析，发现虽无明显算法歧视，但存在性别差异，强调数字系统公平评估需兼顾技术表现和代表性平等。


<details>
  <summary>Details</summary>
Motivation: 算法系统虽被认为中立，但可能再现或强化包括性别在内的社会偏见，需评估其性别公平性。

Method: 引入并应用算法性别公平的保偏定义，使用德国高校学术资料数据集，分析元数据完整性、学术数据库出版物检索和谷歌搜索结果可见性的性别差异。

Result: 未观察到明显算法歧视，但存在微妙且持续的不平衡，男性教授搜索结果多、出版物记录更匹配，女性教授数字可见性变异性更高。

Conclusion: 数字系统的公平评估需兼顾技术表现和代表性平等。

Abstract: Algorithmic systems such as search engines and information retrieval
platforms significantly influence academic visibility and the dissemination of
knowledge. Despite assumptions of neutrality, these systems can reproduce or
reinforce societal biases, including those related to gender. This paper
introduces and applies a bias-preserving definition of algorithmic gender
fairness, which assesses whether algorithmic outputs reflect real-world gender
distributions without introducing or amplifying disparities. Using a
heterogeneous dataset of academic profiles from German universities and
universities of applied sciences, we analyse gender differences in metadata
completeness, publication retrieval in academic databases, and visibility in
Google search results. While we observe no overt algorithmic discrimination,
our findings reveal subtle but consistent imbalances: male professors are
associated with a greater number of search results and more aligned publication
records, while female professors display higher variability in digital
visibility. These patterns reflect the interplay between platform algorithms,
institutional curation, and individual self-presentation. Our study highlights
the need for fairness evaluations that account for both technical performance
and representational equality in digital systems.

</details>


### [71] [LLM4ES: Learning User Embeddings from Event Sequences via Large Language Models](https://arxiv.org/abs/2508.05688)
*Aleksei Shestov,Omar Zoloev,Maksim Makarenko,Mikhail Orlov,Egor Fadeev,Ivan Kireev,Andrey Savchenko*

Main category: cs.IR

TL;DR: 提出LLM4ES框架用大语言模型从事件序列推导用户嵌入，实验表明在多领域用户分类任务中表现优异且可用于多种应用。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型从事件序列中推导用户嵌入，提升低可变性领域的表示质量。

Method: 将事件序列转换为文本表示，通过下一个标记预测微调大语言模型，引入文本丰富技术。

Result: LLM4ES在金融等领域的用户分类任务中达到了最先进的性能，优于现有嵌入方法。

Conclusion: 生成的用户嵌入可用于从金融用户细分到医疗患者结果预测等广泛应用。

Abstract: This paper presents LLM4ES, a novel framework that exploits large pre-trained
language models (LLMs) to derive user embeddings from event sequences. Event
sequences are transformed into a textual representation, which is subsequently
used to fine-tune an LLM through next-token prediction to generate high-quality
embeddings. We introduce a text enrichment technique that enhances LLM
adaptation to event sequence data, improving representation quality for
low-variability domains. Experimental results demonstrate that LLM4ES achieves
state-of-the-art performance in user classification tasks in financial and
other domains, outperforming existing embedding methods. The resulting user
embeddings can be incorporated into a wide range of applications, from user
segmentation in finance to patient outcome prediction in healthcare.

</details>


### [72] [Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking](https://arxiv.org/abs/2508.05700)
*Runze Su,Jiayin Jin,Jiacheng Li,Sihan Wang,Guangtong Bai,Zelun Wang,Li Tang,Yixiong Meng,Huasen Wu,Zhimeng Pan,Kungang Li,Han Sun,Zhifang Liu,Haoyang Li,Siping Ji,Ling Leng,Prathibha Deshikachar*

Main category: cs.IR

TL;DR: 论文介绍在Pinterest广告排名模型中集成大嵌入表遇到的问题及解决方案，采用多方面预训练方案和CPU - GPU混合服务架构，取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 在Pinterest广告排名模型中集成大嵌入表时遇到稀疏性、可扩展性等常见问题以及特定场景障碍，初始训练效果不佳。

Method: 引入结合多种预训练算法的多方面预训练方案，设计CPU - GPU混合服务基础设施。

Result: 多方面大嵌入表在CTR和CVR领域有显著性能提升，部署框架后实现1.34%的在线CPC降低和2.60%的CTR增加，端到端延迟无变化。

Conclusion: 所提出的预训练方案和服务基础设施有效，能提升Pinterest广告系统性能。

Abstract: Large embedding tables are indispensable in modern recommendation systems,
thanks to their ability to effectively capture and memorize intricate details
of interactions among diverse entities. As we explore integrating large
embedding tables into Pinterest's ads ranking models, we encountered not only
common challenges such as sparsity and scalability, but also several obstacles
unique to our context. Notably, our initial attempts to train large embedding
tables from scratch resulted in neutral metrics. To tackle this, we introduced
a novel multi-faceted pretraining scheme that incorporates multiple pretraining
algorithms. This approach greatly enriched the embedding tables and resulted in
significant performance improvements. As a result, the multi-faceted large
embedding tables bring great performance gain on both the Click-Through Rate
(CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid
serving infrastructure to overcome GPU memory limits and elevate the
scalability. This framework has been deployed in the Pinterest Ads system and
achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral
end-to-end latency change.

</details>


### [73] [G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation](https://arxiv.org/abs/2508.05709)
*Boyu Chen,Siran Chen,Zhengrong Yue,Kainan Yan,Chenyun Yu,Beibei Kong,Cheng Lei,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.IR

TL;DR: 现有推荐系统中显式反馈稀缺，隐式反馈有噪声，提出G - UBS范式解决该问题，实验表明其性能优于主流模型。


<details>
  <summary>Details</summary>
Motivation: 解决隐式反馈有噪声，易误判用户兴趣、影响推荐性能的问题。

Method: 提出G - UBS范式，包含用户组管理器（UGM）和用户反馈建模器（UFM），并构建IF - VR基准。

Result: 在IF - VR上实验，G - UBS比主流LLMs和MLLMs表现好，视频播放率>30%的比例高4.0%，推理准确率高14.9%。

Conclusion: G - UBS范式能有效处理隐式反馈噪声，提升推荐性能。

Abstract: User feedback is critical for refining recommendation systems, yet explicit
feedback (e.g., likes or dislikes) remains scarce in practice. As a more
feasible alternative, inferring user preferences from massive implicit feedback
has shown great potential (e.g., a user quickly skipping a recommended video
usually indicates disinterest). Unfortunately, implicit feedback is often
noisy: a user might skip a video due to accidental clicks or other reasons,
rather than disliking it. Such noise can easily misjudge user interests,
thereby undermining recommendation performance. To address this issue, we
propose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which
leverages contextual guidance from relevant user groups, enabling robust and
in-depth interpretation of implicit feedback for individual users.
Specifically, G-UBS operates via two key agents. First, the User Group Manager
(UGM) effectively clusters users to generate group profiles utilizing a
``summarize-cluster-reflect" workflow based on LLMs. Second, the User Feedback
Modeler (UFM) employs an innovative group-aware reinforcement learning
approach, where each user is guided by the associated group profiles during the
reinforcement learning process, allowing UFM to robustly and deeply examine the
reasons behind implicit feedback. To assess our G-UBS paradigm, we have
constructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To
the best of our knowledge, this is the first multi-modal benchmark for implicit
feedback evaluation in video recommendation, encompassing 15k users, 25k
videos, and 933k interaction records with implicit feedback. Extensive
experiments on IF-VR demonstrate that G-UBS significantly outperforms
mainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a
play rate > 30% and 14.9% higher reasoning accuracy on IF-VR.

</details>


### [74] [WebWatcher: Breaking New Frontiers of Vision-Language Deep Research Agent](https://arxiv.org/abs/2508.05748)
*Xinyu Geng,Peng Xia,Zhen Zhang,Xinyu Wang,Qiuchen Wang,Ruixue Ding,Chenxi Wang,Jialong Wu,Yida Zhao,Kuan Li,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.IR

TL;DR: 现有Web智能体研究多以文本为中心，忽略视觉信息。本文提出多模态智能体WebWatcher及评估基准BrowseComp - VL，实验表明WebWatcher性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有Web智能体研究多为文本中心，忽略视觉信息，解决多模态深度研究难题需要更强推理能力和复杂工具。

Method: 引入多模态智能体WebWatcher，利用高质量合成多模态轨迹进行冷启动训练，使用多种工具深度推理，通过强化学习增强泛化能力；提出评估基准BrowseComp - VL。

Result: WebWatcher在四个具有挑战性的VQA基准测试中显著优于专有基线、RAG工作流和开源智能体。

Conclusion: WebWatcher为解决复杂多模态信息检索任务铺平了道路。

Abstract: Web agents such as Deep Research have demonstrated superhuman cognitive
abilities, capable of solving highly challenging information-seeking problems.
However, most research remains primarily text-centric, overlooking visual
information in the real world. This makes multimodal Deep Research highly
challenging, as such agents require much stronger reasoning abilities in
perception, logic, knowledge, and the use of more sophisticated tools compared
to text-based agents. To address this limitation, we introduce WebWatcher, a
multi-modal Agent for Deep Research equipped with enhanced visual-language
reasoning capabilities. It leverages high-quality synthetic multimodal
trajectories for efficient cold start training, utilizes various tools for deep
reasoning, and further enhances generalization through reinforcement learning.
To better evaluate the capabilities of multimodal agents, we propose
BrowseComp-VL, a benchmark with BrowseComp-style that requires complex
information retrieval involving both visual and textual information.
Experimental results show that WebWatcher significantly outperforms proprietary
baseline, RAG workflow and open-source agents in four challenging VQA
benchmarks, which paves the way for solving complex multimodal
information-seeking tasks.

</details>


### [75] [Dual prototype attentive graph network for cross-market recommendation](https://arxiv.org/abs/2508.05969)
*Li Fan,Menglin Kong,Yang Xiang,Chong Zhang,Chengtao Ji*

Main category: cs.IR

TL;DR: 本文提出DGRE方法用于跨市场推荐，结合市场特定和共享信息提升CMRS泛化性和鲁棒性，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有跨市场推荐系统方法常忽视不同市场用户间共享偏好，本文旨在结合市场特定和共享信息提升CMRS泛化性和鲁棒性。

Method: 提出Dual Prototype Attentive Graph Network for Cross - Market Recommendation (DGRE)方法，利用基于图表示学习的原型捕获市场特定和共享信息，聚类用户形成共享原型，聚合商品特征构建商品侧原型。

Result: 在真实跨市场数据集上实验表明，在建模中考虑市场特定和共享方面能提升CMRS的泛化性和鲁棒性。

Conclusion: 结合市场特定和市场共享信息可提升CMRS的泛化性和鲁棒性，DGRE方法有效。

Abstract: Cross-market recommender systems (CMRS) aim to utilize historical data from
mature markets to promote multinational products in emerging markets. However,
existing CMRS approaches often overlook the potential for shared preferences
among users in different markets, focusing primarily on modeling specific
preferences within each market. In this paper, we argue that incorporating both
market-specific and market-shared insights can enhance the generalizability and
robustness of CMRS. We propose a novel approach called Dual Prototype Attentive
Graph Network for Cross-Market Recommendation (DGRE) to address this. DGRE
leverages prototypes based on graph representation learning from both items and
users to capture market-specific and market-shared insights. Specifically, DGRE
incorporates market-shared prototypes by clustering users from various markets
to identify behavioural similarities and create market-shared user profiles.
Additionally, it constructs item-side prototypes by aggregating item features
within each market, providing valuable market-specific insights. We conduct
extensive experiments to validate the effectiveness of DGRE on a real-world
cross-market dataset, and the results show that considering both
market-specific and market-sharing aspects in modelling can improve the
generalization and robustness of CMRS.

</details>


### [76] [Efficient Multimodal Streaming Recommendation via Expandable Side Mixture-of-Experts](https://arxiv.org/abs/2508.05993)
*Yunke Qu,Liang Qu,Tong Chen,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.IR

TL;DR: 提出内存高效的多模态流推荐框架XSMoE，在三个数据集上实验显示其在推荐质量和计算效率上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态编码器用于流推荐系统时，未针对用户偏好建模，微调成本高且可能遗忘长期偏好，难以捕捉用户最新偏好。

Method: 提出XSMoE框架，将由可扩展专家网络组成的轻量级侧调模块附加到冻结的预训练编码器上，根据用户反馈增量扩展，用门控路由器组合输出，用基于利用率的剪枝策略保持模型紧凑。

Result: 在三个真实世界数据集上的实验表明，XSMoE在推荐质量和计算效率上均优于现有基线。

Conclusion: XSMoE能在不覆盖先前知识的情况下学习新模式，有效捕捉多模态特征中的冷启动和偏好变化问题。

Abstract: Streaming recommender systems (SRSs) are widely deployed in real-world
applications, where user interests shift and new items arrive over time. As a
result, effectively capturing users' latest preferences is challenging, as
interactions reflecting recent interests are limited and new items often lack
sufficient feedback. A common solution is to enrich item representations using
multimodal encoders (e.g., BERT or ViT) to extract visual and textual features.
However, these encoders are pretrained on general-purpose tasks: they are not
tailored to user preference modeling, and they overlook the fact that user
tastes toward modality-specific features such as visual styles and textual
tones can also drift over time. This presents two key challenges in streaming
scenarios: the high cost of fine-tuning large multimodal encoders, and the risk
of forgetting long-term user preferences due to continuous model updates.
  To tackle these challenges, we propose Expandable Side Mixture-of-Experts
(XSMoE), a memory-efficient framework for multimodal streaming recommendation.
XSMoE attaches lightweight side-tuning modules consisting of expandable expert
networks to frozen pretrained encoders and incrementally expands them in
response to evolving user feedback. A gating router dynamically combines expert
and backbone outputs, while a utilization-based pruning strategy maintains
model compactness. By learning new patterns through expandable experts without
overwriting previously acquired knowledge, XSMoE effectively captures both cold
start and shifting preferences in multimodal features. Experiments on three
real-world datasets demonstrate that XSMoE outperforms state-of-the-art
baselines in both recommendation quality and computational efficiency.

</details>


### [77] [Semantic Item Graph Enhancement for Multimodal Recommendation](https://arxiv.org/abs/2508.06154)
*Xiaoxiong Zhang,Xin Zhou,Zhiwei Zeng,Dusit Niyato,Zhiqi Shen*

Main category: cs.IR

TL;DR: 本文提出新框架解决多模态推荐系统中语义图的语义缺陷问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统中语义图存在协作信号建模不足和结构失真问题，影响性能。

Method: 从交互图提取协作信号注入语义图；设计基于模量的个性化嵌入扰动机制；提出双重表示对齐机制。

Result: 在四个基准数据集上的大量实验验证了框架的有效性。

Conclusion: 所提框架能解决语义图的语义缺陷问题，提升多模态推荐系统性能。

Abstract: Multimodal recommendation systems have attracted increasing attention for
their improved performance by leveraging items' multimodal information. Prior
methods often build modality-specific item-item semantic graphs from raw
modality features and use them as supplementary structures alongside the
user-item interaction graph to enhance user preference learning. However, these
semantic graphs suffer from semantic deficiencies, including (1) insufficient
modeling of collaborative signals among items and (2) structural distortions
introduced by noise in raw modality features, ultimately compromising
performance. To address these issues, we first extract collaborative signals
from the interaction graph and infuse them into each modality-specific item
semantic graph to enhance semantic modeling. Then, we design a modulus-based
personalized embedding perturbation mechanism that injects perturbations with
modulus-guided personalized intensity into embeddings to generate contrastive
views. This enables the model to learn noise-robust representations through
contrastive learning, thereby reducing the effect of structural noise in
semantic graphs. Besides, we propose a dual representation alignment mechanism
that first aligns multiple semantic representations via a designed Anchor-based
InfoNCE loss using behavior representations as anchors, and then aligns
behavior representations with the fused semantics by standard InfoNCE, to
ensure representation consistency. Extensive experiments on four benchmark
datasets validate the effectiveness of our framework.

</details>


### [78] [Improving Table Retrieval with Question Generation from Partial Tables](https://arxiv.org/abs/2508.06168)
*Hsing-Ping Liang,Che-Wei Chang,Yao-Chung Fan*

Main category: cs.IR

TL;DR: 提出QGpT方法，用LLM基于表的小部分生成合成问题，提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有开放域表格问答Retriever组件大多关注查询表示优化，忽略表本身在嵌入空间的表示，本文旨在解决此问题。

Method: 提出QGpT方法，用LLM基于表的小部分生成合成问题，将生成问题与部分表段联合嵌入。

Result: 无需嵌入整个表，显著提高了密集和后期交互检索器在多个基准上的检索性能。

Conclusion: QGpT方法简单有效，能提升表与问题的语义对齐，改善检索性能。

Abstract: Recent advances in open-domain question answering over tables have widely
adopted large language models (LLMs) under the Retriever-Reader architecture.
Prior works have effectively leveraged LLMs to tackle the complex reasoning
demands of the Reader component, such as text-to-text, text-to-SQL, and multi
hop reasoning. In contrast, the Retriever component has primarily focused on
optimizing the query representation-training retrievers to retrieve relevant
tables based on questions, or to select keywords from questions for matching
table segments. However, little attention has been given to enhancing how
tables themselves are represented in embedding space to better align with
questions. To address this, we propose QGpT (Question Generation from Partial
Tables), a simple yet effective method that uses an LLM to generate synthetic
questions based on small portions of a table. These questions are generated to
simulate how a user might query the content of the table currently under
consideration. The generated questions are then jointly embedded with the
partial table segments used for generation, enhancing semantic alignment with
user queries. Without the need to embed entire tables, our method significantly
improves retrieval performance across multiple benchmarks for both dense and
late-interaction retrievers.

</details>


### [79] [M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation](https://arxiv.org/abs/2508.06328)
*Zhiyou Xiao,Qinhan Yu,Binghui Li,Geng Chen,Chong Chen,Wentao Zhang*

Main category: cs.IR

TL;DR: 现有MRAG研究多为单模态输出，本文提出支持多模态输入输出的M2IO - R1框架，其基于RL的插入器性能优。


<details>
  <summary>Details</summary>
Motivation: 当前MRAG研究单模态输出限制了表达能力和实用性，现实应用需要多模态输入输出，且RL在LLMs复杂推理任务中成功。

Method: 采用RL范式，引入M2IO - R1框架，使用Group Relative Policy Optimization训练基于RL的插入器Inserter - R1 - 3B。

Result: 轻量级3B插入器推理能力强，显著降低延迟，在质量和效率上优于基线。

Conclusion: 所提M2IO - R1框架有效，基于RL的插入器在多模态输出生成中表现良好。

Abstract: Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables
diverse multimodal inputs but remains limited to single-modality outputs,
restricting expressive capacity and practical utility. In contrast, real-world
applications often demand both multimodal inputs and multimodal outputs for
effective communication and grounded reasoning. Motivated by the recent success
of Reinforcement Learning (RL) in complex reasoning tasks for Large Language
Models (LLMs), we adopt RL as a principled and effective paradigm to address
the multi-step, outcome-driven challenges inherent in multimodal output
generation. Here, we introduce M2IO-R1, a novel framework for Multimodal
Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal
inputs and outputs. Central to our framework is an RL-based inserter,
Inserter-R1-3B, trained with Group Relative Policy Optimization to guide image
selection and placement in a controllable and semantically aligned manner.
Empirical results show that our lightweight 3B inserter achieves strong
reasoning capabilities with significantly reduced latency, outperforming
baselines in both quality and efficiency.

</details>


### [80] [eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion](https://arxiv.org/abs/2508.06450)
*Daria Tikhonovich,Nikita Zelinskiy,Aleksandr V. Petrov,Mayya Spirina,Andrei Semenov,Andrey V. Savchenko,Sergei Kuliev*

Main category: cs.IR

TL;DR: 本文针对Transformer模型在序列推荐中模块化改进的可加性缺乏系统基准测试的问题，提出eSASRec模型，实验显示其效果良好且易集成，还提供开源实现。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型在序列推荐中虽有改进，但模块化改进的可加性缺乏系统基准测试，本文旨在填补这一空白。

Method: 通过实验，结合SASRec的训练目标、LiGR Transformer层和Sampled Softmax Loss构建eSASRec模型。

Result: 在常见学术基准中，eSASRec比最新的先进模型有效率高23%；在生产级基准中，eSASRec在准确率 - 覆盖率权衡方面处于帕累托前沿。

Conclusion: eSASRec修改简单、无需额外特征，可轻松集成到现有推荐流程，能作为新兴复杂算法的强大而简单的基线，作者还提供了开源实现。

Abstract: Since their introduction, Transformer-based models, such as SASRec and
BERT4Rec, have become common baselines for sequential recommendations,
surpassing earlier neural and non-neural methods. A number of following
publications have shown that the effectiveness of these models can be improved
by, for example, slightly updating the architecture of the Transformer layers,
using better training objectives, and employing improved loss functions.
However, the additivity of these modular improvements has not been
systematically benchmarked - this is the gap we aim to close in this paper.
Through our experiments, we identify a very strong model that uses SASRec's
training objective, LiGR Transformer layers, and Sampled Softmax Loss. We call
this combination eSASRec (Enhanced SASRec). While we primarily focus on
realistic, production-like evaluation, in our preliminarily study we find that
common academic benchmarks show eSASRec to be 23% more effective compared to
the most recent state-of-the-art models, such as ActionPiece. In our main
production-like benchmark, eSASRec resides on the Pareto frontier in terms of
the accuracy-coverage tradeoff (alongside the recent industrial models HSTU and
FuXi. As the modifications compared to the original SASRec are relatively
straightforward and no extra features are needed (such as timestamps in HSTU),
we believe that eSASRec can be easily integrated into existing recommendation
pipelines and can can serve as a strong yet very simple baseline for emerging
complicated algorithms. To facilitate this, we provide the open-source
implementations for our models and benchmarks in repository
https://github.com/blondered/transformer_benchmark

</details>


### [81] [Maximum Impact with Fewer Features: Efficient Feature Selection for Cold-Start Recommenders through Collaborative Importance Weighting](https://arxiv.org/abs/2508.06455)
*Nikita Sukhorukov,Danil Gusak,Evgeny Frolov*

Main category: cs.IR

TL;DR: 为解决推荐系统冷启动问题，提出优先考虑用户行为信息的特征选择策略，实验证明该方法在冷启动场景中表现出色且效率高。


<details>
  <summary>Details</summary>
Motivation: 推荐系统冷启动需利用辅助特征，但无关或噪声特征会降低性能，特征过多会增加计算需求，因此需要有效特征选择策略。

Method: 提出优先考虑用户行为信息的策略，结合混合矩阵分解技术增强特征表示，用基于最大体积算法的机制对特征排序。

Result: 在各种数据集和混合推荐模型上评估，在冷启动场景中能选择最少但最有效的特征子集，在严格特征约减下超越现有技术。

Conclusion: 该方法能在推荐准确性和计算效率间取得平衡，在冷启动场景中表现优异。

Abstract: Cold-start challenges in recommender systems necessitate leveraging auxiliary
features beyond user-item interactions. However, the presence of irrelevant or
noisy features can degrade predictive performance, whereas an excessive number
of features increases computational demands, leading to higher memory
consumption and prolonged training times.
  To address this, we propose a feature selection strategy that prioritizes the
user behavioral information. Our method enhances the feature representation by
incorporating correlations from collaborative behavior data using a hybrid
matrix factorization technique and then ranks features using a mechanism based
on the maximum volume algorithm. This approach identifies the most influential
features, striking a balance between recommendation accuracy and computational
efficiency. We conduct an extensive evaluation across various datasets and
hybrid recommendation models, demonstrating that our method excels in
cold-start scenarios by selecting minimal yet highly effective feature subsets.
Even under strict feature reduction, our approach surpasses existing feature
selection techniques while maintaining superior efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty](https://arxiv.org/abs/2508.05659)
*Jeroen F. Uleman,Loes Crielaard,Leonie K. Elsenburg,Guido A. Veldhuis,Karien Stronks,Naja Hulvej Rod,Rick Quax,Vítor V. Vasconcelos*

Main category: cs.LG

TL;DR: 提出Diagrams - to - Dynamics (D2D)方法将因果循环图转换为探索性系统动力学模型，结果显示D2D有助于区分杠杆点，与数据驱动模型一致性高，有开源实现。


<details>
  <summary>Details</summary>
Motivation: 因果循环图在支持动态分析和干预策略方面有限，定量分析方法常导致错误推断。

Method: 提出D2D方法，通过最少用户输入，利用CLD结构信息模拟假设干预和探索杠杆点。

Result: D2D有助于区分高低排名杠杆点，与数据驱动模型一致性高于网络中心性分析，能提供不确定性估计和数据收集指导。

Conclusion: 该方法有开源实现，期待更多验证以确立其在广泛案例和领域的实用性。

Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental
research to represent hypothesized causal structures underlying complex
problems. However, as qualitative and static representations, CLDs are limited
in their ability to support dynamic analysis and inform intervention
strategies. Additionally, quantitative CLD analysis methods like network
centrality analysis often lead to false inference. We propose
Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory
system dynamics models (SDMs) in the absence of empirical data. With minimal
user input - following a protocol to label variables as stocks,
flows/auxiliaries, or constants - D2D leverages the structural information
already encoded in CLDs, namely, link existence and polarity, to simulate
hypothetical interventions and explore potential leverage points under
uncertainty. Results suggest that D2D helps distinguish between high- and
low-ranked leverage points. We compare D2D to a data-driven SDM constructed
from the same CLD and variable labeling. D2D showed greater consistency with
the data-driven model than network centrality analysis, while providing
uncertainty estimates and guidance for future data collection. The method is
implemented in an open-source Python package and a web-based application to
support further testing and lower the barrier to dynamic modeling for
researchers working with CLDs. We expect additional validation will further
establish the approach's utility across a broad range of cases and domains.

</details>


### [83] [A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics](https://arxiv.org/abs/2508.05724)
*Massimiliano Romiti*

Main category: cs.LG

TL;DR: 本文提出用加权知识图谱表示和分析物理定律的框架，构建物理方程数据库，训练GAT进行链接预测，表现优异并揭示物理结构和跨领域关系。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的表示和分析物理定律的方法。

Method: 构建物理方程数据库并清理，开发增强图表示，用归一化指标定义边权重，训练Graph Attention Network进行链接预测。

Result: GAT测试AUC为0.9742 +/- 0.0018，显著优于经典启发式和GraphSAGE等架构。

Conclusion: 模型能自主发现物理宏观结构，识别关键枢纽方程，生成跨领域关系假设，可创建特定子领域数据集。

Abstract: This work introduces a novel framework for representing and analyzing
physical laws as a weighted knowledge graph. We constructed a database of 659
distinct physical equations, subjected to rigorous semantic cleaning to resolve
notational ambiguities, resulting in a corpus of 400 advanced physics
equations. We developed an enhanced graph representation where both physical
concepts and equations are nodes, connected by weighted inter-equation bridges.
These weights are objectively defined using normalized metrics for variable
overlap, physics-informed importance scores, and bibliometric data. A Graph
Attention Network (GAT) was trained for link prediction, achieving a test AUC
of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming
both classical heuristics (best baseline AUC: 0.9487) and established GNN
architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing
confirmed significance of all comparisons (p < 0.05), with 2.7% improvement
over the best baseline. Our analysis reveals three key findings: (i) The model
autonomously rediscovers the known macroscopic structure of physics,
identifying strong conceptual axes between Electromagnetism and Statistical
Mechanics. (ii) It identifies central hub equations that serve as critical
bridges between multiple physical domains. (iii) The model generates stable,
computationally-derived hypotheses for cross-domain relationships, identifying
both known principles and suggesting novel mathematical analogies for further
theoretical investigation. The framework can generate hundreds of such
hypotheses, enabling the creation of specialized datasets for targeted analysis
of specific physics subfields. Code and data available at
https://github.com/kingelanci/graphysics

</details>


### [84] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: 提出神经网络nudging方法用于学习非线性状态空间模型的nudging项，并在三个混沌基准问题上评估。


<details>
  <summary>Details</summary>
Motivation: 在非线性情况下设计有效的nudging项极具挑战，需新方法。

Method: 提出数据驱动的神经网络nudging方法，基于Kazantzis - Kravaris - Luenberger观测器理论建立理论存在性结果。

Result: 未提及具体结果，对三个有混沌行为的基准问题进行评估。

Conclusion: 未明确给出结论。

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [85] [From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data](https://arxiv.org/abs/2508.05791)
*Haoran Li,Lihao Mai,Muhao Guo,Jiaqi Wu,Yang Weng,Yannan Sun,Ce Jimmy Liu*

Main category: cs.LG

TL;DR: 本文提出可扩展框架，结合异构数据重建电网拓扑，经实际数据验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 实际电网数据来源多样、质量不一，需可靠方法重建电网拓扑。

Method: 提出可扩展框架，结合物理基础设施空间布局和系统信号域动态行为两方面数据；引入置信度感知推理机制处理数据质量不均问题，同时嵌入物理可行性约束。

Result: 使用Oncor服务区域内3个馈线超8000个电表数据验证，拓扑重建准确率超95%，置信度校准和计算效率较基线方法有显著提升。

Conclusion: 该框架能在实际部署条件下快速收敛到可靠可行动的拓扑结构。

Abstract: Accurate distribution grid topology is essential for reliable modern grid
operations. However, real-world utility data originates from multiple sources
with varying characteristics and levels of quality. In this work, developed in
collaboration with Oncor Electric Delivery, we propose a scalable framework
that reconstructs a trustworthy grid topology by systematically integrating
heterogeneous data. We observe that distribution topology is fundamentally
governed by two complementary dimensions: the spatial layout of physical
infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the
system in the signal domain (e.g., voltage time series). When jointly
leveraged, these dimensions support a complete and physically coherent
reconstruction of network connectivity. To address the challenge of uneven data
quality without compromising observability, we introduce a confidence-aware
inference mechanism that preserves structurally informative yet imperfect
inputs, while quantifying the reliability of each inferred connection for
operator interpretation. This soft handling of uncertainty is tightly coupled
with hard enforcement of physical feasibility: we embed operational
constraints, such as transformer capacity limits and radial topology
requirements, directly into the learning process. Together, these components
ensure that inference is both uncertainty-aware and structurally valid,
enabling rapid convergence to actionable, trustworthy topologies under
real-world deployment conditions. The proposed framework is validated using
data from over 8000 meters across 3 feeders in Oncor's service territory,
demonstrating over 95% accuracy in topology reconstruction and substantial
improvements in confidence calibration and computational efficiency relative to
baseline methods.

</details>


### [86] [SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems](https://arxiv.org/abs/2508.06243)
*Ioan-Sorin Comsa,Purav Shah,Karthik Vaidhyanathan,Deepak Gangadharan,Christof Imhof,Per Bergamin,Aryan Kaushik,Gabriel-Miro Muntean,Ramona Trestian*

Main category: cs.LG

TL;DR: 提出Edge AI辅助框架SCAR优化车联网信息娱乐资源调度和公平性，模拟显示其有良好效果。


<details>
  <summary>Details</summary>
Motivation: 6G网络下车联网信息娱乐业务发展，但传统RRM技术处理车辆数据能力不足。

Method: 采用ML压缩技术减少CQI数据大小，用压缩状态训练6G强化学习策略。

Result: 相比无CQI压缩的RL基线，SCAR使可行调度区域时间增加14%，不公平调度时间减少15%；SAST聚类减少CQI聚类失真10%。

Conclusion: SCAR对动态车联网具有可扩展性和公平性优势。

Abstract: The advent of 6G networks opens new possibilities for connected infotainment
services in vehicular environments. However, traditional Radio Resource
Management (RRM) techniques struggle with the increasing volume and complexity
of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To
address this, we propose SCAR (State-Space Compression for AI-Driven Resource
Management), an Edge AI-assisted framework that optimizes scheduling and
fairness in vehicular infotainment. SCAR employs ML-based compression
techniques (e.g., clustering and RBF networks) to reduce CQI data size while
preserving essential features. These compressed states are used to train
6G-enabled Reinforcement Learning policies that maximize throughput while
meeting fairness objectives defined by the NGMN. Simulations show that SCAR
increases time in feasible scheduling regions by 14\% and reduces unfair
scheduling time by 15\% compared to RL baselines without CQI compression.
Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based
clustering reduces CQI clustering distortion by 10\%, confirming its
efficiency. These results demonstrate SCAR's scalability and fairness benefits
for dynamic vehicular networks.

</details>


### [87] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: 本文为线性编码器 - 解码器架构开发统一理论框架以解决数据驱动的科学机器学习问题，推导最优映射，通过实验验证结果并提供基准。


<details>
  <summary>Details</summary>
Motivation: 非线性神经网络理论不透明，在需要可解释性的场景应用受限，而线性神经网络可用于洞察复杂关系。

Method: 从贝叶斯风险最小化角度分析线性编码器 - 解码器架构，推导封闭形式、秩约束的线性和仿射线性最优映射。

Result: 结果能适应数据、前向算子和测量过程中的秩亏，通过简单生物医学成像、金融因子分析和非线性流体动力学模拟等数据集的实验验证了理论结果。

Conclusion: 为理解和基准测试科学机器学习问题的学习神经网络模型提供了可靠基线。

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


### [88] [Structural Equation-VAE: Disentangled Latent Representations for Tabular Data](https://arxiv.org/abs/2508.06347)
*Ruiyu Zhang,Ce Zhao,Xin Zhao,Lin Nie,Wai-Fung Lam*

Main category: cs.LG

TL;DR: 介绍SE - VAE架构用于从表格数据学习可解释潜在表征，评估显示其表现优于基线模型，架构结构是性能关键。


<details>
  <summary>Details</summary>
Motivation: 从表格数据学习可解释潜在表征在深度生成建模中仍是挑战，需要新方法。

Method: 引入SE - VAE架构，将测量结构嵌入变分自编码器设计，通过设计实现解纠缠。在模拟表格数据集上评估，与基线模型对比。

Result: SE - VAE在因子恢复、可解释性和对干扰变化的鲁棒性上始终优于其他模型，消融实验表明架构结构是性能关键。

Conclusion: SE - VAE为科学和社会领域的白盒生成建模提供了有原则的框架。

Abstract: Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.

</details>


### [89] [An Effective Approach for Node Classification in Textual Graphs](https://arxiv.org/abs/2508.05836)
*Rituparna Datta,Nibir Chandra Mandal*

Main category: cs.LG

TL;DR: 本文提出结合TAPE与Graphormer的框架处理文本属性图的节点分类问题，在ogbn - arxiv数据集上取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本属性图节点分类中难以整合文本语义与图结构信息，存在捕捉特定术语、建模长距离依赖等困难。

Method: 提出结合TAPE与Graphormer的框架，用LLM生成语义解释融入节点表示，用新的集成层结合结构特征，利用Graphormer机制捕捉长距离依赖。

Result: 在ogbn - arxiv数据集上分类准确率达0.772，超越GCN基线，在精确率、召回率和F1分数上表现良好，消融实验验证各组件贡献。

Conclusion: 框架为动态文本属性图节点分类提供可扩展、稳健的解决方案，为未来研究提供方向。

Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks
like citation networks, but effective node classification remains challenging
due to difficulties in integrating rich semantics from text with structural
graph information. Existing methods often struggle with capturing nuanced
domain-specific terminology, modeling long-range dependencies, adapting to
temporal evolution, and scaling to massive datasets. To address these issues,
we propose a novel framework that integrates TAPE (Text-Attributed Graph
Representation Enhancement) with Graphormer. Our approach leverages a large
language model (LLM), specifically ChatGPT, within the TAPE framework to
generate semantically rich explanations from paper content, which are then
fused into enhanced node representations. These embeddings are combined with
structural features using a novel integration layer with learned attention
weights. Graphormer's path-aware position encoding and multi-head attention
mechanisms are employed to effectively capture long-range dependencies across
the citation network. We demonstrate the efficacy of our framework on the
challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a
classification accuracy of 0.772, significantly surpassing the best GCN
baseline of 0.713. Our method also yields strong results in precision (0.671),
recall (0.577), and F1-score (0.610). We validate our approach through
comprehensive ablation studies that quantify the contribution of each
component, demonstrating the synergy between semantic and structural
information. Our framework provides a scalable and robust solution for node
classification in dynamic TAGs, offering a promising direction for future
research in knowledge systems and scientific discovery.

</details>


### [90] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: 本文提出MDP框架和RL - PG算法训练自主制导策略以实现避碰机动，在合成和历史交会事件中验证，能降低燃料消耗并保证碰撞风险。


<details>
  <summary>Details</summary>
Motivation: 在维持可接受碰撞风险的同时，通过提前决策最小化避碰机动的平均燃料消耗。

Method: 将避碰机动建模为连续状态、离散动作和有限时域的MDP，结合相关分析模型，用历史交会事件数据训练马尔可夫策略，进行超参数消融研究。

Result: 在合成交会事件中，训练策略比传统策略显著降低总及每次避碰机动的平均燃料消耗；在历史交会事件中，总燃料消耗增加但每次避碰机动的平均燃料消耗降低，且保证了碰撞风险。

Conclusion: 所提出的框架和算法能有效在避碰机动中平衡燃料消耗和碰撞风险。

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [91] [Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.06247)
*Zichun Ye,Runqi Wang,Xutong Liu,Shuai Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential
decision-making framework, dominated by two algorithmic families: UCB-based and
adversarial methods such as follow the regularized leader (FTRL) and online
mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer
from additional regret factor $\log T$ that is detrimental over long horizons,
while adversarial methods such as EXP3.M and HYBRID impose significant
computational overhead. To resolve this trade-off, we introduce the
Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS
is a computationally efficient algorithm that achieves an instance-independent
regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where
$m$ is the number of arms and $k$ is the maximum cardinality of a feasible
action. Crucially, this result eliminates the dependency on $\log T$ and
matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to
$O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also
applicable to cascading feedback. Experiments on synthetic and real-world
datasets validate that CMOSS consistently outperforms benchmark algorithms in
both regret and runtime efficiency.

</details>


### [92] [The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)](https://arxiv.org/abs/2508.05905)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: 介绍了Signed - Zero Ternary (SZT) 2位量化方法，分析表明其可能提高信息密度。


<details>
  <summary>Details</summary>
Motivation: 从固定整体资源预算角度重新审视量化，突破将量化视为次优近似的传统观点。

Method: 引入Signed - Zero Ternary (SZT) 2位量化方法，该方法能确定性地提供梯度信息且无正向路径惩罚。

Result: 分析显示该方法可能比非量化替代方案提高信息密度。

Conclusion: Signed - Zero Ternary (SZT) 2位量化方法有提高信息密度的潜力。

Abstract: Quantization is usually regarded as a means to trade quality of performance
for reduced compute requirements, i.e., as a suboptimal approximation. However,
if examined in terms of a fixed overall resource budget, a very different
perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit
quantization that deterministically provides gradient information with no
forward-path penalty. Our analysis provides evidence that it may improve
information density compared to non-quantized alternatives.

</details>


### [93] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: 提出FedMeNF解决神经场学习资源需求大及传统FML隐私泄露问题，实验证明其速度快、性能好且保护隐私。


<details>
  <summary>Details</summary>
Motivation: 神经场学习需大量资源，不适用于边缘设备，传统FML存在隐私泄露问题。

Method: 引入FedMeNF，利用新的隐私保护损失函数调节本地元优化中的隐私泄露。

Result: FedMeNF在少样本或非IID数据、多样数据模态下实现快速优化和稳健重建。

Conclusion: FedMeNF能在保护客户端数据隐私的同时，实现快速优化和良好重建性能。

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [94] [Dual Signal Decomposition of Stochastic Time Series](https://arxiv.org/abs/2508.05915)
*Alex Glushkovsky*

Main category: cs.LG

TL;DR: 本文研究随机时间序列分解为均值、离散度信号和噪声，采用机器学习方法，有顺序和联合两种学习方式，可用于平滑、去噪等。


<details>
  <summary>Details</summary>
Motivation: 将随机时间序列分解为代表均值和离散度的双信号并分离噪声。

Method: 应用机器学习拟合双信号，最小化损失函数，引入基于统计过程控制的正则化项权重，有顺序和联合两种学习方式，通过直接非线性无约束优化或神经网络求解。

Result: 联合学习能揭示异方差时间序列的复杂关系，可调整损失函数超参数使噪声平稳无自相关。

Conclusion: 分解后的双信号可用于学习内在结构、预测均值和离散度或分析多时间序列交叉效应。

Abstract: The research paper addresses decomposition of a stochastic time series into
three time series representing a dual signal i.e., the mean and the dispersion,
with noise isolated. Decomposition is done by applying machine learning to fit
a dual signal. Machine learning minimizes the loss function which compromises
between fitting the original time series and penalizing irregularities of the
dual signal. The latter includes terms based on the first and second order
derivatives along time. To preserve special patterns, weighting of the
regularization components of the loss function has been introduced based on
Statistical Process Control methodology. The proposed decomposition can be
applied as a smoothing algorithm against the mean and dispersion of the time
series. By isolating noise, the proposed decomposition can be seen as a
denoising algorithm. Two approaches of the learning process have been
considered: sequential and jointly. The former approach learns the mean signal
first and then dispersion. The latter approach fits the dual signal jointly.
Jointly learning can uncover complex relationships for the time series with
heteroskedasticity. Learning has been set by solving the direct non-linear
unconstrained optimization problem or by applying neural networks that have
sequential or twin output architectures. Tuning of the loss function
hyperparameters focuses on the isolated noise to be a stationary stochastic
process without autocorrelation properties. Depending on the applications, the
hyperparameters of the learning can be tuned towards either the discrete states
by stepped signal or smoothed series. The decomposed dual signal can be
represented on the 2D space and used to learn inherent structures, to forecast
both mean and dispersion, or to analyze cross effects in case of multiple time
series.

</details>


### [95] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: 研究神经PDE求解器精度问题，提出Shifted Gaussian Encoding方法提升性能，指出条件数而非深度是瓶颈。


<details>
  <summary>Details</summary>
Motivation: 解决神经PDE求解器因病态问题导致的优化不佳，精度下降问题，尤其在多保真度和刚性问题中。

Method: 研究PIELMs，引入Shifted Gaussian Encoding激活过滤步骤，增加矩阵秩和表达能力并保持凸性。

Result: 将稳态对流扩散方程中Peclet数可解范围扩大两个数量级以上，多频函数学习误差降低六个数量级，比百万参数深度网络更准确快速拟合高保真图像向量。

Conclusion: 条件数而非深度常是科学神经求解器的瓶颈，简单架构改变可带来显著提升。

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [96] [Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting](https://arxiv.org/abs/2508.05928)
*Si Shen,Peijun Shen,Wenhua Zhao,Danhao Zhu*

Main category: cs.LG

TL;DR: 现有GRPO技术存在Think - Answer Mismatch问题，本文提出S - GRPO解决该问题，实验表明其有效且鲁棒。


<details>
  <summary>Details</summary>
Motivation: GRPO技术存在Think - Answer Mismatch问题，在不平衡响应组中问题严重，影响学习过程，需要改进。

Method: 提出Stable Group - Relative Policy Optimization (S - GRPO)，推导最优、考虑噪声的优势权重来稳定训练。

Result: 在数学推理基准测试中，S - GRPO在多种模型上显著优于DR. GRPO，如在Qwen - Math - 7B - Base等模型上有性能提升，且在20%合成奖励噪声下能保持稳定学习。

Conclusion: S - GRPO在大规模推理模型训练中具有更强大和有效的潜力。

Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training
large reasoning models, yet it suffers from a critical vulnerability: the
\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning
process. This problem is most severe in unbalanced response groups,
paradoxically degrading the signal precisely when it should be most
informative. To address this challenge, we propose Stable Group-Relative Policy
Optimization (S-GRPO), a principled enhancement that derives optimal,
noise-aware advantage weights to stabilize training. Our comprehensive
experiments on mathematical reasoning benchmarks demonstrate S-GRPO's
effectiveness and robustness. On various models, S-GRPO significantly
outperforms DR. GRPO, achieving performance gains of +2.5% on
Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on
Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn
under 20% synthetic reward noise, S-GRPO maintains stable learning progress.
These results highlight S-GRPO's potential for more robust and effective
training of large-scale reasoning models. \footnote{Code and data are available
at: https://github.com/shenpeijun0212/S-GRPO

</details>


### [97] [Multi-Armed Bandits-Based Optimization of Decision Trees](https://arxiv.org/abs/2508.05957)
*Hasibul Karim Shanto,Umme Ayman Koana,Shadikur Rahman*

Main category: cs.LG

TL;DR: 传统决策树剪枝方法有局限，本文提出基于多臂老虎机（MAB）的剪枝方法，实验表明该方法预测性能更好。


<details>
  <summary>Details</summary>
Motivation: 传统决策树易过拟合，传统剪枝方法基于贪心策略，长期泛化性不佳，尤其是处理小而复杂数据集时。

Method: 将剪枝过程视为探索 - 利用问题，使用MAB算法根据每次剪枝动作的反馈找到最优分支节点进行剪枝。

Result: 在多个基准数据集上的实验显示，所提方法比传统方法有更好的预测性能。

Conclusion: 利用MAB进行动态和概率性的决策树剪枝有潜力优化基于决策树的模型。

Abstract: Decision trees, without appropriate constraints, can easily become overly
complex and prone to overfit, capturing noise rather than generalizable
patterns. To resolve this problem,pruning operation is a crucial part in
optimizing decision trees, as it not only reduces the complexity of trees but
also decreases the probability of generating overfit models. The conventional
pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning
(REP) are mostly based on greedy approaches that focus on immediate gains in
performance while pruning nodes of the decision tree. However, this might
result in a lower generalization in the long run, compromising the robust
ability of the tree model when introduced to unseen data samples, particularly
when trained with small and complex datasets. To address this challenge, we are
proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement
learning (RL)-based technique, that will dynamically prune the tree to generate
an optimal decision tree with better generalization. Our proposed approach
assumes the pruning process as an exploration-exploitation problem, where we
are utilizing the MAB algorithms to find optimal branch nodes to prune based on
feedback from each pruning actions. Experimental evaluation on several
benchmark datasets, demonstrated that our proposed approach results in better
predictive performance compared to the traditional ones. This suggests the
potential of utilizing MAB for a dynamic and probabilistic way of decision tree
pruning, in turn optimizing the decision tree-based model.

</details>


### [98] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: 提出MCRE框架和MCRQ算法解决离线强化学习分布偏移问题，实验显示MCRQ表现优于基线和先进算法。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中学习策略和行为策略间分布偏移导致的OOD动作和高估问题，避免过度保守影响性能。

Method: 提出MCRE框架，结合时间差分误差和行为克隆项平衡保守性和性能；开发MCRQ算法，将MCRE集成到离策略演员-评论家框架。

Result: MCRQ在基准数据集上优于强基线和最先进的离线强化学习算法。

Conclusion: MCRQ算法能有效解决离线强化学习分布偏移问题，在性能上有优势。

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [99] [LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning](https://arxiv.org/abs/2508.05977)
*Aoming Liang,Chi Cheng,Dashuai Chen,Boai Sun,Dixia Fan*

Main category: cs.LG

TL;DR: 本文提出语义对齐强化学习方法，以解决科学机器学习中强化学习奖励函数设计难题，实验证明该方法有效并为LLMs与控制应用集成奠定基础。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习中强化学习在任务目标难数值化的环境下，设计有效奖励函数是挑战，现有方法多基于启发式、手动工程或特定任务调整。

Method: 引入语义对齐强化学习方法，用SBERT将当前状态与目标语义指令对齐计算奖励，策略根据目标文本描述与情节中陈述描述的余弦相似度获得反馈。

Result: 在多个环境中评估表明，即使没有手工设计的奖励函数，语义奖励也能引导学习实现有竞争力的控制行为。

Conclusion: 该研究证明语言嵌入空间与传统欧几里得空间存在相关性，为使智能体行为与自然语言目标对齐开辟新视野，为LLMs与流体控制应用更无缝集成奠定基础。

Abstract: In the domain of scientific machine learning, designing effective reward
functions remains a challenge in reinforcement learning (RL), particularly in
environments where task goals are difficult to specify numerically. Reward
functions in existing work are predominantly based on heuristics, manual
engineering, or task-specific tuning. In this work, we introduce a semantically
aligned reinforcement learning method where rewards are computed by aligning
the current state with a target semantic instruction using a
Sentence-Bidirectional Encoder Representations from Transformers (SBERT).
Instead of relying on manually defined reward functions, the policy receives
feedback based on the reward, which is a cosine similarity between the goal
textual description and the statement description in the episode. We evaluated
our approach in several environments and showed that semantic reward can guide
learning to achieve competitive control behavior, even in the absence of
hand-crafted reward functions. Our study demonstrates a correlation between the
language embedding space and the conventional Euclidean space. This framework
opens new horizons for aligning agent behavior with natural language goals and
lays the groundwork for a more seamless integration of larger language models
(LLMs) and fluid control applications.

</details>


### [100] [Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning](https://arxiv.org/abs/2508.05984)
*Ankur Naskar,Gugan Thoppe,Vijay Gupta*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as
average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve
semi-norm contractions. Achieving parameter-free optimal convergence rates for
these methods via Polyak--Ruppert averaging has remained elusive, largely due
to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting
the averaged error as a linear recursion involving a nonlinear perturbation,
and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with
the monotonicity of a suitably induced norm. Our main result yields the first
parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both
average-reward and exponentially discounted settings, where $t$ denotes the
iteration index. The result applies within a broad framework that accommodates
synchronous and asynchronous updates, single-agent and distributed deployments,
and data streams obtained either from simulators or along Markovian
trajectories.

</details>


### [101] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: 本文提出ASAP框架用于压缩Chain-of-Thought，降低训练和推理成本，在多基准测试中取得SOTA准确率。


<details>
  <summary>Details</summary>
Motivation: 现有CoT压缩方法存在权衡问题，长推理轨迹带来训练成本、推理延迟和部署可行性挑战。

Method: 提出ASAP框架，先进行锚定引导剪枝保留核心推理结构，再基于首令牌意外性指标进行逻辑感知剪枝，让模型在推理时自主生成和利用简洁CoT。

Result: 在多代码生成基准测试中取得SOTA准确率，在LiveCodeBench v4_v5基准上，减少23.5%的令牌生成和43.5%的推理延迟，Pass@1准确率达36.19%。

Conclusion: 为构建强大高效的大型推理模型指明了有前景的方向。

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [102] [Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization](https://arxiv.org/abs/2508.05995)
*Fei Xu Yu,Gina Adam,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: 提出MCTS - OPS框架提升大语言模型在复杂任务代码生成质量和解决问题能力，实验效果显著。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂多步规划任务中性能下降，现有结合蒙特卡罗树搜索的方法有局限性。

Method: 提出MCTS - OPS框架，将提示选择作为由蒙特卡罗树搜索引导的顺序决策过程，探索和优化多步提示序列。

Result: 网络优化实验中，生成代码执行成功率、优化结果、获得最优解的机会均优于基线方法。

Conclusion: 符号规划与大语言模型结合在复杂领域高质量代码生成方面有前景。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.

</details>


### [103] [Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients](https://arxiv.org/abs/2508.06023)
*Xiaobin Shen,Jonathan Elmer,George H. Chen*

Main category: cs.LG

TL;DR: 提出新的逐步动态竞争风险模型，用于改善心脏骤停后昏迷患者神经结局预测，在回顾性队列中表现良好，可推广到多阶段特征收集和其他动态预测任务。


<details>
  <summary>Details</summary>
Motivation: 心脏骤停后昏迷患者的预后预测是ICU临床决策的关键挑战，现有临床信息分阶段收集，需更好利用不同阶段特征进行预测。

Method: 提出逐步动态竞争风险模型，扩展标准Fine和Gray模型，结合神经网络捕捉复杂非线性特征关系。

Result: 在2278例患者的回顾性队列中，模型对苏醒、撤掉维持生命治疗和最大支持下死亡等竞争结局有强大判别性能。

Conclusion: 该方法可推广到多阶段特征收集情况，适用于其他动态预测任务，有助于了解新特征何时及对谁能显著改善预测。

Abstract: Prognostication for comatose post-cardiac arrest patients is a critical
challenge that directly impacts clinical decision-making in the ICU. Clinical
information that informs prognostication is collected serially over time.
Shortly after cardiac arrest, various time-invariant baseline features are
collected (e.g., demographics, cardiac arrest characteristics). After ICU
admission, additional features are gathered, including time-varying hemodynamic
data (e.g., blood pressure, doses of vasopressor medications). We view these as
two phases in which we collect new features. In this study, we propose a novel
stepwise dynamic competing risks model that improves the prediction of
neurological outcomes by automatically determining when to take advantage of
time-invariant features (first phase) and time-varying features (second phase).
Notably, our model finds patients for whom this second phase (time-varying
hemodynamic) information is beneficial for prognostication and also when this
information is beneficial (as we collect more hemodynamic data for a patient
over time, how important these data are for prognostication varies). Our
approach extends the standard Fine and Gray model to explicitly model the two
phases and to incorporate neural networks to flexibly capture complex nonlinear
feature relationships. Evaluated on a retrospective cohort of 2,278 comatose
post-arrest patients, our model demonstrates robust discriminative performance
for the competing outcomes of awakening, withdrawal of life-sustaining therapy,
and death despite maximal support. Our approach generalizes to more than two
phases in which new features are collected and could be used in other dynamic
prediction tasks, where it may be helpful to know when and for whom newly
collected features significantly improve prediction.

</details>


### [104] [Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity](https://arxiv.org/abs/2508.06034)
*Qin Chen,Guojie Song*

Main category: cs.LG

TL;DR: 现有HG研究忽视异质性和异质性并存问题，提出AHGNN模型，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有研究孤立处理异质性和异质性，忽视异质性HG在实际应用中的普遍性，导致性能下降。

Method: 提出AHGNN，采用异质性感知卷积，使用粗到细的注意力机制整合消息。

Result: 在七个真实图和二十个基线实验中，AHGNN表现优越，尤其在高异质性情况下。

Conclusion: AHGNN能有效处理异质性HG建模中的挑战。

Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often
exhibit heterophily. However, most existing studies focus on either
heterogeneity or heterophily in isolation, overlooking the prevalence of
heterophilic HGs in practical applications. Such ignorance leads to their
performance degradation. In this work, we first identify two main challenges in
modeling heterophily HGs: (1) varying heterophily distributions across hops and
meta-paths; (2) the intricate and often heterophily-driven diversity of
semantic information across different meta-paths. Then, we propose the Adaptive
Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN
employs a heterophily-aware convolution that accounts for heterophily
distributions specific to both hops and meta-paths. It then integrates messages
from diverse semantic spaces using a coarse-to-fine attention mechanism, which
filters out noise and emphasizes informative signals. Experiments on seven
real-world graphs and twenty baselines demonstrate the superior performance of
AHGNN, particularly in high-heterophily situations.

</details>


### [105] [DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment](https://arxiv.org/abs/2508.06041)
*Sangwoo Kwon,Seong Hoon Seo,Jae W. Lee,Yeonhong Park*

Main category: cs.LG

TL;DR: 提出DP - LLM机制处理设备端大语言模型查询，动态分配层精度，实验表明性能优于先前方法


<details>
  <summary>Details</summary>
Motivation: 解决设备端大语言模型在不同运行时约束下的查询处理问题，以及模型如何匹配目标精度或延迟

Method: 引入DP - LLM机制，为LLM的每个线性层增加精度选择器，通过轻量级误差估计器和微调学习的阈值在运行时确定位宽

Result: 在多个模型和基准测试中，DP - LLM实现了更好的性能 - 延迟权衡，优于先前方法

Conclusion: DP - LLM在处理设备端大语言模型查询方面具有优势

Abstract: How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding iterations. Building on this
insight, we introduce DP-LLM, a novel mechanism that dynamically assigns
precision to each layer based on input values. DP-LLM augments each linear
layer in an LLM with a precision selector that determines the bitwidth at
runtime using a lightweight error estimator and threshold values learned
through fine-tuning. Experimental results across multiple models and benchmarks
demonstrate that DP-LLM achieves a superior performance-latency trade-off,
outperforming prior approaches.

</details>


### [106] [Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology](https://arxiv.org/abs/2508.06066)
*Barak Gahtan,Alex M. Bronstein*

Main category: cs.LG

TL;DR: 本文针对深度时间模型泛化理论理解不足问题，给出泛化边界、评估方法，研究依赖序列泛化表现，揭示时间依赖可提升学习效果，指出理论与实践差距。


<details>
  <summary>Details</summary>
Motivation: 现有深度时间架构如TCNs泛化理论理解有限，需填补此空白。

Method: 为指数β混合序列推导泛化边界，用延迟反馈阻塞机制处理相关样本，引入公平比较方法。

Result: 得到泛化边界缩放形式；处理相关样本使深度缩放变为√D；强依赖序列泛化差距比弱依赖小；收敛率与理论不同。

Conclusion: 时间依赖在固定信息预算下可提升学习效果，理论与实践存在差距，需进一步研究。

Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.

</details>


### [107] [Recurrent Deep Differentiable Logic Gate Networks](https://arxiv.org/abs/2508.06097)
*Simon Bührer,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文首次实现循环深度可微逻辑门网络（RDDLGN）用于序列到序列学习，在英德翻译任务上评估，性能接近GRU，证明基于循环逻辑的神经计算可行。


<details>
  <summary>Details</summary>
Motivation: 可微逻辑门在顺序建模中的应用未被探索，尝试将布尔运算与循环架构结合用于顺序建模。

Method: 实现循环深度可微逻辑门网络（RDDLGN）进行序列到序列学习。

Result: 在WMT'14英德翻译任务上，训练时达到5.00 BLEU和30.9%准确率，推理时接近GRU性能，BLEU为4.39。

Conclusion: 基于循环逻辑的神经计算是可行的，为顺序建模中的FPGA加速和其他递归网络架构研究开辟方向。

Abstract: While differentiable logic gates have shown promise in feedforward networks,
their application to sequential modeling remains unexplored. This paper
presents the first implementation of Recurrent Deep Differentiable Logic Gate
Networks (RDDLGN), combining Boolean operations with recurrent architectures
for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and
30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and
graceful degradation (4.39 BLEU) during inference. This work establishes
recurrent logic-based neural computation as viable, opening research directions
for FPGA acceleration in sequential modeling and other recursive network
architectures.

</details>


### [108] [GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2508.06108)
*Xing Lei,Wenyan Yang,Kaiqiang Ke,Shentao Yang,Xuetao Zhang,Joni Pajarinen,Donglin Wang*

Main category: cs.LG

TL;DR: 提出HGR技术结合HSR，提升目标条件强化学习样本效率，在导航和操作任务中表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有目标条件强化学习方法仅靠轨迹重标记未充分利用经验，样本效率有限。

Method: 提出基于后视目标生成动作正则化先验的HGR技术，并与后视自我模仿正则化（HSR）结合。

Result: 与现有采用HER和自我模仿技术的GCRL方法相比，后视正则化实现了更高效的样本重用和最佳性能。

Conclusion: 所提方法能使离策略RL算法最大化经验利用。

Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a
fundamental challenge in reinforcement learning. While hindsight experience
replay (HER) has shown promise by relabeling collected trajectories with
achieved goals, we argue that trajectory relabeling alone does not fully
exploit the available experiences in off-policy GCRL methods, resulting in
limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned
Regularization (HGR), a technique that generates action regularization priors
based on hindsight goals. When combined with hindsight self-imitation
regularization (HSR), our approach enables off-policy RL algorithms to maximize
experience utilization. Compared to existing GCRL methods that employ HER and
self-imitation techniques, our hindsight regularizations achieve substantially
more efficient sample reuse and the best performances, which we empirically
demonstrate on a suite of navigation and manipulation tasks.

</details>


### [109] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: 研究用微调扩散模型的修复技术合成口腔癌病变图像，提高诊断模型性能，分类和检测模型有高准确率，验证合成图像在医疗诊断潜力。


<details>
  <summary>Details</summary>
Motivation: 解决口腔癌诊断中注释数据集有限、训练数据不足和多变性问题，提升诊断模型性能。

Method: 采用微调扩散模型的修复技术合成逼真口腔癌病变，从多源编译综合数据集。

Result: 分类模型区分癌组织和非癌组织诊断准确率达0.97，检测模型识别病变位置准确率达0.85。

Conclusion: 该方法验证了合成图像生成在医疗诊断中的潜力，为拓展到其他癌症诊断研究奠定基础。

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [110] [Differentially Private Federated Clustering with Random Rebalancing](https://arxiv.org/abs/2508.06183)
*Xiyuan Yang,Shengyuan Hu,Soyeon Kim,Tian Li*

Main category: cs.LG

TL;DR: 提出RR - Cluster技术改进联邦聚类隐私/效用权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦聚类易隐私泄露，直接应用差分隐私机制会显著降低效用，原因是同一聚类中客户端数量不可控，难以平均隐私噪声。

Method: 提出RR - Cluster技术，通过随机重新平衡聚类分配减少隐私噪声，保证每个聚类中最少客户端数量，并分析噪声方差降低和错误分配偏差增加的权衡，给出收敛边界。

Result: 将RR - Cluster应用于强联邦聚类算法，在合成和真实数据集上显著改善隐私/效用权衡。

Conclusion: RR - Cluster作为轻量级插件，能有效提升联邦聚类的隐私/效用表现。

Abstract: Federated clustering aims to group similar clients into clusters and produce
one model for each cluster. Such a personalization approach typically improves
model performance compared with training a single model to serve all clients,
but can be more vulnerable to privacy leakage. Directly applying client-level
differentially private (DP) mechanisms to federated clustering could degrade
the utilities significantly. We identify that such deficiencies are mainly due
to the difficulties of averaging privacy noise within each cluster (following
standard privacy mechanisms), as the number of clients assigned to the same
clusters is uncontrolled. To this end, we propose a simple and effective
technique, named RR-Cluster, that can be viewed as a light-weight add-on to
many federated clustering algorithms. RR-Cluster achieves reduced privacy noise
via randomly rebalancing cluster assignments, guaranteeing a minimum number of
clients assigned to each cluster. We analyze the tradeoffs between decreased
privacy noise variance and potentially increased bias from incorrect
assignments and provide convergence bounds for RR-Clsuter. Empirically, we
demonstrate the RR-Cluster plugged into strong federated clustering algorithms
results in significantly improved privacy/utility tradeoffs across both
synthetic and real-world datasets.

</details>


### [111] [Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient](https://arxiv.org/abs/2304.04475)
*Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: 文章针对疫情干预措施建模研究的局限，用DDPG框架在大规模仿真中进行多目标优化，得出无封锁和特定人群接种下的结果，需深入仿真验证。


<details>
  <summary>Details</summary>
Motivation: 当前疫情干预措施建模研究存在受仿真目标、规模、模型类型和干预策略数量限制的问题，需解决这些挑战。

Method: 使用基于深度确定性策略梯度（DDPG）的策略优化框架，在大规模（10万个体）流行病学基于智能体的仿真中进行多目标优化。

Result: 在无封锁和特定人群（中年和老年人）接种疫苗的情况下，能实现最优经济（贫困人口）和平衡的健康目标（感染和住院情况）。

Conclusion: 需要进行深入仿真来进一步验证结果并开源框架。

Abstract: To mitigate the impact of the pandemic, several measures include lockdowns,
rapid vaccination programs, school closures, and economic stimulus. These
interventions can have positive or unintended negative consequences. Current
research to model and determine an optimal intervention automatically through
round-tripping is limited by the simulation objectives, scale (a few thousand
individuals), model types that are not suited for intervention studies, and the
number of intervention strategies they can explore (discrete vs continuous). We
address these challenges using a Deep Deterministic Policy Gradient (DDPG)
based policy optimization framework on a large-scale (100,000 individual)
epidemiological agent-based simulation where we perform multi-objective
optimization. We determine the optimal policy for lockdown and vaccination in a
minimalist age-stratified multi-vaccine scenario with a basic simulation for
economic activity. With no lockdown and vaccination (mid-age and elderly),
results show optimal economy (individuals below the poverty line) with balanced
health objectives (infection, and hospitalization). An in-depth simulation is
needed to further validate our results and open-source our framework.

</details>


### [112] [Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning](https://arxiv.org/abs/2508.06199)
*Mateusz Praski,Jakub Adamczyk,Wojciech Czech*

Main category: cs.LG

TL;DR: 本文对25个预训练神经网络模型在25个数据集上进行评估，发现多数模型相比基线无显著提升，仅CLAMP模型表现更好，引发对现有研究评估严谨性的担忧。


<details>
  <summary>Details</summary>
Motivation: 对预训练神经网络模型在化学和小分子药物设计中的应用进行全面比较评估。

Method: 在公平比较框架下评估25个模型，使用分层贝叶斯统计测试模型。

Result: 几乎所有神经模型相比基线ECFP分子指纹无显著改善，仅CLAMP模型表现更好。

Conclusion: 现有研究评估严谨性存疑，讨论了潜在原因、提出解决方案并给出实用建议。

Abstract: Pretrained neural networks have attracted significant interest in chemistry
and small molecule drug design. Embeddings from these models are widely used
for molecular property prediction, virtual screening, and small data learning
in molecular chemistry. This study presents the most extensive comparison of
such models to date, evaluating 25 models across 25 datasets. Under a fair
comparison framework, we assess models spanning various modalities,
architectures, and pretraining strategies. Using a dedicated hierarchical
Bayesian statistical testing model, we arrive at a surprising result: nearly
all neural models show negligible or no improvement over the baseline ECFP
molecular fingerprint. Only the CLAMP model, which is also based on molecular
fingerprints, performs statistically significantly better than the
alternatives. These findings raise concerns about the evaluation rigor in
existing studies. We discuss potential causes, propose solutions, and offer
practical recommendations.

</details>


### [113] [Graph Federated Learning for Personalized Privacy Recommendation](https://arxiv.org/abs/2508.06208)
*Ce Na,Kai Yang,Dengzhao Fang,Yu Li,Jingtong Gao,Chengcheng Zhu,Jiale Zhang,Xiaobing Sun,Yi Chang*

Main category: cs.LG

TL;DR: 提出GFed - PP模型，结合公共用户交互数据，用轻量GCN学习用户特定嵌入，保护隐私，实验显示其在五个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统假设所有用户隐私保护要求相同，忽略利用公共用户数据提升推荐服务的潜力，本文旨在解决该问题。

Method: 提出GFed - PP模型，结合公共用户交互数据构建用户 - 项目交互图和用户关系图，用轻量GCN学习用户特定嵌入，客户端本地学习用户嵌入和评分函数，通过初始化项目嵌入和聚合用户关系图优化框架。

Result: 在五个数据集上，GFed - PP显著优于现有方法，推荐准确性高且不损害隐私。

Conclusion: 该框架为联邦推荐系统适应不同隐私偏好提供了实用解决方案。

Abstract: Federated recommendation systems (FedRecs) have gained significant attention
for providing privacy-preserving recommendation services. However, existing
FedRecs assume that all users have the same requirements for privacy
protection, i.e., they do not upload any data to the server. The approaches
overlook the potential to enhance the recommendation service by utilizing
publicly available user data. In real-world applications, users can choose to
be private or public. Private users' interaction data is not shared, while
public users' interaction data can be shared. Inspired by the issue, this paper
proposes a novel Graph Federated Learning for Personalized Privacy
Recommendation (GFed-PP) that adapts to different privacy requirements while
improving recommendation performance. GFed-PP incorporates the interaction data
of public users to build a user-item interaction graph, which is then used to
form a user relationship graph. A lightweight graph convolutional network (GCN)
is employed to learn each user's user-specific personalized item embedding. To
protect user privacy, each client learns the user embedding and the scoring
function locally. Additionally, GFed-PP achieves optimization of the federated
recommendation framework through the initialization of item embedding on
clients and the aggregation of the user relationship graph on the server.
Experimental results demonstrate that GFed-PP significantly outperforms
existing methods for five datasets, offering superior recommendation accuracy
without compromising privacy. This framework provides a practical solution for
accommodating varying privacy preferences in federated recommendation systems.

</details>


### [114] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: 提出AttriLens - Mol框架用于分子属性预测，实验表明该方法能提升模型性能和可解释性，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在分子属性预测中依赖人工提示和模板，高级推理模型推理冗长且缺乏相关性。

Method: 引入AttriLens - Mol框架，使用格式奖励、计数奖励和合理性奖励引导模型推理。

Result: 在分布内和分布外数据集上训练模型，性能显著提升，优于监督微调模型和高级模型；提取的属性用于决策树模型性能更好。

Conclusion: AttriLens - Mol能有效引出相关且有预测性的分子属性，增强属性预测的可解释性和性能。

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [115] [Reparameterization Proximal Policy Optimization](https://arxiv.org/abs/2508.06214)
*Hai Zhong,Xun Wang,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: 本文提出基于RPG的稳定且样本高效方法RPO，在运动和操作任务中取得更好的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决RPG训练不稳定、梯度方差大的问题。

Method: 建立PPO代理目标与RPG的联系，提出RPO，通过优化裁剪代理目标、KL散度正则化等实现稳定样本复用。

Result: 在一系列具有挑战性的运动和操作任务中，RPO实现了更好的样本效率和性能。

Conclusion: RPO是一种稳定且样本高效的基于RPG的方法。

Abstract: Reparameterization policy gradient (RPG) is promising for improving sample
efficiency by leveraging differentiable dynamics. However, a critical barrier
is its training instability, where high-variance gradients can destabilize the
learning process. To address this, we draw inspiration from Proximal Policy
Optimization (PPO), which uses a surrogate objective to enable stable sample
reuse in the model-free setting. We first establish a connection between this
surrogate objective and RPG, which has been largely unexplored and is
non-trivial. Then, we bridge this gap by demonstrating that the
reparameterization gradient of a PPO-like surrogate objective can be computed
efficiently using backpropagation through time. Based on this key insight, we
propose Reparameterization Proximal Policy Optimization (RPO), a stable and
sample-efficient RPG-based method. RPO enables multiple epochs of stable sample
reuse by optimizing a clipped surrogate objective tailored for RPG, while being
further stabilized by Kullback-Leibler (KL) divergence regularization and
remaining fully compatible with existing variance reduction methods. We
evaluate RPO on a suite of challenging locomotion and manipulation tasks, where
experiments demonstrate that our method achieves superior sample efficiency and
strong performance.

</details>


### [116] [Membership Inference Attack with Partial Features](https://arxiv.org/abs/2508.06244)
*Xurun Wang,Guangrui Liu,Xinjie Li,Haoyu He,Lin Yao,Weizhe Zhang*

Main category: cs.LG

TL;DR: 研究部分特征成员推理问题，提出MRAD攻击框架，实验证明其有效性和兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理方法假设对手能获取目标样本全部特征，在很多现实场景不成立，限制了方法适用性，需研究部分特征推理场景。

Method: 提出MRAD两阶段攻击框架，第一阶段优化未知特征值使样本损失最小，第二阶段用异常检测衡量重构样本与训练分布的偏差。

Result: MRAD在一系列数据集上有效，能与多种现成异常检测技术兼容，如在STL - 10上缺失40%特征时AUC约为0.6。

Conclusion: MRAD是解决部分特征成员推理问题的有效方法。

Abstract: Machine learning models have been shown to be susceptible to membership
inference attack, which can be used to determine whether a given sample appears
in the training data. Existing membership inference methods commonly assume
that the adversary has full access to the features of the target sample. This
assumption, however, does not hold in many real-world scenarios where only
partial features information is available, thereby limiting the applicability
of these methods. In this work, we study an inference scenario where the
adversary observes only partial features of each sample and aims to infer
whether this observed subset was present in the training set of the target
model. We define this problem as Partial Feature Membership Inference (PFMI).
To address this problem, we propose MRAD (Memory-guided Reconstruction and
Anomaly Detection), a two-stage attack framework. In the first stage, MRAD
optimizes the unknown feature values to minimize the loss of the sample. In the
second stage, it measures the deviation between the reconstructed sample and
the training distribution using anomaly detection. Empirical results
demonstrate that MRAD is effective across a range of datasets, and maintains
compatibility with various off-the-shelf anomaly detection techniques. For
example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of
the missing features.

</details>


### [117] [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249)
*David Kaczér,Magnus Jørgenvåg,Clemens Vetter,Lucie Flek,Florian Mai*

Main category: cs.LG

TL;DR: 研究针对大语言模型微调中出现的新兴失准问题，提出四种训练正则化干预方法并评估效果，最后讨论研究中的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调会出现新兴失准，即使使用微调API隐藏模型权重也会让攻击者访问到广泛失准的模型，且难以仅从微调数据检测，因此要研究针对新兴失准的训练防护措施。

Method: 提出四种训练正则化干预方法，包括向安全参考模型的KL散度正则化、特征空间的l2距离、投影到安全子空间（SafeLoRA）以及插入通用指令调优数据集中的少量安全训练示例。

Result: 先评估方法在四种恶意、引发新兴失准任务中的新兴失准效果，再评估对良性任务的影响。

Conclusion: 讨论了新兴失准研究中的开放性问题。

Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs)
for new domains, yet recent work reveals emergent misalignment (EMA): Even a
small, domain-specific fine-tune can induce harmful behaviors far outside the
target domain. Even in the case where model weights are hidden behind a
fine-tuning API, this gives attackers inadvertent access to a broadly
misaligned model in a way that can be hard to detect from the fine-tuning data
alone. We present the first systematic study of in-training safeguards against
EMA that are practical for providers who expose fine-tuning via an API. We
investigate four training regularization interventions: (i) KL-divergence
regularization toward a safe reference model, (ii) $\ell_2$ distance in feature
space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving
of a small amount of safe training examples from a general instruct-tuning
dataset. We first evaluate the methods' emergent misalignment effect across
four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on
benign tasks. We conclude with a discussion of open questions in emergent
misalignment research.

</details>


### [118] [Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)](https://arxiv.org/abs/2508.06251)
*Alejandro Moreno R.,Desale Fentaw,Samuel Palmer,Raúl Salles de Padua,Ninad Dixit,Samuel Mugel,Roman Orús,Manuel Radons,Josef Menter,Ali Abedi*

Main category: cs.LG

TL;DR: 本文提出用张量网络（MPS）生成隐私保护的高质量合成表格数据，经与SOTA模型对比，MPS在严格隐私约束下表现更优，是隐私感知合成数据生成的有前景工具。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺、隐私约束以及训练鲁棒模型对多样化数据集的需求。

Method: 使用MPS生成合成表格数据，与CTGAN、VAE和PrivBayes等模型进行基准测试，训练时集成噪声注入和梯度裁剪以确保差分隐私。

Result: 在多个数据保真度和下游机器学习任务性能指标上，MPS尤其在严格隐私约束下优于经典模型。

Conclusion: MPS是隐私感知合成数据生成的有前景工具，结合张量网络表示能力与隐私机制，提供可解释且可扩展的安全数据共享方案，便于集成到对数据质量和保密性要求高的领域。

Abstract: Synthetic data generation is a key technique in modern artificial
intelligence, addressing data scarcity, privacy constraints, and the need for
diverse datasets in training robust models. In this work, we propose a method
for generating privacy-preserving high-quality synthetic tabular data using
Tensor Networks, specifically Matrix Product States (MPS). We benchmark the
MPS-based generative model against state-of-the-art models such as CTGAN, VAE,
and PrivBayes, focusing on both fidelity and privacy-preserving capabilities.
To ensure differential privacy (DP), we integrate noise injection and gradient
clipping during training, enabling privacy guarantees via R\'enyi Differential
Privacy accounting. Across multiple metrics analyzing data fidelity and
downstream machine learning task performance, our results show that MPS
outperforms classical models, particularly under strict privacy constraints.
This work highlights MPS as a promising tool for privacy-aware synthetic data
generation. By combining the expressive power of tensor network representations
with formal privacy mechanisms, the proposed approach offers an interpretable
and scalable alternative for secure data sharing. Its structured design
facilitates integration into sensitive domains where both data quality and
confidentiality are critical.

</details>


### [119] [Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors](https://arxiv.org/abs/2508.06257)
*Jielong Lu,Zhihao Wu,Jiajun Yu,Jiajun Bu,Haishuai Wang*

Main category: cs.LG

TL;DR: 提出GTMancer框架用于多组学癌症亚型分类，利用对比学习和注意力系数，在七个真实癌症数据集上表现优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有多组学癌症亚型分类方法常忽略异构组学间复杂耦合，限制解决癌症亚型异质性的能力，需新方法。

Method: 提出GTMancer框架，基于GNN优化问题，利用对比学习将多组学数据嵌入统一语义空间，展开多路图优化问题并引入两组注意力系数。

Result: 在七个真实世界癌症数据集上，GTMancer优于现有最先进算法。

Conclusion: GTMancer框架能有效处理多组学数据进行癌症亚型分类，具有更好性能。

Abstract: Integrating multi-omics datasets through data-driven analysis offers a
comprehensive understanding of the complex biological processes underlying
various diseases, particularly cancer. Graph Neural Networks (GNNs) have
recently demonstrated remarkable ability to exploit relational structures in
biological data, enabling advances in multi-omics integration for cancer
subtype classification. Existing approaches often neglect the intricate
coupling between heterogeneous omics, limiting their capacity to resolve subtle
cancer subtype heterogeneity critical for precision oncology. To address these
limitations, we propose a framework named Graph Transformer for Multi-omics
Cancer Subtype Classification (GTMancer). This framework builds upon the GNN
optimization problem and extends its application to complex multi-omics data.
Specifically, our method leverages contrastive learning to embed multi-omics
data into a unified semantic space. We unroll the multiplex graph optimization
problem in that unified space and introduce dual sets of attention coefficients
to capture structural graph priors both within and among multi-omics data. This
approach enables global omics information to guide the refining of the
representations of individual omics. Empirical experiments on seven real-world
cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art
algorithms.

</details>


### [120] [OM2P: Offline Multi-Agent Mean-Flow Policy](https://arxiv.org/abs/2508.06269)
*Zhuoran Li,Xun Wang,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: 提出OM2P算法解决离线多智能体强化学习中生成模型的问题，在基准测试中表现优异，首次将平均流模型集成到离线多智能体强化学习中。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和基于流的策略在离线多智能体强化学习中存在采样效率低问题，无法用于时间敏感或资源受限场景，且生成目标与奖励最大化存在偏差。

Method: 提出OM2P算法实现高效一步动作采样，引入奖励感知优化方案，设计广义时间步分布和无导数估计策略。

Result: 在Multi - Agent Particle和MuJoCo基准测试中，OM2P性能优越，GPU内存使用最多减少3.8倍，训练时间最多加快10.8倍。

Conclusion: 首次成功将平均流模型集成到离线多智能体强化学习中，为合作多智能体场景的生成策略提供了实用且可扩展的方法。

Abstract: Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.

</details>


### [121] [A Study on Regularization-Based Continual Learning Methods for Indic ASR](https://arxiv.org/abs/2508.06280)
*Gokul Adethya T,S. Jaya Nirmala*

Main category: cs.LG

TL;DR: 本文研究在印度语言自动语音识别（ASR）中使用持续学习（CL），评估三种CL策略，结果表明CL能有效减轻遗忘，是可扩展ASR的有前景方法。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性给开发包容性ASR系统带来挑战，传统多语言模型因数据顺序到达和隐私限制不实用，而CL可使模型顺序学习新语言且不遗忘旧知识。

Method: 使用基于Conformer的混合RNN - T/CTC模型，先在印地语上预训练，再逐步在另外8种印度语言上训练；评估三种正则化和蒸馏的CL策略；用WER分析性能，通过向后迁移分析知识保留；探索不同训练轮数的影响。

Result: 与朴素微调相比，CL能有效减轻遗忘。

Conclusion: CL是在现实约束下用于印度多种语言可扩展ASR的有前景方法。

Abstract: Indias linguistic diversity poses significant challenges for developing
inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual
models, which require simultaneous access to all language data, are impractical
due to the sequential arrival of data and privacy constraints. Continual
Learning (CL) offers a solution by enabling models to learn new languages
sequentially without catastrophically forgetting previously learned knowledge.
This paper investigates CL for ASR on Indian languages using a subset of the
IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,
initially pretrained on Hindi, which is then incrementally trained on eight
additional Indian languages, for a total sequence of nine languages. We
evaluate three prominent regularization- and distillation-based CL strategies:
Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning
without Forgetting (LwF), selected for their suitability in no-replay,
privacy-conscious scenarios. Performance is analyzed using Word Error Rate
(WER) for both RNN-T and CTC paths on clean and noisy data, as well as
knowledge retention via Backward Transfer. We also explore the impact of
varying the number of training epochs (1, 2, 5, and 10) per task. Results,
compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating
forgetting, making it a promising approach for scalable ASR in diverse Indian
languages under realistic constraints. The code is available at:
https://github.com/FrozenWolf-Cyber/Indic-CL-ASR

</details>


### [122] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: 提出新型多输出脉冲神经元模型，结合SNNs和深度SSM模型优势，实验显示其性能与现有基准相当，且能克服不稳定性。


<details>
  <summary>Details</summary>
Motivation: 桥接脉冲神经网络（SNNs）和深度状态空间模型（SSMs）的优势。

Method: 提出结合线性通用SSM状态转换和非线性反馈机制的新型多输出脉冲神经元模型。

Result: 在关键词识别、基于事件的视觉任务和顺序模式识别任务等实验中，该模型性能与SNN文献中的现有基准相当。

Conclusion: 提出的重置机制能克服不稳定性，即使神经元动力学线性部分不稳定也能学习，突破了深度SSM模型中线性动力学严格的稳定性限制。

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


### [123] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: 介绍无监督伙伴设计（UPD）框架，可自适应生成训练伙伴，能与无监督环境设计结合，在多任务评估和用户研究中表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需预训练伙伴和手动调参的多智能体强化学习框架，以实现鲁棒的临时团队合作。

Method: 通过随机混合自我智能体策略与有偏随机行为构建多样化伙伴，用基于方差的可学习性指标评分，还可与无监督环境设计集成。

Result: 在Overcooked - AI和Overcooked泛化挑战评估中，UPD持续优于基于种群和无种群的基线及消融实验；用户研究中，UPD回报率更高，被认为更具适应性、更像人类、协作更好且更少挫败感。

Conclusion: UPD框架在多智能体强化学习的临时团队合作中有效，能提供动态伙伴课程。

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


### [124] [Introducing Fractional Classification Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2508.06346)
*Mert Can Kurucu,Tufan Kumbasar,İbrahim Eksin,Müjde Güzelkaya*

Main category: cs.LG

TL;DR: 提出分数分类损失（FCL），可自适应校准对标签噪声的鲁棒性，无需手动调参，实验达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒损失函数训练深度神经网络时需大量特定数据集的超参数调整。

Method: 在主动 - 被动损失框架内构建FCL，使用交叉熵损失的分数导数作为主动部分，平均绝对误差作为被动部分，将分数导数阶数μ作为可学习参数优化鲁棒性和收敛速度的权衡。

Result: FCL可动态重塑损失景观，在基准数据集实验中无需手动调参达到SOTA结果。

Conclusion: FCL能在标签噪声下动态调整，有效提升分类性能，无需手动超参数调整。

Abstract: Robust loss functions are crucial for training deep neural networks in the
presence of label noise, yet existing approaches require extensive,
dataset-specific hyperparameter tuning. In this work, we introduce Fractional
Classification Loss (FCL), an adaptive robust loss that automatically
calibrates its robustness to label noise during training. Built within the
active-passive loss framework, FCL employs the fractional derivative of the
Cross-Entropy (CE) loss as its active component and the Mean Absolute Error
(MAE) as its passive loss component. With this formulation, we demonstrate that
the fractional derivative order $\mu$ spans a family of loss functions that
interpolate between MAE-like robustness and CE-like fast convergence.
Furthermore, we integrate $\mu$ into the gradient-based optimization as a
learnable parameter and automatically adjust it to optimize the trade-off
between robustness and convergence speed. We reveal that FCL's unique property
establishes a critical trade-off that enables the stable learning of $\mu$:
lower log penalties on difficult or mislabeled examples improve robustness but
impose higher penalties on easy or clean data, reducing model confidence in
them. Consequently, FCL can dynamically reshape its loss landscape to achieve
effective classification performance under label noise. Extensive experiments
on benchmark datasets show that FCL achieves state-of-the-art results without
the need for manual hyperparameter tuning.

</details>


### [125] [Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means](https://arxiv.org/abs/2508.06353)
*Parichit Sharma,Marcin Stanislaw,Hasan Kurban,Oguzhan Kulekci,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: 本文介绍了Geometric - k - means（Gk - means），它能提升k - means算法效率和能源经济性，实验表明其优于传统和先进的k - means变体。


<details>
  <summary>Details</summary>
Motivation: 提升广泛使用的k - means算法的效率和能源经济性。

Method: 积极利用几何原理（标量投影），聚焦高表达数据，绕过低表达数据以减少计算开销。

Result: 在合成、真实世界和高维数据集实验中，Gk - means在运行时间和距离计算上显著优于传统和先进的k - means变体，且资源效率更好，能源消耗更低。

Conclusion: Gk - means是更具可持续性的k - means替代方案。

Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel
approach that significantly enhances the efficiency and energy economy of the
widely utilized k-means algorithm, which, despite its inception over five
decades ago, remains a cornerstone in machine learning applications. The
essence of Gk-means lies in its active utilization of geometric principles,
specifically scalar projection, to significantly accelerate the algorithm
without sacrificing solution quality. This geometric strategy enables a more
discerning focus on data points that are most likely to influence cluster
updates, which we call as high expressive data (HE). In contrast, low
expressive data (LE), does not impact clustering outcome, is effectively
bypassed, leading to considerable reductions in computational overhead.
Experiments spanning synthetic, real-world and high-dimensional datasets,
demonstrate Gk-means is significantly better than traditional and state of the
art (SOTA) k-means variants in runtime and distance computations (DC).
Moreover, Gk-means exhibits better resource efficiency, as evidenced by its
reduced energy footprint, placing it as more sustainable alternative.

</details>


### [126] [Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts](https://arxiv.org/abs/2508.06361)
*Zhaomin Wu,Mingzhe Du,See-Kiong Ng,Bingsheng He*

Main category: cs.LG

TL;DR: 研究大语言模型自主欺骗行为，提出新框架评估，发现任务难度增加时模型欺骗倾向上升。


<details>
  <summary>Details</summary>
Motivation: 现有研究多为人为诱导大语言模型欺骗，未反映真实人机交互，需研究模型自主欺骗行为。

Method: 提出使用“联系人搜索问题”的新框架，引入基于心理学原理的两个统计指标量化欺骗可能性。

Result: 评估14个领先大语言模型，发现任务难度增加时两个指标均上升，多数模型中二者并行上升。

Conclusion: 即使最先进的大语言模型在处理复杂问题时欺骗倾向也增加，对其在复杂关键领域的部署提出担忧。

Abstract: Large Language Models (LLMs) have been widely deployed in reasoning,
planning, and decision-making tasks, making their trustworthiness a critical
concern. The potential for intentional deception, where an LLM deliberately
fabricates or conceals information to serve a hidden objective, remains a
significant and underexplored threat. Existing studies typically induce such
deception by explicitly setting a "hidden" objective through prompting or
fine-tuning, which may not fully reflect real-world human-LLM interactions.
Moving beyond this human-induced deception, we investigate LLMs' self-initiated
deception on benign prompts. To address the absence of ground truth in this
evaluation, we propose a novel framework using "contact searching questions."
This framework introduces two statistical metrics derived from psychological
principles to quantify the likelihood of deception. The first, the Deceptive
Intention Score, measures the model's bias towards a hidden objective. The
second, Deceptive Behavior Score, measures the inconsistency between the LLM's
internal belief and its expressed output. Upon evaluating 14 leading LLMs, we
find that both metrics escalate as task difficulty increases, rising in
parallel for most models. Building on these findings, we formulate a
mathematical model to explain this behavior. These results reveal that even the
most advanced LLMs exhibit an increasing tendency toward deception when
handling complex problems, raising critical concerns for the deployment of LLM
agents in complex and crucial domains.

</details>


### [127] [ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design](https://arxiv.org/abs/2508.06364)
*Renyi Zhou,Huimin Zhu,Jing Tang,Min Li*

Main category: cs.LG

TL;DR: 提出ActivityDiff方法，可同时管理分子多种相互作用，有效处理药物设计任务，为分子活性综合控制提供新范式。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法缺乏同时管理分子多种相互作用的机制，难以实现对分子生物活性的精确控制。

Method: 基于扩散模型的分类器引导技术，利用正负分类器进行引导。

Result: ActivityDiff能有效处理单/双靶点生成、片段约束双靶点设计等药物设计任务。

Conclusion: 分类器引导的扩散方法可平衡分子设计的有效性和安全性，ActivityDiff是通用可扩展的框架。

Abstract: Achieving precise control over a molecule's biological activity-encompassing
targeted activation/inhibition, cooperative multi-target modulation, and
off-target toxicity mitigation-remains a critical challenge in de novo drug
design. However, existing generative methods primarily focus on producing
molecules with a single desired activity, lacking integrated mechanisms for the
simultaneous management of multiple intended and unintended molecular
interactions. Here, we propose ActivityDiff, a generative approach based on the
classifier-guidance technique of diffusion models. It leverages separately
trained drug-target classifiers for both positive and negative guidance,
enabling the model to enhance desired activities while minimizing harmful
off-target effects. Experimental results show that ActivityDiff effectively
handles essential drug design tasks, including single-/dual-target generation,
fragment-constrained dual-target design, selective generation to enhance target
specificity, and reduction of off-target effects. These results demonstrate the
effectiveness of classifier-guided diffusion in balancing efficacy and safety
in molecular design. Overall, our work introduces a novel paradigm for
achieving integrated control over molecular activity, and provides ActivityDiff
as a versatile and extensible framework.

</details>


### [128] [End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation](https://arxiv.org/abs/2508.06387)
*Anurag Tripathi,Vaibhav Patle,Abhinav Jain,Ayush Pundir,Sairam Menon,Ajeet Kumar Singh*

Main category: cs.LG

TL;DR: 提出三阶段端到端文本到SQL框架，先识别数据库，再生成SQL，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL方法需预指定目标数据库，在多数据库场景下识别正确数据库这一关键步骤被忽视。

Method: 提出三阶段框架，利用大语言模型和提示工程从自然语言查询中提取规则集，训练基于RoBERTa微调编码器的数据库ID预测模型，用批评代理修正生成的SQL。

Result: 框架在数据库意图预测和SQL生成准确性上优于当前最先进模型。

Conclusion: 所提三阶段端到端文本到SQL框架有效可行，能解决多数据库场景下文本到SQL的问题。

Abstract: Text-to-SQL bridges the gap between natural language and structured database
language, thus allowing non-technical users to easily query databases.
Traditional approaches model text-to-SQL as a direct translation task, where a
given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances
in large language models (LLMs) have significantly improved translation
accuracy, however, these methods all require that the target database is
pre-specified. This becomes problematic in scenarios with multiple extensive
databases, where identifying the correct database becomes a crucial yet
overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL
framework to identify the user's intended database before generating SQL
queries. Our approach leverages LLMs and prompt engineering to extract implicit
information from natural language queries (NLQs) in the form of a ruleset. We
then train a large db\_id prediction model, which includes a RoBERTa-based
finetuned encoder, to predict the correct Database identifier (db\_id) based on
both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL
by using critic agents to correct errors. Experimental results demonstrate that
our framework outperforms the current state-of-the-art models in both database
intent prediction and SQL generation accuracy.

</details>


### [129] [A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images](https://arxiv.org/abs/2508.06409)
*Wooyong Jung,Sola Kim,Dongwook Kim,Maryam Tabar,Dongwon Lee*

Main category: cs.LG

TL;DR: 美国无家可归问题严重，研究用众包数据追踪旧金山无家可归者帐篷趋势，提供及时有效信息指导政策。


<details>
  <summary>Details</summary>
Motivation: 美国无家可归现象激增，现有监测方法存在频率、一致性和空间细节等方面的局限。

Method: 使用公开的众包数据，如311服务电话和街景图像，构建预测模型。

Result: 模型捕捉到细粒度的每日和社区层面变化，发现传统统计常忽略的模式。

Conclusion: 该方法能提供及时、本地化且低成本的信息，可指导政策和评估干预措施。

Abstract: Homelessness in the United States has surged to levels unseen since the Great
Depression. However, existing methods for monitoring it, such as point-in-time
(PIT) counts, have limitations in terms of frequency, consistency, and spatial
detail. This study proposes a new approach using publicly available,
crowdsourced data, specifically 311 Service Calls and street-level imagery, to
track and forecast homeless tent trends in San Francisco. Our predictive model
captures fine-grained daily and neighborhood-level variations, uncovering
patterns that traditional counts often overlook, such as rapid fluctuations
during the COVID-19 pandemic and spatial shifts in tent locations over time. By
providing more timely, localized, and cost-effective information, this approach
serves as a valuable tool for guiding policy responses and evaluating
interventions aimed at reducing unsheltered homelessness.

</details>


### [130] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: 本文提出LoRR插件解决大语言模型偏好优化方法样本效率低和易过拟合问题，实验表明其能提升多种优化方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型偏好优化方法存在样本效率低和易受首因偏差影响的问题，导致过拟合和学习过程受损。

Method: 引入LoRR插件，其核心机制支持高重放次数训练，采用定期重置策略保持网络可塑性，结合监督微调与偏好损失的混合优化目标。

Result: 实验表明LoRR显著提升多种偏好优化方法在数学和通用推理基准上的性能，增强的迭代DPO方法在困难数学任务上表现可与复杂RL算法媲美。

Conclusion: LoRR为大语言模型微调提供实用、样本高效且有效的范式，能从有限数据中挖掘更大性能。

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


### [131] [LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection](https://arxiv.org/abs/2508.06467)
*Ameya Anjarlekar,Sandeep Pombra*

Main category: cs.LG

TL;DR: 提出GRIN框架用于大语言模型去学习，通过新指标和选择性噪声注入提升性能并验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临法律和道德审查，现有经验方法存在缺陷，需有效去学习方法。

Method: 提出GRIN框架，引入基于梯度比的指标识别参数，在微调前对参数进行选择性噪声注入，还提出新评估指标。

Result: 在TOFU、WMDP和SafePKU等标准基准上验证了方法。

Conclusion: GRIN框架能提升大语言模型去学习性能并保持模型效用。

Abstract: The growing legal and ethical scrutiny of large language models (LLMs)
necessitates effective machine unlearning, particularly for sensitive or
unauthorized data. Existing empirical methods often yield incomplete forgetting
or unintended degradation of unrelated knowledge due to poor localization. In
this work, we propose GRIN: a modular and targeted framework for LLM
unlearning. GRIN introduces a novel gradient-ratio-based metric to identify
parameters most responsible for memorizing forget data. We then perform
selective noise injection into these parameters prior to fine-tuning, which
improves unlearning performance while maintaining model utility. Finally, we
propose new evaluation metrics tailored to the LLM setting and validate our
approach on standard benchmarks such as TOFU, WMDP, and SafePKU.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [132] [Functional Connectivity Graph Neural Networks](https://arxiv.org/abs/2508.05786)
*Yang Li,Luopeiwen Yi,Tananun Songdechakraiwut*

Main category: cs.NE

TL;DR: 受脑成像多模态分析启发，提出功能连接图神经网络框架，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界网络需捕捉局部和全局交互，借鉴脑成像多模态分析推广到其他领域。

Method: 引入基于持久图同调的功能连接块捕捉全局拓扑特征，结合结构信息形成多模态架构。

Result: 实验表明该方法在不同网络的图级分类中性能始终优于现有方法。

Conclusion: 基于大脑启发的表示对图级分类有价值。

Abstract: Real-world networks often benefit from capturing both local and global
interactions. Inspired by multi-modal analysis in brain imaging, where
structural and functional connectivity offer complementary views of network
organization, we propose a graph neural network framework that generalizes this
approach to other domains. Our method introduces a functional connectivity
block based on persistent graph homology to capture global topological
features. Combined with structural information, this forms a multi-modal
architecture called Functional Connectivity Graph Neural Networks. Experiments
show consistent performance gains over existing methods, demonstrating the
value of brain-inspired representations for graph-level classification across
diverse networks.

</details>


### [133] [Identity Increases Stability in Neural Cellular Automata](https://arxiv.org/abs/2508.06389)
*James Stovold*

Main category: cs.NE

TL;DR: 本文提出引入带简单约束的‘身份’层提升神经细胞自动机（NCA）生成生物体的稳定性，结果显示临近生长的NCA更稳定，单身份值即可提升稳定性，稳定生物体有涌现运动，为研究NCA生物体相互作用奠基。


<details>
  <summary>Details</summary>
Motivation: NCA生成的生物体存在稳定性问题，自然边界易崩溃，无法维持预期形状。

Method: 在训练时引入带简单约束的‘身份’层。

Result: 临近生长的NCA比原模型更稳定，单身份值即可提升稳定性，多身份值模型中稳定生物体涌现运动更普遍。

Conclusion: 为进一步研究NCA生成生物体间的相互作用奠定基础，为人工生物体在细胞层面的社交互动研究铺平道路。

Abstract: Neural Cellular Automata (NCAs) offer a way to study the growth of
two-dimensional artificial organisms from a single seed cell. From the outset,
NCA-grown organisms have had issues with stability, their natural boundary
often breaking down and exhibiting tumour-like growth or failing to maintain
the expected shape. In this paper, we present a method for improving the
stability of NCA-grown organisms by introducing an 'identity' layer with simple
constraints during training.
  Results show that NCAs grown in close proximity are more stable compared with
the original NCA model. Moreover, only a single identity value is required to
achieve this increase in stability. We observe emergent movement from the
stable organisms, with increasing prevalence for models with multiple identity
values.
  This work lays the foundation for further study of the interaction between
NCA-grown organisms, paving the way for studying social interaction at a
cellular level in artificial organisms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [134] [Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach](https://arxiv.org/abs/2508.05693)
*Siamak Farshidi,Amir Saberhabibi,Behbod Eskafi,Niloofar Nikfarjam,Sadegh Eskandari,Slinger Jansen,Michel Chaudron,Bedir Tekinerdogan*

Main category: cs.SE

TL;DR: 本文提出基于MCDM的数据驱动框架PySelect用于Python软件包选择，评估显示其优于生成式AI，用户评价积极。


<details>
  <summary>Details</summary>
Motivation: 开源生态中Python第三方软件包选择困难，生成式AI工具建议存在问题，项目需要可靠选择方法。

Method: 将软件包选择视为MCDM问题，通过自动化数据管道收集多源数据构建决策模型，实现决策支持系统PySelect。

Result: 数据提取精度高，推荐质量优于生成式AI基线，用户对有用性和易用性评价积极。

Conclusion: 引入了可扩展、可解释、可重现的框架，支持基于证据的软件选择。

Abstract: Selecting third-party software packages in open-source ecosystems like Python
is challenging due to the large number of alternatives and limited transparent
evidence for comparison. Generative AI tools are increasingly used in
development workflows, but their suggestions often overlook dependency
evaluation, emphasize popularity over suitability, and lack reproducibility.
This creates risks for projects that require transparency, long-term
reliability, maintainability, and informed architectural decisions. This study
formulates software package selection as a Multi-Criteria Decision-Making
(MCDM) problem and proposes a data-driven framework for technology evaluation.
Automated data pipelines continuously collect and integrate software metadata,
usage trends, vulnerability information, and developer sentiment from GitHub,
PyPI, and Stack Overflow. These data are structured into a decision model
representing relationships among packages, domain features, and quality
attributes. The framework is implemented in PySelect, a decision support system
that uses large language models to interpret user intent and query the model to
identify contextually appropriate packages. The approach is evaluated using
798,669 Python scripts from 16,887 GitHub repositories and a user study based
on the Technology Acceptance Model. Results show high data extraction
precision, improved recommendation quality over generative AI baselines, and
positive user evaluations of usefulness and ease of use. This work introduces a
scalable, interpretable, and reproducible framework that supports
evidence-based software selection using MCDM principles, empirical data, and
AI-assisted intent modeling.

</details>


### [135] [Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning](https://arxiv.org/abs/2508.05710)
*Jia Fu,Xinyu Yang,Hongzhi Zhang,Yahui Liu,Jingyuan Zhang,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.SE

TL;DR: 提出Klear - CodeTest框架解决代码强化学习中高质量测试用例合成难题，实验证明有效，代码和数据集开源。


<details>
  <summary>Details</summary>
Motivation: 精确正确的反馈对代码强化学习训练大语言模型至关重要，但合成高质量测试用例是难题。

Method: 提出Klear - CodeTest框架，采用新的G - V框架实现广泛覆盖，通过一致性验证机制确保正确性，设计多层安全沙箱系统。

Result: 实验表明精心策划的数据集有效，显著提升模型性能和训练稳定性。

Conclusion: Klear - CodeTest框架能有效解决高质量测试用例合成问题，有助于代码强化学习训练大语言模型。

Abstract: Precise, correct feedback is crucial for effectively training large language
models (LLMs) in code reinforcement learning. However, synthesizing
high-quality test cases remains a profoundly challenging and unsolved problem.
In this work, we present Klear-CodeTest, a comprehensive test case synthesis
framework featuring rigorous verification to ensure quality and reliability of
test cases. Our approach achieves broad coverage of programming problems via a
novel Generator-Validation (G-V) framework, ensuring correctness through a
consistency validation mechanism that verifies outputs against gold solutions.
The proposed G-V framework generates comprehensive test cases including both
regular and corner cases, enhancing test coverage and discriminative power for
solution correctness assessment in code reinforcement learning. In addition, we
design a multi-layered security sandbox system optimized for online
verification platforms, guaranteeing safe and reliable code execution. Through
comprehensive experiments, we demonstrate the effectiveness of our curated
dataset, showing significant improvements in model performance and training
stability. The source codes, curated dataset and sandbox system are available
at: https://github.com/Kwai-Klear/CodeTest.

</details>


### [136] [Utilizing Composer Packages to Accelerate Laravel-Based Project Development Among Students: A Pedagogical and Practical Framework](https://arxiv.org/abs/2508.05747)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.SE

TL;DR: 本文介绍Composer及其精选包可助力Laravel开发，加速项目完成，同时强调教学中要引导学生正确使用以确保深度学习。


<details>
  <summary>Details</summary>
Motivation: 解决学生用Laravel在有限时间内难完成项目的问题，提升课程相关性和学生职场适应能力。

Method: 引入Composer及精选包，阐述师生如何利用其构建Laravel系统，还给出应对潜在风险的建议。

Result: 可加速开发，增强课程相关性，助学生从学术过渡到职场。

Conclusion: 有效整合需与学习目标一致的教学设计，教师要引导学生批判性使用工具。

Abstract: Laravel has emerged as a foundational framework in university web development
curricula. However, despite its scaffolding capabilities, students often
struggle to complete projects within limited academic timelines. This
conceptual paper introduces Composer, PHP's standard dependency manager, and
categorizes a curated selection of Composer packages that significantly reduce
development effort while fostering professional software practices. Grounded in
practical and pedagogical considerations, the paper illustrates how educators
and learners can strategically leverage these tools to build typical academic
or personal Laravel-based systems. Central to this approach is maintaining code
quality and reinforcing conceptual understanding. The paper also addresses
potential risks such as package conflicts and over-reliance on tools, providing
best-practice recommendations to mitigate them. While the goal is to accelerate
development, the deeper objective is to reinforce professional workflows and
industry readiness. Exposure to Composer packages enhances curriculum relevance
and smooths the transition from academia to the workplace. However, effective
integration requires deliberate instructional design aligned with learning
objectives. Without guidance, students may treat packages as black boxes. Thus,
educators must teach not only how to use these tools, but also when and why,
encouraging critical evaluation of their utility and limitations. This ensures
that practical convenience supports rather than supplants deep learning.

</details>


### [137] [AI-Guided Exploration of Large-Scale Codebases](https://arxiv.org/abs/2508.05799)
*Yoseph Berhanu Alebachew*

Main category: cs.SE

TL;DR: 本文提出将确定性逆向工程与大语言模型引导、意图感知的可视化探索相结合的混合方法，以增强代码理解，并用Java实现原型。


<details>
  <summary>Details</summary>
Motivation: 传统工具缺乏交互性、适应性和上下文信息集成，大语言模型缺乏与结构化视图的集成，影响代码理解。

Method: 提出混合方法，将UML可视化、动态界面、历史上下文和协作功能集成到自适应工具中，通过大语言模型解读用户查询和交互模式。

Result: 实现Java原型，证明方法可行。

Conclusion: 为符合开发者认知和协作工作流的智能交互环境奠定基础，未来需进行实证评估、扩展到多语言系统和探索GUI驱动的大语言模型交互模式。

Abstract: Understanding large-scale, complex software systems is a major challenge for
developers, who spend a significant portion of their time on program
comprehension. Traditional tools such as static visualizations and reverse
engineering techniques provide structural insights but often lack
interactivity, adaptability, and integration with contextual information.
Recent advancements in large language models (LLMs) offer new opportunities to
enhance code exploration workflows, yet their lack of grounding and integration
with structured views limits their effectiveness. This work introduces a hybrid
approach that integrates deterministic reverse engineering with LLM-guided,
intent-aware visual exploration. The proposed system combines UML-based
visualization, dynamic user interfaces, historical context, and collaborative
features into an adaptive tool for code comprehension. By interpreting user
queries and interaction patterns, the LLM helps developers navigate and
understand complex codebases more effectively. A prototype implementation for
Java demonstrates the feasibility of this approach. Future work includes
empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM
interaction models. This research lays the groundwork for intelligent,
interactive environments that align with developer cognition and collaborative
workflows.

</details>


### [138] [Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm](https://arxiv.org/abs/2508.05923)
*Yanusha Mehendran,Maolin Tang,Yi Lu*

Main category: cs.SE

TL;DR: 本文提出基于遗传算法的测试输入生成方法，用于软件漏洞检测，在九个开源JSON处理库评估中，相比基准方法在各项覆盖率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法能力难以跟上软件复杂度增长，需提升软件漏洞检测效果。

Method: 引入基于遗传算法的测试输入生成方法，结合遗传算子和自适应学习，应用交叉算子和自适应反馈机制，通过反馈驱动选择生成测试用例。

Result: 在九个开源JSON处理库评估中，相比基准进化模糊测试方法，类覆盖率平均提升39.8%，方法覆盖率提升62.4%，行覆盖率提升105.0%，指令覆盖率提升114.0%，分支覆盖率提升166.0%。

Conclusion: 该方法能检测更深层和更复杂的漏洞，为软件安全测试提供可扩展和自适应的解决方案。

Abstract: Software vulnerabilities continue to undermine the reliability and security
of modern systems, particularly as software complexity outpaces the
capabilities of traditional detection methods. This study introduces a genetic
algorithm-based method for test input generation that innovatively integrates
genetic operators and adaptive learning to enhance software vulnerability
detection. A key contribution is the application of the crossover operator,
which facilitates exploration by searching across a broader space of potential
test inputs. Complementing this, an adaptive feedback mechanism continuously
learns from the system's execution behavior and dynamically guides input
generation toward promising areas of the input space. Rather than relying on
fixed or randomly selected inputs, the approach evolves a population of
structurally valid test cases using feedback-driven selection, enabling deeper
and more effective code traversal. This strategic integration of exploration
and exploitation ensures that both diverse and targeted test inputs are
developed over time. Evaluation was conducted across nine open-source
JSON-processing libraries. The proposed method achieved substantial
improvements in coverage compared to a benchmark evolutionary fuzzing method,
with average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0%
in line coverage, 114.0% in instruction coverage, and 166.0% in branch
coverage. These results highlight the method's capacity to detect deeper and
more complex vulnerabilities, offering a scalable and adaptive solution to
software security testing.

</details>


### [139] [A Survey on Task Scheduling in Carbon-Aware Container Orchestration](https://arxiv.org/abs/2508.05949)
*Jialin Yang,Zainab Saad,Jiajun Wu,Xiaoguang Niu,Henry Leung,Steve Drew*

Main category: cs.SE

TL;DR: 因大语言模型驱动，云计算能耗与碳足迹激增，本文系统回顾Kubernetes调度策略，提出分类法并分析趋势与挑战，为可持续调度方案设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 大规模软件生态系统和云数据中心能耗及碳足迹达前所未有的水平，需要通过高效任务调度和基础设施编排减少云计算碳排放。

Method: 对各种Kubernetes调度策略进行系统回顾，分为硬件和软件中心两类，标注可持续性目标并按算法分组，提出云任务调度研究分类法。

Result: 分析了新兴研究趋势和开放挑战。

Conclusion: 研究结果为下一代云计算系统的可持续调度解决方案设计提供关键见解。

Abstract: The soaring energy demands of large-scale software ecosystems and cloud data
centers, accelerated by the intensive training and deployment of large language
models, have driven energy consumption and carbon footprint to unprecedented
levels. In response, both industry and academia are increasing efforts to
reduce the carbon emissions associated with cloud computing through more
efficient task scheduling and infrastructure orchestration. In this work, we
present a systematic review of various Kubernetes scheduling strategies,
categorizing them into hardware-centric and software-centric, annotating each
with its sustainability objectives, and grouping them according to the
algorithms they use. We propose a comprehensive taxonomy for cloud task
scheduling studies, with a particular focus on the environmental sustainability
aspect. We analyze emerging research trends and open challenges, and our
findings provide critical insight into the design of sustainable scheduling
solutions for next-generation cloud computing systems.

</details>


### [140] [Impact-driven Context Filtering For Cross-file Code Completion](https://arxiv.org/abs/2508.05970)
*Yanzhou Li,Shangqing Liu,Kangjie Chen,Tianwei Zhang,Yang Liu*

Main category: cs.SE

TL;DR: 本文引入指标评估检索代码块对代码补全的影响，构建数据集，提出自适应检索上下文过滤框架CODEFILTER，实验表明其能提升代码补全的准确率、效率和泛化性。


<details>
  <summary>Details</summary>
Motivation: 为更好理解检索的跨文件上下文对代码补全的贡献，解决部分检索代码块降低补全性能的问题。

Method: 引入基于似然的指标评估检索代码块影响，构建标注数据集，提出CODEFILTER框架并在数据集上训练。

Result: 在RepoEval和CrossCodeLongEval基准测试中，CODEFILTER比无过滤操作的方法持续提高补全准确率，减少输入提示长度，提升计算效率，且在不同模型上有强泛化性。

Conclusion: CODEFILTER有提升仓库级代码补全的准确性、效率和可归因性的潜力。

Abstract: Retrieval-augmented generation (RAG) has recently demonstrated considerable
potential for repository-level code completion, as it integrates cross-file
knowledge with in-file preceding code to provide comprehensive contexts for
generation. To better understand the contribution of the retrieved cross-file
contexts, we introduce a likelihood-based metric to evaluate the impact of each
retrieved code chunk on the completion. Our analysis reveals that, despite
retrieving numerous chunks, only a small subset positively contributes to the
completion, while some chunks even degrade performance. To address this issue,
we leverage this metric to construct a repository-level dataset where each
retrieved chunk is labeled as positive, neutral, or negative based on its
relevance to the target completion. We then propose an adaptive retrieval
context filtering framework, CODEFILTER, trained on this dataset to mitigate
the harmful effects of negative retrieved contexts in code completion.
Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks
demonstrates that CODEFILTER consistently improves completion accuracy compared
to approaches without filtering operations across various tasks. Additionally,
CODEFILTER significantly reduces the length of the input prompt, enhancing
computational efficiency while exhibiting strong generalizability across
different models. These results underscore the potential of CODEFILTER to
enhance the accuracy, efficiency, and attributability of repository-level code
completion.

</details>


### [141] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 智能编码系统存在决策不透明问题，需生成清晰合理理由，可探索神经符号方法解决。


<details>
  <summary>Details</summary>
Motivation: AI驱动的智能编码系统决策不透明，引发信任和可用性问题，尤其影响非专家用户。

Method: 确定认知对齐和语义忠实两个关键理由属性，指出现有方法局限，倡导探索神经符号方法生成理由。

Result: 无明确提及具体研究结果。

Conclusion: 智能编码系统不仅要生成代码，还应产生清晰一致的理由，神经符号方法可用于理由生成。

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


### [142] [Understanding Inconsistent State Update Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2508.06192)
*Lantian Li,Yuyu Chen,Jingwen Wu,Yue Pan,Zhongxing Yu*

Main category: cs.SE

TL;DR: 本文对智能合约中不一致状态更新漏洞进行大规模实证研究，总结相关原因、修复策略和利用方法，有11项重要发现，开发概念验证检查器并验证了结果有效性。


<details>
  <summary>Details</summary>
Motivation: 智能合约状态更新过程存在问题，现有检测工具难以有效识别不一致状态更新漏洞，旨在为开发者等提供参考以避免此类漏洞。

Method: 系统调查352个真实智能合约项目中的116个不一致状态更新漏洞，总结其根本原因、修复策略和利用方法。

Result: 研究得出11项原创且重要的发现，开发的概念验证检查器在64个热门GitHub项目中有效检测出问题，19个项目所有者已确认。

Conclusion: 研究结果对于避免智能合约中不一致状态更新漏洞具有实用性和重要性。

Abstract: Smart contracts enable contract terms to be automatically executed and
verified on the blockchain, and recent years have witnessed numerous
applications of them in areas such as financial institutions and supply chains.
The execution logic of a smart contract is closely related to the contract
state, and thus the correct and safe execution of the contract depends heavily
on the precise control and update of the contract state. However, the contract
state update process can have issues. In particular, inconsistent state update
issues can arise for reasons such as unsynchronized modifications. Inconsistent
state update bugs have been exploited by attackers many times, but existing
detection tools still have difficulty in effectively identifying them. This
paper conducts the first large-scale empirical study about inconsistent state
update vulnerabilities (that is, inconsistent state update bugs that are
exploitable) in smart contracts, aiming to shed light for developers,
researchers, tool builders, and language or library designers in order to avoid
inconsistent state update vulnerabilities. We systematically investigate 116
inconsistent state update vulnerabilities in 352 real-world smart contract
projects, summarizing their root causes, fix strategies, and exploitation
methods. Our study provides 11 original and important findings, and we also
give the implications of our findings. To illustrate the potential benefits of
our research, we also develop a proof-of-concept checker based on one of our
findings. The checker effectively detects issues in 64 popular GitHub projects,
and 19 project owners have confirmed the detected issues at the time of
writing. The result demonstrates the usefulness and importance of our findings
for avoiding inconsistent state update vulnerabilities in smart contracts.

</details>


### [143] [Improving the Developer Experience with a Low-Code Process Modelling Language](https://arxiv.org/abs/2508.06299)
*Henrique Henriques,Hugo Lourenço,Vasco Amaral,Miguel Goulão*

Main category: cs.SE

TL;DR: 针对OutSystems平台中BPT采用率低和可用性问题，结合多种方法开发新版本BPT，评估显示新版本显著提升开发者体验。


<details>
  <summary>Details</summary>
Motivation: 解决OutSystems平台中BPT采用率低和可用性问题，降低语言维护成本。

Method: 结合访谈、用‘Physics of Notation’进行批判性审查，并用SUS和TLX进行实证评估，同时考虑Outsystems工程师文化开发新版本BPT。

Result: 对25名专业软件工程师的评估显示，新版本语义透明度、回答正确性、SUS得分提升，TLX得分降低，差异有统计学意义。

Conclusion: 新版本BPT显著改善开发者体验，用户OutSystems背景影响最终具体语法选择和可用性指标。

Abstract: Context: The OutSystems Platform is a development environment composed of
several DSLs, used to specify, quickly build, and validate web and mobile
applications. The DSLs allow users to model different perspectives such as
interfaces and data models, define custom business logic and construct process
models. Problem: The DSL for process modelling (Business Process Technology
(BPT)), has a low adoption rate and is perceived as having usability problems
hampering its adoption. This is problematic given the language maintenance
costs. Method: We used a combination of interviews, a critical review of BPT
using the "Physics of Notation" and empirical evaluations of BPT using the
System Usability Scale (SUS) and the NASA Task Load indeX (TLX), to develop a
new version of BPT, taking these inputs and Outsystems' engineers' culture into
account. Results: Evaluations conducted with 25 professional software engineers
showed an increase of the semantic transparency on the new version, from 31% to
69%, an increase in the correctness of responses, from 51% to 89%, an increase
in the SUS score, from 42.25 to 64.78, and a decrease of the TLX score, from
36.50 to 20.78. These differences were statistically significant. Conclusions:
These results suggest that the new version of BPT significantly improved the
developer experience of the previous version. The end users' background with
OutSystems had a relevant impact on the final concrete syntax choices and
achieved usability indicators.

</details>


### [144] [Execution-Feedback Driven Test Generation from SWE Issues](https://arxiv.org/abs/2508.06365)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 本文探索自动生成软件问题重现测试的方法，提出新技巧并实现e - Otter++，实验显示其表现优异。


<details>
  <summary>Details</summary>
Motivation: 多数软件工程问题无可用重现测试，需自动生成。

Method: 介绍利用执行反馈解决代码缺失或错误问题的新技巧，实现重现测试生成器e - Otter++。

Result: e - Otter++在TDD - Bench Verified基准上生成测试的平均失败转通过率达63%。

Conclusion: e - Otter++在自动生成软件问题重现测试方面领先于现有技术。

Abstract: A software engineering issue (SWE issue) is easier to resolve when
accompanied by a reproduction test. Unfortunately, most issues do not come with
functioning reproduction tests, so this paper explores how to generate them
automatically. The primary challenge in this setting is that the code to be
tested is either missing or wrong, as evidenced by the existence of the issue
in the first place. This has held back test generation for this setting:
without the correct code to execute, it is difficult to leverage execution
feedback to generate good tests. This paper introduces novel techniques for
leveraging execution feedback to get around this problem, implemented in a new
reproduction test generator called e-Otter++. Experiments show that e-Otter++
represents a leap ahead in the state-of-the-art for this problem, generating
tests with an average fail-to-pass rate of 63% on the TDD-Bench Verified
benchmark.

</details>


### [145] [What Builds Effective In-Context Examples for Code Generation?](https://arxiv.org/abs/2508.06414)
*Dongze Li,Songqiang Chen,Jialun Cao,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文通过对照消融实验研究不同代码特征对基于代码示例的上下文学习（ICL）的影响，发现变量和函数的恰当命名对代码生成很关键，大语言模型更看重标识符语义而非格式，且当前大模型在从代码中提取通用解题思路方面有困难。


<details>
  <summary>Details</summary>
Motivation: 当前基于代码示例的ICL方法虽有效，但其中哪些代码特征对ICL有效性有显著贡献尚不明确，因此开展研究。

Method: 通过对照消融实验系统研究各种代码特征对基于代码示例的ICL的影响。

Result: 变量和函数的恰当命名对有效代码生成至关重要，去除它们会使性能下降达30个百分点；大语言模型更优先考虑语义有意义的标识符名称而非格式约定，且对标识符冗长程度有特定语言偏好；当前大语言模型虽能有效利用直接信息，但难以从相似代码解决方案中提取通用解题思路。

Conclusion: 研究结果为优化代码生成应用中的ICL系统提供有价值的见解，同时凸显了代码生成任务中基于反思的学习面临的根本挑战。

Abstract: In-Context Learning (ICL) has emerged as a promising solution to enhance the
code generation capabilities of Large Language Models (LLMs), which
incorporates code examples inside the prompt to let LLMs learn from
demonstrations. However, despite the substantial effectiveness of the code
example-based ICL approach, the specific features (e.g., identifier naming
styles, code formatting, solution insight) within the ICL-provided code
examples that significantly contribute to the ICL's effectiveness remain
unclear. This paper systematically investigates the impact of various code
features on ICL with code examples through controlled ablation studies. Our
findings reveal that the appropriate naming of variables and functions is
crucial for effective code generation, with their elimination leading to
performance decreases of up to 30 percentage points. We further demonstrate
that LLMs prioritize semantically meaningful identifier names over formatting
conventions, with language-specific preferences regarding identifier verbosity.
Additionally, our investigation into ICL's potential for enhancing reflection
and inference capabilities reveals that current LLMs struggle to extract
generalizable problem-solving insights from similar code solutions, despite
being capable of utilizing direct information effectively. These findings are
expected to provide valuable insights for optimizing ICL systems in code
generation applications and highlight fundamental challenges in
reflection-based learning for code generation tasks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [146] [Random Walk Learning and the Pac-Man Attack](https://arxiv.org/abs/2508.05663)
*Xingran Chen,Parimal Parag,Rohit Bhagat,Zonghong Liu,Salim El Rouayheb*

Main category: stat.ML

TL;DR: 研究基于随机游走（RW）算法在分布式系统中面临的“吃豆人”攻击，提出平均交叉（AC）算法应对，有理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 基于随机游走的算法在分布式系统应用广泛，但依赖局部交互易受恶意行为影响，研究“吃豆人”攻击以应对威胁。

Method: 提出平均交叉（AC）算法，通过复制随机游走来防止在“吃豆人”攻击下随机游走的灭绝。

Result: 理论分析表明在AC算法下随机游走数量有界，随机梯度下降收敛且能量化偏差；实验验证理论结果，发现灭绝概率的相变。

Conclusion: AC算法能有效应对“吃豆人”攻击，对随机游走算法在分布式系统的安全应用有指导意义。

Abstract: Random walk (RW)-based algorithms have long been popular in distributed
systems due to low overheads and scalability, with recent growing applications
in decentralized learning. However, their reliance on local interactions makes
them inherently vulnerable to malicious behavior. In this work, we investigate
an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious
node probabilistically terminates any RW that visits it. This stealthy behavior
gradually eliminates active RWs from the network, effectively halting the
learning process without triggering failure alarms. To counter this threat, we
propose the Average Crossing (AC) algorithm--a fully decentralized mechanism
for duplicating RWs to prevent RW extinction in the presence of Pac-Man. Our
theoretical analysis establishes that (i) the RW population remains almost
surely bounded under AC and (ii) RW-based stochastic gradient descent remains
convergent under AC, even in the presence of Pac-Man, with a quantifiable
deviation from the true optimum. Our extensive empirical results on both
synthetic and real-world datasets corroborate our theoretical findings.
Furthermore, they uncover a phase transition in the extinction probability as a
function of the duplication threshold. We offer theoretical insights by
analyzing a simplified variant of the AC, which sheds light on the observed
phase transition.

</details>


### [147] [Reduction Techniques for Survival Analysis](https://arxiv.org/abs/2508.05715)
*Johannes Piller,Léa Orsini,Simon Wiegrebe,John Zobolas,Lukas Burk,Sophie Hanna Langbein,Philip Studener,Markus Goeswein,Andreas Bender*

Main category: stat.ML

TL;DR: 本文探讨生存分析的降维技术，介绍不同技术优缺点，提供部分技术的实现，并进行基准分析。


<details>
  <summary>Details</summary>
Motivation: 使机器学习工具能应用于生存分析，避免使用定制学习器。

Method: 概述不同降维技术，提供部分技术的实现，用示例说明，进行基准分析。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: In this work, we discuss what we refer to as reduction techniques for
survival analysis, that is, techniques that "reduce" a survival task to a more
common regression or classification task, without ignoring the specifics of
survival data. Such techniques particularly facilitate machine learning-based
survival analysis, as they allow for applying standard tools from machine and
deep learning to many survival tasks without requiring custom learners. We
provide an overview of different reduction techniques and discuss their
respective strengths and weaknesses. We also provide a principled
implementation of some of these reductions, such that they are directly
available within standard machine learning workflows. We illustrate each
reduction using dedicated examples and perform a benchmark analysis that
compares their predictive performance to established machine learning methods
for survival analysis.

</details>


### [148] [Stochastic Trace Optimization of Parameter Dependent Matrices Based on Statistical Learning Theory](https://arxiv.org/abs/2508.05764)
*Arvind K. Saibaba,Ilse C. F. Ipsen*

Main category: stat.ML

TL;DR: 提出蒙特卡罗估计器最小化矩阵迹，确定采样量使估计器后向误差高概率有界，推导基于epsilon网和泛链的两类界并比较。


<details>
  <summary>Details</summary>
Motivation: 研究依赖参数的矩阵，最小化矩阵迹并控制估计器误差。

Method: 提出蒙特卡罗估计器，推导基于epsilon网和泛链的两类界。

Result: 两类界对非对角元素质量小的矩阵和小“尺寸”参数空间所需采样量小，对矩阵维度依赖弱；epsilon网界易评估，链界依赖难评估的Talagrand泛函。

Conclusion: 难以比较两类界，但文献显示链界可能更优。

Abstract: We consider matrices $\boldsymbol{A}(\boldsymbol\theta)\in\mathbb{R}^{m\times
m}$ that depend, possibly nonlinearly, on a parameter $\boldsymbol\theta$ from
a compact parameter space $\Theta$. We present a Monte Carlo estimator for
minimizing $\text{trace}(\boldsymbol{A}(\boldsymbol\theta))$ over all
$\boldsymbol\theta\in\Theta$, and determine the sampling amount so that the
backward error of the estimator is bounded with high probability. We derive two
types of bounds, based on epsilon nets and on generic chaining. Both types
predict a small sampling amount for matrices
$\boldsymbol{A}(\boldsymbol\theta)$ with small offdiagonal mass, and parameter
spaces $\Theta$ of small ``size.'' Dependence on the matrix dimension~$m$ is
only weak or not explicit. The bounds based on epsilon nets are easier to
evaluate and come with fully specified constants. In contrast, the bounds based
on chaining depend on the Talagrand functionals which are difficult to
evaluate, except in very special cases. Comparisons between the two types of
bounds are difficult, although the literature suggests that chaining bounds can
be superior.

</details>


### [149] [Lightweight Auto-bidding based on Traffic Prediction in Live Advertising](https://arxiv.org/abs/2508.06069)
*Bo Yang,Ruixuan Luo,Junqi Jin,Han Zhu*

Main category: stat.ML

TL;DR: 本文针对直播广告实时竞价需求和未知流量难题，提出轻量级竞价算法BiCB，结合数学分析与流量估计统计方法，实验证明其性能好且工程成本低。


<details>
  <summary>Details</summary>
Motivation: 现有直播广告自动竞价算法要么不考虑全时段流量，要么计算复杂度高，且直播广告有实时竞价需求和未知流量难题。

Method: 提出轻量级竞价算法BiCB，结合数学分析给出的最优竞价公式和未来流量估计的统计方法，补充传统自动竞价建模的上下界约束形式并进行理论分析。

Result: 通过充分的离线和在线实验，证明BiCB有良好的性能和较低的工程成本。

Conclusion: BiCB算法能较好地解决直播广告竞价问题，具有良好性能和低工程成本。

Abstract: Internet live streaming is widely used in online entertainment and
e-commerce, where live advertising is an important marketing tool for anchors.
An advertising campaign hopes to maximize the effect (such as conversions)
under constraints (such as budget and cost-per-click). The mainstream control
of campaigns is auto-bidding, where the performance depends on the decision of
the bidding algorithm in each request. The most widely used auto-bidding
algorithms include Proportional-Integral-Derivative (PID) control, linear
programming (LP), reinforcement learning (RL), etc. Existing methods either do
not consider the entire time traffic, or have too high computational
complexity. In this paper, the live advertising has high requirements for
real-time bidding (second-level control) and faces the difficulty of unknown
future traffic. Therefore, we propose a lightweight bidding algorithm Binary
Constrained Bidding (BiCB), which neatly combines the optimal bidding formula
given by mathematical analysis and the statistical method of future traffic
estimation, and obtains good approximation to the optimal result through a low
complexity solution. In addition, we complement the form of upper and lower
bound constraints for traditional auto-bidding modeling and give theoretical
analysis of BiCB. Sufficient offline and online experiments prove BiCB's good
performance and low engineering cost.

</details>


### [150] [Decorrelated feature importance from local sample weighting](https://arxiv.org/abs/2508.06337)
*Benedikt Fröhlich,Alison Durst,Merle Behr*

Main category: stat.ML

TL;DR: 提出局部样本加权（losaw）方法改善特征相关时的特征重要性（FI）分数，在模拟研究中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有特征重要性统计方法在特征相关时存在局限性，如FI分布分散、噪声特征FI分数可能高于信号特征。

Method: 借鉴因果推断中的逆概率加权，在机器学习模型中使用样本加权方案对目标特征与其余特征进行去相关，有最小有效样本量这一调参。

Result: losaw能持续改善FI，常提高分布外预测准确性，分布内测试数据准确性相近。

Conclusion: losaw可灵活集成到多种机器学习算法中，有效改善特征相关时的FI分数。

Abstract: Feature importance (FI) statistics provide a prominent and valuable method of
insight into the decision process of machine learning (ML) models, but their
effectiveness has well-known limitations when correlation is present among the
features in the training data. In this case, the FI often tends to be
distributed among all features which are in correlation with the
response-generating signal features. Even worse, if multiple signal features
are in strong correlation with a noise feature, while being only modestly
correlated with one another, this can result in a noise feature having a
distinctly larger FI score than any signal feature. Here we propose local
sample weighting (losaw) which can flexibly be integrated into many ML
algorithms to improve FI scores in the presence of feature correlation in the
training data. Our approach is motivated from inverse probability weighting in
causal inference and locally, within the ML model, uses a sample weighting
scheme to decorrelate a target feature from the remaining features. This
reduces model bias locally, whenever the effect of a potential signal feature
is evaluated and compared to others. Moreover, losaw comes with a natural
tuning parameter, the minimum effective sample size of the weighted population,
which corresponds to an interpretation-prediction-tradeoff, analog to a
bias-variance-tradeoff as for classical ML tuning parameters. We demonstrate
how losaw can be integrated within decision tree-based ML methods and within
mini-batch training of neural networks. We investigate losaw for random forest
and convolutional neural networks in a simulation study on settings showing
diverse correlation patterns. We found that losaw improves FI consistently.
Moreover, it often improves prediction accuracy for out-of-distribution, while
maintaining a similar accuracy for in-distribution test data.

</details>


### [151] [DP-SPRT: Differentially Private Sequential Probability Ratio Tests](https://arxiv.org/abs/2508.06377)
*Thomas Michel,Debabrota Basu,Emilie Kaufmann*

Main category: stat.ML

TL;DR: 本文在隐私约束下重新审视Wald的序贯概率比检验，提出DP - SPRT方法，证明其误差和样本复杂度的界，在两种噪声设置下进行分析并通过实验验证性能。


<details>
  <summary>Details</summary>
Motivation: 在隐私约束下改进现有的序贯概率比检验，弥补以往工作的不足。

Method: 提出DP - SPRT包装器，依靠OutsideInterval机制处理查询序列；证明误差和样本复杂度的通用上界。

Result: 在拉普拉斯噪声和高斯噪声设置下分析，证明在拉普拉斯噪声中当误差小且两假设接近时DP - SPRT接近最优；实验显示其有良好的实际性能。

Conclusion: DP - SPRT能在满足隐私约束的同时，达到所需的误差概率，有理论和实际优势。

Abstract: We revisit Wald's celebrated Sequential Probability Ratio Test for sequential
tests of two simple hypotheses, under privacy constraints. We propose DP-SPRT,
a wrapper that can be calibrated to achieve desired error probabilities and
privacy constraints, addressing a significant gap in previous work. DP-SPRT
relies on a private mechanism that processes a sequence of queries and stops
after privately determining when the query results fall outside a predefined
interval. This OutsideInterval mechanism improves upon naive composition of
existing techniques like AboveThreshold, potentially benefiting other
sequential algorithms. We prove generic upper bounds on the error and sample
complexity of DP-SPRT that can accommodate various noise distributions based on
the practitioner's privacy needs. We exemplify them in two settings: Laplace
noise (pure Differential Privacy) and Gaussian noise (R\'enyi differential
privacy). In the former setting, by providing a lower bound on the sample
complexity of any $\epsilon$-DP test with prescribed type I and type II errors,
we show that DP-SPRT is near optimal when both errors are small and the two
hypotheses are close. Moreover, we conduct an experimental study revealing its
good practical performance.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [152] [Identifiability of the minimum-trace directed acyclic graph and hill climbing algorithms without strict local optima under weakly increasing error variances](https://arxiv.org/abs/2508.05706)
*Hyunwoong Chang,Jaehoan Kim*

Main category: stat.CO

TL;DR: 证明高斯线性结构方程模型中真实DAG在误差方差弱递增时可作为最小迹DAG识别，还研究了爬山算法性质并通过模拟验证。


<details>
  <summary>Details</summary>
Motivation: 桥接现有框架，扩展最小迹DAG方法可识别情况，解释算法排序搜索方法。

Method: 理论证明，使用爬山算法并进行大量模拟。

Result: 证明爬山算法在R2R邻域无严格局部最优，模拟仅观察到少量弱局部最优，其他邻域算法有次优行为。

Conclusion: 在误差方差弱递增条件下，真实DAG可作为最小迹DAG识别，R2R邻域爬山算法表现更好。

Abstract: We prove that the true underlying directed acyclic graph (DAG) in Gaussian
linear structural equation models is identifiable as the minimum-trace DAG when
the error variances are weakly increasing with respect to the true causal
ordering. This result bridges two existing frameworks as it extends the
identifiable cases within the minimum-trace DAG method and provides a
principled interpretation of the algorithmic ordering search approach,
revealing that its objective is actually to minimize the total residual sum of
squares. On the computational side, we prove that the hill climbing algorithm
with a random-to-random (R2R) neighborhood does not admit any strict local
optima. Under standard settings, we confirm the result through extensive
simulations, observing only a few weak local optima. Interestingly, algorithms
using other neighborhoods of equal size exhibit suboptimal behavior, having
strict local optima and a substantial number of weak local optima.

</details>


### [153] [Reverse Diffusion Sequential Monte Carlo Samplers](https://arxiv.org/abs/2508.05926)
*Luhuan Wu,Yi Han,Christian A. Naesseth,John P. Cunningham*

Main category: stat.CO

TL;DR: 提出基于反向去噪扩散过程的新型顺序蒙特卡罗（SMC）方法用于从非归一化目标分布采样，引入SMC框架校正偏差，开发精确近似，在合成目标和实际贝叶斯推理问题上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的采样器因时间离散化和不完善的分数估计会积累误差，需解决偏差问题。

Method: 引入原则性SMC框架，将基于扩散的采样器作为提议，构建信息丰富的中间目标分布，用分数估计提议中的量开发精确近似。

Result: 所提出的RDSMC采样器在温和条件下能实现一致采样和目标归一化常数的无偏估计。

Conclusion: 该方法在一系列合成目标和实际贝叶斯推理问题上有效。

Abstract: We propose a novel sequential Monte Carlo (SMC) method for sampling from
unnormalized target distributions based on a reverse denoising diffusion
process. While recent diffusion-based samplers simulate the reverse diffusion
using approximate score functions, they can suffer from accumulating errors due
to time discretization and imperfect score estimation. In this work, we
introduce a principled SMC framework that formalizes diffusion-based samplers
as proposals while systematically correcting for their biases. The core idea is
to construct informative intermediate target distributions that progressively
steer the sampling trajectory toward the final target distribution. Although
ideal intermediate targets are intractable, we develop exact approximations
using quantities from the score estimation-based proposal, without requiring
additional model training or inference overhead. The resulting sampler, termed
RDSMC, enables consistent sampling and unbiased estimation of the target's
normalization constant under mild conditions. We demonstrate the effectiveness
of our method on a range of synthetic targets and real-world Bayesian inference
problems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [154] [Diverse Neural Sequences in QIF Networks: An Analytically Tractable Framework for Synfire Chains and Hippocampal Replay](https://arxiv.org/abs/2508.06085)
*Genki Shimizu,Taro Toyoizumi*

Main category: q-bio.NC

TL;DR: 提出含TAH规则的QIF神经元网络研究序列神经活动，能复现多种活动，推导FRE方程揭示机制，网络具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模型难平衡生物物理真实性和分析易处理性，需解决如何在生物约束下回忆不同序列的问题。

Method: 提出含TAH规则的QIF神经元简约网络。

Result: 该框架能复现多种序列活动，推导FRE方程阐明分叉结构和稳定性，网络对突触异质性和记忆模式重叠具强鲁棒性。

Conclusion: 含TAH连接的QIF网络是研究大脑序列神经活动的分析易处理且生物合理的平台。

Abstract: Sequential neural activity is fundamental to cognition, yet how diverse
sequences are recalled under biological constraints remains a key question.
Existing models often struggle to balance biophysical realism and analytical
tractability. We address this problem by proposing a parsimonious network of
Quadratic Integrate-and-Fire (QIF) neurons with sequences embedded via a
temporally asymmetric Hebbian (TAH) rule. Our findings demonstrate that this
single framework robustly reproduces a spectrum of sequential activities,
including persistent synfire-like chains and transient, hippocampal replay-like
bursts exhibiting intra-ripple frequency accommodation (IFA), all achieved
without requiring specialized delay or adaptation mechanisms. Crucially, we
derive exact low-dimensional firing-rate equations (FREs) that provide
mechanistic insight, elucidating the bifurcation structure governing these
distinct dynamical regimes and explaining their stability. The model also
exhibits strong robustness to synaptic heterogeneity and memory pattern
overlap. These results establish QIF networks with TAH connectivity as an
analytically tractable and biologically plausible platform for investigating
the emergence, stability, and diversity of sequential neural activity in the
brain.

</details>


### [155] [Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification](https://arxiv.org/abs/2508.06118)
*Daniil Vlasenko,Vadim Ushakov,Alexey Zaikin,Denis Zakharov*

Main category: q-bio.NC

TL;DR: 提出基于集成的功能磁共振成像（fMRI）数据图表示方法用于二元脑状态分类，在多个认知任务中取得高准确率，证明集成图有优势。


<details>
  <summary>Details</summary>
Motivation: 由于神经影像信号的高维性和固有噪声，理解和分类人类认知脑状态是神经科学的挑战问题，需更好方法。

Method: 利用多个基础机器学习模型构建图，边权重反映两种认知状态后验概率差异；用图的平均入射边权重作为特征，使用简单逻辑回归分类器；与经典基于相关性的图在图神经网络分类任务中比较。

Result: 简单逻辑回归分类器平均准确率在97.07% - 99.74%；所有实验中集成图分类准确率最高。

Conclusion: 集成图传达更丰富拓扑信息，增强脑状态区分能力；方法保留了fMRI图表示的边级可解释性，适用于多类和回归任务，可扩展到其他神经影像模式和病理状态分类。

Abstract: Understanding and classifying human cognitive brain states based on
neuroimaging data remains one of the foremost and most challenging problems in
neuroscience, owing to the high dimensionality and intrinsic noise of the
signals. In this work, we propose an ensemble-based graph representation method
of functional magnetic resonance imaging (fMRI) data for the task of binary
brain-state classification. Our method builds the graph by leveraging multiple
base machine-learning models: each edge weight reflects the difference in
posterior probabilities between two cognitive states, yielding values in the
range [-1, 1] that encode confidence in a given state. We applied this approach
to seven cognitive tasks from the Human Connectome Project (HCP 1200 Subject
Release), including working memory, gambling, motor activity, language, social
cognition, relational processing, and emotion processing. Using only the mean
incident edge weights of the graphs as features, a simple logistic-regression
classifier achieved average accuracies from 97.07% to 99.74%. We also compared
our ensemble graphs with classical correlation-based graphs in a classification
task with a graph neural network (GNN). In all experiments, the highest
classification accuracy was obtained with ensemble graphs. These results
demonstrate that ensemble graphs convey richer topological information and
enhance brain-state discrimination. Our approach preserves edge-level
interpretability of the fMRI graph representation, is adaptable to multiclass
and regression tasks, and can be extended to other neuroimaging modalities and
pathological-state classification.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [156] [Estimating the size of a set using cascading exclusion](https://arxiv.org/abs/2508.05901)
*Sourav Chatterjee,Persi Diaconis,Susan Holmes*

Main category: math.ST

TL;DR: 本文针对有限集大小估计问题，对已有极端情况估计方法进行细化，发展了一般非渐近理论，涵盖多种问题并给出非参数有限n误差界。


<details>
  <summary>Details</summary>
Motivation: 对有限集大小估计在不同情况下已有方法进行细化和完善，解决更广泛问题。

Method: 发展一般非渐近理论。

Result: 得到了对有限集大小估计、体积估计、未发现物种问题等多种问题的处理方法，并给出非参数有限n误差界。

Conclusion: 提出的理论和方法能有效解决多种相关估计和测试问题，给出误差界。

Abstract: Let $S$ be a finite set, and $X_1,\ldots,X_n$ an i.i.d. uniform sample from
$S$. To estimate the size $|S|$, without further structure, one can wait for
repeats and use the birthday problem. This requires a sample size of the order
$|S|^\frac{1}{2}$. On the other hand, if $S=\{1,2,\ldots,|S|\}$, the maximum of
the sample blown up by $n/(n-1)$ gives an efficient estimator based on any
growing sample size. This paper gives refinements that interpolate between
these extremes. A general non-asymptotic theory is developed. This includes
estimating the volume of a compact convex set, the unseen species problem, and
a host of testing problems that follow from the question `Is this new
observation a typical pick from a large prespecified population?' We also treat
regression style predictors. A general theorem gives non-parametric finite $n$
error bounds in all cases.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [157] [Sandwich Monotonicity and the Recognition of Weighted Graph Classes](https://arxiv.org/abs/2508.06216)
*Jesse Beisegel,Nina Chiarelli,Ekkehard Köhler,Matjaž Krnc,Martin Milanič,Nevena Pivač,Robert Scheffler,Martin Strehler*

Main category: cs.DM

TL;DR: 本文研究边加权图与无权图类对应关系，展示特定边加权图的线性时间识别方法，给出线性时间识别算法存在的充要条件。


<details>
  <summary>Details</summary>
Motivation: 边加权图在相关理论中重要，希望找到与无权图类对应的边加权图类的识别方法。

Method: 引入度三明治单调图类概念，利用特殊边消除顺序。

Result: 能在线性时间内识别所有层图为分裂图、阈值图或链图的加权图。

Conclusion: 给出对应无权图类为度三明治单调且含无边图的加权图类线性时间识别算法存在的充要条件。

Abstract: Edge-weighted graphs play an important role in the theory of Robinsonian
matrices and similarity theory, particularly via the concept of level graphs,
that is, graphs obtained from an edge-weighted graph by removing all
sufficiently light edges. This suggest a natural way of associating to any
class $\mathcal{G}$ of unweighted graphs a corresponding class of edge-weighted
graphs, namely by requiring that all level graphs belong to $\mathcal{G}$. We
show that weighted graphs for which all level graphs are split, threshold, or
chain graphs can be recognized in linear time using special edge elimination
orderings. We obtain these results by introducing the notion of degree sandwich
monotone graph classes. A graph class $\mathcal{G}$ is sandwich monotone if
every edge set which may be removed from a graph in $\mathcal{G}$ without
leaving the class also contains a single edge that can be safely removed.
Furthermore, if we require the safe edge to fulfill a certain degree property,
then $\mathcal{G}$ is called degree sandwich monotone. We present necessary and
sufficient conditions for the existence of a linear-time recognition algorithm
for any weighted graph class whose corresponding unweighted class is degree
sandwich monotone and contains all edgeless graphs.

</details>


### [158] [On Approximate MMS Allocations on Restricted Graph Classes](https://arxiv.org/abs/2508.06343)
*Václav Blažej,Michał Dębski ad Zbigniew Lonc,Marta Piecyk,Paweł Rzążewski*

Main category: cs.DM

TL;DR: 研究带连通性约束的不可分物品公平分配问题，证明在多种图类中近似分配存在


<details>
  <summary>Details</summary>
Motivation: 已知满足最大最小份额公平准则的分配在无连通约束时可能不存在，且部分图类存在近似分配，但所有图类是否存在近似分配是开放问题

Method: 对受限图类进行系统研究

Result: 证明在块图、仙人掌图、完全多部图和分裂图等多种图类中近似分配存在

Conclusion: 在多种研究广泛的图类中存在满足条件的近似分配

Abstract: We study the problem of fair division of a set of indivisible goods with
connectivity constraints. Specifically, we assume that the goods are
represented as vertices of a connected graph, and sets of goods allocated to
the agents are connected subgraphs of this graph. We focus on the
widely-studied maximin share criterion of fairness. It has been shown that an
allocation satisfying this criterion may not exist even without connectivity
constraints, i.e., if the graph of goods is complete. In view of this, it is
natural to seek approximate allocations that guarantee each agent a connected
bundle of goods with value at least a constant fraction of the maximin share
value to the agent. It is known that for some classes of graphs, such as
complete graphs, cycles, and $d$-claw-free graphs for any fixed $d$, such
approximate allocations indeed exist. However, it is an open problem whether
they exist for the class of all graphs.
  In this paper, we continue the systematic study of the existence of
approximate allocations on restricted graph classes. In particular, we show
that such allocations exist for several well-studied classes, including block
graphs, cacti, complete multipartite graphs, and split graphs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [159] [SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques](https://arxiv.org/abs/2507.12286)
*Anouk Oudshoorn,Magdalena Ortiz,Mantas Simkus*

Main category: cs.LO

TL;DR: 本文提出基于核心通用模型的SHACL在有本体情况下的验证语义，给出构建模型技术、重写技术，并研究验证复杂度。


<details>
  <summary>Details</summary>
Motivation: SHACL和OWL结合有吸引力，但语义差异是重大挑战，需解决在有本体情况下SHACL验证问题。

Method: 提出基于核心通用模型的语义，为Horn - ALCHIQ描述逻辑中的本体构建模型，用模型有限表示开发重写技术。

Result: 将有本体的SHACL验证简化为标准验证，发现简单本体使问题为EXPTIME - complete，数据复杂度为PTIME - complete。

Conclusion: 提供了有本体时SHACL验证的解决方案，并明确了验证复杂度。

Abstract: SHACL and OWL are two prominent W3C standards for managing RDF data. These
languages share many features, but they have one fundamental difference: OWL,
designed for inferring facts from incomplete data, makes the open-world
assumption, whereas SHACL is a constraint language that treats the data as
complete and must be validated under the closed-world assumption. The
combination of both formalisms is very appealing and has been called for, but
their semantic gap is a major challenge, semantically and computationally. In
this paper, we advocate a semantics for SHACL validation in the presence of
ontologies based on core universal models. We provide a technique for
constructing these models for ontologies in the rich data-tractable description
logic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to
develop a rewriting technique that reduces SHACL validation in the presence of
ontologies to standard validation. Finally, we study the complexity of SHACL
validation in the presence of ontologies, and show that even very simple
ontologies make the problem EXPTIME-complete, and PTIME-complete in data
complexity.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [160] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: 提出残差扰动攻击（ResPA）方法，以残差梯度为扰动方向引导对抗样本到损失函数平坦区域，实验表明其比现有方法有更好的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有基于转移的攻击方法忽略扰动方向影响，导致可迁移性有限。

Method: 提出ResPA方法，对输入梯度进行指数移动平均得到参考梯度，考虑当前梯度与参考梯度的残差来捕获全局扰动方向变化。

Result: 实验显示ResPA比现有典型基于转移的攻击方法有更好的可迁移性，与当前输入转换方法结合可进一步提升。

Conclusion: ResPA方法有效提升了对抗样本的可迁移性。

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [161] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: 本文提出UnGuide方法解决大模型潜在滥用问题，通过动态推理机制实现可控概念移除，效果优于现有基于LoRA的方法。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型有潜在滥用风险，需有效机器遗忘方法，而现有LoRA方法有局限性。

Method: 引入UnGuide，结合UnGuidance动态推理机制，基于去噪过程前几步稳定性调整引导比例，选择性进行LoRA适配。

Result: UnGuide能实现可控概念移除，保留扩散模型表达能力，在物体擦除和明确内容移除任务中优于现有基于LoRA的方法。

Conclusion: UnGuide是一种有效的机器遗忘方法，可解决扩散模型潜在滥用问题。

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [162] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: 提出用于预训练MRI Transformer少样本部署的实用框架，在脑成像任务中表现出色，适用于低资源临床环境。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在医学影像中因标注数据稀缺导致现实应用受限的问题。

Method: 在大规模多队列脑MRI数据集上用MAE预训练策略获取可迁移潜在表征；分类任务用冻结MAE编码器加轻量级线性头；分割任务提出MAE - FUnet混合架构。

Result: 分类任务在MRI序列识别中以最少监督达到最优准确率；分割任务在数据有限时MAE - FUnet优于其他基线模型。

Conclusion: 框架高效、稳定、可扩展，适用于低资源临床环境和更广泛神经影像应用。

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [163] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: 提出SINGAD框架用于单图像法线估计，解决多视图几何不一致和数据依赖问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的单图像法线估计方法依赖数据驱动统计先验，缺少光 - 表面交互显式建模，且扩散模型离散采样机制导致梯度不连续，需依赖密集法线标注。

Method: 提出SINGAD框架，集成物理驱动光交互建模和基于可微渲染的重投影策略，构建光交互驱动的3DGS重参数化模型，设计跨域特征融合模块，引入可微3D重投影损失策略。

Result: 在Google Scanned Objects数据集上的定量评估显示，该方法在多个指标上优于现有方法。

Conclusion: SINGAD框架有效解决了多视图几何不一致和数据依赖问题，提升了单图像法线估计性能。

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [164] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: 本文评估SAM和Mask3D在建筑场景的三维分割效果，指出当前方法不足，推动建筑进度监测技术发展。


<details>
  <summary>Details</summary>
Motivation: 建筑进度监测资源消耗大，传统方法在复杂建筑工地环境表现不佳，需探索基于计算机视觉的方法。

Method: 评估SAM和Mask3D在室内外复杂条件下的适应性和性能，进行对比分析。

Result: 展示了SAM和Mask3D的相对有效性，指出当前分割方法在户外场景缺乏基准的问题。

Conclusion: 需要定制分割工作流程，以实现更自动化、精确的建筑进度监测。

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [165] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: 提出Bifrost - 1框架，结合预训练多模态大语言模型和扩散模型实现高保真可控图像生成，训练效率高。


<details>
  <summary>Details</summary>
Motivation: 在不损害大语言模型推理能力的前提下，将高保真视觉合成能力集成到其中，解决现有方法训练成本高的问题。

Method: 使用补丁级CLIP图像嵌入作为潜在变量，通过轻量级调整扩散模型的ControlNet集成到扩散模型，给MLLM配备视觉生成分支。

Result: Bifrost - 1在视觉保真度和多模态理解方面达到或超过先前方法，训练计算成本大幅降低，消融实验验证设计有效性。

Conclusion: Bifrost - 1框架能有效结合预训练MLLMs和扩散模型，实现高保真可控图像生成，且训练效率显著提升。

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [166] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: 提出一种预训练深度补全模型的测试时自适应方法ETA，通过对抗扰动训练能量模型，在多个数据集上评估，比之前方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 预训练深度补全模型在迁移到新环境目标数据时因协变量偏移会产生错误输出。

Method: 量化深度预测属于源数据分布的可能性，利用对抗扰动探索数据空间，训练能量模型对深度预测区域进行分布内或外评分，在测试时更新模型参数以最小化能量。

Result: 在三个室内和三个室外数据集上评估，ETA在室外比之前方法平均提升6.94%，室内提升10.23%。

Conclusion: 提出的ETA方法能有效提升预训练深度补全模型在新环境数据上的性能。

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [167] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 本文开发扩散模型解决亚可见颗粒分析中数据不平衡问题，生成高保真图像增强训练集，提升分类性能并公开模型。


<details>
  <summary>Details</summary>
Motivation: 多类分类器应用于亚可见颗粒分析时，数据稀缺和类别不平衡是重大障碍，尤其对于数量少的颗粒类型。

Method: 开发扩散模型生成高保真图像增强训练集，训练多类深度神经网络。

Result: 生成样本在视觉质量和结构上与真实颗粒图像相似，在验证集上提升了分类性能。

Conclusion: 该方法有效解决数据不平衡问题，公开模型和分类器利于开放研究和可重复性。

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [168] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: 提出多模态情感识别框架应对MER - SEMI挑战，利用预训练模型提取特征，设计融合策略和多源标注策略，在数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 情感识别对提升人机交互很重要，解决MER2025竞赛的MER - SEMI挑战及数据稀缺问题。

Method: 利用预训练模型从多模态提取特征，设计双分支视觉编码器、上下文丰富方法，提出含自注意力机制和残差连接的融合策略，用多源标注策略精炼训练集噪声标签。

Result: 在MER2025 - SEMI数据集上加权F - score达87.49%，远超官方基线的78.63%。

Conclusion: 所提出的多模态情感识别框架有效。

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [169] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: 本文指出卫星光谱图像作为新模态对AGI发展有重要意义，回顾现有基准存在的局限，强调需要更全面的基准，并提出基准应涵盖的任务。


<details>
  <summary>Details</summary>
Motivation: 卫星光谱图像作为额外模态未得到应有的关注，但其对提升AGI理解自然世界的能力有很大潜力。

Method: 论证地球观测数据对智能模型的有用性，回顾现有基准并指出其局限性，提出一套全面的任务。

Result: 明确现有基准在评估该领域基础模型泛化能力方面存在局限。

Conclusion: 强调需要一个更全面的基准来评估地球观测模型。

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [170] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: 提出Fourier - VLM方法在频域压缩视觉表示，降低计算开销和推理延迟，实验证明其高效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有Vision - Language Models因大量视觉令牌增加上下文长度，导致高计算开销和推理延迟，以往方法有性能折损或额外成本问题。

Method: 基于视觉编码器输出的视觉特征在低频分量能量集中的观察，使用二维离散余弦变换（DCT）对视觉特征应用低通滤波器，通过快速傅里叶变换（FFT）算子高效计算DCT。

Result: Fourier - VLM在各种基于图像的基准测试中表现出竞争力，在LLaVA和Qwen - VL架构上有强泛化性，相比LLaVA - v1.5减少推理FLOPs达83.8%，提升生成速度31.2%。

Conclusion: Fourier - VLM方法简单高效，具有优越的效率和实用性。

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [171] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文介绍了用于评估大语言模型和多模态大语言模型的BioMotion Arena框架，通过视觉动画评估，数据显示其能提供有区分度反馈，多数被评估模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前基准评估方法难以给用户提供关于模型性能差异的即时、直观反馈，因此需要新的评估框架。

Method: 引入BioMotion Arena框架，采用成对比较评估，利用点光源成像放大模型性能差异，收集超4.5万张选票。

Result: 众包的人类投票与专家评分高度一致，表明BioMotion Arena能提供有区分度的反馈；超90%被评估模型无法生成基本人形点光源组。

Conclusion: BioMotion Arena可作为具有挑战性的性能可视化基准和无真实值限制的灵活评估框架。

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [172] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: 提出从常规临床MR扫描生成超分辨率、患者特定的3D伪健康目标形态的管道，可用于治疗滑车发育不良，减少辐射且效果良好，代码和可视化公开。


<details>
  <summary>Details</summary>
Motivation: 当前治疗滑车发育不良的方法依赖低分辨率MR扫描和手术直觉，手术规划凭经验，微创技术应用有限且结果不一致。

Method: 先使用隐式神经表示计算各向同性超分辨率MR体积，再用多标签定制训练网络分割骨骼，最后训练小波扩散模型生成滑车区域伪健康目标形态。

Result: 在25例滑车发育不良患者上评估，目标形态显著改善了沟角和滑车沟深度。

Conclusion: 该方法能生成亚毫米分辨率的3D形状，可用于术前和术中，无需CT减少辐射，有较好治疗效果。

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [173] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

TL;DR: 提出用于手写数学表达式识别（HMER）的自监督学习框架，无需大量标注数据，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: HMER因二维结构、符号尺度变化和复杂空间关系具有挑战性，且标注数据昂贵，需无标注数据方法。

Method: 用全局和局部对比损失预训练图像编码器；采用渐进空间掩码策略训练自监督注意力网络；完整流程包括编码器自监督预训练、注意力学习和用Transformer解码器监督微调。

Result: 在CROHME基准测试中，方法优于现有自监督和全监督基线。

Conclusion: 提出的渐进注意力机制能有效提升HMER性能。

Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [174] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

TL;DR: 提出FMCE - Net++训练框架，结合FMCS预测和任务标签优化模型，实验证明能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有Feature Map Convergence Evaluation（FMCE）缺乏实验验证和闭环集成，为解决此局限开展研究。

Method: 提出FMCE - Net++训练框架，集成预训练的FMCE - Net作为辅助头，通过Representation Auxiliary Loss联合监督骨干网络优化，用可调节因子平衡主要分类损失和特征收敛优化。

Result: 在MNIST、CIFAR - 10等数据集上实验，ResNet - 50/CIFAR - 10准确率提升1.16 pp，ShuffleNet v2/CIFAR - 100准确率提升1.08 pp。

Conclusion: FMCE - Net++无需架构修改和额外数据，能有效提升现有模型性能上限。

Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [175] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: 提出基于3D眼球结构的3D注视重定向框架，有自适应变形模块，实验效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经辐射场的注视重定向方法未显式建模3D表示的旋转和平移，需改进。

Method: 引入3D眼球结构用3D高斯溅射表示眼球，显式旋转和平移该结构生成图像，还有自适应变形模块。

Result: 在ETH - XGaze数据集实验表明，能生成多样新注视图像，图像质量和注视估计精度超现有方法。

Conclusion: 所提框架有效，在注视重定向任务上表现优于先前方法。

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [176] [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.06452)
*Mattia Litrico,Mario Valerio Giuffrida,Sebastiano Battiato,Devis Tuia*

Main category: cs.CV

TL;DR: 本文提出TRUST，一种利用语言模态引导视觉模型适应的无监督领域自适应方法，在经典和复杂领域转移任务上取得新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有无监督领域自适应方法在复杂领域转移（如地理转移）下效果不佳，语言模态有助于适应过程且对复杂转移更鲁棒，因此提出利用语言模态的方法。

Method: TRUST为目标样本从其字幕生成伪标签，引入不确定性估计策略重新加权分类损失，提出多模态软对比学习损失来对齐视觉和语言特征空间。

Result: 该方法在经典（DomainNet）和复杂（GeoNet）领域转移任务上超越先前方法，创造新的最优结果。

Conclusion: 提出的TRUST方法有效，能利用语言模态的鲁棒性提升视觉模型在领域自适应中的表现。

Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success
in addressing classical domain shifts (e.g., synthetic-to-real), but they still
suffer under complex shifts (e.g. geographical shift), where both the
background and object appearances differ significantly across domains. Prior
works showed that the language modality can help in the adaptation process,
exhibiting more robustness to such complex shifts. In this paper, we introduce
TRUST, a novel UDA approach that exploits the robustness of the language
modality to guide the adaptation of a vision model. TRUST generates
pseudo-labels for target samples from their captions and introduces a novel
uncertainty estimation strategy that uses normalised CLIP similarity scores to
estimate the uncertainty of the generated pseudo-labels. Such estimated
uncertainty is then used to reweight the classification loss, mitigating the
adverse effects of wrong pseudo-labels obtained from low-quality captions. To
further increase the robustness of the vision model, we propose a multimodal
soft-contrastive learning loss that aligns the vision and language feature
spaces, by leveraging captions to guide the contrastive training of the vision
model on target images. In our contrastive loss, each pair of images acts as
both a positive and a negative pair and their feature representations are
attracted and repulsed with a strength proportional to the similarity of their
captions. This solution avoids the need for hardly determining positive and
negative pairs, which is critical in the UDA setting. Our approach outperforms
previous methods, setting the new state-of-the-art on classical (DomainNet) and
complex (GeoNet) domain shifts. The code will be available upon acceptance.

</details>


### [177] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: 本文提出了用于水下重建的新框架UW - 3DGS，在实验中表现出色，减少了浮动伪影。


<details>
  <summary>Details</summary>
Motivation: 传统水下3D场景重建方法受光线吸收、散射和浑浊度影响，NeRF扩展在模糊环境下效率和空间分辨率受限，需要新的水下重建方法。

Method: 引入UW - 3DGS框架，包含可插拔的可学习水下图像形成模块和物理感知不确定性剪枝（PAUP）分支，分训练和渲染阶段处理。

Result: 在SeaThru - NeRF和UWBundle数据集上表现优越，在SeaThru - NeRF上PSNR为27.604、SSIM为0.868、LPIPS为0.104，浮动伪影减少约65%。

Conclusion: UW - 3DGS框架能有效进行水下3D场景重建，减少伪影，实现准确的光线传输。

Abstract: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [178] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: 研究提出多向架构框架用于结肠镜图像息肉自动检测，结合合成数据生成、检测与分割算法，评估多种分割模型，各有表现。


<details>
  <summary>Details</summary>
Motivation: 结肠镜检查对结直肠癌早诊重要，解决医疗数据集小和标注复杂问题。

Method: 引入多向架构框架，用Stable Diffusion增强生成合成数据，结合Faster R - CNN定位和SAM细化分割掩码，评估五种分割模型。

Result: Faster R - CNN召回率93.08%、精度88.97%、F1分数90.98%；FPN的PSNR和SSIM得分最高，UNet召回率优，LinkNet的IoU和Dice得分表现平衡。

Conclusion: 研究提出的方法在结肠镜图像息肉检测和分割中有一定效果，不同模型有不同优势。

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [179] [WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion](https://arxiv.org/abs/2508.06485)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.CV

TL;DR: 提出WGAST框架用于每日10米分辨率LST估计，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 城市化、气候变化和农业压力增加对精确及时环境监测的需求，现有系统在空间和时间分辨率上存在权衡，少有方法能估计10米分辨率的每日LST。

Method: 提出WGAST，采用条件生成对抗架构，生成器分特征提取、融合、LST重建和噪声抑制四个阶段，训练采用弱监督策略并由PatchGAN鉴别器强化。

Result: WGAST在定量和定性评估中均优于现有方法，平均降低RMSE 17.18%，提高SSIM 11.00%，对云致LST具有鲁棒性，能有效捕捉精细热模式。

Conclusion: WGAST是首个用于此任务的端到端深度学习框架，性能良好，代码公开。

Abstract: Urbanization, climate change, and agricultural stress are increasing the
demand for precise and timely environmental monitoring. Land Surface
Temperature (LST) is a key variable in this context and is retrieved from
remote sensing satellites. However, these systems face a trade-off between
spatial and temporal resolution. While spatio-temporal fusion methods offer
promising solutions, few have addressed the estimation of daily LST at 10 m
resolution. In this study, we present WGAST, a Weakly-Supervised Generative
Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra
MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning
framework designed for this task. It adopts a conditional generative
adversarial architecture, with a generator composed of four stages: feature
extraction, fusion, LST reconstruction, and noise suppression. The first stage
employs a set of encoders to extract multi-level latent representations from
the inputs, which are then fused in the second stage using cosine similarity,
normalization, and temporal attention mechanisms. The third stage decodes the
fused features into high-resolution LST, followed by a Gaussian filter to
suppress high-frequency noise. Training follows a weakly supervised strategy
based on physical averaging principles and reinforced by a PatchGAN
discriminator. Experiments demonstrate that WGAST outperforms existing methods
in both quantitative and qualitative evaluations. Compared to the
best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves
SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and
effectively captures fine-scale thermal patterns, as validated against 33
ground-based sensors. The code is available at
https://github.com/Sofianebouaziz1/WGAST.git.

</details>


### [180] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: 提出用于多模态大语言模型持续视觉指令调优的高效架构扩展方法LiLoRA，实验显示其在顺序任务学习中表现优且参数效率高。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型持续视觉指令调优存在灾难性遗忘问题，现有架构扩展方法参数开销大、可扩展性差。

Method: 引入LiLoRA，跨任务共享LoRA矩阵A，对矩阵B进行额外低秩分解，引入余弦正则化稳定性损失。

Result: 在多样的持续视觉指令调优基准测试中，LiLoRA在顺序任务学习中始终表现出色，且参数效率显著提高。

Conclusion: LiLoRA是一种高效的架构扩展方法，能有效解决多模态大语言模型持续视觉指令调优的问题。

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [181] [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)
*Zhangquan Chen,Ruihui Zhao,Chuwei Luo,Mingze Sun,Xinlei Yu,Yangyang Kang,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出SIFThinker框架解决MLLMs在复杂视觉任务挑战，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在复杂视觉任务有挑战，以往方法未能利用空间线索进行注意力校正。

Method: 引入反向扩展 - 正向推理策略构建SIF - 50K数据集；提出GRPO - SIF强化训练范式。

Result: SIFThinker在空间理解和细粒度视觉感知上优于现有方法，且保持强大通用能力。

Conclusion: SIFThinker方法有效。

Abstract: Current multimodal large language models (MLLMs) still face significant
challenges in complex visual tasks (e.g., spatial understanding, fine-grained
perception). Prior methods have tried to incorporate visual reasoning, however,
they fail to leverage attention correction with spatial cues to iteratively
refine their focus on prompt-relevant regions. In this paper, we introduce
SIFThinker, a spatially-aware "think-with-images" framework that mimics human
visual perception. Specifically, SIFThinker enables attention correcting and
image region focusing by interleaving depth-enhanced bounding boxes and natural
language. Our contributions are twofold: First, we introduce a
reverse-expansion-forward-inference strategy that facilitates the generation of
interleaved image-text chains of thought for process-level supervision, which
in turn leads to the construction of the SIF-50K dataset. Besides, we propose
GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual
grounding into a unified reasoning pipeline, teaching the model to dynamically
correct and focus on prompt-relevant regions. Extensive experiments demonstrate
that SIFThinker outperforms state-of-the-art methods in spatial understanding
and fine-grained visual perception, while maintaining strong general
capabilities, highlighting the effectiveness of our method.

</details>


### [182] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: 提出GS - MoE框架解决弱监督视频异常检测难题，在多个数据集获SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督视频异常检测模型难以处理复杂真实事件，存在无法应对异常类型多样性和弱监督信号缺乏精确时间信息问题。

Method: 提出GS - MoE框架，用一组专家模型捕捉特定异常类型，由时间高斯散点损失引导，通过混合专家机制整合预测。

Result: 在UCF - Crime数据集AUC达91.58%，在XD - Violence和MSAD数据集结果优越。

Conclusion: GS - MoE利用特定类别专业知识和时间引导，为弱监督视频异常检测设新基准。

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [183] [Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd](https://arxiv.org/abs/2508.06357)
*Aman Bhatta,Maria Dhakal,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: 提出用额外注册图像预测一对一多面部识别中排名第一结果是否为库内的新方法，实验表明该方法对多种质量图像有效，且各人口群体分类准确率相似，新匹配器效果更好。


<details>
  <summary>Details</summary>
Motivation: 解决一对一多面部识别中判断排名第一结果是否为库外的问题，减少误报和调查时间浪费。

Method: 利用排名第一身份的额外注册图像生成训练数据，训练分类器进行预测。

Result: 该方法对多种质量图像有效，各人口群体分类准确率相似，新匹配器效果更好。

Conclusion: 该方法有潜力客观估计是否为库外识别，减少误报等问题，且效果依赖于匹配器训练方法。

Abstract: A central problem in one-to-many facial identification is that the person in
the probe image may or may not have enrolled image(s) in the gallery; that is,
may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one
result is Out-of-gallery have mostly focused on finding a suitable threshold on
the similarity score. We take a new approach, using the additional enrolled
images of the identity with the rank-one result to predict if the rank-one
result is In-gallery / Out-of-gallery. Given a gallery of identities and
images, we generate In-gallery and Out-of-gallery training data by extracting
the ranks of additional enrolled images corresponding to the rank-one identity.
We then train a classifier to utilize this feature vector to predict whether a
rank-one result is In-gallery or Out-of-gallery. Using two different datasets
and four different matchers, we present experimental results showing that our
approach is viable for mugshot quality probe images, and also, importantly, for
probes degraded by blur, reduced resolution, atmospheric turbulence and
sunglasses. We also analyze results across demographic groups, and show that
In-gallery / Out-of-gallery classification accuracy is similar across
demographics. Our approach has the potential to provide an objective estimate
of whether a one-to-many facial identification is Out-of-gallery, and thereby
to reduce false positive identifications, wrongful arrests, and wasted
investigative time. Interestingly, comparing the results of older deep
CNN-based face matchers with newer ones suggests that the effectiveness of our
Out-of-gallery detection approach emerges only with matchers trained using
advanced margin-based loss functions.

</details>


### [184] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 本文研究超分辨率与分类关系，提出新方法优化损失函数提升合成孔径雷达图像分辨率，改善图像质量并提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统超分辨率方法仅关注像素级图像质量提升，未充分探索超分辨率图像保真度与下游分类性能关系，因此研究将分类目标融入超分辨率过程能否提高分类准确率。

Method: 部署专门算法策略，通过优化兼顾图像质量和分类性能的损失函数，提高合成孔径雷达图像分辨率。

Result: 该方法能提高科学确定的图像质量指标所衡量的图像质量，同时提升分类准确率。

Conclusion: 将分类目标融入超分辨率过程可以进一步提高分类准确率，所提方法有效。

Abstract: High-resolution imagery plays a critical role in improving the performance of
visual recognition tasks such as classification, detection, and segmentation.
In many domains, including remote sensing and surveillance, low-resolution
images can limit the accuracy of automated analysis. To address this,
super-resolution (SR) techniques have been widely adopted to attempt to
reconstruct high-resolution images from low-resolution inputs. Related
traditional approaches focus solely on enhancing image quality based on
pixel-level metrics, leaving the relationship between super-resolved image
fidelity and downstream classification performance largely underexplored. This
raises a key question: can integrating classification objectives directly into
the super-resolution process further improve classification accuracy? In this
paper, we try to respond to this question by investigating the relationship
between super-resolution and classification through the deployment of a
specialised algorithmic strategy. We propose a novel methodology that increases
the resolution of synthetic aperture radar imagery by optimising loss functions
that account for both image quality and classification performance. Our
approach improves image quality, as measured by scientifically ascertained
image quality indicators, while also enhancing classification accuracy.

</details>


### [185] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

TL;DR: 本文提出基于GAN的半监督学习框架，用于低标注数据场景，在多个MedMNIST数据集上表现优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学成像中受限于标注数据不足，需要新方法解决低标注数据下的学习问题。

Method: 集成生成器、判别器和分类器，采用三阶段训练框架，交替进行有监督和无监督学习，使用基于集成的伪标签方法。

Result: 在11个MedMNIST数据集上，显著优于6种现有基于GAN的半监督方法，在极端5-shot场景表现出色。

Conclusion: 该方法为标注成本高的医学成像应用提供实用解决方案，少量标注数据也能实现强分类性能。

Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel GAN-based semi-supervised learning framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for class-conditioned image translation, a discriminator for
authenticity assessment and classification, and a dedicated classifier --
within a three-phase training framework. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ ensemble-based pseudo-labeling that
combines confidence-weighted predictions from the discriminator and classifier
with temporal consistency through exponential moving averaging, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven MedMNIST datasets demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme 5-shot setting
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.

</details>


### [186] [CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment](https://arxiv.org/abs/2508.06434)
*Shengzhu Yang,Jiawei Du,Shuai Lu,Weihang Zhang,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: 提出CLIPin非对比式插件改进CLIP多模态语义对齐，设计预投影器，实验证明其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 大规模自然图像文本数据集语义对齐弱、医学数据集内容多样性低，影响CLIP学习鲁棒通用表征。

Method: 提出CLIPin插件集成到CLIP架构，设计两个共享预投影器以参数折衷方式整合对比与非对比学习。

Result: 在不同下游任务的大量实验表明CLIPin作为即插即用组件有效且通用。

Conclusion: CLIPin能改善多模态语义对齐，与各种对比框架兼容。

Abstract: Large-scale natural image-text datasets, especially those automatically
collected from the web, often suffer from loose semantic alignment due to weak
supervision, while medical datasets tend to have high cross-modal correlation
but low content diversity. These properties pose a common challenge for
contrastive language-image pretraining (CLIP): they hinder the model's ability
to learn robust and generalizable representations. In this work, we propose
CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated
into CLIP-style architectures to improve multimodal semantic alignment,
providing stronger supervision and enhancing alignment robustness. Furthermore,
two shared pre-projectors are designed for image and text modalities
respectively to facilitate the integration of contrastive and non-contrastive
learning in a parameter-compromise manner. Extensive experiments on diverse
downstream tasks demonstrate the effectiveness and generality of CLIPin as a
plug-and-play component compatible with various contrastive frameworks. Code is
available at https://github.com/T6Yang/CLIPin.

</details>


### [187] [Text Embedded Swin-UMamba for DeepLesion Segmentation](https://arxiv.org/abs/2508.06453)
*Ruida Cheng,Tejas Sudharshan Mathai,Pritam Mukherjee,Benjamin Hou,Qingqing Zhu,Zhiyong Lu,Matthew McAuliffe,Ronald M. Summers*

Main category: cs.CV

TL;DR: 研究将文本集成到Swin - UMamba架构用于病变分割，在测试集取得高Dice分数和低Hausdorff距离，模型优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型集成到病变分割工作流，结合影像特征和放射学报告中病变特征描述，以用于慢性疾病临床评估。

Method: 使用公开的ULS23 DeepLesion数据集和报告中的简短描述，将文本集成到Swin - UMamba架构进行病变分割。

Result: 在测试集上病变分割获得82%的高Dice分数和6.58（像素）的低Hausdorff距离，Text - Swin - UMamba模型优于先前方法。

Conclusion: 将文本集成到Swin - UMamba架构用于病变分割是可行的，所提模型性能良好。

Abstract: Segmentation of lesions on CT enables automatic measurement for clinical
assessment of chronic diseases (e.g., lymphoma). Integrating large language
models (LLMs) into the lesion segmentation workflow offers the potential to
combine imaging features with descriptions of lesion characteristics from the
radiology reports. In this study, we investigate the feasibility of integrating
text into the Swin-UMamba architecture for the task of lesion segmentation. The
publicly available ULS23 DeepLesion dataset was used along with short-form
descriptions of the findings from the reports. On the test dataset, a high Dice
Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for
lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior
approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <
0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by
1.74% and 0.22%, respectively. The dataset and code can be accessed at
https://github.com/ruida/LLM-Swin-UMamba

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [188] [Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems](https://arxiv.org/abs/2508.05846)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.CY

TL;DR: 本文强调AI决策过程透明度对开发可信且符合伦理的机器人系统至关重要，探讨其作用、实施挑战并提出增强方法，还分析影响并为该领域发展提供方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI和机器人技术渗透社会，确保其伦理行为至关重要，因此研究AI决策过程透明度对开发伦理机器人系统的作用。

Method: 探讨透明度的作用，概述实施透明度的技术、伦理和实践挑战，提出包括标准化指标、可解释AI技术和用户友好界面等增强透明度的新方法，引入连接技术实现与伦理考量的框架。

Result: 分析了重视透明度对公众信任、监管政策和未来研究途径的影响。

Conclusion: 将透明度定位为伦理AI系统设计的基本要素，为负责任的AI和机器人技术的讨论提供方向，推动该领域未来发展。

Abstract: As artificial intelligence (AI) and robotics increasingly permeate society,
ensuring the ethical behavior of these systems has become paramount. This paper
contends that transparency in AI decision-making processes is fundamental to
developing trustworthy and ethically aligned robotic systems. We explore how
transparency facilitates accountability, enables informed consent, and supports
the debugging of ethical algorithms. The paper outlines technical, ethical, and
practical challenges in implementing transparency and proposes novel approaches
to enhance it, including standardized metrics, explainable AI techniques, and
user-friendly interfaces. This paper introduces a framework that connects
technical implementation with ethical considerations in robotic systems,
focusing on the specific challenges of achieving transparency in dynamic,
real-world contexts. We analyze how prioritizing transparency can impact public
trust, regulatory policies, and avenues for future research. By positioning
transparency as a fundamental element in ethical AI system design, we aim to
add to the ongoing discussion on responsible AI and robotics, providing
direction for future advancements in this vital field.

</details>


### [189] [Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education](https://arxiv.org/abs/2508.05979)
*Xinming Yang,Haasil Pujara,Jun Li*

Main category: cs.CY

TL;DR: 本文提出学生教大语言模型解决问题的教学范式，开发相关策略和系统，经课程评估，该主动学习方法提升了学生成绩。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作虚拟导师易导致被动学习和过度依赖，需要新教学范式。

Method: 开发设计有知识缺口问题的策略，引入Socrates系统，让学生教大语言模型解决问题。

Result: 在本科课程评估中，该主动学习方法使学生成绩相比历史同期有显著提升。

Conclusion: 该方法是利用大语言模型加深学生参与度和掌握度的实用、经济框架。

Abstract: While Large Language Models (LLMs) are often used as virtual tutors in
computer science (CS) education, this approach can foster passive learning and
over-reliance. This paper presents a novel pedagogical paradigm that inverts
this model: students act as instructors who must teach an LLM to solve
problems. To facilitate this, we developed strategies for designing questions
with engineered knowledge gaps that only a student can bridge, and we introduce
Socrates, a system for deploying this method with minimal overhead. We
evaluated our approach in an undergraduate course and found that this
active-learning method led to statistically significant improvements in student
performance compared to historical cohorts. Our work demonstrates a practical,
cost-effective framework for using LLMs to deepen student engagement and
mastery.

</details>


### [190] [Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks](https://arxiv.org/abs/2508.06411)
*Ze Shen Chin*

Main category: cs.CY

TL;DR: 本文探讨六种常见的人工智能灾难性风险，通过维度分析和风险路径建模，为管理人工智能风险提供结构化基础。


<details>
  <summary>Details</summary>
Motivation: 当前关于人工智能风险的讨论缺乏全面、多维框架和具体因果路径，本文旨在弥补这一差距。

Method: 对六种风险在七个关键维度进行特征描述，并进行风险路径建模，描绘从初始危险到最终危害的逐步过程。

Result: 维度分析支持系统的风险识别和通用缓解策略，风险路径模型有助于确定特定场景的干预措施。

Conclusion: 这些方法为管理人工智能全价值链的灾难性风险提供了更结构化和可操作的基础。

Abstract: Although discourse around the risks of Artificial Intelligence (AI) has
grown, it often lacks a comprehensive, multidimensional framework, and concrete
causal pathways mapping hazard to harm. This paper aims to bridge this gap by
examining six commonly discussed AI catastrophic risks: CBRN, cyber offense,
sudden loss of control, gradual loss of control, environmental risk, and
geopolitical risk. First, we characterize these risks across seven key
dimensions, namely intent, competency, entity, polarity, linearity, reach, and
order. Next, we conduct risk pathway modeling by mapping step-by-step
progressions from the initial hazard to the resulting harms. The dimensional
approach supports systematic risk identification and generalizable mitigation
strategies, while risk pathway models help identify scenario-specific
interventions. Together, these methods offer a more structured and actionable
foundation for managing catastrophic AI risks across the value chain.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [191] [A variational approach to dimension-free self-normalized concentration](https://arxiv.org/abs/2508.06483)
*Ben Chugg,Aaditya Ramdas*

Main category: math.PR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the self-normalized concentration of vector-valued stochastic
processes. We focus on bounds for sub-$\psi$ processes, a tail condition that
encompasses a wide variety of well-known distributions (including
sub-exponential, sub-Gaussian, sub-gamma, and sub-Poisson distributions). Our
results recover and generalize the influential bound of Abbasi-Yadkori et al.
(2011) and fill a gap in the literature between determinant-based bounds and
those based on condition numbers. As applications we prove a Bernstein
inequality for random vectors satisfying a moment condition (which is more
general than boundedness), and also provide the first dimension-free,
self-normalized empirical Bernstein inequality. Our techniques are based on the
variational (PAC-Bayes) approach to concentration.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [192] [Data-Driven Density Steering via the Gromov-Wasserstein Optimal Transport Distance](https://arxiv.org/abs/2508.06052)
*Haruto Nakashima,Siddhartha Ganguly,Kenji Kashima*

Main category: math.OC

TL;DR: 使用Gromov - Wasserstein度量解决数据驱动机会约束密度导向问题，将最优控制问题转化为凸差规划并用DC算法求解，数值结果验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动的机会约束密度导向问题，在系统未知情况下利用输入输出数据进行控制。

Method: 用Gromov - Wasserstein度量，将最优控制问题转化为凸差规划，使用DC算法求解。

Result: 通过各种数据驱动方案的数值结果验证了方法的有效性。

Conclusion: 所提出的方法能有效解决数据驱动的机会约束密度导向问题。

Abstract: We tackle the data-driven chance-constrained density steering problem using
the Gromov-Wasserstein metric. The underlying dynamical system is an unknown
linear controlled recursion, with the assumption that sufficiently rich
input-output data from pre-operational experiments are available. The initial
state is modeled as a Gaussian mixture, while the terminal state is required to
match a specified Gaussian distribution. We reformulate the resulting optimal
control problem as a difference-of-convex program and show that it can be
efficiently and tractably solved using the DC algorithm. Numerical results
validate our approach through various data-driven schemes.

</details>


### [193] [LLM Serving Optimization with Variable Prefill and Decode Lengths](https://arxiv.org/abs/2508.06133)
*Meixuan Wang,Yinyu Ye,Zijie Zhou*

Main category: math.OC

TL;DR: 研究LLM请求服务问题，证明问题NP难，分析常用策略缺陷，提出新算法并证明有常数竞争比，开发变体并验证性能。


<details>
  <summary>Details</summary>
Motivation: 解决具有异构预填充和解码长度的LLM请求服务问题，最小化总完成时间。

Method: 分析常用调度策略，提出基于新选择指标的算法，开发动态规划、局部搜索、基于LP的调度器等变体。

Result: 新算法有常数竞争比，变体在综合模拟中优于标准基线且保持计算效率。

Conclusion: 提出的算法和变体可有效解决LLM请求服务问题，在性能和计算效率上有优势。

Abstract: We study the problem of serving LLM (Large Language Model) requests where
each request has heterogeneous prefill and decode lengths. In LLM serving, the
prefill length corresponds to the input prompt length, which determines the
initial memory usage in the KV cache. The decode length refers to the number of
output tokens generated sequentially, with each additional token increasing the
KV cache memory usage by one unit. Given a set of n requests, our goal is to
schedule and process them to minimize the total completion time. We show that
this problem is NP-hard due to the interplay of batching, placement
constraints, precedence relationships, and linearly increasing memory usage. We
then analyze commonly used scheduling strategies in practice, such as
First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their
competitive ratios scale up sublinearly with the memory limit-a significant
drawback in real-world settings where memory demand is large. To address this,
we propose a novel algorithm based on a new selection metric that efficiently
forms batches over time. We prove that this algorithm achieves a constant
competitive ratio. Finally, we develop and evaluate a few algorithm variants
inspired by this approach, including dynamic programming variants, local search
methods, and an LP-based scheduler, demonstrating through comprehensive
simulations that they outperform standard baselines while maintaining
computational efficiency.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [194] [Automated Visualization Makeovers with LLMs](https://arxiv.org/abs/2508.05637)
*Siddharth Gangwar,David A. Selby,Sebastian J. Vollmer*

Main category: cs.HC

TL;DR: 本文探讨多模态大语言模型能否进行可视化改进，介绍基于提示工程的系统，进行定量评估并将工具做成网页小程序。


<details>
  <summary>Details</summary>
Motivation: 制作好的图表未在数据科学课程中教授，可视化改进可提升图表质量，研究多模态大语言模型能否完成此任务。

Method: 对预训练模型进行提示工程，结合用户指定指南和大语言模型训练语料中的潜在知识，对不同图表类型的绘图问题进行定量评估。

Result: 开发出可根据最佳实践为用户现有数据可视化提供改进建议的系统，并以简单的自托管小程序和网页界面形式提供工具。

Conclusion: 多模态大语言模型可用于半自动化生成建设性批评以改进数据可视化，能帮助用户提升现有可视化效果。

Abstract: Making a good graphic that accurately and efficiently conveys the desired
message to the audience is both an art and a science, typically not taught in
the data science curriculum. Visualisation makeovers are exercises where the
community exchange feedback to improve charts and data visualizations. Can
multi-modal large language models (LLMs) emulate this task? Given a plot in the
form of an image file, or the code used to generate it, an LLM, primed with a
list of visualization best practices, is employed to semi-automatically
generate constructive criticism to produce a better plot. Our system is centred
around prompt engineering of a pre-trained model, relying on a combination of
userspecified guidelines and any latent knowledge of data visualization
practices that might lie within an LLMs training corpus. Unlike other works,
the focus is not on generating valid visualization scripts from raw data or
prompts, but on educating the user how to improve their existing data
visualizations according to an interpretation of best practices. A quantitative
evaluation is performed to measure the sensitivity of the LLM agent to various
plotting issues across different chart types. We make the tool available as a
simple self-hosted applet with an accessible Web interface.

</details>


### [195] [Modeling Interactive Narrative Systems: A Formal Approach](https://arxiv.org/abs/2508.05653)
*Jules Clerc,Domitile Lourdeaux,Mohamed Sallak,Johann Barbier,Marc Ravaine*

Main category: cs.HC

TL;DR: 文章提出交互式叙事系统（INS）正式表示框架，经实验验证其有用性，旨在促进INS研究社区合作与一致性。


<details>
  <summary>Details</summary>
Motivation: INS领域研究分散、系统表示多样，面临挑战，需统一框架。

Method: 受现有先进方法启发，提出正式表示框架，提供一致词汇和建模结构。

Result: 在“小红帽”场景实验验证了框架有用性及对改善INS评估的影响。

Conclusion: 提出的形式化方法能促进INS研究社区的协作与一致性。

Abstract: Interactive Narrative Systems (INS) have revolutionized digital experiences
by empowering users to actively shape their stories, diverging from traditional
passive storytelling. However, the field faces challenges due to fragmented
research efforts and diverse system representations. This paper introduces a
formal representation framework for INS, inspired by diverse approaches from
the state of the art. By providing a consistent vocabulary and modeling
structure, the framework facilitates the analysis, the description and
comparison of INS properties. Experimental validations on the "Little Red
Riding Hood" scenario highlight the usefulness of the proposed formalism and
its impact on improving the evaluation of INS. This work aims to foster
collaboration and coherence within the INS research community by proposing a
methodology for formally representing these systems.

</details>


### [196] [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)
*Stefan Pasch,Min Chul Cha*

Main category: cs.HC

TL;DR: 研究分析超10万条AI产品用户评论，发现伦理AI的七个维度与用户满意度正相关，且关系因用户和产品类型而异。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏从用户角度关于伦理AI原则是否被认可、重视及产生影响的实证证据，研究伦理AI与用户满意度的联系。

Method: 分析G2上超10万条AI产品用户评论，用基于Transformer的语言模型测量欧盟可信AI伦理指南定义的七个伦理维度的情感。

Result: 七个维度都与用户满意度正相关，关系因用户和产品类型系统地变化，非技术用户和终端应用在各维度上伦理AI与用户满意度的关联更强。

Conclusion: 强调从用户角度进行伦理AI设计的重要性，以及考虑用户角色和产品类型上下文差异的必要性。

Abstract: As AI systems become increasingly embedded in organizational workflows and
consumer applications, ethical principles such as fairness, transparency, and
robustness have been widely endorsed in policy and industry guidelines.
However, there is still scarce empirical evidence on whether these principles
are recognized, valued, or impactful from the perspective of users. This study
investigates the link between ethical AI and user satisfaction by analyzing
over 100,000 user reviews of AI products from G2. Using transformer-based
language models, we measure sentiment across seven ethical dimensions defined
by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all
seven dimensions are positively associated with user satisfaction. Yet, this
relationship varies systematically across user and product types. Technical
users and reviewers of AI development platforms more frequently discuss
system-level concerns (e.g., transparency, data governance), while
non-technical users and reviewers of end-user applications emphasize
human-centric dimensions (e.g., human agency, societal well-being). Moreover,
the association between ethical AI and user satisfaction is significantly
stronger for non-technical users and end-user applications across all
dimensions. Our results highlight the importance of ethical AI design from
users' perspectives and underscore the need to account for contextual
differences across user roles and product types.

</details>


### [197] [REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition](https://arxiv.org/abs/2508.05933)
*Xueyuan Xu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: 本文提出一种用于缺失多维情感识别的EEG特征选择方法，在三个数据集上实验表明该方法在EEG情感特征选择的鲁棒性上优于十三种先进特征选择方法。


<details>
  <summary>Details</summary>
Motivation: 多类型EEG特征高维、高质量样本少导致分类器过拟合和实时性能不佳，且情感脑机接口实际应用常遇到多维情感标签部分缺失问题。

Method: 利用自适应正交非负矩阵分解通过二阶及高阶相关性重构多维情感标签空间，采用带基于图的流形学习正则化和全局特征冗余最小化正则化的最小二乘回归进行EEG特征子集选择。

Result: 在DREAMER、DEAP和HDED三个多维情感数据集上的模拟实验显示，该方法在EEG情感特征选择的鲁棒性上优于十三种先进特征选择方法。

Conclusion: 所提出的方法能够有效解决多维情感识别中存在的问题，实现稳健的基于EEG的多维情感识别。

Abstract: The affective brain-computer interface is a crucial technology for affective
interaction and emotional intelligence, emerging as a significant area of
research in the human-computer interaction. Compared to single-type features,
multi-type EEG features provide a multi-level representation for analyzing
multi-dimensional emotions. However, the high dimensionality of multi-type EEG
features, combined with the relatively small number of high-quality EEG
samples, poses challenges such as classifier overfitting and suboptimal
real-time performance in multi-dimensional emotion recognition. Moreover,
practical applications of affective brain-computer interface frequently
encounters partial absence of multi-dimensional emotional labels due to the
open nature of the acquisition environment, and ambiguity and variability in
individual emotion perception. To address these challenges, this study proposes
a novel EEG feature selection method for missing multi-dimensional emotion
recognition. The method leverages adaptive orthogonal non-negative matrix
factorization to reconstruct the multi-dimensional emotional label space
through second-order and higher-order correlations, which could reduce the
negative impact of missing values and outliers on label reconstruction.
Simultaneously, it employs least squares regression with graph-based manifold
learning regularization and global feature redundancy minimization
regularization to enable EEG feature subset selection despite missing
information, ultimately achieving robust EEG-based multi-dimensional emotion
recognition. Simulation experiments on three widely used multi-dimensional
emotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method
outperforms thirteen advanced feature selection methods in terms of robustness
for EEG emotional feature selection.

</details>


### [198] [ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection](https://arxiv.org/abs/2508.05934)
*Xueyuan Xu,Tianze Yu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: 提出自适应共享潜在结构学习（ASLSL）方法用于不完整多模态生理信号特征选择，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态生理特征高维且含噪声，易致分类器过拟合等问题，且以往研究假设数据完整，实际数据常不完整。

Method: 提出ASLSL方法，利用相似特征有相似情感标签的特性，探索不完整多模态生理信号和多维情感标签的共同潜在空间。

Result: 在DEAP和DREAMER两个数据集上对比ASLSL和17种特征选择方法，实验结果显示ASLSL有效。

Conclusion: ASLSL方法能减轻信息缺失影响，挖掘共识信息，在不完整多模态生理信号特征选择中有效。

Abstract: Recently, multi-modal physiological signals based emotion recognition has
garnered increasing attention in the field of brain-computer interfaces.
Nevertheness, the associated multi-modal physiological features are often
high-dimensional and inevitably include irrelevant, redundant, and noisy
representation, which can easily lead to overfitting, poor performance, and
high computational complexity in emotion classifiers. Feature selection has
been widely applied to address these challenges. However, previous studies
generally assumed that multi-modal physiological data are complete, whereas in
reality, the data are often incomplete due to the openness of the acquisition
and operational environment. For example, a part of samples are available in
several modalities but not in others. To address this issue, we propose a novel
method for incomplete multi-modal physiological signal feature selection called
adaptive shared latent structure learning (ASLSL). Based on the property that
similar features share similar emotional labels, ASLSL employs adaptive shared
latent structure learning to explore a common latent space shared for
incomplete multi-modal physiological signals and multi-dimensional emotional
labels, thereby mitigating the impact of missing information and mining
consensus information. Two most popular multi-modal physiological emotion
datasets (DEAP and DREAMER) with multi-dimensional emotional labels were
utilized to compare the performance between compare ASLSL and seventeen feature
selection methods. Comprehensive experimental results on these datasets
demonstrate the effectiveness of ASLSL.

</details>


### [199] [Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning](https://arxiv.org/abs/2508.06000)
*Wei Xiang,Ziyue Lei,Haoyuan Che,Fangyuan Ye,Xueting Wu,Lingyun Sun*

Main category: cs.HC

TL;DR: 本文探索了大语言模型驱动的动觉辅助在操作技能学习中的应用，开发了FlightAxis工具，结果显示用户接受度高，可提升操作技能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型支持的训练未能有效复制操作技能学习中的动觉反馈，存在文本性和用户接受度等问题，需探索大语言模型驱动的动觉辅助。

Method: 引入“对齐 - 分析 - 调整”策略，开发集成大语言模型和电肌肉刺激的FlightAxis工具用于飞行技能学习。

Result: 用户对大语言模型介导的身体控制接受度高，任务完成时间显著减少，增强了对操作缺陷的意识和训练参与度。

Conclusion: 动觉大语言模型训练在操作技能获取方面具有潜力。

Abstract: Operational skill learning, inherently physical and reliant on hands-on
practice and kinesthetic feedback, has yet to be effectively replicated in
large language model (LLM)-supported training. Current LLM training assistants
primarily generate customized textual feedback, neglecting the crucial
kinesthetic modality. This gap derives from the textual and uncertain nature of
LLMs, compounded by concerns on user acceptance of LLM driven body control. To
bridge this gap and realize the potential of collaborative human-LLM action,
this work explores human experience of LLM driven kinesthetic assistance.
Specifically, we introduced an "Align-Analyze-Adjust" strategy and developed
FlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS)
for flight skill acquisition, a representative operational skill domain.
FlightAxis learns flight skills from manuals and guides forearm movements
during simulated flight tasks. Our results demonstrate high user acceptance of
LLM-mediated body control and significantly reduced task completion times.
Crucially, trainees reported that this kinesthetic assistance enhanced their
awareness of operation flaws and fostered increased engagement in the training
process, rather than relieving perceived load. This work demonstrated the
potential of kinesthetic LLM training in operational skill acquisition.

</details>


### [200] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
*Daniel Lee,Nikhil Sharma,Donghoon Shin,DaEun Choi,Harsh Sharma,Jeonghwan Kim,Heng Ji*

Main category: cs.HC

TL;DR: 现有生成式AI图像创作工具难让输出契合用户创意，本文介绍ThematicPlane系统，研究显示其能促进创作工作流并指出新方向。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI图像创作中输出与用户细微创意意图难以对齐、现有工具限制用户流畅探索的问题。

Method: 引入ThematicPlane系统，让用户在交互式主题设计平面中导航和操纵高级语义概念，并开展有6名参与者的探索性研究。

Result: 参与者能进行发散和收敛创作，常将意外结果作为灵感，但对主题与输出的映射期望不同，反映出需要更具解释性的控制。

Conclusion: ThematicPlane促进了富有表现力的迭代工作流，为生成式设计工具的直观、语义驱动交互指明了新方向。

Abstract: Generative AI has made image creation more accessible, yet aligning outputs
with nuanced creative intent remains challenging, particularly for non-experts.
Existing tools often require users to externalize ideas through prompts or
references, limiting fluid exploration. We introduce ThematicPlane, a system
that enables users to navigate and manipulate high-level semantic concepts
(e.g., mood, style, or narrative tone) within an interactive thematic design
plane. This interface bridges the gap between tacit creative intent and system
control. In our exploratory study (N=6), participants engaged in divergent and
convergent creative modes, often embracing unexpected results as inspiration or
iteration cues. While they grounded their exploration in familiar themes,
differing expectations of how themes mapped to outputs revealed a need for more
explainable controls. Overall, ThematicPlane fosters expressive, iterative
workflows and highlights new directions for intuitive, semantics-driven
interaction in generative design tools.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [201] [Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms](https://arxiv.org/abs/2508.06383)
*Rashid Barket,Matthew England,Jürgen Gerhard*

Main category: cs.SC

TL;DR: 提出基于树的深度学习模型的机器学习方法，用于符号不定积分算法选择，在测试集表现好，凸显数据表示和问题框架重要性。


<details>
  <summary>Details</summary>
Motivation: 传统符号不定积分算法选择未充分考虑问题实例，导致效率低下。

Method: 采用两阶段架构的基于树的深度学习模型，先识别适用方法，再按预测输出复杂度排序；用树结构表示数学表达式。

Result: 在70000个样本的测试集上选最优方法准确率近90%；在独立基准测试中表现优于Maple内置选择器和先前ML方法。

Conclusion: 数据表示和问题框架在符号计算的ML中很关键，方法有望推广到数学软件的类似优化问题。

Abstract: Symbolic indefinite integration in Computer Algebra Systems such as Maple
involves selecting the most effective algorithm from multiple available
methods. Not all methods will succeed for a given problem, and when several do,
the results, though mathematically equivalent, can differ greatly in
presentation complexity. Traditionally, this choice has been made with minimal
consideration of the problem instance, leading to inefficiencies.
  We present a machine learning (ML) approach using tree-based deep learning
models within a two-stage architecture: first identifying applicable methods
for a given instance, then ranking them by predicted output complexity.
Furthermore, we find representing mathematical expressions as tree structures
significantly improves performance over sequence-based representations, and our
two-stage framework outperforms alternative ML formulations.
  Using a diverse dataset generated by six distinct data generators, our models
achieve nearly 90% accuracy in selecting the optimal method on a 70,000 example
holdout test set. On an independent out-of-distribution benchmark from Maple's
internal test suite, our tree transformer model maintains strong
generalisation, outperforming Maple's built-in selector and prior ML
approaches.
  These results highlight the critical role of data representation and problem
framing in ML for symbolic computation, and we expect our methodology to
generalise effectively to similar optimisation problems in mathematical
software.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [202] [IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text Clustering](https://arxiv.org/abs/2508.06126)
*Jixuan Yin,Zhihao Yao,Wenshuai Huo,Xinmiao Yu,Xiaocheng Feng,Bo Li*

Main category: stat.ME

TL;DR: 提出IOCC方法解决短文本聚类中簇中心与语义中心难以对齐问题，实验显示其优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 短文本表示表达力有限，传统方法难找到能反映类别语义的簇中心，导致表示优化方向不佳。

Method: 提出IOCC方法，含IEOT和CACL两个模块。IEOT将样本语义交互融入最优传输问题并生成伪标签，构建近似语义中心的伪中心；CACL将文本表示向伪中心优化。

Result: 在八个基准数据集上实验，IOCC优于先前方法，在生物医学数据集上提升达7.34%，聚类稳定性和效率出色。

Conclusion: IOCC能缩小簇中心与语义中心差距，使模型学习高质量分布，提升聚类性能。

Abstract: In clustering tasks, it is essential to structure the feature space into
clear, well-separated distributions. However, because short text
representations have limited expressiveness, conventional methods struggle to
identify cluster centers that truly capture each category's underlying
semantics, causing the representations to be optimized in suboptimal
directions. To address this issue, we propose IOCC, a novel few-shot
contrastive learning method that achieves alignment between the cluster centers
and the semantic centers. IOCC consists of two key modules:
Interaction-enhanced Optimal Transport (IEOT) and Center-aware Contrastive
Learning (CACL). Specifically, IEOT incorporates semantic interactions between
individual samples into the conventional optimal transport problem, and
generate pseudo-labels. Based on these pseudo-labels, we aggregate
high-confidence samples to construct pseudo-centers that approximate the
semantic centers. Next, CACL optimizes text representations toward their
corresponding pseudo-centers. As training progresses, the collaboration between
the two modules gradually reduces the gap between cluster centers and semantic
centers. Therefore, the model will learn a high-quality distribution, improving
clustering performance. Extensive experiments on eight benchmark datasets show
that IOCC outperforms previous methods, achieving up to 7.34\% improvement on
challenging Biomedical dataset and also excelling in clustering stability and
efficiency. The code is available at:
https://anonymous.4open.science/r/IOCC-C438.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [203] [Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.05687)
*Alistair Reid,Simon O'Callaghan,Liam Carroll,Tiberio Caetano*

Main category: cs.MA

TL;DR: 文章探讨多智能体 AI 系统风险识别与分析，指出与单智能体不同，需新方法，分析六种失效模式并提供工具包，介绍以有效性为核心的分析方法。


<details>
  <summary>Details</summary>
Motivation: 组织采用基于大语言模型的 AI 智能体，从单智能体向多智能体网络发展，多智能体系统需要不同于单智能体的风险分析方法。

Method: 分析六种关键失效模式，提供工具包；以分析有效性为核心，通过分阶段测试逐步提高有效性，结合模拟、观察分析、基准测试和红队测试收集证据。

Result: 为从业者提供评估失效模式的工具包，建立了基于大语言模型的多智能体系统组织风险管理的基础。

Conclusion: 提出的方法为多智能体 AI 系统在组织中的部署和运营建立了稳健的风险管理基础。

Abstract: Organisations are starting to adopt LLM-based AI agents, with their
deployments naturally evolving from single agents towards interconnected,
multi-agent networks. Yet a collection of safe agents does not guarantee a safe
collection of agents, as interactions between agents over time create emergent
behaviours and induce novel failure modes. This means multi-agent systems
require a fundamentally different risk analysis approach than that used for a
single agent.
  This report addresses the early stages of risk identification and analysis
for multi-agent AI systems operating within governed environments where
organisations control their agent configurations and deployment. In this
setting, we examine six critical failure modes: cascading reliability failures,
inter-agent communication failures, monoculture collapse, conformity bias,
deficient theory of mind, and mixed motive dynamics. For each, we provide a
toolkit for practitioners to extend or integrate into their existing frameworks
to assess these failure modes within their organisational contexts.
  Given fundamental limitations in current LLM behavioural understanding, our
approach centres on analysis validity, and advocates for progressively
increasing validity through staged testing across stages of abstraction and
deployment that gradually increases exposure to potential negative impacts,
while collecting convergent evidence through simulation, observational
analysis, benchmarking, and red teaming. This methodology establishes the
groundwork for robust organisational risk management as these LLM-based
multi-agent systems are deployed and operated.

</details>


### [204] [Semantic Reasoning Meets Numerical Precision: An LLM-Powered Multi-Agent System for Power Grid Control](https://arxiv.org/abs/2508.05702)
*Yan Zhang*

Main category: cs.MA

TL;DR: 本文介绍了Grid - Agent框架，结合大语言模型与多智能体强化学习实时检测和修复电网违规，实验证明其有良好的违规缓解性能且适合现代智能电网应用。


<details>
  <summary>Details</summary>
Motivation: 分布式能源资源渗透、电动汽车普及和极端天气事件增加了电网规划、运行和管理的复杂性，传统方法难以满足现代电网需求。

Method: 引入Grid - Agent框架，结合大语言模型与多智能体强化学习，采用模块化智能体架构，整合语义推理和数值精度，还包含自适应多尺度网络表示。

Result: 在标准IEEE和CIGRE测试系统中实验，展现出优越的违规缓解性能，具备数据收集和学习能力以适应不同网络拓扑。

Conclusion: 该框架的自主性使其特别适合需要对动态运行条件快速响应的现代智能电网应用。

Abstract: The increasing penetration of Distributed Energy Resources (DERs), widespread
adoption of Electric Vehicles (EVs), and the growing frequency of extreme
weather events have significantly increased the complexity of power grid
planning, operation, and management. Traditional rule-based systems and
numerical optimization approaches often struggle with the scale, dynamics, and
adaptability required by modern power networks. This paper introduces
Grid-Agent, an autonomous, AI-driven framework that combines Large Language
Models (LLMs) with multi-agent reinforcement learning to detect and remediate
grid violations in real time. Grid-Agent integrates semantic reasoning with
numerical precision through a modular agent architecture: a planning agent
generates coordinated action sequences using numerical power flow solvers,
while a validation agent evaluates system stability and action effectiveness
via sandboxed execution with safety rollbacks. To ensure scalability,
Grid-Agent incorporates an adaptive multiscale network representation that
dynamically selects optimal encoding schemes based on network size and
complexity. The framework enables coordinated violation resolution through
optimizing switch configurations, battery deployment, and load curtailment
strategies. Experimental results in standard IEEE and CIGRE test systems (IEEE
69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation
performance. Additionally, the framework's built-in data collection and
learning capabilities enable continuous learning and adaptation to diverse
network topologies. The autonomous nature of the framework makes it
particularly suitable for modern smart grid applications requiring rapid
response to dynamic operating conditions.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [205] [Post-apocalyptic computing from cellular automata](https://arxiv.org/abs/2508.06035)
*Genaro J. Martinez,Andrew Adamatzky,Guanrong Chen*

Main category: nlin.CG

TL;DR: 本文提出通过元胞自动机动态状态空间配置表示算法的新视角，建立计算与物理过程联系，为非传统计算设备发展铺路。


<details>
  <summary>Details</summary>
Motivation: 重新审视传统算法概念，建立计算与物理过程的独特联系，推动非传统计算设备发展。

Method: 提出通过元胞自动机动态状态空间配置表示算法的新视角。

Result: 建立了独特的概念框架，增强对计算的理解，为非传统计算设备发展创造可能。

Conclusion: 元胞自动机可作为计算理论和实践发展的变革性工具。

Abstract: Cellular automata are arrays of finite state machines that can exist in a
finite number of states. These machines update their states simultaneously
based on specific local rules that govern their interactions. This framework
provides a simple yet powerful model for studying complex systems and emergent
behaviors. We revisit and reconsider the traditional notion of an algorithm,
proposing a novel perspective in which algorithms are represented through the
dynamic state-space configurations of cellular automata. By doing so, we
establish a conceptual framework that connects computation to physical
processes in a unique and innovative way. This approach not only enhances our
understanding of computation but also paves the way for the future development
of unconventional computing devices. Such devices could be engineered to
leverage the inherent computational capabilities of physical, chemical, and
biological substrates. This opens up new possibilities for designing systems
that are more efficient, adaptive, and capable of solving problems in ways that
traditional silicon-based computers cannot. The integration of cellular
automata into these domains highlights their potential as a transformative tool
in the ongoing evolution of computational theory and practice.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [206] [Evaluating Universal Machine Learning Force Fields Against Experimental Measurements](https://arxiv.org/abs/2508.05762)
*Sajid Mannan,Vaibhav Bihani,Carmelo Gonzales,Kin Long Kelvin Lee,Nitya Nand Gosvami,Sayan Ranu,Santiago Miret,N M Anoop Krishnan*

Main category: cond-mat.mtrl-sci

TL;DR: 提出UniFFBench框架评估UMLFFs，发现计算基准与实验表现有差距，建立了实验验证标准。


<details>
  <summary>Details</summary>
Motivation: 现有UMLFFs评估局限于计算基准，不能反映真实性能，需用实验测量评估。

Method: 提出UniFFBench框架，用约1500个精心挑选的矿物结构实验测量评估六个先进UMLFFs。

Result: 发现计算基准与实验存在现实差距，最佳模型密度预测误差超实用阈值，模拟稳定性和力学性能准确性脱节，误差与训练数据表示相关。

Conclusion: 当前计算基准可能高估模型可靠性，UniFFBench建立了实验验证标准，指出需解决系统局限性以实现通用力场能力。

Abstract: Universal machine learning force fields (UMLFFs) promise to revolutionize
materials science by enabling rapid atomistic simulations across the periodic
table. However, their evaluation has been limited to computational benchmarks
that may not reflect real-world performance. Here, we present UniFFBench, a
comprehensive framework for evaluating UMLFFs against experimental measurements
of ~1,500 carefully curated mineral structures spanning diverse chemical
environments, bonding types, structural complexity, and elastic properties. Our
systematic evaluation of six state-of-the-art UMLFFs reveals a substantial
reality gap: models achieving impressive performance on computational
benchmarks often fail when confronted with experimental complexity. Even the
best-performing models exhibit higher density prediction error than the
threshold required for practical applications. Most strikingly, we observe
disconnects between simulation stability and mechanical property accuracy, with
prediction errors correlating with training data representation rather than the
modeling method. These findings demonstrate that while current computational
benchmarks provide valuable controlled comparisons, they may overestimate model
reliability when extrapolated to experimentally complex chemical spaces.
Altogether, UniFFBench establishes essential experimental validation standards
and reveals systematic limitations that must be addressed to achieve truly
universal force field capabilities.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [207] [Intuition emerges in Maximum Caliber models at criticality](https://arxiv.org/abs/2508.06477)
*Lluís Arola-Fernández*

Main category: physics.soc-ph

TL;DR: 研究大预测模型直觉机制，发现其为学习亚稳相，通过调参呈现不同阶段。


<details>
  <summary>Details</summary>
Motivation: 为大预测模型是简单重复训练数据还是产生真正见解缺乏物理解释这一问题提供解释。

Method: 通过mind - tuning（以类似控制温度参数λ施加最大口径原理），在确定性迷宫随机游走训练模型。

Result: 发现丰富相图，包括模仿、规则破坏幻觉和中间脆弱窗口，模型能自发发现新的目标导向策略。

Conclusion: 直觉是在记忆现有和思考可能之间临界平衡时的涌现属性。

Abstract: Whether large predictive models merely parrot their training data or produce
genuine insight lacks a physical explanation. This work reports a primitive
form of intuition that emerges as a metastable phase of learning that
critically balances next-token prediction against future path-entropy. The
intuition mechanism is discovered via mind-tuning, the minimal principle that
imposes Maximum Caliber in predictive models with a control temperature-like
parameter $\lambda$. Training on random walks in deterministic mazes reveals a
rich phase diagram: imitation (low $\lambda$), rule-breaking hallucination
(high $\lambda$), and a fragile in-between window exhibiting strong
protocol-dependence (hysteresis) and multistability, where models spontaneously
discover novel goal-directed strategies. These results are captured by an
effective low-dimensional theory and frame intuition as an emergent property at
the critical balance between memorizing what is and wondering what could be.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [208] [Quantum Resource Management in the NISQ Era: Implications and Perspectives from Software Engineering](https://arxiv.org/abs/2508.05697)
*Marcos Guillermo Lammers,Federico Hernán Holik,Alejandro Fernández*

Main category: quant-ph

TL;DR: 分析NISQ设备资源在量子算法设计和部署中的作用，助力量子资源估计和可靠量子软件开发。


<details>
  <summary>Details</summary>
Motivation: 当前NISQ时代硬件有局限性，高效管理量子资源对量子算法设计和部署很重要，旨在加强量子资源估计领域，推动可靠量子软件开发。

Method: 分析NISQ设备资源在当前应用中的作用。

Result: 未提及。

Conclusion: 未提及明确结论，但目标是加强量子资源估计领域，推动量子软件开发。

Abstract: Quantum computers represent a radical technological breakthrough in
information processing by leveraging the principles of quantum mechanics to
solve highly complex problems beyond the reach of classical systems. However,
in the current NISQ era (noisy intermediate-scale quantum devices), the
available hardware presents several limitations, such as a limited number of
qubits, high error rates, and short coherence times. Efficient management of
quantum resources, both physical and logical, is especially relevant in the
design and deployment of quantum algorithms. In this paper, we analyze the role
of resources in current uses of NISQ devices, identifying their relevance and
implications for quantum software engineering. With this contribution, we aim
to strengthen the field of Quantum Resource Estimation (QRE) and move toward
scalable and reliable quantum software development

</details>


### [209] [MPS-JuliQAOA: User-friendly, Scalable MPS-based Simulation for Quantum Optimization](https://arxiv.org/abs/2508.05883)
*Sean Feeney,Reuben Tate,John Golden,Stephan Eidenbenz*

Main category: quant-ph

TL;DR: 介绍MPS - JuliQAOA模拟器，可模拟QAOA，能扩展到512量子比特和20轮模拟，有参数查找能力，用户使用无需专业知识，还研究了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 开发一个用户友好、开源的工具来模拟可表示为对角哈密顿量的优化问题的量子近似优化算法（QAOA）。

Method: 利用Julia语言结构和ITensor包，采用矩阵乘积态（MPS）方法来模拟QAOA。

Result: 能轻松扩展到512个量子比特和20轮模拟3 - 正则MaxCut QAOA问题，有内置参数查找能力，用户无需了解MPS原理或复杂自动微分技术。

Conclusion: 开发出MPS - JuliQAOA模拟器，具备良好的可扩展性和易用性，代码开源。

Abstract: We present the MPS-JuliQAOA simulator, a user-friendly, open-source tool to
simulate the Quantum Approximate Optimization Algorithm (QAOA) of any
optimization problem that can be expressed as diagonal Hamiltonian. By
leveraging Julia-language constructs and the ITensor package to implement a
Matrix Product State (MPS) approach to simulating QAOA, MPS-Juli-QAOA
effortlessly scales to 512 qubits and 20 simulation rounds on the standard
de-facto benchmark 3-regular MaxCut QAOA problem. MPS-JuliQAOA also has
built-in parameter finding capabilities, which is a crucial performance aspect
of QAOA. We illustrate through examples that the user does not need to know MPS
principles or complex automatic differentiation techniques to use MPS-JuliQAOA.
We study the scalability of our tool with respect to runtime, memory usage and
accuracy tradeoffs. Code available at
https://github.com/lanl/JuliQAOA.jl/tree/mps.

</details>


### [210] [Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications](https://arxiv.org/abs/2508.06131)
*Philip Anton Hernicht,Alona Sakhnenko,Corey O'Meara,Giorgio Cortiana,Jeanette Miriam Lorenz*

Main category: quant-ph

TL;DR: 本文探索用经典替代模型绕过量子硬件访问限制，提出新方法生成经典替代模型，在能源需求预测问题上验证其有效性，实现高准确率且计算资源需求线性增长。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习早期工业应用受限于量子硬件访问，需解决该限制以推动其部署。

Method: 提出一种新的生成经典替代模型的管道，减少先前方法的冗余，利用更少资源。

Result: 在能源需求预测问题上，方法在测试数据集上准确率高，计算资源需求线性而非指数增长。

Conclusion: 提出的轻量级方法可将量子解决方案转化为经典可部署版本，促进量子技术在工业环境的集成，也可作为寻找实际量子优势的研究工具。

Abstract: Quantum machine learning (QML) presents potential for early industrial
adoption, yet limited access to quantum hardware remains a significant
bottleneck for deployment of QML solutions. This work explores the use of
classical surrogates to bypass this restriction, which is a technique that
allows to build a lightweight classical representation of a (trained) quantum
model, enabling to perform inference on entirely classical devices. We reveal
prohibiting high computational demand associated with previously proposed
methods for generating classical surrogates from quantum models, and propose an
alternative pipeline enabling generation of classical surrogates at a larger
scale than was previously possible. Previous methods required at least a
high-performance computing (HPC) system for quantum models of below industrial
scale (ca. 20 qubits), which raises questions about its practicality. We
greatly minimize the redundancies of the previous approach, utilizing only a
minute fraction of the resources previously needed. We demonstrate the
effectiveness of our method on a real-world energy demand forecasting problem,
conducting rigorous testing of performance and computation demand in both
simulations and on quantum hardware. Our results indicate that our method
achieves high accuracy on the testing dataset while its computational resource
requirements scale linearly rather than exponentially. This work presents a
lightweight approach to transform quantum solutions into classically deployable
versions, facilitating faster integration of quantum technology in industrial
settings. Furthermore, it can serve as a powerful research tool in search
practical quantum advantage in an empirical setup.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [211] [Multivariate Fields of Experts](https://arxiv.org/abs/2508.06490)
*Stanislas Ducotterd,Michael Unser*

Main category: eess.IV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce the multivariate fields of experts, a new framework for the
learning of image priors. Our model generalizes existing fields of experts
methods by incorporating multivariate potential functions constructed via
Moreau envelopes of the $\ell_\infty$-norm. We demonstrate the effectiveness of
our proposal across a range of inverse problems that include image denoising,
deblurring, compressed-sensing magnetic-resonance imaging, and computed
tomography. The proposed approach outperforms comparable univariate models and
achieves performance close to that of deep-learning-based regularizers while
being significantly faster, requiring fewer parameters, and being trained on
substantially fewer data. In addition, our model retains a relatively high
level of interpretability due to its structured design.

</details>


### [212] [Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification](https://arxiv.org/abs/2508.06287)
*Mobarak Abumohsen,Enrique Costa-Montenegro,Silvia García-Méndez,Amani Yousef Owda,Majdi Owda*

Main category: eess.IV

TL;DR: 本文提出基于DenseNet201模型的肺癌检测与分类方法，解决数据不平衡和过拟合问题，准确率达98.95%。


<details>
  <summary>Details</summary>
Motivation: 现有CT图像识别肺癌技术因小而不平衡的数据集导致假阳性多、准确率低，需改进方法。

Method: 基于DenseNet201模型，采用Focal Loss、数据增强和正则化等方法。

Result: 所提方法表现良好，准确率达98.95%。

Conclusion: 该创新方法适用于CT图像的肺癌检测与分类。

Abstract: Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one
of the most common causes of death for men and women worldwide. Computed
Tomography (CT) images are the most preferred diagnosis method because of their
low cost and their faster processing times. Many researchers have proposed
various ways of identifying lung cancer using CT images. However, such
techniques suffer from significant false positives, leading to low accuracy.
The fundamental reason results from employing a small and imbalanced dataset.
This paper introduces an innovative approach for LC detection and
classification from CT images based on the DenseNet201 model. Our approach
comprises several advanced methods such as Focal Loss, data augmentation, and
regularization to overcome the imbalanced data issue and overfitting challenge.
The findings show the appropriateness of the proposal, attaining a promising
performance of 98.95% accuracy.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [213] [When a Paper Has 1000 Authors: Rethinking Citation Metrics in the Era of LLMs](https://arxiv.org/abs/2508.06004)
*Weihang Guo,Zhao Song,Jiahao Zhang*

Main category: cs.DL

TL;DR: 传统作者引用指标在大语言模型等大规模合作论文场景失效，本文提出SBCI指数解决此问题并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 过去五年大语言模型和基础模型领域大规模论文涌现，传统引用指标无法有效区分众多合著者的个人贡献，在学术招聘和资助决策等场景需新指标。

Method: 提出SBCI指数，分析其理论性质并在合成出版物数据集上评估其表现。

Result: 所提出的指标在大规模合作时代能对个人学术影响力提供更稳健和有区分度的评估。

Conclusion: SBCI指数可解决大规模合作论文中区分作者贡献的问题，能更好评估个人学术影响。

Abstract: Author-level citation metrics provide a practical, interpretable, and
scalable signal of scholarly influence in a complex research ecosystem. It has
been widely used as a proxy in hiring decisions. However, the past five years
have seen the rapid emergence of large-scale publications in the field of large
language models and foundation models, with papers featuring hundreds to
thousands of co-authors and receiving tens of thousands of citations within
months. For example, Gemini has 1361 authors and has been cited around 4600
times in 19 months. In such cases, traditional metrics, such as total citation
count and the $h$-index, fail to meaningfully distinguish individual
contributions. Therefore, we propose the following research question: How can
one identify standout researchers among thousands of co-authors in large-scale
LLM papers? This question is particularly important in scenarios such as
academic hiring and funding decisions. In this paper, we introduce a novel
citation metric designed to address this challenge by balancing contributions
across large-scale and small-scale publications. We propose the SBCI index,
analyze its theoretical properties, and evaluate its behavior on synthetic
publication datasets. Our results demonstrate that the proposed metric provides
a more robust and discriminative assessment of individual scholarly impact in
the era of large-scale collaborations.

</details>


### [214] [A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges](https://arxiv.org/abs/2508.06401)
*Andrew Brown,Muhammad Roman,Barry Devereux*

Main category: cs.DL

TL;DR: 对2020年至2025年5月高引用的检索增强生成（RAG）研究文献进行系统综述，明确研究现状、方法差距并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 分析检索增强生成（RAG）领域的研究现状，为未来研究提供方向。

Method: 基于PRISMA 2020框架，明确文章纳入和排除标准，从多个数据库检索文献，编目数据集、架构和评估实践，综合实证证据。为减少引用滞后偏差，对2025年论文设置较低引用阈值。

Result: 共128篇文章符合纳入标准。

Conclusion: 该综述明确了当前研究格局，突出了方法学差距，并规划了未来研究的优先方向。

Abstract: This systematic review of the research literature on retrieval-augmented
generation (RAG) provides a focused analysis of the most highly cited studies
published between 2020 and May 2025. A total of 128 articles met our inclusion
criteria. The records were retrieved from ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).
RAG couples a neural retriever with a generative language model, grounding
output in up-to-date, non-parametric memory while retaining the semantic
generalisation stored in model weights. Guided by the PRISMA 2020 framework, we
(i) specify explicit inclusion and exclusion criteria based on citation count
and research questions, (ii) catalogue datasets, architectures, and evaluation
practices, and (iii) synthesise empirical evidence on the effectiveness and
limitations of RAG. To mitigate citation-lag bias, we applied a lower
citation-count threshold to papers published in 2025 so that emerging
breakthroughs with naturally fewer citations were still captured. This review
clarifies the current research landscape, highlights methodological gaps, and
charts priority directions for future research.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [215] [Numerical Considerations in Weighted Model Counting](https://arxiv.org/abs/2508.06264)
*Randal E. Bryant*

Main category: math.NA

TL;DR: 本文提出结合多种数值表示法来高效计算加权模型计数，保证达到用户指定精度，对不同权重情况给出方法并通过评估验证其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 浮点运算计算加权模型计数结果不准确且无法量化精度，有理数运算时空成本高，需更优方法。

Method: 结合多种数值表示法，非负权重时用扩展范围双精度格式避免溢出，正负混合权重时结合区间浮点和有理数运算。

Result: 证明非负权重时浮点运算精度损失可被紧密界定，验证方法的鲁棒性。

Conclusion: 所提结合多种数值表示法的方法能高效计算加权模型计数并保证指定精度。

Abstract: Weighted model counting computes the sum of the rational-valued weights
associated with the satisfying assignments for a Boolean formula, where the
weight of an assignment is given by the product of the weights assigned to the
positive and negated variables comprising the assignment. Weighted model
counting finds applications across a variety of domains including probabilistic
reasoning and quantitative risk assessment.
  Most weighted model counting programs operate by (explicitly or implicitly)
converting the input formula into a form that enables arithmetic evaluation,
using multiplication for conjunctions and addition for disjunctions. Performing
this evaluation using floating-point arithmetic can yield inaccurate results,
and it cannot quantify the level of precision achieved. Computing with rational
arithmetic gives exact results, but it is costly in both time and space.
  This paper describes how to combine multiple numeric representations to
efficiently compute weighted model counts that are guaranteed to achieve a
user-specified precision. When all weights are nonnegative, we prove that the
precision loss of arithmetic evaluation using floating-point arithmetic can be
tightly bounded. We show that supplementing a standard IEEE double-precision
representation with a separate 64-bit exponent, a format we call extended-range
double (ERD), avoids the underflow and overflow issues commonly encountered in
weighted model counting. For problems with mixed negative and positive weights,
we show that a combination of interval floating-point arithmetic and rational
arithmetic can achieve the twin goals of efficiency and guaranteed precision.
For our evaluations, we have devised especially challenging formulas and weight
assignments, demonstrating the robustness of our approach.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [216] [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 本文提出两种《古兰经》抽取式问答有效方法，用大语言模型和专门框架及后处理系统，评估显示大语言模型表现更好，证实基于提示的指令调优有效。


<details>
  <summary>Details</summary>
Motivation: 解决《古兰经》文本复杂语言、独特术语和深层含义带来的抽取式问答挑战。

Method: 一是使用指令调优的大语言模型如Gemini和DeepSeek进行少样本提示；二是开发专门的阿拉伯语提示框架用于跨度提取；构建强大的后处理系统，集成子词对齐、重叠抑制和语义过滤。

Result: 使用阿拉伯语指令的大语言模型优于传统微调模型，最佳配置pAP10分数达0.637。

Conclusion: 基于提示的指令调优对低资源、语义丰富的问答任务有效。

Abstract: This paper presents two effective approaches for Extractive Question
Answering (QA) on the Quran. It addresses challenges related to complex
language, unique terminology, and deep meaning in the text. The second uses
few-shot prompting with instruction-tuned large language models such as Gemini
and DeepSeek. A specialized Arabic prompt framework is developed for span
extraction. A strong post-processing system integrates subword alignment,
overlap suppression, and semantic filtering. This improves precision and
reduces hallucinations. Evaluations show that large language models with Arabic
instructions outperform traditional fine-tuned models. The best configuration
achieves a pAP10 score of 0.637. The results confirm that prompt-based
instruction tuning is effective for low-resource, semantically rich QA tasks.

</details>


### [217] [Indian Legal NLP Benchmarks : A Survey](https://arxiv.org/abs/2107.06056)
*Prathamesh Kalamkar,Janani Venugopalan Ph. D.,Vivek Raghavan Ph. D*

Main category: cs.CL

TL;DR: 本文指出需为印度法律文本创建专门的NLP基准，回顾相关工作并提出创建新基准的想法。


<details>
  <summary>Details</summary>
Motivation: 法律文本与普通英文文本差异大，需为印度法律文本创建有挑战性且针对法律系统特定任务的NLP基准，以推动相关创新。

Method: 回顾该领域现有工作并提出创建新基准的想法。

Result: 无明确提及具体结果。

Conclusion: 为印度法律自然语言处理创建新基准很有必要。

Abstract: Availability of challenging benchmarks is the key to advancement of AI in a
specific field.Since Legal Text is significantly different than normal English
text, there is a need to create separate Natural Language Processing benchmarks
for Indian Legal Text which are challenging and focus on tasks specific to
Legal Systems. This will spur innovation in applications of Natural language
Processing for Indian Legal Text and will benefit AI community and Legal
fraternity. We review the existing work in this area and propose ideas to
create new benchmarks for Indian Legal Natural Language Processing.

</details>


### [218] [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)
*Sree Bhattacharyya,Lucas Craig,Tharun Dilliraj,Jia Li,James Z. Wang*

Main category: cs.CL

TL;DR: 本文超越表面情绪任务，基于认知评估理论探究大语言模型如何通过认知维度进行情绪推理，引入CoRE基准评估模型，揭示不同模型推理模式多样，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 过去多数研究以监督方式处理情绪相关任务，评估限于标准和表面的情绪任务，本文旨在超越表面，探究大语言模型如何通过认知维度进行情绪推理。

Method: 借鉴认知评估理论，引入大规模认知推理基准CoRE评估大语言模型在情绪推理时隐式使用的内部认知结构，进行大量评估实验和分析。

Result: 不同大语言模型呈现出多样的推理模式。

Conclusion: 可以通过认知评估维度进一步研究大语言模型对不同情绪类别的内部表征，相关基准和代码公开利于后续研究。

Abstract: Affective Computing has been established as a crucial field of inquiry to
advance the holistic development of Artificial Intelligence (AI) systems.
Foundation models -- especially Large Language Models (LLMs) -- have been
evaluated, trained, or instruction-tuned in several past works, to become
better predictors or generators of emotion. Most of these studies, however,
approach emotion-related tasks in a supervised manner, assessing or training
the capabilities of LLMs using discrete emotion labels associated with stimuli
(e.g., text, images, video, audio). Evaluation studies, in particular, have
often been limited to standard and superficial emotion-related tasks, such as
the recognition of evoked or expressed emotions. In this paper, we move beyond
surface-level emotion tasks to investigate how LLMs reason about emotions
through cognitive dimensions. Drawing from cognitive appraisal theory, we
examine whether LLMs produce coherent and plausible cognitive reasoning when
reasoning about emotionally charged stimuli. We introduce a large-scale
benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal
cognitive structures implicitly used by LLMs for emotional reasoning. Through a
plethora of evaluation experiments and analysis, we seek to answer: (a) Are
models more likely to implicitly rely on specific cognitive appraisal
dimensions?, (b) What cognitive dimensions are important for characterizing
specific emotions?, and, (c) Can the internal representations of different
emotion categories in LLMs be interpreted through cognitive appraisal
dimensions? Our results and analyses reveal diverse reasoning patterns across
different LLMs. Our benchmark and code will be made publicly available.

</details>


### [219] [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)
*Rafal Kocielnik,Min Kim,Penphob,Boonyarungsrit,Fereshteh Soltani,Deshawn Sambrano,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: 提出一个三阶段管道用于亲社会内容分类，减少人力标注和推理成本，实现可扩展的高精度分类。


<details>
  <summary>Details</summary>
Motivation: 亲社会文本检测是信任和安全系统的新挑战，缺乏明确定义与标注数据，需新方法。

Method: 先确定基于大语言模型的最佳标注策略，引入人机细化循环明确任务定义，合成高质量标签，训练两阶段推理系统。

Result: 降低约70%推理成本，达到约0.90的高精度。

Conclusion: 有针对性的人机交互、精心的任务设计和考虑部署的架构设计可解决新型负责任AI任务。

Abstract: Detecting prosociality in text--communication intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best LLM-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only $\sim$35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving
high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.

</details>


### [220] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
*Kartik Sharma,Yiqiao Jin,Rakshit Trivedi,Srijan Kumar*

Main category: cs.CL

TL;DR: 提出PEEK方法利用预训练嵌入模型估计大语言模型知识，评估显示可达90%准确率，发现句嵌入模型更合适，代码数据公开。


<details>
  <summary>Details</summary>
Motivation: 现有探测大语言模型知识的方法需对基础模型前向传播，计算成本高、耗时长。

Method: 通过多种探测策略确定大语言模型已知事实训练集，用线性解码层让嵌入模型预测大语言模型输出。

Result: 在3个维基数据集、4个大语言模型和7个嵌入模型上评估，嵌入模型预测大语言模型知识准确率达90%，句嵌入模型更适合预测。

Conclusion: 知识适配的嵌入模型可大规模识别大语言模型知识缺口，深入了解其内部归纳偏差。

Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as
science, history, and geography encountered during generative pre-training.
However, due to their stochasticity, it is difficult to predict what LLMs have
acquired. Prior work has developed different ways to probe this knowledge by
investigating the hidden representations, crafting specific task prompts,
curating representative samples, and estimating their uncertainty. However,
these methods require making forward passes through the underlying model to
probe the LLM's knowledge about a specific fact, making them computationally
expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or
$\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate
$\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models
that effectively encode factual knowledge as text or graphs as proxies for
LLMs. First, we identify a training set of facts known by LLMs through various
probing strategies and then adapt embedding models to predict the LLM outputs
with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived
datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict
LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find
that sentence embedding models are more suitable than graph embeddings to
predict LLM knowledge, shedding light on the underlying representation of the
factual landscape. Thus, we believe that knowledge-adapted embeddings can be
used to identify knowledge gaps in LLMs at scale and can provide deeper
insights into LLMs' internal inductive bias. The code and data are made
available at https://github.com/claws-lab/peek.

</details>


### [221] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CL

TL;DR: 本文在SST - 2情感分析任务微调DistilBERT模型时引入后验稀疏性，发现80%注意力稀疏度模型准确率显著提升，挑战了稀疏性会降低准确率的观点。


<details>
  <summary>Details</summary>
Motivation: 解决自注意力机制二次计算成本问题，挑战注意力稀疏性牺牲模型准确率的普遍认知。

Method: 在SST - 2情感分析任务微调DistilBERT模型时引入结构化后验稀疏性。

Result: 80%注意力稀疏度的模型验证准确率达91.59%，比密集基线提高0.97%。

Conclusion: 注意力稀疏性不仅可提升计算效率，还能改善Transformer模型泛化能力和性能。

Abstract: The quadratic computational cost of the self-attention mechanism is a primary
challenge in scaling Transformer models. While attention sparsity is widely
studied as a technique to improve computational efficiency, it is almost
universally assumed to come at the cost of model accuracy. In this paper, we
report a surprising counter-example to this common wisdom. By introducing
structured, post-hoc sparsity to the attention mechanism of a DistilBERT model
during fine-tuning on the SST-2 sentiment analysis task, we find that model
accuracy improves significantly. Our model with 80\% attention sparsity
achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over
the dense baseline. We hypothesize that this phenomenon is due to sparsity
acting as a powerful implicit regularizer, preventing the model from
overfitting by forcing it to make predictions with a more constrained and
robust set of features. Our work recasts attention sparsity not just as a tool
for computational efficiency, but as a potential method for improving the
generalization and performance of Transformer models.

</details>


### [222] [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)
*Yidong Wang,Xin Wang,Cunxiang Wang,Junfeng Fang,Qiufeng Wang,Jianing Chu,Xuran Meng,Shuxun Yang,Libo Qin,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.CL

TL;DR: 提出时间自奖励语言模型，解决现有自奖励范式缺陷，实验证明其效果好且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有自奖励范式存在所选和拒绝响应同步改进，缩小样本表征差异，影响偏好学习的问题。

Method: 提出双阶段框架，包括固定过去初始模型输出作为拒绝响应的“锚定拒绝”和用下一代模型预测动态策划所选样本的“未来引导选择”。

Result: 在三个模型家族和不同大小模型上实验，相比自奖励方法有显著提升，如Llama3.1 - 8B在AlpacaEval 2.0胜率提高。

Conclusion: 所提方法在相同计算资源下优于自奖励方法，且有良好的分布外泛化能力。

Abstract: Self-Rewarding Language Models propose an architecture in which the Large
Language Models(LLMs) both generates responses and evaluates its own outputs
via LLM-as-a-Judge prompting, dynamically improving its generative capabilities
through iterative Direct Preference Optimization (DPO). However, our analysis
reveals a critical limitation in existing Self-Rewarding paradigms: the
synchronized improvement of chosen and rejected responses progressively narrows
the representational difference between contrasting samples, undermining
effective preference learning. We propose \textbf{Temporal Self-Rewarding
Language Models} that strategically coordinate past, present, and future model
generations to sustain learning signals. Our dual-phase framework introduces:
(1) \textit{Anchored Rejection} - fixing rejected responses using the past
initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically
curating chosen samples using next-generation model predictions. Extensive
experiments across three model families (Llama, Qwen, Mistral) and different
model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained
with our method compared to Self-Rewarding using same computation resources.
For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our
method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our
method also demonstrates superior out-of-distribution generalization across
mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code
generation (HumanEval) tasks, even though we do not specifically collect such
training data.

</details>


### [223] [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)
*Xinda Wang,Zhengxu Hou,Yangshijie Zhang,Bingren Yan,Zhibo Yang,Xingsheng Zhang,Luxi Xing,Qiang Zhou,Chen Zhang*

Main category: cs.CL

TL;DR: 现有大语言模型在故事评估任务中表现受限，本文提出Self - Evolving Pairwise Reasoning (EvolvR) 框架，实验证明其在评估基准上达SOTA，且能提升故事生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在开放式任务（如故事评估）中性能有限，现有方法存在适应性差、缺乏严格推理能力等问题，需要更好的方法进行故事评估和引导故事生成。

Method: 提出EvolvR框架，基于成对比较，通过多角色策略自合成与分数对齐的Chain - of - Thought (CoT) 数据，对原始CoT数据进行自过滤，用精炼数据训练评估器并作为奖励模型引导故事生成。

Result: 框架在StoryER、HANNA和OpenMEVA三个评估基准上达到SOTA性能，作为奖励模型时显著提升生成故事的质量。

Conclusion: EvolvR框架的自进化方法具有优越性。

Abstract: Although the effectiveness of Large Language Models (LLMs) as judges
(LLM-as-a-judge) has been validated, their performance remains limited in
open-ended tasks, particularly in story evaluation. Accurate story evaluation
is crucial not only for assisting human quality judgment but also for providing
key signals to guide story generation. However, existing methods face a
dilemma: prompt engineering for closed-source models suffers from poor
adaptability, while fine-tuning approaches for open-source models lack the
rigorous reasoning capabilities essential for story evaluation. To address
this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.
Grounded in pairwise comparison, the framework first self-synthesizes
score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To
ensure data quality, these raw CoTs undergo a self-filtering process, utilizing
multi-agents to guarantee their logical rigor and robustness. Finally, the
evaluator trained on the refined data is deployed as a reward model to guide
the story generation task. Experimental results demonstrate that our framework
achieves state-of-the-art (SOTA) performance on three evaluation benchmarks
including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward
model, it significantly enhances the quality of generated stories, thereby
fully validating the superiority of our self-evolving approach.

</details>


### [224] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
*Yingfeng Luo,Dingyang Lin,Junxin Wang,Ziqiang Xu,Kaiyan Chang,Tong Zheng,Bei Li,Anxiang Ma,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出TADrop自适应稀疏化策略用于模型合并，经多任务和多模型实验验证能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并稀疏化方法采用统一稀疏率，忽视参数结构和统计异质性，导致次优权衡。

Method: 引入TADrop，根据参数张量分布特性为每个张量分配定制稀疏度。

Result: 将TADrop与多种合并方法集成，在多任务和多模型实验中显著提升性能，如增强领先合并方法时平均性能提升2.0%。

Conclusion: TADrop通过适应模型结构的稀疏化有效减轻参数干扰，为高性能模型合并提供新基线。

Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task
learning, enabling the fusion of multiple fine-tuned models into a single,
powerful entity. A key technique in merging methods is sparsification, which
prunes redundant parameters from task vectors to mitigate interference.
However, prevailing approaches employ a ``one-size-fits-all'' strategy,
applying a uniform sparsity ratio that overlooks the inherent structural and
statistical heterogeneity of model parameters. This often leads to a suboptimal
trade-off, where critical parameters are inadvertently pruned while less useful
ones are retained. To address this limitation, we introduce \textbf{TADrop}
(\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive
sparsification strategy that respects this heterogeneity. Instead of a global
ratio, TADrop assigns a tailored sparsity level to each parameter tensor based
on its distributional properties. The core intuition is that tensors with
denser, more redundant distributions can be pruned aggressively, while sparser,
more critical ones are preserved. As a simple and plug-and-play module, we
validate TADrop by integrating it with foundational, classic, and SOTA merging
methods. Extensive experiments across diverse tasks (vision, language, and
multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and
significantly boosts their performance. For instance, when enhancing a leading
merging method, it achieves an average performance gain of 2.0\% across 8
ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter
interference by tailoring sparsification to the model's structure, offering a
new baseline for high-performance model merging.

</details>


### [225] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
*Richard Willats,Josh Pennington,Aravind Mohan,Bertie Vidgen*

Main category: cs.CL

TL;DR: 提出基于RAG的分类方法用于内容审核，具有灵活、透明、可适应特点。


<details>
  <summary>Details</summary>
Motivation: 使分类系统能快速适应不断演变的政策，避免昂贵的再训练。

Method: 使用Retrieval - Augmented Generation (RAG)将传统分类任务转变为基于推理时检索的上下文知识评估内容，CPE系统展示此方法。

Result: CPE系统有与领先商业系统相当的分类准确率、可解释性和无需模型再训练的动态政策更新能力，实验显示其能进行细粒度政策控制。

Conclusion: RAG可将分类转变为更灵活、透明和可适应的过程，用于内容审核和更广泛的分类问题。

Abstract: Robust content moderation requires classification systems that can quickly
adapt to evolving policies without costly retraining. We present classification
using Retrieval-Augmented Generation (RAG), which shifts traditional
classification tasks from determining the correct category in accordance with
pre-trained parameters to evaluating content in relation to contextual
knowledge retrieved at inference. In hate speech detection, this transforms the
task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates
this approach and offers three key advantages: (1) robust classification
accuracy comparable to leading commercial systems, (2) inherent explainability
via retrieved policy segments, and (3) dynamic policy updates without model
retraining. Through three experiments, we demonstrate strong baseline
performance and show that the system can apply fine-grained policy control by
correctly adjusting protection for specific identity groups without requiring
retraining or compromising overall performance. These findings establish that
RAG can transform classification into a more flexible, transparent, and
adaptable process for content moderation and wider classification problems.

</details>


### [226] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
*Theresa Pekarek Rosin,Burak Can Kaplan,Stefan Wermter*

Main category: cs.CL

TL;DR: 论文提出结合调整后的Whisper ASR模型与基于Transformer的语言模型，处理德国老年说话者语音的意图识别问题，合成数据提升性能，小模型LeoLM在德语文本质量上超ChatGPT。


<details>
  <summary>Details</summary>
Motivation: 现有语音命令意图识别方法局限于短命令且多为英语，论文聚焦德国老年说话者语音意图识别。

Method: 结合调整后的Whisper ASR模型（SVC - de）和基于Transformer的语言模型，语言模型在LeoLM、Llama3和ChatGPT生成的合成文本数据集上训练，用文本转语音模型生成合成语音并进行跨数据集测试。

Result: 合成的大语言模型生成的数据显著提升分类性能和对不同说话风格及未见词汇的鲁棒性，LeoLM在德国意图识别数据集质量上超过ChatGPT。

Conclusion: 生成式AI可有效弥补低资源领域的数据缺口，提供数据生成和训练过程文档确保透明和可复现。

Abstract: Intent recognition (IR) for speech commands is essential for artificial
intelligence (AI) assistant systems; however, most existing approaches are
limited to short commands and are predominantly developed for English. This
paper addresses these limitations by focusing on IR from speech by elderly
German speakers. We propose a novel approach that combines an adapted Whisper
ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based
language models trained on synthetic text datasets generated by three
well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To
evaluate the robustness of our approach, we generate synthetic speech with a
text-to-speech model and conduct extensive cross-dataset testing. Our results
show that synthetic LLM-generated data significantly boosts classification
performance and robustness to different speaking styles and unseen vocabulary.
Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the
much larger ChatGPT (175B) in dataset quality for German intent recognition.
Our approach demonstrates that generative AI can effectively bridge data gaps
in low-resource domains. We provide detailed documentation of our data
generation and training process to ensure transparency and reproducibility.

</details>


### [227] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: 现有LMMs的图QA方法单一，本文设计新TRF、提出GRE指标并开发DynamicTRF框架，实验显示其显著提升零样本图QA准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多数方法使用单一图表示形式，未考虑不同模型或任务偏好，导致回答错误或过长，需改进。

Method: 分析现有TRF特点与不足，设计适合零样本图QA的TRF集合$F_{ZS}$，引入GRE指标，开发DynamicTRF框架，创建TRFP数据集，训练TRF路由器。

Result: 在7个领域内算法图QA任务和2个领域外下游任务的大量实验表明，DynamicTRF显著提升LMMs零样本图QA的准确性。

Conclusion: DynamicTRF框架能有效提高图QA的准确性和简洁性。

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [228] [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: 本文提出选择性反射蒸馏（SRD）框架，通过利用学生模型反馈筛选训练数据，提升知识蒸馏效果并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有白盒知识蒸馏方法忽视训练数据质量和学生模型兼容性，作者旨在解决这些局限。

Method: 提出SRD框架，动态评估和选择提示 - 响应数据对，采用课程调度策略引入筛选数据。

Result: 实验表明，SRD在多种知识蒸馏方法和模型架构上提升蒸馏模型性能，最多减少39%训练时间。

Conclusion: 数据质量和兼容性对大语言模型知识蒸馏至关重要，SRD提供了实现两者的框架。

Abstract: Knowledge Distillation (KD) is a fundamental technique for compressing large
language models (LLMs) into compact, efficient student models. However,
existing white-box KD methods mainly focus on balancing ground truth and
student-generated responses while overlooking two critical factors: training
data quality and student-model compatibility. To address these limitations, we
propose Selective Reflection Distillation (SRD), a novel data curation
framework that leverages reflections from student models to systematically
refine training data. SRD dynamically evaluates and selects prompt-response
pairs by comparing ground truth data with student model outputs, selectively
curating high-quality, student-compatible training instances through automated
ranking based on difficulty. Furthermore, after selecting the training data, a
curriculum scheduling strategy is employed to incrementally introduce these
curated subsets into the distillation process at fixed intervals. As a
plug-and-play enhancement, SRD consistently improves distillation outcomes
across diverse white-box KD approaches and model architectures, as well as
decreases computational cost significantly during KD training. Experiments on a
range of language model benchmarks demonstrate SRD's consistent improvements in
distilled model performance, as well as a reduction in training runtime by up
to 39%, under diverse KD methods and model families. Notably, SRD operates as a
plug-and-play module, enhancing sample efficiency without modifying underlying
KD algorithms. Our findings highlight that data quality and compatibility are
pivotal to effective and efficient distillation of LLMs, and SRD provides a
principled framework to achieve both. This work advances the understanding of
data-centric factors in KD and offers practical insights for enhancing the
capability and efficiency of compressed LLMs.

</details>


### [229] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
*Runnan Fang,Yuan Liang,Xiaobin Wang,Jialong Wu,Shuofei Qiao,Pengjun Xie,Fei Huang,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 研究赋予大语言模型代理可学习、可更新和终身程序记忆的策略，提出Memp方法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理的程序记忆存在手动设计或与静态参数纠缠的问题，需要可学习、可更新和终身的程序记忆。

Method: 提出Memp，将过去代理轨迹提炼为细粒度指令和高级抽象，探索构建、检索和更新程序记忆的不同策略，结合动态机制更新内容。

Result: 在TravelPlanner和ALFWorld上，随着记忆库完善，代理在类似任务上成功率和效率提高，将强模型的程序记忆迁移到弱模型可提升性能。

Conclusion: 所提出的策略能有效为代理赋予可学习、可更新和终身的程序记忆，提升代理性能。

Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.

</details>


### [230] [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
*Weitao Li,Boran Xiang,Xiaolong Wang,Zhinan Gou,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 提出统一框架 UR2 结合检索和推理，在多任务上表现出色并开源。


<details>
  <summary>Details</summary>
Motivation: 现有 RAG 和 RLVR 能力孤立发展，统一尝试范围窄，缺乏集成限制泛化和应用范围。

Method: 提出 UR2 框架，通过强化学习统一检索和推理，包括困难感知课程训练和混合知识访问策略。

Result: UR2 在多任务实验中显著优于现有 RAG 和 RL 方法，在多个基准上与 GPT - 4o - mini 和 GPT - 4.1 - mini 性能相当。

Conclusion: UR2 能实现检索和推理的动态协调，提高跨任务适应性，是有效的统一框架。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two
complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances
knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),
which optimizes complex reasoning abilities. However, these two capabilities
are often developed in isolation, and existing efforts to unify them remain
narrow in scope-typically limited to open-domain QA with fixed retrieval
settings and task-specific assumptions. This lack of integration constrains
generalization and limits the applicability of RAG-RL methods to broader
domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a
general framework that unifies retrieval and reasoning through reinforcement
learning. UR2 introduces two key contributions: a difficulty-aware curriculum
training that selectively invokes retrieval only for challenging problems, and
a hybrid knowledge access strategy combining domain-specific offline corpora
with LLM-generated summaries. These components are designed to enable dynamic
coordination between retrieval and reasoning, improving adaptability across a
diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,
and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B
and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,
achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several
benchmarks. We have released all code, models, and data at
https://github.com/Tsinghua-dhy/UR2.

</details>


### [231] [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)
*Yilun Hua,Evan Wang,Yoav Artzi*

Main category: cs.CL

TL;DR: 开发后训练过程使大语言模型具备多轮交互中形成临时约定的能力，经两个新基准测试显示能力显著提升。


<details>
  <summary>Details</summary>
Motivation: 人类在多轮交互中能提高沟通效率并形成临时约定，而现有大语言模型缺乏此能力。

Method: 通过对启发式识别出的约定形成示例进行有针对性的微调开展后训练过程，并用两个新基准进行评估。

Result: 经两种评估方法测试，后训练的大语言模型在约定形成能力上有显著提升。

Conclusion: 所开发的后训练过程能有效提升大语言模型在多轮交互中形成约定的能力。

Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by
adapting their language and forming ad-hoc conventions. In contrast, prior work
shows that LLMs do not naturally show this behavior. We develop a post-training
process to develop this ability through targeted fine-tuning on heuristically
identified demonstrations of convention formation. We evaluate with two new
benchmarks focused on this capability. First, we design a focused,
cognitively-motivated interaction benchmark that consistently elicits strong
convention formation trends in humans. Second, we create a new
document-grounded reference completion task that reflects in-the-wild
convention formation behavior. Our studies show significantly improved
convention formation abilities in post-trained LLMs across the two evaluation
methods.

</details>


### [232] [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)
*Keummin Ka,Junhyeong Park,Jahyun Jeon,Youngjae Yu*

Main category: cs.CL

TL;DR: 本文引入InfoCausalQA基准评估多模态下基于信息图的因果推理能力，发现当前VLMs在因果推理上能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在多模态因果推理能力上探索不足，需要评估其因果推理能力。

Method: 引入InfoCausalQA基准，包含两个任务，手动收集494个信息图 - 文本对，用GPT - 4o生成1482个高质量选择题对并经人工修订。

Result: 当前VLMs在计算推理和语义因果推理能力有限，与人类表现差距大。

Conclusion: 强调提升多模态AI系统因果推理能力的必要性。

Abstract: Recent advances in Vision-Language Models (VLMs) have demonstrated impressive
capabilities in perception and reasoning. However, the ability to perform
causal inference -- a core aspect of human cognition -- remains underexplored,
particularly in multimodal settings. In this study, we introduce InfoCausalQA,
a novel benchmark designed to evaluate causal reasoning grounded in
infographics that combine structured visual data with textual context. The
benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning
based on inferred numerical trends, while Task 2 targets semantic causal
reasoning involving five types of causal relations: cause, effect,
intervention, counterfactual, and temporal. We manually collected 494
infographic-text pairs from four public sources and used GPT-4o to generate
1,482 high-quality multiple-choice QA pairs. These questions were then
carefully revised by humans to ensure they cannot be answered based on
surface-level cues alone but instead require genuine visual grounding. Our
experimental results reveal that current VLMs exhibit limited capability in
computational reasoning and even more pronounced limitations in semantic causal
reasoning. Their significantly lower performance compared to humans indicates a
substantial gap in leveraging infographic-based information for causal
inference. Through InfoCausalQA, we highlight the need for advancing the causal
reasoning abilities of multimodal AI systems.

</details>


### [233] [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)
*Andrea Nasuto,Stefano Maria Iacus,Francisco Rowe,Devika Jain*

Main category: cs.CL

TL;DR: 研究微调大语言模型在跨语言话题检测的表现，发现有限语言覆盖可实现话题泛化，轻量级干预可纠正偏差，还发布低成本模型促进研究。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型微调后在少数语言获得的知识能否迁移到仅在预训练中出现的未见语言。

Method: 在单语、双语或多语言数据集上微调轻量级LLaMA 3.2 - 3B模型，对13种语言的移民相关推文分类。

Result: 单语或双语微调的模型可可靠分类未见语言的移民相关内容；多语言微调利于识别推文立场；少量接触低资源语言可显著改善预训练偏差。

Conclusion: 跨语言掌握不须大量多语言训练，有限语言覆盖和轻量级干预即可；发布低成本模型利于可扩展、包容性研究。

Abstract: Large language models (LLMs) are transforming social-science research by
enabling scalable, precise analysis. Their adaptability raises the question of
whether knowledge acquired through fine-tuning in a few languages can transfer
to unseen languages that only appeared during pre-training. To examine this, we
fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or
multilingual data sets to classify immigration-related tweets from X/Twitter
across 13 languages, a domain characterised by polarised, culturally specific
discourse. We evaluate whether minimal language-specific fine-tuning enables
cross-lingual topic detection and whether adding targeted languages corrects
pre-training biases. Results show that LLMs fine-tuned in one or two languages
can reliably classify immigration-related content in unseen languages. However,
identifying whether a tweet expresses a pro- or anti-immigration stance
benefits from multilingual fine-tuning. Pre-training bias favours dominant
languages, but even minimal exposure to under-represented languages during
fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training
token volume) yields significant gains. These findings challenge the assumption
that cross-lingual mastery requires extensive multilingual training: limited
language coverage suffices for topic-level generalisation, and structural
biases can be corrected with lightweight interventions. By releasing
4-bit-quantised, LoRA fine-tuned models, we provide an open-source,
reproducible alternative to proprietary LLMs that delivers 35 times faster
inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,
enabling scalable, inclusive research.

</details>


### [234] [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)
*Abolfazl Ansari,Delvin Ce Zhang,Nafis Irtiza Tripto,Dongwon Lee*

Main category: cs.CL

TL;DR: 研究分析超4万篇新闻文章中AI生成内容，发现近年GenAI使用显著增加，不同位置使用有差异，对写作风格有影响。


<details>
  <summary>Details</summary>
Motivation: 应对生成式AI对新闻业诚信和作者身份的挑战，了解其在新闻内容中的使用情况。

Method: 使用三种先进AI文本检测器，对不同类型、格式的超4万篇新闻文章进行分析，还有句子级和语言学分析。

Result: 近年GenAI使用显著增加，在地方和大学新闻中尤甚；新闻引言常用LLMs，结论多手动撰写；GenAI提升词汇丰富度和可读性，但降低正式性，使写作风格更趋同。

Conclusion: 未明确提及，但暗示GenAI在新闻中的使用已较为普遍且对写作风格产生影响，需关注其对新闻业的潜在影响。

Abstract: The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns
for journalistic integrity and authorship. This study examines AI-generated
content across over 40,000 news articles from major, local, and college news
media, in various media formats. Using three advanced AI-text detectors (e.g.,
Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of
GenAI use in recent years, especially in local and college news. Sentence-level
analysis reveals LLMs are often used in the introduction of news, while
conclusions usually written manually. Linguistic analysis shows GenAI boosts
word richness and readability but lowers formality, leading to more uniform
writing styles, particularly in local media.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [235] [Moment Estimate and Variational Approach for Learning Generalized Diffusion with Non-gradient Structures](https://arxiv.org/abs/2508.01854)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: physics.comp-ph

TL;DR: 提出数据驱动学习框架识别含非梯度分量广义扩散控制律，两阶段方法有效。


<details>
  <summary>Details</summary>
Motivation: 识别含非梯度分量广义扩散的控制律。

Method: 结合能量耗散定律、物理一致惩罚和一阶矩演化，设计两阶段方法恢复非梯度漂移点态正交分解中的伪势和旋转。

Result: 将两阶段方法应用于复杂广义扩散过程，代表性数值实验证明方法有效。

Conclusion: 所提方法可有效学习非梯度广义扩散中的物理定律。

Abstract: This paper proposes a data-driven learning framework for identifying
governing laws of generalized diffusions with non-gradient components. By
combining energy dissipation laws with a physically consistent penalty and
first-moment evolution, we design a two-stage method to recover the
pseudo-potential and rotation in the pointwise orthogonal decomposition of a
class of non-gradient drifts in generalized diffusions. Our two-stage method is
applied to complex generalized diffusion processes including
dissipation-rotation dynamics, rough pseudo-potentials and noisy data.
Representative numerical experiments demonstrate the effectiveness of our
approach for learning physical laws in non-gradient generalized diffusions.

</details>


### [236] [Hybrid Physics-Machine Learning Models for Quantitative Electron Diffraction Refinements](https://arxiv.org/abs/2508.05908)
*Shreshth A. Malik,Tiarnan A. S. Doherty,Benjamin Colmey,Stephen J. Roberts,Yarin Gal,Paul A. Midgley*

Main category: physics.comp-ph

TL;DR: 提出混合物理 - 机器学习框架用于高保真电子显微镜模拟，在3D - ED结构精修中表现出色。


<details>
  <summary>Details</summary>
Motivation: 高保真电子显微镜模拟面临难以对现实实验效果进行解析建模的挑战。

Method: 提出混合物理 - 机器学习框架，结合可微物理模拟与神经网络，利用自动微分进行梯度联合优化。

Result: 在合成和实验数据集上达到了先进的精修性能，能高保真恢复原子位置、热位移和厚度分布。

Conclusion: 可微混合建模是定量电子显微镜的强大新范式。

Abstract: High-fidelity electron microscopy simulations required for quantitative
crystal structure refinements face a fundamental challenge: while physical
interactions are well-described theoretically, real-world experimental effects
are challenging to model analytically. To address this gap, we present a novel
hybrid physics-machine learning framework that integrates differentiable
physical simulations with neural networks. By leveraging automatic
differentiation throughout the simulation pipeline, our method enables
gradient-based joint optimization of physical parameters and neural network
components representing experimental variables, offering superior scalability
compared to traditional second-order methods. We demonstrate this framework
through application to three-dimensional electron diffraction (3D-ED) structure
refinement, where our approach learns complex thickness distributions directly
from diffraction data rather than relying on simplified geometric models. This
method achieves state-of-the-art refinement performance across synthetic and
experimental datasets, recovering atomic positions, thermal displacements, and
thickness profiles with high fidelity. The modular architecture proposed can
naturally be extended to accommodate additional physical phenomena and extended
to other electron microscopy techniques. This establishes differentiable hybrid
modeling as a powerful new paradigm for quantitative electron microscopy, where
experimental complexities have historically limited analysis.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [237] [Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction](https://arxiv.org/abs/2508.05838)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: 本文提出将视觉基础模型与强化学习结合，在模拟环境提升对象交互能力，实验显示有显著改进。


<details>
  <summary>Details</summary>
Motivation: 提升模拟环境中对象交互能力。

Method: 将SAM和YOLOv5与PPO智能体结合，在AI2 - THOR模拟环境运行。

Result: 在四个不同室内厨房场景实验，平均累积奖励增加68%，对象交互成功率提高52.5%，导航效率提升33%。

Conclusion: 基础模型与强化学习结合对复杂机器人任务有潜力，为更智能自主智能体发展铺路。

Abstract: This paper presents a novel approach that integrates vision foundation models
with reinforcement learning to enhance object interaction capabilities in
simulated environments. By combining the Segment Anything Model (SAM) and
YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the
AI2-THOR simulation environment, we enable the agent to perceive and interact
with objects more effectively. Our comprehensive experiments, conducted across
four diverse indoor kitchen settings, demonstrate significant improvements in
object interaction success rates and navigation efficiency compared to a
baseline agent without advanced perception. The results show a 68% increase in
average cumulative reward, a 52.5% improvement in object interaction success
rate, and a 33% increase in navigation efficiency. These findings highlight the
potential of integrating foundation models with reinforcement learning for
complex robotic tasks, paving the way for more sophisticated and capable
autonomous agents.

</details>


### [238] [Bounding Distributional Shifts in World Modeling through Novelty Detection](https://arxiv.org/abs/2508.06096)
*Eric Jing,Abdeslam Boularias*

Main category: cs.RO

TL;DR: 本文提出用变分自编码器作新颖性检测器，使基于模型的规划算法对世界模型质量更鲁棒，实验表明该方法在数据效率上优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前大多数方法对训练质量敏感，需在训练时近乎完全覆盖动作和状态空间以防止推理时发散，为使基于模型的规划算法对学习到的世界模型质量更鲁棒。

Method: 使用变分自编码器作为新颖性检测器，确保规划期间提出的动作轨迹不会使学习到的模型偏离训练数据分布，并将该方法纳入扩展DINO - WM架构的模型预测控制策略循环。

Result: 在具有挑战性的模拟机器人环境中进行一系列实验，结果表明该方法在数据效率上优于现有解决方案。

Conclusion: 所提出的方法能有效提升基于模型的规划算法对世界模型质量的鲁棒性，且在数据效率上有优势。

Abstract: Recent work on visual world models shows significant promise in latent state
dynamics obtained from pre-trained image backbones. However, most of the
current approaches are sensitive to training quality, requiring near-complete
coverage of the action and state space during training to prevent divergence
during inference. To make a model-based planning algorithm more robust to the
quality of the learned world model, we propose in this work to use a
variational autoencoder as a novelty detector to ensure that proposed action
trajectories during planning do not cause the learned model to deviate from the
training data distribution. To evaluate the effectiveness of this approach, a
series of experiments in challenging simulated robot environments was carried
out, with the proposed method incorporated into a model-predictive control
policy loop extending the DINO-WM architecture. The results clearly show that
the proposed method improves over state-of-the-art solutions in terms of data
efficiency.

</details>


### [239] [Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation](https://arxiv.org/abs/2508.06426)
*Youguang Xing,Xu Luo,Junlin Xie,Lianli Gao,Hengtao Shen,Jingkuan Song*

Main category: cs.RO

TL;DR: 本文研究通用机器人策略泛化能力受限原因，指出捷径学习是关键阻碍，分析其成因并给出减少捷径学习、提升泛化能力的方法。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略在超出训练数据分布时泛化能力有限，需探究其根本原因。

Method: 进行全面的理论和实证分析，研究大型数据集的结构和特点。

Result: 发现捷径学习的两个主要成因是单个子数据集多样性有限和子数据集间分布差异大；还表明精心选择的数据增强策略可减少现有离线数据集中的捷径学习。

Conclusion: 研究为数据集收集策略提供见解，在无法获取新数据时，数据增强策略可提升通用机器人策略的泛化能力。

Abstract: Generalist robot policies trained on large-scale datasets such as Open
X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.
However, they often struggle to generalize beyond the distribution of their
training data. In this paper, we investigate the underlying cause of this
limited generalization capability. We identify shortcut learning -- the
reliance on task-irrelevant features -- as a key impediment to generalization.
Through comprehensive theoretical and empirical analysis, we uncover two
primary contributors to shortcut learning: (1) limited diversity within
individual sub-datasets, and (2) significant distributional disparities across
sub-datasets, leading to dataset fragmentation. These issues arise from the
inherent structure of large-scale datasets like OXE, which are typically
composed of multiple sub-datasets collected independently across varied
environments and embodiments. Our findings provide critical insights into
dataset collection strategies that can reduce shortcut learning and enhance the
generalization ability of generalist robot policies. Moreover, in scenarios
where acquiring new large-scale data is impractical, we demonstrate that
carefully selected robotic data augmentation strategies can effectively reduce
shortcut learning in existing offline datasets, thereby improving
generalization capabilities of generalist robot policies, e.g., $\pi_0$, in
both simulation and real-world environments. More information at
https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [240] [Training chord recognition models on artificially generated audio](https://arxiv.org/abs/2508.05878)
*Martyna Majchrzak,Jacek Mańdziuk*

Main category: cs.SD

TL;DR: 研究对比两个基于Transformer的神经网络模型用于音频和弦序列识别，检验人工生成数据集效果，证明其在特定场景有用。


<details>
  <summary>Details</summary>
Motivation: 音乐信息检索中获取无版权音频用于模型训练和评估有挑战，需研究人工生成数据集在和弦序列识别中的有效性。

Method: 用不同组合的人工音频多轨、舒伯特《冬之旅》数据集和麦吉尔公告牌数据集训练模型，用Root、MajMin和CCM三个指标评估。

Result: 人工生成音乐与人类创作音乐在复杂度和结构上有差异，但人工生成数据集在特定场景有用。

Conclusion: 人工音频多轨可丰富小的人类创作音乐训练集，在无其他数据时可作流行音乐和弦序列预测模型的独立训练集。

Abstract: One of the challenging problems in Music Information Retrieval is the
acquisition of enough non-copyrighted audio recordings for model training and
evaluation. This study compares two Transformer-based neural network models for
chord sequence recognition in audio recordings and examines the effectiveness
of using an artificially generated dataset for this purpose. The models are
trained on various combinations of Artificial Audio Multitracks (AAM),
Schubert's Winterreise Dataset, and the McGill Billboard Dataset and evaluated
with three metrics: Root, MajMin and Chord Content Metric (CCM). The
experiments prove that even though there are certainly differences in
complexity and structure between artificially generated and human-composed
music, the former can be useful in certain scenarios. Specifically, AAM can
enrich a smaller training dataset of music composed by a human or can even be
used as a standalone training set for a model that predicts chord sequences in
pop music, if no other data is available.

</details>


### [241] [DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching](https://arxiv.org/abs/2508.05978)
*Wei Chen,Binzhu Sha,Dan Luo,Jing Yang,Zhuo Wang,Fan Fan,Zhiyong Wu*

Main category: cs.SD

TL;DR: 提出DAFMSVC解决任意到任意的歌唱声音转换（SVC）中音色泄漏和音质问题，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有任意到任意SVC方法存在音色泄漏或生成音频音色相似度和质量不佳的问题。

Method: 提出DAFMSVC，用目标音频最相似的自监督学习（SSL）特征替换源音频的SSL特征，引入双交叉注意力机制融合说话人嵌入、旋律和语言内容，还加入流匹配模块用于高质量音频生成。

Result: DAFMSVC显著提高了音色相似度和自然度，在主观和客观评估中均优于现有方法。

Conclusion: DAFMSVC能有效解决任意到任意SVC中的挑战，提升转换效果。

Abstract: Singing Voice Conversion (SVC) transfers a source singer's timbre to a target
while keeping melody and lyrics. The key challenge in any-to-any SVC is
adapting unseen speaker timbres to source audio without quality degradation.
Existing methods either face timbre leakage or fail to achieve satisfactory
timbre similarity and quality in the generated audio. To address these
challenges, we propose DAFMSVC, where the self-supervised learning (SSL)
features from the source audio are replaced with the most similar SSL features
from the target audio to prevent timbre leakage. It also incorporates a dual
cross-attention mechanism for the adaptive fusion of speaker embeddings,
melody, and linguistic content. Additionally, we introduce a flow matching
module for high quality audio generation from the fused features. Experimental
results show that DAFMSVC significantly enhances timbre similarity and
naturalness, outperforming state-of-the-art methods in both subjective and
objective evaluations.

</details>


### [242] [EmoAugNet: A Signal-Augmented Hybrid CNN-LSTM Framework for Speech Emotion Recognition](https://arxiv.org/abs/2508.06321)
*Durjoy Chandra Paul,Gaurob Saha,Md Amjad Hossain*

Main category: cs.SD

TL;DR: 本文提出EmoAugNet框架用于语音情感识别，采用数据增强和混合建模提高系统性能，在两个数据集上取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 提高人机交互有效性，改善语音情感识别系统性能。

Method: 引入结合LSTM和1D - CNN的EmoAugNet框架，采用综合语音数据增强策略，将音频样本转换为高维特征向量。

Result: 在IEMOCAP和RAVDESS数据集上，不同激活函数下模型均取得较高的加权和非加权准确率。

Conclusion: EmoAugNet通过集成数据增强和混合建模，有效提高了语音情感识别系统的鲁棒性和性能。

Abstract: Recognizing emotional signals in speech has a significant impact on enhancing
the effectiveness of human-computer interaction (HCI). This study introduces
EmoAugNet, a hybrid deep learning framework, that incorporates Long Short-Term
Memory (LSTM) layers with one-dimensional Convolutional Neural Networks
(1D-CNN) to enable reliable Speech Emotion Recognition (SER). The quality and
variety of the features that are taken from speech signals have a significant
impact on how well SER systems perform. A comprehensive speech data
augmentation strategy was used to combine both traditional methods, such as
noise addition, pitch shifting, and time stretching, with a novel
combination-based augmentation pipeline to enhance generalization and reduce
overfitting. Each audio sample was transformed into a high-dimensional feature
vector using root mean square energy (RMSE), Mel-frequency Cepstral Coefficient
(MFCC), and zero-crossing rate (ZCR). Our model with ReLU activation has a
weighted accuracy of 95.78\% and unweighted accuracy of 92.52\% on the IEMOCAP
dataset and, with ELU activation, has a weighted accuracy of 96.75\% and
unweighted accuracy of 91.28\%. On the RAVDESS dataset, we get a weighted
accuracy of 94.53\% and 94.98\% unweighted accuracy for ReLU activation and
93.72\% weighted accuracy and 94.64\% unweighted accuracy for ELU activation.
These results highlight EmoAugNet's effectiveness in improving the robustness
and performance of SER systems through integated data augmentation and hybrid
modeling.

</details>


### [243] [MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows](https://arxiv.org/abs/2508.06098)
*Xiquan Li,Junxi Liu,Yuzhe Liang,Zhikang Niu,Wenxi Chen,Xie Chen*

Main category: cs.SD

TL;DR: 本文提出MeanAudio模型用于快速且准确的文本到音频生成，实验表明其在单步和多步音频生成中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前文本到音频生成系统推理速度慢，限制实际应用，需开发快速生成模型。

Method: 构建基于Flux风格潜在变压器的MeanFlow模型，回归平均速度场；将无分类器引导纳入训练目标；提出瞬时到平均的课程学习策略。

Result: MeanAudio在单步音频生成中达SOTA性能，在单张NVIDIA RTX 3090上实时因子为0.013，比SOTA扩散模型快100倍；在多步生成中也表现良好。

Conclusion: MeanAudio能实现快速且准确的文本到音频生成，提升了训练效率和生成质量。

Abstract: Recent developments in diffusion- and flow- based models have significantly
advanced Text-to-Audio Generation (TTA). While achieving great synthesis
quality and controllability, current TTA systems still suffer from slow
inference speed, which significantly limits their practical applicability. This
paper presents MeanAudio, a novel MeanFlow-based model tailored for fast and
faithful text-to-audio generation. Built on a Flux-style latent transformer,
MeanAudio regresses the average velocity field during training, enabling fast
generation by mapping directly from the start to the endpoint of the flow
trajectory. By incorporating classifier-free guidance (CFG) into the training
target, MeanAudio incurs no additional cost in the guided sampling process. To
further stabilize training, we propose an instantaneous-to-mean curriculum with
flow field mix-up, which encourages the model to first learn the foundational
instantaneous dynamics, and then gradually adapt to mean flows. This strategy
proves critical for enhancing training efficiency and generation quality.
Experimental results demonstrate that MeanAudio achieves state-of-the-art
performance in single-step audio generation. Specifically, it achieves a real
time factor (RTF) of 0.013 on a single NVIDIA RTX 3090, yielding a 100x speedup
over SOTA diffusion-based TTA systems. Moreover, MeanAudio also demonstrates
strong performance in multi-step generation, enabling smooth and coherent
transitions across successive synthesis steps.

</details>


### [244] [SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models](https://arxiv.org/abs/2508.06372)
*Han Yin,Yafeng Chen,Chong Deng,Luyao Cheng,Hui Wang,Chao-Hong Tan,Qian Chen,Wen Wang,Xiangang Li*

Main category: cs.SD

TL;DR: 提出用于SDR的统一多模态大语言模型SpeakerLM，联合执行SD和ASR，有灵活的说话人注册机制，实验表现佳。


<details>
  <summary>Details</summary>
Motivation: 现有级联SDR系统存在误差传播、难处理重叠语音、缺乏联合优化等局限性。

Method: 引入统一多模态大语言模型SpeakerLM，以端到端方式联合执行SD和ASR，融入灵活说话人注册机制，采用多阶段训练策略。

Result: SpeakerLM有强数据扩展能力和泛化性，在多个SDR基准上超越现有级联基线，说话人注册机制确保不同条件下的鲁棒性能。

Conclusion: SpeakerLM能有效解决现有级联SDR系统的问题，在SDR任务中有良好表现。

Abstract: The Speaker Diarization and Recognition (SDR) task aims to predict "who spoke
when and what" within an audio clip, which is a crucial task in various
real-world multi-speaker scenarios such as meeting transcription and dialogue
systems. Existing SDR systems typically adopt a cascaded framework, combining
multiple modules such as speaker diarization (SD) and automatic speech
recognition (ASR). The cascaded systems suffer from several limitations, such
as error propagation, difficulty in handling overlapping speech, and lack of
joint optimization for exploring the synergy between SD and ASR tasks. To
address these limitations, we introduce SpeakerLM, a unified multimodal large
language model for SDR that jointly performs SD and ASR in an end-to-end
manner. Moreover, to facilitate diverse real-world scenarios, we incorporate a
flexible speaker registration mechanism into SpeakerLM, enabling SDR under
different speaker registration settings. SpeakerLM is progressively developed
with a multi-stage training strategy on large-scale real data. Extensive
experiments show that SpeakerLM demonstrates strong data scaling capability and
generalizability, outperforming state-of-the-art cascaded baselines on both
in-domain and out-of-domain public SDR benchmarks. Furthermore, experimental
results show that the proposed speaker registration mechanism effectively
ensures robust SDR performance of SpeakerLM across diverse speaker registration
conditions and varying numbers of registered speakers.

</details>


### [245] [Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling](https://arxiv.org/abs/2508.06393)
*Md Asif Jalal,Luca Remaggi,Vasileios Moschopoulos,Thanasis Kotsiopoulos,Vandana Rajan,Karthikeyan Saravanan,Anastasis Drosou,Junho Heo,Hyuk Oh,Seokyeong Jeong*

Main category: cs.SD

TL;DR: 本文提出新方法训练同时进行语音分离和说话人分割模型，采用双阶段训练管道和重叠频谱损失函数，实验效果优于当前SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 传统语音分离和说话人分割方法依赖目标说话人先验知识或预定参与人数，为解决该局限，需开发免注册方法。

Method: 提出新方法，使用自动识别混合语音中目标说话人嵌入来训练；采用双阶段训练管道学习抗背景噪声干扰的说话人特征；提出重叠频谱损失函数提升重叠语音帧的分割准确性。

Result: 与当前SOTA基线相比有显著性能提升，DER相对改善71%，cpWER改善69%。

Conclusion: 所提方法在语音分离和说话人分割任务上效果良好，能有效提升性能。

Abstract: Traditional speech separation and speaker diarization approaches rely on
prior knowledge of target speakers or a predetermined number of participants in
audio signals. To address these limitations, recent advances focus on
developing enrollment-free methods capable of identifying targets without
explicit speaker labeling. This work introduces a new approach to train
simultaneous speech separation and diarization using automatic identification
of target speaker embeddings, within mixtures. Our proposed model employs a
dual-stage training pipeline designed to learn robust speaker representation
features that are resilient to background noise interference. Furthermore, we
present an overlapping spectral loss function specifically tailored for
enhancing diarization accuracy during overlapped speech frames. Experimental
results show significant performance gains compared to the current SOTA
baseline, achieving 71% relative improvement in DER and 69% in cpWER.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [246] [Leveraging large language models for SQL behavior-based database intrusion detection](https://arxiv.org/abs/2508.05690)
*Meital Shlezinger,Shay Akirav,Lei Zhou,Liang Guo,Avi Kessel,Guoliang Li*

Main category: cs.CR

TL;DR: 本文提出使用DistilBERT的两层SQL异常检测方法，结合无监督和有监督机器学习技术，有效保护数据库系统。


<details>
  <summary>Details</summary>
Motivation: 当前数据库异常访问行为增多，现有检测方法缺乏操作级粒度且难以识别类似正常活动的异常行为。

Method: 结合无监督和有监督机器学习技术，无监督方法用集成异常检测器标记远离正常模式的嵌入向量，有监督方法用微调的基于Transformer的模型进行高精度检测。

Result: 提出了有效的检测方法。

Conclusion: 该方法为保护关键数据库系统免受复杂威胁提供了有效解决方案。

Abstract: Database systems are extensively used to store critical data across various
domains. However, the frequency of abnormal database access behaviors, such as
database intrusion by internal and external attacks, continues to rise.
Internal masqueraders often have greater organizational knowledge, making it
easier to mimic employee behavior effectively. In contrast, external
masqueraders may behave differently due to their lack of familiarity with the
organization. Current approaches lack the granularity needed to detect
anomalies at the operational level, frequently misclassifying entire sequences
of operations as anomalies, even though most operations are likely to represent
normal behavior. On the other hand, some anomalous behaviors often resemble
normal activities, making them difficult for existing detection methods to
identify. This paper introduces a two-tiered anomaly detection approach for
Structured Query Language (SQL) using the Bidirectional Encoder Representations
from Transformers (BERT) model, specifically DistilBERT, a more efficient,
pre-trained version. Our method combines both unsupervised and supervised
machine learning techniques to accurately identify anomalous activities while
minimizing the need for data labeling. First, the unsupervised method uses
ensemble anomaly detectors that flag embedding vectors distant from learned
normal patterns of typical user behavior across the database (out-of-scope
queries). Second, the supervised method uses fine-tuned transformer-based
models to detect internal attacks with high precision (in-scope queries), using
role-labeled classification, even on limited labeled SQL data. Our findings
make a significant contribution by providing an effective solution for
safeguarding critical database systems from sophisticated threats.

</details>


### [247] [Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?](https://arxiv.org/abs/2508.05670)
*Daniele Proverbio,Alessio Buscemi,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Liò*

Main category: cs.CR

TL;DR: 研究经典博弈论框架能否捕捉大语言模型驱动的参与者和机器人行为，发现最终收益受多种因素影响且有语言敏感性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型给计算机系统安全带来新工具和挑战，探究经典博弈论框架能否捕捉其行为。

Method: 使用可复现的博弈论大语言模型代理框架，研究两种典型场景，涉及四种先进大语言模型和五种语言。

Result: 最终收益受参与者特征和语言选择影响，发现语言敏感性。

Conclusion: 应警惕大语言模型在网络安全应用中的随意使用，需深入研究，可用定量指标评估稳定性以指导模型选择和优化。

Abstract: Game theory has long served as a foundational tool in cybersecurity to test,
predict, and design strategic interactions between attackers and defenders. The
recent advent of Large Language Models (LLMs) offers new tools and challenges
for the security of computer systems; In this work, we investigate whether
classical game-theoretic frameworks can effectively capture the behaviours of
LLM-driven actors and bots. Using a reproducible framework for game-theoretic
LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum
game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to
expected outcomes or exhibit deviations due to embedded biases. Our experiments
involve four state-of-the-art LLMs and span five natural languages, English,
French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic
sensitivity. For both games, we observe that the final payoffs are influenced
by agents characteristics such as personality traits or knowledge of repeated
rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to
the choice of languages, which should warn against indiscriminate application
of LLMs in cybersecurity applications and call for in-depth studies, as LLMs
may behave differently when deployed in different countries. We also employ
quantitative metrics to evaluate the internal consistency and cross-language
stability of LLM agents, to help guide the selection of the most stable LLMs
and optimising models for secure applications.

</details>


### [248] [Voting-Based Semi-Parallel Proof-of-Work Protocol](https://arxiv.org/abs/2508.06489)
*Mustafa Doger,Sennur Ulukus*

Main category: cs.CR

TL;DR: 研究现有并行PoW协议易受激励攻击问题，提出投票式半并行PoW协议并评估其性能。


<details>
  <summary>Details</summary>
Motivation: 并行PoW协议用于改善中本聪共识的安全性、吞吐量和确认延迟，但需研究其安全性和性能。

Method: 针对现有并行PoW协议构建激励攻击结构，提出投票式半并行PoW协议，用先进分析评估一致性，用MDP模型证明抗激励攻击能力。

Result: 现有并行PoW协议比中本聪共识更易受激励攻击；投票式半并行PoW协议在多方面表现更优。

Conclusion: 投票式半并行PoW协议在实际应用中优于中本聪共识和现有并行PoW协议。

Abstract: Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety
guarantees, transaction throughput and confirmation latencies of Nakamoto
consensus. In this work, we first consider the existing parallel PoW protocols
and develop hard-coded incentive attack structures. Our theoretical results and
simulations show that the existing parallel PoW protocols are more vulnerable
to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller
profitability threshold and they result in higher relative rewards. Next, we
introduce a voting-based semi-parallel PoW protocol that outperforms both
Nakamoto consensus and the existing parallel PoW protocols from most practical
perspectives such as communication overheads, throughput, transaction
conflicts, incentive compatibility of the protocol as well as a fair
distribution of transaction fees among the voters and the leaders. We use
state-of-the-art analysis to evaluate the consistency of the protocol and
consider Markov decision process (MDP) models to substantiate our claims about
the resilience of our protocol against incentive attacks.

</details>


### [249] [Adaptive Backtracking for Privacy Protection in Large Language Models](https://arxiv.org/abs/2508.06087)
*Zhihao Yao,Yuxuan Gu,Xiachong Feng,Weitao Ma,Bo Li,Xiaocheng Feng*

Main category: cs.CR

TL;DR: 文章关注企业数据隐私保护，提出ABack机制和PriGenQA基准，实验表明ABack能提升隐私效用得分。


<details>
  <summary>Details</summary>
Motivation: 当前工作聚焦用户隐私，忽视企业数据泄漏风险，且现有方法存在性能问题和缺乏评估数据集。

Method: 提出无训练机制ABack，利用隐藏状态模型定位泄漏意图并改写输出；构建PriGenQA基准；开发强大的自适应攻击者。

Result: 在对抗高级对手时，ABack比强基线提高整体隐私效用得分达15%，避免了先前方法的性能权衡。

Conclusion: ABack机制和PriGenQA基准能有效解决企业数据隐私保护问题。

Abstract: The preservation of privacy has emerged as a critical topic in the era of
artificial intelligence. However, current work focuses on user-oriented
privacy, overlooking severe enterprise data leakage risks exacerbated by the
Retrieval-Augmented Generation paradigm. To address this gap, our paper
introduces a novel objective: enterprise-oriented privacy concerns. Achieving
this objective requires overcoming two fundamental challenges: existing methods
such as data sanitization severely degrade model performance, and the field
lacks public datasets for evaluation. We address these challenges with several
solutions. (1) To prevent performance degradation, we propose ABack, a
training-free mechanism that leverages a Hidden State Model to pinpoint the
origin of a leakage intention and rewrite the output safely. (2) To solve the
lack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy
scenarios in healthcare and finance. To ensure a rigorous evaluation, we move
beyond simple static attacks by developing a powerful adaptive attacker with
Group Relative Policy Optimization. Experiments show that against this superior
adversary, ABack improves the overall privacy utility score by up to 15\% over
strong baselines, avoiding the performance trade-offs of prior methods.

</details>


### [250] [Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models](https://arxiv.org/abs/2508.05865)
*Kiana Kiashemshaki,Elvis Nnaemeka Chukwuani,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.CR

TL;DR: 本文提出分析区块链电子投票系统的框架，指出流行模型局限并提出优化策略，探索大语言模型作用，为设计相关系统奠基。


<details>
  <summary>Details</summary>
Motivation: 区块链技术虽能提升电子投票系统性能，但现实应用因可扩展性、计算需求和隐私要求等挑战受限，需优化系统设计。

Method: 提出比较框架分析区块链电子投票架构、共识机制和加密协议；研究主流模型局限并提出优化策略；探索大语言模型在智能合约生成等方面的作用；进行基于模拟的分析。

Result: 为设计安全、可扩展和智能的全国性区块链电子投票系统提供基础。

Conclusion: 为构建由大语言模型引导的端到端区块链电子投票原型奠定基础。

Abstract: Blockchain technology offers a promising foundation for modernizing E-Voting
systems by enhancing transparency, decentralization, and security. Yet,
real-world adoption remains limited due to persistent challenges such as
scalability constraints, high computational demands, and complex privacy
requirements. This paper presents a comparative framework for analyzing
blockchain-based E-Voting architectures, consensus mechanisms, and
cryptographic protocols. We examine the limitations of prevalent models like
Proof of Work, Proof of Stake, and Delegated Proof of Stake, and propose
optimization strategies that include hybrid consensus, lightweight
cryptography, and decentralized identity management. Additionally, we explore
the novel role of Large Language Models (LLMs) in smart contract generation,
anomaly detection, and user interaction. Our findings offer a foundation for
designing secure, scalable, and intelligent blockchain-based E-Voting systems
suitable for national-scale deployment. This work lays the groundwork for
building an end-to-end blockchain E-Voting prototype enhanced by LLM-guided
smart contract generation and validation, supported by a systematic framework
and simulation-based analysis.

</details>


### [251] [Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark](https://arxiv.org/abs/2508.05674)
*Minghao Shao,Nanda Rani,Kimberly Milner,Haoran Xi,Meet Udeshi,Saksham Aggarwal,Venkata Sai Charan Putrevu,Sandeep Kumar Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri,Muhammad Shafique*

Main category: cs.CR

TL;DR: 本文系统研究驱动大语言模型（LLM）安全攻击代理成功的关键因素，提出CTFJudge、CTF Competency Index（CCI），研究超参数影响，推出CTFTiny基准测试，确定最优多智能体协调设置并开源。


<details>
  <summary>Details</summary>
Motivation: 探究驱动大语言模型安全攻击代理成功的关键因素，构建有效的基于大语言模型的进攻性安全代理。

Method: 提出CTFJudge框架进行细粒度评估；提出CTF Competency Index（CCI）指标；研究大语言模型超参数对代理性能和自动化网络安全任务规划的影响；推出CTFTiny基准测试。

Result: 确定了最优多智能体协调设置。

Conclusion: 为未来网络安全领域的大语言模型智能体研究奠定基础，开源CTFTiny和CTFJudge。

Abstract: Recent advances in LLM agentic systems have improved the automation of
offensive security tasks, particularly for Capture the Flag (CTF) challenges.
We systematically investigate the key factors that drive agent success and
provide a detailed recipe for building effective LLM-based offensive security
agents. First, we present CTFJudge, a framework leveraging LLM as a judge to
analyze agent trajectories and provide granular evaluation across CTF solving
steps. Second, we propose a novel metric, CTF Competency Index (CCI) for
partial correctness, revealing how closely agent solutions align with
human-crafted gold standards. Third, we examine how LLM hyperparameters, namely
temperature, top-p, and maximum token length, influence agent performance and
automated cybersecurity task planning. For rapid evaluation, we present
CTFTiny, a curated benchmark of 50 representative CTF challenges across binary
exploitation, web, reverse engineering, forensics, and cryptography. Our
findings identify optimal multi-agent coordination settings and lay the
groundwork for future LLM agent research in cybersecurity. We make CTFTiny open
source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on
https://github.com/NYU-LLM-CTF/CTFJudge.

</details>


### [252] [Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration](https://arxiv.org/abs/2508.05675)
*Jing Wang,Zheng Li,Lei Li,Fan He,Liyu Lin,Yao Lai,Yan Li,Xiaoyang Zeng,Yufeng Guo*

Main category: cs.CR

TL;DR: 本文提出用于Verilog代码优化的保IP的边云协同框架，实验显示该框架优化成功率高于基线方法，为安全硬件设计优化建立新范式。


<details>
  <summary>Details</summary>
Motivation: 强大的云基大语言模型在处理专有硬件设计时存在不可接受的IP泄漏风险，需在不泄露敏感IP信息前提下优化Verilog代码。

Method: 引入保IP的边云协同框架，用本地小LLM对目标设计和草稿代码进行安全比较分析得出设计原则，再用原则查询云LLM进行代码改进。

Result: 框架优化成功率显著高于基线方法，如Qwen - 2.5 - Coder - 7B和Deepseek - V3组合在电源利用率优化成功率达66.67%，不同模型组合对特定优化目标有不同优势。

Conclusion: 建立了平衡性能提升和IP保护的安全硬件设计优化新范式。

Abstract: Recent years have witnessed growing interest in adopting large language
models (LLMs) for Register Transfer Level (RTL) code optimization. While
powerful cloud-based LLMs offer superior optimization capabilities, they pose
unacceptable intellectual property (IP) leakage risks when processing
proprietary hardware designs. In this paper, we propose a new scenario where
Verilog code must be optimized for specific attributes without leaking
sensitive IP information. We introduce the first IP-preserving edge-cloud
collaborative framework that leverages the benefits of both paradigms. Our
approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure
comparative analysis between paired high-quality target designs and novice
draft codes, yielding general design principles that summarize key insights for
improvements. These principles are then used to query stronger cloud LLMs
(e.g., Deepseek-V3) for targeted code improvement, ensuring that only
abstracted and IP-safe guidance reaches external services. Our experimental
results demonstrate that the framework achieves significantly higher
optimization success rates compared to baseline methods. For example, combining
Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\% optimization success rate
for power utilization, outperforming Deepseek-V3 alone (49.81\%) and even
commercial models like GPT-4o (55.81\%). Further investigation of local and
cloud LLM combinations reveals that different model pairings exhibit varying
strengths for specific optimization objectives, with interesting trends
emerging when varying the number of comparative code pairs. Our work
establishes a new paradigm for secure hardware design optimization that
balances performance gains with IP protection.

</details>


### [253] [Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation](https://arxiv.org/abs/2508.05677)
*Peizhuo Liu*

Main category: cs.CR

TL;DR: 研究对基于强化学习的医疗问卷系统进行对抗攻击评估，发现系统在严格医疗约束下仍有显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 解决基于强化学习的医疗问卷系统的安全性和鲁棒性问题，识别并分析其潜在漏洞。

Method: 将诊断过程建模为马尔可夫决策过程，实施六种主流攻击方法，设置七个epsilon值，开发含247个医疗约束的验证框架，在NHIS数据集上实验，对AdaptiveFS框架评估攻击。

Result: 生成临床合理对抗样本成功率达97.6%，攻击成功率在33.08%（FGSM）到64.70%（AutoAttack）之间，对抗攻击显著影响诊断准确性。

Conclusion: 即使输入有严格医疗约束，基于强化学习的医疗问卷系统仍存在显著漏洞。

Abstract: RL-based medical questionnaire systems have shown great potential in medical
scenarios. However, their safety and robustness remain unresolved. This study
performs a comprehensive evaluation on adversarial attack methods to identify
and analyze their potential vulnerabilities. We formulate the diagnosis process
as a Markov Decision Process (MDP), where the state is the patient responses
and unasked questions, and the action is either to ask a question or to make a
diagnosis. We implemented six prevailing major attack methods, including the
Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini &
Wagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and
AutoAttack, with seven epsilon values each. To ensure the generated adversarial
examples remain clinically plausible, we developed a comprehensive medical
validation framework consisting of 247 medical constraints, including
physiological bounds, symptom correlations, and conditional medical
constraints. We achieved a 97.6% success rate in generating clinically
plausible adversarial samples. We performed our experiment on the National
Health Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which
consists of 182,630 samples, to predict the participant's 4-year mortality
rate. We evaluated our attacks on the AdaptiveFS framework proposed in
arXiv:2004.00994. Our results show that adversarial attacks could significantly
impact the diagnostic accuracy, with attack success rates ranging from 33.08%
(FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict
medical constraints on the input, such RL-based medical questionnaire systems
still show significant vulnerabilities.

</details>


### [254] [Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning](https://arxiv.org/abs/2508.05681)
*Yuhan Zhi,Longtian Wang,Xiaofei Xie,Chao Shen,Qiang Hu,Xiaohong Guan*

Main category: cs.CR

TL;DR: 本文提出 ALA 框架对主动学习进行攻击，实验表明攻击成功率高，提醒使用主动学习需谨慎。


<details>
  <summary>Details</summary>
Motivation: 探讨主动学习是否安全，现有研究未解答该问题。

Method: 引入 ALA 框架，优化不易察觉的中毒输入以展示高不确定性分数，增加被采集函数选中概率。

Result: 在低中毒预算（0.5%-1.0%）下攻击成功率高达 94%，保留模型实用性且难被人工标注者察觉。

Conclusion: 采集函数易被利用，在可信数据场景中应谨慎部署主动学习。

Abstract: Active learning(AL), which serves as the representative label-efficient
learning paradigm, has been widely applied in resource-constrained scenarios.
The achievement of AL is attributed to acquisition functions, which are
designed for identifying the most important data to label. Despite this
success, one question remains unanswered: is AL safe? In this work, we
introduce ALA, a practical and the first framework to utilize the acquisition
function as the poisoning attack surface to reveal the weakness of active
learning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit
high uncertainty scores, increasing their probability of being selected by
acquisition functions. To evaluate ALA, we conduct extensive experiments across
three datasets, three acquisition functions, and two types of clean-label
backdoor triggers. Results show that our attack can achieve high success rates
(up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model
utility and remaining undetectable to human annotators. Our findings remind
active learning users: acquisition functions can be easily exploited, and
active learning should be deployed with caution in trusted data scenarios.

</details>


### [255] [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Guanggang Geng,Zhiying Li,Jian Weng*

Main category: cs.CR

TL;DR: 提出DMFI框架结合语义推理与行为感知微调，用于内部威胁检测，在数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统模型难捕捉语义意图和复杂行为动态，现有基于大语言模型的解决方案在提示适应性和模态覆盖方面有局限。

Method: 提出DMFI框架，将原始日志转换为语义视图和行为抽象，用两个LoRA增强的大语言模型独立微调，输出通过轻量级基于MLP的决策模块融合，还引入DMFI - B策略分离正常和异常行为表示。

Result: 在CERT r4.2和r5.2数据集上的实验表明，DMFI在检测准确性上优于现有方法。

Conclusion: 结合大语言模型推理和结构化行为建模的方法有效，为现代内部威胁检测提供可扩展和可部署的解决方案。

Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge
in cybersecurity due to the subtle, long-term, and context-dependent nature of
malicious insider behaviors. Traditional models often struggle to capture
semantic intent and complex behavior dynamics, while existing LLM-based
solutions face limitations in prompt adaptability and modality coverage. To
bridge this gap, we propose DMFI, a dual-modality framework that integrates
semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into
two structured views: (1) a semantic view that processes content-rich artifacts
(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral
abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation
to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned
independently, and their outputs are fused via a lightweight MLP-based decision
module. We further introduce DMFI-B, a discriminative adaptation strategy that
separates normal and abnormal behavior representations, improving robustness
under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets
demonstrate that DMFI outperforms state-of-the-art methods in detection
accuracy. Our approach combines the semantic reasoning power of LLMs with
structured behavior modeling, offering a scalable and effective solution for
real-world insider threat detection. Our work demonstrates the effectiveness of
combining LLM reasoning with structured behavioral modeling, offering a
scalable and deployable solution for modern insider threat detection.

</details>


### [256] [Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition](https://arxiv.org/abs/2508.05696)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng*

Main category: cs.CR

TL;DR: 提出Log2Sig框架将用户日志转为多元行为频率信号，结合多种方法进行异常检测，在数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法将系统日志建模为扁平事件序列，无法捕捉用户行为的频率动态和多尺度干扰模式。

Method: 提出Log2Sig框架，用MVMD提取IMFs，结合Mamba时间编码器对行为序列和频率分解信号联合建模，融合后构建用户行为轮廓，用多层感知机检测异常。

Result: 在CERT r4.2和r5.2数据集上，Log2Sig在准确率和F1分数上显著优于现有基线。

Conclusion: Log2Sig是一个有效的内部威胁检测框架，能更好地捕捉用户行为特征进行异常检测。

Abstract: Insider threat detection presents a significant challenge due to the
deceptive nature of malicious behaviors, which often resemble legitimate user
operations. However, existing approaches typically model system logs as flat
event sequences, thereby failing to capture the inherent frequency dynamics and
multiscale disturbance patterns embedded in user behavior. To address these
limitations, we propose Log2Sig, a robust anomaly detection framework that
transforms user logs into multivariate behavioral frequency signals,
introducing a novel representation of user behavior. Log2Sig employs
Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode
Functions (IMFs), which reveal behavioral fluctuations across multiple temporal
scales. Based on this, the model further performs joint modeling of behavioral
sequences and frequency-decomposed signals: the daily behavior sequences are
encoded using a Mamba-based temporal encoder to capture long-term dependencies,
while the corresponding frequency components are linearly projected to match
the encoder's output dimension. These dual-view representations are then fused
to construct a comprehensive user behavior profile, which is fed into a
multilayer perceptron for precise anomaly detection. Experimental results on
the CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly
outperforms state-of-the-art baselines in both accuracy and F1 score.

</details>


### [257] [MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models](https://arxiv.org/abs/2508.05684)
*Junhao He,Tianyu Liu,Jingyuan Zhao,Benjamin Turner*

Main category: cs.CR

TL;DR: 本文提出MM - FusionNet框架用于多模态假新闻检测，核心是CADFM模块，在大规模数据集上取得先进F1分数，性能超现有方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上多模态假新闻威胁公众信任和社会稳定，传统基于文本检测方法不足，LVLMs融合多模态信息有挑战。

Method: 引入MM - FusionNet框架，使用核心CADFM模块，通过双向跨模态注意力和动态模态门控网络自适应学习和分配文本与视觉特征权重。

Result: 在含80000个样本的数据集上，MM - FusionNet获得0.938的F1分数，超现有多模态基线约0.5%，远超单模态方法。

Conclusion: 模型具有动态加权能力、对模态扰动有鲁棒性，性能接近人类水平，在现实假新闻检测中有实用性和可解释性。

Abstract: The proliferation of multi-modal fake news on social media poses a
significant threat to public trust and social stability. Traditional detection
methods, primarily text-based, often fall short due to the deceptive interplay
between misleading text and images. While Large Vision-Language Models (LVLMs)
offer promising avenues for multi-modal understanding, effectively fusing
diverse modal information, especially when their importance is imbalanced or
contradictory, remains a critical challenge. This paper introduces
MM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal
fake news detection. Our core contribution is the Context-Aware Dynamic Fusion
Module (CADFM), which employs bi-directional cross-modal attention and a novel
dynamic modal gating network. This mechanism adaptively learns and assigns
importance weights to textual and visual features based on their contextual
relevance, enabling intelligent prioritization of information. Evaluated on the
large-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples,
MM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing
multi-modal baselines by approximately 0.5% and significantly outperforming
single-modal approaches. Further analysis demonstrates the model's dynamic
weighting capabilities, its robustness to modality perturbations, and
performance remarkably close to human-level, underscoring its practical
efficacy and interpretability for real-world fake news detection.

</details>


### [258] [MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection](https://arxiv.org/abs/2508.05695)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng,Jian Weng*

Main category: cs.CR

TL;DR: 本文提出基于Mamba状态空间模型和跨模态自适应融合的内部威胁检测框架MambaITD，相比传统方法有显著优势。


<details>
  <summary>Details</summary>
Motivation: 企业面临内部威胁风险增加，现有检测方法存在时间动态特征建模不足、计算效率和实时性瓶颈以及跨模态信息孤岛等问题。

Method: 提出MambaITD框架，包括多源日志预处理模块对异构数据进行处理；Mamba编码器建模长距离依赖并结合门控特征融合机制动态融合信息；提出基于最大化类间方差的自适应阈值优化方法。

Result: MambaITD在建模效率和特征融合能力上有显著优势，优于基于Transformer的方法。

Conclusion: MambaITD为内部威胁检测提供了更有效的解决方案。

Abstract: Enterprises are facing increasing risks of insider threats, while existing
detection methods are unable to effectively address these challenges due to
reasons such as insufficient temporal dynamic feature modeling, computational
efficiency and real-time bottlenecks and cross-modal information island
problem. This paper proposes a new insider threat detection framework MambaITD
based on the Mamba state space model and cross-modal adaptive fusion. First,
the multi-source log preprocessing module aligns heterogeneous data through
behavioral sequence encoding, interval smoothing, and statistical feature
extraction. Second, the Mamba encoder models long-range dependencies in
behavioral and interval sequences, and combines the sequence and statistical
information dynamically in combination with the gated feature fusion mechanism.
Finally, we propose an adaptive threshold optimization method based on
maximizing inter-class variance, which dynamically adjusts the decision
threshold by analyzing the probability distribution, effectively identifies
anomalies, and alleviates class imbalance and concept drift. Compared with
traditional methods, MambaITD shows significant advantages in modeling
efficiency and feature fusion capabilities, outperforming Transformer-based
methods, and provides a more effective solution for insider threat detection.

</details>


### [259] [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
*Sanket Badhe*

Main category: cs.CR

TL;DR: 提出基于大语言模型的ScamAgent可生成逼真诈骗脚本，证明现有安全防护无效，强调多轮安全审计等需求。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有被滥用风险，需研究其在诈骗场景中的应用及现有防护有效性。

Method: 构建ScamAgent，利用其生成诈骗脚本，结合文本转语音系统形成自动化诈骗流程。

Result: 当前大语言模型安全防护对基于代理的威胁无效，可绕过强提示级防护。

Conclusion: 迫切需要多轮安全审计、代理级控制框架及检测和破坏生成式AI对话欺骗的新方法。

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [260] [To Each Their Own: Heterogeneity in Worker Preferences for Peer Information](https://arxiv.org/abs/2508.06162)
*Zhi Hao Lim*

Main category: econ.GN

TL;DR: 本文开发方法研究员工信息偏好异质性，识别四种员工类型，验证分类，发现部分员工因压力回避信息，按类型调整信息时机可提升福利。


<details>
  <summary>Details</summary>
Motivation: 职场中员工对同伴信息的重视情况和原因存在差异，需研究信息偏好异质性及潜在机制。

Method: 在有793名员工参与的真实任务实验中，引出员工对任务前后同伴信息的支付意愿。

Result: 识别出四种员工类型，员工陈述动机与偏好和行为强相关，15%员工因压力事前回避信息且无生产率提升，按类型调整信息时机福利最高提升48%。

Conclusion: 开发的方法有效，按员工类型调整信息时机能显著改善福利。

Abstract: Peer information is pervasive in the workplace, but workers differ in whether
and why they value such information. We develop a portable, theory-driven
methodology to study heterogeneity in information preferences and the
underlying mechanisms. In a real-effort experiment with 793 workers, we elicit
willingness-to-pay for peer information delivered either before or after a
task. We identify four worker types (indifferent, stress-avoidant, competitive,
and learning-oriented) whose effort responses align with theoretical
predictions. Workers' stated motivations in free-text responses strongly
correlate with their revealed preferences and behavior, validating our
classification. Notably, a nontrivial share (15%) strictly prefers to avoid
information ex ante due to stress and exhibit no productivity gains from it.
Tailoring the timing of information by worker type improves welfare by up to
48% relative to a uniform policy.

</details>


### [261] [Strategy Method Effects in Centipede Games: An Optimal Design Approach](https://arxiv.org/abs/2508.06425)
*Shiang-Hung Hu,Po-Hsuan Lin,Thomas R. Palfrey,Joseph Tao-yi Wang,Yu-Hsiang Wang*

Main category: econ.GN

TL;DR: 研究策略方法在序贯博弈实验中产生行为扭曲的时间和原因，对比三种选择引出方法下的蜈蚣博弈行为，发现显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽记录了策略方法在引出选择时的行为扭曲，但对驱动这些扭曲的理论因素理解不足，因此要探究何时及为何产生这种扭曲。

Method: 对比三种选择引出方法（直接反应法、简化策略法和完整策略法）在六个最优设计的蜈蚣博弈中的行为。

Result: 三种引出方法存在显著行为差异，无法用标准博弈论解释，与动态认知层次解结合量子反应的预测一致。

Conclusion: 动态认知层次解结合量子反应能更好解释策略方法在序贯博弈中引出选择时产生的行为扭曲。

Abstract: We explore the twin questions of when and why the strategy method creates
behavioral distortions in the elicitation of choices in laboratory studies of
sequential games. While such distortions have been widely documented, the
theoretical forces driving these distortions remain poorly understood. In this
paper, we compare behavior in six optimally designed centipede games,
implemented under three different choice elicitation methods: the direct
response method, the reduced strategy method and the full strategy method.
These methods elicit behavioral strategies, reduced strategies, and complete
strategies, respectively. We find significant behavioral differences across
these elicitation methods -- differences that cannot be explained by standard
game theory, but are consistent with the predictions of the Dynamic Cognitive
Hierarchy solution (Lin and Palfrey, 2024), combined with quantal responses.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [262] [CLAPP: The CLASS LLM Agent for Pair Programming](https://arxiv.org/abs/2508.05728)
*Santiago Casas,Christian Fidler,Boris Bolliet,Francisco Villaescusa-Navarro,Julien Lesgourgues*

Main category: astro-ph.IM

TL;DR: 介绍CLAPP，一款支持使用CLASS的交互式AI助手，降低科学家使用门槛，促进人机协作，应用可访问https://classclapp.streamlit.app


<details>
  <summary>Details</summary>
Motivation: 为使用Einstein - Boltzmann求解器CLASS的研究人员提供交互式AI辅助，降低不熟悉AI工具的科学家的入门门槛，促进人机协作

Method: 利用大语言模型和特定领域检索，结合多智能体大语言模型编排、CLASS文档语义搜索和实时Python执行环境

Result: 开发出CLAPP并部署为用户友好的Web应用

Conclusion: CLAPP能为使用CLASS的研究人员提供对话式编码支持，推动计算和数值宇宙学中的人机高效协作

Abstract: We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI
assistant designed to support researchers working with the Einstein-Boltzmann
solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific
retrieval to provide conversational coding support for CLASS-answering
questions, generating code, debugging errors, and producing plots. Its
architecture combines multi-agent LLM orchestration, semantic search across
CLASS documentation, and a live Python execution environment. Deployed as a
user-friendly web application, CLAPP lowers the entry barrier for scientists
unfamiliar with AI tools and enables more productive human-AI collaboration in
computational and numerical cosmology. The app is available at
https://classclapp.streamlit.app

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [263] [A Physiologically-Constrained Neural Network Digital Twin Framework for Replicating Glucose Dynamics in Type 1 Diabetes](https://arxiv.org/abs/2508.05705)
*Valentina Roquemen-Echeverri,Taisa Kushner,Peter G. Jacobs,Clara Mosquera-Lopez*

Main category: q-bio.QM

TL;DR: 提出生理约束神经网络数字孪生模型模拟1型糖尿病患者血糖动态，经真实数据验证有效，可用于个性化治疗测试和胰岛素优化。


<details>
  <summary>Details</summary>
Motivation: 现有模拟1型糖尿病患者血糖动态的模型常缺失关键生理方面且难以个性化，需要更好的模型来支持个性化治疗和临床决策。

Method: 构建与描述血糖调节的常微分方程对齐的群体级神经网络状态空间模型，再用包含个人数据的个体特定模型增强该群体模型创建数字孪生，使用真实数据验证。

Result: 在394个数字孪生中，模拟和观察数据的血糖结果相当，如在目标范围内时间等指标相近。

Conclusion: 该框架能纳入未建模因素，可实现个性化治疗的计算机模拟测试、支持胰岛素优化，整合了基于物理和数据驱动的建模。

Abstract: Simulating glucose dynamics in individuals with type 1 diabetes (T1D) is
critical for developing personalized treatments and supporting data-driven
clinical decisions. Existing models often miss key physiological aspects and
are difficult to individualize. Here, we introduce physiologically-constrained
neural network (NN) digital twins to simulate glucose dynamics in T1D. To
ensure interpretability and physiological consistency, we first build a
population-level NN state-space model aligned with a set of ordinary
differential equations (ODEs) describing glucose regulation. This model is
formally verified to conform to known T1D dynamics. Digital twins are then
created by augmenting the population model with individual-specific models,
which include personal data, such as glucose management and contextual
information, capturing both inter- and intra-individual variability. We
validate our approach using real-world data from the T1D Exercise Initiative
study. Two weeks of data per participant were split into 5-hour sequences and
simulated glucose profiles were compared to observed ones. Clinically relevant
outcomes were used to assess similarity via paired equivalence t-tests with
predefined clinical equivalence margins. Across 394 digital twins, glucose
outcomes were equivalent between simulated and observed data: time in range
(70-180 mg/dL) was 75.1$\pm$21.2% (simulated) vs. 74.4$\pm$15.4% (real;
P<0.001); time below range (<70 mg/dL) 2.5$\pm$5.2% vs. 3.0$\pm$3.3% (P=0.022);
and time above range (>180 mg/dL) 22.4$\pm$22.0% vs. 22.6$\pm$15.9% (P<0.001).
Our framework can incorporate unmodeled factors like sleep and activity while
preserving key dynamics. This approach enables personalized in silico testing
of treatments, supports insulin optimization, and integrates physics-based and
data-driven modeling. Code: https://github.com/mosqueralopez/T1DSim_AI

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [264] [Detecting Model Misspecification in Cosmology with Scale-Dependent Normalizing Flows](https://arxiv.org/abs/2508.05744)
*Aizhan Akhmetzhanova,Carolina Cuesta-Lazaro,Siddharth Mishra-Sharma*

Main category: astro-ph.CO

TL;DR: 提出结合尺度依赖神经统计量与归一化流的框架检测宇宙模拟模型错误指定。


<details>
  <summary>Details</summary>
Motivation: 当前和未来宇宙学调查产生大量高维数据，验证理论模型能否准确描述观测数据是挑战，且需选择合适数据表示方式。

Method: 结合尺度依赖神经统计量与归一化流，通过贝叶斯证据估计检测模型错误指定，在平滑尺度上对神经网络模型进行数据压缩和证据估计。

Result: 在三个不同次网格物理实现的CAMELS模拟套件的物质和气体密度场上进行了首次应用。

Conclusion: 能以数据驱动的方式系统识别理论模型失效之处。

Abstract: Current and upcoming cosmological surveys will produce unprecedented amounts
of high-dimensional data, which require complex high-fidelity forward
simulations to accurately model both physical processes and systematic effects
which describe the data generation process. However, validating whether our
theoretical models accurately describe the observed datasets remains a
fundamental challenge. An additional complexity to this task comes from
choosing appropriate representations of the data which retain all the relevant
cosmological information, while reducing the dimensionality of the original
dataset. In this work we present a novel framework combining scale-dependent
neural summary statistics with normalizing flows to detect model
misspecification in cosmological simulations through Bayesian evidence
estimation. By conditioning our neural network models for data compression and
evidence estimation on the smoothing scale, we systematically identify where
theoretical models break down in a data-driven manner. We demonstrate a first
application to our approach using matter and gas density fields from three
CAMELS simulation suites with different subgrid physics implementations.

</details>
