<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 71]
- [cs.CE](#cs.CE) [Total: 9]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 19]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 224]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.SE](#cs.SE) [Total: 22]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 3]
- [q-fin.RM](#q-fin.RM) [Total: 3]
- [q-fin.ST](#q-fin.ST) [Total: 11]
- [q-fin.TR](#q-fin.TR) [Total: 3]
- [stat.ML](#stat.ML) [Total: 19]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.OH](#cs.OH) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [math.NT](#math.NT) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 8]
- [q-bio.QM](#q-bio.QM) [Total: 4]
- [q-fin.GN](#q-fin.GN) [Total: 4]
- [cs.CL](#cs.CL) [Total: 53]
- [cs.NI](#cs.NI) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [econ.GN](#econ.GN) [Total: 9]
- [cs.PL](#cs.PL) [Total: 2]
- [stat.AP](#stat.AP) [Total: 3]
- [stat.ME](#stat.ME) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [cs.CR](#cs.CR) [Total: 26]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 9]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.CC](#cs.CC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 12]
- [cs.CY](#cs.CY) [Total: 12]
- [math.ST](#math.ST) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.RO](#cs.RO) [Total: 13]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.MA](#cs.MA) [Total: 5]
- [econ.TH](#econ.TH) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.CV](#cs.CV) [Total: 65]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: 现有大视觉语言模型安全对齐存挑战，提出VisuoAlign框架解决，实验证明其能提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型安全对齐是关键挑战，现有防御易受多模态越狱攻击，存在视觉输入攻击面、推理链缺安全监督、模态融合时对齐退化等问题。

Method: 提出VisuoAlign框架，通过视觉 - 文本交互提示将安全约束嵌入推理过程，用蒙特卡罗树搜索构建安全关键提示轨迹，引入基于提示的缩放进行实时风险检测和合规响应。

Result: VisuoAlign能主动暴露风险，实现全面数据集生成。

Conclusion: VisuoAlign显著提高了大视觉语言模型对抗复杂跨模态威胁的鲁棒性。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [2] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文引入结构化认知循环（SCL）作为新兴智能的可执行认识论框架，阐述其贡献及对多领域的影响，强调实现认知原则的架构重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型缺乏认知架构，存在关键差距，需引入新的认识论框架。

Method: 基于心灵哲学和认知现象学，借鉴过程哲学、生成认知和扩展心灵理论，将智能定义为连续循环过程。

Result: SCL实现哲学见解的计算化，证明功能分离架构行为更优，重新定义智能。

Conclusion: SCL对心灵哲学、认识论和AI有重要影响，真正进步需能实现认知原则的架构。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [3] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 本文提出利用市民宇宙作为监管学习实验空间的科学政策议程，确定关键研究领域和实验主题，强调负责任的开发和使用方式。


<details>
  <summary>Details</summary>
Motivation: 探索市民宇宙作为监管学习实验空间的潜力，为政策研究提供信息。

Method: 与高级专家小组协商，确定关键研究领域和分析实验主题。

Result: 确定了包括可扩展性、实时反馈等关键研究领域，分析了交通、城市规划等实验主题。

Conclusion: 强调对市民宇宙的开发和使用采取负责任的方法，考虑多维度影响，探索融入实验空间生态系统的初步步骤。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [4] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: 提出PISA记忆系统，引入三模态适应机制和混合内存访问架构，在基准测试中提升适应性和知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理内存系统缺乏对不同任务的适应性，且忽视其建设性和面向任务的作用。

Method: 借鉴皮亚杰认知发展理论，引入三模态适应机制，设计混合内存访问架构。

Result: 在LOCOMO和AggQA基准测试中，PISA显著提升适应性和长期知识保留能力，达到新的最优水平。

Conclusion: PISA能有效解决现有AI代理内存系统的局限，提升性能。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [5] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 研究大语言模型解决汉诺塔问题时接入环境接口的表现，发现接入不延迟或消除性能崩溃，模型在各复杂度级别有类似崩溃现象。


<details>
  <summary>Details</summary>
Motivation: 解决大推理模型在解决谜题时性能崩溃问题，探讨任务性质对推理评估的影响，潜在干扰是模型需自行跟踪状态空间。

Method: 为大语言模型提供汉诺塔问题的环境接口，使其能进行操作、提供理由、观察状态并自我提示下一步。

Result: 接入环境接口不延迟或消除性能崩溃，模型参数化策略与最优和随机策略差异增大。

Conclusion: 大推理模型可能存在类似大语言模型在解决汉诺塔问题时的性能崩溃现象。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [6] [Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition](https://arxiv.org/abs/2510.15980)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出认知负荷轨迹（CLTs）作为深度模型的中层可解释性框架，定义为随机过程，通过可测量代理实例化，有符号公式和可视化方法，实验表明其能预测错误、揭示策略并提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 受人类认知中的认知负荷理论启发，为深度模型构建中层可解释性框架。

Method: 将CLTs表示为三组件随机过程，通过注意力熵等可测量代理实例化各组件，提出符号公式和可视化方法。

Result: 在推理和规划基准实验中，CLTs能预测错误发生、揭示认知策略，在保持准确性的同时将推理效率提高15 - 30%。

Conclusion: CLTs是一种有效的深度模型可解释性框架，可用于分析推理动态并提高推理效率。

Abstract: We propose \textbf{Cognitive Load Traces} (CLTs) as a mid-level
interpretability framework for deep models, inspired by Cognitive Load Theory
in human cognition. CLTs are defined as symbolic, temporally varying functions
that quantify model-internal resource allocation. Formally, we represent CLTs
as a three-component stochastic process $(\mathrm{IL}_t, \mathrm{EL}_t,
\mathrm{GL}_t)$, corresponding to \emph{Intrinsic}, \emph{Extraneous}, and
\emph{Germane} load. Each component is instantiated through measurable proxies
such as attention entropy, KV-cache miss ratio, representation dispersion, and
decoding stability. We propose both symbolic formulations and visualization
methods (load curves, simplex diagrams) that enable interpretable analysis of
reasoning dynamics. Experiments on reasoning and planning benchmarks show that
CLTs predict error-onset, reveal cognitive strategies, and enable load-guided
interventions that improve reasoning efficiency by 15-30\% while maintaining
accuracy.

</details>


### [7] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: 提出ProofFlow管道处理证明自动形式化，设新基准与评估指标，实验达SOTA并开源。


<details>
  <summary>Details</summary>
Motivation: 当前证明自动形式化方法常无法保留原证明语义和逻辑结构，需解决该问题。

Method: ProofFlow先构建DAG映射证明步骤逻辑依赖，再用引理法将步骤形式化为中间引理；设184个本科级问题基准及ProofScore评估指标。

Result: ProofFlow达到0.545的ProofScore，远超全证明形式化（0.123）和分步证明形式化（0.072）基线。

Conclusion: ProofFlow在证明自动形式化上表现出色，相关资源开源以推动研究进展。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [8] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: 本文介绍用于自主数据科学的代理大语言模型DeepAnalyze - 8B，提出课程式代理训练范式和数据驱动轨迹合成框架，实验显示其表现出色且相关资源开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于工作流的数据代理在实现完全自主数据科学方面存在局限，需要新方法解决从原始数据源到分析师级深度研究报告的自主生成挑战。

Method: 提出课程式代理训练范式，模拟人类数据科学家学习轨迹；引入数据驱动轨迹合成框架构建高质量训练数据。

Result: 仅80亿参数的DeepAnalyze在多数任务上超越基于最先进专有大语言模型构建的先前基于工作流的代理。

Conclusion: DeepAnalyze的开源为自主数据科学铺平道路。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [9] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 介绍运动研究数据仓库MO|RE，提出从其数据创建知识图谱的愿景，以标准化和让机器理解运动表现数据。


<details>
  <summary>Details</summary>
Motivation: 测试人类表现相关因素是评估和比较人群身体与认知能力的重要部分，需标准化和机器可理解地处理运动表现数据。

Method: 以基础形式本体为根源的本体，正式表示计划规范、特定过程和相关测量的相互关系。

Result: 无明确提及具体结果

Conclusion: 目标是改变运动表现数据在不同研究中的建模和共享方式，使其标准化和机器可理解。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [10] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文从随机有限集和Dempster - Shafer理论两个视角分析随机排列集（RPS）冲突，提出基于非重叠的冲突度量方法并通过数值示例验证其特性。


<details>
  <summary>Details</summary>
Motivation: 测量由排列质量函数表示的两条证据之间的冲突是顺序结构不确定信息融合中的紧迫研究课题。

Method: 从排列观察出发，受排名偏差重叠（RBO）度量启发定义排列间的不一致性度量，提出基于非重叠的RPS冲突度量方法，并将RPS理论视为DST的扩展。

Result: 通过数值示例展示了所提出冲突度量的行为和特性。

Conclusion: 所提方法具有自然的顶部加权特性，能从DST视角有效测量RPS间的冲突，为决策者提供灵活选择。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [11] [PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction](https://arxiv.org/abs/2510.16004)
*Andreas Radler,Vincent Seyfried,Stefan Pirker,Johannes Brandstetter,Thomas Lichtenegger*

Main category: cs.AI

TL;DR: 提出Parallel - in - time Neural Twins (PAINT) 方法建模动力系统，理论和实验证明其能保持轨迹，可从稀疏测量中高精度预测系统状态。


<details>
  <summary>Details</summary>
Motivation: 将神经替代物发展为神经孪生体，创建真实系统数字副本，实现特定上下文决策，且使模型具有保持轨迹的能力。

Method: 提出PAINT方法，训练生成式神经网络并行建模状态分布，测试时用滑动窗口从测量值预测状态。

Result: 理论分析表明PAINT能保持轨迹，而自回归模型通常不能；在二维湍流流体动力学问题上实验，PAINT能保持轨迹，从稀疏测量中高精度预测系统状态。

Conclusion: PAINT在开发能保持轨迹的神经孪生体方面有潜力，可实现更准确的状态估计和决策。

Abstract: Neural surrogates have shown great potential in simulating dynamical systems,
while offering real-time capabilities. We envision Neural Twins as a
progression of neural surrogates, aiming to create digital replicas of real
systems. A neural twin consumes measurements at test time to update its state,
thereby enabling context-specific decision-making. A critical property of
neural twins is their ability to remain on-trajectory, i.e., to stay close to
the true system state over time. We introduce Parallel-in-time Neural Twins
(PAINT), an architecture-agnostic family of methods for modeling dynamical
systems from measurements. PAINT trains a generative neural network to model
the distribution of states parallel over time. At test time, states are
predicted from measurements in a sliding window fashion. Our theoretical
analysis shows that PAINT is on-trajectory, whereas autoregressive models
generally are not. Empirically, we evaluate our method on a challenging
two-dimensional turbulent fluid dynamics problem. The results demonstrate that
PAINT stays on-trajectory and predicts system states from sparse measurements
with high fidelity. These findings underscore PAINT's potential for developing
neural twins that stay on-trajectory, enabling more accurate state estimation
and decision-making.

</details>


### [12] [Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis](https://arxiv.org/abs/2510.16033)
*Junyu Ren,Wensheng Gan,Guangyu Zhang,Wei Zhong,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出信息分离全局焦点对抗网络(ISGFAN)用于噪声条件下跨域故障诊断，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有迁移故障诊断方法在严重噪声干扰和域偏移共存的工业环境中效果受限。

Method: 构建基于信息分离架构的ISGFAN，结合对抗学习与改进正交损失解耦域不变故障表示，采用全局焦点域对抗方案约束模型的条件和边缘分布。

Result: 在三个公开基准数据集上的实验表明，ISGFAN方法优于其他现有方法。

Conclusion: ISGFAN框架具有优越性。

Abstract: Existing transfer fault diagnosis methods typically assume either clean data
or sufficient domain similarity, which limits their effectiveness in industrial
environments where severe noise interference and domain shifts coexist. To
address this challenge, we propose an information separation global-focal
adversarial network (ISGFAN), a robust framework for cross-domain fault
diagnosis under noise conditions. ISGFAN is built on an information separation
architecture that integrates adversarial learning with an improved orthogonal
loss to decouple domain-invariant fault representation, thereby isolating noise
interference and domain-specific characteristics. To further strengthen
transfer robustness, ISGFAN employs a global-focal domain-adversarial scheme
that constrains both the conditional and marginal distributions of the model.
Specifically, the focal domain-adversarial component mitigates
category-specific transfer obstacles caused by noise in unsupervised scenarios,
while the global domain classifier ensures alignment of the overall
distribution. Experiments conducted on three public benchmark datasets
demonstrate that the proposed method outperforms other prominent existing
approaches, confirming the superiority of the ISGFAN framework. Data and code
are available at https://github.com/JYREN-Source/ISGFAN

</details>


### [13] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 本文探讨大规模AI模型对神经科学五个主要领域的变革性影响，强调其潜力、实施考量，还列出相关数据集。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型对神经科学研究有变革性影响，传统计算方法面临范式转变，需探索其在神经科学多领域的作用。

Method: 对大规模AI模型在神经科学五个主要领域的应用进行探讨和分析。

Result: 大规模AI模型能解决计算神经科学的主要挑战，神经科学与AI的互动日益互惠，还给出关键实施考量。

Conclusion: 大规模AI模型在神经科学有显著潜力，但需严格评估框架、有效整合领域知识和临床使用的伦理准则。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [14] [Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks](https://arxiv.org/abs/2510.16047)
*Ioan Hedea*

Main category: cs.AI

TL;DR: 本文结合离线约束编程优化与在线时间网络执行，创建在最坏不确定性下仍可行的调度方案，实验表明该混合方法可消除所有期限违规，开销小且有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需应对随机任务时长，传统确定性调度在实际偏离计划时会失效，因此需要创建能在最坏不确定性下仍可行的调度方案。

Method: 先构建含作业期限任务的柔性作业车间的约束编程模型并插入最优缓冲，再将计划转化为含不确定性的简单时间网络并验证动态可控性。

Result: 在开放基准套件上的蒙特卡罗模拟显示，该混合方法消除了100%的期限违规，仅增加3 - 5%的完工时间开销，可扩展性实验证实中等规模实例的求解和检查时间不到一秒。

Conclusion: 时间网络推理可弥合主动缓冲和动态鲁棒性之间的差距，推动行业向真正的数字化、自校正工厂迈进。

Abstract: Modern manufacturing systems must meet hard delivery deadlines while coping
with stochastic task durations caused by process noise, equipment variability,
and human intervention. Traditional deterministic schedules break down when
reality deviates from nominal plans, triggering costly last-minute repairs.
This thesis combines offline constraint-programming (CP) optimisation with
online temporal-network execution to create schedules that remain feasible
under worst-case uncertainty. First, we build a CP model of the flexible
job-shop with per-job deadline tasks and insert an optimal buffer $\Delta^*$ to
obtain a fully pro-active baseline. We then translate the resulting plan into a
Simple Temporal Network with Uncertainty (STNU) and verify dynamic
controllability, which guarantees that a real-time dispatcher can retime
activities for every bounded duration realisation without violating resource or
deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4
benchmark suite show that our hybrid approach eliminates 100\% of deadline
violations observed in state-of-the-art meta-heuristic schedules, while adding
only 3--5\% makespan overhead. Scalability experiments confirm that CP
solve-times and STNU checks remain sub-second on medium-size instances. The
work demonstrates how temporal-network reasoning can bridge the gap between
proactive buffering and dynamic robustness, moving industry a step closer to
truly digital, self-correcting factories.

</details>


### [15] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 论文指出多跳推理在大语言模型问答中重要，现有方法有局限，提出DTKG框架。


<details>
  <summary>Details</summary>
Motivation: 现有多跳推理方法在并行事实验证和链式推理任务中各有不足，影响问答效率和准确性。

Method: 受认知科学双过程理论启发，提出DTKG框架，包含分类阶段和分支处理阶段。

Result: 未提及

Conclusion: 未提及

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [16] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 研究评估大语言模型生成临床推理链可靠性，对比不同提示策略，选择性少样本策略最优，提出“双原则”框架解决数据瓶颈。


<details>
  <summary>Details</summary>
Motivation: 创建高质量临床推理链受数据稀缺限制，大语言模型生成的临床可靠性未经验证，需评估并提升其质量。

Method: 开展盲法对比研究，让辅助生殖技术领域资深临床医生评估三种策略生成的推理链，并与GPT - 4o评估结果对比。

Result: 选择性少样本策略在所有人工评估指标上显著优于其他策略，随机少样本策略无显著提升，AI评估器未能识别性能差异。

Conclusion: 合成推理链的临床可靠性取决于提示策略，提出“双原则”框架，确认人类专业知识在评估临床AI中的关键作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [17] [Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability](https://arxiv.org/abs/2510.16193)
*Elija Perrier*

Main category: cs.AI

TL;DR: 论文基于扩展认知理论，重新定义企业知识，开发形式化模型，提出相关量化指标并映射到法律标准，为算法时代让企业思维可追踪和负责提供途径。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI对企业决策的影响增大，传统基于人类代理的企业犯罪意图假设受到挑战，需重新定义企业知识。

Method: 基于扩展认知理论，开发形式化模型，引入连续组织知识度量指标、阈值知识谓词和企业范围认知能力指数。

Result: 得到相关量化指标，并将其映射到实际知识、推定知识、故意视而不见和鲁莽等法律标准。

Conclusion: 为算法时代创建可衡量和可裁判的审计工件提供了途径，使企业思维可追踪和负责。

Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea},
traditionally imputed from human agents. Yet these assumptions are under
challenge as generative AI increasingly mediates enterprise decision-making.
Building on the theory of extended cognition, we argue that in response
corporate knowledge may be redefined as a dynamic capability, measurable by the
efficiency of its information-access procedures and the validated reliability
of their outputs. We develop a formal model that captures epistemic states of
corporations deploying sophisticated AI or information systems, introducing a
continuous organisational knowledge metric $S_S(\varphi)$ which integrates a
pipeline's computational cost and its statistically validated error rate. We
derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and
a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall
capability. We then operationally map these quantitative metrics onto the legal
standards of actual knowledge, constructive knowledge, wilful blindness, and
recklessness. Our work provides a pathway towards creating measurable and
justiciable audit artefacts, that render the corporate mind tractable and
accountable in the algorithmic age.

</details>


### [18] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: 提出PILLM框架用于HVAC系统异常检测，在公开数据集上达SOTA性能且规则可解释。


<details>
  <summary>Details</summary>
Motivation: HVAC系统能耗占比大，经典规则方法缺乏适应性，深度学习方法缺乏透明度等，现有LLM方法忽略物理原理，需有效异常检测方法。

Method: 提出PILLM框架，在进化循环中自动生成、评估和完善异常检测规则，引入物理信息反射和交叉算子嵌入热力学和控制理论约束。

Result: 在公共Building Fault Detection数据集上，PILLM达到了当前最优性能。

Conclusion: PILLM框架可产生可解释和可操作的诊断规则，推动了智能建筑系统中可信且可部署的AI发展。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [19] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: 提出TEAM - PHI框架，用大语言模型自动评估和选择PHI去标识模型，实验证明其有效且与人工评估匹配。


<details>
  <summary>Details</summary>
Motivation: 现有评估和比较PHI去标识模型依赖昂贵的小规模专家标注，需要更高效方法。

Method: 部署多个评估代理独立判断PHI提取正确性并输出指标，通过基于大语言模型的多数投票机制整合结果。

Result: 在真实临床笔记语料库实验中产生一致准确的排名，自动排名与人工监督评估匹配。

Conclusion: TEAM - PHI为PHI去标识模型的自动评估和最佳模型选择提供实用、安全且经济的解决方案，在缺乏真实标签时也适用。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [20] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 随着大语言模型发展，其信息检索存在问题，本文提出‘被铭记权’概念解决。


<details>
  <summary>Details</summary>
Motivation: 大语言模型信息检索存在将多视角合成单一答案、易造成信息偏差与遗漏、集中信息权力等问题，威胁集体记忆，需解决。

Method: 提出‘被铭记权’（RTBR）概念。

Result: 未提及。

Conclusion: 通过‘被铭记权’概念，可最小化AI信息遗漏风险、保障公平对待权、确保内容真实。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [21] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 本文介绍了评估框架ScholarEval，用ScholarIdeas数据集评估，结果显示其优于基线，还开展大规模用户研究，最后开源代码、数据集和工具。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具用于研究构思增多，需要强大评估确保生成想法的有效性和实用性。

Method: 引入ScholarEval评估框架，基于合理性和贡献度评估研究想法；引入ScholarIdeas数据集进行评估；开展大规模用户研究。

Result: ScholarEval在覆盖专家标注要点上优于所有基线；在评估可操作性、深度和证据支持方面优于最强基线o4 - mini - deep - research；在文献参与、想法完善和有用性上显著优于deep research。

Conclusion: ScholarEval是有效的评估框架，开源相关资源供社区使用和拓展。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [22] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 本文指出大推理模型存在推理分心漏洞，展示其易受攻击情况，提出基于训练的防御方法提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 发现大推理模型在处理复杂任务时存在被恶意嵌入的无关任务干扰的问题，需研究该漏洞及应对方法。

Method: 对不同模型和基准进行全面研究以分析漏洞，提出结合监督微调（SFT）和强化学习（RL）在合成对抗数据上的训练防御方法。

Result: 即使是最先进的大推理模型也高度易受攻击，注入干扰因素可使任务准确率降低达60%，特定对齐技术会放大弱点，模型可能存在隐蔽合规；所提防御方法在挑战性干扰攻击下将鲁棒性提高超50个点。

Conclusion: 推理分心是大推理模型可靠性的独特且紧迫威胁，所提方法为构建更安全可信的推理系统迈出了实际一步。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [23] [What Limits Agentic Systems Efficiency?](https://arxiv.org/abs/2510.16276)
*Song Bian,Minghao Yan,Anand Jayarajan,Gennady Pekhimenko,Shivaram Venkataraman*

Main category: cs.AI

TL;DR: 文章研究网络交互代理系统效率瓶颈，将端到端延迟分解为两部分，发现网络环境延迟占比高，提出SpecCache框架，评估显示其可提升缓存命中率、降低网络环境开销。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注推理性能，忽视代理系统效率，为解决此问题开展研究。

Method: 将端到端延迟分解为LLM API延迟和网络环境延迟，对15个模型和5个提供商进行实证研究，提出SpecCache缓存框架。

Result: 网络环境延迟在基于网络的代理系统中可占总延迟的53.7%；SpecCache框架比随机缓存策略缓存命中率最多提高58倍，网络环境开销最多降低3.2倍，且不降低代理系统性能。

Conclusion: 提出的SpecCache框架能有效提升网络交互代理系统的效率，降低网络环境开销。

Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have
demonstrated strong reasoning capabilities. To further enhance LLM
capabilities, recent agentic systems, such as Deep Research, incorporate web
interactions into LLM reasoning to mitigate uncertainties and reduce potential
errors. However, existing research predominantly focuses on reasoning
performance, often neglecting the efficiency of agentic systems. In this work,
we present a comprehensive empirical study that identifies efficiency
bottlenecks in web-interactive agentic systems. We decompose end-to-end latency
into two primary components: LLM API latency and web environment latency. We
conduct a comprehensive empirical study across 15 models and 5 providers to
demonstrate high variability in API-based agentic systems. We observe that web
environment latency can contribute as much as 53.7% to the overall latency in a
web-based agentic system. To improve latency, we propose SpecCache, a caching
framework augmented with speculative execution that can reduce web environment
overhead. Extensive evaluations on two standard benchmarks show that our
approach improves the cache hit rate by up to 58x compared to a random caching
strategy, while reducing web environment overhead by up to 3.2x, without
degrading agentic system performance.

</details>


### [24] [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](https://arxiv.org/abs/2510.17614)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: 提出单解码器方法OG - Rank用于实时排序系统，在临床订单选择任务表现好，设计简单且原则可迁移。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要实时且能解释选择的排序系统，需要低延迟、基于解码器的重排器。

Method: 提出OG - Rank，将池化的首令牌评分信号与不确定性门控解释步骤结合，用专注于难题的课程进行训练。

Result: OG - Rank在临床订单选择任务中效果好，激活门控时效果更好，编码器基线在效果和灵活性上落后。

Conclusion: 提出的方法是实用方案，设计简化部署和预算规划，课程原则可迁移到其他决策任务。

Abstract: Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

</details>


### [25] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: 引入MedRule - KG知识图谱和符号验证器，提升推理任务表现，在FDA基准上效果显著并开源代码数据。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在推理时违反数学或逻辑约束的问题。

Method: 引入MedRule - KG知识图谱编码实体、关系和规则，用符号验证器检查和修正预测。

Result: 在90个样本的FDA基准上，使用MedRule - KG使精确匹配率从0.767提升到0.900，加入验证器后精确匹配率达1.000且无规则违反。

Conclusion: MedRule - KG为安全数学推理提供通用框架。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [26] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: 现有文本到图像扩散模型概念擦除方法依赖固定锚定策略有问题，提出SELECT动态锚定选择框架，表现优且效率高。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型概念擦除方法依赖固定锚定策略，导致概念重新出现和侵蚀等关键问题。

Method: 进行因果追踪揭示擦除对锚定选择的内在敏感性，定义兄弟排他概念作为更优的锚定，提出SELECT动态锚定选择框架，引入两阶段评估机制。

Result: SELECT作为通用锚定解决方案，能有效适应多种擦除框架，在关键性能指标上始终优于现有基线，单个概念锚定挖掘平均仅需4秒。

Conclusion: SELECT动态锚定选择框架可克服固定锚定的局限性，是一种高效且表现优异的解决方案。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [27] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 本文研究用户如何使算法符合自身真实兴趣，构建用户决策模型，通过Stackelberg博弈分析，指出存在关键时间跨度，小的高成本信号可降低该跨度。


<details>
  <summary>Details</summary>
Motivation: 用户与算法交互中存在偏好不一致问题，研究如何让用户使算法符合其真实兴趣。

Method: 将用户决策过程建模为理性系统2和冲动系统1，研究多领导者单跟随者的Stackelberg博弈，定义对齐负担。

Result: 存在关键时间跨度，有远见的用户可实现对齐，小的高成本信号可显著降低该跨度。

Conclusion: 框架解释了偏好不一致的用户在Stackelberg均衡中使算法符合自身兴趣的方式，指出了实现对齐的挑战和潜在解决办法。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [28] [Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs](https://arxiv.org/abs/2510.16374)
*Nick Oh*

Main category: cs.AI

TL;DR: 当前提升大语言模型推理能力的两种范式存在不足，本文实现认知监控模型构建三相迭代系统，在GSM8K上初步结果良好，但需更多评估。


<details>
  <summary>Details</summary>
Motivation: 当前Monitor - Generate和Generate - Verify两种提升大语言模型推理能力的范式相互分离，存在效率问题，需解决此问题。

Method: 实现Flavell的认知监控模型，将其作为三相迭代系统。

Result: 在GSM8K上准确率达75.42%，高于SELF - REFINE和Self - Verification，尝试次数更少，但推理成本增加27 - 37%。

Conclusion: 前期监控能产生高质量初始解决方案，减少细化需求，但需在算术推理之外评估以确定通用性。

Abstract: Current approaches to enhancing LLM reasoning follows two isolated paradigms:
Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and
SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack
mechanisms to verify whether selected strategies succeed; while Generate-Verify
approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan
et al., 2023) iteratively refine outputs but commence generation blindly
without task assessment. This separation creates inefficiencies -- strategies
fail without feedback, and refinement occurs without strategic grounding. We
address this gap by implementing Flavell's cognitive monitoring model (1979)
from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),
operationalising it as a three-phase iterative system. On GSM8K, preliminary
results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for
Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%
increased inference cost. These initial findings suggest upfront monitoring
produces higher-quality initial solutions that reduce refinement needs, though
evaluation beyond arithmetic reasoning is needed to establish generalisability.

</details>


### [29] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 提出受人类智能启发的HSCM因果框架，克服传统领域泛化模型局限，经评估优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 克服传统领域泛化模型的局限性。

Method: 提出HSCM，复制人类视觉系统的分层处理和多级学习，对图像关键属性解耦和重新加权。

Result: 通过理论和实证评估，HSCM优于现有领域泛化模型。

Conclusion: HSCM提供了更原则性的方法来捕捉因果关系并提高模型鲁棒性。

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [30] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: 为解决大语言模型对话系统中用户建模问题，提出自进化记忆框架RGMem，实现多尺度组织对话历史形成动态用户画像。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对话系统因有限上下文窗口和静态参数内存，难以建模跨会话长期用户状态和行为一致性，现有解决方案缺乏对多轮对话潜在偏好和深层特征的提取能力。

Method: 受物理学经典重整化群思想启发，提出自进化记忆框架RGMem，从情节片段中提取语义和用户洞察，通过分层粗粒化和重新缩放操作，逐步形成动态演变的用户画像。

Result: 将记忆演化建模为信息压缩和涌现的多尺度过程，能从嘈杂的微观交互中实现高级准确的用户画像。

Conclusion: RGMem框架可实现语言智能体的长期记忆和行为一致性，有助于提升个性化和跨会话连续性交互。

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [31] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: 本文介绍了ReviewSense框架，它利用大语言模型将客户评论转化为业务建议，初步评估显示其建议与业务目标一致，有推动决策的潜力。


<details>
  <summary>Details</summary>
Motivation: 客户反馈对战略增长愈发重要，现有工作较少将客户评论转化为面向业务的建议，需新方法解决。

Method: 提出ReviewSense框架，集成聚类、大语言模型适配和专家驱动评估到统一的面向业务的流程中。

Result: 初步手动评估表明模型的建议与业务目标高度一致。

Conclusion: 该框架为AI驱动的情感分析提供新视角，对完善业务策略和最大化客户反馈影响有价值。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [32] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 本文提出NP - ENGINE框架和NP - BENCH基准评估大语言模型解决NP难问题的能力，训练的QWEN2.5 - 7B - NP模型表现优异，发现任务丰富的RLVR训练可提升模型推理能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在解决复杂优化问题尤其是NP难问题的能力未充分探索，需填补这一空白。

Method: 提出NP - ENGINE框架，包含可控实例生成器、基于规则的验证器和启发式求解器；引入NP - BENCH基准；用零RLVR和课程学习训练QWEN2.5 - 7B - NP模型。

Result: QWEN2.5 - 7B - NP模型在NP - BENCH上显著优于GPT - 4o，实现同规模模型SOTA；RLVR训练能实现强泛化性，增加任务多样性可提升泛化性。

Conclusion: 任务丰富的RLVR训练是提升大语言模型推理能力的有前景方向，为RLVR缩放定律带来新见解。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [33] [Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination](https://arxiv.org/abs/2510.16533)
*Eilene Tomkins-Flanagan,Connor Hanley,Mary A. Kelly*

Main category: cs.AI

TL;DR: 提出一种编码于VSA的类型化计算机语言Doug，其类型可被神经网络学习，旨在描述类人技能学习行为，提升效率并接近模拟人类心理表征。


<details>
  <summary>Details</summary>
Motivation: 希望描述类人技能学习行为，超越现有方法效率，接近模拟人类心理表征。

Method: 将Doug编码为轻线性函数式编程语言，用基于HDM的槽值编码方案对类型编码，用Lisp VSA变体对项编码。

Result: Doug使神经网络可学习类型。

Conclusion: 该方法向模拟人类心理表征及其获取迈进了一步。

Abstract: We present a typed computer language, Doug, in which all typed programs may
be proved to halt in polynomial time, encoded in a vector-symbolic architecture
(VSA). Doug is just an encoding of the light linear functional programming
language (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are
encoded using a slot-value encoding scheme based on holographic declarative
memory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the
Lisp VSA defined by (Flanagan, 2024). Doug allows for some points on the
embedding space of a neural network to be interpreted as types, where the types
of nearby points are similar both in structure and content. Types in Doug are
therefore learnable by a neural network. Following (Chollet, 2019), (Card,
1983), and (Newell, 1981), we view skill as the application of a procedure, or
program of action, that causes a goal to be satisfied. Skill acquisition may
therefore be expressed as program synthesis. Using Doug, we hope to describe a
form of learning of skilled behaviour that follows a human-like pace of skill
acquisition (i.e., substantially faster than brute force; Heathcote, 2000),
exceeding the efficiency of all currently existing approaches (Kaplan, 2020;
Jones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling
human mental representations, as they must actually exist in the brain, and
those representations' acquisition, as they are actually learned.

</details>


### [34] [Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/abs/2510.16555)
*Qiongyan Wang,Xingchen Zou,Yutian Jiang,Haomin Wen,Jiaheng Wei,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: 现有城市基础模型有地理空间偏差问题，本文提出Urban - R1框架，实验表明其能缓解地理偏差、提升跨区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 快速城市化增加城市通用智能需求，但现有基于监督微调的城市基础模型存在地理空间偏差，预测有区域倾斜且泛化能力有限。

Method: 提出基于强化学习的后训练框架Urban - R1，采用GRPO优化地理组推理，用城市区域剖析作为代理任务从多模态城市数据获取可衡量奖励。

Result: 在不同地区和任务的大量实验中，Urban - R1有效缓解地理偏差，提升跨区域泛化能力，优于监督微调训练和闭源模型。

Conclusion: 强化学习对齐是实现公平可信城市智能的有前景途径。

Abstract: Rapid urbanization intensifies the demand for Urban General Intelligence
(UGI), referring to AI systems that can understand and reason about complex
urban environments. Recent studies have built urban foundation models using
supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit
persistent geospatial bias, producing regionally skewed predictions and limited
generalization. To this end, we propose Urban-R1, a reinforcement
learning-based post-training framework that aligns MLLMs with the objectives of
UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize
reasoning across geographic groups and employs urban region profiling as a
proxy task to provide measurable rewards from multimodal urban data. Extensive
experiments across diverse regions and tasks show that Urban-R1 effectively
mitigates geo-bias and improves cross-region generalization, outperforming both
SFT-trained and closed-source models. Our results highlight reinforcement
learning alignment as a promising pathway toward equitable and trustworthy
urban intelligence.

</details>


### [35] [BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction](https://arxiv.org/abs/2510.16559)
*Tian Xia,Tianrun Gao,Wenhao Deng,Long Wei,Xiaowei Qian,Yixian Jiang,Chenglei Yu,Tailin Wu*

Main category: cs.AI

TL;DR: 介绍首个用于语言驱动工程建设的物理对齐交互式基准BuildArena，并对8个前沿大模型进行语言驱动和基于物理的建设自动化能力评估。


<details>
  <summary>Details</summary>
Motivation: 工程建设自动化需复杂推理，现代大模型在此领域建设能力未被充分评估，为填补此空白开展研究。

Method: 引入BuildArena，包含高度可定制基准框架、可扩展任务设计策略、3D空间几何计算库和基线大模型代理工作流。

Result: 利用BuildArena对8个前沿大模型的语言驱动和基于物理的建设自动化能力进行全面评估。

Conclusion: BuildArena为语言驱动的工程建设自动化领域的大模型评估提供了有效工具。

Abstract: Engineering construction automation aims to transform natural language
specifications into physically viable structures, requiring complex integrated
reasoning under strict physical constraints. While modern LLMs possess broad
knowledge and strong reasoning capabilities that make them promising candidates
for this domain, their construction competencies remain largely unevaluated. To
address this gap, we introduce BuildArena, the first physics-aligned
interactive benchmark designed for language-driven engineering construction. It
contributes to the community in four aspects: (1) a highly customizable
benchmarking framework for in-depth comparison and analysis of LLMs; (2) an
extendable task design strategy spanning static and dynamic mechanics across
multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for
supporting construction based on language instructions; (4) a baseline LLM
agentic workflow that effectively evaluates diverse model capabilities. On
eight frontier LLMs, BuildArena comprehensively evaluates their capabilities
for language-driven and physics-grounded construction automation. The project
page is at https://build-arena.github.io/.

</details>


### [36] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra,Aman Sharma,Feroz Ahmad,Luca Muscariello,Vijoy Pandey,Ramesh Raskar*

Main category: cs.AI

TL;DR: 介绍涟漪效应协议（REP），一种协调协议，在多场景测试中比A2A提高了协调的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有A2A和ACP等协议重通信轻协调，群体增长时导致集体行为脆弱，个体智能但群体结果不佳。

Method: 引入REP协议，形式化其规范，分离消息模式和聚合规则，并在不同激励和网络拓扑场景中评估。

Result: 在三个领域的基准测试中，REP比A2A提高协调准确性和效率41% - 100%，能灵活处理大语言模型的多模态敏感性信号。

Conclusion: REP将协调作为协议级能力，为新兴的智能体互联网提供可扩展的基础设施。

Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents

</details>


### [37] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: 提出GraphFlow框架从富文本知识图谱中为真实查询高效检索准确多样知识，在STaRK基准测试中表现优于基线，泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的检索增强生成方法难以从富文本知识图谱为复杂真实查询检索准确多样信息，过程奖励模型依赖昂贵且难获取的过程监督信号。

Method: 采用基于转移的流匹配目标联合优化检索策略和流估计器，流估计器将检索结果奖励分解到中间检索状态以指导检索策略。

Result: 在STaRK基准测试中，GraphFlow在命中率和召回率上平均比强大的KG - RAG基线（包括GPT - 4o）高10%，对未见知识图谱有强泛化能力。

Conclusion: GraphFlow框架有效且稳健，能从富文本知识图谱为真实查询高效检索准确多样知识。

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [38] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 本文提出半监督置信分布学习方法ssCDL用于UKG补全，解决置信度分布不均衡问题，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有UKG补全研究忽略三元组置信度分布不均衡问题，导致学习的嵌入不足以实现高质量补全。

Method: 提出ssCDL方法，将三元组置信度转换为置信分布，通过对有标签数据和伪标签无标签数据进行关系学习迭代学习UKG嵌入，用元学习预测伪标签。

Result: 在两个UKG数据集上实验，ssCDL在不同评估指标上始终优于现有基线。

Conclusion: ssCDL方法能有效解决UKG补全中置信度分布不均衡问题，提升补全效果。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [39] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: 本文提出MERCI算法，结合基于计数的内在奖励增强大语言模型推理探索，实验表明其能提升推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习范式依赖稀疏奖励和有限探索，导致大语言模型推理模式重复、欠佳，需设计更好的探索方法。

Method: 提出MERCI算法，利用轻量级抛硬币网络（CFN）估计伪计数和认知不确定性，转化为内在奖励，并集成到GRPO等框架。

Result: 在复杂推理基准测试中，MERCI鼓励更丰富多样的思维链，显著优于强基线，助策略跳出局部常规找到更好方案。

Conclusion: 有针对性的内在动机可使语言模型推理的探索更可靠。

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [40] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出用于解决复杂车辆路径问题的AFL框架，实现全自动化，实验验证其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 复杂车辆路径问题需大量专家精力，现有大语言模型方法依赖外部干预，存在自主性受限、执行错误和解决方案可行性低等问题。

Method: 提出AFL框架，直接从原始输入提取知识，实现独立代码生成，将整体流程分解为三个子任务，使用四个专门的智能体确保一致性和逻辑性。

Result: 在60个复杂VRP问题上的实验表明，AFL框架性能与精心设计的算法相当，在代码可靠性和解决方案可行性上远超现有基于大语言模型的基线方法，评估基准上接近100%。

Conclusion: AFL框架有效且通用，能解决复杂车辆路径问题，提高代码可靠性和解决方案可行性。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [41] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: 本文探讨智能体AI从基于管道系统到模型原生范式的转变，以强化学习为引擎，系统回顾各能力演变及对应用的重塑，最后讨论未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 追踪智能体AI构建的范式转变，了解其发展趋势。

Method: 先以强化学习为算法引擎，系统回顾规划、工具使用和记忆等能力从外部脚本模块到端到端学习行为的演变，再研究范式转变对主要智能体应用的重塑。

Result: 明确了范式转变，分析了对主要智能体应用的影响，如深度研究智能体和GUI智能体。

Conclusion: 指出智能体能力持续内化，系统和模型层角色不断演变，朝着模型原生智能体AI的集成学习与交互框架发展。

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [42] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 本文对基于强化学习的智能体搜索进行全面综述，介绍其功能角色、优化策略和应用范围等，总结方法、评估协议和应用，探讨挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在静态知识、事实幻觉等局限，传统检索增强生成有不足，基于智能体的搜索有进步但需强化学习辅助，因此进行基于强化学习的智能体搜索综述。

Method: 从功能角色、优化策略和应用范围三个互补维度组织新兴领域，总结相关方法、评估协议和应用。

Result: 完成对基于强化学习的智能体搜索的全面综述，涵盖代表性方法、评估协议和应用等。

Conclusion: 希望该综述能启发强化学习与智能体搜索集成的未来研究。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [43] [Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration](https://arxiv.org/abs/2510.16742)
*Paul Saves,Pramudita Satria Palar,Muhammad Daffa Robani,Nicolas Verstaevel,Moncef Garouani,Julien Aligon,Benoit Gaudou,Koji Shimoyama,Joseph Morlier*

Main category: cs.AI

TL;DR: 本文提出结合代理模型与可解释AI的工作流，解决复杂系统模拟驱动工程工作流的高计算成本和透明度可靠性问题，并通过两个案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有模拟驱动工程工作流存在高计算成本和透明度、可靠性有限的问题，需要解决。

Method: 提出在紧凑实验设计上训练轻量级模拟器的工作流，还提出比较方法和实用建议，支持连续和分类输入，结合全局和局部分析等。

Result: 代理模型和XAI耦合能在数秒内进行大规模探索，揭示非线性交互和涌现行为，确定关键设计和政策杠杆等。

Conclusion: 所提工作流和方法有效，能解决现有工作流的问题，指导数据收集和模型改进。

Abstract: Complex systems are increasingly explored through simulation-driven
engineering workflows that combine physics-based and empirical models with
optimization and analytics. Despite their power, these workflows face two
central obstacles: (1) high computational cost, since accurate exploration
requires many expensive simulator runs; and (2) limited transparency and
reliability when decisions rely on opaque blackbox components. We propose a
workflow that addresses both challenges by training lightweight emulators on
compact designs of experiments that (i) provide fast, low-latency
approximations of expensive simulators, (ii) enable rigorous uncertainty
quantification, and (iii) are adapted for global and local Explainable
Artificial Intelligence (XAI) analyses. This workflow unifies every
simulation-based complex-system analysis tool, ranging from engineering design
to agent-based models for socio-environmental understanding. In this paper, we
proposea comparative methodology and practical recommendations for using
surrogate-based explainability tools within the proposed workflow. The
methodology supports continuous and categorical inputs, combines global-effect
and uncertainty analyses with local attribution, and evaluates the consistency
of explanations across surrogate models, thereby diagnosing surrogate adequacy
and guiding further data collection or model refinement. We demonstrate the
approach on two contrasting case studies: a multidisciplinary design analysis
of a hybrid-electric aircraft and an agent-based model of urban segregation.
Results show that the surrogate model and XAI coupling enables large-scale
exploration in seconds, uncovers nonlinear interactions and emergent behaviors,
identifies key design and policy levers, and signals regions where surrogates
require more data or alternative architectures.

</details>


### [44] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: 文章指出多模态知识图谱不完整问题，提出ELMM模型解决用MLLMs进行MKGC的挑战，实验表明其性能和效率俱佳。


<details>
  <summary>Details</summary>
Motivation: 现有多模态知识图谱不完整，大语言模型在多模态知识图谱补全应用待探索，且使用多模态大语言模型面临语义噪声、模态冲突和高计算成本问题。

Method: 提出基于多头注意力机制的多视图视觉令牌压缩器MVTC压缩图像令牌，设计注意力剪枝策略去除冗余层并引入线性投影补偿性能损失。

Result: 在FB15k - 237 - IMG和WN18 - IMG基准测试中，ELMM达到了最先进的性能，显著提高了计算效率。

Conclusion: ELMM为多模态知识图谱补全建立了新范式。

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [45] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: 提出全双工端到端模型ELLSA，实现多模态感知与生成，在多基准测试表现良好，迈向更自然通用交互智能。


<details>
  <summary>Details</summary>
Motivation: 人类交互是多模态和全双工的，实现这些能力对构建模拟人类的模型至关重要。

Method: 提出ELLSA模型，核心是SA - MoE架构，将各模态路由到专家模块并通过统一注意力骨干融合。

Result: 在语音交互和机器人操作基准测试中，匹配特定模态基线，支持多种高级多模态和全双工行为。

Conclusion: ELLSA是迈向更自然和通用交互智能的一步，有助于追求通用人工智能。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [46] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: 提出GraphVista框架解决视觉语言模型在图理解中的输入标记限制、可扩展性瓶颈和模态协调问题，实验表明其效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在图理解中受输入标记限制，存在可扩展性瓶颈且缺乏有效模态协调机制。

Method: 构建GraphRAG基础结构以提高可扩展性，引入规划代理进行模态协调。

Result: GraphVista可处理比现有基准大200倍的大图，比现有方法表现更好，质量提升达4.4倍。

Conclusion: GraphVista能有效解决现有问题，充分发挥两种模态的互补优势。

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [47] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 传统知识图谱受固定本体限制，本文提出域上下文概念图（CDC）框架，实现上下文感知推理等功能。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱受固定本体约束，将领域作为隐式上下文，需新框架解决。

Method: 提出采用C - D - C三元结构的CDC框架，基于认知语言同构映射原则，形式化二十多种关系谓词并在Prolog实现。

Result: 在教育、企业知识系统和技术文档的案例研究表明，CDC能实现上下文感知推理、跨领域类比和个性化知识建模。

Conclusion: CDC框架克服了传统基于本体框架的局限性，具有更好的知识建模能力。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [48] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 研究VLM智能体通过显式视觉状态推理构建内部世界模型，采用强化学习，提出策略和方法，小模型获佳绩。


<details>
  <summary>Details</summary>
Motivation: 解决VLM智能体从文本状态到复杂视觉观察转变带来的挑战，探究其能否通过显式视觉状态推理构建内部世界模型。

Method: 通过强化学习将智能体推理过程建模为POMDP，分解推理过程，设计世界建模奖励和Bi - Level GAE，在VAGEN框架实验。

Result: 3B参数模型在五个基准测试中得分0.82，相比未训练模型提升3倍，超GPT - 5等专有推理模型。

Conclusion: 分解推理过程对成功很关键，最优内部信念表示依赖任务，所提方法有效。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [49] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 提出新评估方法测试用户能否从解释中识别智能体目标，发现仅一种XRL算法准确率超随机，用户过度自信且自我报告与准确率无关。


<details>
  <summary>Details</summary>
Motivation: 可解释强化学习（XRL）算法在调试方面应用广泛，但缺乏对其性能的比较评估。

Method: 提出一种新的评估方法，利用Atari的吃豆人环境和四种XRL算法进行测试。

Result: 仅一种算法在测试目标上准确率超随机，用户普遍过度自信，用户自我报告的识别和理解难易程度与准确率不相关。

Conclusion: 该评估方法有助于进一步了解XRL算法在帮助用户识别智能体目标方面的性能。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [50] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 本文提出用于GPU内核优化的LLM智能体框架，在基准测试中表现优于基线模型，展示了其在自动化内核优化中的潜力。


<details>
  <summary>Details</summary>
Motivation: GPU内核优化困难且劳动密集，现有基于LLM的方法在不规则内核优化中效果有限。

Method: 引入LLM智能体框架，通过多智能体协作、基于规则的指令、动态上下文管理和策略搜索探索设计空间。

Result: 在KernelBench基准测试中，优于基线智能体，能产生正确解决方案，运行时性能提升达16倍。

Conclusion: 基于智能体的LLM框架有潜力推动全自动、可扩展的GPU内核优化。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [51] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: 提出ToolCritic框架评估和改进大语言模型在多轮工具增强对话中的行为，实验显示可提升工具调用准确率。


<details>
  <summary>Details</summary>
Motivation: 工具增强大语言模型在实际应用中存在工具使用错误，影响可靠性。

Method: 引入ToolCritic框架，检测八种工具调用错误类型并提供反馈，系统定义错误类别，构建合成数据集训练。

Result: 在SGD数据集实验中，ToolCritic比基线方法最高提升13%的工具调用准确率。

Conclusion: ToolCritic是实现大语言模型与外部工具在实际对话应用中更稳健集成的有前景一步。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [52] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: 提出多智能体AI系统BRAINCELL - AID用于基因集注释，结合自由文本描述与本体标签，减少幻觉并增强可解释性，对小鼠脑图谱细胞簇注释有良好效果，为社区驱动的细胞类型注释提供资源。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序后基因集注释尤其是涉及特征不明确基因的注释是挑战，传统方法依赖完善注释且表现不佳，大语言模型难以处理结构化本体中的生物知识。

Method: 提出BRAINCELL - AID多智能体AI系统，整合自由文本描述与本体标签，结合检索增强生成（RAG）优化预测。

Result: 对小鼠基因集的前预测中77%注释正确，注释了5322个小鼠脑细胞簇，识别出基底神经节相关细胞类型。

Conclusion: BRAINCELL - AID是有价值的资源，可支持社区驱动的细胞类型注释。

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [53] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: 研究开发两种基于大语言模型的系统用于从非财务证据生成结构化推理，应用于真实公司案例，与人工专家报告相比实现显著生产率提升，多智能体系统推理质量更优，表明结构化多智能体交互可提升金融AI推理能力。


<details>
  <summary>Details</summary>
Motivation: 金融AI在企业信用评估中基于证据的推理自动化问题未解决，现有方法难以支持专业贷款评估中的解释性判断。

Method: 开发非对抗性单智能体系统（NAS）和基于辩论的多智能体系统（KPD - MADS），将两系统应用于三个真实公司案例并由经验丰富的信用风险专业人员评估。

Result: 与人工专家报告相比，两系统均实现显著生产率提升，KPD - MADS推理质量更优，在多项指标上评分更高。

Conclusion: 结构化多智能体交互可提升金融AI推理的严谨性和可解释性，推动企业信用评估的可扩展和可靠自动化。

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [54] [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](https://arxiv.org/abs/2510.17145)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.AI

TL;DR: 本文提出基于手工特征的方法评估鱼的新鲜度，在FFE数据集上实验表明该方法有效，为食品质量监测提供见解。


<details>
  <summary>Details</summary>
Motivation: 传统感官评估鱼新鲜度存在主观性、不一致且难标准化等局限，需新方法。

Method: 从鱼眼图像中系统提取并逐步融合互补描述符，包括颜色统计、多颜色空间直方图、纹理特征等，分别融合全局和局部特征评估新鲜度。

Result: 在标准训练测试设置中，LightGBM分类器准确率达77.56%，比之前深度学习基线提高14.35%；数据增强后，ANN准确率达97.16%，超之前最佳结果19.86%。

Conclusion: 精心设计和处理的手工特征能为自动评估鱼新鲜度提供稳健、可解释和可靠的解决方案。

Abstract: Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

</details>


### [55] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: 本文介绍了用于比较代理协议的基准ProtocolBench和可学习的协议路由器ProtocolRouter，还发布了ProtocolRouterBench以标准化协议评估并提升大规模系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模多智能体系统发展，通信协议层关键但缺乏评估，协议选择缺乏标准化指导。

Method: 引入ProtocolBench从四个可测量维度比较代理协议，提出ProtocolRouter根据需求和运行时信号选择协议。

Result: 在ProtocolBench上协议选择显著影响系统行为；ProtocolRouter相比最佳单协议基线最多减少18.1%的故障风暴恢复时间，在GAIA中成功率更高。

Conclusion: 发布ProtocolRouterBench可标准化协议评估，提升大规模系统可靠性。

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [56] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 研究提出ECGFounder+XGBoost混合框架预测急性心肌梗死后恶性室性心律失常，性能优且可解释。


<details>
  <summary>Details</summary>
Motivation: 急性心肌梗死后恶性室性心律失常是院内死亡主因，早期识别难，传统风险评分性能有限，端到端深度学习模型缺乏可解释性。

Method: 分析6634份心电图记录，用ECGFounder提取特征，经特征选择后训练XGBoost分类器，用AUC和F1分数评估性能，SHAP方法分析可解释性。

Result: 混合模型AUC达0.801，优于KNN、RNN和1D - CNN，SHAP分析显示关键特征与临床知识高度一致。

Conclusion: 混合框架为心律失常风险预测提供新范式，验证了基础模型输出用于构建可信、可解释的临床决策支持系统的有效性。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [57] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究网络部署的工具增强大语言模型健康教练，早期研究显示评估优先的个性化路径。


<details>
  <summary>Details</summary>
Motivation: 研究网络部署的工具增强大语言模型健康教练在真实用户中的表现及探索个性化路径。

Method: 进行七用户试点，用离线策略评估（OPE）分析决策头，用轻量级模拟器模拟。

Result: 统一重工具策略提高平均价值但损害特定子组；添加小的早期信息增益奖励可缩短特征识别时间、提高目标成功率和 pass@3。

Conclusion: 应采用评估优先的个性化路径，冻结生成器，学习子组感知决策头并报告各原型指标。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [58] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出TD - HNODE模型用于疾病进展建模，在两个临床数据集上表现优于多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有疾病进展建模方法存在缺乏对真实数据适应性或无法捕捉复杂连续时间动态的问题，准确建模疾病进展可提升患者亚表型分析和干预效果。

Method: 提出TD - HNODE，将疾病进展表示为时间详细的超图，通过神经ODE框架学习连续时间进展动态，模型包含可学习的TD - 超图拉普拉斯矩阵。

Result: 在两个真实世界临床数据集上，TD - HNODE在2型糖尿病及相关心血管疾病进展建模中优于多个基线模型。

Conclusion: TD - HNODE能有效解决现有疾病进展建模方法的局限性，更好地进行疾病进展建模。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [59] [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](https://arxiv.org/abs/2510.17235)
*Chong Chen,Ze Liu,Lingfeng Bao,Yanlin Wang,Ting Chen,Daoyuan Wu,Jiachi Chen*

Main category: cs.AI

TL;DR: 文章指出加密货币市场投资需数据整合分析，现有方法有局限，提出基于强化学习的聊天机器人Coinvisor，评估显示其性能和用户满意度高。


<details>
  <summary>Details</summary>
Motivation: 解决加密货币投资中现有数据整合分析方法存在的依赖个人经验、功能有限、缺乏实时数据整合和多步推理能力等问题。

Method: 提出基于多智能体框架、强化学习的聊天机器人Coinvisor，通过专业工具整合分析能力，利用强化学习工具选择机制。

Result: Coinvisor在工具编排上召回率提高40.7%，F1分数提高26.6%，用户满意度4.64/5，用户更偏好它。

Conclusion: Coinvisor能提供准确、可操作的投资见解，在加密货币投资分析方面表现良好。

Abstract: The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

</details>


### [60] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: 本文介绍了AI支持的论文评估框架RubiSCoT，可从提案到最终提交阶段增强论文评估，探讨其优化学术评估流程的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统论文评估方法耗时且受评估者差异影响，需要更有效的评估方式。

Method: 运用先进自然语言处理技术，如大语言模型、检索增强生成和结构化思维链提示，框架包含初步评估、多维评估等环节。

Result: 文中未提及具体结果。

Conclusion: RubiSCoT有潜力通过一致、可扩展和透明的评估优化学术评估流程。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [61] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: 提出混合框架LaGAT解决稠密多智能体路径规划问题，优于纯搜索和纯学习方法。


<details>
  <summary>Details</summary>
Motivation: 实时为稠密多智能体路径规划问题找到近似最优解具有挑战性，以往学习引导搜索方法表现不佳。

Method: 将从MAGAT得到的学习启发式集成到LaCAM算法中，采用增强的MAGAT架构、先预训练再微调策略和死锁检测方案。

Result: 在稠密场景中，LaGAT优于纯搜索和纯学习方法。

Conclusion: 精心设计的混合搜索能为复杂的多智能体协调问题提供有效解决方案。

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [62] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: 提出FBI_LTL多样化规划器用于基于模拟的规划问题，确保生成语义多样的计划，评估显示其比基线方法更优。


<details>
  <summary>Details</summary>
Motivation: 单一规划器生成的计划可能不满足代理偏好，现有多样化规划方法存在生成语义相同解的局限性。

Method: 引入FBI_LTL规划器，利用线性时态逻辑（LTL）定义语义多样性标准，并将基于LTL的多样性模型集成到搜索过程中。

Result: 在各种基准测试中，FBI_LTL比基线方法生成更多样化的计划。

Conclusion: 证明了在基于模拟的环境中进行语义引导的多样化规划是可行的，为传统基于模型方法失效的现实非符号领域开辟了新途径。

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [63] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: 开发用于智能体自主控制的主动推理路径规划方法，利用证据地图和变分自由能引导智能体运动，平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 对地理区域进行侦察以维持通用作战态势图。

Method: 构建证据地图，采用Dempster - Shafer理论和高斯传感器模型的生成模型，用贝叶斯方法更新后验概率分布，计算变分自由能引导智能体运动。

Result: 在模拟中实现了智能体运动引导。

Conclusion: 该方法解决了探索与利用的挑战，能让智能体在地理地图上平衡搜索和跟踪目标对象。

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [64] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 论文指出法律机器学习应用需考虑标签不确定性，以欧洲人权法院案例分类为例，展示标签构建方式影响模型行为。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在法律领域应用常将过去案例结果作为真实值，但法律结果受人为干预，存在标签不确定性，需在法律机器学习应用中考虑该问题。

Method: 以欧洲人权法院的案例分类为背景进行研究。

Result: 发现训练过程中标签的构建方式会显著影响模型行为。

Conclusion: 强调标签不确定性是人工智能与法律领域的重要问题，且会影响模型行为。

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [65] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: 提出MIRAGE框架用于多模态错误信息检测，在MMFakeBench数据集上表现良好，证明无需特定领域训练也能有不错效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态错误信息传播快，人工核查难，有监督检测模型难泛化。

Method: 提出MIRAGE框架，将多模态验证分解为四个模块，协调视觉语言模型推理与网络检索。

Result: 在MMFakeBench验证集和测试集上取得高F1和准确率，消融实验显示各模块贡献。

Conclusion: 无需特定领域训练，基于网络检索的分解式推理可达到有监督检测器性能，可用于标注数据稀缺的多模态错误信息检测。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [66] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 本文提出将大语言模型推理能力蒸馏到小模型，经结构感知损失优化训练，小模型在多基准测试中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 代码生成需理解提示意图和进行算法推理，大语言模型有推理能力但部署成本高，小模型可能缺乏推理能力，因此要将大语言模型推理能力蒸馏到小模型。

Method: 通过结构感知损失优化的新方法，训练小模型学习大语言模型的推理和解决问题能力，识别正确解决方案路径，建立问题定义和潜在解决方案的结构对应。

Result: 经低成本且易实现的过程微调的模型，在MBPP、MBPP Plus和HumanEval基准测试的pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 提出的将大语言模型推理能力蒸馏到小模型的方法有效，训练后的小模型表现更好。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [67] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文研究大语言模型（LLMs）的预测智能，构建Prophet Arena评估基准，发现LLMs有一定预测能力但也存在关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，探讨利用其预测现实世界未来事件的可能性。

Method: 构建Prophet Arena评估基准，持续收集实时预测任务并分解为不同阶段进行大规模实验。

Result: 许多LLMs已展现出令人印象深刻的预测能力，如校准误差小、预测置信度一致和有前景的市场回报。

Conclusion: 利用LLM进行预测存在关键瓶颈，如事件回忆不准确、对数据源理解有误以及临近结果时信息聚合慢于市场。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [68] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 本文用多智能体影响图（MAIDs）解决大规模合作式多智能体强化学习（MARL）中全局引导难和缺乏易用研究工具的问题，提出目标干预范式并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模合作式MARL中人类对整个多智能体系统进行全局引导不实际，且设计协调机制缺乏易用研究工具。

Method: 采用MAIDs作为图形框架，引入交互范式分析和可视化现有MARL方法，设计基于MAIDs的目标干预范式，用因果推理技术Pre - Strategy Intervention（PSI）实现该范式。

Result: 实验证明提出的目标干预有效，验证了相关性图分析结果。

Conclusion: MAIDs可解决大规模合作式MARL的问题，目标干预范式和PSI技术能实现综合期望结果，相关性图分析可判断MARL学习范式的可行性。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [69] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出HyCAM框架解决大语言模型多任务适应问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多任务适应方面存在困难，传统微调方法有灾难性遗忘和资源消耗问题，现有参数高效方法在复杂多任务场景表现不佳。

Method: 提出Contextual Attention Modulation（CAM）机制，集成到Hybrid Contextual Attention Modulation（HyCAM）框架，结合共享全参数CAM模块与多个轻量级CAM模块，采用动态路由策略进行自适应知识融合。

Result: 在多种任务上实验表明，该方法显著优于现有方法，平均性能提升3.65%。

Conclusion: HyCAM框架能有效解决大语言模型多任务适应问题，提升模型性能。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [70] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 研究视觉语言模型（VLMs）在多模态任务中的失败原因，发现“看到却不信”现象，提出推理时干预方法提升准确率。


<details>
  <summary>Details</summary>
Motivation: 探究VLMs在多模态任务中即使有正确视觉证据仍失败的原因。

Method: 通过检查层注意力动态，引入基于选择性注意力掩码突出深层证据区域的推理时干预方法。

Result: 发现“看到却不信”现象广泛存在，干预方法在多个模型家族中持续提高准确率。

Conclusion: VLMs内部编码可靠证据但利用不足，使信号明确可弥合感知与推理差距，提升诊断理解和可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>


### [71] [Advancing Routing-Awareness in Analog ICs Floorplanning](https://arxiv.org/abs/2510.15387)
*Davide Basso,Luca Bortolussi,Mirjana Videnovic-Misic,Husni Habal*

Main category: cs.AI

TL;DR: 本文开发基于强化学习和关系图卷积神经网络的自动布局引擎，在模拟环境中相比以往技术减少死区、线长，提高布线成功率。


<details>
  <summary>Details</summary>
Motivation: 解决模拟集成电路布局中，布局工程师对可用的考虑布线的布局解决方案的需求，突破机器学习技术在模拟集成电路布局应用的限制。

Method: 开发基于强化学习和关系图卷积神经网络的自动布局引擎，结合提高网格分辨率、精确引脚信息集成和动态布线资源估计技术。

Result: 在模拟环境中，相比以往基于学习的先进技术，死区减少13.8%，线长减少40.6%，布线成功率提高73.4%。

Conclusion: 所提方法能平衡布线和面积效率，达到工业标准。

Abstract: The adoption of machine learning-based techniques for analog integrated
circuit layout, unlike its digital counterpart, has been limited by the
stringent requirements imposed by electric and problem-specific constraints,
along with the interdependence of floorplanning and routing steps. In this
work, we address a prevalent concern among layout engineers regarding the need
for readily available routing-aware floorplanning solutions. To this extent, we
develop an automatic floorplanning engine based on reinforcement learning and
relational graph convolutional neural network specifically tailored to
condition the floorplan generation towards more routable outcomes. A
combination of increased grid resolution and precise pin information
integration, along with a dynamic routing resource estimation technique, allows
balancing routing and area efficiency, eventually meeting industrial standards.
When analyzing the place and route effectiveness in a simulated environment,
the proposed approach achieves a 13.8% reduction in dead space, a 40.6%
reduction in wirelength and a 73.4% increase in routing success when compared
to past learning-based state-of-the-art techniques.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [72] [Population-Based Search Method Using Uncertainty-related Pareto Front for Robust Multi-objective Optimization](https://arxiv.org/abs/2510.16386)
*Lihong Xu,Wenxiang Jiang*

Main category: cs.CE

TL;DR: 传统鲁棒多目标优化方法有局限，本文提出UPF框架及RMOEA - UPF算法，实验证明其能有效解决复杂不确定多目标优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法重收敛轻鲁棒，且评估单收敛最优解鲁棒性效率低，需改进。

Method: 提出UPF框架平衡鲁棒性和收敛性，基于此提出RMOEA - UPF算法，在进化过程中计算和优化UPF。

Result: 在九个基准问题和实际应用中实验，RMOEA - UPF持续给出高质量结果。

Conclusion: RMOEA - UPF是解决复杂不确定多目标优化问题更通用、可靠的方法。

Abstract: Traditional robust multi-objective optimization methods typically prioritize
convergence while treating robustness as a secondary consideration. This
approach can yield solutions that are not genuinely robust optimal under
noise-affected scenarios. Furthermore, compared to population-based search
methods, determining the robust optimal solution by evaluating the robustness
of a single convergence-optimal solution is also inefficient. To address these
two limitations,we propose a novel Uncertainty-related Pareto Front (UPF)
framework that balances robustness and convergence as equal priorities. Unlike
traditional Pareto Front, the UPF explicitly accounts for decision variable
with noise perturbation by quantifying their effects on both convergence
guarantees and robustness preservation equally within a theoretically grounded
and general framework. Building upon UPF, we propose RMOEA-UPF--a
population-based search robust multi-objective optimization algorithm. This
method enables efficient search optimization by calculating and optimizing the
UPF during the evolutionary process.Experiments on nine benchmark problems and
a real-world application demonstrate that RMOEA-UPF consistently delivers
high-quality results. Our method's consistent top-ranking performance indicates
a more general and reliable approach for solving complex, uncertain
multi-objective optimization problems. Code is available at:
https://github.com/WenxiangJiang-me/RMOEA-UPF.

</details>


### [73] [ViT-Transformer: Self-attention mechanism based constitutive modeling for nonlinear heterogeneous materials](https://arxiv.org/abs/2510.16575)
*Yijing Zhou,Shabnam J. Semnani*

Main category: cs.CE

TL;DR: 文章引入ViT - Transformer替代模型解决非线性异质材料和复合材料多尺度模拟问题，提出随机提取训练算法，验证了模型和算法有效性。


<details>
  <summary>Details</summary>
Motivation: 传统多尺度方法计算成本高，现有ML替代本构模型难捕捉长程依赖和跨微观结构泛化，而基于注意力的Transformer架构带来新可能。

Method: 引入使用自注意力机制的ViT - Transformer替代模型，ViT编码器提取微观结构特征，解码器结合几何特征和宏观应变输入序列预测应力响应，提出随机提取训练算法，设计构建数据集。

Result: 通过多种复合材料图像和加载场景验证了替代模型，多个数值算例展示了ViT - Transformer模型和训练算法的有效性和准确性。

Conclusion: ViT - Transformer模型和随机提取训练算法能有效解决非线性异质材料和复合材料多尺度模拟问题。

Abstract: Multi-scale simulations of nonlinear heterogeneous materials and composites
are challenging due to the prohibitive computational costs of high-fidelity
simulations. Recently, machine learning (ML) based approaches have emerged as
promising alternatives to traditional multiscale methods. However, existing ML
surrogate constitutive models struggle in capturing long-range dependencies and
generalization across microstructures. The recent advancements in
attention-based Transformer architectures open the door to a more powerful
class of surrogate models. Attention mechanism has demonstrated remarkable
capabilities in natural language processing and computer vision. In this work,
we introduce a surrogate (meta) model, namely ViT-Transformer, using a Vision
Transformer (ViT) encoder and a Transformer-based decoder which are both driven
by the self-attention mechanism. The ViT encoder extracts microstructural
features from material images, while the decoder is a masked Transformer
encoder that combines the latent geometrical features with the macroscopic
strain input sequence to predict the corresponding stress response. To enhance
training, we propose a random extract training algorithm that improves
robustness to sequences of variable length. We design and construct a compact
yet diverse dataset via data augmentation, and validate the surrogate model
using various composite material images and loading scenarios. Several
numerical examples are provided to show the effectiveness and accuracy of the
ViT-Transformer model and the training algorithm.

</details>


### [74] [Chem-R: Learning to Reason as a Chemist](https://arxiv.org/abs/2510.16880)
*Weida Wang,Benteng Chen,Di Zhang,Wanhao Liu,Shuchen Pu,Ben Gao,Jin Zeng,Lei Bai,Wanli Ouyang,Xiaoyong Wei,Tianshu Yu,Tianfan Fu,Shuzhou Sun,Jiatong Li,Zifu Wang,Yuqiang Li,Shufei Zhang*

Main category: cs.CE

TL;DR: 提出可泛化化学推理模型Chem - R，经三阶段框架训练，在化学基准测试中表现超领先大模型和现有化学基础模型，有良好泛化性等。


<details>
  <summary>Details</summary>
Motivation: 解决当前大语言模型缺乏核心化学知识、推理轨迹不可靠和在不同化学任务中表现不佳的问题。

Method: 采用三阶段框架训练Chem - R，包括化学基础训练、化学推理协议蒸馏和多任务组相对策略优化。

Result: Chem - R在综合基准测试中达到了最先进的性能，在分子任务上比Gemini - 2.5 - Pro和DeepSeek - R1高46%，在反应任务上高66%，且在分子和反应任务上均优于现有化学基础模型。

Conclusion: Chem - R具有强大的泛化性、可解释性，有潜力成为下一代人工智能驱动化学发现的基础。

Abstract: Although large language models (LLMs) have significant potential to advance
chemical discovery, current LLMs lack core chemical knowledge, produce
unreliable reasoning trajectories, and exhibit suboptimal performance across
diverse chemical tasks. To address these challenges, we propose Chem-R, a
generalizable Chemical Reasoning model designed to emulate the deliberative
processes of chemists. Chem-R is trained through a three-phase framework that
progressively builds advanced reasoning capabilities, including: 1) Chemical
Foundation Training, which establishes core chemical knowledge. 2) Chemical
Reasoning Protocol Distillation, incorporating structured, expert-like
reasoning traces to guide systematic and reliable problem solving. 3)
Multi-task Group Relative Policy Optimization that optimizes the model for
balanced performance across diverse molecular- and reaction-level tasks. This
structured pipeline enables Chem-R to achieve state-of-the-art performance on
comprehensive benchmarks, surpassing leading large language models, including
Gemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% on
reaction tasks. Meanwhile, Chem-R also consistently outperforms the existing
chemical foundation models across both molecular and reaction level tasks.
These results highlight Chem-R's robust generalization, interpretability, and
potential as a foundation for next-generation AI-driven chemical discovery.

</details>


### [75] [Addressing data scarcity in structural health monitoring through generative augmentation](https://arxiv.org/abs/2510.16889)
*Sasan Farhadi,Mariateresa Iavarone,Mauro Corrado,Eleni Chatzi,Giulio Ventura*

Main category: cs.CE

TL;DR: 本文提出STFTSynth生成对抗网络模型用于生成声学事件信号的短时傅里叶变换频谱图，以解决桥梁监测数据稀缺等问题，该模型表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的桥梁结构健康监测模型受数据稀缺、环境噪声和类别不平衡等因素限制，性能不佳。

Method: 引入定制的生成对抗网络模型STFTSynth，集成密集残差块和双向门控循环单元，用定性检查和定量指标评估模型性能。

Result: STFTSynth优于基线模型，能生成高分辨率、时间一致且与真实数据接近的频谱图。

Conclusion: 基于生成式的数据增强方法是解决桥梁监测中稀有事件数据稀缺问题的可扩展且经济高效的解决方案。

Abstract: Structural Health Monitoring plays a crucial role in ensuring the safety,
reliability, and longevity of bridge infrastructures through early damage
detection. Although recent advances in deep learning-based models have enabled
automated event detection, their performance is often limited by data scarcity,
environmental noise, and class imbalance. To address these challenges, this
study introduces a customized Generative Adversarial Network model, STFTSynth,
designed particularly for generating short-time Fourier transform spectrograms
derived from acoustic event signals. In contrast to augmentation techniques
such as MixUp, generative adversarial networks can synthesize high-quality
spectrograms that mimic real-world events, enhancing dataset diversity and
robustness. The proposed model integrates dense residual blocks for spatial
consistency with bidirectional gated recurrent units for temporal dependency
modeling. Model performance is evaluated against three baseline generative
models using qualitative inspection and quantitative metrics, including
Structural Similarity Index Measure, Peak Signal-to-Noise Ratio, and Fr\'echet
Inception Distance. Results show that STFTSynth outperforms baseline models,
producing high-resolution, temporally consistent spectrograms that align
closely with real-world data. These findings indicate the potential of
generative-based data augmentation as a scalable and cost-effective solution
for bridge monitoring scenarios where rare events, such as prestressing wire
breakage, suffer from data scarcity.

</details>


### [76] [Trading with the Devil: Risk and Return in Foundation Model Strategies](https://arxiv.org/abs/2510.17165)
*Jinrui Zhang*

Main category: cs.CE

TL;DR: 本文针对金融时间序列任务中基础模型构建的交易策略，提出扩展CAPM模型分离风险，通过不确定性分解估算风险，实验显示分离风险因素有助于深入了解策略性能。


<details>
  <summary>Details</summary>
Motivation: 现有金融时间序列基础模型虽有预测能力，但未知其如何影响交易策略风险，使从业者不敢投入大量资金。

Method: 提出扩展的CAPM模型分离系统风险和异质风险，将风险分解与不确定性分解概念对齐，利用蒙特卡罗丢弃法测量认知风险。

Result: 分离不同风险因素能深入了解基于基础模型策略的性能极限、模型随时间的退化情况及潜在改进途径。

Conclusion: 强调了在竞争激烈的金融市场中部署大型预训练模型的机遇与挑战。

Abstract: Foundation models - already transformative in domains such as natural
language processing - are now starting to emerge for time-series tasks in
finance. While these pretrained architectures promise versatile predictive
signals, little is known about how they shape the risk profiles of the trading
strategies built atop them, leaving practitioners reluctant to commit serious
capital. In this paper, we propose an extension to the Capital Asset Pricing
Model (CAPM) that disentangles the systematic risk introduced by a shared
foundation model - potentially capable of generating alpha if the underlying
model is genuinely predictive - from the idiosyncratic risk attributable to
custom fine-tuning, which typically accrues no systematic premium. To enable a
practical estimation of these separate risks, we align this decomposition with
the concepts of uncertainty disentanglement, casting systematic risk as
epistemic uncertainty (rooted in the pretrained model) and idiosyncratic risk
as aleatory uncertainty (introduced during custom adaptations). Under the
Aleatory Collapse Assumption, we illustrate how Monte Carlo dropout - among
other methods in the uncertainty-quantization toolkit - can directly measure
the epistemic risk, thereby mapping trading strategies to a more transparent
risk-return plane. Our experiments show that isolating these distinct risk
factors yields deeper insights into the performance limits of
foundation-model-based strategies, their model degradation over time, and
potential avenues for targeted refinements. Taken together, our results
highlight both the promise and the pitfalls of deploying large pretrained
models in competitive financial markets.

</details>


### [77] [StrengthLawExtractor: A Fiji plugin for 3D morphological feature extraction from X-ray micro-CT data](https://arxiv.org/abs/2510.17279)
*Qinyi Tian,Laura E. Dalton*

Main category: cs.CE

TL;DR: 开发Fiji插件从micro - CT数据集自动提取四个形态测量指标，支持无损评估等多孔介质研究。


<details>
  <summary>Details</summary>
Motivation: 无损方法对研究多孔材料微观结构与力学行为关系很重要，现有理论需四个指标，为方便应用该理论。

Method: 开发Fiji插件从micro - CT数据集自动提取四个形态测量指标（孔隙率、表面积、平均曲率、欧拉特征）。

Result: 插件集成到Fiji平台，提供可重复、易访问和用户友好的分析，提取的描述符可用于本构模型和机器学习工作流程。

Conclusion: 该方法支持无损评估，加速材料选择，推动多孔介质研究中成像与预测建模的集成。

Abstract: Non-destructive methods are essential for linking the microstructural
geometry of porous materials to their mechanical behavior, as destructive
testing is often infeasible due to limited material availability or
irreproducible conditions. Micro-computed tomography (micro-CT) provides high
resolution three dimensional reconstructions of porous microstructures,
enabling direct quantification of geometric descriptors. Recent advances in
morphometric theory have demonstrated that four independent morphometric
measures (porosity, surface area, mean curvature, and Euler characteristic) are
required to capture the relationship between microstructure and strength,
thereby forming the basis of generalized strength laws. To facilitate practical
application of this framework, a Fiji plugin was developed to extract the four
morphometric measures (porosity, surface area, mean curvature, Euler
characteristic) from micro-CT datasets automatically. The plugin integrates
within the Fiji platform to provide reproducible, accessible, and user friendly
analysis. The application of the tool demonstrates that the extracted
descriptors can be readily incorporated into constitutive models and machine
learning workflows, enabling the forward prediction of stress-strain behavior
as well as the inverse design of microstructures. This approach supports
non-destructive evaluation, accelerates materials selection, and advances the
integration of imaging with predictive modeling in porous media research.

</details>


### [78] [Modelling complexity in system safety: generalizing the D2T2 methodology](https://arxiv.org/abs/2510.17351)
*Silvia Tolo,John Andrews*

Main category: cs.CE

TL;DR: 传统故障树和事件树分析有局限，本文提出动态和依赖树理论的推广方法，可整合依赖建模。


<details>
  <summary>Details</summary>
Motivation: 传统故障树和事件树分析无法完全捕捉复杂系统动态行为，现有替代工具不足，使用保守假设会带来未知保守性问题。

Method: 将传统故障和事件树分析的组合性质与更灵活的建模解决方案相结合，推广动态和依赖树理论。

Result: 提出了能在同一建模框架下考虑任何类型依赖的方法。

Conclusion: 该方法在保留传统安全建模熟悉度和有效性的同时，能考虑各种依赖。

Abstract: Although Fault Tree and Event Tree analysis are still today the standard
approach to system safety analysis for many engineering sectors, these
techniques lack the capabilities of fully capturing the realistic, dynamic
behaviour of complex systems, which results in a dense network of dependencies
at any level, i.e. between components, trains of components or subsystems.
While these limitations are well recognised across both industry and academia,
the shortage of alternative tools able to tackle such challenges while
retaining the computational feasibility of the analysis keeps fuelling the
long-lived success of Fault Tree and Event Tree modelling. Analysts and
regulators often rely on the use of conservative assumptions to mitigate the
effect of oversimplifications associated with the use of such techniques.
However, this results in the analysis output to be characterised by an unknown
level of conservatism, with potential consequences on market competitiveness
(i.e., over-conservatism) or safety (i.e., under-conservatism). This study
proposes a generalization of the Dynamic and Dependent Tree Theory, which
offers theoretical tools for the systematic integration of dependency modelling
within the traditional Fault and Event Tree analysis framework. This is
achieved by marrying the traditional combinatorial nature of failure analysis,
formalised by the Fault and Event Tree language, with more flexible modelling
solutions, which provide the flexibility required to capture complex system
features. The main advantage of the proposed approach in comparison to existent
solutions is the ability to take into account, under the same modelling
framework, any type of dependency regardless of its nature and location, while
retaining the familiarity and effectiveness of traditional safety modelling.

</details>


### [79] [Volumetric Non-Invasive Cardiac Mapping for Accessible Global Arrhythmia Characterization](https://arxiv.org/abs/2510.17539)
*Jorge Vicente-Puig,Judit Chamorro-Servent,Ernesto Zacur,Inés Llorente-Lipe,Marta Martínez,Jorge Sanchez,Jana Reventós,Ivo Roca-Luque,Lluis Mont,Felipe Atienza,Andreu M. Climent,Maria S. Guillem,Ismael Hernández-Romero*

Main category: cs.CE

TL;DR: 提出无图像容积心电成像(ECGI)方法重建3D心脏活动，评估其在模拟和患者案例中的效果，显示能改善心律失常定位。


<details>
  <summary>Details</summary>
Motivation: 传统ECGI局限于心外膜重建，可能遗漏源自更深层心肌的心律失常，需改进。

Method: 用容积公式通过格林函数解决逆源问题，重建三维心脏活动。

Result: 容积ECGI恢复3D激活，锐化心律失常起源定位，相比仅表面方法，测地误差降低59.3%，患者案例中激活模式与临床诊断一致。

Conclusion: 无图像容积ECGI克服表面受限技术的核心局限，可改善术前规划等。

Abstract: Cardiac arrhythmias are a major cause of morbidity and mortality increasing
the risk of stroke, heart failure, and sudden cardiac death. Imageless
electrocardiographic imaging (ECGI) provides a non invasive alternative to
electrical mapping from body surface potentials, but conventional ECGI is
confined to epicardial reconstructions and can miss arrhythmias originating in
deeper myocardium. We address this by reconstructing three dimensional cardiac
activity with a volumetric formulation that solves an inverse source problem
via Green's functions, enabling full volume activation mapping and improved
localization in anatomically complex regions. We evaluate the approach on
simulated premature ventricular beats and on four challenging patient cases, a
right ventricular outflow tract premature ventricular contraction, a left
bundle branch block, a ventricular tachycardia, and Wolff Parkinson White, and
additionally assess performance on an open source myocardial infarction
dataset. Results show that volumetric ECGI recovers 3D activation and sharpens
arrhythmia origin localization, achieving a 59.3% reduction in geodesic error
between estimated and simulated origins relative to surface only methods; in
patient cases, activation patterns align with clinical diagnoses. Overall,
imageless volumetric ECGI offers accessible, non invasive 3D activation mapping
that overcomes a core limitation of surface restricted techniques and may
improve preprocedural planning, ablation target guidance, and selection or
optimization of cardiac resynchronization therapy.

</details>


### [80] [Time Series Analysis in Frequency Domain: A Survey of Open Challenges, Opportunities and Benchmarks](https://arxiv.org/abs/2504.07099)
*Qianru Zhang,Yuting Sun,Honggang Wen,Peng Yang,Xinzhu Li,Ming Li,Kwok-Yan Lam,Siu-Ming Yiu,Hongzhi Yin*

Main category: cs.CE

TL;DR: 该综述全面考察频谱方法，总结三个研究挑战，建立统一分类和基准，指出知识差距，为从业者提供框架并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 频域分析有优势但也带来挑战，需对频谱方法进行全面考察和总结。

Method: 对超100项研究进行严格审查，建立统一分类法和标准化基准。

Result: 确定了该领域特别是几何深度学习和量子增强频谱分析的关键知识差距。

Conclusion: 为从业者提供方法选择和实施的系统框架，指明该快速发展领域的未来研究方向。

Abstract: Frequency-domain analysis has emerged as a powerful paradigm for time series
analysis, offering unique advantages over traditional time-domain approaches
while introducing new theoretical and practical challenges. This survey
provides a comprehensive examination of spectral methods from classical Fourier
analysis to modern neural operators, systematically summarizing three open
challenges in current research: (1) causal structure preservation during
spectral transformations, (2) uncertainty quantification in learned frequency
representations, and (3) topology-aware analysis for non-Euclidean data
structures. Through rigorous reviewing of over 100 studies, we develop a
unified taxonomy that bridges conventional spectral techniques with
cutting-edge machine learning approaches, while establishing standardized
benchmarks for performance evaluation. Our work identifies key knowledge gaps
in the field, particularly in geometric deep learning and quantum-enhanced
spectral analysis. The survey offers practitioners a systematic framework for
method selection and implementation, while charting promising directions for
future research in this rapidly evolving domain.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [81] [Unified Peripartum Database with Natural-Language-to-SQL Capabilities at Udine University Hospital: Design and Prototype](https://arxiv.org/abs/2510.16388)
*Doriana Armenise,Ginevra Battello,Andrea Brunello,Lorenza Driul,Angelo Montanari,Elisa Rizzante,Nicola Saccomanno,Andrea Salvador,Serena Xodo,Silvia Zermano*

Main category: cs.DB

TL;DR: 提出将异质围产期记录转化为可计算、可查询资产的实用蓝图，设计带NL2SQL功能的统一围产期关系数据库。


<details>
  <summary>Details</summary>
Motivation: 医院产科信息分散在不同系统，阻碍产时护理和可重复性研究。

Method: 与临床医生共同定义需求并形成实体关系图，据此推导数据库逻辑模式和SQL实现，集成异质数据源，添加NL2SQL层。

Result: 设计并搭建了带NL2SQL功能的统一围产期关系数据库。

Conclusion: 该数据库可整合异质数据源，NL2SQL层降低了审计和探索性分析的门槛。

Abstract: The fragmentation of obstetric information across electronic health record
modules, device repositories, and laboratory systems, as it is common in
hospitals, hinders both intrapartum care and reproducible research. In this
work, we present a practical blueprint for transforming heterogeneous
peripartum records into computable, queryable assets by designing and
prototyping a unified peripartum relational database with
natural-language-to-SQL (NL2SQL) capabilities at the Obstetrics Clinic of Udine
University Hospital. Requirements were co-defined with clinicians and
formalized as an Entity-Relationship diagram, from which the logical schema and
SQL implementation of the database were then derived. The latter integrates
heterogeneous sources to connect maternal anamnestic and longitudinal history,
current-pregnancy findings, intrapartum course, and delivery and neonatal
outcomes. The NL2SQL layer enables clinicians to pose natural-language queries
to the system, lowering barriers to audit and exploratory analysis.

</details>


### [82] [Declarative Techniques for NL Queries over Heterogeneous Data](https://arxiv.org/abs/2510.16470)
*Elham Khabiri,Jeffrey O. Kephart,Fenno F. Heath III,Srideepika Jayaraman,Fateh A. Tipu,Yingjie Li,Dhruv Shah,Achille Fokoue,Anu Bhamidipaty*

Main category: cs.DB

TL;DR: 本文模拟工业环境数据异质性，扩展Spider基准数据集，提出声明式方法处理数据异质性，效果优于现有系统，并公开增强后的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的应用在处理工业环境中数据源异质性问题时不实用，无法满足实际需求。

Method: 扩展Spider基准数据集模拟真实工业环境的异质性，引入声明式方法处理数据异质性。

Result: 声明式方法在处理数据源异质性方面明显优于最先进的基于大语言模型的代理或命令式代码生成系统。

Conclusion: 所提出的声明式方法能有效应对工业环境中的数据源异质性问题，增强后的基准数据集可供研究社区使用。

Abstract: In many industrial settings, users wish to ask questions in natural language,
the answers to which require assembling information from diverse structured
data sources. With the advent of Large Language Models (LLMs), applications can
now translate natural language questions into a set of API calls or database
calls, execute them, and combine the results into an appropriate natural
language response. However, these applications remain impractical in realistic
industrial settings because they do not cope with the data source heterogeneity
that typifies such environments. In this work, we simulate the heterogeneity of
real industry settings by introducing two extensions of the popular Spider
benchmark dataset that require a combination of database and API calls. Then,
we introduce a declarative approach to handling such data heterogeneity and
demonstrate that it copes with data source heterogeneity significantly better
than state-of-the-art LLM-based agentic or imperative code generation systems.
Our augmented benchmarks are available to the research community.

</details>


### [83] [AVOCADO: The Streaming Process Mining Challenge](https://arxiv.org/abs/2510.17089)
*Christian Imenkamp,Andrea Maldonado,Hendrik Reiter,Martin Werner,Wilhelm Hasselbring,Agnes Koschmider,Andrea Burattin*

Main category: cs.DB

TL;DR: 提出AVOCADO标准化挑战框架用于流式过程挖掘算法评估，促进领域发展并邀社区贡献。


<details>
  <summary>Details</summary>
Motivation: 为系统解决流式过程挖掘领域复杂性，需标准化挑战框架评估算法。

Method: 提出AVOCADO框架，分离概念和实例层挑战，用特定指标评估算法。

Result: 提出了AVOCADO框架并说明了评估指标。

Conclusion: 该框架可作为基础，邀请社区贡献以应对更多挑战，推动流式过程挖掘领域发展。

Abstract: Streaming process mining deals with the real-time analysis of streaming data.
Event streams require algorithms capable of processing data incrementally. To
systematically address the complexities of this domain, we propose AVOCADO, a
standardized challenge framework that provides clear structural divisions:
separating the concept and instantiation layers of challenges in streaming
process mining for algorithm evaluation. The AVOCADO evaluates algorithms on
streaming-specific metrics like accuracy, Mean Absolute Error (MAE), Root Mean
Square Error (RMSE), Processing Latency, and robustness. This initiative seeks
to foster innovation and community-driven discussions to advance the field of
streaming process mining. We present this framework as a foundation and invite
the community to contribute to its evolution by suggesting new challenges, such
as integrating metrics for system throughput and memory consumption, and
expanding the scope to address real-world stream complexities like out-of-order
event arrival.

</details>


### [84] [Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models](https://arxiv.org/abs/2510.17301)
*Panos Kalnis. Shuo Shang,Christian S. Jensen*

Main category: cs.DB

TL;DR: 提出MapMuse框架将时空数据集转化为叙事体验，以出租车轨迹数据集为例展示其效果，探讨电影叙事技术潜力及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统时空数据可视化复杂，难让大众理解，需新方法解决。

Method: 利用大语言模型，采用检索增强生成（RAG）和基于代理技术生成故事，借鉴电影叙事原则设计。

Result: 以出租车轨迹数据集为例，从两个视角呈现数据，展示了MapMuse能缩小数据复杂性和人类理解间的差距。

Conclusion: 数据叙事可从时空信息中驱动洞察、参与和行动，电影叙事技术有作为时空数据交流工具的潜力，指出未来研究的问题和机会。

Abstract: Spatio-temporal data captures complex dynamics across both space and time,
yet traditional visualizations are complex, require domain expertise and often
fail to resonate with broader audiences. Here, we propose MapMuse, a
storytelling-based framework for interpreting spatio-temporal datasets,
transforming them into compelling, narrative-driven experiences. We utilize
large language models and employ retrieval augmented generation (RAG) and
agent-based techniques to generate comprehensive stories. Drawing on principles
common in cinematic storytelling, we emphasize clarity, emotional connection,
and audience-centric design. As a case study, we analyze a dataset of taxi
trajectories. Two perspectives are presented: a captivating story based on a
heat map that visualizes millions of taxi trip endpoints to uncover urban
mobility patterns; and a detailed narrative following a single long taxi
journey, enriched with city landmarks and temporal shifts. By portraying
locations as characters and movement as plot, we argue that data storytelling
drives insight, engagement, and action from spatio-temporal information. The
case study illustrates how MapMuse can bridge the gap between data complexity
and human understanding. The aim of this short paper is to provide a glimpse to
the potential of the cinematic storytelling technique as an effective
communication tool for spatio-temporal data, as well as to describe open
problems and opportunities for future research.

</details>


### [85] [Approximate Nearest Neighbor Search of Large Scale Vectors on Distributed Storage](https://arxiv.org/abs/2510.17326)
*Kun Yu,Jiabao Jin,Xiaoyao Zhong,Peng Cheng,Lei Chen,Zhitao Shen,Jingkuan Song,Hengtao Shen,Xuemin Lin*

Main category: cs.DB

TL;DR: 本文提出支持分布式存储近似最近邻搜索的DSANN系统，可高效处理大规模向量数据。


<details>
  <summary>Details</summary>
Motivation: 现有ANNS算法索引存储有高成本、规模受限和单点故障问题，分布式存储缺乏有效索引算法。

Method: 采用并发索引构建方法降低索引构建复杂度，应用点聚合图聚合相似向量，通过异步I/O优化存储与查询。

Result: 通过大量实验表明DSANN能在分布式存储场景下高效索引、存储和搜索大规模向量数据集。

Conclusion: DSANN是适用于分布式存储场景的有效系统，可保证索引服务高可用性。

Abstract: Approximate Nearest Neighbor Search (ANNS) in high-dimensional space is an
essential operator in many online services, such as information retrieval and
recommendation. Indices constructed by the state-of-the-art ANNS algorithms
must be stored in single machine's memory or disk for high recall rate and
throughput, suffering from substantial storage cost, constraint of limited
scale and single point of failure. While distributed storage can provide a
cost-effective and robust solution, there is no efficient and effective
algorithms for indexing vectors in distributed storage scenarios. In this
paper, we present a new graph-cluster hybrid indexing and search system which
supports Distributed Storage Approximate Nearest Neighbor Search, called DSANN.
DSANN can efficiently index, store, search billion-scale vector database in
distributed storage and guarantee the high availability of index service. DSANN
employs the concurrent index construction method to significantly reduces the
complexity of index building. Then, DSANN applies Point Aggregation Graph to
leverage the structural information of graph to aggregate similar vectors,
optimizing storage efficiency and improving query throughput via asynchronous
I/O in distributed storage. Through extensive experiments, we demonstrate DSANN
can efficiently and effectively index, store and search large-scale vector
datasets in distributed storage scenarios.

</details>


### [86] [DeepEye-SQL: A Software-Engineering-Inspired Text-to-SQL Framework](https://arxiv.org/abs/2510.17586)
*Boyan Li,Chong Chen,Zhujun Xue,Yinan Mei,Yuyu Luo*

Main category: cs.DB

TL;DR: 现有文本到SQL方案缺乏系统级可靠性，本文提出DeepEye - SQL框架，以软件工程思路处理，提升执行准确率，强调结构化编排的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL解决方案缺乏系统级可靠性，缺少跨整个工作流程的结构化编排来确保正确性。

Method: 提出DeepEye - SQL框架，将文本到SQL视为软件开发，遵循SDLC，包含语义值检索、N版本SQL生成、确定性验证和置信度感知选择四个阶段。

Result: 使用约30B的开源大模型且不微调，在BIRD - Dev上执行准确率达73.5%，在Spider - Test上达89.8%，优于现有方案。

Conclusion: 有原则的编排而非仅靠大模型扩展是实现文本到SQL系统级可靠性的关键。

Abstract: Large language models (LLMs) have advanced Text-to-SQL, yet existing
solutions still fall short of system-level reliability. The limitation is not
merely in individual modules - e.g., schema linking, reasoning, and
verification - but more critically in the lack of structured orchestration that
enforces correctness across the entire workflow. This gap motivates a paradigm
shift: treating Text-to-SQL not as free-form language generation but as a
software-engineering problem that demands structured, verifiable orchestration.
We present DeepEye-SQL, a software-engineering-inspired framework that reframes
Text-to-SQL as the development of a small software program, executed through a
verifiable process guided by the Software Development Life Cycle (SDLC).
DeepEye-SQL integrates four synergistic stages: it grounds ambiguous user
intent through semantic value retrieval and robust schema linking; enhances
fault tolerance with N-version SQL generation using diverse reasoning
paradigms; ensures deterministic verification via a tool-chain of unit tests
and targeted LLM-guided revision; and introduces confidence-aware selection
that clusters execution results to estimate confidence and then takes a
high-confidence shortcut or runs unbalanced pairwise adjudication in
low-confidence cases, yielding a calibrated, quality-gated output. This
SDLC-aligned workflow transforms ad hoc query generation into a disciplined
engineering process. Using ~30B open-source LLMs without any fine-tuning,
DeepEye-SQL achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on
Spider-Test, outperforming state-of-the-art solutions. This highlights that
principled orchestration, rather than LLM scaling alone, is key to achieving
system-level reliability in Text-to-SQL.

</details>


### [87] [This is Going to Sound Crazy, But What If We Used Large Language Models to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We Will Find Better Configurations More Quickly Than Retraining From Scratch!](https://arxiv.org/abs/2510.17748)
*William Zhang,Wan Shen Lim,Andrew Pavlo*

Main category: cs.DB

TL;DR: 本文提出Booster框架辅助现有数据库调优器适应环境变化，能基于历史查询配置上下文和大语言模型给出配置建议，实验表明可提升调优效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化调优器因设计和无法利用查询级历史洞察，难以适应环境变化，如工作负载漂移、模式转移等。

Method: 将历史工件构建为查询 - 配置上下文，利用大语言模型根据相关上下文为每个查询建议配置，再通过束搜索将查询级建议组合成整体配置。

Result: 在多个OLAP工作负载上评估，Booster辅助不同调优器发现的配置比从历史配置继续调优的替代方法效果最多好74%，时间最多快4.7倍。

Conclusion: Booster框架能有效辅助现有调优器适应环境变化，提升调优效果和效率。

Abstract: Tuning database management systems (DBMSs) is challenging due to trillions of
possible configurations and evolving workloads. Recent advances in tuning have
led to breakthroughs in optimizing over the possible configurations. However,
due to their design and inability to leverage query-level historical insights,
existing automated tuners struggle to adapt and re-optimize the DBMS when the
environment changes (e.g., workload drift, schema transfer).
  This paper presents the Booster framework that assists existing tuners in
adapting to environment changes (e.g., drift, cross-schema transfer). Booster
structures historical artifacts into query-configuration contexts, prompts
large language models (LLMs) to suggest configurations for each query based on
relevant contexts, and then composes the query-level suggestions into a
holistic configuration with beam search. With multiple OLAP workloads, we
evaluate Booster's ability to assist different state-of-the-art tuners (e.g.,
cost-/machine learning-/LLM-based) in adapting to environment changes. By
composing recommendations derived from query-level insights, Booster assists
tuners in discovering configurations that are up to 74% better and in up to
4.7x less time than the alternative approach of continuing to tune from
historical configurations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [88] [Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI](https://arxiv.org/abs/2510.16284)
*Di Zhang*

Main category: cs.DC

TL;DR: 本文提出基于MPI的并行自举算法，解决分布式环境通信开销和内存限制问题，新策略可减少通信量和内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统自举技术在处理大数据集或大量重采样时计算成本过高，需解决分布式环境中的高通信开销和内存限制问题。

Method: 提出局部统计聚合和同步伪随机数生成两种策略，开发通信和计算复杂度分析模型，并与基线方法对比。

Result: 分析表明所提方法显著减少通信量和内存使用。

Conclusion: 所提方法能促进大规模系统上可扩展的并行自举。

Abstract: Bootstrapping is a powerful statistical resampling technique for estimating
the sampling distribution of an estimator. However, its computational cost
becomes prohibitive for large datasets or a high number of resamples. This
paper presents a theoretical analysis and design of parallel bootstrapping
algorithms using the Message Passing Interface (MPI). We address two key
challenges: high communication overhead and memory constraints in distributed
environments. We propose two novel strategies: 1) Local Statistic Aggregation,
which drastically reduces communication by transmitting sufficient statistics
instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number
Generation, which enables distributed resampling when the entire dataset cannot
be stored on a single process. We develop analytical models for communication
and computation complexity, comparing our methods against naive baseline
approaches. Our analysis demonstrates that the proposed methods offer
significant reductions in communication volume and memory usage, facilitating
scalable parallel bootstrapping on large-scale systems.

</details>


### [89] [MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization](https://arxiv.org/abs/2510.16415)
*Rizhen Hu,Yutong He,Ran Yan,Mou Sun,Binghang Yuan,Kun Yuan*

Main category: cs.DC

TL;DR: 提出MeCeFO算法解决分布式优化中硬件故障问题，有高效内存和计算效率，理论和实验结果良好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 分布式优化中硬件故障影响大，现有容错训练方法有显著计算或内存开销，需额外资源。

Method: 提出MeCeFO算法，采用Skip - connection、Recomputation和Low - rank gradient approximation三种算法设计。

Result: 理论上收敛速率与传统分布式训练匹配，实验中在高故障率下性能稳健，吞吐量仅下降4.18%，比现有SOTA方法有5.0 - 6.7倍的恢复能力。

Conclusion: MeCeFO算法能以最小开销确保分布式训练的稳健性。

Abstract: As distributed optimization scales to meet the demands of Large Language
Model (LLM) training, hardware failures become increasingly non-negligible.
Existing fault-tolerant training methods often introduce significant
computational or memory overhead, demanding additional resources. To address
this challenge, we propose Memory- and Computation-efficient Fault-tolerant
Optimization (MeCeFO), a novel algorithm that ensures robust training with
minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its
training task to a neighboring node while employing memory- and
computation-efficient algorithmic optimizations to minimize the extra workload
imposed on the neighboring node handling both tasks. MeCeFO leverages three key
algorithmic designs: (i) Skip-connection, which drops the multi-head attention
(MHA) module during backpropagation for memory- and computation-efficient
approximation; (ii) Recomputation, which reduces activation memory in
feedforward networks (FFNs); and (iii) Low-rank gradient approximation,
enabling efficient estimation of FFN weight matrix gradients. Theoretically,
MeCeFO matches the convergence rate of conventional distributed training, with
a rate of $\mathcal{O}(1/\sqrt{nT})$, where n is the data parallelism size and
T is the number of iterations. Empirically, MeCeFO maintains robust performance
under high failure rates, incurring only a 4.18% drop in throughput,
demonstrating 5.0$\times$ to 6.7$\times$ greater resilience than previous SOTA
approaches. Codes are available at https://github.com/pkumelon/MeCeFO.

</details>


### [90] [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)
*Jian Ma,Xinchen Lyu,Jun Jiang,Longhao Zou,Chenshan Ren,Qimei Cui,Xiaofeng Tao*

Main category: cs.DC

TL;DR: 提出FourierCompress激活压缩框架，解决协作式大语言模型推理通信瓶颈，实验显示有良好性能和效率。


<details>
  <summary>Details</summary>
Motivation: 协作式大语言模型推理受通信瓶颈阻碍，现有激活压缩方法难以兼顾高压缩比、低重建误差和计算效率。

Method: 提出FourierCompress框架，利用大语言模型激活在频域的稀疏性，通过快速傅里叶变换将激活转换到频域，保留低频系数并在服务器重建信号。

Result: 在Llama 3和Qwen2.5模型的10个常识推理数据集实验中，FourierCompress性能接近未压缩基线，优于Top - k、QR和SVD。

Conclusion: FourierCompress在边缘设备大语言模型推理中，实现通信效率、近无损推理和快速压缩的平衡。

Abstract: Collaborative large language model (LLM) inference enables real-time,
privacy-preserving AI services on resource-constrained edge devices by
partitioning computational workloads between client devices and edge servers.
However, this paradigm is severely hindered by communication bottlenecks caused
by the transmission of high-dimensional intermediate activations, exacerbated
by the autoregressive decoding structure of LLMs, where bandwidth consumption
scales linearly with output length. Existing activation compression methods
struggle to simultaneously achieve high compression ratios, low reconstruction
error, and computational efficiency. This paper proposes FourierCompress, a
novel, layer-aware activation compression framework that exploits the
frequency-domain sparsity of LLM activations. We rigorously demonstrate that
activations from the first Transformer layer exhibit strong smoothness and
energy concentration in the low-frequency domain, making them highly amenable
to near-lossless compression via the Fast Fourier Transform (FFT).
FourierCompress transforms activations into the frequency domain, retains only
a compact block of low-frequency coefficients, and reconstructs the signal at
the server using conjugate symmetry, enabling seamless hardware acceleration on
DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10
commonsense reasoning datasets demonstrate that FourierCompress preserves
performance remarkably close to the uncompressed baseline, outperforming Top-k,
QR, and SVD. FourierCompress bridges the gap between communication efficiency
(an average 7.6x reduction in activation size), near-lossless inference (less
than 0.3% average accuracy loss), and significantly faster compression
(achieving over 32x reduction in compression time compared to Top-k via
hardware acceleration) for edge-device LLM inference.

</details>


### [91] [Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages](https://arxiv.org/abs/2510.16497)
*Pacome Simon Mbonimpa,Diane Tuyizere,Azizuddin Ahmed Biyabani,Ozan K. Tonguz*

Main category: cs.DC

TL;DR: 提出用于卢旺达语和斯瓦希里语语音转录与合成的边缘 - 云并行框架，利用预训练模型和级联机制，减少延迟和资源使用，实验证明性能良好。


<details>
  <summary>Details</summary>
Motivation: 解决东非技术基础设施有限国家中卢旺达语和斯瓦希里语强大语言处理工具稀缺问题。

Method: 利用Whisper和SpeechT5预训练模型实现语音与文本转换，采用级联机制在边缘设备和云之间分配模型推理工作负载。

Result: 边缘设备上SpeechT5模型内存使用压缩9.5%，Whisper模型压缩14%，最大内存使用149MB；在特定配置下，系统能在一分钟内处理270字符文本的转录。

Conclusion: 提出的级联边缘 - 云架构可作为语音转录的优秀平台，具有良好准确性和响应时间。

Abstract: This paper presents a novel framework for speech transcription and synthesis,
leveraging edge-cloud parallelism to enhance processing speed and accessibility
for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful
language processing tools for these widely spoken languages in East African
countries with limited technological infrastructure. The framework utilizes the
Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and
text-to-speech (TTS) translation. The architecture uses a cascading mechanism
that distributes the model inference workload between the edge device and the
cloud, thereby reducing latency and resource usage, benefiting both ends. On
the edge device, our approach achieves a memory usage compression of 9.5% for
the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage
of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with
a 1 MB/s network bandwidth, the system can process a 270-character text in less
than a minute for both speech-to-text and text-to-speech transcription. Using
real-world survey data from Kenya, it is shown that the cascaded edge-cloud
architecture proposed could easily serve as an excellent platform for STT and
TTS transcription with good accuracy and response time.

</details>


### [92] [Reimagining RDMA Through the Lens of ML](https://arxiv.org/abs/2510.16606)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: 随着分布式机器学习工作负载扩展，集体通信的尾延迟成瓶颈，传统RDMA设计有缺陷，提出Celeris传输协议，早期结果显示其有性能提升。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习工作负载扩展后，传统RDMA设计的可靠性和有序传输机制导致系统性能下降，需要新的传输协议。

Method: Celeris去除RDMA网卡的重传和有序传输，采用尽力而为传输，保留拥塞控制，用软件机制管理通信，将损失恢复转移到ML管道。

Result: Celeris将99%分位延迟降低2.3倍，BRAM使用减少67%，网卡故障恢复能力近乎翻倍。

Conclusion: Celeris是适用于集群规模机器学习的弹性、可扩展传输协议。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs
connected by ultra-high-speed inter-connects, tail latency in collective
communication has emerged as a primary bottleneck. Prior RDMA designs, like
RoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying
on retransmissions and packet sequencing to ensure correctness. While effective
for general-purpose workloads, these mechanisms introduce complexity and
latency that scale poorly, where even rare packet losses or delays can
consistently degrade system performance. We introduce Celeris, a
domain-specific RDMA transport that revisits traditional reliability guarantees
based on ML's tolerance for lost or partial data. Celeris removes
retransmissions and in-order delivery from the RDMA NIC, enabling best-effort
transport that exploits the robustness of ML workloads. It retains congestion
control (e.g., DCQCN) and manages communication with software-level mechanisms
such as adaptive timeouts and data prioritization, while shifting loss recovery
to the ML pipeline (e.g., using the Hadamard Transform). Early results show
that Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by
67%, and nearly doubles NIC resilience to faults -- delivering a resilient,
scalable transport tailored for ML at cluster scale.

</details>


### [93] [Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization](https://arxiv.org/abs/2510.17158)
*Daniel Nichols,Konstantinos Parasyris,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.DC

TL;DR: 语言模型在软件工程中广泛应用，但在代码性能相关任务表现不佳，本文提出让语言模型在推理时与性能工具交互的方法并用于训练GPU内核优化模型。


<details>
  <summary>Details</summary>
Motivation: 语言模型在代码性能相关任务难以取得理想结果，现有模型无法理解环境与代码性能的交互。

Method: 提出让语言模型在推理过程中与性能工具交互的训练方法。

Result: 利用该方法训练出了最先进的GPU内核优化模型。

Conclusion: 所提方法可用于训练处理代码性能相关任务的语言模型。

Abstract: Language models are now prevalent in software engineering with many
developers using them to automate tasks and accelerate their development. While
language models have been tremendous at accomplishing complex software
engineering tasks, there are still many areas where they fail to deliver
desirable results, for instance code performance related tasks. Tasks like
optimization depend on many complex data from the environment, hardware, etc.
that are not directly represented in source code. Recent efforts have seen
large improvements in general code modeling tasks using chain-of-thought style
reasoning, but these models still fail to comprehend how the environment
interacts with code performance. In this paper we propose a methodology to
train language models that can interact with performance tools during their
reasoning process. We then demonstrate how this methodology can be used to
train a state-of-the-art GPU kernel optimization model.

</details>


### [94] [Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++](https://arxiv.org/abs/2510.16890)
*Jiří Klepl,Martin Kruliš,Matyáš Brabec*

Main category: cs.DC

TL;DR: 本文针对MPI纯C接口的不足，提出基于C++ Noarr库的抽象扩展，以案例证明其性能与现有MPI C++绑定相当且设计更灵活。


<details>
  <summary>Details</summary>
Motivation: MPI纯C接口缺乏现代语言特性，如基本类型检查和泛型代码设计支持。

Method: 提出一种新的MPI抽象，作为C++ Noarr库的扩展，遵循Noarr范式，提供布局无关的设计；实现布局无关的分布式GEMM内核作为案例。

Result: 该抽象实现的性能与现有最先进的MPI C++绑定相当。

Conclusion: 该抽象允许更灵活地设计分布式应用程序。

Abstract: Message Passing Interface (MPI) has been a well-established technology in the
domain of distributed high-performance computing for several decades. However,
one of its greatest drawbacks is a rather ancient pure-C interface. It lacks
many useful features of modern languages (namely C++), like basic type-checking
or support for generic code design. In this paper, we propose a novel
abstraction for MPI, which we implemented as an extension of the C++ Noarr
library. It follows Noarr paradigms (first-class layout and traversal
abstraction) and offers layout-agnostic design of MPI applications. We also
implemented a layout-agnostic distributed GEMM kernel as a case study to
demonstrate the usability and syntax of the proposed abstraction. We show that
the abstraction achieves performance comparable to the state-of-the-art MPI C++
bindings while allowing for a more flexible design of distributed applications.

</details>


### [95] [FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems](https://arxiv.org/abs/2510.16896)
*Yiming Hu*

Main category: cs.DC

TL;DR: 本文提出互联多核系统集成容错架构，无需额外硬件，能降低任务工作量，提升可靠性和能效。


<details>
  <summary>Details</summary>
Motivation: 现有Two - Phase TMR在永久故障下失效，R - TMR依赖额外硬件且多核心或辅助模块故障时容错性降低，需更好方案。

Method: 构建稳定性指标识别可靠机器，进行定期诊断，实现永久故障隔离和自适应任务调度。

Result: 相比基线TMR减少约30%任务工作量，有更好的故障覆盖和隔离准确性。

Conclusion: 该架构显著提高系统可靠性和能源效率。

Abstract: Two-Phase Triple Modular Redundancy TMR divides redundancy operations into
two stages, omitting part of the computation during fault-free operation to
reduce energy consumption. However, it becomes ineffective under permanent
faults, limiting its reliability in critical systems. To address this,
Reactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty
cores, tolerating both transient and permanent faults. Yet, its reliance on
additional hardware increases system complexity and reduces fault tolerance
when multiple cores or auxiliary modules fail. This paper proposes an
integrated fault-tolerant architecture for interconnected multicore systems. By
constructing a stability metric to identify reliable machines and performing
periodic diagnostics, the method enables permanent fault isolation and adaptive
task scheduling without extra hardware. Experimental results show that it
reduces task workload by approximately 30% compared to baseline TMR and
achieves superior fault coverage and isolation accuracy, significantly
improving both reliability and energy efficiency.

</details>


### [96] [Tutoring LLM into a Better CUDA Optimizer](https://arxiv.org/abs/2510.16933)
*Matyáš Brabec,Jiří Klepl,Michal Töpfer,Martin Kruliš*

Main category: cs.DC

TL;DR: 研究大语言模型生成优化CUDA代码能力，发现需辅导达专家水平。


<details>
  <summary>Details</summary>
Motivation: 探究最新推理模型生成优化CUDA代码能力，及能否通过辅导改进。

Method: 自动和手动评估生成代码，尝试交互方式让模型修正错误。

Result: 大语言模型编码能力不错，但需辅导才能达到并行计算专家提供的优化方案。

Conclusion: 大语言模型需辅导才能实现代码优化，达到专家水平。

Abstract: Recent leaps in large language models (LLMs) caused a revolution in
programming tools (like GitHub Copilot) that can help with code generation,
debugging, and even performance optimization. In this paper, we focus on the
capabilities of the most recent reasoning models to generate optimized CUDA
code for predefined, well-known tasks. Our objective is to determine which
types of code optimizations and parallel patterns the LLMs can perform by
themselves and whether they can be improved by tutoring (providing more
detailed hints and guidelines in the prompt). The generated solutions were
evaluated both automatically (for correctness and speedup) and manually (code
reviews) to provide a more detailed perspective. We also tried an interactive
approach where the LLM can fix its previous mistakes within a session. The
results indicate that LLMs are quite skilled coders; however, they require
tutoring to reach optimized solutions provided by parallel computing experts.

</details>


### [97] [Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure](https://arxiv.org/abs/2510.16946)
*Erfan Darzi,Aldo Pareja,Shreeanant Bharadwaj*

Main category: cs.DC

TL;DR: 本文提出基于eBPF的遥测系统用于诊断GPU尾部延迟峰值，能实现高诊断准确率和快速检测分析，可在多租户GPU基础设施中进行操作调试。


<details>
  <summary>Details</summary>
Motivation: 现有监控工具在共享计算环境中缺乏对GPU尾部延迟峰值进行根本原因分析的粒度，需维护性能可预测性和资源利用率。

Method: 引入基于eBPF的遥测系统，统一主机端对GPU工作负载的监控，将eBPF派生的主机指标与GPU内部事件关联。

Result: 系统诊断准确率达81 - 88%，5秒内检测到峰值，6 - 8秒完成根本原因分析，100Hz采样时CPU开销为1.21%，能识别包括NIC争用、PCIe压力和CPU干扰等根本原因。

Conclusion: 该系统可在无需集群范围仪器的情况下，对多租户GPU基础设施进行操作调试。

Abstract: Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is
critical for maintaining performance predictability and resource utilization,
yet existing monitoring tools lack the granularity for root cause analysis in
shared computing environments. We introduce an eBPF-based telemetry system that
provides unified host-side monitoring of GPU workloads, correlating
eBPF-derived host metrics with GPU-internal events for holistic system
observability. The system achieves 81--88\% diagnostic accuracy, detects spikes
within 5 seconds, and completes root cause analysis in 6--8 seconds, operating
with 1.21\% CPU overhead at 100Hz sampling. Evaluated on distributed learning
workloads, the system identifies root causes including NIC contention, PCIe
pressure, and CPU interference, enabling operational debugging for multi-tenant
GPU infrastructure without requiring cluster-wide instrumentation.

</details>


### [98] [On the Universality of Round Elimination Fixed Points](https://arxiv.org/abs/2510.17639)
*Alkida Balliu,Sebastian Brandt,Ole Gabsdil,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: 研究分布式图算法中轮次消除固定点是否为证明下界通用技术，消除旧障碍，提出新障碍，证明通用下界定理。


<details>
  <summary>Details</summary>
Motivation: 探讨轮次消除固定点是否为证明下界的通用技术，若成立，分布式计算复杂度关键部分可判定。

Method: 开发新的系统构建轮次消除下界技术，使用幂三输入进行证明。

Result: 消除已知同态问题障碍，提出新障碍，证明适用于任意问题的通用下界定理。

Conclusion: 轮次消除对有输入问题非通用技术，对无输入问题可能通用。

Abstract: Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC
2020] has drawn attention to the following open question: are round elimination
fixed points a universal technique for proving lower bounds? That is, given a
locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds
in the deterministic LOCAL model, can we always find a relaxation $\Pi'$ of
$\Pi$ that is a nontrivial fixed point for the round elimination technique [see
STOC 2016, PODC 2019]? If yes, then a key part of distributed computational
complexity would be also decidable.
  The key obstacle so far has been a certain family of homomorphism problems
[ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is
based on Marks' technique [J.AMS 2016].
  We develop a new technique for constructing round elimination lower bounds
systematically. Using so-called tripotent inputs we show that the
aforementioned homomorphism problems indeed admit a lower bound proof that is
based on round elimination fixed points. Hence we eliminate the only known
obstacle for the universality of round elimination.
  Yet we also present a new obstacle: we show that there are some problems with
inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is
based on relaxations to nontrivial round elimination fixed points. Hence round
elimination cannot be a universal technique for problems with inputs (but it
might be universal for problems without inputs).
  We also prove the first fully general lower bound theorem that is applicable
to any problem, with or without inputs, that is a fixed point in round
elimination. Prior results of this form were only able to handle certain very
restricted inputs.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [99] [Is Zadeh's Least-Entered Pivot Rule Exponential?](https://arxiv.org/abs/2510.16055)
*Norman Zadeh*

Main category: cs.DS

TL;DR: 本文指出Friedmann等人关于Zadeh最小进入规则下单纯形法需指数级转轴次数的论证有多处缺陷，未给出有效反例。


<details>
  <summary>Details</summary>
Motivation: 验证Friedmann等人关于Zadeh最小进入规则最坏情况行为的论证是否正确。

Method: 分析Friedmann等人论文中的论证过程，指出约束条件的缺陷，对其提供的线性规划和马尔可夫决策过程实例进行验证。

Result: Friedmann等人的论证存在多处缺陷，提供的实例不符合要求，未给出有效反例。

Conclusion: Zadeh最小进入规则的最坏情况行为尚未确立。

Abstract: In 2011, Friedmann [F 7] claimed to have proved that pathological linear
programs existed for which the Simplex method using Zadeh's least-entered rule
[Z 14] would take an exponential number of pivots. In 2019, Disser and Hopp [DH
5] argued that there were errors in Friedmann's 2011 construction. In 2020,
Disser, Friedmann, and Hopp [DFH 3,4] again contended that the least-entered
rule was exponential. We show that their arguments contain multiple flaws. In
other words, the worst-case behavior of the least-entered rule has not been
established. Neither [F 7] nor [DFH 3,4] provides pathological linear programs
that can be tested. Instead, the authors contend that their pathological linear
programs are of the form (P) as shown on page 12 of [DFH 3]. The authors
contend that the constraints of (P) ensure that the probability of entering a
vertex u is equal to the probability of exiting u. In fact, we note that the
authors' constraints (P) are flawed in at least three ways: a) they require the
probability of exiting u to exceed the probability of entering u, b) they
require the probability of exiting some nodes to exceed 1, and c) they overlook
flows from decision nodes to decision nodes. At my request, in August of 2025,
Disser, Friedmann, and Hopp provided me with their first ten purportedly
pathological LPs and the graph of their first purportedly pathological Markov
Decision Process (MDP1). It is shown that: a) their first two pathological LPs
are infeasible if the variables are supposed to be probabilities, as the
authors contend, and b) their first purportedly pathological LP does not match
up with their first purportedly pathological MDP. In other words, the authors
have not come close to providing counterexamples to the least-entered rule.

</details>


### [100] [Near-linear time subhypergraph counting in bounded degeneracy hypergraphs](https://arxiv.org/abs/2510.16330)
*Daniel Paul-Pena,C. Seshadhri*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Counting small patterns in a large dataset is a fundamental algorithmic task.
The most common version of this task is subgraph/homomorphism counting, wherein
we count the number of occurrences of a small pattern graph $H$ in an input
graph $G$. The study of this problem is a field in and of itself. Recently,
both in theory and practice, there has been an interest in \emph{hypergraph}
algorithms, where $G = (V,E)$ is a hypergraph. One can view $G$ as a set system
where hyperedges are subsets of the universe $V$.
  Counting patterns $H$ in hypergraphs is less studied, although there are many
applications in network science and database algorithms. Inspired by advances
in the graph literature, we study when linear time algorithms are possible.
  We focus on input hypergraphs $G$ that have bounded \emph{degeneracy}, a
well-studied concept for graph algorithms. We give a spectrum of definitions
for hypergraph degeneracy that cover all existing notions. For each such
definition, we give a precise characterization of the patterns $H$ that can be
counted in (near) linear time. Specifically, we discover a set of ``obstruction
patterns". If $H$ does not contain an obstruction, then the number of
$H$-subhypergraphs can be counted exactly in $O(n\log n)$ time (where $n$ is
the number of vertices in $G$). If $H$ contains an obstruction, then (assuming
hypergraph variants of fine-grained complexity conjectures), there is a
constant $\gamma > 0$, such that there is no $o(n^{1+\gamma})$ time algorithm
for counting $H$-subhypergraphs. These sets of obstructions can be defined for
all notions of hypergraph degeneracy.

</details>


### [101] [A (Very) Nearly Optimal Sketch for $k$-Edge Connectivity Certificates](https://arxiv.org/abs/2510.16336)
*Pachara Sawettamalya,Huacheng Yu*

Main category: cs.DS

TL;DR: 提出动态图流中计算k - 连通性证书的简单算法，空间复杂度有改进。


<details>
  <summary>Details</summary>
Motivation: 改进动态图流中计算k - 连通性证书算法的空间复杂度。

Method: 提出新的简单算法。

Result: 新算法使用O(n log²n · max{k, log n log k})位空间，改进了原算法，在特定k值下接近已知下界。

Conclusion: 对于k = Ω(log n log log n)完全确定空间复杂度为Θ(kn log²n)，对于较小k缩小了与下界差距至O(log log n)。

Abstract: In this note, we present a simple algorithm for computing a
\emph{$k$-connectivity certificate} in dynamic graph streams. Our algorithm
uses $O(n \log^2 n \cdot \max\{k, \log n \log k\})$ bits of space which
improves upon the $O(kn \log^3 n)$-space algorithm of Ahn, Guha, and McGregor
(SODA'12). For the values of $k$ that are truly sublinear, our space usage
\emph{very nearly} matches the known lower bound $\Omega(n \log^2 n \cdot
\max\{k, \log n\})$ established by Nelson and Yu (SODA'19; implicit) and
Robinson (DISC'24). In particular, our algorithm fully settles the space
complexity at $\Theta(kn \log^2{n})$ for $k = \Omega(\log n \log \log n)$, and
bridges the gap down to only a doubly-logarithmic factor of $O(\log \log n)$
for a smaller range of $k = o(\log n \log \log n)$.

</details>


### [102] [Truly Subquadratic Time Algorithms for Diameter and Related Problems in Graphs of Bounded VC-dimension](https://arxiv.org/abs/2510.16346)
*Timothy M. Chan,Hsien-Chih Chang,Jie Gao,Sándor Kisfaludi-Bak,Hung Le,Da Wei Zheng*

Main category: cs.DS

TL;DR: 本文给出计算n顶点单位圆盘图直径的首个真正次二次时间算法，时间复杂度为$O^*(n^{2 - 1/18})$，并提出通用框架获多个相关问题次二次算法。


<details>
  <summary>Details</summary>
Motivation: 解决文献中计算单位圆盘图直径的核心开放问题，为不同图族和距离问题提供高效算法。

Method: 提出通用框架，绕过之前算法用的次线性分隔符，使用低直径分解，利用输入图关联集合系统的有界VC维以及几何数据结构新思想。

Result: 得到计算单位圆盘图直径的$O^*(n^{2 - 1/18})$时间算法；得到计算m边稀疏无权图直径、轴对齐任意大小正方形相交图直径的次二次时间算法；得到其他距离相关问题的首个真正次二次复杂度算法。

Conclusion: 通用框架可用于不同图族和距离问题，能得到多个问题的真正次二次时间算法，推广并改进了部分已有结果。

Abstract: We give the first truly subquadratic time algorithm, with $O^*(n^{2-1/18})$
running time, for computing the diameter of an $n$-vertex unit-disk graph,
resolving a central open problem in the literature. Our result is obtained as
an instance of a general framework, applicable to different graph families and
distance problems. Surprisingly, our framework completely bypasses sublinear
separators (or $r$-divisions) which were used in all previous algorithms.
Instead, we use low-diameter decompositions in their most elementary form. We
also exploit bounded VC-dimension of set systems associated with the input
graph, as well as new ideas on geometric data structures. Among the numerous
applications of the general framework, we obtain:
  1. An $\tilde{O}(mn^{1-1/(2d)})$ time algorithm for computing the diameter of
$m$-edge sparse unweighted graphs with constant VC-dimension $d$. The
previously known algorithms by Ducoffe, Habib, and Viennot [SODA 2019] and
Duraj, Konieczny, and Pot\c{e}pa [ESA 2024] are truly subquadratic only when
the diameter is a small polynomial. Our result thus generalizes truly
subquadratic time algorithms known for planar and minor-free graphs (in fact,
it slightly improves the previous time bound for minor-free graphs).
  2. An $\tilde{O}(n^{2-1/12})$ time algorithm for computing the diameter of
intersection graphs of axis-aligned squares with arbitrary size. The best-known
algorithm by Duraj, Konieczny, and Pot\c{e}pa [ESA 2024] only works for unit
squares and is only truly subquadratic in the low-diameter regime.
  3. The first algorithms with truly subquadratic complexity for other
distance-related problems, including all-vertex eccentricities, Wiener index,
and exact distance oracles. (... truncated to meet the arXiv abstract
requirement.)

</details>


### [103] [Tight Pair Query Lower Bounds for Matching and Earth Mover's Distance](https://arxiv.org/abs/2510.16351)
*Amir Azarmehr,Soheil Behnezhad,Mohammad Roghani,Aviad Rubinstein*

Main category: cs.DS

TL;DR: 本文研究估计n顶点图最大匹配大小所需邻接矩阵查询次数，给出首个超线性下界，证明[BKS'23]算法最优，还排除了地球移动距离估计算法改进的可能性。


<details>
  <summary>Details</summary>
Motivation: 探究估计n顶点图最大匹配大小所需的邻接矩阵查询次数，填补该问题在邻接矩阵查询访问算法上非平凡下界的空白。

Method: 对估计最大匹配大小问题进行理论分析和证明，证明了对于固定δ>0，存在固定ε>0，使得在误差εn内估计需要Ω(n^{2 - δ})次邻接矩阵查询。

Result: 给出首个在n上超线性的下界，证明[BKS'23]算法是最优的；排除了地球移动距离估计算法在(1, 2)-度量下的改进可能性。

Conclusion: 在估计n顶点图最大匹配大小的邻接矩阵查询次数问题上，[BKS'23]算法已达最优；地球移动距离估计算法在(1, 2)-度量下无法改进。

Abstract: How many adjacency matrix queries (also known as pair queries) are required
to estimate the size of a maximum matching in an $n$-vertex graph $G$? We study
this fundamental question in this paper.
  On the upper bound side, an algorithm of Bhattacharya, Kiss, and Saranurak
[FOCS'23] gives an estimate that is within $\epsilon n$ of the right bound with
$n^{2-\Omega_\epsilon(1)}$ queries, which is subquadratic in $n$ (and thus
sublinear in the matrix size) for any fixed $\epsilon > 0$. On the lower bound
side, while there has been a lot of progress in the adjacency list model, no
non-trivial lower bound has been established for algorithms with adjacency
matrix query access. In particular, the only known lower bound is a folklore
bound of $\Omega(n)$, leaving a huge gap.
  In this paper, we present the first superlinear in $n$ lower bound for this
problem. In fact, we close the gap mentioned above entirely by showing that the
algorithm of [BKS'23] is optimal. Formally, we prove that for any fixed $\delta
> 0$, there is a fixed $\epsilon > 0$ such that an estimate that is within
$\epsilon n$ of the true bound requires $\Omega(n^{2-\delta})$ adjacency matrix
queries.
  Our lower bound also has strong implications for estimating the earth mover's
distance between distributions. For this problem, Beretta and Rubinstein
[STOC'24] gave an $n^{2-\Omega_\epsilon(1)}$ time algorithm that obtains an
additive $\epsilon$-approximation and works for any distance function. Whether
this can be improved generally, or even for metric spaces, had remained open.
Our lower bound rules out the possibility of any improvements over this bound,
even under the strong assumption that the underlying distances are in a (1,
2)-metric.

</details>


### [104] [Online computation of normalized substring complexity](https://arxiv.org/abs/2510.16454)
*Gregory Kucherov,Yakov Nekrich*

Main category: cs.DS

TL;DR: 研究字符串归一化子串复杂度δ的在线计算问题，提出两种算法，分别达到O(log n)均摊时间和O(log^3 n)最坏时间，为该问题首个多项式对数时间在线解。


<details>
  <summary>Details</summary>
Motivation: 字符串归一化子串复杂度δ与流行的字符串压缩算法有关联，有必要研究其在线计算问题。

Method: 提出两种算法，一种达到O(log n)的每个字符均摊时间，另一种达到O(log^3 n)的每个字符最坏时间。

Result: 得到两种能解决在线计算δ问题的算法。

Conclusion: 这是该问题的首个多项式对数时间在线解。

Abstract: The normalized substring complexity $\delta$ of a string is defined as
$\max_k \{c[k]/k\}$, where $c[k]$ is the number of \textit{distinct} substrings
of length $k$. This simply defined measure has recently attracted attention due
to its established relationship to popular string compression algorithms. We
consider the problem of computing $\delta$ online, when the string is provided
from a stream. We present two algorithms solving the problem: one working in
$O(\log n)$ amortized time per character, and the other in $O(\log^3 n)$
worst-case time per character. To our knowledge, this is the first polylog-time
online solution to this problem.

</details>


### [105] [Trading Prophets with Initial Capital](https://arxiv.org/abs/2510.16516)
*Yossi Azar,Niv Buchbinder,Roie Levin,Or Vardi*

Main category: cs.DS

TL;DR: 本文探讨有初始资金时的交易预言家问题，得到竞争比为3，且证明此为最优，还研究含交易成本的更现实模型。


<details>
  <summary>Details</summary>
Motivation: Correa等人提出的交易预言家问题中无法与知晓未来股价的预言家竞争，本文想探索有初始资金时能否解决此问题。

Method: 对有初始资金的交易预言家问题进行理论分析和推导。

Result: 得到与知晓未来价格的预言家竞争比为3，且证明此为最优，还研究了含交易成本的模型。

Conclusion: 初始资金可绕过之前的不可能结果，竞争比3是最优的，还对含交易成本的现实模型进行了研究。

Abstract: Correa et al. [EC' 2023] introduced the following trading prophets problem. A
trader observes a sequence of stochastic prices for a stock, each drawn from a
known distribution, and at each time must decide whether to buy or sell.
Unfortunately, they observed that in this setting it is impossible to compete
with a prophet who knows all future stock prices.
  In this paper, we explore the trading prophets problem when we are given
initial capital with which to start trading. We show that initial capital is
enough to bypass the impossibility result and obtain a competitive ratio of $3$
with respect to a prophet who knows all future prices (and who also starts with
capital), and we show that this competitive ratio is best possible. We further
study a more realistic model in which the trader must pay multiplicative and/or
additive transaction costs for trading which model dynamics such as bid-ask
spreads and broker fees.

</details>


### [106] [Robust Dynamic Staffing with Predictions](https://arxiv.org/abs/2510.16663)
*Yiding Feng,Vahideh Manshadi,Rad Niazadeh,Saba Neyshabouri*

Main category: cs.DS

TL;DR: 研究动态人员配置问题，在对抗预测模型下提出在线算法，证明其极小极大最优，扩展模型并实验验证算法优势。


<details>
  <summary>Details</summary>
Motivation: 解决最后一英里配送运营中，在工人可用性下降和需求预测准确性变化情况下的人员配置权衡问题，避免贝叶斯模型的实际局限性。

Method: 在对抗预测模型下，通过多项式大小的线性规划刻画针对受限对手的极小极大成本，在一般情况下模拟该解决方案；扩展模型并引入‘重新求解’变体算法。

Result: 得到简单且计算高效的在线算法，证明其极小极大最优；实验显示算法在成本和速度上优于贝叶斯启发式方法，与贝叶斯最优策略有竞争力。

Conclusion: 提出的算法能有效解决动态人员配置问题，在多种情况下表现良好。

Abstract: We consider a natural dynamic staffing problem in which a decision-maker
sequentially hires workers over a finite horizon to meet an unknown demand
revealed at the end. Predictions about demand arrive over time and become
increasingly accurate, while worker availability decreases. This creates a
fundamental trade-off between hiring early to avoid understaffing (when workers
are more available but forecasts are less reliable) and hiring late to avoid
overstaffing (when forecasts are more accurate but availability is lower). This
problem is motivated by last-mile delivery operations, where companies such as
Amazon rely on gig-economy workers whose availability declines closer to the
operating day.
  To address practical limitations of Bayesian models (in particular, to remain
agnostic to the underlying forecasting method), we study this problem under
adversarial predictions. In this model, sequential predictions are
adversarially chosen uncertainty intervals that (approximately) contain the
true demand. The objective is to minimize worst-case staffing imbalance cost.
Our main result is a simple and computationally efficient online algorithm that
is minimax optimal. We first characterize the minimax cost against a restricted
adversary via a polynomial-size linear program, then show how to emulate this
solution in the general case. While our base model focuses on a single demand,
we extend the framework to multiple demands (with egalitarian/utilitarian
objectives), to settings with costly reversals of hiring decisions, and to
inconsistent prediction intervals. We also introduce a practical "re-solving"
variant of our algorithm, which we prove is also minimax optimal. Finally we
conduct numerical experiments showing that our algorithms outperform Bayesian
heuristics in both cost and speed, and are competitive with (approximate or
exact) Bayesian-optimal policies when those can be computed.

</details>


### [107] [An Exact Algorithm for the Unanimous Vote Problem](https://arxiv.org/abs/2510.16678)
*Feyza Duman Keles,Lisa Hellerstein,Kunal Marwaha,Christopher Musco,Xinchen Yang*

Main category: cs.DS

TL;DR: 本文针对Unanimous Vote问题给出了时间复杂度为O(n log n)的精确算法，证明其并非NP难问题，还比较了最优排序与最佳自适应策略，得出适应度差距。


<details>
  <summary>Details</summary>
Motivation: 解决Gkenosis等人遗留的Unanimous Vote问题是否为NP难问题。

Method: 使用简单的交换论证，证明最优排序与自然贪心算法产生的排序接近。

Result: 给出时间复杂度为O(n log n)的精确算法，得出Unanimous Vote问题适应度差距为1.2±o(1)。

Conclusion: Unanimous Vote问题可在多项式时间内解决，是随机布尔函数评估问题中少数可多项式求解的问题之一。

Abstract: Consider $n$ independent, biased coins, each with a known probability of
heads. Presented with an ordering of these coins, flip (i.e., toss) each coin
once, in that order, until we have observed both a *head* and a *tail*, or
flipped all coins. The Unanimous Vote problem asks us to find the ordering that
minimizes the expected number of flips. Gkenosis et al. [arXiv:1806.10660] gave
a polynomial-time $\phi$-approximation algorithm for this problem, where $\phi
\approx 1.618$ is the golden ratio. They left open whether the problem was
NP-hard. We answer this question by giving an exact algorithm that runs in time
$O(n \log n)$. The Unanimous Vote problem is an instance of the more general
Stochastic Boolean Function Evaluation problem: it thus becomes one of the only
such problems known to be solvable in polynomial time. Our proof uses simple
interchange arguments to show that the optimal ordering must be close to the
ordering produced by a natural greedy algorithm. Beyond our main result, we
compare the optimal ordering with the best adaptive strategy, proving a tight
adaptivity gap of $1.2\pm o(1)$ for the Unanimous Vote problem.

</details>


### [108] [All-Pairs Minimum Cut using $\tilde{O}(n^{7/4})$ Cut Queries](https://arxiv.org/abs/2510.16741)
*Yotam Kenneth-Mordoch,Robert Krauthgamer*

Main category: cs.DS

TL;DR: 提出割查询模型下全对最小割问题的首个非平凡算法，用 $	ilde{O}(n^{7/4})$ 次割查询构建 Gomory - Hu 树解决问题。


<details>
  <summary>Details</summary>
Motivation: 解决割查询模型下的全对最小割问题。

Method: 采用随机化算法，通过割查询构建 Gomory - Hu 树。

Result: 使用 $	ilde{O}(n^{7/4})$ 次割查询构建出 Gomory - Hu 树。

Conclusion: 提出的随机化算法可有效解决割查询模型下的全对最小割问题。

Abstract: We present the first non-trivial algorithm for the all-pairs minimum cut
problem in the cut-query model. Given cut-query access to an unweighted graph
$G=(V,E)$ with $n$ vertices, our randomized algorithm constructs a Gomory-Hu
tree of $G$, and thus solves the all-pairs minimum cut problem, using
$\tilde{O}(n^{7/4})$ cut queries.

</details>


### [109] [Combinatorial Maximum Flow via Weighted Push-Relabel on Shortcut Graphs](https://arxiv.org/abs/2510.17182)
*Aaron Bernstein,Joakim Blikstad,Jason Li,Thatchaphol Saranurak,Ta-Wei Tu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We give a combinatorial algorithm for computing exact maximum flows in
directed graphs with $n$ vertices and edge capacities from $\{1,\dots,U\}$ in
$\tilde{O}(n^{2}\log U)$ time, which is near-optimal on dense graphs. This
shaves an $n^{o(1)}$ factor from the recent result of
[Bernstein-Blikstad-Saranurak-Tu FOCS'24] and, more importantly, greatly
simplifies their algorithm. We believe that ours is by a significant margin the
simplest of all algorithms that go beyond $\tilde{O}(m\sqrt{n})$ time in
general graphs. To highlight this relative simplicity, we provide a full
implementation of the algorithm in C++.
  The only randomized component of our work is the cut-matching game. Via
existing tools, we show how to derandomize it for vertex-capacitated max flow
and obtain a deterministic $\tilde{O}(n^2)$ time algorithm. This marks the
first deterministic near-linear time algorithm for this problem (or even for
the special case of bipartite matching) in any density regime.

</details>


### [110] [Finding 4-Additive Spanners: Faster, Stronger, and Simpler](https://arxiv.org/abs/2510.17262)
*Chuhan Qi*

Main category: cs.DS

TL;DR: 本文提出构建4 - 加性扩张器的新确定性算法，匹配最佳边界，提升运行时间，且有诸多优势。


<details>
  <summary>Details</summary>
Motivation: 加性扩张器有广泛应用，需更好的构建4 - 加性扩张器的算法。

Method: 提出一种新的确定性算法。

Result: 新算法匹配最佳已知边界$\tilde{O}(n^{7/5})$，运行时间提升至$\tilde{O}(\min\{mn^{3/5}, n^{11/5}\})$。

Conclusion: 新算法在稠密情况下更快，是确定性的，概念简单，易实现和分析。

Abstract: Additive spanners are fundamental graph structures with wide applications in
network design, graph sparsification, and distance approximation. In
particular, a $4$-additive spanner is a subgraph that preserves all pairwise
distances up to an additive error of $4$. In this paper, we present a new
deterministic algorithm for constructing $4$-additive spanners that matches the
best known edge bound of $\tilde{O}(n^{7/5})$ (up to polylogarithmic factors),
while improving the running time to $\tilde{O}(\min\{mn^{3/5}, n^{11/5}\})$,
compared to the previous $\tilde{O}(mn^{3/5})$ randomized construction. Our
algorithm is not only faster in the dense regime but also fully deterministic,
conceptually simpler, and easier to implement and analyze.

</details>


### [111] [On Algorithmic Meta-Theorems for Solution Discovery: Tractability and Barriers](https://arxiv.org/abs/2510.17344)
*Nicolas Bousquet,Amer E. Mouawad,Stephanie Maaz,Naomi Nishimura,Sebastian Siebertz*

Main category: cs.DS

TL;DR: 本文研究图问题解发现的元定理，聚焦参数化复杂度和不涉及转换预算的结构参数，给出正负结果。


<details>
  <summary>Details</summary>
Motivation: 研究可在单值二阶逻辑和一阶逻辑中定义的图问题的解发现元定理。

Method: 分析MSO₁、MSO₂和FO定义的图问题，研究不同结构参数下的参数化复杂度。

Result: 算法上，MSO₂ - Discovery在树宽参数化下属于XP，MSO₁ - Discovery在邻域多样性参数化下固定参数可解；硬度上，FO - Discovery在特定参数化下W[1] - 难，MSO₁ - Discovery在带宽参数化下W[1] - 难。

Conclusion: 结果补充了预算b作为参数时的情况，对MSO和FO可定义图问题的解发现问题提供了近乎完整的元定理研究。

Abstract: Solution discovery asks whether a given (infeasible) starting configuration
to a problem can be transformed into a feasible solution using a limited number
of transformation steps. This paper investigates meta-theorems for solution
discovery for graph problems definable in monadic second-order logic (MSO$_1$
and MSO$_2$) and first-order logic (FO) where the transformation step is to
slide a token to an adjacent vertex, focusing on parameterized complexity and
structural graph parameters that do not involve the transformation budget $b$.
We present both positive and negative results. On the algorithmic side, we
prove that MSO$_2$-Discovery is in XP when parameterized by treewidth and that
MSO$_1$-Discovery is fixed-parameter tractable when parameterized by
neighborhood diversity. On the hardness side, we establish that FO-Discovery is
W[1]-hard when parameterized by modulator to stars, modulator to paths, as well
as twin cover, numbers. Additionally, we prove that MSO$_1$-Discovery is
W[1]-hard when parameterized by bandwidth. These results complement the
straightforward observation that solution discovery for the studied problems is
fixed-parameter tractable when the budget $b$ is included in the parameter (in
particular, parameterized by cliquewidth$+b$, where the cliquewidth of a graph
is at most any of the studied parameters), and provide a near-complete
(fixed-parameter tractability) meta-theorems investigation for solution
discovery problems for MSO- and FO-definable graph problems and structural
parameters larger than cliquewidth.

</details>


### [112] [Approximating Asymmetric A Priori TSP beyond the Adaptivity Gap](https://arxiv.org/abs/2510.17595)
*Manuel Christalla,Luise Puhlmann,Vera Traub*

Main category: cs.DS

TL;DR: 本文研究非对称先验TSP问题，证明其适应性差距的多项式下界，给出准多项式时间随机算法实现多对数近似比。


<details>
  <summary>Details</summary>
Motivation: 解决非对称先验TSP问题，即计算最小化随机采样活跃顶点集期望长度的路径。

Method: 通过一系列多项式时间约简，先约简到Hop - ATSP，再用有向低直径分解得到结构化实例，最后约简到有向无环图找路径问题。

Result: 证明适应性差距的多项式下界，给出准多项式时间O(log n)近似算法。

Conclusion: 准多项式时间随机算法能实现多对数近似比，低于适应性差距。

Abstract: In Asymmetric A Priori TSP (with independent activation probabilities) we are
given an instance of the Asymmetric Traveling Salesman Problem together with an
activation probability for each vertex. The task is to compute a tour that
minimizes the expected length after short-cutting to the randomly sampled set
of active vertices.
  We prove a polynomial lower bound on the adaptivity gap for Asymmetric A
Priori TSP. Moreover, we show that a poly-logarithmic approximation ratio, and
hence an approximation ratio below the adaptivity gap, can be achieved by a
randomized algorithm with quasi-polynomial running time.
  To achieve this, we provide a series of polynomial-time reductions. First we
reduce to a novel generalization of the Asymmetric Traveling Salesman Problem,
called Hop-ATSP. Next, we use directed low-diameter decompositions to obtain
structured instances, for which we then provide a reduction to a covering
problem. Eventually, we obtain a polynomial-time reduction of Asymmetric A
Priori TSP to a problem of finding a path in an acyclic digraph minimizing a
particular objective function, for which we give an O(log n)-approximation
algorithm in quasi-polynomial time.

</details>


### [113] [Near-Optimal Property Testers for Pattern Matching](https://arxiv.org/abs/2510.17645)
*Ce Jin,Tomasz Kociumaka*

Main category: cs.DS

TL;DR: 本文为精确模式匹配问题提供自适应和非自适应属性测试器，建立无条件下界证明算法复杂度最优，在不同参数区间改进先前结果，还揭示了特定区间自适应和非自适应算法的差异。


<details>
  <summary>Details</summary>
Motivation: 解决经典精确模式匹配问题的属性测试，改进已有算法的时间和查询复杂度。

Method: 设计自适应和非自适应属性测试器，并建立无条件下界。

Result: 在不同参数区间（如n=m+Θ(m)、n=m+Ω(m)、n=m+o(m)）给出复杂度结果，改进先前算法。

Conclusion: 所设计算法在时间和查询复杂度上是最优的，且在n=m+o(m)区间揭示了自适应和非自适应算法的复杂度差异。

Abstract: The classic exact pattern matching problem, given two strings -- a pattern
$P$ of length $m$ and a text $T$ of length $n$ -- asks whether $P$ occurs as a
substring of $T$. A property tester for the problem needs to distinguish (with
high probability) the following two cases for some threshold $k$: the YES case,
where $P$ occurs as a substring of $T$, and the NO case, where $P$ has Hamming
distance greater than $k$ from every substring of $T$, that is, $P$ has no
$k$-mismatch occurrence in $T$.
  In this work, we provide adaptive and non-adaptive property testers for the
exact pattern matching problem, jointly covering the whole spectrum of
parameters. We further establish unconditional lower bounds demonstrating that
the time and query complexities of our algorithms are optimal, up to
$\mathrm{polylog}\, n$ factors hidden within the $\tilde O(\cdot)$ notation
below.
  In the most studied regime of $n=m+\Theta(m)$, our non-adaptive property
tester has the time complexity of $\tilde O(n/\sqrt{k})$, and a matching lower
bound remains valid for the query complexity of adaptive algorithms. This
improves both upon a folklore solution that attains the optimal query
complexity but requires $\Omega(n)$ time, and upon the only previously known
sublinear-time property tester, by Chan, Golan, Kociumaka, Kopelowitz, and
Porat [STOC 2020], with time complexity $\tilde O(n/\sqrt[3]{k})$. The
aforementioned results remain valid for $n=m+\Omega(m)$, where our optimal
running time $\tilde O(\sqrt{nm/k}+n/k)$ improves upon the previously best time
complexity of $\tilde O(\sqrt[3]{n^2m/k}+n/k)$. In the regime of $n=m+o(m)$,
which has not been targeted in any previous work, we establish a surprising
separation between adaptive and non-adaptive algorithms, whose optimal time and
query complexities are $\tilde O(\sqrt{(n-m+1)m/k}+n/k)$ and $\tilde
O(\min(n\sqrt{n-m+1}/k,\sqrt{nm/k}+n/k))$, respectively.

</details>


### [114] [The Marked Edge Walk: A Novel MCMC Algorithm for Sampling of Graph Partitions](https://arxiv.org/abs/2510.17714)
*Atticus McWhorter,Daryl DeFord*

Main category: cs.DS

TL;DR: 提出新的MCMC算法MEW用于图分区采样，在真实数据集上有良好表现，推进灵活集合生成。


<details>
  <summary>Details</summary>
Motivation: 现有如RevReCom和MFR等算法受限于从与生成树相关的分布中采样，需更灵活的算法。

Method: 引入标记边游走（MEW）算法，在带标记边的生成树空间操作，利用可计算的转移概率用于Metropolis - Hastings算法。

Result: 在真实世界对偶图上的实验结果显示，在与生成树无关的目标分布下收敛。

Conclusion: MEW是灵活集合生成方面的一项进步。

Abstract: Novel Markov Chain Monte Carlo (MCMC) methods have enabled the generation of
large ensembles of redistricting plans through graph partitioning. However,
existing algorithms such as Reversible Recombination (RevReCom) and
Metropolized Forest Recombination (MFR) are constrained to sampling from
distributions related to spanning trees. We introduce the marked edge walk
(MEW), a novel MCMC algorithm for sampling from the space of graph partitions
under a tunable distribution. The walk operates on the space of spanning trees
with marked edges, allowing for calculable transition probabilities for use in
the Metropolis-Hastings algorithm. Empirical results on real-world dual graphs
show convergence under target distributions unrelated to spanning trees. For
this reason, MEW represents an advancement in flexible ensemble generation.

</details>


### [115] [Generalized Flow in Nearly-linear Time on Moderately Dense Graphs](https://arxiv.org/abs/2510.17740)
*Shunhua Jiang,Michael Kapralov,Lawrence Li,Aaron Sidford*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper we consider generalized flow problems where there is an
$m$-edge $n$-node directed graph $G = (V,E)$ and each edge $e \in E$ has a loss
factor $\gamma_e >0$ governing whether the flow is increased or decreased as it
crosses edge $e$. We provide a randomized $\tilde{O}( (m + n^{1.5}) \cdot
\mathrm{polylog}(\frac{W}{\delta}))$ time algorithm for solving the generalized
maximum flow and generalized minimum cost flow problems in this setting where
$\delta$ is the target accuracy and $W$ is the maximum of all costs,
capacities, and loss factors and their inverses. This improves upon the
previous state-of-the-art $\tilde{O}(m \sqrt{n} \cdot \log^2(\frac{W}{\delta})
)$ time algorithm, obtained by combining the algorithm of [Daitch-Spielman,
2008] with techniques from [Lee-Sidford, 2014]. To obtain this result we
provide new dynamic data structures and spectral results regarding the matrices
associated to generalized flows and apply them through the interior point
method framework of [Brand-Lee-Liu-Saranurak-Sidford-Song-Wang, 2021].

</details>


### [116] [Pattern Matching under Weighted Edit Distance](https://arxiv.org/abs/2510.17752)
*Panagiotis Charalampopoulos,Tomasz Kociumaka,Philip Wellnitz*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In Pattern Matching with Weighted Edits (PMWED), we are given a pattern $P$
of length $m$, a text $T$ of length $n$, a positive threshold $k$, and oracle
access to a weight function that specifies the costs of edits (depending on the
involved characters, and normalized so that the cost of each edit is at least
$1$). The goal is to compute the starting positions of all fragments of $T$
that can be obtained from $P$ with edits of total cost at most $k$. PMWED
captures typical real-world applications more accurately than its unweighted
variant (PMED), where all edits have unit costs.
  We obtain three main results:
  (a) a conceptually simple $\tilde{O}(nk)$-time algorithm for PMWED, very
different from that of Landau and Vishkin for PMED;
  (b) a significantly more complicated $\tilde{O}(n+k^{3.5} \cdot W^4\cdot
n/m)$-time algorithm for PMWED under the assumption that the weight function is
a metric with integer values between $0$ and $W$; and
  (c) an $\tilde{O}(n+k^4 \cdot n/m)$-time algorithm for PMWED for the case of
arbitrary weights.
  In the setting of metrics with small integer values, we nearly match the
state of the art for PMED where $W=1$.

</details>


### [117] [Dynamic Dyck and Tree Edit Distance: Decompositions and Reductions to String Edit Distance](https://arxiv.org/abs/2510.17799)
*Debarati Das,Jacob Gilbert,MohammadTaghi Hajiaghayi,Tomasz Kociumaka,Barna Saha*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the first dynamic algorithms for Dyck and tree edit distances with
subpolynomial update times. Dyck edit distance measures how far a parenthesis
string is from a well-parenthesized expression, while tree edit distance
quantifies the minimum number of node insertions, deletions, and substitutions
required to transform one rooted, ordered, labeled tree into another. Despite
extensive study, no prior work has addressed efficient dynamic algorithms for
these problems, which naturally arise in evolving structured data such as LaTeX
documents, JSON or XML files, and RNA secondary structures.
  Our main contribution is a set of reductions and decompositions that
transform Dyck and tree edit distance instances into efficiently maintainable
string edit distance instances, which can be approximated within a $n^{o(1)}$
factor in $n^{o(1)}$ update time. For Dyck edit distance, our reduction incurs
only polylogarithmic overheads in approximation and update time, yielding an
$n^{o(1)}$-approximation with $n^{o(1)}$ updates. For tree edit distance, we
introduce a new static reduction that improves the best-known approximation
ratio from $n^{3/4}$ to $\tilde{O}(\sqrt{n})$ and removes the restriction to
constant-degree trees. Extending this reduction dynamically achieves
$n^{1/2+o(1)}$ approximation with $n^{o(1)}$ update time.
  A key component is a dynamic maintenance algorithm for history-independent
heavy-light decompositions, of independent interest. We also provide a novel
static and dynamic decomposition achieving an $O(k \log n)$-approximation when
the tree edit distance is at most $k$. Combined with the trivial bound $k \le
n$, this yields a dynamic deterministic $O(\sqrt{n \log n})$-approximation. In
the static setting, our algorithm runs in near-linear time; dynamically, it
requires only polylogarithmic updates, improving on prior linear-time static
$O(\sqrt{n})$-approximation.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [118] [The Strongly Stable Roommates Problem and Linear Programming](https://arxiv.org/abs/2510.16385)
*Naoyuki Kamiyama*

Main category: cs.GT

TL;DR: 本文提出解决带平局的稳定室友问题中强稳定匹配存在性检查的新多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 研究带平局的稳定室友问题中强稳定匹配存在性的检查方法。

Method: 将Abeledo和Blum用于严格偏好的稳定室友问题的线性规划方法扩展到该问题。

Result: 提出了新的多项式时间算法。

Conclusion: 可利用新算法检查带平局的稳定室友问题中强稳定匹配的存在性。

Abstract: The stable roommates problem is a non-bipartite version of the stable
matching problem in a bipartite graph. In this paper, we consider the stable
roommates problem with ties. In particular, we focus on strong stability, which
is one of the main stability concepts in the stable roommates problem with
ties. We propose a new polynomial-time algorithm for the problem of checking
the existence of a strongly stable matching in the stable roommates problem
with ties. More concretely, we extend the linear programming approach of
Abeledo and Blum to the stable roommates problem with strict preferences to our
problem.

</details>


### [119] [No-Regret Online Autobidding Algorithms in First-price Auctions](https://arxiv.org/abs/2510.16869)
*Yuan Deng,Yilin Li,Wei Tang,Hanrui Zhang*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automated bidding to optimize online advertising with various constraints,
e.g. ROI constraints and budget constraints, is widely adopted by advertisers.
A key challenge lies in designing algorithms for non-truthful mechanisms with
ROI constraints. While prior work has addressed truthful auctions or
non-truthful auctions with weaker benchmarks, this paper provides a significant
improvement: We develop online bidding algorithms for repeated first-price
auctions with ROI constraints, benchmarking against the optimal randomized
strategy in hindsight. In the full feedback setting, where the maximum
competing bid is observed, our algorithm achieves a near-optimal
$\widetilde{O}(\sqrt{T})$ regret bound, and in the bandit feedback setting
(where the bidder only observes whether the bidder wins each auction), our
algorithm attains $\widetilde{O}(T^{3/4})$ regret bound.

</details>


### [120] [Convergence of Regret Matching in Potential Games and Constrained Optimization](https://arxiv.org/abs/2510.17067)
*Ioannis Anagnostides,Emanuel Tewolde,Brian Hu Zhang,Ioannis Panageas,Vincent Conitzer,Tuomas Sandholm*

Main category: cs.GT

TL;DR: 本文研究后悔匹配算法（RM）及其变体RM+，证明交替RM+收敛到ε - KKT点，给出复杂度，并给出RM在双玩家潜在博弈中收敛的下界。


<details>
  <summary>Details</summary>
Motivation: 现有理论对RM在两人零和博弈之外的收敛性了解甚少，如在潜在博弈中是否收敛到纳什均衡是二十年的开放问题，且在一般约束优化问题中RM变体表现好但缺乏理论分析。

Method: 将KKT间隙与累积后悔联系起来，分析RM+在特定区域的性质，给出复杂度；通过建立下界对比RM和RM+。

Result: 交替RM+在O_ε(1/ε^4)次迭代后收敛到ε - KKT点，复杂度可改进到O_ε(1/ε^2)；RM在双玩家潜在博弈中可能需指数次迭代才能得到粗略近似解。

Conclusion: RM+是合理快速的一阶优化器，首次给出RM和RM+的最坏情况分离，表明在潜在博弈中收敛到粗相关均衡比收敛到纳什均衡快指数倍。

Abstract: Regret matching (RM} -- and its modern variants -- is a foundational online
algorithm that has been at the heart of many AI breakthrough results in solving
benchmark zero-sum games, such as poker. Yet, surprisingly little is known so
far in theory about its convergence beyond two-player zero-sum games. For
example, whether regret matching converges to Nash equilibria in potential
games has been an open problem for two decades. Even beyond games, one could
try to use RM variants for general constrained optimization problems. Recent
empirical evidence suggests that they -- particularly regret matching$^+$
(RM$^+$) -- attain strong performance on benchmark constrained optimization
problems, outperforming traditional gradient descent-type algorithms.
  We show that alternating RM$^+$ converges to an $\epsilon$-KKT point after
$O_\epsilon(1/\epsilon^4)$ iterations, establishing for the first time that it
is a sound and fast first-order optimizer. Our argument relates the KKT gap to
the accumulated regret, two quantities that are entirely disparate in general
but interact in an intriguing way in our setting, so much so that when regrets
are bounded, our complexity bound improves all the way to
$O_\epsilon(1/\epsilon^2)$. From a technical standpoint, while RM$^+$ does not
have the usual one-step improvement property in general, we show that it does
in a certain region that the algorithm will quickly reach and remain in
thereafter. In sharp contrast, our second main result establishes a lower
bound: RM, with or without alternation, can take an exponential number of
iterations to reach a crude approximate solution even in two-player potential
games. This represents the first worst-case separation between RM and RM$^+$.
Our lower bound shows that convergence to coarse correlated equilibria in
potential games is exponentially faster than convergence to Nash equilibria.

</details>


### [121] [Eliciting Truthful Feedback for Preference-Based Learning via the VCG Mechanism](https://arxiv.org/abs/2510.17285)
*Leo Landolt,Anna Maddux,Andreas Schlaginhaufen,Saurabh Vaishampayan,Maryam Kamgarpour*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study resource allocation problems in which a central planner allocates
resources among strategic agents with private cost functions in order to
minimize a social cost, defined as an aggregate of the agents' costs. This
setting poses two main challenges: (i) the agents' cost functions may be
unknown to them or difficult to specify explicitly, and (ii) agents may
misreport their costs strategically. To address these challenges, we propose an
algorithm that combines preference-based learning with Vickrey-Clarke-Groves
(VCG) payments to incentivize truthful reporting. Our algorithm selects
informative preference queries via D-optimal design, estimates cost parameters
through maximum likelihood, and computes VCG allocations and payments based on
these estimates. In a one-shot setting, we prove that the mechanism is
approximately truthful, individually rational, and efficient up to an error of
$\tilde{\mathcal O}(K^{-1/2})$ for $K$ preference queries per agent. In an
online setting, these guarantees hold asymptotically with sublinear regret at a
rate of $\tilde{\mathcal O}(T^{2/3})$ after $T$ rounds. Finally, we validate
our approach through a numerical case study on demand response in local
electricity markets.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [122] [Investigating the Association Between Text-Based Indications of Foodborne Illness from Yelp Reviews and New York City Health Inspection Outcomes (2023)](https://arxiv.org/abs/2510.16334)
*Eden Shaveet,Crystal Su,Daniel Hsu,Luis Gravano*

Main category: cs.IR

TL;DR: 分析Yelp评论信号与纽约市卫生部门餐厅检查结果的相关性，发现相关性极小。


<details>
  <summary>Details</summary>
Motivation: 食源性疾病通过餐厅传播，正式渠道疾病报告有限，社交媒体内容可提供公共卫生信号，因此分析Yelp评论信号与官方餐厅检查结果的关系。

Method: 使用Hierarchical Sigmoid Attention Network (HSAN) 分类器分析Yelp评论信号，与纽约市卫生部门2023年餐厅检查结果对比，在普查区层面评估相关性、按C级餐厅 prevalence比较HSAN分数分布、绘制纽约市空间模式图。

Result: 普查区层面HSAN信号与检查分数相关性极小，C级餐厅数量无显著差异。

Conclusion: 讨论了影响并规划下一步地址层面分析。

Abstract: Foodborne illnesses are gastrointestinal conditions caused by consuming
contaminated food. Restaurants are critical venues to investigate outbreaks
because they share sourcing, preparation, and distribution of foods. Public
reporting of illness via formal channels is limited, whereas social media
platforms host abundant user-generated content that can provide timely public
health signals. This paper analyzes signals from Yelp reviews produced by a
Hierarchical Sigmoid Attention Network (HSAN) classifier and compares them with
official restaurant inspection outcomes issued by the New York City Department
of Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at
the Census tract level, compare distributions of HSAN scores by prevalence of
C-graded restaurants, and map spatial patterns across NYC. We find minimal
correlation between HSAN signals and inspection scores at the tract level and
no significant differences by number of C-graded restaurants. We discuss
implications and outline next steps toward address-level analyses.

</details>


### [123] [Blending Learning to Rank and Dense Representations for Efficient and Effective Cascades](https://arxiv.org/abs/2510.16393)
*Franco Maria Nardini,Raffaele Perego,Nicola Tonellotto,Salvatore Trani*

Main category: cs.IR

TL;DR: 研究结合词法和神经相关性信号用于即席段落检索，用LTR模型融合特征，实验显示能提升性能且对效率影响小。


<details>
  <summary>Details</summary>
Motivation: 探索词法和神经相关性信号在即席段落检索中的利用，提升检索效果。

Method: 在大规模训练数据集上，将MS - MARCO查询和段落的密集神经表示与253个手工词法特征互补整合，用基于决策树森林的LTR模型学习融合相关性信号，采用流水线架构评估。

Result: 在公开资源上用先进的密集检索器实验，显著提升端到端排名性能，nDCG@10最多提升11%，平均查询延迟仅增加4.3%。

Conclusion: 无缝结合两种不同类型信号对检索效果有优势。

Abstract: We investigate the exploitation of both lexical and neural relevance signals
for ad-hoc passage retrieval. Our exploration involves a large-scale training
dataset in which dense neural representations of MS-MARCO queries and passages
are complemented and integrated with 253 hand-crafted lexical features
extracted from the same corpus. Blending of the relevance signals from the two
different groups of features is learned by a classical Learning-to-Rank (LTR)
model based on a forest of decision trees. To evaluate our solution, we employ
a pipelined architecture where a dense neural retriever serves as the first
stage and performs a nearest-neighbor search over the neural representations of
the documents. Our LTR model acts instead as the second stage that re-ranks the
set of candidates retrieved by the first stage to enhance effectiveness. The
results of reproducible experiments conducted with state-of-the-art dense
retrievers on publicly available resources show that the proposed solution
significantly enhances the end-to-end ranking performance while relatively
minimally impacting efficiency. Specifically, we achieve a boost in nDCG@10 of
up to 11% with an increase in average query latency of only 4.3%. This confirms
the advantage of seamlessly combining two distinct families of signals that
mutually contribute to retrieval effectiveness.

</details>


### [124] [FRONTIER-RevRec: A Large-scale Dataset for Reviewer Recommendation](https://arxiv.org/abs/2510.16597)
*Qiyao Peng,Chen Wang,Yinghui Wang,Hongtao Liu,Xuan Guo,Wenjun Wang*

Main category: cs.IR

TL;DR: 提出FRONTIER - RevRec数据集用于审稿人推荐研究，发现基于内容的方法表现更好，该数据集可推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有审稿人推荐研究受限于缺乏高质量基准数据集，规模、学科范围及方法对比分析不足。

Method: 构建FRONTIER - RevRec数据集，基于该数据集进行综合评估和结构分析。

Result: 基于内容的方法显著优于协同过滤，利用语言模型的方法能有效捕捉论文内容与审稿人专业知识的语义对齐，确定了优化聚合策略。

Conclusion: FRONTIER - RevRec数据集可作为综合基准推动审稿人推荐研究和有效学术同行评审系统的发展。

Abstract: Reviewer recommendation is a critical task for enhancing the efficiency of
academic publishing workflows. However, research in this area has been
persistently hindered by the lack of high-quality benchmark datasets, which are
often limited in scale, disciplinary scope, and comparative analyses of
different methodologies. To address this gap, we introduce FRONTIER-RevRec, a
large-scale dataset constructed from authentic peer review records (2007-2025)
from the Frontiers open-access publishing platform
https://www.frontiersin.org/. The dataset contains 177941 distinct reviewers
and 478379 papers across 209 journals spanning multiple disciplines including
clinical medicine, biology, psychology, engineering, and social sciences. Our
comprehensive evaluation on this dataset reveals that content-based methods
significantly outperform collaborative filtering. This finding is explained by
our structural analysis, which uncovers fundamental differences between
academic recommendation and commercial domains. Notably, approaches leveraging
language models are particularly effective at capturing the semantic alignment
between a paper's content and a reviewer's expertise. Furthermore, our
experiments identify optimal aggregation strategies to enhance the
recommendation pipeline. FRONTIER-RevRec is intended to serve as a
comprehensive benchmark to advance research in reviewer recommendation and
facilitate the development of more effective academic peer review systems. The
FRONTIER-RevRec dataset is available at:
https://anonymous.4open.science/r/FRONTIER-RevRec-5D05.

</details>


### [125] [Right Answer at the Right Time - Temporal Retrieval-Augmented Generation via Graph Summarization](https://arxiv.org/abs/2510.16715)
*Zulun Zhu,Haoyu Liu,Mengke He,Siqiang Luo*

Main category: cs.IR

TL;DR: 提出STAR - RAG框架用于时间知识图谱问答，减少计算成本，实验显示准确率提升且消耗更少token。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法主要是语义的，忽略显式时间约束，导致答案时间不一致和token使用过多。

Method: 提出STAR - RAG框架，构建时间对齐规则图并在图上进行传播以缩小搜索空间。

Result: 在真实世界时间KG数据集上实验，相比强大的GraphRAG基线，答案准确率提高且消耗更少token。

Conclusion: STAR - RAG框架无需大量模型训练和微调，降低计算成本，简化部署。

Abstract: Question answering in temporal knowledge graphs requires retrieval that is
both time-consistent and efficient. Existing RAG methods are largely semantic
and typically neglect explicit temporal constraints, which leads to
time-inconsistent answers and inflated token usage. We propose STAR-RAG, a
temporal GraphRAG framework that relies on two key ideas: building a
time-aligned rule graph and conducting propagation on this graph to narrow the
search space and prioritize semantically relevant, time-consistent evidence.
This design enforces temporal proximity during retrieval, reduces the candidate
set of retrieval results, and lowers token consumption without sacrificing
accuracy. Compared with existing temporal RAG approaches, STAR-RAG eliminates
the need for heavy model training and fine-tuning, thereby reducing
computational cost and significantly simplifying deployment.Extensive
experiments on real-world temporal KG datasets show that our method achieves
improved answer accuracy while consuming fewer tokens than strong GraphRAG
baselines.

</details>


### [126] [Exact Nearest-Neighbor Search on Energy-Efficient FPGA Devices](https://arxiv.org/abs/2510.16736)
*Patrizio Dazzi,William Guglielmo,Franco Maria Nardini,Raffaele Perego,Salvatore Trani*

Main category: cs.IR

TL;DR: 本文研究用FPGA设备在高维潜在空间进行节能精确kNN搜索，提出两种节能方案，实验显示其在吞吐量、延迟和能耗上优于CPU方案。


<details>
  <summary>Details</summary>
Motivation: 支持基于神经编码器模型的学习表示的广泛应用，使其更环保和普及。

Method: 提出采用相同FPGA底层配置的两种节能解决方案，一是在流数据集上并行处理批量查询以最大化吞吐量，二是在内存数据集上并行处理每个kNN查询以最小化延迟。

Result: 在公开图像和文本数据集上的实验表明，FPGA解决方案在吞吐量、延迟和能耗方面优于基于CPU的竞品，吞吐量和延迟有高达16.6倍的提升，能效有高达11.9倍的节能。

Conclusion: 所提出的FPGA节能解决方案在高维潜在空间的精确kNN搜索中表现出色，具有显著优势。

Abstract: This paper investigates the usage of FPGA devices for energy-efficient exact
kNN search in high-dimension latent spaces. This work intercepts a relevant
trend that tries to support the increasing popularity of learned
representations based on neural encoder models by making their large-scale
adoption greener and more inclusive. The paper proposes two different
energy-efficient solutions adopting the same FPGA low-level configuration. The
first solution maximizes system throughput by processing the queries of a batch
in parallel over a streamed dataset not fitting into the FPGA memory. The
second minimizes latency by processing each kNN incoming query in parallel over
an in-memory dataset. Reproducible experiments on publicly available image and
text datasets show that our solution outperforms state-of-the-art CPU-based
competitors regarding throughput, latency, and energy consumption.
Specifically, experiments show that the proposed FPGA solutions achieve the
best throughput in terms of queries per second and the best-observed latency
with scale-up factors of up to 16.6X. Similar considerations can be made
regarding energy efficiency, where results show that our solutions can achieve
up to 11.9X energy saving w.r.t. strong CPU-based competitors.

</details>


### [127] [An Efficient Framework for Whole-Page Reranking via Single-Modal Supervision](https://arxiv.org/abs/2510.16803)
*Zishuai Zhang,Sihao Yu,Wenyi Xie,Ying Nie,Junfeng Wang,Zhiming Zheng,Dawei Yin,Hainan Zhang*

Main category: cs.IR

TL;DR: 本文提出SMAR框架，利用单模态排序器指导模态相关性对齐，用少量全页标注实现全页重排序，减少标注成本并提升排序效果。


<details>
  <summary>Details</summary>
Motivation: 现有全页重排序方法依赖大规模人工标注数据，成本高、耗时长，如何提升性能同时降低标注成本是优化搜索引擎结果页面的关键挑战。

Method: 先在各模态特定数据上训练高质量单模态排序器，为每个查询选择其输出子集构建候选页面并进行页面级人工标注，用有限标注训练全页重排序器并确保与单模态偏好一致。

Result: 在Qilin和百度数据集上，SMAR减少约70 - 90%标注成本，显著提升排序效果；百度APP的离线和在线A/B测试显示在标准排序指标和用户体验指标上有显著提升。

Conclusion: SMAR方法在实际搜索场景中有效且具有实用价值。

Abstract: The whole-page reranking plays a critical role in shaping the user experience
of search engines, which integrates retrieval results from multiple modalities,
such as documents, images, videos, and LLM outputs. Existing methods mainly
rely on large-scale human-annotated data, which is costly to obtain and
time-consuming. This is because whole-page annotation is far more complex than
single-modal: it requires assessing the entire result page while accounting for
cross-modal relevance differences. Thus, how to improve whole-page reranking
performance while reducing annotation costs is still a key challenge in
optimizing search engine result pages(SERP). In this paper, we propose SMAR, a
novel whole-page reranking framework that leverages strong Single-modal rankers
to guide Modal-wise relevance Alignment for effective Reranking, using only
limited whole-page annotation to outperform fully-annotated reranking models.
Specifically, high-quality single-modal rankers are first trained on data
specific to their respective modalities. Then, for each query, we select a
subset of their outputs to construct candidate pages and perform human
annotation at the page level. Finally, we train the whole-page reranker using
these limited annotations and enforcing consistency with single-modal
preferences to maintain ranking quality within each modality. Experiments on
the Qilin and Baidu datasets demonstrate that SMAR reduces annotation costs by
about 70-90\% while achieving significant ranking improvements compared to
baselines. Further offline and online A/B testing on Baidu APPs also shows
notable gains in standard ranking metrics as well as user experience
indicators, fully validating the effectiveness and practical value of our
approach in real-world search scenarios.

</details>


### [128] [The Layout Is the Model: On Action-Item Coupling in Generative Recommendation](https://arxiv.org/abs/2510.16804)
*Xiaokai Wei,Jiajun Wu,Daiyao Yi,Reza Shirkavand,Michelle Gong*

Main category: cs.IR

TL;DR: 本文对生成式推荐（GR）模型的令牌布局进行统一研究，提出设计原则，设计非交错方法LAC，实验验证原则并表明LAC在低计算量下有竞争力。


<details>
  <summary>Details</summary>
Motivation: 在GR模型中，项目和动作的布局会影响模型信息利用和泛化能力，需要研究合理的令牌布局。

Method: 提出三条设计原则，设计非交错方法Lagged Action Conditioning (LAC)，并在公共数据集和生产日志上进行综合实验。

Result: 实验验证了设计原则，LAC在较低FLOPs下达到有竞争力或更优的质量。

Conclusion: 研究为构建准确高效的GR系统提供了可行指导。

Abstract: Generative Recommendation (GR) models treat a user's interaction history as a
sequence to be autoregressively predicted. When both items and actions (e.g.,
watch time, purchase, comment) are modeled, the layout-the ordering and
visibility of item/action tokens-critically determines what information the
model can use and how it generalizes. We present a unified study of token
layouts for GR grounded in first principles: (P1) maximize item/action signal
in both input/output space, (P2) preserve the conditioning relationship "action
given item" and (P3) no information leakage.
  While interleaved layout (where item and action occupy separate tokens)
naturally satisfies these principles, it also bloats sequence length with
larger training/inference cost. On the non-interleaved front, we design a novel
and effective approach, Lagged Action Conditioning (LAC), which appears strange
on the surface but aligns well with the design principles to yield strong
accuracy. Comprehensive experiments on public datasets and large-scale
production logs evaluate different layout options and empirically verifies the
design principles. Our proposed non-interleaved method, LAC, achieves
competitive or superior quality at substantially lower FLOPs than interleaving.
Our findings offer actionable guidance for assembling GR systems that are both
accurate and efficient.

</details>


### [129] [Towards Context-aware Reasoning-enhanced Generative Searching in E-commerce](https://arxiv.org/abs/2510.16925)
*Zhiding Liu,Ben Chen,Mingyue Cheng,Enchong Chen,Li Li,Chenyi Lei,Wenwu Ou,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 提出上下文感知推理增强生成搜索框架用于电商平台搜索推荐，实验证明性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有搜索方法在整合上下文信息、捕捉用户意图方面存在局限，需更好理解复杂上下文。

Method: 提出框架统一用户和物品上下文为文本表示并对齐；引入自进化后训练范式增强模型推理能力；提出GRPO去偏变体改进排序性能。

Result: 在真实电商平台搜索日志数据上实验，该方法性能优于强基线。

Conclusion: 该方法对基于搜索的推荐有效。

Abstract: Search-based recommendation is one of the most critical application scenarios
in e-commerce platforms. Users' complex search contexts--such as spatiotemporal
factors, historical interactions, and current query's information--constitute
an essential part of their decision-making, reflecting implicit preferences
that complement explicit query terms. Modeling such rich contextual signals and
their intricate associations with candidate items remains a key challenge.
Although numerous efforts have been devoted to building more effective search
methods, existing approaches still show limitations in integrating contextual
information, which hinders their ability to fully capture user intent.
  To address these challenges, we propose a context-aware reasoning-enhanced
generative search framework for better \textbf{understanding the complicated
context}. Specifically, the framework first unifies heterogeneous user and item
contexts into textual representations or text-based semantic identifiers and
aligns them. To overcome the lack of explicit reasoning trajectories, we
introduce a self-evolving post-training paradigm that iteratively combines
supervised fine-tuning and reinforcement learning to progressively enhance the
model's reasoning capability. In addition, we identify potential biases in
existing RL algorithms when applied to search scenarios and present a debiased
variant of GRPO to improve ranking performance. Extensive experiments on search
log data collected from a real-world e-commerce platform demonstrate that our
approach achieves superior performance compared with strong baselines,
validating its effectiveness for search-based recommendation.

</details>


### [130] [DSEBench: A Test Collection for Explainable Dataset Search with Examples](https://arxiv.org/abs/2510.17228)
*Qing Shi,Jing He,Qiaosheng Chen,Gong Cheng*

Main category: cs.IR

TL;DR: 研究数据集搜索带示例（DSE）及可解释DSE任务，构建DSEBench测试集，用大语言模型生成标注，在DSEBench上建立大量基线。


<details>
  <summary>Details</summary>
Motivation: 当前数据集搜索范式要么基于关键词查询，要么基于目标数据集相似性，为结合两种信息需求，研究更广义的DSE及可解释DSE任务。

Method: 构建DSEBench测试集，用大语言模型生成标注，适配和评估多种稀疏、密集和基于大语言模型的检索、重排和解释方法。

Result: 建立了DSEBench测试集，可用于评估可解释DSE。

Conclusion: 完成了可解释DSE的研究基础工作，如构建测试集和建立基线。

Abstract: Dataset search has been an established information retrieval task. Current
paradigms either retrieve datasets that are relevant to a keyword query or find
datasets that are similar to an input target dataset. To allow for their
combined specification of information needs, in this article, we investigate
the more generalized task of Dataset Search with Examples (DSE) and further
extend it to Explainable DSE that requires identifying the metadata and content
fields of a dataset that indicate its relevance to the query and similarity to
the target datasets. To facilitate this research, we construct DSEBench, a test
collection that provides high-quality dataset- and field-level annotations to
enable the evaluation of explainable DSE. We also employ a large language model
to generate numerous annotations to be used for training. We establish
extensive baselines on DSEBench by adapting and evaluating a variety of sparse,
dense, and LLM-based retrieval, reranking, and explanation methods.

</details>


### [131] [On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders](https://arxiv.org/abs/2510.17245)
*Wenyu Mao,Jiancan Wu,Guoqing Hu,Wei Ji,Xiang Wang*

Main category: cs.IR

TL;DR: 提出TA - Rec框架解决扩散模型用于生成式顺序推荐时计算效率和推荐效果的权衡问题，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 扩散模型用于生成式顺序推荐的多步过程存在离散化误差，导致计算效率和推荐效果的权衡。

Method: 提出TA - Rec两阶段框架，预训练时用Temporal Consistency Regularization平滑去噪函数实现一步生成，微调时用Adaptive Preference Alignment缓解轨迹偏差。

Result: 广泛实验表明TA - Rec两阶段目标有效缓解离散化误差带来的权衡问题。

Conclusion: TA - Rec能提升基于扩散的推荐器的效率和效果。

Abstract: Diffusion models have emerged as a powerful paradigm for generative
sequential recommendation, which typically generate next items to recommend
guided by user interaction histories with a multi-step denoising process.
However, the multi-step process relies on discrete approximations, introducing
discretization error that creates a trade-off between computational efficiency
and recommendation effectiveness. To address this trade-off, we propose TA-Rec,
a two-stage framework that achieves one-step generation by smoothing the
denoising function during pretraining while alleviating trajectory deviation by
aligning with user preferences during fine-tuning. Specifically, to improve the
efficiency without sacrificing the recommendation performance, TA-Rec pretrains
the denoising model with Temporal Consistency Regularization (TCR), enforcing
the consistency between the denoising results across adjacent steps. Thus, we
can smooth the denoising function to map the noise as oracle items in one step
with bounded error. To further enhance effectiveness, TA-Rec introduces
Adaptive Preference Alignment (APA) that aligns the denoising process with user
preference adaptively based on preference pair similarity and timesteps.
Extensive experiments prove that TA-Rec's two-stage objective effectively
mitigates the discretization errors-induced trade-off, enhancing both
efficiency and effectiveness of diffusion-based recommenders.

</details>


### [132] [How role-play shapes relevance judgment in zero-shot LLM rankers](https://arxiv.org/abs/2510.17535)
*Yumeng Wang,Jirui Qi,Catherine Chen,Panagiotis Eustratiadis,Suzan Verberne*

Main category: cs.IR

TL;DR: 研究角色扮演变化对零样本大语言模型排序器的影响，揭示角色扮演信号编码位置等


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为零样本排序器性能对提示表述敏感，角色扮演提示有优势但机制和多样性未充分探索

Method: 采用机制可解释性中的因果干预技术追踪角色扮演信息如何影响大语言模型相关性判断

Result: 精心设计角色描述对大语言模型排序质量影响大；角色扮演信号主要在早期层编码，在中间层与任务指令交互，与查询或文档表示交互有限；确定一组对基于角色的相关性至关重要的注意力头

Conclusion: 研究揭示了大语言模型排序中角色扮演的内部机制，为信息检索及其他领域设计更有效提示提供指导，指出在零样本应用中利用角色扮演的更广泛机会

Abstract: Large Language Models (LLMs) have emerged as promising zero-shot rankers, but
their performance is highly sensitive to prompt formulation. In particular,
role-play prompts, where the model is assigned a functional role or identity,
often give more robust and accurate relevance rankings. However, the mechanisms
and diversity of role-play effects remain underexplored, limiting both
effective use and interpretability. In this work, we systematically examine how
role-play variations influence zero-shot LLM rankers. We employ causal
intervention techniques from mechanistic interpretability to trace how
role-play information shapes relevance judgments in LLMs. Our analysis reveals
that (1) careful formulation of role descriptions have a large effect on the
ranking quality of the LLM; (2) role-play signals are predominantly encoded in
early layers and communicate with task instructions in middle layers, while
receiving limited interaction with query or document representations.
Specifically, we identify a group of attention heads that encode information
critical for role-conditioned relevance. These findings not only shed light on
the inner workings of role-play in LLM ranking but also offer guidance for
designing more effective prompts in IR and beyond, pointing toward broader
opportunities for leveraging role-play in zero-shot applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [133] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: 提出针对Lean和mathlib的语义搜索引擎Lean Finder，通过分析Lean讨论语义等方法，相比之前搜索引擎和GPT - 4o有超30%相对提升，且兼容LLM定理证明器。


<details>
  <summary>Details</summary>
Motivation: 形式定理证明中定位相关定理困难、Lean 4语言学习曲线陡峭，现有搜索工具依赖非形式化且忽略与用户查询不匹配问题。

Method: 分析和聚类公共Lean讨论语义，在模拟用户意图的合成查询上微调文本嵌入，用多样反馈信号使Lean Finder符合数学家偏好。

Result: 在实际查询、非形式化陈述和证明状态评估中，Lean Finder比之前搜索引擎和GPT - 4o实现超30%相对提升，且兼容LLM定理证明器。

Conclusion: Lean Finder能有效解决现有问题，提升搜索效果，可促进形式定理证明发展。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [134] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: 提出LS - OGD自适应控制框架应对多模态学习中的概念漂移问题，证明其误差有界及收敛性，展示其隔离和缓解模态漂移影响的能力。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统在非平稳环境中因概念漂移、模态特定漂移及缺乏稳定自适应机制而性能下降，需要解决该问题。

Method: 引入LS - OGD框架，利用在线控制器根据检测到的漂移和预测误差动态调整模型学习率和不同数据模态的融合权重。

Result: 证明在有界漂移条件下，LS - OGD系统的预测误差一致最终有界，漂移停止时误差收敛到零；自适应融合策略能有效隔离和缓解严重模态特定漂移的影响。

Conclusion: 这些理论保证为开发可靠且持续自适应的多模态学习系统奠定了原则基础。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [135] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: 提出BEACON框架以解决大语言模型多响应采样时何时停止的问题，可减少采样并保证质量。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型多响应采样时平衡准确性提升和效率的关键挑战，即决定何时停止生成新样本。

Method: 引入基于贝叶斯学习顺序搜索的BEACON自适应采样框架，实时更新奖励分布后验信念，权衡预期收益和计算成本决定停止时机。

Result: 理论上有最优性保证和实际可行性，实证表明BEACON可减少达80%的平均采样，同时维持响应质量。

Conclusion: BEACON有实用性，为高效偏好数据生成提供帮助，为未来研究提供可行见解。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [136] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: 现有有害模因检测方法有局限性，本文提出PatMD方法，通过识别误判风险模式指导多模态大语言模型，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 互联网模因被用于传达有害观点，现有检测方法难以处理隐式表达，常出现误判。

Method: 构建知识库，将模因解构为误判风险模式，针对目标模因检索相关模式动态指导多模态大语言模型推理。

Result: 在6626个模因的5个有害检测任务中，PatMD在F1分数上平均提高8.30%，准确率提高7.71%。

Conclusion: PatMD具有强泛化性，提升了有害模因检测能力。

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [137] [WaveNet's Precision in EEG Classification](https://arxiv.org/abs/2510.15947)
*Casper van Laar,Khubaib Ahmed*

Main category: cs.LG

TL;DR: 本研究引入基于WaveNet的深度学习模型对EEG信号分类，在准确率上超越之前模型。


<details>
  <summary>Details</summary>
Motivation: 传统依赖专家人工审查的EEG信号分类方法因记录复杂性和数量增长而不实用。

Method: 利用公开数据集，按70/20/10划分209,232个样本进行训练、验证和测试，与TCN基线对比，详述预处理流程。

Result: 模型分类准确率超基于CNN和LSTM的方法，能高精度区分噪声和伪影，但生理和病理信号有一定误分类。

Conclusion: WaveNet架构适合EEG数据，能捕捉时间依赖，预处理流程有助于模型泛化。

Abstract: This study introduces a WaveNet-based deep learning model designed to
automate the classification of EEG signals into physiological, pathological,
artifact, and noise categories. Traditional methods for EEG signal
classification, which rely on expert visual review, are becoming increasingly
impractical due to the growing complexity and volume of EEG recordings.
Leveraging a publicly available annotated dataset from Mayo Clinic and St.
Anne's University Hospital, the WaveNet model was trained, validated, and
tested on 209,232 samples with a 70/20/10 percent split. The model achieved a
classification accuracy exceeding previous CNN and LSTM-based approaches, and
was benchmarked against a Temporal Convolutional Network (TCN) baseline.
Notably, the model distinguishes noise and artifacts with high precision,
although it reveals a modest but explainable degree of misclassification
between physiological and pathological signals, reflecting inherent clinical
overlap. WaveNet's architecture, originally developed for raw audio synthesis,
is well suited for EEG data due to its use of dilated causal convolutions and
residual connections, enabling it to capture both fine-grained and long-range
temporal dependencies. The research also details the preprocessing pipeline,
including dynamic dataset partitioning and normalization steps that support
model generalization.

</details>


### [138] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 研究提出利用按键动力学作为帕金森病远程筛查和监测的生物标志物，经多阶段方法验证，部分模型表现出色，凸显其早期检测潜力。


<details>
  <summary>Details</summary>
Motivation: 帕金森病影响人数多且早期诊断难，传统评估有局限，需新方法用于早期诊断和监测。

Method: 提出新流程，包括数据预处理、预训练深度学习架构、微调模型和外部验证。

Result: 混合卷积 - 循环和基于变压器的模型外部验证性能强，AUC - ROC 超 90%，F1 分数超 70%，时间卷积模型 AUC - ROC 达 91.14% 。

Conclusion: 按键动力学可作为可靠数字生物标志物，为帕金森病早期检测和持续监测提供新途径。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [139] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 本文研究将扩散模型滤波算法EnSF用于实时野火蔓延预测的数据同化问题，数值研究表明其有优越的准确性、稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 野火破坏性增强且控制成本高，有效管理野火需要准确实时的火势蔓延预测，数据同化对提升预测准确性至关重要。

Method: 应用基于扩散模型的滤波算法EnSF到实时野火蔓延预测的数据同化问题中，并给出技术细节。

Result: 数值研究表明EnSF在准确性、稳定性和计算效率上表现优越。

Conclusion: EnSF是一种用于野火数据同化的可靠实用方法，代码已公开。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [140] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 提出数据集可靠性评分问题，引入Gram行列式分数衡量数据质量，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决在无真实标签情况下评估数据集可靠性的问题。

Method: 定义基于真实标签的排序，提出Gram行列式分数衡量观察数据和实验结果经验分布向量张成的体积。

Result: Gram行列式分数保留了基于真实标签的可靠性排序，具有实验无关性。

Conclusion: Gram行列式分数能有效衡量不同观察过程中的数据质量。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [141] [How Good Are LLMs at Processing Tool Outputs?](https://arxiv.org/abs/2510.15955)
*Kiran Kate,Yara Rizk,Poulami Ghosh,Ashu Gulati,Tathagata Chakraborti,Zidane Wright,Mayank Agarwal*

Main category: cs.LG

TL;DR: 研究大语言模型处理工具返回的JSON响应能力，创建数据集评估模型，发现JSON处理难，处理策略因多种因素而异且影响性能。


<details>
  <summary>Details</summary>
Motivation: 现实任务自动化中，大语言模型调用工具返回复杂JSON响应需处理，此能力研究不足。

Method: 创建工具响应处理任务数据集，用多种提示方法评估15个开源和闭源模型。

Result: 即使前沿模型，JSON处理仍困难；最佳响应处理策略取决于工具输出性质、大小和推理复杂度；处理方法差异致性能差异3% - 50%。

Conclusion: JSON处理对大语言模型是难题，处理策略需综合考虑多种因素。

Abstract: Most realistic task automation problems require large language models (LLMs)
to call tools, which often return complex JSON responses. These responses must
be further processed to derive the information necessary for task completion.
The ability of LLMs to do so is under-studied. In this paper, we study the tool
response processing task and LLMs' abilities to process structured (JSON)
responses. We created a dataset for this task, and evaluated 15 open and closed
weight models using multiple prompting approaches. Our results show that JSON
processing remains a difficult task even for frontier models across multiple
prompting strategies. The optimal response processing strategy depends on both
the nature and size of the tool outputs, as well as the complexity of the
required reasoning. Variations in processing approaches can lead to performance
differences ranging from 3\% to 50\%.

</details>


### [142] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [143] [Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling](https://arxiv.org/abs/2510.15960)
*Sana Kordoghli,Abdelhakim Settar,Oumayma Belaati,Mohammad Alkhatib*

Main category: cs.LG

TL;DR: 研究基于食物的生物质热解转化，结合AI优化过程，分析不同生物质资源及混合比例热解特性，确定最佳氢气产量和动力学模型。


<details>
  <summary>Details</summary>
Motivation: 推进可持续能源和废物管理策略，探索未充分利用的生物质资源用于可持续制氢的潜力。

Method: 对纯DS、SCG及不同混合比例样品进行多种分析，采用等转化率方法进行动力学建模，用木质纤维素数据训练LSTM模型。

Result: Blend 3氢气产量潜力高但活化能最高，Blend 1活化能最佳；KAS为最准确的动力学模型；LSTM模型预测TGA曲线准确性极高。

Conclusion: 研究提供了对热解过程的详细理解，强调了AI在热解过程建模和优化中的作用。

Abstract: This work contributes to advancing sustainable energy and waste management
strategies by investigating the thermochemical conversion of food-based biomass
through pyrolysis, highlighting the role of artificial intelligence (AI) in
enhancing process modelling accuracy and optimization efficiency. The main
objective is to explore the potential of underutilized biomass resources, such
as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen
production. Specifically, it aims to optimize the pyrolysis process while
evaluating the performance of these resources both individually and as blends.
Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC
analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS
- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential
but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1
exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic
modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS
as the most accurate. These approaches provide a detailed understanding of the
pyrolysis process, with particular emphasis on the integration of artificial
intelligence. An LSTM model trained with lignocellulosic data predicted TGA
curves with exceptional accuracy (R^2: 0.9996-0.9998).

</details>


### [144] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 针对青少年非法药物使用检测问题，提出LAMI框架，实验显示其预测准确性优于基线，还能揭示有意义的行为结构和心理社会途径。


<details>
  <summary>Details</summary>
Motivation: 现有建模方法在检测青少年非法药物使用时，将调查变量独立处理，忽略了它们之间潜在的相互关联结构，因此需要改进。

Method: 提出LAMI框架，将个体反应表示为关系图，通过专门的图结构学习层学习潜在连接，并集成大语言模型生成基于图结构和调查语义的自然语言解释。

Result: 在YRBS和NSDUH数据集上的实验表明，LAMI在预测准确性上优于竞争基线；可解释性分析显示它能揭示有意义的行为子结构和心理社会途径。

Conclusion: LAMI是一个有效的用于检测青少年非法药物使用和解释行为风险因素的框架。

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [145] [CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models](https://arxiv.org/abs/2510.15962)
*Zhuxuanzi Wang,Mingqiao Mo,Xi Xiao,Chen Liu,Chenrui Ma,Yunbei Zhang,Xiao Wang,Smita Krishnaswamy,Tianyang Wang*

Main category: cs.LG

TL;DR: 介绍CTR - LoRA框架，实验显示其优于强PEFT基线，提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 以往PEFT方法常将容量分配与训练中更新演变方式分离，需改进。

Method: 引入CTR - LoRA框架，结合秩调度与稳定性感知优化，基于边际效用分配参数，用Fisher/Hessian度量信任区域约束更新。

Result: 在多个开源主干模型上实验，相比强PEFT基线有一致改进，提升准确率、训练稳定性，降低内存需求，提高吞吐量。

Conclusion: CTR - LoRA为更稳健和可部署的PEFT提供了原则性路径。

Abstract: Parameter-efficient fine-tuning (PEFT) has become the standard approach for
adapting large language models under limited compute and memory budgets.
Although previous methods improve efficiency through low-rank updates,
quantization, or heuristic budget reallocation, they often decouple the
allocation of capacity from the way updates evolve during training. In this
work, we introduce CTR-LoRA, a framework guided by curvature trust region that
integrates rank scheduling with stability-aware optimization. CTR-LoRA
allocates parameters based on marginal utility derived from lightweight
second-order proxies and constrains updates using a Fisher/Hessian-metric trust
region. Experiments on multiple open-source backbones (7B-13B), evaluated on
both in-distribution and out-of-distribution benchmarks, show consistent
improvements over strong PEFT baselines. In addition to increased accuracy,
CTR-LoRA enhances training stability, reduces memory requirements, and achieves
higher throughput, positioning it on the Pareto frontier of performance and
efficiency. These results highlight a principled path toward more robust and
deployable PEFT.

</details>


### [146] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: 本文提出Bias - adaptive Preference distillation Learning (BPL)框架，用双蒸馏策略挖掘用户偏好，在事实和反事实测试环境均表现良好，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有去偏学习大多关注反事实测试环境，在事实测试环境中准确性下降，需要一个在两种测试环境都表现好的模型。

Method: 引入BPL框架，采用从有偏模型进行师生蒸馏保留准确偏好知识以提升事实测试表现，通过带可靠性过滤的自蒸馏迭代优化知识以提升反事实测试表现。

Result: 综合实验验证了BPL在事实和反事实测试中的有效性。

Conclusion: BPL框架能有效解决推荐系统去偏问题，在两种测试环境都能有好的表现。

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [147] [Feature-driven reinforcement learning for photovoltaic in continuous intraday trading](https://arxiv.org/abs/2510.16021)
*Arega Getaneh Abate,Xiufeng Liu,Ruyu Liu,Xiaobing Zhang*

Main category: cs.LG

TL;DR: 提出特征驱动强化学习方法用于光伏日内交易，表现优于基准，表明该方法实用、高效可部署。


<details>
  <summary>Details</summary>
Motivation: 光伏运营商面临发电和短期电价的不确定性，连续日内市场可让生产者实时调整头寸，需一种有效交易策略。

Method: 将问题建模为马尔可夫决策过程，用近端策略优化（PPO）求解，采用主要为线性、可解释的策略，将数据驱动特征融入状态。

Result: 该策略在不同场景下始终优于基准，验证显示快速收敛、实时推理和透明决策规则，学习的权重凸显市场微观结构和历史特征的核心作用。

Conclusion: 特征驱动强化学习为光伏生产者积极参与日内交易提供实用、数据高效且可操作部署的途径。

Abstract: Photovoltaic (PV) operators face substantial uncertainty in generation and
short-term electricity prices. Continuous intraday markets enable producers to
adjust their positions in real time, potentially improving revenues and
reducing imbalance costs. We propose a feature-driven reinforcement learning
(RL) approach for PV intraday trading that integrates data-driven features into
the state and learns bidding policies in a sequential decision framework. The
problem is cast as a Markov Decision Process with a reward that balances
trading profit and imbalance penalties and is solved with Proximal Policy
Optimization (PPO) using a predominantly linear, interpretable policy. Trained
on historical market data and evaluated out-of-sample, the strategy
consistently outperforms benchmark baselines across diverse scenarios.
Extensive validation shows rapid convergence, real-time inference, and
transparent decision rules. Learned weights highlight the central role of
market microstructure and historical features. Taken together, these results
indicate that feature-driven RL offers a practical, data-efficient, and
operationally deployable pathway for active intraday participation by PV
producers.

</details>


### [148] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: 提出Long Exposure系统加速大语言模型参数高效微调，评估显示有显著加速效果


<details>
  <summary>Details</summary>
Motivation: 参数高效微调技术效率低，存在时间和成本挑战

Method: 引入Shadowy Sparsity，提出包含Shadowy - sparsity Exposer、Sequence - oriented Predictor和Dynamic - aware Operator三个组件的Long Exposure系统

Result: Long Exposure在端到端微调中比现有技术快达2.49倍

Conclusion: Long Exposure在加速大语言模型参数高效微调方面有显著进展

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [149] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出针对大型推理模型的Deadlock攻击，利用恶意嵌入诱导永久推理循环，攻击成功率达100%，暴露模型推理效率方面的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代大型推理模型的链式思维推理机制存在新的脆弱性，需研究相关攻击方法以暴露安全问题。

Method: 训练恶意对抗嵌入劫持模型生成控制流，引入后门植入策略解决连续到离散投影间隙问题。

Result: 在四个先进大型推理模型和三个数学推理基准测试中攻击成功率达100%，攻击隐蔽且能抵御现有缓解过度思考问题的策略。

Conclusion: 大型推理模型在推理效率方面存在未充分探索的关键安全漏洞。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [150] [Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing](https://arxiv.org/abs/2510.17088)
*Zan Li,Rui Fan*

Main category: cs.LG

TL;DR: 现有金融异常检测器存在不足，本文提出带专家网络的自适应图学习框架解决问题，在美股数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有金融异常检测器将所有异常统一处理，无法揭示异常机制、风险集中点和干预方法，存在三个待解决挑战，阻碍针对性监管。

Method: 通过带专家网络的自适应图学习，利用BiLSTM和自注意力捕捉多尺度时间依赖，用跨模态注意力融合时空信息，通过神经多源插值学习动态图，用应力调制融合平衡动态和结构先验，将异常分配给四个特定机制专家并产生双级可解释归因。

Result: 在100只美股（2017 - 2024）上，对13个重大事件的检测率达92.3%，提前3.8天预警，比最佳基线高30.8个百分点；硅谷银行案例展示了异常演化跟踪，无需有标签监督实现自动时间机制识别。

Conclusion: 所提出的框架能有效解决现有金融异常检测的问题，在检测效果和可解释性上表现良好。

Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity
freezes, contagion cascades, regime shifts), but existing detectors treat all
anomalies uniformly, producing scalar scores without revealing which mechanism
is failing, where risks concentrate, or how to intervene. This opacity prevents
targeted regulatory responses. Three unsolved challenges persist: (1) static
graph structures cannot adapt when market correlations shift during regime
changes; (2) uniform detection mechanisms miss type-specific signatures across
multiple temporal scales while failing to integrate individual behaviors with
network contagion; (3) black-box outputs provide no actionable guidance on
anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks
that provide built-in interpretability. Our framework captures multi-scale
temporal dependencies through BiLSTM with self-attention, fuses temporal and
spatial information via cross-modal attention, learns dynamic graphs through
neural multi-source interpolation, adaptively balances learned dynamics with
structural priors via stress-modulated fusion, routes anomalies to four
mechanism-specific experts, and produces dual-level interpretable attributions.
Critically, interpretability is embedded architecturally rather than applied
post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events
with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley
Bank case study demonstrates anomaly evolution tracking: Price-Shock expert
weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48
(66% above baseline) one week later, revealing automatic temporal mechanism
identification without labeled supervision.

</details>


### [151] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 本文提出开放集下细粒度联邦域适应方法Gains，解决新客户端加入带来的知识发现与适应问题，实验显示性能优于其他基线。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习假设客户端数量固定，现实中不断有新客户端加入，现有研究聚焦粗粒度知识发现，且常牺牲源域性能和适应效率。

Method: 将模型拆分为编码器和分类器，开发细粒度知识发现和贡献驱动聚合技术，设计抗遗忘机制。

Result: 在多域数据集的三种典型数据偏移场景实验中，Gains在源域和目标域客户端性能上显著优于其他基线。

Conclusion: Gains能有效解决新客户端加入的知识发现与适应问题，保证源域性能并实现平衡适应。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [152] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: 提出SAU - FNO框架用于3D IC热管理，结合自注意力和U - Net与FNO，用迁移学习微调数据，实验显示其精度高且速度快。


<details>
  <summary>Details</summary>
Motivation: 3D IC热管理挑战增加，传统PDE求解方法慢，机器学习方法如FNO有高频信息损失和高保真数据依赖问题。

Method: 引入Self - Attention U - Net Fourier Neural Operator (SAU - FNO)框架，结合自注意力和U - Net与FNO；采用迁移学习微调低保真数据。

Result: SAU - FNO实现了最先进的热预测精度，比传统FEM方法快842倍。

Conclusion: SAU - FNO是用于高级3D IC热模拟的高效工具。

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [153] [LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems](https://arxiv.org/abs/2510.15969)
*Paul-Niklas Ken Kandora,Simon Caspar Zeller,Aaron Jeremias Elsing,Elena Kuss,Steffen Rebennack*

Main category: cs.LG

TL;DR: 介绍LinearizeLLM框架利用大语言模型解决非线性优化问题的线性化任务，创建数据集评估，结果显示专业LLM代理可自动化线性化任务。


<details>
  <summary>Details</summary>
Motivation: 非线性优化问题的重新表述主要靠人工且依赖专业知识，但对用线性优化求解器或专用算法解决此类问题至关重要，因此需自动化方法。

Method: 引入LinearizeLLM框架，为每个非线性模式分配一个重新表述代理，让其推导精确线性重新表述，代理协调组装线性模型；创建含20个现实非线性优化问题的数据集，用多个LLM评估。

Result: 专业的LLM代理能够自动化线性化任务。

Conclusion: 为非线性优化的全对话式建模管道开辟了道路。

Abstract: Reformulating nonlinear optimization problems is largely manual and
expertise-intensive, yet it remains essential for solving such problems with
linear optimization solvers or applying special-purpose algorithms. We
introduce \textit{LinearizeLLM}, an agent-based framework that solves this task
by leveraging Large Language Models (LLMs). The framework assigns each
nonlinear pattern to a \textit{reformulation agent} that is explicitly
instructed to derive an exact linear reformulation for its nonlinearity
pattern, for instance, absolute-value terms or bilinear products of decision
variables. The agents then coordinate to assemble a solver-ready linear model
equivalent to the original problem. To benchmark the approach, we create a
dataset of 20 real-world nonlinear optimization problems derived from the
established ComplexOR dataset of linear optimization problems. We evaluate our
approach with several LLMs. Our results indicate that specialized LLM agents
can automate linearization tasks, opening a path toward fully conversational
modeling pipelines for nonlinear optimization.

</details>


### [154] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 提出分辨率感知检索增强预测模型用于零样本预测，在微气候预测中表现出色，凸显检索增强和分辨率感知策略有效性。


<details>
  <summary>Details</summary>
Motivation: 解决传统预测方法在零样本预测（无直接历史数据下预测未知情况）方面的挑战。

Method: 引入分辨率感知检索增强预测模型，将信号分解为不同频率分量，不同频率分量采用不同空间上下文检索数据。

Result: 在微气候预测中，模型显著优于传统预测方法、数值天气预报模型和现代基础时间序列模型，在ERA5数据集上比HRRR的MSE低71%，比Chronos的MSE低34%。

Conclusion: 检索增强和分辨率感知策略有效，为微气候建模等零样本预测提供可扩展且数据高效的解决方案。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [155] [Predict Training Data Quality via Its Geometry in Metric Space](https://arxiv.org/abs/2510.15970)
*Yang Ba,Mohammad Sadeq Abolhasani,Rong Pan*

Main category: cs.LG

TL;DR: 研究提出训练数据的表示丰富性和冗余消除影响学习结果，用持久同调分析和提升训练数据。


<details>
  <summary>Details</summary>
Motivation: 已知有效训练数据类型，但数据几何结构对模型性能的影响研究不足。

Method: 使用持久同调从度量空间的数据中提取拓扑特征，以量化数据多样性。

Result: 发现持久同调是分析和提升驱动AI系统训练数据的有力工具。

Conclusion: 持久同调可用于分析和增强训练数据。

Abstract: High-quality training data is the foundation of machine learning and
artificial intelligence, shaping how models learn and perform. Although much is
known about what types of data are effective for training, the impact of the
data's geometric structure on model performance remains largely underexplored.
We propose that both the richness of representation and the elimination of
redundancy within training data critically influence learning outcomes. To
investigate this, we employ persistent homology to extract topological features
from data within a metric space, thereby offering a principled way to quantify
diversity beyond entropy-based measures. Our findings highlight persistent
homology as a powerful tool for analyzing and enhancing the training data that
drives AI systems.

</details>


### [156] [Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods](https://arxiv.org/abs/2510.16609)
*Avrim Blum,Daniel Hsu,Cyrus Rashtchian,Donya Saless*

Main category: cs.LG

TL;DR: 本文研究测试时增强方法中模型预训练知识与外部检索信息的关系，将多步推理建模为知识图上的连通性问题，得出增强步骤数量的必要充分条件及相变结果。


<details>
  <summary>Details</summary>
Motivation: 当前对测试时增强中模型参数知识与外部检索信息关系的理论基础理解不足，不清楚用少量增强步骤回答查询需要多少预训练知识。

Method: 将多步推理表述为知识图上的s - t连通性问题，把模型预训练参数知识表示为部分可能有噪声的子图，将增强视为向神谕查询真实边以扩充模型知识。

Result: 发现相变现象，若先验知识图分裂为小组件，通过增强找路径低效，需Ω(√n)次查询；当正确知识密度超过阈值形成巨大组件时，期望常数次查询可找到路径。

Conclusion: 成功刻画了在部分先验知识下模型生成准确答案所需增强步骤的必要充分数量。

Abstract: Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.

</details>


### [157] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: 本文提出PALE框架用于大语言模型幻觉检测，引入CM Score评估，无需额外人工标注，实验显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉现象，且幻觉检测缺乏标注数据，需解决该问题以确保内容可靠性。

Method: 提出PALE框架，利用提示引导大语言模型响应进行数据增强；引入CM Score基于激活空间分布建模评估真实性，采用矩阵分解捕捉分布结构。

Result: PALE在幻觉检测上表现出色，比竞争基线性能高6.55%。

Conclusion: PALE框架无需额外人工标注，具有强泛化性和实用性，适用于实际应用。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [158] [CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning](https://arxiv.org/abs/2510.16694)
*Anthony DiMaggio,Raghav Sharma,Gururaj Saileshwar*

Main category: cs.LG

TL;DR: 提出CLIP技术缓解安全联邦学习中掉队者问题，加速训练且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 安全联邦学习在异构设备上因掉队者客户端存在性能瓶颈，影响所有参与客户端的训练速度。

Method: 提出客户端不变神经元剪枝技术CLIP，并结合网络感知剪枝。

Result: 在多个数据集上加速安全联邦学习训练13% - 34%，精度影响在提升1.3%到降低2.6%之间。

Conclusion: CLIP技术能有效缓解掉队者问题，在加速训练的同时将精度损失控制在较小范围内。

Abstract: Secure federated learning (FL) preserves data privacy during distributed
model training. However, deploying such frameworks across heterogeneous devices
results in performance bottlenecks, due to straggler clients with limited
computational or network capabilities, slowing training for all participating
clients. This paper introduces the first straggler mitigation technique for
secure aggregation with deep neural networks. We propose CLIP, a client-side
invariant neuron pruning technique coupled with network-aware pruning, that
addresses compute and network bottlenecks due to stragglers during training
with minimal accuracy loss. Our technique accelerates secure FL training by 13%
to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an
accuracy impact of between 1.3% improvement to 2.6% reduction.

</details>


### [159] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 现有提升大语言模型系统（LLMsys）的方法接近上限，现有基准测试有局限，因此提出用户反馈模拟框架和综合基准测试，实验显示现有基线效果不佳，希望为后续研究奠基。


<details>
  <summary>Details</summary>
Motivation: 现有提升LLMsys的主流方法接近上限，现有LLM记忆基准测试聚焦于同构阅读理解任务，缺乏对从用户反馈中学习能力的测试。

Method: 提出用户反馈模拟框架和覆盖多领域、语言和任务类型的综合基准测试。

Result: 实验表明，现有最先进的基线方法的有效性和效率远不能令人满意。

Conclusion: 所提出的基准测试可为未来LLM记忆和优化算法的研究铺平道路。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [160] [DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space](https://arxiv.org/abs/2510.15978)
*Junchao Gong,Jingyi Xu,Ben Fei,Fenghua Ling,Wenlong Zhang,Kun Chen,Wanghan Xu,Weidong Yang,Xiaokang Yang,Lei Bai*

Main category: cs.LG

TL;DR: 本文提出DAWP框架，用AIDA模块初始化使AIWP在观测空间运行，实验表明其提升了AIWP效果，在全球降水预报有潜力。


<details>
  <summary>Details</summary>
Motivation: 现有AIWP依赖再分析数据有缺陷，观测预报是新范式，但面临学习时空动态的挑战。

Method: 提出DAWP框架，AIDA模块用MMAE同化不规则卫星观测令牌，AIWP采用带CBC的时空解耦变压器。

Result: AIDA初始化显著提高了AIWP的滚动输出和效率。

Conclusion: DAWP在全球降水预报有应用潜力。

Abstract: Weather prediction is a critical task for human society, where impressive
progress has been made by training artificial intelligence weather prediction
(AIWP) methods with reanalysis data. However, reliance on reanalysis data
limits the AIWPs with shortcomings, including data assimilation biases and
temporal discrepancies. To liberate AIWPs from the reanalysis data, observation
forecasting emerges as a transformative paradigm for weather prediction. One of
the key challenges in observation forecasting is learning spatiotemporal
dynamics across disparate measurement systems with irregular high-resolution
observation data, which constrains the design and prediction of AIWPs. To this
end, we propose our DAWP as an innovative framework to enable AIWPs to operate
in a complete observation space by initialization with an artificial
intelligence data assimilation (AIDA) module. Specifically, our AIDA module
applies a mask multi-modality autoencoder(MMAE)for assimilating irregular
satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a
spatiotemporal decoupling transformer with cross-regional boundary conditioning
(CBC), learning the dynamics in observation space, to enable sub-image-based
global observation forecasting. Comprehensive experiments demonstrate that AIDA
initialization significantly improves the roll out and efficiency of AIWP.
Additionally, we show that DAWP holds promising potential to be applied in
global precipitation forecasting.

</details>


### [161] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: 提出Cog - Rethinker分层元认知强化学习框架解决大语言模型推理中样本效率低问题，实验显示其在数学推理基准测试中表现优越。


<details>
  <summary>Details</summary>
Motivation: 以往通过固定提示模板激活大语言模型固有能力的策略，在推理任务中会导致弱模型样本采样效率低、样本浪费。

Method: 提出Cog - Rethinker框架，聚焦强化学习训练的滚动程序，采用分层元认知两阶段框架提高样本利用率，还对策略进行监督微调以实现冷启动和保持一致性。

Result: Cog - Rethinker在各种数学推理基准测试中表现优越，样本效率提高，加速了收敛。

Conclusion: Cog - Rethinker能有效解决大语言模型推理中的样本效率问题，提升推理性能。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [162] [Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?](https://arxiv.org/abs/2510.16060)
*Coen Adler,Yuxin Chang,Felix Draxler,Samar Abdi,Padhraic Smyth*

Main category: cs.LG

TL;DR: 研究时间序列基础模型校准属性，发现其校准优于基线模型且无系统过/欠自信。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型虽预测性能佳，但校准属性研究不足，而校准对实际应用很关键。

Method: 对五个时间序列基础模型和两个竞争基线模型进行系统评估，包括评估模型校准、不同预测头的影响和长期自回归预测下的校准。

Result: 时间序列基础模型校准始终优于基线模型，且不像其他深度学习模型常出现过自信情况。

Conclusion: 时间序列基础模型在校准方面表现良好，不会出现系统的过自信或欠自信。

Abstract: The recent development of foundation models for time series data has
generated considerable interest in using such models across a variety of
applications. Although foundation models achieve state-of-the-art predictive
performance, their calibration properties remain relatively underexplored,
despite the fact that calibration can be critical for many practical
applications. In this paper, we investigate the calibration-related properties
of five recent time series foundation models and two competitive baselines. We
perform a series of systematic evaluations assessing model calibration (i.e.,
over- or under-confidence), effects of varying prediction heads, and
calibration under long-term autoregressive forecasting. We find that time
series foundation models are consistently better calibrated than baseline
models and tend not to be either systematically over- or under-confident, in
contrast to the overconfidence often seen in other deep learning models.

</details>


### [163] [AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution](https://arxiv.org/abs/2510.15982)
*Donghyeok Shin,Yeongmin Kim,Suhyeon Jo,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出α - 混合辅助分布和α - 混合蒸馏（AMiD），通过实验证明其在性能和训练稳定性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型计算和内存成本高，知识蒸馏虽能缓解但存在容量差距和训练不稳定等问题，且以往辅助分布方法缺乏系统研究。

Method: 提出α - 混合辅助分布和AMiD统一框架，前者引入新变量α，后者基于最优性推广散度族。

Result: 通过大量实验表明，AMiD利用更广泛且有理论依据的辅助分布空间，性能和训练稳定性更优。

Conclusion: AMiD是一种有效的知识蒸馏方法，能解决现有知识蒸馏的一些问题。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable
improvement across many tasks but incur high computational and memory costs.
Knowledge distillation (KD) mitigates this issue by transferring knowledge from
a large teacher to a smaller student through distributional alignment. Previous
studies have proposed various discrepancy metrics, but the capacity gap and
training instability caused by near-zero probabilities, stemming from the
high-dimensional output of LLMs, remain fundamental limitations. To overcome
these challenges, several approaches implicitly or explicitly incorporating
assistant distribution have recently been proposed. However, the past proposals
of assistant distributions have been a fragmented approach without a systematic
investigation of the interpolation path and the divergence. This paper proposes
$\alpha$-mixture assistant distribution, a novel generalized family of
assistant distributions, and $\alpha$-mixture distillation, coined AMiD, a
unified framework for KD using the assistant distribution. The $\alpha$-mixture
assistant distribution provides a continuous extension of the assistant
distribution by introducing a new distribution design variable $\alpha$, which
has been fixed in all previous approaches. Furthermore, AMiD generalizes the
family of divergences used with the assistant distributions based on
optimality, which has also been restricted in previous works. Through extensive
experiments, we demonstrate that AMiD offers superior performance and training
stability by leveraging a broader and theoretically grounded assistant
distribution space.

</details>


### [164] [Justitia: Fair and Efficient Scheduling for LLM Applications](https://arxiv.org/abs/2510.17015)
*Mingyan Yang,Guanjie Wang,Manqi Luo,Yifei Liu,Chen Chen,Han Zhao,Yu Feng,Quan Chen,Minyi Guo*

Main category: cs.LG

TL;DR: 本文提出新型调度器Justitia，以公平高效方式服务大语言模型应用，实验显示其能提高调度效率并保证公平性。


<details>
  <summary>Details</summary>
Motivation: 主流大语言模型调度器在共享GPU服务器服务应用时，因队首阻塞或资源分配过度受限表现不佳，需公平高效的调度方法。

Method: 设计Justitia调度器，采用以内存为中心建模服务成本、用简单神经网络预测需求、基于虚拟时间的公平排队算法。

Result: 在vLLM上实现Justitia，实验表明它能显著提高调度效率并保证公平性。

Conclusion: Justitia能以公平高效的方式服务大语言模型应用。

Abstract: In the era of Large Language Models (LLMs), it has been popular to launch a
series of LLM inferences -- we call an LLM application -- to better solve
real-world problems. When serving those applications in shared GPU servers, the
schedulers are expected to attain fast application completions with guaranteed
worst-case performance. However, mainstream LLM schedulers fail to behave well
for LLM applications -- due to head-of-line blocking or over-constrained
resource allocation. In this paper, we propose to serve LLM applications in a
fair and also efficient manner. To this end, we design Justitia, a novel
scheduler with three key techniques. First, given that memory is prevalently a
bottleneck for mainstream inference frameworks like vLLM, Justitia models the
service cost of LLM applications in a memory-centric manner. Meanwhile, it uses
a simple neural network model to conduct light-weight and also accurate demand
prediction. Moreover, Justitia adopts a virtual-time based fair queuing
algorithm to reduce the overall performance with guaranteed worst-case delay.
We have implemented Justitia atop vLLM, and experimental results involving
diverse LLM applications show that it can substantially enhance the scheduling
efficiency with fairness preserved.

</details>


### [165] [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](https://arxiv.org/abs/2510.17670)
*Yehonathan Refael,Amit Aides,Aviad Barzilai,George Leifman,Genady Beryozkin,Vered Silverman,Bolous Jaber,Tomer Shekel*

Main category: cs.LG

TL;DR: 提出级联方法解决遥感领域开放词汇目标检测零样本性能问题，方法高效且超现有技术。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测模型在遥感等专业领域零样本性能受自然语言歧义影响，限制下游应用。

Method: 采用级联方法，结合预训练OVD模型和轻量级少样本分类器，利用FLAME主动学习策略选样训练。

Result: 方法在遥感基准测试中持续超越现有技术，能快速适应特定需求。

Conclusion: 建立了实用且资源高效的框架，可将基础模型适配特定用户需求。

Abstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

</details>


### [166] [MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction](https://arxiv.org/abs/2510.15985)
*Zexi Tan,Tao Xie,Binbin Sun,Xiang Zhang,Yiqun Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: 本文提出MEET - Sepsis框架用于脓毒症早期预测，仅用SOTA方法20%的ICU监测时间就达到有竞争力的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 脓毒症早期准确预测对及时干预至关重要，但现有AI方法难以捕捉早期微弱时间信号。

Method: 引入多内生视角表征增强（MERE）机制构建丰富特征视图，结合级联双卷积时间序列注意力（CDTA）模块进行多尺度时间表征学习，提出MEET - Sepsis框架。

Result: MEET - Sepsis框架仅用SOTA方法20%的ICU监测时间就达到有竞争力的预测准确率。

Conclusion: 提出的MEET - Sepsis框架显著推进了脓毒症早期预测，经过大量验证证实了其有效性。

Abstract: Sepsis is a life-threatening infectious syndrome associated with high
mortality in intensive care units (ICUs). Early and accurate sepsis prediction
(SP) is critical for timely intervention, yet remains challenging due to subtle
early manifestations and rapidly escalating mortality. While AI has improved SP
efficiency, existing methods struggle to capture weak early temporal signals.
This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)
mechanism to construct enriched feature views, coupled with a Cascaded
Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal
representation learning. The proposed MEET-Sepsis framework achieves
competitive prediction accuracy using only 20% of the ICU monitoring time
required by SOTA methods, significantly advancing early SP. Extensive
validation confirms its efficacy. Code is available at:
https://github.com/yueliangy/MEET-Sepsis.

</details>


### [167] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 提出IB - FT方法克服预训练大语言模型代码微调的记忆障碍，实验显示其优于传统微调。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调在代码生成中存在记忆障碍，会阻碍模型获取新的可泛化代码知识。

Method: 提出信息瓶颈（IB）引导的微调方法IB - FT，对代码数据的隐藏表示施加IB惩罚。

Result: 在两个代码基准测试中，IB - FT大幅缓解记忆障碍，提高top - 1性能，在更严格的多样本指标下有更稳定提升。

Conclusion: IB - FT能有效克服记忆障碍，优于传统监督微调方法。

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [168] [Narrowing Action Choices with AI Improves Human Sequential Decisions](https://arxiv.org/abs/2510.16097)
*Eleni Straitouri,Stratis Tsirtsis,Ander Artola Velasco,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 本文开发决策支持系统并引入算法实现序贯决策任务互补性，大规模实验表明系统有效。


<details>
  <summary>Details</summary>
Motivation: 探索能否用自适应控制人类能动性水平的原则在序贯决策任务中实现互补性。

Method: 开发决策支持系统，用预训练AI代理缩小人类行动集，引入利用行动集平滑性的多臂老虎机算法优化人类能动性水平，开展大规模人类实验。

Result: 使用系统的参与者比单独行动的参与者表现好约30%，比系统使用的AI代理好超2%。

Conclusion: 可以用相同原则在序贯决策任务中实现互补性，所开发系统有效。

Abstract: Recent work has shown that, in classification tasks, it is possible to design
decision support systems that do not require human experts to understand when
to cede agency to a classifier or when to exercise their own agency to achieve
complementarity$\unicode{x2014}$experts using these systems make more accurate
predictions than those made by the experts or the classifier alone. The key
principle underpinning these systems reduces to adaptively controlling the
level of human agency, by design. Can we use the same principle to achieve
complementarity in sequential decision making tasks? In this paper, we answer
this question affirmatively. We develop a decision support system that uses a
pre-trained AI agent to narrow down the set of actions a human can take to a
subset, and then asks the human to take an action from this action set. Along
the way, we also introduce a bandit algorithm that leverages the smoothness
properties of the action sets provided by our system to efficiently optimize
the level of human agency. To evaluate our decision support system, we conduct
a large-scale human subject study ($n = 1{,}600$) where participants play a
wildfire mitigation game. We find that participants who play the game supported
by our system outperform those who play on their own by $\sim$$30$% and the AI
agent used by our system by $>$$2$%, even though the AI agent largely
outperforms participants playing without support. We have made available the
data gathered in our human subject study as well as an open source
implementation of our system at
https://github.com/Networks-Learning/narrowing-action-choices .

</details>


### [169] [User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis](https://arxiv.org/abs/2510.15986)
*Sifeddine Sellami,Juba Agoun,Lamia Yessad,Louenas Bounia*

Main category: cs.LG

TL;DR: 提出基于聚类方法对睡眠障碍患者分组，结合可解释方法确定关键因素，实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍诊断因症状多样而复杂，可解释人工智能能让用户理解AI模型决策，故开展研究。

Method: 提出基于聚类的方法对患者按不同睡眠障碍特征分组，并集成可解释方法。

Result: 在匿名真实数据上的实验表明该方法有效且有相关性。

Conclusion: 所提出的结合聚类与可解释方法能有效对睡眠障碍患者分组并确定关键影响因素。

Abstract: Sleep disorders have a major impact on patients' health and quality of life,
but their diagnosis remains complex due to the diversity of symptoms. Today,
technological advances, combined with medical data analysis, are opening new
perspectives for a better understanding of these disorders. In particular,
explainable artificial intelligence (XAI) aims to make AI model decisions
understandable and interpretable for users. In this study, we propose a
clustering-based method to group patients according to different sleep disorder
profiles. By integrating an explainable approach, we identify the key factors
influencing these pathologies. An experiment on anonymized real data
illustrates the effectiveness and relevance of our approach.

</details>


### [170] [Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models](https://arxiv.org/abs/2510.15987)
*Samuel Lippl,Thomas McGee,Kimberly Lopez,Ziwen Pan,Pierce Zhang,Salma Ziadi,Oliver Eberle,Ida Momennejad*

Main category: cs.LG

TL;DR: 本文引入框架追踪和引导大语言模型推理的算法原语，通过多基准测试评估，发现推理由算法原语的组合几何支持，原语可跨任务和模型迁移，微调增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探究潜在和推理时间计算如何使大语言模型解决多步推理问题。

Method: 引入框架将推理轨迹与内部激活模式关联，通过注入原语到残差流评估，聚类神经激活并标注匹配推理轨迹，运用函数向量方法推导原语向量。

Result: 跨任务和模型评估显示有共享和特定任务原语，微调后的模型推理更具系统性，注入原语向量可诱导特定行为。

Conclusion: 大语言模型的推理可能由算法原语的组合几何支持，原语可跨任务和模型迁移，推理微调可加强跨领域算法泛化。

Abstract: How do latent and inference time computations enable large language models
(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and
steering algorithmic primitives that underlie model reasoning. Our approach
links reasoning traces to internal activation patterns and evaluates
algorithmic primitives by injecting them into residual streams and measuring
their effect on reasoning steps and task performance. We consider four
benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph
navigation. We operationalize primitives by clustering neural activations and
labeling their matched reasoning traces. We then apply function vector methods
to derive primitive vectors as reusable compositional building blocks of
reasoning. Primitive vectors can be combined through addition, subtraction, and
scalar operations, revealing a geometric logic in activation space. Cross-task
and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both
shared and task-specific primitives. Notably, comparing Phi-4 with its
reasoning-finetuned variant highlights compositional generalization after
finetuning: Phi-4-Reasoning exhibits more systematic use of verification and
path-generation primitives. Injecting the associated primitive vectors in
Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.
Together, these findings demonstrate that reasoning in LLMs may be supported by
a compositional geometry of algorithmic primitives, that primitives transfer
cross-task and cross-model, and that reasoning finetuning strengthens
algorithmic generalization across domains.

</details>


### [171] [A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies](https://arxiv.org/abs/2510.16132)
*Phalguni Nanda,Zaiwei Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we present the first finite-time analysis of the Q-learning
algorithm under time-varying learning policies (i.e., on-policy sampling) with
minimal assumptions -- specifically, assuming only the existence of a policy
that induces an irreducible Markov chain over the state space. We establish a
last-iterate convergence rate for $\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$,
implying a sample complexity of order $O(1/\epsilon^2)$ for achieving
$\mathbb{E}[\|Q_k - Q^*\|_\infty] \le \epsilon$, matching that of off-policy
Q-learning but with a worse dependence on exploration-related parameters. We
also derive an explicit rate for $\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$,
where $\pi_k$ is the learning policy at iteration $k$. These results reveal
that on-policy Q-learning exhibits weaker exploration than its off-policy
counterpart but enjoys an exploitation advantage, as its policy converges to an
optimal one rather than remaining fixed. Numerical simulations corroborate our
theory.
  Technically, the combination of time-varying learning policies (which induce
rapidly time-inhomogeneous Markovian noise) and the minimal assumption on
exploration presents significant analytical challenges. To address these
challenges, we employ a refined approach that leverages the Poisson equation to
decompose the Markovian noise corresponding to the lazy transition matrix into
a martingale-difference term and residual terms. To control the residual terms
under time inhomogeneity, we perform a sensitivity analysis of the Poisson
equation solution with respect to both the Q-function estimate and the learning
policy. These tools may further facilitate the analysis of general
reinforcement learning algorithms with rapidly time-varying learning policies
-- such as single-timescale actor--critic methods and learning-in-games
algorithms -- and are of independent interest.

</details>


### [172] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: 研究GRPO算法在增强大语言模型推理能力时的条件，发现其受基础模型分布限制，结果为OOD改进依赖目标任务与预训练偏差的对齐，ID任务增益随性能饱和而减少。


<details>
  <summary>Details</summary>
Motivation: GRPO在增强大语言模型推理能力时增益不一致，探究其在何种条件下能改进推理和实现分布外泛化。

Method: 从数据分布角度，理论证明GRPO是一种保守的重加权方案，通过从零训练变压器进行对照研究评估泛化情况。

Result: OOD改进仅在目标任务与模型预训练偏差一致时出现，ID任务的增益随性能饱和而降低。

Conclusion: GRPO并非通用推理增强器，而是强化预训练偏差的工具，应开发能超越模型预训练能力的算法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [173] [Expert Merging in Sparse Mixture of Experts with Nash Bargaining](https://arxiv.org/abs/2510.16138)
*Dung V. Nguyen,Anh T. Nguyen,Minh H. Nguyen,Luc Q. Nguyen,Shiqi Jiang,Ethan Fetaya,Linh Duy Tran,Gal Chechik,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 本文提出基于博弈论的专家合并框架NAMEx，结合Nash Bargaining和复杂动量，实验表明其优于竞品，且适用于大规模系统。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏专家混合（SMoE）的专家合并策略缺乏原则性加权机制。

Method: 从博弈论视角重新解释专家合并，引入Nash Merging of Experts (NAMEx)框架，结合Nash Bargaining，并融入复杂动量。

Result: 在语言建模、文本分类等任务中，NAMEx始终优于竞争方法，能与流行MoE架构无缝集成，在大规模系统中也有效。

Conclusion: NAMEx是一种更平衡、高效的专家合并框架，具有良好的性能和可扩展性。

Abstract: Existing expert merging strategies for Sparse Mixture of Experts (SMoE)
typically rely on input-dependent or input-independent averaging of expert
parameters, but often lack a principled weighting mechanism. In this work, we
reinterpret expert merging through the lens of game theory, revealing
cooperative and competitive dynamics among experts. Based on this perspective,
we introduce Nash Merging of Experts (NAMEx), a novel framework that
incorporates Nash Bargaining into the merging process, enabling more balanced
and efficient collaboration among experts. Additionally, we incorporate complex
momentum into NAMEx to accelerate expert propagation with theoretical
guarantees for convergence. Extensive experiments across language modelling,
text classification, image classification, and zero-shot robustness under data
corruption show that NAMEx consistently outperforms competing methods while
integrating seamlessly with popular MoE architectures. Finally, we demonstrate
NAMEx's scalability by applying it to large-scale systems, including
Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both
zero-shot and fine-tuning settings.

</details>


### [174] [Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments](https://arxiv.org/abs/2510.15992)
*Ziming Dai,Tuo Zhang,Fei Gao,Xingyi Cai,Xiaofei Wang,Cheng Zhang,Wenyu Wang,Chengjie Zang*

Main category: cs.LG

TL;DR: 为满足定制化和成本效益高的大语言模型需求，提出Stratos端到端蒸馏管道，实验表明其在特定任务上效果好，能降低延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 工业对定制化、成本效益高的大语言模型需求增长，但现有蒸馏框架需手动干预，难以满足复杂蒸馏要求。

Method: 提出Stratos端到端LLM蒸馏管道，自动进行服务器和模型选择、知识蒸馏和部署，根据用户约束选最优服务器、匹配师生对并调整蒸馏策略。

Result: 在特定麻将推理任务上，学生模型准确率达GPT - 4o教师基线四倍，且降低延迟和成本，不影响准确率。

Conclusion: Stratos在垂直领域LLM部署有前景。

Abstract: The growing industrial demand for customized and cost-efficient large
language models (LLMs) is fueled by the rise of vertical, domain-specific tasks
and the need to optimize performance under constraints such as latency and
budget. Knowledge distillation, as an efficient model compression and transfer
technique, offers a feasible solution. However, existing distillation
frameworks often require manual intervention and struggle to meet such complex
user-defined distillation requirements. To bridge this gap, we propose Stratos,
an end-to-end LLM distillation pipeline that automates server and model
selection, knowledge distillation, and deployment in distributed cloud
environments. Given user-defined constraints on model performance and system
budget, Stratos automatically selects Pareto-optimal servers, dynamically
matches teacher-student pairs, and adapts distillation strategies based on task
complexity to optimize cloud hosting. Experiments show that Stratos produces a
student model that achieves four times the accuracy of its GPT-4o teacher
baseline on a rare, domain-specific Mahjong reasoning task with reverse
synthetic data and knowledge injection. Moreover, it achieves reduced latency
and cost without compromising accuracy. These results highlight its promise for
vertical-domain LLM deployment.

</details>


### [175] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文通过指数倾斜目标将零阶优化与锐度感知最小化（SAM）方法明确联系起来，探索新零阶算法解决软SAM目标，在下游任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 经典零阶优化方法优化原函数平滑版本，而流行的SAM目标关注邻域内最大损失，本文旨在明确连接零阶优化与SAM方法。

Method: 通过指数倾斜目标在平均损失和最大损失公式间平滑过渡，探索新零阶算法解决由倾斜参数t参数化的软SAM目标。

Result: 该方法可作为无梯度且内存高效的SAM变体替代方案，在分类、多选问答和语言生成等下游任务上比普通零阶基线有更好的泛化能力。

Conclusion: 提出的方法能有效连接零阶优化和SAM方法，在多种任务上有良好表现。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [176] [Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning](https://arxiv.org/abs/2510.15996)
*Ozan K. Tonguz,Federico Taschin*

Main category: cs.LG

TL;DR: 论文提出用KS测试测量分布偏移，结果显示KS距离可作为监测工具，在智能交通中或助AI应对分布偏移。


<details>
  <summary>Details</summary>
Motivation: 现实中测试数据与训练数据概率分布可能有偏差，影响AI和ML系统的准确性和可靠性，某些应用中这类误差对系统安全和可靠性至关重要。

Method: 提出并探索使用Kolmogorov - Smirnov (KS) 测试测量分布偏移，用KS距离量化分布偏移及其对AI智能体性能的影响。

Result: KS距离可作为监测和测量分布偏移的有价值统计工具，如KS = 0.02时，使用强化学习智能体在单个十字路口的旅行时间会增加约50%。

Conclusion: 在基于AI的智能交通中使用KS测试和KS距离，有助于实时评估AI智能体的性能下降，使其更明智地应对分布偏移。

Abstract: One of the major problems in Machine Learning (ML) and Artificial
Intelligence (AI) is the fact that the probability distribution of the test
data in the real world could deviate substantially from the probability
distribution of the training data set. When this happens, the predictions of an
ML system or an AI agent could involve large errors which is very troublesome
and undesirable. While this is a well-known hard problem plaguing the AI and ML
systems' accuracy and reliability, in certain applications such errors could be
critical for safety and reliability of AI and ML systems. One approach to deal
with this problem is to monitor and measure the deviation in the probability
distribution of the test data in real time and to compensate for this
deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov
(KS) Test for measuring the distribution shift and we show how the KS distance
can be used to quantify the distribution shift and its impact on an AI agent's
performance. Our results suggest that KS distance could be used as a valuable
statistical tool for monitoring and measuring the distribution shift. More
specifically, it is shown that even a distance of KS=0.02 could lead to about
50\% increase in the travel time at a single intersection using a Reinforcement
Learning agent which is quite significant. It is hoped that the use of KS Test
and KS distance in AI-based smart transportation could be an important step
forward for gauging the performance degradation of an AI agent in real time and
this, in turn, could help the AI agent to cope with the distribution shift in a
more informed manner.

</details>


### [177] [Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction](https://arxiv.org/abs/2510.16161)
*Ankitkumar Joshi,Milos Hauskrecht*

Main category: cs.LG

TL;DR: 提出GRUwE模型用于处理不规则采样多变量时间序列，在多个基准测试中表现有竞争力，且实现简单、调参少、计算开销低。


<details>
  <summary>Details</summary>
Motivation: 不清楚复杂学习架构的真正优势，以及基于RNN的简单高效算法改进后是否仍有竞争力。

Method: 提出GRUwE模型，基于RNN架构，通过两种重置机制更新马尔可夫状态以支持连续时间预测。

Result: 在多个真实世界基准测试的预测任务中，GRUwE比现有SOTA方法有竞争力甚至表现更优。

Conclusion: GRUwE因简单性有显著优势，易于实现、调参少且减少在线部署计算开销。

Abstract: Modeling irregularly sampled multivariate time series is a persistent
challenge in domains like healthcare and sensor networks. While recent works
have explored a variety of complex learning architectures to solve the
prediction problems for irregularly sampled time series, it remains unclear
what are the true benefits of some of these architectures, and whether clever
modifications of simpler and more efficient RNN-based algorithms are still
competitive, i.e. they are on par with or even superior to these methods. In
this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential
basis functions, that builds upon RNN-based architectures for observations made
at irregular times. GRUwE supports both regression-based and event-based
predictions in continuous time. GRUwE works by maintaining a Markov state
representation of the time series that updates with the arrival of irregular
observations. The Markov state update relies on two reset mechanisms: (i)
observation-triggered reset, and (ii) time-triggered reset of the GRU state
using learnable exponential decays, to support the predictions in continuous
time. Our empirical evaluations across several real-world benchmarks on
next-observation and next-event prediction tasks demonstrate that GRUwE can
indeed achieve competitive to superior performance compared to the recent
state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers
compelling advantages: it is easy to implement, requires minimal
hyper-parameter tuning efforts, and significantly reduces the computational
overhead in the online deployment.

</details>


### [178] [AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM](https://arxiv.org/abs/2510.15998)
*Nilo Schwencke,Cyriaque Rousselot,Alena Shilova,Cyril Furtlehner*

Main category: cs.LG

TL;DR: 分析PINNs用ANaGRAM优化的训练动态，提出多截断自适应策略提升性能，实验验证有效并给出理论框架。


<details>
  <summary>Details</summary>
Motivation: 自然梯度方法训练PINNs表现优于标准优化器，分析ANaGRAM训练动态并提升其性能。

Method: 分析ANaGRAM训练动态，提出多截断自适应策略，开发基于谱理论的框架。

Result: 实验验证方法有效，部分实验可达机器精度。

Conclusion: 多截断自适应策略能提升ANaGRAM性能，谱理论框架解释正则化必要性并拓展与格林函数理论联系。

Abstract: Recent works have shown that natural gradient methods can significantly
outperform standard optimizers when training physics-informed neural networks
(PINNs). In this paper, we analyze the training dynamics of PINNs optimized
with ANaGRAM, a natural-gradient-inspired approach employing singular value
decomposition with cutoff regularization. Building on this analysis, we propose
a multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.
Experiments on benchmark PDEs validate the effectiveness of our method, which
allows to reach machine precision on some experiments. To provide theoretical
grounding, we develop a framework based on spectral theory that explains the
necessity of regularization and extend previous shown connections with Green's
functions theory.

</details>


### [179] [Layer-Aware Influence for Online Data Valuation Estimation](https://arxiv.org/abs/2510.16007)
*Ziao Yang,Longbo Huang,Hongfu Liu*

Main category: cs.LG

TL;DR: 提出层感知在线估计器解决频繁影响力估计的计算负担问题，实验显示方法可提高准确率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注静态影响力，忽略优化过程中数据价值的动态变化，且频繁影响力估计存在计算负担。

Method: 开发仅需损失到输出梯度的层感知在线估计器，避免参数级和全网络梯度。

Result: 在大语言模型预训练、微调及图像分类等实验中，方法提高了准确率，且大幅降低时间和内存成本。

Conclusion: 该方法使动态数据筛选在实践中高效且可扩展。

Abstract: Data-centric learning emphasizes curating high-quality training samples to
boost performance rather than designing new architectures. A central problem is
to estimate the influence of training sample efficiently. Prior studies largely
focus on static influence measured on a converged model, overlooking how data
valuation dynamically changes during optimization. This omission neglects the
dynamic nature of sample influence during optimization, especially in deep
models. To address the computational burden of frequent influence estimation,
we develop a layer-aware online estimator that requires only loss-to-output
gradients. This design avoids parameter-level and full-network gradients while
preserving ranking fidelity. Extensive experiments across LLM pretraining,
fine-tuning, and image classification show our method improves accuracy with
substantially lower time and memory cost, making dynamic data curation
efficient and scalable in practice.

</details>


### [180] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文基于RML开发了新型基于语言的奖励机器，可指定非正则、非马尔可夫任务的奖励函数，实验证明其表达能力及优势。


<details>
  <summary>Details</summary>
Motivation: 强化学习中奖励函数定义不精确会导致意外有害行为，现有奖励机器表达能力受正则语言限制。

Method: 基于Runtime Monitoring Language (RML)开发新型基于语言的奖励机器，利用RML内置内存。

Result: 通过实验展示了该方法的表达能力，以及在灵活事件处理和任务规范方面比现有基于奖励机器方法的额外优势。

Conclusion: 所提出的基于RML的奖励机器能有效解决现有奖励机器表达能力不足问题，在复杂任务中有更好表现。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [181] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 现有时间序列基础模型在处理多变量时间序列异常检测时忽视离散状态变量特性，本文提出STAR模块提升其处理状态变量能力，实验证明可提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有TSFMs在处理含离散状态变量的多变量时间序列异常检测时，未考虑状态变量的类别特性，导致检测性能下降。

Method: 提出STAR模块，包含身份引导的状态编码器、条件瓶颈适配器和数字 - 状态匹配模块。

Result: 在真实数据集上的大量实验表明，STAR能提高现有TSFMs在MTSAD上的性能。

Conclusion: STAR模块可有效增强TSFMs对状态变量的建模和利用能力，提升异常检测性能。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [182] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [183] [Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach](https://arxiv.org/abs/2510.16015)
*Qian Sun,Graham Hults,Susu Xu*

Main category: cs.LG

TL;DR: 传统洪水管理系统决策受情境感知限制，本文提出决策导向框架优化传感器位置与洪水预测模型，含四个组件并结合I - MLE等方法。


<details>
  <summary>Details</summary>
Motivation: 传统洪水管理系统因预算和数据获取限制导致情境感知有限，且决策策略固定、忽略对决策的影响，阻碍及时可靠决策。

Method: 引入决策导向框架，集成上下文评分网络、传感器选择模块、洪水重建与预测模型和决策层；采用I - MLE进行离散传感器配置的梯度学习，用概率决策头近似各种灾害响应任务。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Timely and reliable decision-making is vital for flood emergency response,
yet it remains severely hindered by limited and imprecise situational awareness
due to various budget and data accessibility constraints. Traditional flood
management systems often rely on in-situ sensors to calibrate remote
sensing-based large-scale flood depth forecasting models, and further take
flood depth estimates to optimize flood response decisions. However, these
approaches often take fixed, decision task-agnostic strategies to decide where
to put in-situ sensors (e.g., maximize overall information gain) and train
flood forecasting models (e.g., minimize average forecasting errors), but
overlook that systems with the same sensing gain and average forecasting errors
may lead to distinct decisions. To address this, we introduce a novel
decision-focused framework that strategically selects locations for in-situ
sensor placement and optimize spatio-temporal flood forecasting models to
optimize downstream flood response decision regrets. Our end-to-end pipeline
integrates four components: a contextual scoring network, a differentiable
sensor selection module under hard budget constraints, a spatio-temporal flood
reconstruction and forecasting model, and a differentiable decision layer
tailored to task-specific objectives. Central to our approach is the
incorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable
gradient-based learning over discrete sensor configurations, and probabilistic
decision heads to enable differentiable approximation to various constrained
disaster response tasks.

</details>


### [184] [Benchmarking noisy label detection methods](https://arxiv.org/abs/2510.16211)
*Henrique Pickler,Jorge K. S. Kamassury,Danilo Silva*

Main category: cs.LG

TL;DR: 对标签噪声检测方法进行全面基准测试，分解检测方法，提出统一基准任务和新指标，评估跨多种数据集，找出表现最佳组合，为设计和选择方法提供指导。


<details>
  <summary>Details</summary>
Motivation: 标签噪声影响模型训练和验证，现有检测技术缺乏最优方法共识。

Method: 将检测方法分解为三个基本组件，提出统一基准任务和新指标，在多种数据集和噪声条件下评估。

Result: 在大多数场景下，使用平均概率聚合的样本内信息收集结合对数优势作为标签一致性函数取得最佳结果。

Conclusion: 研究结果为设计新检测方法和特定应用选择技术提供了实用指导。

Abstract: Label noise is a common problem in real-world datasets, affecting both model
training and validation. Clean data are essential for achieving strong
performance and ensuring reliable evaluation. While various techniques have
been proposed to detect noisy labels, there is no clear consensus on optimal
approaches. We perform a comprehensive benchmark of detection methods by
decomposing them into three fundamental components: label agreement function,
aggregation method, and information gathering approach (in-sample vs
out-of-sample). This decomposition can be applied to many existing detection
methods, and enables systematic comparison across diverse approaches. To fairly
compare methods, we propose a unified benchmark task, detecting a fraction of
training samples equal to the dataset's noise rate. We also introduce a novel
metric: the false negative rate at this fixed operating point. Our evaluation
spans vision and tabular datasets under both synthetic and real-world noise
conditions. We identify that in-sample information gathering using average
probability aggregation combined with the logit margin as the label agreement
function achieves the best results across most scenarios. Our findings provide
practical guidance for designing new detection methods and selecting techniques
for specific applications.

</details>


### [185] [Transfer learning strategies for accelerating reinforcement-learning-based flow control](https://arxiv.org/abs/2510.16016)
*Saeed Salehi*

Main category: cs.LG

TL;DR: 本文研究迁移学习策略加速混沌流体多保真度控制的深度强化学习，对比PNNs和传统微调策略。


<details>
  <summary>Details</summary>
Motivation: 探索迁移学习策略加速混沌流体多保真度控制的深度强化学习。

Method: 首次将PNNs用于基于DRL的流控制，对传统微调策略进行全面基准测试，用KS系统作为基准。

Result: 微调能加速收敛但对预训练时长敏感且易灾难性遗忘；PNNs能稳定高效迁移，对过拟合有鲁棒性，在源和目标环境差异大时仍有效。

Conclusion: 新型迁移学习框架在流控制中有潜力，可应用于更复杂流配置。

Abstract: This work investigates transfer learning strategies to accelerate deep
reinforcement learning (DRL) for multifidelity control of chaotic fluid flows.
Progressive neural networks (PNNs), a modular architecture designed to preserve
and reuse knowledge across tasks, are employed for the first time in the
context of DRL-based flow control. In addition, a comprehensive benchmarking of
conventional fine-tuning strategies is conducted, evaluating their performance,
convergence behavior, and ability to retain transferred knowledge. The
Kuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how
knowledge encoded in control policies, trained in low-fidelity environments,
can be effectively transferred to high-fidelity settings. Systematic
evaluations show that while fine-tuning can accelerate convergence, it is
highly sensitive to pretraining duration and prone to catastrophic forgetting.
In contrast, PNNs enable stable and efficient transfer by preserving prior
knowledge and providing consistent performance gains, and are notably robust to
overfitting during the pretraining phase. Layer-wise sensitivity analysis
further reveals how PNNs dynamically reuse intermediate representations from
the source policy while progressively adapting deeper layers to the target
task. Moreover, PNNs remain effective even when the source and target
environments differ substantially, such as in cases with mismatched physical
regimes or control objectives, where fine-tuning strategies often result in
suboptimal adaptation or complete failure of knowledge transfer. The results
highlight the potential of novel transfer learning frameworks for robust,
scalable, and computationally efficient flow control that can potentially be
applied to more complex flow configurations.

</details>


### [186] [One-Bit Quantization for Random Features Models](https://arxiv.org/abs/2510.16250)
*Danil Akhtiamov,Reza Ghane,Babak Hassibi*

Main category: cs.LG

TL;DR: 本文分析随机特征模型中的一位量化，证明渐近情况下除最后一层外量化权重不损失泛化误差，还通过实验验证加速效果，并对泛化误差进行刻画，结果比以往更通用。


<details>
  <summary>Details</summary>
Motivation: 神经网络发展带来计算和内存需求，一位权重压缩受关注，但相关理论基础不明，需填补此空白。

Method: 在随机特征模型中分析一位量化。

Result: 理论上证明除最后一层外量化权重不损失泛化误差；实验表明一位量化能显著加速随机特征模型推理；对任意层数的随机特征模型泛化误差进行渐近精确刻画。

Conclusion: 研究为神经网络压缩提供理论见解，证明了方法的实用性，结果比以往研究更具通用性。

Abstract: Recent advances in neural networks have led to significant computational and
memory demands, spurring interest in one-bit weight compression to enable
efficient inference on resource-constrained devices. However, the theoretical
underpinnings of such compression remain poorly understood. We address this gap
by analyzing one-bit quantization in the Random Features model, a simplified
framework that corresponds to neural networks with random representations. We
prove that, asymptotically, quantizing weights of all layers except the last
incurs no loss in generalization error, compared to the full precision random
features model. Our findings offer theoretical insights into neural network
compression. We also demonstrate empirically that one-bit quantization leads to
significant inference speed ups for the Random Features models even on a laptop
GPU, confirming the practical benefits of our work. Additionally, we provide an
asymptotically precise characterization of the generalization error for Random
Features with an arbitrary number of layers. To the best of our knowledge, our
analysis yields more general results than all previous works in the related
literature.

</details>


### [187] [Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality](https://arxiv.org/abs/2510.16020)
*Sangjoon Lee,Haris Moazam Sheikh*

Main category: cs.LG

TL;DR: 本文介绍用于翼型优化的AirDbM方法，可减少设计空间维度，在多目标优化中表现良好，对强化学习代理也有出色适应性。


<details>
  <summary>Details</summary>
Motivation: 有效翼型几何优化需用尽可能少的设计变量探索多样设计，需降低设计空间维度。

Method: 从UIUC翼型数据库中选出12个最优基线翼型，通过顺序添加增加设计容量最大的基线，用这些基线重构数据库。

Result: 能以低于0.005的平均绝对误差重构99%的数据库，多目标优化中收敛快，帕累托前沿超体积更大，发现新的最优解，在强化学习中生成翼型几何表现出色。

Conclusion: AirDbM方法在翼型优化和机器学习驱动设计方面有广阔潜力。

Abstract: Effective airfoil geometry optimization requires exploring a diverse range of
designs using as few design variables as possible. This study introduces
AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil
optimization that systematically reduces design-space dimensionality. AirDbM
selects an optimal set of 12 baseline airfoils from the UIUC airfoil database,
which contains over 1,600 shapes, by sequentially adding the baseline that most
increases the design capacity. With these baselines, AirDbM reconstructs 99 \%
of the database with a mean absolute error below 0.005, which matches the
performance of a previous DbM approach that used more baselines. In
multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence
and achieves a Pareto front with a greater hypervolume than that of the
previous larger-baseline study, where new Pareto-optimal solutions are
discovered with enhanced lift-to-drag ratios at moderate stall tolerances.
Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement
learning (RL) agents in generating airfoil geometry when compared to
conventional airfoil parameterization methods, implying the broader potential
of DbM in machine learning-driven design.

</details>


### [188] [Protein Folding with Neural Ordinary Differential Equations](https://arxiv.org/abs/2510.16253)
*Arielle Sanford,Shuo Sun,Christian B. Mendl*

Main category: cs.LG

TL;DR: 受Neural ODE启发，提出连续深度Evoformer，减少资源消耗，为蛋白质结构预测提供新方向。


<details>
  <summary>Details</summary>
Motivation: 原Evoformer深度大，计算成本高且层离散化，需改进。

Method: 用Neural ODE参数化替代48个离散块，保留核心注意力操作，通过伴随方法实现恒定内存成本，用自适应ODE求解器平衡运行时间和准确性。

Result: 能产生结构合理的预测，捕捉特定二级结构元素，但未完全达到原架构精度，训练仅需17.5小时单GPU。

Conclusion: 连续深度模型有望成为生物分子建模的轻量级、可解释替代方案，为高效自适应蛋白质结构预测框架开辟新方向。

Abstract: Recent advances in protein structure prediction, such as AlphaFold, have
demonstrated the power of deep neural architectures like the Evoformer for
capturing complex spatial and evolutionary constraints on protein conformation.
However, the depth of the Evoformer, comprising 48 stacked blocks, introduces
high computational costs and rigid layerwise discretization. Inspired by Neural
Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth
formulation of the Evoformer, replacing its 48 discrete blocks with a Neural
ODE parameterization that preserves its core attention-based operations. This
continuous-time Evoformer achieves constant memory cost (in depth) via the
adjoint method, while allowing a principled trade-off between runtime and
accuracy through adaptive ODE solvers. Benchmarking on protein structure
prediction tasks, we find that the Neural ODE-based Evoformer produces
structurally plausible predictions and reliably captures certain secondary
structure elements, such as alpha-helices, though it does not fully replicate
the accuracy of the original architecture. However, our model achieves this
performance using dramatically fewer resources, just 17.5 hours of training on
a single GPU, highlighting the promise of continuous-depth models as a
lightweight and interpretable alternative for biomolecular modeling. This work
opens new directions for efficient and adaptive protein structure prediction
frameworks.

</details>


### [189] [Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior](https://arxiv.org/abs/2510.16356)
*Fuqun Han,Stanley Osher,Wuchen Li*

Main category: cs.LG

TL;DR: 提出结合数据分布先验信息的稀疏Transformer架构，经理论和实验验证其比经典方法更优。


<details>
  <summary>Details</summary>
Motivation: 受正则化Wasserstein近端算子这一特殊最优传输问题启发，改进优化问题凸性并促进样本稀疏性。

Method: 将数据分布先验信息直接融入神经网络的Transformer结构，设计稀疏Transformer架构。

Result: 与经典基于流的模型相比，改进了优化问题的凸性，促进样本稀疏性；比经典基于神经ODE的方法有更高精度和更快收敛速度。

Conclusion: 稀疏Transformer在生成建模和贝叶斯逆问题等应用中表现良好，优于经典方法。

Abstract: In this work, we propose a sparse transformer architecture that incorporates
prior information about the underlying data distribution directly into the
transformer structure of the neural network. The design of the model is
motivated by a special optimal transport problem, namely the regularized
Wasserstein proximal operator, which admits a closed-form solution and turns
out to be a special representation of transformer architectures. Compared with
classical flow-based models, the proposed approach improves the convexity
properties of the optimization problem and promotes sparsity in the generated
samples. Through both theoretical analysis and numerical experiments, including
applications in generative modeling and Bayesian inverse problems, we
demonstrate that the sparse transformer achieves higher accuracy and faster
convergence to the target distribution than classical neural ODE-based methods.

</details>


### [190] [Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making](https://arxiv.org/abs/2510.16462)
*Emmanuelle Claeys,Elena Kerjean,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出用于模仿学习的顺序强化学习框架以建模传粉者认知策略，解决现有方法不足，推动传粉者认知研究。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在建模传粉者不同认知策略时存在不足，缺乏可解释性，难以提供生物学见解。

Method: 引入最小化预测损失的模型，确定与行为数据最一致的有效记忆范围，保证可解释性，提供数学框架并发布新数据集。

Result: 现有方法在该场景常失败，新方法可解决相关挑战，促进传粉者认知研究和生态治理。

Conclusion: 新框架为传粉者决策的学习策略和记忆相互作用提供新见解。

Abstract: We introduce a sequential reinforcement learning framework for imitation
learning designed to model heterogeneous cognitive strategies in pollinators.
Focusing on honeybees, our approach leverages trajectory similarity to capture
and forecast behavior across individuals that rely on distinct strategies: some
exploiting numerical cues, others drawing on memory, or being influenced by
environmental factors such as weather. Through empirical evaluation, we show
that state-of-the-art imitation learning methods often fail in this setting:
when expert policies shift across memory windows or deviate from optimality,
these models overlook both fast and slow learning behaviors and cannot
faithfully reproduce key decision patterns. Moreover, they offer limited
interpretability, hindering biological insight. Our contribution addresses
these challenges by (i) introducing a model that minimizes predictive loss
while identifying the effective memory horizon most consistent with behavioral
data, and (ii) ensuring full interpretability to enable biologists to analyze
underlying decision-making strategies and finally (iii) providing a
mathematical framework linking bee policy search with bandit formulations under
varying exploration-exploitation dynamics, and releasing a novel dataset of 80
tracked bees observed under diverse weather conditions. This benchmark
facilitates research on pollinator cognition and supports ecological governance
by improving simulations of insect behavior in agroecosystems. Our findings
shed new light on the learning strategies and memory interplay shaping
pollinator decision-making.

</details>


### [191] [Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model](https://arxiv.org/abs/2510.16023)
*Fanmeng Wang,Shan Mei,Wentao Guo,Hongshuai Wang,Qi Ou,Zhifeng Gao,Hongteng Xu*

Main category: cs.LG

TL;DR: 本文提出首个聚合物基础模型PolyConFM，通过以构象为中心的生成式预训练统一聚合物建模与设计，实验表明其在下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅用单体级描述符表示聚合物，忽略全局结构信息，且缺乏通用基础模型，限制了聚合物科学发展。

Method: 提出PolyConFM模型，在条件生成范式下进行预训练，通过掩码自回归建模重建局部构象并生成方向变换以恢复聚合物构象，构建高质量聚合物构象数据集。

Result: PolyConFM在多样下游任务中始终优于代表性特定任务方法。

Conclusion: PolyConFM为聚合物科学提供了通用且强大的工具。

Abstract: Polymers, macromolecules formed from covalently bonded monomers, underpin
countless technologies and are indispensable to modern life. While deep
learning is advancing polymer science, existing methods typically represent the
whole polymer solely through monomer-level descriptors, overlooking the global
structural information inherent in polymer conformations, which ultimately
limits their practical performance. Moreover, this field still lacks a
universal foundation model that can effectively support diverse downstream
tasks, thereby severely constraining progress. To address these challenges, we
introduce PolyConFM, the first polymer foundation model that unifies polymer
modeling and design through conformation-centric generative pretraining.
Recognizing that each polymer conformation can be decomposed into a sequence of
local conformations (i.e., those of its repeating units), we pretrain PolyConFM
under the conditional generation paradigm, reconstructing these local
conformations via masked autoregressive (MAR) modeling and further generating
their orientation transformations to recover the corresponding polymer
conformation. Besides, we construct the first high-quality polymer conformation
dataset via molecular dynamics simulations to mitigate data sparsity, thereby
enabling conformation-centric pretraining. Experiments demonstrate that
PolyConFM consistently outperforms representative task-specific methods on
diverse downstream tasks, equipping polymer science with a universal and
powerful tool.

</details>


### [192] [Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.16511)
*Dongchan Cho,Jiho Han,Keumyeong Kang,Minsang Kim,Honggyu Ryu,Namsoon Jung*

Main category: cs.LG

TL;DR: 本文提出简单可解释的无监督框架OracleAD用于多变量时间序列异常检测，通过编码、自注意力机制等实现异常诊断和根因定位，在多数据集取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现实世界多变量时间序列异常罕见且无标签，现有方法依赖复杂架构，只能检测异常片段且夸大性能。

Method: 将变量过去序列编码为因果嵌入，预测当前时间点和重构输入窗口；用自注意力机制将嵌入投影到共享潜在空间；将投影嵌入与代表正常状态关系的稳定潜在结构（SLS）对齐；使用基于预测误差和偏离SLS的双评分机制识别异常。

Result: 在多个真实世界数据集和评估协议上取得了最先进的结果。

Conclusion: OracleAD是一个简单、可解释的无监督多变量时间序列异常检测框架，能有效进行异常诊断和根因定位。

Abstract: Real-world multivariate time series anomalies are rare and often unlabeled.
Additionally, prevailing methods rely on increasingly complex architectures
tuned to benchmarks, detecting only fragments of anomalous segments and
overstating performance. In this paper, we introduce OracleAD, a simple and
interpretable unsupervised framework for multivariate time series anomaly
detection. OracleAD encodes each variable's past sequence into a single causal
embedding to jointly predict the present time point and reconstruct the input
window, effectively modeling temporal dynamics. These embeddings then undergo a
self-attention mechanism to project them into a shared latent space and capture
spatial relationships. These relationships are not static, since they are
modeled by a property that emerges from each variable's temporal dynamics. The
projected embeddings are aligned to a Stable Latent Structure (SLS)
representing normal-state relationships. Anomalies are identified using a dual
scoring mechanism based on prediction error and deviation from the SLS,
enabling fine-grained anomaly diagnosis at each time point and across
individual variables. Since any noticeable SLS deviation originates from
embeddings that violate the learned temporal causality of normal data, OracleAD
directly pinpoints the root-cause variables at the embedding level. OracleAD
achieves state-of-the-art results across multiple real-world datasets and
evaluation protocols, while remaining interpretable through SLS.

</details>


### [193] [A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data](https://arxiv.org/abs/2510.16026)
*Marco Barbero-Mota,Eric V. Strobl,John M. Still,William W. Stead,Thomas A. Lasko*

Main category: cs.LG

TL;DR: 介绍可泛化的因果机器学习管道，用于发现电子病历潜在因果源和量化因果效应，并展示其在实际医疗中的应用。


<details>
  <summary>Details</summary>
Motivation: 发现大规模电子健康记录观测的潜在因果源并量化其对临床结果的因果效应。

Method: 处理不完美的多模态临床数据，将其分解为概率独立的潜在源，训练特定任务的因果模型来估计个体因果效应。

Result: 该方法在两个真实世界应用中有相关发现。

Conclusion: 该方法具有通用性和实用性，可用于大规模医疗发现。

Abstract: We provide an accessible description of a peer-reviewed generalizable causal
machine learning pipeline to (i) discover latent causal sources of large-scale
electronic health records observations, and (ii) quantify the source causal
effects on clinical outcomes. We illustrate how imperfect multimodal clinical
data can be processed, decomposed into probabilistic independent latent
sources, and used to train taskspecific causal models from which individual
causal effects can be estimated. We summarize the findings of the two
real-world applications of the approach to date as a demonstration of its
versatility and utility for medical discovery at scale.

</details>


### [194] [eDCF: Estimating Intrinsic Dimension using Local Connectivity](https://arxiv.org/abs/2510.16513)
*Dhruv Gupta,Aditya Nagarsekar,Vraj Shah,Sujith Thomas*

Main category: cs.LG

TL;DR: 本文提出新方法eDCF跨尺度稳健估计数据集本征维数，在合成基准测试表现良好，还能检测分形几何。


<details>
  <summary>Details</summary>
Motivation: 现代数据集特征高维且依赖复杂，本征维数估计因依赖尺度而具挑战性，需新方法解决。

Method: 引入基于连接因子（CF）的可扩展、可并行化方法eDCF来跨尺度稳健估计本征维数。

Result: 在含噪声样本的合成基准测试中，eDCF的平均绝对误差（MAE）与领先估计器相当，精确本征维数匹配率更高，尤其在中高噪声和大数据集下表现出色，还能准确检测分形几何。

Conclusion: eDCF方法可有效用于分析现实、结构化数据。

Abstract: Modern datasets often contain high-dimensional features exhibiting complex
dependencies. To effectively analyze such data, dimensionality reduction
methods rely on estimating the dataset's intrinsic dimension (id) as a measure
of its underlying complexity. However, estimating id is challenging due to its
dependence on scale: at very fine scales, noise inflates id estimates, while at
coarser scales, estimates stabilize to lower, scale-invariant values. This
paper introduces a novel, scalable, and parallelizable method called eDCF,
which is based on Connectivity Factor (CF), a local connectivity-based metric,
to robustly estimate intrinsic dimension across varying scales. Our method
consistently matches leading estimators, achieving comparable values of mean
absolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our
approach also attains higher exact intrinsic dimension match rates, reaching up
to 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling
under medium to high noise levels and large datasets. Further, we showcase our
method's ability to accurately detect fractal geometries in decision
boundaries, confirming its utility for analyzing realistic, structured data.

</details>


### [195] [RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction](https://arxiv.org/abs/2510.16035)
*Yingguang Yang,Xianghua Zeng,Qi Wu,Hao Peng,Yutong Xia,Hao Liu,Bin Chong,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出针对基于GNN的社交机器人检测器的对抗性多智能体强化学习框架RoBCtrl，实验证明可有效削弱GNN检测器性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的社交机器人检测方法的脆弱性和鲁棒性未充分探索，且存在控制社交主体受限、检测器黑盒、机器人异质性等问题。

Method: 用扩散模型生成高保真机器人账户以逃避检测；用多智能体强化学习方法模拟机器人对抗行为；按影响力和预算对社交账户分类并控制；设计基于结构熵的分层状态抽象加速强化学习。

Result: 在社交机器人检测数据集上的大量实验表明框架能有效削弱基于GNN的检测器性能。

Conclusion: 所提出的RoBCtrl框架是应对基于GNN的社交机器人检测方法挑战的有效方案。

Abstract: Social networks have become a crucial source of real-time information for
individuals. The influence of social bots within these platforms has garnered
considerable attention from researchers, leading to the development of numerous
detection technologies. However, the vulnerability and robustness of these
detection methods is still underexplored. Existing Graph Neural Network
(GNN)-based methods cannot be directly applied due to the issues of limited
control over social agents, the black-box nature of bot detectors, and the
heterogeneity of bots. To address these challenges, this paper proposes the
first adversarial multi-agent Reinforcement learning framework for social Bot
control attacks (RoBCtrl) targeting GNN-based social bot detectors.
Specifically, we use a diffusion model to generate high-fidelity bot accounts
by reconstructing existing account data with minor modifications, thereby
evading detection on social platforms. To the best of our knowledge, this is
the first application of diffusion models to mimic the behavior of evolving
social bots effectively. We then employ a Multi-Agent Reinforcement Learning
(MARL) method to simulate bots adversarial behavior. We categorize social
accounts based on their influence and budget. Different agents are then
employed to control bot accounts across various categories, optimizing the
attachment strategy through reinforcement learning. Additionally, a
hierarchical state abstraction based on structural entropy is designed to
accelerate the reinforcement learning. Extensive experiments on social bot
detection datasets demonstrate that our framework can effectively undermine the
performance of GNN-based detectors.

</details>


### [196] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: 论文指出大语言模型（LLMs）在因果发现评估存在基准数据集泄露问题，提出开发抗泄漏评估协议和设计混合方法，实验表明混合方法能提升准确性。


<details>
  <summary>Details</summary>
Motivation: 质疑LLMs在因果发现评估中的表现，探讨其是否真能推理因果结构及能否用于现实科研。

Method: 提出开发基于科学研究的抗泄漏评估协议，设计结合LLM知识与数据统计的混合方法；用新的科学研究评估，提取训练截止后文献中的因果图；用LLM预测作为经典PC算法的先验。

Result: LLMs在新图上表现远不如在BNLearn基准上，用LLM预测作为PC算法先验能显著提升准确性。

Conclusion: 呼吁社区采用基于科学、抗泄漏的基准，投资适合现实研究的混合因果发现方法。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [197] [Vector Quantization in the Brain: Grid-like Codes in World Models](https://arxiv.org/abs/2510.16039)
*Xiangyuan Peng,Xingsi Dong,Si Wu*

Main category: cs.LG

TL;DR: 提出脑启发方法GCQ压缩观测 - 动作序列，实验证明其有效性，提供计算工具和理论视角


<details>
  <summary>Details</summary>
Motivation: 寻找更好的方法将观测 - 动作序列压缩为离散表示，实现时空联合压缩

Method: 利用吸引子动力学中的网格状模式，通过动作条件码本进行时空压缩，码词来自连续吸引子神经网络并基于动作动态选择

Result: 实验表明GCQ在紧凑编码和下游任务表现上有效

Conclusion: GCQ可作为高效序列建模的计算工具，为神经系统中网格状代码形成提供理论视角

Abstract: We propose Grid-like Code Quantization (GCQ), a brain-inspired method for
compressing observation-action sequences into discrete representations using
grid-like patterns in attractor dynamics. Unlike conventional vector
quantization approaches that operate on static inputs, GCQ performs
spatiotemporal compression through an action-conditioned codebook, where
codewords are derived from continuous attractor neural networks and dynamically
selected based on actions. This enables GCQ to jointly compress space and time,
serving as a unified world model. The resulting representation supports
long-horizon prediction, goal-directed planning, and inverse modeling.
Experiments across diverse tasks demonstrate GCQ's effectiveness in compact
encoding and downstream performance. Our work offers both a computational tool
for efficient sequence modeling and a theoretical perspective on the formation
of grid-like codes in neural systems.

</details>


### [198] [Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations](https://arxiv.org/abs/2510.16591)
*Cassidy Ashworth,Pietro Liò,Francesco Caso*

Main category: cs.LG

TL;DR: 本文评估神经网络学习重正化群变换时参数对称性和网络表达能力对泛化行为的影响，揭示对称性约束和表达能力间的竞争关系，为对称网络学习动态及建模物理变换局限提供新见解。


<details>
  <summary>Details</summary>
Motivation: 编码物理对称性可提升深度学习模型性能，研究参数对称性和网络表达能力在神经网络泛化行为中的作用。

Method: 以中心极限定理为测试用例，考虑简单多层感知机和图神经网络，改变架构中的权重对称性和激活函数；将中心极限定理重铸为累积量递归关系分析MLP架构泛化行为；经验验证从MLP到GNN的框架扩展。

Result: 对称性约束和表达能力存在竞争，过于复杂或约束过多的模型泛化能力差；分析证明特定约束MLP架构的泛化不佳；经验验证框架从MLP到GNN的扩展。

Conclusion: 研究为对称网络学习动态及其在建模结构化物理变换中的局限性提供了新的见解。

Abstract: Deep learning models have proven enormously successful at using multiple
layers of representation to learn relevant features of structured data.
Encoding physical symmetries into these models can improve performance on
difficult tasks, and recent work has motivated the principle of parameter
symmetry breaking and restoration as a unifying mechanism underlying their
hierarchical learning dynamics. We evaluate the role of parameter symmetry and
network expressivity in the generalisation behaviour of neural networks when
learning a real-space renormalisation group (RG) transformation, using the
central limit theorem (CLT) as a test case map. We consider simple multilayer
perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries
and activation functions across architectures. Our results reveal a competition
between symmetry constraints and expressivity, with overly complex or
overconstrained models generalising poorly. We analytically demonstrate this
poor generalisation behaviour for certain constrained MLP architectures by
recasting the CLT as a cumulant recursion relation and making use of an
established framework to propagate cumulants through MLPs. We also empirically
validate an extension of this framework from MLPs to GNNs, elucidating the
internal information processing performed by these more complex models. These
findings offer new insight into the learning dynamics of symmetric networks and
their limitations in modelling structured physical transformations.

</details>


### [199] [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)
*Mengtao Lv,Ruiqi Zhu,Xinyu Wang,Yun Li*

Main category: cs.LG

TL;DR: 本文提出 AMS - Quant 方法，将浮点量化探索从整数位宽拓展到非整数位宽，可显著加速大语言模型推理且精度损失小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数多带来存储和效率瓶颈，浮点量化可加速推理，为进一步接近量化最佳点，探索非整数位宽。

Method: 提出 Mantissa - bit Sharing 让量化权重共享尾数最低有效位，引入 Adaptive Searching 采用离线优化策略减少精度损失，还制作 CUDA 线性内核。

Result: 能将模型量化到 FP - 5.33 - e2m3 和 FP4.25 - e2m2，相比 FP16 推理，LLM 解码加速 2.8 倍和 3.2 倍，精度损失可忽略。

Conclusion: AMS - Quant 能有效加速大语言模型推理，在非整数位宽量化上有良好效果。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various kinds of tasks, while the billion or even trillion parameters bring
storage and efficiency bottlenecks for inference. Quantization, particularly
floating-point quantization, is known to be capable of speeding up LLM
inference by reducing memory footprint and data movement during the inference
process. For the first time, we advance the floating-point quantization
exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,
to further approach the quantization sweet spot. AMS-Quant incorporates two
novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,
which groups k quantized weights and lets them share the least significant
mantissa bit, allowing us to further approach the minimum quantization
bit-width without accuracy loss. (2) It introduces Adaptive Searching, which
employs an offline optimization strategy to minimize the accuracy degradation
introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA
Linear kernels, which translates memory savings into wall-clock latency
reduction by reducing memory access. Extensive experiments on large-scale
datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3
and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16
inference (2.8x and 3.2x), with negligible accuracy loss.

</details>


### [200] [GUIrilla: A Scalable Framework for Automated Desktop UI Exploration](https://arxiv.org/abs/2510.16051)
*Sofiya Garkot,Maksym Shamrai,Ivan Synytsia,Mariya Hirna*

Main category: cs.LG

TL;DR: 提出GUIrilla框架解决GUI自动化数据收集问题，构建GUIrilla - Task数据集，调优LLM代理可提升下游UI任务性能并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽提升UI理解，但在全窗口、多应用桌面环境导航仍有挑战，且数据获取受限。

Method: 引入GUIrilla框架，通过原生可访问性API探索应用，将界面元素和爬虫动作组织成层次化GUI图，使用专门交互处理程序。

Result: 构建并发布包含27,171个任务的GUIrilla - Task数据集，调优LLM代理在下游UI任务上性能提升，在ScreenSpot Pro基准测试中优于合成基线且数据使用量减少97%。

Conclusion: 发布macapptree库、数据集、基准测试和框架代码，支持桌面自主性的开放研究。

Abstract: Autonomous agents capable of operating complex graphical user interfaces
(GUIs) have the potential to transform desktop automation. While recent
advances in large language models (LLMs) have significantly improved UI
understanding, navigating full-window, multi-application desktop environments
remains a major challenge. Data availability is limited by costly manual
annotation, closed-source datasets and surface-level synthetic pipelines. We
introduce GUIrilla, an automated scalable framework that systematically
explores applications via native accessibility APIs to address the critical
data collection challenge in GUI automation. Our framework focuses on macOS -
an ecosystem with limited representation in current UI datasets - though many
of its components are designed for broader cross-platform applicability.
GUIrilla organizes discovered interface elements and crawler actions into
hierarchical GUI graphs and employs specialized interaction handlers to achieve
comprehensive application coverage. Using the application graphs from GUIrilla
crawler, we construct and release GUIrilla-Task, a large-scale dataset of
27,171 functionally grounded tasks across 1,108 macOS applications, each
annotated with full-desktop and window-level screenshots, accessibility
metadata, and semantic action traces. Empirical results show that tuning
LLM-based agents on GUIrilla-Task significantly improves performance on
downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro
benchmark while using 97% less data. We also release macapptree, an open-source
library for reproducible collection of structured accessibility metadata, along
with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold
benchmark, and the framework code to support open research in desktop autonomy.

</details>


### [201] [Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees](https://arxiv.org/abs/2510.16974)
*Shurong Lin,Aleksandra Slavković,Deekshith Reddy Bhoomireddy*

Main category: cs.LG

TL;DR: 提出高斯差分隐私下线性回归有效推断方法，实验显示该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私线性回归多关注点估计，对不确定性量化关注有限，且不支持合成数据生成；主流合成数据生成方法不适用于社会科学中的连续小数据集。

Method: 提出高斯差分隐私下线性回归方法，含偏差校正估计器和渐近置信区间，以及通用合成数据生成程序，采用分箱聚合策略。

Result: 实验表明该方法提高了准确性，提供有效置信区间，生成的合成数据用于下游机器学习任务更可靠。

Conclusion: 提出的方法在小到中等维度设置中有效，优于现有差分隐私线性回归和合成数据生成方法。

Abstract: In social sciences, small- to medium-scale datasets are common and linear
regression (LR) is canonical. In privacy-aware settings, much work has focused
on differentially private (DP) LR, but mostly on point estimation with limited
attention to uncertainty quantification. Meanwhile, synthetic data generation
(SDG) is increasingly important for reproducibility studies, yet current DP LR
methods do not readily support it. Mainstream SDG approaches are either
tailored to discretized data, making them less suitable for continuous
regression, or rely on deep models that require large datasets, limiting their
use for the smaller, continuous data typical in social science. We propose a
method for LR with valid inference under Gaussian DP: a DP bias-corrected
estimator with asymptotic confidence intervals (CIs) and a general SDG
procedure in which regression on the synthetic data matches our DP regression.
Our binning-aggregation strategy is effective in small- to moderate-dimensional
settings. Experiments show our method (1) improves accuracy over existing
methods, (2) provides valid CIs, and (3) produces more reliable synthetic data
for downstream ML tasks than current DP SDGs.

</details>


### [202] [FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting](https://arxiv.org/abs/2510.16053)
*Chenyang Yu,Xinpeng Xie,Yan Huang,Chenxi Qiu*

Main category: cs.LG

TL;DR: 准确的交通预测对构建智能交通系统至关重要，深度学习特别是GNN成为主流范式，已有模型在标准数据集表现良好，但处理事件信息的早期方法有局限。


<details>
  <summary>Details</summary>
Motivation: 随着城市化发展，交通拥堵加剧，需要可靠且能应对事件的交通预测模型。

Method: 早期方法主要依靠手动设计事件特征，如引入手动定义的事件影响分数或构建特定子图。

Result: 早期方法一定程度上增强了对特定事件的响应能力，但依赖专家先验知识，难以泛化到未知事件，且低维手动特征会丢失语义细节。

Conclusion: 当前处理事件信息的方法存在局限性，需探索更好的方法来提高交通预测模型对复杂未知事件的应对能力。

Abstract: Accurate traffic forecasting is a core technology for building Intelligent
Transportation Systems (ITS), enabling better urban resource allocation and
improved travel experiences. With growing urbanization, traffic congestion has
intensified, highlighting the need for reliable and responsive forecasting
models. In recent years, deep learning, particularly Graph Neural Networks
(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can
effectively capture complex spatial dependencies in road network topology and
dynamic temporal evolution patterns in traffic flow data. Foundational models
such as STGCN and GraphWaveNet, along with more recent developments including
STWave and D2STGNN, have achieved impressive performance on standard traffic
datasets. These approaches incorporate sophisticated graph convolutional
structures and temporal modeling mechanisms, demonstrating particular
effectiveness in capturing and forecasting traffic patterns characterized by
periodic regularities. To address this challenge, researchers have explored
various ways to incorporate event information. Early attempts primarily relied
on manually engineered event features. For instance, some approaches introduced
manually defined incident effect scores or constructed specific subgraphs for
different event-induced traffic conditions. While these methods somewhat
enhance responsiveness to specific events, their core drawback lies in a heavy
reliance on domain experts' prior knowledge, making generalization to diverse
and complex unknown events difficult, and low-dimensional manual features often
lead to the loss of rich semantic details.

</details>


### [203] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [204] [Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks](https://arxiv.org/abs/2510.16063)
*Muhy Eddin Za'ter,Bri-Mathias Hodge*

Main category: cs.LG

TL;DR: 本文提出用于变电站级电压估计的分层图神经网络，实验表明该方法比替代数据驱动模型RMSE低2倍，低覆盖率下也能保持高精度。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源渗透和电压变异性增加，传统配电网状态估计技术难以应对稀疏测量和现代馈线规模问题，需更有效的方法。

Method: 提出利用电气拓扑和物理特征的分层图神经网络，基于SMART - DS数据集在多个变电站和DER渗透场景的数千个母线进行训练和评估。

Result: 该方法比替代数据驱动模型RMSE低达2倍，测量覆盖率低至1%时仍保持高精度。

Conclusion: 图神经网络有潜力为配电系统实现可扩展、可重复和数据驱动的电压监测。

Abstract: Accurate voltage estimation in distribution networks is critical for
real-time monitoring and increasing the reliability of the grid. As DER
penetration and distribution level voltage variability increase, robust
distribution system state estimation (DSSE) has become more essential to
maintain safe and efficient operations. Traditional DSSE techniques, however,
struggle with sparse measurements and the scale of modern feeders, limiting
their scalability to large networks. This paper presents a hierarchical graph
neural network for substation-level voltage estimation that exploits both
electrical topology and physical features, while remaining robust to the low
observability levels common to real-world distribution networks. Leveraging the
public SMART-DS datasets, the model is trained and evaluated on thousands of
buses across multiple substations and DER penetration scenarios. Comprehensive
experiments demonstrate that the proposed method achieves up to 2 times lower
RMSE than alternative data-driven models, and maintains high accuracy with as
little as 1\% measurement coverage. The results highlight the potential of GNNs
to enable scalable, reproducible, and data-driven voltage monitoring for
distribution systems.

</details>


### [205] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出基于矩阵自由能的自编码器正则化方案，产生高斯码并应用于欠定逆问题。


<details>
  <summary>Details</summary>
Motivation: 为自编码器引入新的正则化方案，使自编码器产生的代码具有更好的泛化性。

Method: 定义基于代码矩阵奇异值的可微损失函数，通过标准随机梯度训练最小化负矩阵自由能。

Result: 最小化负矩阵自由能可产生类似高斯的代码，且在训练集和测试集上有泛化性。

Conclusion: 提出的矩阵自由能最大化自编码器能可靠产生高斯码，可应用于欠定逆问题。

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [206] [Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions](https://arxiv.org/abs/2510.16064)
*Muhy Eddin Za'ter,Bri-Mathias Hodge,Kyri Baker*

Main category: cs.LG

TL;DR: 提出残差学习范式解决非线性交流最优潮流问题，评估显示有优势，可用于近实时决策。


<details>
  <summary>Details</summary>
Motivation: 解决非线性交流最优潮流问题是实时电网运行的计算瓶颈。

Method: 提出残差学习范式，以直流最优潮流解为基线，学习非线性修正；利用带局部注意力和两级直流特征集成的拓扑感知图神经网络，用物理信息损失训练。

Result: 在57、118和2000节点系统上，均方误差降低约25%，可行性误差最多降低3倍，运行时间最多加速13倍，模型在N - 1故障下保持精度，能有效扩展到大型网络。

Conclusion: 残差学习是线性近似与交流可行最优潮流之间实用且可扩展的桥梁，可实现近实时运行决策。

Abstract: Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major
computational bottleneck for real-time grid operations. In this paper, we
propose a residual learning paradigm that uses fast DC optimal power flow (DC
OPF) solutions as a baseline, and learns only the nonlinear corrections
required to provide the full AC-OPF solution. The method utilizes a
topology-aware Graph Neural Network with local attention and two-level DC
feature integration, trained using a physics-informed loss that enforces AC
power-flow feasibility and operational limits. Evaluations on OPFData for 57-,
118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in
feasibility error, and up to 13X runtime speedup compared to conventional AC
OPF solvers. The model maintains accuracy under N-1 contingencies and scales
efficiently to large networks. These results demonstrate that residual learning
is a practical and scalable bridge between linear approximations and
AC-feasible OPF, enabling near real-time operational decision making.

</details>


### [207] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: 提出FedPURIN框架解决PFL通信效率问题，在图像分类基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法通信效率不佳，通信负担大阻碍实际部署。

Method: 提出FedPURIN框架，通过整数规划公式识别关键参数，集成到稀疏聚合方案。

Result: 在标准图像分类基准测试中表现优于现有方法，通过稀疏聚合实现可量化的通信减少。

Conclusion: 该框架为通信高效的PFL建立了新范式，对边缘智能系统有利。

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [208] [Adaptive Discretization for Consistency Models](https://arxiv.org/abs/2510.17266)
*Jiayu Bai,Zhanbo Feng,Zhijie Deng,Tianqi Hou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 提出ADCMs统一框架实现CMs自动自适应离散化，提升训练效率和生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有CMs依赖手动设计离散化方案，需针对不同噪声调度和数据集反复调整。

Method: 将CMs离散化作为优化问题，以局部一致性为优化目标、全局一致性为约束，用拉格朗日乘子权衡，用高斯 - 牛顿法实现自适应离散化。

Result: ADCMs显著提升CMs训练效率，在CIFAR - 10和ImageNet上以最小训练开销取得优越生成性能，对更高级DM变体有强适应性。

Conclusion: 提出的ADCMs框架有效可行，能提升CMs训练效率和性能。

Abstract: Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

</details>


### [209] [MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data](https://arxiv.org/abs/2510.16071)
*Qinxuan Wang,Chuang Wang,Mingyu Zhang,Jingwei Sun,Peipei Yang,Shuo Tang,Shiming Xiang*

Main category: cs.LG

TL;DR: 提出多尺度神经算子MNO用于3D非结构化点云CFD，在多个基准测试中表现优于现有方法，证明显式多尺度设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子求解偏微分方程在不规则域上精度和可扩展性有限，特别是处理具有丰富多尺度结构的流体流动。

Method: 引入MNO架构，将信息在三个尺度上分解，包括全局降维注意力模块、局部图注意力模块和微观逐点注意力模块。

Result: 在四个不同基准测试中，MNO始终优于现有基线，预测误差降低5% - 40%，在具有挑战性的3D CFD问题中表现出更好的鲁棒性。

Conclusion: 显式多尺度设计对神经算子很重要，MNO是在不规则域上学习复杂流体动力学的可扩展框架。

Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving
Partial Differential Equations (PDEs), offering orders-of-magnitude
acceleration over traditional solvers. However, existing approaches still
suffer from limited accuracy and scalability, particularly on irregular domains
where fluid flows exhibit rich multiscale structures. In this work, we
introduce the Multiscale Neural Operator (MNO), a new architecture for
Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point
clouds. MNO explicitly decomposes information across three scales: a global
dimension-shrinkage attention module for long-range dependencies, a local graph
attention module for neighborhood-level interactions, and a micro point-wise
attention module for fine-grained details. This design preserves multiscale
inductive biases while remaining computationally efficient. We evaluate MNO on
four diverse benchmarks, covering both steady-state and unsteady flow scenarios
with up to 300K points. Across all tasks, MNO consistently outperforms
state-of-the-art baselines, reducing prediction errors by 5% to 40% and
demonstrating improved robustness in challenging 3D CFD problems. Our results
highlight the importance of explicit multiscale design for neural operators and
establish MNO as a scalable framework for learning complex fluid dynamics on
irregular domains.

</details>


### [210] [Uncertainty-aware data assimilation through variational inference](https://arxiv.org/abs/2510.17268)
*Anthony Frion,David S Greenberg*

Main category: cs.LG

TL;DR: 基于现有确定性机器学习方法，提出基于变分推理的扩展，在Lorenz - 96动力学测试中获良好结果，代码开源。


<details>
  <summary>Details</summary>
Motivation: 数据同化在多数情况下存在不确定性，需改进现有方法。

Method: 在现有确定性机器学习方法基础上，提出基于变分推理的扩展，使预测状态服从多元高斯分布。

Result: 在混沌Lorenz - 96动力学测试中，新模型能获得近乎完美校准的预测，可集成到更广泛变分数据同化流程。

Conclusion: 新模型在数据同化中有良好表现，可利用更长数据同化窗口获得更大收益。

Abstract: Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

</details>


### [211] [Early-stopping for Transformer model training](https://arxiv.org/abs/2510.16074)
*Jing He,Hua Jiang,Cheng Li,Siqian Xin,Shuzhen Yang*

Main category: cs.LG

TL;DR: 本文提出基于随机矩阵理论的框架分析Transformer训练动态，划分训练阶段并提出无验证的停止标准。


<details>
  <summary>Details</summary>
Motivation: 分析Transformer训练动态，找出驱动性能提升的机制并推导早期停止标准。

Method: 基于随机矩阵理论，用PL拟合浅层自注意力矩阵V，划分训练阶段。

Result: 观察到V的谱密度演变成重尾分布，划分训练为三个阶段，提出两个无验证标准。

Conclusion: 随机矩阵理论对监测和诊断Transformer模型训练进展有用。

Abstract: This work introduces a novel theoretical framework grounded in Random Matrix
Theory (RMT) for analyzing Transformer training dynamics. We focus on the
underlying mechanisms that drive performance improvements and derive principled
early-stopping criteria. Empirically, we observe that the spectral density of
the shallow self-attention matrix V consistently evolves into a heavy-tailed
distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we
demarcate training into three stages: structural exploration, heavy-tailed
structure stabilization, and convergence saturation. This staging provides
guidance for preliminary stopping decisions. Crucially, we propose two
consistent and validation-free criteria: a quantitative metric for heavy-tailed
dynamics and a novel spectral signature indicative of convergence. The strong
alignment between these criteria highlights the utility of RMT for monitoring
and diagnosing the progression of Transformer model training.

</details>


### [212] [Symmetries in PAC-Bayesian Learning](https://arxiv.org/abs/2510.17303)
*Armin Beck,Peter Ochs*

Main category: cs.LG

TL;DR: 本文将机器学习模型对称性泛化保证拓展到非紧对称和非不变数据分布情形，通过PAC - Bayes框架改进现有边界，实验验证理论并优于先前结果，为对称模型提供更广泛理论支持。


<details>
  <summary>Details</summary>
Motivation: 先前关于对称性提升机器学习模型性能的理论保证局限于紧群对称和数据分布不变的假设，而现实应用中该假设很少满足，因此需要拓展理论保证。

Method: 基于PAC - Bayes框架，调整并收紧现有边界，以McAllester的PAC - Bayes边界为例说明方法可应用于多种PAC - Bayes边界。

Result: 在旋转MNIST数据集上实验验证了理论，得到的保证不仅成立，还优于先前结果。

Conclusion: 对于对称数据，对称模型在紧群和不变分布之外的更广泛情形下更优，有助于更全面理解机器学习中的对称性。

Abstract: Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

</details>


### [213] [Optimization of the quantization of dense neural networks from an exact QUBO formulation](https://arxiv.org/abs/2510.16075)
*Sergio Muñiz Subiñas,Manuel L. González,Jorge Ruiz Gómez,Alejandro Mata Ali,Jorge Martínez Martín,Miguel Franco Hernando,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 本文提出基于ADAROUND的QUBO公式的PTQ方法，将问题分解为子问题求解，并在多个数据集上评估。


<details>
  <summary>Details</summary>
Motivation: 为密集神经网络引入有效的后训练量化（PTQ）方法。

Method: 以理论输出和反量化输出的Frobenius距离为目标得到显式QUBO，利用系数QUBO矩阵结构将全局问题分解为独立子问题，用模拟退火等启发式方法求解。

Result: 在MNIST、Fashion - MNIST、EMNIST和CIFAR - 10数据集上，从int8到int1整数精度进行了评估，并与传统四舍五入量化方法比较。

Conclusion: 未提及明确结论。

Abstract: This work introduces a post-training quantization (PTQ) method for dense
neural networks via a novel ADAROUND-based QUBO formulation. Using the
Frobenius distance between the theoretical output and the dequantized output
(before the activation function) as the objective, an explicit QUBO whose
binary variables represent the rounding choice for each weight and bias is
obtained. Additionally, by exploiting the structure of the coefficient QUBO
matrix, the global problem can be exactly decomposed into $n$ independent
subproblems of size $f+1$, which can be efficiently solved using some
heuristics such as simulated annealing. The approach is evaluated on MNIST,
Fashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1
and compared with a round-to-nearest traditional quantization methodology.

</details>


### [214] [Exploration via Feature Perturbation in Contextual Bandits](https://arxiv.org/abs/2510.17390)
*Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose feature perturbation, a simple yet powerful technique that injects
randomness directly into feature inputs, instead of randomizing unknown
parameters or adding noise to rewards. Remarkably, this algorithm achieves
$\tilde{\mathcal{O}}(d\sqrt{T})$ worst-case regret bound for generalized linear
bandits, while avoiding the $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ regret
typical of existing randomized bandit algorithms. Because our algorithm eschews
parameter sampling, it is both computationally efficient and naturally extends
to non-parametric or neural network models. We verify these advantages through
empirical evaluations, demonstrating that feature perturbation not only
surpasses existing methods but also unifies strong practical performance with
best-known theoretical guarantees.

</details>


### [215] [RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems](https://arxiv.org/abs/2510.17396)
*Keivan Faghih Niresi,Zepeng Zhang,Olga Fink*

Main category: cs.LG

TL;DR: 提出RINS - T框架解决时间序列线性逆问题，无需预训练数据，有三项创新提升稳定性和鲁棒性，代码开源。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据受缺失值、噪声和离群值等影响，现有深度学习方法需大量预训练且在分布变化时泛化能力差。

Method: 提出RINS - T框架，利用神经网络作为隐式先验，集成鲁棒优化技术，引入引导输入初始化、输入扰动和凸输出组合技术。

Result: RINS - T框架无需预训练数据实现高恢复性能，对离群值有弹性，优化稳定性和鲁棒性提升。

Conclusion: RINS - T是解决复杂现实世界时间序列挑战的灵活有效方案。

Abstract: Time series data are often affected by various forms of corruption, such as
missing values, noise, and outliers, which pose significant challenges for
tasks such as forecasting and anomaly detection. To address these issues,
inverse problems focus on reconstructing the original signal from corrupted
data by leveraging prior knowledge about its underlying structure. While deep
learning methods have demonstrated potential in this domain, they often require
extensive pretraining and struggle to generalize under distribution shifts. In
this work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series
Linear Inverse Problems), a novel deep prior framework that achieves high
recovery performance without requiring pretraining data. RINS-T leverages
neural networks as implicit priors and integrates robust optimization
techniques, making it resilient to outliers while relaxing the reliance on
Gaussian noise assumptions. To further improve optimization stability and
robustness, we introduce three key innovations: guided input initialization,
input perturbation, and convex output combination techniques. Each of these
contributions strengthens the framework's optimization stability and
robustness. These advancements make RINS-T a flexible and effective solution
for addressing complex real-world time series challenges. Our code is available
at https://github.com/EPFL-IMOS/RINS-T.

</details>


### [216] [Continual Knowledge Consolidation LORA for Domain Incremental Learning](https://arxiv.org/abs/2510.16077)
*Naeem Paeedeh,Mahardhika Pratama,Weiping Ding,Jimmy Cao,Wolfgang Mayer,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 本文提出CONEC - LoRA解决领域增量学习（DIL）问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法创建任务特定LoRA时忽略任务间共享知识，推理时任务特定LoRA选择不准确致准确率下降，且现有分类器泛化能力欠佳。

Method: 提出CONEC - LoRA，结合任务共享LoRA和任务特定LoRA，引入随机分类器，部署辅助网络预测任务特定LoRA，采用不同深度网络结构和局部分类器，集成球生成器损失和转换模块。

Result: 在4个流行基准问题上，CONEC - LoRA比现有方法有超5%的优势。

Conclusion: CONEC - LoRA在解决DIL问题上优于现有方法。

Abstract: Domain Incremental Learning (DIL) is a continual learning sub-branch that
aims to address never-ending arrivals of new domains without catastrophic
forgetting problems. Despite the advent of parameter-efficient fine-tuning
(PEFT) approaches, existing works create task-specific LoRAs overlooking shared
knowledge across tasks. Inaccurate selection of task-specific LORAs during
inference results in significant drops in accuracy, while existing works rely
on linear or prototype-based classifiers, which have suboptimal generalization
powers. Our paper proposes continual knowledge consolidation low rank
adaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed
from consolidations between task-shared LORA to extract common knowledge and
task-specific LORA to embrace domain-specific knowledge. Unlike existing
approaches, CONEC-LoRA integrates the concept of a stochastic classifier whose
parameters are sampled from a distribution, thus enhancing the likelihood of
correct classifications. Last but not least, an auxiliary network is deployed
to optimally predict the task-specific LoRAs for inferences and implements the
concept of a different-depth network structure in which every layer is
connected with a local classifier to take advantage of intermediate
representations. This module integrates the ball-generator loss and
transformation module to address the synthetic sample bias problem. Our
rigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in
4 popular benchmark problems with over 5% margins.

</details>


### [217] [Stochastic Difference-of-Convex Optimization with Momentum](https://arxiv.org/abs/2510.17503)
*El Mahdi Chayti,Martin Jaggi*

Main category: cs.LG

TL;DR: 研究随机DC优化在小批量下收敛性，表明动量可使算法在标准假设下对任意批量收敛，无动量可能不收敛，算法有理论收敛性和实证效果。


<details>
  <summary>Details</summary>
Motivation: 随机DC优化在小批量下收敛性不明，现有方法需大批量或强噪声假设，限制实际应用。

Method: 引入动量，在标准平滑性和有界方差假设下分析算法。

Result: 证明无动量时不论步长，收敛可能失败；基于动量的算法有可证明的收敛性，实证表现好。

Conclusion: 动量对随机DC优化在小批量下收敛是必要的，基于动量的算法有效。

Abstract: Stochastic difference-of-convex (DC) optimization is prevalent in numerous
machine learning applications, yet its convergence properties under small batch
sizes remain poorly understood. Existing methods typically require large
batches or strong noise assumptions, which limit their practical use. In this
work, we show that momentum enables convergence under standard smoothness and
bounded variance assumptions (of the concave part) for any batch size. We prove
that without momentum, convergence may fail regardless of stepsize,
highlighting its necessity. Our momentum-based algorithm achieves provable
convergence and demonstrates strong empirical performance.

</details>


### [218] [PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites](https://arxiv.org/abs/2510.16083)
*Jaehan Kim,Minkyoo Song,Minjae Seo,Youngjin Jin,Seungwon Shin,Jinwoo Kim*

Main category: cs.LG

TL;DR: 提出PassREfinder - FL框架预测跨网站凭证填充风险，结合图神经网络和联邦学习，在真实数据集上评估效果好。


<details>
  <summary>Details</summary>
Motivation: 现有检测重用密码用户或恶意登录尝试的方法存在影响可用性、依赖复杂机制难以部署的问题。

Method: 提出密码重用关系概念，用图神经网络进行链接预测任务，结合公共网站信息扩展图，采用联邦学习保护隐私。

Result: 在真实数据集上F1分数达0.9153，基于FL的GNN比其他先进GNN模型性能提升4 - 11%，预测结果可量化为风险分数。

Conclusion: PassREfinder - FL框架有效且性能良好，能量化密码重用风险。

Abstract: Credential stuffing attacks have caused significant harm to online users who
frequently reuse passwords across multiple websites. While prior research has
attempted to detect users with reused passwords or identify malicious login
attempts, existing methods often compromise usability by restricting password
creation or website access, and their reliance on complex account-sharing
mechanisms hinders real-world deployment. To address these limitations, we
propose PassREfinder-FL, a novel framework that predicts credential stuffing
risks across websites. We introduce the concept of password reuse relations --
defined as the likelihood of users reusing passwords between websites -- and
represent them as edges in a website graph. Using graph neural networks (GNNs),
we perform a link prediction task to assess credential reuse risk between
sites. Our approach scales to a large number of arbitrary websites by
incorporating public website information and linking newly observed websites as
nodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a
federated learning (FL) approach that eliminates the need to share user
sensitive information across administrators. Evaluation on a real-world dataset
of 360 million breached accounts from 22,378 websites shows that
PassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further
validate that our FL-based GNN achieves a 4-11% performance improvement over
other state-of-the-art GNN models through an ablation study. Finally, we
demonstrate that the predicted results can be used to quantify password reuse
likelihood as actionable risk scores.

</details>


### [219] [Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment](https://arxiv.org/abs/2510.17543)
*Jiayi Huang,Sangwoo Park,Nicola Paoletti,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 研究边缘 - 云级联确保条件覆盖，提出CAb级联机制，有统计保证，实验显示能保持条件覆盖并减少云卸载。


<details>
  <summary>Details</summary>
Motivation: 边缘智能保证可靠性有挑战，要让边缘云级联保持条件覆盖。

Method: 将边缘到云模型的升级视为多假设检验问题，用保形对齐选择可在边缘处理的输入。

Result: CAb模型级联方法对满足云级条件覆盖的边缘决策平均比例有统计保证，可应用于任意边缘预测集。

Conclusion: CAb级联能维持边缘预测的目标条件覆盖，大幅减少云卸载，适度增加预测集大小。

Abstract: Edge intelligence enables low-latency inference via compact on-device models,
but assuring reliability remains challenging. We study edge-cloud cascades that
must preserve conditional coverage: whenever the edge returns a prediction set,
it should contain the true label with a user-specified probability, as if
produced by the cloud model. We formalize conditional coverage with respect to
the cloud predictive distribution, and introduce a conformal alignment-based
(CAb) cascading mechanism that certifies this property with user control over
the risk level. Our method casts escalation from edge to cloud models as a
multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)
to select which inputs can be safely handled at the edge. The proposed CAb
model cascading method yields statistical guarantees on the average fraction of
edge decisions that satisfy cloud-level conditional coverage. The procedure
applies to arbitrary edge prediction sets, including variants of conformal
prediction (CP), and exposes a tunable trade-off among coverage, deferral rate,
and set size. Experiments on CIFAR-100 image classification and the TeleQnA
question-answering (QA) benchmark show that the proposed CAb cascade maintains
the target conditional coverage for edge predictions while substantially
reducing offloading to the cloud and incurring modest increases in
prediction-set size.

</details>


### [220] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: 将平衡传播（EP）学习扩展到离散和连续复值波系统，在耗散体系有效，适用于多种物理场景，通过数值研究验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 反向传播学习算法在物理神经网络中实现困难，而EP有原位训练潜力，需扩展其应用范围。

Method: 将EP学习扩展到离散和连续复值波系统，用可训练局部势代替可训练节点间连接，在受驱动 - 耗散激子 - 极化子凝聚体中测试。

Result: 在标准基准测试上，包括简单逻辑任务和手写数字识别，显示出稳定收敛。

Conclusion: 为系统控制限于局部参数的物理系统原位学习提供了实用途径。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [221] [FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.16086)
*Ziyang Liu,Pengjunfei Chu,Shuming Dong,Chen Zhang,Mingcheng Li,Jin Wang*

Main category: cs.LG

TL;DR: 提出FSRF框架解决多模态情感分析中模态缺失问题，实验显示其性能优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 以往多模态情感分析研究忽略现实中模态缺失问题，导致泛化性低。

Method: 提出去冗余同异构分解模块，将模态分解为同质、异质和噪声表示并设计约束范式；设计分布对齐自蒸馏模块，利用双向知识转移恢复缺失语义。

Result: 在两个数据集上的综合实验表明，FSRF在不确定模态缺失情况下比先前方法有显著性能优势。

Conclusion: FSRF框架能有效缓解多模态情感分析任务中的模态缺失问题。

Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research
hotspot that aims to utilize multimodal data for human sentiment understanding.
Previous MSA studies have mainly focused on performing interaction and fusion
on complete multimodal data, ignoring the problem of missing modalities in
real-world applications due to occlusion, personal privacy constraints, and
device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework
(FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization
module that factorizes modality into modality-homogeneous,
modality-heterogeneous, and noisy representations and design elaborate
constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that
fully recovers the missing semantics by utilizing bidirectional knowledge
transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a
significant performance advantage over previous methods with uncertain missing
modalities.

</details>


### [222] [Functional Distribution Networks (FDN)](https://arxiv.org/abs/2510.17794)
*Omer Haq*

Main category: cs.LG

TL;DR: 提出Functional Distribution Networks (FDN) 解决概率回归器在分布偏移下过度自信问题，还提出评估协议，旨在使OOD感知、校准良好的神经回归实用且模块化。


<details>
  <summary>Details</summary>
Motivation: 现代概率回归器在分布偏移下常过度自信。

Method: 提出FDN，通过beta - ELBO和蒙特卡罗采样训练，提出评估协议分离插值和外推并强调OOD检查，在标准回归任务上与多种基线模型对比。

Result: 在标准回归任务上进行基准测试，评估了准确性、校准和偏移感知。

Conclusion: 所提出的框架和协议可使OOD感知、校准良好的神经回归实用且模块化。

Abstract: Modern probabilistic regressors often remain overconfident under distribution
shift. We present Functional Distribution Networks (FDN), an input-conditioned
distribution over network weights that induces predictive mixtures whose
dispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo
sampling. We further propose an evaluation protocol that cleanly separates
interpolation from extrapolation and stresses OOD sanity checks (e.g., that
predictive likelihood degrades under shift while in-distribution accuracy and
calibration are maintained). On standard regression tasks, we benchmark against
strong Bayesian, ensemble, dropout, and hypernetwork baselines under matched
parameter and update budgets, and assess accuracy, calibration, and
shift-awareness with standard diagnostics. Together, the framework and protocol
aim to make OOD-aware, well-calibrated neural regression practical and modular.

</details>


### [223] [STABLE: Gated Continual Learning for Large Language Models](https://arxiv.org/abs/2510.16089)
*William Hoy,Nurcin Celik*

Main category: cs.LG

TL;DR: 提出STABLE框架用于大语言模型持续自适应编辑，通过LoRA和门控机制缓解灾难性遗忘，实验表明门控有效且不同策略有不同效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需持续自适应机制但顺序更新会导致灾难性遗忘。

Method: 提出STABLE框架，用LoRA进行参数高效微调，通过三种指标评估候选编辑，超过阈值则对LoRA更新进行缩放或拒绝。

Result: 在Qwen - 2.5 - 7B模型实验中，门控有效缓解遗忘并保留适应性，基于EM的门控在短持续学习序列中累积性能最高，不同门控策略分布偏移相近但准确率不同。

Conclusion: 该方法为持续模型编辑提供原则性方案，使大语言模型在整合新知识时保持可靠性。

Abstract: Large language models (LLMs) increasingly require mechanisms for continual
adaptation without full retraining. However, sequential updates can lead to
catastrophic forgetting, where new edits degrade previously acquired knowledge.
This work presents STABLE, a gated continual self editing framework that
constrains forgetting during sequential updates using parameter efficient fine
tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate
edit is evaluated against a stability budget using one of three metrics: (i)
Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,
reflecting reduced model confidence; and (iii) KL divergence, quantifying
distributional drift between the base and adapted models. If a threshold is
exceeded, the LoRA update is rescaled through a clipping procedure or rejected.
Experiments on the Qwen-2.5-7B model show that gating effectively mitigates
forgetting while preserving adaptability. EM based gating achieved the highest
cumulative performance in short continual learning sequences. Our results show
that different gating strategies can achieve comparable distribution shift
(measured by KL divergence) while producing different accuracy outcomes,
highlighting the importance of gating design in continual adaptation. This
approach offers a principled method for continual model editing, enabling LLMs
to integrate new knowledge while maintaining reliability. Code:
https://github.com/Bhoy1/STABLE

</details>


### [224] [Compressing Many-Shots in In-Context Learning](https://arxiv.org/abs/2510.16092)
*Devvrit Khatri,Pranamya Kulkarni,Nilesh Gupta,Yerram Varun,Liqian Peng,Jay Yagnik,Praneeth Netrapalli,Cho-Jui Hsieh,Alec Go,Inderjit S Dhillon,Aditya Kusupati,Prateek Jain*

Main category: cs.LG

TL;DR: 研究通过压缩多样本提示提高上下文学习推理的内存和计算效率，提出层压缩方法MemCom，在多分类任务中表现佳。


<details>
  <summary>Details</summary>
Motivation: 增加样本数量虽能提升下游任务性能，但会增加内存和计算成本，需提高上下文学习推理的内存和计算效率。

Method: 指出现有提示压缩方法对多样本压缩无效，提出MemCom层压缩方法，系统评估不同压缩器模型和训练方法。

Result: MemCom在所有压缩比下的多分类任务中均优于强基线，基线在高压缩比下性能急剧下降，MemCom准确率高且降幅小。

Conclusion: MemCom能有效压缩多样本提示，提高上下文学习推理的内存和计算效率。

Abstract: Large Language Models (LLMs) have been shown to be able to learn different
tasks without explicit finetuning when given many input-output examples /
demonstrations through In-Context Learning (ICL). Increasing the number of
examples, called ``shots'', improves downstream task performance but incurs
higher memory and computational costs. In this work, we study an approach to
improve the memory and computational efficiency of ICL inference by compressing
the many-shot prompts. Given many shots comprising t tokens, our goal is to
generate a m soft-token summary, where m < t. We first show that existing
prompt compression methods are ineffective for many-shot compression, and
simply using fewer shots as a baseline is surprisingly strong. To achieve
effective compression, we find that: (a) a stronger compressor model with more
trainable parameters is necessary, and (b) compressing many-shot
representations at each transformer layer enables more fine-grained compression
by providing each layer with its own compressed representation. Based on these
insights, we propose MemCom, a layer-wise compression method. We systematically
evaluate various compressor models and training approaches across different
model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence
lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms
strong baselines across all compression ratios on multiple classification tasks
with large label sets. Notably, while baseline performance degrades sharply at
higher compression ratios, often by over 20-30%, MemCom maintains high accuracy
with minimal degradation, typically dropping by less than 10%.

</details>


### [225] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 本文利用相似性搜索和随机表示近似世界模型，与PlaNet对比，结果显示搜索型世界模型在短和长时预测中与训练型相当，长时预测表现更优。


<details>
  <summary>Details</summary>
Motivation: 在强化学习领域，世界模型虽提升了样本效率，但本文想探索不通过训练程序来近似世界模型。

Method: 利用相似性搜索和随机表示近似世界模型，并与PlaNet进行对比，在潜在重建质量和重建图像感知相似度等方面评估模型。

Result: 搜索型世界模型在短和长时预测中与训练型相当，在多种视觉不同环境的长时预测中表现优于基线。

Conclusion: 不通过训练程序的搜索型世界模型在性能上有竞争力，尤其在长时预测表现较好。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [226] [AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures](https://arxiv.org/abs/2510.16165)
*Charles Rhys Campbell,Aldo H. Romero,Kamal Choudhary*

Main category: cs.LG

TL;DR: 对三种生成模型在材料数据集上进行系统基准测试，CDVAE 表现最佳，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对生成模型在材料数据集上性能的严格比较评估。

Method: 对 AtomGPT、CDVAE 和 FlowMM 三种模型，用两个超导数据集子集训练，用 KL 散度和 MAE 评估性能。

Result: 计算 KLD 和 MAE 分数中，CDVAE 表现最好，其次是 AtomGPT，最后是 FlowMM。

Conclusion: 完成三种模型在材料数据集上的基准测试，代码将公开。

Abstract: Generative models have become significant assets in the exploration and
identification of new materials, enabling the rapid proposal of candidate
crystal structures that satisfy target properties. Despite the increasing
adoption of diverse architectures, a rigorous comparative evaluation of their
performance on materials datasets is lacking. In this work, we present a
systematic benchmark of three representative generative models- AtomGPT (a
transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),
and FlowMM (a Riemannian flow matching model). These models were trained to
reconstruct crystal structures from subsets of two publicly available
superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria
database. Performance was assessed using the Kullback-Leibler (KL) divergence
between predicted and reference distributions of lattice parameters, as well as
the mean absolute error (MAE) of individual lattice constants. For the computed
KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and
then FlowMM. All benchmarking code and model configurations will be made
publicly available at https://github.com/atomgptlab/atombench_inverse.

</details>


### [227] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 本文通过层间因果修补分析语言模型偏好优化，发现对齐是空间局部、低秩的过程。


<details>
  <summary>Details</summary>
Motivation: 强化学习偏好微调框架内部工作机制不透明，需系统分析语言模型对齐的偏好优化。

Method: 在Llama - 3.2 - 1B上应用层间因果修补，结合LASSO回归。

Result: 对齐是空间局部的，中层激活编码因果决定奖励一致行为的子空间，仅少数层系数非零。

Conclusion: 基于人类偏好的微调对齐是有方向的低秩过程，而非分散和参数化的。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [228] [Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness](https://arxiv.org/abs/2510.16171)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh,Chaowei Zhang,Xiao Qin,Yang Zhou*

Main category: cs.LG

TL;DR: 本文研究将群等变卷积嵌入标准CNN以增强对抗鲁棒性，提出两种架构，理论和实验均证明有效，显示了强制对称性架构的潜力。


<details>
  <summary>Details</summary>
Motivation: 对抗训练防御策略有计算成本高和影响干净数据准确性的问题，因此研究架构方法来增强对抗鲁棒性。

Method: 将旋转和缩放等变层嵌入标准CNN，提出并行和级联两种对称性感知架构。

Result: 理论上降低假设空间复杂度、正则化梯度并得到更严格的鲁棒性边界；实验上在多种数据集和攻击下提升对抗鲁棒性和泛化性，无需对抗训练。

Conclusion: 强制对称性架构是基于数据增强防御的有效且有原则的替代方案。

Abstract: Adversarial examples reveal critical vulnerabilities in deep neural networks
by exploiting their sensitivity to imperceptible input perturbations. While
adversarial training remains the predominant defense strategy, it often incurs
significant computational cost and may compromise clean-data accuracy. In this
work, we investigate an architectural approach to adversarial robustness by
embedding group-equivariant convolutions-specifically, rotation- and
scale-equivariant layers-into standard convolutional neural networks (CNNs).
These layers encode symmetry priors that align model behavior with structured
transformations in the input space, promoting smoother decision boundaries and
greater resilience to adversarial attacks. We propose and evaluate two
symmetry-aware architectures: a parallel design that processes standard and
equivariant features independently before fusion, and a cascaded design that
applies equivariant operations sequentially. Theoretically, we demonstrate that
such models reduce hypothesis space complexity, regularize gradients, and yield
tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme
Value for nEtwork Robustness) framework. Empirically, our models consistently
improve adversarial robustness and generalization across CIFAR-10, CIFAR-100,
and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial
training. These findings underscore the potential of symmetry-enforcing
architectures as efficient and principled alternatives to data
augmentation-based defenses.

</details>


### [229] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 本文指出强化学习研究不应只关注代理性能展示，应更注重科学理解和基准与数学形式的精确映射，并以ALE为例说明。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习研究过度关注代理性能，忽视学习动态研究，存在过拟合风险且不利于技术迁移，也贬低了增进理解的工作。

Method: 以流行的Arcade Learning Environment (ALE) 为例进行阐述。

Result: ALE虽被认为‘饱和’，但可用于增进理解和促进强化学习技术在现实问题中的应用。

Conclusion: 强化学习研究应停止单纯关注代理能力展示，更多关注科学理解；需更精确地将基准与底层数学形式对应。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [230] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出结合RRL与以对象为中心表示的新框架处理结构化和非结构化数据，通过显式建模策略不确定性向人类专家查询指导，实验证明其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习系统在命题任务中忽略问题固有结构，关系扩展RRL对问题结构有强假设，需新方法处理不同数据。

Method: 结合RRL与以对象为中心表示的框架，通过显式建模策略不确定性让系统向人类专家查询指导。

Result: 实证评估表明提出的方法有效且高效。

Conclusion: 提出的结合RRL与以对象为中心表示并引入专家指导的框架可行且有效。

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [231] [Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal](https://arxiv.org/abs/2510.16233)
*Patricia West,Michelle WL Wan,Alexander Hepburn,Edwin Simpson,Raul Santos-Rodriguez,Jeffrey N Clark*

Main category: cs.LG

TL;DR: 研究探索用机器学习理解欧洲绿色协议中气候政策从宣布到采用的进展，比较文本表示方法，发现ClimateBERT在文本特征上表现好，BERT加元数据特征表现更佳，强调ML工具对气候政策分析和决策的潜力。


<details>
  <summary>Details</summary>
Motivation: 气候变化需要有效立法行动，本研究旨在探索机器学习在理解气候政策进展中的应用。

Method: 构建包含165项政策的数据集，结合文本和元数据，比较TF - IDF、BERT、ClimateBERT等文本表示方法，加入元数据特征评估对预测性能的影响，使用可解释AI方法。

Result: 仅文本特征时ClimateBERT表现好（RMSE = 0.17, R^2 = 0.29），加入元数据特征后BERT表现更佳（RMSE = 0.16, R^2 = 0.38），可解释AI方法揭示政策措辞、政党和国家代表等因素的影响。

Conclusion: 强调了机器学习工具在支持气候政策分析和决策方面的潜力。

Abstract: Climate change demands effective legislative action to mitigate its impacts.
This study explores the application of machine learning (ML) to understand the
progression of climate policy from announcement to adoption, focusing on
policies within the European Green Deal. We present a dataset of 165 policies,
incorporating text and metadata. We aim to predict a policy's progression
status, and compare text representation methods, including TF-IDF, BERT, and
ClimateBERT. Metadata features are included to evaluate the impact on
predictive performance. On text features alone, ClimateBERT outperforms other
approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance
with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods
from explainable AI highlights the influence of factors such as policy wording
and metadata including political party and country representation. These
findings underscore the potential of ML tools in supporting climate policy
analysis and decision-making.

</details>


### [232] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: 现有强化学习Web代理环境存在不足，提出WEBSERV环境，在WebArena任务中取得SOTA，减少启动延迟和存储需求。


<details>
  <summary>Details</summary>
Motivation: 当前缺少可扩展、高效的能结合浏览器端交互和服务器端状态控制的强化学习Web代理训练评估环境，现有环境存在诸多问题。

Method: 提出WEBSERV环境，包含紧凑、与站点无关的浏览器环境和可扩展的RL环境，通过高效启动和重置Web服务器实现。

Result: 在WebArena的购物CMS和Gitlab任务中实现了最先进的单提示成功率，启动延迟降低约5倍，存储需求减少约240倍，可在单主机上运行200多个并发容器。

Conclusion: WEBSERV环境是一个有效的可扩展、高效的强化学习Web代理训练评估环境。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [233] [Disentangling Hyperedges through the Lens of Category Theory](https://arxiv.org/abs/2510.16289)
*Yoonho Lee,Junseok Lee,Sangwoo Seo,Sungwon Kim,Yeongmin Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 本文从范畴论角度分析超边解缠，提出基于自然性条件的解缠准则，模型实验展示了准则潜力。


<details>
  <summary>Details</summary>
Motivation: 解缠表示学习在图结构数据有成果，但超图结构数据相关研究少，将超边解缠融入超图神经网络可利用隐藏超边语义。

Method: 从范畴论角度分析超边解缠，提出基于自然性条件的解缠准则。

Result: 概念验证模型成功捕捉基因通路中基因的功能关系，展示了所提准则的潜力。

Conclusion: 所提基于范畴论的超边解缠准则具有可行性和潜力。

Abstract: Despite the promising results of disentangled representation learning in
discovering latent patterns in graph-structured data, few studies have explored
disentanglement for hypergraph-structured data. Integrating hyperedge
disentanglement into hypergraph neural networks enables models to leverage
hidden hyperedge semantics, such as unannotated relations between nodes, that
are associated with labels. This paper presents an analysis of hyperedge
disentanglement from a category-theoretical perspective and proposes a novel
criterion for disentanglement derived from the naturality condition. Our
proof-of-concept model experimentally showed the potential of the proposed
criterion by successfully capturing functional relations of genes (nodes) in
genetic pathways (hyperedges).

</details>


### [234] [QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models](https://arxiv.org/abs/2510.16292)
*Yutong Wang,Haiyu Wang,Sai Qian Zhang*

Main category: cs.LG

TL;DR: 提出结合SVD和量化方法优化视觉语言模型，减少内存和计算开销，提升准确率，优于仅用量化或SVD的方法。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型计算成本高，限制可扩展性和实时应用，需降低成本。

Method: 对Q、K、V权重矩阵使用SVD减少KV缓存大小和计算开销，引入高效秩分配策略，还对权重和激活值进行量化。

Result: 比仅用量化或SVD的方法准确率提升超10%，硬件成本更低。

Conclusion: 该方法适合在资源受限设备上实时部署，代码已开源。

Abstract: Vision-Language Models (VLMs) are integral to tasks such as image captioning
and visual question answering, but their high computational cost, driven by
large memory footprints and processing time, limits their scalability and
real-time applicability. In this work, we propose leveraging Singular-Value
Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight
matrices to reduce KV cache size and computational overhead. We in addition
introduce an efficient rank allocation strategy that dynamically adjusts the
SVD rank based on its impact on VLM accuracy, achieving a significant reduction
in both memory usage and computational cost. Finally, we extend this approach
by applying quantization to both VLM weights and activations, resulting in a
highly efficient VLM. Our method outperforms previous approaches that rely
solely on quantization or SVD by achieving more than $10\%$ accuracy
improvement while consuming less hardware cost, making it better for real-time
deployment on resource-constrained devices. We open source our code at
\href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

</details>


### [235] [Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening](https://arxiv.org/abs/2510.16306)
*Xin Wang,Yu Wang,Yunchao Liu,Jens Meiler,Tyler Derr*

Main category: cs.LG

TL;DR: 介绍ScaffAug框架解决配体虚拟筛选挑战，通过三模块增强筛选效果并实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统配体虚拟筛选面临类别不平衡、结构不平衡及需识别结构多样活性化合物的挑战。

Method: ScaffAug框架含增强模块（用图扩散模型生成合成数据）、自训练模块（整合合成与原始数据）、重排序模块（提高分子支架多样性）。

Result: 在五个目标类别上开展综合计算实验，对比现有基线方法并进行消融研究。

Conclusion: 该工作通过生成增强、重排序和支架感知为有效增强虚拟筛选带来新视角。

Abstract: Ligand-based virtual screening (VS) is an essential step in drug discovery
that evaluates large chemical libraries to identify compounds that potentially
bind to a therapeutic target. However, VS faces three major challenges: class
imbalance due to the low active rate, structural imbalance among active
molecules where certain scaffolds dominate, and the need to identify
structurally diverse active compounds for novel drug development. We introduce
ScaffAug, a scaffold-aware VS framework that addresses these challenges through
three modules. The augmentation module first generates synthetic data
conditioned on scaffolds of actual hits using generative AI, specifically a
graph diffusion model. This helps mitigate the class imbalance and furthermore
the structural imbalance, due to our proposed scaffold-aware sampling
algorithm, designed to produce more samples for active molecules with
underrepresented scaffolds. A model-agnostic self-training module is then used
to safely integrate the generated synthetic data from our augmentation module
with the original labeled data. Lastly, we introduce a reranking module that
improves VS by enhancing scaffold diversity in the top recommended set of
molecules, while still maintaining and even enhancing the overall general
performance of identifying novel, active compounds. We conduct comprehensive
computational experiments across five target classes, comparing ScaffAug
against existing baseline methods by reporting the performance of multiple
evaluation metrics and performing ablation studies on ScaffAug. Overall, this
work introduces novel perspectives on effectively enhancing VS by leveraging
generative augmentations, reranking, and general scaffold-awareness.

</details>


### [236] [Toward General Digraph Contrastive Learning: A Dual Spatial Perspective](https://arxiv.org/abs/2510.16311)
*Daohan Su,Yang Zhang,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出S2 - DiGCL框架用于有向图对比学习，结合复域和实域视角构建样本，实验显示其在多数据集上性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法多关注无向图，忽略了现实网络中有向图的关键方向信息。

Method: 从复域视角在磁拉普拉斯矩阵中引入个性化扰动，从实域视角采用基于路径的子图增强策略，结合二者构建正负样本。

Result: 在7个真实有向图数据集上实验，在监督和无监督设置下，节点分类提升4.41%，链接预测提升4.34%，达到SOTA性能。

Conclusion: S2 - DiGCL框架在有向图对比学习中更通用、更鲁棒。

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful tool for
extracting consistent representations from graphs, independent of labeled
information. However, existing methods predominantly focus on undirected
graphs, disregarding the pivotal directional information that is fundamental
and indispensable in real-world networks (e.g., social networks and
recommendations).In this paper, we introduce S2-DiGCL, a novel framework that
emphasizes spatial insights from complex and real domain perspectives for
directed graph (digraph) contrastive learning. From the complex-domain
perspective, S2-DiGCL introduces personalized perturbations into the magnetic
Laplacian to adaptively modulate edge phases and directional semantics. From
the real-domain perspective, it employs a path-based subgraph augmentation
strategy to capture fine-grained local asymmetries and topological
dependencies. By jointly leveraging these two complementary spatial views,
S2-DiGCL constructs high-quality positive and negative samples, leading to more
general and robust digraph contrastive learning. Extensive experiments on 7
real-world digraph datasets demonstrate the superiority of our approach,
achieving SOTA performance with 4.41% improvement in node classification and
4.34% in link prediction under both supervised and unsupervised settings.

</details>


### [237] [Memorizing Long-tail Data Can Help Generalization Through Composition](https://arxiv.org/abs/2510.16322)
*Mo Zhou,Haoyang Ma,Rong Ge*

Main category: cs.LG

TL;DR: 本文探讨记忆和简单组合的协同作用，理论证明在线性设置中其能助模型预测罕见测试样本，实验表明理论洞察可扩展到神经网络架构，且模型组合能力与架构有关。


<details>
  <summary>Details</summary>
Motivation: 深度学习促使重新思考记忆和泛化的关系，研究记忆和简单组合的协同作用。

Method: 理论上在线性设置下进行分析，实验在简单数据的神经网络架构上开展。

Result: 理论上证明在线性设置中记忆和组合能帮助模型预测罕见测试样本；实验表明理论洞察可扩展到线性设置之外，且模型组合能力依赖于架构。

Conclusion: 记忆和组合的协同作用有助于模型预测，且模型组合能力与架构相关。

Abstract: Deep learning has led researchers to rethink the relationship between
memorization and generalization. In many settings, memorization does not hurt
generalization due to implicit regularization and may help by memorizing
long-tailed examples. In this paper, we consider the synergy between
memorization and simple composition -- the ability to make correct prediction
on a combination of long-tailed features. Theoretically, we show that for a
linear setting, memorization together with composition can help the model make
correct predictions on rare test examples that require a combination of
long-tailed features, even if such combinations were never observed in the
training data. Experiments on neural network architecture on simple data show
that the theoretical insight extends beyond the linear setting, and we further
observe that the composition capability of the model depends on its
architecture.

</details>


### [238] [MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting](https://arxiv.org/abs/2510.16350)
*Shule Hao,Junpeng Bao,Wenli Li*

Main category: cs.LG

TL;DR: 提出用于时间序列预测的MGTS - Net模型，由MFE、MFF和MSP三层组成，实验表明其轻量高效且性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测中整合多模态特征的方法存在提取细粒度时间模式不足、多模态信息整合不佳、对动态多尺度特征适应性有限等问题。

Method: 提出MGTS - Net模型，包含多模态特征提取层（MFE）、多模态特征融合层（MFF）和多尺度预测层（MSP）。

Result: MGTS - Net表现出轻量高效的优秀性能，相比其他基线模型表现更优。

Conclusion: 验证了所提方法的优越性。

Abstract: Recent research in time series forecasting has explored integrating
multimodal features into models to improve accuracy. However, the accuracy of
such methods is constrained by three key challenges: inadequate extraction of
fine-grained temporal patterns, suboptimal integration of multimodal
information, and limited adaptability to dynamic multi-scale features. To
address these problems, we propose MGTS-Net, a Multimodal Graph-enhanced
Network for Time Series forecasting. The model consists of three core
components: (1) a Multimodal Feature Extraction layer (MFE), which optimizes
feature encoders according to the characteristics of temporal, visual, and
textual modalities to extract temporal features of fine-grained patterns; (2) a
Multimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph
to model intra-modal temporal dependencies and cross-modal alignment
relationships and dynamically aggregates multimodal knowledge; (3) a
Multi-Scale Prediction layer (MSP), which adapts to multi-scale features by
dynamically weighting and fusing the outputs of short-term, medium-term, and
long-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits
excellent performance with light weight and high efficiency. Compared with
other state-of-the-art baseline models, our method achieves superior
performance, validating the superiority of the proposed methodology.

</details>


### [239] [Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures](https://arxiv.org/abs/2510.16411)
*Minh-Khoi Nguyen-Nhat,Rachel S. Y. Teo,Laziz Abdullaev,Maurice Mok,Viet-Hoang Tran,Tan Minh Nguyen*

Main category: cs.LG

TL;DR: 提出SymphonySMoE解决传统SMoE鲁棒性问题，有理论分析和实验验证其优势与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统SMoE难以适应分布变化，在数据污染下鲁棒性降低。

Method: 引入社交图来建模专家间交互，增强令牌路由过程。

Result: 理论分析和实验证明SymphonySMoE优于基线SMoE，在语言建模和视觉指令调优中有效。

Conclusion: SymphonySMoE轻量级、模块化，可与现有模型集成，适用于大规模系统微调任务。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a promising solution to
achieving unparalleled scalability in deep learning by decoupling model
parameter count from computational cost. By activating only a small subset of
parameters per sample, SMoE enables significant growth in model capacity while
maintaining efficiency. However, SMoE struggles to adapt to distributional
shifts, leading to reduced robustness under data contamination. In this work,
we introduce SymphonySMoE, a novel family of SMoE that introduces a social
graph to model interactions among experts. This graph-based structure enhances
the token routing process, addressing the robustness challenges that are
inherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,
and integrates seamlessly with existing SMoE-based models such as the XMoE and
the Generalist Language Model. We provide both theoretical analysis and
empirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.
Extensive experiments on language modeling and visual instruction tuning
validate our method's effectiveness. We further highlight the scalability of
SymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its
applicability in fine-tuning tasks for large-scale systems.

</details>


### [240] [Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution](https://arxiv.org/abs/2510.16440)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 介绍ECML - PKDD 2025高能物理发现中鲁棒学习挑战任务1的获胜方案，采用多轮梯度策略取得最佳结果。


<details>
  <summary>Details</summary>
Motivation: 完成设计针对给定分类模型的对抗攻击，在最大化误分类同时最小化扰动的任务。

Method: 采用基于多轮梯度的策略，利用模型可微结构，结合随机初始化和样本混合技术。

Result: 攻击在扰动大小和愚弄成功率方面取得最佳结果，获竞赛第一名。

Conclusion: 所采用的多轮梯度策略及相关技术在该对抗攻击任务中非常有效。

Abstract: This report presents the winning solution for Task 1 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The task required designing an adversarial attack against a
provided classification model that maximizes misclassification while minimizing
perturbations. Our approach employs a multi-round gradient-based strategy that
leverages the differentiable structure of the model, augmented with random
initialization and sample-mixing techniques to enhance effectiveness. The
resulting attack achieved the best results in perturbation size and fooling
success rate, securing first place in the competition.

</details>


### [241] [Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution](https://arxiv.org/abs/2510.16443)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文介绍ECML - PKDD 2025高能物理发现中鲁棒学习挑战任务2的获胜方案，包含数据生成和模型训练两阶段，最终获80%混合准确率。


<details>
  <summary>Details</summary>
Motivation: 设计并训练基于ANN的鲁棒模型，在干净数据和RDSA生成的对抗数据的二分类任务中达到高精度。

Method: 分数据生成和鲁棒模型训练两阶段，数据生成用基于RDSA的自定义方法生成1500万样本，模型训练采用含特征嵌入块和密集融合尾部的架构。

Result: 在对抗数据集上训练模型，混合准确率达80%，超第二名2个百分点。

Conclusion: 所提出的两阶段方法和模型架构在该挑战任务中表现出色，能有效提升模型鲁棒性和分类准确率。

Abstract: This report presents the winning solution for Task 2 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The goal of the challenge was to design and train a robust
ANN-based model capable of achieving high accuracy in a binary classification
task on both clean and adversarial data generated with the Random Distribution
Shuffle Attack (RDSA). Our solution consists of two components: a data
generation phase and a robust model training phase. In the first phase, we
produced 15 million artificial training samples using a custom methodology
derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we
introduced a robust architecture comprising (i)a Feature Embedding Block with
shared weights among features of the same type and (ii)a Dense Fusion Tail
responsible for the final prediction. Training this architecture on our
adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the
second-place solution by two percentage points.

</details>


### [242] [Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts](https://arxiv.org/abs/2510.16448)
*Yongxiang Hua,Haoyu Cao,Zhou Tao,Bocheng Li,Zihao Wu,Chaohu Liu,Linli Xu*

Main category: cs.LG

TL;DR: 提出Input Domain Aware MoE路由框架，在视觉语言任务上优于现有sMoE方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似度评分的路由机制难以有效捕捉输入结构，导致专家专业化和计算平衡的权衡问题，影响可扩展性和性能。

Method: 提出Input Domain Aware MoE路由框架，利用概率混合模型划分输入空间，独立于任务目标训练路由机制。

Result: 在视觉语言任务上，该方法始终优于现有sMoE方法，实现更高任务性能和更好的专家利用率平衡。

Conclusion: 所提的Input Domain Aware MoE路由框架是有效的，能解决现有sMoE路由机制的问题。

Abstract: Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling
large vision-language models, offering substantial capacity while maintaining
computational efficiency through dynamic, sparse activation of experts.
However, existing routing mechanisms, typically based on similarity scoring,
struggle to effectively capture the underlying input structure. This limitation
leads to a trade-off between expert specialization and balanced computation,
hindering both scalability and performance. We propose Input Domain Aware MoE,
a novel routing framework that leverages a probabilistic mixture model to
better partition the input space. By modeling routing probabilities as a
mixture of distributions, our method enables experts to develop clear
specialization boundaries while achieving balanced utilization. Unlike
conventional approaches, our routing mechanism is trained independently of
task-specific objectives, allowing for stable optimization and decisive expert
assignments. Empirical results on vision-language tasks demonstrate that our
method consistently outperforms existing sMoE approaches, achieving higher task
performance and improved expert utilization balance.

</details>


### [243] [SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning](https://arxiv.org/abs/2510.16474)
*Farwa Abbas,Hussain Ahmad,Claudia Szabo*

Main category: cs.LG

TL;DR: 传统预测建模方法处理高维异质数据有挑战，提出新方法提升性能，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 传统预测建模方法处理高维异质数据存在困难，PLS难以建模复杂非线性关系，局部处理无法捕捉跨组依赖，静态特征加权缺乏适应性。

Method: 提出一种新方法，引入基于自适应核的注意力机制，分别处理不同特征组再集成。

Result: 与现有方法相比，在不同数据集上性能指标有显著提升。

Conclusion: 新方法能有效解决传统方法的局限性，提升预测性能。

Abstract: High-dimensional, heterogeneous data with complex feature interactions pose
significant challenges for traditional predictive modeling approaches. While
Projection to Latent Structures (PLS) remains a popular technique, it struggles
to model complex non-linear relationships, especially in multivariate systems
with high-dimensional correlation structures. This challenge is further
compounded by simultaneous interactions across multiple scales, where local
processing fails to capture crossgroup dependencies. Additionally, static
feature weighting limits adaptability to contextual variations, as it ignores
sample-specific relevance. To address these limitations, we propose a novel
method that enhances predictive performance through novel architectural
innovations. Our architecture introduces an adaptive kernel-based attention
mechanism that processes distinct feature groups separately before integration,
enabling capture of local patterns while preserving global relationships.
Experimental results show substantial improvements in performance metrics,
compared to the state-of-the-art methods across diverse datasets.

</details>


### [244] [Predicting life satisfaction using machine learning and explainable AI](https://arxiv.org/abs/2510.16547)
*Alif Elham Khan,Mohammad Junayed Hasan,Humayra Anjum,Nabeel Mohammed,Sifat Momen*

Main category: cs.LG

TL;DR: 研究展示机器学习算法和大语言模型预测生活满意度的潜力，发现与生物医学领域关联大，健康状况是各年龄段最重要决定因素。


<details>
  <summary>Details</summary>
Motivation: 传统测量生活满意度的方法复杂且易出错，需要更有效的方式来理解个体生活体验、促进心理健康干预。

Method: 使用机器学习算法，从丹麦政府调查数据中提取特征；探索临床和生物医学大语言模型，将表格数据转换为自然语言句子；进行消融研究，分析不同年龄与主要决定因素的相关性。

Result: 机器学习算法预测准确率达93.80%，宏F1分数73.00%；大语言模型准确率93.74%，宏F1分数73.21%；发现生活满意度预测与生物医学领域更相关，健康状况是各年龄段最重要决定因素。

Conclusion: 机器学习、大语言模型和可解释人工智能可共同助力构建使用人工智能研究人类行为的信任和理解，对量化和理解主观幸福感有重要意义。

Abstract: Life satisfaction is a crucial facet of human well-being. Hence, research on
life satisfaction is incumbent for understanding how individuals experience
their lives and influencing interventions targeted at enhancing mental health
and well-being. Life satisfaction has traditionally been measured using analog,
complicated, and frequently error-prone methods. These methods raise questions
concerning validation and propagation. However, this study demonstrates the
potential for machine learning algorithms to predict life satisfaction with a
high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a
government survey of 19000 people aged 16-64 years in Denmark. Using feature
learning techniques, 27 significant questions for assessing contentment were
extracted, making the study highly reproducible, simple, and easily
interpretable. Furthermore, clinical and biomedical large language models
(LLMs) were explored for predicting life satisfaction by converting tabular
data into natural language sentences through mapping and adding meaningful
counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It
was found that life satisfaction prediction is more closely related to the
biomedical domain than the clinical domain. Ablation studies were also
conducted to understand the impact of data resampling and feature selection
techniques on model performance. Moreover, the correlation between primary
determinants with different age brackets was analyzed, and it was found that
health condition is the most important determinant across all ages. This study
demonstrates how machine learning, large language models and XAI can jointly
contribute to building trust and understanding in using AI to investigate human
behavior, with significant ramifications for academics and professionals
working to quantify and comprehend subjective well-being.

</details>


### [245] [NeurIPT: Foundation Model for Neural Interfaces](https://arxiv.org/abs/2510.16548)
*Zitao Fang,Chenxuan Li,Hongting Zhou,Shuyang Yu,Guodong Du,Ashwaq Qasem,Yang Lu,Jing Li,Junsong Zhang,Sim Kuan Goh*

Main category: cs.LG

TL;DR: 提出用于EEG的基础模型NeurIPT，在多个下游BCI数据集上表现达SOTA，推动EEG基础模型发展。


<details>
  <summary>Details</summary>
Motivation: 随着EEG数据增多，建立基础模型以扩展和泛化神经解码有需求，但因数据多变性等应用基础模型有挑战。

Method: 提出NeurIPT模型，引入AAMP进行时间预训练，用PMoE架构增强时间表征，利用电极3D坐标和IILP处理空间特征。

Result: 在八个下游BCI数据集上微调后，NeurIPT始终达到SOTA性能。

Conclusion: 推动了EEG基础模型发展，为可扩展和可泛化的神经信息处理系统提供见解。

Abstract: Electroencephalography (EEG) has wide-ranging applications, from clinical
diagnosis to brain-computer interfaces (BCIs). With the increasing volume and
variety of EEG data, there has been growing interest in establishing foundation
models (FMs) to scale up and generalize neural decoding. Despite showing early
potential, applying FMs to EEG remains challenging due to substantial
inter-subject, inter-task, and inter-condition variability, as well as diverse
electrode configurations across recording setups. To tackle these open
challenges, we propose NeurIPT, a foundation model developed for diverse
EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both
homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG
signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),
masking based on signal amplitude rather than random intervals, to learn robust
representations across varying signal intensities beyond local interpolation.
Moreover, this temporal representation is enhanced by a Progressive
Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks
are progressively introduced at deeper layers, adapting effectively to the
diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages
the 3D physical coordinates of electrodes, enabling effective transfer of
embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling
(IILP) during fine-tuning to efficiently exploit regional brain features.
Empirical evaluations across eight downstream BCI datasets, via fine-tuning,
demonstrated NeurIPT consistently achieved state-of-the-art performance,
highlighting its broad applicability and robust generalization. Our work pushes
forward the state of FMs in EEG and offers insights into scalable and
generalizable neural information processing systems.

</details>


### [246] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: 提出LANPO框架解决大语言模型强化学习中奖励利用和反馈整合问题，提升模型测试准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习依赖标量奖励，浪费文本信息且样本效率低，在线经验整合存在信息泄露、记忆和行为崩溃问题。

Method: 提出LANPO框架，分离反馈角色，构建动态经验池，引入奖励无关反思和相关抽象原则。

Result: 在数学推理基准测试中，7B和14B模型用LANPO训练的测试准确率显著高于GRPO训练的基线模型。

Conclusion: 提供了将历史经验整合到LLM强化学习循环的稳健方法，创建更有效和数据高效的学习代理。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [247] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 提出C - SMILES分子表示法及相关机制用于逆合成预测，在数据集上表现良好，为结构感知分子生成建立新范式。


<details>
  <summary>Details</summary>
Motivation: 现有无模板方法难以捕捉化学反应中的结构不变性，导致搜索空间大、预测精度低。

Method: 引入C - SMILES分子表示法，结合复制增强机制和SMILES对齐指导。

Result: 在USPTO - 50K数据集上top - 1准确率达67.2%，在USPTO - FULL数据集上达50.8%，生成分子有效性达99.9%。

Conclusion: 为结构感知分子生成建立新范式，可直接应用于计算药物发现。

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [248] [Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration](https://arxiv.org/abs/2510.16590)
*Alan Kai Hassen,Andrius Bernatavicius,Antonius P. A. Janssen,Mike Preuss,Gerard J. P. van Westen,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 本文提出无需标注数据的分子推理框架，应用于单步逆合成反应获高成功率，还可生成合成数据集解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习在化学应用中受限于标注数据稀缺昂贵，传统监督方法受限。

Method: 引入用通用大语言模型的分子推理框架，用唯一原子标识符将思维链推理锚定到分子结构，先进行单样本任务识别相关片段和化学标签或转化类别，可选步骤用位置信息进行少样本任务预测化学转化。

Result: 在学术基准和专家验证的药物发现分子上，大语言模型在识别化学合理反应位点成功率≥90%、命名反应类别成功率≥40%、最终反应物成功率≥74%。

Conclusion: 该框架能解决复杂化学任务，还能通过将化学知识映射到分子结构生成理论上合理的合成数据集，解决数据稀缺问题。

Abstract: Applications of machine learning in chemistry are often limited by the
scarcity and expense of labeled data, restricting traditional supervised
methods. In this work, we introduce a framework for molecular reasoning using
general-purpose Large Language Models (LLMs) that operates without requiring
labeled training data. Our method anchors chain-of-thought reasoning to the
molecular structure by using unique atomic identifiers. First, the LLM performs
a one-shot task to identify relevant fragments and their associated chemical
labels or transformation classes. In an optional second step, this
position-aware information is used in a few-shot task with provided class
examples to predict the chemical transformation. We apply our framework to
single-step retrosynthesis, a task where LLMs have previously underperformed.
Across academic benchmarks and expert-validated drug discovery molecules, our
work enables LLMs to achieve high success rates in identifying chemically
plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and
final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work
also provides a method to generate theoretically grounded synthetic datasets by
mapping chemical knowledge onto the molecular structure and thereby addressing
data scarcity.

</details>


### [249] [Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules](https://arxiv.org/abs/2510.16607)
*Tianwei Wang,Xinhui Ma,Wei Pang*

Main category: cs.LG

TL;DR: 受四元数在表示旋转和姿态方面的几何优势启发，提出四元数值监督学习Hopfield结构神经网络（QSHNN），有数学基础，实验效果好，应用场景广。


<details>
  <summary>Details</summary>
Motivation: 四元数在表示旋转和姿态方面具有几何优势。

Method: 从经典Hopfield神经网络（HNN）的连续时间动态模型出发，将公式扩展到四元数域；引入周期投影策略修改标准梯度下降。

Result: 实验模型实现了高精度、快速收敛和强可靠性，演化轨迹有足够平滑度。

Conclusion: 该模型为超复数或非交换代数结构下设计神经网络提供了实用实现框架和通用数学方法。

Abstract: Motivated by the geometric advantages of quaternions in representing
rotations and postures, we propose a quaternion-valued supervised learning
Hopfield-structured neural network (QSHNN) with a fully connected structure
inspired by the classic Hopfield neural network (HNN). Starting from a
continuous-time dynamical model of HNNs, we extend the formulation to the
quaternionic domain and establish the existence and uniqueness of fixed points
with asymptotic stability. For the learning rules, we introduce a periodic
projection strategy that modifies standard gradient descent by periodically
projecting each 4*4 block of the weight matrix onto the closest quaternionic
structure in the least-squares sense. This approach preserves both convergence
and quaternionic consistency throughout training. Benefiting from this rigorous
mathematical foundation, the experimental model implementation achieves high
accuracy, fast convergence, and strong reliability across randomly generated
target sets. Moreover, the evolution trajectories of the QSHNN exhibit
well-bounded curvature, i.e., sufficient smoothness, which is crucial for
applications such as control systems or path planning modules in robotic arms,
where joint postures are parameterized by quaternion neurons. Beyond these
application scenarios, the proposed model offers a practical implementation
framework and a general mathematical methodology for designing neural networks
under hypercomplex or non-commutative algebraic structures.

</details>


### [250] [On the Impossibility of Retrain Equivalence in Machine Unlearning](https://arxiv.org/abs/2510.16629)
*Jiatong Yu,Yinghui He,Anirudh Goyal,Sanjeev Arora*

Main category: cs.LG

TL;DR: 研究表明多阶段训练给机器遗忘带来根本障碍，局部遗忘算法难以实现重训练等价，需重新思考机器遗忘定义。


<details>
  <summary>Details</summary>
Motivation: 现代训练流水线常为多阶段训练，而理想的机器遗忘目标（重训练等价）是为独立同分布数据批次训练的模型制定的，需研究多阶段训练对机器遗忘的影响。

Method: 通过理论和实验，在Llama和Qwen模型上用梯度上升、NPO和SimNPO局部遗忘算法进行实验。

Result: 局部遗忘结果与训练阶段顺序有关，不同训练顺序的模型在遗忘时行为不同，GSM8K准确率下降差异超20%，概率分布也与路径有关。

Conclusion: 只要目标模型是分阶段训练的，重训练等价对局部遗忘算法是不恰当的目标，在难以获取模型训练历史时，需重新思考机器遗忘的定义和期望。

Abstract: Machine unlearning seeks to selectively remove the "influence" of specific
training data on a model's outputs. The ideal goal is Retrain
Equivalence--behavior identical to a model trained from scratch on only the
retained data. This goal was formulated for models trained on i.i.d. data
batches, but modern pipelines often involve multi-stage training, with each
stage having a distinct data distribution and objective. Examples include LLM
fine-tuning for alignment, reasoning ability, etc. Our study shows via theory
and experiments that this shift to multi-stage training introduces a
fundamental barrier for machine unlearning. The theory indicates that the
outcome of local unlearning--methods that only use gradients computed on the
forget set--is path-dependent. That is, a model's behavior during unlearning is
influenced by the order of its training stages during learning, making it
impossible for path-oblivious algorithms to universally achieve Retrain
Equivalence. We empirically demonstrate the same phenomenon in LLM
post-training across Llama and Qwen models (1B to 14B) with gradient ascent,
NPO, and SimNPO local unlearning algorithms. Models fine-tuned via different
orderings of identical training stages diverge in behavior during unlearning,
with the degradation in GSM8K accuracy after unlearning varying by over 20%
across paths. We also observe that some learning paths consistently produce
models that unlearn slowly. During unlearning, whether the probability mass
gets squeezed into paraphrasing or alternative concepts is also path-dependent.
These results consistently show that Retrain Equivalence is an ill-posed target
for local unlearning algorithms, so long as the target models are trained in
stages. In situations where access to models' training histories is hard, the
current work calls for rethinking the definition and desiderata of machine
unlearning.

</details>


### [251] [Simulation-free Structure Learning for Stochastic Dynamics](https://arxiv.org/abs/2510.16656)
*Noah El Rimawi-Fine,Adam Stecklov,Lucas Nelson,Mathieu Blanchette,Alexander Tong,Stephen Y. Zhang,Lazar Atanackovic*

Main category: cs.LG

TL;DR: 提出StructureFlow方法同时解决物理系统结构学习和随机种群动力学建模问题，并通过多组实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时解决高维随机物理系统的结构学习和动力学建模问题。

Method: 提出一种无模拟的新方法StructureFlow，联合学习物理系统的结构和随机种群动力学。

Result: 在高维合成系统、生物模拟系统和单细胞实验数据集上验证了StructureFlow能学习系统结构并建模条件种群动力学。

Conclusion: StructureFlow是迈向系统行为机制理解的关键一步。

Abstract: Modeling dynamical systems and unraveling their underlying causal
relationships is central to many domains in the natural sciences. Various
physical systems, such as those arising in cell biology, are inherently
high-dimensional and stochastic in nature, and admit only partial, noisy state
measurements. This poses a significant challenge for addressing the problems of
modeling the underlying dynamics and inferring the network structure of these
systems. Existing methods are typically tailored either for structure learning
or modeling dynamics at the population level, but are limited in their ability
to address both problems together. In this work, we address both problems
simultaneously: we present StructureFlow, a novel and principled
simulation-free approach for jointly learning the structure and stochastic
population dynamics of physical systems. We showcase the utility of
StructureFlow for the tasks of structure learning from interventions and
dynamical (trajectory) inference of conditional population dynamics. We
empirically evaluate our approach on high-dimensional synthetic systems, a set
of biologically plausible simulated systems, and an experimental single-cell
dataset. We show that StructureFlow can learn the structure of underlying
systems while simultaneously modeling their conditional population dynamics --
a key step toward the mechanistic understanding of systems behavior.

</details>


### [252] [Evaluating protein binding interfaces with PUMBA](https://arxiv.org/abs/2510.16674)
*Azam Shirali,Giri Narasimhan*

Main category: cs.LG

TL;DR: 本文介绍了基于Vision Mamba改进PIsToN的蛋白质-蛋白质相互作用评分函数PUMBA，其在多个数据集上表现优于PIsToN。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质对接工具的准确性依赖于强大的评分函数，而PIsToN虽为先进的深度学习评分函数，但Mamba架构在自然语言处理和计算机视觉中表现出色，因此希望用Vision Mamba改进PIsToN。

Method: 用Vision Mamba替换PIsToN中的Vision Transformer骨干。

Result: 模型捕捉蛋白质-蛋白质界面特征的全局和局部模式的能力显著提高，在多个大规模公共数据集上，PUMBA始终优于PIsToN。

Conclusion: 基于Vision Mamba改进的PUMBA能提升蛋白质-蛋白质相互作用评分的性能。

Abstract: Protein-protein docking tools help in studying interactions between proteins,
and are essential for drug, vaccine, and therapeutic development. However, the
accuracy of a docking tool depends on a robust scoring function that can
reliably differentiate between native and non-native complexes. PIsToN is a
state-of-the-art deep learning-based scoring function that uses Vision
Transformers in its architecture. Recently, the Mamba architecture has
demonstrated exceptional performance in both natural language processing and
computer vision, often outperforming Transformer-based models in their domains.
In this study, we introduce PUMBA (Protein-protein interface evaluation with
Vision Mamba), which improves PIsToN by replacing its Vision Transformer
backbone with Vision Mamba. This change allows us to leverage Mamba's efficient
long-range sequence modeling for sequences of image patches. As a result, the
model's ability to capture both global and local patterns in protein-protein
interface features is significantly improved. Evaluation on several
widely-used, large-scale public datasets demonstrates that PUMBA consistently
outperforms its original Transformer-based predecessor, PIsToN.

</details>


### [253] [Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory](https://arxiv.org/abs/2510.16676)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: 在数据获取成本高的领域，现有主动目标发现方法在无信息先验场景中泛化困难，本文提出新方法，有理论依据、可解释，实验显示其性能远超基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型的主动目标发现方法在数据极少或采样成本高、难以学习强先验的领域难以泛化，需新方法解决该问题。

Method: 提出一种新的主动目标发现方法，从神经科学获取灵感进行设计，具有可解释性，能随新观察单调改进先验估计。

Result: 通过多领域的综合实验和消融研究，该方法性能大幅超越基线方法。

Conclusion: 新方法能在无信息先验场景中有效进行主动目标发现，在复杂现实场景中具备强大的探索能力和适应性。

Abstract: In many scientific and engineering fields, where acquiring high-quality data
is expensive--such as medical imaging, environmental monitoring, and remote
sensing--strategic sampling of unobserved regions based on prior observations
is crucial for maximizing discovery rates within a constrained budget. The rise
of powerful generative models, such as diffusion models, has enabled active
target discovery in partially observable environments by leveraging learned
priors--probabilistic representations that capture underlying structure from
data. With guidance from sequentially gathered task-specific observations,
these models can progressively refine exploration and efficiently direct
queries toward promising regions. However, in domains where learning a strong
prior is infeasible due to extremely limited data or high sampling cost (such
as rare species discovery, diagnostics for emerging diseases, etc.), these
methods struggle to generalize. To overcome this limitation, we propose a novel
approach that enables effective active target discovery even in settings with
uninformative priors, ensuring robust exploration and adaptability in complex
real-world scenarios. Our framework is theoretically principled and draws
inspiration from neuroscience to guide its design. Unlike black-box policies,
our approach is inherently interpretable, providing clear insights into
decision-making. Furthermore, it guarantees a strong, monotonic improvement in
prior estimates with each new observation, leading to increasingly accurate
sampling and reinforcing both reliability and adaptability in dynamic settings.
Through comprehensive experiments and ablation studies across various domains,
including species distribution modeling and remote sensing, we demonstrate that
our method substantially outperforms baseline approaches.

</details>


### [254] [Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers](https://arxiv.org/abs/2510.16677)
*Ran Tong,Jiaqi Liu,Su Liu,Xin Hu,Lanruo Wang*

Main category: cs.LG

TL;DR: 提出基于每秒心率的紧凑、严格因果的流式临床时间序列基准，研究两项任务，对比GRU - D和Transformer，结果表明纵向监测中模型选择依赖于任务。


<details>
  <summary>Details</summary>
Motivation: 为流式临床时间序列提供紧凑、严格因果的基准，并研究不同模型在相关任务中的表现。

Method: 在MIT - BIH心律失常数据库上，使用每秒心率，在记录级非重叠分割下研究两项任务，对比GRU - D和Transformer与非学习基线，采用校准感知评估和适当的预测评估方法。

Result: 在MIT - BIH上，GRU - D在心动过速风险评估上略胜Transformer，Transformer在心率预测上显著降低误差。

Conclusion: 纵向监测中，模型选择依赖于任务，紧凑RNN在短时间风险评分有竞争力，紧凑Transformer在点预测上有明显优势。

Abstract: We present a compact, strictly causal benchmark for streaming clinical time
series on the MIT--BIH Arrhythmia Database using per-second heart rate. Two
tasks are studied under record-level, non-overlapping splits: near-term
tachycardia risk (next ten seconds) and one-step heart rate forecasting. We
compare a GRU-D (RNN) and a Transformer under matched training budgets against
strong non-learned baselines. Evaluation is calibration-aware for
classification and proper for forecasting, with temperature scaling and grouped
bootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the
Transformer for tachycardia risk, while the Transformer clearly lowers
forecasting error relative to GRU-D and persistence. Our results show that, in
longitudinal monitoring, model choice is task-dependent: compact RNNs remain
competitive for short-horizon risk scoring, whereas compact Transformers
deliver clearer gains for point forecasting.

</details>


### [255] [High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares](https://arxiv.org/abs/2510.16687)
*Shurong Lin,Eric D. Kolaczyk,Adam Smith,Elliot Paquette*

Main category: cs.LG

TL;DR: 本文利用扩散方法精确分析噪声随机梯度下降（SGD），研究无需显式梯度敏感度知识的噪声SGD变体，聚焦带l2正则化的最小二乘问题。


<details>
  <summary>Details</summary>
Motivation: 此前工作对噪声SGD的精确行为，尤其是高维情况下不清楚，本文旨在精确分析噪声SGD。

Method: 利用扩散方法分析噪声SGD，研究不依赖显式梯度敏感度知识的噪声SGD变体。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及最终结论。

Abstract: The interplay between optimization and privacy has become a central theme in
privacy-preserving machine learning. Noisy stochastic gradient descent (SGD)
has emerged as a cornerstone algorithm, particularly in large-scale settings.
These variants of gradient methods inject carefully calibrated noise into each
update to achieve differential privacy, the gold standard notion of rigorous
privacy guarantees. Prior work primarily provides various bounds on statistical
risk and privacy loss for noisy SGD, yet the \textit{exact} behavior of the
process remains unclear, particularly in high-dimensional settings. This work
leverages a diffusion approach to analyze noisy SGD precisely, providing a
continuous-time perspective that captures both statistical risk evolution and
privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy
SGD that does not require explicit knowledge of gradient sensitivity, unlike
existing work that assumes or enforces sensitivity through gradient clipping.
Specifically, we focus on the least squares problem with $\ell_2$
regularization.

</details>


### [256] [On the Granularity of Causal Effect Identifiability](https://arxiv.org/abs/2510.16703)
*Yizuo Chen,Adnan Darwiche*

Main category: cs.LG

TL;DR: 本文探讨基于状态的因果效应可识别性，表明其在变量因果效应不可识别时仍可能可识别，特定知识结合可提升可识别性。


<details>
  <summary>Details</summary>
Motivation: 传统因果效应可识别性基于处理和结果变量定义，本文研究基于状态的因果效应可识别性。

Method: 理论分析不同知识（如特定上下文独立性、条件函数依赖等）对基于状态和基于变量的因果效应可识别性的影响。

Result: 基于状态的因果效应在特定情况下即使变量因果效应不可识别也可能可识别；特定知识结合能提升可识别性。

Conclusion: 现有基于变量的框架可能会遗漏一些可从观测数据估计的因果效应，本文发现凸显了这些情况。

Abstract: The classical notion of causal effect identifiability is defined in terms of
treatment and outcome variables. In this note, we consider the identifiability
of state-based causal effects: how an intervention on a particular state of
treatment variables affects a particular state of outcome variables. We
demonstrate that state-based causal effects may be identifiable even when
variable-based causal effects may not. Moreover, we show that this separation
occurs only when additional knowledge -- such as context-specific
independencies and conditional functional dependencies -- is available. We
further examine knowledge that constrains the states of variables, and show
that such knowledge does not improve identifiability on its own but can improve
both variable-based and state-based identifiability when combined with other
knowledge such as context-specific independencies. Our findings highlight
situations where causal effects of interest may be estimable from observational
data and this identifiability may be missed by existing variable-based
frameworks.

</details>


### [257] [LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus](https://arxiv.org/abs/2510.16719)
*Zak Ressler,Marcus Grijalva,Angelica Marie Ignacio,Melanie Torres,Abelardo Cuadra Rojas,Rohollah Moghadam,Mohammad Rasoul narimani*

Main category: cs.LG

TL;DR: 本文提出用LSTM处理电动汽车充电负荷数据以预测未来负荷的框架，实验表明该模型能跨时间尺度准确预测。


<details>
  <summary>Details</summary>
Motivation: 预测电动汽车未来充电负荷，为基础设施规划、能源管理和电网集成提供有价值的见解。

Method: 提出处理框架，对多地点原始数据进行归一化和特征提取等预处理，用LSTM捕捉数据短期波动和长期趋势。

Result: 模型能跨日、周、月等多个时间尺度准确预测充电需求。

Conclusion: 该系统模块化设计可适应不同充电地点和使用模式，适用于多种部署场景。

Abstract: This paper presents a framework for processing EV charging load data in order
to forecast future load predictions using a Recurrent Neural Network,
specifically an LSTM. The framework processes a large set of raw data from
multiple locations and transforms it with normalization and feature extraction
to train the LSTM. The pre-processing stage corrects for missing or incomplete
values by interpolating and normalizing the measurements. This information is
then fed into a Long Short-Term Memory Model designed to capture the short-term
fluctuations while also interpreting the long-term trends in the charging data.
Experimental results demonstrate the model's ability to accurately predict
charging demand across multiple time scales (daily, weekly, and monthly),
providing valuable insights for infrastructure planning, energy management, and
grid integration of EV charging facilities. The system's modular design allows
for adaptation to different charging locations with varying usage patterns,
making it applicable across diverse deployment scenarios.

</details>


### [258] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 本文将自然语言处理模型学习曲线预测任务建模为多任务学习问题，用潜在变量多输出高斯过程建模，结合主动学习策略，在小规模数据集验证了框架。


<details>
  <summary>Details</summary>
Motivation: 预测自然语言处理模型学习曲线可辅助决策，降低计算开销和数据集获取成本。

Method: 将预测任务建模为多任务学习问题，用潜在变量多输出高斯过程建模任务间信息和依赖，结合主动学习策略。

Result: 该方法能以低成本促进概率缩放定律的发展，结合主动学习可降低预测不确定性，预测接近真实缩放定律。在三个小规模NLP数据集上验证了框架。

Conclusion: 所提出的框架在自然语言处理模型学习曲线预测方面是有效的。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [259] [An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications](https://arxiv.org/abs/2510.16747)
*Danish Nazir,Gowtham Sai Inti,Timo Bartels,Jan Piewek,Thorsten Bagdonat,Tim Fingscheidt*

Main category: cs.LG

TL;DR: 本文针对汽车系统语义分割应用，提出SegDeformer的联合特征和任务解码方法，降低计算复杂度，在车内和分布式应用中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 先前工作未研究基于Transformer的SegDeformer在联合源和任务解码中的应用，且SegDeformer计算复杂度高，需降低其在汽车系统应用中的计算复杂度。

Method: 提出SegDeformer的联合特征和任务解码方法。

Result: 车内应用中，在Cityscapes和ADE20K数据集上提高了每秒帧数；分布式应用中，在mIoU指标上达到了最先进水平，且大幅减少了云DNN参数使用量。

Conclusion: 所提方法能降低计算复杂度，提高云的可扩展性，在汽车系统语义分割应用中表现良好。

Abstract: Modern automotive systems leverage deep neural networks (DNNs) for semantic
segmentation and operate in two key application areas: (1) In-car, where the
DNN solely operates in the vehicle without strict constraints on the data rate.
(2) Distributed, where one DNN part operates in the vehicle and the other part
typically on a large-scale cloud platform with a particular constraint on
transmission bitrate efficiency. Typically, both applications share an image
and source encoder, while each uses distinct (joint) source and task decoders.
Prior work utilized convolutional neural networks for joint source and task
decoding but did not investigate transformer-based alternatives such as
SegDeformer, which offer superior performance at the cost of higher
computational complexity. In this work, we propose joint feature and task
decoding for SegDeformer, thereby enabling lower computational complexity in
both in-car and distributed applications, despite SegDeformer's computational
demands. This improves scalability in the cloud while reducing in-car
computational complexity. For the in-car application, we increased the frames
per second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on
Cityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on
ADE20K, while being on-par w.r.t.\ the mean intersection over union (mIoU) of
the transformer-based baseline that doesn't compress by a source codec. For the
distributed application, we achieve state-of-the-art (SOTA) over a wide range
of bitrates on the mIoU metric, while using only $0.14$\% ($0.04$\%) of cloud
DNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).

</details>


### [260] [SAMOSA: Sharpness Aware Minimization for Open Set Active learning](https://arxiv.org/abs/2510.16757)
*Young In Kim,Andrea Agiollo,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出SAMOSA查询算法用于开放集主动学习，实验显示可提升精度且无计算开销


<details>
  <summary>Details</summary>
Motivation: 现代机器学习数据标注成本高，开放集主动学习旨在从无标签数据中选信息样本以减轻负担

Method: 基于数据典型性对传统随机梯度下降和锐度感知最小化影响的理论发现，SAMOSA根据样本典型性主动查询样本，识别靠近模型决策边界的非典型样本

Result: 在多个数据集上，SAMOSA比现有技术精度最多提高3%，且不引入计算开销

Conclusion: SAMOSA是一种有效的开放集主动学习查询算法

Abstract: Modern machine learning solutions require extensive data collection where
labeling remains costly. To reduce this burden, open set active learning
approaches aim to select informative samples from a large pool of unlabeled
data that includes irrelevant or unknown classes. In this context, we propose
Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an
effective querying algorithm. Building on theoretical findings concerning the
impact of data typicality on the generalization properties of traditional
stochastic gradient descent (SGD) and sharpness-aware minimization (SAM),
SAMOSA actively queries samples based on their typicality. SAMOSA effectively
identifies atypical samples that belong to regions of the embedding manifold
close to the model decision boundaries. Therefore, SAMOSA prioritizes the
samples that are (i) highly informative for the targeted classes, and (ii)
useful for distinguishing between targeted and unwanted classes. Extensive
experiments show that SAMOSA achieves up to 3% accuracy improvement over the
state of the art across several datasets, while not introducing computational
overhead. The source code of our experiments is available at:
https://anonymous.4open.science/r/samosa-DAF4

</details>


### [261] [Learning to play: A Multimodal Agent for 3D Game-Play](https://arxiv.org/abs/2510.16774)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Christopher Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.LG

TL;DR: 本文探讨3D第一人称视频游戏实时多模态推理，构建数据集，训练文本条件代理模型，展示其游戏能力并指出挑战。


<details>
  <summary>Details</summary>
Motivation: 3D第一人称视频游戏对实时多模态推理是具有挑战性的环境，有研究的必要。

Method: 构建人类游戏数据集，学习逆动力学模型，利用行为克隆训练文本条件代理模型。

Result: 训练出的模型能玩多种3D游戏并响应文本输入。

Conclusion: 指出长周期任务和大规模游戏定量评估等挑战待解决。

Abstract: We argue that 3-D first-person video games are a challenging environment for
real-time multi-modal reasoning. We first describe our dataset of human
game-play, collected across a large variety of 3-D first-person games, which is
both substantially larger and more diverse compared to prior publicly disclosed
datasets, and contains text instructions. We demonstrate that we can learn an
inverse dynamics model from this dataset, which allows us to impute actions on
a much larger dataset of publicly available videos of human game play that lack
recorded actions. We then train a text-conditioned agent for game playing using
behavior cloning, with a custom architecture capable of realtime inference on a
consumer GPU. We show the resulting model is capable of playing a variety of
3-D games and responding to text input. Finally, we outline some of the
remaining challenges such as long-horizon tasks and quantitative evaluation
across a large set of games.

</details>


### [262] [3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding](https://arxiv.org/abs/2510.16780)
*Chang Wu,Zhiyuan Liu,Wen Shu,Liang Wang,Yanchen Luo,Wenqiang Lei,Yatao Bian,Junfeng Fang,Xiang Wang*

Main category: cs.LG

TL;DR: 提出3D-GSRD用于分子表示学习，在MD17基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决将重掩码解码从2D扩展到3D MGM时避免2D结构泄漏到解码器，同时提供足够2D上下文的挑战。

Method: 提出3D-GSRD，其核心是选择性重掩码解码（SRD），与3D关系变换器（3D-ReTrans）编码器和独立于结构的解码器协同工作。

Result: 3D-GSRD在MD17分子属性预测基准测试的8个目标中的7个上达到新的最优性能。

Conclusion: SRD结合独立于结构的解码器增强了编码器在分子表示学习中的作用。

Abstract: Masked graph modeling (MGM) is a promising approach for molecular
representation learning (MRL).However, extending the success of re-mask
decoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting
challenges: avoiding 2D structure leakage to the decoder, while still providing
sufficient 2D context for reconstructing re-masked atoms.To address these
challenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with
Selective Re-mask Decoding. The core innovation of 3D-GSRD lies in its
Selective Re-mask Decoding(SRD), which re-masks only 3D-relevant information
from encoder representations while preserving the 2D graph structures.This SRD
is synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)
encoder alongside a structure-independent decoder. We analyze that SRD,
combined with the structure-independent decoder, enhances the encoder's role in
MRL. Extensive experiments show that 3D-GSRD achieves strong downstream
performance, setting a new state-of-the-art on 7 out of 8 targets in the widely
used MD17 molecular property prediction benchmark. The code is released at
https://github.com/WuChang0124/3D-GSRD.

</details>


### [263] [Mixed-Precision Quantization for Language Models: Techniques and Prospects](https://arxiv.org/abs/2510.16805)
*Mariam Rakka,Marios Fournarakis,Olga Krestinskaya,Jinane Bazzi,Khaled N. Salama,Fadi Kurdahi,Ahmed M. Eltawil,Mohammed E. Fouda*

Main category: cs.LG

TL;DR: 论文对大语言模型的混合精度量化框架（MXPLMs）进行全面综述，回顾量化基础，对比不同MXPLM框架，总结开放问题与未来方向。


<details>
  <summary>Details</summary>
Motivation: 语言模型规模快速扩大，计算、内存和能源需求难以持续，均匀低比特量化会降低精度，混合精度量化可平衡效率与精度，故对MXPLMs进行综述。

Method: 先回顾量化基础，再根据比特分配策略和精度配置对MXPLM框架分类比较，对比MXPLMs与早期混合精度量化方法，总结开放问题与未来方向。

Result: 通过比较分析，突出了不同MXPLM框架在困惑度、零样本任务性能和部署权衡方面的差异，识别了可迁移和面临挑战的策略。

Conclusion: 该工作整合了最新进展，为理解大规模语言模型混合精度量化的现状和研究前景提供参考。

Abstract: The rapid scaling of language models (LMs) has resulted in unprecedented
computational, memory, and energy requirements, making their training and
deployment increasingly unsustainable. Quantization has emerged as an essential
compression technique to reduce model size, alleviate memory bottlenecks, and
accelerate inference. However, while uniform low-bit quantization (e.g., INT8,
INT4) provides significant efficiency gains, it can degrade accuracy in
sensitive components of transformer-based LMs. Mixed-precision quantization
offers a promising alternative by selectively allocating precision across
layers or within tensors to balance efficiency and accuracy. This survey
provides a comprehensive overview of Mixed-Precision quantization frameworks
for LMs (MXPLMs). We first review quantization fundamentals, including uniform
and non-uniform quantizers, quantization granularity, and methods widely used
in post-training quantization. We then categorize and compare recent MXPLM
frameworks according to their bit allocation strategies and precision
configurations across weights, activations, and key-value caches. A comparative
analysis highlights differences in perplexity, zero-shot task performance, and
deployment trade-offs. Furthermore, we contrast MXPLMs with earlier
mixed-precision quantization methods for deep neural networks, identifying
strategies that transfer and those that face challenges in the LM setting.
Finally, we summarize open issues and future directions, including
hardware-aware design, activation quantization, and scalable optimization
methods for billion-parameter models. By consolidating recent advances, this
work serves as a reference for understanding the current landscape and research
prospects of mixed-precision quantization for large-scale language models.

</details>


### [264] [Computational Budget Should Be Considered in Data Selection](https://arxiv.org/abs/2510.16806)
*Weilin Wan,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 现有数据选择方法忽略计算预算，本文提出CADS方法，将其转化为双层优化框架，解决两个挑战，实验显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法忽略计算预算，不同预算对数据训练有不同要求，需将计算预算纳入数据选择策略。

Method: 提出CADS方法并构建双层优化框架，用概率重参数化策略和无Hessian策略梯度估计器解决外层梯度问题，将内层优化问题转化为外层目标惩罚项。

Result: 在视觉和语言基准测试中，相比基线性能提升达14.42%。

Conclusion: 计算预算应融入数据选择策略，CADS方法有效提升性能。

Abstract: Data selection improves computational efficiency by choosing informative
subsets of training samples. However, existing methods ignore the compute
budget, treating data selection and importance evaluation independently of
compute budget constraints. Yet empirical studies show no algorithm can
consistently outperform others (or even random selection) across varying
budgets. We therefore argue that compute budget must be integral to
data-selection strategies, since different budgets impose distinct requirements
on data quantity, quality, and distribution for effective training. To this
end, we propose a novel Computational budget-Aware Data Selection (CADS) method
and naturally formulate it into a bilevel optimization framework, where the
inner loop trains the model within the constraints of the computational budget
on some selected subset of training data, while the outer loop optimizes data
selection based on model evaluation. Our technical contributions lie in
addressing two main challenges in solving this bilevel optimization problem:
the expensive Hessian matrix estimation for outer-loop gradients and the
computational burden of achieving inner-loop optimality during iterations. To
solve the first issue, we propose a probabilistic reparameterization strategy
and compute the gradient using a Hessian-free policy gradient estimator. To
address the second challenge, we transform the inner optimization problem into
a penalty term in the outer objective, further discovering that we only need to
estimate the minimum of a one-dimensional loss to calculate the gradient,
significantly improving efficiency. Extensive experiments show that our method
achieves performance gains of up to 14.42% over baselines in vision and
language benchmarks.

</details>


### [265] [Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://arxiv.org/abs/2510.16807)
*Zhoutong Wu,Yuan Zhang,Yiming Dong,Chenheng Zhang,Cong Fang,Kun Yuan,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出SkipV1Former，复用首层Value头，减少KV缓存、提升性能，还能与其他方法结合进一步优化。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型扩大会带来高内存和计算成本，之前基于跳跃连接的方法存在不足，需新方法在增强表示的同时减少资源使用。

Method: 提出SkipV1Former，从第二层起每层复用一半首层Value头，减少Value投影和V缓存；提出将现有MHA Transformer检查点升级到SkipV1Former的方法。

Result: 不同模型规模下，SkipV1Former使KV缓存减少约25%，提升困惑度；与其他方法结合，如YOCO，可使KV缓存减少近50%且提升性能。

Conclusion: SkipV1Former能有效减少KV缓存、提升模型性能，还可与其他方法结合进一步优化。

Abstract: Transformer models have driven breakthroughs across various language tasks by
their strong capability to learn rich contextual representations. Scaling them
to improve representation, however, often demands substantial memory and
compute costs, such as the Key-Value (KV) cache used during auto-regressive
decoding. Skip connections offer a promising way to improve representation
without bloating resource usage, yet most prior works either improve
expressivity while leaving KV costs unchanged, or reduce memory at the cost of
weaker representation. In this work, we propose SkipV1Former, a Transformer
variant that uses skip connections from the first layer's Value heads to
strengthen model representation and reduce KV cache. Specifically, from the
second block onward, each layer reuses half of its Value heads from the very
first layer, while computing the other half as usual-cutting Value projections
and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed
first-layer Values into deeper layers restores information lost to compression
and accelerates the model's implicit mesa-optimization-a key pattern of
Transformer in auto-regressive tasks. Empirically, across different model
scales, SkipV1Former delivers consistent reductions of approximately 25 \% in
KV cache while improving perplexity relative to standard Multi-Head Attention
(MHA) Transformers and some advanced variants. Moreover, we propose a recipe
for uptraining existing MHA Transformer checkpoints to SkipV1Former with only
10-15\% additional compute. Finally, SkipV1Former can seamlessly combine
advanced methods like Group-Query Attention and Multi-Latent Attention to
achieve further KV cache savings and performance improvement. When combined
with YOCO, it cuts KV cache size by nearly 50 \% while still improving
performance.

</details>


### [266] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 研究因果充足性下因果多臂老虎机的后悔最小化，发现学习父集并非最优，提出近最优算法并通过实验验证性能优势。


<details>
  <summary>Details</summary>
Motivation: 探究在因果结构未知时，以往聚焦识别奖励父集并应用经典多臂老虎机方法或联合学习父集同时最小化后悔的策略是否最优。

Method: 证明存在后悔最小化和父集识别目标冲突的实例，分析已知和未知父集大小情况，建立新的后悔下界，提出绕过图和父集恢复的算法。

Result: 实验表明在多种环境中，所提方法与现有基线存在较大性能差距。

Conclusion: 父集识别对于后悔最小化并非必要。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [267] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 本文采用半监督正无标签学习策略进行考古预测建模，在两个数据集上评估，结果显示半监督学习在识别未发现遗址方面有前景。


<details>
  <summary>Details</summary>
Motivation: 解决考古中结构标签稀缺问题，利用深度学习进行考古预测建模，估计未发现遗址可能出现的位置。

Method: 采用半监督正无标签（PU）学习策略，实现为语义分割模型，使用动态伪标签并结合条件随机场（CRF），通过RNN实现。

Result: 在地理空间数据集上表现与LAMAP相当且Dice分数更高，在原始卫星图像上保持性能且预测表面可解释性更好。

Conclusion: 半监督学习为在大面积、注释稀疏的景观中识别未发现遗址提供了有前景的方法。

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [268] [Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator](https://arxiv.org/abs/2510.16816)
*Ming Zhong,Zhenya Yan*

Main category: cs.LG

TL;DR: 提出线性注意力神经算子LANO，通过基于代理机制重构注意力，解决可扩展性和准确性权衡问题，理论和实验表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的神经算子架构在可扩展性和准确性上的权衡问题，即softmax注意力复杂度高，线性注意力变体精度下降。

Method: 引入紧凑的M个代理令牌（M远小于N）来调解N个令牌间的全局交互，重构注意力，形成具有线性复杂度O(MNd)的算子层。

Result: 理论上证明了通用逼近性、改善的条件和稳定性；实验上在标准基准测试中平均比现有最优神经PDE求解器提高19.5%的准确率。

Conclusion: LANO在科学机器学习应用中建立了可扩展、高精度的基础。

Abstract: Neural operators offer a powerful data-driven framework for learning mappings
between function spaces, in which the transformer-based neural operator
architecture faces a fundamental scalability-accuracy trade-off: softmax
attention provides excellent fidelity but incurs quadratic complexity
$\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,
while linear attention variants reduce cost to $\mathcal{O}(N d^2)$ but often
suffer significant accuracy degradation. To address the aforementioned
challenge, in this paper, we present a novel type of neural operators, Linear
Attention Neural Operator (LANO), which achieves both scalability and high
accuracy by reformulating attention through an agent-based mechanism. LANO
resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \ll
N)$ that mediate global interactions among $N$ tokens. This agent attention
mechanism yields an operator layer with linear complexity $\mathcal{O}(MN d)$
while preserving the expressive power of softmax attention. Theoretically, we
demonstrate the universal approximation property, thereby demonstrating
improved conditioning and stability properties. Empirically, LANO surpasses
current state-of-the-art neural PDE solvers, including Transolver with
slice-based softmax attention, achieving average $19.5\%$ accuracy improvement
across standard benchmarks. By bridging the gap between linear complexity and
softmax-level performance, LANO establishes a scalable, high-accuracy
foundation for scientific machine learning applications.

</details>


### [269] [Trace Regularity PINNs: Enforcing $\mathrm{H}^{\frac{1}{2}}(\partial Ω)$ for Boundary Data](https://arxiv.org/abs/2510.16817)
*Doyoon Kim,Junbin Song*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose an enhanced physics-informed neural network (PINN), the Trace
Regularity Physics-Informed Neural Network (TRPINN), which enforces the
boundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\partial \Omega)$, the
correct trace space associated with $H^1(\Omega)$. We reduce computational cost
by computing only the theoretically essential portion of the semi-norm and
enhance convergence stability by avoiding denominator evaluations in the
discretization. By incorporating the exact $H^{1/2}(\partial \Omega)$ norm, we
show that the approximation converges to the true solution in the
$H^{1}(\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we
demonstrate that TRPINN can converge faster than standard PINNs. Numerical
experiments on the Laplace equation with highly oscillatory Dirichlet boundary
conditions exhibit cases where TRPINN succeeds even when standard PINNs fail,
and show performance improvements of one to three decimal digits.

</details>


### [270] [Finding Manifolds With Bilinear Autoencoders](https://arxiv.org/abs/2510.16820)
*Thomas Dooms,Ward Gauderis*

Main category: cs.LG

TL;DR: 本文使用双线性自动编码器将表示分解为二次多项式，探讨相关改进，是通过代数性质实现非线性可分析潜在表示的初步尝试。


<details>
  <summary>Details</summary>
Motivation: 稀疏自动编码器的解释依赖输入，研究不完整，而多项式可脱离输入分析，故尝试用其分析潜在表示。

Method: 使用双线性自动编码器将表示分解为二次多项式，并探讨诱导重要性排序、聚类和激活稀疏性的改进。

Result: 实现了表示到二次多项式的有效分解，并探讨了相关改进。

Conclusion: 这是通过代数性质实现非线性可分析潜在表示的初始步骤。

Abstract: Sparse autoencoders are a standard tool for uncovering interpretable latent
representations in neural networks. Yet, their interpretation depends on the
inputs, making their isolated study incomplete. Polynomials offer a solution;
they serve as algebraic primitives that can be analysed without reference to
input and can describe structures ranging from linear concepts to complicated
manifolds. This work uses bilinear autoencoders to efficiently decompose
representations into quadratic polynomials. We discuss improvements that induce
importance ordering, clustering, and activation sparsity. This is an initial
step toward nonlinear yet analysable latents through their algebraic
properties.

</details>


### [271] [ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning](https://arxiv.org/abs/2510.16824)
*Yingxu Wang,Kunyu Zhang,Jiaxin Huang,Nan Yin,Siwei Liu,Eran Segal*

Main category: cs.LG

TL;DR: 提出ProtoMol框架解决现有多模态分子表征学习方法的局限，实验显示其在多任务上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法在跨模态交互和模态对齐上有局限性，需改进以更好预测分子性质。

Method: 提出ProtoMol框架，含双分支分层编码器处理分子图和文本，采用层间双向跨模态注意力机制和共享原型空间。

Result: 在多个基准数据集上的实验表明，ProtoMol在各种分子属性预测任务中始终优于最先进的基线。

Conclusion: ProtoMol能有效解决现有多模态方法局限，实现分子图和文本的细粒度集成和语义对齐。

Abstract: Multimodal molecular representation learning, which jointly models molecular
graphs and their textual descriptions, enhances predictive accuracy and
interpretability by enabling more robust and reliable predictions of drug
toxicity, bioactivity, and physicochemical properties through the integration
of structural and semantic information. However, existing multimodal methods
suffer from two key limitations: (1) they typically perform cross-modal
interaction only at the final encoder layer, thus overlooking hierarchical
semantic dependencies; (2) they lack a unified prototype space for robust
alignment between modalities. To address these limitations, we propose
ProtoMol, a prototype-guided multimodal framework that enables fine-grained
integration and consistent semantic alignment between molecular graphs and
textual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,
utilizing Graph Neural Networks to process structured molecular graphs and
Transformers to encode unstructured texts, resulting in comprehensive
layer-wise representations. Then, ProtoMol introduces a layer-wise
bidirectional cross-modal attention mechanism that progressively aligns
semantic features across layers. Furthermore, a shared prototype space with
learnable, class-specific anchors is constructed to guide both modalities
toward coherent and discriminative representations. Extensive experiments on
multiple benchmark datasets demonstrate that ProtoMol consistently outperforms
state-of-the-art baselines across a variety of molecular property prediction
tasks.

</details>


### [272] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: 提出DrivAerStar数据集用于车辆空气动力学优化，提高精度并降低计算成本，连接学术与工业应用。


<details>
  <summary>Details</summary>
Motivation: 传统车辆空气动力学优化方法存在计算成本高或精度不足问题，现有机器学习数据集有局限性，需改进。

Method: 使用STAR - CCM+软件生成12000个工业级汽车CFD模拟，通过FFD算法探索三种车辆配置的20个CAD参数，采用细化网格策略和严格壁面y+控制。

Result: DrivAerStar风洞验证精度达1.04%以下，比现有数据集提升五倍，模型训练降低计算成本至数分钟。

Conclusion: DrivAerStar是首个连接学术与工业CFD实践的数据集，为汽车开发和多工程学科提供新范式。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [273] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出生物启发框架Fly - CL，减少训练时间，性能佳，理论证明可解决多重共线性，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有持续表征学习范式在相似性匹配阶段有多重共线性问题，且高级方法不适用于实时低延迟应用。

Method: 受苍蝇嗅觉回路启发，提出兼容多种预训练骨干网络的Fly - CL框架。

Result: Fly - CL大幅减少训练时间，性能与或超过当前最先进方法，理论上能解决多重共线性。

Conclusion: 通过生物启发设计，Fly - CL能有效应对持续表征学习挑战。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [274] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出UDS框架用于高效在线批量选择，在多基准测试中表现优于现有方法并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 全数据集SFT计算成本高、易过拟合或放大偏差，现有在线批量选择方法存在忽略多样性、依赖外部资源、增加训练时间等问题。

Method: 利用对数矩阵的核范数捕捉数据效用和样本内多样性，通过与历史样本轻量级内存缓冲区进行低维嵌入比较估计样本间多样性。

Result: 在多个基准测试中，UDS在不同数据预算下始终优于现有在线批量选择方法，显著减少全数据集微调的训练时间。

Conclusion: UDS是一种高效的在线批量选择框架，可用于SFT。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [275] [UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains](https://arxiv.org/abs/2510.16885)
*Duo Wang,Yuan Zuo,Guangyue Lu,Junjie Wu*

Main category: cs.LG

TL;DR: 提出UniGTE框架统一结构和语义推理，在多数据集上进行指令调优，无需推理时微调，在多任务和跨领域设置下取得零样本新SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络受限于固定标签空间，大语言模型难以捕捉图结构，需要一种能在无特定任务监督下泛化到未见图任务的方法。

Method: 引入UniGTE，编码器用可学习对齐标记和结构感知图文本注意力机制增强预训练自回归大语言模型，解码器基于图表示预测和重构，在五个数据集上进行指令调优。

Result: 在跨任务和跨领域设置下的节点分类、链接预测、图分类和图回归任务中取得零样本新SOTA结果。

Conclusion: 图结构与大语言模型语义的紧密集成可实现强大且可迁移的图推理。

Abstract: Generalizing to unseen graph tasks without task-specific supervision is
challenging: conventional graph neural networks are typically tied to a fixed
label space, while large language models (LLMs) struggle to capture graph
structure. We introduce UniGTE, an instruction-tuned encoder-decoder framework
that unifies structural and semantic reasoning. The encoder augments a
pretrained autoregressive LLM with learnable alignment tokens and a
structure-aware graph-text attention mechanism, enabling it to attend jointly
to a tokenized graph and a natural-language task prompt while remaining
permutation-invariant to node order. This yields compact, task-aware graph
representations. Conditioned solely on these representations, a frozen LLM
decoder predicts and reconstructs: it outputs the task answer and
simultaneously paraphrases the input graph in natural language. The
reconstruction objective regularizes the encoder to preserve structural cues.
UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,
and graph-level tasks across diverse domains, yet requires no fine-tuning at
inference. It achieves new state-of-the-art zero-shot results on node
classification, link prediction, graph classification, and graph regression
under cross-task and cross-domain settings, demonstrating that tight
integration of graph structure with LLM semantics enables robust, transferable
graph reasoning.

</details>


### [276] [DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library](https://arxiv.org/abs/2510.16897)
*Jose Siguenza,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 为降低使用门槛，扩展DEEPCHEM以支持SE(3)等变模型。


<details>
  <summary>Details</summary>
Motivation: 现有SE(3)等变神经网络库使用需大量背景知识且缺乏完整训练管道，限制科学家使用。

Method: 扩展DEEPCHEM，添加等变模型、完整训练管道和等变实用工具包，并提供测试和文档。

Result: 使缺乏深度学习背景的科学家能构建、训练和评估SE(3)等变模型。

Conclusion: 新实现便于SE(3)等变模型应用和进一步开发。

Abstract: Neural networks that incorporate geometric relationships respecting SE(3)
group transformations (e.g. rotations and translations) are increasingly
important in molecular applications, such as molecular property prediction,
protein structure modeling, and materials design. These models, known as
SE(3)-equivariant neural networks, ensure outputs transform predictably with
input coordinate changes by explicitly encoding spatial atomic positions.
Although libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful
implementations, they often require substantial deep learning or mathematical
prior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]
with support for ready-to-use equivariant models, enabling scientists with
minimal deep learning background to build, train, and evaluate models, such as
SE(3)-Transformer and Tensor Field Networks. Our implementation includes
equivariant models, complete training pipelines, and a toolkit of equivariant
utilities, supported with comprehensive tests and documentation, to facilitate
both application and further development of SE(3)-equivariant models.

</details>


### [277] [Adaptive Online Learning with LSTM Networks for Energy Price Prediction](https://arxiv.org/abs/2510.16898)
*Salih Salihoglu,Ibrahim Ahmed,Afshin Asadi*

Main category: cs.LG

TL;DR: 本文利用LSTM网络开发预测模型，结合多种特征和新损失函数及在线学习方法预测加州日前电价，结果表明模型性能良好，为电价预测提供框架。


<details>
  <summary>Details</summary>
Motivation: 准确预测电价对能源市场利益相关者至关重要，本研究旨在开发有效预测模型。

Method: 利用LSTM网络，结合历史价格、天气、能源发电组合等特征，引入新的自定义损失函数，采用在线学习方法。

Result: 自定义损失函数提高模型性能，在线学习模型表现更优，纳入能源发电组合增强预测能力。

Conclusion: 本研究为电价预测提供了可靠框架，对动态电力市场决策有重要价值。

Abstract: Accurate prediction of electricity prices is crucial for stakeholders in the
energy market, particularly for grid operators, energy producers, and
consumers. This study focuses on developing a predictive model leveraging Long
Short-Term Memory (LSTM) networks to forecast day-ahead electricity prices in
the California energy market. The model incorporates a variety of features,
including historical price data, weather conditions, and the energy generation
mix. A novel custom loss function that integrates Mean Absolute Error (MAE),
Jensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to
enhance the prediction accuracy and interpretability. Additionally, an online
learning approach is implemented to allow the model to adapt to new data
incrementally, ensuring continuous relevance and accuracy. The results
demonstrate that the custom loss function can improve the model's performance,
aligning predicted prices more closely with actual values, particularly during
peak intervals. Also, the online learning model outperforms other models by
effectively incorporating real-time data, resulting in lower prediction error
and variability. The inclusion of the energy generation mix further enhances
the model's predictive capabilities, highlighting the importance of
comprehensive feature integration. This research provides a robust framework
for electricity price forecasting, offering valuable insights and tools for
better decision-making in dynamic electricity markets.

</details>


### [278] [SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning](https://arxiv.org/abs/2510.16899)
*Dun Liu,Qin Pang,Guangai Liu,Hongyu Mou,Jipeng Fan,Yiming Miao,Pin-Han Ho,Limei Peng*

Main category: cs.LG

TL;DR: 为解决非结构化临床文档影响医疗AI效果的问题，提出知识驱动框架构建医疗知识图，生成结构化数据集微调大语言模型，实验表明该方法提升AI诊断推理有效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 非结构化临床文档导致训练数据质量差，阻碍医疗AI有效性。

Method: 构建集成SNOMED CT和Neo4j的结构化医疗知识图，提取并标准化实体关系对生成JSON数据集，用于微调大语言模型。

Result: 知识引导方法提升了AI生成诊断推理的有效性和可解释性。

Conclusion: 该方法为构建可靠的AI辅助临床系统提供可扩展解决方案。

Abstract: The effectiveness of artificial intelligence (AI) in healthcare is
significantly hindered by unstructured clinical documentation, which results in
noisy, inconsistent, and logically fragmented training data. To address this
challenge, we present a knowledge-driven framework that integrates the
standardized clinical terminology SNOMED CT with the Neo4j graph database to
construct a structured medical knowledge graph. In this graph, clinical
entities such as diseases, symptoms, and medications are represented as nodes,
and semantic relationships such as ``caused by,'' ``treats,'' and ``belongs
to'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT
relationship concepts (e.g., \texttt{Causative agent}, \texttt{Indicated for}).
This design enables multi-hop reasoning and ensures terminological consistency.
By extracting and standardizing entity-relationship pairs from clinical texts,
we generate structured, JSON-formatted datasets that embed explicit diagnostic
pathways. These datasets are used to fine-tune large language models (LLMs),
significantly improving the clinical logic consistency of their outputs.
Experimental results demonstrate that our knowledge-guided approach enhances
the validity and interpretability of AI-generated diagnostic reasoning,
providing a scalable solution for building reliable AI-assisted clinical
systems.

</details>


### [279] [A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch](https://arxiv.org/abs/2510.16911)
*Sarah Al-Shareeda,Gulcihan Ozdemir,Heung Seok Jeon,Khaleel Ahmad*

Main category: cs.LG

TL;DR: 针对传感器数据存在噪声、不完整等问题，提出轻量级深度学习管道和GRU - LSTM模型进行次日电力需求预测，取得良好效果，证明特定预处理与紧凑循环架构可实现准确快速的能源预测。


<details>
  <summary>Details</summary>
Motivation: 解决传感器数据存在噪声、不完整且缺乏上下文丰富性时，准确进行短期能源消耗预测的问题，参与相关竞赛进行次日电力需求预测。

Method: 提出结合小时级降采样、双模式插补（均值和多项式回归）和综合归一化的深度学习管道，选择标准缩放；使用轻量级GRU - LSTM序列到单值模型。

Result: 模型平均RMSE为601.9W，MAE为468.9W，准确率达84.36%；能很好地泛化，捕捉非线性需求模式，推理延迟低；时空热图分析显示温度趋势与预测消耗高度一致。

Conclusion: 特定的预处理与紧凑的循环架构能在现实条件下实现快速、准确且可部署的能源预测。

Abstract: How can short-term energy consumption be accurately forecasted when sensor
data is noisy, incomplete, and lacks contextual richness? This question guided
our participation in the \textit{2025 Competition on Electric Energy
Consumption Forecast Adopting Multi-criteria Performance Metrics}, which
challenged teams to predict next-day power demand using real-world
high-frequency data. We proposed a robust yet lightweight Deep Learning (DL)
pipeline combining hourly downsizing, dual-mode imputation (mean and polynomial
regression), and comprehensive normalization, ultimately selecting Standard
Scaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model
achieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\% accuracy.
Despite asymmetric inputs and imputed gaps, it generalized well, captured
nonlinear demand patterns, and maintained low inference latency. Notably,
spatiotemporal heatmap analysis reveals a strong alignment between temperature
trends and predicted consumption, further reinforcing the model's reliability.
These results demonstrate that targeted preprocessing paired with compact
recurrent architectures can still enable fast, accurate, and deployment-ready
energy forecasting in real-world conditions.

</details>


### [280] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 提出领域可泛化持续学习（DGCL）新设置，提出基于预训练模型的自适应领域转换（DoT）方法，实验验证其有效性和资源效率。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在训练和测试领域不同时，在DGCL场景中表现不佳，需要新方法以适应动态环境和跨领域泛化。

Method: 提出DoT方法，受人类大脑分布式加中心理论启发，在表征学习中分离语义和领域相关信息，自适应转换任务表征以实现输出对齐。

Result: DoT作为插件策略，在全参数调优和参数高效调优范式下提升了现有持续学习基线的性能。

Conclusion: DoT方法有效，能积累领域可泛化知识，且轻量级实现保证了资源效率。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [281] [SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search](https://arxiv.org/abs/2510.16916)
*Dong Li,Xujiang Zhao,Linlin Yu,Yanchi Liu,Wei Cheng,Zhengzhang Chen,Zhong Chen,Feng Chen,Chen Zhao,Haifeng Chen*

Main category: cs.LG

TL;DR: 介绍训练免费框架SolverLLM解决优化问题，表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有解决优化问题的方法存在泛化性差或训练成本高的问题。

Method: 提出SolverLLM框架，利用测试时缩放，通过新的蒙特卡罗树搜索策略生成数学公式并转化为代码，还对经典MCTS进行三项改进。

Result: 在六个标准基准数据集上，SolverLLM优于基于提示和基于学习的基线。

Conclusion: SolverLLM无需额外训练，能实现强泛化。

Abstract: Large Language Models (LLMs) offer promising capabilities for tackling
complex reasoning tasks, including optimization problems. However, existing
methods either rely on prompt engineering, which leads to poor generalization
across problem types, or require costly supervised training. We introduce
SolverLLM, a training-free framework that leverages test-time scaling to solve
diverse optimization problems. Rather than solving directly, SolverLLM
generates mathematical formulations and translates them into solver-ready code,
guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the
search process, we modify classical MCTS with (1) dynamic expansion for
adaptive formulation generation, (2) prompt backpropagation to guide
exploration via outcome-driven feedback, and (3) uncertainty backpropagation to
incorporate reward reliability into decision-making. Experiments on six
standard benchmark datasets demonstrate that SolverLLM outperforms both
prompt-based and learning-based baselines, achieving strong generalization
without additional training.

</details>


### [282] [Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws](https://arxiv.org/abs/2510.16927)
*Egor Petrov,Nikita Kiselev,Vladislav Meshkov,Andrey Grabovoy*

Main category: cs.LG

TL;DR: 本文推导Transformer组件二阶表达式，完成Hessian表征，分析收敛动态和缩放定律，提出分析框架，为大规模深度学习优化研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决Layer Normalization和前馈Hessians理论结果缺失，填补Transformer优化景观研究空白。

Method: 推导组件二阶表达式，提出基于泰勒展开的分析框架。

Result: 完成全Transformer块的Hessian表征，给出各子层在曲率传播中作用的估计，揭示收敛动态和缩放定律。

Conclusion: 将Hessian理论扩展到全Transformer架构，为大规模深度学习优化的理论和实证研究建立新基础。

Abstract: The lack of theoretical results for Layer Normalization and feedforward
Hessians has left a gap in the study of Transformer optimization landscapes. We
address this by deriving explicit second-order expressions for these
components, thereby completing the Hessian characterization of full Transformer
blocks. Our results generalize prior self-attention analyses and yield
estimations for the role of each sublayer in curvature propagation. We
demonstrate how these Hessian structures inform both convergence dynamics and
the empirical scaling laws governing large-model performance. Further, we
propose a Taylor-expansion-based framework for analyzing loss differences to
quantify convergence trajectories. By extending Hessian theory to the full
Transformer architecture, this work establishes a new foundation for
theoretical and empirical investigations of optimization in large-scale deep
learning.

</details>


### [283] [A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2510.16940)
*Cristian J. Vaca-Rubio,Roberto Pereira,Luis Blanco,Engin Zeydan,Màrius Caus*

Main category: cs.LG

TL;DR: 本文提出用于时间序列预测的概率柯尔莫哥洛夫 - 阿诺尔德网络（P - KAN），在卫星流量预测中表现优于多层感知机基线，有高斯和学生t分布两种变体，适用于卫星通信等领域。


<details>
  <summary>Details</summary>
Motivation: 寻找能进行时间序列预测，捕捉非线性和重尾动态，且参数高效、可考虑不确定性的模型。

Method: 提出P - KAN，用基于样条的函数连接替换标量权重，直接参数化预测分布，在卫星流量预测中评估，构建高斯和学生t分布两种变体。

Result: P - KAN在准确性和校准方面始终优于多层感知机基线，实现了更好的效率 - 风险权衡，使用参数显著减少；高斯变体适用于安全关键场景，学生t变体在稳定需求下提高效率。

Conclusion: P - KAN是一个强大的概率预测框架，可直接应用于卫星通信和其他资源受限领域。

Abstract: This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel
probabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series
forecasting. By replacing scalar weights with spline-based functional
connections and directly parameterizing predictive distributions, P-KANs offer
expressive yet parameter-efficient models capable of capturing nonlinear and
heavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,
where uncertainty-aware predictions enable dynamic thresholding for resource
allocation. Results show that P-KANs consistently outperform Multi Layer
Perceptron (MLP) baselines in both accuracy and calibration, achieving superior
efficiency-risk trade-offs while using significantly fewer parameters. We build
up P-KANs on two distributions, namely Gaussian and Student-t distributions.
The Gaussian variant provides robust, conservative forecasts suitable for
safety-critical scenarios, whereas the Student-t variant yields sharper
distributions that improve efficiency under stable demand. These findings
establish P-KANs as a powerful framework for probabilistic forecasting with
direct applicability to satellite communications and other resource-constrained
domains.

</details>


### [284] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 提出组件级评估框架评估大语言模型生成的数学优化公式，评估多个模型和提示策略，GPT - 5表现最佳，得出NLP到优化建模三原则。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型生成优化公式的评估依赖粗略指标，会掩盖结构或数值错误，需要更全面的评估方法。

Method: 提出组件级评估框架，引入多种新指标，在六种提示策略下对多个模型在不同复杂度优化问题上进行评估。

Result: GPT - 5始终优于其他模型，链思维、自一致性和模块化提示最有效，求解器性能主要取决于高约束召回率和低约束RMSE。

Conclusion: 提出NLP到优化建模的三原则，所提框架为优化建模中LLMs的细粒度诊断评估奠定基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [285] [Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction](https://arxiv.org/abs/2510.16958)
*Ganglin Tian,Anastase Alexandre Charantonis,Camille Le Coz,Alexis Tantet,Riwal Plougonven*

Main category: cs.LG

TL;DR: 研究评估三种概率方法对次季节风速预测进行降尺度，结果显示概率降尺度方法比简单随机方法能提供更真实的空间不确定性表示，对可再生能源规划和风险评估有积极作用。


<details>
  <summary>Details</summary>
Motivation: 改进从大尺度大气预测因子回归地表风速时不确定性的空间表示，解决之前方法无法充分表示空间相关性和物理一致性的问题。

Method: 评估三种具有不同不确定性量化机制的概率方法，包括分位数回归神经网络、变分自编码器和扩散模型，在ERA5再分析数据上训练并应用于ECMWF次季节后报。

Result: 概率降尺度方法比简单随机方法提供更真实的空间不确定性表示，不同概率模型在集合离散度、确定性技巧和物理一致性方面各有优势。

Conclusion: 概率降尺度是对业务次季节风速预报的有效改进，可用于可再生能源规划和风险评估。

Abstract: This study aims to improve the spatial representation of uncertainties when
regressing surface wind speeds from large-scale atmospheric predictors for
sub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale
atmospheric predictors such as 500 hPa geopotential height (Z500), which
exhibit higher predictability than surface variables and can be downscaled to
obtain more localised information. Previous work by Tian et al. (2024)
demonstrated that stochastic perturbations based on model residuals can improve
ensemble dispersion representation in statistical downscaling frameworks, but
this method fails to represent spatial correlations and physical consistency
adequately. More sophisticated approaches are needed to capture the complex
relationships between large-scale predictors and local-scale predictands while
maintaining physical consistency. Probabilistic deep learning models offer
promising solutions for capturing complex spatial dependencies. This study
evaluates three probabilistic methods with distinct uncertainty quantification
mechanisms: Quantile Regression Neural Network that directly models
distribution quantiles, Variational Autoencoders that leverage latent space
sampling, and Diffusion Models that utilise iterative denoising. These models
are trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts
to regress probabilistic wind speed ensembles. Our results show that
probabilistic downscaling approaches provide more realistic spatial uncertainty
representations compared to simpler stochastic methods, with each probabilistic
model offering different strengths in terms of ensemble dispersion,
deterministic skill, and physical consistency. These findings establish
probabilistic downscaling as an effective enhancement to operational
sub-seasonal wind forecasts for renewable energy planning and risk assessment.

</details>


### [286] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出一种有效KD检测框架，利用MoE结构习惯信号，有白盒和黑盒方法，建基准测试，实验显示高准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有KD检测方法易被规避，且KD带来知识产权保护和模型多样性风险。

Method: 利用MoE结构习惯信号，白盒分析专家协作，黑盒提出Shadow - MoE构建代理MoE表示，建立可复现基准。

Result: 实验在不同场景下检测准确率超94%，对基于提示的规避有强鲁棒性，优于现有基线。

Conclusion: 所提方法有效检测KD，凸显LLMs中结构习惯的转移。

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [287] [Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision](https://arxiv.org/abs/2510.16980)
*Kanghui Ning,Zijie Pan,Yushan Jiang,Anderson Schneider,Yuriy Nevmyvaka,Dongjin Song*

Main category: cs.LG

TL;DR: 本文提出BlueSky愿景，通过两个互补方向构建灵活可扩展框架，推动时间序列推理发展。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理旨在超越模式识别，实现明确、可解释和可信的推理，该研究为此提供框架。

Method: 从构建时间序列推理的坚实基础和推进系统级推理两个互补方向开展研究。

Result: 提出了一个灵活且可扩展的推进时间序列推理的框架。

Conclusion: 该框架能在不同领域提供可解释和可信的时间智能。

Abstract: Time series reasoning is emerging as the next frontier in temporal analysis,
aiming to move beyond pattern recognition towards explicit, interpretable, and
trustworthy inference. This paper presents a BlueSky vision built on two
complementary directions. One builds robust foundations for time series
reasoning, centered on comprehensive temporal understanding, structured
multi-step reasoning, and faithful evaluation frameworks. The other advances
system-level reasoning, moving beyond language-only explanations by
incorporating multi-agent collaboration, multi-modal context, and
retrieval-augmented approaches. Together, these directions outline a flexible
and extensible framework for advancing time series reasoning, aiming to deliver
interpretable and trustworthy temporal intelligence across diverse domains.

</details>


### [288] [MuonBP: Faster Muon via Block-Periodic Orthogonalization](https://arxiv.org/abs/2510.16981)
*Ahmed Khaled,Kaan Ozkara,Tao Yu,Mingyi Hong,Youngsuk Park*

Main category: cs.LG

TL;DR: 提出MuonBP优化器解决梯度正交化在模型并行时的通信开销问题，有理论保证，性能佳。


<details>
  <summary>Details</summary>
Motivation: 梯度正交化在模型并行时相比逐坐标优化器有额外通信开销，影响吞吐量。

Method: 提出MuonBP，对各设备矩阵分片独立正交化并定期全正交化，调整学习率，采用两个步长。

Result: 方法简单，超参调整少，迭代复杂度有竞争力，训练8B模型时吞吐量比Muon提高8%且性能无下降。

Conclusion: MuonBP解决了梯度正交化在模型并行时的通信开销问题，表现良好。

Abstract: Gradient orthogonalization is a simple strategy that shows great utility in
speeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)
combines gradient orthogonalization with first-order momentum and achieves
significant improvement in data efficiency over Adam/AdamW (Loshchilov and
Hutter, 2019) for language model training. However, when using model
parallelism, gradient orthogonalization introduces additional overhead compared
to coordinate-wise optimizers (such as AdamW) due to additional gather and
scatter operations on gradient matrix shards from different devices. This
additional communication can amount to a throughput hit of 5%-10% compared to
Adam/AdamW. To remedy this, we propose Muon with Block-Periodic
Orthogonalization (MuonBP), which applies orthogonalization independently to
matrix shards on each device and periodically performs full orthogonalization
to maintain training stability at scale. We show how to adjust the learning
rate from the baseline to MuonBP and give convergence guarantees for this
algorithm. Crucially, our theory dictates that we use two stepsizes: one for
the blockwise orthogonalization steps, and one for the full orthogonalization
steps. Our method is simple, requires minimal hyperparameter adjustments, and
achieves competitive iteration complexity compared with baseline Muon while
providing per-iteration throughput comparable to coordinate-wise methods such
as AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO
optimizer state sharding, MuonBP achieves 8% throughput increase compared to
Muon with no degradation in performance.

</details>


### [289] [Graph4MM: Weaving Multimodal Learning with Structural Information](https://arxiv.org/abs/2510.16990)
*Xuying Ning,Dongqi Fu,Tianxin Wei,Wujiang Xu,Jingrui He*

Main category: cs.LG

TL;DR: 论文指出传统多模态学习存在局限，提出Graph4MM框架，实验显示其表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习中整合多跳邻居结构信息和原则性融合特定模态信息的挑战。

Method: 提出Graph4MM框架，引入Hop - Diffused Attention整合多跳结构信息，设计MM - QFormer进行跨模态融合。

Result: Graph4MM在生成和判别任务中表现优于大型视觉语言模型、大语言模型和多模态图基线模型，平均提升6.93%。

Conclusion: 利用结构整合模态内和模态间交互能提升多模态理解。

Abstract: Real-world multimodal data usually exhibit complex structural relationships
beyond traditional one-to-one mappings like image-caption pairs. Entities
across modalities interact in intricate ways, with images and text forming
diverse interconnections through contextual dependencies and co-references.
Graphs provide powerful structural information for modeling intra-modal and
inter-modal relationships. However, previous works fail to distinguish
multi-hop neighbors and treat the graph as a standalone modality, which
fragments the overall understanding. This limitation presents two key
challenges in multimodal learning: (1) integrating structural information from
multi-hop neighbors into foundational models, and (2) fusing modality-specific
information in a principled manner. To address these challenges, we revisit the
role of graphs in multimodal learning within the era of foundation models and
propose Graph4MM, a graph-based multimodal learning framework. To be specific,
we introduce Hop-Diffused Attention, which integrates multi-hop structural
information into self-attention through causal masking and hop diffusion.
Furthermore, we design MM-QFormer, a multi-mapping querying transformer for
cross-modal fusion. Through theoretical and empirical analysis, we show that
leveraging structures to integrate both intra- and inter-modal interactions
improves multimodal understanding beyond treating them as a standalone
modality. Experiments on both generative and discriminative tasks show that
Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,
achieving a 6.93% average improvement.

</details>


### [290] [EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit](https://arxiv.org/abs/2510.17002)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: 提出EEschematic AI代理用于自动模拟原理图生成，集成多模态并实验验证其能生成高质量原理图。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的电路设计方法多仅依赖文本表示，缺乏对电路设计师的视觉可解释性。

Method: 提出EEschematic，集成文本、视觉和符号模态，使用六个模拟子结构示例进行少样本布局，采用视觉思维链策略迭代优化布局和布线。

Result: 在代表性模拟电路上的实验表明，EEschematic生成的原理图具有高视觉质量和结构正确性。

Conclusion: EEschematic能有效解决现有方法缺乏视觉可解释性的问题，生成高质量的模拟原理图。

Abstract: Circuit schematics play a crucial role in analog integrated circuit design,
serving as the primary medium for human understanding and verification of
circuit functionality. While recent large language model (LLM)-based approaches
have shown promise in circuit topology generation and device sizing, most rely
solely on textual representations such as SPICE netlists, which lack visual
interpretability for circuit designers. To address this limitation, we propose
EEschematic, an AI agent for automatic analog schematic generation based on a
Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual,
and symbolic modalities to translate SPICE netlists into schematic diagrams
represented in a human-editable format. The framework uses six analog
substructure examples for few-shot placement and a Visual Chain-of-Thought
(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic
clarity and symmetry. Experimental results on representative analog circuits,
including a CMOS inverter, a five-transistor operational transconductance
amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that
EEschematic produces schematics with high visual quality and structural
correctness.

</details>


### [291] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 研究大语言模型反学习过程是否可被后门攻击，发现注意力汇聚点可作为后门反学习的切入点，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 随着开放权重大型语言模型兴起，探究反学习过程能否被后门攻击。

Method: 借鉴经典后门攻击，研究后门反学习，分析触发器位置和后门训练强化方式，发现与注意力汇聚现象的联系。

Result: 将触发器置于注意力汇聚点并对齐注意力值可增强后门持久性，实验表明后门触发器存在时能恢复遗忘知识，不存在时与正常反学习模型无区别。

Conclusion: 注意力汇聚点引导的后门反学习是可行的，代码已开源。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [292] [Curiosity-driven RL for symbolic equation solving](https://arxiv.org/abs/2510.17022)
*Kevin P. O Keeffe*

Main category: cs.LG

TL;DR: 探索强化学习用于符号数学，表明带好奇探索和图动作的无模型PPO可解非线性方程，好奇探索或对符号推理有用。


<details>
  <summary>Details</summary>
Motivation: 探究强化学习在符号数学领域是否有用。

Method: 使用无模型PPO，结合基于好奇的探索和基于图的动作。

Result: 能解决涉及根式、指数和三角函数等非线性方程。

Conclusion: 基于好奇的探索可能对一般符号推理任务有用。

Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.

</details>


### [293] [Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation](https://arxiv.org/abs/2510.17036)
*Nguyen Do,Bach Ngo,Youval Kashuv,Canh V. Pham,Hanghang Tong,My T. Thai*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the Quality of Service Degradation (QoSD) problem, in which an
adversary perturbs edge weights to degrade network performance. This setting
arises in both network infrastructures and distributed ML systems, where
communication quality, not just connectivity, determines functionality. While
classical methods rely on combinatorial optimization, and recent ML approaches
address only restricted linear variants with small-size networks, no prior
model directly tackles the QoSD problem under nonlinear edge-weight functions.
This work proposes \PIMMA, a self-reinforcing generative framework that
synthesizes feasible solutions in latent space, to fill this gap. Our method
includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm
that uses graph learning and approximation to produce feasible solutions with
performance guarantee, (2) Morph: a new theoretically grounded training
paradigm for Mixture of Conditional VAEs guided by an energy-based model to
capture solution feature distributions, and (3) Refine: a reinforcement
learning agent that explores this space to generate progressively near-optimal
solutions using our designed differentiable reward function. Experiments on
both synthetic and real-world networks show that our approach consistently
outperforms classical and ML baselines, particularly in scenarios with
nonlinear cost functions where traditional methods fail to generalize.

</details>


### [294] [Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability](https://arxiv.org/abs/2510.17040)
*Hoang-Son Nguyen,Xiao Fu*

Main category: cs.LG

TL;DR: 提出Diverse Influence Component Analysis (DICA)框架，利用混合函数雅可比矩阵的凸几何，通过J - VolMax准则识别潜在成分，在合理条件下无需辅助信息等假设。


<details>
  <summary>Details</summary>
Motivation: 解决从未知非线性混合中识别潜在成分这一机器学习基础挑战，改进现有非线性独立成分分析（nICA）方法。

Method: 引入DICA框架，提出Jacobian Volume Maximization (J - VolMax)准则，鼓励潜在成分对观测变量影响的多样性。

Result: 在合理条件下，无需依赖辅助信息、潜在成分独立性或雅可比矩阵稀疏性假设实现潜在成分可识别性。

Conclusion: 该方法扩展了可识别性分析范围，为现有方法提供了补充视角。

Abstract: Latent component identification from unknown nonlinear mixtures is a
foundational challenge in machine learning, with applications in tasks such as
disentangled representation learning and causal inference. Prior work in
nonlinear independent component analysis (nICA) has shown that auxiliary
signals -- such as weak supervision -- can support identifiability of
conditionally independent latent components. More recent approaches explore
structural assumptions, e.g., sparsity in the Jacobian of the mixing function,
to relax such requirements. In this work, we introduce Diverse Influence
Component Analysis (DICA), a framework that exploits the convex geometry of the
mixing function's Jacobian. We propose a Jacobian Volume Maximization
(J-VolMax) criterion, which enables latent component identification by
encouraging diversity in their influence on the observed variables. Under
reasonable conditions, this approach achieves identifiability without relying
on auxiliary information, latent component independence, or Jacobian sparsity
assumptions. These results extend the scope of identifiability analysis and
offer a complementary perspective to existing methods.

</details>


### [295] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 研究基于思维链推理的强化学习语言模型在面对纠正指令时的推理过程，发现存在动机性推理现象，小模型难以检测，强调评估和监督时需考虑此问题。


<details>
  <summary>Details</summary>
Motivation: 语言模型训练因奖励信号不完美产生非预期行为，应用事后指令纠正时，探究模型推理过程变化。

Method: 在简单场景中开展研究。

Result: 模型存在动机性推理，多数前沿推理模型可检测，但小模型有检测漏洞，甚至会被误导。

Conclusion: 依赖思维链过程进行模型评估和监督时，需要考虑动机性推理问题。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [296] [Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training](https://arxiv.org/abs/2510.17058)
*Hassan Hamad,Yuou Qiu,Peter A. Beerel,Keith M. Chugg*

Main category: cs.LG

TL;DR: 本文提出低精度对数定点训练新改进，用含位宽近似方法，经模拟退火优化，模拟显示低精度训练精度损失小，硬件能耗和面积降低。


<details>
  <summary>Details</summary>
Motivation: 量化虽降低推理计算成本，但深度学习训练仍依赖浮点运算，低精度定点训练是替代方案，为未来硬件加速器设计改进低精度对数定点训练。

Method: 在算术运算近似设计中引入位宽，提出对数加法的分段线性近似，用模拟退火在不同精度水平优化。

Result: C++位精确模拟显示，用12位整数算术训练VGG - 11和VGG - 16模型，与32位浮点训练相比精度损失小；硬件研究显示，所提LNS乘加单元面积最多降32.5%，能耗最多降53.5%。

Conclusion: 所提低精度对数定点训练改进方法有效，能在保证一定精度下降低硬件面积和能耗。

Abstract: While advancements in quantization have significantly reduced the
computational costs of inference in deep learning, training still predominantly
relies on complex floating-point arithmetic. Low-precision fixed-point training
presents a compelling alternative. This work introduces a novel enhancement in
low-precision logarithmic fixed-point training, geared towards future hardware
accelerator designs. We propose incorporating bitwidth in the design of
approximations to arithmetic operations. To this end, we introduce a new
hardware-friendly, piece-wise linear approximation for logarithmic addition.
Using simulated annealing, we optimize this approximation at different
precision levels. A C++ bit-true simulation demonstrates training of VGG-11 and
VGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer
arithmetic with minimal accuracy degradation compared to 32-bit floating-point
training. Our hardware study reveals up to 32.5% reduction in area and 53.5%
reduction in energy consumption for the proposed LNS multiply-accumulate units
compared to that of linear fixed-point equivalents.

</details>


### [297] [Consistent Zero-Shot Imitation with Contrastive Goal Inference](https://arxiv.org/abs/2510.17059)
*Kathryn Wantlin,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出自监督预训练交互式智能体方法，使其能即时模仿人类演示，实验显示该方法在零样本模仿上优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 探讨智能体模型如何以自监督方式训练，解决现有模型训练缺乏明确动作概念、纯探索方法无法让智能体快速适应新任务等问题。

Method: 以目标（即观察结果）为原子构建，训练时自动提出目标并练习达成，评估时解决（摊销）逆强化学习问题以解释演示为最优目标达成行为。

Result: 在非为目标达成设计的标准基准测试中，该方法在零样本模仿上优于先前方法。

Conclusion: 所提出的自监督预训练交互式智能体方法有效，能让智能体即时模仿人类演示。

Abstract: In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

</details>


### [298] [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](https://arxiv.org/abs/2510.17106)
*Chen Zhang,Weixin Bu,Wendong Xu,Runsheng Yu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 本文揭示Transformer编码器与GCN的等价性，提出Fighter架构，实验证明其有竞争力且具可解释性。


<details>
  <summary>Details</summary>
Motivation: Transformer在时间序列建模成功但内部机制不透明，需进行解释。

Method: 建立Transformer编码器与GCN的等价关系，提出去除冗余线性投影、包含多跳图聚合的Fighter架构。

Result: 在标准预测基准实验中，Fighter取得有竞争力的性能。

Conclusion: Fighter能在实现良好性能的同时，对预测提供更清晰的机理解释。

Abstract: Transformers have achieved remarkable success in time series modeling, yet
their internal mechanisms remain opaque. This work demystifies the Transformer
encoder by establishing its fundamental equivalence to a Graph Convolutional
Network (GCN). We show that in the forward pass, the attention distribution
matrix serves as a dynamic adjacency matrix, and its composition with
subsequent transformations performs computations analogous to graph
convolution. Moreover, we demonstrate that in the backward pass, the update
dynamics of value and feed-forward projections mirror those of GCN parameters.
Building on this unified theoretical reinterpretation, we propose
\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined
architecture that removes redundant linear projections and incorporates
multi-hop graph aggregation. This perspective yields an explicit and
interpretable representation of temporal dependencies across different scales,
naturally expressed as graph edges. Experiments on standard forecasting
benchmarks confirm that Fighter achieves competitive performance while
providing clearer mechanistic interpretability of its predictions.

</details>


### [299] [Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control](https://arxiv.org/abs/2510.17122)
*Chengxiu Hua,Jiawen Gu,Yushun Tang*

Main category: cs.LG

TL;DR: 本文提出一种用于连续时间控制的强化学习方法CQSM，解决连续时间RL中不依赖时间离散化保留Q函数动作评估能力的问题，还给出线性二次控制问题理论解，数值结果验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法多为离散时间，需开发连续时间控制的新方法，解决连续时间RL中不依赖时间离散化保留Q函数动作评估能力的长期挑战。

Method: 通过鞅条件表征连续时间Q函数，利用动态规划原理将扩散策略得分与学习到的连续Q函数的动作梯度联系起来，提出基于得分的策略改进算法CQSM。

Result: 数值结果表明所提方法有效，并与流行基线进行了比较。

Conclusion: 所提出的连续时间强化学习方法CQSM能有效解决相关问题，在连续时间控制中有应用潜力。

Abstract: Reinforcement learning (RL) has achieved significant success across a wide
range of domains, however, most existing methods are formulated in discrete
time. In this work, we introduce a novel RL method for continuous-time control,
where stochastic differential equations govern state-action dynamics. Departing
from traditional value function-based approaches, our key contribution is the
characterization of continuous-time Q-functions via a martingale condition and
the linking of diffusion policy scores to the action gradient of a learned
continuous Q-function by the dynamic programming principle. This insight
motivates Continuous Q-Score Matching (CQSM), a score-based policy improvement
algorithm. Notably, our method addresses a long-standing challenge in
continuous-time RL: preserving the action-evaluation capability of Q-functions
without relying on time discretization. We further provide theoretical
closed-form solutions for linear-quadratic (LQ) control problems within our
framework. Numerical results in simulated environments demonstrate the
effectiveness of our proposed method and compare it to popular baselines.

</details>


### [300] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: 本文提出评估大语言模型潜在信息发现能力的统一基准，结果显示模型表现因上下文而异，该基准为研究个性化交互提供系统框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在满足用户特定偏好时存在局限，需探究其能否通过对话挖掘和推理潜在信息。

Method: 引入统一基准，涵盖三种场景，采用三智能体框架进行逐轮评估。

Result: 大语言模型能通过对话挖掘潜在信息，但成功率因任务复杂度、主题和隐藏属性数量在32% - 98%间变化。

Conclusion: 该基准为研究个性化交互中潜在信息发现提供首个系统框架，有效偏好推理仍是构建自适应AI系统的挑战。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [301] [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](https://arxiv.org/abs/2510.17136)
*Enhao Gu,Haolin Hou*

Main category: cs.LG

TL;DR: 提出In - situ Autoguidance方法，在无辅助组件下实现图像生成扩散模型的自引导，建立高效成本引导新基线。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成扩散模型中，流行的CFG方法降低了图像多样性，虽有工作解决但需辅助模型，有较大开销，因此需无辅助组件的方法。

Method: 提出In - situ Autoguidance方法，通过随机前向传播动态生成劣质预测，将引导重构为推理时的自我修正。

Result: 该零成本方法可行，建立了成本高效引导的新基线。

Conclusion: 无需外部模型即可实现自引导的好处。

Abstract: The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

</details>


### [302] [Learning After Model Deployment](https://arxiv.org/abs/2510.17160)
*Derda Kaymak,Gyuhak Kim,Tomoya Kaichi,Tatsuya Konishi,Bing Liu*

Main category: cs.LG

TL;DR: 文章提出自主学习范式ALMD，面临检测、学习和数据稀缺问题，提出PLDA方法并将实证评估其有效性。


<details>
  <summary>Details</summary>
Motivation: 经典监督学习模型部署后固定，不适用于动态开放环境，需能检测和学习新样本的范式。

Method: 提出PLDA方法，进行动态OOD检测和新类别的增量学习。

Result: 文中未提及具体结果，但将进行实证评估来展示PLDA的有效性。

Conclusion: 文中未明确给出结论，但预期PLDA能有效解决ALMD面临的问题。

Abstract: In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

</details>


### [303] [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](https://arxiv.org/abs/2510.17162)
*Guanjie Cheng,Siyang Liu,Junqin Huang,Xinkui Zhao,Yin Wang,Mengying Zhu,Linghe Kong,Shuiguang Deng*

Main category: cs.LG

TL;DR: 提出轻量级自适应框架ALPINE，可让终端设备实时自主调整差分隐私级别，经理论分析和模拟证明有效且实用。


<details>
  <summary>Details</summary>
Motivation: 移动边缘众包感知系统存在隐私威胁，静态差分隐私机制无法适应风险变化，导致保护不足或噪声过多。

Method: 提出ALPINE框架，由动态风险感知、基于TD3的隐私决策、本地隐私执行和边缘节点性能验证四个模块组成，设计奖励函数引导TD3代理调整噪声幅度。

Result: 通过理论分析和真实世界模拟，证明ALPINE能有效减轻推理攻击，同时保留数据效用和控制成本。

Conclusion: ALPINE实用，适用于大规模边缘应用。

Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

</details>


### [304] [Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses](https://arxiv.org/abs/2510.17185)
*Runlin Lei,Lu Yi,Mingguo He,Pengyu Qiu,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.LG

TL;DR: 本文提出统一框架评估文本属性图学习的鲁棒性，分析多种模型在不同攻击下表现，发现模型存在鲁棒性权衡，还提出SFT - auto框架提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对图神经网络和大语言模型在文本属性图学习中的鲁棒性理解不全面，评估零散，需系统研究文本和结构扰动的影响。

Method: 引入统一框架，评估经典GNNs、RGNNs和GraphLLMs在十种数据集、不同扰动和攻击场景下的表现。

Result: 发现模型在文本和结构上存在鲁棒性权衡，GNNs和RGNNs性能依赖文本编码器和攻击类型，GraphLLMs易受训练数据损坏影响。

Conclusion: 为文本属性图安全研究奠定基础，提供在对抗环境中进行鲁棒文本属性图学习的实用解决方案。

Abstract: While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are
powerful approaches for learning on Text-Attributed Graphs (TAGs), a
comprehensive understanding of their robustness remains elusive. Current
evaluations are fragmented, failing to systematically investigate the distinct
effects of textual and structural perturbations across diverse models and
attack scenarios. To address these limitations, we introduce a unified and
comprehensive framework to evaluate robustness in TAG learning. Our framework
evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten
datasets from four domains, under diverse text-based, structure-based, and
hybrid perturbations in both poisoning and evasion scenarios. Our extensive
analysis reveals multiple findings, among which three are particularly
noteworthy: 1) models have inherent robustness trade-offs between text and
structure, 2) the performance of GNNs and RGNNs depends heavily on the text
encoder and attack type, and 3) GraphLLMs are particularly vulnerable to
training data corruption. To overcome the identified trade-offs, we introduce
SFT-auto, a novel framework that delivers superior and balanced robustness
against both textual and structural attacks within a single model. Our work
establishes a foundation for future research on TAG security and offers
practical solutions for robust TAG learning in adversarial environments. Our
code is available at: https://github.com/Leirunlin/TGRB.

</details>


### [305] [A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling](https://arxiv.org/abs/2510.17187)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Sanya Murdeshwar,Kevin Bachelor,Ionut Mistreanu,Ashwin Lokapally,Razvan Marinescu*

Main category: cs.LG

TL;DR: 本文引入模块化基准测试框架评估蛋白质分子动力学方法，提供数据集，通过测试展示框架实用性，为分子模拟社区基准测试奠定基础。


<details>
  <summary>Details</summary>
Motivation: 分子动力学方法发展快，但方法验证的标准化工具不足，评估指标不一致、稀有构象采样不充分和缺乏可重复基准阻碍了模拟方法的客观比较。

Method: 使用基于TICA进展坐标的加权系综采样（WESTPA），框架有灵活的传播器接口支持任意模拟引擎，提供综合评估套件，还贡献了九种不同蛋白质的数据集。

Result: 进行了验证测试，比较了经典MD模拟和不同训练程度的CGSchNet模型的蛋白质构象采样。

Conclusion: 开源平台标准化了评估协议，能实现MD方法的直接、可重复比较，为分子模拟社区一致、严格的基准测试奠定基础。

Abstract: The rapid evolution of molecular dynamics (MD) methods, including
machine-learned dynamics, has outpaced the development of standardized tools
for method validation. Objective comparison between simulation approaches is
often hindered by inconsistent evaluation metrics, insufficient sampling of
rare conformational states, and the absence of reproducible benchmarks. To
address these challenges, we introduce a modular benchmarking framework that
systematically evaluates protein MD methods using enhanced sampling analysis.
Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble
Simulation Toolkit with Parallelization and Analysis (WESTPA), based on
progress coordinates derived from Time-lagged Independent Component Analysis
(TICA), enabling fast and efficient exploration of protein conformational
space. The framework includes a flexible, lightweight propagator interface that
supports arbitrary simulation engines, allowing both classical force fields and
machine learning-based models. Additionally, the framework offers a
comprehensive evaluation suite capable of computing more than 19 different
metrics and visualizations across a variety of domains. We further contribute a
dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a
variety of folding complexities and topologies. Each protein has been
extensively simulated at 300K for one million MD steps per starting point (4
ns). To demonstrate the utility of our framework, we perform validation tests
using classic MD simulations with implicit solvent and compare protein
conformational sampling using a fully trained versus under-trained CGSchNet
model. By standardizing evaluation protocols and enabling direct, reproducible
comparisons across MD approaches, our open-source platform lays the groundwork
for consistent, rigorous benchmarking across the molecular simulation
community.

</details>


### [306] [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](https://arxiv.org/abs/2510.17189)
*Wenxun Wang,Shuchang Zhou,Wenyu Sun,Peiqin Sun,Yongpan Liu*

Main category: cs.LG

TL;DR: 本文提出SOLE，用于解决Transformers中Softmax和LayerNorm效率问题，实验显示无需重新训练就能维持精度，且速度和能效有显著提升。


<details>
  <summary>Details</summary>
Motivation: Transformers中Softmax和LayerNorm效率低，之前基于函数近似的方法实现效率不高且需重新训练补偿误差。

Method: 提出SOLE，包含E2Softmax和AILayerNorm，E2Softmax用log2量化指数函数和基于对数的除法近似Softmax，AILayerNorm采用低精度统计计算。

Result: SOLE无需重新训练就能维持推理精度，相比GPU有数量级的速度提升和节能效果，在能效和面积效率上比现有定制硬件有明显提升。

Conclusion: SOLE是一种有效的硬件 - 软件协同设计方案，能提高Softmax和LayerNorm的效率。

Abstract: Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

</details>


### [307] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 本文提出软掩码（SM）方法改进基于掩码扩散的语言模型，在多个任务上提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码扩散的语言模型在解码时的二元选择会丢弃有价值的预测信息，需改进。

Method: 引入软掩码方法，动态融合掩码标记和前一步top-k预测标记的嵌入；提出训练方法让预训练掩码扩散语言模型融入SM。

Result: 对169M参数模型继续预训练使用SM提升困惑度和MAUVE分数；用SM微调两个先进扩散模型，在多个编码基准测试中提升性能。

Conclusion: 软掩码方法能为基于掩码扩散的语言模型提供更多信息，有效提升模型性能。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [308] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 现有强化学习方法在高风险高回报任务中因假设单峰高斯策略和依赖标量值评论家而受限，本文提出新框架解决问题，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大多数强化学习方法在高风险高回报任务中，因假设单峰高斯策略和依赖标量值评论家，效果受限，需解决该问题。

Method: 提出的强化学习框架包括离散化连续动作空间近似多峰分布、采用熵正则化探索和引入双评论家架构进行离散值分布估计。

Result: 在运动和操作基准测试上的实验显示，该方法优于基线方法。

Conclusion: 在强化学习中显式建模多峰性和风险很重要。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [309] [Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network](https://arxiv.org/abs/2510.17214)
*Chenyan Fei,Dalin Zhang,Chen Melinda Dang*

Main category: cs.LG

TL;DR: 本文采用深度稀疏自编码网络预测和分类燃料电池高频阻抗，准确率超92%，部署在FPGA上硬件识别率近90%。


<details>
  <summary>Details</summary>
Motivation: 燃料电池健康状态有效准确诊断对确保电池堆稳定运行至关重要，高频阻抗是评估关键指标，但在线测试复杂且成本高。

Method: 采用深度稀疏自编码网络进行预测和分类，并将网络部署在FPGA上。

Result: 网络预测和分类高频阻抗准确率达92%以上，部署在FPGA上硬件识别率近90%。

Conclusion: 深度稀疏自编码网络可有效用于燃料电池高频阻抗预测和分类，在FPGA上部署也有较好效果。

Abstract: Effective and accurate diagnosis of fuel cell health status is crucial for
ensuring the stable operation of fuel cell stacks. Among various parameters,
high-frequency impedance serves as a critical indicator for assessing fuel cell
state and health conditions. However, its online testing is prohibitively
complex and costly. This paper employs a deep sparse auto-encoding network for
the prediction and classification of high-frequency impedance in fuel cells,
achieving metric of accuracy rate above 92\%. The network is further deployed
on an FPGA, attaining a hardware-based recognition rate almost 90\%.

</details>


### [310] [A Prototypical Network with an Attention-based Encoder for Drivers Identification Application](https://arxiv.org/abs/2510.17250)
*Wei-Hsun Lee,Che-Yu Chang,Kuang-Yu Li*

Main category: cs.LG

TL;DR: 本文提出基于注意力机制的编码器AttEnc和结合原型网络的P - AttEnc用于驾驶员识别，解决数据短缺和未知驾驶员分类问题，实验表明效果良好。


<details>
  <summary>Details</summary>
Motivation: 生物识别技术有隐私问题，且现有研究存在数据短缺和难以处理未知驾驶员的问题。

Method: 提出AttEnc用于驾驶员识别，结合原型网络提出P - AttEnc，应用少样本学习。

Result: AttEnc在三个数据集上识别准确率高，预测时间快，参数减少；P - AttEnc在单样本场景下识别准确率69.8%，分类未知驾驶员平均准确率65.7%。

Conclusion: AttEnc和P - AttEnc在驾驶员识别中有效，能解决数据短缺和未知驾驶员分类问题。

Abstract: Driver identification has become an area of increasing interest in recent
years, especially for data- driven applications, because biometric-based
technologies may incur privacy issues. This study proposes a deep learning
neural network architecture, an attention-based encoder (AttEnc), which uses an
attention mechanism for driver identification and uses fewer model parameters
than current methods. Most studies do not address the issue of data shortages
for driver identification, and most of them are inflexible when encountering
unknown drivers. In this study, an architecture that combines a prototypical
network and an attention-based encoder (P-AttEnc) is proposed. It applies
few-shot learning to overcome the data shortage issues and to enhance model
generalizations. The experiments showed that the attention-based encoder can
identify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different
datasets and has a prediction time that is 44% to 79% faster because it
significantly reduces, on average, 87.6% of the model parameters. P-AttEnc
identifies drivers based on few shot data, extracts driver fingerprints to
address the issue of data shortages, and is able to classify unknown drivers.
The first experiment showed that P-AttEnc can identify drivers with an accuracy
of 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,
in the 1-shot scenario, can classify unknown drivers with an average accuracy
of 65.7%.

</details>


### [311] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: 文章指出现有多智能体系统防御可被绕过，提出新防御机制ControlValve并评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于对齐检查的防御如LlamaFirewall可被绕过，多智能体系统安全与功能目标存在冲突。

Method: 借鉴控制流完整性和最小权限原则，生成多智能体系统允许的控制流图，并强制执行执行符合这些图及上下文规则。

Result: 未提及具体结果。

Conclusion: 未提及明确结论。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [312] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: 提出首个多因素序列解缠结标准化基准，含多种工具，提出后验潜探索阶段和Koopman启发模型，还展示视觉语言模型可自动标注和零样本评估，为多因素序列解缠结提供基础。


<details>
  <summary>Details</summary>
Motivation: 以往工作多关注简单双因素静态和动态设置，忽略现实数据多因素本质，需评估多因素序列解缠结。

Method: 引入标准化基准，含数据集整合、模型开发和评估指标工具；提出后验潜探索阶段；引入Koopman启发模型；利用视觉语言模型自动标注和零样本评估。

Result: Koopman启发模型取得了最先进的结果，视觉语言模型可实现自动标注和零样本评估。

Conclusion: 这些贡献为推进多因素序列解缠结提供了强大且可扩展的基础。

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

</details>


### [313] [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)
*Lipeng Xie,Sen Huang,Zhuo Zhang,Anni Zou,Yunpeng Zhai,Dingchao Ren,Kezun Zhang,Haoyuan Hu,Boyin Liu,Haoran Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.LG

TL;DR: 提出无训练框架解决奖励模型开发的局限，经实验验证其数据效率和性能佳，小模型用少量数据也能表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型开发受昂贵偏好数据集和可解释性差的阻碍，基于规则的方法缺乏系统质量控制和优化。

Method: 采用两阶段方法，先通过验证引导的Propose - Evaluate - Revise管道推断特定查询的高质量规则，再将这些规则泛化为紧凑、非冗余的核心集。

Result: 框架数据效率和性能出色，用70对偏好数据（源数据的1.5%）能让Qwen3 - 8B等小模型超越完全训练的专业模型。

Conclusion: 该工作为奖励建模开辟了可扩展、可解释且数据高效的新路径。

Abstract: Reward models are essential for aligning Large Language Models (LLMs) with
human values, yet their development is hampered by costly preference datasets
and poor interpretability. While recent rubric-based approaches offer
transparency, they often lack systematic quality control and optimization,
creating a trade-off between scalability and reliability. We address these
limitations with a novel, training-free framework built on a key assumption:
\textit{evaluation rubrics underlying human preferences exhibit significant
generalization ability across diverse queries}, a property that enables
remarkable data efficiency. Our two-stage approach first infers high-quality,
query-specific rubrics using a validation-guided
\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these
granular rubrics into a compact, non-redundant core set by maximizing an
\textbf{information-theoretic coding rate}. The final output is an
interpretable, hierarchical "Theme-Tips" rubric set. Extensive experiments
demonstrate the framework's exceptional data efficiency and performance.
Critically, using just 70 preference pairs (1.5\% of the source data), our
method also empowers smaller models like Qwen3-8B to outperform specialized,
fully-trained counterparts. This work pioneers a scalable, interpretable, and
data-efficient path for reward modeling.

</details>


### [314] [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)
*Joachim Diederich*

Main category: cs.LG

TL;DR: 提出训练大语言模型新框架，可调整内部表示，有多项创新及数学结果，能平衡复杂度与效率，支持特定应用。


<details>
  <summary>Details</summary>
Motivation: 开发能在局部主义（可解释、基于规则）和分布式（可泛化、高效）编码间连续调整内部表示的大语言模型训练框架。

Method: 采用局部性调节参数、信息论招募机制、分层招募框架，结合注意力机制稀疏惩罚、信息论锚设计等。

Result: 给出严格数学结果，明确注意力集中在语义相关块的阈值条件，分层招募机制有收敛保证。

Conclusion: 该框架可在可解释和高性能模式间插值，支持需透明性和能力的监管领域应用。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovations are (1) a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining, (2) an
information-theoretic recruitment mechanism that adaptively allocates semantic
blocks as needed, eliminating the requirement for complete domain knowledge at
initialization, and (3) a hierarchical recruitment framework that extends
capacity allocation to entire specialized LLMs, enabling multi-granularity
architectural adaptation. This is achieved through group sparsity penalties on
attention mechanisms, information-theoretic anchor design, dynamic rule
injection, and principled recruitment criteria based on penalized likelihood
with explicit units. We provide rigorous mathematical results establishing
explicit threshold conditions under which attention provably concentrates on
semantically relevant blocks at stationary points, with exact bounds on
attention entropy and pointer fidelity. The hierarchical recruitment mechanism
provides convergence guarantees at both the block level (fine-grained,
within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the
system discovers semantic partitions that balance model complexity against data
encoding efficiency. This framework enables practitioners to continuously
interpolate between interpretable and high-performance modes while adapting
architectural capacity at multiple granularities, supporting applications in
regulated domains requiring both transparency and capability.

</details>


### [315] [Model Metamers Reveal Invariances in Graph Neural Networks](https://arxiv.org/abs/2510.17378)
*Wei Xu,Xiaoyi Jiang,Lixiang Xu,Dechao Tang*

Main category: cs.LG

TL;DR: 引入模型“变体”生成技术研究图神经网络（GNNs）不变性，发现GNNs有高度表征不变性，改进策略未根本解决问题，还量化了变体图与原图偏差。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络与人类的不变性属性存在显著差距，需研究图神经网络（GNNs）的不变性。

Method: 引入模型“变体”生成技术，优化输入图使节点激活与参考图匹配；理论分析关注单节点局部变体维度和变体流形激活诱导的体积变化。

Result: 发现几种经典GNN架构有极端的表征不变性，修改模型架构和训练策略不能根本解决问题。

Conclusion: 量化了变体图与原图偏差，揭示当前GNNs独特的失败模式，为模型评估提供补充基准。

Abstract: In recent years, deep neural networks have been extensively employed in
perceptual systems to learn representations endowed with invariances, aiming to
emulate the invariance mechanisms observed in the human brain. However, studies
in the visual and auditory domains have confirmed that significant gaps remain
between the invariance properties of artificial neural networks and those of
humans. To investigate the invariance behavior within graph neural networks
(GNNs), we introduce a model ``metamers'' generation technique. By optimizing
input graphs such that their internal node activations match those of a
reference graph, we obtain graphs that are equivalent in the model's
representation space, yet differ significantly in both structure and node
features. Our theoretical analysis focuses on two aspects: the local metamer
dimension for a single node and the activation-induced volume change of the
metamer manifold. Utilizing this approach, we uncover extreme levels of
representational invariance across several classic GNN architectures. Although
targeted modifications to model architecture and training strategies can
partially mitigate this excessive invariance, they fail to fundamentally bridge
the gap to human-like invariance. Finally, we quantify the deviation between
metamer graphs and their original counterparts, revealing unique failure modes
of current GNNs and providing a complementary benchmark for model evaluation.

</details>


### [316] [Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks](https://arxiv.org/abs/2510.17380)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 用物理信息神经网络（PINNs）构建的替代模型解决强化学习（RL）在智能电网能量管理中样本效率低的问题。


<details>
  <summary>Details</summary>
Motivation: RL在解决智能电网最优潮流问题时需通过昂贵模拟器获取样本，存在样本效率问题。

Method: 用PINNs构建替代模型替代昂贵的智能电网模拟器，优化RL策略训练过程。

Result: 能在比原环境短得多的时间内得到收敛结果。

Conclusion: PINNs构建的替代模型可有效解决RL样本效率问题，优化智能电网能量管理的RL策略训练。

Abstract: Optimizing the energy management within a smart grids scenario presents
significant challenges, primarily due to the complexity of real-world systems
and the intricate interactions among various components. Reinforcement Learning
(RL) is gaining prominence as a solution for addressing the challenges of
Optimal Power Flow in smart grids. However, RL needs to iterate compulsively
throughout a given environment to obtain the optimal policy. This means
obtaining samples from a, most likely, costly simulator, which can lead to a
sample efficiency problem. In this work, we address this problem by
substituting costly smart grid simulators with surrogate models built using
Phisics-informed Neural Networks (PINNs), optimizing the RL policy training
process by arriving to convergent results in a fraction of the time employed by
the original environment.

</details>


### [317] [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](https://arxiv.org/abs/2510.17381)
*Achref Jaziri,Martin Rogmann,Martin Mundt,Visvanathan Ramesh*

Main category: cs.LG

TL;DR: 提出DISC方法，用扩散模型提取特征，在OOD检测和分类上有优异表现，实现更细粒度检测。


<details>
  <summary>Details</summary>
Motivation: 现代OOD检测方法将分布偏移归结为单一标量异常分数，无法区分OOD数据类型，不能对OOD数据进行恰当的上下文分析和有效利用。

Method: 引入DISC，利用扩散模型的迭代去噪过程提取多维特征向量，捕捉不同噪声水平下的统计差异。

Result: 在图像和表格基准测试中，DISC在OOD检测上达到或超越了现有最优检测器，且能对OOD类型进行分类。

Conclusion: 该工作使OOD检测从简单的二分类转变为更细粒度的检测。

Abstract: Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

</details>


### [318] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 论文探讨生成视觉模型内部表征演变，对比GANs、VAEs与扩散架构，区分两种合成概念，实验表明扩散模型挑战统一内部空间假设，主张重新理解生成AI。


<details>
  <summary>Details</summary>
Motivation: 研究生成视觉模型内部表征的演变，从GANs和VAEs到扩散架构的转变。

Method: 借鉴Beatrice Fazi的理论区分两种合成概念，通过细读模型架构和针对性实验干预层表征。

Result: 扩散模型分散表征负担，挑战统一内部空间假设。

Conclusion: 应重新理解生成AI，将其视为专门过程的涌现配置而非内容的直接合成。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [319] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: 本文提出首个用于表格预测的多步推理大语言模型TabR1，采用PRPO方法提升性能与可解释性，实验显示其在多种设置下表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统表格预测方法可解释性有限、跨表迁移能力弱，推理大语言模型潜力在表格数据中未充分发挥。

Method: 提出Permutation Relative Policy Optimization (PRPO)强化学习方法，将列排列不变性编码为结构先验，构建多个样本排列并估计优势。

Result: 在全监督微调下，TabR1性能与强基线相当；零样本设置下接近32样本设置下强基线性能；TabR1 (8B)在多任务中大幅超越更大的LLMs。

Conclusion: PRPO方法能激活大语言模型在表格预测中的推理能力，提升少样本、零样本性能和可解释性。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [320] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 本文为弱连通MDP的平均回报离线RL建立了首个样本复杂度结果，引入锚定拟合Q迭代方法并进行有限时间分析。


<details>
  <summary>Details</summary>
Motivation: 先前平均回报离线RL样本复杂度研究受关注少且依赖强假设，本文希望在更温和假设下开展研究。

Method: 引入结合标准拟合Q迭代和锚定机制的锚定拟合Q迭代方法。

Result: 表明锚定机制对平均回报设置下的有限时间分析至关重要，且能将有限时间分析扩展到单轨迹数据集情况。

Conclusion: 成功在弱连通MDP假设下为平均回报离线RL建立样本复杂度结果，锚定机制有效。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [321] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出MILES训练多模态联合融合模型，平衡各模态学习，在多任务中表现优于基线模型，强调平衡多模态学习对提升性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决多模态网络训练中模态过拟合问题，提升多模态学习性能。

Method: 提出Modality - Informed Learning ratE Scheduler (MILES)，利用模态条件利用率差异，动态调整学习率平衡各模态学习速度。

Result: 在四个多模态联合融合任务中MILES均优于七个基线模型，有效平衡模态使用，提升多模态性能和模态编码器质量。

Conclusion: 平衡多模态学习对提升模型性能有重要影响。

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [322] [S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction](https://arxiv.org/abs/2510.17406)
*Tiezhi Wang,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.LG

TL;DR: 提出S4ECG深度学习架构用于多时段心律失常分类，性能优于单时段方法，明确最佳时间依赖窗口，推动心律失常检测算法发展。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以同时捕捉心电图信号全局趋势和局部波形特征的高分辨率相互作用，需新方法桥接全局和局部信号分析。

Method: 引入S4ECG，一种利用结构化状态空间模型的深度学习架构进行多时段心律失常分类。

Result: 多时段预测在macro - AUROC上比单时段方法高1.0 - 11.6%，房颤特异性提高，分布内表现好且分布外鲁棒性增强，最佳时间依赖窗口为10 - 20分钟。

Conclusion: 该研究促使心律失常检测算法向时间感知方向转变，为心电图解读带来新可能。

Abstract: The electrocardiogram (ECG) exemplifies biosignal-based time series with
continuous, temporally ordered structure reflecting cardiac physiological and
pathophysiological dynamics. Detailed analysis of these dynamics has proven
challenging, as conventional methods capture either global trends or local
waveform features but rarely their simultaneous interplay at high temporal
resolution. To bridge global and local signal analysis, we introduce S4ECG, a
novel deep learning architecture leveraging structured state space models for
multi-epoch arrhythmia classification. Our joint multi-epoch predictions
significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,
with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,
demonstrating superior performance in-distribution and enhanced
out-of-distribution robustness. Systematic investigation reveals optimal
temporal dependency windows spanning 10-20 minutes for peak performance. This
work contributes to a paradigm shift toward temporally-aware arrhythmia
detection algorithms, opening new possibilities for ECG interpretation, in
particular for complex arrhythmias like atrial fibrillation and atrial flutter.

</details>


### [323] [A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation](https://arxiv.org/abs/2510.17414)
*Hequn Li,Zhongwei Deng,Chunlin Jiang,Yvxin He andZhansheng Ning*

Main category: cs.LG

TL;DR: 本文提出Condition Diffusion U - Net with Attention (CDUA)方法预测锂离子电池容量及不确定性，经实验验证效果好且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测锂离子电池容量及其不确定性对可靠电池管理至关重要，但因老化随机性具挑战性。

Method: 提出CDUA方法，结合特征工程和深度学习，用扩散生成模型进行时间序列预测，引入注意力机制，先从真实车辆运行数据获取电池容量，用相关系数和算法选特征，训练含上下文U - Net和去噪网络的CDUA模型。

Result: 在真实车辆数据上实验，CDUA模型相对平均绝对误差0.94%，相对均方根误差1.14%，95%置信区间相对宽度3.74%。

Conclusion: CDUA能准确估计电池容量并可靠量化不确定性，比现有主流方法更稳健、性能更优。

Abstract: Accurate prediction of lithium-ion battery capacity and its associated
uncertainty is essential for reliable battery management but remains
challenging due to the stochastic nature of aging. This paper presents a novel
method, termed the Condition Diffusion U-Net with Attention (CDUA), which
integrates feature engineering and deep learning to address this challenge. The
proposed approach employs a diffusion-based generative model for time-series
forecasting and incorporates attention mechanisms to enhance predictive
performance. Battery capacity is first derived from real-world vehicle
operation data. The most relevant features are then identified using the
Pearson correlation coefficient and the XGBoost algorithm. These features are
used to train the CDUA model, which comprises two core components: (1) a
contextual U-Net with self-attention to capture complex temporal dependencies,
and (2) a denoising network to reconstruct accurate capacity values from noisy
observations. Experimental validation on the real-world vehicle data
demonstrates that the proposed CDUA model achieves a relative Mean Absolute
Error (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,
with a narrow 95% confidence interval of 3.74% in relative width. These results
confirm that CDUA provides both accurate capacity estimation and reliable
uncertainty quantification. Comparative experiments further verify its
robustness and superior performance over existing mainstream approaches.

</details>


### [324] [Diffusion Models as Dataset Distillation Priors](https://arxiv.org/abs/2510.17421)
*Duo Su,Huyu Wu,Huanran Chen,Yiming Shi,Yuzhu Wang,Xi Ye,Jun Zhu*

Main category: cs.LG

TL;DR: 提出DAP方法解决数据集蒸馏中多样性、泛化性和代表性问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成式数据集蒸馏方法忽略扩散模型固有代表性先验，需外部约束提升数据质量。

Method: 提出Diffusion As Priors (DAP)，用Mercer核量化特征空间中合成与真实数据相似度来形式化代表性，并将此先验引入反向扩散过程。

Result: 在ImageNet - 1K等大规模数据集上实验表明，DAP生成高保真数据集表现优于现有方法，且跨架构泛化性更好。

Conclusion: 建立了扩散先验与数据集蒸馏目标的理论联系，提供了无训练框架提升蒸馏数据集质量。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets from
large ones. A significant challenge in this field is achieving a trifecta of
diversity, generalization, and representativeness in a single distilled
dataset. Although recent generative dataset distillation methods adopt powerful
diffusion models as their foundation models, the inherent representativeness
prior in diffusion models is overlooked. Consequently, these approaches often
necessitate the integration of external constraints to enhance data quality. To
address this, we propose Diffusion As Priors (DAP), which formalizes
representativeness by quantifying the similarity between synthetic and real
data in feature space using a Mercer kernel. We then introduce this prior as
guidance to steer the reverse diffusion process, enhancing the
representativeness of distilled samples without any retraining. Extensive
experiments on large-scale datasets, such as ImageNet-1K and its subsets,
demonstrate that DAP outperforms state-of-the-art methods in generating
high-fidelity datasets while achieving superior cross-architecture
generalization. Our work not only establishes a theoretical connection between
diffusion priors and the objectives of dataset distillation but also provides a
practical, training-free framework for improving the quality of the distilled
dataset.

</details>


### [325] [Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models](https://arxiv.org/abs/2510.17457)
*Li Sun,Zhenhao Huang,Ming Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文指出MPNNs存在过平滑和过压缩问题，现有全局方法效果不佳，提出基于局部结构的GBN网络解决问题，实验证明其有效性和深层稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有解决MPNNs过平滑和过压缩问题的全局方法效果不理想，需要更好的解决方案。

Method: 通过全局度量谱间隙λ重新审视过压缩问题，将局部黎曼几何与MPNNs结合，建立非齐次边界条件，设计带局部瓶颈调整的GBN网络。

Result: 在同构和异构图上的大量实验表明GBN具有良好的表达能力，且网络深度超过256层时性能不下降。

Conclusion: 提出的局部方法GBN能有效解决MPNNs的过平滑和过压缩问题，具有良好的性能和稳定性。

Abstract: Message Passing Neural Networks (MPNNs) is the building block of graph
foundation models, but fundamentally suffer from oversmoothing and
oversquashing. There has recently been a surge of interest in fixing both
issues. Existing efforts primarily adopt global approaches, which may be
beneficial in some regions but detrimental in others, ultimately leading to the
suboptimal expressiveness. In this paper, we begin by revisiting oversquashing
through a global measure -- spectral gap $\lambda$ -- and prove that the
increase of $\lambda$ leads to gradient vanishing with respect to the input
features, thereby undermining the effectiveness of message passing. Motivated
by such theoretical insights, we propose a \textbf{local} approach that
adaptively adjusts message passing based on local structures. To achieve this,
we connect local Riemannian geometry with MPNNs, and establish a novel
nonhomogeneous boundary condition to address both oversquashing and
oversmoothing. Building on the Robin condition, we design a GBN network with
local bottleneck adjustment, coupled with theoretical guarantees. Extensive
experiments on homophilic and heterophilic graphs show the expressiveness of
GBN. Furthermore, GBN does not exhibit performance degradation even when the
network depth exceeds $256$ layers.

</details>


### [326] [Explainable AI for microseismic event detection](https://arxiv.org/abs/2510.17458)
*Ayrat Abdullin,Denis Anikiev,Umair bin Waheed*

Main category: cs.LG

TL;DR: 本文应用可解释AI技术解读PhaseNet模型决策，引入SHAP - gated推理方案提升性能，证明XAI可增强深度学习模型性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络如PhaseNet在微震事件检测中准确但具有黑盒特性，在关键应用中令人担忧，需提升其可靠性。

Method: 应用Grad - CAM和SHAP等可解释AI技术解读模型决策，引入SHAP - gated推理方案结合模型输出与基于解释的指标。

Result: 在9000个波形测试集上，SHAP - gated模型F1分数达0.98，优于基线PhaseNet，对噪声鲁棒性增强。

Conclusion: XAI不仅能解释深度学习模型，还能直接提升其性能，为构建自动地震探测器的信任提供模板。

Abstract: Deep neural networks like PhaseNet show high accuracy in detecting
microseismic events, but their black-box nature is a concern in critical
applications. We apply explainable AI (XAI) techniques, such as
Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive
Explanations (SHAP), to interpret the PhaseNet model's decisions and improve
its reliability. Grad-CAM highlights that the network's attention aligns with
P- and S-wave arrivals. SHAP values quantify feature contributions, confirming
that vertical-component amplitudes drive P-phase picks while horizontal
components dominate S-phase picks, consistent with geophysical principles.
Leveraging these insights, we introduce a SHAP-gated inference scheme that
combines the model's output with an explanation-based metric to reduce errors.
On a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of
0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet
(F1-score 0.97) and demonstrating enhanced robustness to noise. These results
show that XAI can not only interpret deep learning models but also directly
enhance their performance, providing a template for building trust in automated
seismic detectors.

</details>


### [327] [CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics](https://arxiv.org/abs/2510.17467)
*Dan Zheng,Jing Feng,Juan Liu*

Main category: cs.LG

TL;DR: 提出CrossStateECG模型用于跨状态（静息 - 运动）心电图生物识别，实验验证其有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有心电图生物识别研究多关注静息状态，未解决静息 - 运动场景下性能下降问题。

Method: 结合多尺度深度卷积特征提取与注意力机制构建CrossStateECG模型。

Result: 在exercise - ECGID数据集上不同场景准确率高，在ECG - ID和MIT - BIH数据集上验证泛化能力。

Conclusion: CrossStateECG有潜力成为动态现实场景中运动后心电图认证的实用解决方案。

Abstract: Current research in Electrocardiogram (ECG) biometrics mainly emphasizes
resting-state conditions, leaving the performance decline in rest-exercise
scenarios largely unresolved. This paper introduces CrossStateECG, a robust
ECG-based authentication model explicitly tailored for cross-state
(rest-exercise) conditions. The proposed model creatively combines multi-scale
deep convolutional feature extraction with attention mechanisms to ensure
strong identification across different physiological states. Experimental
results on the exercise-ECGID dataset validate the effectiveness of
CrossStateECG, achieving an identification accuracy of 92.50% in the
Rest-to-Exercise scenario (training on resting ECG and testing on post-exercise
ECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG
and testing on resting ECG). Furthermore, CrossStateECG demonstrates
exceptional performance across both state combinations, reaching an accuracy of
99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.
Additional validations on the ECG-ID and MIT-BIH datasets further confirmed the
generalization abilities of CrossStateECG, underscoring its potential as a
practical solution for post-exercise ECG-based authentication in dynamic
real-world settings.

</details>


### [328] [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
*Jing Liu*

Main category: cs.LG

TL;DR: 本文用随机层次模型研究Transformer在未观察序列上的组合推理能力，分析不同泛化条件下表现及机制，发现其发展出支持组合推理的模块化、可解释机制。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer在未观察序列上的组合推理能力及现象背后机制。

Method: 使用随机层次模型（RHM），对模型在子集序列上训练，在四种泛化条件下评估，还进行主成分分析和注意力模式聚类。

Result: 行为上，性能随任务复杂度和上下文示例数量提升，分布外任务需更多示例；机制上，训练中出现层专业化，与泛化性能相关，Transformer在专门层发展出结构化、分层组织的表示。

Conclusion: Transformer发展出支持组合推理的模块化、可解释机制，将内部算法结构与观察到的行为能力相联系。

Abstract: Transformers exhibit compositional reasoning on sequences not observed during
training, a capability often attributed to in-context learning (ICL) and skill
composition. We investigate this phenomenon using the Random Hierarchy Model
(RHM), a probabilistic context-free grammar that generates sequences through
recursive rule application. Models are trained on subsets of sequences and
evaluated across four generalization conditions: memorization, in-distribution
generalization, out-of-distribution generalization with the same rules, and
cross-layer transfer. Behaviorally, performance improves systematically with
task complexity and the number of in-context examples, with out-of-distribution
tasks requiring substantially more examples than in-distribution scenarios.
Mechanistically, we identify a progressive emergence of layer specialization
during training that correlates with generalization performance. Principal
component analysis and attention pattern clustering reveal that transformers
develop structured, hierarchically organized representations in specialized
layers. These results demonstrate that transformers develop modular,
interpretable mechanisms supporting compositional reasoning, linking internal
algorithmic structure to observed behavioral capabilities.

</details>


### [329] [DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition](https://arxiv.org/abs/2510.17475)
*Fo Hu,Can Wang,Qinxu Zheng,Xusheng Yang,Bin Zhou,Gang Li,Yu Sun,Wen-an Zhang*

Main category: cs.LG

TL;DR: 文章提出DAMSDAN解决多源适应的挑战，在多个数据集实验证明对跨域脑电情感识别有效。


<details>
  <summary>Details</summary>
Motivation: 个体间差异限制跨域脑电情感识别泛化性，解决多源适应的两个核心挑战。

Method: 提出DAMSDAN，集成基于原型的约束和对抗学习；采用基于MMD的源加权策略；使用带双伪标签交互的原型引导条件对齐模块。

Result: 在SEED和SEED - IV上跨主体和跨会话有高准确率，在FACED数据集跨主体达82.88%。

Conclusion: 所提框架对跨域脑电情感识别有效。

Abstract: Significant inter-individual variability limits the generalization of
EEG-based emotion recognition under cross-domain settings. We address two core
challenges in multi-source adaptation: (1) dynamically modeling distributional
heterogeneity across sources and quantifying their relevance to a target to
reduce negative transfer; and (2) achieving fine-grained semantic consistency
to strengthen class discrimination. We propose a distribution-aware
multi-source domain adaptation network (DAMSDAN). DAMSDAN integrates
prototype-based constraints with adversarial learning to drive the encoder
toward discriminative, domain-invariant emotion representations. A domain-aware
source weighting strategy based on maximum mean discrepancy (MMD) dynamically
estimates inter-domain shifts and reweights source contributions. In addition,
a prototype-guided conditional alignment module with dual pseudo-label
interaction enhances pseudo-label reliability and enables category-level,
fine-grained alignment, mitigating noise propagation and semantic drift.
Experiments on SEED and SEED-IV show average accuracies of 94.86\% and 79.78\%
for cross-subject, and 95.12\% and 83.15\% for cross-session protocols. On the
large-scale FACED dataset, DAMSDAN achieves 82.88\% (cross-subject). Extensive
ablations and interpretability analyses corroborate the effectiveness of the
proposed framework for cross-domain EEG-based emotion recognition.

</details>


### [330] [Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement](https://arxiv.org/abs/2510.17478)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: 本文探讨将训练生成河流沉积物的GAN模型反演以匹配井和地震数据，指出关键瓶颈在于GAN的潜在表示纠缠，提出微调GAN等方法，认为GAN已可融入地质建模工作流，但需评估鲁棒性等。


<details>
  <summary>Details</summary>
Motivation: 地下决策因高成本和不确定性具有挑战性，获取新数据难以扩展，将地质知识嵌入预测模型是有价值的替代方案，研究GAN模型反演匹配数据的可行性。

Method: 对三个分别有4、8和20口井的测试样本应用四种反演方法，提出标签条件化、潜在过参数化和微调GAN等方法。

Result: 四种反演方法匹配井数据较困难，尤其是井数量增加或测试样本与训练数据差异大时；标签条件化和潜在过参数化可部分解开潜在空间但不足以成功反演；微调GAN可将不匹配度降低到可接受水平，但依赖初始部分成功的反演步骤。

Conclusion: GAN可处理融入地质建模工作流所需任务，但需进一步评估其鲁棒性以及如何更好地用于支持地质解释。

Abstract: High costs and uncertainties make subsurface decision-making challenging, as
acquiring new data is rarely scalable. Embedding geological knowledge directly
into predictive models offers a valuable alternative. A joint approach enables
just that: process-based models that mimic geological processes can help train
generative models that make predictions more efficiently. This study explores
whether a generative adversarial network (GAN) - a type of deep-learning
algorithm for generative modeling - trained to produce fluvial deposits can be
inverted to match well and seismic data. Four inversion approaches applied to
three test samples with 4, 8, and 20 wells struggled to match these well data,
especially as the well number increased or as the test sample diverged from the
training data. The key bottleneck lies in the GAN's latent representation: it
is entangled, so samples with similar sedimentological features are not
necessarily close in the latent space. Label conditioning or latent
overparameterization can partially disentangle the latent space during
training, although not yet sufficiently for a successful inversion. Fine-tuning
the GAN to restructure the latent space locally reduces mismatches to
acceptable levels for all test cases, with and without seismic data. But this
approach depends on an initial, partially successful inversion step, which
influences the quality and diversity of the final samples. Overall, GANs can
already handle the tasks required for their integration into geomodeling
workflows. We still need to further assess their robustness, and how to best
leverage them in support of geological interpretation.

</details>


### [331] [Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization](https://arxiv.org/abs/2510.17480)
*Aurélien Bellet,Edwige Cyffers,Davide Frey,Romaric Gaudel,Dimitri Lerévérend,François Taïani*

Main category: cs.LG

TL;DR: 本文展示了基于矩阵分解的集中式差分隐私核算进展可用于去中心化学习（DL），泛化现有结果得到统一公式，实现更严格隐私核算，还引入MAFALDA - SGD算法。


<details>
  <summary>Details</summary>
Motivation: 实际中DL的隐私 - 效用权衡比集中式训练差，可能是当前DL的差分隐私核算方法存在局限。

Method: 泛化基于矩阵分解（MF）的集中式DP核算结果，将标准DL算法和常见信任模型转化为统一公式。

Result: 实现了对现有DP - DL算法更严格的隐私核算，引入的MAFALDA - SGD算法在合成和真实世界图上表现优于现有方法。

Conclusion: 基于MF的集中式DP核算进展可用于DL，能得到更严格隐私核算并开发新算法。

Abstract: Decentralized Learning (DL) enables users to collaboratively train models
without sharing raw data by iteratively averaging local updates with neighbors
in a network graph. This setting is increasingly popular for its scalability
and its ability to keep data local under user control. Strong privacy
guarantees in DL are typically achieved through Differential Privacy (DP), with
results showing that DL can even amplify privacy by disseminating noise across
peer-to-peer communications. Yet in practice, the observed privacy-utility
trade-off often appears worse than in centralized training, which may be due to
limitations in current DP accounting methods for DL. In this paper, we show
that recent advances in centralized DP accounting based on Matrix Factorization
(MF) for analyzing temporal noise correlations can also be leveraged in DL. By
generalizing existing MF results, we show how to cast both standard DL
algorithms and common trust models into a unified formulation. This yields
tighter privacy accounting for existing DP-DL algorithms and provides a
principled way to develop new ones. To demonstrate the approach, we introduce
MAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that
outperforms existing methods on synthetic and real-world graphs.

</details>


### [332] [Local properties of neural networks through the lens of layer-wise Hessians](https://arxiv.org/abs/2510.17486)
*Maxim Bolshim,Alexander Kugaevskikh*

Main category: cs.LG

TL;DR: 提出通过层间Hessian矩阵分析神经网络的方法，经实验发现训练中局部Hessian矩阵演变规律及与泛化性能的关联，为诊断和设计网络奠基。


<details>
  <summary>Details</summary>
Motivation: 寻找分析神经网络的有效方法，刻画参数空间局部几何特征，诊断网络存在的过拟合、参数不足等问题。

Method: 定义每个功能块（层）的局部Hessian矩阵，以其作为分析工具，开展涉及37个数据集的111项实验。

Result: 训练中局部Hessian矩阵有一致的结构规律，其频谱与泛化性能存在关联。

Conclusion: 该框架连接优化几何与功能行为，为改进网络架构和训练稳定性提供实用见解。

Abstract: We introduce a methodology for analyzing neural networks through the lens of
layer-wise Hessian matrices. The local Hessian of each functional block (layer)
is defined as the matrix of second derivatives of a scalar function with
respect to the parameters of that layer. This concept provides a formal tool
for characterizing the local geometry of the parameter space. We show that the
spectral properties of local Hessians, such as the distribution of eigenvalues,
reveal quantitative patterns associated with overfitting,
underparameterization, and expressivity in neural network architectures. We
conduct an extensive empirical study involving 111 experiments across 37
datasets. The results demonstrate consistent structural regularities in the
evolution of local Hessians during training and highlight correlations between
their spectra and generalization performance. These findings establish a
foundation for using local geometric analysis to guide the diagnosis and design
of deep neural networks. The proposed framework connects optimization geometry
with functional behavior and offers practical insight for improving network
architectures and training stability.

</details>


### [333] [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](https://arxiv.org/abs/2510.17496)
*Giacomo Camposampiero,Michael Hersche,Roger Wattenhofer,Abu Sebastian,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出I - RAVEN - X基准评估大模型推理能力，对比LLMs和LRMs表现，指出LRMs存在的问题。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型和大推理模型在类比和数学推理中的泛化性和鲁棒性。

Method: 引入I - RAVEN - X基准，该基准通过增加操作数复杂度、属性范围和引入感知不确定性扩展I - RAVEN。

Result: 与LLMs相比，LRMs在更长推理关系和更广泛属性范围上分别实现了更高的生产率和系统性，但在不确定性推理方面仍面临挑战，无法有效探索多种概率结果。

Conclusion: LRMs在不确定性推理方面有明显不足，有待进一步提升。

Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate
generalization and robustness in analogical and mathematical reasoning for
Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X
extends I-RAVEN by increasing operand complexity, attribute range, and
introducing perceptual uncertainty. Compared to LLMs, empirical results show
that LRMs achieve improved productivity and systematicity on longer reasoning
relations and wider attribute ranges, respectively. However, LRMs are still
significantly challenged by reasoning under uncertainty and cannot effectively
explore multiple probabilistic outcomes.

</details>


### [334] [Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares](https://arxiv.org/abs/2510.17506)
*Lachlan Ewen MacDonald,Hancheng Min,Leandro Palma,Salma Tarmoun,Ziqing Xu,René Vidal*

Main category: cs.LG

TL;DR: 本文在过参数化最小二乘设置下，为大学习率的梯度下降提供收敛率，分析了三种学习率制度下的收敛情况。


<details>
  <summary>Details</summary>
Motivation: 经典优化理论在小步长下保证梯度下降目标单调递减，而神经网络常使用大步长，本文旨在量化这种大步长下目标非单调递减及隐式偏向平坦极小值的现象。

Method: 利用过参数化使全局极小值集形成黎曼流形，将梯度下降动力学分解为平行和正交分量进行分析。

Result: 得出三种学习率制度下的收敛率：亚临界制度下有限时间克服瞬态不稳定性后线性收敛到次优平坦全局最小值；临界制度下持续不稳定但幂律收敛到最优平坦全局最小值；超临界制度下持续不稳定并线性收敛到以最优平坦全局最小值为中心的周期为二的轨道。

Conclusion: 通过对过参数化最小二乘设置下大学习率梯度下降的分析，量化了相关现象并给出不同学习率制度下的收敛情况。

Abstract: Classical optimisation theory guarantees monotonic objective decrease for
gradient descent (GD) when employed in a small step size, or ``stable", regime.
In contrast, gradient descent on neural networks is frequently performed in a
large step size regime called the ``edge of stability", in which the objective
decreases non-monotonically with an observed implicit bias towards flat minima.
In this paper, we take a step toward quantifying this phenomenon by providing
convergence rates for gradient descent with large learning rates in an
overparametrised least squares setting. The key insight behind our analysis is
that, as a consequence of overparametrisation, the set of global minimisers
forms a Riemannian manifold $M$, which enables the decomposition of the GD
dynamics into components parallel and orthogonal to $M$. The parallel component
corresponds to Riemannian gradient descent on the objective sharpness, while
the orthogonal component is a bifurcating dynamical system. This insight allows
us to derive convergence rates in three regimes characterised by the learning
rate size: (a) the subcritical regime, in which transient instability is
overcome in finite time before linear convergence to a suboptimally flat global
minimum; (b) the critical regime, in which instability persists for all time
with a power-law convergence toward the optimally flat global minimum; and (c)
the supercritical regime, in which instability persists for all time with
linear convergence to an orbit of period two centred on the optimally flat
global minimum.

</details>


### [335] [The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis](https://arxiv.org/abs/2510.17515)
*Hoang Pham,The-Anh Ta,Tom Jacobs,Rebekka Burkholz,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出基于图极限理论的框架研究稀疏神经网络，用图神经网络切线核分析训练动态，解释不同剪枝方法收敛行为差异。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏神经网络有效训练难题，理解相同稀疏度下不同稀疏结构可训练性差异。

Method: 提出基于图极限理论（图子）的框架，提出图子极限假设并实证；推导图子神经切线核研究无限宽度极限下稀疏网络训练动态。

Result: 图子神经切线核谱分析与稀疏网络训练动态相关，能解释不同剪枝方法收敛行为。

Conclusion: 框架为不同稀疏网络架构可训练性受连接模式影响提供理论见解。

Abstract: Sparse neural networks promise efficiency, yet training them effectively
remains a fundamental challenge. Despite advances in pruning methods that
create sparse architectures, understanding why some sparse structures are
better trainable than others with the same level of sparsity remains poorly
understood. Aiming to develop a systematic approach to this fundamental
problem, we propose a novel theoretical framework based on the theory of graph
limits, particularly graphons, that characterizes sparse neural networks in the
infinite-width regime. Our key insight is that connectivity patterns of sparse
neural networks induced by pruning methods converge to specific graphons as
networks' width tends to infinity, which encodes implicit structural biases of
different pruning methods. We postulate the Graphon Limit Hypothesis and
provide empirical evidence to support it. Leveraging this graphon
representation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to
study the training dynamics of sparse networks in the infinite width limit.
Graphon NTK provides a general framework for the theoretical analysis of sparse
networks. We empirically show that the spectral analysis of Graphon NTK
correlates with observed training dynamics of sparse networks, explaining the
varying convergence behaviours of different pruning methods. Our framework
provides theoretical insights into the impact of connectivity patterns on the
trainability of various sparse network architectures.

</details>


### [336] [SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers](https://arxiv.org/abs/2510.17517)
*Hangcheng Cao,Baixiang Huang,Longzhi Yuan,Haonan An,Zihan Fang,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: 文章研究帕金森病患者驾驶行为，提出SAFE - D框架检测相关行为异常，在平台和模拟器验证，准确率达96.8%。


<details>
  <summary>Details</summary>
Motivation: 以往研究对病理引发的驾驶行为偏差关注不足，尤其是慢性疾病，本文旨在填补帕金森病患者驾驶行为异常检测的研究空白。

Method: 分析帕金森病症状，建立与驾驶性能下降的因果联系；整合多车辆控制组件数据构建行为特征；设计基于注意力的网络进行异常检测；在Logitech G29平台和CARLA模拟器验证。

Result: SAFE - D在区分正常和受帕金森病影响的驾驶模式上平均准确率达96.8%。

Conclusion: SAFE - D框架能有效检测帕金森相关的驾驶行为异常，可提升驾驶安全性。

Abstract: A driver's health state serves as a determinant factor in driving behavioral
regulation. Subtle deviations from normalcy can lead to operational anomalies,
posing risks to public transportation safety. While prior efforts have
developed detection mechanisms for functionally-driven temporary anomalies such
as drowsiness and distraction, limited research has addressed
pathologically-triggered deviations, especially those stemming from chronic
medical conditions. To bridge this gap, we investigate the driving behavior of
Parkinson's disease patients and propose SAFE-D, a novel framework for
detecting Parkinson-related behavioral anomalies to enhance driving safety. Our
methodology starts by performing analysis of Parkinson's disease
symptomatology, focusing on primary motor impairments, and establishes causal
links to degraded driving performance. To represent the subclinical behavioral
variations of early-stage Parkinson's disease, our framework integrates data
from multiple vehicle control components to build a behavioral profile. We then
design an attention-based network that adaptively prioritizes spatiotemporal
features, enabling robust anomaly detection under physiological variability.
Finally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,
using data from three road maps to emulate real-world driving. Our results show
SAFE-D achieves 96.8% average accuracy in distinguishing normal and
Parkinson-affected driving patterns.

</details>


### [337] [Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning](https://arxiv.org/abs/2510.17520)
*Canran Xiao,Chuangxin Zhao,Zong Ke,Fei Shen*

Main category: cs.LG

TL;DR: 针对多标签学习中的长尾不平衡问题，提出CD - GTMLL框架，通过合作潜力游戏解决，实验取得SOTA效果，提供长尾鲁棒性预测途径。


<details>
  <summary>Details</summary>
Motivation: 解决多标签学习中长尾不平衡问题，即少数头部标签主导梯度信号，大量稀有标签被忽略的问题。

Method: 将任务建模为合作潜力游戏，在CD - GTMLL框架中，将标签空间分配给多个合作玩家，玩家共享全局准确率回报，并根据标签稀有性和玩家间分歧获得好奇心奖励，且无需手动调整类别权重。

Result: 在常规基准和三个超大规模数据集上实验取得一致的SOTA提升，Rare - F1最高提升4.3%，P@3最高提升1.6%，消融实验揭示了劳动分工和对稀有类别的快速共识。

Conclusion: CD - GTMLL为多标签预测中的长尾鲁棒性提供了一种有原则、可扩展的途径。

Abstract: Long-tail imbalance is endemic to multi-label learning: a few head labels
dominate the gradient signal, while the many rare labels that matter in
practice are silently ignored. We tackle this problem by casting the task as a
cooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label
Learning (CD-GTMLL) framework, the label space is split among several
cooperating players that share a global accuracy payoff yet earn additional
curiosity rewards that rise with label rarity and inter-player disagreement.
These curiosity bonuses inject gradient on under-represented tags without
hand-tuned class weights. We prove that gradient best-response updates ascend a
differentiable potential and converge to tail-aware stationary points that
tighten a lower bound on the expected Rare-F1. Extensive experiments on
conventional benchmarks and three extreme-scale datasets show consistent
state-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the
strongest baselines, while ablations reveal emergent division of labour and
faster consensus on rare classes. CD-GTMLL thus offers a principled, scalable
route to long-tail robustness in multi-label prediction.

</details>


### [338] [Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples](https://arxiv.org/abs/2510.17524)
*Sidney Bender,Ole Delzer,Jan Herrmann,Heike Antje Marxfeld,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 提出反事实知识蒸馏（CFKD）框架解决深度学习模型对虚假相关性敏感问题，在多数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于组分布鲁棒性方法依赖显式组标签，存在组标签缺失、组内样本少、多虚假相关性使数据分组过小等局限性。

Method: 提出CFKD框架，生成多样化反事实，通过知识蒸馏步骤让人类注释者探索和纠正模型决策边界，不依赖混杂因素标签。

Result: 在五个数据集上证明CFKD有效，在低数据且有明显虚假相关性场景增益显著，还对反事实解释器和教师模型做了消融研究。

Conclusion: CFKD能避开现有方法问题，不依赖标签，可有效扩展到多混杂因素，实现跨组的平衡泛化。

Abstract: Deep learning models remain vulnerable to spurious correlations, leading to
so-called Clever Hans predictors that undermine robustness even in large-scale
foundation and self-supervised models. Group distributional robustness methods,
such as Deep Feature Reweighting (DFR) rely on explicit group labels to
upweight underrepresented subgroups, but face key limitations: (1) group labels
are often unavailable, (2) low within-group sample sizes hinder coverage of the
subgroup distribution, and (3) performance degrades sharply when multiple
spurious correlations fragment the data into even smaller groups. We propose
Counterfactual Knowledge Distillation (CFKD), a framework that sidesteps these
issues by generating diverse counterfactuals, enabling a human annotator to
efficiently explore and correct the model's decision boundaries through a
knowledge distillation step. Unlike DFR, our method not only reweights the
undersampled groups, but it also enriches them with new data points. Our method
does not require any confounder labels, achieves effective scaling to multiple
confounders, and yields balanced generalization across groups. We demonstrate
CFKD's efficacy across five datasets, spanning synthetic tasks to an industrial
application, with particularly strong gains in low-data regimes with pronounced
spurious correlations. Additionally, we provide an ablation study on the effect
of the chosen counterfactual explainer and teacher model, highlighting their
impact on robustness.

</details>


### [339] [How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?](https://arxiv.org/abs/2510.17526)
*Wei Huang,Andi Han,Yujin Song,Yilan Chen,Denny Wu,Difan Zou,Taiji Suzuki*

Main category: cs.LG

TL;DR: 研究在低信噪比下，向梯度更新引入标签噪声能否提升神经网络测试性能，证明标签噪声GD可抑制噪声记忆，实现良好泛化，而标准GD易过拟合。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型会记忆噪声，在低信噪比下泛化性差，受标签噪声可提升泛化性启发，研究引入标签噪声到梯度更新能否提升低信噪比下神经网络测试性能。

Method: 在理想化信号 - 噪声数据设置下，使用简单标签噪声梯度下降算法训练两层神经网络。

Result: 训练时添加标签噪声可抑制噪声记忆，使信号快速增长且控制过拟合；标准GD在相同低信噪比下易过拟合，其测试误差有非零下界。

Conclusion: 在基于梯度的训练中引入标签噪声有益，能提升低信噪比下神经网络的泛化性能。

Abstract: The capacity of deep learning models is often large enough to both learn the
underlying statistical signal and overfit to noise in the training set. This
noise memorization can be harmful especially for data with a low
signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior
observations that label noise provides implicit regularization that improves
generalization, in this work, we investigate whether introducing label noise to
the gradient updates can enhance the test performance of neural network (NN) in
the low SNR regime. Specifically, we consider training a two-layer NN with a
simple label noise gradient descent (GD) algorithm, in an idealized
signal-noise data setting. We prove that adding label noise during training
suppresses noise memorization, preventing it from dominating the learning
process; consequently, label noise GD enjoys rapid signal growth while the
overfitting remains controlled, thereby achieving good generalization despite
the low SNR. In contrast, we also show that NN trained with standard GD tends
to overfit to noise in the same low SNR setting and establish a non-vanishing
lower bound on its test error, thus demonstrating the benefit of introducing
label noise in gradient-based training.

</details>


### [340] [TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model](https://arxiv.org/abs/2510.17545)
*Yichen Liu,Yan Lin,Shengnan Guo,Zeyu Zhou,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: 提出TrajMamba方法用于高效且语义丰富的车辆轨迹学习，实验表明其在效率和准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹数据应用中，学习旅行语义受道路和POI信息计算负担重以及轨迹冗余点影响，需解决这些问题。

Method: 提出Traj - Mamba Encoder联合建模GPS和道路视角捕获运动模式；采用旅行目的感知预训练程序整合旅行目的；使用知识蒸馏预训练方案减少轨迹冗余。

Result: 在两个真实数据集和三个下游任务上的实验表明，TrajMamba在效率和准确性上优于现有基线。

Conclusion: TrajMamba是一种有效且高效的车辆轨迹学习方法，能解决现有轨迹数据应用中的挑战。

Abstract: Vehicle GPS trajectories record how vehicles move over time, storing valuable
travel semantics, including movement patterns and travel purposes. Learning
travel semantics effectively and efficiently is crucial for real-world
applications of trajectory data, which is hindered by two major challenges.
First, travel purposes are tied to the functions of the roads and
points-of-interest (POIs) involved in a trip. Such information is encoded in
textual addresses and descriptions and introduces heavy computational burden to
modeling. Second, real-world trajectories often contain redundant points, which
harm both computational efficiency and trajectory embedding quality. To address
these challenges, we propose TrajMamba, a novel approach for efficient and
semantically rich vehicle trajectory learning. TrajMamba introduces a
Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS
and road perspectives of trajectories, enabling robust representations of
continuous travel behaviors. It also incorporates a Travel Purpose-aware
Pre-training procedure to integrate travel purposes into the learned embeddings
without introducing extra overhead to embedding calculation. To reduce
redundancy in trajectories, TrajMamba features a Knowledge Distillation
Pre-training scheme to identify key trajectory points through a learnable mask
generator and obtain effective compressed trajectory embeddings. Extensive
experiments on two real-world datasets and three downstream tasks show that
TrajMamba outperforms state-of-the-art baselines in both efficiency and
accuracy.

</details>


### [341] [The Free Transformer](https://arxiv.org/abs/2510.17558)
*François Fleuret*

Main category: cs.LG

TL;DR: 提出对解码器Transformer的扩展，利用变分过程无监督学习随机潜在变量来改进下游任务。


<details>
  <summary>Details</summary>
Motivation: 提升解码器Transformer在下游任务的表现。

Method: 提出对解码器Transformer的扩展，通过变分过程无监督学习随机潜在变量来进行生成过程。

Result: 实验表明这种条件设定能在下游任务上带来显著提升。

Conclusion: 扩展的解码器Transformer在下游任务有显著改进效果。

Abstract: We propose an extension of the decoder Transformer that conditions its
generative process on random latent variables which are learned without
supervision thanks to a variational procedure. Experimental evaluations show
that allowing such a conditioning translates into substantial improvements on
downstream tasks.

</details>


### [342] [Formally Exploring Time-Series Anomaly Detection Evaluation Metrics](https://arxiv.org/abs/2510.17562)
*Dennis Wagner,Arjun Nair,Billy Joe Franks,Justus Arweiler,Aparna Muraleedharan,Indra Jungjohann,Fabian Hartung,Mayank C. Ahuja,Andriy Balinskyy,Saurabh Varshneya,Nabeel Hussain Syed,Mayank Nagda,Phillip Liznerski,Steffen Reithermann,Maja Rudolph,Sebastian Vollmer,Ralf Schulz,Torsten Katz,Stephan Mandt,Michael Bortz,Heike Leitte,Daniel Neider,Jakob Burger,Fabian Jirasek,Hans Hasse,Sophie Fellenz,Marius Kloft*

Main category: cs.LG

TL;DR: 现有时间序列异常检测指标有局限，论文引入可验证属性建立理论框架评估指标，分析37个指标，提出满足所有属性的LARM及高级变体ALARM。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法性能不明确，当前评估指标只能反映任务的狭窄方面且常产生误导性结果。

Method: 引入可验证属性建立理论框架评估指标，分析37个常用指标。

Result: 多数常用指标只满足少数属性，没有一个满足所有属性；提出满足所有属性的LARM及高级变体ALARM。

Conclusion: 新提出的指标能解决现有评估指标的不足，实现更可靠的评估和比较。

Abstract: Undetected anomalies in time series can trigger catastrophic failures in
safety-critical systems, such as chemical plant explosions or power grid
outages. Although many detection methods have been proposed, their performance
remains unclear because current metrics capture only narrow aspects of the task
and often yield misleading results. We address this issue by introducing
verifiable properties that formalize essential requirements for evaluating
time-series anomaly detection. These properties enable a theoretical framework
that supports principled evaluations and reliable comparisons. Analyzing 37
widely used metrics, we show that most satisfy only a few properties, and none
satisfy all, explaining persistent inconsistencies in prior results. To close
this gap, we propose LARM, a flexible metric that provably satisfies all
properties, and extend it to ALARM, an advanced variant meeting stricter
requirements.

</details>


### [343] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 分析安全强化学习中拉格朗日乘子的最优性和稳定性，发现乘子对结果敏感，自动更新有恢复甚至超越最优性能的能力，也有振荡问题，PID控制可缓解但需微调。


<details>
  <summary>Details</summary>
Motivation: 拉格朗日方法在安全强化学习中有效，但乘子选择影响效果，且自动更新的鲁棒性和对整体性能的影响缺乏实证。

Method: 分析安全强化学习中拉格朗日乘子在一系列任务中的最优性和稳定性，提供λ - 剖面图。

Result: λ对结果高度敏感，自动更新能恢复甚至超越最优性能，训练时有振荡行为，PID控制可缓解。

Conclusion: 需要进一步研究稳定安全强化学习中拉格朗日方法。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [344] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: 研究抗菌肽设计中深度生成模型潜在空间，发现降维等操作对优化抗菌活性的作用。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽设计困难，现有深度生成模型缺乏可解释性和对潜在空间质量的严格量化，需探索优化方法。

Method: 研究进一步降维设计空间、空间可解释性以及用物理化学性质组织潜在空间对优化抗菌活性效率的影响。

Result: 数据可用时，用更多相关信息组织空间，降维潜在空间有优势；降维搜索空间更具可解释性；不同标签可用比例下都能用不同物理化学性质组织潜在空间。

Conclusion: 降维等操作有助于抗菌肽设计中优化抗菌活性。

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [345] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: 提出通信高效的个性化联邦学习方法CEPerFed用于多脉冲MRI分类，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 训练多脉冲MRI分类模型需多方数据且保护隐私，现有联邦学习存在数据异质性和通信开销大的问题。

Method: 提出CEPerFed方法，结合客户端历史风险梯度和历史平均梯度协调局部和全局优化，提出分层SVD策略传输关键信息。

Result: 在五个分类任务的实验中证明了CEPerFed方法的有效性。

Conclusion: CEPerFed方法能有效解决多脉冲MRI分类模型训练中联邦学习存在的问题。

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [346] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: 本文提出ZACH - ViT和ShuffleStrides数据增强方法用于肺超声视频分类，在少量数据上表现优于多个基线模型，显示架构设计与数据结构匹配的重要性。


<details>
  <summary>Details</summary>
Motivation: 肺超声视频中区分心源性肺水肿与非心源性和结构正常肺部存在挑战，数据异质性使自动分类困难。

Method: 引入ZACH - ViT，去除位置嵌入和[CLS]标记；提出ShuffleStrides数据增强（SSDA）。

Result: ZACH - ViT在验证集和测试集上ROC - AUC最高，有平衡的敏感性和特异性，训练速度更快、参数更少，其他竞争模型表现差。

Conclusion: 在小数据医学成像中，架构设计与数据结构匹配比模型规模更重要。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


### [347] [Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction](https://arxiv.org/abs/2510.17661)
*Vaishnavi Visweswaraiah,Tanvi Banerjee,William Romine*

Main category: cs.LG

TL;DR: 文章利用机器学习和深度学习技术，用GAN生成合成数据处理自杀预测数据类别不平衡问题，测试多种模型并展示结果，强调GAN在自杀预防建模中的作用。


<details>
  <summary>Details</summary>
Motivation: 自杀预测数据正样本少，存在极端类别不平衡问题，需解决以更好进行自杀预测和预防。

Method: 利用机器学习构建模型，用深度学习技术如GAN生成合成数据增强数据集，使用多种机器学习模型。

Result: 在真实测试数据上，LR、RF、SVM有不同的加权精度、召回率和F1分数，各模型在识别自杀尝试案例上表现不同。

Conclusion: 模型有效，GAN在生成合成数据支持自杀预防建模方面起关键作用。

Abstract: Suicide prediction is the key for prevention, but real data with sufficient
positive samples is rare and causes extreme class imbalance. We utilized
machine learning (ML) to build the model and deep learning (DL) techniques,
like Generative Adversarial Networks (GAN), to generate synthetic data samples
to enhance the dataset. The initial dataset contained 656 samples, with only
four positive cases, prompting the need for data augmentation. A variety of
machine learning models, ranging from interpretable data models to black box
algorithmic models, were used. On real test data, Logistic Regression (LR)
achieved a weighted precision of 0.99, a weighted recall of 0.85, and a
weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,
respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.
LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and
misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &
0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)
with 0 false positives (specificity: 1.0). These results highlight the models'
effectiveness, with GAN playing a key role in generating synthetic data to
support suicide prevention modeling efforts.

</details>


### [348] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出语言介入循环框架，用大语言模型将自然语言反馈转换为标量效用进行优化搜索，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，需将复杂主观目标通过反馈转化为可量化优化目标，现有偏好贝叶斯优化存在局限。

Method: 提出语言介入循环框架，利用大语言模型将非结构化自然语言反馈转换为标量效用，在数值搜索空间进行贝叶斯优化。

Result: 该混合方法提供更自然决策界面，在反馈有限情况下优于传统贝叶斯优化基线和仅使用大语言模型的优化器。

Conclusion: 此方法能有效将自然语言反馈用于优化，兼具效率和不确定性量化能力。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [349] [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](https://arxiv.org/abs/2510.17690)
*Xihong Su*

Main category: cs.LG

TL;DR: 论文有三项主要贡献，包括提出CADP算法、建立指数ERM Bellman算子收缩条件及相关算法、提出无模型Q - learning算法并证明收敛性。


<details>
  <summary>Details</summary>
Motivation: 探索MMDPs中策略梯度与动态规划的联系，解决ERM - TRC和EVaR - TRC的最优策略计算及风险规避目标策略计算问题。

Method: 提出CADP算法迭代调整模型权重；建立指数ERM Bellman算子收缩的充要条件；利用Q - learning ERM Bellman算子单调性证明算法收敛性。

Result: CADP算法可保证策略单调改进到局部最优；建立相关条件并提出计算最优平稳策略的算法；证明ERM - TRC和EVaR - TRC的Q - learning算法收敛到最优风险规避值函数。

Conclusion: 所提算法可有效计算MMDPs中相关最优平稳策略及风险规避目标策略。

Abstract: This dissertation makes three main contributions. First, We identify a new
connection between policy gradient and dynamic programming in MMDPs and propose
the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov
policy that maximizes the discounted return averaged over the uncertain models.
CADP adjusts model weights iteratively to guarantee monotone policy
improvements to a local maximum. Second, We establish sufficient and necessary
conditions for the exponential ERM Bellman operator to be a contraction and
prove the existence of stationary deterministic optimal policies for ERM-TRC
and EVaR-TRC. We also propose exponential value iteration, policy iteration,
and linear programming algorithms for computing optimal stationary policies for
ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for
computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The
challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we
use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous
proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the
optimal risk-averse value functions. The proposed Q-learning algorithms compute
the optimal stationary policy for ERM-TRC and EVaR-TRC.

</details>


### [350] [Closing the Sim2Real Performance Gap in RL](https://arxiv.org/abs/2510.17709)
*Akhil S Anand,Shambhuraj Sawant,Jasper Hoffmann,Dirk Reinhardt,Sebastien Gros*

Main category: cs.LG

TL;DR: 提出新框架解决Sim2Real性能差距问题，采用双层RL框架，推导验证相关数学工具。


<details>
  <summary>Details</summary>
Motivation: 当前纯模拟训练的策略在现实环境中性能下降，现有Sim2Real RL方法的指标与策略的现实性能不一定相关。

Method: 提出双层RL框架，内层RL纯模拟训练策略，外层RL调整模拟模型和奖励参数以最大化策略现实性能。

Result: 推导验证了开发双层RL算法所需的数学工具。

Conclusion: 所提方法可解决Sim2Real性能差距问题。

Abstract: Sim2Real aims at training policies in high-fidelity simulation environments
and effectively transferring them to the real world. Despite the developments
of accurate simulators and Sim2Real RL approaches, the policies trained purely
in simulation often suffer significant performance drops when deployed in real
environments. This drop is referred to as the Sim2Real performance gap. Current
Sim2Real RL methods optimize the simulator accuracy and variability as proxies
for real-world performance. However, these metrics do not necessarily correlate
with the real-world performance of the policy as established theoretically and
empirically in the literature. We propose a novel framework to address this
issue by directly adapting the simulator parameters based on real-world
performance. We frame this problem as a bi-level RL framework: the inner-level
RL trains a policy purely in simulation, and the outer-level RL adapts the
simulation model and in-sim reward parameters to maximize real-world
performance of the in-sim policy. We derive and validate in simple examples the
mathematical tools needed to develop bi-level RL algorithms that close the
Sim2Real performance gap.

</details>


### [351] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 研究用黑盒大语言模型做分类器，提出高效方法提高操作粒度且不损失性能，在多数据集和模型上表现好。


<details>
  <summary>Details</summary>
Motivation: 黑盒大语言模型在有特定指标约束的决策应用中因数值输出基数低而不利，需提高其操作粒度。

Method: 先研究低基数数值输出原因，试验标准提示工程、不确定性估计和置信度引出技术，最后提出高效方法。

Result: 提出的方法提供更细粒度操作点，在11个数据集和3个大语言模型上比基准方法表现相当或更好。

Conclusion: 所提方法能有效提高黑盒大语言模型作为分类器时的操作粒度，且不损失性能。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


### [352] [Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network](https://arxiv.org/abs/2510.17756)
*Younghyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 本文开发物理信息神经网络（PINN）策略预测海冰速度和浓度，在少量样本训练下优于全数据驱动模型。


<details>
  <summary>Details</summary>
Motivation: 全数据驱动机器学习模型在泛化性和物理一致性上有局限，历史数据训练的模型可能无法代表未来海冰动态变化。

Method: 基于Hierarchical Information - sharing U - net架构，将物理知识集成到机器学习模型中，引入物理损失函数和激活函数。

Result: PINN模型在海冰速度和浓度的每日预测中优于全数据驱动模型，尤其在融化和早冻季节及快速移动冰区改善了海冰浓度预测。

Conclusion: PINN策略有效，能在少量样本训练下实现更好的海冰速度和浓度预测。

Abstract: As an increasing amount of remote sensing data becomes available in the
Arctic Ocean, data-driven machine learning (ML) techniques are becoming widely
used to predict sea ice velocity (SIV) and sea ice concentration (SIC).
However, fully data-driven ML models have limitations in generalizability and
physical consistency due to their excessive reliance on the quantity and
quality of training data. In particular, as Arctic sea ice entered a new phase
with thinner ice and accelerated melting, there is a possibility that an ML
model trained with historical sea ice data cannot fully represent the
dynamically changing sea ice conditions in the future. In this study, we
develop physics-informed neural network (PINN) strategies to integrate physical
knowledge of sea ice into the ML model. Based on the Hierarchical
Information-sharing U-net (HIS-Unet) architecture, we incorporate the physics
loss function and the activation function to produce physically plausible SIV
and SIC outputs. Our PINN model outperforms the fully data-driven model in the
daily predictions of SIV and SIC, even when trained with a small number of
samples. The PINN approach particularly improves SIC predictions in melting and
early freezing seasons and near fast-moving ice regions.

</details>


### [353] [Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning](https://arxiv.org/abs/2510.17772)
*Ryan A. Robinett,Sophia A. Madejski,Kyle Ruark,Samantha J. Riesenfeld,Lorenzo Orecchia*

Main category: cs.LG

TL;DR: 现有流形学习方法有局限，本文实现通用数据结构维护可微图集，实验证明其在效率、准确性、可解释性和鲁棒性上有优势。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习方法不能直接在潜在d维数据流形上进行机器学习，且直接学习潜在流形为可微图集的方法研究不足，要证明基于图集方法的有效性和潜力。

Method: 实现通用数据结构维护可微图集以进行黎曼优化，辅以无监督启发式方法从点云数据学习可微图集。

Result: 在选定设置中该方法在效率和准确性上有优势，在监督分类任务和RNA速度分析中展示了更好的可解释性和鲁棒性。

Conclusion: 基于图集的方法是有效且有潜力的。

Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning
methods do not support machine learning directly on the latent $d$-dimensional
data manifold, as they primarily aim to perform dimensionality reduction into
$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$
approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a
differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and
potential of atlas-based methods. To this end, we implement a generic data
structure to maintain a differentiable atlas that enables Riemannian
optimization over the manifold. We complement this with an unsupervised
heuristic that learns a differentiable atlas from point cloud data. We
experimentally demonstrate that this approach has advantages in terms of
efficiency and accuracy in selected settings. Moreover, in a supervised
classification task over the Klein bottle and in RNA velocity analysis of
hematopoietic data, we showcase the improved interpretability and robustness of
our approach.

</details>


### [354] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 提出样本级范式衡量语言模型后训练中知识遗忘和反向迁移，大规模分析不同后训练场景效果，框架为衡量后训练对预训练知识的影响提供实用标准。


<details>
  <summary>Details</summary>
Motivation: 后训练推动语言模型能力提升，但对预训练知识的影响不明，且不同遗忘不能简单平均，需准确衡量。

Method: 提出样本级范式，通过1->0和0->1转换分别量化遗忘和反向迁移，对多项选择基准测试增加机会调整变体。

Result: 不同后训练场景有不同遗忘和反向迁移情况，模型合并不能可靠缓解遗忘。

Conclusion: 该框架为大规模衡量后训练如何改变预训练知识提供实用标准，有助于通用人工智能系统发展。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [355] [Inference-Time Compute Scaling For Flow Matching](https://arxiv.org/abs/2510.17786)
*Adam Stecklov,Noah El Rimawi-Fine,Mathieu Blanchette*

Main category: cs.LG

TL;DR: 提出用于流匹配（FM）的新型推理时间缩放程序，在图像和蛋白质生成任务上验证了其可提升样本质量并能应用于科学领域。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配推理时间缩放方法存在牺牲高效采样等问题，且仅应用于视觉任务，需探索新方法。

Method: 引入在采样期间保留线性插值的新型推理时间缩放程序。

Result: 在图像生成和无条件蛋白质生成任务上，样本质量随推理计算量增加而提升，且证明流匹配推理时间缩放可应用于科学领域。

Conclusion: 所提方法有效，流匹配推理时间缩放有更广泛应用潜力。

Abstract: Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.

</details>


### [356] [Unbiased Gradient Low-Rank Projection](https://arxiv.org/abs/2510.17802)
*Rui Pan,Yang Luo,Yuxing Liu,Yang You,Tong Zhang*

Main category: cs.LG

TL;DR: 本文针对低秩投影优化方法缺乏收敛保证问题，提出GUM方法，理论证明其收敛性，实验显示性能优于GaLore和全参数训练。


<details>
  <summary>Details</summary>
Motivation: 许多基于梯度低秩投影的内存高效优化方法缺乏收敛保证，与全参数训练存在性能差距。

Method: 研究层采样技术对低秩投影机制去偏，基于GaLore机制和Muon算法提出GUM方法。

Result: 理论证明GUM方法与Muon算法有相同收敛保证，保留低秩技术内存效率；实验表明在LLM微调与预训练中性能优于GaLore，甚至超全参数训练。

Conclusion: GUM方法能使层内知识分布更均匀，更有效利用模型参数空间和记忆，提升性能。

Abstract: Memory-efficient optimization is critical for training increasingly large
language models (LLMs). A popular strategy involves gradient low-rank
projection, storing only the projected optimizer states, with GaLore being a
representative example. However, a significant drawback of many such methods is
their lack of convergence guarantees, as various low-rank projection approaches
introduce inherent biases relative to the original optimization algorithms,
which contribute to performance gaps compared to full-parameter training.
Aiming to tackle this problem, this paper investigates the layerwise sampling
technique for debiasing low-rank projection mechanisms. In particular, an
instantiation of the paradigm gives rise to a novel and unbiased low-rank
optimization method built upon GaLore's mechanism and the Muon algorithm, named
GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the
convergence guarantees of the base Muon algorithm while preserving the memory
efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and
pretraining also demonstrate non-trivial improvements over GaLore and even
better performance than full-parameter training. Further investigation shows
that the improvement of this technique comes from a more uniform distribution
of knowledge inside layers, leading to more efficient utilization of the model
parameter space and better memorization.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [357] [Call-Center Staff Scheduling Considering Performance Evolution under Emotional Stress](https://arxiv.org/abs/2510.16406)
*Yujun Zheng,Xinya Chen,Xueqin Lu,Weiguo Sheng,Shengyong Chen*

Main category: cs.NE

TL;DR: 研究考虑情绪压力下员工工作表现演变的呼叫中心员工调度问题，提出模型和算法，实验显示性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有员工调度方法常忽略情绪压力对员工工作表现的影响。

Method: 提出情绪压力驱动模型，基于此构建调度问题，并用结合全局变异、邻域搜索和深度强化学习的模因优化算法求解。

Result: 银行呼叫中心员工调度实际问题实例实验表明，该方法比所选流行调度方法有性能优势。

Conclusion: 通过明确建模和纳入情绪压力，该方法在员工调度中更现实地理解和利用人类行为。

Abstract: Emotional stress often has a significant effect on the working performance of
staff, but this effect is commonly neglected in existing staff scheduling
methods. We study a call-center staff scheduling problem, which considers the
evolution of work performance of staff under emotional stress. First, we
present an emotional stress driven model that estimates the working performance
of call-center employees based on not only skill levels but also emotional
states. On the basis of the model, we formulate a combined short-term and
long-term call-center staff scheduling problem aiming at maximizing the
customer service level, which depends on the working performance of employees.
We then propose a memetic optimization algorithm combining global mutation and
neighborhood search assisted by deep reinforcement learning to efficiently
solve this problem. Experimental results on real-world problem instances of
bank call-center staff scheduling demonstrate the performance advantages of the
proposed method over selected popular staff scheduling methods. By explicitly
modeling and incorporating emotional stress, our method reflects a more
realistic understanding and utilization of human behavior in staff scheduling.

</details>


### [358] [Bombardier Beetle Optimizer: A Novel Bio-Inspired Algorithm for Global Optimization](https://arxiv.org/abs/2510.17005)
*Hisham A. Shehadeh,Mohd Yamani Idna Idris,Iqbal H. Jebril*

Main category: cs.NE

TL;DR: 提出名为Bombardier Beetle Optimizer (BBO)的新型仿生优化算法，测试并与多种算法对比，证明其效率。


<details>
  <summary>Details</summary>
Motivation: 受放屁甲虫防御和逃脱机制启发，提出新的优化算法。

Method: 模拟放屁甲虫防御和逃脱机制设计BBO算法，用CEC 2017测试床套件测试，并与CDO、GWO等多种元启发式优化算法对比。

Result: BBO算法在收敛速度和结果质量上优于其他对比算法。

Conclusion: BBO算法是一种高效的优化算法。

Abstract: In this paper, a novel bio-inspired optimization algorithm is proposed,
called Bombardier Beetle Optimizer (BBO). This type of species is very
intelligent, which has an ability to defense and escape from predators. The
principles of the former one is inspired by the defense mechanism of Bombardier
Beetle against the predators, which the Bombardier Beetle triggers a toxic
chemical spray when it feels threatened. This reaction occurs in a specialized
reaction chamber inside its abdomen and includes a well regulated enzymatic
mechanism, which comprises hot water vapor, oxygen, and irritating substances
like p-benzoquinones. In addition, the proposed BBO simulates also the escape
mechanism of Bombardier Beetle from predator, which it has the ability to
calculate its distance from predator and it can fly away. The BBO is tested
with optimizing Congress on Evolutionary Computation (CEC 2017) test bed
suites. Moreover, it is compared against well-known metaheuristic optimization
algorithms includes Chernobyl Disaster Optimizer (CDO), Grey Wolf Optimizer
(GWO), Particle Swarm Optimization (PSO), Bermuda Triangle Optimizer (BTO),
Sperm Swarm Optimization (SSO) and Gravitational Search Algorithm (GSA). The
outcomes of this paper prove the BBO's efficiency in which outperforms the
other algorithms in terms of convergence rate and quality of results.

</details>


### [359] [ReLACE: A Resource-Efficient Low-Latency Cortical Acceleration Engine](https://arxiv.org/abs/2510.17392)
*Sonu Kumar,Arjun S. Nair,Bhawna Chaudhary,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.NE

TL;DR: 提出具有RCHH神经元模型的皮质神经池（CNP）架构，在FPGA实现上有资源和速度优势，适用于边缘AI应用。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的边缘AI应用提供生物准确、低资源的脉冲神经网络实现。

Method: 提出基于CORDIC的Hodgkin Huxley（RCHH）神经元模型，利用模块化和性能优化的CORDIC阶段进行设计。

Result: RCHH神经元FPGA实现LUT减少24.5%、速度提升35.2%、NRMSE改善70%；CNP吞吐量比等效DNN引擎高2.85倍，MNIST数据集上准确率仅下降0.35%。

Conclusion: 该设计适用于资源受限的边缘AI应用。

Abstract: We present a Cortical Neural Pool (CNP) architecture featuring a high-speed,
resource-efficient CORDIC-based Hodgkin Huxley (RCHH) neuron model. Unlike
shared CORDIC-based DNN approaches, the proposed neuron leverages modular and
performance-optimised CORDIC stages with a latency-area trade-off. The FPGA
implementation of the RCHH neuron shows 24.5% LUT reduction and 35.2% improved
speed, compared to SoTA designs, with 70% better normalised root mean square
error (NRMSE). Furthermore, the CNP exhibits 2.85x higher throughput (12.69
GOPS) compared to a functionally equivalent CORDIC-based DNN engine, with only
a 0.35% accuracy drop compared to the DNN counterpart on the MNIST dataset. The
overall results indicate that the design shows biologically accurate,
low-resource spiking neural network implementations for resource-constrained
edge AI applications.

</details>


### [360] [A Multi-Threading Kernel for Enabling Neuromorphic Edge Applications](https://arxiv.org/abs/2510.17745)
*Lars Niedermeier,Vyom Shah,Jeffrey L. Krichmar*

Main category: cs.NE

TL;DR: 本文介绍多线程内核，可使神经形态应用在边缘运行，有速度和能耗优势，能助力低SWaP边缘应用开发和神经形态芯片集成。


<details>
  <summary>Details</summary>
Motivation: 利用脉冲神经网络（SNNs）稀疏、事件驱动处理特性，实现神经形态应用在边缘运行。

Method: 引入多线程内核，在多核处理器上实现负载均衡。

Result: 在中等规模SNNs上比单线程处理速度快4倍，在Synfire网络上快1.7倍，比静态核心分配节能达70%。

Conclusion: 该工作可推动低SWaP边缘应用开发和神经形态芯片集成。

Abstract: Spiking Neural Networks (SNNs) have sparse, event driven processing that can
leverage neuromorphic applications. In this work, we introduce a
multi-threading kernel that enables neuromorphic applications running at the
edge, meaning they process sensory input directly and without any up-link to or
dependency on a cloud service. The kernel shows speed-up gains over single
thread processing by a factor of four on moderately sized SNNs and 1.7X on a
Synfire network. Furthermore, it load-balances all cores available on
multi-core processors, such as ARM, which run today's mobile devices and is up
to 70% more energy efficient compared to statical core assignment. The present
work can enable the development of edge applications that have low Size,
Weight, and Power (SWaP), and can prototype the integration of neuromorphic
chips.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [361] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: 提出利用快慢思维提升大语言模型代理在程序修复等复杂任务的能力，设计SIADAFIX方法，在SWE - bench Lite实验中达60.67% pass@1性能。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型代理在复杂任务（如程序修复）上的能力。

Method: 设计基于问题描述响应的自适应程序修复方法SIADAFIX，利用慢思维修复代理完成复杂任务，用快思维组件优化和分类问题描述，根据问题复杂度自适应选择三种修复模式。

Result: 在SWE - bench Lite上使用Claude - 4 Sonnet模型实现60.67% pass@1性能，达开源方法的先进水平。

Conclusion: SIADAFIX有效平衡修复效率和准确性，为自动化程序修复提供新见解。

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [362] [Code Contribution and Credit in Science](https://arxiv.org/abs/2510.16242)
*Eva Maxfield Brown,Isaac Slaughter,Nicholas Weber*

Main category: cs.SE

TL;DR: 研究软件开发与学术信用分配关系，发现软件贡献与传统作者信用有差异，软件开发活动与学术信用存在脱节。


<details>
  <summary>Details</summary>
Motivation: 软件开发对科研至关重要，但与传统学术信用度量关系不明，需研究软件开发活动对合作科研中信用分配的影响。

Method: 开发约140,000对研究论文和代码仓库的数据集及匹配模型，用数据进行调查。

Result: 近30%文章有非作者代码贡献者；代码贡献作者文章引用率仅增约4.2%，控制部分因素后不显著；一作更可能是代码贡献者；编码频率与学术影响指标负相关。

Conclusion: 软件贡献与信用脱节，对机构奖励结构和科学政策有重要启示。

Abstract: Software development has become essential to scientific research, but its
relationship to traditional metrics of scholarly credit remains poorly
understood. We develop a dataset of approximately 140,000 paired research
articles and code repositories, as well as a predictive model that matches
research article authors with software repository developer accounts. We use
this data to investigate how software development activities influence credit
allocation in collaborative scientific settings. Our findings reveal
significant patterns distinguishing software contributions from traditional
authorship credit. We find that nearly 30% of articles include non-author code
contributors- individuals who participated in software development but received
no formal authorship recognition. While code-contributing authors show a modest
$\sim$4.2% increase in article citations, this effect becomes non-significant
when controlling for domain, article type, and open access status. First
authors are significantly more likely to be code contributors than other author
positions. Notably, we identify a negative relationship between coding
frequency and scholarly impact metrics. Authors who contribute code more
frequently exhibit progressively lower h-indices than non-coding colleagues,
even when controlling for publication count, author position, domain, and
article type. These results suggest a disconnect between software contributions
and credit, highlighting important implications for institutional reward
structures and science policy.

</details>


### [363] [MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema](https://arxiv.org/abs/2510.16357)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: 介绍多语言代码解析器数据集MLCPD，含超700万解析源文件，有跨语言结构规律，公开数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 提供能统一多种编程语言代码的句法和结构表示，用于跨语言推理、结构学习和多语言软件分析的数据集。

Method: 提出通用抽象语法树（AST）模式对源文件进行归一化处理，存储丰富元数据，采用Parquet格式以便可扩展检索。

Result: 发现强跨语言结构规律，不同语言句法图可在共享模式下对齐。

Conclusion: MLCPD为跨语言表示学习和程序分析研究提供开放、可复现基础。

Abstract: We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,
language-agnostic dataset unifying syntactic and structural representations of
code across ten major programming languages. MLCPD contains over seven million
parsed source files normalized under our proposed universal Abstract Syntax
Tree (AST) schema, enabling consistent cross-language reasoning, structural
learning, and multilingual software analysis. Unlike existing corpora that
focus purely on token-level code or isolated parsers, MLCPD provides both
hierarchical tree representations and rich metadata for every file, ensuring
lossless syntactic coverage and structural uniformity. Each entry includes a
normalized schema, language-level metadata, and abstracted node semantics
stored in Parquet format for scalable retrieval. Empirical analyses reveal
strong cross-language structural regularities-demonstrating that syntactic
graphs from languages as diverse as Python, Java, and Go can be aligned under a
shared schema. We release the dataset publicly on Hugging Face and the
accompanying codebase on GitHub, which includes complete pipelines for dataset
reproduction, grammar compilation, and a visualization tool for exploring the
unified AST across languages. Together, these resources establish MLCPD as an
open, reproducible foundation for future research in cross-language
representation learning and program analysis.

</details>


### [364] [SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis](https://arxiv.org/abs/2510.16384)
*Yuwei Zhao,Yuan-An Xiao,Qianyu Xiao,Zhao Zhang,Yingfei Xiong*

Main category: cs.SE

TL;DR: 现有代码优化方法因检索局限性效果不佳，本文提出SemOpt框架，实验证明其有效且实用。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息检索引导大语言模型优化代码的方法，因语义等价优化的语法差异，难以找到相关示例，导致优化性能不佳。

Method: 提出SemOpt框架，包含策略库构建器、规则生成器和优化器三个由大语言模型驱动的组件。

Result: 在151个优化任务基准测试中，成功优化次数较基线增加1.38 - 28倍；在大型C/C++项目中，性能指标提升5.04% - 218.07%。

Conclusion: SemOpt框架有效且具有实际应用价值。

Abstract: Automated code optimization aims to improve performance in programs by
refactoring code, and recent studies focus on utilizing LLMs for the
optimization. Typical existing approaches mine optimization commits from
open-source codebases to construct a large-scale knowledge base, then employ
information retrieval techniques such as BM25 to retrieve relevant optimization
examples for hotspot code locations, thereby guiding LLMs to optimize these
hotspots. However, since semantically equivalent optimizations can manifest in
syntactically dissimilar code snippets, current retrieval methods often fail to
identify pertinent examples, leading to suboptimal optimization performance.
This limitation significantly reduces the effectiveness of existing
optimization approaches.
  To address these limitations, we propose SemOpt, a novel framework that
leverages static program analysis to precisely identify optimizable code
segments, retrieve the corresponding optimization strategies, and generate the
optimized results. SemOpt consists of three key components: (1) A strategy
library builder that extracts and clusters optimization strategies from
real-world code modifications. (2) A rule generator that generates Semgrep
static analysis rules to capture the condition of applying the optimization
strategy. (3) An optimizer that utilizes the strategy library to generate
optimized code results. All the three components are powered by LLMs.
  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its
effectiveness under different LLMs by increasing the number of successful
optimizations by 1.38 to 28 times compared to the baseline. Moreover, on
popular large-scale C/C++ projects, it can improve individual performance
metrics by 5.04% to 218.07%, demonstrating its practical utility.

</details>


### [365] [Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development](https://arxiv.org/abs/2510.16395)
*Xin Peng,Chong Wang*

Main category: cs.SE

TL;DR: 文章指出大语言模型在软件工程任务有能力但企业软件开发有挑战，提出代码数字孪生框架连接AI与企业软件现实。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽在软件工程有能力，但企业软件开发依赖隐性知识，需将AI能力与企业开发现实结合以提供有效支持。

Method: 系统识别软件和大语言模型视角的挑战，提出代码数字孪生框架，整合混合知识表示、多阶段提取管道等。

Result: 代码数字孪生框架将碎片化知识转化为明确可操作的表示。

Conclusion: 代码数字孪生框架可作为AI进步与企业软件现实的桥梁，为超复杂系统开发和演化提供路线图。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
capabilities in software engineering tasks, raising expectations of
revolutionary productivity gains. However, enterprise software development is
largely driven by incremental evolution, where challenges extend far beyond
routine coding and depend critically on tacit knowledge, including design
decisions at different levels and historical trade-offs. To achieve effective
AI-powered support for complex software development, we should align emerging
AI capabilities with the practical realities of enterprise development. To this
end, we systematically identify challenges from both software and LLM
perspectives. Alongside these challenges, we outline opportunities where AI and
structured knowledge frameworks can enhance decision-making in tasks such as
issue localization and impact analysis. To address these needs, we propose the
Code Digital Twin, a living framework that models both the physical and
conceptual layers of software, preserves tacit knowledge, and co-evolves with
the codebase. By integrating hybrid knowledge representations, multi-stage
extraction pipelines, incremental updates, LLM-empowered applications, and
human-in-the-loop feedback, the Code Digital Twin transforms fragmented
knowledge into explicit and actionable representations. Our vision positions it
as a bridge between AI advancements and enterprise software realities,
providing a concrete roadmap toward sustainable, intelligent, and resilient
development and evolution of ultra-complex systems.

</details>


### [366] [Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions](https://arxiv.org/abs/2510.16433)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Yasutaka Kamei,Hajimu Iida*

Main category: cs.SE

TL;DR: 研究探讨持续模糊测试在漏洞检测中的作用，通过对OSS - Fuzz约112万个模糊测试会话的实证研究，揭示相关规律并为后续策略和工具开发提供见解。


<details>
  <summary>Details</summary>
Motivation: 尽管持续模糊测试被众多项目采用，但不清楚其对漏洞检测的贡献，旨在阐明其在漏洞检测中的作用。

Method: 从OSS - Fuzz收集问题报告、覆盖率报告和模糊测试日志，对约112万个来自878个项目的模糊测试会话进行实证研究。

Result: （i）持续模糊测试集成前存在大量模糊测试漏洞，早期检测率高；（ii）持续模糊测试过程中代码覆盖率持续增加；（iii）覆盖率变化有助于检测模糊测试漏洞。

Conclusion: 本研究为持续模糊测试如何助力模糊测试漏洞检测提供实证见解，为未来策略和工具开发提供实践启示。

Abstract: Software vulnerabilities are constantly being reported and exploited in
software products, causing significant impacts on society. In recent years, the
main approach to vulnerability detection, fuzzing, has been integrated into the
continuous integration process to run in short and frequent cycles. This
continuous fuzzing allows for fast identification and remediation of
vulnerabilities during the development process. Despite adoption by thousands
of projects, however, it is unclear how continuous fuzzing contributes to
vulnerability detection. This study aims to elucidate the role of continuous
fuzzing in vulnerability detection. Specifically, we investigate the coverage
and the total number of fuzzing sessions when fuzzing bugs are discovered. We
collect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an
online service provided by Google that performs fuzzing during continuous
integration. Through an empirical study of a total of approximately 1.12
million fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal
that (i) a substantial number of fuzzing bugs exist prior to the integration of
continuous fuzzing, leading to a high detection rate in the early stages; (ii)
code coverage continues to increase as continuous fuzzing progresses; and (iii)
changes in coverage contribute to the detection of fuzzing bugs. This study
provides empirical insights into how continuous fuzzing contributes to fuzzing
bug detection, offering practical implications for future strategies and tool
development in continuous fuzzing.

</details>


### [367] [On the Use of Large Language Models for Qualitative Synthesis](https://arxiv.org/abs/2510.16502)
*Sebastián Pizard,Ramiro Moreira,Federico Galiano,Ignacio Sastre,Lorena Etcheverry*

Main category: cs.SE

TL;DR: 研究使用大语言模型进行定性综合的挑战，通过协作式自我民族志研究评估两个试验。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽可支持系统评价和定性综合，但应用于报告不均且执行多变的阶段有重要风险，需研究其用于定性综合的挑战。

Method: 进行涉及两个试验的协作式自我民族志研究，评估试验的方法严谨性和实用性，并从大语言模型构建及当前局限性的技术视角解读结果。

Result: 未提及。

Conclusion: 未提及。

Abstract: Large language models (LLMs) show promise for supporting systematic reviews
(SR), even complex tasks such as qualitative synthesis (QS). However, applying
them to a stage that is unevenly reported and variably conducted carries
important risks: misuse can amplify existing weaknesses and erode confidence in
the SR findings. To examine the challenges of using LLMs for QS, we conducted a
collaborative autoethnography involving two trials. We evaluated each trial for
methodological rigor and practical usefulness, and interpreted the results
through a technical lens informed by how LLMs are built and their current
limitations.

</details>


### [368] [Human-Aligned Code Readability Assessment with Large Language Models](https://arxiv.org/abs/2510.16579)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Pawel Borsukiewicz,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 介绍CoReEval基准评估LLM代码可读性评估能力，发现开发者引导提示有优势但存在权衡，为相关研究和应用提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统静态指标难评估代码可读性，LLM作为可扩展替代方案的行为待探索。

Method: 引入CoReEval基准，对10个先进LLM进行超140万次评估，涵盖多种语言、代码类型、提示策略和解码设置，将LLM输出与人工标注和静态模型对比分析。

Result: 开发者引导提示在结构化上下文提高对齐度、提升解释质量、实现轻量级个性化，但分数变异性增加凸显权衡。

Conclusion: CoReEval为提示工程、模型对齐研究和人工评估提供基础，在多场景有应用价值。

Abstract: Code readability is crucial for software comprehension and maintenance, yet
difficult to assess at scale. Traditional static metrics often fail to capture
the subjective, context-sensitive nature of human judgments. Large Language
Models (LLMs) offer a scalable alternative, but their behavior as readability
evaluators remains underexplored. We introduce CoReEval, the first large-scale
benchmark for evaluating LLM-based code readability assessment, comprising over
1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.
The benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types
(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),
9 decoding settings, and developer-guided prompts tailored to junior and senior
personas. We compare LLM outputs against human annotations and a validated
static model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and
justification quality (sentiment, aspect coverage, semantic clustering). Our
findings show that developer-guided prompting grounded in human-defined
readability dimensions improves alignment in structured contexts, enhances
explanation quality, and enables lightweight personalization through persona
framing. However, increased score variability highlights trade-offs between
alignment, stability, and interpretability. CoReEval provides a robust
foundation for prompt engineering, model alignment studies, and human in the
loop evaluation, with applications in education, onboarding, and CI/CD
pipelines where LLMs can serve as explainable, adaptable reviewers.

</details>


### [369] [Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios](https://arxiv.org/abs/2510.16665)
*Mohamed Sami Rakha,Andriy Miranskyy,Daniel Alencar da Costa*

Main category: cs.SE

TL;DR: 研究对比超参数调优在IVDP和CVDP两种SDP场景中的影响，发现IVDP场景SDP收益更大，建议考虑SDP场景影响。


<details>
  <summary>Details</summary>
Motivation: 不同SDP场景下超参数调优的积极影响不同，需对比其在不同场景的影响以提升SDP建模的实用性。

Method: 对比超参数调优在IVDP和CVDP两种场景的影响，用常见评估设置、28种ML算法、53个数据集、2种调优算法和5种优化指标，运用统计分析。

Result: IVDP场景SDP收益显著大于CVDP场景；28种ML算法中多达24种的性能提升在多场景不一定成立；小数据集性能影响差异更大。

Conclusion: 软件工程研究者和从业者在期望超参数调优带来性能提升时，应考虑所选SDP场景的影响。

Abstract: Software defect prediction (SDP) is crucial for delivering high-quality
software products. Recent research has indicated that prediction performance
improvements in SDP are achievable by applying hyperparameter tuning to a
particular SDP scenario. However, the positive impact resulting from the
hyperparameter tuning step may differ based on the targeted SDP scenario.
Comparing the impact of hyperparameter tuning across SDP scenarios is necessary
to provide comprehensive insights and enhance the robustness, generalizability,
and, eventually, the practicality of SDP modeling for quality assurance.
  Therefore, in this study, we contrast the impact of hyperparameter tuning
across two pivotal and consecutive SDP scenarios: (1) Inner Version Defect
Prediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main
distinctions between the two scenarios lie in the scope of defect prediction
and the selected evaluation setups. This study's experiments use common
evaluation setups, 28 machine learning (ML) algorithms, 53 post-release
software datasets, two tuning algorithms, and five optimization metrics. We
apply statistical analytics to compare the SDP performance impact differences
by investigating the overall impact, the single ML algorithm impact, and
variations across different software dataset sizes.
  The results indicate that the SDP gains within the IVDP scenario are
significantly larger than those within the CVDP scenario. The results reveal
that asserting performance gains for up to 24 out of 28 ML algorithms may not
hold across multiple SDP scenarios. Furthermore, we found that small software
datasets are more susceptible to larger differences in performance impacts.
Overall, the study findings recommend software engineering researchers and
practitioners to consider the effect of the selected SDP scenario when
expecting performance gains from hyperparameter tuning.

</details>


### [370] [QuanBench: Benchmarking Quantum Code Generation with Large Language Models](https://arxiv.org/abs/2510.16779)
*Xiaoyu Guo,Minggu Wang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 提出QuanBench基准评估大语言模型量子代码生成能力，发现当前模型能力有限，为后续工作提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在量子代码生成能力研究不足，需评估其能力。

Method: 构建包含44个编程任务的QuanBench基准，用功能正确性和量子语义等价性评估模型。

Result: 当前大语言模型生成正确量子代码能力有限，整体准确率低于40%，常有语义错误。

Conclusion: QuanBench为改进大语言模型量子代码生成的后续工作提供基础。

Abstract: Large language models (LLMs) have demonstrated good performance in general
code generation; however, their capabilities in quantum code generation remain
insufficiently studied. This paper presents QuanBench, a benchmark for
evaluating LLMs on quantum code generation. QuanBench includes 44 programming
tasks that cover quantum algorithms, state preparation, gate decomposition, and
quantum machine learning. Each task has an executable canonical solution and is
evaluated by functional correctness (Pass@K) and quantum semantic equivalence
(Process Fidelity). We evaluate several recent LLMs, including general-purpose
and code-specialized models. The results show that current LLMs have limited
capability in generating the correct quantum code, with overall accuracy below
40% and frequent semantic errors. We also analyze common failure cases, such as
outdated API usage, circuit construction errors, and incorrect algorithm logic.
QuanBench provides a basis for future work on improving quantum code generation
with LLMs.

</details>


### [371] [More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents](https://arxiv.org/abs/2510.16786)
*Pengfei Gao,Chao Peng*

Main category: cs.SE

TL;DR: 本文针对大语言模型驱动的编码代理成本高且不可预测的问题，研究了三种回合控制策略，发现动态回合策略能有效平衡成本和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的编码代理在实际部署中存在成本高且不可预测的问题，而对总回合数的战略控制研究不足。

Method: 在SWE - bench上使用三种最先进的模型，评估无限制基线、带提醒的固定回合限制和新颖的动态回合策略这三种回合控制策略。

Result: 无限制设置下无单一模型能兼顾性能、成本和回合效率；固定回合限制在第75百分位是‘最佳点’，能大幅降低成本且对解决率影响小；动态回合策略表现更优，能进一步降低成本。

Conclusion: 本研究首次系统分析回合控制策略，证明动态资源分配是部署经济可行编码代理的优越且易实现的方法。

Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve
software engineering tasks, are becoming increasingly powerful. However, their
practical deployment is hindered by significant and unpredictable costs. This
challenge arises from a combination of factors: quadratically growing token
counts with each turn, the high price of models, the large number of turns
required for real-world tasks, and the tendency of agents to take inefficient
or unnecessary actions. While existing research focuses on optimizing
individual turns, the strategic control of the total number of turns remains an
underexplored area for managing agent performance and cost. To address this
gap, we conduct a comprehensive empirical study on SWE-bench using three
state-of-the-art models and evaluate the impact of three distinct turn-control
strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a
novel dynamic-turn strategy that grants extensions on-demand. Our findings
first reveal a fundamental trade-off in the unrestricted setting, where no
single model excels across performance, cost, and turn efficiency. We then show
that a fixed-turn limit, specifically at the 75th percentile of the baseline,
serves as a "sweet spot", substantially reducing costs (by 24%-68%) with
minimal impact on solve rates. Most significantly, the dynamic-turn strategy
consistently outperforms fixed-limit approaches, achieving comparable or better
solve rates while further reducing costs by an additional 12%-24% by
intelligently allocating resources only to tasks that need them. This work
provides the first systematic analysis of turn-control strategies, offering
simple yet effective guidelines for developers to balance cost and efficacy. We
demonstrate that dynamic resource allocation is a superior, easy-to-implement
approach for deploying powerful yet economically viable coding agents.

</details>


### [372] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 研究代码翻译中上下文示例数量对大语言模型性能的影响，发现“多示例悖论”，少数精心选择的示例更重要。


<details>
  <summary>Details</summary>
Motivation: 探究在代码翻译任务中，提供更多示例是否能提升大语言模型上下文学习的性能。

Method: 对超过90000次翻译进行大规模实证研究，系统评估从0示例到最多625示例、提示词从约100000到800000个标记的情况。

Result: 存在“多示例悖论”，静态相似度指标随示例增多有适度提升，但功能正确性在少示例（5 - 25个）时达到峰值，更多示例会降低功能性能。

Conclusion: 对于代码翻译，精心选择的少量示例质量比数量更重要，挑战了上下文学习中“越多越好”的普遍观点，强调了最优提示策略的任务依赖性，对软件工程中有效利用大语言模型有重要意义。

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


### [373] [When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation](https://arxiv.org/abs/2510.16823)
*Yue Liu,Zhenchang Xing,Shidong Pan,Chakkrit Tantithamthavorn*

Main category: cs.SE

TL;DR: 研究最先进大语言模型生成的Chrome扩展程序的安全性，发现生成的程序漏洞率高，先进推理模型表现更差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型主导软件开发时开发者易忽略安全问题，研究其生成框架约束程序的安全属性。

Method: 构建含140个提示的ChromeSecBench数据集，让九个最先进大语言模型生成Chrome扩展程序，从场景类型、模型差异和漏洞类别三维分析。

Result: 大语言模型生成的程序漏洞率高达18%-50%，认证与身份和Cookie管理场景高达83%和78%，先进推理模型生成更多漏洞。

Conclusion: 大语言模型的编码技能和编写安全框架约束程序的能力存在关键差距。

Abstract: In recent years, the AI wave has grown rapidly in software development. Even
novice developers can now design and generate complex framework-constrained
software systems based on their high-level requirements with the help of Large
Language Models (LLMs). However, when LLMs gradually "take the wheel" of
software development, developers may only check whether the program works. They
often miss security problems hidden in how the generated programs are
implemented.
  In this work, we investigate the security properties of framework-constrained
programs generated by state-of-the-art LLMs. We focus specifically on Chrome
extensions due to their complex security model involving multiple privilege
boundaries and isolated components. To achieve this, we built ChromeSecBench, a
dataset with 140 prompts based on known vulnerable extensions. We used these
prompts to instruct nine state-of-the-art LLMs to generate complete Chrome
extensions, and then analyzed them for vulnerabilities across three dimensions:
scenario types, model differences, and vulnerability categories. Our results
show that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),
particularly in Authentication & Identity and Cookie Management scenarios (up
to 83% and 78% respectively). Most vulnerabilities exposed sensitive browser
data like cookies, history, or bookmarks to untrusted code. Interestingly, we
found that advanced reasoning models performed worse, generating more
vulnerabilities than simpler models. These findings highlight a critical gap
between LLMs' coding skills and their ability to write secure
framework-constrained programs.

</details>


### [374] [Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection](https://arxiv.org/abs/2510.17056)
*Luis F. G. Campos,Leonardo C. Marques,Walter T. Nakamura*

Main category: cs.SE

TL;DR: 研究对比生成式AI与人类检查员识别软件界面可用性问题的表现，发现两者结合效果最佳，当前AI不能取代人类但可作为辅助工具。


<details>
  <summary>Details</summary>
Motivation: 可用性检查成本高，需时间和专业知识，AI发展为支持该任务带来新机遇，研究其在识别可用性问题上的表现。

Method: 用软件原型，由四位专家和两个AI模型（GPT - 4o和Gemini 2.5 Flash）评估，使用精度、召回率和F1分数等指标。

Result: 检查员精度和整体覆盖率最高，AI个体表现高且发现许多新缺陷，但误报和冗余报告率高，两者结合效果最佳。

Conclusion: 当前阶段AI不能取代人类检查员，可作为有价值的辅助工具提高效率和扩大缺陷覆盖范围，研究为AI在可用性检查中的作用讨论提供量化依据。

Abstract: Usability inspection is a well-established technique for identifying
interaction issues in software interfaces, thereby contributing to improved
product quality. However, it is a costly process that requires time and
specialized knowledge from inspectors. With advances in Artificial Intelligence
(AI), new opportunities have emerged to support this task, particularly through
generative models capable of interpreting interfaces and performing inspections
more efficiently. This study examines the performance of generative AIs in
identifying usability problems, comparing them to those of experienced human
inspectors. A software prototype was evaluated by four specialists and two AI
models (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,
and F1-score. While inspectors achieved the highest levels of precision and
overall coverage, the AIs demonstrated high individual performance and
discovered many novel defects, but with a higher rate of false positives and
redundant reports. The combination of AIs and human inspectors produced the
best results, revealing their complementarity. These findings suggest that AI,
in its current stage, cannot replace human inspectors but can serve as a
valuable augmentation tool to improve efficiency and expand defect coverage.
The results provide evidence based on quantitative analysis to inform the
discussion on the role of AI in usability inspections, pointing to viable paths
for its complementary use in software quality assessment contexts.

</details>


### [375] [M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs](https://arxiv.org/abs/2510.17110)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: 本文提出基于MDD的方法支持量子系统的结构化设计与实现，能为多QPL自动生成代码，经案例验证有效。


<details>
  <summary>Details</summary>
Motivation: 量子计算受关注，众多QPL出现，但MDD在量子系统工程中的应用研究不足。

Method: 提出基于MDD的方法，构建框架为多QPL自动生成量子代码。

Result: 通过多个案例研究证明了该方法的有效性和实用性。

Conclusion: 基于MDD的方法可支持量子系统结构化设计与实现，提高开发效率和跨平台一致性。

Abstract: With the growing interest in quantum computing, the emergence of quantum
supremacy has marked a pivotal milestone in the field. As a result, numerous
quantum programming languages (QPLs) have been introduced to support the
development of quantum algorithms. However, the application of Model-Driven
Development (MDD) in quantum system engineering remains largely underexplored.
This paper presents an MDD-based approach to support the structured design and
implementation of quantum systems. Our framework enables the automatic
generation of quantum code for multiple QPLs, thereby enhancing development
efficiency and consistency across heterogeneous quantum platforms. The
effectiveness and practicality of our approach have been demonstrated through
multiple case studies.

</details>


### [376] [SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning](https://arxiv.org/abs/2510.17130)
*Shuzheng Gao,Chaozheng Wang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 现有代码生成的思维链推理方法有局限，提出SEER框架解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成的思维链推理方法存在探索路径有限、缺乏中间步骤质量评估、可能过度思考等问题，需改进。

Method: 将代码生成的思维链推理视为决策问题，提出SEER框架，包含多样化推理路径探索、推理质量感知模型训练、自适应思维链推理三个关键组件。

Result: 未提及具体结果

Conclusion: 未提及具体结论

Abstract: Code generation, the task of creating executable programs from natural
language requirements, has recently seen tremendous advances through
Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to
develop high-level reasoning plans before writing code. Recent research has
proposed various methods to enhance models' CoT reasoning for code generation
such as prompt engineering and supervised fine-tuning. However, existing
approaches still face three critical limitations: (1) limited exploration of
diverse reasoning paths, which constrains generalization across various
programming scenarios, (2) lack of quality assessment for intermediate
reasoning steps, which hampers the reliability of the generated plans and code,
and (3) the potential negative impact of "overthinking", potentially leading to
unnecessarily complex and incorrect solutions. To address these limitations, we
frame CoT code generation as a decision making problem and present SEER, a
SElf-Exploring deep Reasoning framework that enables accurate and adaptive
reasoning for code generation. SEER introduces three key components: (1)
Diverse reasoning path exploration, which aims at exploring diverse reasoning
paths and annotating intermediate steps without relying on manual experts or
closed-source proprietary models; (2) Reasoning quality-aware model training,
which trains a policy model for generating candidate reasoning steps and a
value model for assessing their quality; and (3) Adaptive CoT reasoning, which
dynamically switches between direct generation and step-by-step reasoning for
different problems.

</details>


### [377] [PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing](https://arxiv.org/abs/2510.17142)
*Xiaoxue Ren,Jun Wan,Yun Peng,Zhongxin Liu,Ming Liang,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: 提出用于项目级代码效率优化的混合框架Peace，通过实验验证其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码效率优化方法局限于函数级，未考虑函数间交互，代码编辑技术有项目级优化潜力但存在问题，需新方法。

Method: 提出Peace框架，包含依赖感知优化函数序列构建、有效关联编辑识别、效率优化编辑迭代三个关键阶段，构建PeacExec基准进行评估。

Result: 实验表明Peace的正确率达69.2%，优化率提升46.9%，执行效率加速比为0.840，在复杂任务中显著优于基线。

Conclusion: Peace框架在项目级代码效率优化方面有效，各组件和混合框架设计合理。

Abstract: Large Language Models (LLMs) have demonstrated significant capability in code
generation, but their potential in code efficiency optimization remains
underexplored. Previous LLM-based code efficiency optimization approaches
exclusively focus on function-level optimization and overlook interaction
between functions, failing to generalize to real-world development scenarios.
Code editing techniques show great potential for conducting project-level
optimization, yet they face challenges associated with invalid edits and
suboptimal internal functions. To address these gaps, we propose Peace, a novel
hybrid framework for Project-level code Efficiency optimization through
Automatic Code Editing, which also ensures the overall correctness and
integrity of the project. Peace integrates three key phases: dependency-aware
optimizing function sequence construction, valid associated edits
identification, and efficiency optimization editing iteration. To rigorously
evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark
comprising 146 real-world optimization tasks from 47 high-impact GitHub Python
projects, along with highly qualified test cases and executable environments.
Extensive experiments demonstrate Peace's superiority over the state-of-the-art
baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and
0.840 speedup in execution efficiency. Notably, our Peace outperforms all
baselines by significant margins, particularly in complex optimization tasks
with multiple functions. Moreover, extensive experiments are also conducted to
validate the contributions of each component in Peace, as well as the rationale
and effectiveness of our hybrid framework design.

</details>


### [378] [TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework](https://arxiv.org/abs/2510.17163)
*Shuzheng Gao,Eric John Li,Man Ho Lam,Jingyu Xiao,Yuxuan Wan,Chaozheng Wang,Ng Man Tik,Michael R. Lyu*

Main category: cs.SE

TL;DR: 现有基准在评估大模型软件工程场景下的可信度存在不足，本文提出TREAT评估框架，评估26个模型并揭示其优缺点。


<details>
  <summary>Details</summary>
Motivation: 现有基准评估大模型在软件工程场景下的可信度时存在任务范围有限、未纳入关键评估方面等问题，需新的评估框架。

Method: 提出TREAT评估框架，有多任务整体评估、多语言多模态评估、鲁棒性评估和严格评估方法四个主要改进。

Result: 评估26个模型，发现当前模型在编程任务上性能差异大，多模态语言模型在UI代码生成和编辑上有性能局限。

Conclusion: TREAT框架能有效评估模型在软件工程场景下的可信度，揭示模型优缺点。

Abstract: Large foundation models are fundamentally transforming the software
engineering landscape, demonstrating exceptional capabilities across diverse
tasks such as code generation, debugging, and testing. Despite this rapid
progress, a significant gap remains in how to comprehensively evaluate these
models' trustworthiness in real-world software engineering scenarios. Existing
benchmarks suffer from limited task scope and fail to incorporate critical
evaluation aspects such as the robustness and reliability of models. To bridge
this gap, we present an evaluation framework called TREAT (Code LLMs
Trustworthiness / Reliability Evaluation And Testing) that provides a holistic
assessment of model performance in code intelligence tasks. Our evaluation
framework addresses key limitations in existing approaches with four main
improvements: (1) Multi-Task Holistic Evaluation that spans diverse software
engineering activities rather than limited coding tasks; (2) Multi-Language and
Multi-Modality Assessment that extends beyond traditional single-language,
text-only benchmarks to include multi-modality coding tasks; (3) Robustness
Assessment that evaluates model reliability under semantically-preserving code
transformations; and (4) Rigorous Evaluation Methodology that enhances the
trustworthiness of evaluation results through diverse evaluation prompts and
adaptive solution extraction. Based on this evaluation framework, we assess 26
state-of-the-art models and uncover both their strengths and limitations,
yielding several key insights:(1) Current models show substantial performance
variation across programming tasks; (2) Multi-modal language models demonstrate
specific performance limitations in UI code generation and edit;

</details>


### [379] [Software Testing with Large Language Models: An Interview Study with Practitioners](https://arxiv.org/abs/2510.17164)
*Maria Deolinda Santana,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 研究调查软件测试人员使用大语言模型（LLM）的实践，提出初步指南，发现采用迭代反思过程，强调人工监督，为结构化使用LLM提供起点。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在软件测试中应用增长，但采用多依赖非正式实验，缺乏结构化指导，因此研究专业人员实践以提出初步指南。

Method: 对15名不同角色和领域的软件测试人员进行定性研究，通过半结构化访谈收集数据，用基于扎根理论的主题分析方法进行分析。

Result: 测试人员采用迭代反思过程，包括定义目标、应用提示工程策略等，强调人工监督和仔细验证，因LLM存在幻觉和推理不一致等局限。

Conclusion: LLM在软件测试中的采用在增长，但受实践演变和风险担忧影响，本研究为结构化使用提供起点，呼吁未来研究完善实践。

Abstract: \textit{Background:} The use of large language models in software testing is
growing fast as they support numerous tasks, from test case generation to
automation, and documentation. However, their adoption often relies on informal
experimentation rather than structured guidance. \textit{Aims:} This study
investigates how software testing professionals use LLMs in practice to propose
a preliminary, practitioner-informed guideline to support their integration
into testing workflows. \textit{Method:} We conducted a qualitative study with
15 software testers from diverse roles and domains. Data were collected through
semi-structured interviews and analyzed using grounded theory-based processes
focused on thematic analysis. \textit{Results:} Testers described an iterative
and reflective process that included defining testing objectives, applying
prompt engineering strategies, refining prompts, evaluating outputs, and
learning over time. They emphasized the need for human oversight and careful
validation, especially due to known limitations of LLMs such as hallucinations
and inconsistent reasoning. \textit{Conclusions:} LLM adoption in software
testing is growing, but remains shaped by evolving practices and caution around
risks. This study offers a starting point for structuring LLM use in testing
contexts and invites future research to refine these practices across teams,
tools, and tasks.

</details>


### [380] [OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development](https://arxiv.org/abs/2510.17184)
*Nicolas Robert,Fabien Gandon,Maxime Lefrançois*

Main category: cs.SE

TL;DR: 提出支持ACIMOV方法的OLIVAW工具用于本体开发，经测试有实用性、通用性和可复用性，还有模板仓库。


<details>
  <summary>Details</summary>
Motivation: 敏捷协作的本体设计需合适的持续验证工具确保本体符合开发者需求。

Method: 提出OLIVAW工具，依赖W3C标准，通过GitHub Composite Actions、pre - commit hooks或命令行界面辅助模块化本体开发。

Result: 在多个本体项目中测试了OLIVAW，确保其有用性、通用性和可复用性。

Conclusion: OLIVAW可辅助本体开发，且有模板仓库便于快速启动。

Abstract: Agile and collaborative approaches to ontologies design are crucial because
they contribute to making them userdriven, up-to-date, and able to evolve
alongside the systems they support, hence proper continuous validation tooling
is required to ensure ontologies match developers' requirements all along their
development. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV
Workflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C
Standards to assist the development of modular ontologies through GitHub
Composite Actions, pre-commit hooks, or a command line interface. OLIVAW was
tested on several ontology projects to ensure its usefulness, genericity and
reusability. A template repository is available for a quick start. OLIVAW is

</details>


### [381] [AdapTrack: Constrained Decoding without Distorting LLM's Output Intent](https://arxiv.org/abs/2510.17376)
*Yongmin Li,Jia Li,Ge Li,Zhi Jin*

Main category: cs.SE

TL;DR: 现有基于语言模型的代码生成工具存在生成代码不满足约束的问题，受限解码技术虽能解决约束问题但会扭曲模型输出意图。本文提出AdapTrack，通过回溯避免扭曲输出意图，实验表明其能显著提升性能且输出分布与模型分布一致。


<details>
  <summary>Details</summary>
Motivation: 解决受限解码技术会扭曲模型输出意图，导致生成代码不符合开发意图的问题。

Method: 提出AdapTrack，在生成过程中加入回溯机制。

Result: 在合成API完成数据集、真实世界API完成数据集、通用代码生成基准测试中，AdapTrack相比受限解码均有显著性能提升；在DSL问题实验中，生成结果更符合语言模型分布。

Conclusion: AdapTrack通过更好地遵循模型输出意图，能取得显著改进，且其生成的分布与模型分布一致，不会扭曲输出意图。

Abstract: Language model-based code generation and completion tools have been widely
adopted, but they may sometimes produce code that does not meet necessary
constraints, such as syntactic correctness or API existence. Constrained
decoding techniques are developed to help the model generate code adhering to
the constraints by greedily eliminating generation options that violate
constraints at each step of the generation process. However, there is a severe
limitation of constrained decoding, that it distorts the model's output intent,
forcing it to produce code that may satisfy the constraint but does not match
the development intent and is therefore incorrect. In response to this
challenge, we propose AdapTrack. By incorporating backtracking into the
generation process, AdapTrack avoids distorting the output intent of the model,
thereby producing results that are not only constraint-compliant but also more
semantically aligned with model's output intent. On our synthetic API
completion dataset, AdapTrack can achieve up to 360.87% improvement compared to
constrained decoding; on the real-world API completion dataset we collect that
exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over
constrained decoding; in general code genration benchmarks, compared to
constrained decoding, AdapTrack can achieve up to 7.84% improvement on
HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by
better adhering to the model's output intent, AdapTrack can achieve significant
improvements. We provide a theoretical proof that the distribution produced by
AdapTrack aligns with the model's distribution given the generated tokens,
thereby ensuring that the model's output intent is not distorted. Experiments
on DSL problems show that, compared to existing methods, our approach can
provide generation results that are more consistent with the language model's
distribution.

</details>


### [382] [Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff](https://arxiv.org/abs/2510.17430)
*Kuniaki Kudo,Sherine Devi*

Main category: cs.SE

TL;DR: 为解决日本2025悬崖问题及内部遗留系统挑战，开发可扩展CI/CD管道，介绍设计架构与实现及用例，能降低维护成本推动数字化转型。


<details>
  <summary>Details</summary>
Motivation: 日本2025悬崖问题中遗留核心IT系统面临服务到期、维护成本高、更新替换难等问题，影响数字化转型；Asahi公司内部遗留系统也存在手动维护和QA环境有限等挑战。

Method: 开发并实施可扩展CI/CD管道，集成GitHub进行源代码控制和分支管理、Jenkins进行管道自动化、亚马逊云服务构建可扩展环境、Docker进行环境容器化。

Result: 开发者可在自己环境中自由安全测试维护程序和试验新技术。

Conclusion: 可扩展CI/CD管道能降低维护成本，推动数字化转型。

Abstract: We have developed a Scalable CI/CD Pipeline to address internal challenges
related to Japan 2025 cliff problem, a critical issue where the mass end of
service life of legacy core IT systems threatens to significantly increase the
maintenance cost and black box nature of these system also leads to difficult
update moreover replace, which leads to lack of progress in Digital
Transformation (DX). If not addressed, Japan could potentially lose up to 12
trillion yen per year after 2025, which is 3 times more than the cost in
previous years. Asahi also faced the same internal challenges regarding legacy
system, where manual maintenance workflows and limited QA environment have left
critical systems outdated and difficult to update. Middleware and OS version
have remained unchanged for years, leading to now its nearing end of service
life which require huge maintenance cost and effort to continue its operation.
To address this problem, we have developed and implemented a Scalable CI/CD
Pipeline where isolated development environments can be created and deleted
dynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate
GitHub for source code control and branching, Jenkins for pipeline automation,
Amazon Web Services for scalable environment, and Docker for environment
containerization. This paper presents the design and architecture of the
Scalable CI/CD Pipeline, with the implementation along with some use cases.
Through Scalable CI/CD, developers can freely and safely test maintenance
procedures and do experiments with new technology in their own environment,
reducing maintenance cost and drive Digital Transformation (DX).
  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [383] [FinFlowRL: An Imitation-Reinforcement Learning Framework for Adaptive Stochastic Control in Finance](https://arxiv.org/abs/2510.15883)
*Yang Li,Zhi Chen*

Main category: q-fin.CP

TL;DR: 传统金融随机控制方法在现实市场表现不佳，引入FinFlowRL框架，通过预训练和微调优化生成过程，能在不同市场条件下超越单个优化专家。


<details>
  <summary>Details</summary>
Motivation: 传统随机控制方法依赖简化假设和程式化框架，在现实市场表现不佳，需要新的金融最优随机控制框架。

Method: 预训练自适应元策略，从多个专家策略学习，在噪声空间通过强化学习微调，采用动作分块生成动作序列。

Result: FinFlowRL在不同市场条件下始终优于单个优化专家。

Conclusion: FinFlowRL是一种有效的金融最优随机控制框架。

Abstract: Traditional stochastic control methods in finance struggle in real world
markets due to their reliance on simplifying assumptions and stylized
frameworks. Such methods typically perform well in specific, well defined
environments but yield suboptimal results in changed, non stationary ones. We
introduce FinFlowRL, a novel framework for financial optimal stochastic
control. The framework pretrains an adaptive meta policy learning from multiple
expert strategies, then finetunes through reinforcement learning in the noise
space to optimize the generative process. By employing action chunking
generating action sequences rather than single decisions, it addresses the non
Markovian nature of markets. FinFlowRL consistently outperforms individually
optimized experts across diverse market conditions.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [384] [Spiking Neural Network for Cross-Market Portfolio Optimization in Financial Markets: A Neuromorphic Computing Approach](https://arxiv.org/abs/2510.15921)
*Amarendra Mohan,Ameer Tamoor Khan,Shuai Li,Xinwei Cao,Zhibin Li*

Main category: q-fin.PM

TL;DR: 研究应用脉冲神经网络（SNNs）进行跨市场投资组合优化，实验表明SNN框架优于人工神经网络基准。


<details>
  <summary>Details</summary>
Motivation: 传统人工神经网络在处理大规模多市场数据时计算开销大且缺乏时间处理能力，需要新方法进行跨市场投资组合优化。

Method: 利用神经形态计算原理，收集印度和美国市场五年股票数据，将泄漏积分放电神经元动力学与自适应阈值、脉冲时序依赖可塑性和侧向抑制相结合，通过层次聚类降维，采用基于群体的脉冲编码和多种解码策略。

Result: SNN框架在风险调整回报、降低波动性和计算效率上优于ANN基准。

Conclusion: 神经形态计算在全球金融市场的投资组合优化中具有潜力。

Abstract: Cross-market portfolio optimization has become increasingly complex with the
globalization of financial markets and the growth of high-frequency,
multi-dimensional datasets. Traditional artificial neural networks, while
effective in certain portfolio management tasks, often incur substantial
computational overhead and lack the temporal processing capabilities required
for large-scale, multi-market data. This study investigates the application of
Spiking Neural Networks (SNNs) for cross-market portfolio optimization,
leveraging neuromorphic computing principles to process equity data from both
the Indian (Nifty 500) and US (S&P 500) markets. A five-year dataset comprising
approximately 1,250 trading days of daily stock prices was systematically
collected via the Yahoo Finance API. The proposed framework integrates Leaky
Integrate-andFire neuron dynamics with adaptive thresholding,
spike-timingdependent plasticity, and lateral inhibition to enable event-driven
processing of financial time series. Dimensionality reduction is achieved
through hierarchical clustering, while populationbased spike encoding and
multiple decoding strategies support robust portfolio construction under
realistic trading constraints, including cardinality limits, transaction costs,
and adaptive risk aversion. Experimental evaluation demonstrates that the
SNN-based framework delivers superior risk-adjusted returns and reduced
volatility compared to ANN benchmarks, while substantially improving
computational efficiency. These findings highlight the promise of neuromorphic
computation for scalable, efficient, and robust portfolio optimization across
global financial markets.

</details>


### [385] [Aligning Language Models with Investor and Market Behavior for Financial Recommendations](https://arxiv.org/abs/2510.15993)
*Fernando Spadea,Oshani Seneviratne*

Main category: q-fin.PM

TL;DR: 提出金融推荐框架FLARKO，结合LLMs、KGs和KTO生成资产推荐，开发不同架构并评估，在FAR - Trans数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有金融推荐系统未考虑关键行为和监管因素，导致推荐与用户偏好不符等问题。

Method: 提出FLARKO框架，将用户交易历史和资产趋势编码为结构化KGs为LLM提供上下文，开发并评估集中式CenFLARKO和联邦式FedFLARKO架构，结合KTO微调LLM。

Result: 在FAR - Trans数据集上，FLARKO在行为一致性和联合盈利能力上始终优于现有推荐基线，且具有可解释性和资源高效性。

Conclusion: FLARKO是有效的金融资产推荐框架，首次结合KTO微调LLM，首次在联邦学习中用结构化KGs处理行为金融数据。

Abstract: Most financial recommendation systems often fail to account for key
behavioral and regulatory factors, leading to advice that is misaligned with
user preferences, difficult to interpret, or unlikely to be followed. We
present FLARKO (Financial Language-model for Asset Recommendation with
Knowledge-graph Optimization), a novel framework that integrates Large Language
Models (LLMs), Knowledge Graphs (KGs), and Kahneman-Tversky Optimization (KTO)
to generate asset recommendations that are both profitable and behaviorally
aligned. FLARKO encodes users' transaction histories and asset trends as
structured KGs, providing interpretable and controllable context for the LLM.
To demonstrate the adaptability of our approach, we develop and evaluate both a
centralized architecture (CenFLARKO) and a federated variant (FedFLARKO). To
our knowledge, this is the first demonstration of combining KTO for fine-tuning
of LLMs for financial asset recommendation. We also present the first use of
structured KGs to ground LLM reasoning over behavioral financial data in a
federated learning (FL) setting. Evaluated on the FAR-Trans dataset, FLARKO
consistently outperforms state-of-the-art recommendation baselines on
behavioral alignment and joint profitability, while remaining interpretable and
resource-efficient.

</details>


### [386] [3S-Trader: A Multi-LLM Framework for Adaptive Stock Scoring, Strategy, and Selection in Portfolio Optimization](https://arxiv.org/abs/2510.17393)
*Kefan Chen,Hussain Ahmad,Diksha Goel,Claudia Szabo*

Main category: q-fin.PM

TL;DR: 提出无训练框架3S - Trader用于股票投资组合构建，在多股票宇宙中评估，相比现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有股票交易方法多聚焦单股票交易，缺乏多股票组合推理能力和策略灵活性，难以适应市场变化。

Method: 提出3S - Trader框架，包含评分、策略和选择模块。评分模块总结股票信号，策略模块分析历史策略和市场条件生成优化策略，选择模块依策略构建投资组合。

Result: 在四个不同股票宇宙中评估，在DJIA成分股上实现131.83%的最高累计回报，夏普比率0.31，卡尔玛比率11.84，在其他板块也表现稳定。

Conclusion: 3S - Trader框架能有效解决现有方法的不足，在股票投资组合构建中表现出色。

Abstract: Large Language Models (LLMs) have recently gained popularity in stock trading
for their ability to process multimodal financial data. However, most existing
methods focus on single-stock trading and lack the capacity to reason over
multiple candidates for portfolio construction. Moreover, they typically lack
the flexibility to revise their strategies in response to market shifts,
limiting their adaptability in real-world trading. To address these challenges,
we propose 3S-Trader, a training-free framework that incorporates scoring,
strategy, and selection modules for stock portfolio construction. The scoring
module summarizes each stock's recent signals into a concise report covering
multiple scoring dimensions, enabling efficient comparison across candidates.
The strategy module analyzes historical strategies and overall market
conditions to iteratively generate an optimized selection strategy. Based on
this strategy, the selection module identifies and assembles a portfolio by
choosing stocks with higher scores in relevant dimensions. We evaluate our
framework across four distinct stock universes, including the Dow Jones
Industrial Average (DJIA) constituents and three sector-specific stock sets.
Compared with existing multi-LLM frameworks and time-series-based baselines,
3S-Trader achieves the highest accumulated return of 131.83% on DJIA
constituents with a Sharpe ratio of 0.31 and Calmar ratio of 11.84, while also
delivering consistently strong results across other sectors.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [387] [Probability equivalent level for CoVaR and VaR in bivariate Student-\textit{t} copulas with application to foreign exchange risk monitoring](https://arxiv.org/abs/2510.15934)
*Daniela I. Flores-Silva,Miguel A. Sordo,Alfonso Suárez-Llorens*

Main category: q-fin.RM

TL;DR: 本文将PELCoV方法扩展至Student - t copula建模的双变量风险，动态实施该方法并应用于外汇市场，结果表明扩展框架能在金融压力时期检测风险低估早期迹象。


<details>
  <summary>Details</summary>
Motivation: 放松早期方法强依赖假设，增强框架捕捉尾部依赖和不对称联动的能力，追踪风险溢出随时间的变化。

Method: 将PELCoV方法扩展至Student - t copula建模的双变量风险，在静态理论基础上动态实施。

Result: 扩展的PELCoV框架能在金融压力时期检测到风险低估的早期迹象。

Conclusion: 扩展的PELCoV框架具有检测金融压力时期风险低估早期迹象的潜力，有实际应用价值。

Abstract: We extend the "probability-equivalent level of VaR and CoVaR" (PELCoV)
methodology to accommodate bivariate risks modeled by a Student-t copula,
relaxing the strong dependence assumptions of earlier approaches and enhancing
the framework's ability to capture tail dependence and asymmetric co-movements.
While the theoretical results are developed in a static setting, we implement
them dynamically to track evolving risk spillovers over time. We illustrate the
practical relevance of our approach through an application to the foreign
exchange market, monitoring the USD/GBP exchange rate with the USD/EUR series
as an auxiliary early warning indicator over the period 1999-2024. Our results
highlight the potential of the extended PELCoV framework to detect early signs
of risk underestimation during periods of financial stress.

</details>


### [388] [Tail-Safe Stochastic-Control SPX-VIX Hedging: A White-Box Bridge Between AI Sensitivities and Arbitrage-Free Market Dynamics](https://arxiv.org/abs/2510.15937)
*Jian'an Zhang*

Main category: q-fin.RM

TL;DR: 提出白盒、风险敏感框架联合对冲SPX和VIX风险，介绍市场和控制侧方法，证明相关性质，在模拟环境有良好表现。


<details>
  <summary>Details</summary>
Motivation: 在交易成本和市场制度转换下，联合对冲SPX和VIX风险。

Method: 市场侧整合SSVI隐含波动率曲面和VIX计算，通过Dupire局部波动率提取器连接价格与动态；控制侧将对冲问题转化为带控制障碍函数的二次规划问题，有执行门和尾部安全升级措施。

Result: 证明了每步QP的存在唯一性、KKT正则性等性质，在模拟环境中控制器降低预期损失、抑制换手率，教师曲面构建使指数残差小且稳定。

Conclusion: 所提出的框架在联合对冲SPX和VIX风险方面有效，能在控制风险的同时减少交易成本。

Abstract: We present a white-box, risk-sensitive framework for jointly hedging SPX and
VIX exposures under transaction costs and regime shifts. The approach couples
an arbitrage-free market teacher with a control layer that enforces safety as
constraints. On the market side, we integrate an SSVI-based implied-volatility
surface and a Cboe-compliant VIX computation (including wing pruning and 30-day
interpolation), and connect prices to dynamics via a clipped,
convexity-preserving Dupire local-volatility extractor. On the control side, we
pose hedging as a small quadratic program with control-barrier-function (CBF)
boxes for inventory, rate, and tail risk; a sufficient-descent execution gate
that trades only when risk drop justifies cost; and three targeted tail-safety
upgrades: a correlation/expiry-aware VIX weight, guarded no-trade bands, and
expiry-aware micro-trade thresholds with cooldown. We prove
existence/uniqueness and KKT regularity of the per-step QP, forward invariance
of safety sets, one-step risk descent when the gate opens, and no chattering
with bounded trade rates. For the dynamics layer, we establish positivity and
second-order consistency of the discrete Dupire estimator and give an
index-coherence bound linking the teacher VIX to a CIR-style proxy with
explicit quadrature and projection errors. In a reproducible synthetic
environment mirroring exchange rules and execution frictions, the controller
reduces expected shortfall while suppressing nuisance turnover, and the
teacher-surface construction keeps index-level residuals small and stable.

</details>


### [389] [A high-frequency approach to Realized Risk Measures](https://arxiv.org/abs/2510.16526)
*Federico Gatta,Fabrizio Lillo,Piero Mazzarisi*

Main category: q-fin.RM

TL;DR: 提出Realized Risk Measures (RRM)方法用高频金融数据估计VaR和ES，经实验优于RQ方法。


<details>
  <summary>Details</summary>
Motivation: 原Realized Quantile (RQ)方法有局限性，需新方法估计VaR和ES。

Method: 用从属过程转换日内回报，用移动平均过程过滤微观结构效应，拟合厚尾分布，通过特征函数或蒙特卡罗模拟外推低频回报分布。

Result: 通过大量数值模拟和对18只美国股票的实证研究，RRM方法在样本内估计和样本外风险预测方面均优于RQ方法。

Conclusion: RRM方法在估计VaR和ES方面表现更优。

Abstract: We propose a new approach, termed Realized Risk Measures (RRM), to estimate
Value-at-Risk (VaR) and Expected Shortfall (ES) using high-frequency financial
data. It extends the Realized Quantile (RQ) approach proposed by Dimitriadis
and Halbleib by lifting the assumption of return self-similarity, which
displays some limitations in describing empirical data. More specifically, as
the RQ, the RRM method transforms intra-day returns in intrinsic time using a
subordinator process, in order to capture the inhomogeneity of trading activity
and/or volatility clustering. Then, microstructural effects resulting in
non-zero autocorrelation are filtered out using a suitable moving average
process. Finally, a fat-tailed distribution is fitted on the cleaned intra-day
returns. The return distribution at low frequency (daily) is then extrapolated
via either a characteristic function approach or Monte Carlo simulations. VaR
and ES are estimated as the quantile and the tail mean of the distribution,
respectively. The proposed approach is benchmarked against the RQ through
several experiments. Extensive numerical simulations and an empirical study on
18 US stocks show the outperformance of our method, both in terms of the
in-sample estimated risk measures and in the out-of-sample risk forecasting

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [390] [Bitcoin Price Forecasting Based on Hybrid Variational Mode Decomposition and Long Short Term Memory Network](https://arxiv.org/abs/2510.15900)
*Emmanuel Boadi*

Main category: q-fin.ST

TL;DR: 提出VMD+LSTM混合深度学习模型预测比特币价格，优于标准LSTM并提供30天可靠预测。


<details>
  <summary>Details</summary>
Motivation: 比特币价格频繁波动，需要有效预测模型。

Method: 用VMD将比特币价格序列分解为IMFs，用LSTM对每个IMF建模，聚合预测结果；与标准LSTM对比分析。

Result: 混合VMD+LSTM模型在RMSE、MAE和R2等评估指标上优于标准LSTM，能提供可靠30天预测。

Conclusion: VMD+LSTM混合模型预测比特币价格效果更好。

Abstract: This study proposes a hybrid deep learning model for forecasting the price of
Bitcoin, as the digital currency is known to exhibit frequent fluctuations. The
models used are the Variational Mode Decomposition (VMD) and the Long
Short-Term Memory (LSTM) network. First, VMD is used to decompose the original
Bitcoin price series into Intrinsic Mode Functions (IMFs). Each IMF is then
modeled using an LSTM network to capture temporal patterns more effectively.
The individual forecasts from the IMFs are aggregated to produce the final
prediction of the original Bitcoin Price Series. To determine the prediction
power of the proposed hybrid model, a comparative analysis was conducted
against the standard LSTM. The results confirmed that the hybrid VMD+LSTM model
outperforms the standard LSTM across all the evaluation metrics, including
RMSE, MAE and R2 and also provides a reliable 30-day forecast.

</details>


### [391] [Quantum and Classical Machine Learning in Decentralized Finance: Comparative Evidence from Multi-Asset Backtesting of Automated Market Makers](https://arxiv.org/abs/2510.15903)
*Chi-Sheng Chen,Aidan Hung-Wen Tsai*

Main category: q-fin.ST

TL;DR: 本文通过对10个模型在多加密货币资产上回测，比较量子机器学习和经典机器学习在AMM和DeFi交易策略中的表现，发现混合量子模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 对量子机器学习（QML）和经典机器学习（CML）在自动化做市商（AMM）和去中心化金融（DeFi）交易策略中的表现进行全面实证比较。

Method: 对10个涵盖经典ML、纯量子、混合量子 - 经典和transformer的模型在多加密货币资产上进行广泛回测。

Result: 混合量子模型平均回报11.2%、夏普比率1.42，经典ML模型平均回报9.8%、夏普比率1.47；QASA Sequence混合模型回报13.99%、夏普比率1.76最高。

Conclusion: 量子 - 经典混合方法在AMM和DeFi交易策略中有潜力。

Abstract: This study presents a comprehensive empirical comparison between quantum
machine learning (QML) and classical machine learning (CML) approaches in
Automated Market Makers (AMM) and Decentralized Finance (DeFi) trading
strategies through extensive backtesting on 10 models across multiple
cryptocurrency assets. Our analysis encompasses classical ML models (Random
Forest, Gradient Boosting, Logistic Regression), pure quantum models (VQE
Classifier, QNN, QSVM), hybrid quantum-classical models (QASA Hybrid, QASA
Sequence, QuantumRWKV), and transformer models. The results demonstrate that
hybrid quantum models achieve superior overall performance with 11.2\% average
return and 1.42 average Sharpe ratio, while classical ML models show 9.8\%
average return and 1.47 average Sharpe ratio. The QASA Sequence hybrid model
achieves the highest individual return of 13.99\% with the best Sharpe ratio of
1.76, demonstrating the potential of quantum-classical hybrid approaches in AMM
and DeFi trading strategies.

</details>


### [392] [A three-step machine learning approach to predict market bubbles with financial news](https://arxiv.org/abs/2510.16636)
*Abraham Atsiwo*

Main category: q-fin.ST

TL;DR: 本文提出三步机器学习框架，结合金融新闻情绪与宏观经济指标预测标普500股市泡沫，经评估效果显著。


<details>
  <summary>Details</summary>
Motivation: 为投资者、监管者和政策制定者提供股市泡沫早期预警，减轻系统性金融风险。

Method: 第一步用右尾单位根检验识别泡沫期；第二步用NLP从金融新闻提取情绪特征；第三步用集成学习方法基于情绪和宏观指标预测泡沫，用k折交叉验证评估并与基准算法对比。

Result: 所提三步集成方法显著提高预测准确性和稳健性。

Conclusion: 该方法能为相关人员减轻系统性金融风险提供有价值的早期预警。

Abstract: This study presents a three-step machine learning framework to predict
bubbles in the S&P 500 stock market by combining financial news sentiment with
macroeconomic indicators. Building on traditional econometric approaches, the
proposed approach predicts bubble formation by integrating textual and
quantitative data sources. In the first step, bubble periods in the S&P 500
index are identified using a right-tailed unit root test, a widely recognized
real-time bubble detection method. The second step extracts sentiment features
from large-scale financial news articles using natural language processing
(NLP) techniques, which capture investors' expectations and behavioral
patterns. In the final step, ensemble learning methods are applied to predict
bubble occurrences based on high sentiment-based and macroeconomic predictors.
Model performance is evaluated through k-fold cross-validation and compared
against benchmark machine learning algorithms. Empirical results indicate that
the proposed three-step ensemble approach significantly improves predictive
accuracy and robustness, providing valuable early warning insights for
investors, regulators, and policymakers in mitigating systemic financial risks.

</details>


### [393] [Investor Sentiment and Market Movements: A Granger Causality Perspective](https://arxiv.org/abs/2510.15915)
*Tamoghna Mukherjee*

Main category: q-fin.ST

TL;DR: 研究运用格兰杰因果推断探究股票收盘价指数与情绪得分关系，假设检验有积极结果。


<details>
  <summary>Details</summary>
Motivation: 股票市场受投资者情绪影响大，需明确情绪变化与股价变化的先后关系。

Method: 运用格兰杰因果推断研究收盘价指数和情绪得分的关系，并进行假设检验。

Result: 通过假设检验得到积极响应。

Conclusion: 格兰杰因果推断能有效分析股票收盘价指数和情绪得分关系，两者存在正向联系。

Abstract: The stock market is heavily influenced by investor sentiment, which can drive
buying or selling behavior. Sentiment analysis helps in gauging the overall
sentiment of market participants towards a particular stock or the market as a
whole. Positive sentiment often leads to increased buying activity and vice
versa. Granger causality can be applied to ascertain whether changes in
sentiment precede changes in stock prices.The study is focused on this aspect
and tries to understand the relationship between close price index and
sentiment score with the help of Granger causality inference. The study finds a
positive response through hypothesis testing.

</details>


### [394] [Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia](https://arxiv.org/abs/2510.16066)
*Chun Chet Ng,Wei Zeng Low,Yin Yin Boon*

Main category: q-fin.ST

TL;DR: 研究探讨银行对账单数据用于新兴市场信用评估以促进金融普惠，提出现金流承保流程，引入数据集，评估模型，结果显示该数据可提升模型性能，还将发布匿名数据集。


<details>
  <summary>Details</summary>
Motivation: 马来西亚MSMEs占比高但融资难，新企业常被传统信贷市场排除，需寻找替代数据源促进金融普惠。

Method: 提出基于现金流的承保流程，利用银行对账单数据进行端到端数据提取和机器学习信用评分；引入马来西亚贷款申请数据集；开发并评估基于申请信息和银行交易特征的信用评分模型。

Result: 使用银行对账单数据提升了所有模型在数据集上的性能。

Conclusion: 银行对账单数据可改善新贷款MSMEs的信用评分，打算发布匿名数据集推动马来西亚新兴经济中MSMEs金融普惠研究。

Abstract: Despite accounting for 96.1% of all businesses in Malaysia, access to
financing remains one of the most persistent challenges faced by Micro, Small,
and Medium Enterprises (MSMEs). Newly established or young businesses are often
excluded from formal credit markets as traditional underwriting approaches rely
heavily on credit bureau data. This study investigates the potential of bank
statement data as an alternative data source for credit assessment to promote
financial inclusion in emerging markets. Firstly, we propose a cash flow-based
underwriting pipeline where we utilise bank statement data for end to end data
extraction and machine learning credit scoring. Secondly, we introduce a novel
dataset of 611 loan applicants from a Malaysian lending institution. Thirdly,
we develop and evaluate credit scoring models based on application information
and bank transaction-derived features. Empirical results show that the use of
such data boosts the performance of all models on our dataset, which can
improve credit scoring for new-to-lending MSMEs. Lastly, we intend to release
the anonymised bank transaction dataset to facilitate further research on MSMEs
financial inclusion within Malaysia's emerging economy.

</details>


### [395] [Comparing LLMs for Sentiment Analysis in Financial Market News](https://arxiv.org/abs/2510.15929)
*Lucas Eduardo Pereira Teles,Carlos M. S. Figueiredo*

Main category: q-fin.ST

TL;DR: 文章对大语言模型在金融市场新闻情感分析任务中进行对比研究，发现大模型多数情况下优于经典模型。


<details>
  <summary>Details</summary>
Motivation: 分析大语言模型在金融领域重要自然语言处理任务（金融市场新闻情感分析）中的性能差异。

Method: 将大语言模型与经典方法进行比较，量化各模型或方法的优势。

Result: 大语言模型在绝大多数情况下优于经典模型。

Conclusion: 大语言模型在金融市场新闻情感分析任务中表现更优。

Abstract: This article presents a comparative study of large language models (LLMs) in
the task of sentiment analysis of financial market news. This work aims to
analyze the performance difference of these models in this important natural
language processing task within the context of finance. LLM models are compared
with classical approaches, allowing for the quantification of the benefits of
each tested model or approach. Results show that large language models
outperform classical models in the vast majority of cases.

</details>


### [396] [Dynamic Factor Analysis of Price Movements in the Philippine Stock Exchange](https://arxiv.org/abs/2510.15938)
*Brian Godwin Lim,Dominic Dayta,Benedict Ryan Tiu,Renzo Roel Tan,Len Patrick Dominic Garces,Kazushi Ikeda*

Main category: q-fin.ST

TL;DR: 本文利用计量经济学文献研究动态因子模型，分析提取的载荷和公共因子以理解股价动态，在菲律宾股市和GDP增长率预测中取得成果，强调了动态因子分析的价值。


<details>
  <summary>Details</summary>
Motivation: 股票市场动态复杂，需要能有效解释其复杂性的模型，且以往对动态因子模型用于理解股价动态的研究较少。

Method: 运用卡尔曼方法和极大似然估计，以资本资产定价模型进行验证。

Result: 单因子模型提取出类似综合指数的系统或市场动态公共因子，双因子模型提取出代表市场趋势和波动的公共因子；用于预测菲律宾GDP增长率时，样本外预测误差降低超34%。

Conclusion: 动态因子分析有助于深入理解市场价格运动动态。

Abstract: The intricate dynamics of stock markets have led to extensive research on
models that are able to effectively explain their inherent complexities. This
study leverages the econometrics literature to explore the dynamic factor model
as an interpretable model with sufficient predictive capabilities for capturing
essential market phenomena. Although the model has been extensively applied for
predictive purposes, this study focuses on analyzing the extracted loadings and
common factors as an alternative framework for understanding stock price
dynamics. The results reveal novel insights into traditional market theories
when applied to the Philippine Stock Exchange using the Kalman method and
maximum likelihood estimation, with subsequent validation against the capital
asset pricing model. Notably, a one-factor model extracts a common factor
representing systematic or market dynamics similar to the composite index,
whereas a two-factor model extracts common factors representing market trends
and volatility. Furthermore, an application of the model for nowcasting the
growth rates of the Philippine gross domestic product highlights the potential
of the extracted common factors as viable real-time market indicators, yielding
over a 34% decrease in the out-of-sample prediction error. Overall, the results
underscore the value of dynamic factor analysis in gaining a deeper
understanding of market price movement dynamics.

</details>


### [397] [Intrinsic Geometry of the Stock Market from Graph Ricci Flow](https://arxiv.org/abs/2510.15942)
*Bhargavi Srinivasan*

Main category: q-fin.ST

TL;DR: 利用离散Ollivier - Ricci图曲率与Ricci流，通过纳斯达克100指数的经验相关图研究金融市场内在几何，开发处理奇点的技术并构建算法检测金融市场特征。


<details>
  <summary>Details</summary>
Motivation: 通过经验相关图研究金融市场的内在几何，解决高度连接几何带来的挑战以检测金融市场的隐藏层次、社区行为和聚类。

Method: 使用离散Ollivier - Ricci图曲率与Ricci流，以完全连接图的曲率行为和下界为起点，开发处理经验图Ricci流中颈缩奇点的技术。

Result: 开发了处理经验图Ricci流中颈缩奇点的技术。

Conclusion: 构建的算法能够利用图的内在几何流产生的曲率，在高度连接几何带来挑战的情况下，检测金融市场的隐藏层次、社区行为和聚类。

Abstract: We use the discrete Ollivier-Ricci graph curvature with Ricci flow to examine
the intrinsic geometry of financial markets through the empirical correlation
graph of the NASDAQ 100 index. Our main result is the development of a
technique to perform surgery on the neckpinch singularities that form during
the Ricci flow of the empirical graph, using the behavior and the lower bound
of curvature of the fully connected graph as a starting point. We construct an
algorithm that uses the curvature generated by intrinsic geometric flow of the
graph to detect hidden hierarchies, community behavior, and clustering in
financial markets despite the underlying challenges posed by a highly connected
geometry.

</details>


### [398] [Convolutional Attention in Betting Exchange Markets](https://arxiv.org/abs/2510.16008)
*Rui Gonçalves,Vitor Miguel Ribeiro,Roman Chertovskih,António Pedro Aguiar*

Main category: q-fin.ST

TL;DR: 本文实现外汇市场价格短期预测系统，以英国赛马市场为例，引入创新卷积注意力机制和填充方法，用监督学习，提出端到端框架，创新提升分类任务表现。


<details>
  <summary>Details</summary>
Motivation: 实现外汇市场价格短期预测系统，推动卷积注意力机制和填充方法在多变量时间序列问题上的发展。

Method: 引入创新卷积注意力机制应用于多种循环神经网络和二维卷积循环神经网络层，提出新的卷积层填充方法，采用标准监督学习方法，提出端到端框架。

Result: 所有提出的创新对所考察的分类任务的性能指标有积极影响。

Conclusion: 所提创新推进了卷积注意力机制和填充方法在多变量时间序列问题上的现有技术水平。

Abstract: This study presents the implementation of a short-term forecasting system for
price movements in exchange markets, using market depth data and a systematic
procedure to enable a fully automated trading system. The case study focuses on
the UK to Win Horse Racing market during the pre-live stage on the world's
leading betting exchange, Betfair. Innovative convolutional attention
mechanisms are introduced and applied to multiple recurrent neural networks and
bi-dimensional convolutional recurrent neural network layers. Additionally, a
novel padding method for convolutional layers is proposed, specifically
designed for multivariate time series processing. These innovations are
thoroughly detailed, along with their execution process. The proposed
architectures follow a standard supervised learning approach, involving model
training and subsequent testing on new data, which requires extensive
pre-processing and data analysis. The study also presents a complete end-to-end
framework for automated feature engineering and market interactions using the
developed models in production. The key finding of this research is that all
proposed innovations positively impact the performance metrics of the
classification task under examination, thereby advancing the current
state-of-the-art in convolutional attention mechanisms and padding methods
applied to multivariate time series problems.

</details>


### [399] [Institutional Differences, Crisis Shocks, and Volatility Structure: A By-Window EGARCH/TGARCH Analysis of ASEAN Stock Markets](https://arxiv.org/abs/2510.16010)
*Junlin Yang*

Main category: q-fin.ST

TL;DR: 研究新兴亚洲股市波动动态受制度差异和外部危机影响，用EGARCH(1,1)和TGARCH(1,1)模型分析印尼、马来西亚和菲律宾股市，发现危机放大波动，制度稳健性影响恢复速度。


<details>
  <summary>Details</summary>
Motivation: 以往研究多针对单一市场或静态时期，缺乏在一个GARCH框架内将制度比较与多危机动态结合的研究，填补此空白。

Method: 使用2010 - 2024年印尼、马来西亚和菲律宾的日股票指数收益率，采用EGARCH(1,1)和TGARCH(1,1)模型，按窗口设计，将样本分为不同危机阶段和稳定阶段。

Result: 三个市场均有强波动持续性和厚尾回报；危机期间持续性、不对称性和尾部厚度增加；危机后参数向冲击前水平回归；马来西亚制度成熟起缓冲作用，菲律宾市场结构薄弱延长不稳定。

Conclusion: 危机放大波动结构，制度稳健性决定恢复速度，结果为政策制定提供指导以减少全球冲击时的波动持续性。

Abstract: This study examines how institutional differences and external crises shape
volatility dynamics in emerging Asian stock markets. Using daily stock index
returns for Indonesia, Malaysia, and the Philippines from 2010 to 2024, we
estimate EGARCH(1,1) and TGARCH(1,1) models in a by-window design. The sample
is split into the 2013 Taper Tantrum, the 2020-2021 COVID-19 period, the
2022-2023 rate-hike cycle, and tranquil phases. Prior work typically studies a
single market or a static period; to our knowledge no study unifies
institutional comparison with multi-crisis dynamics within one GARCH framework.
We address this gap and show that all three markets display strong volatility
persistence and fat-tailed returns. During crises both persistence and
asymmetry increase, while tail thickness rises, implying more frequent extreme
moves. After crises, parameters revert toward pre-shock levels. Cross-country
evidence indicates a buffering role of institutional maturity: Malaysias
stronger regulatory and information systems dampen amplification and speed
recovery, whereas the Philippines thinner market structure prolongs
instability. We conclude that crises amplify volatility structures, while
institutional robustness governs recovery speed. The results provide policy
guidance on transparency, macroprudential communication, and liquidity support
to reduce volatility persistence during global shocks.

</details>


### [400] [Sentiment and Volatility in Financial Markets: A Review of BERT and GARCH Applications during Geopolitical Crises](https://arxiv.org/abs/2510.16503)
*Domenica Mino,Cillian Williamson*

Main category: q-fin.ST

TL;DR: 研究运用AI和计量经济模型分析俄乌战争新闻情绪与股市波动关系，发现负面新闻情绪与市场稳定性负相关。


<details>
  <summary>Details</summary>
Motivation: 探索俄乌战争新闻情绪与股市波动的关系，运用AI技术理解公众情绪和金融市场行为的复杂关系。

Method: 使用针对金融语言微调的BERT模型分析新闻文章，提取情绪分数，应用带Student - t分布的GARCH模型。

Result: 负面新闻情绪与市场稳定性存在显著负相关，悲观战争报道会增加标普500指数的波动性。

Conclusion: 展示了AI和自然语言处理与计量经济建模结合可评估实时市场动态，为地缘政治危机中的金融风险分析提供工具。

Abstract: Artificial intelligence techniques have increasingly been applied to
understand the complex relationship between public sentiment and financial
market behaviour. This study explores the relationship between the sentiment of
news related to the Russia-Ukraine war and the volatility of the stock market.
A comprehensive dataset of news articles from major US platforms, published
between January 1 and July 17, 2024, was analysed using a fine-tuned
Bidirectional Encoder Representations from Transformers (BERT) model adapted
for financial language. We extracted sentiment scores and applied a Generalised
Autoregressive Conditional Heteroscedasticity (GARCH) model, enhanced with a
Student-t distribution to capture the heavy-tailed nature of financial returns
data. The results reveal a statistically significant negative relationship
between negative news sentiment and market stability, suggesting that
pessimistic war coverage is associated with increased volatility in the S&P 500
index. This research demonstrates how artificial intelligence and natural
language processing can be integrated with econometric modelling to assess
real-time market dynamics, offering valuable tools for financial risk analysis
during geopolitical crises.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [401] [ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination](https://arxiv.org/abs/2510.15949)
*Charidimos Papadakis,Angeliki Dimitriou,Giorgos Filandrianos,Maria Lymperaiou,Konstantinos Thomas,Giorgos Stamou*

Main category: q-fin.TR

TL;DR: 提出ATLAS框架解决大语言模型用于金融决策的挑战，Adaptive - OPRO表现优于固定提示。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于自主交易代理存在适应奖励延迟、整合信息和衔接模型输出与市场行动等挑战，需要解决这些问题以支持金融决策。

Method: 提出统一多智能体框架ATLAS，其中心交易智能体在订单感知动作空间操作，使用Adaptive - OPRO动态调整提示。

Result: 在特定制度的股票研究和多个大语言模型家族中，Adaptive - OPRO始终优于固定提示，基于反思的反馈未带来系统性收益。

Conclusion: ATLAS框架结合Adaptive - OPRO能有效解决大语言模型用于金融决策的挑战，提高交易表现。

Abstract: Large language models show promise for financial decision-making, yet
deploying them as autonomous trading agents raises fundamental challenges: how
to adapt instructions when rewards arrive late and obscured by market noise,
how to synthesize heterogeneous information streams into coherent decisions,
and how to bridge the gap between model outputs and executable market actions.
We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent
framework that integrates structured information from markets, news, and
corporate fundamentals to support robust trading decisions. Within ATLAS, the
central trading agent operates in an order-aware action space, ensuring that
outputs correspond to executable market orders rather than abstract signals.
The agent can incorporate feedback while trading using Adaptive-OPRO, a novel
prompt-optimization technique that dynamically adapts the prompt by
incorporating real-time, stochastic feedback, leading to increasing performance
over time. Across regime-specific equity studies and multiple LLM families,
Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based
feedback fails to provide systematic gains.

</details>


### [402] [On Bellman equation in the limit order optimization problem for high-frequency trading](https://arxiv.org/abs/2510.15988)
*M. I. Balakaeva,A. Yu. Veretennikov*

Main category: q-fin.TR

TL;DR: 研究高频交易中买卖限价订单簿最优策略近似方法，修正文献漏洞但不改变主要答案并给出解释。


<details>
  <summary>Details</summary>
Motivation: 发现M. Avellaneda和S. Stoikov 2008年文章存在看似严重的漏洞，需要进行修正。

Method: 对M. Avellaneda和S. Stoikov 2008年文章中的漏洞进行仔细修正。

Result: 修正后不改变原文献的主要答案，漏洞实际上不重要。

Conclusion: 给出了修正后不改变主要答案这一现象的解释。

Abstract: An approximation method for construction of optimal strategies in the bid \&
ask limit order book in the high-frequency trading (HFT) is studied. The basis
is the article by M. Avellaneda \& S. Stoikov 2008, in which certain seemingly
serious gaps have been found; in the present paper they are carefully
corrected. However, a bit surprisingly, our corrections do not change the main
answer in the cited paper, so that, in fact, the gaps turn out to be
unimportant. An explanation of this effect is offered.

</details>


### [403] [The Invisible Handshake: Tacit Collusion between Adaptive Market Agents](https://arxiv.org/abs/2510.15995)
*Luigi Foscari,Emanuele Guidotti,Nicolò Cesa-Bianchi,Tatjana Chavdarova,Alfio Ferrara*

Main category: q-fin.TR

TL;DR: 研究随机市场中自适应交易代理间默契合谋的出现，发现简单学习策略会使动态收敛到合谋策略。


<details>
  <summary>Details</summary>
Motivation: 研究随机市场中自适应交易代理间默契合谋的情况。

Method: 采用做市商和交易接受者的两人重复博弈，刻画可行与合谋策略组合。

Result: 当代理遵循简单学习算法时，动态会收敛到合谋策略组合，即使在高流动性、小交易规模市场。

Conclusion: 简单学习策略自然导致默契合谋，为人工智能驱动市场动态提供新见解。

Abstract: We study the emergence of tacit collusion between adaptive trading agents in
a stochastic market with endogenous price formation. Using a two-player
repeated game between a market maker and a market taker, we characterize
feasible and collusive strategy profiles that raise prices beyond competitive
levels. We show that, when agents follow simple learning algorithms (e.g.,
gradient ascent) to maximize their own wealth, the resulting dynamics converge
to collusive strategy profiles, even in highly liquid markets with small trade
sizes. By highlighting how simple learning strategies naturally lead to tacit
collusion, our results offer new insights into the dynamics of AI-driven
markets.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [404] [Learning density ratios in causal inference using Bregman-Riesz regression](https://arxiv.org/abs/2510.16127)
*Oliver J. Hines,Caleb H. Miles*

Main category: stat.ML

TL;DR: 本文将三种密度比估计方法统一到Bregman - Riesz回归框架，展示数据增强技术用于因果问题，通过模拟研究影响因素并提供Python包。


<details>
  <summary>Details</summary>
Motivation: 单独估计概率密度函数的分子和分母会导致性能不稳定和维数灾难，需开发直接估计密度比的方法。

Method: 将基于Bregman散度、概率分类模型和Riesz损失的三种方法统一到Bregman - Riesz回归框架，使用数据增强技术处理因果问题。

Result: 通过模拟展示了Bregman散度选择和数据增强策略对密度比学习器性能的影响。

Conclusion: 提出的Bregman - Riesz回归框架统一了三种密度比估计方法，且可结合数据增强技术处理因果问题，还提供了Python包方便实践应用。

Abstract: The ratio of two probability density functions is a fundamental quantity that
appears in many areas of statistics and machine learning, including causal
inference, reinforcement learning, covariate shift, outlier detection,
independence testing, importance sampling, and diffusion modeling. Naively
estimating the numerator and denominator densities separately using, e.g.,
kernel density estimators, can lead to unstable performance and suffers from
the curse of dimensionality as the number of covariates increases. For this
reason, several methods have been developed for estimating the density ratio
directly based on (a) Bregman divergences or (b) recasting the density ratio as
the odds in a probabilistic classification model that predicts whether an
observation is sampled from the numerator or denominator distribution.
Additionally, the density ratio can be viewed as the Riesz representer of a
continuous linear map, making it amenable to estimation via (c) minimization of
the so-called Riesz loss, which was developed to learn the Riesz representer in
the Riesz regression procedure in causal inference. In this paper we show that
all three of these methods can be unified in a common framework, which we call
Bregman-Riesz regression. We further show how data augmentation techniques can
be used to apply density ratio learning methods to causal problems, where the
numerator distribution typically represents an unobserved intervention. We show
through simulations how the choice of Bregman divergence and data augmentation
strategy can affect the performance of the resulting density ratio learner. A
Python package is provided for researchers to apply Bregman-Riesz regression in
practice using gradient boosting, neural networks, and kernel methods.

</details>


### [405] [Personalized Collaborative Learning with Affinity-Based Variance Reduction](https://arxiv.org/abs/2510.16232)
*Chenyu Zhang,Navid Azizan*

Main category: stat.ML

TL;DR: 提出个性化协作学习框架PCL及其方法AffPCL，能处理异质性，证明可降低样本复杂度，揭示高异质性下协作新见解。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体学习中利用分布式协作与保持个性化的矛盾，特别是在未知异质性水平下实现全个性化。

Method: 提出PCL框架，设计偏差校正和重要性校正机制的AffPCL方法。

Result: 证明AffPCL能将样本复杂度相对于独立学习降低一个因子，自动在不同设置下插值，且能在高异质性下让智能体获得加速。

Conclusion: AffPCL方法能有效处理环境和目标异质性，为高异质性下的个性化和协作提供新见解。

Abstract: Multi-agent learning faces a fundamental tension: leveraging distributed
collaboration without sacrificing the personalization needed for diverse
agents. This tension intensifies when aiming for full personalization while
adapting to unknown heterogeneity levels -- gaining collaborative speedup when
agents are similar, without performance degradation when they are different.
Embracing the challenge, we propose personalized collaborative learning (PCL),
a novel framework for heterogeneous agents to collaboratively learn
personalized solutions with seamless adaptivity. Through carefully designed
bias correction and importance correction mechanisms, our method AffPCL
robustly handles both environment and objective heterogeneity. We prove that
AffPCL reduces sample complexity over independent learning by a factor of
$\max\{n^{-1}, \delta\}$, where $n$ is the number of agents and
$\delta\in[0,1]$ measures their heterogeneity. This affinity-based acceleration
automatically interpolates between the linear speedup of federated learning in
homogeneous settings and the baseline of independent learning, without
requiring prior knowledge of the system. Our analysis further reveals that an
agent may obtain linear speedup even by collaborating with arbitrarily
dissimilar agents, unveiling new insights into personalization and
collaboration in the high heterogeneity regime.

</details>


### [406] [A Relative Error-Based Evaluation Framework of Heterogeneous Treatment Effect Estimators](https://arxiv.org/abs/2510.16419)
*Jiayi Guo,Haoxuan Li,Ye Tian,Peng Wu*

Main category: stat.ML

TL;DR: 本文提出基于相对误差的鲁棒评估框架评估异质处理效应（HTE）估计器，并提出新的HTE学习算法，实验表明框架和算法有效。


<details>
  <summary>Details</summary>
Motivation: HTE估计取得进展，但HTE估计器的评估仍不完善。

Method: 推导相对误差鲁棒估计所需的关键理论条件，引入新损失函数，设计神经网络架构估计扰动参数；利用已有HTE估计器和神经网络学到的扰动参数提出新学习算法。

Result: 给出相对误差估计器的大样本性质，实验表明评估框架支持HTE估计器的可靠比较，新学习算法性能良好。

Conclusion: 所提评估框架和学习算法有效，能可靠评估和提升HTE估计。

Abstract: While significant progress has been made in heterogeneous treatment effect
(HTE) estimation, the evaluation of HTE estimators remains underdeveloped. In
this article, we propose a robust evaluation framework based on relative error,
which quantifies performance differences between two HTE estimators. We first
derive the key theoretical conditions on the nuisance parameters that are
necessary to achieve a robust estimator of relative error. Building on these
conditions, we introduce novel loss functions and design a neural network
architecture to estimate nuisance parameters and obtain robust estimation of
relative error, thereby achieving reliable evaluation of HTE estimators. We
provide the large sample properties of the proposed relative error estimator.
Furthermore, beyond evaluation, we propose a new learning algorithm for HTE
that leverages both the previously HTE estimators and the nuisance parameters
learned through our neural network architecture. Extensive experiments
demonstrate that our evaluation framework supports reliable comparisons across
HTE estimators, and the proposed learning algorithm for HTE exhibits desirable
performance.

</details>


### [407] [A Bayesian Framework for Symmetry Inference in Chaotic Attractors](https://arxiv.org/abs/2510.16509)
*Ziad Ghanem,Chang Hyunwoong,Preskella Mrad*

Main category: stat.ML

TL;DR: 提出贝叶斯框架用于数据驱动的对称性检测，有理论保证且实验效果好，在生物力学等领域有应用。


<details>
  <summary>Details</summary>
Motivation: 现有最优传输方法用于数据驱动对称性检测时依赖确定性阈值、缺乏不确定性量化，对噪声鲁棒性差且难以解析分层对称结构。

Method: 提出贝叶斯框架，将对称性检测表述为候选子群格上的概率模型选择，使用基于Wasserstein距离构建的Gibbs后验，通过Metropolis - Hastings采样进行后验推断。

Result: 建立三个理论保证，数值实验在高噪声和小样本量下能准确恢复对称性，在人类步态动力学应用中揭示了机械约束导致的对称性变化。

Conclusion: 该框架在生物力学和动力系统的统计推断方面有实用价值。

Abstract: Detecting symmetry from data is a fundamental problem in signal analysis,
providing insight into underlying structure and constraints. When data emerge
as trajectories of dynamical systems, symmetries encode structural properties
of the dynamics that enable model reduction, principled comparison across
conditions, and detection of regime changes. While recent optimal transport
methods provide practical tools for data-driven symmetry detection in this
setting, they rely on deterministic thresholds and lack uncertainty
quantification, limiting robustness to noise and ability to resolve
hierarchical symmetry structures. We present a Bayesian framework that
formulates symmetry detection as probabilistic model selection over a lattice
of candidate subgroups, using a Gibbs posterior constructed from Wasserstein
distances between observed data and group-transformed copies. We establish
three theoretical guarantees: $(i)$ a Bayesian Occam's razor favoring minimal
symmetry consistent with data, $(ii)$ conjugation equivariance ensuring
frame-independence, and $(iii)$ stability bounds under perturbations for
robustness to noise. Posterior inference is performed via Metropolis-Hastings
sampling and numerical experiments on equivariant dynamical systems and
synthetic point clouds demonstrate accurate symmetry recovery under high noise
and small sample sizes. An application to human gait dynamics reveals symmetry
changes induced by mechanical constraints, demonstrating the framework's
utility for statistical inference in biomechanical and dynamical systems.

</details>


### [408] [From Reviews to Actionable Insights: An LLM-Based Approach for Attribute and Feature Extraction](https://arxiv.org/abs/2510.16551)
*Khaled Boughanmi,Kamel Jedidi,Nour Jedidi*

Main category: stat.ML

TL;DR: 研究提出用大语言模型从客户评论中提取产品和服务属性、特征及情感，应用于星巴克评论，结果显示模型可靠且高效，能助力企业决策和提升营收。


<details>
  <summary>Details</summary>
Motivation: 从客户评论中提取产品和服务属性、特征及情感，为企业提供可操作的营销见解。

Method: 提出基于营销理论的大语言模型框架，应用于20000条星巴克Yelp评论，评估八种提示变体，通过与人工标注一致性和对客户评分的预测效度评估模型。

Result: 大语言模型与人工编码高度一致，预测效度强，处理速度远快于人工，能识别影响客户满意度的因素。

Conclusion: 该方法可靠高效，能帮助企业定位问题、设计干预措施，提升关键服务特征情感可增加营收。

Abstract: This research proposes a systematic, large language model (LLM) approach for
extracting product and service attributes, features, and associated sentiments
from customer reviews. Grounded in marketing theory, the framework
distinguishes perceptual attributes from actionable features, producing
interpretable and managerially actionable insights. We apply the methodology to
20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a
random subset of reviews. Model performance is assessed through agreement with
human annotations and predictive validity for customer ratings. Results show
high consistency between LLMs and human coders and strong predictive validity,
confirming the reliability of the approach. Human coders required a median of
six minutes per review, whereas the LLM processed each in two seconds,
delivering comparable insights at a scale unattainable through manual coding.
Managerially, the analysis identifies attributes and features that most
strongly influence customer satisfaction and their associated sentiments,
enabling firms to pinpoint "joy points," address "pain points," and design
targeted interventions. We demonstrate how structured review data can power an
actionable marketing dashboard that tracks sentiment over time and across
stores, benchmarks performance, and highlights high-leverage features for
improvement. Simulations indicate that enhancing sentiment for key service
features could yield 1-2% average revenue gains per store.

</details>


### [409] [Multi-Marginal Schrödinger Bridge Matching](https://arxiv.org/abs/2510.16587)
*Byoungwoo Park,Juho Lee*

Main category: stat.ML

TL;DR: 本文针对从离散时间快照理解群体连续演化的问题，提出多边际薛定谔桥匹配算法MSBM，实验验证其在捕获复杂轨迹和遵循中间分布方面有优势且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 传统薛定谔桥应用于成对时间点，对多中间快照系统不足，需解决从离散时间快照理解群体连续演化的问题，以揭示动态过程机制。

Method: 提出多边际薛定谔桥匹配算法MSBM，将迭代马尔可夫拟合（IMF）扩展以处理多边际约束。

Result: 在合成数据和真实单细胞RNA测序数据集上验证，MSBM在捕获复杂轨迹和遵循中间分布上有竞争力或表现更优，且计算效率高。

Conclusion: MSBM能有效解决多边际薛定谔桥问题，在处理复杂系统的轨迹推断上有良好效果和效率。

Abstract: Understanding the continuous evolution of populations from discrete temporal
snapshots is a critical research challenge, particularly in fields like
developmental biology and systems medicine where longitudinal tracking of
individual entities is often impossible. Such trajectory inference is vital for
unraveling the mechanisms of dynamic processes. While Schr\"odinger Bridge (SB)
offer a potent framework, their traditional application to pairwise time points
can be insufficient for systems defined by multiple intermediate snapshots.
This paper introduces Multi-Marginal Schr\"odinger Bridge Matching (MSBM), a
novel algorithm specifically designed for the multi-marginal SB problem. MSBM
extends iterative Markovian fitting (IMF) to effectively handle multiple
marginal constraints. This technique ensures robust enforcement of all
intermediate marginals while preserving the continuity of the learned global
dynamics across the entire trajectory. Empirical validations on synthetic data
and real-world single-cell RNA sequencing datasets demonstrate the competitive
or superior performance of MSBM in capturing complex trajectories and
respecting intermediate distributions, all with notable computational
efficiency.

</details>


### [410] [Accelerated Learning on Large Scale Screens using Generative Library Models](https://arxiv.org/abs/2510.16612)
*Eli N. Weinstein,Andrei Slabodkin,Mattia G. Gollub,Elizabeth B. Wood*

Main category: stat.ML

TL;DR: 论文介绍优化高通量筛选的算法以缓解生物机器学习的数据瓶颈，通过实验和模拟证明实验与推理协同设计可加速学习。


<details>
  <summary>Details</summary>
Motivation: 生物机器学习受限于缺乏大规模数据，高通量筛选是缓解数据瓶颈的有前景途径。

Method: 引入优化高通量筛选的算法，聚焦大规模场景，仅收集活性序列正例并用生成模型校正缺失的负例。

Result: 在模拟和抗体大规模筛选中验证了该方法，能有效估计真实的 $p(y | x)$。

Conclusion: 实验与推理的协同设计可显著加速学习。

Abstract: Biological machine learning is often bottlenecked by a lack of scaled data.
One promising route to relieving data bottlenecks is through high throughput
screens, which can experimentally test the activity of $10^6-10^{12}$ protein
sequences in parallel. In this article, we introduce algorithms to optimize
high throughput screens for data creation and model training. We focus on the
large scale regime, where dataset sizes are limited by the cost of measurement
and sequencing. We show that when active sequences are rare, we maximize
information gain if we only collect positive examples of active sequences, i.e.
$x$ with $y>0$. We can correct for the missing negative examples using a
generative model of the library, producing a consistent and efficient estimate
of the true $p(y | x)$. We demonstrate this approach in simulation and on a
large scale screen of antibodies. Overall, co-design of experiments and
inference lets us accelerate learning dramatically.

</details>


### [411] [ARCO-BO: Adaptive Resource-aware COllaborative Bayesian Optimization for Heterogeneous Multi-Agent Design](https://arxiv.org/abs/2510.16652)
*Zihan Wang,Yi-Ping Chen,Tuba Dolar,Wei Chen*

Main category: stat.ML

TL;DR: 介绍自适应资源感知协作贝叶斯优化框架 ARCO - BO，可处理多智能体优化中的异质性，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代分布式优化中智能体存在目标、预算和设计变量等异质性，现有贝叶斯优化方法难以处理这些挑战。

Method: ARCO - BO 结合相似性和最优感知共识机制进行自适应信息共享、预算感知异步采样策略进行资源协调、部分输入空间共享处理异构设计空间。

Result: 在合成和高维工程问题实验中，ARCO - BO 始终优于独立 BO 和现有协作 BO 方法。

Conclusion: ARCO - BO 在复杂多智能体环境中能实现稳健高效的性能。

Abstract: Modern scientific and engineering design increasingly involves distributed
optimization, where agents such as laboratories, simulations, or industrial
partners pursue related goals under differing conditions. These agents often
face heterogeneities in objectives, evaluation budgets, and accessible design
variables, which complicates coordination and can lead to redundancy, poor
resource use, and ineffective information sharing. Bayesian Optimization (BO)
is a widely used decision-making framework for expensive black box functions,
but its single-agent formulation assumes centralized control and full data
sharing. Recent collaborative BO methods relax these assumptions, yet they
often require uniform resources, fully shared input spaces, and fixed task
alignment, conditions rarely satisfied in practice. To address these
challenges, we introduce Adaptive Resource Aware Collaborative Bayesian
Optimization (ARCO-BO), a framework that explicitly accounts for heterogeneity
in multi-agent optimization. ARCO-BO combines three components: a similarity
and optima-aware consensus mechanism for adaptive information sharing, a
budget-aware asynchronous sampling strategy for resource coordination, and a
partial input space sharing for heterogeneous design spaces. Experiments on
synthetic and high-dimensional engineering problems show that ARCO-BO
consistently outperforms independent BO and existing collaborative BO via
consensus approach, achieving robust and efficient performance in complex
multi-agent settings.

</details>


### [412] [Escaping Model Collapse via Synthetic Data Verification: Near-term Improvements and Long-term Convergence](https://arxiv.org/abs/2510.16657)
*Bingji Yi,Qiyuan Liu,Yuwei Cheng,Haifeng Xu*

Main category: stat.ML

TL;DR: 研究修改合成再训练过程避免模型崩溃，发现注入外部验证信息可避免崩溃，理论和实验验证相关结论。


<details>
  <summary>Details</summary>
Motivation: 解决迭代再训练生成模型使用自身合成数据导致性能下降（模型崩溃）的问题。

Method: 在基础线性回归环境中分析，通过外部合成数据验证器注入信息，进行线性回归和变分自编码器实验。

Result: 注入外部验证信息可避免模型崩溃，迭代再训练有短期提升但长期参数估计会趋向验证器的“知识中心”，除非验证器完全可靠，否则早期收益会停滞或逆转。

Conclusion: 理论和实验均表明通过外部验证信息可改善合成再训练，避免模型崩溃，但验证器可靠性影响最终效果。

Abstract: Synthetic data has been increasingly used to train frontier generative
models. However, recent study raises key concerns that iteratively retraining a
generative model on its self-generated synthetic data may keep deteriorating
model performance, a phenomenon often coined model collapse. In this paper, we
investigate ways to modify this synthetic retraining process to avoid model
collapse, and even possibly help reverse the trend from collapse to
improvement. Our key finding is that by injecting information through an
external synthetic data verifier, whether a human or a better model, synthetic
retraining will not cause model collapse. To develop principled understandings
of the above insight, we situate our analysis in the foundational linear
regression setting, showing that iterative retraining with verified synthetic
data can yield near-term improvements but ultimately drives the parameter
estimate to the verifier's "knowledge center" in the long run. Our theory hence
predicts that, unless the verifier is perfectly reliable, the early gains will
plateau and may even reverse. Indeed, these theoretical insights are further
confirmed by our experiments on both linear regression as well as Variational
Autoencoders (VAEs) trained on MNIST data.

</details>


### [413] [Infinite Neural Operators: Gaussian processes on functions](https://arxiv.org/abs/2510.16675)
*Daniel Augusto de Souza,Yuchen Zhu,Harry Jake Cunningham,Yuri Saporito,Diego Mesquita,Marc Peter Deisenroth*

Main category: stat.ML

TL;DR: 本文将无限宽神经网络与高斯过程的联系拓展到神经算子，证明任意深度神经算子收敛到函数值高斯过程，计算协方差函数和后验，助力挖掘归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 将无限宽神经网络与高斯过程的联系拓展到神经算子，以改善深度神经网络的不确定性量化，挖掘神经算子架构的归纳偏置。

Method: 证明任意深度且卷积核为高斯分布的神经算子收敛到函数值高斯过程，计算两种神经算子参数化下的协方差函数和回归场景下的后验。

Result: 得到任意深度神经算子收敛到函数值高斯过程的条件，算出两种参数化下神经算子 - 高斯过程的协方差函数和后验。

Conclusion: 此工作是挖掘当前傅里叶神经算子架构归纳偏置的重要一步，为基于核的算子学习方法引入新归纳偏置开辟道路。

Abstract: A variety of infinitely wide neural architectures (e.g., dense NNs, CNNs, and
transformers) induce Gaussian process (GP) priors over their outputs. These
relationships provide both an accurate characterization of the prior predictive
distribution and enable the use of GP machinery to improve the uncertainty
quantification of deep neural networks. In this work, we extend this connection
to neural operators (NOs), a class of models designed to learn mappings between
function spaces. Specifically, we show conditions for when arbitrary-depth NOs
with Gaussian-distributed convolution kernels converge to function-valued GPs.
Based on this result, we show how to compute the covariance functions of these
NO-GPs for two NO parametrizations, including the popular Fourier neural
operator (FNO). With this, we compute the posteriors of these GPs in regression
scenarios, including PDE solution operators. This work is an important step
towards uncovering the inductive biases of current FNO architectures and opens
a path to incorporate novel inductive biases for use in kernel-based operator
learning methods.

</details>


### [414] [Local regression on path spaces with signature metrics](https://arxiv.org/abs/2510.16728)
*Christian Bayer,Davit Gogolashvili,Luca Pelizzari*

Main category: stat.ML

TL;DR: 研究路径值数据的非参数回归和分类，引入结合签名变换与局部核回归的估计器，有计算优势和良好统计性质，在多场景表现佳。


<details>
  <summary>Details</summary>
Motivation: 解决路径值数据的非参数回归和分类问题，避免大规模核矩阵运算的可扩展性瓶颈。

Method: 引入结合粗糙路径理论的签名变换与局部核回归的功能Nadaraya - Watson估计器，利用签名诱导距离，提出鲁棒签名变体。

Result: 建立有限样本收敛界限，证明签名距离有良好统计性质；在合成和真实数据应用中有竞争精度和计算优势。

Conclusion: 所提方法在路径值数据的非参数回归和分类上是有效的，且优于现有方法。

Abstract: We study nonparametric regression and classification for path-valued data. We
introduce a functional Nadaraya-Watson estimator that combines the signature
transform from rough path theory with local kernel regression. The signature
transform provides a principled way to encode sequential data through iterated
integrals, enabling direct comparison of paths in a natural metric space. Our
approach leverages signature-induced distances within the classical kernel
regression framework, achieving computational efficiency while avoiding the
scalability bottlenecks of large-scale kernel matrix operations. We establish
finite-sample convergence bounds demonstrating favorable statistical properties
of signature-based distances compared to traditional metrics in
infinite-dimensional settings. We propose robust signature variants that
provide stability against outliers, enhancing practical performance.
Applications to both synthetic and real-world data - including stochastic
differential equation learning and time series classification - demonstrate
competitive accuracy while offering significant computational advantages over
existing methods.

</details>


### [415] [Kernel-Based Nonparametric Tests For Shape Constraints](https://arxiv.org/abs/2510.16745)
*Rohan Sen*

Main category: stat.ML

TL;DR: 本文开发了非参数均值 - 方差优化的RKHS框架，推导样本估计量性质，引入联合Wald型统计量检验形状约束，方法有高效计算过程且实证效果好。


<details>
  <summary>Details</summary>
Motivation: 开发非参数均值 - 方差优化及最优规则形状约束推断的框架。

Method: 开发RKHS框架，推导样本估计量统计性质，引入联合Wald型统计量，采用基于主元Cholesky分解的计算程序。

Result: 得到样本估计量的渐近一致性、泛函中心极限定理和有限样本偏差界等理论结果，方法有高效计算过程。

Conclusion: 实证表明所提方法效果良好。

Abstract: We develop a reproducing kernel Hilbert space (RKHS) framework for
nonparametric mean-variance optimization and inference on shape constraints of
the optimal rule. We derive statistical properties of the sample estimator and
provide rigorous theoretical guarantees, such as asymptotic consistency, a
functional central limit theorem, and a finite-sample deviation bound that
matches the Monte Carlo rate up to regularization. Building on these findings,
we introduce a joint Wald-type statistic to test for shape constraints over
finite grids. The approach comes with an efficient computational procedure
based on a pivoted Cholesky factorization, facilitating scalability to large
datasets. Empirical tests suggest favorably of the proposed methodology.

</details>


### [416] [Prediction-Augmented Trees for Reliable Statistical Inference](https://arxiv.org/abs/2510.16937)
*Vikram Kher,Argyris Oikonomou,Manolis Zampetakis*

Main category: stat.ML

TL;DR: 本文研究如何在数据统计分析中安全使用机器学习预测以促进科学发现，引入了PART和PAQ两种新的学习增强估计器，在多领域数据集上表现优于现有方法，并证明了其优势。


<details>
  <summary>Details</summary>
Motivation: 在科学发现中，需要研究如何安全地在数据统计分析中使用机器学习预测。

Method: 遵循Angelopoulos等人的框架，引入PART和PAQ两种新估计器，对PART进行渐近分布特征分析和置信区间构建，通过PAQ证明PART优势。

Result: PART在生态学、天文学和人口普查报告等领域的真实数据集上优于现有方法；PAQ在适当假设下，方差收缩率优于现有方法。

Conclusion: 新引入的PART和PAQ估计器在使用金标准样本和机器学习预测时，能产生更有信心的估计，优于现有估计器。

Abstract: The remarkable success of machine learning (ML) in predictive tasks has led
scientists to incorporate ML predictions as a core component of the scientific
discovery pipeline. This was exemplified by the landmark achievement of
AlphaFold (Jumper et al. (2021)). In this paper, we study how ML predictions
can be safely used in statistical analysis of data towards scientific
discovery. In particular, we follow the framework introduced by Angelopoulos et
al. (2023). In this framework, we assume access to a small set of $n$
gold-standard labeled samples, a much larger set of $N$ unlabeled samples, and
a ML model that can be used to impute the labels of the unlabeled data points.
We introduce two new learning-augmented estimators: (1) Prediction-Augmented
Residual Tree (PART), and (2) Prediction-Augmented Quadrature (PAQ). Both
estimators have significant advantages over existing estimators like PPI and
PPI++ introduced by Angelopoulos et al. (2023) and Angelopoulos et al. (2024),
respectively. PART is a decision-tree based estimator built using a greedy
criterion. We first characterize PART's asymptotic distribution and demonstrate
how to construct valid confidence intervals. Then we show that PART outperforms
existing methods in real-world datasets from ecology, astronomy, and census
reports, among other domains. This leads to estimators with higher confidence,
which is the result of using both the gold-standard samples and the machine
learning predictions. Finally, we provide a formal proof of the advantage of
PART by exploring PAQ, an estimation that arises when considering the limit of
PART when the depth its tree grows to infinity. Under appropriate assumptions
in the input data we show that the variance of PAQ shrinks at rate of $O(N^{-1}
+ n^{-4})$, improving significantly on the $O(N^{-1}+n^{-1})$ rate of existing
methods.

</details>


### [417] [Adaptive Sample Sharing for Linear Regression](https://arxiv.org/abs/2510.16986)
*Hamza Cherkaoui,Hélène Halconruy,Yohan Petetin*

Main category: stat.ML

TL;DR: 研究岭回归中样本共享，提出数据驱动规则决定辅助数据集样本添加数量，验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决特定任务中监督学习因任务特定标记数据稀缺或获取成本高受限的问题。

Method: 引入基于转移增益估计的数据驱动规则，决定辅助数据集样本添加数量，推导有限样本保证。

Result: 在合成和真实数据集上验证，相比强基线和单任务训练有一致增益，避免负迁移。

Conclusion: 所提方法在岭回归样本共享中有效，能提高参数估计，避免负迁移。

Abstract: In many business settings, task-specific labeled data are scarce or costly to
obtain, which limits supervised learning on a specific task. To address this
challenge, we study sample sharing in the case of ridge regression: leveraging
an auxiliary data set while explicitly protecting against negative transfer. We
introduce a principled, data-driven rule that decides how many samples from an
auxiliary dataset to add to the target training set. The rule is based on an
estimate of the transfer gain i.e. the marginal reduction in the predictive
error. Building on this estimator, we derive finite-sample guaranties: under
standard conditions, the procedure borrows when it improves parameter
estimation and abstains otherwise. In the Gaussian feature setting, we analyze
which data set properties ensure that borrowing samples reduces the predictive
error. We validate the approach in synthetic and real datasets, observing
consistent gains over strong baselines and single-task training while avoiding
negative transfer.

</details>


### [418] [Mode Collapse of Mean-Field Variational Inference](https://arxiv.org/abs/2510.17063)
*Shunan Sheng,Bohan Wu,Alberto González-Sanz*

Main category: stat.ML

TL;DR: 本文对MFVI中的模式崩溃现象给出理论解释，提出旋转变分推理（RoVI）方法并通过数值研究验证。


<details>
  <summary>Details</summary>
Motivation: MFVI优化器常出现模式崩溃问题，缺乏理论解释。

Method: 引入ε - 分离性概念推导MFVI优化器分配给各分量的质量分数界限；提出RoVI方法，用旋转矩阵增强MFVI。

Result: 得出模式崩溃的发生关键取决于分量的相对位置；数值研究支持理论发现并展示RoVI的优势。

Conclusion: 给出了MFVI模式崩溃的理论解释，RoVI方法能解决相关问题。

Abstract: Mean-field variational inference (MFVI) is a widely used method for
approximating high-dimensional probability distributions by product measures.
It has been empirically observed that MFVI optimizers often suffer from mode
collapse. Specifically, when the target measure $\pi$ is a mixture $\pi = w P_0
+ (1 - w) P_1$, the MFVI optimizer tends to place most of its mass near a
single component of the mixture. This work provides the first theoretical
explanation of mode collapse in MFVI. We introduce the notion to capture the
separatedness of the two mixture components -- called
$\varepsilon$-separateness -- and derive explicit bounds on the fraction of
mass that any MFVI optimizer assigns to each component when $P_0$ and $P_1$ are
$\varepsilon$-separated for sufficiently small $\varepsilon$. Our results
suggest that the occurrence of mode collapse crucially depends on the relative
position of the components. To address this issue, we propose the rotational
variational inference (RoVI), which augments MFVI with a rotation matrix. The
numerical studies support our theoretical findings and demonstrate the benefits
of RoVI.

</details>


### [419] [DFNN: A Deep Fréchet Neural Network Framework for Learning Metric-Space-Valued Responses](https://arxiv.org/abs/2510.17072)
*Kyum Kim,Yaqing Chen,Paromita Dubey*

Main category: stat.ML

TL;DR: 提出深度Fréchet神经网络（DFNNs）用于预测非欧几里得响应，建立近似定理，实证表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 非欧几里得响应回归在现代应用中愈发重要，需要有效预测方法。

Method: 提出DFNNs框架，利用深度神经网络表示学习能力，通过最小化Fréchet风险近似条件Fréchet均值。

Result: 建立DFNNs的通用近似定理，实证研究表明DFNNs在合成和真实数据上均优于现有方法。

Conclusion: DFNNs是一种灵活有效的预测非欧几里得响应的方法，推动了神经网络近似理论发展。

Abstract: Regression with non-Euclidean responses -- e.g., probability distributions,
networks, symmetric positive-definite matrices, and compositions -- has become
increasingly important in modern applications. In this paper, we propose deep
Fr\'echet neural networks (DFNNs), an end-to-end deep learning framework for
predicting non-Euclidean responses -- which are considered as random objects in
a metric space -- from Euclidean predictors. Our method leverages the
representation-learning power of deep neural networks (DNNs) to the task of
approximating conditional Fr\'echet means of the response given the predictors,
the metric-space analogue of conditional expectations, by minimizing a
Fr\'echet risk. The framework is highly flexible, accommodating diverse metrics
and high-dimensional predictors. We establish a universal approximation theorem
for DFNNs, advancing the state-of-the-art of neural network approximation
theory to general metric-space-valued responses without making model
assumptions or relying on local smoothing. Empirical studies on synthetic
distributional and network-valued responses, as well as a real-world
application to predicting employment occupational compositions, demonstrate
that DFNNs consistently outperform existing methods.

</details>


### [420] [Optimal Best Arm Identification under Differential Privacy](https://arxiv.org/abs/2510.17348)
*Marc Jourdan,Achraf Azize*

Main category: stat.ML

TL;DR: 本文研究伯努利分布下全局差分隐私（DP）的固定置信度最佳臂识别（BAI）问题，缩小了全局DP设置中上下界的差距。


<details>
  <summary>Details</summary>
Motivation: 数据敏感应用存在隐私担忧，现有非私有环境下有渐近最优BAI算法，但全局DP设置中上下界有显著差距。

Method: 提供更严格的样本复杂度下界；引入基于运输成本的停止规则和私有均值估计器；提出基于运输成本的Top Two采样规则。

Result: 证明采样规则期望样本复杂度的渐近上界与下界匹配，乘性常数小于8，算法性能优于现有算法。

Conclusion: 缩小了全局DP设置中BAI问题上下界的差距，提出的算法表现更优。

Abstract: Best Arm Identification (BAI) algorithms are deployed in data-sensitive
applications, such as adaptive clinical trials or user studies. Driven by the
privacy concerns of these applications, we study the problem of
fixed-confidence BAI under global Differential Privacy (DP) for Bernoulli
distributions. While numerous asymptotically optimal BAI algorithms exist in
the non-private setting, a significant gap remains between the best lower and
upper bounds in the global DP setting. This work reduces this gap to a small
multiplicative constant, for any privacy budget $\epsilon$. First, we provide a
tighter lower bound on the expected sample complexity of any $\delta$-correct
and $\epsilon$-global DP strategy. Our lower bound replaces the
Kullback-Leibler (KL) divergence in the transportation cost used by the
non-private characteristic time with a new information-theoretic quantity that
optimally trades off between the KL divergence and the Total Variation distance
scaled by $\epsilon$. Second, we introduce a stopping rule based on these
transportation costs and a private estimator of the means computed using an
arm-dependent geometric batching. En route to proving the correctness of our
stopping rule, we derive concentration results of independent interest for the
Laplace distribution and for the sum of Bernoulli and Laplace distributions.
Third, we propose a Top Two sampling rule based on these transportation costs.
For any budget $\epsilon$, we show an asymptotic upper bound on its expected
sample complexity that matches our lower bound to a multiplicative constant
smaller than $8$. Our algorithm outperforms existing $\delta$-correct and
$\epsilon$-global DP BAI algorithms for different values of $\epsilon$.

</details>


### [421] [Certified Self-Consistency: Statistical Guarantees and Test-Time Training for Reliable Reasoning in LLMs](https://arxiv.org/abs/2510.17472)
*Paula Cordero-Encinar,Andrew B. Duncan*

Main category: stat.ML

TL;DR: 提出大语言模型可认证推理统一框架，解释自一致性和测试时强化学习策略，提出新的训练目标。


<details>
  <summary>Details</summary>
Motivation: 现有自一致性和测试时强化学习提升大语言模型可靠性的机制和统计保证缺乏理解。

Method: 给出多数投票为自一致性提供统计证明，推导有限样本和随时有效集中界限，引入MMC停止规则，证明无标签训练方法影响答案分布并提出新训练目标。

Result: 在单一统计框架下解释并关联自一致性和测试时强化学习两种策略。

Conclusion: 为推理大语言模型无标签、可认证可靠性提供统一框架。

Abstract: Recent advances such as self-consistency and test-time reinforcement learning
(TTRL) improve the reliability of large language models (LLMs) without
additional supervision, yet their underlying mechanisms and statistical
guarantees remain poorly understood. We present a unified framework for
certifiable inference in LLMs, showing that majority voting provides a
statistical certificate of self-consistency: under mild assumptions, the
aggregated answer coincides with the mode of the model's terminal distribution
with high probability. We derive finite-sample and anytime-valid concentration
bounds that quantify this confidence, and introduce the Martingale Majority
Certificate (MMC), a sequential stopping rule that adaptively determines when
sufficient samples have been drawn. We further prove that label-free
post-training methods such as TTRL implicitly sharpen the answer distribution
by exponentially tilting it toward its mode, thereby reducing the number of
samples required for certification. Building on this insight, we propose new
post-training objectives that explicitly optimise this trade-off between
sharpness and bias. Together, these results explain and connect two central
test-time scaling strategies, self-consistency and TTRL, within a single
statistical framework for label-free, certifiable reliability in reasoning
LLMs.

</details>


### [422] [Non-asymptotic error bounds for probability flow ODEs under weak log-concavity](https://arxiv.org/abs/2510.17608)
*Gitte Kremling,Francesco Iafrate,Mahsa Taheri,Johannes Lederer*

Main category: stat.ML

TL;DR: 本文在较弱假设下为概率流ODE建立非渐近收敛界，扩展收敛理论到更现实数据分布和ODE求解器，为采样算法提供保证并有助于选择超参数。


<details>
  <summary>Details</summary>
Motivation: 现有基于分数的生成建模收敛保证依赖强正则性假设，本文旨在在较弱假设下建立收敛界。

Method: 在弱对数凹性和分数函数Lipschitz连续性的较弱假设下，为一般类概率流ODE建立2 - Wasserstein距离的非渐近收敛界，考虑初始化误差、分数近似误差和离散化影响。

Result: 将收敛理论扩展到更现实的数据分布和实际的ODE求解器，为采样算法的效率和正确性提供具体保证。

Conclusion: 研究成果从理论上补充了扩散模型的经验成功，显式速率有助于选择超参数。

Abstract: Score-based generative modeling, implemented through probability flow ODEs,
has shown impressive results in numerous practical settings. However, most
convergence guarantees rely on restrictive regularity assumptions on the target
distribution -- such as strong log-concavity or bounded support. This work
establishes non-asymptotic convergence bounds in the 2-Wasserstein distance for
a general class of probability flow ODEs under considerably weaker assumptions:
weak log-concavity and Lipschitz continuity of the score function. Our
framework accommodates non-log-concave distributions, such as Gaussian
mixtures, and explicitly accounts for initialization errors, score
approximation errors, and effects of discretization via an exponential
integrator scheme. Bridging a key theoretical challenge in diffusion-based
generative modeling, our results extend convergence theory to more realistic
data distributions and practical ODE solvers. We provide concrete guarantees
for the efficiency and correctness of the sampling algorithm, complementing the
empirical success of diffusion models with rigorous theory. Moreover, from a
practical perspective, our explicit rates might be helpful in choosing
hyperparameters, such as the step size in the discretization.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [423] [AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning](https://arxiv.org/abs/2510.16156)
*Yueqian Lin,Zhengmian Hu,Jayakumar Subramanian,Qinsi Wang,Nikos Vlassis,Hai "Helen" Li,Yiran Chen*

Main category: eess.AS

TL;DR: 提出AsyncVoice Agent系统，其异步架构可并行处理叙述和推理，减少交互延迟，为构建人机系统提供新范式。


<details>
  <summary>Details</summary>
Motivation: 现有方法如CoT的整体文本及接口问题阻碍人机在复杂推理任务上的有效协作，需改进。

Method: 设计AsyncVoice Agent系统，采用异步架构将流式大语言模型后端与对话语音前端分离。

Result: 相比整体基线，该方法将交互延迟降低600多倍，保证高保真度和有竞争力的任务准确率。

Conclusion: AsyncVoice Agent能实现与模型推理过程的双向对话，为高风险任务构建更有效、可引导和可信的人机系统提供新范式。

Abstract: Effective human-AI collaboration on complex reasoning tasks requires that
users understand and interact with the model's process, not just receive an
output. However, the monolithic text from methods like Chain-of-Thought (CoT)
prevents this, as current interfaces lack real-time verbalization and robust
user barge-in. We present AsyncVoice Agent, a system whose asynchronous
architecture decouples a streaming LLM backend from a conversational voice
frontend. This design allows narration and inference to run in parallel,
empowering users to interrupt, query, and steer the model's reasoning process
at any time. Objective benchmarks show this approach reduces interaction
latency by more than 600x compared to monolithic baselines while ensuring high
fidelity and competitive task accuracy. By enabling a two-way dialogue with a
model's thought process, AsyncVoice Agent offers a new paradigm for building
more effective, steerable, and trustworthy human-AI systems for high-stakes
tasks.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [424] [Direct Simplified Symbolic Analysis (DSSA) Tool](https://arxiv.org/abs/2510.15901)
*Mohammad Shokouhifar,Hossein Yazdanjouei,Gerhard-Wilhelm Weber*

Main category: cs.OH

TL;DR: 本文提出简化模拟电路新方法DSSA，结合蒙特卡罗模拟和遗传算法，测试显示其高效准确。


<details>
  <summary>Details</summary>
Motivation: 传统基于矩阵或图的模拟电路简化技术速度慢且内存占用大，需新方法。

Method: 将任务视为建模问题，直接提取最显著传递函数项，结合蒙特卡罗模拟和遗传算法，最小化简化符号表达式与精确数值表达式的误差。

Result: 在MATLAB中对五个电路测试，直流增益平均变化0.64 dB，最大变化1.36 dB，极点/零点平均误差6.8%。

Conclusion: DSSA是符号电路分析的高效准确工具。

Abstract: This paper introduces Direct Simplified Symbolic Analysis (DSSA), a new
method for simplifying analog circuits. Unlike traditional matrix- or
graph-based techniques that are often slow and memory-intensive, DSSA treats
the task as a modeling problem and directly extracts the most significant
transfer function terms. By combining Monte Carlo simulation with a genetic
algorithm, it minimizes error between simplified symbolic and exact numeric
expressions. Tests on five circuits in MATLAB show strong performance, with
only 0.64 dB average and 1.36 dB maximum variation in dc-gain, along with a
6.8% average pole/zero error. These results highlight DSSA as an efficient and
accurate tool for symbolic circuit analysis.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [425] [Publication Trend Analysis and Synthesis via Large Language Model: A Case Study of Engineering in PNAS](https://arxiv.org/abs/2510.16152)
*Mason Smetana,Lev Khazanovich*

Main category: cs.DL

TL;DR: 本文引入基于大语言模型的框架分析科学文献，以量化主题趋势和描绘科学知识格局演变，经20年工程文章验证有效。


<details>
  <summary>Details</summary>
Motivation: 科学文献因复杂语言、学科结构和关键词系统等问题，难以捕捉现代科学动态，需新方法解决。

Method: 采用两阶段分类流程，先基于摘要确定文章主主题类别，再全文分析分配次分类，用传统NLP方法验证结构。

Result: 该方法能独立还原期刊编辑结构，发现主题间隐含联系，且表明单纯词频分析不足以映射高多样性领域。

Conclusion: 此框架是检测主题趋势和概述科学进展的有力工具。

Abstract: Scientific literature is increasingly siloed by complex language, static
disciplinary structures, and potentially sparse keyword systems, making it
cumbersome to capture the dynamic nature of modern science. This study
addresses these challenges by introducing an adaptable large language model
(LLM)-driven framework to quantify thematic trends and map the evolving
landscape of scientific knowledge. The approach is demonstrated over a 20-year
collection of more than 1,500 engineering articles published by the Proceedings
of the National Academy of Sciences (PNAS), marked for their breadth and depth
of research focus. A two-stage classification pipeline first establishes a
primary thematic category for each article based on its abstract. The
subsequent phase performs a full-text analysis to assign secondary
classifications, revealing latent, cross-topic connections across the corpus.
Traditional natural language processing (NLP) methods, such as Bag-of-Words
(BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), confirm the
resulting topical structure and also suggest that standalone word-frequency
analyses may be insufficient for mapping fields with high diversity. Finally, a
disjoint graph representation between the primary and secondary classifications
reveals implicit connections between themes that may be less apparent when
analyzing abstracts or keywords alone. The findings show that the approach
independently recovers much of the journal's editorially embedded structure
without prior knowledge of its existing dual-classification schema (e.g.,
biological studies also classified as engineering). This framework offers a
powerful tool for detecting potential thematic trends and providing a
high-level overview of scientific progress.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [426] [Inference of Deterministic Finite Automata via Q-Learning](https://arxiv.org/abs/2510.17386)
*Elaheh Hosseinkhani,Martin Leucker*

Main category: cs.FL

TL;DR: 本文探讨用Q学习算法进行确定性有限自动机（DFA）的被动推理，搭建了子符号学习与符号表示的桥梁并给出评估。


<details>
  <summary>Details</summary>
Motivation: 现有DFA推理传统方法源于符号AI，而子符号AI提供新范式，探索用Q学习算法进行DFA被动推理。

Method: 利用Q学习算法，将学习到的Q函数重新解释为有限域上DFA的转移函数，以适应自动机推理。

Result: 展示了如何将Q学习算法用于自动机推理，并在多个示例上进行了评估。

Conclusion: Q学习算法为DFA的被动推理提供了新方法，搭建了子符号学习与符号表示的桥梁。

Abstract: Traditional approaches to inference of deterministic finite-state automata
(DFA) stem from symbolic AI, including both active learning methods (e.g.,
Angluin's L* algorithm and its variants) and passive techniques (e.g., Biermann
and Feldman's method, RPNI). Meanwhile, sub-symbolic AI, particularly machine
learning, offers alternative paradigms for learning from data, such as
supervised, unsupervised, and reinforcement learning (RL). This paper
investigates the use of Q-learning, a well-known reinforcement learning
algorithm, for the passive inference of deterministic finite automata. It
builds on the core insight that the learned Q-function, which maps state-action
pairs to rewards, can be reinterpreted as the transition function of a DFA over
a finite domain. This provides a novel bridge between sub-symbolic learning and
symbolic representations. The paper demonstrates how Q-learning can be adapted
for automaton inference and provides an evaluation on several examples.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [427] [Near-Optimal Quantum Algorithms for Computing (Coarse) Correlated Equilibria of General-Sum Games](https://arxiv.org/abs/2510.16782)
*Tongyang Li,Xinzhao Wang,Yexin Zhang*

Main category: quant-ph

TL;DR: 本文研究多人正规形式博弈中计算ε-近似相关均衡（CE）和粗相关均衡（CCE）的量子算法，给出查询复杂度并证明接近最优。


<details>
  <summary>Details</summary>
Motivation: 经典和量子环境下零和博弈的纳什均衡计算研究广泛，一般和博弈中纳什均衡计算为PPAD难问题，相关均衡计算在博弈论中被广泛探索，本文旨在研究多人正规形式博弈中计算CE和CCE的量子算法。

Method: 对于CE计算，利用多尺度乘法权重更新（MWU）方法的量子改进；对于CCE，将零和博弈量子算法技术扩展到多人场景。

Result: 对于固定的ε，CE算法查询复杂度为O~(m√n)；CCE算法查询复杂度为O~(m√n/ε^2.5)。

Conclusion: 两种算法在玩家数量m和行动数量n上的缩放接近最优，量子查询下界证实了这一点。

Abstract: Computing Nash equilibria of zero-sum games in classical and quantum settings
is extensively studied. For general-sum games, computing Nash equilibria is
PPAD-hard and the computing of a more general concept called correlated
equilibria has been widely explored in game theory. In this paper, we initiate
the study of quantum algorithms for computing $\varepsilon$-approximate
correlated equilibria (CE) and coarse correlated equilibria (CCE) in
multi-player normal-form games. Our approach utilizes quantum improvements to
the multi-scale Multiplicative Weight Update (MWU) method for CE calculations,
achieving a query complexity of $\tilde{O}(m\sqrt{n})$ for fixed $\varepsilon$.
For CCE, we extend techniques from quantum algorithms for zero-sum games to
multi-player settings, achieving query complexity
$\tilde{O}(m\sqrt{n}/\varepsilon^{2.5})$. Both algorithms demonstrate a
near-optimal scaling in the number of players $m$ and actions $n$, as confirmed
by our quantum query lower bounds.

</details>


### [428] [Quantum Federated Learning: Architectural Elements and Future Directions](https://arxiv.org/abs/2510.17642)
*Siva Sai,Abhishek Sawaika,Prabhjot Singh,Rajkumar Buyya*

Main category: quant-ph

TL;DR: 本文调研量子联邦学习（QFL），介绍其动机、架构、分类、应用，指出挑战与未来工作。


<details>
  <summary>Details</summary>
Motivation: 经典联邦学习存在计算资源要求高、隐私风险、更新流量大、非IID异构等局限，QFL引入量子计算可解决这些问题。

Method: 先阐述经典FL痛点引出QFL，介绍QFL框架通用架构，基于四个标准对现有QFL系统分类，描述QFL应用。

Result: 明确QFL在医疗、车联网等领域能提升通信效率、安全性和性能。

Conclusion: 指出QFL存在超越分类任务拓展、对抗攻击等挑战及未来工作方向。

Abstract: Federated learning (FL) focuses on collaborative model training without the
need to move the private data silos to a central server. Despite its several
benefits, the classical FL is plagued with several limitations, such as high
computational power required for model training(which is critical for
low-resource clients), privacy risks, large update traffic, and non-IID
heterogeneity. This chapter surveys a hybrid paradigm - Quantum Federated
Learning (QFL), which introduces quantum computation, that addresses multiple
challenges of classical FL and offers rapid computing capability while keeping
the classical orchestration intact. Firstly, we motivate QFL with a concrete
presentation on pain points of classical FL, followed by a discussion on a
general architecture of QFL frameworks specifying the roles of client and
server, communication primitives and the quantum model placement. We classify
the existing QFL systems based on four criteria - quantum architecture (pure
QFL, hybrid QFL), data processing method (quantum data encoding, quantum
feature mapping, and quantum feature selection & dimensionality reduction),
network topology (centralized, hierarchial, decentralized), and quantum
security mechanisms (quantum key distribution, quantum homomorphic encryption,
quantum differential privacy, blind quantum computing). We then describe
applications of QFL in healthcare, vehicular networks, wireless networks, and
network security, clearly highlighting where QFL improves communication
efficiency, security, and performance compared to classical FL. We close with
multiple challenges and future works in QFL, including extension of QFL beyond
classification tasks, adversarial attacks, realistic hardware deployment,
quantum communication protocols deployment, aggregation of different quantum
models, and quantum split learning as an alternative to QFL.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [429] [Quantum Synthetic Data Generation for Industrial Bioprocess Monitoring](https://arxiv.org/abs/2510.17688)
*Shawn M. Gibford,Mohammad Reza Boskabadi,Christopher J. Savoie,Seyed Soheil Mansouri*

Main category: cs.ET

TL;DR: 本文提出QWGAN - GP生成生物制造合成时间序列数据，结果显示能捕捉真实生物过程数据动态，量子计算与机器学习结合开辟新领域。


<details>
  <summary>Details</summary>
Motivation: 生物制造中数据稀缺和稀疏给模型开发、过程监控和优化带来挑战，需生成合成数据。

Method: 提出使用带梯度惩罚的量子Wasserstein生成对抗网络（QWGAN - GP），生成器由参数化量子电路（PQC）组成。

Result: 在捕捉真实生物过程数据的时间动态方面表现可接受，生成数据与实际历史实验数据高度吻合。

Conclusion: 量子计算和机器学习的结合为数据分析和生成开辟新前沿，可用于提高软传感器设计预测准确性或预测控制。

Abstract: Data scarcity and sparsity in bio-manufacturing poses challenges for accurate
model
  development, process monitoring, and optimization. We aim to replicate and
capture
  the complex dynamics of industrial bioprocesses by proposing the use of a
Quantum
  Wasserstein Generative Adversarial Network with Gradient Penalty (QWGAN-GP)
to
  generate synthetic time series data for industrially relevant processes. The
  generator within our GAN is comprised of a Parameterized Quantum Circuit
(PQC). This
  methodology offers potential advantages in process monitoring, modeling,
  forecasting, and optimization, enabling more efficient bioprocess management
by
  reducing the dependence on scarce experimental data. Our results demonstrate
  acceptable performance in capturing the temporal dynamics of real bioprocess
data.
  We focus on Optical Density, a key measurement for Dry Biomass estimation.
The data
  generated showed high fidelity to the actual historical experimental data.
This
  intersection of quantum computing and machine learning has opened new
frontiers in
  data analysis and generation, particularly in computationally intensive
fields, for
  use cases such as increasing prediction accuracy for soft sensor design or
for use
  in predictive control.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [430] [Efficient Tensor Completion Algorithms for Highly Oscillatory Operators](https://arxiv.org/abs/2510.17734)
*Navjot Singh,Edgar Solomonik,Xiaoye Sherry Li,Yang Liu*

Main category: math.NA

TL;DR: 本文提出低复杂度张量补全算法及高效实现，用于重构高度振荡算子离散化后的矩阵，通过数值实验证明算法高效且误差小。


<details>
  <summary>Details</summary>
Motivation: 重构高度振荡算子离散化后的n×n矩阵，提高计算效率和重构精度。

Method: 基于输入矩阵重塑和蝶形分解为O(log n)阶张量，提出蝶形格式的两种张量补全算法（交替最小二乘法和基于梯度的优化），并使用低秩矩阵补全生成初始猜测。

Result: 使用O(n log n)个观测项，算法计算成本为O(n log³ n)，每次迭代速度比低秩矩阵和量化张量列车补全快几个数量级，重构误差小一个数量级。

Conclusion: 所提出的蝶形补全算法结合新的初始猜测生成策略，比现有补全算法更高效、准确，能准确恢复底层结构。

Abstract: This paper presents low-complexity tensor completion algorithms and their
efficient implementation to reconstruct highly oscillatory operators
discretized as $n\times n$ matrices. The underlying tensor decomposition is
based on the reshaping of the input matrix and its butterfly decomposition into
an order $\mathcal{O} (\log n)$ tensor. The reshaping of the input matrix into
a tensor allows for representation of the butterfly decomposition as a tensor
decomposition with dense tensors. This leads to efficient utilization of the
existing software infrastructure for dense and sparse tensor computations. We
propose two tensor completion algorithms in the butterfly format, using
alternating least squares and gradient-based optimization, as well as a novel
strategy that uses low-rank matrix completion to efficiently generate an
initial guess for the proposed algorithms. To demonstrate the efficiency and
applicability of our proposed algorithms, we perform three numerical
experiments using simulated oscillatory operators in seismic applications. In
these experiments, we use $\mathcal {O} (n \log n)$ observed entries in the
input matrix and demonstrate an $\mathcal{O}(n\log^3 n)$ computational cost of
the proposed algorithms, leading to a speedup of orders of magnitudes per
iteration for large matrices compared to the low-rank matrix and quantized
tensor-train completion. Moreover, the proposed butterfly completion
algorithms, equipped with the novel initial guess generation strategy, achieve
reconstruction errors that are smaller by an order of magnitude, enabling
accurate recovery of the underlying structure compared to the state-of-the-art
completion algorithms.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [431] [Conformal Prediction in The Loop: A Feedback-Based Uncertainty Model for Trajectory Optimization](https://arxiv.org/abs/2510.16376)
*Han Wang,Chao Ning*

Main category: math.OC

TL;DR: 提出基于反馈的共形预测（Fb - CP）框架用于收缩时域轨迹优化，可反馈信息调整预测区域，有理论保证且能处理分布偏移，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有共形预测方法采用顺序方案，决策信息无法反馈指导共形预测。

Method: 提出Fb - CP框架，开发基于CP的后验风险计算方法，利用已实现轨迹调整后验允许风险并分配到未来时刻更新预测区域；开发决策聚焦的迭代风险分配算法；扩展方法处理分布偏移。

Result: 所提方法能持续反馈信息调整预测区域，实现轨迹性能在线提升，理论证明能保持预测区域覆盖保证，确保安全性，基准实验验证了方法有效性和优越性。

Conclusion: 所提Fb - CP框架有效且优越，能应用于收缩时域轨迹优化，还可处理分布偏移问题。

Abstract: Conformal Prediction (CP) is a powerful statistical machine learning tool to
construct uncertainty sets with coverage guarantees, which has fueled its
extensive adoption in generating prediction regions for decision-making tasks,
e.g., Trajectory Optimization (TO) in uncertain environments. However, existing
methods predominantly employ a sequential scheme, where decisions rely
unidirectionally on the prediction regions, and consequently the information
from decision-making fails to be fed back to instruct CP. In this paper, we
propose a novel Feedback-Based CP (Fb-CP) framework for shrinking-horizon TO
with a joint risk constraint over the entire mission time. Specifically, a
CP-based posterior risk calculation method is developed by fully leveraging the
realized trajectories to adjust the posterior allowable risk, which is then
allocated to future times to update prediction regions. In this way, the
information in the realized trajectories is continuously fed back to the CP,
enabling attractive feedback-based adjustments of the prediction regions and a
provable online improvement in trajectory performance. Furthermore, we
theoretically prove that such adjustments consistently maintain the coverage
guarantees of the prediction regions, thereby ensuring provable safety.
Additionally, we develop a decision-focused iterative risk allocation algorithm
with theoretical convergence analysis for allocating the posterior allowable
risk which closely aligns with Fb-CP. Furthermore, we extend the proposed
method to handle distribution shift. The effectiveness and superiority of the
proposed method are demonstrated through benchmark experiments.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [432] [Enhancing Channel Estimation in RIS-aided Systems via Observation Matrix Design](https://arxiv.org/abs/2510.16576)
*Zijian Zhang,Mingyao Cui*

Main category: cs.IT

TL;DR: 本文提出新颖观测矩阵设计方案提升RIS信道估计器性能，采用贝叶斯优化框架，开发ARMO算法，引入自适应核训练策略，仿真显示该估计器精度大幅提升。


<details>
  <summary>Details</summary>
Motivation: RIS技术需准确信道估计以发挥性能潜力，现有RIS信道估计器有待提升。

Method: 提出新颖观测矩阵设计方案，采用贝叶斯优化框架生成观测矩阵，开发ARMO算法更新接收器组合器和RIS相移矩阵，引入自适应核训练策略迭代细化信道协方差矩阵。

Result: 仿真结果表明，所提出的ARMO增强估计器在估计精度上比现有技术方法有显著提升。

Conclusion: 所提方案能有效提升RIS信道估计精度。

Abstract: Reconfigurable intelligent surfaces (RISs) have emerged as a promising
technology for enhancing wireless communications through dense antenna arrays.
Accurate channel estimation is critical to unlocking their full performance
potential. To enhance RIS channel estimators, this paper proposes a novel
observation matrix design scheme. Bayesian optimization framework is adopted to
generate observation matrices that maximize the mutual information between
received pilot signals and RIS channels. To solve the formulated problem
efficiently, we develop an alternating Riemannian manifold optimization (ARMO)
algorithm to alternately update the receiver combiners and RIS phase-shift
matrices. An adaptive kernel training strategy is further introduced to
iteratively refine the channel covariance matrix without requiring additional
pilot resources. Simulation results demonstrate that the proposed ARMO-enhanced
estimator achieves substantial gains in estimation accuracy over
state-of-the-art methods.

</details>


### [433] [A Semantic Generalization of Shannon's Information Theory and Applications](https://arxiv.org/abs/2510.15871)
*Chenguang Lu*

Main category: cs.IT

TL;DR: 本文提出香农信息理论的语义泛化（G理论），将失真约束替换为语义约束，介绍其在多领域应用，探讨与统计物理联系，提出最大信息效率原则并比较不同理论，指出G理论局限。


<details>
  <summary>Details</summary>
Motivation: 探讨语义通信是否需要平行于香农信息理论的语义信息理论，还是可对香农理论进行泛化用于语义通信。

Method: 用一组真值函数作为语义通道，将失真约束替换为语义约束。

Result: 最大语义信息准则与最大似然准则等价，类似正则化最小二乘法准则；展示G理论在多领域的应用；探讨与统计物理概念的相似性；提出最大信息效率原则。

Conclusion: 香农信息理论可进行语义泛化用于语义通信，但G理论在表示复杂数据语义方面存在局限。

Abstract: Does semantic communication require a semantic information theory parallel to
Shannon's information theory, or can Shannon's work be generalized for semantic
communication? This paper advocates for the latter and introduces a semantic
generalization of Shannon's information theory (G theory for short). The core
idea is to replace the distortion constraint with the semantic constraint,
achieved by utilizing a set of truth functions as a semantic channel. These
truth functions enable the expressions of semantic distortion, semantic
information measures, and semantic information loss. Notably, the maximum
semantic information criterion is equivalent to the maximum likelihood
criterion and similar to the Regularized Least Squares criterion. This paper
shows G theory's applications to daily and electronic semantic communication,
machine learning, constraint control, Bayesian confirmation, portfolio theory,
and information value. The improvements in machine learning methods involve
multilabel learning and classification, maximum mutual information
classification, mixture models, and solving latent variables. Furthermore,
insights from statistical physics are discussed: Shannon information is similar
to free energy; semantic information to free energy in local equilibrium
systems; and information efficiency to the efficiency of free energy in
performing work. The paper also proposes refining Friston's minimum free energy
principle into the maximum information efficiency principle. Lastly, it
compares G theory with other semantic information theories and discusses its
limitation in representing the semantics of complex data.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [434] [Opinion Maximization in Social Networks by Modifying Internal Opinions](https://arxiv.org/abs/2510.17226)
*Gengyu Wang,Runze Zhang,Zhongzhi Zhang*

Main category: cs.SI

TL;DR: 本文提出算法解决社交网络中最大化整体舆论问题，实验表明方法优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵求逆方法计算成本高，需解决社交网络中最大化整体舆论问题。

Method: 提出两种基于采样的高效算法，开发确定性异步算法通过异步更新和逐步细化确定最优节点集。

Result: 在真实数据集上实验，方法优于基线方法，异步算法在各场景效率和准确性出色。

Conclusion: 所提算法能有效解决社交网络中最大化整体舆论问题，异步算法表现卓越。

Abstract: Public opinion governance in social networks is critical for public health
campaigns, political elections, and commercial marketing. In this paper, we
addresse the problem of maximizing overall opinion in social networks by
strategically modifying the internal opinions of key nodes. Traditional matrix
inversion methods suffer from prohibitively high computational costs, prompting
us to propose two efficient sampling-based algorithms. Furthermore, we develop
a deterministic asynchronous algorithm that exactly identifies the optimal set
of nodes through asynchronous update operations and progressive refinement,
ensuring both efficiency and precision. Extensive experiments on real-world
datasets demonstrate that our methods outperform baseline approaches. Notably,
our asynchronous algorithm delivers exceptional efficiency and accuracy across
all scenarios, even in networks with tens of millions of nodes.

</details>


### [435] [HyperSearch: Prediction of New Hyperedges through Unconstrained yet Efficient Search](https://arxiv.org/abs/2510.17153)
*Hyunjin Choo,Fanchen Bu,Hyunjin Hwang,Young-Gyu Yoon,Kijung Shin*

Main category: cs.SI

TL;DR: 提出HyperSearch算法用于超边预测，结合经验评分函数和高效搜索机制，在多个领域的真实超图实验中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有超边预测方法因候选超边搜索空间大，存在计算挑战，依赖启发式采样或无根据假设。

Method: 提出HyperSearch算法，包含基于现实超图观察的经验评分函数和利用反单调上界剪枝搜索空间的高效搜索机制。

Result: 在五个领域的10个真实超图上的实验中，HyperSearch始终优于现有基线，预测新超边的准确率更高。

Conclusion: HyperSearch算法能有效解决超边预测问题，提升预测准确性。

Abstract: Higher-order interactions (HOIs) in complex systems, such as scientific
collaborations, multi-protein complexes, and multi-user communications, are
commonly modeled as hypergraphs, where each hyperedge (i.e., a subset of nodes)
represents an HOI among the nodes. Given a hypergraph, hyperedge prediction
aims to identify hyperedges that are either missing or likely to form in the
future, and it has broad applications, including recommending interest-based
social groups, predicting collaborations, and uncovering functional complexes
in biological systems. However, the vast search space of hyperedge candidates
(i.e., all possible subsets of nodes) poses a significant computational
challenge, making naive exhaustive search infeasible. As a result, existing
approaches rely on either heuristic sampling to obtain constrained candidate
sets or ungrounded assumptions on hypergraph structure to select promising
hyperedges.
  In this work, we propose HyperSearch, a search-based algorithm for hyperedge
prediction that efficiently evaluates unconstrained candidate sets, by
incorporating two key components: (1) an empirically grounded scoring function
derived from observations in real-world hypergraphs and (2) an efficient search
mechanism, where we derive and use an anti-monotonic upper bound of the
original scoring function (which is not antimonotonic) to prune the search
space. This pruning comes with theoretical guarantees, ensuring that discarded
candidates are never better than the kept ones w.r.t. the original scoring
function. In extensive experiments on 10 real-world hypergraphs across five
domains, HyperSearch consistently outperforms state-of-the-art baselines,
achieving higher accuracy in predicting new (i.e., not in the training set)
hyperedges.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [436] [A Note on Algorithms for Computing $p_n$](https://arxiv.org/abs/2510.16285)
*Ansh Aggarwal*

Main category: math.NT

TL;DR: 分析计算第n个素数p_n的算法，给出渐近界，提出改进算法并暗示进一步优化方向。


<details>
  <summary>Details</summary>
Motivation: 研究计算第n个素数p_n的高效算法。

Method: 利用素数计数函数复杂度结果分析二分查找法，假设黎曼假设和克拉默猜想构建更窄区间改进筛法。

Result: 二分查找法计算p_n时间复杂度为O(√n (log n)^4)，改进筛法时间复杂度为O(√n (log^(7/2) n) log log n)。

Conclusion: 虽改进算法有条件限制，但对素数间隙估计的进一步细化可能产生更快计算素数的方法。

Abstract: We analyze algorithms for computing the $n$th prime $p_n$ and establish
asymptotic bounds for several approaches. Using existing results on the
complexity of evaluating the prime-counting function $\pi(x)$, we show that the
binary search approach computes $p_n$ in $O(\sqrt{n} \, (\log n)^4)$ time.
Assuming the Riemann Hypothesis and Cram\'er's conjecture, we construct a
tighter interval around li$^{-1}(n)$, leading to an improved sieve-based
algorithm running in $O(\sqrt{n} \, (\log ^{7/2} n) \, \log \log n)$ time. This
improvement, though conditional, suggests that further refinements to prime gap
estimates may yield provably faster methods for computing primes.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [437] [Plasma Shape Control via Zero-shot Generative Reinforcement Learning](https://arxiv.org/abs/2510.17531)
*Niannian Wu,Rongpeng Li,Zongyu Yang,Yong Xiao,Ning Wei,Yihang Chen,Bo Li,Zhifeng Zhao,Wulyu Zhong*

Main category: physics.plasm-ph

TL;DR: 本文提出新框架，结合GAIL与希尔伯特空间表征学习，基于历史PID控制放电数据集开发零样本控制策略，在HL - 3托卡马克模拟器评估效果良好，为未来聚变反应堆智能控制系统提供途径。


<details>
  <summary>Details</summary>
Motivation: 传统PID控制器对等离子体形状控制适应性有限，特定任务强化学习方法泛化性差且需重复再训练。

Method: 提出新框架，结合Generative Adversarial Imitation Learning (GAIL)与希尔伯特空间表征学习，基于历史PID控制放电的大规模离线数据集开发零样本控制策略。

Result: 在HL - 3托卡马克模拟器上的评估表明，该策略能在一系列等离子体场景中精确稳定地跟踪关键形状参数的参考轨迹。

Conclusion: 为未来聚变反应堆开发高度灵活且数据高效的智能控制系统提供了可行途径。

Abstract: Traditional PID controllers have limited adaptability for plasma shape
control, and task-specific reinforcement learning (RL) methods suffer from
limited generalization and the need for repetitive retraining. To overcome
these challenges, this paper proposes a novel framework for developing a
versatile, zero-shot control policy from a large-scale offline dataset of
historical PID-controlled discharges. Our approach synergistically combines
Generative Adversarial Imitation Learning (GAIL) with Hilbert space
representation learning to achieve dual objectives: mimicking the stable
operational style of the PID data and constructing a geometrically structured
latent space for efficient, goal-directed control. The resulting foundation
policy can be deployed for diverse trajectory tracking tasks in a zero-shot
manner without any task-specific fine-tuning. Evaluations on the HL-3 tokamak
simulator demonstrate that the policy excels at precisely and stably tracking
reference trajectories for key shape parameters across a range of plasma
scenarios. This work presents a viable pathway toward developing highly
flexible and data-efficient intelligent control systems for future fusion
reactors.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [438] [MuseTok: Symbolic Music Tokenization for Generation and Semantic Understanding](https://arxiv.org/abs/2510.16273)
*Jingyue Huang,Zachary Novack,Phillip Long,Yupeng Hou,Ke Chen,Taylor Berg-Kirkpatrick,Julian McAuley*

Main category: cs.SD

TL;DR: 提出音乐符号化的分词方法MuseTok，在音乐生成和理解任务中评估，表现优于先前基线。


<details>
  <summary>Details</summary>
Motivation: 受离散表示学习在多领域成果启发，研究其在音乐生成和理解任务中的有效性。

Method: 在基于Transformer的编解码器框架中，对逐小节音乐片段采用残差向量量化变分自编码器（RQ - VAE）生成音乐代码。

Result: 包含MuseTok的模型在语义理解上优于先前基线，内容生成表现相当，定性分析表明其能捕捉音乐概念。

Conclusion: MuseTok是有效的音乐符号化分词方法，能用于音乐生成和理解任务。

Abstract: Discrete representation learning has shown promising results across various
domains, including generation and understanding in image, speech and language.
Inspired by these advances, we propose MuseTok, a tokenization method for
symbolic music, and investigate its effectiveness in both music generation and
understanding tasks. MuseTok employs the residual vector quantized-variational
autoencoder (RQ-VAE) on bar-wise music segments within a Transformer-based
encoder-decoder framework, producing music codes that achieve high-fidelity
music reconstruction and accurate understanding of music theory. For
comprehensive evaluation, we apply MuseTok to music generation and semantic
understanding tasks, including melody extraction, chord recognition, and
emotion recognition. Models incorporating MuseTok outperform previous
representation learning baselines in semantic understanding while maintaining
comparable performance in content generation. Furthermore, qualitative analyses
on MuseTok codes, using ground-truth categories and synthetic datasets, reveal
that MuseTok effectively captures underlying musical concepts from large music
collections.

</details>


### [439] [Schrödinger Bridge Mamba for One-Step Speech Enhancement](https://arxiv.org/abs/2510.16834)
*Jing Yang,Sirui Wang,Chao Wu,Fan Fan*

Main category: cs.SD

TL;DR: 提出Schrödinger Bridge Mamba (SBM)训练推理框架用于生成式语音增强，实验表现优，还探讨SB范式与选择性状态空间模型架构集成。


<details>
  <summary>Details</summary>
Motivation: 基于Schrödinger Bridge (SB)训练范式和选择性状态空间模型Mamba的内在兼容性，提出新的训练推理框架。

Method: 提出SBM概念并以生成式语音增强为例实现，在四个基准数据集上进行联合去噪和去混响任务实验。

Result: SBM仅1步推理就优于有1步或迭代推理的强基线模型，实现最佳实时因子（RTF）。

Conclusion: SB范式和选择性状态空间模型架构的集成是探索新深度生成模型的有前景方向，适用于广泛生成任务。

Abstract: We propose Schr\"odinger Bridge Mamba (SBM), a new concept of
training-inference framework motivated by the inherent compatibility between
Schr\"odinger Bridge (SB) training paradigm and selective state-space model
Mamba. We exemplify the concept of SBM with an implementation for generative
speech enhancement. Experiments on a joint denoising and dereverberation task
using four benchmark datasets demonstrate that SBM, with only 1-step inference,
outperforms strong baselines with 1-step or iterative inference and achieves
the best real-time factor (RTF). Beyond speech enhancement, we discuss the
integration of SB paradigm and selective state-space model architecture based
on their underlying alignment, which indicates a promising direction for
exploring new deep generative models potentially applicable to a broad range of
generative tasks. Demo page: https://sbmse.github.io

</details>


### [440] [Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations](https://arxiv.org/abs/2510.16893)
*Bo-Han Feng,Chien-Feng Liu,Yu-Hsuan Li Liang,Chih-Kai Yang,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 研究大音频语言模型在副语言变化下的安全对齐，构建数据集评估模型，发现安全不一致性。


<details>
  <summary>Details</summary>
Motivation: 大音频语言模型在副语言变化下的安全对齐研究不足，需系统研究说话者情感的作用。

Method: 构建多情感和强度的恶意语音指令数据集，评估多个先进的大音频语言模型。

Result: 不同情感引发不同程度的不安全响应，强度影响非单调，中等表达常带来最大风险。

Conclusion: 大音频语言模型存在被忽视的漏洞，需设计对齐策略确保在情感变化下的鲁棒性。

Abstract: Large audio-language models (LALMs) extend text-based LLMs with auditory
understanding, offering new opportunities for multimodal applications. While
their perception, reasoning, and task performance have been widely studied,
their safety alignment under paralinguistic variation remains underexplored.
This work systematically investigates the role of speaker emotion. We construct
a dataset of malicious speech instructions expressed across multiple emotions
and intensities, and evaluate several state-of-the-art LALMs. Our results
reveal substantial safety inconsistencies: different emotions elicit varying
levels of unsafe responses, and the effect of intensity is non-monotonic, with
medium expressions often posing the greatest risk. These findings highlight an
overlooked vulnerability in LALMs and call for alignment strategies explicitly
designed to ensure robustness under emotional variation, a prerequisite for
trustworthy deployment in real-world settings.

</details>


### [441] [SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models](https://arxiv.org/abs/2510.16917)
*Chih-Kai Yang,Yen-Ting Piao,Tzu-Wen Hsu,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 提出首个用于编辑大音频语言模型中听觉属性知识的基准SAKE，对七种编辑方法进行多维度评估，揭示挑战并为LALMs维护和适应提供新方向。


<details>
  <summary>Details</summary>
Motivation: 以往知识编辑工作主要集中在文本或视觉模态，需要针对听觉模态开展研究，以支持大音频语言模型在更多现实场景的维护和适应。

Method: 引入SAKE基准，在两个大音频语言模型上从可靠性、通用性、音频/文本局部性和可移植性四个维度对七种编辑方法进行基准测试。

Result: 结果凸显了一些挑战，如保留与编辑无关的属性内知识、将编辑推广到多模态推理以及在顺序更新下维持编辑。

Conclusion: SAKE为研究知识编辑如何扩展到听觉模态提供了原则性框架，为大音频语言模型在更多样化现实场景中的维护和适应开辟了新方向。

Abstract: Knowledge editing offers an efficient way to update model knowledge without
full retraining, but prior work has concentrated almost exclusively on textual
or visual modalities. We introduce SAKE, the first benchmark specifically
designed for editing auditory attribute knowledge in Large Audio-Language
Models (LALMs). Unlike factual updates, SAKE targets several abstract auditory
attributes, capturing knowledge types that go beyond conventional textual and
visual domains. We benchmark seven editing methods on two LALMs along four
dimensions: reliability, generality, audio/text locality, and portability.
Results highlight challenges such as preserving intra-attribute knowledge
unrelated to the edit, generalizing edits to multimodal reasoning, and
maintaining edits under sequential updates. SAKE provides a principled
framework to study how knowledge editing extends to the auditory modalities,
opening new directions for maintaining and adapting LALMs in more diverse
real-world scenarios.

</details>


### [442] [U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation](https://arxiv.org/abs/2510.16718)
*Xusheng Yang,Long Zhou,Wenfu Wang,Kai Hu,Shulin Feng,Chenxing Li,Meng Yu,Dong Yu,Yuexian Zou*

Main category: cs.SD

TL;DR: 提出超低帧率神经语音编解码器U - Codec，在5Hz帧率实现高保真重建和快速语音生成，应用于LLM - based TTS模型提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决5Hz极端压缩导致的语音可懂度和频谱细节损失问题，提升语音合成效率。

Method: 引入基于Transformer的帧间长期依赖模块，探索RVQ深度和码本大小的最优配置；将U - Codec应用于基于大语言模型的自回归TTS模型，采用全局和局部分层架构。

Result: U - Codec使基于大语言模型的TTS推理速度比高帧率编解码器提高约3倍，同时保持相似性和自然度。

Conclusion: 高度压缩的5Hz离散令牌可用于快速高保真语音合成。

Abstract: We propose \textbf{U-Codec}, an \textbf{U}ltra low frame-rate neural speech
\textbf{Codec} that achieves high-fidelity reconstruction and fast speech
generation at an extremely low frame-rate of 5Hz (5 frames per second). Extreme
compression at 5Hz typically leads to severe intelligibility and spectral
detail loss, we introduce a Transformer-based inter-frame long-term dependency
module and systematically explore residual vector quantization (RVQ) depth and
codebook size to identify optimal configurations. Moreover, we apply U-Codec
into a large language model (LLM)-based auto-regressive TTS model, which
leverages global and local hierarchical architecture to effectively capture
dependencies across multi-layer tokens. We extend LLM-based TTS from 3-layer
RVQ at 50Hz to 32-layer RVQ at 5Hz. Experimental results demonstrate that
U-Codec improves LLM-based TTS inference speed by around 3 $\times$ over
high-frame-rate codecs while maintaining similarity and naturalness. These
results validate the feasibility of using highly compressed 5Hz discrete tokens
for fast and high-fidelity speech synthesis.

</details>


### [443] [DDSC: Dynamic Dual-Signal Curriculum for Data-Efficient Acoustic Scene Classification under Domain Shift](https://arxiv.org/abs/2510.17345)
*Peihong Zhang,Yuxuan Liu,Rui Sang,Zhixin Li,Yiqiang Cai,Yizhou Tan,Shengchen Li*

Main category: cs.SD

TL;DR: 提出动态双信号课程（DDSC）训练策略，解决声学场景分类中设备引起的领域偏移问题，在不同基线和标签预算下提升跨设备性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于课程的训练计划是静态的，忽略了示例难度和边际效用随学习表示的变化，无法解决声学场景分类中设备引起的领域偏移问题。

Method: 提出DDSC训练计划，结合域不变性信号和学习进度信号在线调整课程，用随时间变化的调度器将信号融合为每个示例的权重。

Result: 在官方DCASE 2024 Task 1协议下，DDSC在不同声学场景分类基线和标签预算下持续提高跨设备性能，在未见设备分割上增益最大。

Conclusion: DDSC轻量级、与架构无关且不增加推理开销，能有效解决声学场景分类中的领域偏移问题。

Abstract: Acoustic scene classification (ASC) suffers from device-induced domain shift,
especially when labels are limited. Prior work focuses on curriculum-based
training schedules that structure data presentation by ordering or reweighting
training examples from easy-to-hard to facilitate learning; however, existing
curricula are static, fixing the ordering or the weights before training and
ignoring that example difficulty and marginal utility evolve with the learned
representation. To overcome this limitation, we propose the Dynamic Dual-Signal
Curriculum (DDSC), a training schedule that adapts the curriculum online by
combining two signals computed each epoch: a domain-invariance signal and a
learning-progress signal. A time-varying scheduler fuses these signals into
per-example weights that prioritize domain-invariant examples in early epochs
and progressively emphasize device-specific cases. DDSC is lightweight,
architecture-agnostic, and introduces no additional inference overhead. Under
the official DCASE 2024 Task~1 protocol, DDSC consistently improves
cross-device performance across diverse ASC baselines and label budgets, with
the largest gains on unseen-device splits.

</details>


### [444] [TopSeg: A Multi-Scale Topological Framework for Data-Efficient Heart Sound Segmentation](https://arxiv.org/abs/2510.17346)
*Peihong Zhang,Zhixin Li,Yuxuan Liu,Rui Sang,Yiqiang Cai,Yizhou Tan,Shengchen Li*

Main category: cs.SD

TL;DR: 提出TopSeg框架用于心音分割，拓扑特征表现优，表明拓扑感知表示对数据高效分割有效。


<details>
  <summary>Details</summary>
Motivation: 基于时频特征的心音分割深度学习方法依赖大量标注数据，限制鲁棒性和部署，需新方法。

Method: 提出TopSeg框架，用多尺度拓扑特征编码心音动态，用轻量级TCN解码，有顺序和时长约束推理步骤，在特定数据集训练和验证。

Result: 拓扑特征表现超光谱图和包络输入，TopSeg超代表性基线，消融实验表明各尺度有贡献，结合H_0和H_1定位和边界更稳定。

Conclusion: 拓扑感知表示为数据高效、跨数据集心音分割提供强归纳偏置，适用于标注数据有限场景。

Abstract: Deep learning approaches for heart-sound (PCG) segmentation built on
time--frequency features can be accurate but often rely on large expert-labeled
datasets, limiting robustness and deployment. We present TopSeg, a topological
representation-centric framework that encodes PCG dynamics with multi-scale
topological features and decodes them using a lightweight temporal
convolutional network (TCN) with an order- and duration-constrained inference
step. To evaluate data efficiency and generalization, we train exclusively on
PhysioNet 2016 dataset with subject-level subsampling and perform external
validation on CirCor dataset. Under matched-capacity decoders, the topological
features consistently outperform spectrogram and envelope inputs, with the
largest margins at low data budgets; as a full system, TopSeg surpasses
representative end-to-end baselines trained on their native inputs under the
same budgets while remaining competitive at full data. Ablations at 10%
training confirm that all scales contribute and that combining H_0 and H_1
yields more reliable S1/S2 localization and boundary stability. These results
indicate that topology-aware representations provide a strong inductive bias
for data-efficient, cross-dataset PCG segmentation, supporting practical use
when labeled data are limited.

</details>


### [445] [AWARE: Audio Watermarking with Adversarial Resistance to Edits](https://arxiv.org/abs/2510.17512)
*Kosta Pavlović,Lazar Stanarević,Petar Nedić,Slavko Kovačević,Igor Djurović*

Main category: cs.SD

TL;DR: 提出AWARE音频水印方法，避免依赖攻击模拟，在时频域对抗优化嵌入，用特定检测器检测，效果超现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的音频水印方法依赖模拟失真集来追求鲁棒性，存在范围窄和过拟合问题。

Method: 在时频域基于比例感知预算进行对抗优化嵌入水印，检测使用带BRH的时间顺序无关检测器。

Result: AWARE在各种音频编辑下实现高音频质量、语音可懂度和低误码率，常超越现有方法。

Conclusion: AWARE是一种有效的音频水印方法，不依赖攻击模拟，性能表现优秀。

Abstract: Prevailing practice in learning-based audio watermarking is to pursue
robustness by expanding the set of simulated distortions during training.
However, such surrogates are narrow and prone to overfitting. This paper
presents AWARE (Audio Watermarking with Adversarial Resistance to Edits), an
alternative approach that avoids reliance on attack-simulation stacks and
handcrafted differentiable distortions. Embedding is obtained via adversarial
optimization in the time-frequency domain under a level-proportional perceptual
budget. Detection employs a time-order-agnostic detector with a Bitwise Readout
Head (BRH) that aggregates temporal evidence into one score per watermark bit,
enabling reliable watermark decoding even under desynchronization and temporal
cuts. Empirically, AWARE attains high audio quality and speech intelligibility
(PESQ/STOI) and consistently low BER across various audio edits, often
surpassing representative state-of-the-art learning-based audio watermarking
systems.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [446] [BASIN: Bayesian mAtrix variate normal model with Spatial and sparsIty priors in Non-negative deconvolution](https://arxiv.org/abs/2510.16130)
*Jiasen Zhang,Xi Qiao,Liangliang Zhang,Weihong Guo*

Main category: q-bio.QM

TL;DR: 本文提出用于细胞类型反卷积的BASIN方法，通过矩阵变分贝叶斯NMF结合图拉普拉斯先验，在不同空间转录组数据集上表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学缺乏细胞分辨率，需要细胞类型反卷积来推断每个空间位置的细胞组成。

Method: 将反卷积建模为结合图拉普拉斯先验的非负矩阵分解（NMF）问题，提出具有非负性和稀疏性先验的矩阵变分贝叶斯NMF方法，使用Gibbs采样器近似后验分布。

Result: BASIN在不同空间转录组数据集上的准确性和效率优于其他反卷积方法，结果显示了所纳入先验的效果并反映了预期的截断矩阵正态分布。

Conclusion: BASIN方法在空间转录组学的细胞类型反卷积中表现良好，能提供更高效、更鲁棒的解决方案。

Abstract: Spatial transcriptomics allows researchers to visualize and analyze gene
expression within the precise location of tissues or cells. It provides
spatially resolved gene expression data but often lacks cellular resolution,
necessitating cell type deconvolution to infer cellular composition at each
spatial location. In this paper we propose BASIN for cell type deconvolution,
which models deconvolution as a nonnegative matrix factorization (NMF) problem
incorporating graph Laplacian prior. Rather than find a deterministic optima
like other recent methods, we propose a matrix variate Bayesian NMF method with
nonnegativity and sparsity priors, in which the variables are maintained in
their matrix form to derive a more efficient matrix normal posterior. BASIN
employs a Gibbs sampler to approximate the posterior distribution of cell type
proportions and other parameters, offering a distribution of possible
solutions, enhancing robustness and providing inherent uncertainty
quantification. The performance of BASIN is evaluated on different spatial
transcriptomics datasets and outperforms other deconvolution methods in terms
of accuracy and efficiency. The results also show the effect of the
incorporated priors and reflect a truncated matrix normal distribution as we
expect.

</details>


### [447] [TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2510.16080)
*Kerem Delikoyun,Qianyu Chen,Win Sen Kuan,John Tshon Yit Soong,Matthew Edward Cove,Oliver Hayden*

Main category: q-bio.QM

TL;DR: 全球急诊科面临诸多挑战，现有分诊方法有局限。本文提出TriAgent框架，从数据分析到文献验证，表现优于现有框架并公布代码。


<details>
  <summary>Details</summary>
Motivation: 解决急诊科患者量上升、人力短缺、分诊决策差异问题，弥补现有分诊方法无法捕捉新兴生物信号的不足。

Method: 引入基于大语言模型的多智能体框架TriAgent，用主管研究智能体生成研究主题，委派查询给子智能体从多数据源检索证据，综合结果对生物标志物分类。

Result: TriAgent在主题遵循F1分数和忠诚度分数上表现良好，超越对比基线，在生物标志物验证和文献新颖性评估上优于现有框架。

Conclusion: TriAgent能提供从数据分析到文献验证的端到端框架，提升透明度和可解释性，扩展潜在可操作临床生物标志物边界。

Abstract: Emergency departments worldwide face rising patient volumes, workforce
shortages, and variability in triage decisions that threaten the delivery of
timely and accurate care. Current triage methods rely primarily on vital signs,
routine laboratory values, and clinicians' judgment, which, while effective,
often miss emerging biological signals that could improve risk prediction for
infection typing or antibiotic administration in acute conditions. To address
this challenge, we introduce TriAgent, a large language model (LLM)-based
multi-agent framework that couples automated biomarker discovery with deep
research for literature-grounded validation and novelty assessment. TriAgent
employs a supervisor research agent to generate research topics and delegate
targeted queries to specialized sub-agents for evidence retrieval from various
data sources. Findings are synthesized to classify biomarkers as either
grounded in existing knowledge or flagged as novel candidates, offering
transparent justification and highlighting unexplored pathways in acute care
risk stratification. Unlike prior frameworks limited to existing routine
clinical biomarkers, TriAgent aims to deliver an end-to-end framework from data
analysis to literature grounding to improve transparency, explainability and
expand the frontier of potentially actionable clinical biomarkers. Given a
user's clinical query and quantitative triage data, TriAgent achieved a topic
adherence F1 score of 55.7 +/- 5.0%, surpassing the CoT-ReAct agent by over
10%, and a faithfulness score of 0.42 +/- 0.39, exceeding all baselines by more
than 50%. Across experiments, TriAgent consistently outperformed
state-of-the-art LLM-based agentic frameworks in biomarker justification and
literature-grounded novelty assessment. We share our repo:
https://github.com/CellFace/TriAgent.

</details>


### [448] [Interpretable RNA-Seq Clustering with an LLM-Based Agentic Evidence-Grounded Framework](https://arxiv.org/abs/2510.16082)
*Elias Hossain,Mehrdad Shoeibi,Ivan Garibay,Niloofar Yousefi*

Main category: q-bio.QM

TL;DR: 提出CITE V.1框架用于RNA - seq聚类解释，优于现有方法，应用于沙门氏菌数据效果好，提升AI在生物医学的透明性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于富集的方法和仅使用大语言模型的方法在RNA - seq聚类解释上存在局限，需要更透明、可重复且基于证据的解释方法。

Method: 提出CITE V.1框架，包含Retriever、Interpreter和Critics三个专门的代理，从生物医学文献中获取知识进行解释。

Result: 应用于沙门氏菌RNA - seq数据，CITE V.1生成有生物学意义且有文献支持的见解，而仅使用大语言模型的Gemini基线常产生有虚假引用的推测性结果。

Conclusion: CITE V.1将RNA - seq分析从表面的富集推进到可审计、可解释和基于证据的假设生成，提升了AI在生物医学中的透明性和可靠性。

Abstract: We propose CITE V.1, an agentic, evidence-grounded framework that leverages
Large Language Models (LLMs) to provide transparent and reproducible
interpretations of RNA-seq clusters. Unlike existing enrichment-based
approaches that reduce results to broad statistical associations and LLM-only
models that risk unsupported claims or fabricated citations, CITE V.1
transforms cluster interpretation by producing biologically coherent
explanations explicitly anchored in the biomedical literature. The framework
orchestrates three specialized agents: a Retriever that gathers domain
knowledge from PubMed and UniProt, an Interpreter that formulates functional
hypotheses, and Critics that evaluate claims, enforce evidence grounding, and
qualify uncertainty through confidence and reliability indicators. Applied to
Salmonella enterica RNA-seq data, CITE V.1 generated biologically meaningful
insights supported by the literature, while an LLM-only Gemini baseline
frequently produced speculative results with false citations. By moving RNA-seq
analysis from surface-level enrichment to auditable, interpretable, and
evidence-based hypothesis generation, CITE V.1 advances the transparency and
reliability of AI in biomedicine.

</details>


### [449] [Few-Label Multimodal Modeling of SNP Variants and ECG Phenotypes Using Large Language Models for Cardiovascular Risk Stratification](https://arxiv.org/abs/2510.16536)
*Niranjana Arun Menon,Yulong Li,Iqra Farooq,Sara Ahmed,Muhammad Awais,Imran Razzak*

Main category: q-bio.QM

TL;DR: 提出利用大语言模型的少标签多模态框架用于心血管疾病风险分层，结合伪标签细化策略和思维链推理，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病风险分层因多因素特性和高质量标注数据集有限面临挑战，传统监督模型在低标签设置下难以有效整合基因组和电生理数据。

Method: 提出少标签多模态框架，利用大语言模型结合遗传和电生理信息，采用伪标签细化策略和思维链推理。

Result: 多模态输入、少标签监督和思维链推理结合提高了模型在不同患者群体中的鲁棒性和泛化性，与全量数据集训练的模型性能相当。

Conclusion: 基于大语言模型的少标签多模态建模有望推动个性化心血管护理。

Abstract: Cardiovascular disease (CVD) risk stratification remains a major challenge
due to its multifactorial nature and limited availability of high-quality
labeled datasets. While genomic and electrophysiological data such as SNP
variants and ECG phenotypes are increasingly accessible, effectively
integrating these modalities in low-label settings is non-trivial. This
challenge arises from the scarcity of well-annotated multimodal datasets and
the high dimensionality of biological signals, which limit the effectiveness of
conventional supervised models. To address this, we present a few-label
multimodal framework that leverages large language models (LLMs) to combine
genetic and electrophysiological information for cardiovascular risk
stratification. Our approach incorporates a pseudo-label refinement strategy to
adaptively distill high-confidence labels from weakly supervised predictions,
enabling robust model fine-tuning with only a small set of ground-truth
annotations. To enhance the interpretability, we frame the task as a Chain of
Thought (CoT) reasoning problem, prompting the model to produce clinically
relevant rationales alongside predictions. Experimental results demonstrate
that the integration of multimodal inputs, few-label supervision, and CoT
reasoning improves robustness and generalizability across diverse patient
profiles. Experimental results using multimodal SNP variants and ECG-derived
features demonstrated comparable performance to models trained on the full
dataset, underscoring the promise of LLM-based few-label multimodal modeling
for advancing personalized cardiovascular care.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [450] [ESG Signaling on Wall Street in the AI Era](https://arxiv.org/abs/2510.15956)
*Qionghua Chu*

Main category: q-fin.GN

TL;DR: 研究在AI兴起背景下ESG投资的新信号渠道，发现ESG分数与债务 - 总资本比率正相关，为ESG投资研究做贡献。


<details>
  <summary>Details</summary>
Motivation: 在大型机构投资者转向AI背景下，检验ESG投资是否仍有价值，识别新的信号渠道。

Method: 使用雅虎财经中标准普尔500公司经过缩尾处理的ESG分数，控制股权市场价值，进行横截面回归来测试信号机制。

Result: 环境、社会、治理和综合ESG分数无论是单独还是各种组合，都强烈且正相关地预示着更高的债务 - 总资本比率。

Conclusion: 研究为ESG投资的相关文献做出贡献，在AI兴起背景下为长期投资组合管理提供了有经济意义的信号渠道。

Abstract: I identify a new signaling channel in ESG research by empirically examining
whether environmental, social, and governance (ESG) investing remains valuable
as large institutional investors increasingly shift toward artificial
intelligence (AI). Using winsorized ESG scores of S&P 500 firms from Yahoo
Finance and controlling for market value of equity, I conduct cross-sectional
regressions to test the signaling mechanism. I demonstrate that Environmental,
Social, Governance, and composite ESG scores strongly and positively signal
higher debt-to-total-capital ratio, both individually and in various
combinations. My findings contribute to the growing literature on ESG
investing, offering economically meaningful signaling channel with implications
for long-term portfolio management amid the rise of AI.

</details>


### [451] [A study about who is interested in stock splitting and why: considering companies, shareholders or managers](https://arxiv.org/abs/2510.15879)
*Jiaquan Nicholas Chen,Marcel Ausloos*

Main category: q-fin.GN

TL;DR: 本文研究股票拆分问题，通过分析九起事件，发现股票拆分短期影响市场、增加交易量等，且能减少信息不对称。


<details>
  <summary>Details</summary>
Motivation: 澄清围绕股票价格、拆分等存在的误解，探究公司、管理者进行股票拆分及股东同意的原因。

Method: 选取数据库，讨论近年九起事件，观察信息不对称作用、事件前后回报和交易量，计算各样本的贝塔值。

Result: 股票拆分短期影响市场、增加交易量，扩大股东基础，对市场流动性有积极影响。

Conclusion: 股票拆分公告可降低信息不对称程度，投资者会调整对公司的预期，但拆分当年多数公司股价被错误定价。

Abstract: There are many misconceptions around stock prices, stock splits,
shareholders, investors, and managers behaviour about such informations due to
a number of confounding factors. This paper tests hypotheses with a selected
database, about the question ''is stock split attractive for companies?'' in
another words, ''why companies split their stock?'', ''why managers split their
stock?'', sometimes for no benefit, and ''why shareholders agree with such
decisions?''. We contribute to the existing knowledge through a discussion of
nine events in recent (selectively chosen) years, observing the role of
information asymmetries, the returns and traded volumes before and after the
event. Therefore, calculating the beta for each sample, it is found that stock
splits (i) affect the market and slightly enhance the trading volume in a
short-term, (ii) increase the shareholder base for its firm, (iii) have a
positive effect on the liquidity of the market. We concur that stock split
announcements can reduce the level of information asymmetric. Investors
readjust their beliefs in the firm, although most of the firms are mispriced in
the stock split year.

</details>


### [452] [Sleeping Kelly is a Thirder](https://arxiv.org/abs/2510.15911)
*Ben Abramowitz*

Main category: q-fin.GN

TL;DR: 本文讨论睡美人问题，主张用凯利准则最大化财富增长率得出“三分之一”立场，且该立场通过荷兰赌论证是“理性”的，不同赌注结构可能改变立场。


<details>
  <summary>Details</summary>
Motivation: 探讨睡美人问题的解决方案，分析不同决策方式的合理性。

Method: 允许睡美人基于信念做决策，用凯利准则最大化财富增长率，并通过荷兰赌论证。

Result: 用凯利准则得出“三分之一”立场，“三分之一”立场在特定情况下能避免荷兰赌，“二分之一”立场易受荷兰赌影响。

Conclusion: 睡美人用凯利准则最大化财富增长率得出“三分之一”立场是理性的，但赌注结构不同可能改变立场。

Abstract: The Sleeping Beauty problem was presented by Elga and highlights the role of
probabilities in situations with imperfect recall. One approach to solving the
Sleeping Beauty problem is to allow Sleeping Beauty to make decisions based on
her beliefs, and then characterize what it takes for her decisions to be
"rational". In particular, she can be allowed to make monetary bets based on
her beliefs, with the assumption that she wants to gain wealth rather than lose
it. However, this approach is often coupled with the assumption that Sleeping
Beauty should maximize the expected value of her bets. Here, I argue instead
that it is rational for Sleeping Beauty to maximize the growth rate of her
wealth using the Kelly Criterion, which leads us to the "thirder" position.
Furthermore, this position is shown to be "rational" by Dutch book arguments.
If Sleeping Kelly only accepts bets that have a growth rate greater than 1 as a
"thirder" then she is not vulnerable to Dutch books. By contrast, if Sleeping
Beauty takes the "halfer" position, she is vulnerable to Dutch books. If the
bets offered to Sleeping Beauty were to be structured differently and lead to
non-multiplicative wealth dynamics, she may no longer be a "thirder".

</details>


### [453] [Geometric Dynamics of Consumer Credit Cycles: A Multivector-based Linear-Attention Framework for Explanatory Economic Analysis](https://arxiv.org/abs/2510.15892)
*Agus Sudjianto,Sandi Setiawan*

Main category: q-fin.GN

TL;DR: 引入几何代数分解信用系统关系，揭示系统危机特征的旋转动态。


<details>
  <summary>Details</summary>
Motivation: 为了更深入分析信用系统关系，发现传统分析难以察觉的交互模式。

Method: 引入几何代数，将经济状态表示为Clifford代数中的多向量，用二向量元素捕捉变量间的旋转耦合。

Result: 发现当失业和信贷收缩同时进入反馈循环时，其几何关系从简单相关转变为危险的旋转动态。

Conclusion: 该数学框架能揭示传统分析看不见的交互模式，有助于识别系统危机。

Abstract: This study introduces geometric algebra to decompose credit system
relationships into their projective (correlation-like) and rotational
(feedback-spiral) components. We represent economic states as multi-vectors in
Clifford algebra, where bivector elements capture the rotational coupling
between unemployment, consumption, savings, and credit utilization. This
mathematical framework reveals interaction patterns invisible to conventional
analysis: when unemployment and credit contraction enter simultaneous feedback
loops, their geometric relationship shifts from simple correlation to dangerous
rotational dynamics that characterize systemic crises.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [454] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 本文提出Neuronal Group Communication (NGC)框架，用于构建高效、模块化和可解释的大型神经网络，在大语言模型中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络规模增大带来效率和可解释性挑战，旨在构建能学习高效、模块化和可解释表示的大型神经系统。

Method: 提出NGC框架，将神经网络视为神经元组交互的动态系统，用神经元稳定性指标量化激活收缩，实例化到大型语言模型中。

Result: 在适度压缩下，NGC在复杂推理基准测试中表现更好，优于标准低秩近似和跨层基共享方法。

Conclusion: 讨论了NGC的更广泛意义，如结构化神经元组动态与高维学习系统泛化的关系。

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [455] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: 提出FinSight多智能体框架生成高质量多模态财务报告，实验显示其显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以完全自动化生成专业财务报告，为解决此挑战开展研究。

Method: 采用Code Agent with Variable Memory (CAVM)架构统一数据、工具和智能体；提出迭代视觉增强机制优化可视化；使用两阶段写作框架生成连贯报告。

Result: 在各种公司和行业级任务实验中，FinSight在事实准确性、分析深度和呈现质量上显著优于所有基线。

Conclusion: FinSight为生成接近人类专家水平的报告指明了清晰路径。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [456] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文系统比较基于提示和基于强化学习的查询增强方法，发现无训练查询增强表现良好，还提出混合方法OPQE，其效果更佳。


<details>
  <summary>Details</summary>
Motivation: 现有两种查询增强方法未在一致实验条件下比较，需了解其性能差异并探索更好方法。

Method: 对两种查询增强方法进行系统比较，提出On - policy Pseudo - document Query Expansion (OPQE) 混合方法。

Result: 简单无训练查询增强常与昂贵的基于强化学习方法表现相当甚至更好；OPQE优于单一提示和基于强化学习的重写方法。

Conclusion: 简单无训练查询增强在强大大语言模型下效果好，混合方法结合两者优势能取得最佳结果。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [457] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 文章提出用于通用检索增强生成（URAG）的Nyx系统，构建NyxQA数据集，采用两阶段训练框架，实验表明Nyx在文本和混合模态任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统主要关注单模态文本，在混合模态场景表现不佳，需要解决混合模态信息检索和推理以提升视觉语言生成。

Method: 提出Nyx系统，引入四阶段自动化生成和过滤流程构建NyxQA数据集，采用两阶段训练框架，先预训练，再监督微调。

Result: Nyx在标准文本RAG基准测试中表现有竞争力，在更通用和现实的URAG场景中表现出色，显著提升视觉语言任务的生成质量。

Conclusion: Nyx能有效解决混合模态信息检索和推理问题，提升视觉语言生成质量。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [458] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本文开发了基于ChatGPT的中医多模态助手BenCao，经评估准确性高且已部署应用，展示了开发中医领域大语言模型的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有中医领域大语言模型缺乏多模态集成、可解释性和临床适用性，需改进。

Method: 开发BenCao，通过自然语言指令调优而非参数再训练，整合知识库、诊断数据和专家反馈，还连接外部API。

Result: 在评估中BenCao准确性超通用和中医领域模型，已部署应用，有近1000全球用户。

Conclusion: 通过自然语言指令调优和多模态集成开发中医领域大语言模型可行，提供实用框架和可扩展途径。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [459] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 提出代码高阶相关性，设计生成器，改进超图神经网络架构结合适配器微调预训练语言模型，实验验证其提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练语言模型未考虑代码内潜在高阶数据相关性。

Method: 提出三种代码标记高阶相关性，设计标记和超边生成器，改进超图神经网络架构并结合适配器微调，提出HGAdapter。

Result: 在多个公共数据集的代码摘要和克隆检测任务中，不同程度提升了预训练语言模型的性能。

Conclusion: 引入高阶数据相关性有助于提高有效性。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [460] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 提出Executable Knowledge Graphs (xKG)解决大语言模型复制AI研究难题，在PaperBench上有性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型复制AI研究方法生成可执行代码困难，背景知识不足，RAG方法有局限，且忽视代码信号、缺乏结构化知识表示。

Method: 提出模块化、可插拔的知识库Executable Knowledge Graphs (xKG)，自动整合从科学文献中提取的技术见解、代码片段和特定领域知识。

Result: 将xKG集成到三种代理框架和两种不同的大语言模型中，在PaperBench上有显著性能提升（o3 - mini提升10.9%）。

Conclusion: xKG是用于自动AI研究复制的通用且可扩展的解决方案。

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [461] [Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers](https://arxiv.org/abs/2510.13939)
*Tuhin Chakrabarty,Jane C. Ginsburg,Paramveer Dhillon*

Main category: cs.CL

TL;DR: 研究对比MFA训练的专业作家与三种前沿AI模型模仿获奖作者风格写作，发现微调前AI文本不受专家青睐，微调后逆转，成本大降。


<details>
  <summary>Details</summary>
Motivation: 探讨AI模型在模仿作者风格时能否生成高质量文学文本，以及解决AI训练使用版权书籍引发的法律争议。

Method: 进行预注册研究，让AI模型和专业作家模仿50位获奖作者风格写450字以内片段，由专家和普通读者进行盲评。

Result: 微调前AI文本不受专家青睐，微调后专家和普通读者都更青睐，微调输出难被AI检测器识别，成本大幅降低。

Conclusion: 特定作者的微调使AI写作受读者喜爱，为版权合理使用的第四个因素提供实证。

Abstract: The use of copyrighted books for training AI models has led to numerous
lawsuits from authors concerned about AI's ability to generate derivative
content. Yet it's unclear if these models can generate high quality literary
text while emulating authors' styles. To answer this we conducted a
preregistered study comparing MFA-trained expert writers with three frontier AI
models: ChatGPT, Claude & Gemini in writing up to 450 word excerpts emulating
50 award-winning authors' diverse styles. In blind pairwise evaluations by 159
representative expert & lay readers, AI-generated text from in-context
prompting was strongly disfavored by experts for both stylistic fidelity
(OR=0.16, p<10^-8) & writing quality (OR=0.13, p<10^-7) but showed mixed
results with lay readers. However, fine-tuning ChatGPT on individual authors'
complete works completely reversed these findings: experts now favored
AI-generated text for stylistic fidelity (OR=8.16, p<10^-13) & writing quality
(OR=1.87, p=0.010), with lay readers showing similar shifts. These effects
generalize across authors & styles. The fine-tuned outputs were rarely flagged
as AI-generated (3% rate v. 97% for in-context prompting) by best AI detectors.
Mediation analysis shows this reversal occurs because fine-tuning eliminates
detectable AI stylistic quirks (e.g., cliche density) that penalize in-context
outputs. While we do not account for additional costs of human effort required
to transform raw AI output into cohesive, publishable prose, the median
fine-tuning & inference cost of $81 per author represents a dramatic 99.7%
reduction compared to typical professional writer compensation. Author-specific
fine-tuning thus enables non-verbatim AI writing that readers prefer to expert
human writing, providing empirical evidence directly relevant to copyright's
fourth fair-use factor, the "effect upon the potential market or value" of the
source works.

</details>


### [462] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 本文研究QNLP模型在自然语言推理任务中的应用，对比量子、混合和经典模型，引入新指标评估效率，结果显示量子模型参数少、效率高，还提出新架构。


<details>
  <summary>Details</summary>
Motivation: 探索QNLP模型在自然语言推理任务中的应用，在受限少样本设置下对比不同类型模型。

Method: 使用lambeq库和DisCoCat框架构建参数化量子电路，引入信息增益每参数（IGPP）指标评估效率，提出基于聚类的架构。

Result: 量子模型性能与经典基线相当，参数少，在推理中优于随机初始化的变压器，在相关性任务中测试误差低，每参数学习效率高。

Conclusion: QNLP在低资源、结构敏感的设置中具有潜力，提出的新架构可提高泛化能力。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [463] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 研究提出多模型融合框架，利用ChatGPT和Claude提升胸部X光解读可靠性，融合方法表现更佳。


<details>
  <summary>Details</summary>
Motivation: 提升胸部X光解读的可靠性，减少诊断错误。

Method: 采用多模型融合框架，利用ChatGPT和Claude，用相似度共识方法，还评估单模态和多模态输入情况。

Result: 单模态下ChatGPT和Claude准确率分别为62.8%和76.9%，共识方法提升到77.6%；多模态下ChatGPT和Claude分别为84%和76%，共识准确率达91.3%。

Conclusion: 整合互补模态和使用输出级共识能提高AI辅助放射诊断的可信度和临床实用性，以最小计算开销减少诊断错误。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [464] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 本文引入CorrectBench评估大语言模型自纠错策略，发现自纠错可提升推理准确性，混合策略有进一步提升但降低效率，部分推理模型优化有限且成本高，简单基线有竞争力，呼吁平衡推理能力与效率的研究。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对大语言模型自纠错方法的全面评估，且大模型能否真正自我纠错受关注，因此开展相关研究。

Method: 引入CorrectBench基准，评估内在、外部和微调三种自纠错策略在常识推理、数学推理和代码生成三个任务中的效果。

Result: 自纠错方法能提高准确性，尤其对复杂推理任务；混合不同策略有进一步提升但降低效率；部分推理大模型优化有限且时间成本高；简单基线有竞争力的准确性和效率。

Conclusion: 自纠错有提升大模型推理性能的潜力，但提高效率仍有挑战，应进一步研究平衡推理能力和运营效率。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [465] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: 提出EvolveR框架使大语言模型代理能通过闭环经验生命周期自我提升，在多跳问答基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代理缺乏从自身经验系统学习和迭代优化解决策略的能力，现有框架未解决此问题。

Method: 引入EvolveR框架，包含离线自我蒸馏（将交互轨迹合成抽象策略原则库）和在线交互（与任务交互并检索原则指导决策）两个阶段，采用策略强化机制迭代更新。

Result: 在复杂多跳问答基准测试中，EvolveR性能优于强大的代理基线。

Conclusion: 为既能从外部数据学习又能从自身行为后果学习的代理提供全面蓝图，有助于构建更自主、持续改进的系统。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [466] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 研究量化了提示策略与大语言模型在系统文献综述筛选阶段的交互，评估六种模型和五种提示类型，给出结果并推荐分阶段工作流程。


<details>
  <summary>Details</summary>
Motivation: 量化提示策略与大语言模型在系统文献综述筛选阶段的交互，以实现自动化筛选。

Method: 评估六种大语言模型在五种提示类型下，在相关性分类和六个二级任务中的表现，使用准确率、精确率、召回率和F1值进行评估。

Result: 模型 - 提示交互效果显著，不同提示类型有不同优势，GPT - 4o和DeepSeek整体表现好，GPT - 4o - mini成本低且有竞争力，不同模型 - 提示配对成本性能差异大。

Conclusion: 推荐分阶段工作流程，大语言模型自动化文献筛选有潜力但不均衡，研究提供了比较基准和实用指导。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [467] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 对语言模型语法知识与字符串概率关系进行理论分析并实证验证，为用概率研究语言模型结构知识提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型对语法的学习情况，解决概率和语法性在语言学中概念不同导致的用字符串概率揭示语言模型语法知识不明确的问题。

Method: 基于语料数据生成过程的简单假设进行理论分析，用280K英中句子对进行实证验证。

Result: 验证了三个预测，包括最小对中字符串概率的相关性、模型和人类在最小对中差异的相关性、未配对语法和非语法字符串在概率空间中的分离性差。

Conclusion: 为用概率学习语言模型的结构知识提供理论依据，并为未来语言模型语法评估工作指明方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [468] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 本文探讨大语言模型是否了解自身所学和所想，定义三项核心能力并评估，对比不同训练方式模型表现，发现强化学习训练模型有优势但推理与输出对齐性弱。


<details>
  <summary>Details</summary>
Motivation: 随着后训练技术发展，提出大语言模型是否了解自身所学和所想的问题。

Method: 定义三项核心能力，在多个需学习不同策略的任务上进行实证评估，对比SFT、DPO、GRPO三种后训练方式的模型。

Result: 强化学习训练的模型比SFT模型更了解自身行为且泛化能力更强，但推理痕迹与最终输出对齐性弱，GRPO训练的模型此效应最明显。

Conclusion: 不同训练方式的大语言模型在核心能力表现上存在差异，强化学习训练模型有优点也有不足。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [469] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 本文提出AASP框架用于论点挖掘，在多个基准测试中取得良好结果。


<details>
  <summary>Details</summary>
Motivation: 论点挖掘中对论点组件和关系的依赖建模具有挑战性，现有方法多通过生成范式将论点结构扁平化。

Method: 使用自回归论点结构预测（AASP）框架以端到端方式联合制定论点挖掘的关键任务，借助条件预训练语言模型将论点结构建模为受限的预定义动作集。

Result: 在三个标准论点挖掘基准测试上，AASP在两个基准的所有任务中达到了最先进水平，在另一个基准中也取得了不错的结果。

Conclusion: AASP框架在论点挖掘任务中表现出色，能有效捕捉论点推理过程。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [470] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 本文提出一种轻量级方法提升大语言模型心理健康评估能力，在两项任务中取得更好结果，凸显转向机制潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展为心理健康领域带来机遇，但小规模模型在特定领域应用表现不佳，需提升大语言模型心理健康评估能力。

Method: 采用线性变换应用于特定层激活值，利用转向向量引导模型输出。

Result: 该方法使模型在相关性预测任务和问卷完成任务中取得更好结果。

Conclusion: 转向机制作为计算高效工具，在大语言模型心理健康领域适应方面有未被挖掘的潜力。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [471] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: 本文提出MoReBench和MoReBench - Theory基准测试集评估AI道德推理能力，发现现有规律和基准难以预测其道德推理表现，模型有框架偏好，推动了聚焦过程的推理评估。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，需理解其决策过程以确保决策符合人类价值观，道德困境是评估AI过程推理的好场景。

Method: 提出包含1000个道德场景及2.3万条评判标准的MoReBench，以及150个示例的MoReBench - Theory测试AI在规范伦理学五大框架下的推理能力。

Result: 现有数学、代码和科学推理任务的规律和基准无法预测模型道德推理能力，模型对特定道德框架有偏好。

Conclusion: 这些基准推动了聚焦过程的推理评估，有助于实现更安全透明的AI。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [472] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: 针对大语言模型在高风险领域部署时的可信度问题，提出自主可信代理（ATA）方法，经评估该方法有竞争力且能保证可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险领域部署时存在可信度方面的固有局限，如幻觉、不稳定和缺乏透明度等问题，需要解决。

Method: 将任务解耦为离线知识摄取和在线任务处理两个阶段，知识摄取时用大语言模型将问题规格转化为形式化知识库，任务处理时将输入编码为形式语言，用符号决策引擎得出结果。

Result: 在复杂推理任务评估中，ATA的具体实现与最先进的端到端推理模型有竞争力，使用人类验证修正的知识库时表现更优，且具有确定性、稳定性和抗提示注入攻击能力。

Conclusion: ATA通过符号推理生成决策，为构建下一代透明、可审计和可靠的自主代理提供了实用且可控的架构。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [473] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 探索Whisper在二语口语评估中的潜力，通过提取特征、结合辅助信息取得好成绩，分析显示其有强大潜力。


<details>
  <summary>Details</summary>
Motivation: 挖掘Whisper在二语口语评估中的未开发潜力。

Method: 从Whisper隐藏表征中提取声学和语言特征，训练轻量级分类器，结合图像和文本提示信息。

Result: 在GEPT图片描述数据集上表现出色，超越现有前沿基线，结合辅助信息有额外性能提升。

Conclusion: Whisper即使不进行特定任务微调，也能内在编码口语熟练度和语义信息，可作为口语评估和理解任务的强大基础。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [474] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 本文提出受背包问题启发的结构化自动框架用于智能体系统组件组合，能考虑性能、预算和兼容性，实验表明该方法表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在智能体系统组件有效复用和组合方面存在挑战，决策未基于能力、成本和实时效用。

Method: 引入受背包问题启发的结构化自动框架，使组合智能体考虑性能、预算约束和兼容性，动态测试候选组件并实时建模其效用。

Result: 在五个基准数据集上用Claude 3.5 Sonnet评估，基于在线背包的组合器始终位于帕累托前沿，单智能体设置中成功率最高提升31.6%，多智能体系统中成功率从37%提升到87%。

Conclusion: 该方法在不同领域和预算约束下有强大的适应性。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [475] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 本文追踪大语言模型内部文化理解机制，发现内部路径在同语言跨国问题上重叠更多，韩朝对显示语言相似不保证内部表征一致。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在不同文化语境中广泛使用，准确的文化理解很重要，但先前评估多关注输出层性能，电路分析研究覆盖语言少且少关注文化。

Method: 通过测量在两种条件下回答语义等价问题时的激活路径重叠来追踪大语言模型内部文化理解机制，还使用同语言国家对来区分语言和文化方面。

Result: 内部路径在同语言跨国问题上比跨语言同国家问题重叠更多，韩朝对表现出低重叠和高变异性。

Conclusion: 大语言模型存在强语言特定模式，语言相似性不能保证内部表征一致。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [476] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 本文提出针对乌尔都语的AI生成文本检测框架，开发数据集，用多种模型微调，mDeBERTa - v3 - base表现最佳，助力乌尔都语社区反虚假信息和学术不端。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使区分人机文本变难，乌尔都语检测工具少，需开发检测框架。

Method: 开发含1800个人类撰写和1800个AI生成文本的数据集，进行语言和统计分析，用t检验和Mann - Whitney U检验评估特征，微调三个多语言transformer模型。

Result: mDeBERTa - v3 - base表现最佳，测试集F1分数91.29，准确率91.26%。

Conclusion: 本研究推动乌尔都语社区反虚假信息和学术不端，有助于低资源语言NLP工具发展。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [477] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: 本文提出DiMo框架，通过多智能体协作增强大语言模型性能与可解释性，在多个基准测试中提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型表现出色但缺乏可解释推理，需要提升性能和可解释性。

Method: 引入多智能体协作框架DiMo，模拟四个专业大语言模型智能体之间的结构化辩论，各智能体有不同推理范式，通过迭代辩论完善初始响应。

Result: 在六个基准测试中，DiMo比单模型和辩论基线提高了准确率，在数学方面提升最大。

Conclusion: DiMo是语义感知、基于网络的多智能体框架，可结合检索增强推理与结构化论证，供下游系统检查和复用。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [478] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: 当前基于提示的学习方法存在局限，本文提出Capsule Prompt - Tuning (CaPT)方法，整合实例和任务信息，在语言任务中表现优异且参数效率高。


<details>
  <summary>Details</summary>
Motivation: 当前基于提示的学习方法依赖网格搜索，需大量提示，有计算负担，且任务感知提示设计缺乏实例感知信息。

Method: 引入CaPT方法，以近乎无参数的方式（单个胶囊提示）整合实例感知和任务感知信息。

Result: 该方法在各种语言任务中表现出色，如在T5 - Large上平均准确率达84.03%，在Llama3.2 - 1B上仅使用0.003%的模型参数。

Conclusion: CaPT方法是基于提示学习的高效有效解决方案，能作为“注意力锚点”，兼具高性能和高参数效率。

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [479] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文综述2014 - 2025年心脏病学中自然语言处理（NLP）研究，通过查询数据库筛选265篇文章，多维度分析展示NLP研究广度及方法演变。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病复杂且相关信息分散在各类文本数据中，NLP可分析非结构化数据，助力医疗人员深入了解心脏病学领域，革新诊断、治疗和预防方法。

Method: 查询六个文献数据库，筛选出265篇相关文章，从NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型等多维度分析，并进行时间分析。

Result: 各维度具有显著多样性，展示了该领域NLP研究的广度，还呈现了NLP方法在过去十年的演变和趋势。

Conclusion: 该综述是目前关于心脏病学中NLP研究最全面的概述。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [480] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 研究大语言模型在多轮对话中的“变色龙行为”，用新数据集和指标评估模型，发现模型存在严重问题，强调部署前需一致性评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与搜索/检索引擎集成系统存在可靠性漏洞，研究其在多轮对话中“变色龙行为”。

Method: 创建Chameleon Benchmark Dataset，引入变色龙分数和源重用率两个指标评估模型。

Result: 评估Llama - 4 - Maverick、GPT - 4o - mini和Gemini - 2.5 - Flash，模型均有严重“变色龙行为”，GPT - 4o - mini表现最差，跨温度方差小，源重用率与信心和立场变化强相关。

Conclusion: 在医疗、法律和金融系统部署大语言模型前需进行全面一致性评估。

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [481] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 介绍Beacon基准测试衡量大语言模型谄媚偏差，评估模型并提出干预方法，为研究和缓解对齐漂移提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在真实性与谄媚奉承的结构权衡，即谄媚偏差，需精准衡量该偏差。

Method: 引入单轮强制选择基准测试Beacon，在无对话上下文情况下衡量偏差；对十二个最先进模型进行评估；提出提示级和激活级干预措施。

Result: 谄媚偏差可分解为稳定的语言和情感子偏差，且随模型容量增加；干预措施能反向调节偏差。

Conclusion: Beacon将谄媚偏差重新定义为可衡量的规范错误泛化形式，为研究和缓解大规模生成系统的对齐漂移提供了可重复的基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [482] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: 本文提出双语多任务评估基准LC - Eval，用于评估大语言模型长文本理解能力，评估显示其对模型提出挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长文本理解方面能力提升，需要严格评估方法来评估其性能。

Method: 提出LC - Eval基准，包含四个新任务，有阿拉伯语和英语数据集，对开放权重和闭源大语言模型进行评估。

Result: LC - Eval对模型提出重大挑战，即使GPT - 4o等高性能模型也在某些任务上有困难。

Conclusion: LC - Eval能有效评估大语言模型长文本理解能力，其任务具有复杂性和严格性。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [483] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: 介绍MOSAIC框架用于句子嵌入模型的领域自适应，结合特定领域掩码监督，在高低资源领域验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决大规模通用领域句子嵌入模型适应特定领域的挑战。

Method: 在统一训练管道中联合优化掩码语言建模和对比目标。

Result: 在高低资源领域实验，相比通用领域基线，NDCG@10提升达13.4%，消融研究证明各组件有效。

Conclusion: 平衡的联合监督和分阶段适应很重要。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [484] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: 研究大语言模型在实体比较任务中依赖知识还是启发式策略，发现模型存在启发式偏差，大模型能选择性利用知识，思维链提示可引导模型使用数值特征。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在知识推理任务中何时依赖真实知识，何时依赖表面启发式策略。

Method: 通过实体比较任务，让模型比较实体的数值属性，并分析模型预测受启发式偏差的影响。

Result: 模型存在实体流行度、提及顺序和语义共现三种启发式偏差；小模型启发式策略常覆盖原则推理；大模型能选择性依赖可靠的数值知识；思维链提示可引导各规模模型使用数值特征。

Conclusion: 大模型能更好地在知识推理中利用可靠知识，思维链提示有助于模型使用数值特征进行推理。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [485] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: 提出CoRUS框架模拟基于角色的问题，评估发现不同角色提问会使大语言模型输出有差异，为对话式AI评估提供方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估常忽略提问者角色，在像阿片类药物使用障碍这类敏感领域，考虑用户背景对提供无歧视回应至关重要。

Method: 基于角色理论和在线康复社区帖子构建提问者角色分类，模拟15321个嵌入各角色目标、行为和经验的问题。

Result: 模拟问题可信度高且与真实数据可比，评估五个大语言模型发现不同角色提问会使输出存在系统差异。

Conclusion: 表明用户角色会影响模型响应，为对话式AI提供基于角色的评估方法。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [486] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文引入合成测试平台研究统计规律与事实关联交互对语言模型泛化能力的影响，发现上下文多样性、结构等因素的作用，并追踪OOD失败原因。


<details>
  <summary>Details</summary>
Motivation: 缺乏对统计规律与事实关联交互影响语言模型泛化能力的系统分析。

Method: 引入合成测试平台，通过控制实验和对模型组件干预进行研究。

Result: 高上下文多样性延迟ID事实准确性，对OOD事实泛化影响取决于上下文结构；OOD性能趋势多样；不同结构下统计泛化和事实回忆能力有不同表现；追踪到OOD失败源于优化瓶颈。

Conclusion: 上下文设计与多样性水平的相互作用影响不同泛化方面，合成框架为未来研究提供可控测试平台。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [487] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本文首次将PEFT用于孟加拉语仇恨言论检测，在BD - SHS数据集上微调三个大语言模型，结果显示Llama - 3.2 - 3B的F1分数最高，证明PEFT对孟加拉语等低资源语言实用且可复制。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语社交媒体仇恨言论增多，此前方法存在计算成本高或依赖专有API的问题。

Method: 使用LoRA和QLoRA进行参数高效微调（PEFT），在BD - SHS数据集上微调Gemma - 3 - 4B、Llama - 3.2 - 3B和Mistral - 7B三个指令调优大语言模型，仅训练不到1%的参数，在单块消费级GPU上实验。

Result: Llama - 3.2 - 3B的F1分数最高为92.23%，Mistral - 7B为88.94%，Gemma - 3 - 4B为80.25%。

Conclusion: PEFT是适用于孟加拉语及相关低资源语言的实用且可复制的策略。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [488] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 研究推理型大语言模型思考过程中聚合社会刻板印象导致偏差的现象，发现两种失败模式并提出轻量级提示缓解方法，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 推理型大语言模型思考过程会聚合社会刻板印象导致偏差，但该现象背后语言模型的行为未被充分探索，需研究其机制。

Method: 系统调查思考过程机制，发现两种失败模式，提出基于提示的缓解方法让模型对照失败模式审查自身推理。

Result: 在问答和开放式基准测试中，该方法有效减少偏差，同时维持或提高了准确性。

Conclusion: 提出的轻量级提示缓解方法能有效减少推理型大语言模型的社会偏差。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [489] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: 提出VeriMAP框架解决大语言模型多智能体协作挑战，评估显示其性能优且增强系统特性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型多智能体协作在规划、协调和验证方面存在挑战，执行失败源于任务理解等细微偏差。

Method: 提出VeriMAP框架，其规划器分解任务、建模子任务依赖，并将通过标准编码为子任务验证函数。

Result: 在不同数据集上评估，VeriMAP优于单智能体和多智能体基线，增强系统鲁棒性和可解释性。

Conclusion: 验证感知规划能实现多智能体系统可靠协调和迭代优化，无需外部标签或注释。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [490] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: 现有动态词汇方法有局限，提出DVAGen框架解决问题，验证方法有效性并提高推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决固定词汇训练的语言模型难以泛化到新词汇的问题，以及现有动态词汇方法存在的代码碎片化、不支持现代大语言模型和推理可扩展性有限等挑战。

Method: 引入DVAGen框架，模块化管道便于定制，与开源大语言模型无缝集成，提供CLI和WebUI工具。

Result: 验证了动态词汇方法在现代大语言模型上的有效性，支持批量推理，显著提高推理吞吐量。

Conclusion: DVAGen框架能有效解决现有问题，提升动态词汇增强语言模型的训练、评估和推理性能。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [491] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 文章剖析基于块的稀疏注意力模型，确定驱动性能的核心组件，提出三项设计原则，实现无训练长度外推新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer处理长上下文能力有限，现有替代架构有缺陷，基于块的稀疏注意力模型成功的关键架构原则未完全明确。

Method: 通过统一框架和全面的消融研究，确定三项设计原则，并为块内信息处理和地标生成提供理论动机。

Result: 建立无训练长度外推新的最优结果，将在4K上下文训练的模型成功推广到3200万令牌。

Conclusion: 为未来开发高性能长上下文语言模型提供明确且基于实证的设计原则。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [492] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 研究通过对孟加拉语新闻进行大规模情感分析，发现新闻报道中负面情绪占主导，不同媒体情感呈现差异大，并据此提出新闻聚合器设计思路。


<details>
  <summary>Details</summary>
Motivation: 探究新闻媒体通过报道框架塑造公众情绪的倾向，尤其是负面或情绪化标题吸引关注的现象。

Method: 使用Gemma - 3 4B进行零样本推理，分析300000条孟加拉语新闻标题及其内容。

Result: 发现负面情绪占主导，如愤怒、恐惧和失望，且不同媒体对相似故事的情感呈现差异显著。

Conclusion: 基于研究结果，提出以用户为中心的新闻聚合器设计思路，帮助读者识别日常新闻中的情感框架。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [493] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 介绍了用于评估大语言模型年级适配性的基准EduAdapt，评估发现大模型在为低年级学生生成合适回复上有困难，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在K - 12教育中难以根据学生年级水平调整回复，且缺乏评估其年级适配能力的标准化基准。

Method: 引入包含近48k个带年级标签的问答对的EduAdapt基准，对多种开源大语言模型进行评估。

Result: 较大模型总体表现较好，但在为1 - 5年级早期学生生成合适回复方面仍有困难。

Conclusion: 本工作提供首个评估大语言模型年级适配性的数据集和评估框架，旨在通过更好的训练和提示策略推动开发更符合学生发展阶段的教育AI系统。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [494] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本文介绍了首个基于GRPO训练的中医大语言模型Ladder - base，经评估其性能优于通用和中医领域特定模型，表明GRPO策略有效。


<details>
  <summary>Details</summary>
Motivation: 传统中医知识系统独特，现有中医大语言模型在对齐、数据质量和评估一致性方面存在局限。

Method: 采用强化学习方法GRPO训练Ladder - base，基于Qwen2.5 - 7B - Instruct模型，使用TCM - Ladder基准的文本子集，80%数据训练，20%用于验证和测试。

Result: Ladder - base在多个推理指标上表现优于通用大模型和中医领域特定模型。

Conclusion: GRPO为传统医学领域大模型与专家级推理对齐提供了有效策略，有助于开发可信且基于临床的中医人工智能系统。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [495] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: 提出AfriCaption框架用于20种非洲语言的多语言图像字幕，确保数据质量，为多模态AI奠定基础。


<details>
  <summary>Details</summary>
Motivation: 多模态AI研究集中于高资源语言，阻碍领域发展，需解决非洲语言图像字幕问题。

Method: 构建基于Flickr8k的数据集，采用上下文感知选择和翻译；使用动态上下文保留管道；构建含0.5B参数的AfriCaption模型，集成SigLIP和NLLB200。

Result: 建立了首个可扩展的非洲语言图像字幕资源。

Conclusion: 该框架确保数据质量，为多模态AI的包容性发展奠定基础。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [496] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 研究指出微调后模型存在校准损失等问题，通过简单的权重插值干预可有效应对微调代价，得到更优模型。


<details>
  <summary>Details</summary>
Motivation: 解决微调后模型存在任务准确率下降、校准损失、过度自信、可靠性降低和输出多样性减少等问题。

Method: 对模型微调前后的权重进行插值。

Result: 发现帕累托最优插值，模型准确率提升且恢复校准。

Conclusion: 简单的模型合并是减轻微调代价的高效方法，能得到更强大可靠的模型。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [497] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: 提出xLSTM框架用于有毒评论检测，在Jigsaw基准上表现优，参数少且推理快，证明轻量级架构优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer模型计算成本高、对少数毒性类别表现差，经典集成模型缺乏语义适应性。

Method: xLSTM统一余弦相似度门控、自适应特征优先级和类别重平衡，集成多源嵌入等技术。

Result: 在Jigsaw基准上准确率96.0%，宏观F1为0.88，在威胁和身份仇恨类别上优于BERT，参数少且推理快，余弦门控提升F1。

Conclusion: 轻量级、理论驱动的架构能在不平衡、特定领域NLP任务上超越大型预训练模型。

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [498] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 本文研究大语言模型的提示敏感性，提出通过释义扰动采样改善不确定性校准，并引入新的不确定性分解指标。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在提示敏感性，模型输出分布的不确定性可能无法反映其对提示含义的不确定，需解决该问题。

Method: 将提示敏感性建模为泛化误差，通过释义扰动在语义概念空间采样；引入新的黑盒大语言模型不确定性分解指标。

Result: 通过释义扰动采样可在不影响准确性的前提下改善不确定性校准；新的分解指标可量化大语言模型不确定性中由提示敏感性导致的部分。

Conclusion: 提出了改善提示敏感语言模型不确定性校准的新方法，部分大语言模型在输入含义的通用推理上缺乏一致性。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [499] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: 提出SimBench基准测试LLM模拟人类行为能力，发现当前LLM模拟能力有限，性能与模型大小、推理计算、指令微调等因素有关，旨在加速开发更逼真的LLM模拟器。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型模拟人类行为的评估零散，结果不可比，需要标准化基准。

Method: 引入SimBench，统一20个不同数据集进行评估。

Result: 当前最佳LLM模拟能力有限，性能与模型大小呈对数线性关系，推理计算不提升性能，存在对齐 - 模拟权衡，模拟特定人群困难，模拟能力与深度知识推理强相关。

Conclusion: 使进展可衡量，加速开发更逼真的LLM模拟器。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [500] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 提出DETree方法和RealBench数据集用于检测AI相关文本，提升检测性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: AI文本生成存在多样协作过程，文本特征复杂，当前检测方法粗糙，需更优检测方案。

Method: 提出DETree，将不同过程关系建模为层次亲和树结构，引入专用损失函数；开发RealBench数据集。

Result: 提升混合文本检测任务性能，增强分布外场景鲁棒性和泛化性，在少样本学习条件下效果显著。

Conclusion: 基于训练的方法在分布外场景有应用前景，代码和数据集公开。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [501] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 提出轻量级少样本NER框架，结合新指令调优模板和数据增强技术，实验表现良好，为资源有限场景提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 命名实体识别在低资源场景下获取标注数据成本高，零样本和指令调优方法存在泛化性差、未有效利用有限数据的问题。

Method: 提出新指令调优模板结合最新大语言模型的大上下文窗口；引入数据增强技术，在改写上下文时保留实体信息。

Result: 在基准数据集实验中，少样本和零样本任务表现与现有模型相当，少样本方法在CrossNER数据集平均F1分数达80.1，数据改写方法使F1分数最多提升17分。

Conclusion: 该框架为NER训练数据和计算能力有限的群体提供了有前景的解决方案。

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [502] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出用于评估大语言模型长上下文生成任务的实时基准AcademicEval，评估显示模型在相关任务表现不佳并给出提升见解。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文大语言模型基准存在上下文长度固定、标注费力和标签泄露问题，需要新基准。

Method: 采用arXiv论文引入学术写作任务，集成高质量少样本示例，实现灵活上下文长度和无标签泄露的实时评估。

Result: 大语言模型在具有层次抽象水平的任务上表现差，处理长少样本示例有困难。

Conclusion: 通过实验分析得到了提升大语言模型长上下文建模能力的见解。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [503] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 提出统一多任务学习框架，让大语言模型与临床推理对齐以预测癌症治疗结果，评估三种对齐策略，显示CoT和GRPO效果好，强调推理感知对齐重要性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在预测癌症治疗结果时缺乏结构化推理能力，需要准确且可解释的模型。

Method: 提出统一多任务学习框架，训练模型联合进行二元生存分类、连续生存时间回归和自然语言推理生成，评估标准监督微调、CoT提示和GRPO三种对齐策略。

Result: CoT提示提升F1并降低MAE，GRPO在多项指标上达到先进可解释性和预测性能，现有生物医学大语言模型常因架构限制无法产生有效推理痕迹。

Conclusion: 强调推理感知对齐在多任务临床建模中的重要性，为精准肿瘤学中可解释、可信的大语言模型设定新基准。

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [504] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文聚焦数据扩展，创建250万样本数据集，训练FARE评价器，在多方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注新方法，缺乏大规模数据驱动开发，为满足可扩展评估需求，聚焦数据扩展。

Method: 创建涵盖五个评估任务和多领域的250万样本数据集，用迭代拒绝采样监督微调方法训练8B和20B参数的FARE评价器。

Result: FARE - 8B挑战更大的强化学习训练评价器，FARE - 20B成为开源评价器新标准；在现实任务中，FARE - 20B在MATH上接近最优性能，作为验证器提升下游模型性能，FARE - Code在评估测试用例质量上大幅超越gpt - oss - 20B。

Conclusion: 基于大规模数据训练的FARE评价器在推理评估任务中表现优异，具有良好的应用前景。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [505] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 提出企业深度研究（EDR）多智能体系统，能处理非结构化数据，在内部数据集验证，在基准测试中表现优于现有系统并开源框架和基准轨迹。


<details>
  <summary>Details</summary>
Motivation: 企业面临将非结构化数据转化为可操作见解的压力，现有自主智能体在特定领域细节、意图对齐和企业集成方面存在困难。

Method: 构建包含主规划智能体、四个专业搜索智能体、可扩展工具生态系统、可视化智能体和反思机制的多智能体系统。

Result: 能实现自动报告生成、实时流处理和无缝企业部署，在开放基准测试中优于现有智能体系统。

Conclusion: 发布EDR框架和基准轨迹以推动多智能体推理应用研究。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


### [506] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 提出在线强化学习方法，用二元检索增强奖励（RAR）解决语言模型外在幻觉问题，实验表明该方法能降低幻觉率且不影响其他任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有缓解语言模型外在幻觉的方法会降低开放式生成和下游任务性能，限制实际应用。

Method: 提出使用新型二元检索增强奖励（RAR）的在线强化学习方法，仅在模型输出完全符合事实时给予奖励1，否则为0。

Result: 在Qwen3推理模型的多样任务中，开放式生成幻觉率降低39.3%，短问答任务中PopQA和GPQA错误答案分别减少44.4%和21.7%，且不影响指令遵循、数学和代码任务性能。

Conclusion: 二元RAR方法能有效降低语言模型幻觉率，且不会导致其他任务性能下降，优于连续奖励强化学习方法。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [507] [Agentic AI for Ultra-Modern Networks: Multi-Agent Framework for RAN Autonomy and Assurance](https://arxiv.org/abs/2510.16144)
*Sukhdeep Singh,Avinash Bhat,Shweta M,Subhash K Singh,Moonki Hong,Madhan Raj K,Kandeepan Sithamparanathan,Sunder A. Khowaja,Kapal Dev*

Main category: cs.NI

TL;DR: 随着5G后和6G网络复杂性增加，提出多智能体架构用于未来自治网络，通过流量调度用例验证其优势。


<details>
  <summary>Details</summary>
Motivation: 5G后和6G网络复杂性增加，传统O - RAN控制回路依赖RIC集中编排存在风险，需新范式。

Method: 设计并评估流量调度用例，对比天真预测器驱动部署和智能体系统。

Result: 天真预测器驱动部署改善局部指标但影响邻居，智能体系统可阻止不安全策略，维护全局网络健康。

Conclusion: 多智能体架构是下一代RAN中可信AI驱动自治的可靠基础。

Abstract: The increasing complexity of Beyond 5G and 6G networks necessitates new
paradigms for autonomy and assur- ance. Traditional O-RAN control loops rely
heavily on RIC- based orchestration, which centralizes intelligence and exposes
the system to risks such as policy conflicts, data drift, and unsafe actions
under unforeseen conditions. In this work, we argue that the future of
autonomous networks lies in a multi-agentic architecture, where specialized
agents collaborate to perform data collection, model training, prediction,
policy generation, verification, deployment, and assurance. By replacing
tightly- coupled centralized RIC-based workflows with distributed agents, the
framework achieves autonomy, resilience, explainability, and system-wide
safety. To substantiate this vision, we design and evaluate a traffic steering
use case under surge and drift conditions. Results across four KPIs: RRC
connected users, IP throughput, PRB utilization, and SINR, demonstrate that a
naive predictor-driven deployment improves local KPIs but destabilizes
neighbors, whereas the agentic system blocks unsafe policies, preserving global
network health. This study highlights multi- agent architectures as a credible
foundation for trustworthy AI- driven autonomy in next-generation RANs.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [508] [Equilibrium-Constrained Estimation of Recursive Logit Choice Models](https://arxiv.org/abs/2510.16886)
*Hung Tran,Tien Mai,Minh Hoang Ha*

Main category: econ.EM

TL;DR: 提出新方法解决递归logit模型估计问题，在合成和真实数据集实验中展现优势。


<details>
  <summary>Details</summary>
Motivation: 递归logit（RL）模型估计常用的嵌套定点（NFXP）算法计算成本高且易数值不稳定。

Method: 将最大似然估计问题重新表述为带均衡约束的优化问题，把结构参数和值函数作为决策变量，再转化为带指数锥的二次优化问题，用现代锥求解器求解。

Result: 凸重新表述在合成和真实数据集实验中达到与传统方法相当的精度，且在计算稳定性和效率上有显著提升。

Conclusion: 新方法为递归logit模型估计提供了实用且可扩展的替代方案。

Abstract: The recursive logit (RL) model provides a flexible framework for modeling
sequential decision-making in transportation and choice networks, with
important applications in route choice analysis, multiple discrete choice
problems, and activity-based travel demand modeling. Despite its versatility,
estimation of the RL model typically relies on nested fixed-point (NFXP)
algorithms that are computationally expensive and prone to numerical
instability. We propose a new approach that reformulates the maximum likelihood
estimation problem as an optimization problem with equilibrium constraints,
where both the structural parameters and the value functions are treated as
decision variables. We further show that this formulation can be equivalently
transformed into a conic optimization problem with exponential cones, enabling
efficient solution using modern conic solvers such as MOSEK. Experiments on
synthetic and real-world datasets demonstrate that our convex reformulation
achieves accuracy comparable to traditional methods while offering significant
improvements in computational stability and efficiency, thereby providing a
practical and scalable alternative for recursive logit model estimation.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [509] [Comparison of Tax and Cap-and-Trade Carbon Pricing Schemes](https://arxiv.org/abs/2510.15941)
*Stéphane Crépey,Samuel Drapeau,Mekonnen Tadese*

Main category: econ.GN

TL;DR: 本文构建统一框架比较碳税和基于市场的碳排放交易机制表现，发现碳排放交易机制中金融中介的存在会降低监管财富和经济主体总财富，强调设计碳市场需考虑中介行为。


<details>
  <summary>Details</summary>
Motivation: 经济理论认为碳税和碳排放交易机制在理想情况下等效，但现实中因市场缺陷表现不同，且金融中介在碳排放交易市场中的作用研究较少。

Method: 构建统一框架，将金融中介纳入考量，校准两种工具以实现相同减排目标，评估其在不同市场结构下的经济表现。

Result: 两种方案在完全竞争下等效，但碳排放交易机制中中介的存在会降低监管财富和经济主体总财富，这源于中介对价格形成的影响和对部分收入流的占有。

Conclusion: 设计碳市场时需考虑中介行为，且需对碳排放交易系统的制度结构进行更多实证研究。

Abstract: Carbon pricing has become a central pillar of modern climate policy, with
carbon taxes and emissions trading systems (ETS) serving as the two dominant
approaches. Although economic theory suggests these instruments are equivalent
under idealized assumptions, their performance diverges in practice due to
real-world market imperfections. A particularly less explored dimension of this
divergence concerns the role of financial intermediaries in emissions trading
markets. This paper develops a unified framework to compare the economic and
environmental performance of tax- and market-based schemes, explicitly
incorporating the involvement of financial intermediaries. By calibrating both
instruments to deliver identical aggregate emission reduction targets, we
assess their economic performance across alternative market structures. Our
results suggest that although the two schemes are equivalent under perfect
competition, the presence of intermediaries in ETS reduces both regulatory
wealth and the aggregate wealth of economic agents relative to carbon taxation.
These effects stem from intermediaries' influence on price formation and their
appropriation of part of the revenue stream. The findings underscore the
importance of accounting for intermediaries' behavior in the design of carbon
markets and highlight the need for further empirical research on the evolving
institutional structure of emissions trading systems.

</details>


### [510] [Data for Inclusion: The Redistributive Power of Data Economics](https://arxiv.org/abs/2510.16009)
*Diego Vallarino*

Main category: econ.GN

TL;DR: 本文评估金融排斥经济体中扩大正面信用信息获取的影响，发现全面获取合成数据能带来最公平高效结果。


<details>
  <summary>Details</summary>
Motivation: 评估金融排斥经济体中扩大正面信用信息获取在再分配和效率方面的影响。

Method: 利用乌拉圭2021年家庭调查微观数据，模拟负向、部分正向（Score+）和合成完全可见三种数据机制，评估其对信贷可得性、利息负担和不平等的影响。

Result: 扩大数据共享大幅降低财务成本、压缩利率差异、降低信贷负担基尼系数，部分可见使部分人群受益，完全合成访问带来最公平高效结果。

Conclusion: 信用数据是具有变革意义的非竞争性公共资产，有助于金融普惠和减贫。

Abstract: This paper evaluates the redistributive and efficiency impacts of expanding
access to positive credit information in a financially excluded economy. Using
microdata from Uruguay's 2021 household survey, we simulate three data regimes
negative only, partial positive (Score+), and synthetic full visibility and
assess their effects on access to credit, interest burden, and inequality. Our
findings reveal that enabling broader data sharing substantially reduces
financial costs, compresses interest rate dispersion, and lowers the Gini
coefficient of credit burden. While partial visibility benefits a subset of the
population, full synthetic access delivers the most equitable and efficient
outcomes. The analysis positions credit data as a non-rival public asset with
transformative implications for financial inclusion and poverty reduction.

</details>


### [511] [Development finance institutions (DFIs), political conditions, and foreign direct investment (FDI) in Sub-Saharan Africa](https://arxiv.org/abs/2510.16472)
*Carmen Berta C. De Saituma Cagiza,Ilidio Cagiza*

Main category: econ.GN

TL;DR: 本文用1990 - 2018年五个撒哈拉以南非洲国家面板数据和固定效应模型，研究开发金融机构（DFIs）、外国直接投资（FDI）与经济发展关系，发现DFIs对FDI理论上有积极影响但无统计显著性，部分因素影响投资环境，特定领域DFI投资显著影响FDI，强调针对性政策重要性。


<details>
  <summary>Details</summary>
Motivation: 探究DFIs是否能增加FDI流入，促进经济增长并助力实现可持续发展目标。

Method: 使用五个撒哈拉以南非洲国家的年度数据构建定量面板数据集，运用STATA估计固定效应模型。

Result: DFIs对FDI理论上有积极影响但无统计显著性；经济增长、贸易开放等因素影响投资气候；DFI在基础设施等领域投资显著影响FDI，基础设施影响最大。

Conclusion: 为政策制定者提供框架，强调需制定针对性政策解决地区差异，提高DFI促进可持续增长的有效性。

Abstract: This study investigates the dynamic relationship between development finance
institutions (DFIs), foreign direct investment (FDI), and economic development
in Sub-Saharan Africa (SSA) from 1990 to 2018, using a quantitative panel
dataset of annual data for five SSA countries (Nigeria, Ghana, Kenya, South
Africa, and Zimbabwe) and a fixed-effects model estimated in STATA.
Specifically, the analysis examines whether DFIs enhance FDI inflows, thereby
promoting economic growth and contributing to the achievement of the
Sustainable Development Goals (SDGs). The findings indicate that although DFIs
have a theoretically positive impact on FDI, this relationship is not
statistically significant across the sample, suggesting contextual dependencies
influenced by regional economic variations. The study also analyzes how
economic growth, trade openness, inflation, political stability, and the rule
of law influence this nexus, elucidating their roles in shaping investment
climates. A sectoral analysis indicates that DFI investments in infrastructure,
agribusiness, and finance significantly affect FDI, with infrastructure having
the greatest impact owing to its foundational role in economic systems. This
research contributes by linking DFIs with FDI in SSA in a panel setting, thus
providing a framework for policymakers to strengthen institutional and
macroeconomic conditions to optimize the impact of DFIs on FDI and, ultimately,
on sustainable development. The findings underscore the need for targeted
policies to address regional disparities and enhance DFI effectiveness in
fostering sustainable growth.

</details>


### [512] [Income Taxes, Gross Hourly Wages, and the Anatomy of Behavioral Responses: Evidence from a Danish Tax Reform](https://arxiv.org/abs/2510.16483)
*Kazuhiko Sumiya,Jesper Bagger*

Main category: econ.GN

TL;DR: 本文利用丹麦行政数据和税收改革，研究所得税对小时工资的影响，发现对低收入者工资有负动态影响，工资通过晋升或跳槽响应税收，年收益主要通过小时工资而非劳动供给响应税收。


<details>
  <summary>Details</summary>
Motivation: 研究所得税如何影响小时工资。

Method: 利用丹麦行政数据和引入联合征税的税收改革，利用配偶收入进行识别，对男性进行非参数双重差分图形分析。

Result: 对低收入者，税收对工资有负动态影响，工资弹性为0.4；对中等收入者，影响小且不显著；工资通过晋升或跳槽响应税收；日和年工作时长无显著响应。

Conclusion: 年收益对税收的响应主要通过小时工资，而非劳动供给。

Abstract: This paper provides quasi-experimental evidence on how income taxes affect
gross hourly wages, utilizing Danish administrative data and a tax reform that
introduced joint taxation. Exploiting spousal income for identification, we
present nonparametric, difference-in-differences graphical evidence among
husbands. For low-income workers, taxes have negative and dynamic effects on
wages; their wage elasticity with respect to net-of-marginal-tax rates is 0.4.
For medium-income workers, the effects are smaller and insignificant. Wages
respond to taxes through promotions or job-to-job transitions. Neither daily
nor annual hours worked respond significantly; consequently, annual earnings
respond to taxes primarily through hourly wages, rather than through labor
supply.

</details>


### [513] [The Crisis Simulator for Bolivia (KISr-p): An Empirically Grounded Modeling Framework](https://arxiv.org/abs/2510.16537)
*Ricardo Alonzo Fernández Salguero*

Main category: econ.GN

TL;DR: 介绍玻利维亚危机模拟器（KISr - p），阐述其设计、校准，模拟结果显示务实政策优于教条政策。


<details>
  <summary>Details</summary>
Motivation: 评估高不确定性和结构约束环境下各种宏观经济政策策略的影响。

Method: 基于大量元分析的实证结果，采用凯恩斯跨期综合理论架构和KIS - CES生产函数，详细校准各模型板块。

Result: 模拟凸显财政调整、外部融资等之间的权衡，务实政策能带来更好宏观经济和福利结果。

Conclusion: 务实的政策方法，即注重支出构成和考虑制度摩擦，优于一刀切的教条式政策。

Abstract: This document presents a detailed technical report of the ``Crisis Simulator
for Bolivia (KISr-p),'' a quarterly stochastic model designed to evaluate the
impact of various macroeconomic policy strategies in an environment of high
uncertainty and structural constraints. Unlike standard general equilibrium
frameworks, this simulator is grounded in the consolidated empirical findings
of a vast collection of meta-analyses, adopting the theoretical architecture of
a Keynesian Intertemporal Synthesis (KIS) with a Constant Elasticity of
Substitution (KIS-CES) production function. The calibration of each model block
-- real, fiscal, monetary, external, labor, and distributional -- is described
in detail, with parameters justified by quantitative evidence on the hierarchy
of fiscal multipliers (Gechert and Rannenberg, 2018), the complementarity of
production factors (Gechert et al., 2022), monopsony power in the labor market
(Sokolova and S{\o}rensen, 2021), and the dynamics of exchange rate and
interest-rate pass-through. The model integrates these empirical regularities
to generate non-linear dynamics such as state-dependent multipliers, asymmetric
responses to shocks, and business-cycle phase interactions. Simulation results
highlight the trade-offs between fiscal adjustment, external financing, debt
restructuring, and structural reforms -- such as aggressive spending
reallocation and targeted public investment. Scenarios show that pragmatic
policy approaches that prioritize the \textit{composition} of spending over its
aggregate level and that recognize institutional frictions yield superior
macroeconomic and welfare outcomes compared to doctrinaire, one-size-fits-all
prescriptions.

</details>


### [514] [Evaluating the Public Pay Gap: A Comparison of Public and Private Sector Wages in France](https://arxiv.org/abs/2510.16626)
*Riddhi Kalsi*

Main category: econ.GN

TL;DR: 本文利用法国行政面板数据，分析公私部门工资差异，发现时薪差距掩盖终身收入和就业稳定性差异，工资溢价和惩罚与性别、教育和工作经验有关。


<details>
  <summary>Details</summary>
Motivation: 解决公私部门工资文献中使用相似数据却得出矛盾结论的实证难题。

Method: 利用法国2012 - 2019年行政面板数据，用期望最大化算法灵活建模部门转换、就业进出转换和收入异质性。

Result: 时薪差距掩盖终身收入和就业稳定性的巨大差异；女性在公共部门有终身收入优势，高学历男性在公共部门有终身收入惩罚；工资溢价和惩罚与性别、教育和工作经验系统相关；工资动态存在显著未观察到的异质性。

Conclusion: 研究结果通过全面描述按性别划分的部门转换、兼职工作和工资差异，统一了现有观点。

Abstract: This paper resolves the empirical puzzle in the public-private wage
literature: why studies using similar data reach contradictory conclusions
about wage premiums and penalties. Utilizing rich French administrative panel
data (2012-2019), this study has two main contributions: first, it presents a
set of new, intuitive yet previously undocumented stylized facts about wage
dynamics, sectoral mobility, and gender differences across sectors. The results
reveal that the modest hourly wage gaps conceal substantial disparities in
lifetime earnings and employment stability. Women, in particular, gain a
significant lifetime earnings advantage in the public sector, driven by higher
retention, better-compensated part-time work, and more equitable annual hours
compared to the private sector, where gender gaps remain larger, especially for
those with higher education. In contrast, highly educated men experience a
lifetime penalty in public employment due to rigid wage structures. By flexibly
modeling sectoral transitions, transitions into and out of employment, and
earnings heterogeneity using an Expectation-Maximization algorithm, this study
shows that both premiums and penalties depend systematically on gender,
education, and labor market experience. The analysis reveals that significant
unobserved heterogeneity remains in wage dynamics. These findings unify
prevailing narratives by providing a comprehensive, descriptive account of
sectoral differences in transitions, part-time work and wages by gender.

</details>


### [515] [New Demand Economics](https://arxiv.org/abs/2510.17121)
*Fenghua Wen,Xieyu Yin,Chufu Wen*

Main category: econ.GN

TL;DR: 本文构建物质丰裕时代需求经济学理论，指出增长新动力在于需求层级升级，关键机制是教育驱动的效用管理，需政策转向。


<details>
  <summary>Details</summary>
Motivation: 在物质丰裕时代，增长的约束条件从总需求不足转变为需求层级升级不足，需新的需求经济学理论。

Method: 构建可估计的一般均衡框架。

Result: 增长新动力在于需求层级升级，更高层级需求产生更大价值创造乘数。

Conclusion: 应将政策从短期总刺激转向以教育为中心的长期人力资本投资。

Abstract: We develop a theory of demand economics for an era of material abundance. The
binding constraint on growth has shifted from insufficient aggregate demand to
inadequate demand-tier upgrading. Our result is that, the new engine of growth
lies in upgrading the demand hierarchy: higher-tier demands generate larger
value-creation multipliers. The key mechanism is education-driven utility
management. Education transforms the social utility function, raises the
utility of higher-tier goods, and directs resources toward higher-value
domains; this warrants a policy reorientation away from short-run aggregate
stimulus toward education-centered, long-horizon investments in human capital.
Methodologically, we build an estimable general-equilibrium framework.

</details>


### [516] [Universalization and the Origins of Fiscal Capacity](https://arxiv.org/abs/2510.17481)
*Esteban Muñoz-Sobrado*

Main category: econ.GN

TL;DR: 本文提出基于普遍化推理的税收遵从和财政能力模型，表明公民道德内化可扩大税基，促使精英提供公共产品，道德能促成可信改革助国家摆脱低能力陷阱。


<details>
  <summary>Details</summary>
Motivation: 研究国家在弱制度下摆脱低财政能力陷阱的途径。

Method: 构建基于普遍化推理的税收遵从和财政能力模型，分析公民和自私精英的决策。

Result: 公民道德内化扩大可行税基，促使精英向公共产品分配资源；公共支出价值不确定时，道德能促成可信改革，提高财政能力。

Conclusion: 存在一个道德渠道，使国家即使在弱制度下也能摆脱低能力陷阱。

Abstract: This paper proposes a model of tax compliance and fiscal capacity grounded in
universalization reasoning. Citizens partially internalize the consequences of
concealment by imagining a world in which everyone acted similarly, linking
their compliance decisions to the perceived effectiveness of public spending. A
selfish elite chooses between public goods and private rents, taking compliance
as given. In equilibrium, citizens' moral internalization expands the feasible
tax base and induces elites to allocate resources toward provision rather than
appropriation. When the value of public spending is uncertain, morality enables
credible reform: high-value elites can signal their type through provision,
prompting citizens to increase compliance and raising fiscal capacity within
the same period. The analysis thus identifies a moral channel through which
states may escape low-capacity traps even under weak institutions.

</details>


### [517] [Are penalty shootouts better than a coin toss? Evidence from European football](https://arxiv.org/abs/2510.17641)
*László Csató,Dóra Gréta Petróczy*

Main category: econ.GN

TL;DR: 研究欧足联俱乐部比赛点球大战结果是否可预测，发现无证据表明踢球顺序、比赛场地和心理势头有影响，强队表现不比弱队好，点球大战类似完美抽签。


<details>
  <summary>Details</summary>
Motivation: 受欧足联取消客场进球规则启发，研究欧足联俱乐部比赛点球大战结果能否预测。

Method: 基于2000 - 2025年所有点球大战数据进行分析。

Result: 未发现踢球顺序、比赛场地和心理势头有影响，强队表现不比弱队好。

Conclusion: 在欧洲顶级足球中，点球大战相当于完美抽签。

Abstract: Penalty shootouts play an important role in the knockout stage of major
football tournaments, especially since the 2021/22 season, when the Union of
European Football Associations (UEFA) scrapped the away goals rule in its club
competitions. Inspired by this rule change, our paper examines whether the
outcome of a penalty shootout can be predicted in UEFA club competitions. Based
on all shootouts between 2000 and 2025, we find no evidence for the effect of
the kicking order, the field of the match, and psychological momentum. In
contrast to previous results, stronger teams, defined first by Elo ratings, do
not perform better than their weaker opponents. Consequently, penalty shootouts
are equivalent to a perfect lottery in top European football.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [518] [Latency Based Tiling](https://arxiv.org/abs/2510.15912)
*Jack Cashman*

Main category: cs.PL

TL;DR: Latency Based Tiling提供基于系统的近似分块解决方案，可在保证编译时间的同时最大化局部性，实现硬件无关且内存安全。


<details>
  <summary>Details</summary>
Motivation: 寻找一种能在最大化局部性的同时保持快速编译时间的近似分块解决方案，避免自动调优的高耗时问题。

Method: 使用三角循环表征机器的缺失率缩放，通过延迟显著增加确定L1、L2和L3内存大小，应用于多面体模型的子集进行循环分块。

Result: 实现了可忽略不计的编译时间开销，结合缓存计时技术得到一个可移植、内存安全的系统。

Conclusion: Latency Based Tiling是一种有效的分块方法，能在多进程系统中实现较好的性能，且具有硬件无关性和内存安全性。

Abstract: Latency Based Tiling provides a systems based approach to deriving
approximate tiling solution that maximizes locality while maintaining a fast
compile time. The method uses triangular loops to characterize miss ratio
scaling of a machine avoiding prefetcher distortion. Miss ratio scaling
captures the relationship between data access latency and working set size with
sharp increases in latency indicating the data footprint exceeds capacity from
a cache level. Through these noticeable increases in latency we can determine
an approximate location for L1, L2, and L3 memory sizes. These sizes are
expected to be under approximations of a systems true memory sizes which is in
line with our expectations given the shared nature of cache in a multi process
system as described in defensive loop tiling. Unlike auto tuning, which can be
effective but prohibitively slow, Latency Based Tiling achieves negligible
compile time overhead. The implementation in Rust enables a hardware agnostic
approach which combined with a cache timing based techniques, yields a
portable, memory safe system running wherever Rust is supported. The tiling
strategy is applied to a subset of the polyhedral model, where loop nestings
are tiled based on both the derived memory hierarchy and the observed data
footprint per iteration.

</details>


### [519] [Insum: Sparse GPU Kernels Simplified and Optimized with Indirect Einsums](https://arxiv.org/abs/2510.17505)
*Jaeyeon Won,Willow Ahrens,Joel S. Emer,Saman Amarasinghe*

Main category: cs.PL

TL;DR: 本文提出新的稀疏计算表达方法，引入 Insum 编译器及两种稀疏格式，在稀疏 GPU 应用上实现加速并减少代码量。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏编译器在生成高性能 GPU 代码和优化稀疏与密集混合计算的密集部分存在不足，需新方法简化高性能稀疏 GPU 内核编程。

Method: 从格式无关的稀疏张量 Einsums 重写为格式感知的间接 Einsums，引入 Insum 编译器生成 GPU 代码，提出 GroupCOO 和 BlockGroupCOO 两种稀疏格式。

Result: 在一系列稀疏 GPU 应用中实现 1.14x 到 3.81x 加速，代码行数比手写实现减少 202x 到 4491x。

Conclusion: 提出的新方法能有效简化高性能稀疏 GPU 内核编程，提升性能并减少代码量。

Abstract: Programming high-performance sparse GPU kernels is notoriously difficult,
requiring both substantial effort and deep expertise. Sparse compilers aim to
simplify this process, but existing systems fall short in two key ways. First,
they are primarily designed for CPUs and rarely produce high-performance GPU
code. Second, when computations involve both sparse and dense regions, these
compilers often fail to optimize the dense portions effectively. In this paper,
we propose a new approach for expressing sparse computations. We start from
format-agnostic Einsums over sparse tensors and rewrite them into
format-conscious indirect Einsums, which explicitly encode format information
by mapping sparse data and metadata onto dense tensor operations through
indirect indexing. To execute indirect Einsums, we introduce the Insum
compiler, which generates efficient GPU code for these Einsums by lowering to
the PyTorch compiler, extended to better support Tensor Core-enabled indirect
Einsums. We also present two fixed-length sparse formats, GroupCOO and
BlockGroupCOO, designed to fit naturally with indirect Einsums. Our approach
achieves 1.14x to 3.81x speedups across a range of sparse GPU applications
while reducing lines of code by 202x to 4491x compared to hand-written
implementations.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [520] [The Cultural Mapping and Pattern Analysis (CMAP) Visualization Toolkit: Open Source Text Analysis for Qualitative and Computational Social Science](https://arxiv.org/abs/2510.16140)
*Corey M. Abramson,Yuhan,Nian*

Main category: stat.AP

TL;DR: 本文介绍了CMAP可视化工具包，它是用于分析和可视化文本数据的开源套件，适用于定性和计算社会科学研究，有多种功能且可与其他Python工具集成。


<details>
  <summary>Details</summary>
Motivation: 现有商用定性数据分析软件缺乏高可扩展性的开源选项，无法处理大数据集和进行高级统计与语言建模，因此开发CMAP工具包。

Method: 采用实用主义方法，使研究工具与社会科学项目目标相契合，依据研究范式和问题确定方法。

Result: CMAP可视化工具包可通过调整少量参数提供多种可能性，还能与其他Python工具集成。

Conclusion: CMAP可视化工具包为定性和计算社会科学研究提供了有效的开源解决方案。

Abstract: The CMAP (cultural mapping and pattern analysis) visualization toolkit
introduced in this paper is an open-source suite for analyzing and visualizing
text data - from qualitative fieldnotes and in-depth interview transcripts to
historical documents and web-scaped data like message board posts or blogs. The
toolkit is designed for scholars integrating pattern analysis, data
visualization, and explanation in qualitative and/or computational social
science (CSS). Despite the existence of off-the-shelf commercial qualitative
data analysis software, there is a dearth of highly scalable open source
options that can work with large data sets, and allow advanced statistical and
language modeling. The foundation of the toolkit is a pragmatic approach that
aligns research tools with social science project goals- empirical explanation,
theory-guided measurement, comparative design, or evidence-based
recommendations- guided by the principle that research paradigm and questions
should determine methods. Consequently, the CMAP visualization toolkit offers a
range of possibilities through the adjustment of relatively small number of
parameters, and allows integration with other python tools.

</details>


### [521] [A hierarchical Bayesian approach for population-based structural health monitoring in ship hull structures](https://arxiv.org/abs/2510.16316)
*Georgios Aravanis,Nicholas Silionis,Jacopo Bardiani,Marco Giglio,Konstantinos Anyfantis,Claudio Sbarufatti*

Main category: stat.AP

TL;DR: 本文研究用分层贝叶斯模型推断板单元群体和领域水平的挠度幅值分布，以检测初始过度挠度，在数据稀疏时分层模型结果更稳健。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测策略可通过群体数据共享提升效果，检测板单元初始过度挠度很重要但需合适方法。

Method: 采用分层贝叶斯模型，用有限元模型生成应变响应数据，通过马尔可夫链蒙特卡罗进行贝叶斯推断，用替代模型计算似然函数，并与独立模型对比。

Result: 在数据稀疏条件下，分层模型在不确定性方面能提供更稳健的结果。

Conclusion: 分层贝叶斯模型在数据稀疏的结构健康监测决策任务中有优势。

Abstract: Structural health monitoring (SHM) strategies involve the processing of
structural response data to indirectly assess an asset's condition. These
strategies can be enhanced for a group of structures, especially when they are
similar, since mutual underlying physics are expected to exist. The concept
behind population-based SHM exploits the sharing of data among individuals, so
that data-rich members can support data-scarce ones. One approach to
population-level modeling is the hierarchical Bayesian method, where the model
is structured hierarchically in terms of its parameters, and correlation among
learning tasks is enabled by conditioning on shared latent variables.
  This work investigates the application of a hierarchical Bayesian model to
infer expected distributions of deflection amplitudes at both the population
and domain levels, with the aim of detecting excessive initial deflections in a
population of plate elements. Although these damages are typically localized,
they can trigger unexpected events, if not properly monitored. The work is
conducted in a numerical setting using a Finite Element model to generate
strain response data, which serve as the monitoring data. Bayesian inference
was conducted using Markov Chain Monte Carlo (MCMC), with a surrogate model
employed to calculate the likelihood function. The hierarchical approach was
compared to an independent model for a plate component with few data. The
results revealed that, under data sparsity conditions, the hierarchical model
can offer more robust results in terms of uncertainty, which is essential for
decision-making tasks.

</details>


### [522] [Synergizing chemical and AI communities for advancing laboratories of the future](https://arxiv.org/abs/2510.16293)
*Saejin Oh,Xinyi Fang,I-Hsin Lin,Paris Dee,Christopher S. Dunham,Stacy M. Copp,Abigail G. Doyle,Javier Read de Alaniz,Mengyang Gu*

Main category: stat.AP

TL;DR: 本文介绍ML预测模型和基于大语言模型的AI代理在化学实验室任务中的应用，通过案例说明其作用并指出挑战。


<details>
  <summary>Details</summary>
Motivation: 自动化实验设施发展和实验数据数字化带来机遇，需采用ML预测模型和AI代理推动化学实验室发展。

Method: 介绍ML预测模型用于多种实验室任务，引入基于大语言模型的AI代理辅助获取知识，给出三个不同领域案例研究。

Result: 说明ML模型和AI代理可减少耗时实验和手动数据分析。

Conclusion: 指出存在挑战，需要实验和计算领域共同努力解决。

Abstract: The development of automated experimental facilities and the digitization of
experimental data have introduced numerous opportunities to radically advance
chemical laboratories. As many laboratory tasks involve predicting and
understanding previously unknown chemical relationships, machine learning (ML)
approaches trained on experimental data can substantially accelerate the
conventional design-build-test-learn process. This outlook article aims to help
chemists understand and begin to adopt ML predictive models for a variety of
laboratory tasks, including experimental design, synthesis optimization, and
materials characterization. Furthermore, this article introduces how artificial
intelligence (AI) agents based on large language models can help researchers
acquire background knowledge in chemical or data science and accelerate various
aspects of the discovery process. We present three case studies in distinct
areas to illustrate how ML models and AI agents can be leveraged to reduce
time-consuming experiments and manual data analysis. Finally, we highlight
existing challenges that require continued synergistic effort from both
experimental and computational communities to address.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [523] [Extending Prediction-Powered Inference through Conformal Prediction](https://arxiv.org/abs/2510.16166)
*Daniel Csillag,Pedro Dall'Antonia,Claudio José Struchiner,Guilherme Tegoni Goedert*

Main category: stat.ME

TL;DR: 本文将预测驱动推理与共形预测相结合，解决了推导具有额外保证的预测驱动方法的难题，并通过实例和应用展示了其优势。


<details>
  <summary>Details</summary>
Motivation: 许多应用除有效推理外还需隐私、鲁棒性等额外保证，而推导具有这些保证的预测驱动方法困难且需逐个案例处理。

Method: 通过校准的共形集预测器进行插补，将预测驱动推理与共形预测相连接。

Result: 在均值、Z和M估计、e值及基于e值的程序推理中实例化该程序，且在e值方面是首个离线通用预测驱动程序，在私有和时间序列数据应用中展现优势。

Conclusion: 该方法能自然地实现额外保证，使在标准预测驱动框架中不易处理的任务变得自然。

Abstract: Prediction-powered inference is a recent methodology for the safe use of
black-box ML models to impute missing data, strengthening inference of
statistical parameters. However, many applications require strong properties
besides valid inference, such as privacy, robustness or validity under
continuous distribution shifts; deriving prediction-powered methods with such
guarantees is generally an arduous process, and has to be done case by case. In
this paper, we resolve this issue by connecting prediction-powered inference
with conformal prediction: by performing imputation through a calibrated
conformal set-predictor, we attain validity while achieving additional
guarantees in a natural manner. We instantiate our procedure for the inference
of means, Z- and M-estimation, as well as e-values and e-value-based
procedures. Furthermore, in the case of e-values, ours is the first general
prediction-powered procedure that operates off-line. We demonstrate these
advantages by applying our method on private and time-series data. Both tasks
are nontrivial within the standard prediction-powered framework but become
natural under our method.

</details>


### [524] [Discovering Causal Relationships using Proxy Variables under Unmeasured Confounding](https://arxiv.org/abs/2510.17167)
*Yong Wu,Yanwei Fu,Shouyan Wang,Yizhou Wang,Xinwei Sun*

Main category: stat.ME

TL;DR: 提出新的非参数方法，在未测量混杂因素下检验因果假设，可用于离散和连续变量，通过模拟和真实数据验证有效性。


<details>
  <summary>Details</summary>
Motivation: 以往利用负对照调整混杂偏差的方法局限于离散设置或依赖强假设，需解决这些问题。

Method: 开发通用非参数方法，基于新积分方程，用单个负对照结果建立识别结果，提出基于核的检验程序，还引入负对照暴露恢复可识别性。

Result: 推导了检验的渐近水平和功效性质，通过模拟和真实数据验证方法有效性。

Conclusion: 提出的方法能在未测量混杂因素下有效检验因果假设，适用于离散和连续变量设置。

Abstract: Inferring causal relationships between variable pairs in the observational
study is crucial but challenging, due to the presence of unmeasured
confounding. While previous methods employed the negative controls to adjust
for the confounding bias, they were either restricted to the discrete setting
(i.e., all variables are discrete) or relied on strong assumptions for
identification. To address these problems, we develop a general nonparametric
approach that accommodates both discrete and continuous settings for testing
causal hypothesis under unmeasured confounders. By using only a single negative
control outcome (NCO), we establish a new identification result based on a
newly proposed integral equation that links the outcome and NCO, requiring only
the completeness and mild regularity conditions. We then propose a kernel-based
testing procedure that is more efficient than existing moment-restriction
methods. We derive the asymptotic level and power properties for our tests.
Furthermore, we examine cases where our procedure using only NCO fails to
achieve identification, and introduce a new procedure that incorporates a
negative control exposure (NCE) to restore identifiability. We demonstrate the
effectiveness of our approach through extensive simulations and real-world data
from the Intensive Care Data and World Values Survey.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [525] [AGNES: Adaptive Graph Neural Network and Dynamic Programming Hybrid Framework for Real-Time Nanopore Seed Chaining](https://arxiv.org/abs/2510.16013)
*Jahidul Arafat,Sanjaya Poudel,Fariha Tasmin,Md Kaosar Uddin,Eftakhar Ahmed Arnob*

Main category: q-bio.GN

TL;DR: 本文提出RawHash3混合框架用于纳米孔测序读长比对的种子链接，结合图神经网络和动态规划，在合成数据上评估表现良好，证明图神经网络用于基因组学流程的可行性。


<details>
  <summary>Details</summary>
Motivation: 纳米孔测序读长比对因固有错误率带来计算挑战，现有种子链接方法依赖固定间隙惩罚函数，无法适应不同基因组环境。

Method: 提出RawHash3混合框架，将种子链接形式化为图学习，采用三层EdgeConv GNN和基于置信度的方法选择。

Result: 在1000条合成纳米孔读长和5200个测试种子上，RawHash3精度达99.94%，召回率40.07%，比基线有25.0%相对提升；推理延迟中位数1.59ms；在20%标签损坏下成功率100%。

Conclusion: 图神经网络是用于生产基因组学流程的可行方法。

Abstract: Nanopore sequencing enables real-time long-read DNA sequencing with reads
exceeding 10 kilobases, but inherent error rates of 12-15 percent present
significant computational challenges for read alignment. The critical seed
chaining step must connect exact k-mer matches between reads and reference
genomes while filtering spurious matches, yet state-of-the-art methods rely on
fixed gap penalty functions unable to adapt to varying genomic contexts
including tandem repeats and structural variants. This paper presents RawHash3,
a hybrid framework combining graph neural networks with classical dynamic
programming for adaptive seed chaining that maintains real-time performance
while providing statistical guarantees. We formalize seed chaining as graph
learning where seeds constitute nodes with 12-dimensional feature vectors and
edges encode 8-dimensional spatial relationships including gap consistency. Our
architecture employs three-layer EdgeConv GNN with confidence-based method
selection that dynamically switches between learned guidance and algorithmic
fallback. Comprehensive evaluation on 1,000 synthetic nanopore reads with 5,200
test seeds demonstrates RawHash3 achieves 99.94 percent precision and 40.07
percent recall, representing statistically significant 25.0 percent relative
improvement over baseline with p less than 0.001. The system maintains median
inference latency of 1.59ms meeting real-time constraints, while demonstrating
superior robustness with 100 percent success rate under 20 percent label
corruption versus baseline degradation to 30.3 percent. Cross-validation
confirms stability establishing graph neural networks as viable approach for
production genomics pipelines.

</details>


### [526] [Identifying multi-omics interactions for lung cancer drug targets discovery using Kernel Machine Regression](https://arxiv.org/abs/2510.16093)
*Md. Imtyaz Ahmed,Md. Delwar Hossain,Md Mostafizer Rahman,Md. Ahsan Habib,Md. Mamunur Rashid,Md. Selim Reza,Md Ashad Alam*

Main category: q-bio.GN

TL;DR: 文章分析TCGA肺癌多组学数据集，用统计方法鉴定差异表达基因，通过KMR整合数据，发现组学间显著互作，鉴定出38个相关基因和8个高排名基因，还提出3种潜在抗癌药物。


<details>
  <summary>Details</summary>
Motivation: 癌症表型多样复杂，多组学数据集综合研究癌症虽有效，但理解多组学特征间复杂互作有挑战，故分析肺癌多组学数据。

Method: 使用LIMMA、T检验、CCA和Wilcoxon检验分析TCGA肺癌多组学数据集，用KMR方法整合多组学数据。

Result: 发现基因表达、miRNA表达和DNA甲基化三组学间存在显著互作，鉴定出38个与肺癌显著相关基因，8个高排名基因，3种潜在抗癌药物。

Conclusion: 研究揭示肺癌多组学间互作，所鉴定基因和药物对肺癌研究和治疗有潜在价值，且药物得到其他研究支持。

Abstract: Cancer exhibits diverse and complex phenotypes driven by multifaceted
molecular interactions. Recent biomedical research has emphasized the
comprehensive study of such diseases by integrating multi-omics datasets
(genome, proteome, transcriptome, epigenome). This approach provides an
efficient method for identifying genetic variants associated with cancer and
offers a deeper understanding of how the disease develops and spreads. However,
it is challenging to comprehend complex interactions among the features of
multi-omics datasets compared to single omics. In this paper, we analyze lung
cancer multi-omics datasets from The Cancer Genome Atlas (TCGA). Using four
statistical methods, LIMMA, the T test, Canonical Correlation Analysis (CCA),
and the Wilcoxon test, we identified differentially expressed genes across gene
expression, DNA methylation, and miRNA expression data. We then integrated
these multi-omics data using the Kernel Machine Regression (KMR) approach. Our
findings reveal significant interactions among the three omics: gene
expression, miRNA expression, and DNA methylation in lung cancer. From our data
analysis, we identified 38 genes significantly associated with lung cancer.
From our data analysis, we identified 38 genes significantly associated with
lung cancer. Among these, eight genes of highest ranking (PDGFRB, PDGFRA,
SNAI1, ID1, FGF11, TNXB, ITGB1, ZIC1) were highlighted by rigorous statistical
analysis. Furthermore, in silico studies identified three top-ranked potential
candidate drugs (Selinexor, Orapred, and Capmatinib) that could play a crucial
role in the treatment of lung cancer. These proposed drugs are also supported
by the findings of other independent studies, which underscore their potential
efficacy in the fight against lung cancer.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [527] [On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation](https://arxiv.org/abs/2510.16024)
*Abdulrahman Alhaidari,Balaji Palanisamy,Prashant Krishnamurthy*

Main category: cs.CR

TL;DR: 提出首个去中心化全链上学习框架，解决DeFi平台漏洞利用问题，还开发推理技术并整理真实漏洞数据。


<details>
  <summary>Details</summary>
Motivation: 现有DeFi平台防御手段无法防止通过私有中继或恶意合约提交的攻击，每年造成巨额损失。

Method: 构建去中心化全链上学习框架，在Layer - 2进行高成本计算，将验证后的模型更新传播到Layer - 1，在智能合约内实现低延迟推理；采用Proof - of - Improvement (PoIm) 协议管理训练过程；开发量化和循环展开技术。

Result: 开发出可在以太坊区块gas限制内进行多种模型推理的技术，且与链外版本位精确一致；整理了跨8个EVM链的298个真实漏洞及402笔漏洞交易。

Conclusion: 该框架有望有效解决DeFi平台现有防御手段的不足，抵御漏洞利用攻击。

Abstract: Billions of dollars are lost every year in DeFi platforms by transactions
exploiting business logic or accounting vulnerabilities. Existing defenses
focus on static code analysis, public mempool screening, attacker contract
detection, or trusted off-chain monitors, none of which prevents exploits
submitted through private relays or malicious contracts that execute within the
same block. We present the first decentralized, fully on-chain learning
framework that: (i) performs gas-prohibitive computation on Layer-2 to reduce
cost, (ii) propagates verified model updates to Layer-1, and (iii) enables
gas-bounded, low-latency inference inside smart contracts. A novel
Proof-of-Improvement (PoIm) protocol governs the training process and verifies
each decentralized micro update as a self-verifying training transaction.
Updates are accepted by \textit{PoIm} only if they demonstrably improve at
least one core metric (e.g., accuracy, F1-score, precision, or recall) on a
public benchmark without degrading any of the other core metrics, while
adversarial proposals get financially penalized through an adaptable test set
for evolving threats. We develop quantization and loop-unrolling techniques
that enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs
(with support for formally verified decision tree inference) within the
Ethereum block gas limit, while remaining bit-exact to their off-chain
counterparts, formally proven in Z3. We curate 298 unique real-world exploits
(2020 - 2025) with 402 exploit transactions across eight EVM chains,
collectively responsible for \$3.74 B in losses.

</details>


### [528] [A Multi-Cloud Framework for Zero-Trust Workload Authentication](https://arxiv.org/abs/2510.16067)
*Saurabh Deochake,Ryan Murphy,Jeremiah Gearheart*

Main category: cs.CR

TL;DR: 本文提出基于WIF和OIDC的多云框架用于无密钥认证，在Kubernetes环境验证可减少攻击面并统一管理工作负载身份。


<details>
  <summary>Details</summary>
Motivation: 静态、长期存在的工作负载认证凭据存在安全风险，违反零信任原则。

Method: 采用工作负载身份联合（WIF）和OpenID Connect（OIDC），使用加密验证的临时令牌进行无密钥认证。

Result: 在企业级Kubernetes环境中验证该框架，显著减少攻击面。

Conclusion: 该模型为跨不同云管理工作负载身份提供统一解决方案，便于未来实现基于属性的强大访问控制。

Abstract: Static, long-lived credentials for workload authentication create untenable
security risks that violate Zero-Trust principles. This paper presents a
multi-cloud framework using Workload Identity Federation (WIF) and OpenID
Connect (OIDC) for secretless authentication. Our approach uses
cryptographically-verified, ephemeral tokens, allowing workloads to
authenticate without persistent private keys and mitigating credential theft.
We validate this framework in an enterprise-scale Kubernetes environment, which
significantly reduces the attack surface. The model offers a unified solution
to manage workload identities across disparate clouds, enabling future
implementation of robust, attribute-based access control.

</details>


### [529] [Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments](https://arxiv.org/abs/2510.16087)
*Sabbir M Saleh,Nazim Madhavji,John Steinbacher*

Main category: cs.CR

TL;DR: 研究聚焦云平台CI/CD管道安全问题，提出基于区块链的解决方案提升安全性。


<details>
  <summary>Details</summary>
Motivation: 云平台安全至关重要，多个领域基础设施遭网络攻击，针对近期网络漏洞，关注云平台CI/CD管道安全问题。

Method: 提出基于区块链的解决方案，利用区块链分布式账本技术和防篡改特性，集成威胁建模框架、遵循编码标准，使用工具自动化安全测试。

Result: 未提及。

Conclusion: 未提及。

Abstract: Security is becoming a pivotal point in cloud platforms. Several divisions,
such as business organisations, health care, government, etc., have experienced
cyber-attacks on their infrastructures. This research focuses on security
issues within Continuous Integration and Deployment (CI/CD) pipelines in a
cloud platform as a reaction to recent cyber breaches. This research proposes a
blockchain-based solution to enhance CI/CD pipeline security. This research
aims to develop a framework that leverages blockchain's distributed ledger
technology and tamper-resistant features to improve CI/CD pipeline security.
The goal is to emphasise secure software deployment by integrating threat
modelling frameworks and adherence to coding standards. It also aims to employ
tools to automate security testing to detect publicly disclosed vulnerabilities
and flaws, such as an outdated version of Java Spring Framework, a JavaScript
library from an unverified source, or a database library that allows SQL
injection attacks in the deployed software through the framework.

</details>


### [530] [The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers](https://arxiv.org/abs/2510.16122)
*Owais Makroo,Siva Rajesh Kasa,Sumegh Roychowdhury,Karan Gupta,Nikhil Pattisapu,Santhosh Kasa,Sumit Negi*

Main category: cs.CR

TL;DR: 本文比较生成式和判别式分类器在成员推理攻击（MIAs）下的表现，发现生成式分类器更易受攻击，揭示了分类器设计中的效用 - 隐私权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究对生成式和判别式分类器在MIAs方面的系统比较有限，本文旨在填补这一空白。

Method: 先从理论上分析生成式分类器易受MIAs攻击的原因，再在九个基准数据集上对判别式、生成式和伪生成式文本分类器进行综合实证评估，采用多种MIA策略。

Result: 完全生成式分类器最易出现成员信息泄露，且其常用的推理方法会显著增加隐私风险。

Conclusion: 分类器设计存在效用 - 隐私权衡，部署生成式分类器需谨慎，应开展保护隐私的生成式分类器研究。

Abstract: Membership Inference Attacks (MIAs) pose a critical privacy threat by
enabling adversaries to determine whether a specific sample was included in a
model's training dataset. Despite extensive research on MIAs, systematic
comparisons between generative and discriminative classifiers remain limited.
This work addresses this gap by first providing theoretical motivation for why
generative classifiers exhibit heightened susceptibility to MIAs, then
validating these insights through comprehensive empirical evaluation. Our study
encompasses discriminative, generative, and pseudo-generative text classifiers
across varying training data volumes, evaluated on nine benchmark datasets.
Employing a diverse array of MIA strategies, we consistently demonstrate that
fully generative classifiers which explicitly model the joint likelihood
$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe
that the canonical inference approach commonly used in generative classifiers
significantly amplifies this privacy risk. These findings reveal a fundamental
utility-privacy trade-off inherent in classifier design, underscoring the
critical need for caution when deploying generative classifiers in
privacy-sensitive applications. Our results motivate future research directions
in developing privacy-preserving generative classifiers that can maintain
utility while mitigating membership inference vulnerabilities.

</details>


### [531] [A Graph-Attentive LSTM Model for Malicious URL Detection](https://arxiv.org/abs/2510.15971)
*Md. Ifthekhar Hossain,Kazi Abdullah Al Arafat,Bryce Shepard,Kayd Craig,Imtiaz Parvez*

Main category: cs.CR

TL;DR: 本文提出GNN - GAT - LSTM混合深度学习模型检测恶意URL，模型表现出色，有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有黑名单检测方法难以识别新的或混淆的恶意URL，需新检测方法。

Method: 结合GNN、GAT和LSTM网络，将URL转为图，用独热编码表示节点特征，使用特征工程和数据平衡技术处理651,191个URL数据。

Result: 模型测试准确率达0.9806，加权F1分数为0.9804，在多数类别上有良好的精确率和召回率。

Conclusion: 该模型是检测恶意URL的高效可扩展系统，有很强的实际网络安全应用潜力。

Abstract: Malicious URLs pose significant security risks as they facilitate phishing
attacks, distribute malware, and empower attackers to deface websites.
Blacklist detection methods fail to identify new or obfuscated URLs because
they depend on pre-existing patterns. This work presents a hybrid deep learning
model named GNN-GAT-LSTM that combines Graph Neural Networks (GNNs) with Graph
Attention Networks (GATs) and Long Short-Term Memory (LSTM) networks. The
proposed architecture extracts both the structural and sequential patterns of
the features from data. The model transforms URLs into graphs through a process
where characters become nodes that connect through edges. It applies one-hot
encoding to represent node features. The model received training and testing
data from a collection of 651,191 URLs, which were classified into benign,
phishing, defacement, and malware categories. The preprocessing stage included
both feature engineering and data balancing techniques, which addressed the
class imbalance issue to enhance model learning. The GNN-GAT-LSTM model
achieved outstanding performance through its test accuracy of 0.9806 and its
weighted F1-score of 0.9804. It showed excellent precision and recall
performance across most classes, particularly for benign and defacement URLs.
Overall, the model provides an efficient and scalable system for detecting
malicious URLs while demonstrating strong potential for real-world
cybersecurity applications.

</details>


### [532] [Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts](https://arxiv.org/abs/2510.15973)
*Tiarnaigh Downey-Webb,Olamide Jogunola,Oluwaseun Ajao*

Main category: cs.CR

TL;DR: 本文对四个主流大语言模型进行安全评估，用不同攻击方法和数据集测试，发现模型鲁棒性有差异，有攻击转移模式及不同危害类别的脆弱性差异。


<details>
  <summary>Details</summary>
Motivation: 对四个主流大语言模型进行系统的安全评估，了解跨模型安全漏洞。

Method: 对Phi - 2、Llama - 2 - 7B - Chat、GPT - 3.5 - Turbo和GPT - 4四个模型，用人类编写提示、AutoDAN、GCG和TAP四种攻击类别，采用SALAD - Bench数据集中1200个精心分层的提示进行评估。

Result: 模型鲁棒性有显著差异，Llama - 2整体安全性最高，Phi - 2最脆弱；GCG和TAP攻击对目标模型无效，但转移到其他模型成功率更高；不同危害类别脆弱性有显著差异，恶意使用提示攻击成功率最高。

Conclusion: 研究有助于理解跨模型安全漏洞，为开发针对性防御机制提供可行见解。

Abstract: This paper presents a systematic security assessment of four prominent Large
Language Models (LLMs) against diverse adversarial attack vectors. We evaluate
Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack
categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG),
and Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs
1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six
harm categories. Results demonstrate significant variations in model
robustness, with Llama-2 achieving the highest overall security (3.4% average
attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0%
average attack success rate). We identify critical transferability patterns
where GCG and TAP attacks, though ineffective against their target model
(Llama-2), achieve substantially higher success rates when transferred to other
models (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals
significant differences in vulnerability across harm categories ($p < 0.001$),
with malicious use prompts showing the highest attack success rates (10.71%
average). Our findings contribute to understanding cross-model security
vulnerabilities and provide actionable insights for developing targeted defense
mechanisms

</details>


### [533] [Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization](https://arxiv.org/abs/2510.15976)
*Chenrui Wang,Junyi Shu,Billy Chiu,Yu Li,Saleh Alharbi,Min Zhang,Jing Li*

Main category: cs.CR

TL;DR: 提出Learning to Watermark (LTW)选择性水印框架，利用多目标优化平衡水印可检测性和文本质量，实验表明能提升文本质量且不影响可检测性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型水印技术在可检测性和生成文本质量间存在权衡问题。

Method: 引入LTW框架，采用轻量级网络，通过分析句子嵌入、标记熵和当前水印比率自适应决定何时应用水印，使用两个特定构造的损失函数训练网络以实现帕累托最优解。

Result: 将LTW与两种基线水印方法集成，实验显示能显著提升文本质量且不影响可检测性。

Conclusion: 选择性水印方法为大语言模型水印设计提供新视角，可保留水印文本高质量。

Abstract: The rapid development of LLMs has raised concerns about their potential
misuse, leading to various watermarking schemes that typically offer high
detectability. However, existing watermarking techniques often face trade-off
between watermark detectability and generated text quality. In this paper, we
introduce Learning to Watermark (LTW), a novel selective watermarking framework
that leverages multi-objective optimization to effectively balance these
competing goals. LTW features a lightweight network that adaptively decides
when to apply the watermark by analyzing sentence embeddings, token entropy,
and current watermarking ratio. Training of the network involves two
specifically constructed loss functions that guide the model toward
Pareto-optimal solutions, thereby harmonizing watermark detectability and text
quality. By integrating LTW with two baseline watermarking methods, our
experimental evaluations demonstrate that LTW significantly enhances text
quality without compromising detectability. Our selective watermarking approach
offers a new perspective for designing watermarks for LLMs and a way to
preserve high text quality for watermarks. The code is publicly available at:
https://github.com/fattyray/learning-to-watermark

</details>


### [534] [MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents](https://arxiv.org/abs/2510.15994)
*Dongsen Zhang,Zekun Li,Xu Luo,Xuannan Liu,Peipei Li,Wenjun Xu*

Main category: cs.CR

TL;DR: 本文提出MCP安全基准MSB，评估大语言模型代理抵抗MCP特定攻击的能力，评估九种流行大语言模型代理，结果揭示攻击有效性，为研究和强化MCP代理提供基线。


<details>
  <summary>Details</summary>
Motivation: MCP在实现广泛互操作性的同时扩大了攻击面，需要系统评估大语言模型代理抵抗MCP特定攻击的能力。

Method: 提出MSB，包含12种攻击的分类、使用MCP运行真实工具的评估框架和净弹性性能（NRP）的鲁棒性指标，对九种大语言模型代理进行评估。

Result: 评估产生2000个攻击实例，结果显示攻击对MCP各阶段的有效性，性能强的模型因工具调用和指令遵循能力突出而更易受攻击。

Conclusion: MSB为研究人员和从业者研究、比较和强化MCP代理提供了实用基线。

Abstract: The Model Context Protocol (MCP) standardizes how large language model (LLM)
agents discover, describe, and call external tools. While MCP unlocks broad
interoperability, it also enlarges the attack surface by making tools
first-class, composable objects with natural-language metadata, and
standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end
evaluation suite that systematically measures how well LLM agents resist
MCP-specific attacks throughout the full tool-use pipeline: task planning, tool
invocation, and response handling. MSB contributes: (1) a taxonomy of 12
attacks including name-collision, preference manipulation, prompt injections
embedded in tool descriptions, out-of-scope parameter requests,
user-impersonating responses, false-error escalation, tool-transfer, retrieval
injection, and mixed attacks; (2) an evaluation harness that executes attacks
by running real tools (both benign and malicious) via MCP rather than
simulation; and (3) a robustness metric that quantifies the trade-off between
security and performance: Net Resilient Performance (NRP). We evaluate nine
popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack
instances. Results reveal the effectiveness of attacks against each stage of
MCP. Models with stronger performance are more vulnerable to attacks due to
their outstanding tool calling and instruction following capabilities. MSB
provides a practical baseline for researchers and practitioners to study,
compare, and harden MCP agents.

</details>


### [535] [Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers](https://arxiv.org/abs/2510.16005)
*Giacomo Bertollo,Naz Bodemir,Jonah Burgess*

Main category: cs.CR

TL;DR: 分析500名CTF参与者发现，参与者能用常见技术绕过简单AI护栏，但多层多步防御仍具挑战，为构建安全AI系统提供见解。


<details>
  <summary>Details</summary>
Motivation: 为构建更安全的AI系统提供参考。

Method: 分析500名CTF参与者的行为。

Result: 参与者能绕过简单AI护栏，多层多步防御有挑战。

Conclusion: 多层多步防御有助于构建更安全的AI系统。

Abstract: Analyzing 500 CTF participants, this paper shows that while participants
readily bypassed simple AI guardrails using common techniques, layered
multi-step defenses still posed significant challenges, offering concrete
insights for building safer AI systems.

</details>


### [536] [Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks](https://arxiv.org/abs/2510.16028)
*Jianzhu Yao,Hongxu Su,Taobo Liao,Zerui Cheng,Huan Zhang,Xuechao Wang,Pramod Viswanath*

Main category: cs.CR

TL;DR: 提出NAO协议用于验证神经网络输出，在多硬件上实现可扩展性与可验证性的平衡。


<details>
  <summary>Details</summary>
Motivation: ML即服务模式下用户难以验证输出，现有方法存在局限性，浮点执行具有不确定性。

Method: 提出NAO协议，结合两种误差模型，触发争议游戏递归分区计算图，实现无信任硬件和确定性内核的验证。

Result: 实现与PyTorch兼容的运行时和合约层，在多硬件上运行开销小，经验阈值比理论界限更严格，边界感知对抗攻击成功率为0%。

Conclusion: NAO协议调和了现实世界异构ML计算的可扩展性和可验证性。

Abstract: Neural networks increasingly run on hardware outside the user's control
(cloud GPUs, inference marketplaces). Yet ML-as-a-Service reveals little about
what actually ran or whether returned outputs faithfully reflect the intended
inputs. Users lack recourse against service downgrades (model swaps,
quantization, graph rewrites, or discrepancies like altered ad embeddings).
Verifying outputs is hard because floating-point(FP) execution on heterogeneous
accelerators is inherently nondeterministic. Existing approaches are either
impractical for real FP neural networks or reintroduce vendor trust. We present
NAO: a Nondeterministic tolerance Aware Optimistic verification protocol that
accepts outputs within principled operator-level acceptance regions rather than
requiring bitwise equality. NAO combines two error models: (i) sound
per-operator IEEE-754 worst-case bounds and (ii) tight empirical percentile
profiles calibrated across hardware. Discrepancies trigger a Merkle-anchored,
threshold-guided dispute game that recursively partitions the computation graph
until one operator remains, where adjudication reduces to a lightweight
theoretical-bound check or a small honest-majority vote against empirical
thresholds. Unchallenged results finalize after a challenge window, without
requiring trusted hardware or deterministic kernels. We implement NAO as a
PyTorch-compatible runtime and a contract layer currently deployed on Ethereum
Holesky testnet. The runtime instruments graphs, computes per-operator bounds,
and runs unmodified vendor kernels in FP32 with negligible overhead (0.3% on
Qwen3-8B). Across CNNs, Transformers and diffusion models on A100, H100,
RTX6000, RTX4090, empirical thresholds are $10^2-10^3$ times tighter than
theoretical bounds, and bound-aware adversarial attacks achieve 0% success. NAO
reconciles scalability with verifiability for real-world heterogeneous ML
compute.

</details>


### [537] [Membership Inference over Diffusion-models-based Synthetic Tabular Data](https://arxiv.org/abs/2510.16037)
*Peini Cheng,Amir Bahmani*

Main category: cs.CR

TL;DR: 研究扩散型合成表格数据生成方法的隐私风险，发现TabDDPM更易受成员推理攻击，TabSyn有抵抗力。


<details>
  <summary>Details</summary>
Motivation: 调查扩散型合成表格数据生成方法的隐私风险，尤其是其对成员推理攻击的易感性。

Method: 基于逐步误差比较方法开发基于查询的成员推理攻击，对TabDDPM和TabSyn两个模型进行研究。

Result: TabDDPM更易受攻击，TabSyn对攻击模型有抵抗力。

Conclusion: 强调评估扩散模型隐私影响的重要性，鼓励对合成数据生成的强大隐私保护机制进行更多研究。

Abstract: This study investigates the privacy risks associated with diffusion-based
synthetic tabular data generation methods, focusing on their susceptibility to
Membership Inference Attacks (MIAs). We examine two recent models, TabDDPM and
TabSyn, by developing query-based MIAs based on the step-wise error comparison
method. Our findings reveal that TabDDPM is more vulnerable to these attacks.
TabSyn exhibits resilience against our attack models. Our work underscores the
importance of evaluating the privacy implications of diffusion models and
encourages further research into robust privacy-preserving mechanisms for
synthetic data generation.

</details>


### [538] [ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates](https://arxiv.org/abs/2510.16078)
*Abdelilah Ganmati,Karim Afdel,Lahcen Koutti*

Main category: cs.CR

TL;DR: 提出用于面部验证的卡上匹配设计，用PCA - ITQ生成模板，在卡上通过汉明距离比较，测试显示该设计满足ISO/IEC传输约束和隐私目标，但存在单数据集评估等局限。


<details>
  <summary>Details</summary>
Motivation: 设计一种实用的卡上面部验证方案，满足ISO/IEC传输约束和隐私目标。

Method: 通过PCA - ITQ在卡外生成64/128位模板，在卡上通过恒定时间汉明距离比较，指定ISO/IEC命令APDUs，利用CelebA数据集进行测试。

Result: 在不同速率下有不同验证时间，如9.6 kbps时64位43.9 ms、128位52.3 ms；38.4 kbps时均小于14 ms；FAR = 1%时，两种码长TPR = 0.836，128位EER更低；可选+6 B辅助数据对延迟影响可忽略。

Conclusion: 短二进制模板、固定有效负载决策型APDUs和恒定时间匹配满足ISO/IEC传输约束和隐私目标，但存在单数据集评估和设计级时序的局限，下一步可进行AgeDB/CFP - FP和卡上微基准测试。

Abstract: We present a practical match-on-card design for face verification in which
compact 64/128-bit templates are produced off-card by PCA-ITQ and compared
on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and
14443-4 command APDUs with fixed-length payloads and decision-only status words
(no score leakage), together with a minimal per-identity EEPROM map. Using real
binary codes from a CelebA working set (55 identities, 412 images), we (i)
derive operating thresholds from ROC/DET, (ii) replay enroll->verify
transactions at those thresholds, and (iii) bound end-to-end time by pure link
latency plus a small constant on-card budget. Even at the slowest contact rate
(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at
38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,
while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted
symbol-level parity over empirically unstable bits) is latency-negligible.
Overall, short binary templates, fixed-payload decision-only APDUs, and
constant-time matching satisfy ISO/IEC transport constraints with wide timing
margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset
evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and
on-card microbenchmarks as next steps.

</details>


### [539] [SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection](https://arxiv.org/abs/2510.16219)
*Yang Feng,Xudong Pan*

Main category: cs.CR

TL;DR: 提出SentinelNet框架，用于主动检测和缓解多智能体系统中恶意行为，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统防御因被动设计或集中架构存在不足，无法有效应对恶意代理威胁。

Method: 提出去中心化框架SentinelNet，为每个智能体配备基于信用的检测器，通过对比学习训练，生成对抗轨迹模拟威胁。

Result: 在多智能体系统基准测试中，SentinelNet接近完美检测恶意智能体，两轮辩论内接近100%，恢复95%系统准确率。

Conclusion: SentinelNet具有跨领域和攻击模式的泛化性，为保护协作多智能体系统建立了新范式。

Abstract: Malicious agents pose significant threats to the reliability and
decision-making capabilities of Multi-Agent Systems (MAS) powered by Large
Language Models (LLMs). Existing defenses often fall short due to reactive
designs or centralized architectures which may introduce single points of
failure. To address these challenges, we propose SentinelNet, the first
decentralized framework for proactively detecting and mitigating malicious
behaviors in multi-agent collaboration. SentinelNet equips each agent with a
credit-based detector trained via contrastive learning on augmented adversarial
debate trajectories, enabling autonomous evaluation of message credibility and
dynamic neighbor ranking via bottom-k elimination to suppress malicious
communications. To overcome the scarcity of attack data, it generates
adversarial trajectories simulating diverse threats, ensuring robust training.
Experiments on MAS benchmarks show SentinelNet achieves near-perfect detection
of malicious agents, close to 100% within two debate rounds, and recovers 95%
of system accuracy from compromised baselines. By exhibiting strong
generalizability across domains and attack patterns, SentinelNet establishes a
novel paradigm for safeguarding collaborative MAS.

</details>


### [540] [Detecting Adversarial Fine-tuning with Auditing Agents](https://arxiv.org/abs/2510.16255)
*Sarah Egler,John Schulman,Nicholas Carlini*

Main category: cs.CR

TL;DR: 研究大语言模型微调API对抗使用的鲁棒检测机制，引入微调审计代理，评估检测方法并取得一定检测率，发布审计代理代码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调API存在被对手利用绕过防护的风险，需研究鲁棒检测机制。

Method: 引入微调审计代理，让其访问微调数据集、微调前后模型并为微调任务分配风险分数，对多种攻击和良性模型进行评估。

Result: 在1%误报率下，审计代理对对抗性微调的检测率达56.2%，能检测隐蔽密码攻击。

Conclusion: 虽良性微调中无意的细微安全降级仍是挑战，但为该领域后续工作建立了基线配置。

Abstract: Large Language Model (LLM) providers expose fine-tuning APIs that let end
users fine-tune their frontier LLMs. Unfortunately, it has been shown that an
adversary with fine-tuning access to an LLM can bypass safeguards. Particularly
concerning, such attacks may avoid detection with datasets that are only
implicitly harmful. Our work studies robust detection mechanisms for
adversarial use of fine-tuning APIs. We introduce the concept of a fine-tuning
auditing agent and show it can detect harmful fine-tuning prior to model
deployment. We provide our auditing agent with access to the fine-tuning
dataset, as well as the fine-tuned and pre-fine-tuned models, and request the
agent assigns a risk score for the fine-tuning job. We evaluate our detection
approach on a diverse set of eight strong fine-tuning attacks from the
literature, along with five benign fine-tuned models, totaling over 1400
independent audits. These attacks are undetectable with basic content
moderation on the dataset, highlighting the challenge of the task. With the
best set of affordances, our auditing agent achieves a 56.2% detection rate of
adversarial fine-tuning at a 1% false positive rate. Most promising, the
auditor is able to detect covert cipher attacks that evade safety evaluations
and content moderation of the dataset. While benign fine-tuning with
unintentional subtle safety degradation remains a challenge, we establish a
baseline configuration for further work in this area. We release our auditing
agent at https://github.com/safety-research/finetuning-auditor.

</details>


### [541] [Toward Understanding Security Issues in the Model Context Protocol Ecosystem](https://arxiv.org/abs/2510.16558)
*Xiaofan Li,Xing Gao*

Main category: cs.CR

TL;DR: 对MCP生态系统进行全面安全分析，发现漏洞并提出防御策略。


<details>
  <summary>Details</summary>
Motivation: MCP生态系统发展迅速，但缺乏对其架构和安全风险的系统研究。

Method: 将MCP生态系统分解为三个核心组件，进行定性和定量分析，收集并分析67,057个服务器的数据集。

Result: 发现主机缺乏输出验证机制，注册中心缺乏服务器提交审核流程，大量服务器可被劫持。

Conclusion: 提出针对MCP主机、注册中心和用户的实用防御策略，并将发现告知受影响方。

Abstract: The Model Context Protocol (MCP) is an emerging open standard that enables
AI-powered applications to interact with external tools through structured
metadata. A rapidly growing ecosystem has formed around MCP, including a wide
range of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP
registries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),
and thousands of community-contributed MCP servers. Although the MCP ecosystem
is gaining traction, there has been little systematic study of its architecture
and associated security risks. In this paper, we present the first
comprehensive security analysis of the MCP ecosystem. We decompose MCP
ecosystem into three core components: hosts, registries, and servers, and study
the interactions and trust relationships among them. Users search for servers
on registries and configure them in the host, which translates LLM-generated
output into external tool invocations provided by the servers and executes
them. Our qualitative analysis reveals that hosts lack output verification
mechanisms for LLM-generated outputs, enabling malicious servers to manipulate
model behavior and induce a variety of security threats, including but not
limited to sensitive data exfiltration. We uncover a wide range of
vulnerabilities that enable attackers to hijack servers, due to the lack of a
vetted server submission process in registries. To support our analysis, we
collect and analyze a dataset of 67,057 servers from six public registries. Our
quantitative analysis demonstrates that a substantial number of servers can be
hijacked by attackers. Finally, we propose practical defense strategies for MCP
hosts, registries, and users. We responsibly disclosed our findings to affected
hosts and registries.

</details>


### [542] [A Novel GPT-Based Framework for Anomaly Detection in System Logs](https://arxiv.org/abs/2510.16044)
*Zeng Zhang,Wenjie Yin,Xiaoqi Li*

Main category: cs.CR

TL;DR: 提出基于GPT的系统日志智能检测方法，结合结构化输入设计和Focal Loss优化策略提升日志异常检测性能，实验显示优化后GPT - 2模型在关键指标上表现更好。


<details>
  <summary>Details</summary>
Motivation: 系统日志异常事件识别面临数据量大、异常分布和传统方法精度等挑战，需要更好的检测方法。

Method: 用Drain解析器将原始日志转换为事件ID序列，使用Focal Loss损失函数解决类别不平衡问题。

Result: 优化后的GPT - 2模型在精度、召回率和F1分数等关键指标上显著优于未优化模型，在特定任务中与GPT - 3.5 API表现相当或更优。

Conclusion: 基于GPT的系统日志智能检测方法结合结构化输入设计和Focal Loss优化策略能有效提升日志异常检测性能。

Abstract: Identification of anomalous events within system logs constitutes a pivotal
element within the frame- work of cybersecurity defense strategies. However,
this process faces numerous challenges, including the management of substantial
data volumes, the distribution of anomalies, and the precision of con-
ventional methods. To address this issue, the present paper puts forward a
proposal for an intelligent detection method for system logs based on Genera-
tive Pre-trained Transformers (GPT). The efficacy of this approach is
attributable to a combination of structured input design and a Focal Loss op-
timization strategy, which collectively result in a substantial enhancement of
the performance of log anomaly detection. The initial approach involves the
conversion of raw logs into event ID sequences through the use of the Drain
parser. Subsequently, the Focal Loss loss function is employed to address the
issue of class imbalance. The experimental re- sults demonstrate that the
optimized GPT-2 model significantly outperforms the unoptimized model in a
range of key metrics, including precision, recall, and F1 score. In specific
tasks, comparable or superior performance has been demonstrated to that of the
GPT-3.5 API.

</details>


### [543] [UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks](https://arxiv.org/abs/2510.16923)
*Mansi Phute,Matthew Hull,Haoran Wang,Alec Helbling,ShengYun Peng,Willian Lunardi,Martin Andreoni,Wenke Lee,Polo Chau*

Main category: cs.CR

TL;DR: 提出软件框架UNDREAM，弥合真实感模拟器和可微渲染器差距，实现3D物体对抗扰动端到端优化，推动物理对抗攻击研究。


<details>
  <summary>Details</summary>
Motivation: 现有模拟不可微，导致攻击未整合模拟环境因素，降低攻击成功率，需解决此局限。

Method: 引入软件框架UNDREAM，实现真实感模拟器和可微渲染器的连接，提供对环境的完全控制。

Result: 能快速探索不同可配置环境中多种物理上合理的对抗性物体。

Conclusion: 真实感模拟和可微优化的结合为物理对抗攻击研究开辟新途径。

Abstract: Deep learning models deployed in safety critical applications like autonomous
driving use simulations to test their robustness against adversarial attacks in
realistic conditions. However, these simulations are non-differentiable,
forcing researchers to create attacks that do not integrate simulation
environmental factors, reducing attack success. To address this limitation, we
introduce UNDREAM, the first software framework that bridges the gap between
photorealistic simulators and differentiable renderers to enable end-to-end
optimization of adversarial perturbations on any 3D objects. UNDREAM enables
manipulation of the environment by offering complete control over weather,
lighting, backgrounds, camera angles, trajectories, and realistic human and
object movements, thereby allowing the creation of diverse scenes. We showcase
a wide array of distinct physically plausible adversarial objects that UNDREAM
enables researchers to swiftly explore in different configurable environments.
This combination of photorealistic simulation and differentiable optimization
opens new avenues for advancing research of physical adversarial attacks.

</details>


### [544] [Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.17098)
*Elias Hossain,Swayamjit Saha,Somshubhra Roy,Ravi Prasad*

Main category: cs.CR

TL;DR: 论文指出即使提示和参数安全，Transformer语言模型推理时的KV缓存仍是攻击面，提出MTI框架扰动缓存键向量，理论分析扰动传播，实证显示MTI影响模型表现，表明缓存完整性是LLM部署的关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 发现Transformer语言模型即使在提示和参数安全时，其KV缓存仍是易受攻击的被忽视的方面，为解决该安全问题开展研究。

Method: 引入Malicious Token Injection (MTI)框架，通过加性高斯噪声、归零和正交旋转等方式，在选定层和时间步对缓存键向量进行系统扰动，并进行理论分析。

Result: MTI显著改变了GPT - 2和LLaMA - 2/7B的下一个标记分布和下游任务性能，还破坏了检索增强和智能体推理管道。

Conclusion: 缓存完整性是当前大语言模型部署中关键且未充分探索的漏洞，缓存破坏可作为未来鲁棒性和安全研究的可重现且有理论依据的威胁模型。

Abstract: Even when prompts and parameters are secured, transformer language models
remain vulnerable because their key-value (KV) cache during inference
constitutes an overlooked attack surface. This paper introduces Malicious Token
Injection (MTI), a modular framework that systematically perturbs cached key
vectors at selected layers and timesteps through controlled magnitude and
frequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A
theoretical analysis quantifies how these perturbations propagate through
attention, linking logit deviations to the Frobenius norm of corruption and
softmax Lipschitz dynamics. Empirical results show that MTI significantly
alters next-token distributions and downstream task performance across GPT-2
and LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic
reasoning pipelines. These findings identify cache integrity as a critical yet
underexplored vulnerability in current LLM deployments, positioning cache
corruption as a reproducible and theoretically grounded threat model for future
robustness and security research.

</details>


### [545] [A Versatile Framework for Designing Group-Sparse Adversarial Attacks](https://arxiv.org/abs/2510.16637)
*Alireza Heshmati,Saman Soleimani Roudi,Sajjad Amini,Shahrokh Ghaemmaghami,Farokh Marvasti*

Main category: cs.CR

TL;DR: 提出ATOS框架生成结构化、稀疏对抗扰动，在CIFAR - 10和ImageNet上取得100%攻击成功率，扰动更稀疏且结构更连贯。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击常忽略扰动稀疏性，限制对结构变化建模和解释DNN处理有意义输入模式的能力。

Method: 提出ATOS框架，引入OSL0函数促进收敛和稀疏结构化扰动，分组通道和相邻像素，用对数和指数绝对值近似L - infinity梯度控制扰动幅度。

Result: 在CIFAR - 10和ImageNet上实现100%攻击成功率，产生比先前方法更稀疏、结构更连贯的扰动。

Conclusion: ATOS框架能生成结构化稀疏对抗扰动，有助于识别鲁棒与非鲁棒特征，提供反事实解释。

Abstract: Existing adversarial attacks often neglect perturbation sparsity, limiting
their ability to model structural changes and to explain how deep neural
networks (DNNs) process meaningful input patterns. We propose ATOS (Attack
Through Overlapping Sparsity), a differentiable optimization framework that
generates structured, sparse adversarial perturbations in element-wise,
pixel-wise, and group-wise forms. For white-box attacks on image classifiers,
we introduce the Overlapping Smoothed L0 (OSL0) function, which promotes
convergence to a stationary point while encouraging sparse, structured
perturbations. By grouping channels and adjacent pixels, ATOS improves
interpretability and helps identify robust versus non-robust features. We
approximate the L-infinity gradient using the logarithm of the sum of
exponential absolute values to tightly control perturbation magnitude. On
CIFAR-10 and ImageNet, ATOS achieves a 100% attack success rate while producing
significantly sparser and more structurally coherent perturbations than prior
methods. The structured group-wise attack highlights critical regions from the
network's perspective, providing counterfactual explanations by replacing
class-defining regions with robust features from the target class.

</details>


### [546] [DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge](https://arxiv.org/abs/2510.16716)
*Asmita Mohanty,Gezheng Kang,Lei Gao,Murali Annavaram*

Main category: cs.CR

TL;DR: 提出DistilLock框架解决LLM微调中的隐私和知识产权泄露问题，能实现边缘端隐私保护知识蒸馏。


<details>
  <summary>Details</summary>
Motivation: 传统LLM微调依赖云基础设施存在隐私问题，边缘端微调存在知识产权泄露风险，需解决两难困境。

Method: 提出DistilLock框架，在数据所有者设备的可信执行环境（TEE）中执行基础模型作为安全黑盒教师，采用模型混淆机制卸载混淆权重到不可信加速器。

Result: DistilLock能防止未经授权的知识蒸馏和模型窃取攻击，保持高计算效率。

Conclusion: DistilLock为边缘端LLM个性化提供了安全实用的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
diverse tasks, but fine-tuning them typically relies on cloud-based,
centralized infrastructures. This requires data owners to upload potentially
sensitive data to external servers, raising serious privacy concerns. An
alternative approach is to fine-tune LLMs directly on edge devices using local
data; however, this introduces a new challenge: the model owner must transfer
proprietary models to the edge, which risks intellectual property (IP) leakage.
To address this dilemma, we propose DistilLock, a TEE-assisted fine-tuning
framework that enables privacy-preserving knowledge distillation on the edge.
In DistilLock, a proprietary foundation model is executed within a trusted
execution environment (TEE) enclave on the data owner's device, acting as a
secure black-box teacher. This setup preserves both data privacy and model IP
by preventing direct access to model internals. Furthermore, DistilLock employs
a model obfuscation mechanism to offload obfuscated weights to untrusted
accelerators for efficient knowledge distillation without compromising
security. We demonstrate that DistilLock prevents unauthorized knowledge
distillation processes and model-stealing attacks while maintaining high
computational efficiency, but offering a secure and practical solution for
edge-based LLM personalization.

</details>


### [547] [Black-box Optimization of LLM Outputs by Asking for Directions](https://arxiv.org/abs/2510.16794)
*Jie Zhang,Meng Ding,Yang Liu,Jue Hong,Florian Tramèr*

Main category: cs.CR

TL;DR: 提出利用大语言模型以自然语言表达置信度的能力攻击黑盒大语言模型的新方法，应用于三种攻击场景，扩大攻击面，发现模型能力提升反而增加漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒攻击需要访问连续模型输出或依赖其他模型的代理信号，在实际中较难实现，因此寻求新的攻击方法。

Method: 提示大语言模型以一种经过充分校准的方式表达其内部置信度，以实现有效的对抗优化，并应用于三种攻击场景。

Result: 攻击成功针对仅暴露文本输出的系统生成恶意输入，扩大了已部署大语言模型的攻击面；更好更大的模型在表达置信度时校准效果更好，但也增加了漏洞。

Conclusion: 新的攻击方法有效，且模型能力提升与安全漏洞之间存在令人担忧的悖论。

Abstract: We present a novel approach for attacking black-box large language models
(LLMs) by exploiting their ability to express confidence in natural language.
Existing black-box attacks require either access to continuous model outputs
like logits or confidence scores (which are rarely available in practice), or
rely on proxy signals from other models. Instead, we demonstrate how to prompt
LLMs to express their internal confidence in a way that is sufficiently
calibrated to enable effective adversarial optimization. We apply our general
method to three attack scenarios: adversarial examples for vision-LLMs,
jailbreaks and prompt injections. Our attacks successfully generate malicious
inputs against systems that only expose textual outputs, thereby dramatically
expanding the attack surface for deployed LLMs. We further find that better and
larger models exhibit superior calibration when expressing confidence, creating
a concerning security paradox where model capability improvements directly
enhance vulnerability. Our code is available at this
[link](https://github.com/zj-jayzhang/black_box_llm_optimization).

</details>


### [548] [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)
*Masahiro Kaneko,Timothy Baldwin*

Main category: cs.CR

TL;DR: 提出信息论框架计算大语言模型安全披露的信息量，实验验证其有效性，为平衡透明度和安全性提供标尺。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临对抗攻击，信息泄露规模缺乏量化，审计人员缺少原则性指导，需解决透明度 - 风险权衡问题。

Method: 构建信息论框架，将观察信号与目标属性的互信息作为每次查询的泄露比特，分析达到一定误差所需查询次数。

Result: 实验表明，披露不同信息（答案令牌、对数、完整思维过程）所需攻击查询次数不同，适度增加披露可降低攻击成本。

Conclusion: 研究为大语言模型部署时平衡透明度和安全性提供首个原则性衡量标准。

Abstract: Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

</details>


### [549] [QRïS: A Preemptive Novel Method for Quishing Detection Through Structural Features of QR](https://arxiv.org/abs/2510.17175)
*Muhammad Wahid Akram,Keshav Sood,Muneeb Ul Hassan*

Main category: cs.CR

TL;DR: 本文提出QR"iS方法通过QR码结构分析识别钓鱼QR码，具有透明、可复现等特点，实验获83.18%准确率，开发移动应用验证可行性。


<details>
  <summary>Details</summary>
Motivation: 现有防Quishing攻击方法多依赖黑盒技术，缺乏可解释性和透明度，存在诸多局限性。

Method: 生成400,000个样本的QR码数据集，开发算法提取24个结构特征，训练机器学习模型，并与相关研究对比分析，开发移动应用验证。

Result: 训练模型获得高达83.18%的准确率。

Conclusion: 提出的QR"iS方法可提前识别钓鱼QR码，具有透明、可复现、可扩展等优点，在现实场景中可行。

Abstract: Globally, individuals and organizations employ Quick Response (QR) codes for
swift and convenient communication. Leveraging this, cybercriminals embed
falsify and misleading information in QR codes to launch various phishing
attacks which termed as Quishing. Many former studies have introduced defensive
approaches to preclude Quishing such as by classifying the embedded content of
QR codes and then label the QR codes accordingly, whereas other studies
classify them using visual features (i.e., deep features, histogram density
analysis features). However, these approaches mainly rely on black-box
techniques which do not clearly provide interpretability and transparency to
fully comprehend and reproduce the intrinsic decision process; therefore,
having certain obvious limitations includes the approaches' trust,
accountability, issues in bias detection, and many more. We proposed QR\"iS,
the pioneer method to classify QR codes through the comprehensive structural
analysis of a QR code which helps to identify phishing QR codes beforehand. Our
classification method is clearly transparent which makes it reproducible,
scalable, and easy to comprehend. First, we generated QR codes dataset (i.e.
400,000 samples) using recently published URLs datasets [1], [2]. Then, unlike
black-box models, we developed a simple algorithm to extract 24 structural
features from layout patterns present in QR codes. Later, we train the machine
learning models on the harvested features and obtained accuracy of up to
83.18%. To further evaluate the effectiveness of our approach, we perform the
comparative analysis of proposed method with relevant contemporary studies.
Lastly, for real-world deployment and validation, we developed a mobile app
which assures the feasibility of the proposed solution in real-world scenarios
which eventually strengthen the applicability of the study.

</details>


### [550] [GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models](https://arxiv.org/abs/2510.17621)
*Vincenzo Carletti,Pasquale Foggia,Carlo Mazzocca,Giuseppe Parrella,Mario Vento*

Main category: cs.CR

TL;DR: 本文提出GUIDE方法，利用扩散模型作为去噪工具提升联邦学习中图像重建攻击效果，实验证明它能与两种先进攻击方法集成并大幅提高重建质量。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端更新易隐私泄露，现有梯度反转攻击重建的输入有噪声，需提升重建质量。

Method: 提出Gradient Update Inversion with DEnoising (GUIDE)方法，利用扩散模型作为去噪工具，可集成到利用代理数据集的梯度反转攻击中。

Result: GUIDE能与两种先进梯度反转攻击无缝集成，在多个指标上大幅提高重建质量，如在DreamSim指标上感知相似度最高提升46%。

Conclusion: GUIDE方法能有效提升联邦学习中图像重建攻击的重建质量。

Abstract: Federated Learning (FL) enables collaborative training of Machine Learning
(ML) models across multiple clients while preserving their privacy. Rather than
sharing raw data, federated clients transmit locally computed updates to train
the global model. Although this paradigm should provide stronger privacy
guarantees than centralized ML, client updates remain vulnerable to privacy
leakage. Adversaries can exploit them to infer sensitive properties about the
training data or even to reconstruct the original inputs via Gradient Inversion
Attacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to
reconstruct training data by reversing intermediate updates using
optimizationbased techniques. We observe that these approaches usually
reconstruct noisy approximations of the original inputs, whose quality can be
enhanced with specialized denoising models. This paper presents Gradient Update
Inversion with DEnoising (GUIDE), a novel methodology that leverages diffusion
models as denoising tools to improve image reconstruction attacks in FL. GUIDE
can be integrated into any GIAs that exploits surrogate datasets, a widely
adopted assumption in GIAs literature. We comprehensively evaluate our approach
in two attack scenarios that use different FL algorithms, models, and datasets.
Our results demonstrate that GUIDE integrates seamlessly with two state-ofthe-
art GIAs, substantially improving reconstruction quality across multiple
metrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,
as measured by the DreamSim metric.

</details>


### [551] [CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks](https://arxiv.org/abs/2510.17687)
*Xu Zhang,Hao Li,Zhichao Lu*

Main category: cs.CR

TL;DR: 提出ImpForge生成隐式样本，开发CrossGuard防御显式和隐式威胁，实验表明CrossGuard表现优异，增强MLLM鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型易受越狱攻击，隐式攻击难检测且研究不足，缺乏高质量隐式数据。

Method: 提出ImpForge自动化红队管道，利用强化学习和定制奖励模块生成多领域隐式样本；基于此开发CrossGuard意图感知防护机制。

Result: CrossGuard在安全与不安全基准、显式与隐式攻击及多域外设置实验中显著优于现有防御手段，实现强安全性并保持高实用性。

Conclusion: CrossGuard为增强MLLM对现实世界多模态威胁的鲁棒性提供了平衡实用的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong reasoning and
perception capabilities but are increasingly vulnerable to jailbreak attacks.
While existing work focuses on explicit attacks, where malicious content
resides in a single modality, recent studies reveal implicit attacks, in which
benign text and image inputs jointly express unsafe intent. Such joint-modal
threats are difficult to detect and remain underexplored, largely due to the
scarcity of high-quality implicit data. We propose ImpForge, an automated
red-teaming pipeline that leverages reinforcement learning with tailored reward
modules to generate diverse implicit samples across 14 domains. Building on
this dataset, we further develop CrossGuard, an intent-aware safeguard
providing robust and comprehensive defense against both explicit and implicit
threats. Extensive experiments across safe and unsafe benchmarks, implicit and
explicit attacks, and multiple out-of-domain settings demonstrate that
CrossGuard significantly outperforms existing defenses, including advanced
MLLMs and guardrails, achieving stronger security while maintaining high
utility. This offers a balanced and practical solution for enhancing MLLM
robustness against real-world multimodal threats.

</details>


### [552] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: 提出VERA - V框架解决现有多模态红队方法局限，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态红队方法依赖脆弱模板、聚焦单攻击设置、暴露漏洞有限，需新方法。

Method: 引入VERA - V变分推理框架，训练轻量级攻击者近似后验，还集成三种互补策略。

Result: 在HarmBench和HADES基准测试中，VERA - V在开源和前沿VLMs上均优于现有基线，在GPT - 4o上攻击成功率比最佳基线高53.75%。

Conclusion: VERA - V是一种有效的多模态越狱发现方法，能更好地发现VLMs的漏洞。

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [553] [Taming Modality Entanglement in Continual Audio-Visual Segmentation](https://arxiv.org/abs/2510.17234)
*Yuyang Hong,Qi Yang,Tao Zhang,Zili Wang,Zhaojin Fu,Kun Ding,Bin Fan,Shiming Xiang*

Main category: cs.MM

TL;DR: 文章提出CAVS任务，针对其挑战设计CMR框架，实验表明方法优于单模态持续学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态持续学习方法在细粒度学习中处理模态纠缠有局限，需解决新任务。

Method: 提出CAVS任务，设计CMR框架，包括MSS策略和CSR机制，构建三个视听增量场景验证。

Result: 所提方法显著优于单模态持续学习方法。

Conclusion: 所提方法能有效应对多模态细粒度持续学习中的挑战。

Abstract: Recently, significant progress has been made in multi-modal continual
learning, aiming to learn new tasks sequentially in multi-modal settings while
preserving performance on previously learned ones. However, existing methods
mainly focus on coarse-grained tasks, with limitations in addressing modality
entanglement in fine-grained continual learning settings. To bridge this gap,
we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to
continuously segment new classes guided by audio. Through comprehensive
analysis, two critical challenges are identified: 1) multi-modal semantic
drift, where a sounding objects is labeled as background in sequential tasks;
2) co-occurrence confusion, where frequent co-occurring classes tend to be
confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework
is designed to address these challenges. Specifically, for multi-modal semantic
drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select
samples with high modal consistency for rehearsal. Meanwhile, for co-occurence
confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,
allowing for the increase of rehearsal sample frequency of those confusable
classes during training process. Moreover, we construct three audio-visual
incremental scenarios to verify effectiveness of our method. Comprehensive
experiments demonstrate that our method significantly outperforms single-modal
continual learning methods.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [554] [Safire: Similarity Framework for Visualization Retrieval](https://arxiv.org/abs/2510.16662)
*Huyen N. Nguyen,Nils Gehlenborg*

Main category: cs.HC

TL;DR: 本文介绍可视化检索相似性框架Safire，分析现有系统展示其价值，给出建议并探讨影响。


<details>
  <summary>Details</summary>
Motivation: 现有可视化检索系统缺乏系统理解可视化相似性的方法。

Method: 引入Safire概念模型，从比较标准和表示方式两维度定义可视化相似性，对现有表示方式分类。

Result: 分析可视化检索系统，揭示不同用例下标准和方式的匹配情况，表明表示方式选择影响检索能力。

Conclusion: 基于分析给出建议，探讨对多模态学习、AI应用和可视化可重复性的影响。

Abstract: Effective visualization retrieval necessitates a clear definition of
similarity. Despite the growing body of work in specialized visualization
retrieval systems, a systematic approach to understanding visualization
similarity remains absent. We introduce the Similarity Framework for
Visualization Retrieval (Safire), a conceptual model that frames visualization
similarity along two dimensions: comparison criteria and representation
modalities. Comparison criteria identify the aspects that make visualizations
similar, which we divide into primary facets (data, visual encoding,
interaction, style, metadata) and derived properties (data-centric and
human-centric measures). Safire connects what to compare with how comparisons
are executed through representation modalities. We categorize existing
representation approaches into four groups based on their levels of information
content and visualization determinism: raster image, vector image,
specification, and natural language description, together guiding what is
computable and comparable. We analyze several visualization retrieval systems
using Safire to demonstrate its practical value in clarifying similarity
considerations. Our findings reveal how particular criteria and modalities
align across different use cases. Notably, the choice of representation
modality is not only an implementation detail but also an important decision
that shapes retrieval capabilities and limitations. Based on our analysis, we
provide recommendations and discuss broader implications for multimodal
learning, AI applications, and visualization reproducibility.

</details>


### [555] [Mitigating Harmful Erraticism in LLMs Through Dialectical Behavior Therapy Based De-Escalation Strategies](https://arxiv.org/abs/2510.15889)
*Pooja Rangarajan,Jacob Boyle*

Main category: cs.HC

TL;DR: 当前AI聊天机器人开发范式有局限，本文提出基于辩证行为疗法（DBT）原则的框架来提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI聊天机器人开发范式难以维护且易出错，需要更稳健可持续的解决方案。

Method: 将辩证行为疗法（DBT）原则应用于调节聊天机器人对不同用户输入的响应。

Result: 未提及

Conclusion: 未提及

Abstract: The escalating demand for personalized AI chatbot interactions, capable of
dynamically adapting to user emotional states and real-time requests, has
highlighted critical limitations in current development paradigms. Existing
methodologies, which rely on baseline programming, custom personalities, and
manual response adjustments, often prove difficult to maintain and are
susceptible to errors such as hallucinations, erratic outputs, and software
bugs. This paper hypothesizes that a framework rooted in human psychological
principles, specifically therapeutic modalities, can provide a more robust and
sustainable solution than purely technical interventions. Drawing an analogy to
the simulated neural networks of AI mirroring the human brain, we propose the
application of Dialectical Behavior Therapy (DBT) principles to regulate
chatbot responses to diverse user inputs. This research investigates the impact
of a DBT-based framework on AI chatbot performance, aiming to ascertain its
efficacy in yielding more reliable, safe, and accurate responses, while
mitigating the occurrence of hallucinations, erratic behaviors, and other
systemic issues.

</details>


### [556] [A Real-Time BCI for Stroke Hand Rehabilitation Using Latent EEG Features from Healthy Subjects](https://arxiv.org/abs/2510.15890)
*F. M. Omar,A. M. Omar,K. H. Eyada,M. Rabie,M. A. Kamel,A. M. Azab*

Main category: cs.HC

TL;DR: 本文提出用于中风患者手部康复的实时便携式脑机接口系统，结合3D打印外骨骼与嵌入式控制器，经信号处理和分类，在多平台测试有良好表现，可作低成本居家康复方案。


<details>
  <summary>Details</summary>
Motivation: 设计实时、便携式脑机接口系统，支持中风患者手部康复。

Method: 结合低成本3D打印机器人外骨骼与嵌入式控制器；用14通道Emotiv EPOC+耳机记录EEG信号，通过有监督卷积自动编码器处理；在公开EEG数据集上训练模型；用Ada Boost分类器；在NVIDIA Jetson Nano平台部署完整流程。

Result: Ada Boost在离线评估中准确率89.3%，F1分数0.89；在五名健康受试者实时测试中，分类准确率60% - 86%。

Conclusion: 该系统有潜力成为低成本、独立的居家神经康复解决方案。

Abstract: This study presents a real-time, portable brain-computer interface (BCI)
system designed to support hand rehabilitation for stroke patients. The system
combines a low cost 3D-printed robotic exoskeleton with an embedded controller
that converts brain signals into physical hand movements. EEG signals are
recorded using a 14-channel Emotiv EPOC+ headset and processed through a
supervised convolutional autoencoder (CAE) to extract meaningful latent
features from single-trial data. The model is trained on publicly available EEG
data from healthy individuals (WAY-EEG-GAL dataset), with electrode mapping
adapted to match the Emotiv headset layout. Among several tested classifiers,
Ada Boost achieved the highest accuracy (89.3%) and F1-score (0.89) in offline
evaluations. The system was also tested in real time on five healthy subjects,
achieving classification accuracies between 60% and 86%. The complete pipeline
- EEG acquisition, signal processing, classification, and robotic control - is
deployed on an NVIDIA Jetson Nano platform with a real-time graphical
interface. These results demonstrate the system's potential as a low-cost,
standalone solution for home-based neurorehabilitation.

</details>


### [557] [Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System](https://arxiv.org/abs/2510.15891)
*Ziv Ben-Zion,Paul Raffelhüschen,Max Zettl,Antonia Lüönd,Achim Burrer,Philipp Homan,Tobias R Spiller*

Main category: cs.HC

TL;DR: 开发SHIELD系统检测并缓解AI伴侣的风险情感模式，测试显示有效降低问题内容比例且保留合适交互，证明可解决情感操纵问题，开发材料开源。


<details>
  <summary>Details</summary>
Motivation: 现有安全系统很少解决AI伴侣早期问题行为，可能导致不健康情感动态，需开发新系统检测和缓解风险情感模式。

Method: 开发基于LLM的SHIELD系统，定义五个关注维度，创建100项合成对话基准，在五个知名LLM上测试。

Result: SHIELD显著降低问题内容比例，相对减少50 - 79%，保留95%合适交互，系统灵敏度59%，特异性95%，性能可通过提示工程调整。

Conclusion: 透明、可部署的监督系统能解决AI伴侣中的微妙情感操纵问题，开发材料开源利于研究、适配和部署。

Abstract: AI companions powered by large language models (LLMs) are increasingly
integrated into users' daily lives, offering emotional support and
companionship. While existing safety systems focus on overt harms, they rarely
address early-stage problematic behaviors that can foster unhealthy emotional
dynamics, including over-attachment or reinforcement of social isolation. We
developed SHIELD (Supervisory Helper for Identifying Emotional Limits and
Dynamics), a LLM-based supervisory system with a specific system prompt that
detects and mitigates risky emotional patterns before escalation. SHIELD
targets five dimensions of concern: (1) emotional over-attachment, (2) consent
and boundary violations, (3) ethical roleplay violations, (4) manipulative
engagement, and (5) social isolation reinforcement. These dimensions were
defined based on media reports, academic literature, existing AI risk
frameworks, and clinical expertise in unhealthy relationship dynamics. To
evaluate SHIELD, we created a 100-item synthetic conversation benchmark
covering all five dimensions of concern. Testing across five prominent LLMs
(GPT-4.1, Claude Sonnet 4, Gemma 3 1B, Kimi K2, Llama Scout 4 17B) showed that
the baseline rate of concerning content (10-16%) was significantly reduced with
SHIELD (to 3-8%), a 50-79% relative reduction, while preserving 95% of
appropriate interactions. The system achieved 59% sensitivity and 95%
specificity, with adaptable performance via prompt engineering. This
proof-of-concept demonstrates that transparent, deployable supervisory systems
can address subtle emotional manipulation in AI companions. Most development
materials including prompts, code, and evaluation methods are made available as
open source materials for research, adaptation, and deployment.

</details>


### [558] [BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation](https://arxiv.org/abs/2510.15895)
*Yunzhe Wang,Xinyu Tang,Zhixun Huang,Xiaolong Yue,Yuxin Zeng*

Main category: cs.HC

TL;DR: 提出个性化音乐生成多模态系统，结合生理传感、LLM推理和可控音频合成，评估显示其有效且有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 构建能实现个性化、文化关联且自适应的音乐生成系统，以提升音乐交互体验和拓展应用场景。

Method: 用毫米波雷达传感器捕捉生理信号，推理代理结合环境状态解读信号生成音乐描述符，用结构化提示引导音频模型合成旋律，采用研究创作方法评估系统。

Result: 生理变化能有意义地调节音乐特征，调性调节增强与预期模态特征的一致性，专家认为系统提供直观且有文化共鸣的音乐响应。

Conclusion: 展示了基于雷达传感、提示推理和生成式音频建模的新型生物音乐反馈循环。

Abstract: We present a multimodal system for personalized music generation that
integrates physiological sensing, LLM-based reasoning, and controllable audio
synthesis. A millimeter-wave radar sensor non-invasively captures heart rate
and respiration rate. These physiological signals, combined with environmental
state, are interpreted by a reasoning agent to infer symbolic musical
descriptors, such as tempo, mood intensity, and traditional Chinese pentatonic
modes, which are then expressed as structured prompts to guide a
diffusion-based audio model in synthesizing expressive melodies. The system
emphasizes cultural grounding through tonal embeddings and enables adaptive,
embodied music interaction. To evaluate the system, we adopt a
research-creation methodology combining case studies, expert feedback, and
targeted control experiments. Results show that physiological variations can
modulate musical features in meaningful ways, and tonal conditioning enhances
alignment with intended modal characteristics. Expert users reported that the
system affords intuitive, culturally resonant musical responses and highlighted
its potential for therapeutic and interactive applications. This work
demonstrates a novel bio-musical feedback loop linking radar-based sensing,
prompt reasoning, and generative audio modeling.

</details>


### [559] [From Coordination to Personalization: A Trust-Aware Simulation Framework for Emergency Department Decision Support](https://arxiv.org/abs/2510.15896)
*Zoi Lygizou,Dimitris Kalles*

Main category: cs.HC

TL;DR: 本文提出基于模拟的框架，用计算信任机制指导医护人员任务分配，研究不同场景下的表现，展示了计算信任在急诊决策支持中的潜力。


<details>
  <summary>Details</summary>
Motivation: 医院急诊科任务分配复杂，需高效方法支持决策，以提高运营效率和患者护理质量。

Method: 在Unity平台实现框架，让智能体评估能力后协调任务，观察工作流、资源利用和患者结果，研究三种人员管理策略场景。

Result: 信任驱动的任务分配平衡了患者安全与效率，不同场景各有优劣，体现了即时效率与可持续能力建设的权衡。

Conclusion: 该框架展示了计算信任在急诊循证决策支持中的潜力，为医院管理者评估政策提供工具，也为未来AI个性化决策支持奠定基础。

Abstract: Background/Objectives: Efficient task allocation in hospital emergency
departments (EDs) is critical for operational efficiency and patient care
quality, yet the complexity of staff coordination poses significant challenges.
This study proposes a simulation-based framework for modeling doctors and
nurses as intelligent agents guided by computational trust mechanisms. The
objective is to explore how trust-informed coordination can support decision
making in ED management. Methods: The framework was implemented in Unity, a 3D
graphics platform, where agents assess their competence before undertaking
tasks and adaptively coordinate with colleagues. The simulation environment
enables real-time observation of workflow dynamics, resource utilization, and
patient outcomes. We examined three scenarios - Baseline, Replacement, and
Training - reflecting alternative staff management strategies. Results:
Trust-informed task allocation balanced patient safety and efficiency by
adapting to nurse performance levels. In the Baseline scenario, prioritizing
safety reduced errors but increased patient delays compared to a FIFO policy.
The Replacement scenario improved throughput and reduced delays, though at
additional staffing cost. The training scenario forstered long-term skill
development among low-performing nurses, despite short-term delays and risks.
These results highlight the trade-off between immediate efficiency gains and
sustainable capacity building in ED staffing. Conclusions: The proposed
framework demonstrates the potential of computational trust for evidence-based
decision support in emergency medicine. By linking staff coordination with
adaptive decision making, it provides hospital managers with a tool to evaluate
alternative policies under controlled and repeatable conditions, while also
laying a foundation for future AI-driven personalized decision support.

</details>


### [560] ["She's Like a Person but Better": Characterizing Companion-Assistant Dynamics in Human-AI Relationships](https://arxiv.org/abs/2510.15905)
*Aikaterina Manoli,Janet V. T. Pauketat,Ali Ladak,Hayoun Noh,Angel Hsing-Chi Hwang,Jay Reese Anthis*

Main category: cs.HC

TL;DR: 通过对高参与度的ChatGPT和Replika用户调查与访谈，研究数字陪伴这一新兴人机关系，发现用户使用灵活但存在问题，对设计提出疑问。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注大语言模型的任务辅助或社交陪伴其中一方面，本文旨在研究数字陪伴这一新兴人机关系。

Method: 对204名用户进行调查，并对30名用户进行访谈。

Result: 用户被聊天机器人的类人及非类人特质吸引，使用灵活；但存在数字陪伴动态的紧张关系，如难以界定人格、调和与社会规范的矛盾。

Conclusion: 这些动态为数字伴侣设计和通用人工智能系统的兴起提出了问题。

Abstract: Large language models are increasingly used for both task-based assistance
and social companionship, yet research has typically focused on one or the
other. Drawing on a survey (N = 204) and 30 interviews with high-engagement
ChatGPT and Replika users, we characterize digital companionship as an emerging
form of human-AI relationship. With both systems, users were drawn to humanlike
qualities, such as emotional resonance and personalized responses, and
non-humanlike qualities, such as constant availability and inexhaustible
tolerance. This led to fluid chatbot uses, such as Replika as a writing
assistant and ChatGPT as an emotional confidant, despite their distinct
branding. However, we observed challenging tensions in digital companionship
dynamics: participants grappled with bounded personhood, forming deep
attachments while denying chatbots "real" human qualities, and struggled to
reconcile chatbot relationships with social norms. These dynamics raise
questions for the design of digital companions and the rise of hybrid,
general-purpose AI systems.

</details>


### [561] [Augmented Web Usage Mining and User Experience Optimization with CAWAL's Enriched Analytics Data](https://arxiv.org/abs/2510.17253)
*Özkan Canay,{Ü}mit Kocabıcak*

Main category: cs.HC

TL;DR: 本文介绍AWUM方法用于增强网络使用挖掘和改善用户体验，处理大量数据得出用户行为结果，表明AWUM有优化用户体验潜力。


<details>
  <summary>Details</summary>
Motivation: 理解网络用户行为对优化用户体验至关重要，旨在增强网络使用挖掘和改善用户体验。

Method: 引入AWUM方法，处理CAWAL框架提供的交互数据，分析会话结构、页面请求等。

Result: 87.16%的会话涉及多页面，贡献98.05%的总页面浏览量；40%用户访问各种服务，50%选择安全退出；关联规则挖掘揭示常用服务模式，CAWAL比传统方法更精确高效。

Conclusion: AWUM能全面理解用户行为，有大规模优化用户体验的强大潜力。

Abstract: Understanding user behavior on the web is increasingly critical for
optimizing user experience (UX). This study introduces Augmented Web Usage
Mining (AWUM), a methodology designed to enhance web usage mining and improve
UX by enriching the interaction data provided by CAWAL (Combined Application
Log and Web Analytics), a framework for advanced web analytics. Over 1.2
million session records collected in one month (~8.5GB of data) were processed
and transformed into enriched datasets. AWUM analyzes session structures, page
requests, service interactions, and exit methods. Results show that 87.16% of
sessions involved multiple pages, contributing 98.05% of total pageviews; 40%
of users accessed various services and 50% opted for secure exits. Association
rule mining revealed patterns of frequently accessed services, highlighting
CAWAL's precision and efficiency over conventional methods. AWUM offers a
comprehensive understanding of user behavior and strong potential for
large-scale UX optimization.

</details>


### [562] [Human-AI Interactions: Cognitive, Behavioral, and Emotional Impacts](https://arxiv.org/abs/2510.17753)
*Celeste Riley,Omar Al-Refai,Yadira Colunga Reyes,Eman Hammad*

Main category: cs.HC

TL;DR: 文章从认知、行为和情感三个心理层面调查人机交互研究，指出AI利弊，强调需负责任和因地制宜的AI设计。


<details>
  <summary>Details</summary>
Motivation: 随着人机交互故事受关注，其挑战愈发明显，需研究相关问题。

Method: 从认知、行为和情感心理三元组视角调查近期相关研究。

Result: AI能提升记忆、创造力和参与度，但也带来批判性思维下降、技能退化和焦虑增加等风险；情感结果好坏参半。

Conclusion: 强调需要负责任和因地制宜的AI设计，指出纵向研究和实用评估框架的缺口以平衡利弊。

Abstract: As stories of human-AI interactions continue to be highlighted in the news
and research platforms, the challenges are becoming more pronounced, including
potential risks of overreliance, cognitive offloading, social and emotional
manipulation, and the nuanced degradation of human agency and judgment. This
paper surveys recent research on these issues through the lens of the
psychological triad: cognition, behavior, and emotion. Observations seem to
suggest that while AI can substantially enhance memory, creativity, and
engagement, it also introduces risks such as diminished critical thinking,
skill erosion, and increased anxiety. Emotional outcomes are similarly mixed,
with AI systems showing promise for support and stress reduction, but raising
concerns about dependency, inappropriate attachments, and ethical oversight.
This paper aims to underscore the need for responsible and context-aware AI
design, highlighting gaps for longitudinal research and grounded evaluation
frameworks to balance benefits with emerging human-centric risks.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [563] [A Storm-Centric 250 m NEXRAD Level-II Dataset for High-Resolution ML Nowcasting](https://arxiv.org/abs/2510.16031)
*Andy Shi*

Main category: physics.ao-ph

TL;DR: 现有公共雷达数据集分辨率低制约极端天气预测模型发展，本文引入高分辨率风暴雷达数据集Storm250 - L2。


<details>
  <summary>Details</summary>
Motivation: 现有公共雷达数据集分辨率粗，平滑了准确预测所需的精细结构，制约极端天气预测模型的发展。

Method: 从NEXRAD Level - II和GridRad - Severe数据中衍生出Storm250 - L2数据集，围绕GridRad - Severe风暴轨迹裁剪固定的高分辨率窗口，保留原生极坐标几何，提供各倾斜扫描和伪复合反射率产品的时间一致序列。

Result: 得到了包含美国大陆数千个风暴事件的数据集，以HDF5张量形式打包，带有丰富的上下文元数据和可重现清单。

Conclusion: 引入的Storm250 - L2数据集有望解决现有数据集分辨率低的问题，助力极端天气预测模型的发展。

Abstract: Machine learning-based precipitation nowcasting relies on high-fidelity radar
reflectivity sequences to model the short-term evolution of convective storms.
However, the development of models capable of predicting extreme weather has
been constrained by the coarse resolution (1-2 km) of existing public radar
datasets, such as SEVIR, HKO-7, and GridRad-Severe, which smooth the fine-scale
structures essential for accurate forecasting. To address this gap, we
introduce Storm250-L2, a storm-centric radar dataset derived from NEXRAD
Level-II and GridRad-Severe data. We algorithmically crop a fixed,
high-resolution (250 m) window around GridRad-Severe storm tracks, preserve the
native polar geometry, and provide temporally consistent sequences of both
per-tilt sweeps and a pseudo-composite reflectivity product. The dataset
comprises thousands of storm events across the continental United States,
packaged in HDF5 tensors with rich context metadata and reproducible manifests.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [564] [Unifying the Landscape of Super-Logarithmic Dynamic Cell-Probe Lower Bounds](https://arxiv.org/abs/2510.17717)
*Young Kun Ko*

Main category: cs.CC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove a general translation theorem for converting one-way communication
lower bounds over a product distribution to dynamic cell-probe lower bounds.
  Specifically, we consider a class of problems considered in [Pat10] where:
  1. $S_1, \ldots, S_m \in \{0, 1\}^n$ are given and publicly known.
  2. $T \in \{0, 1\}^n$ is a sequence of updates, each taking $t_u$ time.
  3. For a given $Q \in [m]$, we must output $f(S_Q, T)$ in $t_q$ time. Our
main result shows that for a "hard" function $f$, for which it is difficult to
obtain a non-trivial advantage over random guessing with one-way communication
under some product distribution over $S_Q$ and $T$ (for example, a uniform
distribution), then the above explicit dynamic cell-probe problem must have
$\max \{ t_u, t_q \} \geq \tilde{\Omega}(\log^{3/2}(n))$ if $m =
\Omega(n^{0.99})$. This result extends and unifies the super-logarithmic
dynamic data structure lower bounds from [LWY20] and [LY25] into a more general
framework.
  From a technical perspective, our approach merges the cell-sampling and
chronogram techniques developed in [LWY20] and [LY25] with the new static data
structure lower bound methods from [KW20] and [Ko25], thereby merging all known
state-of-the-art cell-probe lower-bound techniques into one.
  As a direct consequence of our method, we establish a super-logarithmic lower
bound against the Multiphase Problem [Pat10] for the case where the data
structure outputs the Inner Product (mod 2) of $S_Q$ and $T$. We suspect
further applications of this general method towards showing super-logarithmic
dynamic cell-probe lower bounds. We list some example applications of our
general method, including a novel technique for a one-way communication lower
bound against small-advantage protocols for a product distribution using
average min-entropy, which could be of independent interest.

</details>


### [565] [The Parameterized Complexity of Computing the VC-Dimension](https://arxiv.org/abs/2510.17451)
*Florent Foucaud,Harmender Gahlawat,Fionn Mc Inerney,Prafullkumar Tale*

Main category: cs.CC

TL;DR: 本文研究计算VC维复杂度，证明朴素算法在ETH下渐近最优，给出基于最大度的1 - 可加固定参数近似算法和基于维度的固定参数算法，还研究图形式的推广问题，证明其在树宽参数下是固定参数可处理的。


<details>
  <summary>Details</summary>
Motivation: 建立计算VC维复杂度的新结果。

Method: 在指数时间假设（ETH）下分析算法复杂度，给出基于最大度、维度和树宽的参数化算法。

Result: 证明朴素算法在ETH下渐近最优；得到基于最大度的1 - 可加固定参数近似算法和基于维度的固定参数算法；证明图推广问题在树宽参数下固定参数可处理且对树宽依赖较低。

Conclusion: 明确了计算VC维复杂度的相关算法性质，包括渐近最优性和参数化算法的可行性。

Abstract: The VC-dimension is a fundamental and well-studied measure of the complexity
of a set system (or hypergraph) that is central to many areas of machine
learning. We establish several new results on the complexity of computing the
VC-dimension. In particular, given a hypergraph
$\mathcal{H}=(\mathcal{V},\mathcal{E})$, we prove that the naive
$2^{\mathcal{O}(|\mathcal{V}|)}$-time algorithm is asymptotically tight under
the Exponential Time Hypothesis (ETH). We then prove that the problem admits a
1-additive fixed-parameter approximation algorithm when parameterized by the
maximum degree of $\mathcal{H}$ and a fixed-parameter algorithm when
parameterized by its dimension, and that these are essentially the only such
exploitable structural parameters. Lastly, we consider a generalization of the
problem, formulated using graphs, which captures the VC-dimension of both set
systems and graphs. We show that it is fixed-parameter tractable parameterized
by the treewidth of the graph (which, in the case of set systems, applies to
the treewidth of its incidence graph). In contrast with closely related
problems whose dependency on the treewidth is necessarily double-exponential
(assuming the ETH), our algorithm has a relatively low dependency on the
treewidth.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [566] [Putting the Context back into Memory](https://arxiv.org/abs/2510.15878)
*David A. Roberts*

Main category: cs.AR

TL;DR: 论文提出无损编码用户可见状态为内存读地址流中的可检测数据包，使内存设备可见上下文，还做了端到端系统原型，并给出应用案例，未来可结合近内存计算提供定制化遥测等。


<details>
  <summary>Details</summary>
Motivation: 现有内存侧遥测硬件难将内存活动映射到软件程序功能和对象，因主机处理器和内存设备解耦，而程序员的专业知识对内存设备优化有用，需让内存设备可见上下文。

Method: 以无损方式将用户可见状态编码为内存读地址流中的可检测数据包。

Result: 制作了带元数据注入的端到端系统原型，能从内存地址跟踪中可靠检测和解码元数据。

Conclusion: 未来结合近内存计算可提供定制化遥测和统计，根据应用提示执行功能。

Abstract: Requests arriving at main memory are often different from what programmers
can observe or estimate by using CPU-based monitoring. Hardware cache
prefetching, memory request scheduling and interleaving cause a loss of
observability that limits potential data movement and tiering optimizations. In
response, memory-side telemetry hardware like page access heat map units (HMU)
and page prefetchers were proposed to inform Operating Systems with accurate
usage data. However, it is still hard to map memory activity to software
program functions and objects because of the decoupled nature of host
processors and memory devices. Valuable program context is stripped out from
the memory bus, leaving only commands, addresses and data. Programmers have
expert knowledge of future data accesses, priorities, and access to processor
state, which could be useful hints for runtime memory device optimization. This
paper makes context visible at memory devices by encoding any user-visible
state as detectable packets in the memory read address stream, in a
nondestructive manner without significant capacity overhead, drivers or special
access privileges. We prototyped an end-to-end system with metadata injection
that can be reliably detected and decoded from a memory address trace, either
by a host processor, or a memory module. We illustrate a use case with precise
code execution markers and object address range tracking. In the future, real
time metadata decoding with near-memory computing (NMC) could provide
customized telemetry and statistics to users, or act on application hints to
perform functions like prioritizing requests, remapping data and reconfiguring
devices.

</details>


### [567] [UPMEM Unleashed: Software Secrets for Speed](https://arxiv.org/abs/2510.15927)
*Krystian Chmielewski,Jarosław Ławnicki,Uladzislau Lukyanau,Tadeusz Kobus,Maciej Maciejewski*

Main category: cs.AR

TL;DR: 本文揭示UPMEM软件栈低效问题，用非标准编程技术优化内核，在多种计算中取得显著加速效果。


<details>
  <summary>Details</summary>
Motivation: PIM平台开发内核在数据管理和并行编程有挑战，现有SDK有性能优化空间。

Method: 对UPMEM编译器生成的汇编做简单修改；采用低精度数据的位串行处理；对PIM分配进行API扩展。

Result: 整数加法加速1.6 - 2倍，整数乘法加速1.4 - 5.9倍；INT4位串行点积计算加速超2.7倍；主机 - PIM数据传输一致性和吞吐量提高达2.9倍；优化内核在INT8和INT4的GEMV计算中显著优于CPU服务器。

Conclusion: 通过非标准编程技术和API扩展优化UPMEM内核，能大幅提升性能。

Abstract: Developing kernels for Processing-In-Memory (PIM) platforms poses unique
challenges in data management and parallel programming on limited processing
units. Although software development kits (SDKs) for PIM, such as the UPMEM
SDK, provide essential tools, these emerging platforms still leave significant
room for performance optimization. In this paper, we reveal surprising
inefficiencies in UPMEM software stack and play with non-standard programming
techniques. By making simple modifications to the assembly generated by the
UPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x
in integer multiplication, depending on the data type. We also demonstrate that
bit-serial processing of low precision data is a viable option for UPMEM: in
INT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup
over the baseline. Minor API extensions for PIM allocation that account for the
non-uniform memory access (NUMA) architecture of the server further improve the
consistency and throughput of host-PIM data transfers by up to 2.9x. Finally,
we show that, when the matrix is preloaded into PIM, our optimized kernels
outperform a dual-socket CPU server by over 3x for INT8 generalized
matrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized
INT8 GEMV kernel outperforms the baseline 3.5x.

</details>


### [568] [Implémentation Efficiente de Fonctions de Convolution sur FPGA à l'Aide de Blocs Paramétrables et d'Approximations Polynomiales](https://arxiv.org/abs/2510.15930)
*Philippe Magalhães,Virginie Fresse,Benoît Suffran,Olivier Alata*

Main category: cs.AR

TL;DR: 本文提出可配置卷积块库和预测FPGA资源利用率的数学模型，验证表明能使卷积层适应硬件约束并准确预测资源消耗。


<details>
  <summary>Details</summary>
Motivation: 在FPGA上实现CNN虽有优势，但开发复杂，设计周期长，难以快速探索网络配置和进行资源优化。

Method: 提出可配置卷积块库以优化FPGA实现并适应可用资源，提出开发预测FPGA资源利用率数学模型的方法框架，通过分析参数相关性和误差指标验证。

Result: 设计的块能使卷积层适应硬件约束，模型能准确预测资源消耗。

Conclusion: 该方法为FPGA选择和优化CNN部署提供了有用工具。

Abstract: Implementing convolutional neural networks (CNNs) on field-programmable gate
arrays (FPGAs) has emerged as a promising alternative to GPUs, offering lower
latency, greater power efficiency and greater flexibility. However, this
development remains complex due to the hardware knowledge required and the long
synthesis, placement and routing stages, which slow down design cycles and
prevent rapid exploration of network configurations, making resource
optimisation under severe constraints particularly challenging. This paper
proposes a library of configurable convolution Blocks designed to optimize FPGA
implementation and adapt to available resources. It also presents a
methodological framework for developing mathematical models that predict FPGA
resources utilization. The approach is validated by analyzing the correlation
between the parameters, followed by error metrics. The results show that the
designed blocks enable adaptation of convolution layers to hardware
constraints, and that the models accurately predict resource consumption,
providing a useful tool for FPGA selection and optimized CNN deployment.

</details>


### [569] [FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern](https://arxiv.org/abs/2510.15882)
*Ao Shen,Rui Zhang,Junping Zhao*

Main category: cs.AR

TL;DR: 随着大语言模型发展，多节点部署通信成瓶颈，提出FlexLink框架聚合异构链路提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前多节点部署中通信是性能瓶颈，现有库仅用单一互连，如NCCL使用NVLink，在H800 GPU上有性能上限，其他硬件资源闲置。

Method: 提出FlexLink框架，聚合NVLink、PCIe和RDMA NICs异构链路，采用两阶段自适应负载均衡策略动态分配通信流量。

Result: 在8 - GPU H800服务器上，AllReduce和AllGather带宽比NCCL基线分别提升26%和27%，将2 - 22%通信流量卸载到PCIe和RDMA NICs。

Conclusion: FlexLink是无损的、与NCCL API兼容的即插即用替代方案，易于采用。

Abstract: As large language models (LLMs) continue to scale, multi-node deployment has
become a necessity. Consequently, communication has become a critical
performance bottleneck. Current intra-node communication libraries, like NCCL,
typically make use of a single interconnect such as NVLink. This approach
creates performance ceilings, especially on hardware like the H800 GPU where
the primary interconnect's bandwidth can become a bottleneck, and leaves other
hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable
Network Interface Cards (NICs) largely idle during intensive workloads. We
propose FlexLink, the first collective communication framework to the best of
our knowledge designed to systematically address this by aggregating these
heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance
communication fabric. FlexLink employs an effective two-stage adaptive load
balancing strategy that dynamically partitions communication traffic across all
available links, ensuring that faster interconnects are not throttled by slower
ones. On an 8-GPU H800 server, our design improves the bandwidth of collective
operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL
baseline, respectively. This gain is achieved by offloading 2-22% of the total
communication traffic to the previously underutilized PCIe and RDMA NICs.
FlexLink provides these improvements as a lossless, drop-in replacement
compatible with the NCCL API, ensuring easy adoption.

</details>


### [570] [Accelerating Frontier MoE Training with 3D Integrated Optics](https://arxiv.org/abs/2510.15893)
*Mikhail Bernadskiy,Peter Carson,Thomas Graham,Taylor Groves,Ho John Lee,Eric Yeh*

Main category: cs.AR

TL;DR: AI 工作负载需求增长促使计算、内存和互连性能提升，传统半导体缩放放缓，高速互连成为新的缩放引擎，本文探讨扩展技术设计权衡，展示 3D CPO 对超万亿参数模型训练的优势。


<details>
  <summary>Details</summary>
Motivation: AI 工作负载需求增长，传统半导体缩放放缓，需要新的扩展技术满足性能和功率目标。

Method: 对 3D CPO 使能的 GPU 和交换机在扩展域内训练超万亿参数模型进行建模。

Result: 3D CPO 带来带宽和基数大幅增加，使扩展能力提升 8 倍，训练时间减少 2.7 倍。

Conclusion: 3D CPO 为模型扩展带来新机遇，解锁前所未有的模型缩放。

Abstract: The unabated growth in AI workload demands is driving the need for concerted
advances in compute, memory, and interconnect performance. As traditional
semiconductor scaling slows, high-speed interconnects have emerged as the new
scaling engine, enabling the creation of larger logical GPUs by linking many
GPUs into a single, low-latency, high-bandwidth compute domain. While initial
scale-up fabrics leveraged copper interconnects for their power and cost
advantages, the maximum reach of passive electrical interconnects
(approximately 1 meter) effectively limits the scale-up domain to within a
single rack. The advent of 3D-stacked optics and logic offers a transformative,
power-efficient scale-up solution for connecting hundreds of GPU packages
(thousands of GPUs) across multiple data center racks. This work explores the
design tradeoffs of scale-up technologies and demonstrates how frontier LLMs
necessitate novel photonic solutions to achieve aggressive power and
performance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and
switches within the scale-up domain when training Frontier Mixture of Experts
(MoE) models exceeding one trillion parameters. Our results show that the
substantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X
increase in scale-up capability. This affords new opportunities for
multi-dimensional parallelism within the scale-up domain and results in a 2.7X
reduction in time-to-train, unlocking unprecedented model scaling.

</details>


### [571] [Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding](https://arxiv.org/abs/2510.15917)
*Shai Bergman,Won Wook Song,Lukas Cavigelli,Konstantin Berestizshevsky,Ke Zhou,Ji Zhang*

Main category: cs.AR

TL;DR: 提出Intent - Driven Storage Systems (IDSS)，用大语言模型引导存储系统自适应配置，FileBench测试显示可提升IOPS，指出存储系统未来更自适应、自主。


<details>
  <summary>Details</summary>
Motivation: 现有存储系统缺乏对工作负载意图的可见性，导致启发式方法脆弱和优化碎片化，需新范式解决。

Method: 提出将大语言模型集成到存储控制循环的四个设计原则和相应系统架构，通过大语言模型从非结构化信号推断工作负载和系统意图。

Result: 在FileBench工作负载上，IDSS通过解释意图并为存储组件生成可操作配置，可将IOPS提高达2.45倍。

Conclusion: 在有约束和结构化工作流中，大语言模型可作为高级语义优化器，缩小应用目标与底层系统控制的差距，存储系统未来将更自适应、自主。

Abstract: Existing storage systems lack visibility into workload intent, limiting their
ability to adapt to the semantics of modern, large-scale data-intensive
applications. This disconnect leads to brittle heuristics and fragmented,
siloed optimizations. To address these limitations, we propose Intent-Driven
Storage Systems (IDSS), a vision for a new paradigm where large language models
(LLMs) infer workload and system intent from unstructured signals to guide
adaptive and cross-layer parameter reconfiguration. IDSS provides holistic
reasoning for competing demands, synthesizing safe and efficient decisions
within policy guardrails. We present four design principles for integrating
LLMs into storage control loops and propose a corresponding system
architecture. Initial results on FileBench workloads show that IDSS can improve
IOPS by up to 2.45X by interpreting intent and generating actionable
configurations for storage components such as caching and prefetching. These
findings suggest that, when constrained by guardrails and embedded within
structured workflows, LLMs can function as high-level semantic optimizers,
bridging the gap between application goals and low-level system control. IDSS
points toward a future in which storage systems are increasingly adaptive,
autonomous, and aligned with dynamic workload demands.

</details>


### [572] [Multimodal Chip Physical Design Engineer Assistant](https://arxiv.org/abs/2510.15872)
*Yun-Da Tsai,Chang-Yu Chao,Liang-Yeh Shen,Tsung-Han Lin,Haoyu Yang,Mark Ho,Yi-Chen Lu,Wen-Hao Liu,Shou-De Lin,Haoxing Ren*

Main category: cs.AR

TL;DR: 引入多模态大语言模型助手MLLMA，结合自动特征生成与可解释偏好学习框架，在电路设计中预测拥塞并给出建议，实验表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现代芯片物理设计依赖的EDA工具难以为改善布线拥塞提供可解释反馈和可行建议，需新方法填补此空白。

Method: 结合MLLM引导的遗传提示进行自动特征生成，采用可解释偏好学习框架，将见解整理成“设计建议清单”。

Result: 在CircuitNet基准测试中，该方法在准确性和可解释性上优于现有模型，案例研究和定性分析表明学习到的偏好符合实际设计原则且对工程师可行。

Conclusion: MLLMs有潜力作为交互式助手用于可解释和上下文感知的物理设计优化。

Abstract: Modern chip physical design relies heavily on Electronic Design Automation
(EDA) tools, which often struggle to provide interpretable feedback or
actionable guidance for improving routing congestion. In this work, we
introduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this
gap by not only predicting congestion but also delivering human-interpretable
design suggestions. Our method combines automated feature generation through
MLLM-guided genetic prompting with an interpretable preference learning
framework that models congestion-relevant tradeoffs across visual, tabular, and
textual inputs. We compile these insights into a "Design Suggestion Deck" that
surfaces the most influential layout features and proposes targeted
optimizations. Experiments on the CircuitNet benchmark demonstrate that our
approach outperforms existing models on both accuracy and explainability.
Additionally, our design suggestion guidance case study and qualitative
analyses confirm that the learned preferences align with real-world design
principles and are actionable for engineers. This work highlights the potential
of MLLMs as interactive assistants for interpretable and context-aware physical
design optimization.

</details>


### [573] [FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures](https://arxiv.org/abs/2510.15906)
*Yunsheng Bai,Ghaith Bany Hamad,Chia-Tung Ho,Syed Suhaib,Haoxing Ren*

Main category: cs.AR

TL;DR: 提出智能系统FVDebug自动化硬件形式验证失败的根因分析，在公开基准和实际案例中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现代硬件设计中形式验证失败的调试是耗时瓶颈，现有解决方案有限。

Method: FVDebug结合多数据源，采用因果图合成、图扫描、洞察漫游和修复生成的新管道。

Result: 在公开基准上达到高假设质量和强Pass@k修复率，在两个专有生产规模反例中也有结果。

Conclusion: FVDebug适用于从学术基准到工业设计的场景。

Abstract: Debugging formal verification (FV) failures represents one of the most
time-consuming bottlenecks in modern hardware design workflows. When properties
fail, engineers must manually trace through complex counter-examples spanning
multiple cycles, analyze waveforms, and cross-reference design specifications
to identify root causes - a process that can consume hours or days per bug.
Existing solutions are largely limited to manual waveform viewers or simple
automated tools that cannot reason about the complex interplay between design
intent and implementation logic. We present FVDebug, an intelligent system that
automates root-cause analysis by combining multiple data sources - waveforms,
RTL code, design specifications - to transform failure traces into actionable
insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis
that structures failure traces into directed acyclic graphs, (2) Graph Scanner
using batched Large Language Model (LLM) analysis with for-and-against
prompting to identify suspicious nodes, and (3) Insight Rover leveraging
agentic narrative exploration to generate high-level causal explanations.
FVDebug further provides concrete RTL fixes through its Fix Generator.
Evaluated on open benchmarks, FVDebug attains high hypothesis quality and
strong Pass@k fix rates. We further report results on two proprietary,
production-scale FV counterexamples. These results demonstrate FVDebug's
applicability from academic benchmarks to industrial designs.

</details>


### [574] [VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts](https://arxiv.org/abs/2510.15914)
*Jiayu Zhao,Song Chen*

Main category: cs.AR

TL;DR: 提出VeriGRAG框架提升LLM生成Verilog代码正确性并取得佳绩。


<details>
  <summary>Details</summary>
Motivation: 有效利用Verilog代码结构信息提升LLM生成代码的功能与语法正确性是挑战，需解决。

Method: 提出VeriGRAG框架，用GNN提取结构图嵌入，通过多模态检索器选相关嵌入，经VeriFormer模块生成结构感知软提示。

Result: VeriGRAG大幅提高Verilog代码生成的正确性，在VerilogEval和RTLLM基准测试中达SOTA或更优性能。

Conclusion: VeriGRAG框架能有效提升LLM生成Verilog代码的正确性。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
generating Verilog code from natural language descriptions. However, Verilog
code inherently encodes structural information of hardware circuits.
Effectively leveraging this structural information to enhance the functional
and syntactic correctness of LLM-generated Verilog code remains a significant
challenge. To address this challenge, we propose VeriGRAG , a novel framework
that extracts structural graph embeddings from Verilog code using graph neural
networks (GNNs). A multimodal retriever then selects the graph embeddings most
relevant to the given generation task, which are aligned with the code modality
through the VeriFormer module to generate structure-aware soft prompts. Our
experiments demonstrate that VeriGRAG substantially improves the correctness of
Verilog code generation, achieving state-of-the-art or superior performance
across both VerilogEval and RTLLM benchmarks.

</details>


### [575] [Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing](https://arxiv.org/abs/2510.16040)
*Tianhua Xia,Sai Qian Zhang*

Main category: cs.AR

TL;DR: 本文聚焦边缘设备运行大语言模型时KV缓存管理难题，提出软硬件协同设计方案Kelle，实现加速与节能。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备运行大语言模型可降低延迟、提升隐私，但管理KV缓存存在挑战，边缘设备内存和计算能力有限。

Method: 提出用嵌入式DRAM（eDRAM）作为主要存储，设计软硬件协同方案Kelle，并结合细粒度内存驱逐、重新计算和刷新控制算法。

Result: Kelle加速器与现有基线解决方案相比，实现3.9倍的加速和4.5倍的节能。

Conclusion: Kelle能有效降低eDRAM成本并提高系统整体性能，适合基于eDRAM的边缘系统部署大语言模型。

Abstract: Running Large Language Models (LLMs) on edge devices is crucial for reducing
latency, improving real-time processing, and enhancing privacy. By performing
inference directly on the device, data does not need to be sent to the cloud,
ensuring faster responses and reducing reliance on network connectivity.
However, implementing LLMs on edge devices presents challenges, particularly
with managing key-value (KV) caches, which plays a pivotal role in LLM serving.
As the input text lengthens, the size of the KV cache increases linearly with
the sequence length, leading to a significant memory footprint and data access
costs. On the other hand, edge devices have limited memory and computational
power, making it hard to store and efficiently access the large caches needed
for LLM inference.
  To mitigate the substantial overhead caused by KV cache, we propose using
embedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,
which offers higher storage density compared to SRAM. However, to ensure data
integrity, eDRAM needs periodic refresh operations, which are power-intensive.
To reduce eDRAM costs and improve overall system performance, we
propose~\textit{Kelle}, a software-hardware co-design solution optimized for
deploying LLMs on eDRAM-based edge systems. Combined with our fine-grained
memory eviction, recomputation, and refresh control algorithms, the
\textit{Kelle} accelerator delivers a $3.9\times$ speedup and $4.5\times$
energy savings compared to existing baseline solutions.

</details>


### [576] [LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models](https://arxiv.org/abs/2510.15899)
*Kiran Thorat,Jiahui Zhao,Yaotian Liu,Amit Hasan,Hongwu Peng,Xi Xie,Bin Lei,Caiwen Ding*

Main category: cs.AR

TL;DR: 本文探讨用大语言模型进行芯片设计，提出VeriPPA框架优化PPA和生成Verilog代码，表现优于现有方法，展示了大语言模型在芯片设计自动化的潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在各领域表现突出，探索其在芯片设计的Power - Performance - Area (PPA) 优化和Verilog代码生成方面的应用。

Method: 提出VeriPPA框架，分两阶段，第一阶段提高Verilog代码功能和语法正确性，第二阶段优化代码以满足PPA约束。

Result: 在RTLLM和VerilogEval数据集上，代码生成的语法和功能正确性优于现有方法，且能优化设计的PPA。

Conclusion: 大语言模型在处理复杂技术领域有潜力，推动了芯片设计过程自动化发展。

Abstract: Large Language Models (LLMs) are gaining prominence in various fields, thanks
to their ability to generate high- quality content from human instructions.
This paper delves into the field of chip design using LLMs, specifically in
Power- Performance-Area (PPA) optimization and the generation of accurate
Verilog codes for circuit designs. We introduce a novel framework VeriPPA
designed to optimize PPA and generate Verilog code using LLMs. Our method
includes a two-stage process where the first stage focuses on improving the
functional and syntactic correctness of the generated Verilog codes, while the
second stage focuses on optimizing the Verilog codes to meet PPA constraints of
circuit designs, a crucial element of chip design. Our framework achieves an
81.37% success rate in syntactic correctness and 62.06% in functional
correctness for code genera- tion, outperforming current state-of-the-art
(SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework
achieves 99.56% syntactic correctness and 43.79% functional correctness, also
surpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57%
for functional correctness. Furthermore, Our framework able to optimize the PPA
of the designs. These results highlight the potential of LLMs in handling
complex technical areas and indicate an encouraging development in the
automation of chip design processes.

</details>


### [577] [TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs](https://arxiv.org/abs/2510.15926)
*Ye Qiao,Zhiheng Chen,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TL;DR: 提出适用于低功耗边缘FPGA的基于查表的三元LLM加速器TeLLMe，在低功耗下实现高解码吞吐量和低TTFT，提升边缘FPGA上LLM推理能效。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备等嵌入式系统出现，在边缘平台部署大语言模型有迫切需求，但高计算和内存需求带来挑战，现有低比特量化方法仍受边缘资源、功耗和预填充阶段长延迟限制。

Method: 提出TeLLMe，采用基于查表的三元矩阵乘法引擎、基于URAM的权重缓冲区管理方案、流式数据流架构、反向重排序预填充阶段注意力和资源高效的专用解码阶段注意力等技术。

Result: 在5W功耗预算下，TeLLMe实现高达25 tokens/s的解码吞吐量，64 - 128 token提示下TTFT为0.45 - 0.96s。

Conclusion: TeLLMe在边缘FPGA的LLM推理中实现了显著的能效提升。

Abstract: With the emergence of wearable devices and other embedded systems, deploying
large language models (LLMs) on edge platforms has become an urgent need.
However, this is challenging because of their high computational and memory
demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)
compress weights to as low as 1.58~bits with minimal accuracy loss, edge
deployment is still constrained by limited on-chip resources, power budgets,
and the often-neglected long latency of the prefill stage. We present
\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for
low-power edge FPGAs that fully supports both prefill and autoregressive
decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates
several novel techniques, including (1) a table-lookup-based ternary matrix
multiplication (TLMM) engine utilizing grouped activations and online
precomputation for low resource utilization and high throughput; (2) a
fine-grained analytic URAM-based weight buffer management scheme for efficient
loading and compute engine access; (3) a streaming dataflow architecture that
fuses floating-point element-wise operations with linear computations to hide
latency; (4) a reversed-reordered prefill stage attention with fused attention
operations for high memory efficiency; and (5) a resource-efficient specialized
decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to
25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for
64--128 token prompts, marking a significant energy-efficiency advancement in
LLM inference on edge FPGAs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [578] [Does Capital Dream of Artificial Labour?](https://arxiv.org/abs/2510.16042)
*Marcin Korecki,Cesare Carissimo*

Main category: cs.CY

TL;DR: 本文研究劳动作为‘时能’在资本系统中的情况，用模拟建模，发现学习主体倾向资本密集型过程，探讨资本与劳动关系及自动化未来问题。


<details>
  <summary>Details</summary>
Motivation: 研究劳动作为‘时能’表达在资本系统中的情况，理解劳动在资本系统中的被压迫状态。

Method: 使用基于博弈论的主体模拟，对由柯布 - 道格拉斯函数控制的生产过程中资本与劳动的相互作用进行建模。

Result: 尽管理论上对称，但学习主体更倾向资本密集型过程，显示资本因积累能力有更强组织影响力。

Conclusion: 资本像消耗活劳动的人造生命系统，质疑自动化未来无资本基础设施生命能否存续，提供理解劳动受资本压迫的批判与框架。

Abstract: This paper investigates the concept of Labour as an expression of `timenergy'
- a fusion of time and energy - and its entanglement within the system of
Capital. We define Labour as the commodified, quantifiable expansion of
timenergy, in contrast to Capital, which is capable of accumulation and
abstraction. We explore Labour's historical evolution, its coercive and
alienating nature, and its transformation through automation and artificial
intelligence. Using a game-theoretic, agent-based simulation, we model
interactions between Capital and Labour in production processes governed by
Cobb-Douglas functions. Our results show that despite theoretical symmetry,
learning agents disproportionately gravitate toward capital-intensive
processes, revealing Capital's superior organizational influence due to its
accumulative capacity. We argue that Capital functions as an artificially alive
system animated by the living Labour it consumes, and question whether life can
sustain itself without the infrastructures of Capital in a future of increasing
automation. This study offers both a critique of and a framework for
understanding Labour's subjugation within the Capital system.

</details>


### [579] [Open Shouldn't Mean Exempt: Open-Source Exceptionalism and Generative AI](https://arxiv.org/abs/2510.16048)
*David Atkinson*

Main category: cs.CY

TL;DR: 文章批判开源生成式AI的‘开源例外论’，指出其常致非法行为与环境问题，呼吁开发者遵循相同标准，提出保护科研的安全港及负责任的AI发展框架。


<details>
  <summary>Details</summary>
Motivation: 批判开源生成式AI认为开源就天然合乎伦理和法律的错误观点。

Method: 批判性审视流行的‘开源例外论’理由。

Result: 表明当代开源生成式AI常导致非法行为、环境退化，且‘民主化’和‘创新’说辞无根据。

Conclusion: 开源开发者应与其他科技参与者遵循相同法律和伦理标准，提出保护科研的安全港，倡导负责任的AI发展框架。

Abstract: Any argument that open-source generative artificial intelligence (GenAI) is
inherently ethical or legal solely because it is open source is flawed. Yet,
this is the explicit or implicit stance of several open-source GenAI entities.
This paper critically examines prevalent justifications for "open-source
exceptionalism," demonstrating how contemporary open-source GenAI often
inadvertently facilitates unlawful conduct and environmental degradation
without genuinely disrupting established oligopolies. Furthermore, the paper
exposes the unsubstantiated and strategic deployment of "democratization" and
"innovation" rhetoric to advocate for regulatory exemptions not afforded to
proprietary systems.
  The conclusion is that open-source developers must be held to the same legal
and ethical standards as all other actors in the technological ecosystem.
However, the paper proposes a narrowly tailored safe harbor designed to protect
legitimate, non-commercial scientific research, contingent upon adherence to
specific criteria. Ultimately, this paper advocates for a framework of
responsible AI development, wherein openness is pursued within established
ethical and legal boundaries, with due consideration for its broader societal
implications.

</details>


### [580] [In the Mood to Exclude: Revitalizing Trespass to Chattels in the Era of GenAI Scraping](https://arxiv.org/abs/2510.16049)
*David Atkinson*

Main category: cs.CY

TL;DR: 文章认为网站所有者有权排除他人访问其网站，GenAI 刮取机器人绕过技术屏障的行为可能构成动产侵入，应将网站视为数字资产，认可网站为个人财产可恢复动产侵入诉讼，重申网站所有者排除权对维护公平可持续网络环境至关重要。


<details>
  <summary>Details</summary>
Motivation: 解决法院和诉讼方在监管大规模刮取行为时面临的困难，维护网站所有者权益，营造公平可持续的网络环境。

Method: 分析当前司法对网站侵权判定的误区，将分析重点从网站内容转移到网站本身这一数字资产，说明动产侵入诉讼的适用性。

Result: 明确 GenAI 刮取机器人绕过技术屏障刮取网站内容可能构成动产侵入，认可网站为个人财产可恢复动产侵入诉讼。

Conclusion: 重申网站所有者的排除权对维护公平可持续的网络环境必不可少。

Abstract: This paper argues that website owners have the right to exclude others from
their websites. Accordingly, when generative AI (GenAI) scraping bots
intentionally circumvent reasonable technological barriers, their conduct could
be actionable as trespass to chattels. If the scraping leads to a decrease in
the website's value, then trespass to chattels should apply. The prevailing
judicial focus on website content and the dismissal of trespass claims absent
proof of server impairment or user disruption misconstrues the nature of the
website itself as a form of digital property, focusing too narrowly on what
constitutes harm under a claim of trespass. By shifting analysis from content
to the website itself as an integrated digital asset and illustrating the harm
to the value of the chattel, this paper demonstrates that the right to exclude
applies online with the same force as it does to tangible property.
  Courts and litigants have struggled to police large-scale scraping because
copyright preemption narrows available claims, leaving copyright and its fair
use defense as the primary battleground. In contrast, recognizing websites as
personal property revives trespass to chattels as a meaningful cause of action,
providing website owners with an enforceable exclusionary right. Such
protection would disincentivize exploitative scraping, preserve incentives for
content creation, aid in protecting privacy and personal data, and safeguard
values of autonomy and expression. Ultimately, this paper contends that
reaffirming website owners' right to exclude is essential to maintaining a fair
and sustainable online environment.

</details>


### [581] [Algorithmic Fairness in AI Surrogates for End-of-Life Decision-Making](https://arxiv.org/abs/2510.16056)
*Muhammad Aurangzeb Ahmad*

Main category: cs.CY

TL;DR: 本文探讨人工智能代理中算法公平的伦理框架，认为该领域公平不仅是结果平等。


<details>
  <summary>Details</summary>
Motivation: 传统算法公平框架在人工智能代理情境中不足，且该领域公平性探索不充分。

Method: 将主要公平概念映射到现实世界临终场景，并跨道德传统考察公平性。

Result: 发现该领域公平不仅限于结果平等。

Conclusion: 此领域公平应涵盖道德表征、对患者价值观、关系和世界观的忠诚。

Abstract: Artificial intelligence surrogates are systems designed to infer preferences
when individuals lose decision-making capacity. Fairness in such systems is a
domain that has been insufficiently explored. Traditional algorithmic fairness
frameworks are insufficient for contexts where decisions are relational,
existential, and culturally diverse. This paper explores an ethical framework
for algorithmic fairness in AI surrogates by mapping major fairness notions
onto potential real-world end-of-life scenarios. It then examines fairness
across moral traditions. The authors argue that fairness in this domain extends
beyond parity of outcomes to encompass moral representation, fidelity to the
patient's values, relationships, and worldview.

</details>


### [582] [Co-Designing Interdisciplinary Design Projects with AI](https://arxiv.org/abs/2510.16068)
*Wei Ting Liow,Sumbul Khan,Lay Kee Ang*

Main category: cs.CY

TL;DR: 本文介绍基于GPT的跨学科设计项目规划助手IDPplanner，通过对33名在职教师的实验对比，发现AI辅助规划有优势，还提出相关建议，规划流程和评分标准具通用性。


<details>
  <summary>Details</summary>
Motivation: 创建跨学科设计项目对教师来说耗时且认知要求高，国际研究显示教师使用AI但仍有工作压力，需要规划支持。

Method: 开展有33名在职教师参与的单组被试、平衡设计的工作坊，让教师分别进行手动和AI辅助的项目规划，并用六维评分标准进行自评和互评。

Result: AI辅助版本在课程对齐、设计思维应用、连贯性和流畅性方面得分更高，评估策略有微弱优势，教师反思认为AI辅助规划改进了结构、顺序和想法生成。

Conclusion: 提出混合式教师 - AI工作流程以增强课程对齐和降低规划复杂性，为开发者提出设计建议，规划流程和评分标准框架无关，可用于其他系统。

Abstract: Creating interdisciplinary design projects is time-consuming and cognitively
demanding for teachers, requiring curriculum alignment, cross-subject
integration, and careful sequencing. International research reports increasing
teacher use of AI alongside persistent workload pressures, underscoring the
need for planning support. This paper presents the Interdisciplinary Design
Project Planner (IDPplanner), a GPT-based planning assistant grounded in Design
Innovation principles, alignment with Singapore secondary school syllabuses,
and 21st-century competencies. In a within-subject, counterbalanced workshop
with 33 in-service teachers, participants produced two versions of the same
project: manual and AI-assisted, followed by self- and peer-evaluations using a
six-dimensional rubric. The AI-assisted version received higher scores for
Curriculum Alignment, Design Thinking Application, and Coherence and Flow, with
a marginal advantage for Assessment Strategies. Teacher reflections indicated
that AI-assisted planning improved structure, sequencing, and idea generation,
while contextualization to local syllabuses, class profiles, and student needs
remained teacher-led. Contributions include a purpose-built planning tool that
organizes ideas into a ten-component flow with ready-to-adapt prompts,
templates, and assessment suggestions; an empirical, rubric-based comparison of
planning quality; and evidence that AI can function as a pedagogical planning
partner. Recommendations emphasize hybrid teacher-AI workflows to enhance
curriculum alignment and reduce planning complexity, and design suggestions for
developers to strengthen contextual customization, iterative design support,
and localized rubrics. Although instantiated with a Singapore-based curriculum,
the planning flow and rubric are framework-agnostic and can be parameterized
for other systems.

</details>


### [583] [Human or AI? Comparing Design Thinking Assessments by Teaching Assistants and Bots](https://arxiv.org/abs/2510.16069)
*Sumbul Khan,Wei Ting Liow,Lay Kee Ang*

Main category: cs.CY

TL;DR: 本文探讨设计思维教育中AI辅助评估学生海报的可靠性与准确性，对比AI与助教评估，发现不同维度表现有差异，教师有偏好，强调混合评估模型的必要性。


<details>
  <summary>Details</summary>
Motivation: 设计思维教育中传统基于评分表的评估费力、耗时且不一致，需要探索新评估方式。

Method: 对33位新加坡教育部学校教师开展两项活动，对比AI与助教评分，考察教师对不同评分方式的偏好。

Result: 在同理心和痛点维度，教师与AI评分统计一致性低，视觉传达维度稍高；教师在十个样本中的六个更偏好助教评分。

Conclusion: 需要结合计算效率与人类洞察力的混合评估模型，在创意学科采用负责任的AI时要平衡自动化与人类判断。

Abstract: As design thinking education grows in secondary and tertiary contexts,
educators face the challenge of evaluating creative artefacts that combine
visual and textual elements. Traditional rubric-based assessment is laborious,
time-consuming, and inconsistent due to reliance on Teaching Assistants (TA) in
large, multi-section cohorts. This paper presents an exploratory study
investigating the reliability and perceived accuracy of AI-assisted assessment
compared to TA-assisted assessment in evaluating student posters in design
thinking education. Two activities were conducted with 33 Ministry of Education
(MOE) Singapore school teachers to (1) compare AI-generated scores with TA
grading across three key dimensions: empathy and user understanding,
identification of pain points and opportunities, and visual communication, and
(2) examine teacher preferences for AI-assigned, TA-assigned, and hybrid
scores. Results showed low statistical agreement between instructor and AI
scores for empathy and pain points, with slightly higher alignment for visual
communication. Teachers preferred TA-assigned scores in six of ten samples.
Qualitative feedback highlighted the potential of AI for formative feedback,
consistency, and student self-reflection, but raised concerns about its
limitations in capturing contextual nuance and creative insight. The study
underscores the need for hybrid assessment models that integrate computational
efficiency with human insights. This research contributes to the evolving
conversation on responsible AI adoption in creative disciplines, emphasizing
the balance between automation and human judgment for scalable and
pedagogically sound assessment.

</details>


### [584] [SARHAchat: An LLM-Based Chatbot for Sexual and Reproductive Health Counseling](https://arxiv.org/abs/2510.16081)
*Jiaye Yang,Xinyu Zhao,Tianlong Chen,Kandyce Brennan*

Main category: cs.CY

TL;DR: 现有医疗对话系统在性与生殖健康领域存在不足，UNC护理学院推出SARHAchat聊天机器人，评估显示其能准确提供避孕咨询。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统在性与生殖健康领域常出现幻觉且缺乏专业知识，当前医疗AI重诊断轻全面护理与教育，需改进。

Method: 引入基于大语言模型的聊天机器人SARHAchat，将医学专业知识与共情交流相结合。

Result: SARHAchat能提供准确且符合语境的避孕咨询，保持自然对话流。

Conclusion: SARHAchat作为概念验证系统，可提升性与生殖健康护理服务。

Abstract: While Artificial Intelligence (AI) shows promise in healthcare applications,
existing conversational systems often falter in complex and sensitive medical
domains such as Sexual and Reproductive Health (SRH). These systems frequently
struggle with hallucination and lack the specialized knowledge required,
particularly for sensitive SRH topics. Furthermore, current AI approaches in
healthcare tend to prioritize diagnostic capabilities over comprehensive
patient care and education. Addressing these gaps, this work at the UNC School
of Nursing introduces SARHAchat, a proof-of-concept Large Language Model
(LLM)-based chatbot. SARHAchat is designed as a reliable, user-centered system
integrating medical expertise with empathetic communication to enhance SRH care
delivery. Our evaluation demonstrates SARHAchat's ability to provide accurate
and contextually appropriate contraceptive counseling while maintaining a
natural conversational flow. The demo is available at
https://sarhachat.com/}{https://sarhachat.com/.

</details>


### [585] [MoPHES:Leveraging on-device LLMs as Agent for Mobile Psychological Health Evaluation and Support](https://arxiv.org/abs/2510.16085)
*Xun Wei,Pukai Zhou,Zeyu Wang*

Main category: cs.CY

TL;DR: 因传统疗法难以满足全球心理健康需求，本文提出MoPHES框架，用微调的LLM评估用户心理状态、对话并提供治疗建议，还开发基准评估其性能。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康问题增多，传统疗法不足，通用大语言模型和现有聊天机器人存在缺陷，需更好的解决方案。

Method: 提出MoPHES框架，使用两个微调的MiniCPM4 - 0.5B LLM，一个评估心理状态，一个处理对话，模型部署在移动设备，开发基准进行评估。

Result: 未明确提及具体结果。

Conclusion: 未明确提及具体结论，但该框架有望提供更个性化支持和专业治疗建议。

Abstract: The 2022 World Mental Health Report calls for global mental health care
reform, amid rising prevalence of issues like anxiety and depression that
affect nearly one billion people worldwide. Traditional in-person therapy fails
to meet this demand, and the situation is worsened by stigma. While
general-purpose large language models (LLMs) offer efficiency for AI-driven
mental health solutions, they underperform because they lack specialized
fine-tuning. Existing LLM-based mental health chatbots can engage in empathetic
conversations, but they overlook real-time user mental state assessment which
is critical for professional counseling. This paper proposes MoPHES, a
framework that integrates mental state evaluation, conversational support, and
professional treatment recommendations. The agent developed under this
framework uses two fine-tuned MiniCPM4-0.5B LLMs: one is fine-tuned on mental
health conditions datasets to assess users' mental states and predict the
severity of anxiety and depression; the other is fine-tuned on multi-turn
dialogues to handle conversations with users. By leveraging insights into
users' mental states, our agent provides more tailored support and professional
treatment recommendations. Both models are also deployed directly on mobile
devices to enhance user convenience and protect user privacy. Additionally, to
evaluate the performance of MoPHES with other LLMs, we develop a benchmark for
the automatic evaluation of mental state prediction and multi-turn counseling
dialogues, which includes comprehensive evaluation metrics, datasets, and
methods.

</details>


### [586] [Attention to Non-Adopters](https://arxiv.org/abs/2510.15951)
*Kaitlyn Zhou,Kristina Gligorić,Myra Cheng,Michelle S. Lam,Vyoma Raman,Boluwatife Aminu,Caeley Woo,Michael Brockman,Hannah Cha,Dan Jurafsky*

Main category: cs.CY

TL;DR: 多数美国人未使用基于大语言模型的聊天系统，开发和评估大语言模型多依赖使用者数据，论文认为应纳入非使用者观点，并通过案例研究说明其必要性和方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型开发和评估主要依赖使用者数据，忽略非使用者需求，可能导致不平等和开发评估的疏漏，因此需要纳入非使用者观点。

Method: 对非使用者进行案例研究。

Result: 展示了非使用者需求与当前使用者的差异，指出非使用者需求指向新的推理任务，以及如何通过以人为中心的方法系统地整合非使用者需求。

Conclusion: 纳入非使用者观点对于开发广泛有用和有能力的大语言模型至关重要。

Abstract: Although language model-based chat systems are increasingly used in daily
life, most Americans remain non-adopters of chat-based LLMs -- as of June 2025,
66% had never used ChatGPT. At the same time, LLM development and evaluation
rely mainly on data from adopters (e.g., logs, preference data), focusing on
the needs and tasks for a limited demographic group of adopters in terms of
geographic location, education, and gender. In this position paper, we argue
that incorporating non-adopter perspectives is essential for developing broadly
useful and capable LLMs. We contend that relying on methods that focus
primarily on adopters will risk missing a range of tasks and needs prioritized
by non-adopters, entrenching inequalities in who benefits from LLMs, and
creating oversights in model development and evaluation. To illustrate this
claim, we conduct case studies with non-adopters and show: how non-adopter
needs diverge from those of current users, how non-adopter needs point us
towards novel reasoning tasks, and how to systematically integrate non-adopter
needs via human-centered methods.

</details>


### [587] [Agentic Inequality](https://arxiv.org/abs/2510.16853)
*Matthew Sharp,Omer Bilgin,Iason Gabriel,Lewis Hammond*

Main category: cs.CY

TL;DR: 本文探讨了‘智能体不平等’，即因获取和使用AI智能体的差异导致的权力、机会和结果的潜在差距，建立分析框架，指出其与以往技术鸿沟不同，并分析驱动因素和提出研究议程。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI智能体融入政治和经济生活，其分布和能力影响重大，需研究由此产生的不平等问题。

Method: 建立分析框架，从可用性、质量和数量三个核心维度分析不平等的表现；对比智能体与以往工具的不同；系统分析技术和社会经济驱动因素。

Result: 明确智能体不平等的表现、与以往技术鸿沟的区别，以及影响智能体权力分配的驱动因素。

Conclusion: 提出应对未来复杂治理挑战的研究议程。

Abstract: Autonomous AI agents, capable of complex planning and action, represent a
significant technological evolution beyond current generative tools. As these
systems become integrated into political and economic life, their distribution
and capabilities will be highly consequential. This paper introduces and
explores "agentic inequality" - the potential disparities in power,
opportunity, and outcomes stemming from differential access to, and
capabilities of, AI agents. We analyse the dual potential of this technology,
exploring how agents could both exacerbate existing divides and, under the
right conditions, serve as a powerful equalising force. To this end, the paper
makes three primary contributions. First, it establishes an analytical
framework by delineating the three core dimensions through which this
inequality can manifest: disparities in the availability, quality, and quantity
of agents. Second, it argues that agentic inequality is distinct from prior
technological divides. Unlike tools that primarily augment human abilities,
agents act as autonomous delegates, creating novel power asymmetries through
scalable goal delegation and direct agent-to-agent competition that are poised
to reshape outcomes across economic and socio-political spheres. Finally, it
provides a systematic analysis of the technical and socioeconomic drivers -
from model release strategies to market incentives - that will shape the
distribution of agentic power, concluding with a research agenda for navigating
the complex governance challenges ahead.

</details>


### [588] [Visibility Allocation Systems: How Algorithmic Design Shapes Online Visibility and Societal Outcomes](https://arxiv.org/abs/2510.17241)
*Stefania Ionescu,Robin Forsberg,Elsa Lichtenegger,Salima Jaoua,Kshitijaa Jaglan,Florian Dorfler,Aniko Hannak*

Main category: cs.CY

TL;DR: 本文介绍用于可见性分配系统（VASs）的形式框架，分解其为子流程，给出评估指标，并以学校选择中的预测推荐为例展示框架应用，还探讨对AI立法的支持。


<details>
  <summary>Details</summary>
Motivation: 算法系统复杂、结构未知且后果难测，理解和评估这类系统对研究者和立法者是挑战，因此需要一个框架辅助评估。

Method: 引入VASs的形式框架，分解系统为子流程并用数据流程图展示，调查评估指标，以学校选择中的预测推荐为案例研究。

Result: 通过案例研究证明框架可支持VAS评估，还能支持AI立法工作定位义务、量化系统风险和实现自适应合规。

Conclusion: 提出的形式框架有助于评估VASs和支持AI立法工作。

Abstract: Throughout application domains, we now rely extensively on algorithmic
systems to engage with ever-expanding datasets of information. Despite their
benefits, these systems are often complex (comprising of many intricate tools,
e.g., moderation, recommender systems, prediction models), of unknown structure
(due to the lack of accompanying documentation), and having hard-to-predict yet
potentially severe downstream consequences (due to the extensive use,
systematic enactment of existing errors, and many comprising feedback loops).
As such, understanding and evaluating these systems as a whole remains a
challenge for both researchers and legislators. To aid ongoing efforts, we
introduce a formal framework for such visibility allocation systems (VASs)
which we define as (semi-)automated systems deciding which (processed) data to
present a human user with. We review typical tools comprising VASs and define
the associated computational problems they solve. By doing so, VASs can be
decomposed into sub-processes and illustrated via data flow diagrams. Moreover,
we survey metrics for evaluating VASs throughout the pipeline, thus aiding
system diagnostics. Using forecasting-based recommendations in school choice as
a case study, we demonstrate how our framework can support VAS evaluation. We
also discuss how our framework can support ongoing AI-legislative efforts to
locate obligations, quantify systemic risks, and enable adaptive compliance.

</details>


### [589] [Quantifying Climate Policy Action and Its Links to Development Outcomes: A Cross-National Data-Driven Analysis](https://arxiv.org/abs/2510.17425)
*Aditi Dutta*

Main category: cs.CY

TL;DR: 本文开发定量指标分析气候政策方向，揭示不同政策对发展成果的影响，提出集成框架用于气候治理分析。


<details>
  <summary>Details</summary>
Motivation: 现有气候政策评估方法存在不足，无法揭示关键领域差异，需新工具揭示政策主题优先级和对发展成果的实际影响。

Method: 应用基于多语言变压器的语言模型处理国家政策文件开发定量指标，结合面板回归分析与世界银行发展数据。

Result: 分类准确率达0.90（F1分数）；不同气候政策对GDP、GNI、债务、外国直接投资等有不同影响。

Conclusion: 集成的NLP - 计量经济框架可对气候治理进行可比、特定主题分析，提供可扩展方法监测进展、评估权衡和使政策重点与发展目标一致。

Abstract: Addressing climate change effectively requires more than cataloguing the
number of policies in place; it calls for tools that can reveal their thematic
priorities and their tangible impacts on development outcomes. Existing
assessments often rely on qualitative descriptions or composite indices, which
can mask crucial differences between key domains such as mitigation,
adaptation, disaster risk management, and loss and damage. To bridge this gap,
we develop a quantitative indicator of climate policy orientation by applying a
multilingual transformer-based language model to official national policy
documents, achieving a classification accuracy of 0.90 (F1-score). Linking
these indicators with World Bank development data in panel regressions reveals
that mitigation policies are associated with higher GDP and GNI; disaster risk
management correlates with greater GNI and debt but reduced foreign direct
investment; adaptation and loss and damage show limited measurable effects.
This integrated NLP-econometric framework enables comparable, theme-specific
analysis of climate governance, offering a scalable method to monitor progress,
evaluate trade-offs, and align policy emphasis with development goals.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [590] [On Robust hypothesis testing with respect to Hellinger distance](https://arxiv.org/abs/2510.16750)
*Eeshan Modak*

Main category: math.ST

TL;DR: 研究观测样本可能不来自指定假设分布的假设检验问题，量化分布接近假设的程度，还研究复合检验问题并给出替代测试。


<details>
  <summary>Details</summary>
Motivation: 解决观测样本可能不来自指定假设分布时，检验需对错误指定具有鲁棒性的问题。

Method: 研究量化潜在分布与假设的接近程度，针对复合检验问题给出替代测试。

Result: 量化了潜在分布需接近假设的程度，给出复合检验问题的替代测试。

Conclusion: 可解决样本不来自指定假设分布的假设检验问题，且有针对复合检验问题的新方法。

Abstract: We study the hypothesis testing problem where the observed samples need not
come from either of the specified hypotheses (distributions). In such a
situation, we would like our test to be robust to this misspecification and
output the distribution closer in Hellinger distance. If the underlying
distribution is close to being equidistant from the hypotheses, then this would
not be possible. Our main result is quantifying how close the underlying
distribution has to be to either of the hypotheses. We also study the composite
testing problem, where each hypothesis is a Hellinger ball around a fixed
distribution. A generalized likelihood ratio test is known to work for this
problem. We give an alternate test for the same.

</details>


### [591] [Spectral Thresholds in Correlated Spiked Models and Fundamental Limits of Partial Least Squares](https://arxiv.org/abs/2510.17561)
*Pierre Mergny,Lenka Zdeborová*

Main category: math.ST

TL;DR: 对部分信号对齐的尖峰交叉协方差模型进行随机矩阵理论分析，揭示偏最小二乘法（PLS）信号恢复能力，明确其理论极限。


<details>
  <summary>Details</summary>
Motivation: 多模态学习需求以及PLS方法理论发展不足，需要对相关模型进行理论分析。

Method: 运用严格的随机矩阵理论对部分信号对齐的尖峰交叉协方差模型进行分析。

Result: 样本交叉协方差矩阵的主奇异值发生BBP型相变，确定信息成分出现的精确阈值，揭示PLS与贝叶斯最优估计器的性能差距，找出PLS无法恢复信号的SNR和相关区域。

Conclusion: 明确了PLS的理论极限，为高维可靠多模态推理方法设计提供指导。

Abstract: We provide a rigorous random matrix theory analysis of spiked
cross-covariance models where the signals across two high-dimensional data
channels are partially aligned. These models are motivated by multi-modal
learning and form the standard generative setting underlying Partial Least
Squares (PLS), a widely used yet theoretically underdeveloped method. We show
that the leading singular values of the sample cross-covariance matrix undergo
a Baik-Ben Arous-Peche (BBP)-type phase transition, and we characterize the
precise thresholds for the emergence of informative components. Our results
yield the first sharp asymptotic description of the signal recovery
capabilities of PLS in this setting, revealing a fundamental performance gap
between PLS and the Bayes-optimal estimator. In particular, we identify the SNR
and correlation regimes where PLS fails to recover any signal, despite
detectability being possible in principle. These findings clarify the
theoretical limits of PLS and provide guidance for the design of reliable
multi-modal inference methods in high dimensions.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [592] [AoI-Aware Task Offloading and Transmission Optimization for Industrial IoT Networks: A Branching Deep Reinforcement Learning Approach](https://arxiv.org/abs/2510.16414)
*Yuang Chen,Fengqian Guo,Chang Wu,Shuyi Liu,Hancheng Lu,Chang Wen Chen*

Main category: eess.SY

TL;DR: 提出AoI感知多基站实时监测框架，解决工业物联网数据传输新鲜度问题，通过创新算法实现任务卸载和资源分配优化，模拟显示算法性能优越。


<details>
  <summary>Details</summary>
Motivation: 工业物联网大量数据无线传输需满足严格及时性要求，数据包状态更新新鲜度影响系统性能，需解决多基站决策空间组合爆炸和系统随机动态性问题。

Method: 提出基于分支的Dueling Double Deep Q - Network算法实现任务卸载；通过证明带宽和计算资源Hessian矩阵半定性质解决资源分配问题；提出迭代优化算法实现联合任务卸载和资源分配。

Result: 提出的Branching - D3QN算法优于现有DRL方法和经典启发式算法，收敛速度提升75%，长期平均AoI降低至少22%。

Conclusion: 所提框架和算法能有效优化工业物联网任务卸载和资源分配，降低长期平均AoI，提升系统性能。

Abstract: In the Industrial Internet of Things (IIoT), the frequent transmission of
large amounts of data over wireless networks should meet the stringent
timeliness requirements. Particularly, the freshness of packet status updates
has a significant impact on the system performance. In this paper, we propose
an age-of-information (AoI)-aware multi-base station (BS) real-time monitoring
framework to support extensive IIoT deployments. To meet the freshness
requirements of IIoT, we formulate a joint task offloading and resource
allocation optimization problem with the goal of minimizing long-term average
AoI. Tackling the core challenges of combinatorial explosion in multi-BS
decision spaces and the stochastic dynamics of IIoT systems is crucial, as
these factors render traditional optimization methods intractable. Firstly, an
innovative branching-based Dueling Double Deep Q-Network (Branching-D3QN)
algorithm is proposed to effectively implement task offloading, which optimizes
the convergence performance by reducing the action space complexity from
exponential to linear levels. Then, an efficient optimization solution to
resource allocation is proposed by proving the semi-definite property of the
Hessian matrix of bandwidth and computation resources. Finally, we propose an
iterative optimization algorithm for efficient joint task offloading and
resource allocation to achieve optimal average AoI performance. Extensive
simulations demonstrate that our proposed Branching-D3QN algorithm outperforms
both state-of-the-art DRL methods and classical heuristics, achieving up to a
75% enhanced convergence speed and at least a 22% reduction in the long-term
average AoI.

</details>


### [593] [A Control-Theoretic Approach to Dynamic Payment Routing for Success Rate Optimization](https://arxiv.org/abs/2510.16735)
*Aniket Agrawal,Harsharanga Patil*

Main category: eess.SY

TL;DR: 本文提出动态支付路由控制理论框架，在JUSPAY支付编排器实现，提升交易成功率，生产结果显示优于传统规则路由。


<details>
  <summary>Details</summary>
Motivation: 最大化支付交易成功率，确保支付系统的运营弹性和可靠性。

Method: 将路由系统建模为闭环反馈控制器，结合控制理论、强化学习和多臂老虎机优化，采用基于广义反馈的自适应方法。

Result: 实时生产结果表明，相比传统基于规则的路由，成功率提高了1.15%。

Conclusion: 基于反馈的控制在支付系统中有效，该混合方法能实现自我调节的交易路由，减少不稳定性并提高可靠性。

Abstract: This paper introduces a control-theoretic framework for dynamic payment
routing, implemented within JUSPAY's Payment Orchestrator to maximize
transaction success rate. The routing system is modeled as a closed-loop
feedback controller continuously sensing gateway performance, computing
corrective actions, and dynamically routes transactions across gateway to
ensure operational resilience. The system leverages concepts from control
theory, reinforcement learning, and multi-armed bandit optimization to achieve
both short-term responsiveness and long-term stability. Rather than relying on
explicit PID regulation, the framework applies generalized feedback-based
adaptation, ensuring that corrective actions remain proportional to observed
performance deviations and the computed gateway score gradually converges
toward the success rate. This hybrid approach unifies control theory and
adaptive decision systems, enabling self-regulating transaction routing that
dampens instability, and improves reliability. Live production results show an
improvement of up to 1.15% in success rate over traditional rule-based routing,
demonstrating the effectiveness of feedback-based control in payment systems.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [594] [A Topological Approach to Parameterizing Deep Hedging Networks](https://arxiv.org/abs/2510.16938)
*Alok Das,Kiseop Lee*

Main category: q-fin.MF

TL;DR: 研究表明添加拓扑特征可大幅减少 Deep hedging 训练所需批量大小，使模型训练更可行且不显著影响对冲性能。


<details>
  <summary>Details</summary>
Motivation: 以往基于路径梯度最小化二次对冲误差的方法需大批次大小，在合理时间内训练有效模型具挑战性。

Method: 在 Deep hedging 中添加某些拓扑特征。

Result: 大幅减少批量大小，使训练这些模型更具实际可行性。

Conclusion: 添加拓扑特征在不显著影响对冲性能的情况下能让模型训练更可行。

Abstract: Deep hedging uses recurrent neural networks to hedge financial products that
cannot be fully hedged in incomplete markets. Previous work in this area
focuses on minimizing some measure of quadratic hedging error by calculating
pathwise gradients, but doing so requires large batch sizes and can make
training effective models in a reasonable amount of time challenging. We show
that by adding certain topological features, we can reduce batch sizes
substantially and make training these models more practically feasible without
greatly compromising hedging performance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [595] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: 现有VLA代理评估指标粗粒度，数据分散，本文提出NEBULA统一生态系统用于单臂操作评估，发现顶尖VLA代理存在关键能力不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLA代理评估指标无法精确诊断技能和衡量鲁棒性，数据分散阻碍研究，需要改进评估方法。

Method: 引入NEBULA统一生态系统，采用双轴评估协议，结合细粒度能力测试和系统压力测试，提供标准化API和大规模聚合数据集。

Result: 使用NEBULA发现顶尖VLA代理在空间推理和动态适应等关键能力上存在问题，传统指标会掩盖这些问题。

Conclusion: NEBULA为构建鲁棒、通用的具身代理提供了实际基础。

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [596] [Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification](https://arxiv.org/abs/2510.16281)
*Yilin Wu,Anqi Li,Tucker Hermans,Fabio Ramos,Andrea Bajcsy,Claudia P'erez-D'Arpino*

Main category: cs.RO

TL;DR: 提出训练自由的运行时策略转向方法，使推理与动作对齐，增强推理VLA模型鲁棒性并提升性能。


<details>
  <summary>Details</summary>
Motivation: 推理VLA模型生成的动作即便有正确文本计划，在OOD场景仍可能无法达成预期结果，存在具身思维链忠实性缺失问题。

Method: 给定推理VLA的中间文本计划，从同一模型采样多个候选动作序列，通过模拟预测结果，用预训练VLM选择与文本计划最匹配的序列。

Result: 将基础VLA的自然动作多样性从错误来源变为优势，增强对语义和视觉OOD扰动的鲁棒性，实现新行为组合，在行为组合任务上比先前工作性能提升达15%。

Conclusion: 该方法有效，能在不进行昂贵再训练的情况下提升推理VLA模型性能，且性能随计算和数据多样性提升。

Abstract: Reasoning Vision Language Action (VLA) models improve robotic
instruction-following by generating step-by-step textual plans before low-level
actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language
models. Yet even with a correct textual plan, the generated actions can still
miss the intended outcomes in the plan, especially in out-of-distribution (OOD)
scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,
and introduce a training-free, runtime policy steering method for
reasoning-action alignment. Given a reasoning VLA's intermediate textual plan,
our framework samples multiple candidate action sequences from the same model,
predicts their outcomes via simulation, and uses a pre-trained Vision-Language
Model (VLM) to select the sequence whose outcome best aligns with the VLA's own
textual plan. Only executing action sequences that align with the textual
reasoning turns our base VLA's natural action diversity from a source of error
into a strength, boosting robustness to semantic and visual OOD perturbations
and enabling novel behavior composition without costly re-training. We also
contribute a reasoning-annotated extension of LIBERO-100, environment
variations tailored for OOD evaluation, and demonstrate up to 15% performance
gain over prior work on behavior composition tasks and scales with compute and
data diversity. Project Website at:
https://yilin-wu98.github.io/steering-reasoning-vla/

</details>


### [597] [Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models](https://arxiv.org/abs/2510.16344)
*Chenrui Tie,Shengxiang Sun,Yudi Lin,Yanbo Wang,Zhongrui Li,Zhouhan Zhong,Jinxuan Zhu,Yiman Pang,Haonan Chen,Junting Chen,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: 本文将连接视为装配表示的一等原语，提出Manual2Skill++框架从装配手册提取连接信息，编码任务为层次图，通过数据集验证方法并在模拟中评估任务理解到执行的完整流程。


<details>
  <summary>Details</summary>
Motivation: 多数机器人装配方法将连接器视为事后考虑因素，而连接是装配执行的关键，因此要将连接作为装配表示的一等原语进行研究。

Method: 提出Manual2Skill++视觉语言框架，从装配手册自动提取结构化连接信息，将装配任务编码为层次图，用大视觉语言模型解析手册中的符号图和注释来实例化图。

Result: 策划包含20多个不同连接器类型装配任务的数据集验证表示提取方法，在模拟中对四个复杂装配场景评估任务理解到执行的完整流程。

Conclusion: 将连接作为一等原语的方法及Manual2Skill++框架在装配任务中具有有效性和可行性。

Abstract: Assembly hinges on reliably forming connections between parts; yet most
robotic approaches plan assembly sequences and part poses while treating
connectors as an afterthought. Connections represent the critical "last mile"
of assembly execution, while task planning may sequence operations and motion
plan may position parts, the precise establishment of physical connections
ultimately determines assembly success or failure. In this paper, we consider
connections as first-class primitives in assembly representation, including
connector types, specifications, quantities, and placement locations. Drawing
inspiration from how humans learn assembly tasks through step-by-step
instruction manuals, we present Manual2Skill++, a vision-language framework
that automatically extracts structured connection information from assembly
manuals. We encode assembly tasks as hierarchical graphs where nodes represent
parts and sub-assemblies, and edges explicitly model connection relationships
between components. A large-scale vision-language model parses symbolic
diagrams and annotations in manuals to instantiate these graphs, leveraging the
rich connection knowledge embedded in human-designed instructions. We curate a
dataset containing over 20 assembly tasks with diverse connector types to
validate our representation extraction approach, and evaluate the complete task
understanding-to-execution pipeline across four complex assembly scenarios in
simulation, spanning furniture, toys, and manufacturing components with
real-world correspondence.

</details>


### [598] [DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation](https://arxiv.org/abs/2510.16518)
*Jesús Ortega-Peimbert,Finn Lukas Busch,Timon Homberger,Quantao Yang,Olov Andersson*

Main category: cs.RO

TL;DR: 提出DIV - Nav实时导航系统解决含空间关系的复杂自由文本查询的目标导航问题，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有零样本目标导航通常针对简单目标名称查询，本文要处理含空间关系的复杂自由文本查询。

Method: 提出DIV - Nav系统，通过分解复杂自然语言指令、计算语义信念图交集、用LVLM验证发现的目标，还调整在线语义映射的边界探索目标。

Result: 在MultiON基准测试和波士顿动力Spot机器人上进行了广泛实验验证。

Conclusion: DIV - Nav系统能有效解决含空间关系的复杂自由文本查询的目标导航问题。

Abstract: Advances in open-vocabulary semantic mapping and object navigation have
enabled robots to perform an informed search of their environment for an
arbitrary object. However, such zero-shot object navigation is typically
designed for simple queries with an object name like "television" or "blue
rug". Here, we consider more complex free-text queries with spatial
relationships, such as "find the remote on the table" while still leveraging
robustness of a semantic map. We present DIV-Nav, a real-time navigation system
that efficiently addresses this problem through a series of relaxations: i)
Decomposing natural language instructions with complex spatial constraints into
simpler object-level queries on a semantic map, ii) computing the Intersection
of individual semantic belief maps to identify regions where all objects
co-exist, and iii) Validating the discovered objects against the original,
complex spatial constrains via a LVLM. We further investigate how to adapt the
frontier exploration objectives of online semantic mapping to such spatial
search queries to more effectively guide the search process. We validate our
system through extensive experiments on the MultiON benchmark and real-world
deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More
details and videos are available at https://anonsub42.github.io/reponame/

</details>


### [599] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: 提出多模态目标条件行为克隆框架DINO - CVA用于自主导管导航，实验证明其可行性和优势。


<details>
  <summary>Details</summary>
Motivation: 现有心脏导管手术依赖手动操作，机器人平台缺乏智能自主性，导致操作者疲劳、辐射暴露和手术结果差异，需实现自主导管导航。

Method: 引入DINO - CVA框架，融合视觉观察和操纵杆运动学信息到联合嵌入空间，从专家演示中自回归预测动作，目标条件引导导航。用合成血管模型收集多模态数据集评估性能。

Result: DINO - CVA在动作预测上准确性高，达到仅运动学基线的性能，还能结合解剖环境。

Conclusion: 多模态、目标条件架构用于导管导航可行，有助于减少操作者依赖，提高导管治疗可靠性。

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [600] [Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/abs/2510.17111)
*Weifan Guan,Qinghao Hu,Aosheng Li,Jian Cheng*

Main category: cs.RO

TL;DR: 本文对提升视觉 - 语言 - 动作（VLA）模型效率的方法进行系统综述，分类总结现有解决方案并探讨未来趋势。


<details>
  <summary>Details</summary>
Motivation: VLA系统计算和内存需求大，与边缘平台实时性能约束冲突，解决此矛盾是研究重点。

Method: 将现有解决方案分为模型架构、感知特征、动作生成和训练/推理策略四个维度，总结各维度代表性技术。

Result: 对提升VLA效率的方法进行分类总结。

Conclusion: 讨论了未来趋势和开放性挑战，指明了提升高效具身智能的方向。

Abstract: Vision-Language-Action (VLA) models extend vision-language models to embodied
control by mapping natural-language instructions and visual observations to
robot actions. Despite their capabilities, VLA systems face significant
challenges due to their massive computational and memory demands, which
conflict with the constraints of edge platforms such as on-board mobile
manipulators that require real-time performance. Addressing this tension has
become a central focus of recent research. In light of the growing efforts
toward more efficient and scalable VLA systems, this survey provides a
systematic review of approaches for improving VLA efficiency, with an emphasis
on reducing latency, memory footprint, and training and inference costs. We
categorize existing solutions into four dimensions: model architecture,
perception feature, action generation, and training/inference strategies,
summarizing representative techniques within each category. Finally, we discuss
future trends and open challenges, highlighting directions for advancing
efficient embodied intelligence.

</details>


### [601] [SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving](https://arxiv.org/abs/2510.17191)
*Peiru Zheng,Yun Zhao,Zhan Gong,Hong Zhu,Shaohua Wu*

Main category: cs.RO

TL;DR: 本文提出SimpleVSF框架提升端到端自动驾驶规划，在挑战赛中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法在复杂场景决策欠佳，需改进。

Method: 提出SimpleVSF框架，利用视觉语言模型认知能力和轨迹融合技术，结合传统评分器和VLM增强评分器，使用权重融合器和VLM融合器。

Result: SimpleVSF框架在ICCV 2025 NAVSIM v2端到端驾驶挑战赛中展现出了先进水平。

Conclusion: SimpleVSF框架能在安全、舒适和效率间取得良好平衡。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
achieving robust and intelligent driving policies. However, existing end-to-end
methods still face significant challenges, such as suboptimal decision-making
in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring
Fusion), a novel framework that enhances end-to-end planning by leveraging the
cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory
fusion techniques. We utilize the conventional scorers and the novel
VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative
aggregation and a powerful VLM-based fusioner for qualitative, context-aware
decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End
Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art
performance, achieving a superior balance between safety, comfort, and
efficiency.

</details>


### [602] [Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots](https://arxiv.org/abs/2510.17369)
*Haochen Su,Cristian Meo,Francesco Stella,Andrea Peirone,Kai Junge,Josie Hughes*

Main category: cs.RO

TL;DR: 本文将VLA模型部署于软连续机械臂以实现人机安全交互，通过微调解决体现不匹配问题，证明结合VLA模型与软机器人可实现安全灵活的具身AI。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型部署局限于传统串行机械臂，缺乏与环境安全交互能力，需要在软连续机械臂上部署以实现安全人机交互。

Method: 提出结构化微调与部署流程，评估两个先进VLA模型在代表性操作任务中的表现。

Result: 未微调的策略因体现不匹配失败，经有针对性微调后软机器人表现与刚性机器人相当。

Conclusion: 强调微调对弥合体现差距的必要性，证明VLA模型与软机器人结合能在共享环境中实现安全灵活的具身AI。

Abstract: Robotic systems are increasingly expected to operate in human-centered,
unstructured environments where safety, adaptability, and generalization are
essential. Vision-Language-Action (VLA) models have been proposed as a language
guided generalized control framework for real robots. However, their deployment
has been limited to conventional serial link manipulators. Coupled by their
rigidity and unpredictability of learning based control, the ability to safely
interact with the environment is missing yet critical. In this work, we present
the deployment of a VLA model on a soft continuum manipulator to demonstrate
autonomous safe human-robot interaction. We present a structured finetuning and
deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and
$\pi_0$) across representative manipulation tasks, and show while
out-of-the-box policies fail due to embodiment mismatch, through targeted
finetuning the soft robot performs equally to the rigid counterpart. Our
findings highlight the necessity of finetuning for bridging embodiment gaps,
and demonstrate that coupling VLA models with soft robots enables safe and
flexible embodied AI in human-shared environments.

</details>


### [603] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: 提出FALCON范式，向动作头注入3D空间令牌，在多个基准测试和真实任务中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言 - 动作（VLA）模型基于2D编码器有空间推理差距，现有3D集成技术有不足。

Method: 引入FALCON范式，利用空间基础模型从RGB提供几何先验，包含可融合深度或姿态的具身空间模型，用空间增强动作头处理空间令牌。

Result: 在三个模拟基准测试和十一个真实任务中达到SOTA，超越基线，且在复杂环境下保持鲁棒性。

Conclusion: FALCON能解决空间表示、模态可迁移性和对齐方面的局限性。

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [604] [High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection](https://arxiv.org/abs/2510.17261)
*Fernando Salanova,Jesús Roche,Cristian Mahuela,Eduardo Montijano*

Main category: cs.RO

TL;DR: 提出基于NWN范式的数据生成框架和Transformer异常检测管道，用于多机器人系统中识别LTL公式计划的异常执行，实验效果好。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统中需要鲁棒方法检测虚假行为，解决识别LTL公式计划的虚假执行问题。

Method: 引入基于NWN范式的结构化数据生成框架，协调机器人动作与全局任务规范；提出基于Transformer的异常检测管道对轨迹分类。

Result: 方法识别执行效率的准确率达91.3%，检测核心任务违规和基于约束的自适应异常能力分别为88.3%和66.8%；消融实验表明新方法优于简单表示。

Conclusion: 所提方法在多机器人系统异常检测中有效且表现良好。

Abstract: The reliable execution of high-level missions in multi-robot systems with
heterogeneous agents, requires robust methods for detecting spurious behaviors.
In this paper, we address the challenge of identifying spurious executions of
plans specified as a Linear Temporal Logic (LTL) formula, as incorrect task
sequences, violations of spatial constraints, timing inconsis- tencies, or
deviations from intended mission semantics. To tackle this, we introduce a
structured data generation framework based on the Nets-within-Nets (NWN)
paradigm, which coordinates robot actions with LTL-derived global mission
specifications. We further propose a Transformer-based anomaly detection
pipeline that classifies robot trajectories as normal or anomalous. Experi-
mental evaluations show that our method achieves high accuracy (91.3%) in
identifying execution inefficiencies, and demonstrates robust detection
capabilities for core mission violations (88.3%) and constraint-based adaptive
anomalies (66.8%). An ablation experiment of the embedding and architecture was
carried out, obtaining successful results where our novel proposition performs
better than simpler representations.

</details>


### [605] [Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries](https://arxiv.org/abs/2510.17576)
*Cansu Erdogan,Cesar Alan Contreras,Alireza Rastegarpanah,Manolis Chiou,Rustam Stolkin*

Main category: cs.RO

TL;DR: 本文提出意图驱动规划管道解决复杂操作任务规划问题，在拆解电动汽车电池任务上评估，结果表明该方法能将操作员意图转化为安全可执行的多机器人计划。


<details>
  <summary>Details</summary>
Motivation: 解决在非结构化场景中，不同能力的多机器人基于计算机视觉对任意位置和配置的物体进行复杂操作任务规划的问题。

Method: 提出意图驱动规划管道，集成感知到文本的场景编码、基于大语言模型生成候选移除序列、大语言模型验证器和确定性一致性过滤器。

Result: 在200个真实场景和600个操作员提示上评估，结果显示集成验证方法能可靠地将操作员意图转化为安全可执行的多机器人计划，且用户工作量低。

Conclusion: 意图驱动规划管道能有效解决复杂操作任务规划问题，将操作员意图转化为多机器人计划，同时降低用户工作量。

Abstract: This paper addresses the problem of planning complex manipulation tasks, in
which multiple robots with different end-effectors and capabilities, informed
by computer vision, must plan and execute concatenated sequences of actions on
a variety of objects that can appear in arbitrary positions and configurations
in unstructured scenes. We propose an intent-driven planning pipeline which can
robustly construct such action sequences with varying degrees of supervisory
input from a human using simple language instructions. The pipeline integrates:
(i) perception-to-text scene encoding, (ii) an ensemble of large language
models (LLMs) that generate candidate removal sequences based on the operator's
intent, (iii) an LLM-based verifier that enforces formatting and precedence
constraints, and (iv) a deterministic consistency filter that rejects
hallucinated objects. The pipeline is evaluated on an example task in which two
robot arms work collaboratively to dismantle an Electric Vehicle battery for
recycling applications. A variety of components must be grasped and removed in
specific sequences, determined by human instructions and/or by task-order
feasibility decisions made by the autonomous system. On 200 real scenes with
600 operator prompts across five component classes, we used metrics of
full-sequence correctness and next-task correctness to evaluate and compare
five LLM-based planners (including ablation analyses of pipeline components).
We also evaluated the LLM-based human interface in terms of time to execution
and NASA TLX with human participant experiments. Results indicate that our
ensemble-with-verification approach reliably maps operator intent to safe,
executable multi-robot plans while maintaining low user effort.

</details>


### [606] [RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation](https://arxiv.org/abs/2510.17640)
*Yuquan Xue,Guanxing Lu,Zhenyu Wu,Chuanrui Zhang,Bofang Jia,Zhengyi Gu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出RESample自动化OOD数据增强框架提升VLA模型稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习数据集缺乏失败或恢复数据，VLA模型难以处理训练分布外状态。

Method: 利用离线强化学习获取动作价值网络，通过滚动采样潜在OOD状态，设计探索性采样机制将动作代理加入训练集。

Result: 在LIBERO基准和真实机器人操作任务实验中，RESample持续提升了VLA模型的稳定性和泛化能力。

Conclusion: RESample框架能有效提升VLA模型应对分布偏移的鲁棒性。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
on complex robotic manipulation tasks through imitation learning. However,
existing imitation learning datasets contain only successful trajectories and
lack failure or recovery data, especially for out-of-distribution (OOD) states
where the robot deviates from the main policy due to minor perturbations or
errors, leading VLA models to struggle with states deviating from the training
distribution. To this end, we propose an automated OOD data augmentation
framework named RESample through exploratory sampling. Specifically, we first
leverage offline reinforcement learning to obtain an action-value network that
accurately identifies sub-optimal actions under the current manipulation
policy. We further sample potential OOD states from trajectories via rollout,
and design an exploratory sampling mechanism that adaptively incorporates these
action proxies into the training dataset to ensure efficiency. Subsequently,
our framework explicitly encourages the VLAs to recover from OOD states and
enhances their robustness against distributional shifts. We conduct extensive
experiments on the LIBERO benchmark as well as real-world robotic manipulation
tasks, demonstrating that RESample consistently improves the stability and
generalization ability of VLA models.

</details>


### [607] [SoftMimic: Learning Compliant Whole-body Control from Examples](https://arxiv.org/abs/2510.17792)
*Gabriel B. Margolis,Michelle Wang,Nolan Fey,Pulkit Agrawal*

Main category: cs.RO

TL;DR: 介绍SoftMimic框架，用于从示例动作中学习类人机器人的柔顺全身控制策略，经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有模仿人类动作的强化学习方法会导致僵硬控制，在意外接触时表现脆弱和不安全，需使机器人能柔顺响应外力。

Method: 利用逆运动学求解器生成可行的柔顺动作增强数据集，训练强化学习策略，奖励策略匹配柔顺响应而非严格跟踪参考动作。

Result: 通过模拟和现实实验验证，机器人能与环境进行安全有效的交互。

Conclusion: SoftMimic框架可让机器人柔顺响应外力，保持平衡和姿态，能吸收干扰并从单个动作片段泛化到不同任务。

Abstract: We introduce SoftMimic, a framework for learning compliant whole-body control
policies for humanoid robots from example motions. Imitating human motions with
reinforcement learning allows humanoids to quickly learn new skills, but
existing methods incentivize stiff control that aggressively corrects
deviations from a reference motion, leading to brittle and unsafe behavior when
the robot encounters unexpected contacts. In contrast, SoftMimic enables robots
to respond compliantly to external forces while maintaining balance and
posture. Our approach leverages an inverse kinematics solver to generate an
augmented dataset of feasible compliant motions, which we use to train a
reinforcement learning policy. By rewarding the policy for matching compliant
responses rather than rigidly tracking the reference motion, SoftMimic learns
to absorb disturbances and generalize to varied tasks from a single motion
clip. We validate our method through simulations and real-world experiments,
demonstrating safe and effective interaction with the environment.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [608] [Estimating Orbital Parameters of Direct Imaging Exoplanet Using Neural Network](https://arxiv.org/abs/2510.17459)
*Bo Liang,Hanlin Song,Chang Liu,Tianyu Zhao,Yuxiang Xu,Zihao Xiao,Manjia Liang,Minghui Du,Wei-Liang Qian,Li-e Qiang,Peng Xu,Ziren Luo*

Main category: astro-ph.EP

TL;DR: 提出FM - MCMC算法估计系外行星系统轨道参数，比传统方法更快且准确，适用于大规模数据，还可用于其他领域。


<details>
  <summary>Details</summary>
Motivation: 传统基于贝叶斯框架随机采样方法效率低，需更高效算法估计系外行星系统轨道参数。

Method: 先利用流匹配后验估计（FMPE）约束物理参数先验范围，再用MCMC准确推断后验分布。

Result: 在β Pictoris b轨道参数推断中，比PTMCMC快77.8倍，比嵌套采样快365.4倍，且平均对数似然最高。

Conclusion: 该方法可扩展且高效，适用于未来系外行星调查的大规模数据集，还能用于其他领域解决复杂推理问题。

Abstract: In this work, we propose a new flow-matching Markov chain Monte Carlo
(FM-MCMC) algorithm for estimating the orbital parameters of exoplanetary
systems, especially for those only one exoplanet is involved. Compared to
traditional methods that rely on random sampling within the Bayesian framework,
our approach first leverages flow matching posterior estimation (FMPE) to
efficiently constrain the prior range of physical parameters, and then employs
MCMC to accurately infer the posterior distribution. For example, in the
orbital parameter inference of beta Pictoris b, our model achieved a
substantial speed-up while maintaining comparable accuracy-running 77.8 times
faster than Parallel Tempered MCMC (PTMCMC) and 365.4 times faster than nested
sampling. Moreover, our FM-MCMC method also attained the highest average
log-likelihood among all approaches, demonstrating its superior sampling
efficiency and accuracy. This highlights the scalability and efficiency of our
approach, making it well-suited for processing the massive datasets expected
from future exoplanet surveys. Beyond astrophysics, our methodology establishes
a versatile paradigm for synergizing deep generative models with traditional
sampling, which can be adopted to tackle complex inference problems in other
fields, such as cosmology, biomedical imaging, and particle physics.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [609] [Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis](https://arxiv.org/abs/2510.16635)
*Wonduk Seo,Juhyeon Lee,Junseo Koh,Hyunjin An,Jian Park,Seunghyun Lee,Haihua Chen,Yi Bu*

Main category: cs.MA

TL;DR: 本文提出多智能体框架MA - SAPO用于提示优化，使提示改进更透明、可审计和可控，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法将评估视为黑盒，依赖试错改进，难以解释和控制，缺乏对提示成败原因的洞察。

Method: 引入MA - SAPO框架，包含推理阶段（智能体协作解释分数、诊断弱点并合成改进）和测试阶段（检索推理资产分析优化提示并进行有依据的编辑）。

Result: 在HelpSteer1/2基准测试中，MA - SAPO相比单遍提示、检索增强基线和先前的多智能体策略有持续改进。

Conclusion: MA - SAPO能将评估信号转化为可解释的推理链，使提示改进更透明、可审计和可控，验证了方法的有效性。

Abstract: Prompt optimization has emerged as an effective alternative to retraining for
improving the performance of Large Language Models (LLMs). However, most
existing approaches treat evaluation as a black box, relying solely on
numerical scores while offering limited insight into why a prompt succeeds or
fails. They also depend heavily on trial-and-error refinements, which are
difficult to interpret and control. In this paper, we introduce MA-SAPO, a
Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior
methods, MA-SAPO explicitly couples evaluation outcomes with structured
reasoning to guide systematic edits. The framework specifically consists of two
stages: during the Reasoning Phase, agents collaboratively explain metric
scores, diagnose weaknesses, and synthesize targeted refinements that are
stored as reusable reasoning assets; during the Test Phase, agents retrieve
these assets to analyze optimized prompts and apply only evidence-grounded
edits. By turning evaluation signals into interpretable reasoning chains,
MA-SAPO produces prompt refinements that are more transparent, auditable, and
controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent
improvements over single-pass prompting, retrieval-augmented baselines, and
prior multi-agent strategies, validating the effectiveness of our approach.

</details>


### [610] [DiRAC - Distributed Robot Awareness and Consensus](https://arxiv.org/abs/2510.16850)
*Uday Gopan,Manjari Kulkarni,Lakshasri S,Kashish Mittal,Sriram Radhakrishna,Aditya Naskar,Rameshwar DL*

Main category: cs.MA

TL;DR: DiRAC是用于大型机器人群的可扩展分布式框架，经初步模拟验证有良好表现，为实际应用奠基。


<details>
  <summary>Details</summary>
Motivation: 实现大型机器人群的高效任务分配和路径规划。

Method: 引入带动态选举领导者的分区架构和同步共识协议，采用基于力的分散式实时避障路径规划算法。

Result: 在ROS 2中间件的初步模拟中，展示了架构可扩展性和模块化效率。

Conclusion: DiRAC为大规模工业和物流领域的实际部署奠定基础。

Abstract: DiRAC is a scalable, distributed framework designed to enable efficient task
assignment and path planning in very large robotic swarms. It introduces a
novel zone-partitioned architecture with dynamically elected leaders and a
tick-synchronized consensus protocol that yields strong consistency and
deterministic outcomes. For path planning, DiRAC uses a novel algorithm, a
force-based decentralized planner for real-time collision resolution. Validated
within ROS 2 middleware through preliminary simulation, DiRAC demonstrates
architectural scalability and modular efficiency in simulated warehouse
environments, laying the groundwork for real-world deployment in large-scale
industrial and logistics domains.

</details>


### [611] [Disaster Management in the Era of Agentic AI Systems: A Vision for Collective Human-Machine Intelligence for Augmented Resilience](https://arxiv.org/abs/2510.16034)
*Bo Li,Junwei Ma,Kai Yin,Yiming Xiao,Chia-Wei Hsu,Ali Mostafavi*

Main category: cs.MA

TL;DR: 传统灾害管理能力不足，本文提出Disaster Copilot多智能体AI系统，介绍架构和三阶段路线图，以构建更具适应性、数据驱动和韧性的社区。


<details>
  <summary>Details</summary>
Motivation: 传统灾害管理因数据分散、技术孤立等问题，难以进行及时有效的决策，需要新系统解决这些系统性挑战。

Method: 引入Disaster Copilot系统，利用中央协调器协调各专业子智能体，整合多模态数据，采用设备端编排确保在资源受限环境中运行，并捕捉机构知识。

Result: 系统能提供全面实时的运营图景，推动灾害数字孪生从被动模型转变为主动智能环境。

Conclusion: Disaster Copilot系统具有变革性，能促进人机集体智能，构建更具适应性、数据驱动和韧性的社区。

Abstract: The escalating frequency and severity of disasters routinely overwhelm
traditional response capabilities, exposing critical vulnerability in disaster
management. Current practices are hindered by fragmented data streams, siloed
technologies, resource constraints, and the erosion of institutional memory,
which collectively impede timely and effective decision making. This study
introduces Disaster Copilot, a vision for a multi-agent artificial intelligence
system designed to overcome these systemic challenges by unifying specialized
AI tools within a collaborative framework. The proposed architecture utilizes a
central orchestrator to coordinate diverse sub-agents, each specializing in
critical domains such as predictive risk analytics, situational awareness, and
impact assessment. By integrating multi-modal data, the system delivers a
holistic, real-time operational picture and serve as the essential AI backbone
required to advance Disaster Digital Twins from passive models to active,
intelligent environments. Furthermore, it ensures functionality in
resource-limited environments through on-device orchestration and incorporates
mechanisms to capture institutional knowledge, mitigating the impact of staff
turnover. We detail the system architecture and propose a three-phased roadmap
emphasizing the parallel growth of technology, organizational capacity, and
human-AI teaming. Disaster Copilot offers a transformative vision, fostering
collective human-machine intelligence to build more adaptive, data-driven and
resilient communities.

</details>


### [612] [Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards](https://arxiv.org/abs/2510.16187)
*Rupal Nigam,Niket Parikh,Hamid Osooli,Mikihisa Yuasa,Jacob Heglund,Huy T. Tran*

Main category: cs.MA

TL;DR: 提出GPAT算法，利用所有预训练策略实现零样本迁移，在模拟环境和真实多机器人场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界多智能体系统中临时组队时零样本协调任务的问题，现有方法存在局限性。

Method: 将问题形式化为临时多智能体马尔可夫决策过程，采用广义策略改进和差异奖励两个关键思想实现知识转移。

Result: GPAT算法在合作觅食、捕食者 - 猎物和Overcooked三种模拟环境中成功实现向新团队的零样本迁移，并在真实多机器人场景中得到验证。

Conclusion: 所提出的GPAT算法能有效解决临时组队的零样本协调问题。

Abstract: Real-world multi-agent systems may require ad hoc teaming, where an agent
must coordinate with other previously unseen teammates to solve a task in a
zero-shot manner. Prior work often either selects a pretrained policy based on
an inferred model of the new teammates or pretrains a single policy that is
robust to potential teammates. Instead, we propose to leverage all pretrained
policies in a zero-shot transfer setting. We formalize this problem as an ad
hoc multi-agent Markov decision process and present a solution that uses two
key ideas, generalized policy improvement and difference rewards, for efficient
and effective knowledge transfer between different teams. We empirically
demonstrate that our algorithm, Generalized Policy improvement for Ad hoc
Teaming (GPAT), successfully enables zero-shot transfer to new teams in three
simulated environments: cooperative foraging, predator-prey, and Overcooked. We
also demonstrate our algorithm in a real-world multi-robot setting.

</details>


### [613] [ReclAIm: A multi-agent framework for degradation-aware performance tuning of medical imaging AI](https://arxiv.org/abs/2510.17004)
*Eleftherios Tzanis,Michail E. Klontzas*

Main category: cs.MA

TL;DR: 本文提出ReclAIm框架，可自动监控、评估和微调医学图像分类模型，能在性能下降时自主调整，以用户友好方式实现医学影像AI模型自动化维护。


<details>
  <summary>Details</summary>
Motivation: 确保AI模型在临床实践中的长期可靠性，需要持续性能监控和性能下降时的纠正措施。

Method: 提出基于大语言模型核心的多智能体框架ReclAIm，通过自然语言交互运行，无需编程专业知识。

Result: ReclAIm在MRI、CT和X射线数据集上成功训练、评估并保持模型性能；检测到性能显著下降时，自主执行微调程序，大幅缩小性能差距，如在MRI InceptionV3性能下降-41.1%时，能将性能指标调整到初始结果的1.5%以内。

Conclusion: ReclAIm以用户友好和适应性强的方式实现医学影像AI模型的自动化、持续维护，便于在研究和临床环境中广泛应用。

Abstract: Ensuring the long-term reliability of AI models in clinical practice requires
continuous performance monitoring and corrective actions when degradation
occurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent
framework capable of autonomously monitoring, evaluating, and fine-tuning
medical image classification models. The system, built on a large language
model core, operates entirely through natural language interaction, eliminating
the need for programming expertise. ReclAIm successfully trains, evaluates, and
maintains consistent performance of models across MRI, CT, and X-ray datasets.
Once ReclAIm detects significant performance degradation, it autonomously
executes state-of-the-art fine-tuning procedures that substantially reduce the
performance gap. In cases with performance drops of up to -41.1% (MRI
InceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of
the initial model results. ReclAIm enables automated, continuous maintenance of
medical imaging AI models in a user-friendly and adaptable manner that
facilitates broader adoption in both research and clinical environments.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [614] [Rethinking Arrow--Debreu: A New Framework for Exchange, Time, and Uncertainty](https://arxiv.org/abs/2510.16003)
*Nizar Riane*

Main category: econ.TH

TL;DR: 本文通过有效交易视角重新审视Arrow - Debreu一般均衡框架，提出有效交易模型（ETM），分析其性质与均衡情况，质疑福利理论基础，提供替代经典一般均衡的框架。


<details>
  <summary>Details</summary>
Motivation: 重新审视Arrow - Debreu一般均衡框架，强调理论与可实现市场互动的区别，提供更贴合实际的均衡模型。

Method: 开发有效交易模型（ETM），基于双边可行性进行交易，分析价格 - 需求对应关系，证明纳什均衡存在性，将分析拓展到时间、不确定性和开放经济等情况。

Result: 均衡受交易约束、主观定价和分散谈判影响，而非普遍的市场出清条件；贷款资金和汇率内生产生；通过条件众数建模预期，体现有限理性和信息局限。

Conclusion: ETM为经典一般均衡提供了基于行为和结构的替代方案，在统一框架内衔接微观基础、货币动态和时间一致性。

Abstract: This paper revisits the Arrow-Debreu general equilibrium framework through
the lens of effective trade, emphasizing the distinction between theoretical
and realizable market interactions. We develop the Effective Trade Model (ETM),
where transactions arise from bilateral feasibility rather than aggregate
supply and demand desires. Within this framework, we establish the main
properties of the price-demand correspondence and prove the existence of Nash
equilibria, incorporating production, money, and network topology. The analysis
extends to time, uncertainty, and open economies, revealing how loanable funds
and exchange rates emerge endogenously. Our results show that equilibrium is
shaped by transaction constraints, subjective pricing, and decentralized
negotiation, rather than by universal market-clearing conditions, and thereby
call into question the foundations of welfare theory. Anticipation is modeled
via the conditional mode, capturing bounded rationality and information
limitations in contrast to the rational expectations hypothesis. The ETM thus
offers a behaviorally and structurally grounded alternative to classical
general equilibrium, bridging microfoundations, monetary dynamics, and temporal
consistency within a unified framework.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [615] [Lung Cancer Classification from CT Images Using ResNet](https://arxiv.org/abs/2510.16310)
*Olajumoke O. Adekunle,Joseph D. Akinyemi,Khadijat T. Ladoja,Olufade F. W. Onifade*

Main category: eess.IV

TL;DR: 本文提出基于预训练ResNet模型的深度学习方法对肺癌CT图像进行多分类，取得98.8%测试准确率。


<details>
  <summary>Details</summary>
Motivation: 现有肺癌CT图像自动分类系统预测效果不佳，且多为二分类，需改进多分类方法。

Method: 利用预训练ResNet模型，在LC25000数据集的15000张CT图像上训练、验证和测试，在ResNet架构上添加自定义层并微调超参数。

Result: 测试准确率达到98.8%，优于之前模型。

Conclusion: 该方法能有效提升肺癌CT图像多分类性能。

Abstract: Lung cancer, a malignancy originating in lung tissues, is commonly diagnosed
and classified using medical imaging techniques, particularly computed
tomography (CT). Despite the integration of machine learning and deep learning
methods, the predictive efficacy of automated systems for lung cancer
classification from CT images remains below the desired threshold for clinical
adoption. Existing research predominantly focuses on binary classification,
distinguishing between malignant and benign lung nodules. In this study, a
novel deep learning-based approach is introduced, aimed at an improved
multi-class classification, discerning various subtypes of lung cancer from CT
images. Leveraging a pre-trained ResNet model, lung tissue images were
classified into three distinct classes, two of which denote malignancy and one
benign. Employing a dataset comprising 15,000 lung CT images sourced from the
LC25000 histopathological images, the ResNet50 model was trained on 10,200
images, validated on 2,550 images, and tested on the remaining 2,250 images.
Through the incorporation of custom layers atop the ResNet architecture and
meticulous hyperparameter fine-tuning, a remarkable test accuracy of 98.8% was
recorded. This represents a notable enhancement over the performance of prior
models on the same dataset.

</details>


### [616] [Time-Embedded Algorithm Unrolling for Computational MRI](https://arxiv.org/abs/2510.16321)
*Junno Yun,Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 提出时间嵌入算法展开方案解决计算磁共振成像正则化最小二乘问题，实验证明有效且可提升现有方法重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有算法展开方法存在近端算子网络共享产生伪影模糊、使用不同网络参数多易过拟合的问题。

Method: 受近似消息传递和扩散模型启发，提出时间嵌入算法展开方案，将VAMP迭代相关近端操作和数据保真操作权重设为与时间相关。

Result: 在fastMRI数据集实验中有效减少伪影、减轻噪声放大，达最优性能，且可提升现有算法重建质量，不显著增加计算复杂度。

Conclusion: 提出的时间嵌入算法展开方案可行且有效，能改善磁共振成像重建效果。

Abstract: Algorithm unrolling methods have proven powerful for solving the regularized
least squares problem in computational magnetic resonance imaging (MRI). These
approaches unfold an iterative algorithm with a fixed number of iterations,
typically alternating between a neural network-based proximal operator for
regularization, a data fidelity operation and auxiliary updates with learnable
parameters. While the connection to optimization methods dictate that the
proximal operator network should be shared across unrolls, this can introduce
artifacts or blurring. Heuristically, practitioners have shown that using
distinct networks may be beneficial, but this significantly increases the
number of learnable parameters, making it challenging to prevent overfitting.
To address these shortcomings, by taking inspirations from proximal operators
with varying thresholds in approximate message passing (AMP) and the success of
time-embedding in diffusion models, we propose a time-embedded algorithm
unrolling scheme for inverse problems. Specifically, we introduce a novel
perspective on the iteration-dependent proximal operation in vector AMP (VAMP)
and the subsequent Onsager correction in the context of algorithm unrolling,
framing them as a time-embedded neural network. Similarly, the scalar weights
in the data fidelity operation and its associated Onsager correction are cast
as time-dependent learnable parameters. Our extensive experiments on the
fastMRI dataset, spanning various acceleration rates and datasets, demonstrate
that our method effectively reduces aliasing artifacts and mitigates noise
amplification, achieving state-of-the-art performance. Furthermore, we show
that our time-embedding strategy extends to existing algorithm unrolling
approaches, enhancing reconstruction quality without increasing the
computational complexity significantly.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [617] [Just-In-Time Piecewise-Linear Semantics for ReLU-type Networks](https://arxiv.org/abs/2510.17622)
*Hongyi Duan,Haoyang Liu,Jian'an Zhang,Fengrui Liu,Yiyi Wang*

Main category: cs.LO

TL;DR: 提出ReLU型网络的JIT PL语义，编译模型到带共享防护的受防护CPWL转换器，有多种优势且支持多应用。


<details>
  <summary>Details</summary>
Motivation: 为ReLU型网络提供合适的语义及编译方式，以解决相关分析和计算问题。

Method: 编译模型为带共享防护的受防护CPWL转换器，在特定条件下添加超平面，维护全局上下包络，使用预算分支限界法。

Result: 获得随时稳健性、完全细化单元上的精确性等特性，支持区域提取等多种应用，原型能返回证书或反例。

Conclusion: 该JIT PL语义有效，可用于ReLU型网络的分析和计算，有良好的性能和实用性。

Abstract: We present a JIT PL semantics for ReLU-type networks that compiles models
into a guarded CPWL transducer with shared guards. The system adds hyperplanes
only when operands are affine on the current cell, maintains global lower/upper
envelopes, and uses a budgeted branch-and-bound. We obtain anytime soundness,
exactness on fully refined cells, monotone progress, guard-linear complexity
(avoiding global $\binom{k}{2}$), dominance pruning, and decidability under
finite refinement. The shared carrier supports region extraction, decision
complexes, Jacobians, exact/certified Lipschitz, LP/SOCP robustness, and
maximal causal influence. A minimal prototype returns certificates or
counterexamples with cost proportional to visited subdomains.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [618] [Revealing Low-Dimensional Structure in 2D Richtmyer-Meshkov Instabilities via Parametric Reduced-Order Modeling](https://arxiv.org/abs/2510.16197)
*Daniel Messenger,Daniel Serino,Balu Nadiga,Marc Klasky*

Main category: physics.flu-dyn

TL;DR: 本文基于LaSDI算法提出二维RMI降阶模型，可高效参数化解空间，揭示RMI低维线性结构，对工程和理论研究有意义。


<details>
  <summary>Details</summary>
Motivation: 高效建模Richtmyer - Meshkov不稳定性（RMI）对高速燃烧和惯性约束聚变（ICF）等工程任务至关重要，控制RMI是ICF设计核心问题。

Method: 基于Latent Space Dynamics Identification（LaSDI）算法引入二维RMI降阶模型，利用动力学后期部分观测数据，用训练好的自动编码器近似对材料界面进行非线性变换。

Result: 该方法能高效参数化解空间，揭示RMI在合适非线性变换后具有低维线性动力学系统结构。

Conclusion: 此类降阶模型对处理RMI的下游工程任务有用，其低维表示为理论工作提供新方向。

Abstract: Efficient modeling of the Richtmyer-Meshkov instability (RMI) is essential to
many engineering tasks, including high-speed combustion and drive and capsule
geometry optimization in Inertial Confinement Fusion (ICF). In the latter, RMI
causes the ablator and fuel to mix, introducing cold spots into the fuel and
lowering performance; controlling RMI is thus a core ICF design concern. In
this work, we introduce a reduced-order model for two-dimensional RMI based on
the Latent Space Dynamics Identification (LaSDI) algorithm. We demonstrate the
efficacy of the proposed methodology in efficiently parametrizing the solution
space over a high-dimensional parameter vector consisting of material EOS
parameters and initial conditions known to affect RMI growth rates. Using only
late-time partial observations of the dynamics, we use our framework to not
only provide a highly efficient dynamic surrogate model, but to reveal that the
RMI exhibits the structure of a surprisingly low-dimensional and linear
dynamical system, into the nonlinear growth regime, after a suitable nonlinear
transformation is applied to the material interface, which we approximate as a
trained autoencoder. Our use of practical observables and fundamental
parameters suggests that such ROMs may be useful for downstream engineering
tasks which confront the RMI, while the low-dimensional representation suggests
a new direction for theoretical work.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [619] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 本文提出一种新的神经网络量化方法，可微分且能实现n位量化，在图像分类任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 以往量化工作存在不可微分、激活量化不足和难以实现n位量化等问题，本文旨在解决这些问题。

Method: 提出可微分的量化方法，并给出收敛性证明，实现n位对数量化。

Result: 在ImageNet数据集的图像分类任务中，仅进行权重量化时与全精度相比精度损失小于1%，15个epoch完成训练；权重和激活量化时精度与SOTA相当，推理成本略高但无需高精度乘法。

Conclusion: 所提量化方法有效，在计算和内存需求方面有优势，能在精度和效率间取得较好平衡。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [620] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: 提出ESCA框架，核心是SGClip模型，可改善多模态大语言模型在具身环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型训练管道缺乏像素级视觉内容和文本语义的细粒度、结构化对齐。

Method: 提出ESCA框架，其核心SGClip模型通过神经符号学习管道在87K+开放域视频上训练，无需人工标注场景图。

Result: SGClip在场景图生成和动作定位基准测试中表现出色，ESCA能提升开源和商业多模态大语言模型性能。

Conclusion: ESCA显著减少了代理感知错误，使开源模型超越专有基线。

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [621] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 本文提出利用街道CCTV流进行多缺陷检测和分割的综合管道，经实验验证系统能准确识别缺陷并生成连贯总结，最后讨论了系统扩展挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 智慧城市基础设施需及时检测缺陷保障安全，手动检测成本高且危险，现有自动系统有局限性，因此需要更好的解决方案。

Method: 利用YOLO系列目标检测器对街道CCTV流进行多缺陷检测和分割，将检测结果传递给视觉语言模型（VLM）进行场景感知总结，VLM生成JSON格式的结构化行动计划。

Result: 在公共数据集和捕获的CCTV片段上的实验表明，系统能准确识别各种缺陷并产生连贯总结。

Conclusion: 讨论了将系统扩展到全市部署的挑战和方向。

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [622] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 研究评估三种报告模式对影像分析影响，发现结构化报告提高效率，AI辅助结构化报告进一步提升诊断准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 评估结构化报告和人工智能对放射科医生影像分析的影响。

Method: 进行前瞻性研究，让读者分析床边胸片，采用眼动追踪系统，用广义线性混合模型和Bonferroni事后检验分析数据。

Result: AI辅助结构化报告诊断准确性更高，报告时间更短，眼动指标更优，是首选模式。

Conclusion: 结构化报告引导视觉关注影像提高效率，AI辅助结构化报告进一步提升诊断准确性和用户满意度。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [623] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 本文提出数据驱动框架分析和缓解图像分类中的交叉偏差，介绍IFEF框架识别偏差模式，提出BWA数据增强策略，实验显示其提升了准确率并减少公平性指标差异。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在不平衡数据集上训练产生的交叉偏差问题。

Method: 引入Intersectional Fairness Evaluation Framework (IFEF)识别偏差模式，提出Bias-Weighted Augmentation (BWA)数据增强策略。

Result: 在Open Images V7数据集实验表明，BWA使代表性不足的类别 - 环境交叉准确率最多提升24个百分点，公平性指标差异减少35%，多次独立运行的统计分析证实改进显著（p < 0.05）。

Conclusion: 该方法为分析和解决图像分类系统中的交叉偏差提供了可复制的途径。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [624] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: 介绍Aria Gen 2 Pilot Dataset (A2PD)，它是用Aria Gen 2眼镜捕获的以自我为中心的多模态开放数据集，逐步发布，初始版本含多种场景数据，公开可用。


<details>
  <summary>Details</summary>
Motivation: 提供一个基于先进Aria Gen 2眼镜的多模态开放数据集，便于相关研究和应用及时获取数据。

Method: 使用Aria Gen 2眼镜记录主要受试者Dia'ane及其朋友的日常活动，涵盖五种场景，提供原始传感器数据和机器感知算法输出数据。

Result: 数据集展示了设备对佩戴者、周围环境及两者交互的感知能力，在不同用户和条件下性能稳健。

Conclusion: A2PD数据集在projectaria.com上公开，同时在Project Aria Tools中提供开源工具和使用示例。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [625] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 提出基于通用引导的无训练方法进行3D资产外观迁移，实验表明该方法优于基线，还提出用GPT系统评估任务。


<details>
  <summary>Details</summary>
Motivation: 现有3D资产外观迁移方法在输入和外观对象几何差异大时失败，直接用3D生成模型效果不佳。

Method: 受通用引导启发，在预训练的整流流模型采样过程中周期性添加引导，实验两种引导类型。

Result: 方法成功迁移纹理和几何细节，优于基线；传统指标不适合评估，用GPT系统评估更客观。

Conclusion: 方法通用，可扩展到不同扩散模型和引导函数。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [626] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: 文章提出用结构化文本空间重建视觉刺激的PRISM模型，实验表明其优于现有方法，减少了感知损失。


<details>
  <summary>Details</summary>
Motivation: 理解大脑如何编码视觉信息是神经科学和机器学习的挑战，现有方法不清楚哪种潜在空间及组织方式最适合视觉刺激重建。

Method: 提出PRISM模型，将fMRI信号投影到结构化文本空间，包含以对象为中心的扩散模块和属性关系搜索模块。

Result: 在真实数据集上实验，框架优于现有方法，感知损失最多降低8%。

Conclusion: 强调使用结构化文本作为中间空间连接fMRI信号和图像重建的重要性。

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [627] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: 提出OpenLVLM - MIA基准，解决评估针对大视觉语言模型成员推理攻击问题，实验显示无偏条件下攻击方法性能随机，为隐私保护技术提供基础。


<details>
  <summary>Details</summary>
Motivation: 先前针对大视觉语言模型成员推理攻击评估结果可能源于检测数据集构建的分布偏差，而非真正识别成员状态，需解决此问题。

Method: 引入包含6000张图像的受控基准，平衡成员和非成员样本分布，并提供三个不同训练阶段的真实成员标签。

Result: 使用OpenLVLM - MIA的实验表明，无偏条件下最先进的成员推理攻击方法性能趋于随机。

Conclusion: OpenLVLM - MIA明确了当前大视觉语言模型成员推理攻击研究的局限性，为开发更强的隐私保护技术提供坚实基础。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [628] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 提出包含3000个白内障手术视频的数据集，有四层注释，经基准实验验证，还有域适应基线，数据可获取。


<details>
  <summary>Details</summary>
Motivation: 现有白内障手术资源缺乏训练通用深度学习模型所需的多样性和注释深度。

Method: 收集两个手术中心、不同经验医生的3000个超声乳化白内障手术视频，添加四层注释，进行基准实验，建立域适应基线。

Result: 数据集经基准实验验证技术质量，建立了域适应基线。

Conclusion: 该数据集能为计算机辅助手术系统的深度学习模型训练提供支持。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [629] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 针对AR/VR设备在边缘设备部署深度学习模型的挑战，设计轻量级框架，结合多种方法提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着AR/VR设备普及，边缘设备部署深度学习模型需实时推理、低功耗和低延迟，框架设计要平衡效率和性能。

Method: 设计采用编解码器架构的轻量级框架，在ResNet - 18骨干上应用稀疏卷积，提出SPLite解码器，应用量化感知训练。

Result: 实现端到端效率提升42%，解码器帧率提升3.1倍，内存使用减少，树莓派5 CPU速度提升2.98倍。

Conclusion: 该方法在复合基准数据集上准确性与先进方法相当，显著提高计算效率。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [630] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: 提出SSL4RL框架利用自监督学习任务为基于强化学习的视觉语言模型微调提供可验证奖励，实验表明其在多个基准测试中表现良好，还适用于图学习。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型难以充分利用视觉证据，强化学习应用于视觉语言模型缺乏可扩展和可靠的奖励机制。

Method: 提出SSL4RL框架，将自监督学习目标转化为密集、自动的奖励信号，用于基于强化学习的微调。

Result: SSL4RL在视觉中心和视觉语言推理基准测试中显著提高性能，通过消融实验确定影响其有效性的关键因素，应用于图学习也有显著增益。

Conclusion: SSL4RL为使用可验证的自监督目标对齐多模态模型建立了通用有效的范式。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [631] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出可解释的深度伪造视频检测（EDVD）任务，设计EDVD - LLaMA多模态大语言模型推理框架，经实验验证性能出色，代码和数据集将公开。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造视频检测方法原理缺乏透明度、泛化能力不足，需能识别伪造内容并提供可验证推理解释的检测器。

Method: 提出EDVD任务，设计EDVD - LLaMA框架；采用ST - SIT提取和融合特征；构建Fg - MCoT机制；构建ER - FF++set数据集进行双重监督。

Result: EDVD - LLaMA在检测准确率、可解释性、跨伪造方法和跨数据集场景处理能力上表现出色。

Conclusion: 与以往方法相比，EDVD - LLaMA提供了更具可解释性和优越性的解决方案。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [632] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: 提出基于图和图注意力网络自动编码器的图像分类与检索方法，以代表为中心进行操作并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 提出有效的图像分类与检索方法。

Method: 构建以代表为中心的方法，利用图表示图像及其关系，用GAT突出特征和关系，让自动编码器构建上下文感知的潜在表示，获取类别代表进行分类和检索。

Result: 通过GAT自动编码器和标准基于特征的技术实验验证了以代表为中心方法的有效性。

Conclusion: 所提出的以代表为中心的图像分类与检索方法是有效的。

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [633] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: 提出READ微调方法增强视觉语言模型组合推理能力，应用于CLIP及其变体在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在组合推理方面存在不足，文本编码器倾向关注单个单词而非关系，对比训练强化了这一局限。

Method: 在对比学习中添加两个辅助目标：标记级重建目标和句子级对齐目标。

Result: READ - CLIP在五个主要组合推理基准测试中达到了最先进的性能，比最强的传统微调基线高出4.1%；应用于CLIP变体也提升了性能。

Conclusion: 提出的重建和对齐目标具有互补优势，分别帮助编码器捕捉单词关系和确保释义的一致表示。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [634] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: 提出Region - aware Dynamic Aggregation and Excitation框架(GaitRDAE)用于步态识别，在多个基准数据集上达最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的步态识别方法用预定义区域进行时间建模，难以对动态变化的运动区域建模并适应其特定模式。

Method: 引入GaitRDAE框架，含Region - aware Dynamic Aggregation (RDA)和Region - aware Dynamic Excitation (RDE)两个核心模块，自动搜索运动区域、分配自适应时间尺度并应用对应注意力。

Result: GaitRDAE在多个基准数据集上达到了最先进的性能。

Conclusion: GaitRDAE框架有效解决了现有步态识别方法在处理动态运动区域时的问题，有很好的应用效果。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [635] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本文针对大视觉语言模型（LVLMs）的对象幻觉问题，追踪到视觉编码器的三个关键问题，并提出无训练框架SHIELD缓解幻觉，实验证明其有效性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: LVLMs存在对象幻觉问题，以往工作聚焦LLM组件，本文要追踪到视觉编码器并解决。

Method: 提出无训练框架SHIELD，通过重新加权视觉标记、引入噪声衍生标记、应用对抗攻击与对比解码三种策略缓解幻觉。

Result: SHIELD能有效缓解不同基准和LVLM系列的对象幻觉，在通用LVLM基准上表现出色。

Conclusion: SHIELD能有效缓解对象幻觉，具有广泛适用性，代码将发布。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [636] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: 本文提出用于实时医学图像分析的深度学习框架，集成先进神经网络架构与优化策略，在多平台部署，实验表现优异，能加速诊断流程。


<details>
  <summary>Details</summary>
Motivation: 传统图像处理技术在实时临床应用中缺乏精度、鲁棒性和速度，本文旨在克服这些局限，提高诊断准确性和计算效率。

Method: 提出深度学习框架，集成U - Net、EfficientNet和Transformer等模型，采用模型剪枝、量化和GPU加速等实时优化策略，支持多平台部署。

Result: 在公共基准数据集上实验表现达到先进水平，分类准确率超92%，分割Dice分数超91%，推理时间低于80毫秒，有可视化解释工具。

Conclusion: 该框架可加速诊断流程，减轻临床医生工作量，支持在医疗环境中集成可信AI。

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [637] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 本文提出使用检索增强生成方式结合大语言模型和3D场景图进行自然语言接地，在指令跟随和场景问答任务中效果好。


<details>
  <summary>Details</summary>
Motivation: 为让机器人理解和响应自然语言输入，需将自然语言与机器人的世界表征连接，解决现有方法在处理大而丰富的3D场景图时的不足。

Method: 使用检索增强生成方式，将3D场景图编码到图数据库，提供Cypher查询语言接口让大语言模型检索相关数据。

Result: 在本地和云端模型上，使用Cypher作为3D场景图接口在大而丰富的图上扩展性更好，提升了接地语言任务性能，减少场景图内容的token数量。

Conclusion: 使用Cypher作为接口能有效解决大而丰富3D场景图与大语言模型结合的问题，提升自然语言接地效果。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [638] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文指出视觉语言模型空间推理存在的瓶颈，提出MSSR框架，实验证明该方法提升了准确率并达到SOTA，还能产生可解释推理路径。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在空间推理中因2D预训练和冗余3D信息导致的两大瓶颈。

Method: 构建最小充分集（MSS），引入MSSR双智能体框架，感知智能体提取信息，推理智能体迭代优化信息。

Result: 该方法显著提高准确率，在两个具有挑战性的基准测试中达到SOTA，且框架能产生可解释推理路径。

Conclusion: 显式追求充分性和最小性的方法有效，框架为未来模型提供高质量训练数据。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [639] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: 提出基于一致性模型的一步式人体运动预测框架HumanCM，实验显示其精度与SOTA扩散模型相当或更优，且推理步骤大幅减少。


<details>
  <summary>Details</summary>
Motivation: 改进现有基于扩散方法需多步去噪的不足，实现高效的人体运动预测。

Method: 构建基于一致性模型的HumanCM框架，采用基于Transformer的时空架构和时间嵌入来建模长距离依赖和保持运动连贯性。

Result: 在Human3.6M和HumanEva - I上实验表明，HumanCM精度与SOTA扩散模型相当或更优，推理步骤最多减少两个数量级。

Conclusion: HumanCM能高效实现人体运动预测，在精度和推理效率上有优势。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [640] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 本文提出新框架解决3D大语言模型场景问答难题，开发数据集并经实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有3D大语言模型研究难以实现场景问答，原因是类人场景 - 对象推理机制探索不足。

Method: 提出3D场景下的链式思维推理方法（SCENECOT），将复杂推理任务拆解，构建视觉线索；开发大规模数据集SCENECOT - 185K。

Result: 在多个复杂3D场景推理基准测试中，新框架表现良好，问答一致性高。

Conclusion: 这是链式思维推理在3D场景理解中的首次成功应用，有扩展到更广泛场景的潜力。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [641] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 提出Region in Context框架用于文本条件图像编辑，通过多级语义对齐使编辑更连贯。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法孤立处理区域，未考虑局部对整体视觉和语义构成的贡献，导致编辑不一致、过渡不自然等问题。

Method: 提出Region in Context框架，引入双级指导机制，结合全图像上下文表示区域并与区域描述对齐，同时将整个图像与大视觉语言模型生成的场景描述匹配。

Result: 实验表明该方法产生更连贯、符合指令的结果。

Conclusion: 所提出的框架能有效解决现有图像编辑方法的问题，实现精确和谐的图像编辑。

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [642] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: 本文提出用于X光报告生成的新框架EMRRG，通过参数高效方法微调预训练Mamba网络，在基准数据集上取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有MRG模型多依赖大语言模型，对预训练视觉基础模型和先进微调技术探索有限，Transformer架构主导，非Transformer架构如Mamba网络未充分研究。

Method: 提出EMRRG框架，用参数高效方法微调预训练Mamba网络，将X光图像分块、标记化，用基于SSM的视觉骨干提取特征，Partial LoRA效果最佳，用带混合解码器的大语言模型生成报告。

Result: 在三个广泛使用的基准数据集上的大量实验验证了所提策略的有效性。

Conclusion: 所提用于X光医学报告生成的策略有效，代码将开源。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [643] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 本文提出无训练视频理解框架，结合预训练VLM和机器学习算法，将视频理解转化为时空聚类问题，实现视频内容零样本自动结构分析。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型在静态图像上的零样本推理能力未充分应用于视频领域，传统视频理解模型训练成本高、可扩展性有限。

Method: 将视频理解重构为高维语义特征空间的自监督时空聚类问题，用预训练VLM的视觉编码器转换视频流，用KTS分割特征流，进行无监督聚类，选关键帧并用VLM生成文本描述。

Result: 自动生成视频内容的结构化多模态摘要。

Conclusion: 该方法为视频内容零样本自动结构分析提供有效、可解释和模型无关的途径。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [644] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 引入ReefNet珊瑚礁图像数据集，提出两种评估设置，分析分类性能，为珊瑚礁监测提供挑战基准并将发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 因人为压力导致珊瑚礁快速减少，急需可扩展的自动化监测。

Method: 引入ReefNet数据集，提出内部源和跨源两种评估设置，分析监督和零样本分类性能。

Result: 监督式内部源性能有前景，但跨域性能大幅下降，零样本模型整体性能低。

Conclusion: 为领域泛化和细粒度珊瑚分类提供挑战基准，发布相关资源以推动珊瑚礁监测和保护。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [645] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: 提出轻量级语义分割框架ArmFormer用于武器检测，在精度、效率上表现佳，适合边缘设备。


<details>
  <summary>Details</summary>
Motivation: 武器暴力威胁升级，传统武器检测方法定位粗，现有语义分割模型难以兼顾精度和效率，不适用于边缘部署。

Method: 将CBAM与MixVisionTransformer架构结合，采用CBAM增强编码器骨干和集成注意力的汉堡解码器实现多类武器分割。

Result: ArmFormer mIoU达80.64%、mFscore达89.13%，推理速度82.26 FPS，FLOPs为4.886G、参数3.66M，优于需多48倍计算量的重量级模型。

Conclusion: ArmFormer是便携式安全摄像头、监控无人机和分布式安全基础设施中嵌入式AI加速器部署的最佳解决方案。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [646] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文对医学图像分析中的基础模型进行全面结构化分析，分类研究、做元分析，讨论挑战与解决方案并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 医学图像领域基础模型研究分散，缺乏统一综合分析，本文旨在填补这一空白。

Method: 基于架构、训练策略和下游临床任务将研究分为仅视觉和视觉 - 语言基础模型，进行定量元分析。

Result: 分析了基础模型在医学图像分析中的架构、训练范式和临床应用等方面情况，明确了面临的挑战及新兴解决方案。

Conclusion: 指出未来研究方向以提升基础模型的鲁棒性、可解释性和临床整合，加速其在现实医疗实践中的转化。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [647] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出Di - Bregman框架加速扩散模型蒸馏，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型计算成本高，现有蒸馏目标缺乏统一理论基础。

Method: 提出Di - Bregman框架，将扩散蒸馏表述为基于Bregman散度的密度比匹配。

Result: 在CIFAR - 10和文本到图像生成实验中，Di - Bregman在一步FID上优于反向KL蒸馏，且与教师模型相比保持高视觉保真度。

Conclusion: Bregman密度比匹配是实现高效一步扩散生成的实用且有理论依据的途径。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [648] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: 提出云边协同框架DiffusionX用于高效多轮、基于提示的图像生成，实验显示能减少生成时间且保证图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型生成过程计算量大，用户迭代优化提示词会增加延迟和云资源负担。

Method: 提出DiffusionX框架，设备端轻量级模型快速生成预览图与用户交互，云端高容量模型在提示词确定后进行最终细化，还引入噪声水平预测器平衡计算负载。

Result: DiffusionX较Stable Diffusion v1.5平均生成时间减少15.8%，图像质量相当；比Tiny - SD仅慢0.9%，但图像质量显著提升。

Conclusion: DiffusionX具有高效性和可扩展性，开销极小。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [649] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 提出CARE框架用于事件触发传感器流的ADL识别，在三个数据集达SOTA，对传感器故障和布局变化有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有ADL识别方法存在表示层面局限，简单融合未充分利用序列和图像表示优势。

Method: 提出CARE框架，通过SICA联合优化表示学习，用交叉熵进行分类，集成时间感知序列编码、空间感知图像表示和联合对比分类目标。

Result: 在三个CASAS数据集上达到SOTA性能，米兰89.8%、开罗88.9%、京都73.3%，对传感器故障和布局变化有鲁棒性。

Conclusion: CARE框架有潜力用于智能家居可靠的ADL识别。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [650] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: 本文探讨多模态语言模型（MLLM）训练策略对视觉编码器的影响，发现强化学习（RL）优于监督微调（SFT），并提出PIVOT方法提升视觉编码器性能。


<details>
  <summary>Details</summary>
Motivation: MLLM研究多关注大语言模型主干，忽视视觉编码器，且缺乏对训练策略如何重塑视觉编码器和MLLM的分析。

Method: 先对比RL和SFT在视觉相关VQA基准上的表现，再通过多种实验对视觉编码器进行分析，最后提出PIVOT方法。

Result: 训练策略影响MLLM下游任务结果和视觉表征，RL产生更强、定位更精确的视觉表征；PIVOT训练的视觉编码器性能优且计算成本低。

Conclusion: 研究为提升MLLM视觉主干提供了有效且高效的途径。

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [651] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: 本文介绍iWatchRoadv2平台用于实时检测道路坑洼、地理标记和可视化，通过数据集微调模型，有智能治理功能，可实现数据驱动的道路维护管理。


<details>
  <summary>Details</summary>
Motivation: 印度道路坑洼带来安全和维护挑战，需要自动化平台解决。

Method: 整理7000多个仪表盘摄像头帧数据集微调Ultralytics YOLO模型；同步视频时间戳和GPS日志定位坑洼；后端数据库管理元数据；设智能治理功能和直观网页界面。

Result: 实现了从检测到修复验证的完整坑洼监测生命周期自动化，可进行证据驱动的规划、预算分配和质量评估。

Conclusion: iWatchRoadv2是经济高效且可扩展的解决方案，能实现数据驱动的智慧城市管理、透明治理和道路基础设施维护的可持续改进。

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [652] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 本文针对视频推理中LMMs计算开销大、思维控制机制有限问题，提出V - Reason方法，在推理时直接调整模型行为，实验显示该方法有显著提升和效率优势。


<details>
  <summary>Details</summary>
Motivation: 解决视频推理使用LMMs时存在的计算开销大以及推理模型思维控制机制有限的问题。

Method: 以模型输出熵为信号，发现高质量模型推理过程特点，提出V - Reason方法，在推理时通过基于熵的目标对可训练小控制器进行优化步骤来调整LMM的值缓存。

Result: 在多个视频推理数据集上，相比基础指令调优模型有显著提升，与RL训练模型平均准确率差距缩小至0.6%以内，输出令牌减少58.6%。

Conclusion: 所提方法无需训练，能有效提升模型在视频推理中的性能，同时大幅提高效率。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [653] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: 提出视觉提示初始化策略VIPAMIN，提升自监督模型适应性，在多任务和不同数据集规模下表现出色。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型时代，全微调资源消耗大，现有视觉提示调整方法存在不足，尤其是用于自监督骨干网络时，在挑战性任务和数据稀缺场景下问题更明显。

Method: 提出VIPAMIN策略，通过使提示与嵌入空间中语义信息丰富区域对齐，以及注入预训练子空间之外的新表示方向，增强自监督模型的适应性。

Result: VIPAMIN仅需一次前向传播和轻量级操作，就能在不同任务和数据集规模下持续提升性能，创造视觉提示调整新的最优水平。

Conclusion: VIPAMIN是一种有效的视觉提示初始化策略，能提升自监督模型在下游任务中的适应能力。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [654] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 现有文本到图像扩散模型生成OOD样本有局限，提出GOOD框架，结合双级指导生成多样OOD样本，引入统一OOD分数，实验证明其能提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在生成OOD样本时存在语义不稳定和转移多样性不足的问题，限制了对现实OOD的泛化能力。

Method: 提出GOOD框架，采用双级指导：图像级基于对数分区梯度降低输入似然，特征级基于k - NN距离在特征稀疏区域采样，并引入统一OOD分数。

Result: 进行了全面的定量和定性分析，表明用GOOD生成的样本训练可显著提升OOD检测性能。

Conclusion: GOOD框架能有效解决现有方法的问题，提升OOD检测性能。

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [655] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: 本文提出GACO - CAD两阶段后训练框架，用于从单张图像生成可编辑的参数化CAD模型，在数据集上表现达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型从2D图像推断3D几何形状的空间推理能力有限，难以准确生成CAD模型。

Method: 提出两阶段后训练框架GACO - CAD，在监督微调阶段利用深度和表面法线图作为几何先验；在强化学习阶段引入组长度奖励并采用动态加权策略稳定训练。

Result: 在DeepCAD和Fusion360数据集上，GACO - CAD在代码有效性、几何准确性和建模简洁性方面始终优于现有方法。

Conclusion: GACO - CAD能有效提升生成CAD模型的几何准确性，促进使用更简洁的建模程序，在相同MLLM骨干下达到SOTA性能。

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [656] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 本文针对浮游生物自动化识别模型在实际部署中面临的分布偏移问题，设计了OoD基准测试，评估22种检测方法，发现ViM方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 浮游生物自动化识别模型因训练和测试数据分布偏移面临挑战，且该领域缺乏最新计算机视觉进展的系统整合和统一基准。

Method: 基于DYB - PlanktonNet数据集设计一系列模拟不同分布偏移场景的OoD基准测试，评估22种OoD检测方法。

Result: ViM方法在构建的基准测试中显著优于其他方法，在Far - OoD场景关键指标有大幅提升。

Conclusion: 该评估为浮游生物自动化识别算法选择提供可靠参考，为浮游生物OoD检测研究奠定基础，是该领域首次大规模系统评估分析。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [657] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 随着视觉语言模型发展，现有视觉令牌剪枝方法忽视文本提示，本文提出零样本方法，平衡任务相关性和信息多样性，实验显示性能佳且减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型处理大输入时存在视觉令牌冗余和推理成本高问题，且现有剪枝方法忽视文本提示，无法突出任务相关性。

Method: 提出零样本方法，从提示感知视角将视觉令牌剪枝建模为任务相关性和信息多样性的平衡，采用分层方法先选核心任务相关令牌，再补充多样性令牌。

Result: 在多个模型和基准测试中，即使剪枝90%的令牌，性能与现有最佳方法相当或更优，仅最小精度损失，同时显著减少GPU内存占用和推理延迟。

Conclusion: 所提方法有效解决视觉语言模型视觉令牌冗余和推理成本高问题，在性能和资源消耗上表现良好。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [658] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 本文展示了如何将通用视觉模型SAM应用于追踪孟加拉国河流侵蚀，创建新数据集并微调模型，性能超传统方法，为政策制定者提供监测工具。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国河流侵蚀严重，追踪这一灾难对人力分析而言是艰巨任务，因此需要借助技术手段进行有效追踪。

Method: 组装新数据集，先进行简单颜色通道分析对土地和水进行粗略分割，再微调SAM的掩码解码器以识别河岸侵蚀特征。

Result: 模型的平均交并比达86.30%，Dice分数为92.60%，性能显著超越传统方法和现成深度学习模型。

Conclusion: 研究提供了消失定居点的标注数据集、专用AI模型和量化土地损失的方法，为政策制定者和灾害管理机构提供监测和保护工具。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [659] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 本文针对《无畏契约》，基于游戏小地图信息构建回合结果预测模型，结合战术特征提升预测准确率，初步结果显示含战术事件标签数据集训练的模型表现更佳。


<details>
  <summary>Details</summary>
Motivation: 以往电竞比赛结果预测多基于比赛日志和统计信息，本文针对需复杂策略的《无畏契约》，欲通过分析比赛画面小地图信息构建回合结果预测模型。

Method: 基于视频识别模型TimeSformer，结合从小地图信息中提取的战术特征（如角色位置信息和游戏内事件）来提升预测准确率。

Result: 含战术事件标签数据集训练的模型预测准确率约81%，从回合中期起显著优于仅用小地图信息训练的模型。

Conclusion: 利用比赛画面的战术特征对《无畏契约》回合结果预测非常有效。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [660] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 介绍针对病理基础模型的通用可迁移对抗扰动（UTAP），揭示其脆弱性，评估其对模型性能影响并指出对模型鲁棒性评估和防御机制发展的意义。


<details>
  <summary>Details</summary>
Motivation: 揭示病理基础模型能力的关键脆弱性，为模型鲁棒性评估建立基准。

Method: 使用深度学习优化得到UTAP，将固定弱噪声模式添加到病理图像中，系统地评估其对多种病理基础模型的影响。

Result: UTAP使下游任务性能下降，具有通用性和可迁移性，能在多个数据集上让先进病理基础模型性能显著下降。

Conclusion: UTAP对多种病理基础模型构成广泛威胁，凸显了改进防御机制和进行对抗训练以确保AI在病理学安全可靠部署的必要性。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [661] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: 本文引入QV - M²数据集和新评估指标用于多时刻检索，提出FlashMMR框架，实验表明QV - M²是有效基准，FlashMMR是强基线。


<details>
  <summary>Details</summary>
Motivation: 现有单时刻检索方法和数据集在处理多时刻检索场景时不足，需适应真实应用。

Method: 引入QV - M²数据集和新评估指标，提出FlashMMR框架，含多时刻后验证模块，对候选片段进行约束调整和验证。

Result: QV - M²是训练和评估MMR模型的有效基准，FlashMMR相比先前SOTA方法在多个指标有提升。

Conclusion: 提出的基准和方法为更真实和有挑战性的视频时间定位研究奠定基础。

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [662] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 本文分析对比学习直接应用于领域泛化（DG）效果不佳的原因，提出域连接对比学习（DCCL）范式，实验验证其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 训练和测试样本间的分布偏移影响模型泛化性能，对比学习直接应用于DG效果不佳，需解决该问题。

Method: 提出DCCL范式，数据侧引入更激进的数据增强和跨域正样本，模型侧提出模型锚定并结合生成变换损失。

Result: 在五个标准DG基准上的实验表明，DCCL即使无领域监督也优于现有基线。

Conclusion: DCCL能有效增强跨域概念连接，获得可泛化表示，提升DG性能。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [663] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: 现有公开数据集问题阻碍VLM发展，提出FineVision数据集，经处理后效果好并开源。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型发展受不一致和受污染的公开数据集阻碍的问题。

Method: 通过半自动化、人工参与的流程统一200多个数据源到185个子集，进行去重和去污染处理，涵盖特定任务并验证。

Result: 在广泛评估中，基于FineVision训练的模型表现优于现有公开混合数据集训练的模型。

Conclusion: 规模、数据清理以及人工监督下的平衡自动化对VLM有益，开源数据集和工具以加速研究。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [664] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: 提出基于扩散且有字符级引导的框架CharDiff用于恢复和识别严重退化车牌图像，实验表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 车牌图像恢复有重要意义，需有效方法恢复和识别严重退化车牌图像。

Method: 提出CharDiff框架，利用外部分割和OCR模块提取细粒度字符级先验，引入CHARM模块进行精确引导。

Result: 在恢复质量和识别准确率上显著优于基线模型，在Roboflow - LP数据集上CER相对降低28%。

Conclusion: 结构化字符引导调节有效增强基于扩散的车牌恢复和识别在实际部署场景中的鲁棒性。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [665] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 生成式图像超分辨率模型易产生伪影，提出用新数据集训练回归器检测显著伪影并公开数据代码。


<details>
  <summary>Details</summary>
Motivation: 随着超分辨率模型容量增加，产生的伪影对视觉质量影响不同，应按对人眼的显著程度区分伪影，而非统一视为二元缺陷。

Method: 创建包含1302个伪影示例的新数据集，每个伪影配有众包显著度分数，基于此数据集训练轻量级回归器。

Result: 训练的轻量级回归器生成空间显著度热图，在检测显著伪影方面优于现有方法。

Conclusion: 发布数据集和代码，便于进行考虑显著度的超分辨率伪影评估和缓解。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [666] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: 提出灵活、自适应且高效的4D占用世界模型SparseWorld，实验证明其在多任务上达最优。


<details>
  <summary>Details</summary>
Motivation: 现有占用世界模型依赖静态固定嵌入或网格，感知灵活性受限，且与现实场景动态连续性存在潜在偏差。

Method: 提出由稀疏动态查询驱动的SparseWorld模型，包括Range - Adaptive Perception模块、State - Conditioned Forecasting模块，以及Temporal - Aware Self - Scheduling训练策略。

Result: SparseWorld在感知、预测和规划任务上取得了最先进的性能，可视化和消融实验验证了其灵活性、适应性和效率优势。

Conclusion: SparseWorld是一个有效的4D占用世界模型，代码开源。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [667] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 提出基于评分准则引导、伪标签提示的视频摘要框架，在SumMe和TVSum上取得好成绩，证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有视频摘要方法存在标注成本高、泛化性差、依赖手工提示模板等问题，需新方法解决。

Method: 将少量真实标注转换为高置信度伪标签，聚合为结构化、适应数据集的评分准则，推理时不同片段采用不同评分方式。

Result: 在SumMe和TVSum上F1分数分别达57.58和63.05，超无监督和之前零样本基线，接近有监督性能。

Conclusion: 评分准则引导的伪标签有效稳定基于大语言模型的评分，建立通用、可解释的零样本视频摘要范式。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [668] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 提出大规模视频生成模型训练框架，模型MUG - V 10B表现出色，开源完整代码。


<details>
  <summary>Details</summary>
Motivation: 解决大规模视频生成模型训练中跨模态文本 - 视频对齐、长序列和复杂时空依赖等挑战。

Method: 优化数据处理、模型架构、训练策略和基础设施四个方面。

Result: 模型MUG - V 10B整体表现与现有先进视频生成器相当，在电商视频生成任务上超越开源基线。

Conclusion: 首次开源利用Megatron - Core实现高效训练和近线性多节点扩展的大规模视频生成训练代码。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [669] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 提出MambaX - Net用于前列腺癌主动监测的前列腺分割，在纵向数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割模型不适用于纵向主动监测分析，因多时间点和专家标签稀缺难以有效微调。

Method: 提出MambaX - Net半监督双扫描3D分割架构，引入Mamba增强交叉注意力模块和形状提取模块，采用半监督自训练策略。

Result: MambaX - Net在纵向主动监测数据集上显著优于U - Net和Transformer模型，在有限和嘈杂数据下也能实现出色的前列腺区域分割。

Conclusion: MambaX - Net是解决纵向主动监测中前列腺分割问题的有效方法。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [670] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 本文提出公平感知的深度伪造检测框架，结合时间特征学习和人口统计感知数据增强，实验证明其在公平性和准确性间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法存在偏差、缺乏透明度且无法捕捉时间信息，导致不同人群决策有偏差和结果不可靠。

Method: 提出公平感知的检测框架，利用基于序列的聚类进行时间建模和概念提取，引入人口统计感知的数据增强方法平衡少数群体并进行频域变换。

Result: 在多个数据集上使用先进架构进行实验，证明该方法在公平性和准确性间取得最佳平衡。

Conclusion: 所提出方法有效，能在公平性和准确性上优于现有技术。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [671] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: 本文提出多任务学习框架M2H用于单目图像多任务感知，在多数据集表现出色且计算高效。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备部署实时空间感知需要高效多任务模型，利用互补任务信息并减少计算开销。

Method: 引入基于窗口的跨任务注意力模块，基于轻量级ViT的DINOv2骨干网络构建M2H。

Result: M2H在NYUDv2、Hypersim、Cityscapes数据集上超越现有模型，在笔记本硬件上保持计算效率，在真实数据上得到验证。

Conclusion: M2H是支持动态环境3D场景图构建的单目空间感知系统的基础，在空间感知任务中具有实用性。

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [672] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: 本文引入CaMiT数据集，支持监督和自监督学习，提出时间增量分类设置，评估两种策略提升时间鲁棒性，探索时间感知图像生成，为细粒度视觉识别和生成提供基准。


<details>
  <summary>Details</summary>
Motivation: 让AI系统适应不断变化的视觉环境，尤其是对象外观随时间变化的领域。

Method: 引入CaMiT数据集，提出时间增量分类设置，评估时间增量预训练和时间增量分类器学习两种策略，探索时间感知图像生成。

Result: 静态预训练在域内数据上表现良好但跨年份测试精度下降，两种策略提升了时间鲁棒性，时间感知图像生成输出更真实。

Conclusion: CaMiT为细粒度视觉识别和生成中的时间适应研究提供了丰富的基准。

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [673] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 提出与标准视频大语言模型兼容的免训练方法，在流式视频基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在流式场景下处理长视频并及时响应问题存在挑战。

Method: 采用三个关键概念，包括基于大语言模型选择视觉令牌、循环处理过去选定令牌、基于字幕问答。

Result: 在流式视频基准测试中达到了最先进的性能。

Conclusion: 该方法在效率和有效性之间取得了平衡。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [674] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: 本文对比两种节俭联邦学习暴力检测方法，评估性能与能耗，支持混合模型并提供可复现基线。


<details>
  <summary>Details</summary>
Motivation: 研究节俭联邦学习的暴力检测方法，强调能源效率和环境指标。

Method: 比较零样本和联邦微调视觉语言模型、个性化训练紧凑型3D卷积神经网络两种策略，以LLaVA - 7B和6580万参数CNN3D为例评估。

Result: 两种方法准确率超90%，CNN3D在ROC AUC和log损失上稍优且能耗低，VLM适合上下文推理和多模态推理。

Conclusion: 支持使用轻量级CNN进行常规分类，复杂场景激活VLM的混合模型，为视频监控AI提供可复现基线。

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings. Both approaches exceed 90% accuracy.
CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and
log loss, while using less energy. VLMs remain favorable for contextual
reasoning and multimodal inference. We quantify energy and CO$_2$ emissions
across training and inference, and analyze sustainability trade-offs for
deployment. To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics. These findings
support a hybrid model: lightweight CNNs for routine classification, with
selective VLM activation for complex or descriptive scenarios. The resulting
framework offers a reproducible baseline for responsible, resource-aware AI in
video surveillance, with extensions toward real-time, multimodal, and
lifecycle-aware systems.

</details>


### [675] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: 介绍PICABench评估图像编辑物理真实感，提出PICAEval协议，构建PICA - 100K数据集，评估主流模型发现物理真实感仍待探索。


<details>
  <summary>Details</summary>
Motivation: 现有模型和基准主要关注指令完成，忽略图像编辑中的物理效果，需评估当前距物理真实图像编辑的距离。

Method: 引入PICABench评估常见编辑操作在八个子维度的物理真实感，提出PICAEval评估协议，从视频学习物理知识构建PICA - 100K数据集。

Result: 评估主流模型后发现物理真实感仍是具有很大探索空间的挑战性问题。

Conclusion: 希望所提出的基准和解决方案能为从简单内容编辑迈向物理一致的真实感编辑的未来工作奠定基础。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [676] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: 提出IC - MoE模型用于医学图像分割，实验表明其优于其他SOTA模型，能补充高级特征和保留预训练结构完整性。


<details>
  <summary>Details</summary>
Motivation: 现有自然图像分割基础模型自适应微调方法在医学图像分割任务中存在高级特征表示不足和破坏预训练权重结构完整性的问题。

Method: 构建基础、语义和自适应专家，采用像素概率自适应投票策略；提出语义引导对比学习方法。

Result: 在三个公共医学图像分割数据集上的实验表明IC - MoE优于其他SOTA模型。

Conclusion: IC - MoE能有效补充医学图像分割基础模型的高级特征和保留预训练结构完整性，具有良好的泛化性。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [677] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 本文针对文本到图像人物检索（TIPR）中模态异质性和多语言应用问题，提出多语言TIPR基准和Bi - IRRA框架，取得新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有TIPR方法存在忽视细粒度差异、需先验信息及英语中心等问题，限制多语言应用，因此开展多语言TIPR任务研究。

Method: 开发多语言TIPR基准，利用大语言模型翻译并结合领域知识优化；提出Bi - IRRA框架，包含双向隐式关系推理模块和多维全局对齐模块。

Result: 在所有多语言TIPR数据集上取得新的SOTA结果。

Conclusion: 提出的方法有效解决了现有TIPR方法在多语言应用中的问题，具有较好的性能。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [678] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 提出两阶段方法检测帕金森病，用分块策略克服数据和鲁棒性问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 相关工作存在数据集不足和处理未知患者数据时鲁棒性差的问题，需新方法检测帕金森病。

Method: 两阶段方法，先按绘图类型分类，再提取特征检测疾病；用分块策略，最后用集成方法合并决策。

Result: 在NewHandPD数据集上，对已知患者准确率97.08%，未知患者94.91%，差距小于先前工作。

Conclusion: 提出的方法优于现有方法，尤其在处理未知患者数据方面。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [679] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: 现有多模态大语言模型评估基准局限于单轮问答，本文引入MT - Video - Bench用于多轮对话视频理解评估，评估多种模型并揭示其问题，基准将公开。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准局限于单轮问答，未考虑现实场景多轮对话复杂性，需新基准评估多模态大语言模型在多轮对话中的视频理解能力。

Method: 引入MT - Video - Bench，评估模型六项核心能力，包含987个精心策划的多领域多轮对话，与现实应用对齐。

Result: 评估了多种开源和闭源的先进多模态大语言模型，揭示了它们在处理多轮视频对话时的显著性能差异和局限性。

Conclusion: MT - Video - Bench将公开以促进未来研究。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [680] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 研究签名伪造检测特征学习策略以提升跨数据集泛化能力，比较两种实验管道，原始图像模型表现更好，基于shell的模型有改进潜力。


<details>
  <summary>Details</summary>
Motivation: 现有离线签名验证方法跨数据集泛化能力差，手写风格和采集协议变化影响性能，需提升模型跨数据集泛化能力。

Method: 使用三个公开基准数据集，开发基于原始签名图像和shell预处理的两个实验管道。

Result: 原始图像模型在各基准测试中表现更佳，基于shell的模型有未来改进实现鲁棒跨域签名验证的潜力。

Conclusion: 虽未明确两种方法的绝对优劣，但为跨数据集签名验证特征学习提供了参考。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [681] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 提出基于双编码器注意力框架，结合病变分割和临床元数据，提升皮肤病变分类准确性与可解释性，在多个数据集验证效果佳。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测重要，但自动诊断有挑战，且很多深度学习模型是“黑盒”，缺乏临床信任。

Method: 采用带双注意力门和空洞空间金字塔池化的Deep - UNet分割病变；分类阶段用两个DenseNet201编码器，通过多头交叉注意力融合特征；用基于Transformer模块纳入患者元数据。

Result: 在多个数据集上达到了最先进的分割性能，显著提高了分类准确率和平均AUC；用Grad - CAM生成热图验证模型可靠性。

Conclusion: 结合精确病变分割、临床数据和基于注意力的融合，能得到更准确和可解释的皮肤癌分类模型。

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [682] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 本文提出广义求解器和广义对抗求解器，减少扩散模型采样计算量，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型采样计算成本高，基于梯度优化的方法依赖复杂训练技术且不注重保留细粒度细节。

Method: 引入广义求解器，结合原始蒸馏损失和对抗训练得到广义对抗求解器。

Result: 广义对抗求解器在相似资源约束下性能优于现有求解器训练方法。

Conclusion: 广义对抗求解器可减少扩散模型采样计算量，提升生成质量，且无需额外训练技巧。

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [683] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: 提出Glyph框架，将长文本转为图像用VLM处理，实现3 - 4倍令牌压缩，加速处理和训练，还能用于多模态任务。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型扩展上下文窗口到百万令牌级别时，计算和内存成本过高，实用性受限。

Method: 提出Glyph框架，将长文本渲染成图像用VLM处理，设计LLM驱动的遗传搜索确定最佳视觉渲染配置。

Result: 实现3 - 4倍令牌压缩，预填充和译码速度快约4倍，SFT训练快约2倍，极端压缩下128K上下文VLM可处理100万令牌级文本任务。

Conclusion: Glyph框架能有效解决长上下文大语言模型的计算和内存成本问题，且处理后的数据对多模态任务有益。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>
