<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.CE](#cs.CE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 23]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.SE](#cs.SE) [Total: 12]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 3]
- [stat.ML](#stat.ML) [Total: 8]
- [econ.EM](#econ.EM) [Total: 2]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 2]
- [math.ST](#math.ST) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [econ.GN](#econ.GN) [Total: 4]
- [eess.AS](#eess.AS) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [q-fin.PR](#q-fin.PR) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research](https://arxiv.org/abs/2512.19799)
*Tingjia Miao,Jiawen Dai,Jingkun Liu,Jinxin Tan,Muhua Zhang,Wenkai Jin,Yuwen Du,Tian Jin,Xianghe Pang,Zexi Liu,Tu Guo,Zhengliang Zhang,Yunjie Huang,Shuo Chen,Rui Ye,Yuzhi Zhang,Linfeng Zhang,Kun Chen,Wei Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型的智能体PhysMaster，可在高能理论、凝聚态理论和天体物理等问题上实现研究加速、自动化和自主发现。


<details>
  <summary>Details</summary>
Motivation: 现有研究在开放科学场景的端到端问题解决能力有限，尤其是在物理领域，需要新的智能体来解决问题。

Method: 提出PhysMaster，将抽象推理与数值计算相结合，利用LANDAU增强决策的可靠性和稳定性，采用自适应探索策略。

Result: 在高能理论、凝聚态理论到天体物理等问题的评估中，实现了加速研究、自动化执行和自主发现。

Conclusion: PhysMaster能在开放科学场景中展现出良好性能，可辅助、加速和自动化物理研究。

Abstract: Advances in LLMs have produced agents with knowledge and operational capabilities comparable to human scientists, suggesting potential to assist, accelerate, and automate research. However, existing studies mainly evaluate such systems on well-defined benchmarks or general tasks like literature retrieval, limiting their end-to-end problem-solving ability in open scientific scenarios. This is particularly true in physics, which is abstract, mathematically intensive, and requires integrating analytical reasoning with code-based computation. To address this, we propose PhysMaster, an LLM-based agent functioning as an autonomous theoretical and computational physicist. PhysMaster couples absract reasoning with numerical computation and leverages LANDAU, the Layered Academic Data Universe, which preserves retrieved literature, curated prior knowledge, and validated methodological traces, enhancing decision reliability and stability. It also employs an adaptive exploration strategy balancing efficiency and open-ended exploration, enabling robust performance in ultra-long-horizon tasks. We evaluate PhysMaster on problems from high-energy theory, condensed matter theory to astrophysics, including: (i) acceleration, compressing labor-intensive research from months to hours; (ii) automation, autonomously executing hypothesis-driven loops ; and (iii) autonomous discovery, independently exploring open problems.

</details>


### [2] [A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution](https://arxiv.org/abs/2512.19882)
*Mahdi Mostajabdaveh,F. Sibel Salman,Walter J. Gutjahr*

Main category: cs.AI

TL;DR: 本文研究灾害后救援物资分配与车辆路径规划问题，提出双目标模型及解决算法，经测试算法效果好，还给出不同时间约束下的有效策略。


<details>
  <summary>Details</summary>
Motivation: 重大灾害中预存物资常无法满足需求，需解决从配送中心到避难所的车辆路径规划与物资分配问题，平衡效率与公平。

Method: 提出双目标问题，用混合整数规划（MIP）模型和ε -约束法处理，推导最优解数学性质引入有效不等式，设计算法，开发分支定价（B&P）算法求解。

Result: B&P算法显著优于商业MIP求解器，双目标方法减少34%的援助分配不公平性，不同时间约束下有不同有效策略。

Conclusion: 双目标方法能有效平衡效率与公平，不同时间约束应采用不同优化策略。

Abstract: The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the $ε$-constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul's Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers. Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.

</details>


### [3] [Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs](https://arxiv.org/abs/2512.19937)
*Eric Yeh,John Cadigan,Ran Chen,Dick Crouch,Melinda Gervasio,Dayne Freitag*

Main category: cs.AI

TL;DR: 研究利用大语言模型模拟人类行为，针对测试人格配置文件需创建提示的问题，采用插值解码，展示其在大五人格维度和经济游戏中的效果及初步的人类玩家“孪生”结果。


<details>
  <summary>Details</summary>
Motivation: 解决测试人格配置文件时需为每个配置文件创建提示带来的实验开销和可重复性问题。

Method: 利用插值解码，将人格各维度表示为一对对立提示，用插值参数模拟行为。

Result: 插值解码能可靠调节大五人格维度分数，使大语言模型在经济游戏中模仿人类决策行为，有初步的人类玩家“孪生”结果。

Conclusion: 插值解码可有效解决相关问题，使大语言模型在模拟人类行为方面有较好表现。

Abstract: Recent research has explored using very large language models (LLMs) as proxies for humans in tasks such as simulation, surveys, and studies. While LLMs do not possess a human psychology, they often can emulate human behaviors with sufficiently high fidelity to drive simulations to test human behavioral hypotheses, exhibiting more nuance and range than the rule-based agents often employed in behavioral economics. One key area of interest is the effect of personality on decision making, but the requirement that a prompt must be created for every tested personality profile introduces experimental overhead and degrades replicability. To address this issue, we leverage interpolative decoding, representing each dimension of personality as a pair of opposed prompts and employing an interpolation parameter to simulate behavior along the dimension. We show that interpolative decoding reliably modulates scores along each of the Big Five dimensions. We then show how interpolative decoding causes LLMs to mimic human decision-making behavior in economic games, replicating results from human psychological research. Finally, we present preliminary results of our efforts to ``twin'' individual human players in a collaborative game through systematic search for points in interpolation space that cause the system to replicate actions taken by the human subject.

</details>


### [4] [Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification](https://arxiv.org/abs/2512.19957)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Rodrigo Pereira David,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 本文提出一种应对PlantClef 2025挑战的方法，用训练集类原型指导分割ViT模型训练，实现从多类识别到多标签分类的域适应，在挑战赛中获第五名。


<details>
  <summary>Details</summary>
Motivation: 应对PlantClef 2025挑战，解决高分辨率图像的细粒度多标签物种识别问题。

Method: 从训练集图像提取特征并用K - Means聚类得到类原型；用预训练的DinoV2替换patch embedding层构建定制化窄ViT；训练模型从测试集图像重建训练集类原型；用模型获取注意力分数指导分类。

Result: 在PlantCLEF 2025挑战私有排行榜获第五名，F1分数0.33331，比第一名低0.03。

Conclusion: 该方法在基准任务中可能取得有竞争力的表现，代码已开源。

Abstract: This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.

</details>


### [5] [FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification](https://arxiv.org/abs/2512.19960)
*Luciano Araujo Dourado Filho,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 本文提出通过对类内聚类分配进行分类来学习细粒度特征，以提升细粒度视觉分类任务性能，在PlantNet300k数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 类内差异会阻碍深度学习模型学习，尤其是在细粒度视觉分类任务中类样本不足的常见场景下。

Method: 对每个类单独进行聚类，发现能编码图像间潜在相似度的伪标签，并用于分层分类过程以学习更细粒度视觉特征。

Result: 在PlantNet300k数据集上取得了当前最优性能，虽部分组件未完全优化。

Conclusion: 初步实验揭示了未来工作需发展的关键点，以找到方法有效性的更多确凿证据。

Abstract: Intra-class variability is given according to the significance in the degree of dissimilarity between images within a class. In that sense, depending on its intensity, intra-class variability can hinder the learning process for DL models, specially when such classes are also underrepresented, which is a very common scenario in Fine-Grained Visual Categorization (FGVC) tasks. This paper proposes a novel method that aims at leveraging classification performance in FGVC tasks by learning fine-grained features via classification of class-wise cluster assignments. Our goal is to apply clustering over each class individually, which can allow to discover pseudo-labels that encodes a latent degree of similarity between images. In turn, those labels can be employed in a hierarchical classification process that allows to learn more fine-grained visual features and thereby mitigating intra-class variability issues. Initial experiments over the PlantNet300k enabled to shed light upon several key points in which future work will have to be developed in order to find more conclusive evidence regarding the effectiveness of our method. Our method still achieves state-of-the-art performance on the PlantNet300k dataset even though some of its components haven't been shown to be fully optimized. Our code is available at \href{https://github.com/ADAM-UEFS/FGDCC}{https://github.com/ADAM-UEFS/FGDCC}.

</details>


### [6] [S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test](https://arxiv.org/abs/2512.19992)
*Zhe Sun,Xueyuan Yang,Yujie Lu,Zhenliang Zhang*

Main category: cs.AI

TL;DR: 提出S$^{3}$IT基准评估具身社会智能，用座位排序任务测试大语言模型，发现其存在空间智能缺陷但能处理有明确文本线索的冲突。


<details>
  <summary>Details</summary>
Motivation: 现有评估无法解决具身智能体在人类环境中对社会规范和物理约束的集成评估问题。

Method: 引入S$^{3}$IT基准，围绕座位排序任务，生成多样化场景，让智能体通过对话获取偏好、自主探索环境并进行多目标优化。

Result: 评估发现最先进的大语言模型仍难以解决该问题，与人类基线有明显差距，在空间智能上有缺陷，但能在有明确文本线索的冲突解决上接近人类水平。

Conclusion: 大语言模型在具身社会智能的空间智能方面存在不足，需进一步改进。

Abstract: The integration of embodied agents into human environments demands embodied social intelligence: reasoning over both social norms and physical constraints. However, existing evaluations fail to address this integration, as they are limited to either disembodied social reasoning (e.g., in text) or socially-agnostic physical tasks. Both approaches fail to assess an agent's ability to integrate and trade off both physical and social constraints within a realistic, embodied context. To address this challenge, we introduce Spatially Situated Social Intelligence Test (S$^{3}$IT), a benchmark specifically designed to evaluate embodied social intelligence. It is centered on a novel and challenging seat-ordering task, requiring an agent to arrange seating in a 3D environment for a group of large language model-driven (LLM-driven) NPCs with diverse identities, preferences, and intricate interpersonal relationships. Our procedurally extensible framework generates a vast and diverse scenario space with controllable difficulty, compelling the agent to acquire preferences through active dialogue, perceive the environment via autonomous exploration, and perform multi-objective optimization within a complex constraint network. We evaluate state-of-the-art LLMs on S$^{3}$IT and found that they still struggle with this problem, showing an obvious gap compared with the human baseline. Results imply that LLMs have deficiencies in spatial intelligence, yet simultaneously demonstrate their ability to achieve near human-level competence in resolving conflicts that possess explicit textual cues.

</details>


### [7] [Discovering Lie Groups with Flow Matching](https://arxiv.org/abs/2512.20043)
*Jung Yeon Park,Yuxuan Chen,Floor Eijkelboom,Jan-Willem van de Meent,Lawson L. S. Wong,Robin Walters*

Main category: cs.AI

TL;DR: 提出通过李群上的流匹配直接从数据中学习对称性的方法LieFlow，实验证明其可发现离散群，还解决了“最后时刻收敛”问题并引入新插值方案。


<details>
  <summary>Details</summary>
Motivation: 对称性对理解物理系统和提高机器学习性能至关重要，需从数据中了解潜在对称性。

Method: 通过李群上的流匹配直接从数据中学习对称性，将对称性发现表述为在更大假设组上学习分布，使学习的分布与数据中观察到的对称性相匹配。

Result: 在2D和3D点云实验中成功发现离散群，包括通过复数域上的流匹配发现反射；发现了“最后时刻收敛”挑战并针对对称性发现引入新的流匹配插值方案。

Conclusion: 提出的LieFlow方法在对称性发现上比之前的方法更灵活，假设更少，是一种有效的对称性学习方法。

Abstract: Symmetry is fundamental to understanding physical systems, and at the same time, can improve performance and sample efficiency in machine learning. Both pursuits require knowledge of the underlying symmetries in data. To address this, we propose learning symmetries directly from data via flow matching on Lie groups. We formulate symmetry discovery as learning a distribution over a larger hypothesis group, such that the learned distribution matches the symmetries observed in data. Relative to previous works, our method, \lieflow, is more flexible in terms of the types of groups it can discover and requires fewer assumptions. Experiments on 2D and 3D point clouds demonstrate the successful discovery of discrete groups, including reflections by flow matching over the complex domain. We identify a key challenge where the symmetric arrangement of the target modes causes ``last-minute convergence,'' where samples remain stationary until relatively late in the flow, and introduce a novel interpolation scheme for flow matching for symmetry discovery.

</details>


### [8] [Learning Skills from Action-Free Videos](https://arxiv.org/abs/2512.20052)
*Hung-Chieh Fang,Kuo-Han Hung,Chu-Rong Chen,Po-Jung Chou,Chun-Kai Yang,Po-Chen Ko,Yu-Chiang Wang,Yueh-Hua Wu,Min-Hung Chen,Shao-Hua Sun*

Main category: cs.AI

TL;DR: 提出SOF框架从无动作视频中学习潜在技能，实验表明能提高多任务和长程任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难转化为低级动作，潜在动作模型缺乏高级规划能力，需弥合差距。

Method: 引入SOF框架，基于光流的中间表示学习潜在技能空间，在基于流的潜在空间中学习技能。

Result: 在多任务和长程设置中持续提高了性能，能直接从原始视觉数据获取和组合技能。

Conclusion: SOF框架有效，可从无动作视频学习潜在技能并转化为动作，提升机器人任务表现。

Abstract: Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.

</details>


### [9] [Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach](https://arxiv.org/abs/2512.20056)
*Hao Li,Fabian Deuser,Wenping Yin,Steffen Knoblauch,Wufan Zhao,Filip Biljecki,Yong Xue,Wei Huang*

Main category: cs.AI

TL;DR: 本文提出ProbGLC方法用于快速灾害响应，结合概率和确定性地理定位模型，在两个数据集实验中展现高精度和可解释性，代码公开。


<details>
  <summary>Details</summary>
Motivation: 气候变化使灾害频发，快速准确识别灾害位置对灾害响应决策和资源分配至关重要。

Method: 提出Probabilistic Cross - view Geolocalization（ProbGLC）方法，将概率和确定性地理定位模型整合到统一框架。

Result: 在MultiIAN和SAGAINDisaster两个数据集上实验，Acc@1km为0.86，Acc@25km为0.97，展现出优越的地理定位精度和模型可解释性。

Conclusion: ProbGLC方法有很大潜力，利用生成式跨视图方法可促进位置感知，实现更好更快的灾害响应。

Abstract: As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC

</details>


### [10] [Scaling Reinforcement Learning for Content Moderation with Large Language Models](https://arxiv.org/abs/2512.20061)
*Hamed Firooz,Rui Liu,Yuchen Lu,Zhenyu Hou,Fangzhou Xiong,Xiaoyang Zhang,Changshu Jian,Zhicheng Zhu,Jiayuan Ma,Jacob Tao,Chaitali Gupta,Xiaochang Peng,Shike Mei,Hang Cui,Yang Qin,Shuo Tang,Jason Gaedtke,Arpit Mittal*

Main category: cs.AI

TL;DR: 探讨使用强化学习进行内容分类以实现内容审核，研究表明强化学习有类似S型的扩展行为，且在数据效率上高于监督微调。


<details>
  <summary>Details</summary>
Motivation: 当前数字生态中大规模内容审核是挑战，虽大语言模型有潜力，但在现实场景中实现专家级准确率的实际挑战仍待探索。

Method: 对用于内容分类的扩展强化学习进行全面实证研究，系统评估多种强化学习训练方法和奖励塑造策略。

Result: 强化学习呈现类似S型的扩展行为，性能随训练数据、滚动和优化步骤增加而提升直至饱和；在需要复杂基于策略推理的任务上大幅提升性能，数据效率比监督微调高100倍。

Conclusion: 强化学习在工业规模的审核系统中很有效，尤其在专家标注稀缺或成本高的领域。

Abstract: Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.

</details>


### [11] [Reason2Decide: Rationale-Driven Multi-Task Learning](https://arxiv.org/abs/2512.20074)
*H M Quamran Hasan,Housam Khalifa Bashier,Jiayi Dai,Mi-Young Kim,Randy Goebel*

Main category: cs.AI

TL;DR: 提出Reason2Decide框架解决临床决策支持系统预测与解释对齐问题，在多医疗数据集上表现优，减少对人工标注依赖且适合资源受限部署。


<details>
  <summary>Details</summary>
Motivation: 解决临床决策支持系统在实现高预测准确性的同时，生成与预测一致的解释的问题，克服当前方法的暴露偏差。

Method: 提出两阶段训练框架Reason2Decide，第一阶段训练理由生成，第二阶段联合训练标签预测和理由生成，采用计划采样。

Result: 在多个医疗数据集上，Reason2Decide在预测和理由保真度上优于其他微调基线和一些零样本大模型，在分诊中对不同来源理由有鲁棒性，使用大模型生成理由预训练表现好。

Conclusion: Reason2Decide能解决临床决策支持系统的关键问题，减少对人工标注依赖，且小模型也能取得好效果，适合资源受限部署。

Abstract: Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.

</details>


### [12] [Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches](https://arxiv.org/abs/2512.20082)
*Chaithra,Kamesh Kadimisetty,Biju R Mohan*

Main category: cs.AI

TL;DR: 本文提出结合大语言模型与股市反馈的自适应框架用于印度股市情感分类，实验证明该系统提升了性能并验证了该方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析工作未考虑股价或市场反馈影响，为提高情感分类准确性而开展研究。

Method: 利用指令学习在SentiFin数据集上微调LLaMA 3.2 3B模型；采用RAG管道动态选择多源上下文信息；引入反馈驱动模块调整数据源可靠性；结合基于PPO训练的强化学习智能体优化源权重策略。

Result: 在2024 - 2025年NIFTY 50新闻头条实验中，该系统显著提升分类准确率、F1分数和市场一致性。

Conclusion: 结合指令微调大语言模型、动态反馈和强化学习可用于构建健壮的、具有市场感知能力的金融情感模型。

Abstract: Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.

</details>


### [13] [MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization](https://arxiv.org/abs/2512.20135)
*Zhuo Yang,Yeyun chen,Jiaqing Xie,Ben Gao,Shuaike Shen,Wanhao Liu,Liujia Yang,Beilun Wang,Tianfan Fu,Yuqiang Li*

Main category: cs.AI

TL;DR: 提出MolAct强化学习框架解决分子编辑和优化问题，训练两种模型，在相关任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决分子编辑和优化中需迭代改善属性并保持分子化学有效性和结构相似性的多步问题。

Method: 将任务视为顺序、工具引导的决策，采用两阶段训练范式，框架使智能体多轮交互并利用化学工具反馈优化后续编辑。

Result: MolEditAgent在分子编辑任务中表现优于基线，MolOptAgent在分子优化任务中超越部分基线。

Conclusion: 将分子设计视为多步、工具增强过程是实现可靠和可解释改进的关键。

Abstract: Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed "thinking" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open "thinking" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed "thinking" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.

</details>


### [14] [Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection](https://arxiv.org/abs/2512.20140)
*Xingyou Yin,Ceyao Zhang,Min Hu,Kai Chen*

Main category: cs.AI

TL;DR: 本文提出在标记化前向原始时间序列注入噪声的策略，以提升现成大语言模型在时间序列预测中的性能，并进行理论分析和实证验证。


<details>
  <summary>Details</summary>
Motivation: 现有工作常依赖微调专业模块，而使用完全现成、未经微调的大语言模型进行时间序列预测时，其性能对输入数据的文本表示敏感，存在脆性问题。

Method: 在标记化前向原始时间序列注入噪声，作为推理时增强方法；引入两个新的时间序列数据集以消除大语言模型预训练数据污染的潜在偏差。

Result: 通过理论分析和不同基准测试的实证验证，证明该策略有效，在新数据集上观察到性能提升。

Conclusion: 该研究为直接利用现成大语言模型进行时间序列预测迈出了进一步的一步。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.

</details>


### [15] [A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers](https://arxiv.org/abs/2512.20161)
*Dhivya Dharshini Kannan,Anupam Trivedi,Dipti Srinivasan*

Main category: cs.AI

TL;DR: 本文开发基于双向门控循环单元（BiGRU）的PUE预测模型，与GRU对比性能，用能源模拟数据及特征选择算法优化模型。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗大，优化能源管理对全球可持续性至关重要，用神经网络预测PUE可提高能效。

Method: 开发BiGRU模型，用RFECV算法选特征，用不同参数配置找最优超参数，用MSE、MAE和R平方指标对比BiGRU和GRU性能。

Result: 将通过对比MSE、MAE和R平方指标得出BiGRU和GRU模型性能结果，但文中未明确给出。

Conclusion: 文中未明确给出结论，但预期可判断BiGRU模型在PUE预测上是否更优以提高数据中心能效。

Abstract: Data centers account for significant global energy consumption and a carbon footprint. The recent increasing demand for edge computing and AI advancements drives the growth of data center storage capacity. Energy efficiency is a cost-effective way to combat climate change, cut energy costs, improve business competitiveness, and promote IT and environmental sustainability. Thus, optimizing data center energy management is the most important factor in the sustainability of the world. Power Usage Effectiveness (PUE) is used to represent the operational efficiency of the data center. Predicting PUE using Neural Networks provides an understanding of the effect of each feature on energy consumption, thus enabling targeted modifications of those key features to improve energy efficiency. In this paper, we have developed Bidirectional Gated Recurrent Unit (BiGRU) based PUE prediction model and compared the model performance with GRU. The data set comprises 52,560 samples with 117 features using EnergyPlus, simulating a DC in Singapore. Sets of the most relevant features are selected using the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm for different parameter settings. These feature sets are used to find the optimal hyperparameter configuration and train the BiGRU model. The performance of the optimized BiGRU-based PUE prediction model is then compared with that of GRU using mean squared error (MSE), mean absolute error (MAE), and R-squared metrics.

</details>


### [16] [Concept Generalization in Humans and Large Language Models: Insights from the Number Game](https://arxiv.org/abs/2512.20162)
*Arghavan Bazigaran,Hansem Sohn*

Main category: cs.AI

TL;DR: 对比人类与大语言模型在数字游戏中泛化能力，揭示二者在推理和泛化数学概念上的差异


<details>
  <summary>Details</summary>
Motivation: 研究人类和大语言模型在概念推理任务中的泛化能力异同

Method: 采用贝叶斯模型作为分析框架，研究人类和大语言模型的归纳偏差和推理策略

Result: 贝叶斯模型更能体现人类行为，人类更灵活，能少样本泛化，大语言模型依赖数学规则且需更多样本才能泛化

Conclusion: 人类和大语言模型在推理和泛化数学概念上存在根本差异

Abstract: We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.

</details>


### [17] [Offline Safe Policy Optimization From Heterogeneous Feedback](https://arxiv.org/abs/2512.20173)
*Ze Gong,Pradeep Varakantham,Akshat Kumar*

Main category: cs.AI

TL;DR: 本文提出直接基于偏好和安全标签学习策略的框架PreSa，避免显式学习奖惩模型，在连续控制任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有离线基于偏好的强化学习在长时连续控制任务中使用约束强化学习会因奖惩误差累积导致性能受损，需解决安全保障问题。

Method: 引入直接基于成对偏好和安全标签学习策略的框架，提出PreSa方法，在拉格朗日范式下解决约束优化问题，直接学习奖励最大化的安全策略。

Result: 在连续控制任务上，该方法成功学习到高奖励的安全策略，优于现有基线和基于真实奖惩的离线安全强化学习方法。

Conclusion: 所提框架和方法能有效解决离线基于偏好的强化学习中的安全挑战，在连续控制任务中有良好表现。

Abstract: Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.

</details>


### [18] [TongSIM: A General Platform for Simulating Intelligent Machines](https://arxiv.org/abs/2512.20206)
*Zhe Sun,Kunlun Wu,Chuanjian Fu,Zeming Song,Langyong Shi,Zihe Xue,Bohan Jing,Ying Yang,Xiaomeng Gao,Aijia Li,Tianyu Guo,Huiying Li,Xueyuan Yang,Rongkai Liu,Xinyi He,Yuxi Wang,Yue Li,Mingyuan Liu,Yujie Lu,Hongzhao Xie,Shiyun Zhao,Bo Dai,Wei Wang,Tao Yuan,Song-Chun Zhu,Yujia Peng,Zhenliang Zhang*

Main category: cs.AI

TL;DR: 随着AI发展，研究转向多模态和具身AI，但现有模拟平台针对性强，本文推出通用平台TongSIM，可支持多样场景与评估，有灵活性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有模拟平台针对性强，缺乏能支持从低层次具身导航到高层次复合活动的通用训练环境。

Method: 推出高保真、通用的平台TongSIM，提供多样室内外场景，有综合评估框架和基准。

Result: TongSIM能精确评估智能体多种能力，有定制场景等功能，提供灵活性和可扩展性。

Conclusion: TongSIM作为统一平台，可加速通用具身智能的训练、评估和发展。

Abstract: As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.

</details>


### [19] [MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents](https://arxiv.org/abs/2512.20237)
*Xingbo Du,Loka Li,Duzhen Zhang,Le Song*

Main category: cs.AI

TL;DR: 文章构建了名为MemR³的记忆检索代理系统，能通过闭环控制实现自主决策，在LoCoMo基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型代理的记忆系统多注重压缩和存储，对记忆检索的显式闭环控制重视不足。

Method: 构建MemR³系统，包含选择检索、反思和回答操作的路由器及追踪证据收集过程的全局证据差距追踪器。

Result: 在LoCoMo基准测试中，MemR³在LLM评审分数上超过强基线，使用GPT - 4.1 - mini后端，整体提升RAG 7.29%、Zep 1.94%。

Conclusion: MemR³为现有记忆存储提供了即插即用的控制器。

Abstract: Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.

</details>


### [20] [Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks](https://arxiv.org/abs/2512.20275)
*Divya Vijay,Vignesh Ethiraj*

Main category: cs.AI

TL;DR: 随着网络向5G独立和6G演进，传统自动化和强化学习受限，大语言模型存在风险，本文提出G - SPEC框架，试验显示其效果良好且有可扩展性。


<details>
  <summary>Details</summary>
Motivation: 网络向5G独立和6G演进时，传统方法有局限，大语言模型引入随机风险，需要新框架解决问题。

Method: 提出Graph - Symbolic Policy Enforcement and Control (G - SPEC)框架，依赖治理三元组（TSLAM - 4B、NKG和SHACL约束）。

Result: 在模拟450节点5G核心网评估，零安全违规，修复成功率94.1%，远超82.4%基线；消融分析显示NKG验证贡献68%安全增益；可扩展性测试显示验证延迟与子图大小关系及少量处理开销。

Conclusion: G - SPEC在服务管理操作层（SMO）可行。

Abstract: As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [21] [Solving strategies for data-driven one-dimensional elasticity exhibiting nonlinear strains](https://arxiv.org/abs/2512.19912)
*Thi-Hoa Nguyen,Viljar H. Gjerde,Bruno A. Roccia,Cristian G. Gebhardt*

Main category: cs.CE

TL;DR: 本文扩展并推广求解策略，结合贪心优化算法和ADM，数值实验表明其能更好逼近全局最优解，可用于尼龙绳循环测试等，且在非对称数据和噪声数据下提高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩展和推广基于贪心优化算法和交替方向法（ADM）的求解策略，用于多载荷步计算非线性系统。

Method: 将贪心优化算法与基于ADM的直接数据驱动求解器相结合，并结合牛顿 - 拉夫逊方法用于非线性弹性问题。

Result: 通过一维和二维杆及桁架结构数值实验，该策略能更好逼近全局最优解，但计算成本随“贪心”搜索次数增加；可重现尼龙绳循环测试首个周期；在非对称数据和噪声数据下，提高了桁架结构求解的准确性和鲁棒性。

Conclusion: 提出的求解策略有更好的全局最优解逼近能力和鲁棒性，虽有计算成本代价，但可用于实际问题。

Abstract: In this work, we extend and generalize our solving strategy, first introduced in [1], based on a greedy optimization algorithm and the alternating direction method (ADM) for nonlinear systems computed with multiple load steps. In particular, we combine the greedy optimization algorithm with the direct data-driven solver based on ADM which is firstly introduced in [2] and combined with the Newton-Raphson method for nonlinear elasticity in [3]. We numerically illustrate via one- and two-dimensional bar and truss structures exhibiting nonlinear strain measures and different constitutive datasets that our solving strategy generally achieves a better approximation of the globally optimal solution. This, however, comes at the expense of higher computational cost which is scaled by the number of "greedy" searches. Using this solving strategy, we reproduce the first cycle of the cyclic testing for a nylon rope that was performed at industrial testing facilities for mooring lines manufacturers. We also numerically illustrate for a truss structure that our solving strategy generally improves the accuracy and robustness in cases of an unsymmetrical data distribution and noisy data.

</details>


### [22] [A hybrid global local computational framework for ship hull structural analysis using homogenized model and graph neural network](https://arxiv.org/abs/2512.20020)
*Yuecheng Cai,Jasmin Jelovica*

Main category: cs.CE

TL;DR: 提出集成ESL模型与GNN的船体梁全局 - 局部结构分析计算框架，经案例验证表现良好。


<details>
  <summary>Details</summary>
Motivation: 开发高效准确的船体梁全局局部结构分析方法，适用于优化。

Method: 用粗网格均质化ESL模型预测全局位移场，开发全局到局部自由度映射和重建程序，用重建自由度等作为输入，经高保真3D面板有限元模型训练HGT预测局部应力和位移场。

Result: 全局预测误差由粗网格ESL解控制，HGT保持高局部精度，明显优于传统基于ESL的应力估计方法。

Conclusion: 该框架只需全局ESL解就能生成详细局部响应，适用于优化。

Abstract: This study presents a computational framework for global local structural analysis of ship hull girders that integrates an equivalent single layer (ESL) model with a graph neural network (GNN). A coarse mesh homogenized ESL model efficiently predicts the global displacement field, from which degrees of freedom (DOFs) along stiffened panel boundaries are extracted. A global to local DOF mapping and reconstruction procedure is developed to recover detailed boundary kinematics for local analysis. The reconstructed DOFs, together with panel geometry and loading, serve as inputs to a heterogeneous graph transformer (HGT), a subtype of GNN, which rapidly and accurately predicts the detailed stress and displacement fields for any panel within the hull girder. The HGT is trained using high fidelity 3D panel finite element model with reconstructed boundary conditions, enabling it to generalize across varying panel geometries, loadings, and boundary behaviors. Once trained, the framework requires only the global ESL solution in order to generate detailed local responses, making it highly suitable for optimization. Validation on three box beam case studies demonstrates that the global prediction error is governed by the coarse mesh ESL solution, while the HGT maintains high local accuracy and clearly outperforms conventional ESL based stress estimation method.

</details>


### [23] [Auditing Reproducibility in Non-Targeted Analysis: 103 LC/GC--HRMS Tools Reveal Temporal Divergence Between Openness and Operability](https://arxiv.org/abs/2512.20279)
*Sarah Alsubaie,Sakhaa Alsaedi,Xin Gao*

Main category: cs.CE

TL;DR: 评估103个非靶向分析工具，发现工具开放性上升但可操作性下降，且无工具解决食品安全问题，加强部分支柱可使工具可被外部实验室重复使用。


<details>
  <summary>Details</summary>
Motivation: 在非靶向分析用于应对突发化合物监测且需确保结果可重复性的背景下，评估工具的各项特性。

Method: 依据FAIR和BP4NTA原则的六个支柱评估2004 - 2025年的103个工具。

Result: 九成工具共享数据，不到四成支持便携式实现，验证和可移植性很少同时出现，开放性上升但可操作性下降，无工具解决食品安全问题，期刊数据共享政策有局限。

Conclusion: 加强实验室验证、标准化格式和便携式实现，可使工具被外部实验室重复使用。

Abstract: In 2008, melamine in infant formula forced laboratories across three continents to verify a compound they had never monitored. Non-targeted analysis using LC/GC-HRMS handles these cases. But when findings trigger regulatory action, reproducibility becomes operational: can an independent laboratory repeat the analysis and reach the same conclusion?
  We assessed 103 tools (2004-2025) against six pillars drawn from FAIR and BP4NTA principles: laboratory validation (C1), data availability (C2), code availability (C3), standardised formats (C4), knowledge integration (C5), and portable implementation (C6). Health contributed 51 tools, Pharma 31, and Chemistry 21.
  Nine in ten tools shared data (C2, 90/103, 87%). Fewer than four in ten supported portable implementations (C6, 40/103, 39%). Validation and portability rarely appeared together (C1+C6, 18/103, 17%). Over twenty-one years, openness climbed from 56% to 86% while operability dropped from 55% to 43%. No tool addressed food safety.
  Journal data-sharing policies increased what authors share but not what reviewers can run. Tools became easier to find but harder to execute. Strengthening C1, C4, and C6 would turn documented artifacts into workflows that external laboratories can replay.

</details>


### [24] [Replacing Gas with Low-cost, Abundant Long-duration Pumped Hydro in Electricity Systems](https://arxiv.org/abs/2512.20286)
*Timothy Weber,Cheng Cheng,Harry Thawley,Kylie Catchpole,Andrew Blakers,Bin Lu,Jennifer Zhao,Anna Nadolny*

Main category: cs.CE

TL;DR: 研究表明用抽水蓄能平衡太阳能和风能发电可消除对化石气的需求，提出新方法和模型助力电力系统脱碳。


<details>
  <summary>Details</summary>
Motivation: 化石气虽被视为2050年后支持可变太阳能和风能发电的能源，但它是温室气体排放的主要来源，现有长期电力系统计划过度依赖化石气。

Method: 采用基于“分段”的时间聚合方法，开发新的电力系统模型。

Result: 基于“分段”的方法接近全系列优化，能捕捉长时储能行为，找到近乎最优的100%可再生电力解决方案；开发的模型可快速评估数百万其他近乎最优的解决方案。

Conclusion: 世界上几乎每个地区都有足够的离河抽水蓄能选项，可实现未来电力系统的完全脱碳。

Abstract: Fossil gas is sometimes presented as an enabler of variable solar and wind generation beyond 2050, despite being a primary source of greenhouse gas emissions from methane leakage and combustion. We find that balancing solar and wind generation with pumped hydro energy storage eliminates the need for fossil gas without incurring a cost penalty. However, many existing long-term electricity system plans are biased to rely on fossil gas due to using temporal aggregation methods that either heavily constrain storage cycling behaviour or lose track of the state-of-charge, failing to consider the potential of low-cost long-duration off-river pumped hydro, and ignoring the broad suite of near-optimal energy transition pathways. We show that a temporal aggregation method based on 'segmentation' (fitted chronology) closely resembles the full-series optimisation, captures long-duration storage behaviour (48- and 160-hour durations), and finds a near-optimal 100% renewable electricity solution. We develop a new electricity system model to rapidly evaluate millions of other near-optimal solutions, stressing the importance of modelling pumped hydro sites with a low energy volume cost (<US$50 per kilowatt-hour), long economic lifetime (~75 years), and low real discount rate akin to other natural monopolies (<=3%). Almost every region of the world has access to sufficient 50 - 5000 gigawatt-hour off-river pumped hydro options that enable them to entirely decarbonise their future electricity systems.

</details>


### [25] [Expected Revenue, Risk, and Grid Impact of Bitcoin Mining: A Decision-Theoretic Perspective](https://arxiv.org/abs/2512.20518)
*Yuting Cai,Ruthav Sadali,Korok Ray,Chao Tian*

Main category: cs.CE

TL;DR: 当前评估比特币挖矿有局限，引入统一事前统计模型，能进行有效量化并支持可靠分析


<details>
  <summary>Details</summary>
Motivation: 当前评估用事后代理指标，忽略不确定性且难捕捉比特币挖矿快速变化

Method: 从挖矿原理出发构建统一事前统计模型，每个哈希是基于比特币区块难度的伯努利试验

Result: 模型得出每哈希率单位预期收入、不同场景风险指标和不同规模盈利概率，经验校准与先前观测吻合

Conclusion: 该模型能实现统一量化，可支持更可靠的挖矿影响和行为分析

Abstract: Most current assessments use ex post proxies that miss uncertainty and fail to consistently capture the rapid change in bitcoin mining. We introduce a unified, ex ante statistical model that derives expected return, downside risk, and upside potential profit from the first principles of mining: Each hash is a Bernoulli trial with a Bitcoin block difficulty-based success probability. The model yields closed-form expected revenue per hash-rate unit, risk metrics in different scenarios, and upside-profit probabilities for different fleet sizes. Empirical calibration closely matches previously reported observations, yielding a unified, faithful quantification across hardware, pools, and operating conditions. This foundation enables more reliable analysis of mining impacts and behavior.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [26] [Risk-Aware GPU-Assisted Cardinality Estimation for Cost-Based Query Optimizers](https://arxiv.org/abs/2512.19750)
*Ilsun Chang*

Main category: cs.DB

TL;DR: 论文指出基数估计在实际工作负载中存在问题，提出GACE架构，可在稳定区域保持低开销，在问题场景提升计划稳定性和降低尾延迟。


<details>
  <summary>Details</summary>
Motivation: 实际工作负载常违背静态统计假设，导致基数估计失败，降低决策稳定性和增加计划翻转率。

Method: 提出GACE混合辅助架构，通过风险门检测估计不确定性，GPU测量引擎高速探测并明确测量成本，仅在风险区间调用GPU测量。

Result: 在稳定区域保持低开销，在问题场景提升计划稳定性和降低尾延迟（P99）。

Conclusion: GACE架构能有效解决基数估计在实际工作负载中的问题。

Abstract: Cardinality estimation is a cornerstone of cost-based optimizers (CBOs), yet real-world workloads often violate the assumptions behind static statistics, degrading decision stability and increasing plan flip rates. We empirically characterize failures caused by stale statistics, skew, join correlations, hidden distributions in bind variables, and sampling bias, and quantify the overhead and break-even points of hardware-accelerated measurement.
  We propose GACE (GPU-Assisted Cardinality Estimation), a hybrid auxiliary architecture that augments rather than replaces the optimizer. GACE selectively invokes GPU-based measurement only in risky intervals via a Risky Gate that detects estimation uncertainty, and a GPU Measurement Engine that performs high-speed probing with explicit cost accounting for the measurement itself. This design preserves low overhead in stable regions while improving plan stability and reducing tail latency (P99) in problematic scenarios.

</details>


### [27] [Automated Training of Learned Database Components with Generative AI](https://arxiv.org/abs/2512.20271)
*Angjela Davitkova,Sebastian Michel*

Main category: cs.DB

TL;DR: 本文探讨使用生成模型为数据库组件合成训练数据，进行可行性研究并讨论挑战与解决方案，结果显示生成模型可增强训练数据集。


<details>
  <summary>Details</summary>
Motivation: 深度学习用于数据库优化时获取高质量训练数据存在挑战，因此探索用生成模型合成训练数据。

Method: 开展关于生成模型生成数据库工作负载的查询分布和执行计划的可行性研究，并讨论关键挑战与潜在解决方案。

Result: 生成模型能有效增强训练数据集，提升学习型数据库技术的适应性。

Conclusion: 生成模型可用于为数据库组件合成训练数据，是解决数据获取难题的有效途径。

Abstract: The use of deep learning for database optimization has gained significant traction, offering improvements in indexing, cardinality estimation, and query optimization. However, acquiring high-quality training data remains a significant challenge. This paper explores the possibility of using generative models, such as GPT, to synthesize training data for learned database components. We present an initial feasibility study investigating their ability to produce realistic query distributions and execution plans for database workloads. Additionally, we discuss key challenges, such as data scalability and labeling, along with potential solutions. The initial results suggest that generative models can effectively augment training datasets, improving the adaptability of learned database techniques.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [28] [Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform](https://arxiv.org/abs/2512.19842)
*Andrea Sordello,Marco Mellia,Idilio Drago,Rodolfo Valentim,Francesco Musumeci,Massimo Tornatore,Federico Cerutti,Martino Trevisan,Alessio Botta,Willen Borges Coelho*

Main category: cs.DC

TL;DR: 介绍轻量级云原生平台Holoscope，可简化分布式传感器部署管理，在多机构和云网络部署使用。


<details>
  <summary>Details</summary>
Motivation: 互联网攻击的复杂性和规模需要分布式、协作的观测站来监测恶意流量，设计Holoscope平台以满足此需求。

Method: 基于K3s和WireGuard构建，采用模块化设计和基础设施即代码原则，支持动态传感器编排、自动恢复和处理。

Result: 在欧洲和巴西的多个机构和云网络中构建、部署和运行了Holoscope，实现了对大规模攻击现象的统一可见性。

Conclusion: Holoscope能简化分布式传感器部署管理，在多网络环境实现统一攻击监测，且保持集成便捷性和安全合规性。

Abstract: The complexity and scale of Internet attacks call for distributed, cooperative observatories capable of monitoring malicious traffic across diverse networks. Holoscope is a lightweight, cloud-native platform designed to simplify the deployment and management of distributed telescope (passive) and honeypot (active) sensors, used to collect and analyse attack traffic by exposing or simulating vulnerable systems. Built upon K3s and WireGuard, Holoscope offers secure connectivity, automated node onboarding, and resilient operation even in resource-constrained environments. Through modular design and Infrastructure-as-Code principles, it supports dynamic sensor orchestration, automated recovery and processing. We build, deploy and operate Holoscope across multiple institutions and cloud networks in Europe and Brazil, enabling unified visibility into large-scale attack phenomena while maintaining ease of integration and security compliance.

</details>


### [29] [SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication](https://arxiv.org/abs/2512.20178)
*Chen Zhuang,Lingqi Zhang,Benjamin Brock,Du Wu,Peng Chen,Toshio Endo,Satoshi Matsuoka,Mohamed Wahib*

Main category: cs.DC

TL;DR: 本文针对分布式稀疏矩阵乘法通信开销大问题，提出优化策略并实现框架，在真实数据集上评估显示强扩展性和显著加速比。


<details>
  <summary>Details</summary>
Motivation: 分布式稀疏矩阵乘法的主要性能瓶颈在于大量通信开销，限制了性能和可扩展性。

Method: 提出细粒度、感知稀疏性的通信策略和分层通信策略，并在综合分布式稀疏矩阵乘法框架中实现这些优化。

Result: 在真实数据集上评估表明，框架在 128 个 GPU 上具有强扩展性，与四个先进基线相比，分别实现了 221.5×、56.0×、23.4×和 8.8×的几何平均加速比。

Conclusion: 所提出的优化策略和实现的框架有效，能显著提升分布式稀疏矩阵乘法的性能和可扩展性。

Abstract: Distributed Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental operation in numerous high-performance computing and deep learning applications. The major performance bottleneck in distributed SpMM lies in the substantial communication overhead, which limits both performance and scalability. In this paper, we identify and analyze sources of inefficient communication in existing distributed SpMM implementations at two levels and address these inefficiencies by proposing: (1) a fine-grained, sparsity-aware communication strategy that reduces communication overhead by exploiting the sparsity pattern of the sparse matrix, and (2) a hierarchical communication strategy that integrates the sparsity-aware strategy with the common two-tier network architectures in GPU-accelerated systems, to reduce redundant communication across slow network links. We implement these optimizations in a comprehensive distributed SpMM framework, \method{}. Extensive evaluations on real-world datasets show that our framework demonstrates strong scalability up to 128 GPUs, achieving geometric mean speedups of 221.5$\times$, 56.0$\times$, 23.4$\times$, and 8.8$\times$ over four state-of-the-art baselines (CAGNET, SPA, BCL, and CoLa, respectively) at this scale.

</details>


### [30] [UCCL-EP: Portable Expert-Parallel Communication](https://arxiv.org/abs/2512.19849)
*Ziming Mao,Yihan Zhang,Chihan Cui,Kaichao You,Zhongjie Chen,Zhiying Xu,Scott Shenker,Costin Raiciu,Yang Zhou,Ion Stoica*

Main category: cs.DC

TL;DR: 提出可移植的专家并行通信系统UCCL - EP，性能媲美DeepEP且有提升。


<details>
  <summary>Details</summary>
Motivation: 现有专家并行通信系统如DeepEP在异构GPU和NIC平台上可移植性差。

Method: 用高吞吐量GPU - CPU控制通道替代GPU发起的RDMA，用RDMA即时数据模拟特定排序语义。

Result: 在EFA上调度和合并吞吐量比现有最佳方案高2.1倍；在NVIDIA平台性能与DeepEP相当；在不同平台提升了令牌和训练吞吐量。

Conclusion: UCCL - EP具有良好可移植性且能在异构硬件上实现高性能。

Abstract: Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.
  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.

</details>


### [31] [An Adaptive Distributed Stencil Abstraction for GPUs](https://arxiv.org/abs/2512.19851)
*Aditya Bhosale,Laxmikant Kale*

Main category: cs.DC

TL;DR: Python科学计算生态局限于单节点并行，本文提出用于多节点GPU模板计算的自适应分布式抽象，展示资源弹性并获性能提升。


<details>
  <summary>Details</summary>
Motivation: Python科学计算生态单节点并行有局限，硬件加速器普及和能效需求使资源适应性成关键，传统HPC抽象僵化。

Method: 使用基于自适应Charm++运行时的CharmTyles框架构建形似NumPy语法的分布式抽象。

Result: 展示了抽象的资源弹性，对相关开销进行性能分析，相比专用模板DSL和通用NumPy替代方案有显著性能提升。

Conclusion: 所提出的自适应分布式抽象可有效填补Python科学计算原型与高性能执行间的差距。

Abstract: The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.

</details>


### [32] [Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions](https://arxiv.org/abs/2512.19972)
*Pengchao Han,Xi Huang,Yi Fang,Guojun Han*

Main category: cs.DC

TL;DR: 本文对协作学习中的知识蒸馏（KD）进行全面综述，聚焦记忆和知识的作用，分析不同协作学习模式，探讨任务和模型等异质性，对现有工作分类，讨论挑战并提出未来方向。


<details>
  <summary>Details</summary>
Motivation: 协作学习中KD利用跨代理的记忆和知识的潜在机制未得到充分探索，本文旨在弥补这一差距。

Method: 定义和分类KD过程中的记忆和知识，探索其相互关系；研究多种协作学习模式；强调多种任务中的任务异质性；分析模型、数据、资源异质性和隐私问题；对现有工作基于其处理记忆和知识的方式进行分类。

Result: 对现有工作进行了系统分类，明确了记忆和知识在不同协作学习模式中对KD有效性的影响。

Conclusion: 讨论了当前KD技术在协作学习中存在的挑战，并提出了未来的发展方向。

Abstract: Collaborative learning has emerged as a key paradigm in large-scale intelligent systems, enabling distributed agents to cooperatively train their models while addressing their privacy concerns. Central to this paradigm is knowledge distillation (KD), a technique that facilitates efficient knowledge transfer among agents. However, the underlying mechanisms by which KD leverages memory and knowledge across agents remain underexplored. This paper aims to bridge this gap by offering a comprehensive review of KD in collaborative learning, with a focus on the roles of memory and knowledge. We define and categorize memory and knowledge within the KD process and explore their interrelationships, providing a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings. We examine various collaborative learning patterns, including distributed, hierarchical, and decentralized structures, and provide insights into how memory and knowledge dynamics shape the effectiveness of KD in collaborative learning. Particularly, we emphasize task heterogeneity in distributed learning pattern covering federated learning (FL), multi-agent domain adaptation (MADA), federated multi-modal learning (FML), federated continual learning (FCL), federated multi-task learning (FMTL), and federated graph knowledge embedding (FKGE). Additionally, we highlight model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns of these tasks. Our analysis categorizes existing work based on how they handle memory and knowledge. Finally, we discuss existing challenges and propose future directions for advancing KD techniques in the context of collaborative learning.

</details>


### [33] [Scaling Point-based Differentiable Rendering for Large-scale Reconstruction](https://arxiv.org/abs/2512.20017)
*Hexu Zhao,Xiaoteng Liu,Xiwen Min,Jianhao Huang,Youming Deng,Yanfei Li,Ang Li,Jinyang Li,Aurojit Panda*

Main category: cs.DC

TL;DR: 本文提出用于点基可微渲染（PBDR）的分布式训练系统Gaian，可减少通信开销、提升训练吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有PBDR分布式训练系统与特定方法耦合且通信开销大，需高效系统

Method: 提出Gaian系统，提供统一API，利用数据访问信息优化局部性和减少通信

Result: 实现4种PBDR算法，在6个数据集、最多128个GPU上通信减少最多91%，训练吞吐量提升1.5 - 3.71倍

Conclusion: Gaian能高效用于PBDR分布式训练，高性能且资源高效

Abstract: Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.

</details>


### [34] [FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling](https://arxiv.org/abs/2512.20064)
*Yaojian Chen,Si-Qiu Gong,Lin Gan,Yanfei Liu,An Yang,Yinuo Wang,Chao-yang Lu,Guangwen Yang*

Main category: cs.DC

TL;DR: 提出Fast - MPS多级别并行框架用于可扩展MPS采样，在高斯玻色采样评估中表现出色，有高性能潜力。


<details>
  <summary>Details</summary>
Motivation: 传统数据并行在大规模MPS中有内存和I/O限制，模型并行有进程绑定和可扩展性问题。

Method: 结合样本间的数据并行和沿键维度的张量并行，通过压缩和重叠消除内存和I/O压力。

Result: 在高斯玻色采样中比现有模拟器提速超10倍，可扩展到数千个进程，能模拟8176个位点和键维度chi = 10^4的情况。

Conclusion: Fast - MPS在高性能张量网络应用中展现出巨大潜力。

Abstract: Matrix Product State (MPS) is a versatile tensor network representation widely applied in quantum physics, quantum chemistry, and machine learning, etc. MPS sampling serves as a critical fundamental operation in these fields. As the problems become more complex, the scale of MPS is rapidly increasing. Traditional data parallelism is limited by memory and heavy I/O in large-scale MPS. Model parallelism that can handle large-scale MPS imposes rigid process bindings and lacks scalability. This work proposes Fast-MPS, a multi-level parallel framework for scalable MPS sampling. Our design combines data parallelism across samples with tensor parallelism along bond dimensions. We eliminate memory and I/O pressure through compression and overlapping, and revive data parallel in large-scale MPS sampling. We evaluate our approach on Gaussian Boson Sampling, a representative and demanding application. Fast-MPS achieves over 10x speedup compared to existing simulators, scales to thousands of processes, and enables simulations with 8,176 sites and bond dimension chi = 10^4, significantly outperforming the state of the art. Fast-MPS has demonstrated great potential in high-performance tensor network applications.

</details>


### [35] [Population Protocols Revisited: Parity and Beyond](https://arxiv.org/abs/2512.20163)
*Leszek Gąsieniec,Tytus Grodzicki,Tomasz Jurdziński,Jakub Kowalski,Grzegorz Stachowiak*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.
  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\log^3 n)$ states and achieve silent stabilisation in $O(\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.

</details>


### [36] [Reaching Agreement Among Reasoning LLM Agents](https://arxiv.org/abs/2512.20184)
*Chaoyi Ruan,Yiliang Wang,Ziji Shi,Jialin Li*

Main category: cs.DC

TL;DR: 本文指出现有多智能体编排方法存在问题，提出形式化模型和Aegean共识协议，经评估可保障安全和活性，降低延迟且不牺牲答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体编排依赖静态启发式工作流，存在浪费计算资源、高延迟和达成临时协议等问题，需要为可靠的多智能体推理建立形式化基础。

Method: 提出多智能体细化问题的形式化模型，引入Aegean共识协议，在Aegean - Serve服务引擎中实现该协议，进行增量法定人数检测以实现提前终止。

Result: 使用四个数学推理基准评估，Aegean可提供可证明的安全和活性保证，与现有基线相比，延迟降低1.2 - 20倍，答案质量保持在2.5%以内。

Conclusion: 基于共识的编排消除了拖后腿者的延迟，且不牺牲正确性，在本地GPU部署和商业API提供商中均有一致收益。

Abstract: Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.
  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.

</details>


### [37] [Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs](https://arxiv.org/abs/2512.20210)
*Yinan Ni,Xiao Yang,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: 提出P-LoRA系统解决无服务器环境下多微调大语言模型推理挑战，减少冷启动延迟，提高GPU内存利用率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算部署大语言模型推理服务有优势，但通过LoRA服务多微调模型面临冷启动延迟和GPU内存碎片化问题。

Method: 引入轻量级LSTM流量预测器预取热适配器，采用基于页面的适配器内存管理机制。

Result: 实验表明P-LoRA吞吐量比S-LoRA高1.52倍，高并发下平均TTFT降低35%。

Conclusion: P-LoRA能有效解决无服务器环境下LoRA模型推理的问题，提升性能。

Abstract: The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.

</details>


### [38] [Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults](https://arxiv.org/abs/2512.20394)
*Mohammad Walid Charrwi,Zaid Hussain*

Main category: cs.DC

TL;DR: 本文针对高斯互连网络提出故障感知的强化学习路由方案，实验表明该方案在数据包传输率和拥塞管理上优于贪婪自适应路由算法。


<details>
  <summary>Details</summary>
Motivation: 高斯互连网络中自适应路由技术易受节点和链路故障影响，传统确定性路由难以应对节点故障，因此需要新的路由方案。

Method: 提出基于PPO代理和特定奖励结构的故障感知强化学习路由方案，并与贪婪自适应最短路径路由算法对比。

Result: 在40%故障密度下，强化学习代理数据包传输率达0.95，优于贪婪算法的0.66；在20%低网络负载下，传输率为0.57，优于贪婪算法的0.43。

Conclusion: 强化学习路由方案在随机、易故障拓扑中有效，能更好管理拥塞。

Abstract: As Network-on-Chip (NoC) and Wireless Sensor Network architectures continue to scale, the topology of the underlying network becomes a critical factor in performance. Gaussian Interconnected Networks based on the arithmetic of Gaussian integers, offer attractive properties regarding diameter and symmetry. Despite their attractive theoretical properties, adaptive routing techniques in these networks are vulnerable to node and link faults, leading to rapid degradation in communication reliability. Node failures (particularly those following Gaussian distributions, such as thermal hotspots or physical damage clusters) pose severe challenges to traditional deterministic routing. This paper proposes a fault-aware Reinforcement Learning (RL) routing scheme tailored for Gaussian Interconnected Networks. By utilizing a PPO (Proximal Policy Optimization) agent with a specific reward structure designed to penalize fault proximity, the system dynamically learns to bypass faulty regions. We compare our proposed RL-based routing protocol against a greedy adaptive shortest-path routing algorithm. Experimental results demonstrate that the RL agent significantly outperforms the adaptive routing sustaining a Packet Delivery Ratio (PDR) of 0.95 at 40% fault density compared to 0.66 for the greedy. Furthermore, the RL approach exhibits effective delivery rates compared to the greedy adaptive routing, particularly under low network load of 20% at 0.57 vs. 0.43, showing greater proficiency in managing congestion, validating its efficacy in stochastic, fault-prone topologies

</details>


### [39] [WOC: Dual-Path Weighted Object Consensus Made Efficient](https://arxiv.org/abs/2512.20485)
*Tanisha Fonseca,Gengrui Zhang*

Main category: cs.DC

TL;DR: 提出双路径共识协议WOC，可根据操作访问模式路由操作，在特定工作负载下吞吐量比Cabinet高4倍。


<details>
  <summary>Details</summary>
Motivation: 现有共识协议只能优化节点异构性或工作负载独立性其中之一，不能兼顾。

Method: 提出WOC协议，根据操作访问模式将操作动态路由到两条路径，独立操作走快速路径，冲突或共享对象走慢速路径。

Result: 对于独立对象超70%的工作负载，WOC吞吐量比Cabinet高4倍，高竞争下性能相当。

Conclusion: WOC协议有效解决了现有共识协议不能兼顾节点异构性和工作负载独立性的问题。

Abstract: Modern distributed systems face a critical challenge: existing consensus protocols optimize for either node heterogeneity or workload independence, but not both. For example, Cabinet leverages weighted quorums to handle node heterogeneity but serializes all operations through a global leader, limiting parallelism. EPaxos enables parallel execution for independent operations but treats all nodes uniformly, ignoring performance differences. To tackle this problem, we present WOC, a dual-path consensus protocol that dynamically routes operations into two paths based on their access patterns. Independent operations execute through a fast path that uses object-specific weighted quorums and completes in one network round-trip. Conflicting or shared objects route through a leader-coordinated slow path employing node-weighted consensus. Our evaluation demonstrates that WOC achieves up to 4X higher throughput than Cabinet for workloads with >70% independent objects, while maintaining equivalent performance under high contention.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [40] [Approximation and parameterized algorithms for covering disjointness-compliable set families](https://arxiv.org/abs/2512.20180)
*Zeev Nutov,Anael Vaknin*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A set-family ${\cal F}$ is disjointness-compliable if $A' \subseteq A \in {\cal F}$ implies $A' \in {\cal F}$ or $A \setminus A' \in {\cal F}$; if ${\cal F}$ is also symmetric then ${\cal F}$ is proper. A classic result of Goemans and Williamson [SODA 92:307-316] states that the problem of covering a proper set-family by a min-cost edge set admits approximation ratio $2$, by a classic primal-dual algorithm. However, there are several famous algorithmic problems whose set-family ${\cal F}$ is disjointness-compliable but not symmetric -- among them $k$-Minimum Spanning Tree ($k$-MST), Generalized Point-to-Point Connection (G-P2P), Group Steiner, Covering Steiner, multiroot versions of these problems, and others. We will show that any such problem admits approximation ratio $O(α\log τ)$, where $τ$ is the number of inclusion-minimal sets in the family ${\cal F}$ that models the problem and $α$ is the best known approximation ratio for the case when $τ=1$. This immediately implies several results, among them the following two. (i) The first deterministic polynomial time $O(\log n)$-approximation algorithm for the G-P2P problem. Here the $τ=1$ case is the $k$-MST problem. (ii) Approximation ratio $O(\log^4 n)$ for the multiroot version of the Covering Steiner problem, where each root has its own set of groups. Here the $τ=1$ case is the Covering Steiner problem.
  We also discuss the parameterized complexity of covering a disjointness-compliable family ${\cal F}$, when parametrized by $τ$. We will show that if ${\cal F}$ is proper then the problem is fixed parameter tractable and can be solved in time $O^*(3^τ)$. For the non-symmetric case we will show that the problem admits approximation ratio between $α$ and $α+1$ in time $O^*(3^τ)$, which is essentially the best possible.

</details>


### [41] [On the near-tightness of $χ\leq 2r$: a general $σ$-ary construction and a binary case via LFSRs](https://arxiv.org/abs/2512.20598)
*Vinicius T. V. Date,Leandro M. Zatesko*

Main category: cs.DS

TL;DR: 本文探讨压缩字符串索引中后缀集重复度量χ与r的关系，分析χ ≤ 2r界限渐近紧性，用两种构造方法，发现德布鲁因序列在σ ≥ 3时无法缩小差距。


<details>
  <summary>Details</summary>
Motivation: 此前研究证明χ ≤ 2r，但实证显示此界限宽松，为理解差距开展研究。

Method: 给出任意σ值的一般构造以及二进制字母表（德布鲁因序列）的情况。

Result: 给出了两种情况的渐近紧性分析，发现德布鲁因序列在σ ≥ 3时不能缩小界限差距。

Conclusion: 对χ ≤ 2r界限的渐近紧性有更深入理解，德布鲁因序列有一定特性但在σ ≥ 3时无法缩小差距。

Abstract: In the field of compressed string indexes, recent work has introduced suffixient sets and their corresponding repetitiveness measure $χ$. In particular, researchers have explored its relationship to other repetitiveness measures, notably $r$, the number of runs in the Burrows--Wheeler Transform (BWT) of a string. Navarro et al. (2025) proved that $χ\leq 2r$, although empirical results by Cenzato et al. (2024) suggest that this bound is loose, with real data bounding $χ$ by around $1.13r$ to $1.33r$ when the size of the alphabet is $σ= 4$. To better understand this gap, we present two cases for the asymptotic tightness of the $χ\leq 2r$ bound: a general construction for arbitrary $σ$ values, and a binary alphabet case, consisting of de Bruijn sequences constructed by linear-feedback shift registers (LFSRs) from primitive polynomials over $\mathbb{F}_2$. The second is a novel characterization of which de Bruijn sequences achieve the literature run-minimal pattern for the cyclic BWT. Moreover, we show that de Bruijn sequences fail to close the gap for $σ\geq 3$.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [42] [Maximizing the Egalitarian Welfare in Friends and Enemies Games](https://arxiv.org/abs/2512.20261)
*Edith Elkind,Michele Flammini,Giovanna Varricchio*

Main category: cs.GT

TL;DR: 本文研究朋友与敌人博弈中最大化平等福利的复杂性，分析了FA和EA两种场景，给出难解性、近似算法及特殊可行情况。


<details>
  <summary>Details</summary>
Motivation: 探究朋友与敌人博弈中（参与者将他人分为朋友和敌人）的两种经典场景下最大化平等福利的复杂度问题。

Method: 理论分析证明难解性，设计多项式时间近似算法，针对不同条件求解。

Result: EA难以近似，给出(n - 1)近似算法；FA证明NP难并设计近似算法，在不同条件下有不同近似比，还给出两种变体的近似比。此外，找出两种场景下可在多项式时间计算最优解的特殊情况。

Conclusion: 明确了两种场景下最大化平等福利问题的复杂度及相应近似算法，找到特殊可解情况。

Abstract: We consider the complexity of maximizing egalitarian welfare in Friends and Enemies Games -- a subclass of hedonic games in which every agent partitions other agents into friends and enemies. We investigate two classic scenarios proposed in the literature, namely, Friends Appreciation ($\mathsf{FA}$) and Enemies Aversion ($\mathsf{EA}$): in the former, each agent primarily cares about the number of friends in her coalition, breaking ties based on the number of enemies, while in the latter, the opposite is true. For $\mathsf{EA}$, we show that our objective is hard to approximate within $O(n^{1-ε})$, for any fixed $ε>0$, and provide a polynomial-time $(n-1)$-approximation. For $\mathsf{FA}$, we obtain an NP-hardness result and a polynomial-time approximation algorithm. Our algorithm achieves a ratio of $2-Θ(\frac{1}{n})$ when every agent has at least two friends; however, if some agent has at most one friend, its approximation ratio deteriorates to $n/2$. We recover the $2-Θ(\frac{1}{n})$ approximation ratio for two important variants: when randomization is allowed and when the friendship relationship is symmetric. Additionally, for both $\mathsf{EA}$ and $\mathsf{FA}$ we identify special cases where the optimal egalitarian partition can be computed in polynomial time.

</details>


### [43] [Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis](https://arxiv.org/abs/2512.17979)
*Matthieu Mastio,Paul Saves,Benoit Gaudou,Nicolas Verstaevel*

Main category: cs.GT

TL;DR: 本文开发基于主体的模型研究产业共生，揭示分散交易收敛的条件，为政策干预提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有模型常忽略空间结构、市场设计和企业适应性行为的相互作用，限制对产业共生出现地点和方式的理解。

Method: 开发基于主体的模型，企业通过空间嵌入的双向拍卖市场交易副产品，利用强化学习让企业调整投标策略。

Result: 模拟实验揭示分散交易收敛到稳定有效结果的经济和空间条件，反事实遗憾分析和敏感性分析有相应发现。

Conclusion: 模型为探索政策干预提供基础，展示了空间受限市场中适应性主体如何实现分散协调。

Abstract: Industrial symbiosis fosters circularity by enabling firms to repurpose residual resources, yet its emergence is constrained by socio-spatial frictions that shape costs, matching opportunities, and market efficiency. Existing models often overlook the interaction between spatial structure, market design, and adaptive firm behavior, limiting our understanding of where and how symbiosis arises. We develop an agent-based model where heterogeneous firms trade byproducts through a spatially embedded double-auction market, with prices and quantities emerging endogenously from local interactions. Leveraging reinforcement learning, firms adapt their bidding strategies to maximize profit while accounting for transport costs, disposal penalties, and resource scarcity. Simulation experiments reveal the economic and spatial conditions under which decentralized exchanges converge toward stable and efficient outcomes. Counterfactual regret analysis shows that sellers' strategies approach a near Nash equilibrium, while sensitivity analysis highlights how spatial structures and market parameters jointly govern circularity. Our model provides a basis for exploring policy interventions that seek to align firm incentives with sustainability goals, and more broadly demonstrates how decentralized coordination can emerge from adaptive agents in spatially constrained markets.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [44] [Towards Analysing Invoices and Receipts with Amazon Textract](https://arxiv.org/abs/2512.19958)
*Sneha Oommen,Gabby Sanchez,Cassandra T. Britto,Di Wang,Jordan Chiou,Maria Spichkova*

Main category: cs.IR

TL;DR: 评估AWS Textract从收据提取数据的能力，分析其优缺点并提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 评估AWS Textract在从收据提取数据方面的表现。

Method: 使用包含不同格式和条件收据的数据集来分析Textract的功能。

Result: 能持续检测到收据总额，但存在受图像质量和布局影响的典型问题和不规则情况。

Conclusion: 基于观察分析提出了缓解策略。

Abstract: This paper presents an evaluation of the AWS Textract in the context of extracting data from receipts. We analyse Textract functionalities using a dataset that includes receipts of varied formats and conditions. Our analysis provided a qualitative view of Textract strengths and limitations. While the receipts totals were consistently detected, we also observed typical issues and irregularities that were often influenced by image quality and layout. Based on the analysis of the observations, we propose mitigation strategies.

</details>


### [45] [IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation](https://arxiv.org/abs/2512.19983)
*Ziyuan Guo,Jie Guo,Zhenghao Chen,Bin Song,Fei Richard Yu*

Main category: cs.IR

TL;DR: 提出IGDMRec方法解决基于结构的多模态推荐系统语义图噪声问题，实验显示其优于基线。


<details>
  <summary>Details</summary>
Motivation: 基于结构的多模态推荐系统构建的语义图因多模态信息固有噪声和语义与用户 - 物品共现关系不一致而有噪声，导致推荐效果不佳。

Method: 提出IGDMRec方法，引入BGD模块结合用户交互数据指导语义图去噪，设计CD - Net实现去噪，提出对比表示增强方案。

Result: 在四个真实数据集上的大量实验表明IGDMRec优于竞争基线，鲁棒性分析验证去噪能力，消融实验验证关键组件有效性。

Conclusion: IGDMRec能有效解决基于结构的多模态推荐系统语义图噪声问题，提升推荐效果。

Abstract: Multimodal recommender systems (MRSs) are critical for various online platforms, offering users more accurate personalized recommendations by incorporating multimodal information of items. Structure-based MRSs have achieved state-of-the-art performance by constructing semantic item graphs, which explicitly model relationships between items based on modality feature similarity. However, such semantic item graphs are often noisy due to 1) inherent noise in multimodal information and 2) misalignment between item semantics and user-item co-occurrence relationships, which introduces false links and leads to suboptimal recommendations. To address this challenge, we propose Item Graph Diffusion for Multimodal Recommendation (IGDMRec), a novel method that leverages a diffusion model with classifier-free guidance to denoise the semantic item graph by integrating user behavioral information. Specifically, IGDMRec introduces a Behavior-conditioned Graph Diffusion (BGD) module, incorporating interaction data as conditioning information to guide the denoising of the semantic item graph. Additionally, a Conditional Denoising Network (CD-Net) is designed to implement the denoising process with manageable complexity. Finally, we propose a contrastive representation augmentation scheme that leverages both the denoised item graph and the original item graph to enhance item representations. \LL{Extensive experiments on four real-world datasets demonstrate the superiority of IGDMRec over competitive baselines, with robustness analysis validating its denoising capability and ablation studies verifying the effectiveness of its key components.

</details>


### [46] [LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews](https://arxiv.org/abs/2512.20022)
*Kian Godhwani,David Benrimoh*

Main category: cs.IR

TL;DR: 研究开发OLIVER用于大语言模型辅助摘要筛选，评估多种模型，发现单模型性能受多因素影响，演员 - 批评家框架可提升分类质量和可靠度。


<details>
  <summary>Details</summary>
Motivation: 先前大语言模型评估存在局限性，如聚焦早期模型、标准化综述、单模型设置等，需研究可推广性、配置效果和校准情况。

Method: 开发OLIVER开源管道，评估多种当代大语言模型在两个非Cochrane系统评价中的表现，用多种指标评估，测试演员 - 批评家筛选框架。

Result: 单模型性能差异大，在不同综述中各有优缺点，校准普遍较弱；演员 - 批评家筛选提升了区分度，降低校准误差，提高AUC。

Conclusion: 单模型性能受多因素影响，校准有限；演员 - 批评家框架可提升分类质量和信心可靠度，能低成本进行大规模筛选。

Abstract: Introduction: Recent work suggests large language models (LLMs) can accelerate screening, but prior evaluations focus on earlier LLMs, standardized Cochrane reviews, single-model setups, and accuracy as the primary metric, leaving generalizability, configuration effects, and calibration largely unexamined.
  Methods: We developed OLIVER (Optimized LLM-based Inclusion and Vetting Engine for Reviews), an open-source pipeline for LLM-assisted abstract screening. We evaluated multiple contemporary LLMs across two non-Cochrane systematic reviews and performance was assessed at both the full-text screening and final inclusion stages using accuracy, AUC, and calibration metrics. We further tested an actor-critic screening framework combining two lightweight models under three aggregation rules.
  Results: Across individual models, performance varied widely. In the smaller Review 1 (821 abstracts, 63 final includes), several models achieved high sensitivity for final includes but at the cost of substantial false positives and poor calibration. In the larger Review 2 (7741 abstracts, 71 final includes), most models were highly specific but struggled to recover true includes, with prompt design influencing recall. Calibration was consistently weak across single-model configurations despite high overall accuracy. Actor-critic screening improved discrimination and markedly reduced calibration error in both reviews, yielding higher AUCs.
  Discussion: LLMs may eventually accelerate abstract screening, but single-model performance is highly sensitive to review characteristics, prompting, and calibration is limited. An actor-critic framework improves classification quality and confidence reliability while remaining computationally efficient, enabling large-scale screening at low cost.

</details>


### [47] [VSA:Visual-Structural Alignment for UI-to-Code](https://arxiv.org/abs/2512.20034)
*Xian Wu,Ming Zhang,Zhiyu Fang,Fei Li,Bin Wang,Yong Jiang,Hao Zhou*

Main category: cs.IR

TL;DR: 提出VSA范式解决现有设计到代码转换方法生成代码缺乏结构、难维护的问题，实验表明其提升了代码模块化和架构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大跨模态模型进行设计到代码转换时生成的代码无结构、兼容性差、维护难，需要改进。

Method: 采用多阶段范式VSA，先通过空间感知变压器将视觉输入重构为层次树表示，再用算法模式匹配层识别重复UI主题并封装成模块模板，最后通过模式驱动合成引擎生成适合生产环境的组件。

Result: 框架在代码模块化和架构一致性上比现有基准有显著提升。

Conclusion: VSA范式有效弥合了原始像素与可扩展软件工程之间的差距。

Abstract: The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.

</details>


### [48] [Collaborative Group-Aware Hashing for Fast Recommender Systems](https://arxiv.org/abs/2512.20172)
*Yan Zhang,Li Deng,Lixin Duan,Ivor W. Tsang,Guowu Yang*

Main category: cs.IR

TL;DR: 提出CGAH方法用于推荐系统，整合群组信息缓解稀疏问题，实验证明其优于现有离散方法。


<details>
  <summary>Details</summary>
Motivation: 在线推荐需兼顾速度与稀疏场景下的准确性，现有基于哈希的推荐方法在稀疏场景下准确率低。

Method: 提出CGAH方法，先提取用户和物品的群组亲和力，再将偏好表示为群组亲和力和哈希码相似度的内积，融入群组信息学习哈希码。

Result: 在三个公开数据集上的实验表明，CGAH和CGAH - CF在不同稀疏设置下优于现有离散协同过滤和内容感知推荐方法。

Conclusion: CGAH方法在稀疏场景下能提高在线推荐的准确性。

Abstract: The fast online recommendation is critical for applications with large-scale databases; meanwhile, it is challenging to provide accurate recommendations in sparse scenarios. Hash technique has shown its superiority for speeding up the online recommendation by bit operations on Hamming distance computations. However, existing hashing-based recommendations suffer from low accuracy, especially with sparse settings, due to the limited representation capability of each bit and neglected inherent relations among users and items. To this end, this paper lodges a Collaborative Group-Aware Hashing (CGAH) method for both collaborative filtering (namely CGAH-CF) and content-aware recommendations (namely CGAH) by integrating the inherent group information to alleviate the sparse issue. Firstly, we extract inherent group affinities of users and items by classifying their latent vectors into different groups. Then, the preference is formulated as the inner product of the group affinity and the similarity of hash codes. By learning hash codes with the inherent group information, CGAH obtains more effective hash codes than other discrete methods with sparse interactive data. Extensive experiments on three public datasets show the superior performance of our proposed CGAH and CGAH-CF over the state-of-the-art discrete collaborative filtering methods and discrete content-aware recommendations under different sparse settings.

</details>


### [49] [Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register](https://arxiv.org/abs/2512.20458)
*Shuting Wang,Qiaolin Xia,Hao Wang,Yu Lu,Bobsimons,Zhicheng Dou*

Main category: cs.IR

TL;DR: 现有智能体搜索框架有缺陷，本文提出Laser框架，在实验中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有智能体搜索框架存在推理轨迹不稳定、上下文溢出和复杂查询性能下降等问题，需要改进。

Method: 引入Laser框架，定义符号动作协议，将智能体行为分为三个空间，设置明确语义和执行格式，维护紧凑上下文寄存器。

Result: 在Qwen2.5/3系列模型的多跳问答数据集实验中，Laser在提示和微调设置下均优于现有基线。

Conclusion: Laser为强大、可扩展的智能体搜索提供了有效基础。

Abstract: Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.

</details>


### [50] [Making Large Language Models Efficient Dense Retrievers](https://arxiv.org/abs/2512.20612)
*Yibin Lei,Shwai He,Ang Li,Andrew Yates*

Main category: cs.IR

TL;DR: 分析基于大语言模型的密集检索器层冗余，提出EffiR框架，可在保留性能下减小模型大小和推理成本。


<details>
  <summary>Details</summary>
Motivation: 直接微调大语言模型用于密集检索计算效率低，不确定检索任务中是否存在层冗余。

Method: 对基于大语言模型的密集检索器进行层冗余分析，提出EffiR框架，通过粗到细策略进行大规模MLP压缩并结合特定微调。

Result: 在不同BEIR数据集和大语言模型骨干上，EffiR能大幅减小模型大小和推理成本，保留全尺寸模型性能。

Conclusion: EffiR框架能有效提高基于大语言模型的密集检索器效率。

Abstract: Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [Large Language Models for EDA Cloud Job Resource and Lifetime Prediction](https://arxiv.org/abs/2512.19701)
*Yuxuan Yin,Shengke Zhou,Yunjie Zhang,Ajay Mohindra,Boxun Xu,Peng Li*

Main category: cs.LG

TL;DR: 提出微调大语言模型框架解决EDA行业云计算资源和作业寿命预测问题，在真实数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 云计算在EDA行业快速发展，传统机器学习方法难以处理复杂异构的EDA工作负载，需资源和作业寿命预测以实现最优调度。

Method: 提出通过文本到文本回归微调大语言模型的框架，引入科学记数法和前缀填充约束模型，使用全注意力微调与推理。

Result: 提高了输出格式可靠性和滑动窗口注意力大语言模型的预测准确性，在真实云数据集上验证框架有效性。

Conclusion: 所提框架为EDA领域性能预测设定了新基准。

Abstract: The rapid growth of cloud computing in the Electronic Design Automation (EDA) industry has created a critical need for resource and job lifetime prediction to achieve optimal scheduling. Traditional machine learning methods often struggle with the complexity and heterogeneity of EDA workloads, requiring extensive feature engineering and domain expertise. We propose a novel framework that fine-tunes Large Language Models (LLMs) to address this challenge through text-to-text regression. We introduce the scientific notation and prefix filling to constrain the LLM, significantly improving output format reliability. Moreover, we found that full-attention finetuning and inference improves the prediction accuracy of sliding-window-attention LLMs. We demonstrate the effectiveness of our proposed framework on real-world cloud datasets, setting a new baseline for performance prediction in the EDA domain.

</details>


### [52] [Relu and softplus neural nets as zero-sum turn-based games](https://arxiv.org/abs/2512.20582)
*Stephane Gaubert,Yiannis Vlassopoulos*

Main category: cs.LG

TL;DR: 本文将ReLU神经网络输出解释为零和回合制停止游戏的值，推导路径积分公式，用于输出边界推导和鲁棒性验证，训练网络变为逆游戏问题，该方法也适用于Softplus激活函数网络。


<details>
  <summary>Details</summary>
Motivation: 为ReLU神经网络输出提供新的解释视角，探索其与博弈论的联系，以解决输出边界推导、鲁棒性验证和网络训练等问题。

Method: 将神经网络输出与零和回合制停止游戏联系，利用Shapley - Bellman反向递归，推导离散Feynman - Kac型路径积分公式。

Result: 得到神经网络输出的游戏理论表示，可用于推导输出边界、验证鲁棒性，网络训练变为逆游戏问题，该方法也适用于Softplus激活函数网络。

Conclusion: 提出的基于博弈论的方法可有效应用于ReLU和Softplus激活函数的神经网络，在输出分析和网络训练等方面有重要作用。

Abstract: We show that the output of a ReLU neural network can be interpreted as the value of a zero-sum, turn-based, stopping game, which we call the ReLU net game. The game runs in the direction opposite to that of the network, and the input of the network serves as the terminal reward of the game. In fact, evaluating the network is the same as running the Shapley-Bellman backward recursion for the value of the game. Using the expression of the value of the game as an expected total payoff with respect to the path measure induced by the transition probabilities and a pair of optimal policies, we derive a discrete Feynman-Kac-type path-integral formula for the network output. This game-theoretic representation can be used to derive bounds on the output from bounds on the input, leveraging the monotonicity of Shapley operators, and to verify robustness properties using policies as certificates. Moreover, training the neural network becomes an inverse game problem: given pairs of terminal rewards and corresponding values, one seeks transition probabilities and rewards of a game that reproduces them. Finally, we show that a similar approach applies to neural networks with Softplus activation functions, where the ReLU net game is replaced by its entropic regularization.

</details>


### [53] [Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches](https://arxiv.org/abs/2512.19713)
*Taoran Sheng,Manfred Huber*

Main category: cs.LG

TL;DR: 文章对可穿戴设备的人体活动识别（HAR）进行全监督范围研究，比较多种学习方法，结果显示弱监督等方法有优势，新框架在数据有限时表现好。


<details>
  <summary>Details</summary>
Motivation: 解决传统全监督HAR需大量标注数据成本高，无监督方法性能不佳的问题，探索减少标注需求并保持精度的方法。

Method: 开发并比较传统全监督学习、基本无监督学习、带约束的弱监督学习、知识共享的多任务学习、基于领域知识的自监督学习和弱自监督学习框架。

Result: 弱监督方法性能与全监督相当且减少监督需求；多任务框架通过知识共享提升性能；弱自监督方法用10%标注数据效率高。

Conclusion: 不同学习范式优势互补，新的弱自监督框架为标注数据有限的实际HAR应用提供了有前景的解决方案。

Abstract: Human activity recognition (HAR) using wearable sensors has advanced through various machine learning paradigms, each with inherent trade-offs between performance and labeling requirements. While fully supervised techniques achieve high accuracy, they demand extensive labeled datasets that are costly to obtain. Conversely, unsupervised methods eliminate labeling needs but often deliver suboptimal performance. This paper presents a comprehensive investigation across the supervision spectrum for wearable-based HAR, with particular focus on novel approaches that minimize labeling requirements while maintaining competitive accuracy. We develop and empirically compare: (1) traditional fully supervised learning, (2) basic unsupervised learning, (3) a weakly supervised learning approach with constraints, (4) a multi-task learning approach with knowledge sharing, (5) a self-supervised approach based on domain expertise, and (6) a novel weakly self-supervised learning framework that leverages domain knowledge and minimal labeled data. Experiments across benchmark datasets demonstrate that: (i) our weakly supervised methods achieve performance comparable to fully supervised approaches while significantly reducing supervision requirements; (ii) the proposed multi-task framework enhances performance through knowledge sharing between related tasks; (iii) our weakly self-supervised approach demonstrates remarkable efficiency with just 10\% of labeled data. These results not only highlight the complementary strengths of different learning paradigms, offering insights into tailoring HAR solutions based on the availability of labeled data, but also establish that our novel weakly self-supervised framework offers a promising solution for practical HAR applications where labeled data are limited.

</details>


### [54] [Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data](https://arxiv.org/abs/2512.19716)
*Behrooz Mamandipoor,Chun-Nan Hsu,Martin Krause,Ulrich H. Schmidt,Rodney A. Gabriel*

Main category: cs.LG

TL;DR: 本文开发多模态深度学习模型，用结构化和非结构化临床数据预测重症患者住院死亡率，验证模型效果并强调多源信息和外部验证的重要性。


<details>
  <summary>Details</summary>
Motivation: 早期预测重症患者住院死亡率，帮助临床医生优化治疗。

Method: 使用MIMIC - III、MIMIC - IV、eICU和HiRID数据，在MIMIC数据集上开发多模态模型，以24小时内时间序列数据预测住院死亡率，在不同数据集进行外部验证。

Result: 整合结构化数据的模型有较好指标，外部验证AUROC在0.84 - 0.92；加入临床笔记和影像数据后指标提升。

Conclusion: 强调纳入多源患者信息进行死亡率预测以及外部验证的重要性。

Abstract: Early prediction of in-hospital mortality in critically ill patients can aid clinicians in optimizing treatment. The objective was to develop a multimodal deep learning model, using structured and unstructured clinical data, to predict in-hospital mortality risk among critically ill patients after their initial 24 hour intensive care unit (ICU) admission. We used data from MIMIC-III, MIMIC-IV, eICU, and HiRID. A multimodal model was developed on the MIMIC datasets, featuring time series components occurring within the first 24 hours of ICU admission and predicting risk of subsequent inpatient mortality. Inputs included time-invariant variables, time-variant variables, clinical notes, and chest X-ray images. External validation occurred in a temporally separated MIMIC population, HiRID, and eICU datasets. A total of 203,434 ICU admissions from more than 200 hospitals between 2001 to 2022 were included, in which mortality rate ranged from 5.2% to 7.9% across the four datasets. The model integrating structured data points had AUROC, AUPRC, and Brier scores of 0.92, 0.53, and 0.19, respectively. We externally validated the model on eight different institutions within the eICU dataset, demonstrating AUROCs ranging from 0.84-0.92. When including only patients with available clinical notes and imaging data, inclusion of notes and imaging into the model, the AUROC, AUPRC, and Brier score improved from 0.87 to 0.89, 0.43 to 0.48, and 0.37 to 0.17, respectively. Our findings highlight the importance of incorporating multiple sources of patient information for mortality prediction and the importance of external validation.

</details>


### [55] [Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference](https://arxiv.org/abs/2512.19717)
*Zhan Zhang*

Main category: cs.LG

TL;DR: 提出倒置因果聚焦算法（ICFA）用于在大候选空间中寻找稀有有用解，给出相关方法、诊断工具，进行实验并展示混合架构。


<details>
  <summary>Details</summary>
Motivation: 解决在语言生成、规划和强化学习等领域的大候选空间中寻找稀有有用解的实际挑战。

Method: 将搜索视为目标条件重加权过程，重用已有提议采样器和相似度函数形成聚焦采样分布，自适应控制聚焦强度。

Result: 提供清晰方法、稳定性诊断工具、理论解释，进行约束语言生成和稀疏奖励导航实验，展示结构化提示和混合架构。

Conclusion: ICFA可减少样本需求，结构化提示可实现近似语言级ICFA，混合架构结合了提示推理和算法重加权。

Abstract: Finding rare but useful solutions in very large candidate spaces is a recurring practical challenge across language generation, planning, and reinforcement learning. We present a practical framework, \emph{Inverted Causality Focusing Algorithm} (ICFA), that treats search as a target-conditioned reweighting process. ICFA reuses an available proposal sampler and a task-specific similarity function to form a focused sampling distribution, while adaptively controlling focusing strength to avoid degeneracy. We provide a clear recipe, a stability diagnostic based on effective sample size, a compact theoretical sketch explaining when ICFA can reduce sample needs, and two reproducible experiments: constrained language generation and sparse-reward navigation. We further show how structured prompts instantiate an approximate, language-level form of ICFA and describe a hybrid architecture combining prompted inference with algorithmic reweighting.

</details>


### [56] [Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data](https://arxiv.org/abs/2512.19718)
*Vasileios C. Pezoulas,Nikolaos S. Tachos,Eleni Georga,Kostas Marias,Manolis Tsiknakis,Dimitrios I. Fotiadis*

Main category: cs.LG

TL;DR: 在人工智能时代，合成数据应用广泛，但评估方法零散。本文介绍SDB库用于评估合成表格数据保真度，并通过三个用例展示其适用性。


<details>
  <summary>Details</summary>
Motivation: 当前合成数据评估方法存在碎片化、脚本随意和报告不完整的问题，需要统一工具进行评估。

Method: 引入模块化Python库SDB，支持自动化特征类型检测、多种保真度指标和可视化模式，并通过三个不同领域的实际用例进行评估。

Result: 用例显示SDB能应对不同的数据保真度评估挑战，在不同领域提供一致、透明和可重复的基准测试。

Conclusion: SDB可有效解决合成数据评估问题，在不同领域具有广泛适用性。

Abstract: In the rapidly evolving era of Artificial Intelligence (AI), synthetic data are widely used to accelerate innovation while preserving privacy and enabling broader data accessibility. However, the evaluation of synthetic data remains fragmented across heterogeneous metrics, ad-hoc scripts, and incomplete reporting practices. To address this gap, we introduce Synthetic Data Blueprint (SDB), a modular Pythonic based library to quantitatively and visually assess the fidelity of synthetic tabular data. SDB supports: (i) automated feature-type detection, (ii) distributional and dependency-level fidelity metrics, (iii) graph- and embedding-based structure preservation scores, and (iv) a rich suite of data visualization schemas. To demonstrate the breadth, robustness, and domain-agnostic applicability of the SDB, we evaluated the framework across three real-world use cases that differ substantially in scale, feature composition, statistical complexity, and downstream analytical requirements. These include: (i) healthcare diagnostics, (ii) socioeconomic and financial modelling, and (iii) cybersecurity and network traffic analysis. These use cases reveal how SDB can address diverse data fidelity assessment challenges, varying from mixed-type clinical variables to high-cardinality categorical attributes and high-dimensional telemetry signals, while at the same time offering a consistent, transparent, and reproducible benchmarking across heterogeneous domains.

</details>


### [57] [Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries](https://arxiv.org/abs/2512.19719)
*Zihao Lv,Siqi Ai,Yanbin Zhang*

Main category: cs.LG

TL;DR: 提出多尺度双路径特征聚合网络MDFA - Net进行电池剩余使用寿命（RUL）预测，测试显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前评估电池退化序列局部和全局相关性的建模技术效率低，难以满足实际应用需求。

Method: 提出MDFA - Net，包含保持浅层信息的多尺度特征网络MF - Net和捕捉序列连续趋势、保留深度细节的编码器网络EC - Net，整合深浅层属性。

Result: 使用两个公开锂离子电池数据集测试，该方法在RUL预测上超越现有顶级方法，能准确映射容量退化轨迹。

Conclusion: MDFA - Net在电池RUL预测方面有效且优于现有方法。

Abstract: Targeted maintenance strategies, ensuring the dependability and safety of industrial machinery. However, current modeling techniques for assessing both local and global correlation of battery degradation sequences are inefficient and difficult to meet the needs in real-life applications. For this reason, we propose a novel deep learning architecture, multiscale dual-path feature aggregation network (MDFA-Net), for RUL prediction. MDFA-Net consists of dual-path networks, the first path network, multiscale feature network (MF-Net) that maintains the shallow information and avoids missing information, and the second path network is an encoder network (EC-Net) that captures the continuous trend of the sequences and retains deep details. Integrating both deep and shallow attributes effectively grasps both local and global patterns. Testing conducted with two publicly available Lithium-ion battery datasets reveals our approach surpasses existing top-tier methods in RUL forecasting, accurately mapping the capacity degradation trajectory.

</details>


### [58] [Per-Axis Weight Deltas for Frequent Model Updates](https://arxiv.org/abs/2512.19720)
*Stefan Kuyumdzhiev,Radostin Cholakov*

Main category: cs.LG

TL;DR: 为解决多任务LLM变体服务受微调检查点大尺寸和冷启动延迟的限制，提出1位增量方案，可减少冷启动延迟和存储开销。


<details>
  <summary>Details</summary>
Motivation: 多任务LLM变体服务受微调检查点大尺寸和冷启动延迟限制，需新方法解决。

Method: 提出1位增量方案，存储权重差异符号和轻量级每轴FP16缩放因子，用精简加载器减少冷启动延迟和存储开销。

Result: 该方法比标量替代方案有更好的重建质量，工件比完整FP16检查点小几倍。

Conclusion: 方法即插即用，只需最少校准数据，避免密集重建以维持推理效率。

Abstract: Serving many task-specialized LLM variants is often limited by the large size of fine-tuned checkpoints and the resulting cold-start latency. Since fine-tuned weights differ from their base model by relatively small structured residuals, a natural approach is to represent them as compressed deltas. We propose a simple 1-bit delta scheme that stores only the sign of the weight difference together with lightweight per-axis (row/column) FP16 scaling factors, learned from a small calibration set. This design preserves the compactness of 1-bit deltas while more accurately capturing variation across weight dimensions, leading to improved reconstruction quality over scalar alternatives. From a systems perspective, a streamlined loader that transfers packed deltas in a single operation per module reduces cold-start latency and storage overhead, with artifacts several times smaller than a full FP16 checkpoint. The method is drop-in, requires minimal calibration data, and maintains inference efficiency by avoiding dense reconstruction. Our experimental setup and source code are available at https://github.com/kuiumdjiev/Per-Axis-Weight-Deltas-for-Frequent-Model-Updates.

</details>


### [59] [Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals](https://arxiv.org/abs/2512.19721)
*Vineet Yadav*

Main category: cs.LG

TL;DR: 提出带符号感知的多状态Jaccard/Tanimoto框架，扩展重叠距离，用于任意实值和复值信号，具有多种性质和分析方法，支持多应用。


<details>
  <summary>Details</summary>
Motivation: 将基于重叠的距离从非负向量和测度扩展到任意实值和复值信号。

Method: 将信号表示为有符号状态空间上的原子测度，进行嵌入，应用Tanimoto构造，通过Möbius反演进行联盟分析，归一化得到概率测度。

Result: 得到满足三角不等式、定义半正定核的[0,1]距离，可进行信号幅度分解和概率语义分析。

Conclusion: 该框架在一个带符号感知的框架内同时提供有界度量结构、半正定核、概率语义和透明预算核算，支持多领域分析工具。

Abstract: We introduce a sign-aware, multistate Jaccard/Tanimoto framework that extends overlap-based distances from nonnegative vectors and measures to arbitrary real- and complex-valued signals while retaining bounded metric and positive-semidefinite kernel structure. Formally, the construction is a set- and measure-theoretic geometry: signals are represented as atomic measures on a signed state space, and similarity is given by a generalized Jaccard overlap of these measures. Each signal is embedded into a nonnegative multistate representation, using positive/negative splits for real signals, Cartesian and polar decompositions for complex signals, and user-defined state partitions for refined regime analysis. Applying the Tanimoto construction to these embeddings yields a family of $[0,1]$ distances that satisfy the triangle inequality and define positive-semidefinite kernels usable directly in kernel methods and graph-based learning. Beyond pairwise distances, we develop coalition analysis via Möbius inversion, which decomposes signal magnitude into nonnegative, additive contributions with exact budget closure across coalitions of signals. Normalizing the same embeddings produces probability measures on coordinate -- state configurations, so that the distance becomes a monotone transform of total variation and admits a regime -- intensity decomposition. The resulting construction yields a single, mechanistically interpretable distance that simultaneously provides bounded metric structure, positive-semidefinite kernels, probabilistic semantics, and transparent budget accounting within one sign-aware framework, supporting correlograms, feature engineering, similarity graphs, and other analytical tools in scientific and financial applications.

</details>


### [60] [Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism](https://arxiv.org/abs/2512.19722)
*Alessandro Casadei,Clemens Grupp,Sreyoshi Bhaduri,Lu Guo,Wilson Fung,Rohit Malshe,Raj Ratan,Ankush Pole,Arkajit Rakshit*

Main category: cs.LG

TL;DR: 本文提出基于节点特定成本函数不对称性调整预测的方法，能动态结合成本不对称性，有自我调节机制，实现每年510万美元的节省。


<details>
  <summary>Details</summary>
Motivation: 为了利用节点特定成本函数不对称性来调整预测，实现节约成本。

Method: 将成本不对称性动态纳入预测误差概率分布，重点考虑成本最低的情景；通过计算节约额，利用自我调节机制根据观察到的节约额调整调整幅度。

Result: 该模型能够实现每年510万美元的节省。

Conclusion: 所提出的模型能利用成本函数不对称性，有效根据站点特定条件和未建模因素进行调整实现大幅节约。

Abstract: This work introduces a methodology to adjust forecasts based on node-specific cost function asymmetry. The proposed model generates savings by dynamically incorporating the cost asymmetry into the forecasting error probability distribution to favor the least expensive scenario. Savings are calculated and a self-regulation mechanism modulates the adjustments magnitude based on the observed savings, enabling the model to adapt to station-specific conditions and unmodeled factors such as calibration errors or shifting macroeconomic dynamics. Finally, empirical results demonstrate the model's ability to achieve \$5.1M annual savings.

</details>


### [61] [End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment](https://arxiv.org/abs/2512.19723)
*Firas Bayram,Bestoun S. Ahmed,Erik Hallin*

Main category: cs.LG

TL;DR: 本文提出新的端到端框架，结合数据质量评估与机器学习模型操作，在钢铁制造公司验证，提升模型性能并降低预测延迟。


<details>
  <summary>Details</summary>
Motivation: 现有方法将数据质量评估和ML系统视为孤立过程，需填补理论方法与实际应用的差距。

Method: 将动态漂移检测、自适应数据质量指标和MLOps结合成轻量级系统。

Result: 在钢铁制造公司的ESR真空泵过程验证，模型性能提升12%（R2 = 94%），预测延迟降低四倍。

Conclusion: 该框架是MLOps的重大进步，为动态工业环境中时间敏感、数据驱动的决策提供有力解决方案。

Abstract: This paper introduces a novel end-to-end framework that efficiently integrates data quality assessment with machine learning (ML) model operations in real-time production environments. While existing approaches treat data quality assessment and ML systems as isolated processes, our framework addresses the critical gap between theoretical methods and practical implementation by combining dynamic drift detection, adaptive data quality metrics, and MLOps into a cohesive, lightweight system. The key innovation lies in its operational efficiency, enabling real-time, quality-driven ML decision-making with minimal computational overhead. We validate the framework in a steel manufacturing company's Electroslag Remelting (ESR) vacuum pumping process, demonstrating a 12% improvement in model performance (R2 = 94%) and a fourfold reduction in prediction latency. By exploring the impact of data quality acceptability thresholds, we provide actionable insights into balancing data quality standards and predictive performance in industrial applications. This framework represents a significant advancement in MLOps, offering a robust solution for time-sensitive, data-driven decision-making in dynamic industrial environments.

</details>


### [62] [Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking](https://arxiv.org/abs/2512.19725)
*Srishti Gupta,Riccardo Balia,Daniele Angioni,Fabio Brau,Maura Pintor,Ambra Demontis,Alessandro Sebastian,Salvatore Mario Carta,Fabio Roli,Battista Biggio*

Main category: cs.LG

TL;DR: 机器学习模型发展迅速，但在现实场景中面临可靠性和适应性问题，需持续学习和分布外检测来应对。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在实际应用中因数据变化和分布不同，难以保持可靠性和适应性，传统训练方法成本高且不实用，所以需要新方法。

Method: 未提及具体方法。

Result: 未提及研究结果。

Conclusion: 联合解决持续学习和分布外检测问题对开发强大、高效和自适应的人工智能系统至关重要。

Abstract: Recent years have witnessed significant progress in the development of machine learning models across a wide range of fields, fueled by increased computational resources, large-scale datasets, and the rise of deep learning architectures. From malware detection to enabling autonomous navigation, modern machine learning systems have demonstrated remarkable capabilities. However, as these models are deployed in ever-changing real-world scenarios, their ability to remain reliable and adaptive over time becomes increasingly important. For example, in the real world, new malware families are continuously developed, whereas autonomous driving cars are employed in many different cities and weather conditions. Models trained in fixed settings can not respond effectively to novel conditions encountered post-deployment. In fact, most machine learning models are still developed under the assumption that training and test data are independent and identically distributed (i.i.d.), i.e., sampled from the same underlying (unknown) distribution. While this assumption simplifies model development and evaluation, it does not hold in many real-world applications, where data changes over time and unexpected inputs frequently occur. Retraining models from scratch whenever new data appears is computationally expensive, time-consuming, and impractical in resource-constrained environments. These limitations underscore the need for Continual Learning (CL), which enables models to incrementally learn from evolving data streams without forgetting past knowledge, and Out-of-Distribution (OOD) detection, which allows systems to identify and respond to novel or anomalous inputs. Jointly addressing both challenges is critical to developing robust, efficient, and adaptive AI systems.

</details>


### [63] [Tiny, On-Device Decision Makers with the MiniConv Library](https://arxiv.org/abs/2512.19726)
*Carlos Purves*

Main category: cs.LG

TL;DR: 提出分割策略架构解决强化学习视觉策略在边缘设备部署难题，在降低数据传输、决策延迟等方面有效果。


<details>
  <summary>Details</summary>
Motivation: 强化学习视觉策略在资源受限边缘设备部署因计算成本和通信延迟具有挑战性，现有卸载策略存在问题。

Method: 引入分割策略架构，用OpenGL片段着色器实现设备端编码器将观测转换为紧凑特征张量传输给远程策略头。

Result: 减少传输数据、降低带宽受限场景下决策延迟和服务器端计算量，单次运行基准测试中最终回报学习性能相当。在多种设备评估，公开代码。

Conclusion: 所提出的分割策略架构能有效解决边缘设备强化学习视觉策略部署问题，有一定应用潜力。

Abstract: Reinforcement learning (RL) has achieved strong results, but deploying visual policies on resource-constrained edge devices remains challenging due to computational cost and communication latency. Many deployments therefore offload policy inference to a remote server, incurring network round trips and requiring transmission of high-dimensional observations. We introduce a split-policy architecture in which a small on-device encoder, implemented as OpenGL fragment-shader passes for broad embedded GPU support, transforms each observation into a compact feature tensor that is transmitted to a remote policy head. In RL, this communication overhead manifests as closed-loop decision latency rather than only per-request inference latency. The proposed approach reduces transmitted data, lowers decision latency in bandwidth-limited settings, and reduces server-side compute per request, whilst achieving broadly comparable learning performance by final return (mean over the final 100 episodes) in single-run benchmarks, with modest trade-offs in mean return. We evaluate across an NVIDIA Jetson Nano, a Raspberry Pi 4B, and a Raspberry Pi Zero 2 W, reporting learning results, on-device execution behaviour under sustained load, and end-to-end decision latency and scalability measurements under bandwidth shaping. Code for training, deployment, and measurement is released as open source.

</details>


### [64] [Trend Extrapolation for Technology Forecasting: Leveraging LSTM Neural Networks for Trend Analysis of Space Exploration Vessels](https://arxiv.org/abs/2512.19727)
*Peng-Hung Tsai,Daniel Berleant*

Main category: cs.LG

TL;DR: 文章通过更新的系统文献综述评估技术预测方法，开发结合LSTM神经网络与摩尔定律拓展的模型预测航天器寿命，改进STETI方法处理数据偏差，结果对航天任务规划和政策制定有指导意义。


<details>
  <summary>Details</summary>
Motivation: 复杂领域技术预测存在挑战，需评估现有技术预测方法，以更好预测太空探索技术进步。

Method: 进行更新的系统文献综述，开发结合LSTM神经网络与摩尔定律拓展的模型，改进STETI方法处理右删失问题。

Result: 研究获得了可以为太空任务规划和政策决策提供参考的相关结果。

Conclusion: 研究方法有助于解决航天器寿命预测问题，对航天领域规划和决策有重要价值。

Abstract: Forecasting technological advancement in complex domains such as space exploration presents significant challenges due to the intricate interaction of technical, economic, and policy-related factors. The field of technology forecasting has long relied on quantitative trend extrapolation techniques, such as growth curves (e.g., Moore's law) and time series models, to project technological progress. To assess the current state of these methods, we conducted an updated systematic literature review (SLR) that incorporates recent advances. This review highlights a growing trend toward machine learning-based hybrid models.
  Motivated by this review, we developed a forecasting model that combines long short-term memory (LSTM) neural networks with an augmentation of Moore's law to predict spacecraft lifetimes. Operational lifetime is an important engineering characteristic of spacecraft and a potential proxy for technological progress in space exploration. Lifetimes were modeled as depending on launch date and additional predictors.
  Our modeling analysis introduces a novel advance in the recently introduced Start Time End Time Integration (STETI) approach. STETI addresses a critical right censoring problem known to bias lifetime analyses: the more recent the launch dates, the shorter the lifetimes of the spacecraft that have failed and can thus contribute lifetime data. Longer-lived spacecraft are still operating and therefore do not contribute data. This systematically distorts putative lifetime versus launch date curves by biasing lifetime estimates for recent launch dates downward. STETI mitigates this distortion by interconverting between expressing lifetimes as functions of launch time and modeling them as functions of failure time. The results provide insights relevant to space mission planning and policy decision-making.

</details>


### [65] [Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning](https://arxiv.org/abs/2512.20363)
*Daniel M. Jimenez-Gutierrez,Mehrdad Hassanzadeh,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.LG

TL;DR: 提出Clust - PSI - PFL框架，用PSI量化非IID数据，在多数据集和设置下表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据非独立同分布会导致更新偏差和性能下降，需解决此问题。

Method: 提出Clust - PSI - PFL框架，计算加权PSI指标$WPSI^L$，用K - means++对客户端分组，通过轮廓系数选择最优簇数。

Result: 在六个数据集、两种分区协议和多种客户端规模下，比最先进基线全球准确率最高提升18%，客户端公平性相对提升37%。

Conclusion: PSI引导聚类是标签偏斜下实现稳健个性化联邦学习的有效轻量级机制。

Abstract: Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $α$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.

</details>


### [66] [Hard Negative Sample-Augmented DPO Post-Training for Small Language Models](https://arxiv.org/abs/2512.19728)
*Haocheng Lu,Minjun Zhu,Henry Yu*

Main category: cs.LG

TL;DR: 提出轻量级后训练管道，针对大语言模型数学推理的结构化错误，在Qwen2.5模型实验中效果好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型数学推理存在问题，现有后训练管道视角有限，基于人类反馈的强化学习变体有成本高、难扩展等问题。

Method: 从MetaMathQA式思维链数据监督微调开始，引入MathVerifier分解候选解决方案，利用验证器信号挖掘硬负样本和定义样本重要性权重，集成到离线直接偏好优化目标中。

Result: 在1.5B参数的Qwen2.5模型上实验表明，验证器引导的加权DPO比普通SFT和未加权DPO有更有针对性的改进。

Conclusion: 所提轻量级后训练管道能有效解决大语言模型数学推理的结构化错误，且避免训练大奖励模型或依赖外部评判的开销。

Abstract: Large language models (LLMs) continue to struggle with mathematical reasoning, and common post-training pipelines often reduce each generated solution to a binary outcome: correct or incorrect. This perspective is limiting in practice, as failures in chain-of-thought (CoT) reasoning are frequently structured; solutions may appear convincing while containing subtle logical, algebraic, or numerical flaws. Meanwhile, reinforcement learning from human feedback (RLHF) variants that rely on large reward models or LLM-as-a-judge signals are often expensive, difficult to scale, and unstable to iterate. We propose a lightweight and pragmatic post-training pipeline that targets such structured errors under realistic compute budgets. Starting from supervised fine-tuning (SFT) on MetaMathQA-style CoT data, we introduce a compact MathVerifier that decomposes a candidate solution into a six-dimensional error profile and aggregates it into interpretable wrongness and absurdity scores. These verifier signals serve two roles: (i) mining hard negatives that are near-correct yet structurally flawed, and (ii) defining per-sample importance weights that emphasize the most informative preference pairs. We integrate both into an offline Direct Preference Optimization (DPO) objective via a verifier-guided weighted formulation. Experiments on a 1.5B-parameter Qwen2.5 model show that verifier-guided, weighted DPO yields more targeted improvements than vanilla SFT and unweighted DPO, particularly on problems where solutions are numerically close to correct but logically inconsistent, while avoiding the overhead of training large reward models or relying on external judges.

</details>


### [67] [Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs](https://arxiv.org/abs/2512.20573)
*Rui Pan,Zhuofu Chen,Ravi Netravali*

Main category: cs.LG

TL;DR: 本文提出基于dLLM的推测解码框架FailFast，可无损加速自回归大语言模型，在多种模型和工作负载中实现显著速度提升并开源。


<details>
  <summary>Details</summary>
Motivation: Diffusion Large Language Models (dLLMs) 单独使用时存在效率 - 质量权衡问题，探索其在推测解码中的应用以发挥优势。

Method: 提出FailFast框架，通过动态调整推测长度，在难推测区域减少计算量以缩短推测延迟，在易推测区域增加草稿长度以减少验证延迟。

Result: FailFast 实现了对自回归大语言模型的无损加速，在多种模型和工作负载中实现了高达 4.9 倍于普通解码、1.7 倍于最佳的简单 dLLM 起草器、1.4 倍于 EAGLE - 3 的速度提升。

Conclusion: 表明合理利用 dLLM 的属性可在推测解码中发挥优势，FailFast 是一种有效的提升自回归大语言模型解码速度的方法。

Abstract: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It "fails fast" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and "wins big" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.4$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.

</details>


### [68] [High-Performance Self-Supervised Learning by Joint Training of Flow Matching](https://arxiv.org/abs/2512.19729)
*Kosuke Ukita,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 提出FlowFM解决扩散模型在自监督学习中的问题，在可穿戴传感器数据实验中表现良好，代码开源。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在自监督学习中面临生成质量和判别性能权衡问题，且迭代采样成本高，阻碍工业和边缘AI应用。

Method: 提出基于流匹配的基础模型FlowFM，联合训练表示编码器和条件流匹配生成器。

Result: 在可穿戴传感器数据上，FlowFM相比基于扩散的方法减少50.4%训练时间，在下游任务中超越SSL - Wearables，推理速度提升达51.0x并保持高生成质量。

Conclusion: FlowFM通过解耦设计和流匹配方法，有效解决了扩散模型的问题，提高了表示学习效率。

Abstract: Diffusion models can learn rich representations during data generation, showing potential for Self-Supervised Learning (SSL), but they face a trade-off between generative quality and discriminative performance. Their iterative sampling also incurs substantial computational and energy costs, hindering industrial and edge AI applications. To address these issues, we propose the Flow Matching-based Foundation Model (FlowFM), which jointly trains a representation encoder and a conditional flow matching generator. This decoupled design achieves both high-fidelity generation and effective recognition. By using flow matching to learn a simpler velocity field, FlowFM accelerates and stabilizes training, improving its efficiency for representation learning. Experiments on wearable sensor data show FlowFM reduces training time by 50.4\% compared to a diffusion-based approach. On downstream tasks, FlowFM surpassed the state-of-the-art SSL method (SSL-Wearables) on all five datasets while achieving up to a 51.0x inference speedup and maintaining high generative quality. The implementation code is available at https://github.com/Okita-Laboratory/jointOptimizationFlowMatching.

</details>


### [69] [Learning to Reason in LLMs by Expectation Maximization](https://arxiv.org/abs/2512.20169)
*Junghyun Lee,Branislav Kveton,Sunav Choudhary,Subhojyoti Mukherjee,Anup Rao,Ryan A. Rossi,Alexa Siu*

Main category: cs.LG

TL;DR: 本文将推理形式化为潜在变量模型，推导学习推理的目标，比较多种采样方案，发现PPS表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型推理问题，探索学习推理的有效方法。

Method: 将推理形式化为潜在变量模型，推导期望最大化（EM）目标；实例化并比较拒绝采样、STaR和PPS三种采样方案。

Result: 在ARC、MMLU和OpenBookQA数据集上，采样方案显著影响推理模型准确率，PPS表现优于其他采样方案。

Conclusion: PPS虽简单但在大语言模型学习推理中性能更好。

Abstract: Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.

</details>


### [70] [ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures](https://arxiv.org/abs/2512.19730)
*Zhonghao Yang,Cheng Luo,Daojing He,Yiming Li,Yu Li*

Main category: cs.LG

TL;DR: 现有基于学习的神经后门检测方法泛化性差，本文提出ArcGen方法，引入对齐层和对齐损失，在未见模型架构上检测性能提升达42.5%。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的神经后门检测方法在学习阶段未见的新架构上泛化性差，需解决此问题。

Method: 提出ArcGen方法，在特征提取函数中引入额外对齐层处理特征，设计两个对齐损失训练特征提取函数。

Result: 在涉及16896个模型的大规模评估中，在未见模型架构上检测性能（如AUC）提升达42.5%。

Conclusion: ArcGen方法能有效获取架构无关的模型特征，实现更有效的后门检测。

Abstract: Backdoor attacks pose a significant threat to the security and reliability of deep learning models. To mitigate such attacks, one promising approach is to learn to extract features from the target model and use these features for backdoor detection. However, we discover that existing learning-based neural backdoor detection methods do not generalize well to new architectures not seen during the learning phase. In this paper, we analyze the root cause of this issue and propose a novel black-box neural backdoor detection method called ArcGen. Our method aims to obtain architecture-invariant model features, i.e., aligned features, for effective backdoor detection. Specifically, in contrast to existing methods directly using model outputs as model features, we introduce an additional alignment layer in the feature extraction function to further process these features. This reduces the direct influence of architecture information on the features. Then, we design two alignment losses to train the feature extraction function. These losses explicitly require that features from models with similar backdoor behaviors but different architectures are aligned at both the distribution and sample levels. With these techniques, our method demonstrates up to 42.5% improvements in detection performance (e.g., AUC) on unseen model architectures. This is based on a large-scale evaluation involving 16,896 models trained on diverse datasets, subjected to various backdoor attacks, and utilizing different model architectures. Our code is available at https://github.com/SeRAlab/ArcGen.

</details>


### [71] [Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems](https://arxiv.org/abs/2512.19731)
*Xiangzhong Luo,Weichen Liu*

Main category: cs.LG

TL;DR: 本文提出Double - Win NAS用于资源受限嵌入式系统，结合深度和浅层网络优势，还有增强训练技术，实验证明其优于现有NAS方法。


<details>
  <summary>Details</summary>
Motivation: 深度卷积神经网络硬件效率低，浅层网络精度差，需解决此困境。

Method: 提出Double - Win NAS范式自动探索深度网络获高精度，再转化为浅层网络获高硬件效率，还提出混合可转换训练和任意分辨率弹性训练两种增强训练技术。

Result: 在NVIDIA Jetson AGX Xavier和NVIDIA Jetson Nano两种嵌入式系统及ImageNet和ImageNet - 100两个数据集上实验，Double - Win NAS优于先前的NAS方法。

Conclusion: Double - Win NAS能有效解决深度和浅层网络的问题，在资源受限嵌入式系统中有优势。

Abstract: Thanks to the evolving network depth, convolutional neural networks (CNNs) have achieved remarkable success across various embedded scenarios, paving the way for ubiquitous embedded intelligence. Despite its promise, the evolving network depth comes at the cost of degraded hardware efficiency. In contrast to deep networks, shallow networks can deliver superior hardware efficiency but often suffer from inferior accuracy. To address this dilemma, we propose Double-Win NAS, a novel deep-to-shallow transformable neural architecture search (NAS) paradigm tailored for resource-constrained intelligent embedded systems. Specifically, Double-Win NAS strives to automatically explore deep networks to first win strong accuracy, which are then equivalently transformed into their shallow counterparts to further win strong hardware efficiency. In addition to search, we also propose two enhanced training techniques, including hybrid transformable training towards better training accuracy and arbitrary-resolution elastic training towards enabling natural network elasticity across arbitrary input resolutions. Extensive experimental results on two popular intelligent embedded systems (i.e., NVIDIA Jetson AGX Xavier and NVIDIA Jetson Nano) and two representative large-scale datasets (i.e., ImageNet and ImageNet-100) clearly demonstrate the superiority of Double-Win NAS over previous state-of-the-art NAS approaches.

</details>


### [72] [Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis](https://arxiv.org/abs/2512.19732)
*Gaurav Kumar Sharma*

Main category: cs.LG

TL;DR: 对JARVIS - DFT带隙数据集系统分析，去除含能带结构信息描述符得2280种材料子集，用三相建模框架建模，树模型R2约0.88 - 0.90，SHAP分析指出介电张量分量为主要贡献者，为后续带隙预测研究提供数据和指标。


<details>
  <summary>Details</summary>
Motivation: 为未来带隙预测研究提供受泄漏控制的数据集与基线性能指标。

Method: 对数据集系统分析去除特定描述符，得到受泄漏控制的数据集，采用三相建模框架逐步纳入基本物理描述符、工程特征和成分属性。

Result: 树模型在各阶段R2约为0.88 - 0.90，SHAP分析表明介电张量分量是主要贡献者。

Conclusion: 在控制泄漏时，扩大描述符空间对预测准确性提升不显著，本研究提供了有用的数据集和指标。

Abstract: In this study, we perform a systematic analysis of the JARVIS-DFT bandgap dataset and identify and remove descriptors that may inadvertently encode band-structure information, such as effective masses. This process yields a curated, leakage-controlled subset of 2280 materials. Using this dataset, a three-phase modeling framework is implemented that incrementally incorporates basic physical descriptors, engineered features, and compositional attributes. The results show that tree-based models achieve R2 values of approximately 0.88 to 0.90 across all phases, indicating that expanding the descriptor space does not substantially improve predictive accuracy when leakage is controlled. SHAP analysis consistently identifies the dielectric tensor components as the dominant contributors. This work provides a curated dataset and baseline performance metrics for future leakage-aware bandgap prediction studies.

</details>


### [73] [The Deleuzian Representation Hypothesis](https://arxiv.org/abs/2512.19734)
*Clément Cornet,Romaric Besançon,Hervé Le Borgne*

Main category: cs.LG

TL;DR: 提出替代稀疏自编码器的无监督方法提取神经网络中可解释概念，经多模型多模态评估效果好。


<details>
  <summary>Details</summary>
Motivation: 寻找简单有效的无监督方法从神经网络中提取可解释概念。

Method: 在判别分析框架下对激活差异进行聚类，用激活偏度加权聚类以增强概念多样性。

Result: 该方法概念质量超越先前无监督稀疏自编码器变体，接近监督基线，提取概念能控制模型内部表示。

Conclusion: 所提出方法是有效提取可解释概念的无监督方法。

Abstract: We propose an alternative to sparse autoencoders (SAEs) as a simple and effective unsupervised method for extracting interpretable concepts from neural networks. The core idea is to cluster differences in activations, which we formally justify within a discriminant analysis framework. To enhance the diversity of extracted concepts, we refine the approach by weighting the clustering using the skewness of activations. The method aligns with Deleuze's modern view of concepts as differences. We evaluate the approach across five models and three modalities (vision, language, and audio), measuring concept quality, diversity, and consistency. Our results show that the proposed method achieves concept quality surpassing prior unsupervised SAE variants while approaching supervised baselines, and that the extracted concepts enable steering of a model's inner representations, demonstrating their causal influence on downstream behavior.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [74] [Evolutionary Neural Architecture Search with Dual Contrastive Learning](https://arxiv.org/abs/2512.20112)
*Xian-Rong Zhang,Yue-Jiao Gong,Wei-Neng Chen,Jun Zhang*

Main category: cs.NE

TL;DR: 本文提出DCL - ENAS方法，利用两阶段对比学习训练神经预测器用于ENAS，在多个数据集和实际任务中取得较好效果。


<details>
  <summary>Details</summary>
Motivation: 现有ENAS使用神经预测器时收集训练数据计算成本高，在有限计算预算下获得高精度预测器对ENAS成功很关键。

Method: 采用两阶段对比学习训练神经预测器，第一阶段用自监督对比学习从无标签的神经架构中学习表示，第二阶段用对比学习微调以预测相对性能来指导进化搜索。

Result: 在NASBench - 101和NASBench - 201上达到最高验证准确率，在真实ECG心律失常分类任务上比手动设计模型性能提高约2.5个百分点，仅需7.7 GPU - 天。

Conclusion: DCL - ENAS方法有效，能在有限计算预算下提升ENAS性能。

Abstract: Evolutionary Neural Architecture Search (ENAS) has gained attention for automatically designing neural network architectures. Recent studies use a neural predictor to guide the process, but the high computational costs of gathering training data -- since each label requires fully training an architecture -- make achieving a high-precision predictor with { limited compute budget (i.e., a capped number of fully trained architecture-label pairs)} crucial for ENAS success. This paper introduces ENAS with Dual Contrastive Learning (DCL-ENAS), a novel method that employs two stages of contrastive learning to train the neural predictor. In the first stage, contrastive self-supervised learning is used to learn meaningful representations from neural architectures without requiring labels. In the second stage, fine-tuning with contrastive learning is performed to accurately predict the relative performance of different architectures rather than their absolute performance, which is sufficient to guide the evolutionary search. Across NASBench-101 and NASBench-201, DCL-ENAS achieves the highest validation accuracy, surpassing the strongest published baselines by 0.05\% (ImageNet16-120) to 0.39\% (NASBench-101). On a real-world ECG arrhythmia classification task, DCL-ENAS improves performance by approximately 2.5 percentage points over a manually designed, non-NAS model obtained via random search, while requiring only 7.7 GPU-days.

</details>


### [75] [Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds](https://arxiv.org/abs/2512.20245)
*Tarik Houichime,Abdelghani Souhar,Younes El Amrani*

Main category: cs.NE

TL;DR: 文章指出当代大语言模型内存存在物理悖论，提出语音轨迹记忆（PTM）架构，实现超3000倍压缩，检索准确率达约92%，存取延迟约34ms，表明无限上下文不依赖无限存储。


<details>
  <summary>Details</summary>
Motivation: 解决当代大语言模型内存的物理悖论，即学习时填满内存，线性积累键值状态带来失忆或延迟的两难选择。

Method: 引入语音轨迹记忆（PTM）神经符号架构，将语言编码为遍历流形上由无理旋转矩阵控制的连续路径，解耦导航和重建。

Result: PTM实现超3000倍压缩，检索准确率达约92%，存取延迟约34ms。

Conclusion: 无限上下文不需要无限存储，应将记忆视为对守恒物理信号的重建过程。

Abstract: The memory of contemporary Large Language Models is bound by a physical paradox: as they learn, they fill up. The linear accumulation (O(N)) of Key-Value states treats context as a warehouse of static artifacts, eventually forcing a destructive choice between amnesia and latency. We challenge this discrete orthodoxy, proposing that long-term memory is not the storage of items, but the persistence of a trajectory. We introduce Phonetic Trajectory Memory (PTM), a neuro-symbolic architecture that encodes language not as a sequence of tensors, but as a continuous path on an ergodic manifold governed by irrational rotation matrices. By decoupling the navigation (an invariant O(1) geometric signal) from the reconstruction (a probabilistic generative act), PTM achieves a compression magnitude of greater than 3,000x relative to dense caches. We demonstrate that retrieval becomes a process of resonance: the phonetic trace stabilizes the model against hallucination via "Signal Consensus" mechanism, securing up to approximately 92% factual accuracy. While this aggressive abstraction alters generative texture, it unlocks immediate access latency (approximately 34ms) independent of depth. Our results suggest that infinite context does not require infinite silicon; it requires treating memory not as data to be stored, but as a reconstructive process acting on a conserved, undying physical signal.

</details>


### [76] [Cross-Population White Matter Atlas Creation for Concurrent Mapping of Brain Connections in Neonates and Adults with Diffusion MRI Tractography](https://arxiv.org/abs/2512.20370)
*Wei Zhang,Yijie Li,Ruixi Zheng,Nir A. Sochen,Yuqian Chen,Leo R. Zekelman,Ofer Pasternak,Jarrett Rushmore,Yogesh Rathi,Nikos Makris,Lauren J. O'Donnell,Fan Zhang*

Main category: cs.NE

TL;DR: 本文提出新生儿/成人脑图谱NABA，用于比较两者白质连接，通过相关分析证明其是研究白质发育的有用工具。


<details>
  <summary>Details</summary>
Motivation: 现有白质图谱是特定人群的，缺乏统一图谱，无法直接跨人群比较，需构建统一图谱以推进脑发育理解和疾病生物标志物研究。

Method: 构建新生儿/成人脑图谱NABA，用数据驱动的纤维聚类流程，基于扩散磁共振成像数据；利用NABA进行四项分析。

Result: NABA能在两人群中识别白质束；长程联合束FA发展快，小脑内束慢；新生儿女性FA发展快于男性；早产新生儿整体FA发展慢，但特定束FA增长高。

Conclusion: NABA是研究新生儿和成人白质发育的有用工具。

Abstract: Comparing white matter (WM) connections between adults and neonates using diffusion MRI (dMRI) can advance our understanding of typical brain development and potential biomarkers for neurological disorders. However, existing WM atlases are population-specific (adult or neonatal) and reside in separate spaces, preventing direct cross-population comparisons. A unified WM atlas spanning both neonates and adults is still lacking. In this study, we propose a neonatal/adult brain atlas (NABA), a WM tractography atlas built from dMRI data of both neonates and adults. NABA is constructed using a robust, data-driven fiber clustering pipeline, enabling group-wise WM atlasing across populations despite substantial anatomical variability. The atlas provides a standardized template for WM parcellation, allowing direct comparison of WM tracts between neonates and adults. Using NABA, we conduct four analyses: (1) evaluating the feasibility of joint WM mapping across populations, (2) characterizing WM development across neonatal ages relative to adults, (3) assessing sex-related differences in neonatal WM development, and (4) examining the effects of preterm birth. Our results show that NABA robustly identifies WM tracts in both populations. We observe rapid fractional anisotropy (FA) development in long-range association tracts, including the arcuate fasciculus and superior longitudinal fasciculus II, whereas intra-cerebellar tracts develop more slowly. Neonatal females exhibit faster overall FA development than males. Although preterm neonates show lower overall FA development rates, they demonstrate relatively higher FA growth in specific tracts, including the corticospinal tract, corona radiata-pontine pathway, and intracerebellar tracts. These findings demonstrate that NABA is a useful tool for investigating WM development across neonates and adults.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [77] [Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models](https://arxiv.org/abs/2512.19758)
*Wang Bin,Ao Yang,Kedan Li,Aofan Liu,Hui Li,Guibo Luo,Weixiang Huang,Yan Zhuang*

Main category: cs.SE

TL;DR: 现有DGF方法忽略代码段逻辑关系，本文引入注意力距离指标，实验显示能显著提升测试效率。


<details>
  <summary>Details</summary>
Motivation: 现有DGF方法仅考虑种子执行路径与目标位置物理距离，忽略代码段逻辑关系，削弱了DGF在现实中的有效性。

Method: 引入注意力距离这一新颖指标，利用大语言模型的上下文分析计算代码元素间注意力得分，揭示其内在联系，并在AFLGo、DAFL和WindRanger中应用。

Result: 在38个真实漏洞复现实验中，用注意力距离替代物理距离使测试效率平均提高3.43倍；与DAFL和WindRanger相比，分别实现2.89倍和7.13倍的改进；将其集成到DAFL和WindRanger中也能提升原性能。

Conclusion: 注意力距离指标能有效提升软件安全测试效率，且具有良好的通用性。

Abstract: In the domain of software security testing, Directed Grey-Box Fuzzing (DGF) has garnered widespread attention for its efficient target localization and excellent detection performance. However, existing approaches measure only the physical distance between seed execution paths and target locations, overlooking logical relationships among code segments. This omission can yield redundant or misleading guidance in complex binaries, weakening DGF's real-world effectiveness. To address this, we introduce \textbf{attention distance}, a novel metric that leverages a large language model's contextual analysis to compute attention scores between code elements and reveal their intrinsic connections. Under the same AFLGo configuration -- without altering any fuzzing components other than the distance metric -- replacing physical distances with attention distances across 38 real vulnerability reproduction experiments delivers a \textbf{3.43$\times$} average increase in testing efficiency over the traditional method. Compared to state-of-the-art directed fuzzers DAFL and WindRanger, our approach achieves \textbf{2.89$\times$} and \textbf{7.13$\times$} improvements, respectively. To further validate the generalizability of attention distance, we integrate it into DAFL and WindRanger, where it also consistently enhances their original performance. All related code and datasets are publicly available at https://github.com/TheBinKing/Attention\_Distance.git.

</details>


### [78] [A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows](https://arxiv.org/abs/2512.19769)
*Ivan Daunis*

Main category: cs.SE

TL;DR: 提出声明式系统分离代理工作流规范与实现，支持多后端语言和部署环境，在PayPal电商工作流评估中，开发时间减少60%，部署速度提升3倍。


<details>
  <summary>Details</summary>
Motivation: 现有系统将代理逻辑与特定编程语言和部署模型紧密耦合，构建部署就绪的大语言模型代理需要复杂编排。

Method: 利用统一DSL表达代理工作流的常见模式，将代理开发从应用编程转变为配置，支持A/B测试。

Result: 在PayPal电商工作流中，开发时间减少60%，部署速度提升3倍，DSL表达复杂工作流代码量远少于命令式代码，非工程师可安全修改代理行为，编排开销低于100ms。

Conclusion: 声明式系统有效解决现有问题，提升开发和部署效率，降低代码量，让非工程师也能参与代理行为修改。

Abstract: Building deployment-ready LLM agents requires complex orchestration of tools, data sources, and control flow logic, yet existing systems tightly couple agent logic to specific programming languages and deployment models. We present a declarative system that separates agent workflow specification from implementation, enabling the same pipeline definition to execute across multiple backend languages (Java, Python, Go) and deployment environments (cloud-native, on-premises).
  Our key insight is that most agent workflows consist of common patterns -- data serialization, filtering, RAG retrieval, API orchestration -- that can be expressed through a unified DSL rather than imperative code. This approach transforms agent development from application programming to configuration, where adding new tools or fine-tuning agent behaviors requires only pipeline specification changes, not code deployment. Our system natively supports A/B testing of agent strategies, allowing multiple pipeline variants to run on the same backend infrastructure with automatic metric collection and comparison.
  We evaluate our approach on real-world e-commerce workflows at PayPal, processing millions of daily interactions. Our results demonstrate 60% reduction in development time, and 3x improvement in deployment velocity compared to imperative implementations. The language's declarative approach enables non-engineers to modify agent behaviors safely, while maintaining sub-100ms orchestration overhead. We show that complex workflows involving product search, personalization, and cart management can be expressed in under 50 lines of DSL compared to 500+ lines of imperative code.

</details>


### [79] [Larger Is Not Always Better: Leveraging Structured Code Diffs for Comment Inconsistency Detection](https://arxiv.org/abs/2512.19883)
*Phong Nguyen,Anh M. T. Bui,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 本文提出基于CodeT5+骨干的及时代码-注释不一致（CCI）检测方法，在公开基准数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有CCI检测方法忽略代码演化结构复杂性，且存在隐私和资源挑战，保障代码和注释语义一致性很重要。

Method: 将代码变更分解为替换、删除和添加等修改活动的有序序列，构建基于CodeT5+骨干的及时CCI检测方法。

Result: 在JITDATA和CCIBENCH数据集上，该方法F1得分比现有模型最高高出13.54%，比微调的大语言模型提高4.18% - 10.94%。

Conclusion: 提出的及时CCI检测方法有效，能更好地捕捉代码变更与过时注释的关联，性能优于近期模型。

Abstract: Ensuring semantic consistency between source code and its accompanying comments is crucial for program comprehension, effective debugging, and long-term maintainability. Comment inconsistency arises when developers modify code but neglect to update the corresponding comments, potentially misleading future maintainers and introducing errors. Recent approaches to code-comment inconsistency (CCI) detection leverage Large Language Models (LLMs) and rely on capturing the semantic relationship between code changes and outdated comments. However, they often ignore the structural complexity of code evolution, including historical change activities, and introduce privacy and resource challenges. In this paper, we propose a Just-In-Time CCI detection approach built upon the CodeT5+ backbone. Our method decomposes code changes into ordered sequences of modification activities such as replacing, deleting, and adding to more effectively capture the correlation between these changes and the corresponding outdated comments. Extensive experiments conducted on publicly available benchmark datasets-JITDATA and CCIBENCH--demonstrate that our proposed approach outperforms recent state-of-the-art models by up to 13.54% in F1-Score and achieves an improvement ranging from 4.18% to 10.94% over fine-tuned LLMs including DeepSeek-Coder, CodeLlama and Qwen2.5-Coder.

</details>


### [80] [Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?](https://arxiv.org/abs/2512.19980)
*Zhe Yin,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 本文对代码大语言模型进行神经元级别的实证研究，发现语言特定神经元和概念层，且在三个任务中展示其效用。


<details>
  <summary>Details</summary>
Motivation: 代码语言模型内部可解释性研究不足，现有NLP神经元可解释性技术不适用于源代码。

Method: 对Llama - 3.1 - 8B和Qwen2.5 - Coder - 32B在多语言输入下进行实证研究，测量神经元选择性和层贡献。

Result: 发现有特定语言的神经元和支持通用生成的通用子集；较低层编码特定语言语法，中间层捕获跨语言语义抽象形成概念层。

Conclusion: 在代码生成、克隆检测和代码摘要三个多语言任务中，相关方法能带来持续增益。

Abstract: Code language models excel on code intelligence tasks, yet their internal interpretability is underexplored. Existing neuron interpretability techniques from NLP are suboptimal for source code due to programming languages formal, hierarchical, and executable nature. We empirically investigate code LLMs at the neuron level, localizing language-specific neurons (selectively responsive to one language) and concept layers (feed-forward layers encoding language-agnostic code representations). We analyze Llama-3.1-8B and Qwen2.5-Coder-32B on multilingual inputs in C++, Java, Python, Go, and JavaScript, measuring neuron selectivity and layerwise contributions during generation. We find (1) neurons specialized for individual languages alongside a universal subset supporting general-purpose generation; and (2) lower layers mainly encode language-specific syntax, while middle layers capture semantic abstractions shared across languages, emerging as concept layers. We demonstrate utility on three tasks: neuron-guided fine-tuning for code generation, clone detection via concept-layer embeddings, and concept-layer-guided transfer for code summarization, each yielding consistent gains in multilingual settings.

</details>


### [81] [Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing](https://arxiv.org/abs/2512.20083)
*Wenzhao Wu,Yahui Tang,Mingfei Cheng,Wenbing Tang,Yuan Zhou,Yang Liu*

Main category: cs.SE

TL;DR: 本文聚焦具身智能体决策非最优问题，提出NoD - DGMT框架检测非最优决策，实验表明该框架检测效果好，优于多个基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法忽视生成计划的非功能最优性，导致性能下降和资源浪费，需解决非最优决策问题。

Method: 提出NoD - DGMT框架，通过多样性引导的变质测试检测非最优决策，设计四个新的变质关系，引入多样性引导选择策略。

Result: 在AI2 - THOR模拟器上实验，NoD - DGMT平均违规检测率31.9%，多样性引导过滤器使检测率提高4.3%、多样性得分提高3.3，显著优于六个基线方法。

Conclusion: NoD - DGMT能有效检测具身智能体任务规划中的非最优决策，在不同模型架构和任务复杂度下表现一致优越。

Abstract: As embodied agents advance toward real-world deployment, ensuring optimal decisions becomes critical for resource-constrained applications. Current evaluation methods focus primarily on functional correctness, overlooking the non-functional optimality of generated plans. This gap can lead to significant performance degradation and resource waste. We identify and formalize the problem of Non-optimal Decisions (NoDs), where agents complete tasks successfully but inefficiently. We present NoD-DGMT, a systematic framework for detecting NoDs in embodied agent task planning via diversity-guided metamorphic testing. Our key insight is that optimal planners should exhibit invariant behavioral properties under specific transformations. We design four novel metamorphic relations capturing fundamental optimality properties: position detour suboptimality, action optimality completeness, condition refinement monotonicity, and scene perturbation invariance. To maximize detection efficiency, we introduce a diversity-guided selection strategy that actively selects test cases exploring different violation categories, avoiding redundant evaluations while ensuring comprehensive diversity coverage. Extensive experiments on the AI2-THOR simulator with four state-of-the-art planning models demonstrate that NoD-DGMT achieves violation detection rates of 31.9% on average, with our diversity-guided filter improving rates by 4.3% and diversity scores by 3.3 on average. NoD-DGMT significantly outperforms six baseline methods, with 16.8% relative improvement over the best baseline, and demonstrates consistent superiority across different model architectures and task complexities.

</details>


### [82] [AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration](https://arxiv.org/abs/2512.20159)
*Ruiqi Wang,Xinchen Wang,Cuiyun Gao,Chun Yong Chong,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: 传统代码评估指标有局限，现有基准存在问题，提出AXIOM框架合成代码评估基准。


<details>
  <summary>Details</summary>
Motivation: 传统规则指标不能深度分析代码，现有代码评估基准有缺陷，需新基准。

Method: 提出AXIOM框架，含规则引导扰动和多源质量校准两阶段。

Result: 未提及明确结果。

Conclusion: 未提及明确结论。

Abstract: Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.
  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...

</details>


### [83] [Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair](https://arxiv.org/abs/2512.20203)
*Zhenlei Ye,Xiaobing Sun,Sicong Cao,Lili Bo,Bin Li*

Main category: cs.SE

TL;DR: 现有基于大语言模型的软件漏洞修复方法存在忽略补丁位置和缺乏候选补丁质量评估的问题，本文提出sysname方法，在真实数据集上实验表明其比现有方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的漏洞修复方法存在忽略补丁位置和缺乏迭代过程中候选补丁质量评估的局限，需要改进。

Method: 提出sysname方法，先提供补丁位置信息，通过评估测试失败补丁的质量改进迭代修复策略，引入是否引入新漏洞和污点语句覆盖两个维度评估补丁质量。

Result: 在VulnLoc+数据集上实验，sysname能生成27个合理补丁，比基线方法多8 - 22个；在正确补丁生成方面，比现有方法多修复8 - 13个漏洞。

Conclusion: sysname方法相比基于神经机器翻译、程序分析和大语言模型的现有漏洞修复方法有显著改进。

Abstract: The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.
  To tackle the two limitations, we propose \sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches.

</details>


### [84] [Toward Explaining Large Language Models in Software Engineering Tasks](https://arxiv.org/abs/2512.20328)
*Antonio Vitale,Khai-Nguyen Nguyen,Denys Poshyvanyk,Rocco Oliveto,Simone Scalabrino,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 提出针对软件工程任务的自动化、模型无关的可解释性框架FeatureSHAP，评估显示其优于基线方法，能助从业者决策。


<details>
  <summary>Details</summary>
Motivation: 大语言模型黑盒特性阻碍其在高风险和安全关键领域应用，现有方法缺乏特定领域解释。

Method: 基于Shapley值，通过系统输入扰动和特定任务相似性比较，将模型输出归因于高级输入特征，兼容开源和专有大语言模型。

Result: 在代码生成和代码摘要任务中，比基线方法更能降低无关特征重要性，解释保真度更高；从业者调查显示有助于解释模型输出和决策。

Conclusion: FeatureSHAP是迈向实用软件工程可解释AI的有意义一步。

Abstract: Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.

</details>


### [85] [Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation](https://arxiv.org/abs/2512.20334)
*Yuan Huang,Yukang Zhou,Xiangping Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: 研究评估了GitHub Copilot和Cursor受注释掉的有缺陷代码影响，发现会使生成代码缺陷增多，需提升AI编码助手鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型用于代码生成时存在潜在缺陷，以往研究多忽视注释掉的代码（CO代码）缺陷的影响，而AI编码助手对CO代码的解读会影响生成代码。

Method: 评估AI编码助手GitHub Copilot和Cursor受有缺陷的CO代码的影响。

Result: 上下文中有缺陷的CO代码使AI编码助手生成更多缺陷代码，最高达58.17%；工具并非简单复制缺陷代码，会主动推理完成不完整缺陷模式；即便明确要求忽略缺陷CO代码，缺陷减少不超21.84%。

Conclusion: 有必要改进AI编码助手的鲁棒性和安全措施。

Abstract: With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.
  This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.

</details>


### [86] [A Comprehensive Study of Bugs in Modern Distributed Deep Learning Systems](https://arxiv.org/abs/2512.20345)
*Xiaoxue Ma,Wanwei Zhan,Jiale Chen,Yishu Li,Jacky Keung,Federica Sarro*

Main category: cs.SE

TL;DR: 研究对专用分布式框架中从业者面临的挑战进行大规模实证分析，构建问题分类体系，发现问题特点并给出解决建议。


<details>
  <summary>Details</summary>
Motivation: 单设备训练有计算和内存限制，通用框架分布式功能需大量手动工作，需要专用框架，研究从业者在专用分布式框架中的挑战。

Method: 分析DeepSpeed、Megatron - LM和Colossal - AI的849个实际问题，构建症状、根源和修复模式分类体系，建立三者映射。

Result: 45.1%的错误症状是分布式框架特有的，通信设置阶段95%的问题仅在分布式环境出现，超60%的问题可通过版本和依赖管理等解决。

Conclusion: 基于结果给出了可操作的建议。

Abstract: In today's data-driven era, deep learning is vital for processing massive datasets, yet single-device training is constrained by computational and memory limits. Distributed deep learning overcomes these challenges by leveraging multiple GPUs or machines in parallel. While general-purpose frameworks (e.g., TensorFlow and PyTorch) provide distributed capabilities, these are often add-on features that demand significant manual effort for advanced parallelism, underscoring the need for specialized frameworks. This study conducts the first large-scale empirical analysis of practitioner challenges in dedicated distributed frameworks. We examine 849 real-world issues from DeepSpeed, Megatron-LM, and Colossal-AI and construct a taxonomy of 34 bug symptoms, 28 root causes, and 6 fix patterns. Crucially, we establish explicit mappings between symptoms, causes, and fixes across distributed training stages, enabling a systematic understanding of how issues emerge and are resolved. Our results show that 45.1\% of bug symptoms are unique to distributed frameworks, with setup failures, memory issues, and performance anomalies being the most prevalent. Moreover, 95\% of issues in the communication setup stage occur exclusively in distributed contexts. We also find over 60\% of cases can be resolved through version and dependency management, and distributed feature, API, and communication tuning. Based on these findings, we provide actionable implications.

</details>


### [87] [Identifying Appropriately-Sized Services with Deep Reinforcement Learning](https://arxiv.org/abs/2512.20381)
*Syeda Tasnim Fabiha,Saad Shafiq,Wesley Klewerton Guez Assunção,Nenad Medvidović*

Main category: cs.SE

TL;DR: 本文提出基于深度强化学习的Rake技术用于从实现工件中识别合适大小的服务，应用于四个开源项目，效果优于现有技术，强调平衡目标的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有定义合适大小服务的方法在很多现实场景存在局限性，如依赖文档、人员或先验知识。

Method: 提出基于强化学习的Rake技术，利用系统文档和源代码在实现方法层面指导服务分解，支持自定义目标函数。

Result: Rake在四个开源项目中平均模块化质量提高7 - 14%，业务能力对齐度提高18 - 22%。

Conclusion: 在紧密耦合系统中单纯优化业务上下文会降低分解质量，需要平衡目标。

Abstract: Service-based architecture (SBA) has gained attention in industry and academia as a means to modernize legacy systems. It refers to a design style that enables systems to be developed as suites of small, loosely coupled, and autonomous components (services) that encapsulate functionality and communicate via language-agnostic APIs. However, defining appropriately sized services that capture cohesive subsets of system functionality remains challenging. Existing work often relies on the availability of documentation, access to project personnel, or a priori knowledge of the target number of services, assumptions that do not hold in many real-world scenarios. Our work addresses these limitations using a deep reinforcement learning-based approach to identify appropriately sized services directly from implementation artifacts. We present Rake, a reinforcement learning-based technique that leverages available system documentation and source code to guide service decomposition at the level of implementation methods. Rake does not require specific documentation or access to project personnel and is language-agnostic. It also supports a customizable objective function that balances modularization quality and business capability alignment, i.e., the degree to which a service covers the targeted business capability. We applied Rake to four open-source legacy projects and compared it with two state-of-the-art techniques. On average, Rake achieved 7-14 percent higher modularization quality and 18-22 percent stronger business capability alignment. Our results further show that optimizing solely for business context can degrade decomposition quality in tightly coupled systems, highlighting the need for balanced objectives.

</details>


### [88] [SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization](https://arxiv.org/abs/2512.20482)
*Revanth Gangi Reddy,Ye Liu,Wenting Zhao,JaeHyeok Doo,Tarun Suresh,Daniel Lee,Caiming Xiong,Yingbo Zhou,Semih Yavuz,Shafiq Joty*

Main category: cs.SE

TL;DR: 提出SweRank+框架用于多语言代码库问题定位，含SweRankMulti和SweRankAgent，实验达新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有代码库问题定位排名方法多以Python为主且单遍搜索，需更好方法。

Method: 引入SweRank+框架，SweRankMulti含代码嵌入检索器和列表式LLM重排器并在多语言数据集训练，SweRankAgent采用代理搜索循环。

Result: SweRankMulti在问题定位基准测试中达新的最优性能，SweRankAgent进一步提升定位效果。

Conclusion: SweRank+框架能有效解决多语言代码库问题定位，优于现有单遍排名方法。

Abstract: Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [89] [Quantitative Financial Modeling for Sri Lankan Markets: Approach Combining NLP, Clustering and Time-Series Forecasting](https://arxiv.org/abs/2512.20216)
*Linuk Perera*

Main category: q-fin.CP

TL;DR: 研究提出适用于量化金融的新方法，结合ESG情绪分析、宏观指标和时间序列预测，用于预测斯里兰卡股市经济制度和市场信号，方法集成效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有文献忽视新兴市场和整体整合，为量化金融从业者在复杂市场提供可扩展、准确工具。

Method: 采用FinBERT提取ESG文本情绪，用无监督聚类识别5种潜在ESG制度，通过密集神经网络和梯度提升分类器映射经济状况；用时间序列模型预测收盘价；用规则融合逻辑合并输出。

Result: ESG制度分类训练准确率84.04%，验证准确率82.0%；GRU的R平方为0.801，LSTM日内数据方向准确率52.78%；发现S&P SL20和S&P 500强相关性。

Conclusion: 该量化驱动框架结合全球相关性和本地情绪分析，能为量化金融专业人士提供工具。

Abstract: This research introduces a novel quantitative methodology tailored for quantitative finance applications, enabling banks, stockbrokers, and investors to predict economic regimes and market signals in emerging markets, specifically Sri Lankan stock indices (S&P SL20 and ASPI) by integrating Environmental, Social, and Governance (ESG) sentiment analysis with macroeconomic indicators and advanced time-series forecasting. Designed to leverage quantitative techniques for enhanced risk assessment, portfolio optimization, and trading strategies in volatile environments, the architecture employs FinBERT, a transformer-based NLP model, to extract sentiment from ESG texts, followed by unsupervised clustering (UMAP/HDBSCAN) to identify 5 latent ESG regimes, validated via PCA. These regimes are mapped to economic conditions using a dense neural network and gradient boosting classifier, achieving 84.04% training and 82.0% validation accuracy. Concurrently, time-series models (SRNN, MLP, LSTM, GRU) forecast daily closing prices, with GRU attaining an R-squared of 0.801 and LSTM delivering 52.78% directional accuracy on intraday data. A strong correlation between S&P SL20 and S&P 500, observed through moving average and volatility trend plots, further bolsters forecasting precision. A rule-based fusion logic merges ESG and time-series outputs for final market signals. By addressing literature gaps that overlook emerging markets and holistic integration, this quant-driven framework combines global correlations and local sentiment analysis to offer scalable, accurate tools for quantitative finance professionals navigating complex markets like Sri Lanka.

</details>


### [90] [Modeling Bank Systemic Risk of Emerging Markets under Geopolitical Shocks: Empirical Evidence from BRICS Countries](https://arxiv.org/abs/2512.20515)
*Haibo Wang*

Main category: q-fin.CP

TL;DR: 本文提出BRIDGES框架分析金砖国家银行系统风险，模拟显示大型机构倒闭和地缘政治冲击危害大，主要威胁是二阶恐慌和大规模地缘政治冲击。


<details>
  <summary>Details</summary>
Motivation: 金砖国家经济影响力增长，需要能捕捉复杂长期动态的风险模型。

Method: 引入BRIDGES框架，基于信息复杂度分析系统风险，用DTW构建动态网络，以TGNN学习网络演变，进行ABM模拟评估稳定性。

Result: 大型机构倒闭、地缘政治冲击造成系统性破坏，地缘政治冲击更具毁灭性。

Conclusion: 金砖国家金融稳定主要威胁是二阶恐慌和大规模地缘政治冲击，传统模型可能无法检测。

Abstract: The growing economic influence of the BRICS nations requires risk models that capture complex, long-term dynamics. This paper introduces the Bank Risk Interlinkage with Dynamic Graph and Event Simulations (BRIDGES) framework, which analyzes systemic risk based on the level of information complexity (zero-order, first-order, and second-order). BRIDGES utilizes the Dynamic Time Warping (DTW) distance to construct a dynamic network for 551 BRICS banks based on their strategic similarity, using zero-order information such as annual balance sheet data from 2008 to 2024. It then employs first-order information, including trends in risk ratios, to detect shifts in banks' behavior. A Temporal Graph Neural Network (TGNN), as the core of BRIDGES, is deployed to learn network evolutions and detect second-order information, such as anomalous changes in the structural relationships of the bank network. To measure the impact of anomalous changes on network stability, BRIDGES performs Agent-Based Model (ABM) simulations to assess the banking system's resilience to internal financial failure and external geopolitical shocks at the individual country level and across BRICS nations. Simulation results show that the failure of the largest institutions causes more systemic damage than the failure of the financially vulnerable or dynamically anomalous ones, driven by powerful panic effects. Compared to this "too big to fail" scenario, a geopolitical shock with correlated country-wide propagation causes more destructive systemic damage, leading to a near-total systemic collapse. It suggests that the primary threats to BRICS financial stability are second-order panic and large-scale geopolitical shocks, which traditional risk analysis models might not detect.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [91] [Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization](https://arxiv.org/abs/2512.19986)
*Nikolaos Iliopoulos*

Main category: q-fin.PM

TL;DR: 本文提出协方差感知单纯形投影（CASP）修复算子用于基数约束投资组合优化，在S&P 500数据上表现优于标准欧几里得投影，可降低投资组合方差。


<details>
  <summary>Details</summary>
Motivation: 标准欧几里得投影在处理基数约束投资组合优化时忽略资产协方差结构，可能导致投资组合分散性不足。

Method: 提出两阶段的CASP修复算子，先使用波动率归一化分数选择目标数量资产，再使用与跟踪误差风险对齐的协方差感知几何投影候选权重。

Result: 在S&P 500数据（2020 - 2024）上，CASP - Basic显著降低投资组合方差，消融实验表明波动率归一化选择是降低方差的主要因素，协方差感知投影有额外改进，可选的收益感知扩展能提高夏普比率。

Conclusion: CASP可作为元启发式投资组合优化器中欧几里得投影的替代方法。

Abstract: Metaheuristic algorithms for cardinality-constrained portfolio optimization require repair operators to map infeasible candidates onto the feasible region. Standard Euclidean projection treats assets as independent and can ignore the covariance structure that governs portfolio risk, potentially producing less diversified portfolios. This paper introduces Covariance-Aware Simplex Projection (CASP), a two-stage repair operator that (i) selects a target number of assets using volatility-normalized scores and (ii) projects the candidate weights using a covariance-aware geometry aligned with tracking-error risk. This provides a portfolio-theoretic foundation for using a covariance-induced distance in repair operators. On S&P 500 data (2020-2024), CASP-Basic delivers materially lower portfolio variance than standard Euclidean repair without relying on return estimates, with improvements that are robust across assets and statistically significant. Ablation results indicate that volatility-normalized selection drives most of the variance reduction, while the covariance-aware projection provides an additional, consistent improvement. We further show that optional return-aware extensions can improve Sharpe ratios, and out-of-sample tests confirm that gains transfer to realized performance. CASP integrates as a drop-in replacement for Euclidean projection in metaheuristic portfolio optimizers.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [92] [Generative AI for Analysts](https://arxiv.org/abs/2512.19705)
*Jian Xue,Qian Zhang,Wu Zhu*

Main category: q-fin.ST

TL;DR: 研究生成式AI对金融分析师工作的影响，发现采用AI能使报告更丰富全面且及时，但会使预测误差上升，揭示了其在金融信息生产中的利弊。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI如何改变金融分析师的工作。

Method: 以2023年FactSet的AI平台推出作为自然实验，还使用其他数据供应商进行安慰剂测试。

Result: 采用AI后报告更丰富全面及时，有更多信息源、更广泛主题覆盖和更多高级分析方法使用，但预测误差上升59%。

Conclusion: 揭示了生成式AI在金融信息生产中的生产力提升和认知局限。

Abstract: We study how generative artificial intelligence (AI) transforms the work of financial analysts. Using the 2023 launch of FactSet's AI platform as a natural experiment, we find that adoption produces markedly richer and more comprehensive reports -- featuring 40% more distinct information sources, 34% broader topical coverage, and 25% greater use of advanced analytical methods -- while also improving timeliness. However, forecast errors rise by 59% as AI-assisted reports convey a more balanced mix of positive and negative information that is harder to synthesize, particularly for analysts facing heavier cognitive demands. Placebo tests using other data vendors confirm that these effects are unique to FactSet's AI integration. Overall, our findings reveal both the productivity gains and cognitive limits of generative AI in financial information production.

</details>


### [93] [The Aligned Economic Index & The State Switching Model](https://arxiv.org/abs/2512.20460)
*Ilias Aarab*

Main category: q-fin.ST

TL;DR: 研究美国股票回报在不同经济状态的可预测性，引入状态切换预测回归并提出新聚合预测指标。


<details>
  <summary>Details</summary>
Motivation: 现有文献表明股票溢价可预测性依赖状态，作者研究不同经济状态下美国股票回报可预测性。

Method: 一是引入用收益率曲线斜率实时定义市场状态的状态切换预测回归；二是通过偏最小二乘法构建新聚合预测指标对齐经济指数。

Result: 状态切换回归提升预测性能；对齐经济指数在样本内外均有显著预测力，优于基准预测指标和其他组合方法。

Conclusion: 能够捕捉不同经济状态，提出的方法和指标对股票回报预测有良好效果。

Abstract: A growing empirical literature suggests that equity-premium predictability is state dependent, with much of the forecasting power concentrated around recessionary periods \parencite{Henkel2011,DanglHalling2012,Devpura2018}. I study U.S. stock return predictability across economic regimes and document strong evidence of time-varying expected returns across both expansionary and contractionary states. I contribute in two ways. First, I introduce a state-switching predictive regression in which the market state is defined in real time using the slope of the yield curve. Relative to the standard one-state predictive regression, the state-switching specification increases both in-sample and out-of-sample performance for the set of popular predictors considered by \textcite{WelchGoyal2008}, improving the out-of-sample performance of most predictors in economically meaningful ways. Second, I propose a new aggregate predictor, the Aligned Economic Index, constructed via partial least squares (PLS). Under the state-switching model, the Aligned Economic Index exhibits statistically and economically significant predictive power in sample and out of sample, and it outperforms widely used benchmark predictors and alternative predictor-combination methods.

</details>


### [94] [Switching between states and the COVID-19 turbulence](https://arxiv.org/abs/2512.20477)
*Ilias Aarab*

Main category: q-fin.ST

TL;DR: 本文研究美国股票收益跨经济周期的可预测性，引入状态转换模型和对齐经济指数，该指数在不同市场状态下有预测力，对专业人士有经济价值。


<details>
  <summary>Details</summary>
Motivation: 研究美国股票收益跨经济制度的可预测性，为专业人士寻找有价值的预测方法。

Method: 引入由收益率曲线斜率代表市场状态的状态转换模型，构建基于Welch和Goyal（2008）流行预测因子的对齐经济指数。

Result: 对齐经济指数在样本内和样本外均有显著预测力，优于常用预测因子；计算显示对均值 - 方差投资者有显著经济收益。

Conclusion: 对齐经济指数可实时应用，对学术界和从业者都很重要，在新冠疫情市场动荡中也有可观收益。

Abstract: In Aarab (2020), I examine U.S. stock return predictability across economic regimes and document evidence of time-varying expected returns across market states in the long run. The analysis introduces a state-switching specification in which the market state is proxied by the slope of the yield curve, and proposes an Aligned Economic Index built from the popular predictors of Welch and Goyal (2008) (augmented with bond and equity premium measures). The Aligned Economic Index under the state-switching model exhibits statistically and economically meaningful in-sample ($R^2 = 5.9\%$) and out-of-sample ($R^2_{\text{oos}} = 4.12\%$) predictive power across both recessions and expansions, while outperforming a range of widely used predictors. In this work, I examine the added value for professional practitioners by computing the economic gains for a mean-variance investor and find substantial added benefit of using the new index under the state switching model across all market states. The Aligned Economic Index can thus be implemented on a consistent real-time basis. These findings are crucial for both academics and practitioners as expansions are much longer-lived than recessions. Finally, I extend the empirical exercises by incorporating data through September 2020 and document sizable gains from using the Aligned Economic Index, relative to more traditional approaches, during the COVID-19 market turbulence.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [95] [Robust Causal Directionality Inference in Quantum Inference under MNAR Observation and High-Dimensional Noise](https://arxiv.org/abs/2512.19746)
*Joonsung Kang*

Main category: stat.ML

TL;DR: 本文引入统一框架用于量子工程中鲁棒因果方向性推断，方法结合多种技术，模拟和真实数据分析表明该框架表现良好。


<details>
  <summary>Details</summary>
Motivation: 量子力学中观测与系统关系类似于统计中的MNAR，需确定因果方向。

Method: 集成基于CVAE的潜在约束、MNAR感知选择模型、GEE稳定回归、惩罚经验似然和贝叶斯优化等方法。

Result: 模拟和真实数据（TCGA基因表达、蛋白质组学）分析显示，提出的框架偏差和方差更低，覆盖率接近标称，量子专用诊断更优。

Conclusion: 稳健因果方向性推断是可靠量子工程的关键方法学进展。

Abstract: In quantum mechanics, observation actively shapes the system, paralleling the statistical notion of Missing Not At Random (MNAR). This study introduces a unified framework for \textbf{robust causal directionality inference} in quantum engineering, determining whether relations are system$\to$observation, observation$\to$system, or bidirectional.
  The method integrates CVAE-based latent constraints, MNAR-aware selection models, GEE-stabilized regression, penalized empirical likelihood, and Bayesian optimization. It jointly addresses quantum and classical noise while uncovering causal directionality, with theoretical guarantees for double robustness, perturbation stability, and oracle inequalities.
  Simulation and real-data analyses (TCGA gene expression, proteomics) show that the proposed MNAR-stabilized CVAE+GEE+AIPW+PEL framework achieves lower bias and variance, near-nominal coverage, and superior quantum-specific diagnostics. This establishes robust causal directionality inference as a key methodological advance for reliable quantum engineering.

</details>


### [96] [Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification Loss Function](https://arxiv.org/abs/2512.19913)
*Matthew Drnevich,Stephen Jiggins,Kyle Cranmer*

Main category: stat.ML

TL;DR: 将基于分类器的密度比估计任务推广到准概率设置，引入新损失函数和距离度量，在粒子物理实例中获最优结果。


<details>
  <summary>Details</summary>
Motivation: 多数用于该任务的损失函数在最优分类器和目标准概率密度比之间定义的关系不连续或非满射，存在问题。

Method: 引入适合概率和准概率密度比估计的凸损失函数，引入与准概率分布兼容的扩展版切片沃瑟斯坦距离。

Result: 在粒子物理的双希格斯玻色子产生实例中取得了最优结果。

Conclusion: 所提出的方法在准概率密度比估计任务中有效，能取得良好效果。

Abstract: We consider a generalization of the classifier-based density-ratio estimation task to a quasiprobabilistic setting where probability densities can be negative. The problem with most loss functions used for this task is that they implicitly define a relationship between the optimal classifier and the target quasiprobabilistic density ratio which is discontinuous or not surjective. We address these problems by introducing a convex loss function that is well-suited for both probabilistic and quasiprobabilistic density ratio estimation. To quantify performance, an extended version of the Sliced-Wasserstein distance is introduced which is compatible with quasiprobability distributions. We demonstrate our approach on a real-world example from particle physics, of di-Higgs production in association with jets via gluon-gluon fusion, and achieve state-of-the-art results.

</details>


### [97] [Semiparametric KSD test: unifying score and distance-based approaches for goodness-of-fit testing](https://arxiv.org/abs/2512.20007)
*Zhihan Huang,Ziang Niu*

Main category: stat.ML

TL;DR: 本文提出基于指数倾斜模型将得分拟合优度检验与积分概率度量检验建立联系，提出半参数核化斯坦因差异（SKSD）检验，该检验计算高效、具有一致性等优势并通过实例验证。


<details>
  <summary>Details</summary>
Motivation: 传统得分拟合优度检验扩展到强大非参数替代方法存在困难，因缺乏合适得分函数。

Method: 通过指数倾斜模型将得分拟合优度检验和基于积分概率度量（IPM）的检验建立等价关系，提出基于特定IPM类的SKSD检验，并采用通用参数bootstrap过程。

Result: SKSD检验计算高效，能适应一般干扰参数估计，具有一致性并达到皮特曼效率；可用于似然难以处理但得分可处理的模型，效果与特定正态性检验相当。

Conclusion: 提出的SKSD检验是一种有效的非参数得分拟合优度检验方法，可用于多种模型。

Abstract: Goodness-of-fit (GoF) tests are fundamental for assessing model adequacy. Score-based tests are appealing because they require fitting the model only once under the null. However, extending them to powerful nonparametric alternatives is difficult due to the lack of suitable score functions. Through a class of exponentially tilted models, we show that the resulting score-based GoF tests are equivalent to the tests based on integral probability metrics (IPMs) indexed by a function class. When the class is rich, the test is universally consistent. This simple yet insightful perspective enables reinterpretation of classical distance-based testing procedures-including those based on Kolmogorov-Smirnov distance, Wasserstein-1 distance, and maximum mean discrepancy-as arising from score-based constructions. Building on this insight, we propose a new nonparametric score-based GoF test through a special class of IPM induced by kernelized Stein's function class, called semiparametric kernelized Stein discrepancy (SKSD) test. Compared with other nonparametric score-based tests, the SKSD test is computationally efficient and accommodates general nuisance-parameter estimators, supported by a generic parametric bootstrap procedure. The SKSD test is universally consistent and attains Pitman efficiency. Moreover, SKSD test provides simple GoF tests for models with intractable likelihoods but tractable scores with the help of Stein's identity and we use two popular models, kernel exponential family and conditional Gaussian models, to illustrate the power of our method. Our method achieves power comparable to task-specific normality tests such as Anderson-Darling and Lilliefors, despite being designed for general nonparametric alternatives.

</details>


### [98] [Generative Bayesian Hyperparameter Tuning](https://arxiv.org/abs/2512.20051)
*Hedibert Lopes,Nick Polson,Vadim Sokolov*

Main category: stat.ML

TL;DR: 提出超参数调优的生成视角，结合贝叶斯后验近似和优化映射方法，实现超参数快速评估和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中超参数选择是核心实际问题，交叉验证计算成本高，全贝叶斯超参数学习因后验采样成本大而困难。

Method: 结合基于优化的贝叶斯后验近似（加权贝叶斯自助法）和学习从超参数到优化器的传输映射。

Result: 得到估计器的“生成器查找表”，能在超参数网格或连续范围上快速评估，支持预测调优目标和近似贝叶斯不确定性量化。

Conclusion: 该方法将新视角与加权M - 估计、辅助变量表示及相关生成采样器建立了联系，为超参数调优提供新思路。

Abstract: \noindent Hyper-parameter selection is a central practical problem in modern machine learning, governing regularization strength, model capacity, and robustness choices. Cross-validation is often computationally prohibitive at scale, while fully Bayesian hyper-parameter learning can be difficult due to the cost of posterior sampling. We develop a generative perspective on hyper-parameter tuning that combines two ideas: (i) optimization-based approximations to Bayesian posteriors via randomized, weighted objectives (weighted Bayesian bootstrap), and (ii) amortization of repeated optimization across many hyper-parameter settings by learning a transport map from hyper-parameters (including random weights) to the corresponding optimizer. This yields a ``generator look-up table'' for estimators, enabling rapid evaluation over grids or continuous ranges of hyper-parameters and supporting both predictive tuning objectives and approximate Bayesian uncertainty quantification. We connect this viewpoint to weighted $M$-estimation, envelope/auxiliary-variable representations that reduce non-quadratic losses to weighted least squares, and recent generative samplers for weighted $M$-estimators.

</details>


### [99] [Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models](https://arxiv.org/abs/2512.20021)
*Anna R. Flowers,Christopher T. Franck,Robert B. Gramacy,Justin A. Krometis*

Main category: stat.ML

TL;DR: 提出利用计算机实验工具和训练数据元数据指导后续数据采集以提升模型性能的方法，并通过实例验证。


<details>
  <summary>Details</summary>
Motivation: 收集实际操作数据成本高，在收集新数据前需了解模型不足，以提升模型性能。

Method: 根据训练数据元数据改变训练数据来评估学习器，用高斯过程代理拟合响应面以指导新数据采集。

Result: 与随机选择元数据的数据相比，该元学习方法能提升学习器性能。

Conclusion: 所提方法在经典学习示例和飞机航拍图像收集应用中有效。

Abstract: Collecting operationally realistic data to inform machine learning models can be costly. Before collecting new data, it is helpful to understand where a model is deficient. For example, object detectors trained on images of rare objects may not be good at identification in poorly represented conditions. We offer a way of informing subsequent data acquisition to maximize model performance by leveraging the toolkit of computer experiments and metadata describing the circumstances under which the training data was collected (e.g., season, time of day, location). We do this by evaluating the learner as the training data is varied according to its metadata. A Gaussian process (GP) surrogate fit to that response surface can inform new data acquisitions. This meta-learning approach offers improvements to learner performance as compared to data with randomly selected metadata, which we illustrate on both classic learning examples, and on a motivating application involving the collection of aerial images in search of airplanes.

</details>


### [100] [KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis](https://arxiv.org/abs/2512.20305)
*Mebin Jose,Jisha Francis,Sudheesh Kumar Kattumannil*

Main category: stat.ML

TL;DR: 本文引入KAN - AFT框架将KANs应用于AFT模型，有公式、优化策略和可解释性流程，实证表明其性能佳且有可解释性。


<details>
  <summary>Details</summary>
Motivation: CoxPH和传统AFT模型有局限，DeepAFT有黑箱可解释性问题，CoxKAN有可解释性优势，受其启发提出KAN - AFT。

Method: 提出AFT - KAN公式，针对右删失观测采用鲁棒优化策略，构建可解释性流程将样条函数转换为生存时间的闭式符号方程。

Result: 在多个数据集上的实证结果显示，KAN - AFT性能与DeepAFT相当或更好。

Conclusion: KAN - AFT能有效建模复杂非线性关系，在性能良好的同时，能为生存过程提供透明的符号模型。

Abstract: Survival analysis relies fundamentally on the semi-parametric Cox Proportional Hazards (CoxPH) model and the parametric Accelerated Failure Time (AFT) model. CoxPH assumes constant hazard ratios, often failing to capture real-world dynamics, while traditional AFT models are limited by rigid distributional assumptions. Although deep learning models like DeepAFT address these constraints by improving predictive accuracy and handling censoring, they inherit the significant challenge of black-box interpretability. The recent introduction of CoxKAN demonstrated the successful integration of Kolmogorov-Arnold Networks (KANs), a novel architecture that yields highly accurate and interpretable symbolic representations, within the CoxPH framework. Motivated by the interpretability gains of CoxKAN, we introduce KAN-AFT (Kolmogorov Arnold Network-based AFT), the first framework to apply KANs to the AFT model. KAN-AFT effectively models complex nonlinear relationships within the AFT framework. Our primary contributions include: (i) a principled AFT-KAN formulation, (ii) robust optimization strategies for right-censored observations (e.g., Buckley-James and IPCW), and (iii) an interpretability pipeline that converts the learned spline functions into closed-form symbolic equations for survival time. Empirical results on multiple datasets confirm that KAN-AFT achieves performance comparable to or better than DeepAFT, while uniquely providing transparent, symbolic models of the survival process.

</details>


### [101] [Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability](https://arxiv.org/abs/2512.20368)
*Samya Praharaj,Koulik Khamaru*

Main category: stat.ML

TL;DR: 本文提出并分析了线性上下文老虎机的惩罚EXP4算法，证明其满足稳定性条件，可构建有效置信区间，且达到近最优后悔界，模拟实验验证了理论。


<details>
  <summary>Details</summary>
Motivation: 上下文老虎机数据的自适应、非独立同分布特性使统计推断复杂，经典最小二乘推断可能失效，需付出适应性代价，研究旨在克服此局限。

Method: 提出并分析线性上下文老虎机的惩罚EXP4算法。

Result: 该算法满足Lai - Wei稳定性条件，可构建有效Wald型置信区间，达到近最优后悔界，模拟实验验证了估计量的经验正态性和置信区间的尖锐性。

Conclusion: 稳定性和统计效率可在单个上下文老虎机方法中并存。

Abstract: Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $\sqrt{d \log T}$. This phenomenon -- often referred to as the price of adaptivity -- highlights the inherent difficulty of reliable inference under general contextual bandit policies.
  A key structural property that circumvents this limitation is the \emph{stability} condition of Lai and Wei, which requires the empirical feature covariance to concentrate around a deterministic limit. When stability holds, the ordinary least-squares estimator satisfies a central limit theorem, and classical Wald-type confidence intervals -- designed for i.i.d. data -- become asymptotically valid even under adaptation, \emph{without} incurring the $\sqrt{d \log T}$ price of adaptivity.
  In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals.

</details>


### [102] [Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention](https://arxiv.org/abs/2512.20562)
*Yingzhen Yang*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of learning a low-degree spherical polynomial of degree $\ell_0 = Θ(1) \ge 1$ defined on the unit sphere in $\RR^d$ by training an over-parameterized two-layer neural network (NN) with channel attention in this paper. Our main result is the significantly improved sample complexity for learning such low-degree polynomials. We show that, for any regression risk $\eps \in (0,1)$, a carefully designed two-layer NN with channel attention and finite width of $m \ge Θ({n^4 \log (2n/δ)}/{d^{2\ell_0}})$ trained by the vanilla gradient descent (GD) requires the lowest sample complexity of $n \asymp Θ(d^{\ell_0}/\eps)$ with probability $1-δ$ for every $δ\in (0,1)$, in contrast with the representative sample complexity $Θ\pth{d^{\ell_0} \max\set{\eps^{-2},\log d}}$, where $n$ is the training daata size. Moreover, such sample complexity is not improvable since the trained network renders a sharp rate of the nonparametric regression risk of the order $Θ(d^{\ell_0}/{n})$ with probability at least $1-δ$. On the other hand, the minimax optimal rate for the regression risk with a kernel of rank $Θ(d^{\ell_0})$ is $Θ(d^{\ell_0}/{n})$, so that the rate of the nonparametric regression risk of the network trained by GD is minimax optimal. The training of the two-layer NN with channel attention consists of two stages. In Stage 1, a provable learnable channel selection algorithm identifies the ground-truth channel number $\ell_0$ from the initial $L \ge \ell_0$ channels in the first-layer activation, with high probability. This learnable selection is achieved by an efficient one-step GD update on both layers, enabling feature learning for low-degree polynomial targets. In Stage 2, the second layer is trained by standard GD using the activation function with the selected channels.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [103] [Numerical Analysis of Test Optimality](https://arxiv.org/abs/2512.19843)
*Philipp Ketz,Adam McCloskey,Jan Scherer*

Main category: econ.EM

TL;DR: 本文提出数值框架判断特设检验是否有效最优，用嵌套优化算法近似权重函数，有收敛保证并应用于相关检验。


<details>
  <summary>Details</summary>
Motivation: 非标准测试环境下特设检验最优性先验未知且难评估，需确定其是否有效最优。

Method: 使用嵌套优化算法近似权重函数，使特设检验的加权平均功效接近真正的加权平均功效最大化检验。

Result: 发现加权平均功效最大化检验对应的拒绝概率构成特设检验的近似功效包络。

Conclusion: 提出的数值框架可用于判断特设检验的有效性，有收敛保证并可应用于实际检验。

Abstract: In nonstandard testing environments, researchers often derive ad hoc tests with correct (asymptotic) size, but their optimality properties are typically unknown a priori and difficult to assess. This paper develops a numerical framework for determining whether an ad hoc test is effectively optimal - approximately maximizing a weighted average power criterion for some weights over the alternative and attaining a power envelope generated by a single weighted average power-maximizing test. Our approach uses nested optimization algorithms to approximate the weight function that makes an ad hoc test's weighted average power as close as possible to that of a true weighted average power-maximizing test, and we show the surprising result that the rejection probabilities corresponding to the latter form an approximate power envelope for the former. We provide convergence guarantees, discuss practical implementation and apply the method to the weak instrument-robust conditional likelihood ratio test and a recently-proposed test for when a nuisance parameter may be on or near its boundary.

</details>


### [104] [ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification](https://arxiv.org/abs/2512.20523)
*Masahiro Kato*

Main category: econ.EM

TL;DR: 提出基于得分匹配的Riesz表示估计方法，缓解过拟合，为因果推断提供见解。


<details>
  <summary>Details</summary>
Motivation: 直接估计Riesz表示的方法在DRE中易过拟合，需解决该问题。

Method: 将基于得分匹配的DRE方法扩展到Riesz表示估计。

Result: 所提方法缓解过拟合，通过时间得分函数连接边际效应和平均政策效应。

Conclusion: 所提方法有效，为因果推断提供了有价值的信息。

Abstract: This study proposes Riesz representer estimation methods based on score matching. The Riesz representer is a key component in debiased machine learning for constructing $\sqrt{n}$-consistent and efficient estimators in causal inference and structural parameter estimation. To estimate the Riesz representer, direct approaches have garnered attention, such as Riesz regression and the covariate balancing propensity score. These approaches can also be interpreted as variants of direct density ratio estimation (DRE) in several applications such as average treatment effect estimation. In DRE, it is well known that flexible models can easily overfit the observed data due to the estimand and the form of the loss function. To address this issue, recent work has proposed modeling the density ratio as a product of multiple intermediate density ratios and estimating it using score-matching techniques, which are often used in the diffusion model literature. We extend score-matching-based DRE methods to Riesz representer estimation. Our proposed method not only mitigates overfitting but also provides insights for causal inference by bridging marginal effects and average policy effects through time score functions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [105] [Post-Quantum Cryptography in the 5G Core](https://arxiv.org/abs/2512.20243)
*Thomas Attema,Bor de Kock,Sandesh Manganahalli Jayaprakash,Dimitrios Schoinianakis,Thom Sijpesteijn,Rintse van de Vlasakker*

Main category: cs.CR

TL;DR: 本文评估5G核心网用后量子加密算法替代传统算法的实际影响，发现影响小，网络能支持后量子加密。


<details>
  <summary>Details</summary>
Motivation: 评估5G核心网用后量子加密算法替代传统算法的实际影响。

Method: 使用模拟环境，对不同数量用户设备的注册和注销进行建模，测量带宽消耗和延迟。

Result: 部署后量子加密算法对性能有可测量影响，但影响小，额外开销对网络可用性和功能效率无实质影响。

Conclusion: 5G核心网技术上能支持后量子加密，无计算开销增加和消息变大的固有问题。

Abstract: In this work, the conventional cryptographic algorithms used in the 5G Core are replaced with post-quantum alternatives and the practical impact of this transition is evaluated. Using a simulation environment, we model the registration and deregistration of varying numbers of user equipments (UEs) and measure the resulting effects on bandwidth consumption and latency.
  Our results show that the deployment of post-quantum cryptographic algorithms has a measurable effect on performance, but that this effect is small, and perhaps more crucially, that the extra overhead needed in terms of computation and bandwidth does not have any substantial impact on the usability of the network and the efficiency of its network functions.
  Overall the experimental results in this work corroborate earlier research: the 5G Core is technically able to support post-quantum cryptography without any inherent issues connected to the increased computational overhead or larger message size.

</details>


### [106] [BacAlarm: Mining and Simulating Composite API Traffic to Prevent Broken Access Control Violations](https://arxiv.org/abs/2512.19997)
*Yanjing Yang,He Zhang,Bohan Liu,Jinwei Xu,Jinghao Hu,Liming Dong,Zhewen Mao,Dongxue Pan*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Broken Access Control (BAC) violations, which consistently rank among the top five security risks in the OWASP API Security Top 10, refer to unauthorized access attempts arising from BAC vulnerabilities, whose successful exploitation can impose significant risks on exposed application programming interfaces (APIs). In recent years, learning-based methods have demonstrated promising prospects in detecting various types of malicious activities. However, in real-network operation and maintenance scenarios, leveraging learning-based methods for BAC detection faces two critical challenges. Firstly, under the RESTful API design principles, most systems omit recording composite traffic for performance, and together with ethical and legal bans on directly testing real-world systems, this leads to a critical shortage of training data for detecting BAC violations. Secondly, common malicious behaviors such as SQL injection typically generate individual access traffic that is inherently anomalous. In contrast, BAC is usually composed of multiple correlated access requests that appear normal when examined in isolation. To tackle these problems, we introduce \BAC, an approach for establishing a BAC violation detection model by generating and utilizing API traffic data. The \BAC consists of an API Traffic Generator and a BAC Detector. Experimental results show that \BAC outperforms current state-of-the-art invariant-based and learning-based methods with the $\text{F}_1$ and MCC improving by 21.2\% and 24.1\%.

</details>


### [107] [Symmaries: Automatic Inference of Formal Security Summaries for Java Programs](https://arxiv.org/abs/2512.20396)
*Narges Khakpour,Nicolas Berthier*

Main category: cs.CR

TL;DR: 介绍一种为Java字节码程序自动构建安全规范的方法，实现工具Symmaries并验证其可扩展性和精度。


<details>
  <summary>Details</summary>
Motivation: 为Java字节码程序自动构建形式化安全规范，帮助静态代码分析工具和开发者理解代码安全行为。

Method: 提出一种可扩展、模块化且合理的方法，并实现工具Symmaries自动生成安全摘要。

Result: Symmaries可扩展分析数十万行代码的应用程序，根据堆模型达到一定精度。

Conclusion: 证明了方法在保证终止不敏感非干扰方面的合理性。

Abstract: We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference.

</details>


### [108] [iblock: Accurate and Scalable Bitcoin Simulations with OMNeT++](https://arxiv.org/abs/2512.20402)
*Niccolò Scatena,Pericle Perazzo,Giovanni Nardini*

Main category: cs.CR

TL;DR: 本文提出用于OMNeT++的比特币模拟C++库iblock，效率和可扩展性佳，经测试和验证表现良好。


<details>
  <summary>Details</summary>
Motivation: 开发更高效、可扩展且能进行高详细度模拟的比特币模拟库。

Method: 提出iblock库，与现有区块链模拟器对比性能，并模拟不同场景验证。

Result: iblock在相同模拟细节水平下更高效，模拟结果符合理论预期。

Conclusion: iblock是一个高效且可靠的比特币模拟库。

Abstract: This paper proposes iblock, a comprehensive C++ library for Bitcoin simulation, designed for OMNeT++. iblock offers superior efficiency and scalability with respect to state-of-the-art simulators, which are typically written in high-level languages. Moreover, the possible integration with other OMNeT++ libraries allows highly detailed simulations. We measure iblock's performance against a state-of-the-art blockchain simulator, proving that it is more efficient at the same level of simulation detail. We also validate iblock by using it to simulate different scenarios such as the normal Bitcoin operation and the selfish mine attack, showing that simulation results are coherent with theoretical expectations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [109] [Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark](https://arxiv.org/abs/2512.20174)
*Hao Guo,Xugong Qin,Jun Jie Ou Yang,Peng Zhang,Gangyan Zeng,Yubo Li,Hailun Lin*

Main category: cs.CV

TL;DR: 本文提出新的基于自然语言的文档图像检索（NL - DIR）基准，含数据集与评估指标，对现有模型进行评估，并研究两阶段检索方法，希望为VDU社区带来新机遇。


<details>
  <summary>Details</summary>
Motivation: 现有文档图像检索方法难处理具细粒度语义的文本查询，为填补这一空白开展研究。

Method: 引入NL - DIR基准和评估指标，用自然语言描述作查询；构建含41K文档图像及对应查询的数据集；对主流对比视觉 - 语言模型和无OCR视觉文档理解模型进行零样本和微调评估；研究两阶段检索方法。

Result: 构建了NL - DIR数据集，对现有模型评估，研究出两阶段检索方法提升性能并兼顾时空效率。

Conclusion: 提出的NL - DIR基准能为VDU社区带来新机会，推动相关研究，数据集和代码将公开。

Abstract: Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.

</details>


### [110] [High Dimensional Data Decomposition for Anomaly Detection of Textured Images](https://arxiv.org/abs/2512.20432)
*Ji Song,Xing Wang,Jianguo Wu,Xiaowei Yue*

Main category: cs.CV

TL;DR: 本文提出TBSD方法用于纹理图像异常检测，在模拟和真实数据集上表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法在处理纹理缺陷图像时存在误识别、鲁棒性低和依赖大规模结构化数据集等问题。

Method: 提出纹理基集成平滑分解（TBSD）方法，包括学习纹理基函数提取准周期纹理模式和利用纹理基进行异常检测两个主要过程，并研究准周期性的数学公式及其理论性质。

Result: 该方法在模拟和真实数据集上误识别更少、所需训练数据集更小，异常检测性能更优。

Conclusion: TBSD方法在纹理图像异常检测方面表现出色，优于传统方法。

Abstract: In the realm of diverse high-dimensional data, images play a significant role across various processes of manufacturing systems where efficient image anomaly detection has emerged as a core technology of utmost importance. However, when applied to textured defect images, conventional anomaly detection methods have limitations including non-negligible misidentification, low robustness, and excessive reliance on large-scale and structured datasets. This paper proposes a texture basis integrated smooth decomposition (TBSD) approach, which is targeted at efficient anomaly detection in textured images with smooth backgrounds and sparse anomalies. Mathematical formulation of quasi-periodicity and its theoretical properties are investigated for image texture estimation. TBSD method consists of two principal processes: the first process learns the texture basis functions to effectively extract quasi-periodic texture patterns; the subsequent anomaly detection process utilizes that texture basis as prior knowledge to prevent texture misidentification and capture potential anomalies with high accuracy.The proposed method surpasses benchmarks with less misidentification, smaller training dataset requirement, and superior anomaly detection performance on both simulation and real-world datasets.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [111] [Optimal Anytime-Valid Tests for Composite Nulls](https://arxiv.org/abs/2512.20039)
*Shubhanshu Shekhar*

Main category: math.ST

TL;DR: 本文研究复合原假设下最优水平 - α 功效 - 1 检验的设计问题，提出构造性方案以匹配已知下界，在有限字母表情况证明一种检验方法的最优性，并给出任意字母表情况下的通用方法及充分条件，最后讨论实际计算问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究给出水平 - α 功效 - 1 检验的下界，本文旨在开发能在 α 趋近于 0 时达到该下界的构造性方案。

Method: 先考虑有限字母表情况，利用基于通用 e - 过程的检验方法，并通过 Donsker - Varadhan 鞍点表示和 Sion 极小极大定理证明其最优性。然后针对任意字母表情况，基于对丰富测试函数类的鞍点表示的经验解构造 e - 过程。

Result: 证明了有限字母表情况下基于通用 e - 过程的检验方法的最优性，给出任意字母表情况下针对紧致凸原假设检验最优性的充分条件，并在 Hölder 光滑密度模型中验证。

Conclusion: 提出的构造性方案能在一定条件下匹配水平 - α 功效 - 1 检验的下界，还讨论了实际应用中的计算问题。

Abstract: We consider the problem of designing optimal level-$α$ power-one tests for composite nulls. Given a parameter $α\in (0,1)$ and a stream of $\mathcal{X}$-valued observations $\{X_n: n \geq 1\} \overset{i.i.d.}{\sim} P$, the goal is to design a level-$α$ power-one test $τ_α$ for the null $H_0: P \in \mathcal{P}_0 \subset \mathcal{P}(\mathcal{X})$. Prior works have shown that any such $τ_α$ must satisfy $\mathbb{E}_P[τ_α] \geq \tfrac{\log(1/α)}{γ^*(P, \mathcal{P}_0)}$, where $γ^*(P, \mathcal{P}_0)$ is the so-called $\mathrm{KL}_{\inf}$ or minimum divergence of $P$ to the null class. In this paper, our objective is to develop and analyze constructive schemes that match this lower bound as $α\downarrow 0$.
  We first consider the finite-alphabet case~($|\mathcal{X}| = m < \infty$), and show that a test based on \emph{universal} $e$-process~(formed by the ratio of a universal predictor and the running null MLE) is optimal in the above sense. The proof relies on a Donsker-Varadhan~(DV) based saddle-point representation of $\mathrm{KL}_{\inf}$, and an application of Sion's minimax theorem. This characterization motivates a general method for arbitrary $\mathcal{X}$: construct an $e$-process based on the empirical solutions to the saddle-point representation over a sufficiently rich class of test functions. We give sufficient conditions for the optimality of this test for compact convex nulls, and verify them for Hölder smooth density models. We end the paper with a discussion on the computational aspects of implementing our proposed tests in some practical settings.

</details>


### [112] [Structure-Preserving Nonlinear Sufficient Dimension Reduction for Tensors](https://arxiv.org/abs/2512.20057)
*Dianjun Lin,Bing Li,Lingzhou Xue*

Main category: math.ST

TL;DR: 本文介绍两种针对张量值预测变量回归的非线性充分降维方法，分别基于Tucker分解和CP分解，具有良好性质，在模拟和应用中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在降维时保留张量结构以提高可解释性，减少降维参数以实现模型简洁性并提高估计精度。

Method: 两种张量降维方法分别基于Tucker分解（将大张量降为小张量）和CP分解（将任意张量表示为一系列秩一张量）。

Result: 在总体层面证明方法的Fisher一致性，建立一致性和收敛速率，数值上易于实现，在模拟和两个数据应用中精度比现有方法大幅提高。

Conclusion: 提出的两种张量降维方法有效，能改善回归分析的性能。

Abstract: We introduce two nonlinear sufficient dimension reduction methods for regressions with tensor-valued predictors. Our goal is two-fold: the first is to preserve the tensor structure when performing dimension reduction, particularly the meaning of the tensor modes, for improved interpretation; the second is to substantially reduce the number of parameters in dimension reduction, thereby achieving model parsimony and enhancing estimation accuracy. Our two tensor dimension reduction methods echo the two commonly used tensor decomposition mechanisms: one is the Tucker decomposition, which reduces a larger tensor to a smaller one; the other is the CP-decomposition, which represents an arbitrary tensor as a sequence of rank-one tensors. We developed the Fisher consistency of our methods at the population level and established their consistency and convergence rates. Both methods are easy to implement numerically: the Tucker-form can be implemented through a sequence of least-squares steps, and the CP-form can be implemented through a sequence of singular value decompositions. We investigated the finite-sample performance of our methods and showed substantial improvement in accuracy over existing methods in simulations and two data applications.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [113] [Snapshot 3D image projection using a diffractive decoder](https://arxiv.org/abs/2512.20464)
*Cagatay Isil,Alexander Chen,Yuhang Li,F. Onuralp Ardic,Shiqi Chen,Che-Yung Shen,Aydogan Ozcan*

Main category: physics.optics

TL;DR: 本文提出用数字编码器和衍射光学解码器的3D显示系统，实现高保真深度分辨3D图像投影，以解决3D图像投影中深度复用的挑战，并验证了方法有效性及应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决3D图像投影中密度深度复用难题，即衍射引起的串扰随轴向图像平面靠近而增加的问题。

Method: 采用多层衍射波前解码和基于深度学习的端到端优化，数字编码器利用傅里叶编码器网络，结合轴向位置编码，生成统一相位表示，通过联合优化的衍射解码器实现投影。

Result: 表征了衍射解码器深度等因素对轴向分离和3D图像投影质量的影响；展示显示28个轴向切片的体积图像及动态重新配置图像平面轴向位置的能力；实验验证测量结果与目标图像高度一致。

Conclusion: 该衍射3D显示系统是用于深度分辨快照3D图像投影的紧凑且可扩展框架，在全息显示等领域有潜在应用。

Abstract: 3D image display is essential for next-generation volumetric imaging; however, dense depth multiplexing for 3D image projection remains challenging because diffraction-induced cross-talk rapidly increases as the axial image planes get closer. Here, we introduce a 3D display system comprising a digital encoder and a diffractive optical decoder, which simultaneously projects different images onto multiple target axial planes with high axial resolution. By leveraging multi-layer diffractive wavefront decoding and deep learning-based end-to-end optimization, the system achieves high-fidelity depth-resolved 3D image projection in a snapshot, enabling axial plane separations on the order of a wavelength. The digital encoder leverages a Fourier encoder network to capture multi-scale spatial and frequency-domain features from input images, integrates axial position encoding, and generates a unified phase representation that simultaneously encodes all images to be axially projected in a single snapshot through a jointly-optimized diffractive decoder. We characterized the impact of diffractive decoder depth, output diffraction efficiency, spatial light modulator resolution, and axial encoding density, revealing trade-offs that govern axial separation and 3D image projection quality. We further demonstrated the capability to display volumetric images containing 28 axial slices, as well as the ability to dynamically reconfigure the axial locations of the image planes, performed on demand. Finally, we experimentally validated the presented approach, demonstrating close agreement between the measured results and the target images. These results establish the diffractive 3D display system as a compact and scalable framework for depth-resolved snapshot 3D image projection, with potential applications in holographic displays, AR/VR interfaces, and volumetric optical computing.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [114] [Self-motion as a structural prior for coherent and robust formation of cognitive maps](https://arxiv.org/abs/2512.20044)
*Yingchao Yu,Pengfei Sun,Yaochu Jin,Kuangrong Hao,Hao Zhang,Yifeng Zhang,Wenxuan Pan,Wei Chen,Danyal Akarca,Yuchen Xiao*

Main category: q-bio.NC

TL;DR: 研究表明自运动可作为结构先验，主动组织学习的认知地图的几何结构，能增强具身智能体的空间智能。


<details>
  <summary>Details</summary>
Motivation: 多数计算模型认为认知地图的稳定性主要通过感官锚定实现，而生物学上即使感官线索退化或冲突，空间表征仍连贯，因此推测自运动可能有更深层次的组织作用。

Method: 将基于路径积分的运动先验嵌入预测编码框架，采用结合脉冲动力学、模拟调制和自适应阈值的高效能、脑启发式循环机制实现。

Result: 该结构先验在多种环境下可稳定地图形成，提升局部拓扑保真度、全局位置精度和在感官模糊下的下一步预测，运动先验能编码精确轨迹并泛化到未见过的环境，在四足机器人上的应用表明其能增强真实场景中的导航能力。

Conclusion: 自运动可作为连贯空间表征的组织支架，脑启发原则能系统地增强具身智能体的空间智能。

Abstract: Most computational accounts of cognitive maps assume that stability is achieved primarily through sensory anchoring, with self-motion contributing to incremental positional updates only. However, biological spatial representations often remain coherent even when sensory cues degrade or conflict, suggesting that self-motion may play a deeper organizational role. Here, we show that self-motion can act as a structural prior that actively organizes the geometry of learned cognitive maps. We embed a path-integration-based motion prior in a predictive-coding framework, implemented using a capacity-efficient, brain-inspired recurrent mechanism combining spiking dynamics, analog modulation and adaptive thresholds. Across highly aliased, dynamically changing and naturalistic environments, this structural prior consistently stabilizes map formation, improving local topological fidelity, global positional accuracy and next-step prediction under sensory ambiguity. Mechanistic analyses reveal that the motion prior itself encodes geometrically precise trajectories under tight constraints of internal states and generalizes zero-shot to unseen environments, outperforming simpler motion-based constraints. Finally, deployment on a quadrupedal robot demonstrates that motion-derived structural priors enhance online landmark-based navigation under real-world sensory variability. Together, these results reframe self-motion as an organizing scaffold for coherent spatial representations, showing how brain-inspired principles can systematically strengthen spatial intelligence in embodied artificial agents.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [115] [Retrieval-augmented Prompt Learning for Pre-trained Foundation Models](https://arxiv.org/abs/2512.20145)
*Xiang Chen,Yixin Ou,Quan Feng,Lei Li,Piji Li,Haibo Ye,Sheng-Jun Huang,Shuofei Qiao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 提出RetroPrompt方法，通过解耦知识平衡记忆与泛化，在多数据集多任务的零样本和少样本场景中表现优越，减少死记硬背依赖，增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有预训练基础模型的提示学习遵循参数化学习范式，在记忆和死记硬背学习中泛化稳定性不足，全监督训练时难以充分利用非典型实例和避免过拟合。

Method: 提出RetroPrompt方法，利用训练数据生成公共知识库，在输入、训练和推理阶段引入检索机制，让模型从语料库中主动检索上下文信息。

Result: 在自然语言处理和计算机视觉任务的多个数据集上实验表明，RetroPrompt在零样本和少样本场景下表现优越。

Conclusion: RetroPrompt有效减少对死记硬背的依赖，增强了模型的泛化能力。

Abstract: The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [116] [Generalized method of L-moment estimation for stationary and nonstationary extreme value models](https://arxiv.org/abs/2512.20385)
*Yonggwan Shin,Yire Shin,Jihong Park,Jeong-Soo Park*

Main category: stat.ME

TL;DR: 本文提出广义L - 矩估计法（GLME）用于估计广义极值（GEV）分布参数，模拟研究和实际应用表明该方法能纠正L - 矩估计（LME）偏差，或推动基于L - 矩的惩罚或贝叶斯推断研究。


<details>
  <summary>Details</summary>
Motivation: 已有广义最大似然估计（MLE）方法用于GEV分布参数估计，但广义L - 矩估计（LME）方法尚未开发，为更好估计GEV分布参数，提出GLME方法。

Method: 基于广义L - 矩距离和多元正态似然近似提出GLME，将其应用于平稳和非平稳GEV模型，使用两种新型惩罚函数纠正LME偏差。

Result: 模拟研究显示GLME能大幅纠正LME偏差，标准误差略有增加；对美国洪水损失数据和泰国最大降雨量数据的应用证明了该方法的有效性。

Conclusion: 提出的GLME方法有效，可用于更好估计GEV分布参数，有望推动基于L - 矩的惩罚或贝叶斯推断的进一步研究。

Abstract: Precisely estimating out-of-sample upper quantiles is very important in risk assessment and in engineering practice for structural design to prevent a greater disaster. For this purpose, the generalized extreme value (GEV) distribution has been broadly used. To estimate the parameters of GEV distribution, the maximum likelihood estimation (MLE) and L-moment estimation (LME) methods have been primarily employed. For a better estimation using the MLE, several studies considered the generalized MLE (penalized likelihood or Bayesian) methods to cooperate with a penalty function or prior information for parameters. However, a generalized LME method for the same purpose has not been developed yet in the literature. We thus propose the generalized method of L-moment estimation (GLME) to cooperate with a penalty function or prior information. The proposed estimation is based on the generalized L-moment distance and a multivariate normal likelihood approximation. Because the L-moment estimator is more efficient and robust for small samples than the MLE, we reasonably expect the advantages of LME to continue to hold for GLME. The proposed method is applied to the stationary and nonstationary GEV models with two novel (data-adaptive) penalty functions to correct the bias of LME. A simulation study indicates that the biases of LME are considerably corrected by the GLME with slight increases in the standard error. Applications to US flood damage data and maximum rainfall at Phliu Agromet in Thailand illustrate the usefulness of the proposed method. This study may promote further work on penalized or Bayesian inferences based on L-moments.

</details>


### [117] [Bayesian Empirical Bayes: Simultaneous Inference from Probabilistic Symmetries](https://arxiv.org/abs/2512.16239)
*Bohan Wu,Eli N. Weinstein,David M. Blei*

Main category: stat.ME

TL;DR: 提出基于概率对称概念的广义经验贝叶斯方法BEB，拓展EB应用，开发算法，模拟和真实数据表现良好。


<details>
  <summary>Details</summary>
Motivation: 现代应用有复杂结构，需将经验贝叶斯思想应用于这些场景。

Method: 将同时推理问题与潜在变量联合分布的对称假设配对，利用遍历分解推导对应经验贝叶斯方法BEB，开发基于变分推理和神经网络的可扩展算法。

Result: BEB能恢复经典经验贝叶斯方法，可拓展到其他概率对称情况，模拟中在去噪阵列和空间数据上优于现有方法，在真实数据中对癌症基因表达矩阵去噪和分析纽约市空间空气质量数据有效。

Conclusion: 基于概率对称的BEB方法有效拓展了经验贝叶斯的应用，且在模拟和真实数据中表现良好。

Abstract: Empirical Bayes (EB) improves the accuracy of simultaneous inference "by learning from the experience of others" (Efron, 2012). Classical EB theory focuses on latent variables that are iid draws from a fitted prior (Efron, 2019). Modern applications, however, feature complex structure, like arrays, spatial processes, or covariates. How can we apply EB ideas to these settings? We propose a generalized approach to empirical Bayes based on the notion of probabilistic symmetry. Our method pairs a simultaneous inference problem-with an unknown prior-to a symmetry assumption on the joint distribution of the latent variables. Each symmetry implies an ergodic decomposition, which we use to derive a corresponding empirical Bayes method. We call this methodBayesian empirical Bayes (BEB). We show how BEB recovers the classical methods of empirical Bayes, which implicitly assume exchangeability. We then use it to extend EB to other probabilistic symmetries: (i) EB matrix recovery for arrays and graphs; (ii) covariate-assisted EB for conditional data; (iii) EB spatial regression under shift invariance. We develop scalable algorithms based on variational inference and neural networks. In simulations, BEB outperforms existing approaches to denoising arrays and spatial data. On real data, we demonstrate BEB by denoising a cancer gene-expression matrix and analyzing spatial air-quality data from New York City.

</details>


### [118] [Causal Inference with the "Napkin Graph"](https://arxiv.org/abs/2512.19861)
*Anna Guo,David Benkeser,Razieh Nabi*

Main category: stat.ME

TL;DR: 研究‘餐巾图’因果结构，提出新估计量，利用Verma约束提高效率，经模拟验证并应用于芬兰研究，有R包实现。


<details>
  <summary>Details</summary>
Motivation: 未测量的混杂因素会使基于调整函数的识别策略失效，需研究新的识别策略。

Method: 研究‘餐巾图’，开发新的功能估计量，包括双稳健单步和目标最小损失估计量，利用Verma约束。

Result: 提出的方法通过模拟验证，应用于芬兰生命历程研究估计教育程度对收入的影响。

Conclusion: 所提方法有效，R包napkincausal可实现所有程序。

Abstract: Unmeasured confounding can render identification strategies based on adjustment functionals invalid. We study the "Napkin graph", a causal structure that encapsulates patterns of M-bias, instrumental variables, and the classical back-door and front-door models within a single graphical framework, yet requires a nonstandard identification strategy: the average treatment effect is expressed as a ratio of two g-formulas. We develop novel estimators for this functional, including doubly robust one-step and targeted minimum loss-based estimators that remain asymptotically linear when nuisance functions are estimated at slower-than-parametric rates using machine learning. We also show how a generalized independence restriction encoded by the Napkin graph, known as a Verma constraint, can be exploited to improve efficiency, illustrating more generally how such constraints in hidden variable DAGs can inform semiparametric inference. The proposed methods are validated through simulations and applied to the Finnish Life Course study to estimate the effect of educational attainment on income. An accompanying R package, napkincausal, implements all proposed procedures.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [119] [Milton Friedman's spending matrix revisited: 'Spending efficiency' and 'preference compatibility' across different economic systems](https://arxiv.org/abs/2512.19984)
*Ali Zeytoon-Nejad*

Main category: econ.GN

TL;DR: 文章扩展Friedman支出矩阵，对比不同经济系统下支出效率和偏好兼容性，指出政府干预应受限，为政策制定者提供指导。


<details>
  <summary>Details</summary>
Motivation: 分析不同经济系统下的'支出效率'和'偏好兼容性'，为政策制定者设计经济系统和政策提供指导。

Method: 扩展Milton Friedman的支出矩阵，对比不同经济系统从自由放任资本主义到共产主义转变时的效率和自由情况。

Result: 随着经济系统从自由放任资本主义向共产主义转变，关键成果逐渐变差。

Conclusion: 政府干预有时必要，但应仔细限制，以免导致效率低下和与个人偏好不符。

Abstract: This article expands Milton Friedman's spending matrix to analyse 'spending efficiency' and 'preference compatibility' across different economic systems against five key outcome criteria. By generalising Friedman's typology, it compares efficiency and freedom as systems shift from laissez-faire capitalism to communism, illustrating a gradual deterioration in their key outcomes. While government intervention is sometimes necessary to address market failures, its role should always be carefully limited to avoid inefficiency and misalignment with individual preferences. The insights may provide guidance for policymakers in designing economic systems and policies that promote both economic prosperity and personal liberty.

</details>


### [120] [The Quantitative Comparative Economics: indices of similarity to economic systems](https://arxiv.org/abs/2512.19985)
*Ali Zeytoon-Nejad*

Main category: econ.GN

TL;DR: 提出用于比较经济研究的定量方法，引入基于距离的指标并验证其在分类国家经济系统中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有比较经济学分类方法有局限性，易导致主观判断和信息损失。

Method: 提出基于距离的指标，如CapSI、ComSI和SocSI，以客观分类经济系统，基于数学领域知识构建。

Result: 对135个国家分类并制作GIS地图，显示所引入指标具有高解释力。

Conclusion: 建议采用新指标，因其客观且能捕捉经济系统结构和制度细微差别。

Abstract: This paper presents a novel quantitative approach for comparative economic studies, addressing limitations in current classification methods. Conventional approaches in comparative economics often rely on ad hoc and categorical classifications, leading to subjective judgments and disregarding the continuous nature of the spectrum of economic systems. These can result in subjectivity and significant information loss, particularly for countries with systems near categorical borders. To overcome these shortcomings, the present paper proposes distance-based indices for objective categorization, considering economic foundations and using hard data. Accordingly, the paper introduces institutional similarity indices--Capitalism Similarity Index (CapSI), Communism Similarity Index (ComSI), and Socialism Similarity Index (SocSI)-which reflect countries' positions along the economic system continuum. These indices adhere to mathematical rigor and are grounded in the mathematical fields of real analysis, metric spaces, and distance functions. By classifying 135 countries and creating GIS maps, the practical applicability of the proposed approach is demonstrated. Results show a high explanatory power of the introduced indices, suggesting their beneficial usage in comparative economic studies. The paper advocates for their adoption due to their objectivity and ability to capture structural and institutional nuances without subjective judgments while also considering the continuous nature of the spectrum of economic systems.

</details>


### [121] [Allocating Students to Schools: Theory, Methods, and Empirical Insights](https://arxiv.org/abs/2512.20353)
*Yeon-Koo Che,Julien Grenet,Yinghua He*

Main category: econ.GN

TL;DR: 文章从理论和实证两方面，考察匹配理论在学校选择中的应用，涉及设计挑战、估算方法及相关见解。


<details>
  <summary>Details</summary>
Motivation: 因学校分配从邻里分配制转向基于选择的模式，且教育选择无价格调节，故需关注分配机制设计。

Method: 先回顾理论贡献，探讨效率、稳定性和防策略性之间的权衡，以及设计挑战；再聚焦实证，研究从申请数据推断学生偏好的方法。

Result: 回顾了各种估算方法，得出关于家长偏好、市场设计权衡和学校选择政策有效性的关键见解。

Conclusion: 匹配理论在学校选择应用中有诸多设计挑战和实证研究方向，需综合考虑多方面因素。

Abstract: This chapter surveys the application of matching theory to school choice, motivated by the shift from neighborhood assignment systems to choice-based models. Since educational choice is not mediated by price, the design of allocation mechanisms is critical. The chapter first reviews theoretical contributions, exploring the fundamental trade-offs between efficiency, stability, and strategy-proofness, and covers design challenges such as tie-breaking, cardinal welfare, and affirmative action. It then transitions to the empirical landscape, focusing on the central challenge of inferring student preferences from application data, especially under strategic mechanisms. We review various estimation approaches and discuss key insights on parental preferences, market design trade-offs, and the effectiveness of school choice policies?

</details>


### [122] [Structured Event Representation and Stock Return Predictability](https://arxiv.org/abs/2512.19484)
*Gang Li,Dandan Qiao,Mingxuan Zheng*

Main category: econ.GN

TL;DR: 研究发现大语言模型提取的事件特征对基于文本的股票收益预测有效，提出基于结构化事件表示和注意力机制的模型，性能优且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 探索利用大语言模型提取的事件特征进行股票收益预测。

Method: 使用预训练大语言模型从新闻文章中提取事件特征，提出基于结构化事件表示和注意力机制的深度学习模型。

Result: 基于SER的模型在样本外预测股票收益方面优于其他文本驱动模型，且具有高度可解释的特征结构。

Conclusion: 强调结构化模型输入对股票收益预测的重要性，并给出相关启示。

Abstract: We find that event features extracted by large language models (LLMs) are effective for text-based stock return prediction. Using a pre-trained LLM to extract event features from news articles, we propose a novel deep learning model based on structured event representation (SER) and attention mechanisms to predict stock returns in the cross-section. Our SER-based model provides superior performance compared with other existing text-driven models to forecast stock returns out of sample and offers highly interpretable feature structures to examine the mechanisms underlying the stock return predictability. We further provide various implications based on SER and highlight the crucial benefit of structured model inputs in stock return predictability.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [123] [ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval](https://arxiv.org/abs/2512.19703)
*Siyuan Fu,Xuchen Guo,Mingjun Liu,Hongxiang Li,Boyin Tan,Gongxi Zhu,Xianwei Zhuang,Jinghan Ru,Yuxin Xie,Yuguo Yin*

Main category: eess.AS

TL;DR: 现有音频 - 文本检索范式有梯度局部瓶颈和表征漂移不匹配问题，提出ASK框架解决，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有音频 - 文本检索依赖的小批量对比学习存在梯度局部瓶颈，外部知识增强方法有表征漂移不匹配问题。

Method: 提出模型无关、即插即用的ASK框架，通过多粒度知识注入打破GLB，动态知识细化缓解RDM，引入自适应可靠性加权方案。

Result: 在两个基准数据集上取得优于现有技术的性能。

Conclusion: 提出的ASK框架有效。

Abstract: The dominant paradigm for Audio-Text Retrieval (ATR) relies on mini-batch-based contrastive learning. This process, however, is inherently limited by what we formalize as the Gradient Locality Bottleneck (GLB), which structurally prevents models from leveraging out-of-batch knowledge and thus impairs fine-grained and long-tail learning. While external knowledge-enhanced methods can alleviate the GLB, we identify a critical, unaddressed side effect: the Representation-Drift Mismatch (RDM), where a static knowledge base becomes progressively misaligned with the evolving model, turning guidance into noise. To address this dual challenge, we propose the Adaptive Self-improving Knowledge (ASK) framework, a model-agnostic, plug-and-play solution. ASK breaks the GLB via multi-grained knowledge injection, systematically mitigates RDM through dynamic knowledge refinement, and introduces a novel adaptive reliability weighting scheme to ensure consistent knowledge contributes to optimization. Experimental results on two benchmark datasets with superior, state-of-the-art performance justify the efficacy of our proposed ASK framework.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [124] [Modeling Economic Systems as Multiport Networks](https://arxiv.org/abs/2512.20600)
*Coen Hutters,Max B. Mendel*

Main category: eess.SY

TL;DR: 本文展示多端口网络理论可作为经济学强大建模工具，构建多层次抽象模型并分析不同复杂度经济系统。


<details>
  <summary>Details</summary>
Motivation: 探索多端口网络理论在经济学建模中的应用，以更好地模拟复杂宏观经济系统。

Method: 运用端口概念将商品流动与经济互动中主体激励配对，构建多层次抽象网络模型，使用LTSpice电路模拟器分析系统。

Result: 设计并分析了从鲁滨逊经济到整个经济模型等不同复杂度的示例系统。

Conclusion: 多端口网络理论是经济学中建模复杂宏观经济系统的有效工具。

Abstract: In this paper, we demonstrate how multiport network theory can be used as a powerful modeling tool in economics. The critical insight is using the port concept to pair the flow of goods (the electrical current) with the agent's incentive (the voltage) in an economic interaction. By building networks of agents interacting through ports, we create models with multiple levels of abstraction, from the macro level down to the micro level. We are thereby able to model complex macroeconomic systems whose dynamical behavior is emergent from the micro level. Using the LTSpice circuit simulator, we then design and analyze a series of example systems that range in complexity from the textbook Robinson Crusoe economy to a model of an entire economy.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [125] [Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study](https://arxiv.org/abs/2512.19855)
*Andrew Stirling,Mykola Lukashchuk,Dmitry Bagaev,Wouter Kouw,James R. Forbes*

Main category: cs.RO

TL;DR: 本文将ESGVI算法在两个方向扩展，用于状态估计，在UWB定位实验验证效果并开源代码


<details>
  <summary>Details</summary>
Motivation: 将ESGVI算法扩展以处理含方向分量状态估计及适应重尾和偏态噪声分布

Method: 将ESGVI推广到矩阵李群，引入因子适应重尾和偏态噪声分布

Result: 在富非视距测量的UWB定位实验中提高了精度，具有相当的一致性

Conclusion: 扩展能自然融入ESGVI框架，保持其稀疏无导数结构，开源代码支持研究。

Abstract: This letter extends the exactly sparse Gaussian variational inference (ESGVI) algorithm for state estimation in two complementary directions. First, ESGVI is generalized to operate on matrix Lie groups, enabling the estimation of states with orientation components while respecting the underlying group structure. Second, factors are introduced to accommodate heavy-tailed and skewed noise distributions, as commonly encountered in ultra-wideband (UWB) localization due to non-line-of-sight (NLOS) and multipath effects. Both extensions are shown to integrate naturally within the ESGVI framework while preserving its sparse and derivative-free structure. The proposed approach is validated in a UWB localization experiment with NLOS-rich measurements, demonstrating improved accuracy and comparable consistency. Finally, a Python implementation within a factor-graph-based estimation framework is made open-source (https://github.com/decargroup/gvi_ws) to support broader research use.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [126] [Certified Lower Bounds and Efficient Estimation of Minimum Accuracy in Quantum Kernel Methods](https://arxiv.org/abs/2512.20588)
*Demerson N. Gonçalves,Tharso D. Fernandes,Andrias M. M. Cordeiro,Pedro H. G. Lugao,João T. Dias*

Main category: quant-ph

TL;DR: 推广最小准确率启发式指标用于任意二元数据集，引入蒙特卡罗策略有效估计界限，使最小准确率成为近-term量子设备预筛选特征图的有效工具。


<details>
  <summary>Details</summary>
Motivation: 原最小准确率启发式公式计算昂贵、限于平衡数据集且缺乏理论支持，需改进推广。

Method: 将指标推广到任意二元数据集并证明其为最优经验准确率的认证下界；引入蒙特卡罗策略，用随机子集的Pauli方向估计界限并给出概率保证。

Result: 使最小准确率成为可扩展、理论可靠的工具用于近-term量子设备预筛选特征图。

Conclusion: 最小准确率可作为近-term量子设备上预筛选特征图的工具。

Abstract: The minimum accuracy heuristic evaluates quantum feature maps without requiring full quantum support vector machine (QSVM) training. However, the original formulation is computationally expensive, restricted to balanced datasets, and lacks theoretical backing. This work generalizes the metric to arbitrary binary datasets and formally proves it constitutes a certified lower bound on the optimal empirical accuracy of any linear classifier in the same feature space. Furthermore, we introduce Monte Carlo strategies to efficiently estimate this bound using a random subset of Pauli directions, accompanied by rigorous probabilistic guarantees. These contributions establish minimum accuracy as a scalable, theoretically sound tool for pre-screening feature maps on near-term quantum devices.

</details>


### [127] [Towards a point-to-point CV-QKD system: Implementation challenges and perspectives](https://arxiv.org/abs/2512.19834)
*Davi Juvêncio Gomes de Sousa,Nelson Alves Ferreira Neto,Christiano M. S. Nascimento,Lucas Q. Galvão,Mauro Queiroz Nooblath Neto,Micael Andrade Dias,Cássio de Castro Silva,Braian Pinheiro da Silva,Alexandre B. Tacla,Valéria Loureiro da Silva*

Main category: quant-ph

TL;DR: 本文分析光纤点对点连续变量量子密钥分发（CV - QKD）系统的挑战与实现前景，涉及物理层、数字信号处理、后处理流程、硬件架构，并给出巴西部署展望。


<details>
  <summary>Details</summary>
Motivation: 为巴西建立首个点对点CV - QKD系统，提供可扩展和互操作的量子通信网络路线图。

Method: 研究物理层设计，分析数字信号处理作用，详细介绍后处理流程，讨论模块化数字架构并使用参考软件框架验证。

Result: 明确了CV - QKD系统各方面的设计和实现方法，给出巴西部署的初步方案。

Conclusion: 为巴西CV - QKD系统奠定基础，提供量子通信网络发展路线。

Abstract: This article presents an analysis of the practical challenges and implementation perspectives of point-to-point continuous-variable quantum key distribution (CV-QKD) systems over optical fiber. The study addresses the physical layer, including the design of transmitters, quantum channels, and receivers, with emphasis on impairments such as attenuation, chromatic dispersion, polarization fluctuations, and coexistence with classical channels. We further examine the role of digital signal processing (DSP) as the bridge between quantum state transmission and classical post-processing, highlighting its impact on excess noise mitigation, covariance matrix estimation, and reconciliation efficiency. The post-processing pipeline is detailed with a focus on parameter estimation in the finite-size regime, information reconciliation using LDPC-based codes optimized for low-SNR conditions, and privacy amplification employing large-block universal hashing. From a hardware perspective, we discuss modular digital architectures that integrate dedicated accelerators with programmable processors, supported by a reference software framework (CV-QKD-ModSim) for algorithm validation and hardware co-design. Finally, we outline perspectives for the deployment of CV-QKD in Brazil, starting from metropolitan testbeds and extending toward hybrid fiber/FSO and space-based infrastructures. The work establishes the foundations for the first point-to-point CV-QKD system in Brazil, while providing a roadmap for scalable and interoperable quantum communication networks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [128] [Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning](https://arxiv.org/abs/2512.19777)
*Antonio Tarizzo,Mohammad Kazemi,Deniz Gündüz*

Main category: cs.IT

TL;DR: 本文提出一种学习型数字OTA框架，提升了恢复精度、收敛性和对低信噪比的鲁棒性，实验表明其在低信噪比下表现良好。


<details>
  <summary>Details</summary>
Motivation: 联邦边缘学习中重复的模型更新上行传输使通信成为瓶颈，现有数字OTA方案在低信噪比下有局限。

Method: 将无信源随机接入（URA）码本与矢量量化和AMP - DA - Net集成，端到端训练解码器，并将OTA聚合扩展到对称函数。

Result: 在高度异构设备数据集和不同数量活跃设备的实验中，该设计在低信噪比下可靠运行范围扩展超10dB，全信噪比范围性能匹配或提升。

Conclusion: 端到端学习设计在联邦边缘学习的数字OTA通信中有更广泛潜力。

Abstract: Federated edge learning (FEEL) enables wireless devices to collaboratively train a centralised model without sharing raw data, but repeated uplink transmission of model updates makes communication the dominant bottleneck. Over-the-air (OTA) aggregation alleviates this by exploiting the superposition property of the wireless channel, enabling simultaneous transmission and merging communication with computation. Digital OTA schemes extend this principle by incorporating the robustness of conventional digital communication, but current designs remain limited in low signal-to-noise ratio (SNR) regimes. This work proposes a learned digital OTA framework that improves recovery accuracy, convergence behaviour, and robustness to challenging SNR conditions while maintaining the same uplink overhead as state-of-the-art methods. The design integrates an unsourced random access (URA) codebook with vector quantisation and AMP-DA-Net, an unrolled approximate message passing (AMP)-style decoder trained end-to-end with the digital codebook and parameter server local training statistics. The proposed design extends OTA aggregation beyond averaging to a broad class of symmetric functions, including trimmed means and majority-based rules. Experiments on highly heterogeneous device datasets and varying numbers of active devices show that the proposed design extends reliable digital OTA operation by more than 10 dB into low SNR regimes while matching or improving performance across the full SNR range. The learned decoder remains effective under message corruption and nonlinear aggregation, highlighting the broader potential of end-to-end learned design for digital OTA communication in FEEL.

</details>


### [129] [Information-theoretic signatures of causality in Bayesian networks and hypergraphs](https://arxiv.org/abs/2512.20552)
*Sung En Chiang,Zhaolu Liu,Robert L. Peach,Mauricio Barahona*

Main category: cs.IT

TL;DR: 文章建立了Partial Information Decomposition (PID) 组件与贝叶斯网络和超图中因果结构的理论对应关系，为因果发现提供新思路。


<details>
  <summary>Details</summary>
Motivation: 现有高阶信息论测度与因果结构的数学联系未充分发展，需建立对应关系。

Method: 先在贝叶斯网络中分析PID组件与因果结构关系，后扩展到高阶系统，还给出表征因果结构的程序。

Result: 在贝叶斯网络中，唯一信息表征直接因果邻居，协同识别对撞关系；在贝叶斯超图中，PID特征可区分不同节点关系，揭示高阶对撞效应。

Conclusion: PID可作为推断因果结构的严格、与模型无关的基础，引入了局部信息论的因果发现观点。

Abstract: Analyzing causality in multivariate systems involves establishing how information is generated, distributed and combined, and thus requires tools that capture interactions beyond pairwise relations. Higher-order information theory provides such tools. In particular, Partial Information Decomposition (PID) allows the decomposition of the information that a set of sources provides about a target into redundant, unique, and synergistic components. Yet the mathematical connection between such higher-order information-theoretic measures and causal structure remains undeveloped. Here we establish the first theoretical correspondence between PID components and causal structure in both Bayesian networks and hypergraphs. We first show that in Bayesian networks unique information precisely characterizes direct causal neighbors, while synergy identifies collider relationships. This establishes a localist causal discovery paradigm in which the structure surrounding each variable can be recovered from its immediate informational footprint, eliminating the need for global search over graph space. Extending these results to higher-order systems, we prove that PID signatures in Bayesian hypergraphs differentiate parents, children, co-heads, and co-tails, revealing a higher-order collider effect unique to multi-tail hyperedges. We also present procedures by which our results can be used to characterize systematically the causal structure of Bayesian networks and hypergraphs. Our results position PID as a rigorous, model-agnostic foundation for inferring both pairwise and higher-order causal structure, and introduce a fundamentally local information-theoretic viewpoint on causal discovery.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [130] [How to choose my stochastic volatility parameters? A review](https://arxiv.org/abs/2512.19821)
*Fabien Le Floc'h*

Main category: q-fin.PR

TL;DR: 文章在金融衍生品合约定价背景下，介绍随机波动率模型参数选择的不同方式，包括随机局部波动率模型中随机波动率的使用。


<details>
  <summary>Details</summary>
Motivation: 在金融衍生品合约定价背景下，探讨随机波动率模型参数选择方式。

Method: 基于现有文献进行阐述。

Result: 呈现了随机波动率模型参数选择的不同方式。

Conclusion: 未提及明确结论。

Abstract: Based on the existing literature, this article presents the different ways of choosing the parameters of stochastic volatility models in general, in the context of pricing financial derivative contracts. This includes the use of stochastic volatility inside stochastic local volatility models.

</details>


### [131] [Pricing of wrapped Bitcoin and Ethereum on-chain options](https://arxiv.org/abs/2512.20190)
*Anastasiia Zbandut*

Main category: q-fin.PR

TL;DR: 本文测量Arbitrum上Hegic期权报价与基于特定模型的基准价格差异，发现基准价平均高于Hegic报价等结果，框架可用于监测和校准链上期权定价逻辑。


<details>
  <summary>Details</summary>
Motivation: 测量Arbitrum上Hegic期权报价与模型基准价格的差异，进行数据驱动的分析以监测和校准链上期权定价逻辑。

Method: 使用基于两制度MS - AR - (GJR) - GARCH模型估计制度敏感波动率的Black - Scholes模型构建基准，采用期权层面可行广义最小二乘法（GLS）。

Result: 基准价格平均超过Hegic报价，价差随订单规模、执行价、到期期限和估计波动率上升，随交易量下降；包装比特币期权价差更大更持久，以太坊期权更接近基准。

Conclusion: 该框架为监测和校准链上期权定价逻辑提供了数据驱动的分析方法。

Abstract: This paper measures price differences between Hegic option quotes on Arbitrum and a model-based benchmark built on Black--Scholes model with regime-sensitive volatility estimated via a two-regime MS-AR-(GJR)-GARCH model. Using option-level feasible GLS, we find benchmark prices exceed Hegic quotes on average, especially for call options. The price spread rises with order size, strike, maturity, and estimated volatility, and falls with trading volume. By underlying, wrapped Bitcoin options show larger and more persistent spreads, while Ethereum options are closer to the benchmark. The framework offers a data-driven analysis for monitoring and calibrating on-chain option pricing logic.

</details>
