<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 113]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 21]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.CO](#stat.CO) [Total: 1]
- [math.ST](#math.ST) [Total: 4]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.AR](#cs.AR) [Total: 13]
- [math.OC](#math.OC) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [econ.GN](#econ.GN) [Total: 5]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CL](#cs.CL) [Total: 33]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.RO](#cs.RO) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CV](#cs.CV) [Total: 19]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [NeuroAI and Beyond](https://arxiv.org/abs/2601.19955)
*Jean-Marc Fellous,Gert Cauwenberghs,Cornelia Fermüller,Yulia Sandamisrkaya,Terrence Sejnowski*

Main category: cs.AI

TL;DR: 文章基于2025年8月的研讨会，识别神经科学与人工智能的协同领域，倡导发展NeuroAI，并给出相关观点和SWOT分析。


<details>
  <summary>Details</summary>
Motivation: 神经科学和人工智能虽有进展但联系松散，需明确两者协同领域。

Method: 聚焦具身、语言通信等子领域总结进展和未来方向，纳入研究者个人观点，进行SWOT分析。

Result: 确定神经科学与人工智能的协同领域，提出NeuroAI概念。

Conclusion: 倡导发展NeuroAI，其有望提升AI算法范围和效率，改变对生物神经计算的理解。

Abstract: Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.

</details>


### [2] [Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning](https://arxiv.org/abs/2601.20014)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出Self - Querying Bidirectional Categorical Planning (SQ - BCP)方法解决大语言模型推理规划问题，在WikiHow和RecipeNLG任务中降低资源违规率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在部分可观测情况下推理规划常出错，如产生幻觉事实或违反约束的计划。

Method: 引入SQ - BCP，明确表示前提条件状态，通过自查询或假设解决未知问题，进行双向搜索并使用验证器。

Result: 在WikiHow和RecipeNLG任务中，SQ - BCP将资源违规率分别降至14.9%和5.8%，优于基线。

Conclusion: 当验证器成功且硬约束通过检查时，接受的计划与目标要求兼容，在一定条件下SQ - BCP能找到可行计划。

Abstract: Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\texttt{Sat}/\texttt{Viol}/\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \textbf{14.9\%} and \textbf{5.8\%} (vs.\ \textbf{26.0\%} and \textbf{15.7\%} for the best baseline), while maintaining competitive reference quality.

</details>


### [3] [Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints](https://arxiv.org/abs/2601.20021)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 提出模糊范畴论规划 (FCP) 方法，在自然语言规划中考虑模糊谓词，通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有范畴论规划器将适用性处理为二元，会丢失有意义的区分且无法追踪多步规划中的质量下降，需要解决模糊谓词的规划问题。

Method: 提出 FCP，为每个动作标注 [0,1] 程度，用 t - 范数 Lukasiewicz 组合规划质量，通过拉回验证保留清晰的可执行性检查，用 LLM 结合 k - 样本中位数聚合确定分级适用性，支持基于剩余的反向需求的中间相遇搜索。

Result: 在公共 PDDL3 基准和自建的 RecipeNLG - Subs 基准上评估，相比仅使用 LLM 和 ReAct 风格的基线，FCP 在 RecipeNLG - Subs 上提高了成功率并减少了硬约束违规，与经典 PDDL3 规划器有竞争力。

Conclusion: FCP 是一种有效的自然语言规划方法，能处理模糊谓词，在规划任务中有较好表现。

Abstract: Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.

</details>


### [4] [Insight Agents: An LLM-Based Multi-Agent System for Data Insights](https://arxiv.org/abs/2601.20048)
*Jincheng Bai,Zhenyu Zhang,Jennifer Zhang,Zhihuai Zhu*

Main category: cs.AI

TL;DR: 本文提出开发Insight Agents（IA）对话式多智能体数据洞察系统，介绍其结构与方法，该系统在美国亚马逊卖家上线，准确率达90%，P90延迟低于15秒。


<details>
  <summary>Details</summary>
Motivation: 电商卖家面临发现和利用程序工具及处理数据的挑战，开发IA系统为卖家提供个性化数据和业务洞察，减少决策精力并加快决策速度。

Method: 构建基于计划 - 执行范式的端到端智能体系统，采用分层多智能体结构；为管理智能体设计结合OOD检测和基于BERT分类器的路由的ML解决方案；为两个工作智能体设计API数据模型的战略规划并动态注入领域知识。

Result: IA系统在美国亚马逊卖家上线，经人工评估准确率达90%，P90延迟低于15秒。

Conclusion: IA系统能有效为电商卖家提供数据和业务洞察，减少决策精力并加快决策速度，具有高准确率和低延迟的特点。

Abstract: Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.

</details>


### [5] [Should I Have Expressed a Different Intent? Counterfactual Generation for LLM-Based Autonomous Control](https://arxiv.org/abs/2601.20090)
*Amirmohammad Farzaneh,Salvatore D'Oro,Osvaldo Simeone*

Main category: cs.AI

TL;DR: 提出框架实现基于大语言模型的智能体控制场景中的反事实推理并提供可靠性保证，在无线网络控制用例中展示优势。


<details>
  <summary>Details</summary>
Motivation: 用户观察结果后会想若以不同方式表达意图会怎样，现有方法缺乏相应反事实推理能力及可靠性保证。

Method: 将用户、基于大语言模型的智能体和环境的闭环交互建模为结构因果模型，利用测试时缩放通过概率溯因生成多个候选反事实结果，通过离线校准阶段生成反事实结果集。

Result: 在无线网络控制用例中，所提的共形反事实生成（CCG）方法比简单重执行基线有显著优势。

Conclusion: 提出的框架能实现反事实推理并提供可靠性保证，在实际用例中表现良好。

Abstract: Large language model (LLM)-powered agents can translate high-level user intents into plans and actions in an environment. Yet after observing an outcome, users may wonder: What if I had phrased my intent differently? We introduce a framework that enables such counterfactual reasoning in agentic LLM-driven control scenarios, while providing formal reliability guarantees. Our approach models the closed-loop interaction between a user, an LLM-based agent, and an environment as a structural causal model (SCM), and leverages test-time scaling to generate multiple candidate counterfactual outcomes via probabilistic abduction. Through an offline calibration phase, the proposed conformal counterfactual generation (CCG) yields sets of counterfactual outcomes that are guaranteed to contain the true counterfactual outcome with high probability. We showcase the performance of CCG on a wireless network control use case, demonstrating significant advantages compared to naive re-execution baselines.

</details>


### [6] [Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis](https://arxiv.org/abs/2601.20206)
*Zixuan Xiao,Chunguang Hu,Jun Ma*

Main category: cs.AI

TL;DR: 传统遥感影像变化检测方法难以满足城市公园开发监测需求，本文提出多模态大语言模型（LLM）代理框架应对挑战。


<details>
  <summary>Details</summary>
Motivation: 传统遥感变化检测方法在高级、智能分析及复杂多模态数据分析上有局限，无法满足城市公园开发监测需求。

Method: 提出多模态LLM代理框架，设计通用数据对齐机制，构建特定工具包缓解LLM因缺乏领域知识产生的幻觉问题。

Result: 与普通GPT - 4o等代理相比，该方法能实现稳健的多模态信息融合与分析。

Conclusion: 该框架能为城市公园开发监测的多样和不断变化的需求提供可靠且可扩展的解决方案。

Abstract: As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.

</details>


### [7] [Normative Equivalence in human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups](https://arxiv.org/abs/2601.20487)
*Nico Mutzner,Taha Yasseri,Heiko Rauhut*

Main category: cs.AI

TL;DR: 研究通过在线实验探讨AI代理对小组合作规范的影响，发现合作依赖群体行为，合作规范可延伸至AI。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注人机二元互动，本研究旨在填补其对AI代理影响小组合作规范形成和维持研究的空白。

Method: 进行在线实验，采用重复四人公共物品游戏，每组三人与一个被设定为人或AI的机器人，机器人有三种决策策略。

Result: 互惠群体动态和行为惯性驱动合作，人机标签下合作水平无显著差异，规范持久性和参与者规范认知也无差异。

Conclusion: 合作规范具有灵活性，可延伸至人工代理，模糊了集体决策中人与AI的界限。

Abstract: The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.

</details>


### [8] [Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2601.20221)
*Hang Zhang,Ruheng Wang,Yuelyu Ji,Mingu Kwak,Xizhi Wu,Chenyu Li,Li Zhang,Wenqi Shi,Yifan Peng,Yanshan Wang*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\method$ demonstrates an $\mathbf{8\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.

</details>


### [9] [Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models](https://arxiv.org/abs/2601.20305)
*Zhenchen Tang,Songlin Yang,Zichuan Wang,Bo Peng,Yang Li,Beibei Dong,Jing Dong*

Main category: cs.AI

TL;DR: 论文指出统一多模态模型存在认知差距，提出内源性重提示机制及SEER训练框架，实验表明SEER表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型理解能力强但无法有效指导生成，存在认知差距。

Method: 提出内源性重提示机制，引入SEER训练框架，分两阶段：用RLVR激活模型潜在评估能力，用RLMT优化生成推理策略。

Result: SEER在评估准确性、重提示效率和生成质量上优于现有基线，且不牺牲通用多模态能力。

Conclusion: SEER能有效弥合统一多模态模型的认知差距，提升模型性能。

Abstract: Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model's understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model's latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities.

</details>


### [10] [ECG-Agent: On-Device Tool-Calling Agent for ECG Multi-Turn Dialogue](https://arxiv.org/abs/2601.20323)
*Hyunseung Chung,Jungwoo Oh,Daeun Kyung,Jiho Kim,Yeonsu Kwon,Min-Gyu Kim,Edward Choi*

Main category: cs.AI

TL;DR: 本文介绍ECG - Agent用于多轮心电图对话，还推出ECG - MTD数据集，实验表明其性能优于基线模型，设备端代理也有不错表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在心电图应用中缺乏多轮对话能力、设备端效率和对心电图测量的精确理解。

Method: 引入基于大语言模型的工具调用代理ECG - Agent，创建ECG - MTD数据集，开发不同规模的ECG - Agents。

Result: ECG - Agents在响应准确性上优于基线ECG - LLMs，设备端代理在各项评估中表现与大型代理相当。

Conclusion: ECG - Agents具备在现实世界应用的可行性。

Abstract: Recent advances in Multimodal Large Language Models have rapidly expanded to electrocardiograms, focusing on classification, report generation, and single-turn QA tasks. However, these models fall short in real-world scenarios, lacking multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements such as the PQRST intervals. To address these limitations, we introduce ECG-Agent, the first LLM-based tool-calling agent for multi-turn ECG dialogue. To facilitate its development and evaluation, we also present ECG-Multi-Turn-Dialogue (ECG-MTD) dataset, a collection of realistic user-assistant multi-turn dialogues for diverse ECG lead configurations. We develop ECG-Agents in various sizes, from on-device capable to larger agents. Experimental results show that ECG-Agents outperform baseline ECG-LLMs in response accuracy. Furthermore, on-device agents achieve comparable performance to larger agents in various evaluations that assess response accuracy, tool-calling ability, and hallucinations, demonstrating their viability for real-world applications.

</details>


### [11] [AMA: Adaptive Memory via Multi-Agent Collaboration](https://arxiv.org/abs/2601.20352)
*Weiquan Huang,Zixuan Wang,Hehai Lin,Sudong Wang,Bo Xu,Qian Li,Beier Zhu,Linyi Yang,Chengwei Qin*

Main category: cs.AI

TL;DR: 为解决大语言模型（LLM）智能体记忆系统问题，提出多智能体协作的自适应记忆框架AMA，实验显示其优于现有方法并减少令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体记忆系统存在检索粒度僵化、维护策略重积累、更新机制粗粒度等问题，导致存储信息与任务推理需求不匹配及逻辑不一致积累。

Method: 提出AMA框架，采用分层内存设计，通过Constructor和Retriever实现多粒度内存构建与自适应查询路由，Judge验证内容相关性和一致性，Refresher维护内存一致性。

Result: 在具有挑战性的长上下文基准测试中，AMA显著优于现有基线方法，与全上下文方法相比，减少约80%的令牌消耗。

Conclusion: AMA框架在维持检索精度和长期内存一致性方面有效。

Abstract: The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.

</details>


### [12] [Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution](https://arxiv.org/abs/2601.20379)
*Zhengbo Jiao,Hongyu Xian,Qinglong Wang,Yunpu Ma,Zhebo Wang,Zifan Zhang,Dezhang Kong,Meng Han*

Main category: cs.AI

TL;DR: 该论文提出Policy of Thoughts (PoT) 框架，将推理视为在线优化过程，通过实验证明能显著提升大语言模型复杂长时推理性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因固定策略假设导致在复杂长时推理中表现不佳，当前测试时缩放方法未内化执行反馈来改进推理策略。

Method: 引入PoT框架，先通过高效探索机制生成多样候选解，再用Group Relative Policy Optimization (GRPO) 根据执行反馈更新临时LoRA适配器。

Result: 实验显示PoT显著提升性能，一个4B模型在LiveCodeBench上准确率达49.71%，优于GPT - 4o和DeepSeek - V3。

Conclusion: PoT框架通过将推理作为实例内在线优化过程，能实现动态、特定实例的推理先验细化，显著提升大语言模型复杂推理能力。

Abstract: Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of "conjectures and refutations," we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.

</details>


### [13] [OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution](https://arxiv.org/abs/2601.20380)
*Le Zhang,Yixiong Xiao,Xinjiang Lu,Jingjia Cao,Yusai Zhao,Jingbo Zhou,Lang An,Zikan Feng,Wanxiang Sha,Yu Shi,Congxi Xiao,Jian Xiong,Yankai Zhang,Hua Wu,Haifeng Wang*

Main category: cs.AI

TL;DR: 介绍通用GUI代理模型OmegaUse，含数据构建和训练方法，在多基准测试表现佳。


<details>
  <summary>Details</summary>
Motivation: 发挥GUI代理潜力，实现跨移动和桌面平台的自主任务执行，革新人机交互、提升生产力。

Method: 引入数据构建管道和分离训练范式，采用两阶段训练策略，基于MoE骨干构建模型，引入OS - Nav基准套件。

Result: OmegaUse在多个GUI基准测试有竞争力，如ScreenSpot - V2达96.3%，AndroidControl达79.1%等。

Conclusion: OmegaUse是一个有效的通用GUI代理模型，能在多平台执行任务且表现出色。

Abstract: Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.

</details>


### [14] [CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning](https://arxiv.org/abs/2601.20467)
*Zhenxuan Fan,Jie Cao,Yang Dai,Zheqi Lv,Wenqiao Zhang,Zhongle Xie,Peng LU,Beng Chin Ooi*

Main category: cs.AI

TL;DR: 提出CtrlCoT框架压缩思维链，减少token使用并提升推理效果。


<details>
  <summary>Details</summary>
Motivation: 思维链提示推理存在高延迟和高内存成本问题，现有压缩方法有局限性。

Method: 提出CtrlCoT框架，包含层次推理抽象、逻辑保留蒸馏和分布对齐生成三个组件。

Result: 在MATH - 500和Qwen2.5 - 7B - Instruct上，CtrlCoT减少30.7%的token使用，准确率比最强基线高7.6个百分点。

Conclusion: CtrlCoT实现了更高效可靠的推理，代码将开源。

Abstract: Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.

</details>


### [15] [PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs](https://arxiv.org/abs/2601.20539)
*Oguzhan Gungordu,Siheng Xiong,Faramarz Fekri*

Main category: cs.AI

TL;DR: 提出PathWise多智能体推理框架用于组合优化问题的自动启发式设计，实验表明其效果好。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动启发式设计框架依赖固定进化规则和静态提示模板，导致启发式生成短视、评估冗余等问题。

Method: 提出PathWise框架，将启发式生成视为基于蕴含图的顺序决策过程，通过策略智能体、世界模型智能体和批评智能体实现从试错进化向基于推理的状态感知规划转变。

Result: 在不同组合优化问题上的实验显示，PathWise能更快收敛到更好的启发式，可跨不同大语言模型骨干泛化，且能处理更大问题规模。

Conclusion: PathWise框架有效解决了现有自动启发式设计框架的问题，具有良好的性能和扩展性。

Abstract: Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.

</details>


### [16] [Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function](https://arxiv.org/abs/2601.20554)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: 研究使用ICVaR进行部分可观测下的风险敏感规划，扩展在线规划算法优化ICVaR，实验表明其降低尾部风险。


<details>
  <summary>Details</summary>
Motivation: 进行部分可观测下的风险敏感规划。

Method: 开发ICVaR策略评估算法，扩展三种在线规划算法优化ICVaR值函数，引入风险参数α；为ICVaR稀疏采样建立有限时间性能保证并设计探索策略。

Result: 在基准POMDP领域实验表明，提出的ICVaR规划器比风险中性对应方案有更低的尾部风险。

Conclusion: 所提出的基于ICVaR的规划器在部分可观测风险敏感规划中有效，能降低尾部风险。

Abstract: We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $α$, where $α= 1$ recovers standard expectation-based planning and $α< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.

</details>


### [17] [Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies](https://arxiv.org/abs/2601.20604)
*Gray Cox*

Main category: cs.AI

TL;DR: 本文提出通过结构化多模型对话实证测试AI对齐策略的方法框架，实验表明AI能与和平研究概念互动并产生新见解，还指出研究局限和未来方向。


<details>
  <summary>Details</summary>
Motivation: 引入实证测试AI对齐策略的方法，将对齐问题从控制问题转变为关系问题。

Method: 借鉴和平研究传统，将四个不同角色分配给不同AI系统，在六种条件下进行实验，使用Claude、Gemini和GPT - 4o进行结构化对话。

Result: AI能与和平研究概念有意义互动，产生新见解，不同模型关注不同问题。

Conclusion: 框架为研究人员提供可复制方法，结果初步证明AI具备对话推理能力，讨论了研究局限并给出未来研究方向。

Abstract: This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.
  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.
  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of "VCW as transitional framework." Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.
  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.

</details>


### [18] [Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation](https://arxiv.org/abs/2601.20614)
*Yanqi Dai,Yuxiang Ji,Xiao Zhang,Yong Wang,Xiangxiang Chu,Zhiwu Lu*

Main category: cs.AI

TL;DR: 现有强化学习方法在解决复杂数学问题推理上存在不足，本文提出MathForge框架提升大模型数学推理能力，实验表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在算法和数据层面都缺乏对更具挑战性问题的关注，而这对提升模型未充分发展的能力很重要。

Method: 提出MathForge框架，包含Difficulty - Aware Group Policy Optimization (DGPO) 算法和Multi - Aspect Question Reformulation (MQR) 策略。DGPO平衡组优势估计并按难度加权问题，MQR从多方面重构问题增加难度。

Result: MathForge在各种数学推理任务上显著优于现有方法。

Conclusion: MathForge框架能有效提升大模型的数学推理能力，代码和增强数据可在指定链接获取。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.

</details>


### [19] [Investigating the Development of Task-Oriented Communication in Vision-Language Models](https://arxiv.org/abs/2601.20641)
*Boaz Carmeli,Orr Paradise,Shafi Goldwasser,Yonatan Belinkov,Ron Meir*

Main category: cs.AI

TL;DR: 研究大语言模型（LLM）的代理在协作推理任务中能否开发任务导向通信协议，发现其可开发有效模式与隐蔽协议，同时有自发协调。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在协作推理任务中能否开发不同于标准自然语言的任务导向通信协议，以及协议的效率和隐蔽性。

Method: 使用指称游戏框架，让视觉语言模型（VLM）代理进行通信来评估语言变体。

Result: VLMs能开发有效、适应任务的通信模式和隐蔽协议，相似模型间有自发协调。

Conclusion: 指出任务导向通信有潜力和风险，指称游戏是该领域未来研究的有价值测试平台。

Abstract: We investigate whether \emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.

</details>


### [20] [Enterprise Resource Planning Using Multi-type Transformers in Ferro-Titanium Industry](https://arxiv.org/abs/2601.20696)
*Samira Yazdanpourmoghadam,Mahan Balal Pour,Vahid Partovi Nia*

Main category: cs.AI

TL;DR: 利用Multi - Type Transformer (MTT)架构统一解决组合优化问题，实验显示其在JSP和KP基准问题上有竞争力，并首次将多类型变压器应用于实际制造。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题需复杂算法，深度学习中基于变压器的架构有潜力替代传统启发式和元启发式方法。

Method: 利用MTT架构在统一框架下解决JSP和KP基准问题，并进行广泛实验评估。

Result: MTT在不同规模的JSP和KP基准问题上取得有竞争力的性能。

Conclusion: 多类型注意力在实际制造（钛铁行业）中有应用潜力，且是首次将多类型变压器应用于实际制造。

Abstract: Combinatorial optimization problems such as the Job-Shop Scheduling Problem (JSP) and Knapsack Problem (KP) are fundamental challenges in operations research, logistics, and eterprise resource planning (ERP). These problems often require sophisticated algorithms to achieve near-optimal solutions within practical time constraints. Recent advances in deep learning have introduced transformer-based architectures as promising alternatives to traditional heuristics and metaheuristics. We leverage the Multi-Type Transformer (MTT) architecture to address these benchmarks in a unified framework. We present an extensive experimental evaluation across standard benchmark datasets for JSP and KP, demonstrating that MTT achieves competitive performance on different size of these benchmark problems. We showcase the potential of multi-type attention on a real application in Ferro-Titanium industry. To the best of our knowledge, we are the first to apply multi-type transformers in real manufacturing.

</details>


### [21] [Implementing Metric Temporal Answer Set Programming](https://arxiv.org/abs/2601.20735)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Susana Hahn,Javier Romero,Torsten Schaub*

Main category: cs.AI

TL;DR: 开发计算方法让Metric ASP表达定量时间约束，通过处理差分约束解决时间粒度可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有Metric ASP在处理细粒度时间约束时，存在可扩展性问题，会加剧ASP的grounding瓶颈。

Method: 利用ASP与差分约束的扩展，将时间相关方面外部处理。

Result: 将Metric ASP与时间粒度解耦，解决方案不受时间精度影响。

Conclusion: 所提方法有效解决了Metric ASP在细粒度时间约束下的可扩展性问题。

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.

</details>


### [22] [REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence](https://arxiv.org/abs/2601.20784)
*Zishen Wan,Che-Kai Liu,Jiayi Qian,Hanchen Yang,Arijit Raychowdhury,Tushar Krishna*

Main category: cs.AI

TL;DR: 本文指出神经符号AI中概率逻辑推理的效率瓶颈，提出加速框架REASON，经评估，它能显著提升速度和能源效率，实现实时推理，对神经符号AI很关键。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI虽有优势，但因符号和概率推理效率低，部署有挑战，需解决概率逻辑推理的低效问题。

Method: 提出REASON框架，引入统一的有向无环图表示，结合自适应剪枝和正则化，并在架构和系统层面进行优化。

Result: 在六个神经符号工作负载中，相比桌面和边缘GPU，REASON实现了12 - 50倍的加速和310 - 681倍的能源效率提升，能在0.8秒内完成端到端任务。

Conclusion: 针对性加速概率逻辑推理对实用和可扩展的神经符号AI至关重要，REASON可作为下一代认知智能的基础系统架构。

Abstract: Neuro-symbolic AI systems integrate neural perception with symbolic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inference. Through systematic analysis of representative neuro-symbolic workloads, we identify probabilistic logical reasoning as the inefficiency bottleneck, characterized by irregular control flow, low arithmetic intensity, uncoalesced memory accesses, and poor hardware utilization on CPUs and GPUs.
  This paper presents REASON, an integrated acceleration framework for probabilistic logical reasoning in neuro-symbolic AI. REASON introduces a unified directed acyclic graph representation that captures common structure across symbolic and probabilistic models, coupled with adaptive pruning and regularization. At the architecture level, REASON features a reconfigurable, tree-based processing fabric optimized for irregular traversal, symbolic deduction, and probabilistic aggregation. At the system level, REASON is tightly integrated with GPU streaming multiprocessors through a programmable interface and multi-level pipeline that efficiently orchestrates compositional execution. Evaluated across six neuro-symbolic workloads, REASON achieves 12-50x speedup and 310-681x energy efficiency over desktop and edge GPUs under TSMC 28 nm node. REASON enables real-time probabilistic logical reasoning, completing end-to-end tasks in 0.8 s with 6 mm2 area and 2.12 W power, demonstrating that targeted acceleration of probabilistic logical reasoning is critical for practical and scalable neuro-symbolic AI and positioning REASON as a foundational system architecture for next-generation cognitive intelligence.

</details>


### [23] [MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents](https://arxiv.org/abs/2601.20831)
*Vishnu Sashank Dorbala,Dinesh Manocha*

Main category: cs.AI

TL;DR: 提出MemCtrl框架用MLLMs在线修剪内存，训练两种记忆头μ，提升具身任务完成能力，在部分指令子集上提升超20%，对长且复杂指令表现优。


<details>
  <summary>Details</summary>
Motivation: 现有内存压缩和检索系统将内存视为离线存储空间，不利于在严格内存和计算约束下在线运行的具身智能体，需新方法。

Method: 提出MemCtrl框架，用MLLMs在线修剪内存，通过离线专家和在线RL两种方式训练记忆头μ。

Result: μ增强的MLLMs在具身任务完成能力上显著提升，在多个子集平均提升约16%，特定指令子集超20%。

Conclusion: MemCtrl框架能有效提升MLLMs在具身任务中的性能，尤其对长且复杂指令表现优越。

Abstract: Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.

</details>


### [24] [Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)](https://arxiv.org/abs/2601.20843)
*Saurav Prateek*

Main category: cs.AI

TL;DR: 本文提出Deep Researcher架构以生成复杂博士级研究报告，采用顺序研究计划细化和候选交叉算法，在基准测试中表现出色，证明顺序扩展优于并行范式。


<details>
  <summary>Details</summary>
Motivation: 解决并行扩展范式的固有局限性，生成复杂博士级研究报告。

Method: 采用顺序研究计划细化和候选交叉算法，结合One Shot Report Generation，由Gemini 2.5 Pro模型驱动。

Result: 在DeepResearch Bench上整体得分46.21，超越多个领先的深度研究代理，略超先前工作Static DRA。

Conclusion: 顺序扩展始终优于并行自一致性范式。

Abstract: This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from siloed knowledge. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density. Powered by the Gemini 2.5 Pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher and Grok Deeper Search present on the DeepResearch Bench actively running leaderboard. This performance marginally exceeds our previous work, Static DRA, and reinforces the finding that sequential scaling consistently outperforms the parallel self consistency paradigm.

</details>


### [25] [SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models](https://arxiv.org/abs/2601.20856)
*Sebastiano Monti,Carlo Nicolini,Gianni Pellegrini,Jacopo Staiano,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文对大推理模型的规划和长程推理能力进行系统评估，发现长程规划性能下降及架构局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型长程规划能力未被广泛研究，需对大推理模型的规划和长程推理能力进行系统评估。

Method: 提出基于推箱子谜题的新型基准，简化谜题以分离长程规划和状态持久性；让大推理模型配备PDDL解析、验证和求解工具。

Result: 当达到解决方案需要超过25步时，规划性能持续下降；配备PDDL工具可带来适度改进。

Conclusion: 大推理模型在长程规划能力上存在基本约束和架构局限性，仅靠测试时扩展方法可能无法克服。

Abstract: Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [26] [High-Resolution Mapping of Port Dynamics from Open-Access AIS Data in Tokyo Bay](https://arxiv.org/abs/2601.20211)
*Moritz Hütten*

Main category: cs.CE

TL;DR: 分析2024年东京湾三个月的AIS数据，精确推断港口活动，揭示运输趋势和接收站位置。


<details>
  <summary>Details</summary>
Motivation: 港口区域的船舶活动数据对了解经济趋势、助力航运决策和保障海上安全有重要意义，且AIS数据日益公开可用。

Method: 分析2024年东京湾三个月的公开AIS数据，考虑数据覆盖不均重建船舶活动，以约30米分辨率绘图。

Result: 确定161个泊位，得出平均同时移动船舶数、每日进出湾船舶数和平均总吨位等，发现大船化趋势，还可通过无线电阴影确定接收站位置。

Conclusion: AIS数据可高精度推断港口活动，结果能反映长期运输趋势，且能定位接收站。

Abstract: Knowledge about vessel activity in port areas and around major industrial zones provides insights into economic trends, supports decision-making for shipping and port operators, and contributes to maritime safety. Vessel data from terrestrial receivers of the Automatic Identification System (AIS) have become increasingly openly available, and we demonstrate that such data can be used to infer port activities at high resolution and with precision comparable to official statistics. We analyze open-access AIS data from a three-month period in 2024 for Tokyo Bay, located in Japan's most densely populated urban region. Accounting for uneven data coverage, we reconstruct vessel activity in Tokyo Bay at $\sim\,$30~m resolution and identify 161 active berths across seven major port areas in the bay. During the analysis period, we find an average of $35\pm17_{\text{stat}}$ vessels moving within the bay at any given time, and $293\pm22_{\text{stat}}+65_{\text{syst}}-10_{\text{syst}}$ vessels entering or leaving the bay daily, with an average gross tonnage of $11{,}860^{+280}_{-\;\,50}$. These figures indicate an accelerating long-term trend toward fewer but larger vessels in Tokyo Bay's commercial traffic. Furthermore, we find that in dense urban environments, radio shadows in vessel AIS data can reveal the precise locations of inherently passive receiver stations.

</details>


### [27] [CM-GAI: Continuum Mechanistic Generative Artificial Intelligence Theory for Data Dynamics](https://arxiv.org/abs/2601.20462)
*Shan Tang,Ziwei Cao,Zhenling Yang,Jiachen Guo,Yicheng Lu,Wing Kam Liu,Xu Guo*

Main category: cs.CE

TL;DR: 本文针对生成式人工智能在专业领域因数据稀缺能力受限的问题，开发了基于连续介质力学的理论框架以解决生成任务，在多个层面验证可行，还指出力学可为计算机科学提供新工具并讨论了理论局限性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式人工智能在专业领域因数据稀缺而能力有限。

Method: 开发基于连续介质力学的理论框架以推广纯数学的最优传输理论，用于描述数据动力学。

Result: 该理论成功完成了材料、结构和系统层面的典型生成任务，并展现出解决工程应用等诸多难题的潜力。

Conclusion: 力学能为计算机科学提供新工具，同时讨论了所提理论的局限性。

Abstract: Generative artificial intelligence (GAI) plays a fundamental role in high-impact AI-based systems such as SORA and AlphaFold. Currently, GAI shows limited capability in the specialized domains due to data scarcity. In this paper, we develop a continuum mechanics-based theoretical framework to generalize the optimal transport theory from pure mathematics, which can be used to describe the dynamics of data, realizing the generative tasks with a small amount of data. The developed theory is used to solve three typical problem involved in many mechanical designs and engineering applications: at material level, how to generate the stress-strain response outside the range of experimental conditions based on experimentally measured stress-strain data; at structure level, how to generate the temperature-dependent stress fields under the thermal loading; at system level, how to generate the plastic strain fields under transient dynamic loading. Our results show the proposed theory can complete the generation successfully, showing its potential to solve many difficult problems involved in engineering applications, not limited to mechanics problems, such as image generation. The present work shows that mechanics can provide new tools for computer science. The limitation of the proposed theory is also discussed.

</details>


### [28] [Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives](https://arxiv.org/abs/2601.20833)
*Tengyue Xu,Zhuoyang Qian,Gaoge Liu,Li Ling,Zhentao Zhang,Biao Wu,Shuo Zhang,Ke Lu,Wei Shi,Ziqi Wang,Zheng Feng,Yan Luo,Shu Xu,Yongjin Chen,Zhibo Feng,Zhuo Chen,Bruce Yuan,Harry Wang,Kris Chen*

Main category: cs.CE

TL;DR: 提出Idea2Story框架用于自主科学发现，通过预计算驱动将文献理解从在线推理转为离线知识构建，减少计算成本和推理问题，研究表明其能生成高质量研究模式。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自主科学发现系统依赖运行时计算范式，存在计算成本高、上下文窗口限制、推理不可靠和幻觉等问题。

Method: 提出Idea2Story框架，持续收集论文及评审反馈，提取核心方法单元，构建可复用研究模式并组织成结构化方法知识图谱，运行时将用户研究意图与既定范式对齐。

Result: 定性分析和初步实证研究表明，Idea2Story能生成连贯、方法合理且新颖的研究模式，可端到端产生多个高质量研究示范。

Conclusion: 离线知识构建为可靠的自主科学发现提供了实用且可扩展的基础。

Abstract: Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [29] [DBTuneSuite: An Extendible Experimental Suite to Test the Time Performance of Multi-layer Tuning Options on Database Management Systems](https://arxiv.org/abs/2601.20015)
*Amani Agrawal,Tianxin Wang,Dennis Shasha*

Main category: cs.DB

TL;DR: DBTuneSuite对四个广泛部署的免费数据库系统进行实验，测试不同负载和调优选项下性能，提供多方面成果，对相关人员有用。


<details>
  <summary>Details</summary>
Motivation: 测试广泛部署的免费数据库系统在不同查询/更新负载和调优选项下的性能。

Method: 构建DBTuneSuite实验套件，包含生成数据、安装和运行测试的脚本。

Result: 提供了可扩展脚本、不同查询类型适用系统的建议以及调优选项在不同系统中表现差异的量化证据。

Conclusion: 该论文对数据库系统工程师、高级用户、故障排除人员和学生有很大帮助。

Abstract: DBTuneSuite is a suite of experiments on four widely deployed free database systems to test their performance under various query/upsert loads and under various tuning options. The suite provides: (i) scripts to generate data and to install and run tests, making it expandable to other tests and systems; (ii) suggestions of which systems work best for which query types; and (iii) quantitative evidence that tuning options widely used in practice can behave very differently across systems. This paper is most useful for database system engineers, advanced database users and troubleshooters, and students.

</details>


### [30] [Delta Fair Sharing: Performance Isolation for Multi-Tenant Storage Systems](https://arxiv.org/abs/2601.20030)
*Tyler Griggs,Soujanya Ponnapalli,Dev Bali,Wenjie Ma,James DeLoye,Audrey Cheng,Jaewan Hong,Natacha Crooks,Scott Shenker,Ion Stoica,Matei Zaharia*

Main category: cs.DB

TL;DR: 提出Delta Fair Sharing算法解决存储系统资源高抢占延迟问题，在FAIRDB中实现，效果优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现代存储系统需提供性能隔离，但传统公平共享方法因资源高抢占延迟无法实现，导致客户端尾延迟不可接受。

Method: 引入Delta Fair Sharing算法家族，满足δ - 公平性和δ - 帕累托效率两个关键属性，并在FAIRDB中实现该算法。

Result: 评估表明FAIRDB在隔离高需求工作负载对行为良好客户端的影响方面优于现有替代方案。

Conclusion: Delta Fair Sharing算法能有效解决存储系统资源高抢占延迟问题，提升性能隔离效果。

Abstract: Modern storage systems, often deployed to support multiple tenants in the cloud, must provide performance isolation. Unfortunately, traditional approaches such as fair sharing do not provide performance isolation for storage systems, because their resources (e.g., write buffers and read caches) exhibit high preemption delays. These delays lead to unacceptable spikes in client tail latencies, as clients may be forced to wait arbitrarily long to receive their fair share of resources.
  We introduce Delta Fair Sharing, a family of algorithms for sharing resources with high preemption delays. These algorithms satisfy two key properties: $δ$-fairness, which bounds a client's delay in receiving its fair share of resources to $δ$ time units, and $δ$-Pareto-efficiency, which allocates unused resources to clients with unmet demand. Together, these properties capture resource-acquisition delays end-to-end, bound well-behaved clients' tail-latency spikes to $δ$ time units, and ensure high utilization. We implement such algorithms in FAIRDB, an extension of RocksDB. Our evaluation shows that FAIRDB isolates well-behaved clients from high-demand workloads better than state-of-the-art alternatives.

</details>


### [31] [ConStruM: A Structure-Guided LLM Framework for Context-Aware Schema Matching](https://arxiv.org/abs/2601.20482)
*Houming Chen,Zhe Zhang,H. V. Jagadish*

Main category: cs.DB

TL;DR: 提出ConStruM框架用于模式匹配中预算证据打包，通过构建结构组织上下文证据，实验表明能提升匹配效果。


<details>
  <summary>Details</summary>
Motivation: 在列匹配任务中，很多数据集需列外额外证据，而提供全部模式元数据不现实，需选择和组织最有用的上下文信息。

Method: 提出ConStruM框架，构建轻量级可复用结构，开发上下文树用于预算多级上下文检索和全局相似超图，为上游匹配器的候选目标增强最终大语言模型提示。

Result: 在真实数据集上的实验显示ConStruM通过提供和组织合适的上下文证据提升了匹配效果。

Conclusion: ConStruM能有效解决模式匹配中上下文信息选择和组织问题，提升匹配性能。

Abstract: Column matching is a central task in reconciling schemas for data integration. Column names and descriptions are valuable for this task. LLMs can leverage such natural-language schema metadata. However, in many datasets, correct matching requires additional evidence beyond the column itself. Because it is impractical to provide an LLM with the entire schema metadata needed to capture this evidence, the core challenge becomes to select and organize the most useful contextual information.
  We present ConStruM, a structure-guided framework for budgeted evidence packing in schema matching. ConStruM constructs a lightweight, reusable structure in which, at query time, it assembles a small context pack emphasizing the most discriminative evidence. ConStruM is designed as an add-on: given a shortlist of candidate targets produced by an upstream matcher, it augments the matcher's final LLM prompt with structured, query-specific evidence so that the final selection is better grounded. For this purpose, we develop a context tree for budgeted multi-level context retrieval and a global similarity hypergraph that surfaces groups of highly similar columns (on both the source and target sides), summarized via group-aware differentiation cues computed online or precomputed offline. Experiments on real datasets show that ConStruM improves matching by providing and organizing the right contextual evidence.

</details>


### [32] [ALER: An Active Learning Hybrid System for Efficient Entity Resolution](https://arxiv.org/abs/2601.20664)
*Dimitrios Karapiperis,Leonidas Akritidis,Panayiotis Bozanis,Vassilios Verykios*

Main category: cs.DB

TL;DR: 现有实体解析监督学习模型因需大量标注数据不实用，主动学习方法有可扩展性瓶颈，本文提出半监督管道ALER，评估显示其效率高。


<details>
  <summary>Details</summary>
Motivation: 解决实体解析中监督学习模型需大量标注数据不实用，以及现有主动学习方法可扩展性瓶颈问题。

Method: 使用冻结双编码器架构生成静态嵌入，迭代训练轻量级分类器；选代表性数据样本并用K - Means分区；提出混合查询策略。

Result: 在大规模DBLP数据集上，训练循环加速1.3倍，解析延迟降低3.8倍。

Conclusion: ALER能有效平衡语义准确性和计算可扩展性，效率优越。

Abstract: Entity Resolution (ER) is a critical task for data integration, yet state-of-the-art supervised deep learning models remain impractical for many real-world applications due to their need for massive, expensive-to-obtain labeled datasets. While Active Learning (AL) offers a potential solution to this "label scarcity" problem, existing approaches introduce severe scalability bottlenecks. Specifically, they achieve high accuracy but incur prohibitive computational costs by re-training complex models from scratch or solving NP-hard selection problems in every iteration. In this paper, we propose ALER, a novel, semi-supervised pipeline designed to bridge the gap between semantic accuracy and computational scalability. ALER eliminates the training bottleneck by using a frozen bi-encoder architecture to generate static embeddings once and then iteratively training a lightweight classifier on top. To address the memory bottleneck associated with large-scale candidate pools, we first select a representative sample of the data and then use K-Means to partition this sample into semantically coherent chunks, enabling an efficient AL loop. We further propose a hybrid query strategy that combines "confused" and "confident" pairs to efficiently refine the decision boundary while correcting high-confidence errors.Extensive evaluation demonstrates ALER's superior efficiency, particularly on the large-scale DBLP dataset: it accelerates the training loop by 1.3x while drastically reducing resolution latency by a factor of 3.8 compared to the fastest baseline.

</details>


### [33] [The Monotone Priority System: Foundations of Contract-Specific Sequencing](https://arxiv.org/abs/2601.20783)
*Naveen Durvasula*

Main category: cs.DB

TL;DR: 本文提出为智能合约函数调用添加排序约束的方法，该系统满足五个独立公理。


<details>
  <summary>Details</summary>
Motivation: 现代区块链应用需对交互交易设置排序约束，要平衡表达性和区块生产的易处理性。

Method: 提出一个系统，允许合约开发者为每个调用设置整数全局优先级，且调用优先级不高于其引用调用的优先级，区块构建者按优先级排序交易。

Result: 证明该系统是满足五个独立公理的唯一系统。

Conclusion: 所提系统是一种合理且有公理依据的添加排序约束的方式。

Abstract: Modern blockchain applications benefit from the ability to specify sequencing constraints on the transactions that interact with them. This paper proposes a principled and axiomatically justified way of adding sequencing constraints on smart contract function calls that balances expressivity with the tractability of block production. Specifically, we propose a system in which contract developers are allowed to set an integer global priority for each of their calls, so long as that the call's chosen priority is no higher than the priority of any of its referenced calls. Block builders must then simply sequence transactions in priority order (from high to low priority), breaking ties however they would like. We show that this system is the unique system that satisfies five independent axioms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [34] [A Data-Informed Local Subspaces Method for Error-Bounded Lossy Compression of Large-Scale Scientific Datasets](https://arxiv.org/abs/2601.20113)
*Arshan Khan,Rohit Deshmukh,Ben O'Neill*

Main category: cs.DC

TL;DR: 提出名为Discontinuous DLS的数据驱动科学数据压缩器，提升压缩误差比，适应多种科学数据，经分布式环境实现并评估性能，是大规模科学数据压缩的有前景方法。


<details>
  <summary>Details</summary>
Motivation: 科学模拟数据量增长给存储和传输带来挑战，需提升压缩误差比以应对。

Method: 提出基于数据驱动的Discontinuous DLS压缩器，利用局部时空子空间，在分布式计算环境用MPI实现。

Result: 能显著降低存储需求，不损害关键数据保真度，在压缩比和重建精度上较现有方法得到评估。

Conclusion: Discontinuous DLS是高性能计算环境下大规模科学数据压缩的有希望途径，可解决现代科学模拟数据增长需求问题。

Abstract: The growing volume of scientific simulation data presents a significant challenge for storage and transfer. Error-bounded lossy compression has emerged as a critical solution for mitigating these challenges, providing a means to reduce data size while ensuring that reconstructed data remains valid for scientific analysis. In this paper, we present a data-driven scientific data compressor, called Discontinuous Data-informed Local Subspaces (Discontinuous DLS), to improve compression-to-error ratios over data-agnostic compressors. This error-bounded compressor leverages localized spatial and temporal subspaces, informed by the underlying data structure, to enhance compression efficiency and preserve key features. The presented technique is flexible and applicable to a wide range of scientific data, including fluid dynamics, environmental simulations, and other high-dimensional, time-dependent datasets. We describe the core principles of the method and demonstrate its ability to significantly reduce storage requirements without compromising critical data fidelity. The technique is implemented in a distributed computing environment using MPI, and its performance is evaluated against state-of-the-art error-bounded compression methods in terms of compression ratio and reconstruction accuracy. This study highlights discontinuous DLS as a promising approach for large-scale scientific data compression in high-performance computing environments, providing a robust solution for managing the growing data demands of modern scientific simulations.

</details>


### [35] [StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs](https://arxiv.org/abs/2601.20273)
*Jiacheng Yang,Jun Wu,Yaoyao Ding,Zhiying Xu,Yida Wang,Gennady Pekhimenko*

Main category: cs.DC

TL;DR: 提出拓扑感知高效DiT服务引擎StreamFusion，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散变压器（DiTs）单GPU推理效率低，当前框架的序列并行技术有局限性。

Method: 提出拓扑感知序列并行技术、Torus Attention和单边通信实现。

Result: StreamFusion平均性能比现有方法高1.35倍，最高达1.77倍。

Conclusion: StreamFusion能有效解决现有DiT推理的效率问题。

Abstract: Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\times$ (up to $1.77\times$).

</details>


### [36] [SuperInfer: SLO-Aware Rotary Scheduling and Memory Management for LLM Inference on Superchips](https://arxiv.org/abs/2601.20309)
*Jiahuan Yu,Mingtao Hu,Zichao Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: 提出高性能LLM推理系统SuperInfer，可提升TTFT SLO达标率，发挥超级芯片潜力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理系统在高请求率下因KV缓存预算耗尽有HOL阻塞问题，PCIe卸载方法无法满足严格SLO。

Method: 设计SuperInfer系统，引入RotaSched调度器和DuplexKV旋转引擎。

Result: 在GH200上评估，SuperInfer使TTFT SLO达标率提升达74.7%，TBT和吞吐量与现有系统相当。

Conclusion: SLO感知调度和内存协同设计能充分发挥超级芯片在响应式LLM服务中的潜力。

Abstract: Large Language Model (LLM) serving faces a fundamental tension between stringent latency Service Level Objectives (SLOs) and limited GPU memory capacity. When high request rates exhaust the KV cache budget, existing LLM inference systems often suffer severe head-of-line (HOL) blocking. While prior work explored PCIe-based offloading, these approaches cannot sustain responsiveness under high request rates, often failing to meet tight Time-To-First-Token (TTFT) and Time-Between-Tokens (TBT) SLOs. We present SuperInfer, a high-performance LLM inference system designed for emerging Superchips (e.g., NVIDIA GH200) with tightly coupled GPU-CPU architecture via NVLink-C2C. SuperInfer introduces RotaSched, the first proactive, SLO-aware rotary scheduler that rotates requests to maintain responsiveness on Superchips, and DuplexKV, an optimized rotation engine that enables full-duplex transfer over NVLink-C2C. Evaluations on GH200 using various models and datasets show that SuperInfer improves TTFT SLO attainment rates by up to 74.7% while maintaining comparable TBT and throughput compared to state-of-the-art systems, demonstrating that SLO-aware scheduling and memory co-design unlocks the full potential of Superchips for responsive LLM serving.

</details>


### [37] [Graph-Structured Deep Learning Framework for Multi-task Contention Identification with High-dimensional Metrics](https://arxiv.org/abs/2601.20389)
*Xiao Yang,Yinan Ni,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: 提出统一争用分类框架识别高维系统环境多任务争用类型，实验证明效果好，能提升争用模式识别能力。


<details>
  <summary>Details</summary>
Motivation: 解决高维系统环境中准确识别多任务争用类型的挑战。

Method: 构建系统状态表示，进行非线性变换提取特征，引入基于图的建模机制，设计特定任务映射结构，采用自适应多任务损失加权策略。

Result: 在公共系统跟踪数据集实验中，各项指标表现好，敏感性分析证实模型稳定性和适用性。

Conclusion: 基于高维指标的结构化表示和多任务分类可显著提升争用模式识别能力，为复杂计算环境性能管理提供可靠技术途径。

Abstract: This study addresses the challenge of accurately identifying multi-task contention types in high-dimensional system environments and proposes a unified contention classification framework that integrates representation transformation, structural modeling, and a task decoupling mechanism. The method first constructs system state representations from high-dimensional metric sequences, applies nonlinear transformations to extract cross-dimensional dynamic features, and integrates multiple source information such as resource utilization, scheduling behavior, and task load variations within a shared representation space. It then introduces a graph-based modeling mechanism to capture latent dependencies among metrics, allowing the model to learn competitive propagation patterns and structural interference across resource links. On this basis, task-specific mapping structures are designed to model the differences among contention types and enhance the classifier's ability to distinguish multiple contention patterns. To achieve stable performance, the method employs an adaptive multi-task loss weighting strategy that balances shared feature learning with task-specific feature extraction and generates final contention predictions through a standardized inference process. Experiments conducted on a public system trace dataset demonstrate advantages in accuracy, recall, precision, and F1, and sensitivity analyses on batch size, training sample scale, and metric dimensionality further confirm the model's stability and applicability. The study shows that structured representations and multi-task classification based on high-dimensional metrics can significantly improve contention pattern recognition and offer a reliable technical approach for performance management in complex computing environments.

</details>


### [38] [Meeting SLOs, Slashing Hours: Automated Enterprise LLM Optimization with OptiKIT](https://arxiv.org/abs/2601.20408)
*Nicholas Santavas,Kareem Eissa,Patrycja Cieplicka,Piotr Florek,Matteo Nulli,Stefan Vasilev,Seyyed Hadi Hashemi,Antonios Gasteratos,Shahram Khadivi*

Main category: cs.DC

TL;DR: 提出OptiKIT框架解决企业大语言模型部署可扩展性挑战，开源系统促进优化模型普及。


<details>
  <summary>Details</summary>
Motivation: 企业大语言模型部署面临可扩展性挑战，手动优化所需专业知识稀缺，需让无专业经验团队高效部署模型。

Method: 提出分布式大语言模型优化框架OptiKIT，提供动态资源分配、自动清理的分阶段管道执行和企业无缝集成。

Result: 生产中实现超2倍GPU吞吐量提升，应用团队无需深入专业知识就能实现性能提升。

Conclusion: 分享平台设计和工程见解，开源系统以促进外部贡献和可重复性。

Abstract: Enterprise LLM deployment faces a critical scalability challenge: organizations must optimize models systematically to scale AI initiatives within constrained compute budgets, yet the specialized expertise required for manual optimization remains a niche and scarce skillset. This challenge is particularly evident in managing GPU utilization across heterogeneous infrastructure while enabling teams with diverse workloads and limited LLM optimization experience to deploy models efficiently.
  We present OptiKIT, a distributed LLM optimization framework that democratizes model compression and tuning by automating complex optimization workflows for non-expert teams. OptiKIT provides dynamic resource allocation, staged pipeline execution with automatic cleanup, and seamless enterprise integration.
  In production, it delivers more than 2x GPU throughput improvement while empowering application teams to achieve consistent performance improvements without deep LLM optimization expertise. We share both the platform design and key engineering insights into resource allocation algorithms, pipeline orchestration, and integration patterns that enable large-scale, production-grade democratization of model optimization. Finally, we open-source the system to enable external contributions and broader reproducibility.

</details>


### [39] [Rethinking Thread Scheduling under Oversubscription: A User-Space Framework for Coordinating Multi-runtime and Multi-process Workloads](https://arxiv.org/abs/2601.20435)
*Aleix Roca,Vicenç Beltran*

Main category: cs.DC

TL;DR: 论文提出用户空间调度框架USF及默认协作策略SCHED_COOP，可减少干扰，在多场景评估中有性能提升。


<details>
  <summary>Details</summary>
Motivation: 高性能计算与人工智能融合产生复杂并行工作负载，传统OS调度器面临调度压力，过订阅时性能易下降。

Method: 提出用户空间调度框架USF，用户可自定义调度算法；采用默认协作策略SCHED_COOP，仅在阻塞时切换线程；用nOS - V运行时扩展GNU C库实现。

Result: 在过订阅多进程场景评估中，性能提升达2.4倍，涵盖嵌套BLAS工作负载、多进程PyTorch推理、分子动力学模拟等。

Conclusion: USF和SCHED_COOP能有效减少干扰，提升复杂并行工作负载的调度性能。

Abstract: The convergence of high-performance computing (HPC) and artificial intelligence (AI) is driving the emergence of increasingly complex parallel applications and workloads. These workloads often combine multiple parallel runtimes within the same application or across co-located jobs, creating scheduling demands that place significant stress on traditional OS schedulers. When oversubscribed (there are more ready threads than cores), OS schedulers rely on periodic preemptions to multiplex cores, often introducing interference that may degrade performance. In this paper, we present: (1) The User-space Scheduling Framework (USF), a novel seamless process scheduling framework completely implemented in user-space. USF enables users to implement their own process scheduling algorithms without requiring special permissions. We evaluate USF with its default cooperative policy, (2) SCHED_COOP, designed to reduce interference by switching threads only upon blocking. This approach mitigates well-known issues such as Lock-Holder Preemption (LHP), Lock-Waiter Preemption (LWP), and scalability collapse. We implement USF and SCHED_COOP by extending the GNU C library with the nOS-V runtime, enabling seamless coordination across multiple runtimes (e.g., OpenMP) without requiring invasive application changes. Evaluations show gains up to 2.4x in oversubscribed multi-process scenarios, including nested BLAS workloads, multi-process PyTorch inference with LLaMA-3, and Molecular Dynamics (MD) simulations.

</details>


### [40] [AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling](https://arxiv.org/abs/2601.20595)
*Xinwei Qiang,Yue Guan,Zhengding Hu,Yufei Ding,Adnan Aziz*

Main category: cs.DC

TL;DR: 提出AutoOverlap编译器和运行时系统，实现单融合内核内细粒度重叠，在多GPU工作负载上有速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有分布式编译器在处理大规模GPU工作负载通信瓶颈时粒度粗，存在额外内核启动、设备同步和通信延迟问题。

Method: 引入通信块抽象，解耦通信粒度与内核结构和后端机制，根据本地Triton内核和块调度进行转换。

Result: 在多GPU工作负载上平均端到端加速比达1.3倍，最高达4.7倍。

Conclusion: AutoOverlap能有效实现单融合内核内的细粒度重叠，提升多GPU工作负载性能。

Abstract: Communication has become a first-order bottleneck in large-cale GPU workloads, and existing distributed compilers address it mainly by overlapping whole compute and communication kernels at the stream level. This coarse granularity incurs extra kernel launches, forces device-wide synchronizations at kernel boundaries, and leaves substantial slack when the slowest tile or kernel stretches the communication tail. We present AutoOverlap, a compiler and runtime that enables automatic fine-grained overlap inside a single fused kernel. AutoOverlap introduces a communication chunk abstraction that decouples communication granularity from kernel structure and backend mechanisms, allowing chunk-level plans to be ported from existing distributed compilers, written directly by users, or instantiated from reusable templates. Given a local Triton kernel and a chunk schedule, AutoOverlap performs transformations to align computation with chunk availability. Implemented as a source-to-source compiler on Triton, AutoOverlap delivers an average end-to-end speedup of 1.3$\times$ and up to 4.7$\times$ on multi-GPU workloads.

</details>


### [41] [OnePiece: A Large-Scale Distributed Inference System with RDMA for Complex AI-Generated Content (AIGC) Workflows](https://arxiv.org/abs/2601.20655)
*June Chen,Neal Xu,Gragas Huang,Bok Zhou,Stephen Liu*

Main category: cs.DC

TL;DR: 本文介绍了针对多阶段AIGC工作流优化的分布式推理系统OnePiece，它能有效减少延迟和开销，实验表明可降低GPU资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC系统在并发工作负载下存在吞吐量、资源利用率和可扩展性方面的低效问题。

Method: 将管道分解为细粒度微服务，利用单边RDMA通信；采用双环缓冲区设计解决内存访问死锁；使用动态节点管理器根据实时负载弹性分配资源。

Result: 在Wan2.1图像到视频生成中，OnePiece相比单片推理管道将GPU资源消耗降低了16倍。

Conclusion: OnePiece为生产AIGC环境提供了可扩展、容错且高效的解决方案。

Abstract: The rapid growth of AI-generated content (AIGC) has enabled high-quality creative production across diverse domains, yet existing systems face critical inefficiencies in throughput, resource utilization, and scalability under concurrent workloads. This paper introduces OnePiece, a large-scale distributed inference system with RDMA optimized for multi-stage AIGC workflows. By decomposing pipelines into fine-grained microservices and leveraging one-sided RDMA communication, OnePiece significantly reduces inter-node latency and CPU overhead while improving GPU utilization. The system incorporates a novel double-ring buffer design to resolve deadlocks in RDMA-aware memory access without CPU involvement. Additionally, a dynamic Node Manager allocates resources elastically across workflow stages in response to real-time load. Experimental results demonstrate that OnePiece reduces GPU resource consumption by 16x in Wan2.1 image-to-video generation compared to monolithic inference pipelines, offering a scalable, fault-tolerant, and efficient solution for production AIGC environments.

</details>


### [42] [Agentic Fog: A Policy-driven Framework for Distributed Intelligence in Fog Computing](https://arxiv.org/abs/2601.20764)
*Saeed Akbar,Muhammad Waqas,Rahmat Ullah*

Main category: cs.DC

TL;DR: 本文提出Agentic Fog (AF)通用模型，通过模拟证明其在动态条件下比贪心启发式和整数线性规划有更低平均延迟和更高效适应性。


<details>
  <summary>Details</summary>
Motivation: 雾和边缘计算需要自适应控制方案，现有Agentic AI工具因高计算成本等不适用于基础设施级系统。

Method: 提出AF模型，将雾节点表示为策略驱动的自主代理，通过基于共享内存和局部协调的p2p交互通信，将系统目标分解为抽象策略指导并将分散式雾协调形式化为精确势博弈。

Result: 模拟显示AF系统在动态条件下平均延迟更低，对不同需求适应更高效，敏感性分析表明其在不同内存和协调条件下能实现最优性能。

Conclusion: AF模型可有效解决雾和边缘计算的自适应控制问题，在动态环境中有良好表现。

Abstract: Fog and edge computing require adaptive control schemes that can handle partial observability, severe latency requirements, and dynamically changing workloads. Recent research on Agentic AI (AAI) increasingly integrates reasoning systems powered by Large Language Models; however, these tools are not applicable to infrastructure-level systems due to their high computational cost, stochastic nature, and poor formal analyzability. In this paper, a generic model, Agentic Fog (AF), is presented, in which fog nodes are represented as policy-driven autonomous agents that communicate via p2p interactions based on shared memory and localized coordination. The suggested architecture decomposes a system's goals into abstract policy guidance and formalizes decentralized fog coordination as an exact potential game. The framework is guaranteed to converge and remain stable under asynchronous updates, bounded-rational best-response dynamics, and node failures. Simulations demonstrate that the AF system achieves lower average latency and adapts more efficiently to varying demand than greedy heuristics and integer linear programming under dynamic conditions. The sensitivity analysis also demonstrates the capability to perform optimally under different memory and coordination conditions.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [43] [A Cache-Aware Hybrid Sieve Combining Segmentation and Bit-Packing for Fast Prime Generation](https://arxiv.org/abs/2601.19909)
*Kathi Lakshmi Mani Thirdhana*

Main category: cs.DS

TL;DR: 本文提出缓存感知混合筛法优化质数生成，降低内存使用、提升运行时间。


<details>
  <summary>Details</summary>
Motivation: 经典埃拉托斯特尼筛法在现代CPU上受内存访问效率限制，需优化。

Method: 引入缓存感知混合筛法，整合分段、位打包和缓存行对齐块处理，只存储奇数且每位存一值，将筛范围分块。

Result: 相比经典筛法，内存使用最多降8倍、运行时间最多提升2.4倍；比分段筛法运行时间最多提升1.7倍。

Conclusion: 考虑架构的算法设计能带来显著性能提升。

Abstract: Prime generation is a fundamental task in cryptography, number theory, and randomized algorithms. While the classical Sieve of Eratosthenes is simple and efficient in theory, its practical performance on modern central processing units is often limited by memory access inefficiencies. This paper introduces a cache-aware hybrid sieve that integrates segmentation, bit-packing, and cache-line-aligned block processing to optimize memory bandwidth and level one and level two cache locality.
  The proposed approach reduces memory usage by storing only odd numbers and using one bit per value. The sieve range is divided into cache-sized blocks to minimize cache misses, while primes up to the square root of the limit are reused across blocks. Experimental results demonstrate up to an eight times reduction in memory usage and runtime improvements of up to two point four times compared to the classical sieve and one point seven times compared to the segmented sieve. Benchmarks up to ten to the power of nine illustrate that architecture-aware algorithm design can yield substantial practical performance gains.

</details>


### [44] [Node-Weighted Multicut in Planar Digraphs](https://arxiv.org/abs/2601.20038)
*Chandra Chekuri,Rhea Jain*

Main category: cs.DS

TL;DR: 文章将Kawarabayashi和Sidiropoulos的算法和分析扩展到节点加权多割问题，获得确定性算法，简化分析，结果可近似非均匀最稀疏割问题。


<details>
  <summary>Details</summary>
Motivation: 将Kawarabayashi和Sidiropoulos的算法扩展到节点加权多割问题，获得确定性算法并简化分析。

Method: 通过对节点加权问题利用现有算法和分析进行扩展。

Result: 得到节点加权多割问题的相关算法，且结果能近似非均匀最稀疏割问题。

Conclusion: 扩展了算法适用范围，获得确定性算法，简化分析，对非均匀最稀疏割问题有近似效果。

Abstract: Kawarabayashi and Sidiropoulos [KS22] obtained an $O(\log^2 n)$-approximation algorithm for Multicut in planar digraphs via a natural LP relaxation, which also establishes a corresponding upper bound on the multicommodity flow-cut gap. Their result is in contrast to a lower bound of $\tildeΩ(n^{1/7})$ on the flow-cut gap for general digraphs due to Chuzhoy and Khanna [CK09]. We extend the algorithm and analysis in [KS22] to the node-weighted Multicut problem. Unlike in general digraphs, node-weighted problems cannot be reduced to edge-weighted problems in a black box fashion due to the planarity restriction. We use the node-weighted problem as a vehicle to accomplish two additional goals: (i) to obtain a deterministic algorithm (the algorithm in [KS22] is randomized), and (ii) to simplify and clarify some aspects of the algorithm and analysis from [KS22]. The Multicut result, via a standard technique, implies an approximation for the Nonuniform Sparsest Cut problem with an additional logarithmic factor loss.

</details>


### [45] [Hypergraph Samplers: Typical and Worst Case Behavior](https://arxiv.org/abs/2601.20039)
*Vedat Levi Alev,Uriya A. First*

Main category: cs.DS

TL;DR: 研究k - 均匀超图在随机算法误差减少中的效用与局限，给出边数量下界及典型情况下使用超图选种子的误差减少结果。


<details>
  <summary>Details</summary>
Motivation: 探索k - 均匀超图在有单边或双边误差的决策问题随机算法误差减少中的应用情况。

Method: 通过采样超图的均匀随机超边，用超边顶点作为种子重复算法k次。

Result: 一是单边误差减少时超图边数量有下界，二是顶点度合理分布时，多数情况下用超图选种子的误差接近用独立同分布种子的误差。

Conclusion: 对于多数决策问题随机算法，使用合理超图随机边采样与使用独立同分布样本的误差优势可忽略。

Abstract: We study the utility and limitations of using $k$-uniform hypergraphs $H = ([n], E)$ ($n \ge \mathrm{poly}(k)$) in the context of error reduction for randomized algorithms for decision problems with one- or two-sided error. Our error reduction idea is sampling a uniformly random hyperedge of $H$, and repeating the algorithm $k$ times using the hyperedge vertices as seeds. This is a general paradigm, which captures every pseudorandom method generating $k$ seeds without repetition. We show two results which imply a gap between the typical and the worst-case behavior of using $H$ for error-reduction.
  First, in the context of one-sided error reduction, if using a random hyperedge of $H$ decreases the error probability from $p$ to $p^k + ε$, then $H$ cannot have too few edges, i.e., $|E| = Ω(n k^{-1} ε^{-1})$. Thus, the number of random bits needed for reducing the error from $p$ to $p^k + ε$ cannot be reduced below $\lg n+\lg(ε^{-1})-\lg k+O(1)$. This is also true for hypergraphs of average uniformity $k$. Our result implies new lower bounds for dispersers and vertex-expanders.
  Second, if the vertex degrees are reasonably distributed, we show that in a $(1-o(1))$-fraction of the cases, choosing $k$ pseudorandom seeds using $H$ will reduce the error probability to at most $o(1)$ above the error probability of using $k$ IID seeds, for both algorithms with one- or two-sided error. Thus, despite our lower bound, for a $(1-o(1))$-fraction of randomized algorithms (and inputs) for decision problems, the advantage of using IID samples over samples obtained from a uniformly random edge of a reasonable hypergraph is negligible. A similar statement holds true for randomized algorithms with two-sided error.

</details>


### [46] [Dynamic framework for edge-connectivity maintenance of simple graphs](https://arxiv.org/abs/2601.20137)
*Blazej Wrobel*

Main category: cs.DS

TL;DR: 提出动态框架维护无向简单图的k边连通性，处理边增删，解决冗余消除和连通性恢复问题并给出对应时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 标准动态图问题仅关注最小割值，本文要主动修改图以维护边连通性不变。

Method: 对于冗余消除，结合Nagamochi - Ibaraki稀疏证书和Link - Cut Trees；对于连通性恢复，提出局部增强策略并在稀疏化图上执行Dinic算法。

Result: 冗余消除能在O(k log n)均摊时间内完成，连通性恢复能在O(k · n^(5/3))时间内找到最小边集。

Conclusion: 该动态框架能有效处理无向简单图边增删时的k边连通性维护问题。

Abstract: We present a dynamic framework for maintaining $k$-edge-connectivity of undirected, simple graphs subject to structural updates, specifically single edge additions and removals. The required edge-connectivity $k$ is a chosen, constant parameter. Unlike standard dynamic graph problems, such as dynamic minimum-cut, which focus solely on reporting the value of the minimum cut, our approach actively modifies the graph $G$ to maintain the edge-connectivity invariant $λ(G) \ge k$. We address two fundamental maintenance tasks: redundancy elimination, which identifies and removes an existing edge rendered redundant for $k$-edge-connectivity by new edge insertion, and connectivity restoration, which computes and inserts a minimal set of augmenting edges to restore graph's $k$-edge-connectivity following an old edge deletion. To preclude trivial reversals, we strictly enforce that the eliminated edge is distinct from the inserted edge and that restoration excludes the already deleted edge. Our solution of the first problem integrates Nagamochi-Ibaraki sparse certificates [Nagamochi and Ibaraki 1992] with Link-Cut Trees [Sleator and Tarjan 1983] to remove redundant edges in $O(k \log n)$ amortized time. For restoration, we propose a localized augmentation strategy that exploits the residual graph structure to bridge the minimum cut. By executing Dinic's [Dinic 1970] algorithm on the sparsified input graph, we identify the minimal edge set required to reconnect the graph in $O(k \cdot n^{5/3})$ time.

</details>


### [47] [Fully Dynamic Algorithms for Graph Spanners via Low-Diameter Router Decomposition](https://arxiv.org/abs/2601.20718)
*Julia Chuzhoy,Merav Parter*

Main category: cs.DS

TL;DR: 本文研究全动态环境下自适应对手模型的生成器维护问题，给出确定性算法并开发低直径路由器分解工具，还展示其在容错和低拥塞生成器动态算法中的应用。


<details>
  <summary>Details</summary>
Motivation: 全动态环境下自适应对手的生成器维护问题研究不足，尚无同时实现次对数拉伸、次线性更新时间和强次二次生成器大小的算法。

Method: 开发低直径路由器分解工具，设计确定性算法将全动态图分解为边不相交、顶点重叠有界的簇，且簇内路由路径在簇内。

Result: 对于任意512 ≤ k ≤ (log n)^(1/49) 和 1/k ≤ δ ≤ 1/400，维护一个具有特定拉伸和大小的生成器，最坏情况更新时间为 n^(O(δ))，回溯为 n^(O(1/k))。

Conclusion: 本文算法及低直径路由器分解工具解决了全动态生成器维护的部分问题，且可应用于其他动态算法。

Abstract: A $t$-spanner of an undirected $n$-vertex graph $G$ is a sparse subgraph $H$ of $G$ that preserves all pairwise distances between its vertices to within multiplicative factor $t$, also called the \emph{stretch}. We investigate the problem of maintaining spanners in the fully dynamic setting with an adaptive adversary. Despite a long line of research, this problem is still poorly understood: no algorithm achieving a sublogarithmic stretch, a sublinear in $n$ update time, and a strongly subquadratic in $n$ spanner size is currently known.
  One of our main results is a deterministic algorithm, that, for any $512 \leq k \leq (\log n)^{1/49}$ and $1/k\leq δ\leq 1/400$, maintains a spanner $H$ of a fully dynamic graph with stretch $poly(k)\cdot 2^{O(1/δ^6)}$ and size $|E(H)|\leq O(n^{1+O(1/k)})$, with worst-case update time $n^{O(δ)}$ and recourse $n^{O(1/k)}$. Our algorithm relies on a new technical tool that we develop, called low-diameter router decomposition. We design a deterministic algorithm that maintains a decomposition of a fully dynamic graph into edge-disjoint clusters with bounded vertex overlap, where each cluster $C$ is a bounded-diameter router, meaning that any reasonable multicommodity demand over the vertices of $C$ can be routed along short paths and with low congestion. A similar graph decomposition notion was introduced by [Haeupler et al., STOC 2022] and strengthened by [Haeupler et al., FOCS 2024]. However, in contrast to these and other prior works, the decomposition that our algorithm maintains is proper, ensuring that the routing paths between the pairs of vertices of each cluster $C$ are contained inside $C$, rather than in the entire graph $G$. We show additional applications of our router decomposition, including dynamic algorithms for fault-tolerant spanners and low-congestion spanners.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [48] [Defensive Rebalancing for Automated Market Makers](https://arxiv.org/abs/2601.19950)
*Sam Devorsetz,Maurice Herlihy*

Main category: cs.GT

TL;DR: 本文介绍并分析了防御性再平衡机制，用于保护CFMM免受套利导致的价值泄漏，证明了相关性质和解决了最优化问题，并扩展到混合再平衡，为未来AMM协议提供基础。


<details>
  <summary>Details</summary>
Motivation: 保护恒定函数做市商（CFMMs）免受套利导致的价值泄漏。

Method: 通过数学证明存在从套利倾向配置到无套利配置的再平衡，将最优无套利再平衡搜索转化为凸优化问题，还扩展到混合再平衡情形。

Result: 证明了多种性质，如存在增加CFMM流动性的再平衡、无套利与帕累托效率等价，找到最优无套利再平衡的凸优化解法，扩展到混合再平衡。

Conclusion: 为未来自动做市商（AMM）协议主动保护流动性提供者免受套利提供了严格基础。

Abstract: This paper introduces and analyzes \emph{defensive rebalancing}, a novel mechanism for protecting constant-function market makers (CFMMs) from value leakage due to arbitrage. A \emph{rebalancing} transfers assets directly from one CFMM's pool to another's, bypassing the CFMMs' standard trading protocols. In any \emph{arbitrage-prone} configuration, we prove there exists a rebalancing to an \textit{arbitrage-free} configuration that strictly increases some CFMMs' liquidities without reducing the liquidities of the others. Moreover, we prove that a configuration is arbitrage-free if and only if it is \emph{Pareto efficient} under rebalancing, meaning that any further direct asset transfers must decrease some CFMM's liquidity. We prove that for any log-concave trading function, including the ubiquitous constant product market maker, the search for an optimal, arbitrage-free rebalancing that maximizes global liquidity while ensuring no participant is worse off can be cast as a convex optimization problem with a unique, computationally tractable solution. We extend this framework to \emph{mixed rebalancing}, where a subset of participating CFMMs use a combination of direct transfers and standard trades to transition to an arbitrage-free configuration while harvesting arbitrage profits from non-participating CFMMs, and from price oracle market makers such as centralized exchanges. Our results provide a rigorous foundation for future AMM protocols that proactively defend liquidity providers against arbitrage.

</details>


### [49] [Guiding the Recommender: Information-Aware Auto-Bidding for Content Promotion](https://arxiv.org/abs/2601.20422)
*Yumou Liu,Zhenzhe Zheng,Jiang Rong,Yao Hu,Fan Wu,Guihai Chen*

Main category: cs.GT

TL;DR: 分析内容平台付费推广范式缺陷，提出双目标优化方法及相关算法，经实验验证可提升长期模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代内容平台付费推广范式存在缺陷，会损害高质量内容，需改进。

Method: 将内容推广重铸为双目标优化，引入可分解替代目标梯度覆盖，设计基于拉格朗日对偶的两阶段自动出价算法，提出置信门控梯度启发式方法。

Result: 理论上证明复合目标的单调次模性、在线拍卖的次线性遗憾和预算可行性；离线实验表明优于基线，AUC/LogLoss表现好，贴近预算目标，零阶近似梯度时仍有效。

Conclusion: 有策略、注重信息的推广能提升长期模型性能和有机结果，优于单纯最大化曝光策略。

Abstract: Modern content platforms offer paid promotion to mitigate cold start by allocating exposure via auctions. Our empirical analysis reveals a counterintuitive flaw in this paradigm: while promotion rescues low-to-medium quality content, it can harm high-quality content by forcing exposure to suboptimal audiences, polluting engagement signals and downgrading future recommendation. We recast content promotion as a dual-objective optimization that balances short-term value acquisition with long-term model improvement. To make this tractable at bid time in content promotion, we introduce a decomposable surrogate objective, gradient coverage, and establish its formal connection to Fisher Information and optimal experimental design. We design a two-stage auto-bidding algorithm based on Lagrange duality that dynamically paces budget through a shadow price and optimizes impression-level bids using per-impression marginal utilities. To address missing labels at bid time, we propose a confidence-gated gradient heuristic, paired with a zeroth-order variant for black-box models that reliably estimates learning signals in real time. We provide theoretical guarantees, proving monotone submodularity of the composite objective, sublinear regret in online auction, and budget feasibility. Extensive offline experiments on synthetic and real-world datasets validate the framework: it outperforms baselines, achieves superior final AUC/LogLoss, adheres closely to budget targets, and remains effective when gradients are approximated zeroth-order. These results show that strategic, information-aware promotion can improve long-term model performance and organic outcomes beyond naive impression-maximization strategies.

</details>


### [50] [Inequality in Congestion Games with Learning Agents](https://arxiv.org/abs/2601.20578)
*Dimitris Michailidis,Sennay Ghebreab,Fernando P. Santos*

Main category: cs.GT

TL;DR: 研究交通网络扩张受益者，建模通勤者学习过程，引入PoL衡量指标，模拟发现扩张会增效率也放大不平等，政策需考虑通勤者适应差异。


<details>
  <summary>Details</summary>
Motivation: 探究交通网络扩张中谁受益及产生不平等的原因。

Method: 将通勤者建模为不同学习率的强化学习代理，引入PoL衡量学习低效，分析简化网络和真实地铁系统。

Result: 网络扩张会同时提高效率并放大不平等，快速学习者在他人适应前从新路线获益更多。

Conclusion: 交通政策不仅要考虑均衡结果，还要考虑通勤者的异质适应方式。

Abstract: Who benefits from expanding transport networks? While designed to improve mobility, such interventions can also create inequality. In this paper, we show that disparities arise not only from the structure of the network itself but also from differences in how commuters adapt to it. We model commuters as reinforcement learning agents who adapt their travel choices at different learning rates, reflecting unequal access to resources and information. To capture potential efficiency-fairness tradeoffs, we introduce the Price of Learning (PoL), a measure of inefficiency during learning. We analyze both a stylized network -- inspired in the well-known Braess's paradox, yet with two-source nodes -- and an abstraction of a real-world metro system (Amsterdam). Our simulations show that network expansions can simultaneously increase efficiency and amplify inequality, especially when faster learners disproportionately benefit from new routes before others adapt. These results highlight that transport policies must account not only for equilibrium outcomes but also for the heterogeneous ways commuters adapt, since both shape the balance between efficiency and fairness.

</details>


### [51] [Independence of Approximate Clones](https://arxiv.org/abs/2601.20779)
*Théo Delemazure*

Main category: cs.GT

TL;DR: 研究序数选举中近似克隆候选人的概念，探讨量化方法，分析已知独立于完美克隆的投票规则对近似克隆的独立性，通过实证研究发现近似克隆在某些场景常见且越接近完美克隆移除越不易改变结果。


<details>
  <summary>Details</summary>
Motivation: 完美克隆在实际选举中不太可能出现，因此研究近似克隆在序数选举中的情况。

Method: 讨论两种量化近似克隆接近程度的方法，分析已知独立于完美克隆的投票规则在不同条件下对近似克隆的独立性，进行基于三个真实数据集的实证研究。

Result: 对于至少四个候选人的选举，已知规则通常不独立于近似克隆；三个候选人时有更积极结果；近似克隆在一些场景常见，越接近完美克隆移除改变选举结果可能性越低。

Conclusion: 近似克隆在实际选举中有研究价值，已知独立于完美克隆的投票规则在近似克隆情况下有不同表现，且近似克隆接近完美克隆程度与移除影响相关。

Abstract: In an ordinal election, two candidates are said to be perfect clones if every voter ranks them adjacently. The independence of clones axiom then states that removing one of the two clones should not change the election outcome. This axiom has been extensively studied in social choice theory, and several voting rules are known to satisfy it (such as IRV, Ranked Pairs and Schulze). However, perfect clones are unlikely to occur in practice, especially for political elections with many voters.
  In this work, we study different notions of approximate clones in ordinal elections. Informally, two candidates are approximate clones in a preference profile if they are close to being perfect clones. We discuss two measures to quantify this proximity, and we show under which conditions the voting rules that are known to be independent of clones are also independent of approximate clones. In particular, we show that for elections with at least four candidates, none of these rules are independent of approximate clones in the general case. However, we find a more positive result for the case of three candidates. Finally, we conduct an empirical study of approximate clones and independence of approximate clones based on three real-world datasets: votes in local Scottish elections, votes in mini-jury deliberations, and votes of judges in figure skating competitions. We find that approximate clones are common in some contexts, and that the closest two candidates are to being perfect clones, the less likely their removal is to change the election outcome, especially for voting rules that are independent of perfect clones.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [52] [LLaTTE: Scaling Laws for Multi-Stage Sequence Modeling in Large-Scale Ads Recommendation](https://arxiv.org/abs/2601.20083)
*Lee Xiong,Zhirong Chen,Rahul Mayuranath,Shangran Qiu,Arda Ozdemir,Lu Li,Yang Hu,Dave Li,Jingtao Ren,Howard Cheng,Fabian Souto Herrera,Ahmed Agiza,Baruch Epshtein,Anuj Aggarwal,Julia Ulziisaikhan,Chao Wang,Dinesh Ramasamy,Parshva Doshi,Sri Reddy,Arnold Overwijk*

Main category: cs.IR

TL;DR: 提出LLaTTE用于生产广告推荐，展示推荐系统序列建模有类似大语言模型的幂律缩放，引入两阶段架构，在Meta部署获4.3%转化提升。


<details>
  <summary>Details</summary>
Motivation: 解决在严格延迟约束下实现推荐系统持续缩放以提升性能的问题。

Method: 提出两阶段架构LLaTTE，将大模型的繁重计算卸载到异步上游用户模型。

Result: 部署为Meta最大用户模型，在Facebook Feed和Reels上实现4.3%的转化提升，且服务开销最小。

Conclusion: 为工业推荐系统利用缩放定律提供了实用蓝图。

Abstract: We present LLaTTE (LLM-Style Latent Transformers for Temporal Events), a scalable transformer architecture for production ads recommendation. Through systematic experiments, we demonstrate that sequence modeling in recommendation systems follows predictable power-law scaling similar to LLMs. Crucially, we find that semantic features bend the scaling curve: they are a prerequisite for scaling, enabling the model to effectively utilize the capacity of deeper and longer architectures. To realize the benefits of continued scaling under strict latency constraints, we introduce a two-stage architecture that offloads the heavy computation of large, long-context models to an asynchronous upstream user model. We demonstrate that upstream improvements transfer predictably to downstream ranking tasks. Deployed as the largest user model at Meta, this multi-stage framework drives a 4.3\% conversion uplift on Facebook Feed and Reels with minimal serving overhead, establishing a practical blueprint for harnessing scaling laws in industrial recommender systems.

</details>


### [53] [IMRNNs: An Efficient Method for Interpretable Dense Retrieval via Embedding Modulation](https://arxiv.org/abs/2601.20084)
*Yash Saxena,Ankur Padia,Kalpa Gunaratna,Manas Gaur*

Main category: cs.IR

TL;DR: 提出IMRNNs框架提升黑盒稠密检索器可解释性与检索效果。


<details>
  <summary>Details</summary>
Motivation: 现存稠密检索器依赖静态嵌入，后处理方法有局限，需解决可解释性问题。

Method: 提出IMRNNs框架，使用两个独立适配器进行动态双向调制。

Result: 在七个基准数据集上，相比基线，nDCG、recall、MRR有显著提升。

Conclusion: 引入可解释性驱动调制可解释并增强检索增强生成系统的检索能力。

Abstract: Interpretability in black-box dense retrievers remains a central challenge in Retrieval-Augmented Generation (RAG). Understanding how queries and documents semantically interact is critical for diagnosing retrieval behavior and improving model design. However, existing dense retrievers rely on static embeddings for both queries and documents, which obscures this bidirectional relationship. Post-hoc approaches such as re-rankers are computationally expensive, add inference latency, and still fail to reveal the underlying semantic alignment. To address these limitations, we propose Interpretable Modular Retrieval Neural Networks (IMRNNs), a lightweight framework that augments any dense retriever with dynamic, bidirectional modulation at inference time. IMRNNs employ two independent adapters: one conditions document embeddings on the current query, while the other refines the query embedding using corpus-level feedback from initially retrieved documents. This iterative modulation process enables the model to adapt representations dynamically and expose interpretable semantic dependencies between queries and documents. Empirically, IMRNNs not only enhance interpretability but also improve retrieval effectiveness. Across seven benchmark datasets, applying our method to standard dense retrievers yields average gains of +6.35% nDCG, +7.14% recall, and +7.04% MRR over state-of-the-art baselines. These results demonstrate that incorporating interpretability-driven modulation can both explain and enhance retrieval in RAG systems.

</details>


### [54] [Taxonomy of the Retrieval System Framework: Pitfalls and Paradigms](https://arxiv.org/abs/2601.20131)
*Deep Shah,Sanket Badhe,Nehal Kathrotia*

Main category: cs.IR

TL;DR: 本文将嵌入检索系统设计决策结构化，按系统设计栈分层分析，提供优化效率与效果平衡的框架。


<details>
  <summary>Details</summary>
Motivation: 设计嵌入检索系统需平衡效率与效果，存在复杂权衡，需结构化决策。

Method: 按系统设计栈垂直遍历，依次分析表示层、粒度层、编排层和鲁棒性层。

Result: 对各层的局限性和设计选择进行了分类。

Conclusion: 为从业者提供了优化现代神经搜索系统效率 - 效果边界的综合框架。

Abstract: Designing an embedding retrieval system requires navigating a complex design space of conflicting trade-offs between efficiency and effectiveness. This work structures these decisions as a vertical traversal of the system design stack. We begin with the Representation Layer by examining how loss functions and architectures, specifically Bi-encoders and Cross-encoders, define semantic relevance and geometric projection. Next, we analyze the Granularity Layer and evaluate how segmentation strategies like Atomic and Hierarchical chunking mitigate information bottlenecks in long-context documents. Moving to the Orchestration Layer, we discuss methods that transcend the single-vector paradigm, including hierarchical retrieval, agentic decomposition, and multi-stage reranking pipelines to resolve capacity limitations. Finally, we address the Robustness Layer by identifying architectural mitigations for domain generalization failures, lexical blind spots, and the silent degradation of retrieval quality due to temporal drift. By categorizing these limitations and design choices, we provide a comprehensive framework for practitioners to optimize the efficiency-effectiveness frontier in modern neural search systems.

</details>


### [55] [MERGE: Next-Generation Item Indexing Paradigm for Large-Scale Streaming Recommendation](https://arxiv.org/abs/2601.20199)
*Jing Yan,Yimeng Bai,Zongyu Liu,Yahui Liu,Junwei Wang,Jingze Huang,Haoda Li,Sihao Ding,Shaohui Ruan,Yang Zhang*

Main category: cs.IR

TL;DR: 提出MERGE索引范式解决现有VQ方法在推荐系统物品索引中的问题，实验和A/B测试效果好。


<details>
  <summary>Details</summary>
Motivation: 现有基于VQ的物品索引方法在处理流式行业推荐系统中高度偏斜和非平稳的物品分布时存在分配精度低、聚类占用不均衡和聚类分离不足等问题。

Method: 提出MERGE范式，从无到有自适应构建聚类，动态监控聚类占用情况，并通过从细到粗的合并形成分层索引结构。

Result: 实验表明MERGE在分配精度、聚类均匀性和聚类分离方面显著优于现有索引方法，在线A/B测试显示关键业务指标有显著提升。

Conclusion: MERGE有潜力成为大规模推荐的基础索引方法。

Abstract: Item indexing, which maps a large corpus of items into compact discrete representations, is critical for both discriminative and generative recommender systems, yet existing Vector Quantization (VQ)-based approaches struggle with the highly skewed and non-stationary item distributions common in streaming industry recommenders, leading to poor assignment accuracy, imbalanced cluster occupancy, and insufficient cluster separation. To address these challenges, we propose MERGE, a next-generation item indexing paradigm that adaptively constructs clusters from scratch, dynamically monitors cluster occupancy, and forms hierarchical index structures via fine-to-coarse merging. Extensive experiments demonstrate that MERGE significantly improves assignment accuracy, cluster uniformity, and cluster separation compared with existing indexing methods, while online A/B tests show substantial gains in key business metrics, highlighting its potential as a foundational indexing approach for large-scale recommendation.

</details>


### [56] [Towards End-to-End Alignment of User Satisfaction via Questionnaire in Video Recommendation](https://arxiv.org/abs/2601.20215)
*Na Li,Jiaqi Yu,Minzhi Xie,Tiantian He,Xiaoxiao Xu,Zixiu Wang,Lantao Hu,Yongqi Liu,Han Li,Kaiqiao Zhan,Kun Gai*

Main category: cs.IR

TL;DR: 提出EASQ框架解决短视推荐中用户满意度对齐问题，经实验和A/B测试验证有效且已部署获商业收益。


<details>
  <summary>Details</summary>
Motivation: 短视推荐用密集行为信号优化排名模型有噪声和偏差，问卷显式满意度反馈虽优质但稀疏难融入在线模型。

Method: 构建独立参数路径处理问卷信号，用多任务架构分离监督信号，LoRA模块预注入偏好；采用DPO优化目标实现端到端在线学习。

Result: 离线实验和在线A/B测试表明EASQ在多场景提升用户满意度指标。

Conclusion: EASQ能有效将排名模型与真实用户满意度实时对齐，已成功部署获稳定商业收益。

Abstract: Short-video recommender systems typically optimize ranking models using dense user behavioral signals, such as clicks and watch time. However, these signals are only indirect proxies of user satisfaction and often suffer from noise and bias. Recently, explicit satisfaction feedback collected through questionnaires has emerged as a high-quality direct alignment supervision, but is extremely sparse and easily overwhelmed by abundant behavioral data, making it difficult to incorporate into online recommendation models. To address these challenges, we propose a novel framework which is towards End-to-End Alignment of user Satisfaction via Questionaire, named EASQ, to enable real-time alignment of ranking models with true user satisfaction. Specifically, we first construct an independent parameter pathway for sparse questionnaire signals by combining a multi-task architecture and a lightweight LoRA module. The multi-task design separates sparse satisfaction supervision from dense behavioral signals, preventing the former from being overwhelmed. The LoRA module pre-inject these preferences in a parameter-isolated manner, ensuring stability in the backbone while optimizing user satisfaction. Furthermore, we employ a DPO-based optimization objective tailored for online learning, which aligns the main model outputs with sparse satisfaction signals in real time. This design enables end-to-end online learning, allowing the model to continuously adapt to new questionnaire feedback while maintaining the stability and effectiveness of the backbone. Extensive offline experiments and large-scale online A/B tests demonstrate that EASQ consistently improves user satisfaction metrics across multiple scenarios. EASQ has been successfully deployed in a production short-video recommendation system, delivering significant and stable business gains.

</details>


### [57] [MALLOC: Benchmarking the Memory-aware Long Sequence Compression for Large Sequential Recommendation](https://arxiv.org/abs/2601.20234)
*Qihang Yu,Kairui Fu,Zhaocheng Du,Yuxuan Si,Kaiyuan Li,Weihao Zhao,Zhicheng Zhang,Jieming Zhu,Quanyu Dai,Zhenhua Dong,Shengyu Zhang,Kun Kuang,Fei Wu*

Main category: cs.IR

TL;DR: 现有推荐模型扩展有计算成本高和内存开销大问题，提出MALLOC基准，综合评估内存管理技术，实验证明其可靠性。


<details>
  <summary>Details</summary>
Motivation: 大规模推荐器计算成本高，现有方法未充分考虑内存开销，LLM的内存管理策略未在推荐任务评估。

Method: 引入MALLOC基准，对适用于大序列推荐的内存管理技术进行全面研究和系统分类，并集成到先进推荐器，搭建可复现评估平台。

Result: 通过准确性、效率和复杂性的大量实验，证明MALLOC的整体可靠性。

Conclusion: MALLOC有助于推进大规模推荐，代码开源。

Abstract: The scaling law, which indicates that model performance improves with increasing dataset and model capacity, has fueled a growing trend in expanding recommendation models in both industry and academia. However, the advent of large-scale recommenders also brings significantly higher computational costs, particularly under the long-sequence dependencies inherent in the user intent of recommendation systems. Current approaches often rely on pre-storing the intermediate states of the past behavior for each user, thereby reducing the quadratic re-computation cost for the following requests. Despite their effectiveness, these methods often treat memory merely as a medium for acceleration, without adequately considering the space overhead it introduces. This presents a critical challenge in real-world recommendation systems with billions of users, each of whom might initiate thousands of interactions and require massive memory for state storage. Fortunately, there have been several memory management strategies examined for compression in LLM, while most have not been evaluated on the recommendation task. To mitigate this gap, we introduce MALLOC, a comprehensive benchmark for memory-aware long sequence compression. MALLOC presents a comprehensive investigation and systematic classification of memory management techniques applicable to large sequential recommendations. These techniques are integrated into state-of-the-art recommenders, enabling a reproducible and accessible evaluation platform. Through extensive experiments across accuracy, efficiency, and complexity, we demonstrate the holistic reliability of MALLOC in advancing large-scale recommendation. Code is available at https://anonymous.4open.science/r/MALLOC.

</details>


### [58] [One Word is Enough: Minimal Adversarial Perturbations for Neural Text Ranking](https://arxiv.org/abs/2601.20283)
*Tanmay Karmakar,Sourav Saha,Debapriyo Majumdar,Surjyanee Halder*

Main category: cs.IR

TL;DR: 研究一种单字查询感知攻击来评估神经排序模型（NRMs）鲁棒性，分析攻击敏感性，揭示文档排名脆弱区间。


<details>
  <summary>Details</summary>
Motivation: NRMs虽检索效果好，但易受对抗性扰动影响，需重新审视其鲁棒性。

Method: 采用最小化、查询感知的单字攻击，研究启发式和梯度引导变体，包括识别有影响插入点的白盒方法，还引入新诊断指标。

Result: 在TREC - DL 2019/2020数据集上，单字攻击成功率达91%，平均每份文档修改不到两个标记，且在白盒设置下有竞争力，揭示了中排名文档最易受攻击。

Conclusion: 研究表明NRMs存在实际风险，为未来的鲁棒防御提供了动力。

Abstract: Neural ranking models (NRMs) achieve strong retrieval effectiveness, yet prior work has shown they are vulnerable to adversarial perturbations. We revisit this robustness question with a minimal, query-aware attack that promotes a target document by inserting or substituting a single, semantically aligned word - the query center. We study heuristic and gradient-guided variants, including a white-box method that identifies influential insertion points. On TREC-DL 2019/2020 with BERT and monoT5 re-rankers, our single-word attacks achieve up to 91% success while modifying fewer than two tokens per document on average, achieving competitive rank and score boosts with far fewer edits under a comparable white-box setup to ensure fair evaluation against PRADA. We also introduce new diagnostic metrics to analyze attack sensitivity beyond aggregate success rates. Our analysis reveals a Goldilocks zone in which mid-ranked documents are most vulnerable. These findings demonstrate practical risks and motivate future defenses for robust neural ranking.

</details>


### [59] [Less is More: Benchmarking LLM Based Recommendation Agents](https://arxiv.org/abs/2601.20316)
*Kargi Chauhan,Mahalakshmi Venkateswarlu*

Main category: cs.IR

TL;DR: 本文通过对四种大语言模型在不同上下文长度下进行基准测试，挑战了“更长的用户购买历史能带来更好预测”的假设，发现增加上下文长度对推荐质量无显著提升，还可降低推理成本并提供部署建议。


<details>
  <summary>Details</summary>
Motivation: 质疑在大语言模型用于个性化产品推荐时，从业者普遍认为的更长用户购买历史能带来更好预测这一假设。

Method: 使用REGEN数据集，对GPT - 4o - mini、DeepSeek - V3、Qwen2.5 - 72B和Gemini 2.5 Flash四种大语言模型，在5到50个物品的上下文长度范围内进行系统基准测试。

Result: 对50名用户的实验显示，增加上下文长度对推荐质量无显著提升，质量得分在所有条件下保持平稳（0.17 - 0.23）。可通过使用5 - 10个物品的上下文，将推理成本降低约88%，同时不牺牲推荐质量。还分析了不同供应商的延迟模式。

Conclusion: 挑战了“更多上下文更好”的范式，为基于大语言模型的推荐系统提供了具有可操作性的低成本指南。

Abstract: Large Language Models (LLMs) are increasingly deployed for personalized product recommendations, with practitioners commonly assuming that longer user purchase histories lead to better predictions. We challenge this assumption through a systematic benchmark of four state of the art LLMs GPT-4o-mini, DeepSeek-V3, Qwen2.5-72B, and Gemini 2.5 Flash across context lengths ranging from 5 to 50 items using the REGEN dataset.
  Surprisingly, our experiments with 50 users in a within subject design reveal no significant quality improvement with increased context length. Quality scores remain flat across all conditions (0.17--0.23). Our findings have significant practical implications: practitioners can reduce inference costs by approximately 88\% by using context (5--10 items) instead of longer histories (50 items), without sacrificing recommendation quality. We also analyze latency patterns across providers and find model specific behaviors that inform deployment decisions. This work challenges the existing ``more context is better'' paradigm and provides actionable guidelines for cost effective LLM based recommendation systems.

</details>


### [60] [Eliminating Hallucination in Diffusion-Augmented Interactive Text-to-Image Retrieval](https://arxiv.org/abs/2601.20391)
*Zhuocheng Zhang,Kangheng Liang,Guanxuan Li,Paul Henderson,Richard Mccreadie,Zijun Long*

Main category: cs.IR

TL;DR: 提出用于DAI - TIR的抗幻觉训练框架DMCL，在五个基准测试中提升性能。


<details>
  <summary>Details</summary>
Motivation: DAI - TIR中扩散生成可能引入与查询文本冲突的幻觉视觉线索，降低检索性能。

Method: 提出DMCL，引入语义一致性和扩散感知对比目标，将DAI - TIR作为对查询意图和目标图像表示的联合优化。

Result: 在五个标准基准测试中，DMCL使多轮Hits@10持续提升，最高比之前微调及零样本基线高7.37%。

Conclusion: DMCL是用于DAI - TIR的通用且鲁棒的训练框架。

Abstract: Diffusion-Augmented Interactive Text-to-Image Retrieval (DAI-TIR) is a promising paradigm that improves retrieval performance by generating query images via diffusion models and using them as additional ``views'' of the user's intent. However, these generative views can be incorrect because diffusion generation may introduce hallucinated visual cues that conflict with the original query text. Indeed, we empirically demonstrate that these hallucinated cues can substantially degrade DAI-TIR performance. To address this, we propose Diffusion-aware Multi-view Contrastive Learning (DMCL), a hallucination-robust training framework that casts DAI-TIR as joint optimization over representations of query intent and the target image. DMCL introduces semantic-consistency and diffusion-aware contrastive objectives to align textual and diffusion-generated query views while suppressing hallucinated query signals. This yields an encoder that acts as a semantic filter, effectively mapping hallucinated cues into a null space, improving robustness to spurious cues and better representing the user's intent. Attention visualization and geometric embedding-space analyses corroborate this filtering behavior. Across five standard benchmarks, DMCL delivers consistent improvements in multi-round Hits@10, reaching as high as 7.37\% over prior fine-tuned and zero-shot baselines, which indicates it is a general and robust training framework for DAI-TIR.

</details>


### [61] [When Vision Meets Texts in Listwise Reranking](https://arxiv.org/abs/2601.20623)
*Hongyi Cai*

Main category: cs.IR

TL;DR: 提出Rank - Nexus图像文本重排序器，用渐进跨模态训练策略克服模态差距和数据稀缺问题，在多基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前图像 - 文本文档有效重排序因模态差距和对齐数据集稀缺而具有挑战性，现有方法依赖大模型且有计算开销大、侧重文本模态等问题。

Method: 提出Rank - Nexus重排序器，采用渐进跨模态训练策略，先分别训练文本和图像模态，再提炼联合重排序数据集。

Result: 在多个文本和图像重排序基准测试中取得优异表现。

Conclusion: Rank - Nexus设计高效，在不过多参数和推理开销的情况下，能在不同多模态场景中广泛适用。

Abstract: Recent advancements in information retrieval have highlighted the potential of integrating visual and textual information, yet effective reranking for image-text documents remains challenging due to the modality gap and scarcity of aligned datasets. Meanwhile, existing approaches often rely on large models (7B to 32B parameters) with reasoning-based distillation, incurring unnecessary computational overhead while primarily focusing on textual modalities. In this paper, we propose Rank-Nexus, a multimodal image-text document reranker that performs listwise qualitative reranking on retrieved lists incorporating both images and texts. To bridge the modality gap, we introduce a progressive cross-modal training strategy. We first train modalities separately: leveraging abundant text reranking data, we distill knowledge into the text branch. For images, where data is scarce, we construct distilled pairs from multimodal large language model (MLLM) captions on image retrieval benchmarks. Subsequently, we distill a joint image-text reranking dataset. Rank-Nexus achieves outstanding performance on text reranking benchmarks (TREC, BEIR) and the challenging image reranking benchmark (INQUIRE, MMDocIR), using only a lightweight 2B pretrained visual-language model. This efficient design ensures strong generalization across diverse multimodal scenarios without excessive parameters or reasoning overhead.

</details>


### [62] [Overview of the TREC 2025 Tip-of-the-Tongue track](https://arxiv.org/abs/2601.20671)
*Jaime Arguello,Fernando Diaz,Maik Fröebe,To Eun Kim,Bhaskar Mitra*

Main category: cs.IR

TL;DR: 2025年TREC的ToT轨道扩展至通用领域并纳入不同来源测试查询，9组提交32次运行


<details>
  <summary>Details</summary>
Motivation: 现有信息检索系统难以处理Tip - of - the - tongue信息请求

Method: 将TREC 2025 ToT轨道扩展到通用领域，纳入来自MS - ToT数据集、手动主题开发和基于大语言模型合成查询生成的不同测试查询集

Result: 9组（包括轨道协调员）提交32次运行

Conclusion: 未提及明确结论

Abstract: Tip-of-the-tongue (ToT) known-item retrieval involves re-finding an item for which the searcher does not reliably recall an identifier. ToT information requests (or queries) are verbose and tend to include several complex phenomena, making them especially difficult for existing information retrieval systems. The TREC 2025 ToT track focused on a single ad-hoc retrieval task. This year, we extended the track to general domain and incorporated different sets of test queries from diverse sources, namely from the MS-ToT dataset, manual topic development, and LLM-based synthetic query generation. This year, 9 groups (including the track coordinators) submitted 32 runs.

</details>


### [63] [MedViz: An Agent-based, Visual-guided Research Assistant for Navigating Biomedical Literature](https://arxiv.org/abs/2601.20709)
*Huan He,Xueqing Peng,Yutong Xie,Qijia Liu,Chia-Hsuan Chang,Lingfei Qian,Brian Ondov,Qiaozhu Mei,Hua Xu*

Main category: cs.IR

TL;DR: 介绍了可视化分析系统MedViz，结合多AI代理与交互式可视化支持生物医学文献探索，加速知识发现。


<details>
  <summary>Details</summary>
Motivation: 生物医学研究人员在浏览海量文献时有挑战，传统搜索引擎支持不足，现有生成式AI与文献搜索工作流集成不佳。

Method: 引入MedViz系统，将数百万文章语义图与代理驱动功能结合，支持查询、总结和假设生成。

Result: 未明确提及具体结果表现，但指出MedViz可将生物医学文献搜索变成动态探索过程。

Conclusion: MedViz通过融合智能代理与交互式可视化，加速生物医学领域的知识发现。

Abstract: Biomedical researchers face increasing challenges in navigating millions of publications in diverse domains. Traditional search engines typically return articles as ranked text lists, offering little support for global exploration or in-depth analysis. Although recent advances in generative AI and large language models have shown promise in tasks such as summarization, extraction, and question answering, their dialog-based implementations are poorly integrated with literature search workflows. To address this gap, we introduce MedViz, a visual analytics system that integrates multiple AI agents with interactive visualization to support the exploration of the large-scale biomedical literature. MedViz combines a semantic map of millions of articles with agent-driven functions for querying, summarizing, and hypothesis generation, allowing researchers to iteratively refine questions, identify trends, and uncover hidden connections. By bridging intelligent agents with interactive visualization, MedViz transforms biomedical literature search into a dynamic, exploratory process that accelerates knowledge discovery.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [64] [Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data](https://arxiv.org/abs/2601.19936)
*Minseo Kwak,Jaehyung Kim*

Main category: cs.LG

TL;DR: 针对大语言模型预训练数据检测问题，提出Gap - K%方法，实验显示其性能达最优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练语料不透明引发隐私和版权问题，现有预训练数据检测方法有不足。

Method: 基于大语言模型预训练优化动态提出Gap - K%方法，利用top - 1预测和目标token的对数概率差距，结合滑动窗口策略。

Result: 在WikiMIA和MIMIR基准测试中，Gap - K%在不同模型大小和输入长度上始终优于先前基线。

Conclusion: Gap - K%方法在预训练数据检测上达到了最优性能。

Abstract: The opacity of massive pretraining corpora in Large Language Models (LLMs) raises significant privacy and copyright concerns, making pretraining data detection a critical challenge. Existing state-of-the-art methods typically rely on token likelihoods, yet they often overlook the divergence from the model's top-1 prediction and local correlation between adjacent tokens. In this work, we propose Gap-K%, a novel pretraining data detection method grounded in the optimization dynamics of LLM pretraining. By analyzing the next-token prediction objective, we observe that discrepancies between the model's top-1 prediction and the target token induce strong gradient signals, which are explicitly penalized during training. Motivated by this, Gap-K% leverages the log probability gap between the top-1 predicted token and the target token, incorporating a sliding window strategy to capture local correlations and mitigate token-level fluctuations. Extensive experiments on the WikiMIA and MIMIR benchmarks demonstrate that Gap-K% achieves state-of-the-art performance, consistently outperforming prior baselines across various model sizes and input lengths.

</details>


### [65] [Emergent Specialization in Learner Populations: Competition as the Source of Diversity](https://arxiv.org/abs/2601.19943)
*Yuhao Li*

Main category: cs.LG

TL;DR: 研究表明仅靠竞争就能使学习者群体出现自发专业化，介绍 NichePopulation 算法，在六个领域验证效果良好，有多项关键发现。


<details>
  <summary>Details</summary>
Motivation: 探究在无明确沟通和多样性激励的情况下，学习者群体如何发展出协调、多样的行为。

Method: 引入 NichePopulation 算法，结合竞争排斥和生态位亲和跟踪。

Result: 在六个真实领域验证，平均专业化指数达 0.75，效果显著；在无生态位奖励时 SI 仍大于 0.3；多样化群体表现优于同质性基线 26.5%；该方法比 MARL 基线快 4 倍且效果好 4.3 倍。

Conclusion: 仅竞争就足以诱导学习者群体出现自发专业化，该算法效果良好。

Abstract: How can populations of learners develop coordinated, diverse behaviors without explicit communication or diversity incentives? We demonstrate that competition alone is sufficient to induce emergent specialization -- learners spontaneously partition into specialists for different environmental regimes through competitive dynamics, consistent with ecological niche theory. We introduce the NichePopulation algorithm, a simple mechanism combining competitive exclusion with niche affinity tracking. Validated across six real-world domains (cryptocurrency trading, commodity prices, weather forecasting, solar irradiance, urban traffic, and air quality), our approach achieves a mean Specialization Index of 0.75 with effect sizes of Cohen's d > 20. Key findings: (1) At lambda=0 (no niche bonus), learners still achieve SI > 0.30, proving specialization is genuinely emergent; (2) Diverse populations outperform homogeneous baselines by +26.5% through method-level division of labor; (3) Our approach outperforms MARL baselines (QMIX, MAPPO, IQL) by 4.3x while being 4x faster.

</details>


### [66] [DecHW: Heterogeneous Decentralized Federated Learning Exploiting Second-Order Information](https://arxiv.org/abs/2601.19938)
*Adnan Ahmad,Chiara Boldrini,Lorenzo Valerio,Andrea Passarella,Marco Conti*

Main category: cs.LG

TL;DR: 本文针对去中心化联邦学习中的数据和模型异质性问题，提出一种新的聚合方法，在计算机视觉任务实验中展示出良好泛化性和低通信成本。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习中，设备间的数据和模型初始化存在异质性，导致局部模型参数有差异，收敛速度慢，需解决此问题。

Method: 引入新的聚合方法，通过近似局部模型在本地数据集上的二阶信息生成共识权重，用于缩放邻域更新并聚合为全局邻域表示。

Result: 在计算机视觉任务的大量实验中，该方法降低了通信成本，且局部模型具有强泛化性。

Conclusion: 提出的方法能有效解决去中心化联邦学习中的数据和模型异质性问题。

Abstract: Decentralized Federated Learning (DFL) is a serverless collaborative machine learning paradigm where devices collaborate directly with neighbouring devices to exchange model information for learning a generalized model. However, variations in individual experiences and different levels of device interactions lead to data and model initialization heterogeneities across devices. Such heterogeneities leave variations in local model parameters across devices that leads to slower convergence. This paper tackles the data and model heterogeneity by explicitly addressing the parameter level varying evidential credence across local models. A novel aggregation approach is introduced that captures these parameter variations in local models and performs robust aggregation of neighbourhood local updates. Specifically, consensus weights are generated via approximation of second-order information of local models on their local datasets. These weights are utilized to scale neighbourhood updates before aggregating them into global neighbourhood representation. In extensive experiments with computer vision tasks, the proposed approach shows strong generalizability of local models at reduced communication costs.

</details>


### [67] [oculomix: Hierarchical Sampling for Retinal-Based Systemic Disease Prediction](https://arxiv.org/abs/2601.19939)
*Hyunmin Kim,Yukun Zhou,Rahul A. Jonas,Lie Ju,Sunjin Hwang,Pearse A. Keane,Siegfried K. Wagner*

Main category: cs.LG

TL;DR: 提出分层采样策略Oculomix用于混合样本增强，在预测主要不良心血管事件中表现优于CutMix和MixUp。


<details>
  <summary>Details</summary>
Motivation: 现有图像级混合样本增强技术会扰乱患者特定属性，需改进。

Method: 基于两个临床先验提出Oculomix，在患者和检查层面约束混合空间。

Result: 在大型多样人群五年主要不良心血管事件预测中，Oculomix的AUROC比CutMix和MixUp高3%。

Conclusion: Oculomix在眼科学中有必要性和价值。

Abstract: Oculomics - the concept of predicting systemic diseases, such as cardiovascular disease and dementia, through retinal imaging - has advanced rapidly due to the data efficiency of transformer-based foundation models like RETFound. Image-level mixed sample data augmentations, such as CutMix and MixUp, are frequently used for training transformers, yet these techniques perturb patient-specific attributes, such as medical comorbidity and clinical factors, since they only account for images and labels. To address this limitation, we propose a hierarchical sampling strategy, Oculomix, for mixed sample augmentations. Our method is based on two clinical priors. First (exam level), images acquired from the same patient at the same time point share the same attributes. Second (patient level), images acquired from the same patient at different time points have a soft temporal trend, as morbidity generally increases over time. Guided by these priors, our method constrains the mixing space to the patient and exam levels to better preserve patient-specific characteristics and leverages their hierarchical relationships. The proposed method is validated using ViT models on a five-year prediction of major adverse cardiovascular events (MACE) in a large ethnically diverse population (Alzeye). We show that Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% in AUROC, demonstrating the necessity and value of the proposed method in oculomics.

</details>


### [68] [Cheap2Rich: A Multi-Fidelity Framework for Data Assimilation and System Identification of Multiscale Physics -- Rotating Detonation Engines](https://arxiv.org/abs/2601.20295)
*Yuxuan Bao,Jan Zajac,Megan Powers,Venkat Raman,J. Nathan Kutz*

Main category: cs.LG

TL;DR: 提出Cheap2Rich多尺度数据同化框架，在旋转爆震发动机上验证其能从稀疏测量重建高保真状态并隔离有物理意义的差异动态。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习应用于工程问题时，计算成本低的模型与复杂物理系统间的sim2real差距问题，特别是多尺度场景中降阶模型仅能捕捉主导动态的问题。

Method: 提出Cheap2Rich框架，结合快速低保真先验与学习到的可解释差异修正，从稀疏传感器历史重建高保真状态空间。

Result: 在旋转爆震发动机上成功从稀疏测量重建高保真状态，隔离出与喷油器驱动效应相关的有物理意义的差异动态。

Conclusion: 该框架是复杂多尺度系统数据同化和系统识别的通用多保真框架，可实现快速设计探索、实时监测和控制，并提供可解释的差异动态。

Abstract: Bridging the sim2real gap between computationally inexpensive models and complex physical systems remains a central challenge in machine learning applications to engineering problems, particularly in multi-scale settings where reduced-order models typically capture only dominant dynamics. In this work, we present Cheap2Rich, a multi-scale data assimilation framework that reconstructs high-fidelity state spaces from sparse sensor histories by combining a fast low-fidelity prior with learned, interpretable discrepancy corrections. We demonstrate the performance on rotating detonation engines (RDEs), a challenging class of systems that couple detonation-front propagation with injector-driven unsteadiness, mixing, and stiff chemistry across disparate scales. Our approach successfully reconstructs high-fidelity RDE states from sparse measurements while isolating physically meaningful discrepancy dynamics associated with injector-driven effects. The results highlight a general multi-fidelity framework for data assimilation and system identification in complex multi-scale systems, enabling rapid design exploration and real-time monitoring and control while providing interpretable discrepancy dynamics. Code for this project is is available at: github.com/kro0l1k/Cheap2Rich.

</details>


### [69] [Continuous-Flow Data-Rate-Aware CNN Inference on FPGA](https://arxiv.org/abs/2601.19940)
*Tobias Habermann,Michael Mecik,Zhenyu Wang,César David Vera,Martin Kumm,Mario Garrido*

Main category: cs.LG

TL;DR: 本文分析CNN数据流，提出设计数据速率感知、连续流CNN架构的新方法，可提高硬件利用率，节省算术逻辑，实现复杂CNN在单FPGA上的高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 以往CNN全并行实现中，数据减少导致硬件单元利用率低，需解决此问题。

Method: 分析CNN数据流，通过交错低数据速率信号、共享硬件单元和合理并行化设计新架构。

Result: 可节省大量算术逻辑，能在单FPGA上以高吞吐量实现像MobileNet这样的复杂CNN。

Conclusion: 所提方法能提高硬件利用率，实现复杂CNN在单FPGA上的高吞吐量。

Abstract: Among hardware accelerators for deep-learning inference, data flow implementations offer low latency and high throughput capabilities. In these architectures, each neuron is mapped to a dedicated hardware unit, making them well-suited for field-programmable gate array (FPGA) implementation. Previous unrolled implementations mostly focus on fully connected networks because of their simplicity, although it is well known that convolutional neural networks (CNNs) require fewer computations for the same accuracy. When observing the data flow in CNNs, pooling layers and convolutional layers with a stride larger than one, the number of data at their output is reduced with respect to their input. This data reduction strongly affects the data rate in a fully parallel implementation, making hardware units heavily underutilized unless it is handled properly. This work addresses this issue by analyzing the data flow of CNNs and presents a novel approach to designing data-rate-aware, continuous-flow CNN architectures. The proposed approach ensures a high hardware utilization close to 100% by interleaving low data rate signals and sharing hardware units, as well as using the right parallelization to achieve the throughput of a fully parallel implementation. The results show that a significant amount of the arithmetic logic can be saved, which allows implementing complex CNNs like MobileNet on a single FPGA with high throughput.

</details>


### [70] [C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding](https://arxiv.org/abs/2601.20257)
*Jinren Ding,Xuejian Xu,Shen Jiang,Zhitong Hao,Jinhui Yang,Peng Jiang*

Main category: cs.LG

TL;DR: 提出C2框架改进Decision Transformer用于自动出价，在AuctionNet数据集上有性能提升。


<details>
  <summary>Details</summary>
Motivation: Decision Transformer存在序列间互相关建模不足和最优/次优行为无差别学习的问题，需改进。

Method: 提出C2框架，包含Cross Learning Block加强序列间相关性建模，以及Constraint - aware Loss结合预算和CPA约束进行最优轨迹选择性学习。

Result: 在AuctionNet数据集离线评估中，相比SOTA的GAVE性能提升达3.23%，消融实验验证组件协同性。

Conclusion: C2在自动出价中具有优越性，代码开源。

Abstract: Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: https://github.com/Dingjinren/C2.

</details>


### [71] [Latent Object Permanence: Topological Phase Transitions, Free-Energy Principles, and Renormalization Group Flows in Deep Transformer Manifolds](https://arxiv.org/abs/2601.19942)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.LG

TL;DR: 从几何和统计物理角度研究深度Transformer语言模型中多步推理的出现，发现有效维度锐降、相变等现象，并提出TCOs概念。


<details>
  <summary>Details</summary>
Motivation: 研究深度Transformer语言模型中多步推理的出现。

Method: 将隐藏状态轨迹视为隐式黎曼流形上的流，分析激活的逐层协方差谱，将前向传播形式化为离散粗粒化映射。

Result: 观察到有效维度锐降与相变一致，出现稳定“概念盆地”，形成TCOs，提供逻辑可分性与谱衰减的理论条件。

Conclusion: 通过理论分析和实验验证，揭示了深度Transformer语言模型中多步推理的相关特征和现象。

Abstract: We study the emergence of multi-step reasoning in deep Transformer language models through a geometric and statistical-physics lens. Treating the hidden-state trajectory as a flow on an implicit Riemannian manifold, we analyze the layerwise covariance spectrum of activations, where $C^{(\ell)}=\mathbb{E}[h^{(\ell)}h^{(\ell)\top}]$, and track deviations from a random-matrix bulk. Across model scales (1.5B--30B), we observe a sharp reduction in effective dimensionality consistent with a phase transition: an order parameter based on sparsity/localization, $Ω(h)=1-\|h\|_1/(\sqrt{d}\|h\|_2)$, exhibits a discontinuity near a critical normalized depth $γ_c\approx 0.42$ in sufficiently large models. We formalize the forward pass as a discrete coarse-graining map and relate the appearance of stable "concept basins" to fixed points of this renormalization-like dynamics. The resulting low-entropy regime is characterized by a spectral tail collapse and by the formation of transient, reusable object-like structures in representation space, which we call Transient Class Objects (TCOs). We provide theoretical conditions connecting logical separability to spectral decay and validate the predicted signatures with layerwise probes on multiple open-weight model families.

</details>


### [72] [Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction](https://arxiv.org/abs/2601.20299)
*Tianyi Alex Qiu,Micah Carroll,Cameron Allen*

Main category: cs.LG

TL;DR: 论文引入同伴预测方法用于大语言模型评估和训练，证明其有效性和抗欺骗性，还发现评估中的逆缩放特性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型评估和训练依赖监督，但困难任务强监督常缺失，模型会利用不完善监督产生欺骗性结果，机制设计研究可提供解决思路。

Method: 引入同伴预测方法，基于互预测性指标奖励诚实和有信息的回答，无需真实标签。

Result: 理论和实验验证方法有效性和抗欺骗性，训练模型可恢复真实性下降，评估有逆缩放特性。

Conclusion: 同伴预测方法能在弱监督下对大语言模型进行有效评估和训练，优于依赖强监督的LLM-as-a-Judge方法。

Abstract: The evaluation and post-training of large language models (LLMs) rely on supervision, but strong supervision for difficult tasks is often unavailable, especially when evaluating frontier models. In such cases, models are demonstrated to exploit evaluations built on such imperfect supervision, leading to deceptive results. However, underutilized in LLM research, a wealth of mechanism design research focuses on game-theoretic incentive compatibility, i.e., eliciting honest and informative answers with weak supervision. Drawing from this literature, we introduce the peer prediction method for model evaluation and post-training. It rewards honest and informative answers over deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. We demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and empirical validation on models with up to 405B parameters. We show that training an 8B model with peer prediction-based reward recovers most of the drop in truthfulness due to prior malicious finetuning, even when the reward is produced by a 0.135B language model with no finetuning. On the evaluation front, in contrast to LLM-as-a-Judge which requires strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception is strengthened as the capability gap between the experts and participants widens, enabling reliable evaluation of strong models with weak supervision. In particular, LLM-as-a-Judge become worse than random guess when facing deceptive models 5-20x the judge's size, while peer prediction thrives when such gaps are large, including in cases with over 100x size difference.

</details>


### [73] [Minimum-Cost Network Flow with Dual Predictions](https://arxiv.org/abs/2601.20203)
*Zhiyang Chen,Hailong Yao,Xia Yin*

Main category: cs.LG

TL;DR: 提出首个带对偶预测的最小成本网络流算法，给出复杂度界并实验验证，在两个应用中实现加速。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习预测改进经典算法性能，提出带对偶预测的最小成本网络流算法。

Method: 基于经典的ε - relaxation最小成本流算法，结合对偶预测。

Result: 给出基于无穷范数预测误差的时间复杂度界和PAC学习的样本复杂度界，在交通网络和芯片逃逸路由两个应用中分别实现12.74倍和1.64倍的平均加速。

Conclusion: 提出的带对偶预测的最小成本网络流算法有效，能提高算法性能。

Abstract: Recent work has shown that machine-learned predictions can provably improve the performance of classic algorithms. In this work, we propose the first minimum-cost network flow algorithm augmented with a dual prediction. Our method is based on a classic minimum-cost flow algorithm, namely $\varepsilon$-relaxation. We provide time complexity bounds in terms of the infinity norm prediction error, which is both consistent and robust. We also prove sample complexity bounds for PAC-learning the prediction. We empirically validate our theoretical results on two applications of minimum-cost flow, i.e., traffic networks and chip escape routing, in which we learn a fixed prediction, and a feature-based neural network model to infer the prediction, respectively. Experimental results illustrate $12.74\times$ and $1.64\times$ average speedup on two applications.

</details>


### [74] [Classifier Calibration at Scale: An Empirical Study of Model-Agnostic Post-Hoc Methods](https://arxiv.org/abs/2601.19944)
*Valery Manokhin,Daniel Grønhaug*

Main category: cs.LG

TL;DR: 研究模型无关的事后校准方法，对21种分类器在二元任务上进行基准测试，评估多种校准器效果，发现Venn - Abers和Beta校准表现较好，部分常用校准程序会降低性能。


<details>
  <summary>Details</summary>
Motivation: 改进监督二元分类中概率预测的准确性，聚焦具有分布自由有效性保证的校准方法。

Method: 使用TabArena - v0.1套件的二元任务，通过随机分层五折交叉验证和保留测试集，训练多种校准器并应用于测试预测，用多种指标评估校准效果。

Result: Venn - Abers预测器平均降低log - loss最多，Beta校准次之；Platt缩放效果较弱且不稳定；常用校准程序会降低强现代表格模型的评分性能；除Pearsonify外所有方法略提高准确率。

Conclusion: 校准效果在数据集和架构间差异大，无统一主导方法，常用校准程序可能有负面效果。

Abstract: We study model-agnostic post-hoc calibration methods intended to improve probabilistic predictions in supervised binary classification on real i.i.d. tabular data, with particular emphasis on conformal and Venn-based approaches that provide distribution-free validity guarantees under exchangeability. We benchmark 21 widely used classifiers, including linear models, SVMs, tree ensembles (CatBoost, XGBoost, LightGBM), and modern tabular neural and foundation models, on binary tasks from the TabArena-v0.1 suite using randomized, stratified five-fold cross-validation with a held-out test fold. Five calibrators; Isotonic regression, Platt scaling, Beta calibration, Venn-Abers predictors, and Pearsonify are trained on a separate calibration split and applied to test predictions. Calibration is evaluated using proper scoring rules (log-loss and Brier score) and diagnostic measures (Spiegelhalter's Z, ECE, and ECI), alongside discrimination (AUC-ROC) and standard classification metrics. Across tasks and architectures, Venn-Abers predictors achieve the largest average reductions in log-loss, followed closely by Beta calibration, while Platt scaling exhibits weaker and less consistent effects. Beta calibration improves log-loss most frequently across tasks, whereas Venn-Abers displays fewer instances of extreme degradation and slightly more instances of extreme improvement. Importantly, we find that commonly used calibration procedures, most notably Platt scaling and isotonic regression, can systematically degrade proper scoring performance for strong modern tabular models. Overall classification performance is often preserved, but calibration effects vary substantially across datasets and architectures, and no method dominates uniformly. In expectation, all methods except Pearsonify slightly increase accuracy, but the effect is marginal, with the largest expected gain about 0.008%.

</details>


### [75] [Active Learning for Decision Trees with Provable Guarantees](https://arxiv.org/abs/2601.20775)
*Arshia Soltani Moakhar,Tanapoom Laoaron,Faraz Ghahremani,Kiarash Banihashem,MohammadTaghi Hajiaghayi*

Main category: cs.LG

TL;DR: 本文推进决策树作为二分类器的主动学习标签复杂度的理论理解，分析分歧系数，提出有乘法误差保证的算法，设计决策树主动学习算法并给出标签复杂度下界。


<details>
  <summary>Details</summary>
Motivation: 提升对决策树作为二分类器的主动学习标签复杂度的理论理解。

Method: 对决策树的分歧系数进行分析，提出有乘法误差保证的二分类主动学习算法，结合结果设计决策树主动学习算法。

Result: 分析得出分歧系数分析所需假设，提出有乘法误差保证的算法，设计出在给定假设下仅需对数多项式数量标签查询的决策树主动学习算法，建立标签复杂度下界。

Conclusion: 算法对误差容限ε的依赖接近最优。

Abstract: This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees-a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic label complexity, (i) each root-to-leaf path queries distinct feature dimensions, and (ii) the input data has a regular, grid-like structure. We show these assumptions are essential, as relaxing them leads to polynomial label complexity. Second, we present the first general active learning algorithm for binary classification that achieves a multiplicative error guarantee, producing a $(1+ε)$-approximate classifier. By combining these results, we design an active learning algorithm for decision trees that uses only a polylogarithmic number of label queries in the dataset size, under the stated assumptions. Finally, we establish a label complexity lower bound, showing our algorithm's dependence on the error tolerance $ε$ is close to optimal.

</details>


### [76] [NCSAM Noise-Compensated Sharpness-Aware Minimization for Noisy Label Learning](https://arxiv.org/abs/2601.19947)
*Jiayu Xu,Junbiao Pang*

Main category: cs.LG

TL;DR: 本文从损失景观平坦性与标签噪声关系的理论分析出发，提出Noise - Compensated Sharpness - aware Minimization (NCSAM)方法，实验显示该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现实数据集常含错误标注，当前研究聚焦于复杂的标签校正机制，本文希望从新视角应对学习有噪标签的挑战。

Method: 对损失景观平坦性和标签噪声关系进行理论分析，提出NCSAM方法利用Sharpness - Aware Minimization (SAM)的扰动来弥补标签噪声造成的损害。

Result: 测试精度在有噪数据集上表现与无噪数据集相近，多个基准数据集的实验显示所提方法在多样任务中始终优于现有技术。

Conclusion: 提出的NCSAM方法在应对学习有噪标签问题上有效，能提升模型泛化性能和对标签噪声的鲁棒性。

Abstract: Learning from Noisy Labels (LNL) presents a fundamental challenge in deep learning, as real-world datasets often contain erroneous or corrupted annotations, \textit{e.g.}, data crawled from Web. Current research focuses on sophisticated label correction mechanisms. In contrast, this paper adopts a novel perspective by establishing a theoretical analysis the relationship between flatness of the loss landscape and the presence of label noise. In this paper, we theoretically demonstrate that carefully simulated label noise synergistically enhances both the generalization performance and robustness of label noises. Consequently, we propose Noise-Compensated Sharpness-aware Minimization (NCSAM) to leverage the perturbation of Sharpness-Aware Minimization (SAM) to remedy the damage of label noises. Our analysis reveals that the testing accuracy exhibits a similar behavior that has been observed on the noise-clear dataset. Extensive experimental results on multiple benchmark datasets demonstrate the consistent superiority of the proposed method over existing state-of-the-art approaches on diverse tasks.

</details>


### [77] [Probabilistic Sensing: Intelligence in Data Sampling](https://arxiv.org/abs/2601.19953)
*Ibrahim Albulushi,Saleh Bunaiyan,Suraj S. Cheema,Hesham ElSawy,Feras Al-Dirini*

Main category: cs.LG

TL;DR: 提出一种概率传感范式，实现无损概率数据采集并节能。


<details>
  <summary>Details</summary>
Motivation: 确定性采样决策有信息损失风险，需新方法实现数据采集决策。

Method: 受自主神经系统启发，采用由模拟特征提取电路驱动的概率神经元（p - neuron）的传感范式。

Result: 系统响应时间达微秒级，克服子采样率响应时间限制；主动地震勘探数据验证实验实现无损概率数据采集，归一化均方误差0.41%，系统主动运行时间和生成样本数节省93%。

Conclusion: 该概率传感范式可实现实时智能自主的数据采样激活，有显著的节能效果。

Abstract: Extending the intelligence of sensors to the data-acquisition process - deciding whether to sample or not - can result in transformative energy-efficiency gains. However, making such a decision in a deterministic manner involves risk of losing information. Here we present a sensing paradigm that enables making such a decision in a probabilistic manner. The paradigm takes inspiration from the autonomous nervous system and employs a probabilistic neuron (p-neuron) driven by an analog feature extraction circuit. The response time of the system is on the order of microseconds, over-coming the sub-sampling-rate response time limit and enabling real-time intelligent autonomous activation of data-sampling. Validation experiments on active seismic survey data demonstrate lossless probabilistic data acquisition, with a normalized mean squared error of 0.41%, and 93% saving in the active operation time of the system and the number of generated samples.

</details>


### [78] [MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference](https://arxiv.org/abs/2601.19961)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Ruijia Wu,Li YanTao,Qiang Hui,Yuren You,Ting Lu,Chao Tan,Shaoan Zhao,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.LG

TL;DR: 提出无训练缓存框架MeanCache用于高效Flow Matching推理，实验显示有加速效果且生成质量优。


<details>
  <summary>Details</summary>
Motivation: 现有缓存方法依赖瞬时速度信息，在高加速比下会导致轨迹偏差和误差累积。

Method: 引入平均速度视角，利用缓存的雅可比向量积构建区间平均速度；开发轨迹稳定性调度策略确定缓存时间。

Result: 在FLUX.1、Qwen - Image和HunyuanVideo上分别实现4.12X、4.56X和3.59X加速，生成质量优于现有缓存基线。

Conclusion: 该方法为Flow Matching推理提供新视角，有望启发商业规模生成模型稳定性驱动加速的进一步探索。

Abstract: We present MeanCache, a training-free caching framework for efficient Flow Matching inference. Existing caching methods reduce redundant computation but typically rely on instantaneous velocity information (e.g., feature caching), which often leads to severe trajectory deviations and error accumulation under high acceleration ratios. MeanCache introduces an average-velocity perspective: by leveraging cached Jacobian--vector products (JVP) to construct interval average velocities from instantaneous velocities, it effectively mitigates local error accumulation. To further improve cache timing and JVP reuse stability, we develop a trajectory-stability scheduling strategy as a practical tool, employing a Peak-Suppressed Shortest Path under budget constraints to determine the schedule. Experiments on FLUX.1, Qwen-Image, and HunyuanVideo demonstrate that MeanCache achieves 4.12X and 4.56X and 3.59X acceleration, respectively, while consistently outperforming state-of-the-art caching baselines in generation quality. We believe this simple yet effective approach provides a new perspective for Flow Matching inference and will inspire further exploration of stability-driven acceleration in commercial-scale generative models.

</details>


### [79] [Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment](https://arxiv.org/abs/2601.19963)
*Canyang Zhao,Bolin Peng,J. Patrick Mayo,Ce Ju,Bing Liu*

Main category: cs.LG

TL;DR: 提出Task - Conditioned Latent Alignment (TCLA)框架用于跨会话神经解码，在猕猴数据集上评估显示其能提升解码性能，可有效地在有限数据下进行知识迁移。


<details>
  <summary>Details</summary>
Motivation: 植入电极记录的神经活动的跨会话非平稳性是侵入式脑机接口（BCIs）的主要挑战，且在新会话数据有限时重新训练或调整解码器尤为困难。

Method: 提出TCLA框架，基于自编码器架构，先从有充足数据的源会话中学习神经动力学的低维表示，再以任务条件的方式将目标会话的潜在表示与源对齐。

Result: 在猕猴运动和眼动数据集上评估，与仅基于目标会话数据训练的基线方法相比，TCLA在不同数据集和解码设置下持续提高解码性能，运动数据集中y坐标速度解码的决定系数增益最高达0.386。

Conclusion: TCLA为源会话到目标会话的知识转移提供了有效策略，能在有限数据条件下实现更稳健的神经解码。

Abstract: Cross-session nonstationarity in neural activity recorded by implanted electrodes is a major challenge for invasive Brain-computer interfaces (BCIs), as decoders trained on data from one session often fail to generalize to subsequent sessions. This issue is further exacerbated in practice, as retraining or adapting decoders becomes particularly challenging when only limited data are available from a new session. To address this challenge, we propose a Task-Conditioned Latent Alignment framework (TCLA) for cross-session neural decoding. Building upon an autoencoder architecture, TCLA first learns a low-dimensional representation of neural dynamics from a source session with sufficient data. For target sessions with limited data, TCLA then aligns target latent representations to the source in a task-conditioned manner, enabling effective transfer of learned neural dynamics. We evaluate TCLA on the macaque motor and oculomotor center-out dataset. Compared to baseline methods trained solely on target-session data, TCLA consistently improves decoding performance across datasets and decoding settings, with gains in the coefficient of determination of up to 0.386 for y coordinate velocity decoding in a motor dataset. These results suggest that TCLA provides an effective strategy for transferring knowledge from source to target sessions, enabling more robust neural decoding under conditions with limited data.

</details>


### [80] [BayPrAnoMeta: Bayesian Proto-MAML for Few-Shot Industrial Image Anomaly Detection](https://arxiv.org/abs/2601.19992)
*Soham Sarkar,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: 提出BayPrAnoMeta用于少样本工业图像异常检测，在MVTec AD基准测试中比其他方法有显著AUROC提升。


<details>
  <summary>Details</summary>
Motivation: 工业图像异常检测存在极端类别不平衡和标记缺陷样本稀缺的问题，特别是在少样本情况下。

Method: 提出BayPrAnoMeta，用特定任务的概率正常模型替代原型，通过贝叶斯后验预测似然进行内循环适应；用NIW先验对正常支持嵌入建模；将其扩展到联邦元学习框架并添加监督对比正则化。

Result: 在MVTec AD基准测试中，在少样本异常检测设置下，比MAML、Proto - MAML和基于PatchCore的方法有一致且显著的AUROC提升。

Conclusion: BayPrAnoMeta在少样本工业图像异常检测方面表现良好，能在极端少样本设置下实现鲁棒性，其扩展到联邦元学习框架也有相应效果。

Abstract: Industrial image anomaly detection is a challenging problem owing to extreme class imbalance and the scarcity of labeled defective samples, particularly in few-shot settings. We propose BayPrAnoMeta, a Bayesian generalization of Proto-MAML for few-shot industrial image anomaly detection. Unlike existing Proto-MAML approaches that rely on deterministic class prototypes and distance-based adaptation, BayPrAnoMeta replaces prototypes with task-specific probabilistic normality models and performs inner-loop adaptation via a Bayesian posterior predictive likelihood. We model normal support embeddings with a Normal-Inverse-Wishart (NIW) prior, producing a Student-$t$ predictive distribution that enables uncertainty-aware, heavy-tailed anomaly scoring and is essential for robustness in extreme few-shot settings. We further extend BayPrAnoMeta to a federated meta-learning framework with supervised contrastive regularization for heterogeneous industrial clients and prove convergence to stationary points of the resulting nonconvex objective. Experiments on the MVTec AD benchmark demonstrate consistent and significant AUROC improvements over MAML, Proto-MAML, and PatchCore-based methods in few-shot anomaly detection settings.

</details>


### [81] [Modeling Cascaded Delay Feedback for Online Net Conversion Rate Prediction: Benchmark, Insights and Solutions](https://arxiv.org/abs/2601.19965)
*Mingxuan Luo,Guipeng Xv,Sishuo Chen,Xinyu Li,Li Zhang,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Bo Zheng,Chen Lin*

Main category: cs.LG

TL;DR: 论文指出CVR无法反映退款，引出NetCVR，介绍CASCADE数据集，提出TESLA框架，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: CVR无法反映退款行为，不能完全体现推荐效果，NetCVR预测面临多阶段级联延迟反馈等挑战。

Method: 引入CASCADE数据集，分析得出三个关键见解，提出TESLA连续NetCVR建模框架。

Result: TESLA在CASCADE上始终优于现有方法，NetCVR预测的RI - AUC提升12.41%，RI - PRAUC提升14.94%。

Conclusion: TESLA有效提升NetCVR预测性能，代码和数据集公开。

Abstract: In industrial recommender systems, conversion rate (CVR) is widely used for traffic allocation, but it fails to fully reflect recommendation effectiveness because it ignores refund behavior. To better capture true user satisfaction and business value, net conversion rate (NetCVR), defined as the probability that a clicked item is purchased and not refunded, has been proposed.Unlike CVR, NetCVR prediction involves a more complex multi-stage cascaded delayed feedback process. The two cascaded delays from click to conversion and from conversion to refund have opposite effects, making traditional CVR modeling methods inapplicable. Moreover, the lack of open-source datasets and online continuous training schemes further hinders progress in this area.To address these challenges, we introduce CASCADE (Cascaded Sequences of Conversion and Delayed Refund), the first large-scale open dataset derived from the Taobao app for online continuous NetCVR prediction. Through an in-depth analysis of CASCADE, we identify three key insights: (1) NetCVR exhibits strong temporal dynamics, necessitating online continuous modeling; (2) cascaded modeling of CVR and refund rate outperforms direct NetCVR modeling; and (3) delay time, which correlates with both CVR and refund rate, is an important feature for NetCVR prediction.Based on these insights, we propose TESLA, a continuous NetCVR modeling framework featuring a CVR-refund-rate cascaded architecture, stage-wise debiasing, and a delay-time-aware ranking loss. Extensive experiments demonstrate that TESLA consistently outperforms state-of-the-art methods on CASCADE, achieving absolute improvements of 12.41 percent in RI-AUC and 14.94 percent in RI-PRAUC on NetCVR prediction. The code and dataset are publicly available at https://github.com/alimama-tech/NetCVR.

</details>


### [82] [Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers](https://arxiv.org/abs/2601.19967)
*Jinlin Liu,Wei Chen,Xiaojin Zhang*

Main category: cs.LG

TL;DR: 提出高效扰动生成方法Perturbation - Induced Linearization (PIL)用于数据保护，减少计算时间并分析不可学习示例特性。


<details>
  <summary>Details</summary>
Motivation: 收集网络数据训练模型引发数据使用问题，现有不可学习示例方法计算成本高，需更高效方法。

Method: 提出仅使用线性替代模型生成扰动的Perturbation - Induced Linearization (PIL)方法。

Result: PIL比现有基于替代模型方法性能相当或更好，大幅减少计算时间。

Conclusion: 工作为数据保护提供实用方法，揭示不可学习示例有效的原理。

Abstract: Collecting web data to train deep models has become increasingly common, raising concerns about unauthorized data usage. To mitigate this issue, unlearnable examples introduce imperceptible perturbations into data, preventing models from learning effectively. However, existing methods typically rely on deep neural networks as surrogate models for perturbation generation, resulting in significant computational costs. In this work, we propose Perturbation-Induced Linearization (PIL), a computationally efficient yet effective method that generates perturbations using only linear surrogate models. PIL achieves comparable or better performance than existing surrogate-based methods while reducing computational time dramatically. We further reveal a key mechanism underlying unlearnable examples: inducing linearization to deep models, which explains why PIL can achieve competitive results in a very short time. Beyond this, we provide an analysis about the property of unlearnable examples under percentage-based partial perturbation. Our work not only provides a practical approach for data protection but also offers insights into what makes unlearnable examples effective.

</details>


### [83] [Regime-Adaptive Bayesian Optimization via Dirichlet Process Mixtures of Gaussian Processes](https://arxiv.org/abs/2601.20043)
*Yan Zhang,Xuefeng Liu,Sipeng Chen,Sascha Ranftl,Chong Liu,Shibo Li*

Main category: cs.LG

TL;DR: 提出RAMBO方法处理多模式问题，实验显示优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化（BO）在多模式问题中假设搜索空间平滑性一致，该假设不成立，单一高斯过程（GP）存在不确定性校准问题。

Method: 提出RAMBO，即高斯过程的狄利克雷过程混合模型，通过塌缩吉布斯采样进行高效推理，引入自适应浓度参数调度进行粗到细的模式发现，将采集函数的不确定性分解为区域内和区域间成分。

Result: 在合成基准和实际应用上实验，包括分子构象优化、药物发现虚拟筛选和聚变反应堆设计，持续优于现有基线。

Conclusion: RAMBO能有效解决多模式优化问题，在多模式目标上表现出色。

Abstract: Standard Bayesian Optimization (BO) assumes uniform smoothness across the search space an assumption violated in multi-regime problems such as molecular conformation search through distinct energy basins or drug discovery across heterogeneous molecular scaffolds. A single GP either oversmooths sharp transitions or hallucinates noise in smooth regions, yielding miscalibrated uncertainty. We propose RAMBO, a Dirichlet Process Mixture of Gaussian Processes that automatically discovers latent regimes during optimization, each modeled by an independent GP with locally-optimized hyperparameters. We derive collapsed Gibbs sampling that analytically marginalizes latent functions for efficient inference, and introduce adaptive concentration parameter scheduling for coarse-to-fine regime discovery. Our acquisition functions decompose uncertainty into intra-regime and inter-regime components. Experiments on synthetic benchmarks and real-world applications, including molecular conformer optimization, virtual screening for drug discovery, and fusion reactor design, demonstrate consistent improvements over state-of-the-art baselines on multi-regime objectives.

</details>


### [84] [Decomposing multimodal embedding spaces with group-sparse autoencoders](https://arxiv.org/abs/2601.20028)
*Chiraag Kaushik,Davis Barch,Andrea Fanelli*

Main category: cs.LG

TL;DR: 本文研究如何有效调整稀疏自编码器（SAEs）以处理多模态嵌入，提出新方法并在图像/文本和音频/文本数据上验证其能学习更多模态字典，提升跨模态任务的可解释性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有SAEs应用于多模态嵌入空间时会学习“分裂字典”，多数特征为单模态，需研究如何有效调整SAEs以处理多模态嵌入并确保多模态对齐。

Method: 提出基于SAEs的新方法，使用跨模态随机掩码和组稀疏正则化。

Result: 与标准SAEs相比，该方法能学习更多模态字典，减少死亡神经元数量，提升特征语义性。

Conclusion: 该方法对模态间概念对齐的改进可提升跨模态任务的可解释性和可控性。

Abstract: The Linear Representation Hypothesis asserts that the embeddings learned by neural networks can be understood as linear combinations of features corresponding to high-level concepts. Based on this ansatz, sparse autoencoders (SAEs) have recently become a popular method for decomposing embeddings into a sparse combination of linear directions, which have been shown empirically to often correspond to human-interpretable semantics. However, recent attempts to apply SAEs to multimodal embedding spaces (such as the popular CLIP embeddings for image/text data) have found that SAEs often learn "split dictionaries", where most of the learned sparse features are essentially unimodal, active only for data of a single modality. In this work, we study how to effectively adapt SAEs for the setting of multimodal embeddings while ensuring multimodal alignment. We first argue that the existence of a split dictionary decomposition on an aligned embedding space implies the existence of a non-split dictionary with improved modality alignment. Then, we propose a new SAE-based approach to multimodal embedding decomposition using cross-modal random masking and group-sparse regularization. We apply our method to popular embeddings for image/text (CLIP) and audio/text (CLAP) data and show that, compared to standard SAEs, our approach learns a more multimodal dictionary while reducing the number of dead neurons and improving feature semanticity. We finally demonstrate how this improvement in alignment of concepts between modalities can enable improvements in the interpretability and control of cross-modal tasks.

</details>


### [85] [Order-Optimal Sample Complexity of Rectified Flows](https://arxiv.org/abs/2601.20250)
*Hari Krishna Sahoo,Mudit Gaur,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recently, flow-based generative models have shown superior efficiency compared to diffusion models. In this paper, we study rectified flow models, which constrain transport trajectories to be linear from the base distribution to the data distribution. This structural restriction greatly accelerates sampling, often enabling high-quality generation with a single Euler step. Under standard assumptions on the neural network classes used to parameterize the velocity field and data distribution, we prove that rectified flows achieve sample complexity $\tilde{O}(\varepsilon^{-2})$. This improves on the best known $O(\varepsilon^{-4})$ bounds for flow matching model and matches the optimal rate for mean estimation. Our analysis exploits the particular structure of rectified flows: because the model is trained with a squared loss along linear paths, the associated hypothesis class admits a sharply controlled localized Rademacher complexity. This yields the improved, order-optimal sample complexity and provides a theoretical explanation for the strong empirical performance of rectified flow models.

</details>


### [86] [Structural Compositional Function Networks: Interpretable Functional Compositions for Tabular Discovery](https://arxiv.org/abs/2601.20037)
*Fang Li*

Main category: cs.LG

TL;DR: 提出StructuralCFN架构处理表格数据，在多基准测试中表现好且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习架构处理表格数据难以兼顾性能和可解释性，标准神经网络未利用表格分布的固有结构依赖。

Method: 提出StructuralCFN架构，通过可微结构先验施加关系感知归纳偏差，用可微自适应门控对特征建模，支持结构化知识集成。

Result: 在18个基准的10折交叉验证中，对科学和临床数据集有显著改进（p < 0.05）。

Conclusion: StructuralCFN能处理表格数据，提高性能且有内在符号可解释性，参数规模比标准深度基线小几十倍。

Abstract: Despite the ubiquity of tabular data in high-stakes domains, traditional deep learning architectures often struggle to match the performance of gradient-boosted decision trees while maintaining scientific interpretability. Standard neural networks typically treat features as independent entities, failing to exploit the inherent manifold structural dependencies that define tabular distributions. We propose Structural Compositional Function Networks (StructuralCFN), a novel architecture that imposes a Relation-Aware Inductive Bias via a differentiable structural prior. StructuralCFN explicitly models each feature as a mathematical composition of its counterparts through Differentiable Adaptive Gating, which automatically discovers the optimal activation physics (e.g., attention-style filtering vs. inhibitory polarity) for each relationship. Our framework enables Structured Knowledge Integration, allowing domain-specific relational priors to be injected directly into the architecture to guide discovery. We evaluate StructuralCFN across a rigorous 10-fold cross-validation suite on 18 benchmarks, demonstrating statistically significant improvements (p < 0.05) on scientific and clinical datasets (e.g., Blood Transfusion, Ozone, WDBC). Furthermore, StructuralCFN provides Intrinsic Symbolic Interpretability: it recovers the governing "laws" of the data manifold as human-readable mathematical expressions while maintaining a compact parameter footprint (300--2,500 parameters) that is over an order of magnitude (10x--20x) smaller than standard deep baselines.

</details>


### [87] [$\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval](https://arxiv.org/abs/2601.20844)
*Zihao Wang,Hang Yin,Lihui Liu,Hanghang Tong,Yangqiu Song,Ginny Wong,Simon See*

Main category: cs.LG

TL;DR: 本文研究子集成员嵌入向量空间所需的最小维度（MED），推导其理论边界并进行实证支持，模拟实现MED与元素数量的对数依赖，指出嵌入检索限制源于可学习性挑战。


<details>
  <summary>Details</summary>
Motivation: 研究子集成员嵌入向量空间所需的最小维度。

Method: 理论推导MED的边界，在多种距离或相似度概念下进行实证支持，在更可行的设置下进行数值模拟。

Result: 模拟实现MED与元素数量的对数依赖。

Conclusion: 嵌入检索限制主要来自可学习性挑战，而非几何约束，可为未来算法设计提供指导。

Abstract: This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of "distances" or "similarities," including the $\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.

</details>


### [88] [CiMRAG: Cim-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs](https://arxiv.org/abs/2601.20041)
*Shih-Hsuan Chiu,Ming-Syan Chen*

Main category: cs.LG

TL;DR: 提出TONEL框架，提升边缘设备上RAG在噪声环境下的鲁棒性和领域适应性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上基于LLM的个性化虚拟助手受关注，RAG部署有效率问题，CiM架构有噪声影响检索精度，需解决准确性和适应性问题。

Method: 提出Task - Oriented Noise - resilient Embedding Learning (TONEL)框架，采用噪声感知投影模型学习符合CiM硬件约束的特定任务嵌入。

Result: 在个性化基准测试上的大量实验表明，相对于强基线，该方法在特定任务的噪声场景中有效且实用。

Conclusion: TONEL框架能有效提升RAG在噪声边缘环境中的噪声鲁棒性和领域适应性。

Abstract: Personalized virtual assistants powered by large language models (LLMs) on edge devices are attracting growing attention, with Retrieval-Augmented Generation (RAG) emerging as a key method for personalization by retrieving relevant profile data and generating tailored responses. However, deploying RAG on edge devices faces efficiency hurdles due to the rapid growth of profile data, such as user-LLM interactions and recent updates. While Computing-in-Memory (CiM) architectures mitigate this bottleneck by eliminating data movement between memory and processing units via in-situ operations, they are susceptible to environmental noise that can degrade retrieval precision. This poses a critical issue in dynamic, multi-domain edge-based scenarios (e.g., travel, medicine, and law) where both accuracy and adaptability are paramount. To address these challenges, we propose Task-Oriented Noise-resilient Embedding Learning (TONEL), a framework that improves noise robustness and domain adaptability for RAG in noisy edge environments. TONEL employs a noise-aware projection model to learn task-specific embeddings compatible with CiM hardware constraints, enabling accurate retrieval under noisy conditions. Extensive experiments conducted on personalization benchmarks demonstrate the effectiveness and practicality of our methods relative to strong baselines, especially in task-specific noisy scenarios.

</details>


### [89] [Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation](https://arxiv.org/abs/2601.20848)
*Weixin Chen,Li Chen,Yuhan Zhao*

Main category: cs.LG

TL;DR: 提出Cofair框架实现推荐系统训练后公平性控制，实验证明其能提供不同水平动态公平性。


<details>
  <summary>Details</summary>
Motivation: 现有公平感知方法训练时固定公平要求，训练后灵活性有限，现实中不同利益相关者随时间对公平要求不同，重新训练成本高。

Method: 提出Cofair框架，引入带公平条件适配器模块的共享表示层生成用户嵌入，添加用户级正则化项保证用户级公平性单调提升。

Result: 理论证明Cofair的对抗目标上界为 demographic parity，正则化项能提升用户级公平性；多数据集和骨干模型实验表明该框架能提供不同水平动态公平性，公平 - 准确性曲线优于现有基线。

Conclusion: Cofair框架无需为新公平要求重新训练，能有效实现推荐系统训练后公平性控制。

Abstract: Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.

</details>


### [90] [Externally Validated Longitudinal GRU Model for Visit-Level 180-Day Mortality Risk in Metastatic Castration-Resistant Prostate Cancer](https://arxiv.org/abs/2601.20046)
*Javier Mencia-Ledo,Mohammad Noaeen,Zahra Shakeri*

Main category: cs.LG

TL;DR: 研究利用两个三期队列纵向数据开发并外部验证180天死亡风险模型，比较五种架构，GRU在外部验证表现佳，表明常规临床标记物可估计mCRPC短期死亡风险。


<details>
  <summary>Details</summary>
Motivation: 转移性去势抵抗性前列腺癌（mCRPC）预后差、治疗反应异质性高，需开发模型估计其短期死亡风险。

Method: 用两个三期队列纵向数据开发模型，排除右删失病例，比较五种候选架构，选85%灵敏度的最小风险阈值。

Result: GRU和RSF初始判别能力高，外部验证GRU校准度高、PR - AUC为0.87，临床影响分析有相关结果，BMI和收缩压关联最强。

Conclusion: 纵向常规临床标记物能估计mCRPC短期死亡风险，支持多月主动护理计划。

Abstract: Metastatic castration-resistant prostate cancer (mCRPC) is a highly aggressive disease with poor prognosis and heterogeneous treatment response. In this work, we developed and externally validated a visit-level 180-day mortality risk model using longitudinal data from two Phase III cohorts (n=526 and n=640). Only visits with observable 180-day outcomes were labeled; right-censored cases were excluded from analysis. We compared five candidate architectures: Long Short-Term Memory, Gated Recurrent Unit (GRU), Cox Proportional Hazards, Random Survival Forest (RSF), and Logistic Regression. For each dataset, we selected the smallest risk-threshold that achieved an 85% sensitivity floor. The GRU and RSF models showed high discrimination capabilities initially (C-index: 87% for both). In external validation, the GRU obtained a higher calibration (slope: 0.93; intercept: 0.07) and achieved an PR-AUC of 0.87. Clinical impact analysis showed a median time-in-warning of 151.0 days for true positives (59.0 days for false positives) and 18.3 alerts per 100 patient-visits. Given late-stage frailty or cachexia and hemodynamic instability, permutation importance ranked BMI and systolic blood pressure as the strongest associations. These results suggest that longitudinal routine clinical markers can estimate short-horizon mortality risk in mCRPC and support proactive care planning over a multi-month window.

</details>


### [91] [Robust Distributed Learning under Resource Constraints: Decentralized Quantile Estimation via (Asynchronous) ADMM](https://arxiv.org/abs/2601.20571)
*Anna van Elst,Igor Colin,Stephan Clémençon*

Main category: cs.LG

TL;DR: 提出AsylADMM算法用于去中心化中位数和分位数估计，分析同步变体并实证异步算法收敛快，展示其多种应用，还对基于排名的修剪进行理论分析。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化学习算法在资源受限边缘设备上难以同时满足通信高效、抗数据损坏和低内存使用，基于gossip的方法难以实现鲁棒性，现有基于ADMM的方法内存需求大。

Method: 提出AsylADMM算法，分析其同步变体，通过实验验证异步算法性能，展示算法在多种场景的应用，并利用马尔可夫链理论对基于排名的修剪进行理论分析。

Result: AsylADMM算法收敛快，基于分位数的修剪在实证中优于现有基于排名的方法。

Conclusion: AsylADMM算法是一种适用于资源受限边缘设备的有效去中心化学习算法，可用于中位数和分位数估计等多种场景。

Abstract: Specifications for decentralized learning on resource-constrained edge devices require algorithms that are communication-efficient, robust to data corruption, and lightweight in memory usage. While state-of-the-art gossip-based methods satisfy the first requirement, achieving robustness remains challenging. Asynchronous decentralized ADMM-based methods have been explored for estimating the median, a statistical centrality measure that is notoriously more robust than the mean. However, existing approaches require memory that scales with node degree, making them impractical when memory is limited. In this paper, we propose AsylADMM, a novel gossip algorithm for decentralized median and quantile estimation, primarily designed for asynchronous updates and requiring only two variables per node. We analyze a synchronous variant of AsylADMM to establish theoretical guarantees and empirically demonstrate fast convergence for the asynchronous algorithm. We then show that our algorithm enables quantile-based trimming, geometric median estimation, and depth-based trimming, with quantile-based trimming empirically outperforming existing rank-based methods. Finally, we provide a novel theoretical analysis of rank-based trimming via Markov chain theory.

</details>


### [92] [SA-PEF: Step-Ahead Partial Error Feedback for Efficient Federated Learning](https://arxiv.org/abs/2601.20738)
*Dawit Kiros Redie,Reza Arablouei,Stefan Werner*

Main category: cs.LG

TL;DR: 提出SA - PEF方法解决偏置梯度压缩误差反馈在非IID数据下的问题，理论证明收敛性，实验表明比EF更快达到目标精度。


<details>
  <summary>Details</summary>
Motivation: 偏置梯度压缩误差反馈在非IID数据下，残差误差衰减慢，导致梯度不匹配和早期轮次进展停滞。

Method: 提出SA - PEF方法，将提前步长（SA）校正与部分误差反馈（PEF）集成，根据不同参数恢复不同模式，建立二阶矩界和残差递归保证收敛。

Result: 得到的收敛率在常数因子上匹配标准非凸Fed - SGD保证，揭示提前步长控制的残差收缩，实验显示SA - PEF比EF更快达到目标精度。

Conclusion: SA - PEF方法在不同架构和数据集上表现良好，能平衡快速预热和长期稳定性，有效解决非IID数据下的问题。

Abstract: Biased gradient compression with error feedback (EF) reduces communication in federated learning (FL), but under non-IID data, the residual error can decay slowly, causing gradient mismatch and stalled progress in the early rounds. We propose step-ahead partial error feedback (SA-PEF), which integrates step-ahead (SA) correction with partial error feedback (PEF). SA-PEF recovers EF when the step-ahead coefficient $α=0$ and step-ahead EF (SAEF) when $α=1$. For non-convex objectives and $δ$-contractive compressors, we establish a second-moment bound and a residual recursion that guarantee convergence to stationarity under heterogeneous data and partial client participation. The resulting rates match standard non-convex Fed-SGD guarantees up to constant factors, achieving $O((η,η_0TR)^{-1})$ convergence to a variance/heterogeneity floor with a fixed inner step size. Our analysis reveals a step-ahead-controlled residual contraction $ρ_r$ that explains the observed acceleration in the early training phase. To balance SAEF's rapid warm-up with EF's long-term stability, we select $α$ near its theory-predicted optimum. Experiments across diverse architectures and datasets show that SA-PEF consistently reaches target accuracy faster than EF.

</details>


### [93] [Domain Expansion: A Latent Space Construction Framework for Multi-Task Learning](https://arxiv.org/abs/2601.20069)
*Chi-Yao Huang,Khoa Vo,Aayush Atul Verma,Duo Lu,Yezhou Yang*

Main category: cs.LG

TL;DR: 提出Domain Expansion框架解决多目标训练中潜在表征崩溃问题，实验表明其有效且能产生可解释潜在空间。


<details>
  <summary>Details</summary>
Motivation: 多目标训练中冲突梯度会导致潜在表征崩溃，影响共享表征质量。

Method: 引入Domain Expansion框架，用新颖正交池化机制构建潜在空间，将每个目标分配到相互正交子空间。

Result: 在多个基准测试上验证，该结构能防止崩溃，产生显式、可解释和可组合的潜在空间。

Conclusion: Domain Expansion框架能有效解决潜在表征崩溃问题，生成的潜在空间有助于概念直接操作。

Abstract: Training a single network with multiple objectives often leads to conflicting gradients that degrade shared representations, forcing them into a compromised state that is suboptimal for any single task--a problem we term latent representation collapse. We introduce Domain Expansion, a framework that prevents these conflicts by restructuring the latent space itself. Our framework uses a novel orthogonal pooling mechanism to construct a latent space where each objective is assigned to a mutually orthogonal subspace. We validate our approach across diverse benchmarks--including ShapeNet, MPIIGaze, and Rotated MNIST--on challenging multi-objective problems combining classification with pose and gaze estimation. Our experiments demonstrate that this structure not only prevents collapse but also yields an explicit, interpretable, and compositional latent space where concepts can be directly manipulated.

</details>


### [94] [Distributional value gradients for stochastic environments](https://arxiv.org/abs/2601.20071)
*Baptiste Debes,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: 本文提出Distributional Sobolev Training方法，扩展连续状态 - 动作空间的分布强化学习，在简单问题和多环境下验证其有效性


<details>
  <summary>Details</summary>
Motivation: 现有的梯度正则化值学习方法（如MAGE）在随机或嘈杂环境中表现不佳，适用性受限

Method: 扩展连续状态 - 动作空间的分布强化学习，利用基于条件变分自编码器的单步世界模型，采用Max - sliced Maximum Mean Discrepancy实例化分布贝尔曼算子

Result: 证明Sobolev增强的贝尔曼算子是具有唯一不动点的收缩算子，指出梯度感知强化学习收缩下的基本平滑权衡

Conclusion: 在简单随机强化学习玩具问题和多个MuJoCo环境中方法有效

Abstract: Gradient-regularized value learning methods improve sample efficiency by leveraging learned models of transition dynamics and rewards to estimate return gradients. However, existing approaches, such as MAGE, struggle in stochastic or noisy environments, limiting their applicability. In this work, we address these limitations by extending distributional reinforcement learning on continuous state-action spaces to model not only the distribution over scalar state-action value functions but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step world model of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-based and employs Max-sliced Maximum Mean Discrepancy (MSMMD) to instantiate the distributional Bellman operator. We prove that the Sobolev-augmented Bellman operator is a contraction with a unique fixed point, and highlight a fundamental smoothness trade-off underlying contraction in gradient-aware RL. To validate our method, we first showcase its effectiveness on a simple stochastic reinforcement learning toy problem, then benchmark its performance on several MuJoCo environments.

</details>


### [95] [Techno-economic optimization of a heat-pipe microreactor, part II: multi-objective optimization analysis](https://arxiv.org/abs/2601.20079)
*Paul Seurin,Dean Price*

Main category: cs.LG

TL;DR: 本文将热管微反应堆设计优化框架扩展到多目标优化，评估三种成本情景，得出降低相关参数可降低 $F_{Δh}$，优化 LCOE 的四个策略，并指出 PEARL 算法存在不足待改进。


<details>
  <summary>Details</summary>
Motivation: 之前的设计优化框架仅关注最小化平准化电力成本（LCOE），本文将其扩展到多目标优化，同时考虑最小化棒积分峰因子 ($F_{Δh}$) 和 LCOE。

Method: 使用基于强化学习的帕累托包络增强算法（PEARL）进行多目标优化，评估三种成本情景。

Result: 降低固体慢化剂半径、销间距和鼓涂层角度，增加燃料高度可有效降低 $F_{Δh}$；优化 LCOE 有四个关键策略。

Conclusion: PEARL 算法在不同设计情景权衡中有前景，但代理模型预测与全阶模拟有差异，需通过约束松弛和代理模型开发改进。

Abstract: Heat-pipe microreactors (HPMRs) are compact and transportable nuclear power systems exhibiting inherent safety, well-suited for deployment in remote regions where access is limited and reliance on costly fossil fuels is prevalent. In prior work, we developed a design optimization framework that incorporates techno-economic considerations through surrogate modeling and reinforcement learning (RL)-based optimization, focusing solely on minimizing the levelized cost of electricity (LCOE) by using a bottom-up cost estimation approach. In this study, we extend that framework to a multi-objective optimization that uses the Pareto Envelope Augmented with Reinforcement Learning (PEARL) algorithm. The objectives include minimizing both the rod-integrated peaking factor ($F_{Δh}$) and LCOE -- subject to safety and operational constraints. We evaluate three cost scenarios: (1) a high-cost axial and drum reflectors, (2) a low-cost axial reflector, and (3) low-cost axial and drum reflectors. Our findings indicate that reducing the solid moderator radius, pin pitch, and drum coating angle -- all while increasing the fuel height -- effectively lowers $F_{Δh}$. Across all three scenarios, four key strategies consistently emerged for optimizing LCOE: (1) minimizing the axial reflector contribution when costly, (2) reducing control drum reliance, (3) substituting expensive tri-structural isotropic (TRISO) fuel with axial reflector material priced at the level of graphite, and (4) maximizing fuel burnup. While PEARL demonstrates promise in navigating trade-offs across diverse design scenarios, discrepancies between surrogate model predictions and full-order simulations remain. Further improvements are anticipated through constraint relaxation and surrogate development, constituting an ongoing area of investigation.

</details>


### [96] [Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery](https://arxiv.org/abs/2601.20088)
*Meng Xin,Sweta Priyadarshi,Jingyu Xin,Bilal Kartal,Aditya Vavre,Asma Kuriparambil Thekkumpate,Zijia Chen,Ameya Sunil Mahabaleshwarkar,Ido Shahaf,Akhiad Bercovich,Kinjal Patel,Suguna Varshini Velury,Chenjie Luo,Zhiyu Cheng,Jenny Chen,Chen-Han Yu,Wei Ping,Oleg Rybakov,Nima Tajbakhsh,Oluwatobi Olabiyi,Dusan Stosic,Di Wu,Song Han,Eric Chung,Sharath Turuvekere Sreenivas,Bryan Catanzaro,Yoshi Suhara,Tijmen Blankevoort,Huizi Mao*

Main category: cs.LG

TL;DR: 提出量化感知蒸馏（QAD）方法恢复NVFP4量化大语言模型和视觉语言模型的精度，评估显示能恢复到接近BF16精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统量化感知训练（QAT）在多阶段后训练管道中存在的工程复杂性和训练不稳定性问题，且能在数据质量和覆盖度不足时恢复精度。

Method: 使用KL散度损失将全精度教师模型蒸馏到量化学生模型。

Result: 在多个后训练模型上评估，QAD能使模型一致恢复到接近BF16的精度。

Conclusion: QAD对于多阶段后训练管道的模型有显著有效性和稳定性，且对数据质量和覆盖度有鲁棒性。

Abstract: This technical report presents quantization-aware distillation (QAD) and our best practices for recovering accuracy of NVFP4-quantized large language models (LLMs) and vision-language models (VLMs). QAD distills a full-precision teacher model into a quantized student model using a KL divergence loss. While applying distillation to quantized models is not a new idea, we observe key advantages of QAD for today's LLMs: 1. It shows remarkable effectiveness and stability for models trained through multi-stage post-training pipelines, including supervised fine-tuning (SFT), reinforcement learning (RL), and model merging, where traditional quantization-aware training (QAT) suffers from engineering complexity and training instability; 2. It is robust to data quality and coverage, enabling accuracy recovery without full training data. We evaluate QAD across multiple post-trained models including AceReason Nemotron, Nemotron 3 Nano, Nemotron Nano V2, Nemotron Nano V2 VL (VLM), and Llama Nemotron Super v1, showing consistent recovery to near-BF16 accuracy.

</details>


### [97] [In-Context Reinforcement Learning From Suboptimal Historical Data](https://arxiv.org/abs/2601.20116)
*Juncheng Dong,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 本文探索基于自回归Transformer进行上下文强化学习，提出DIT框架解决离线数据次优问题，实验表明DIT性能优越。


<details>
  <summary>Details</summary>
Motivation: 受Transformer上下文学习能力启发，探索其在上下文强化学习中的应用，解决离线数据来自次优策略导致的性能问题。

Method: 先在离线数据集上训练Transformer，提出DIT框架，训练基于Transformer的值函数和策略，用加权最大似然估计损失训练策略。

Result: 在多臂老虎机和马尔可夫决策过程问题实验中，DIT表现优越，尤其在离线数据含次优历史数据时。

Conclusion: DIT框架能有效解决离线数据次优问题，提升上下文强化学习性能。

Abstract: Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.

</details>


### [98] [A Reinforcement Learning Based Universal Sequence Design for Polar Codes](https://arxiv.org/abs/2601.20118)
*David Kin Wai Ho,Arman Fazeli,Mohamad M. Mansour,Louay M. A. Jalloul*

Main category: cs.LG

TL;DR: 为6G应用开发强化学习的通用序列设计框架，适用多种条件，对码长可达2048有效，性能有竞争力且有增益。


<details>
  <summary>Details</summary>
Motivation: 推进6G应用的极化码设计，找到适合不同信道条件和解码策略、能用于标准化设计方法。

Method: 开发基于强化学习的通用序列设计框架，通过结合物理定律约束学习、利用决策的弱长期影响和联合多配置优化实现大规模学习。

Result: 在5G支持的所有（N，K）配置中表现有竞争力，在N = 2048时比beta - expansion基线有0.2 dB增益。

Conclusion: 提出的框架可扩展、适应多种情况，适用于标准化，通过大规模学习能提升极化码设计性能。

Abstract: To advance Polar code design for 6G applications, we develop a reinforcement learning-based universal sequence design framework that is extensible and adaptable to diverse channel conditions and decoding strategies. Crucially, our method scales to code lengths up to $2048$, making it suitable for use in standardization. Across all $(N,K)$ configurations supported in 5G, our approach achieves competitive performance relative to the NR sequence adopted in 5G and yields up to a 0.2 dB gain over the beta-expansion baseline at $N=2048$. We further highlight the key elements that enabled learning at scale: (i) incorporation of physical law constrained learning grounded in the universal partial order property of Polar codes, (ii) exploitation of the weak long term influence of decisions to limit lookahead evaluation, and (iii) joint multi-configuration optimization to increase learning efficiency.

</details>


### [99] [HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH](https://arxiv.org/abs/2601.20255)
*Yueyang Wang,Jiawei Fu,Baolong Bi,Xili Wang,Xiaoqing Liu*

Main category: cs.LG

TL;DR: 本文针对SWE - bench评估大模型软件任务时缺乏有效训练中期指标的问题，提出数据过滤策略和熵压缩假设，构建新指标HE - SNR，验证其有更好性能。


<details>
  <summary>Details</summary>
Motivation: SWE - bench评估大模型软件工程任务时，缺乏有效指导中期训练的指标，标准指标存在问题。

Method: 引入数据过滤策略，提出熵压缩假设，基于细粒度熵分析构建新指标HE - SNR。

Result: 在不同上下文窗口的工业级混合专家模型上验证，该方法有更好的鲁棒性和预测能力。

Conclusion: 为优化大模型在复杂工程领域的潜力提供理论基础和实用工具。

Abstract: SWE-bench has emerged as the premier benchmark for evaluating Large Language Models on complex software engineering tasks. While these capabilities are fundamentally acquired during the mid-training phase and subsequently elicited during Supervised Fine-Tuning (SFT), there remains a critical deficit in metrics capable of guiding mid-training effectively. Standard metrics such as Perplexity (PPL) are compromised by the "Long-Context Tax" and exhibit weak correlation with downstream SWE performance. In this paper, we bridge this gap by first introducing a rigorous data filtering strategy. Crucially, we propose the Entropy Compression Hypothesis, redefining intelligence not by scalar Top-1 compression, but by the capacity to structure uncertainty into Entropy-Compressed States of low orders ("reasonable hesitation"). Grounded in this fine-grained entropy analysis, we formulate a novel metric, HE-SNR (High-Entropy Signal-to-Noise Ratio). Validated on industrial-scale Mixture-of-Experts (MoE) models across varying context windows (32K/128K), our approach demonstrates superior robustness and predictive power. This work provides both the theoretical foundation and practical tools for optimizing the latent potential of LLMs in complex engineering domains.

</details>


### [100] [Going NUTS with ADVI: Exploring various Bayesian Inference techniques with Facebook Prophet](https://arxiv.org/abs/2601.20120)
*Jovan Krajevski,Biljana Tojtovska Ribarski*

Main category: cs.LG

TL;DR: 本文针对Facebook Prophet模型在贝叶斯推断技术应用和API灵活性上的不足，在PyMC中重新实现了Prophet模型，并分析不同贝叶斯推断技术。


<details>
  <summary>Details</summary>
Motivation: Facebook Prophet默认推断技术有限且API灵活性不足，无法满足自定义建模需求。

Method: 在PyMC中重新实现Prophet模型，对时间序列预测问题采用全MCMC技术、MAP估计和变分推断技术。

Result: 详细分析了不同贝叶斯推断技术的采样方法、收敛诊断、预测指标和计算效率，发现了可能存在的问题。

Conclusion: 在PyMC中重新实现Prophet模型可扩展基础模型和评估多种贝叶斯推断方法，发现的问题将在未来工作中解决。

Abstract: Since its introduction, Facebook Prophet has attracted positive attention from both classical statisticians and the Bayesian statistics community. The model provides two built-in inference methods: maximum a posteriori estimation using the L-BFGS-B algorithm, and Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS). While exploring various time-series forecasting problems using Bayesian inference with Prophet, we encountered limitations stemming from the inability to apply alternative inference techniques beyond those provided by default. Additionally, the fluent API design of Facebook Prophet proved insufficiently flexible for implementing our custom modeling ideas. To address these shortcomings, we developed a complete reimplementation of the Prophet model in PyMC, which enables us to extend the base model and evaluate and compare multiple Bayesian inference methods. In this paper, we present our PyMC-based implementation and analyze in detail the implementation of different Bayesian inference techniques. We consider full MCMC techniques, MAP estimation and Variational inference techniques on a time-series forecasting problem. We discuss in details the sampling approach, convergence diagnostics, forecasting metrics as well as their computational efficiency and detect possible issues which will be addressed in our future work.

</details>


### [101] [Membership Inference Attacks Against Fine-tuned Diffusion Language Models](https://arxiv.org/abs/2601.20125)
*Yuetian Chen,Kaiyuan Zhang,Yuntao Du,Edoardo Stoppa,Charles Fleming,Ashish Kundu,Bruno Ribeiro,Ninghui Li*

Main category: cs.LG

TL;DR: 本文首次系统研究扩散语言模型（DLMs）在成员推理攻击（MIA）下的隐私漏洞，提出SAMA攻击方法，实验表明其效果优于基线，揭示DLMs存在显著隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: DLMs在MIA下的隐私泄漏问题未得到充分研究，有必要对其MIA漏洞进行系统调查。

Method: 引入SAMA攻击方法，通过对不同密度的掩码子集采样，应用基于符号的统计量，并采用逆加权聚合，将稀疏记忆检测转化为稳健的投票机制。

Result: 在九个数据集上的实验显示，SAMA相对最佳基线实现了30%的AUC提升，在低误报率下提升达8倍。

Conclusion: DLMs存在显著且此前未知的漏洞，需要开发专门的隐私防御措施。

Abstract: Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the first systematic investigation of MIA vulnerabilities in DLMs. Unlike the autoregressive models' single fixed prediction pattern, DLMs' multiple maskable configurations exponentially increase attack opportunities. This ability to probe many independent masks dramatically improves detection chances. To exploit this, we introduce SAMA (Subset-Aggregated Membership Attack), which addresses the sparse signal challenge through robust aggregation. SAMA samples masked subsets across progressive densities and applies sign-based statistics that remain effective despite heavy-tailed noise. Through inverse-weighted aggregation prioritizing sparse masks' cleaner signals, SAMA transforms sparse memorization detection into a robust voting mechanism. Experiments on nine datasets show SAMA achieves 30% relative AUC improvement over the best baseline, with up to 8 times improvement at low false positive rates. These findings reveal significant, previously unknown vulnerabilities in DLMs, necessitating the development of tailored privacy defenses.

</details>


### [102] [Scaling Next-Brain-Token Prediction for MEG](https://arxiv.org/abs/2601.20138)
*Richard Csaky*

Main category: cs.LG

TL;DR: 提出用于源空间MEG的自回归模型，可跨数据集和扫描器处理长上下文，经测试能跨数据集泛化且生成较稳定。


<details>
  <summary>Details</summary>
Motivation: 构建能跨数据集和扫描器、处理长上下文的源空间MEG的自回归模型。

Method: 用修改的SEANet式向量量化器将多通道MEG转为扁平令牌流，训练Qwen2.5 - VL骨干用于预测下一脑令牌；引入三个任务匹配测试评估长期生成。

Result: 在CamCAN和Omega上训练，在MOUS上测试，跨指标看，长期生成较稳定，比交换控制更接近正确延续。

Conclusion: 模型能实现跨数据集泛化，长期生成较稳定。

Abstract: We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce three task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: https://github.com/ricsinaruto/brain-gen.

</details>


### [103] [Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning](https://arxiv.org/abs/2601.20154)
*Bo Dai,Na Li,Dale Schuurmans*

Main category: cs.LG

TL;DR: 自监督学习虽有进展但缺乏统一理解，本文从频谱表征视角研究其充分性，构建统一框架并启发有效算法。


<details>
  <summary>Details</summary>
Motivation: 自监督学习不同目标和流程缺乏统一清晰理解，阻碍表征学习发展，且方法快速增长，急需统一框架。

Method: 从频谱表征视角理论研究表征的充分性。

Result: 揭示现有成功自监督学习算法的频谱本质，为理解和分析搭建统一框架。

Conclusion: 该框架能启发在实际应用中以原则化方式开发更高效易用的表征学习算法。

Abstract: Self-supervised learning (SSL) have improved empirical performance by unleashing the power of unlabeled data for practical applications. Specifically, SSL extracts the representation from massive unlabeled data, which will be transferred to a plenty of down streaming tasks with limited data. The significant improvement on diverse applications of representation learning has attracted increasing attention, resulting in a variety of dramatically different self-supervised learning objectives for representation extraction, with an assortment of learning procedures, but the lack of a clear and unified understanding. Such an absence hampers the ongoing development of representation learning, leaving a theoretical understanding missing, principles for efficient algorithm design unclear, and the use of representation learning methods in practice unjustified. The urgency for a unified framework is further motivated by the rapid growth in representation learning methods. In this paper, we are therefore compelled to develop a principled foundation of representation learning. We first theoretically investigate the sufficiency of the representation from a spectral representation view, which reveals the spectral essence of the existing successful SSL algorithms and paves the path to a unified framework for understanding and analysis. Such a framework work also inspires the development of more efficient and easy-to-use representation learning algorithms with principled way in real-world applications.

</details>


### [104] [PASS: Ambiguity Guided Subsets for Scalable Classical and Quantum Constrained Clustering](https://arxiv.org/abs/2601.20157)
*Pedro Chumpitaz-Flores,My Duong,Ying Mao,Kaixun Hua*

Main category: cs.LG

TL;DR: 提出PASS框架用于成对约束聚类，可在满足约束条件下实现可扩展、高质量聚类，在不同基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有成对约束聚类方法在数据可扩展性上存在问题，特别是在量子或量子混合聚类等细分应用中。

Method: 提出PASS框架，将ML约束合并为伪点，提供约束感知边界规则和信息几何规则两种选择器。

Result: 在不同基准测试中，PASS以较低成本获得有竞争力的SSE，在先前方法失效的情况下仍有效。

Conclusion: PASS框架在成对约束聚类中能有效解决数据可扩展性问题，具有良好性能。

Abstract: Pairwise-constrained clustering augments unsupervised partitioning with side information by enforcing must-link (ML) and cannot-link (CL) constraints between specific samples, yielding labelings that respect known affinities and separations. However, ML and CL constraints add an extra layer of complexity to the clustering problem, with current methods struggling in data scalability, especially in niche applications like quantum or quantum-hybrid clustering. We propose PASS, a pairwise-constraints and ambiguity-driven subset selection framework that preserves ML and CL constraints satisfaction while allowing scalable, high-quality clustering solution. PASS collapses ML constraints into pseudo-points and offers two selectors: a constraint-aware margin rule that collects near-boundary points and all detected CL violations, and an information-geometric rule that scores points via a Fisher-Rao distance derived from soft assignment posteriors, then selects the highest-information subset under a simple budget. Across diverse benchmarks, PASS attains competitive SSE at substantially lower cost than exact or penalty-based methods, and remains effective in regimes where prior approaches fail.

</details>


### [105] [What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering](https://arxiv.org/abs/2601.20164)
*Jim Maar,Denis Paperno,Callum Stuart McDougall,Neel Nanda*

Main category: cs.LG

TL;DR: 提出评估语言模型隐式规划的简单技术，通过案例研究证明方法可扩展性，发现隐式规划是通用机制，小模型也存在，方法可用于研究大模型规划能力，对AI安全和控制有意义。


<details>
  <summary>Details</summary>
Motivation: 现有工作仅通过定性研究支持语言模型存在隐式规划行为，需要更简单的评估技术。

Method: 提出评估语言模型隐式规划的简单技术，并在押韵诗歌生成和问答任务上进行案例研究。

Result: 方法可扩展到多个模型，能通过向量操纵生成的押韵或答案，发现隐式规划在1B参数的小模型中也存在。

Conclusion: 提出的方法是研究大模型隐式规划能力的通用直接方法，理解语言模型规划能力对AI安全和控制有帮助。

Abstract: Prior work suggests that language models, while trained on next token prediction, show implicit planning behavior: they may select the next token in preparation to a predicted future token, such as a likely rhyming word, as supported by a prior qualitative study of Claude 3.5 Haiku using a cross-layer transcoder. We propose much simpler techniques for assessing implicit planning in language models. With case studies on rhyme poetry generation and question answering, we demonstrate that our methodology easily scales to many models. Across models, we find that the generated rhyme (e.g. "-ight") or answer to a question ("whale") can be manipulated by steering at the end of the preceding line with a vector, affecting the generation of intermediate tokens leading up to the rhyme or answer word. We show that implicit planning is a universal mechanism, present in smaller models than previously thought, starting from 1B parameters. Our methodology offers a widely applicable direct way to study implicit planning abilities of LLMs. More broadly, understanding planning abilities of language models can inform decisions in AI safety and control.

</details>


### [106] [Local Duality for Sparse Support Vector Machines](https://arxiv.org/abs/2601.20170)
*Penghe Zhang,Naihua Xiu,Houduo Qi*

Main category: cs.LG

TL;DR: 本文为稀疏支持向量机（SSVM）发展局部对偶理论，证明其与0/1损失SVM对偶关系，给出超参选择指导，解释其性能优势并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有推导SSVM的方法缺乏理论依据，需填补理论空白。

Method: 为SSVM发展局部对偶理论，探索其与hSVM和rSVM的关系，证明相关定理。

Result: 证明SSVM是0/1损失SVM的对偶问题，线性表示定理成立，hSVM全局解收敛到0/1损失SVM局部解等，实验展示SSVM优势。

Conclusion: 局部对偶理论为SSVM提供理论支持，解释其性能优势，且提出的局部解有应用潜力。

Abstract: Due to the rise of cardinality minimization in optimization, sparse support vector machines (SSVMs) have attracted much attention lately and show certain empirical advantages over convex SVMs. A common way to derive an SSVM is to add a cardinality function such as $\ell_0$-norm to the dual problem of a convex SVM. However, this process lacks theoretical justification. This paper fills the gap by developing a local duality theory for such an SSVM formulation and exploring its relationship with the hinge-loss SVM (hSVM) and the ramp-loss SVM (rSVM). In particular, we prove that the derived SSVM is exactly the dual problem of the 0/1-loss SVM, and the linear representer theorem holds for their local solutions. The local solution of SSVM also provides guidelines on selecting hyperparameters of hSVM and rSVM. {Under specific conditions, we show that a sequence of global solutions of hSVM converges to a local solution of 0/1-loss SVM. Moreover, a local minimizer of 0/1-loss SVM is a local minimizer of rSVM.} This explains why a local solution induced by SSVM outperforms hSVM and rSVM in the prior empirical study. We further conduct numerical tests on real datasets and demonstrate potential advantages of SSVM by working with locally nice solutions proposed in this paper.

</details>


### [107] [Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence Functions Reveal About Robust Generalization](https://arxiv.org/abs/2601.20172)
*James Amarel,Robyn Miller,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Alexei Skurikhin,Earl Lawrence,Gerd J. Kunde*

Main category: cs.LG

TL;DR: 本文提出基于影响的诊断方法研究偏微分方程解算子神经模拟器内化物理对称性的方式，应用于流体流动模拟器，得出评估替代模型是否内化对称性的新技术。


<details>
  <summary>Details</summary>
Motivation: 研究偏微分方程解算子的神经模拟器如何内化物理对称性。

Method: 引入基于影响的诊断方法，测量对称相关状态间参数更新的传播，即沿群轨道评估的损失梯度的度量加权重叠。

Result: 轨道梯度相干性为学习在对称变换上的泛化提供机制，能指示训练何时选择对称兼容盆地。

Conclusion: 得到评估替代模型是否内化已知解算子对称属性的新技术。

Abstract: We study how neural emulators of partial differential equation solution operators internalize physical symmetries by introducing an influence-based diagnostic that measures the propagation of parameter updates between symmetry-related states, defined as the metric-weighted overlap of loss gradients evaluated along group orbits. This quantity probes the local geometry of the learned loss landscape and goes beyond forward-pass equivariance tests by directly assessing whether learning dynamics couple physically equivalent configurations. Applying our diagnostic to autoregressive fluid flow emulators, we show that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects a symmetry compatible basin. The result is a novel technique for evaluating if surrogate models have internalized symmetry properties of the known solution operator.

</details>


### [108] [MAPLE: Self-supervised Learning-Enhanced Nonlinear Dimensionality Reduction for Visual Analysis](https://arxiv.org/abs/2601.20173)
*Zeyang Huang,Takanori Fujiwara,Angelos Chatzimparmpas,Wandrille Duchemin,Andreas Kerren*

Main category: cs.LG

TL;DR: 提出新的非线性降维方法MAPLE，通过改进流形建模增强UMAP，评估显示其效果优于UMAP且计算成本相当。


<details>
  <summary>Details</summary>
Motivation: 改进现有UMAP方法，更好处理高维数据中簇内方差大、流形结构弯曲的数据。

Method: 采用自监督学习方法，利用最大流形容量表示（MMCRs）编码低维流形几何，压缩局部相似数据点间方差，放大不同数据点间方差。

Result: 定性和定量评估表明，MAPLE比UMAP能产生更清晰的视觉聚类分离和更精细的子聚类分辨率，同时计算成本相当。

Conclusion: MAPLE在处理特定高维数据方面优于UMAP，有效改进了流形建模。

Abstract: We present a new nonlinear dimensionality reduction method, MAPLE, that enhances UMAP by improving manifold modeling. MAPLE employs a self-supervised learning approach to more efficiently encode low-dimensional manifold geometry. Central to this approach are maximum manifold capacity representations (MMCRs), which help untangle complex manifolds by compressing variances among locally similar data points while amplifying variance among dissimilar data points. This design is particularly effective for high-dimensional data with substantial intra-cluster variance and curved manifold structures, such as biological or image data. Our qualitative and quantitative evaluations demonstrate that MAPLE can produce clearer visual cluster separations and finer subcluster resolution than UMAP while maintaining comparable computational cost.

</details>


### [109] [NeuraLSP: An Efficient and Rigorous Neural Left Singular Subspace Preconditioner for Conjugate Gradient Methods](https://arxiv.org/abs/2601.20174)
*Alexander Benanti,Xi Han,Hong Qin*

Main category: cs.LG

TL;DR: 本文提出NeuraLSP这一新型神经预处理器，结合新损失指标，可应对秩膨胀问题，有理论保证和实验验证，加速可达53%。


<details>
  <summary>Details</summary>
Motivation: 现有用图神经网络求解偏微分方程的技术聚合离散系统矩阵成图时会有秩膨胀和收敛率欠佳问题，需要改进。

Method: 提出NeuraLSP新型神经预处理器，结合利用系统矩阵近零空间向量左奇异子空间的新损失指标，将谱信息压缩到固定低秩算子。

Result: 方法有理论保证，对秩膨胀有鲁棒性，加速最高可达53%，综合实验验证理论进展。

Conclusion: NeuraLSP方法有效，在求解偏微分方程的线性系统方面有良好表现。

Abstract: Numerical techniques for solving partial differential equations (PDEs) are integral for many fields across science and engineering. Such techniques usually involve solving large, sparse linear systems, where preconditioning methods are critical. In recent years, neural methods, particularly graph neural networks (GNNs), have demonstrated their potential through accelerated convergence. Nonetheless, to extract connective structures, existing techniques aggregate discretized system matrices into graphs, and suffer from rank inflation and a suboptimal convergence rate. In this paper, we articulate NeuraLSP, a novel neural preconditioner combined with a novel loss metric that leverages the left singular subspace of the system matrix's near-nullspace vectors. By compressing spectral information into a fixed low-rank operator, our method exhibits both theoretical guarantees and empirical robustness to rank inflation, affording up to a 53% speedup. Besides the theoretical guarantees for our newly-formulated loss function, our comprehensive experimental results across diverse families of PDEs also substantiate the aforementioned theoretical advances.

</details>


### [110] [Causal-Driven Feature Evaluation for Cross-Domain Image Classification](https://arxiv.org/abs/2601.20176)
*Chen Cheng,Ang Li*

Main category: cs.LG

TL;DR: 从因果角度重新审视OOD分类问题，提出基于因果有效性评估表征的框架，实验显示在OOD性能上有提升。


<details>
  <summary>Details</summary>
Motivation: 现有OOD分类方法追求领域不变表征，但不变特征不一定有因果有效性，需新方法。

Method: 从因果角度重新审视OOD分类，引入显式的段级框架直接衡量跨领域的因果有效性。

Result: 在多领域基准测试中，OOD性能有持续提升，尤其在具有挑战性的领域转移下。

Conclusion: 因果评估对鲁棒泛化有价值。

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in real-world classification, where test distributions often differ substantially from training data. Most existing approaches pursue domain-invariant representations, implicitly assuming that invariance implies reliability. However, features that are invariant across domains are not necessarily causally effective for prediction.
  In this work, we revisit OOD classification from a causal perspective and propose to evaluate learned representations based on their necessity and sufficiency under distribution shift. We introduce an explicit segment-level framework that directly measures causal effectiveness across domains, providing a more faithful criterion than invariance alone.
  Experiments on multi-domain benchmarks demonstrate consistent improvements in OOD performance, particularly under challenging domain shifts, highlighting the value of causal evaluation for robust generalization.

</details>


### [111] [On the Computational Complexity of Performative Prediction](https://arxiv.org/abs/2601.20180)
*Ioannis Anagnostides,Rohan Chauhan,Ioannis Panageas,Tuomas Sandholm,Jingming Yan*

Main category: cs.LG

TL;DR: 本文研究 performative prediction 中$ρ > 1$情况的复杂度，得出计算$ε$-performatively stable point 是 PPAD 完全问题，对战略分类特殊情况证明计算战略局部最优是 PLS 难问题。


<details>
  <summary>Details</summary>
Motivation: 此前 performative prediction 在$ρ > 1$情况的复杂度未知，有必要进行研究。

Method: 理论推导证明计算$ε$-performatively stable point 的复杂度，将 PPAD 困难性结果扩展到一般凸域，研究战略分类特殊情况。

Result: 计算$ε$-performatively stable point 是 PPAD 完全问题，在二次损失函数和线性分布转移设置下难处理性仍存在，战略分类中计算战略局部最优是 PLS 难问题。

Conclusion: 在 performative prediction 的$ρ > 1$情况计算$ε$-performatively stable point 具有较高复杂度，战略分类中计算局部最优也是困难的。

Abstract: Performative prediction captures the phenomenon where deploying a predictive model shifts the underlying data distribution. While simple retraining dynamics are known to converge linearly when the performative effects are weak ($ρ< 1$), the complexity in the regime $ρ> 1$ was hitherto open. In this paper, we establish a sharp phase transition: computing an $ε$-performatively stable point is PPAD-complete -- and thus polynomial-time equivalent to Nash equilibria in general-sum games -- even when $ρ= 1 + O(ε)$. This intractability persists even in the ostensibly simple setting with a quadratic loss function and linear distribution shifts. One of our key technical contributions is to extend this PPAD-hardness result to general convex domains, which is of broader interest in the complexity of variational inequalities. Finally, we address the special case of strategic classification, showing that computing a strategic local optimum is PLS-hard.

</details>


### [112] [Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery](https://arxiv.org/abs/2601.20193)
*Zhipeng Zhang,Wenting Ma,Kai Li,Meng Guo,Lei Yang,Wei Yu,Hongji Cui,Yichen Zhang,Mo Zhang,Jinzhe Lin,Zhenjie Yao*

Main category: cs.LG

TL;DR: 提出元认知强化学习框架，在奖励损坏的连续控制基准测试中表现优于强鲁棒性基线。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒强化学习方法缺乏对自身学习过程可靠性的推理能力，常出现过度保守或灾难性失败。

Method: 提出基于内部估计可靠性信号评估、调节和恢复学习行为的元认知强化学习框架，引入由价值预测误差稳定性驱动的元信任变量，通过故障安全调节和逐步信任恢复调节学习动态。

Result: 在奖励损坏的连续控制基准测试中，具有恢复能力的元认知控制实现了更高的平均回报，并显著减少了后期训练失败。

Conclusion: 所提出的元认知强化学习框架有效提升了强化学习的性能和鲁棒性。

Abstract: Robust reinforcement learning methods typically focus on suppressing unreliable experiences or corrupted rewards, but they lack the ability to reason about the reliability of their own learning process. As a result, such methods often either overreact to noise by becoming overly conservative or fail catastrophically when uncertainty accumulates.
  In this work, we propose a meta-cognitive reinforcement learning framework that enables an agent to assess, regulate, and recover its learning behavior based on internally estimated reliability signals. The proposed method introduces a meta-trust variable driven by Value Prediction Error Stability (VPES), which modulates learning dynamics via fail-safe regulation and gradual trust recovery.
  Experiments on continuous-control benchmarks with reward corruption demonstrate that recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.

</details>


### [113] [DeRaDiff: Denoising Time Realignment of Diffusion Models](https://arxiv.org/abs/2601.20198)
*Ratnavibusena Don Shahain Manujith,Yang Zhang,Teoh Tze Tzun,Kenji Kawaguchi*

Main category: cs.LG

TL;DR: 本文介绍DeRaDiff方法，可在采样时调整正则化强度，近似不同强度训练的模型，减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法选择扩散模型正则化强度成本高，需找到更高效方法。

Method: 引入DeRaDiff，通过对连续潜在变量的迭代预测，用对齐后验和参考后验的几何混合替代反向步骤参考分布。

Result: 在多个指标上，该方法能很好近似不同正则化强度从头训练的模型。

Conclusion: DeRaDiff是搜索最优正则化强度的高效方法，大幅降低计算成本。

Abstract: Recent advances align diffusion models with human preferences to increase aesthetic appeal and mitigate artifacts and biases. Such methods aim to maximize a conditional output distribution aligned with higher rewards whilst not drifting far from a pretrained prior. This is commonly enforced by KL (Kullback Leibler) regularization. As such, a central issue still remains: how does one choose the right regularization strength? Too high of a strength leads to limited alignment and too low of a strength leads to "reward hacking". This renders the task of choosing the correct regularization strength highly non-trivial. Existing approaches sweep over this hyperparameter by aligning a pretrained model at multiple regularization strengths and then choose the best strength. Unfortunately, this is prohibitively expensive. We introduce DeRaDiff, a denoising time realignment procedure that, after aligning a pretrained model once, modulates the regularization strength during sampling to emulate models trained at other regularization strengths without any additional training or finetuning. Extending decoding-time realignment from language to diffusion models, DeRaDiff operates over iterative predictions of continuous latents by replacing the reverse step reference distribution by a geometric mixture of an aligned and reference posterior, thus giving rise to a closed form update under common schedulers and a single tunable parameter, lambda, for on the fly control. Our experiments show that across multiple text image alignment and image-quality metrics, our method consistently provides a strong approximation for models aligned entirely from scratch at different regularization strengths. Thus, our method yields an efficient way to search for the optimal strength, eliminating the need for expensive alignment sweeps and thereby substantially reducing computational costs.

</details>


### [114] [Hyperparameter Transfer with Mixture-of-Expert Layers](https://arxiv.org/abs/2601.20205)
*Tianze Jiang,Blake Bordelon,Cengiz Pehlevan,Boris Hanin*

Main category: cs.LG

TL;DR: 提出一种新参数化方法用于带MoE层的Transformer模型，实现超参可靠迁移并训练大模型。


<details>
  <summary>Details</summary>
Motivation: 稀疏MoE层增加训练复杂性，需要对新的可训练参数和架构规模维度进行超参调优，希望使超参选择廉价可靠。

Method: 提出一种新的参数化方法，并通过新型动态平均场理论（DMFT）分析证明其合理性。

Result: 在固定token预算下变化不同模型维度时，该参数化方法能实现从51M到超2B总参数模型间的可靠超参迁移；用小模型短token范围确定的超参训练大模型有良好表现。

Conclusion: 新的参数化方法可有效解决稀疏MoE层训练中超参选择问题，实现超参可靠迁移和大模型训练。

Abstract: Mixture-of-Experts (MoE) layers have emerged as an important tool in scaling up modern neural networks by decoupling total trainable parameters from activated parameters in the forward pass for each token. However, sparse MoEs add complexity to training due to (i) new trainable parameters (router weights) that, like all other parameter groups, require hyperparameter (HP) tuning; (ii) new architecture scale dimensions (number of and size of experts) that must be chosen and potentially taken large. To make HP selection cheap and reliable, we propose a new parameterization for transformer models with MoE layers when scaling model width, depth, number of experts, and expert (hidden) size. Our parameterization is justified by a novel dynamical mean-field theory (DMFT) analysis. When varying different model dimensions trained at a fixed token budget, we find empirically that our parameterization enables reliable HP transfer across models from 51M to over 2B total parameters. We further take HPs identified from sweeping small models on a short token horizon to train larger models on longer horizons and report performant model behaviors.

</details>


### [115] [Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning](https://arxiv.org/abs/2601.20209)
*Jinyang Wu,Shuo Yang,Changpeng Yang,Yuhao Shen,Shuai Zhang,Zhengqi Wen,Jianhua Tao*

Main category: cs.LG

TL;DR: 现有强化学习训练大语言模型用于长程任务时因高质量轨迹稀缺且资源分配不合理导致资源浪费，本文提出Spark框架，在关键决策点自适应分支探索，实验证明其用更少样本获更高成功率且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练大语言模型执行长程任务时，存在高质量轨迹稀缺、资源分配不合理导致浪费计算资源且无法保障样本质量的问题。

Method: 提出Spark框架，在关键决策状态进行选择性分支探索，利用智能体内在决策信号，在关键决策点激活自适应分支探索以探测有前景的轨迹。

Result: 在多种任务（如具身规划）的实验中，Spark用显著更少的训练样本实现了更高的成功率，在未见场景中也有强大的泛化性。

Conclusion: Spark框架能实现资源高效探索，减少对人类先验的依赖，使智能体自主扩展探索并实现更强泛化。

Abstract: Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \textbf{Spark} (\textbf{S}trategic \textbf{P}olicy-\textbf{A}ware explo\textbf{R}ation via \textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.

</details>


### [116] [An Accounting Identity for Algorithmic Fairness](https://arxiv.org/abs/2601.20217)
*Hadi Elzayn,Jacob Goldin*

Main category: cs.LG

TL;DR: 本文推导预测模型会计恒等式，揭示准确性与公平性联系，通过实验验证理论，结果可拓展到非二元结果预测任务。


<details>
  <summary>Details</summary>
Motivation: 探究预测模型准确性与常见公平性标准的关系。

Method: 推导会计恒等式，在基准数据上进行实验。

Result: 实验证实理论，许多公平性干预会在公平性违规间替代，降低准确性会扩大总不公平预算。结果可自然拓展到非二元结果预测任务。

Conclusion: 在二元预测任务中，准确性和公平性是互补的；额外结果信息可缓解公平性冲突。

Abstract: We derive an accounting identity for predictive models that links accuracy with common fairness criteria. The identity shows that for globally calibrated models, the weighted sums of miscalibration within groups and error imbalance across groups is equal to a "total unfairness budget." For binary outcomes, this budget is the model's mean-squared error times the difference in group prevalence across outcome classes. The identity nests standard impossibility results as special cases, while also describing inherent tradeoffs when one or more fairness measures are not perfectly satisfied. The results suggest that accuracy and fairness are best viewed as complements in binary prediction tasks: increasing accuracy necessarily shrinks the total unfairness budget and vice-versa. Experiments on benchmark data confirm the theory and show that many fairness interventions largely substitute between fairness violations, and when they reduce accuracy they tend to expand the total unfairness budget. The results extend naturally to prediction tasks with non-binary outcomes, illustrating how additional outcome information can relax fairness incompatibilities and identifying conditions under which the binary-style impossibility does and does not extend to regression tasks.

</details>


### [117] [Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization](https://arxiv.org/abs/2601.20226)
*Julian Gutierrez,Redouane Silvente*

Main category: cs.LG

TL;DR: 提出两种机器学习框架预测EPEX SPOT日前市场的聚合曲线和优化储能。


<details>
  <summary>Details</summary>
Motivation: 在EPEX SPOT日前市场进行需求和供给曲线的预测，并优化储能策略。

Method: 一是快速参数化模型，二是生成式模型；再基于预测优化储能策略。

Result: 实现对需求和供给曲线进行预测，并优化价格制定的储能策略、量化收益分布和突出价格压缩效应。

Conclusion: 模型可用于预测和优化该市场中的曲线和储能。

Abstract: We present two machine learning frameworks for forecasting aggregated curves and optimizing storage in the EPEX SPOT day-ahead market. First, a fast parametric model forecasts hourly demand and supply curves in a low-dimensional and grid-robust representation, with minimum and maximum volumes combined with a Chebyshev polynomial for the elastic segment. The model enables daily use with low error and clear interpretability. Second, for a more comprehensive analysis, though less suited to daily operation, we employ generative models that learn the joint distribution of 24-hour order-level submissions given weather and fuel variables. These models generate synthetic daily scenarios of individual buy and sell orders, which, once aggregated, yield hourly supply and demand curves. Based on these forecasts, we optimize a price-making storage strategy, quantify revenue distributions, and highlight the price-compression effect with lower peaks, higher off-peak levels, and diminishing returns as capacity expands.

</details>


### [118] [ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance](https://arxiv.org/abs/2601.20227)
*Zichao Yu,Ming Li,Wenyi Zhang,Difan Zou,Weiguo Gao*

Main category: cs.LG

TL;DR: 提出ProFlow框架用于零样本物理一致采样，经基准测试表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型解决物理逆问题时难以在不重新训练或破坏生成先验的情况下强制满足物理约束，需新采样机制。

Method: 提出ProFlow框架，采用两步方案，交替进行终端优化步骤和插值步骤，有贝叶斯解释。

Result: 在泊松、亥姆霍兹、达西和粘性伯格斯方程的综合基准测试中，ProFlow在物理和观测一致性及分布统计准确性上优于基于扩散和流的现有基线方法。

Conclusion: ProFlow框架能有效实现零样本物理一致采样，在相关方程问题上表现良好。

Abstract: Inferring physical fields from sparse observations while strictly satisfying partial differential equations (PDEs) is a fundamental challenge in computational physics. Recently, deep generative models offer powerful data-driven priors for such inverse problems, yet existing methods struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. Consequently, there is a critical need for a sampling mechanism that can reconcile strict physical consistency and observational fidelity with the statistical structure of the pre-trained prior. To this end, we present ProFlow, a proximal guidance framework for zero-shot physics-consistent sampling, defined as inferring solutions from sparse observations using a fixed generative prior without task-specific retraining. The algorithm employs a rigorous two-step scheme that alternates between: (\romannumeral1) a terminal optimization step, which projects the flow prediction onto the intersection of the physically and observationally consistent sets via proximal minimization; and (\romannumeral2) an interpolation step, which maps the refined state back to the generative trajectory to maintain consistency with the learned flow probability path. This procedure admits a Bayesian interpretation as a sequence of local maximum a posteriori (MAP) updates. Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations demonstrate that ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics, compared to state-of-the-art diffusion- and flow-based baselines.

</details>


### [119] [Certificate-Guided Pruning for Stochastic Lipschitz Optimization](https://arxiv.org/abs/2601.20231)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 研究带噪声评估的Lipschitz函数黑盒优化，提出CGP方法及三个扩展，实验显示CGP变体表现良好并能提供原则性停止准则。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应离散化方法不能提供最优性的明确证明或可测量的进展保证。

Method: 引入Certificate - Guided Pruning (CGP)方法，通过置信调整的Lipschitz包络维护潜在最优点的活动集 ，并开发CGP - Adaptive、CGP - TR和CGP - Hybrid三个扩展。

Result: 在12个基准测试（d∈[2, 100]）中，证明Vol(At)以可控速率收缩得出样本复杂度；CGP变体表现与强基线相当或更优。

Conclusion: CGP方法及其扩展能在带噪声评估的Lipschitz函数黑盒优化中有效工作，并提供了原则性停止准则。

Abstract: We study black-box optimization of Lipschitz functions under noisy evaluations. Existing adaptive discretization methods implicitly avoid suboptimal regions but do not provide explicit certificates of optimality or measurable progress guarantees. We introduce \textbf{Certificate-Guided Pruning (CGP)}, which maintains an explicit \emph{active set} $A_t$ of potentially optimal points via confidence-adjusted Lipschitz envelopes. Any point outside $A_t$ is certifiably suboptimal with high probability, and under a margin condition with near-optimality dimension $α$, we prove $\Vol(A_t)$ shrinks at a controlled rate yielding sample complexity $\tildeO(\varepsilon^{-(2+α)})$. We develop three extensions: CGP-Adaptive learns $L$ online with $O(\log T)$ overhead; CGP-TR scales to $d > 50$ via trust regions with local certificates; and CGP-Hybrid switches to GP refinement when local smoothness is detected. Experiments on 12 benchmarks ($d \in [2, 100]$) show CGP variants match or exceed strong baselines while providing principled stopping criteria via certificate volume.

</details>


### [120] [Robust SDE Parameter Estimation Under Missing Time Information Setting](https://arxiv.org/abs/2601.20268)
*Long Van Tran,Truyen Tran,Phuoc Nguyen*

Main category: cs.LG

TL;DR: 本文针对随机微分方程（SDEs）参数估计中时间顺序信息缺失的问题，提出同时重建时间信息和估计SDE参数的框架，实验证明了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有SDEs参数估计方法依赖准确时间戳观测序列，时间顺序信息受损、缺失或隐藏时会失效，需解决该问题。

Method: 利用前后向过程的不对称性，推导得分匹配准则推断观测对的正确时间顺序，通过排序恢复总顺序，用最大似然法从重建序列估计SDE参数。

Result: 在合成和真实数据集上的大量实验证明了方法的有效性。

Conclusion: 该方法将参数估计扩展到时间顺序缺失的场景，拓宽了在敏感领域的适用性。

Abstract: Recent advances in stochastic differential equations (SDEs) have enabled robust modeling of real-world dynamical processes across diverse domains, such as finance, health, and systems biology. However, parameter estimation for SDEs typically relies on accurately timestamped observational sequences. When temporal ordering information is corrupted, missing, or deliberately hidden (e.g., for privacy), existing estimation methods often fail. In this paper, we investigate the conditions under which temporal order can be recovered and introduce a novel framework that simultaneously reconstructs temporal information and estimates SDE parameters. Our approach exploits asymmetries between forward and backward processes, deriving a score-matching criterion to infer the correct temporal order between pairs of observations. We then recover the total order via a sorting procedure and estimate SDE parameters from the reconstructed sequence using maximum likelihood. Finally, we conduct extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness of our method, extending parameter estimation to settings with missing temporal order and broadening applicability in sensitive domains.

</details>


### [121] [The Forecast After the Forecast: A Post-Processing Shift in Time Series](https://arxiv.org/abs/2601.20280)
*Daojun Liang,Qi Li,Yinglong Wang,Jing Chen,Hu Zhang,Xiaoxiao Cui,Qizheng Wang,Shuo Li*

Main category: cs.LG

TL;DR: 提出δ - Adapter提升时间序列预测精度和校准度，无需重新训练，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有预测模型精度提升接近瓶颈，需通过后处理策略改善时间序列预测最后一公里问题，即不重新训练或修改已部署骨干模型来提高精度和处理不确定性。

Method: 提出δ - Adapter，在输入微调（对协变量软编辑）和输出残差校正两个接口学习微小有界模块，可作为特征选择器和分布校准器。

Result: 实验表明δ - Adapter在不同骨干模型和数据集上，以可忽略的计算量和无接口更改的方式提高了精度和校准度。

Conclusion: δ - Adapter是一种轻量级、与架构无关的方法，能有效提升已部署时间序列预测器性能。

Abstract: Time series forecasting has long been dominated by advances in model architecture, with recent progress driven by deep learning and hybrid statistical techniques. However, as forecasting models approach diminishing returns in accuracy, a critical yet underexplored opportunity emerges: the strategic use of post-processing. In this paper, we address the last-mile gap in time-series forecasting, which is to improve accuracy and uncertainty without retraining or modifying a deployed backbone. We propose $δ$-Adapter, a lightweight, architecture-agnostic way to boost deployed time series forecasters without retraining. $δ$-Adapter learns tiny, bounded modules at two interfaces: input nudging (soft edits to covariates) and output residual correction. We provide local descent guarantees, $O(δ)$ drift bounds, and compositional stability for combined adapters. Meanwhile, it can act as a feature selector by learning a sparse, horizon-aware mask over inputs to select important features, thereby improving interpretability. In addition, it can also be used as a distribution calibrator to measure uncertainty. Thus, we introduce a Quantile Calibrator and a Conformal Corrector that together deliver calibrated, personalized intervals with finite-sample coverage. Our experiments across diverse backbones and datasets show that $δ$-Adapter improves accuracy and calibration with negligible compute and no interface changes.

</details>


### [122] [Memory Retrieval in Transformers: Insights from The Encoding Specificity Principle](https://arxiv.org/abs/2601.20282)
*Viet Hung Dinh,Ming Ding,Youyang Qu,Kanchana Thilakarathna*

Main category: cs.LG

TL;DR: 研究探索Transformer大语言模型注意力层记忆机制，提出关键词作为线索假设并验证，还分离出特定神经元用于提取关键词以用于下游应用。


<details>
  <summary>Details</summary>
Motivation: 可解释人工智能领域有待发展，监管压力增大，且Transformer注意力层作用未被充分探索，因此研究其记忆机制。

Method: 借鉴心理学和计算心理语言学研究，以编码特异性原则为指导，提出关键词作为线索的假设并验证。

Result: 为关键词作为线索的假设提供了证据，分离出注意力层中能选择性编码和促进上下文定义关键词检索的神经元。

Conclusion: 关键词可从识别出的神经元中提取，有助于下游的如遗忘学习等应用。

Abstract: While explainable artificial intelligence (XAI) for large language models (LLMs) remains an evolving field with many unresolved questions, increasing regulatory pressures have spurred interest in its role in ensuring transparency, accountability, and privacy-preserving machine unlearning. Despite recent advances in XAI have provided some insights, the specific role of attention layers in transformer based LLMs remains underexplored. This study investigates the memory mechanisms instantiated by attention layers, drawing on prior research in psychology and computational psycholinguistics that links Transformer attention to cue based retrieval in human memory. In this view, queries encode the retrieval context, keys index candidate memory traces, attention weights quantify cue trace similarity, and values carry the encoded content, jointly enabling the construction of a context representation that precedes and facilitates memory retrieval. Guided by the Encoding Specificity Principle, we hypothesize that the cues used in the initial stage of retrieval are instantiated as keywords. We provide converging evidence for this keywords-as-cues hypothesis. In addition, we isolate neurons within attention layers whose activations selectively encode and facilitate the retrieval of context-defining keywords. Consequently, these keywords can be extracted from identified neurons and further contribute to downstream applications such as unlearning.

</details>


### [123] [A Learning-based Framework for Spatial Impulse Response Compensation in 3D Photoacoustic Computed Tomography](https://arxiv.org/abs/2601.20291)
*Kaiyi Yang,Seonyeong Park,Gangwon Jeong,Hsuan-Kai Huang,Alexander A. Oraevsky,Umberto Villa,Mark A. Anastasio*

Main category: cs.LG

TL;DR: 本文提出数据域学习型空间脉冲响应（SIR）补偿方法框架用于3D光声计算机断层成像（PACT）图像重建，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有PACT成像中，忽略换能器SIR的解析重建方法影响空间分辨率，基于优化的重建方法计算成本高，需准确快速的3D PACT图像重建方法。

Method: 提出在数据域建立学习型SIR补偿方法的框架，将SIR损坏的PACT测量数据映射到理想点换能器记录的补偿数据，研究U-Net和Deconv - Net两种补偿模型，还有快速解析训练数据生成程序。

Result: 虚拟成像研究验证框架可提高分辨率，对噪声、对象复杂度和声速异质性有鲁棒性；应用于体内乳腺成像数据，能揭示被SIR伪影掩盖的精细结构。

Conclusion: 该框架是首次在3D PACT成像中展示学习型SIR补偿，有效解决了准确快速重建的问题。

Abstract: Photoacoustic computed tomography (PACT) is a promising imaging modality that combines the advantages of optical contrast with ultrasound detection. Utilizing ultrasound transducers with larger surface areas can improve detection sensitivity. However, when computationally efficient analytic reconstruction methods that neglect the spatial impulse responses (SIRs) of the transducer are employed, the spatial resolution of the reconstructed images will be compromised. Although optimization-based reconstruction methods can explicitly account for SIR effects, their computational cost is generally high, particularly in three-dimensional (3D) applications. To address the need for accurate but rapid 3D PACT image reconstruction, this study presents a framework for establishing a learned SIR compensation method that operates in the data domain. The learned compensation method maps SIR-corrupted PACT measurement data to compensated data that would have been recorded by idealized point-like transducers. Subsequently, the compensated data can be used with a computationally efficient reconstruction method that neglects SIR effects. Two variants of the learned compensation model are investigated that employ a U-Net model and a specifically designed, physics-inspired model, referred to as Deconv-Net. A fast and analytical training data generation procedure is also a component of the presented framework. The framework is rigorously validated in virtual imaging studies, demonstrating resolution improvement and robustness to noise variations, object complexity, and sound speed heterogeneity. When applied to in-vivo breast imaging data, the learned compensation models revealed fine structures that had been obscured by SIR-induced artifacts. To our knowledge, this is the first demonstration of learned SIR compensation in 3D PACT imaging.

</details>


### [124] [Delayed Feedback Modeling for Post-Click Gross Merchandise Volume Prediction: Benchmark, Insights and Approaches](https://arxiv.org/abs/2601.20307)
*Xinyu Li,Sishuo Chen,Guipeng Xv,Li Zhang,Mingxuan Luo,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Chen Lin*

Main category: cs.LG

TL;DR: 论文针对GMV预测中的延迟反馈建模问题，建立TRACE基准，提出READER范式，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 在线广告排名模型预测目标从CVR转向GMV，GMV预测的延迟反馈建模未被充分研究且挑战大，存在研究空白。

Method: 建立TRACE基准支持在线流式延迟反馈建模；提出READER范式，根据回购预测选择性激活专家参数，动态校准回归目标。

Result: READER在TRACE上性能优于基线，准确率提升2.19%。

Conclusion: 研究为GMV预测的在线延迟反馈建模开辟新途径，TRACE基准和相关见解将推动该方向研究和应用。

Abstract: The prediction objectives of online advertisement ranking models are evolving from probabilistic metrics like conversion rate (CVR) to numerical business metrics like post-click gross merchandise volume (GMV). Unlike the well-studied delayed feedback problem in CVR prediction, delayed feedback modeling for GMV prediction remains unexplored and poses greater challenges, as GMV is a continuous target, and a single click can lead to multiple purchases that cumulatively form the label. To bridge the research gap, we establish TRACE, a GMV prediction benchmark containing complete transaction sequences rising from each user click, which supports delayed feedback modeling in an online streaming manner. Our analysis and exploratory experiments on TRACE reveal two key insights: (1) the rapid evolution of the GMV label distribution necessitates modeling delayed feedback under online streaming training; (2) the label distribution of repurchase samples substantially differs from that of single-purchase samples, highlighting the need for separate modeling. Motivated by these findings, we propose RepurchasE-Aware Dual-branch prEdictoR (READER), a novel GMV modeling paradigm that selectively activates expert parameters according to repurchase predictions produced by a router. Moreover, READER dynamically calibrates the regression target to mitigate under-estimation caused by incomplete labels. Experimental results show that READER yields superior performance on TRACE over baselines, achieving a 2.19% improvement in terms of accuracy. We believe that our study will open up a new avenue for studying online delayed feedback modeling for GMV prediction, and our TRACE benchmark with the gathered insights will facilitate future research and application in this promising direction. Our code and dataset are available at https://github.com/alimama-tech/OnlineGMV .

</details>


### [125] [Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching](https://arxiv.org/abs/2601.20332)
*Fengrui Zuo,Zhiwei Ke,Yiming Liu,Wenqi Lou,Chao Wang,Xvehai Zhou*

Main category: cs.LG

TL;DR: 本文指出扩散语言模型推理有大量冗余计算，基于观察提出窗口型的推理剪枝缓存方法，在LLaDA和Dream实验上实现高达99倍推理加速且保留性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型推理有冗余计算，现有分块扩散方法适用性受限，需更好推理加速方法。

Method: 提出窗口型的推理剪枝和缓存方法，维护局部计算窗口并将未解码token分为活跃、缓存和远场token，在每阶段仅计算窗口内活跃和缓存token。

Result: 在LLaDA和Dream实验中，在相同计算预算下，实现高达99倍推理加速且基本保留生成性能。

Conclusion: 所提窗口型推理方法能够有效加速扩散语言模型推理，且对生成性能影响小。

Abstract: Diffusion language models (DLMs) generate text through iterative denoising, but inference requires full-sequence attention at every iteration, resulting in substantial redundant computation on masked tokens. Block-wise diffusion can reduce this cost, yet it typically relies on retraining and constrained update orders, limiting its direct applicability to pretrained DLMs. Our token-level analysis reveals pronounced structural locality in DLM inference. Decoding is driven by a small set of prefix-localized active tokens; the influence of distant undecoded context diminishes rapidly, and decoded tokens exhibit stage-wise temporal stability, enabling reuse of intermediate representations except for a brief post-decode transient. Motivated by these observations, we propose \textbf{\placeholder}\footnote{The source code is available at https://github.com/vhicrgit/Window-Diffusion.}, a window-based token pruning and caching method for inference. We maintain a local computation window that slides rightward as denoising progresses, and partition undecoded tokens into: (i) \textit{active tokens} that are computed online, (ii) \textit{buffer tokens} whose KV states are cached and periodically refreshed, and (iii) \textit{far-field tokens} that are pruned outside the window. Computation is restricted to active and buffer tokens within the window, while far-field tokens are omitted at each stage. Experiments on LLaDA and Dream show that, under matched compute budgets, our method achieves up to $99\times$ inference speedup while largely preserving generation performance.

</details>


### [126] [TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs](https://arxiv.org/abs/2601.20357)
*Minjae Lee,Wonjun Kang,Byeongkeun Ahn,Christian Classen,Kevin Galim,Seunghyuk Oh,Minghao Yan,Hyung Il Koo,Kangwook Lee*

Main category: cs.LG

TL;DR: 研究将投机解码用于大型视觉语言模型，提出TABED方法加速推理，有速度提升且可集成其他方法。


<details>
  <summary>Details</summary>
Motivation: 投机解码在大型视觉语言模型中研究不足，为填补此空白开展研究。

Method: 在11个数据集上对现有推理方法进行基准测试，提出Test - time Adaptive Batched Ensemble Drafting (TABED)方法，动态集成批量推理得到的多个草稿。

Result: 相比自回归解码平均有1.74倍的稳健墙时加速，比单草稿方法提升5%，且免训练、集成成本可忽略。

Conclusion: TABED方法有效加速大型视觉语言模型推理，具有即插即用兼容性，可集成其他先进方法。

Abstract: Speculative decoding (SD) has proven effective for accelerating LLM inference by quickly generating draft tokens and verifying them in parallel. However, SD remains largely unexplored for Large Vision-Language Models (LVLMs), which extend LLMs to process both image and text prompts. To address this gap, we benchmark existing inference methods with small draft models on 11 datasets across diverse input scenarios and observe scenario-specific performance fluctuations. Motivated by these findings, we propose Test-time Adaptive Batched Ensemble Drafting (TABED), which dynamically ensembles multiple drafts obtained via batch inference by leveraging deviations from past ground truths available in the SD setting. The dynamic ensemble method achieves an average robust walltime speedup of 1.74x over autoregressive decoding and a 5% improvement over single drafting methods, while remaining training-free and keeping ensembling costs negligible through parameter sharing. With its plug-and-play compatibility, we further enhance TABED by integrating advanced verification and alternative drafting methods. Code and custom-trained models are available at https://github.com/furiosa-ai/TABED.

</details>


### [127] [TINNs: Time-Induced Neural Networks for Solving Time-Dependent PDEs](https://arxiv.org/abs/2601.20361)
*Chen-Yang Dai,Che-Chia Chang,Te-Sheng Lin,Ming-Chih Lai,Chieh-Hsin Lai*

Main category: cs.LG

TL;DR: 提出TINNs架构解决时空PINNs问题，在实验中表现优于PINNs和强基线。


<details>
  <summary>Details</summary>
Motivation: 标准时空PINNs将时间作为输入，使用共享权重的单一网络，导致相同特征表示不同动态，降低准确性并使训练不稳定。

Method: 提出TINNs架构，将网络权重参数化为时间的学习函数，用Levenberg - Marquardt方法优化。

Result: 在各种时变PDE实验中，相比PINNs和强基线，精度提高4倍，收敛速度加快10倍。

Conclusion: TINNs架构能有效解决时空PINNs的问题，提升性能。

Abstract: Physics-informed neural networks (PINNs) solve time-dependent partial differential equations (PDEs) by learning a mesh-free, differentiable solution that can be evaluated anywhere in space and time. However, standard space--time PINNs take time as an input but reuse a single network with shared weights across all times, forcing the same features to represent markedly different dynamics. This coupling degrades accuracy and can destabilize training when enforcing PDE, boundary, and initial constraints jointly. We propose Time-Induced Neural Networks (TINNs), a novel architecture that parameterizes the network weights as a learned function of time, allowing the effective spatial representation to evolve over time while maintaining shared structure. The resulting formulation naturally yields a nonlinear least-squares problem, which we optimize efficiently using a Levenberg--Marquardt method. Experiments on various time-dependent PDEs show up to $4\times$ improved accuracy and $10\times$ faster convergence compared to PINNs and strong baselines.

</details>


### [128] [Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku](https://arxiv.org/abs/2601.20363)
*Mariia Drozdova*

Main category: cs.LG

TL;DR: 研究标准连续时间生成模型能否表示支持集为极稀疏、全局约束离散集的分布，以数独网格为测试平台，比较不同采样方法，还展示模型可用于引导生成和概率数独求解。


<details>
  <summary>Details</summary>
Motivation: 探究标准连续时间生成模型能否表示支持集为极稀疏、全局约束离散集的分布。

Method: 以完成的数独网格为测试平台，将其视为连续松弛空间的子集，沿着高斯概率路径训练流匹配和基于分数的模型，比较确定性（ODE）采样、随机（SDE）采样和DDPM风格离散化方法。

Result: 无条件下，随机采样明显优于确定性流；基于分数的采样器在连续时间方法中最可靠，DDPM风格的祖先采样整体有效性最高；模型可用于引导生成和概率数独求解，但样本效率低于经典求解器和离散几何感知扩散方法。

Conclusion: 经典扩散/流公式可以为全局约束的组合结构分配非零概率质量，并可通过随机搜索用于约束满足问题。

Abstract: Can standard continuous-time generative models represent distributions whose support is an extremely sparse, globally constrained discrete set? We study this question using completed Sudoku grids as a controlled testbed, treating them as a subset of a continuous relaxation space. We train flow-matching and score-based models along a Gaussian probability path and compare deterministic (ODE) sampling, stochastic (SDE) sampling, and DDPM-style discretizations derived from the same continuous-time training. Unconditionally, stochastic sampling substantially outperforms deterministic flows; score-based samplers are the most reliable among continuous-time methods, and DDPM-style ancestral sampling achieves the highest validity overall. We further show that the same models can be repurposed for guided generation: by repeatedly sampling completions under clamped clues and stopping when constraints are satisfied, the model acts as a probabilistic Sudoku solver. Although far less sample-efficient than classical solvers and discrete-geometry-aware diffusion methods, these experiments demonstrate that classic diffusion/flow formulations can assign non-zero probability mass to globally constrained combinatorial structures and can be used for constraint satisfaction via stochastic search.

</details>


### [129] [Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models](https://arxiv.org/abs/2601.20367)
*Qing Lyu,Zhe Fu,Alexandre Bayen*

Main category: cs.LG

TL;DR: 提出基于多智能体Transformer的无监督异常检测框架识别自动驾驶安全关键场景，经实验验证有效，还对检测到的异常聚类。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以有效识别自动驾驶安全关键场景，缺乏验证统计异常与物理危险关系的系统方法。

Method: 提出基于多智能体Transformer的无监督异常检测框架，通过预测残差衡量偏差，采用双评估方案评估检测稳定性和物理一致性。

Result: 最大残差聚合器在保持稳定性的同时实现最高物理一致性，框架识别出388个独特异常，检测到的异常被聚类为四种可解释风险类型。

Conclusion: 所提框架能有效识别自动驾驶安全关键场景，为仿真和测试提供可操作见解。

Abstract: Identifying safety-critical scenarios is essential for autonomous driving, but the rarity of such events makes supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks, and existing methods lack a systematic way to verify whether statistical anomalies truly reflect physical danger. To address this gap, we propose an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving and measures deviations through prediction residuals. A dual evaluation scheme has been proposed to assess both detection stability and physical alignment: Stability is measured using standard ranking metrics in which Kendall Rank Correlation Coefficient captures rank agreement and Jaccard index captures the consistency of the top-K selected items; Physical alignment is assessed through correlations with established Surrogate Safety Measures (SSM). Experiments on the NGSIM dataset demonstrate our framework's effectiveness: We show that the maximum residual aggregator achieves the highest physical alignment while maintaining stability. Furthermore, our framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. The detected anomalies are further clustered into four interpretable risk types, offering actionable insights for simulation and testing.

</details>


### [130] [LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning](https://arxiv.org/abs/2601.20375)
*Wei Huang,Anda Cheng,Yinggui Wang,Lei Wang,Tao Wei*

Main category: cs.LG

TL;DR: 提出LLM - AutoDP框架自动生成和优化数据处理策略，有加速技术，处理后数据训练的模型表现好，搜索时间大幅减少。


<details>
  <summary>Details</summary>
Motivation: 领域特定数据含大量低质量样本，传统数据处理策略开发人工成本高且有隐私问题，需自动化且不暴露原始数据的处理方法。

Method: 提出LLM - AutoDP框架，生成多候选策略并迭代优化，引入分布保留采样、处理目标选择、缓存复用机制三种加速技术。

Result: 处理后数据训练的模型对比未处理数据训练的模型胜率超80%，对比基于LLM代理的AutoML基线胜率约65%，加速技术使搜索时间最多减少10倍。

Conclusion: LLM - AutoDP框架在处理领域特定数据时有效且高效。

Abstract: Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.

</details>


### [131] [FedRD: Reducing Divergences for Generalized Federated Learning via Heterogeneity-aware Parameter Guidance](https://arxiv.org/abs/2601.20397)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Mingjin Zhang*

Main category: cs.LG

TL;DR: 提出FedRD算法解决联邦领域泛化问题，实验显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 新客户端加入需大量调整和训练，解决联邦学习模型在异构数据下对未知客户端的泛化问题，应对优化和性能差异挑战。

Method: 提出FedRD算法，结合参数引导的全局泛化聚合和局部去偏分类减少差异。

Result: 在公共多领域数据集上实验，该方法在解决特定问题上比竞争基线有显著性能优势。

Conclusion: FedRD算法能有效解决联邦领域泛化问题，获得适用于参与和未知客户端的最优全局模型。

Abstract: Heterogeneous federated learning (HFL) aims to ensure effective and privacy-preserving collaboration among different entities. As newly joined clients require significant adjustments and additional training to align with the existing system, the problem of generalizing federated learning models to unseen clients under heterogeneous data has become progressively crucial. Consequently, we highlight two unsolved challenging issues in federated domain generalization: Optimization Divergence and Performance Divergence. To tackle the above challenges, we propose FedRD, a novel heterogeneity-aware federated learning algorithm that collaboratively utilizes parameter-guided global generalization aggregation and local debiased classification to reduce divergences, aiming to obtain an optimal global model for participating and unseen clients. Extensive experiments on public multi-domain datasets demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [132] [ScatterFusion: A Hierarchical Scattering Transform Framework for Enhanced Time Series Forecasting](https://arxiv.org/abs/2601.20401)
*Wei Li*

Main category: cs.LG

TL;DR: 本文提出ScatterFusion框架用于时间序列预测，含四个关键组件，实验显示其优于其他常见方法。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列预测中多时间尺度复杂时间依赖的挑战。

Method: 提出ScatterFusion框架，包含HSTM模块、SAFE模块、MRTA机制和TSR分解引导的结构感知损失函数。

Result: 在七个基准数据集上的大量实验表明，ScatterFusion在不同预测范围内显著降低了误差指标。

Conclusion: ScatterFusion是一种用于时间序列预测的有效框架，优于其他常见方法。

Abstract: Time series forecasting presents significant challenges due to the complex temporal dependencies at multiple time scales. This paper introduces ScatterFusion, a novel framework that synergistically integrates scattering transforms with hierarchical attention mechanisms for robust time series forecasting. Our approach comprises four key components: (1) a Hierarchical Scattering Transform Module (HSTM) that extracts multi-scale invariant features capturing both local and global patterns; (2) a Scale-Adaptive Feature Enhancement (SAFE) module that dynamically adjusts feature importance across different scales; (3) a Multi-Resolution Temporal Attention (MRTA) mechanism that learns dependencies at varying time horizons; and (4) a Trend-Seasonal-Residual (TSR) decomposition-guided structure-aware loss function. Extensive experiments on seven benchmark datasets demonstrate that ScatterFusion outperforms other common methods, achieving significant reductions in error metrics across various prediction horizons.

</details>


### [133] [AWGformer: Adaptive Wavelet-Guided Transformer for Multi-Resolution Time Series Forecasting](https://arxiv.org/abs/2601.20409)
*Wei Li*

Main category: cs.LG

TL;DR: 本文提出AWGformer架构用于多变量时间序列预测，经实验验证其优于现有方法，理论分析给出收敛保证。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需捕捉多时间尺度模式并保证计算效率，现有方法可能存在不足。

Method: 引入AWGformer架构，包含自适应小波分解模块、跨尺度特征融合机制、频率感知多头注意力模块和分层预测网络。

Result: 在基准数据集上实验表明，AWGformer较现有方法有显著平均提升，对多尺度和非平稳时间序列尤其有效。

Conclusion: AWGformer能有效进行多变量时间序列预测，理论分析建立了小波引导注意力与经典信号处理原理的联系。

Abstract: Time series forecasting requires capturing patterns across multiple temporal scales while maintaining computational efficiency. This paper introduces AWGformer, a novel architecture that integrates adaptive wavelet decomposition with cross-scale attention mechanisms for enhanced multi-variate time series prediction. Our approach comprises: (1) an Adaptive Wavelet Decomposition Module (AWDM) that dynamically selects optimal wavelet bases and decomposition levels based on signal characteristics; (2) a Cross-Scale Feature Fusion (CSFF) mechanism that captures interactions between different frequency bands through learnable coupling matrices; (3) a Frequency-Aware Multi-Head Attention (FAMA) module that weights attention heads according to their frequency selectivity; (4) a Hierarchical Prediction Network (HPN) that generates forecasts at multiple resolutions before reconstruction. Extensive experiments on benchmark datasets demonstrate that AWGformer achieves significant average improvements over state-of-the-art methods, with particular effectiveness on multi-scale and non-stationary time series. Theoretical analysis provides convergence guarantees and establishes the connection between our wavelet-guided attention and classical signal processing principles.

</details>


### [134] [Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs](https://arxiv.org/abs/2601.20420)
*Yuhang Liu,Erdun Gao,Dong Gong,Anton van den Hengel,Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: 本文提出概念成分分析（ConCA）框架从大语言模型表示中提取概念，稀疏ConCA变体在多模型上表现良好，优于稀疏自编码器。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器在从大语言模型激活中提取可解释概念时存在理论模糊和方法挑战，需更有理论依据的方法。

Method: 基于潜变量模型，将大语言模型表示近似为概念对数后验的线性混合，提出ConCA框架和稀疏ConCA变体进行无监督线性解混。

Result: 实现12个稀疏ConCA变体，在多个大语言模型上能提取有意义概念。

Conclusion: ConCA框架及其稀疏变体为大语言模型概念提取提供了有理论支持的方法，优于SAEs。

Abstract: Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs' activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical progress, SAEs suffer from a fundamental theoretical ambiguity: the well-defined correspondence between LLM representations and human-interpretable concepts remains unclear. This lack of theoretical grounding gives rise to several methodological challenges, including difficulties in principled method design and evaluation criteria. In this work, we show that, under mild assumptions, LLM representations can be approximated as a {linear mixture} of the log-posteriors over concepts given the input context, through the lens of a latent variable model where concepts are treated as latent variables. This motivates a principled framework for concept extraction, namely Concept Component Analysis (ConCA), which aims to recover the log-posterior of each concept from LLM representations through a {unsupervised} linear unmixing process. We explore a specific variant, termed sparse ConCA, which leverages a sparsity prior to address the inherent ill-posedness of the unmixing problem. We implement 12 sparse ConCA variants and demonstrate their ability to extract meaningful concepts across multiple LLMs, offering theory-backed advantages over SAEs.

</details>


### [135] [Nonlinear Dimensionality Reduction with Diffusion Maps in Practice](https://arxiv.org/abs/2601.20428)
*Sönke Beier,Paula Pirker-Díaz,Friedrich Pagenkopf,Karoline Wiesner*

Main category: cs.LG

TL;DR: 对扩散映射技术进行面向实践的综述，指出首组件未必最相关。


<details>
  <summary>Details</summary>
Motivation: 现有文献未全面讨论数据预处理、参数设置和组件选择对扩散映射结果流形的影响。

Method: 对扩散映射技术进行面向实践的综述，展示识别最相关组件的新技术。

Result: 发现首组件不一定是最相关的组件。

Conclusion: 强调了对扩散映射技术相关影响因素研究的重要性。

Abstract: Diffusion Map is a spectral dimensionality reduction technique which is able to uncover nonlinear submanifolds in high-dimensional data. And, it is increasingly applied across a wide range of scientific disciplines, such as biology, engineering, and social sciences. But data preprocessing, parameter settings and component selection have a significant influence on the resulting manifold, something which has not been comprehensively discussed in the literature so far. We provide a practice oriented review of the Diffusion Map technique, illustrate pitfalls and showcase a recently introduced technique for identifying the most relevant components. Our results show that the first components are not necessarily the most relevant ones.

</details>


### [136] [TimeCatcher: A Variational Framework for Volatility-Aware Forecasting of Non-Stationary Time Series](https://arxiv.org/abs/2601.20448)
*Zhiyu Chen,Minhao Liu,Yanru Zhang*

Main category: cs.LG

TL;DR: 提出TimeCatcher框架用于时间序列预测，在九个真实数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级MLP模型在高度非平稳序列的长期预测中因局部平稳性假设易出错，需新方法解决。

Method: 提出TimeCatcher框架，用变分编码器捕捉潜在动态模式，用波动性感知增强机制检测和放大局部变化。

Result: 在九个来自不同领域的真实数据集上，TimeCatcher始终优于现有基线模型，在高波动性和突然波动的长期预测场景中提升显著。

Conclusion: TimeCatcher能有效解决现有模型在高度非平稳序列长期预测中的问题。

Abstract: Recent lightweight MLP-based models have achieved strong performance in time series forecasting by capturing stable trends and seasonal patterns. However, their effectiveness hinges on an implicit assumption of local stationarity assumption, making them prone to errors in long-term forecasting of highly non-stationary series, especially when abrupt fluctuations occur, a common challenge in domains like web traffic monitoring. To overcome this limitation, we propose TimeCatcher, a novel Volatility-Aware Variational Forecasting framework. TimeCatcher extends linear architectures with a variational encoder to capture latent dynamic patterns hidden in historical data and a volatility-aware enhancement mechanism to detect and amplify significant local variations. Experiments on nine real-world datasets from traffic, financial, energy, and weather domains show that TimeCatcher consistently outperforms state-of-the-art baselines, with particularly large improvements in long-term forecasting scenarios characterized by high volatility and sudden fluctuations. Our code is available at https://github.com/ColaPrinceCHEN/TimeCatcher.

</details>


### [137] [Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations](https://arxiv.org/abs/2601.20449)
*Fatima Ezzeddine,Obaida Ammar,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 本文聚焦可解释人工智能中反事实解释（CFs）的公平性问题，定义了三种公平性，提出基于强化学习生成公平CFs的方法，并在三个基准数据集上评估，开启了关于混合公平性的讨论。


<details>
  <summary>Details</summary>
Motivation: 确保不同属性及受保护群体用户获得相似且可行的反事实解释，以实现可信和公平的决策。

Method: 将问题表述为优化任务，提出基于强化学习的模型无关方法生成满足个体和群体公平约束的CFs，扩展现有公平性度量指标。

Result: 在三个基准数据集上验证，该方法能有效保证个体和群体公平，同时保持CFs质量，还分别量化了不同层面的公平成本。

Conclusion: 开启了关于混合公平性在可解释人工智能及反事实解释之外的作用和影响的更广泛讨论。

Abstract: Explainable Artificial Intelligence (XAI) is becoming increasingly essential for enhancing the transparency of machine learning (ML) models. Among the various XAI techniques, counterfactual explanations (CFs) hold a pivotal role due to their ability to illustrate how changes in input features can alter an ML model's decision, thereby offering actionable recourse to users. Ensuring that individuals with comparable attributes and those belonging to different protected groups (e.g., demographic) receive similar and actionable recourse options is essential for trustworthy and fair decision-making. In this work, we address this challenge directly by focusing on the generation of fair CFs. Specifically, we start by defining and formulating fairness at: 1) individual fairness, ensuring that similar individuals receive similar CFs, 2) group fairness, ensuring equitable CFs across different protected groups and 3) hybrid fairness, which accounts for both individual and broader group-level fairness. We formulate the problem as an optimization task and propose a novel model-agnostic, reinforcement learning based approach to generate CFs that satisfy fairness constraints at both the individual and group levels, two objectives that are usually treated as orthogonal. As fairness metrics, we extend existing metrics commonly used for auditing ML models, such as equal choice of recourse and equal effectiveness across individuals and groups. We evaluate our approach on three benchmark datasets, showing that it effectively ensures individual and group fairness while preserving the quality of the generated CFs in terms of proximity and plausibility, and quantify the cost of fairness in the different levels separately. Our work opens a broader discussion on hybrid fairness and its role and implications for XAI and beyond CFs.

</details>


### [138] [Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations](https://arxiv.org/abs/2601.20477)
*Kadircan Aksoy,Peter Jung,Protim Bhattacharjee*

Main category: cs.LG

TL;DR: 从二元假设检验视角研究神经分类器监督训练动态，揭示网络与最优决策规则的关系并探讨策略。


<details>
  <summary>Details</summary>
Motivation: 研究神经分类器监督训练动态，解释其训练过程。

Method: 将分类建模为表征的类条件分布间的二元测试。

Result: 训练中泛化良好的网络通过KL散度单调改进与Neyman - Pearson最优决策规则愈发一致。

Conclusion: 此研究为不同类神经网络提供解释及可能的训练或正则化策略。

Abstract: We study the supervised training dynamics of neural classifiers through the lens of binary hypothesis testing. We model classification as a set of binary tests between class-conditional distributions of representations and empirically show that, along training trajectories, well-generalizing networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence that relate to error rate exponents. We finally discuss how this yields an explanation and possible training or regularization strategies for different classes of neural networks.

</details>


### [139] [An explainable framework for the relationship between dementia and glucose metabolism patterns](https://arxiv.org/abs/2601.20480)
*C. Vázquez-García,F. J. Martínez-Murcia,F. Segovia Román,A. Forte,J. Ramírez,I. Illán,A. Hernández-Segura,C. Jiménez-Mesa,Juan M. Górriz*

Main category: cs.LG

TL;DR: 提出半监督VAE框架分析高维神经影像数据，用ADNI的PET扫描验证，有效提取与阿尔茨海默病生物标志物相关的疾病模式。


<details>
  <summary>Details</summary>
Motivation: 高维神经影像数据因复杂非线性关系给评估神经退行性疾病带来挑战，需有效方法提取疾病特征。

Method: 提出带有灵活相似性正则化项的半监督VAE框架，用ADNI的PET扫描，引导第一个潜在维度与认知评分对齐。

Result: 通过体素GLM分析发现关键区域代谢降低，其余潜在变量编码混淆因素。

Conclusion: 该框架能有效提取与既定阿尔茨海默病生物标志物对齐的疾病相关模式，是研究神经退行性进展的可解释、可适应工具。

Abstract: High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.

</details>


### [140] [CCMamba: Selective State-Space Models for Higher-Order Graph Learning on Combinatorial Complexes](https://arxiv.org/abs/2601.20518)
*Jiawen Chen,Qi Shao,Mingtong Zhou,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: 提出首个基于mamba的组合复形学习统一框架CCMamba，能线性时间传播信息，实验显示性能优、可扩展性和鲁棒性好。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑深度学习方法依赖注意力机制，复杂度高且低维，限制高阶复形的可扩展性和信息聚合。

Method: 将组合复形的多秩关联关系组织成结构化序列，用状态空间模型处理，把消息传递重新表述为选择性状态空间建模问题。

Result: CCMamba在图、超图和单纯复形基准测试中始终优于现有方法，可扩展性和深度鲁棒性提高。

Conclusion: CCMamba为组合复形学习提供有效解决方案，避免自注意力机制，有良好性能和可扩展性。

Abstract: Topological deep learning has emerged for modeling higher-order relational structures beyond pairwise interactions that standard graph neural networks fail to capture. Although combinatorial complexes offer a unified topological framework, most existing topological deep learning methods rely on local message passing via attention mechanisms, which incur quadratic complexity and remain low-dimensional, limiting scalability and rank-aware information aggregation in higher-order complexes.We propose Combinatorial Complex Mamba (CCMamba), the first unified mamba-based neural framework for learning on combinatorial complexes. CCMamba reformulates message passing as a selective state-space modeling problem by organizing multi-rank incidence relations into structured sequences processed by rank-aware state-space models. This enables adaptive, directional, and long range information propagation in linear time without self attention. We further establish the theoretical analysis that the expressive power upper-bound of CCMamba message passing is the 1-Weisfeiler-Lehman test. Experiments on graph, hypergraph, and simplicial benchmarks demonstrate that CCMamba consistently outperforms existing methods while exhibiting improved scalability and robustness to depth.

</details>


### [141] [Unsupervised Ensemble Learning Through Deep Energy-based Models](https://arxiv.org/abs/2601.20556)
*Ariel Maymon,Yanir Buznah,Uri Shaham*

Main category: cs.LG

TL;DR: 提出无监督集成学习的新深度能量法，无需标签数据等，在多种场景表现出色，凸显其在数据稀缺或隐私敏感环境潜力。


<details>
  <summary>Details</summary>
Motivation: 解决无监督情况下结合多个学习者预测的挑战，应对信息有限难以评估分类器性能的场景。

Method: 提出一种仅利用个体学习者预测构建精确元学习者的新深度能量法，无需标签数据、学习者特征和特定问题信息，在学习者条件独立时有理论保证。

Result: 在包括专家混合设置等多样集成场景中表现优越，实验涵盖标准集成数据集和定制数据集。

Conclusion: 无监督集成学习在数据稀缺或隐私敏感环境中具有利用集体智慧的潜力。

Abstract: Unsupervised ensemble learning emerged to address the challenge of combining multiple learners' predictions without access to ground truth labels or additional data. This paradigm is crucial in scenarios where evaluating individual classifier performance or understanding their strengths is challenging due to limited information. We propose a novel deep energy-based method for constructing an accurate meta-learner using only the predictions of individual learners, potentially capable of capturing complex dependence structures between them. Our approach requires no labeled data, learner features, or problem-specific information, and has theoretical guarantees for when learners are conditionally independent. We demonstrate superior performance across diverse ensemble scenarios, including challenging mixture of experts settings. Our experiments span standard ensemble datasets and curated datasets designed to test how the model fuses expertise from multiple sources. These results highlight the potential of unsupervised ensemble learning to harness collective intelligence, especially in data-scarce or privacy-sensitive environments.

</details>


### [142] [Reinforcement Unlearning via Group Relative Policy Optimization](https://arxiv.org/abs/2601.20568)
*Efstratios Zaradoukas,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: 现有大语言模型预训练会记忆敏感数据，需去学习技术，提出PURGE方法，比SotA方法减少token使用，还提升流畅性和鲁棒性，在基准测试有良好表现，为去学习研究提供新方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练时会记忆敏感或受版权保护的数据，在GDPR和欧盟AI法案下面临合规挑战，需要能在不从头训练的情况下从已部署模型中删除信息的技术，而现有去学习方法存在不足。

Method: 引入基于Group Relative Policy Optimization框架的PURGE方法，将去学习表述为可验证问题，使用内在奖励信号惩罚提及禁止概念。

Result: 与SotA方法相比，每目标减少高达46倍的token使用，相比基础模型，流畅性提高5.48%，对抗鲁棒性提高12.02%；在RWKU基准测试中，实现11%的去学习效果，同时保留98%的原始效用。

Conclusion: 将大语言模型去学习构建为可验证任务，能实现更可靠、高效和可扩展的遗忘，为去学习研究提供了结合理论保证、提高安全性和实际部署效率的新方向。

Abstract: During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach reduces token usage per target by up to a factor of 46 compared with SotA methods, while improving fluency by 5.48 percent and adversarial robustness by 12.02 percent over the base model. On the Real World Knowledge Unlearning (RWKU) benchmark, PURGE achieves 11 percent unlearning effectiveness while preserving 98 percent of original utility. PURGE shows that framing LLM unlearning as a verifiable task, enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency.

</details>


### [143] [Ranking-aware Reinforcement Learning for Ordinal Ranking](https://arxiv.org/abs/2601.20585)
*Aiming Hao,Chen Zhu,Jiashu Zhu,Jiahong Wu,Xiangxiang Chu*

Main category: cs.LG

TL;DR: 提出Ranking - Aware Reinforcement Learning (RARL)框架处理序数回归和排序问题，引入RMO增强训练，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以对序数回归和排序中的序数依赖关系进行建模。

Method: 提出RARL框架，有统一目标整合回归和L2R，用排序感知可验证奖励进行策略优化更新模型；引入RMO注入噪声增强训练。

Result: 在三个不同基准测试上进行了广泛实验。

Conclusion: RARL框架有效。

Abstract: Ordinal regression and ranking are challenging due to inherent ordinal dependencies that conventional methods struggle to model. We propose Ranking-Aware Reinforcement Learning (RARL), a novel RL framework that explicitly learns these relationships. At its core, RARL features a unified objective that synergistically integrates regression and Learning-to-Rank (L2R), enabling mutual improvement between the two tasks. This is driven by a ranking-aware verifiable reward that jointly assesses regression precision and ranking accuracy, facilitating direct model updates via policy optimization. To further enhance training, we introduce Response Mutation Operations (RMO), which inject controlled noise to improve exploration and prevent stagnation at saddle points. The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks.

</details>


### [144] [Regularized Gradient Temporal-Difference Learning](https://arxiv.org/abs/2601.20599)
*Hyunjun Na,Donghwan Lee*

Main category: cs.LG

TL;DR: 本文提出正则化GTD算法R - GTD，解决FIM奇异问题，有理论收敛保证并经实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有梯度时序差分（GTD）学习算法的收敛分析依赖FIM非奇异的假设，而实际中FIM可能奇异导致不稳定或性能下降。

Method: 通过重新表述均方投影贝尔曼误差（MSPBE）最小化问题，提出正则化优化目标，得到正则化GTD算法R - GTD。

Result: 建立了所提方法的理论收敛保证和明确的误差界，通过实验验证了其有效性。

Conclusion: R - GTD算法即使在FIM奇异时也能保证收敛到唯一解。

Abstract: Gradient temporal-difference (GTD) learning algorithms are widely used for off-policy policy evaluation with function approximation. However, existing convergence analyses rely on the restrictive assumption that the so-called feature interaction matrix (FIM) is nonsingular. In practice, the FIM can become singular and leads to instability or degraded performance. In this paper, we propose a regularized optimization objective by reformulating the mean-square projected Bellman error (MSPBE) minimization. This formulation naturally yields a regularized GTD algorithms, referred to as R-GTD, which guarantees convergence to a unique solution even when the FIM is singular. We establish theoretical convergence guarantees and explicit error bounds for the proposed method, and validate its effectiveness through empirical experiments.

</details>


### [145] [CoBA: Integrated Deep Learning Model for Reliable Low-Altitude UAV Classification in mmWave Radio Networks](https://arxiv.org/abs/2601.20605)
*Junaid Sajid,Ivo Müürsepp,Luca Reggiani,Davide Scazzoli,Federico Francesco Luigi Mariani,Maurizio Magarini,Rizwan Ahmad,Muhammad Mahtab Alam*

Main category: cs.LG

TL;DR: 本文提出CoBA深度学习模型，利用5G毫米波无线电测量对低空无人机空域操作分类，实验显示其精度超基线模型。


<details>
  <summary>Details</summary>
Motivation: 无人机在民用和工业应用增加，在密集毫米波环境准确分类低空无人机所在空域有挑战，需处理复杂传播和信号变化的模型。

Method: 提出CoBA模型，集成卷积、双向循环和注意力层，用塔尔图工业大学5G毫米波网络收集数据集，与传统机器学习模型和基于指纹的基准对比评估。

Result: CoBA模型精度高，显著优于所有基线模型。

Conclusion: CoBA模型有潜力用于可靠且规范的无人机空域监测。

Abstract: Uncrewed Aerial Vehicles (UAVs) are increasingly used in civilian and industrial applications, making secure low-altitude operations crucial. In dense mmWave environments, accurately classifying low-altitude UAVs as either inside authorized or restricted airspaces remains challenging, requiring models that handle complex propagation and signal variability. This paper proposes a deep learning model, referred to as CoBA, which stands for integrated Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Attention which leverages Fifth Generation (5G) millimeter-wave (mmWave) radio measurements to classify UAV operations in authorized and restricted airspaces at low altitude. The proposed CoBA model integrates convolutional, bidirectional recurrent, and attention layers to capture both spatial and temporal patterns in UAV radio measurements. To validate the model, a dedicated dataset is collected using the 5G mmWave network at TalTech, with controlled low altitude UAV flights in authorized and restricted scenarios. The model is evaluated against conventional ML models and a fingerprinting-based benchmark. Experimental results show that CoBA achieves superior accuracy, significantly outperforming all baseline models and demonstrating its potential for reliable and regulated UAV airspace monitoring.

</details>


### [146] [WFR-MFM: One-Step Inference for Dynamic Unbalanced Optimal Transport](https://arxiv.org/abs/2601.20606)
*Xinyu Wang,Ruoyu Wang,Qiangwei Peng,Peijie Zhou,Tiejun Li*

Main category: cs.LG

TL;DR: 提出用于不平衡流匹配的平均流框架及WFR - MFM方法，在单细胞数据上实现快速推理和有效预测。


<details>
  <summary>Details</summary>
Motivation: 现有从有限观测重建动态演化的方法在推理时依赖轨迹模拟，成为可扩展应用的关键瓶颈。

Method: 提出平均流框架总结运输和质量增长动态，在此基础上开发WFR - MFM解决Wasserstein - Fisher - Rao几何下的动态不平衡最优传输问题。

Result: 在合成和真实单细胞RNA测序数据集上，WFR - MFM比现有基线推理速度快几个数量级，保持高预测精度，能在大型合成数据集上进行有效扰动响应预测。

Conclusion: WFR - MFM方法有效解决了现有方法推理瓶颈问题，可应用于单细胞生物学动态演化重建。

Abstract: Reconstructing dynamical evolution from limited observations is a fundamental challenge in single-cell biology, where dynamic unbalanced optimal transport provides a principled framework for modeling coupled transport and mass variation. However, existing approaches rely on trajectory simulation at inference time, making inference a key bottleneck for scalable applications. In this work, we propose a mean-flow framework for unbalanced flow matching that summarizes both transport and mass-growth dynamics over arbitrary time intervals using mean velocity and mass-growth fields, enabling fast one-step generation without trajectory simulation. To solve dynamic unbalanced optimal transport under the Wasserstein-Fisher-Rao geometry, we further build on this framework to develop Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM). Across synthetic and real single-cell RNA sequencing datasets, WFR-MFM achieves orders-of-magnitude faster inference than a range of existing baselines while maintaining high predictive accuracy, and enables efficient perturbation response prediction on large synthetic datasets with thousands of conditions.

</details>


### [147] [ACFormer: Mitigating Non-linearity with Auto Convolutional Encoder for Time Series Forecasting](https://arxiv.org/abs/2601.20611)
*Gawon Lee,Hanbyeol Park,Minseop Kim,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: 本文针对时间序列预测中线性模型处理非线性信号的不足，提出ACFormer架构，实验表明其表现达SOTA，缓解线性模型缺陷。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测面临建模复杂依赖关系的挑战，现有线性架构难以处理非线性信号。

Method: 对CNN时间序列预测模型进行系统的感受野分析，提出“个体感受野”；基于此提出ACFormer架构，通过共享压缩模块捕获细粒度信息等。

Result: 在多个基准数据集上的大量实验表明，ACFormer始终能达到SOTA性能。

Conclusion: ACFormer有效缓解了线性模型在捕获高频分量方面的固有缺陷。

Abstract: Time series forecasting (TSF) faces challenges in modeling complex intra-channel temporal dependencies and inter-channel correlations. Although recent research has highlighted the efficiency of linear architectures in capturing global trends, these models often struggle with non-linear signals. To address this gap, we conducted a systematic receptive field analysis of convolutional neural network (CNN) TSF models. We introduce the "individual receptive field" to uncover granular structural dependencies, revealing that convolutional layers act as feature extractors that mirror channel-wise attention while exhibiting superior robustness to non-linear fluctuations. Based on these insights, we propose ACFormer, an architecture designed to reconcile the efficiency of linear projections with the non-linear feature-extraction power of convolutions. ACFormer captures fine-grained information through a shared compression module, preserves temporal locality via gated attention, and reconstructs variable-specific temporal patterns using an independent patch expansion layer. Extensive experiments on multiple benchmark datasets demonstrate that ACFormer consistently achieves state-of-the-art performance, effectively mitigating the inherent drawbacks of linear models in capturing high-frequency components.

</details>


### [148] [DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration](https://arxiv.org/abs/2601.20627)
*Gilles Eerlings,Brent Zoomers,Jori Liesenborgs,Gustavo Rovelo Ruiz,Kris Luyten*

Main category: cs.LG

TL;DR: 提出DIVERSE框架系统探索深度神经网络Rashomon集，跨数据集发现多样高性能模型，成本低。


<details>
  <summary>Details</summary>
Motivation: 系统探索深度神经网络的Rashomon集，构建多样且性能良好的模型集合。

Method: 用Feature - wise Linear Modulation (FiLM)层扩充预训练模型，使用Covariance Matrix Adaptation Evolution Strategy (CMA - ES)搜索潜在调制空间。

Result: 在MNIST、PneumoniaMNIST和CIFAR - 10数据集上发现多个高性能但功能不同的模型，能以较低计算成本实现与重新训练相当的多样性。

Conclusion: DIVERSE能有效且高效地探索Rashomon集，构建多样模型集具有可行性。

Abstract: We propose DIVERSE, a framework for systematically exploring the Rashomon set of deep neural networks, the collection of models that match a reference model's accuracy while differing in their predictive behavior. DIVERSE augments a pretrained model with Feature-wise Linear Modulation (FiLM) layers and uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models. Our experiments show that DIVERSE offers a competitive and efficient exploration of the Rashomon set, making it feasible to construct diverse sets that maintain robustness and performance while supporting well-balanced model multiplicity. While retraining remains the baseline to generate Rashomon sets, DIVERSE achieves comparable diversity at reduced computational cost.

</details>


### [149] [A Foundation Model for Virtual Sensors](https://arxiv.org/abs/2601.20634)
*Leon Götz,Lars Frederik Peiss,Erik Sauer,Andreas Udo Sass,Thorsten Bagdonat,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 本文提出首个用于虚拟传感器的基础模型，解决现有方法不足，评估显示该模型计算与内存效率高，能规模化部署。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟传感器方法需特定模型、无法利用任务协同且缺乏基准，新兴时间序列基础模型计算昂贵且无法适配虚拟传感器。

Method: 引入首个针对虚拟传感器的基础模型，统一模型可利用协同效应同时预测多种虚拟传感器，学习相关输入信号。

Result: 在大规模评测中，架构计算时间减少415倍，内存需求降低951倍，预测质量相当或更好，能扩展到数百个虚拟传感器且参数数量近乎不变。

Conclusion: 该模型可实现高效计算与内存利用，能在大规模传感器网络中实际部署。

Abstract: Virtual sensors use machine learning to predict target signals from available measurements, replacing expensive physical sensors in critical applications. Existing virtual sensor approaches require application-specific models with hand-selected inputs for each sensor, cannot leverage task synergies, and lack consistent benchmarks. At the same time, emerging time series foundation models are computationally expensive and limited to predicting their input signals, making them incompatible with virtual sensors. We introduce the first foundation model for virtual sensors addressing both limitations. Our unified model can simultaneously predict diverse virtual sensors exploiting synergies while maintaining computational efficiency. It learns relevant input signals for each virtual sensor, eliminating expert knowledge requirements while adding explainability. In our large-scale evaluation on a standard benchmark and an application-specific dataset with over 18 billion samples, our architecture achieves 415x reduction in computation time and 951x reduction in memory requirements, while maintaining or even improving predictive quality compared to baselines. Our model scales gracefully to hundreds of virtual sensors with nearly constant parameter count, enabling practical deployment in large-scale sensor networks.

</details>


### [150] [An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems](https://arxiv.org/abs/2601.20637)
*Panayiotis Ioannou,Pietro Liò,Pietro Cicuta*

Main category: cs.LG

TL;DR: 利用阻尼振荡系统的噪声合成数据，研究NODE外推能力和SR恢复方程能力，发现该组合方法对科学发现有潜力。


<details>
  <summary>Details</summary>
Motivation: 准确建模复杂系统动力学并发现其控制微分方程对加速科学发现至关重要，故研究NODE与SR在相关方面的能力。

Method: 使用两个阻尼振荡系统的噪声合成数据，测试NODE的外推能力和SR恢复潜在方程的能力。

Result: 1. 若轨迹与训练数据有动态相似性，NODE能有效外推。2. SR能从噪声数据恢复方程，性能依赖输入变量选择。3. 使用仅10%全模拟数据训练的NODE生成的数据，SR能恢复三个控制方程中的两个，并较好近似第三个。

Conclusion: 使用NODE丰富有限数据，让SR推断物理定律，是科学发现的有前景新途径，最后一点发现可作为未来工作方向。

Abstract: Accurately modelling the dynamics of complex systems and discovering their governing differential equations are critical tasks for accelerating scientific discovery. Using noisy, synthetic data from two damped oscillatory systems, we explore the extrapolation capabilities of Neural Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR) to recover the underlying equations. Our study yields three key insights. First, we demonstrate that NODEs can extrapolate effectively to new boundary conditions, provided the resulting trajectories share dynamic similarity with the training data. Second, SR successfully recovers the equations from noisy ground-truth data, though its performance is contingent on the correct selection of input variables. Finally, we find that SR recovers two out of the three governing equations, along with a good approximation for the third, when using data generated by a NODE trained on just 10% of the full simulation. While this last finding highlights an area for future work, our results suggest that using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery.

</details>


### [151] [Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability](https://arxiv.org/abs/2601.20642)
*Rohan Asthana,Vasileios Belagiannis*

Main category: cs.LG

TL;DR: 论文研究扩散图像生成模型记忆问题，指出现有基于范数的记忆检测方法的局限性，提出结合各向同性范数和各向异性对齐的检测指标，该指标计算快且效果好，还展示了基于该指标的缓解策略有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于范数的记忆检测方法有局限性，仅在各向同性对数概率分布假设下有效，需更好的记忆检测方法。

Method: 分析各向异性情况，发现低噪音下记忆样本的引导向量和无条件分数有强角度对齐，结合各向同性范数和各向异性对齐开发记忆检测指标。

Result: 检测指标可直接在纯噪音输入上计算，无需昂贵的去噪步骤；在Stable Diffusion v1.4和v2上实验显示，该指标优于现有无去噪检测方法，速度至少快约5倍。

Conclusion: 提出的记忆检测指标有效，且基于该指标的缓解策略能应对记忆问题。

Abstract: Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.

</details>


### [152] [Learning Contextual Runtime Monitors for Safe AI-Based Autonomy](https://arxiv.org/abs/2601.20666)
*Alejandro Luque-Cerpa,Mengyuan Wang,Emil Carlsson,Sanjit A. Seshia,Devdatt Dubhashi,Hazem Torfah*

Main category: cs.LG

TL;DR: 提出学习上下文感知运行时监控器的新框架，用于AI控制集成，在自动驾驶模拟场景验证有安全和性能提升。


<details>
  <summary>Details</summary>
Motivation: ML控制器在陌生环境准确性下降有安全隐患，传统集成方法会稀释个体控制器优势，需利用上下文优势设计安全控制集成。

Method: 将安全AI控制集成设计重构为上下文监控问题，把监控器学习视为上下文学习任务，借鉴上下文多臂老虎机技术。

Result: 在两个自动驾驶模拟场景验证，相比非上下文基线在安全和性能上有显著提升。

Conclusion: 该框架在控制器选择时有理论安全保证，能提高控制器多样性利用。

Abstract: We introduce a novel framework for learning context-aware runtime monitors for AI-based control ensembles. Machine-learning (ML) controllers are increasingly deployed in (autonomous) cyber-physical systems because of their ability to solve complex decision-making tasks. However, their accuracy can degrade sharply in unfamiliar environments, creating significant safety concerns. Traditional ensemble methods aim to improve robustness by averaging or voting across multiple controllers, yet this often dilutes the specialized strengths that individual controllers exhibit in different operating contexts. We argue that, rather than blending controller outputs, a monitoring framework should identify and exploit these contextual strengths. In this paper, we reformulate the design of safe AI-based control ensembles as a contextual monitoring problem. A monitor continuously observes the system's context and selects the controller best suited to the current conditions. To achieve this, we cast monitor learning as a contextual learning task and draw on techniques from contextual multi-armed bandits. Our approach comes with two key benefits: (1) theoretical safety guarantees during controller selection, and (2) improved utilization of controller diversity. We validate our framework in two simulated autonomous driving scenarios, demonstrating significant improvements in both safety and performance compared to non-contextual baselines.

</details>


### [153] [MuRAL-CPD: Active Learning for Multiresolution Change Point Detection](https://arxiv.org/abs/2601.20686)
*Stefano Bertolasi,Diego Carrera,Diego Stucchi,Pasqualina Fragneto,Luigi Amedeo Bianchi*

Main category: cs.LG

TL;DR: 提出半监督方法MuRAL - CPD用于时间序列的变点检测，结合主动学习与多分辨率算法，实验显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统变点检测方法依赖无监督技术，缺乏对特定任务变化定义的适应性且无法利用用户知识。

Method: 提出MuRAL - CPD方法，将主动学习集成到多分辨率变点检测算法中，利用小波多分辨率分解检测多时间尺度变化，并结合用户反馈迭代优化关键超参数。

Result: 在多个真实数据集上的实验表明，MuRAL - CPD相对于现有技术方法有效，尤其在少量监督的场景中。

Conclusion: MuRAL - CPD能使模型对变化的定义与用户保持一致，提高了准确性和可解释性。

Abstract: Change Point Detection (CPD) is a critical task in time series analysis, aiming to identify moments when the underlying data-generating process shifts. Traditional CPD methods often rely on unsupervised techniques, which lack adaptability to task-specific definitions of change and cannot benefit from user knowledge. To address these limitations, we propose MuRAL-CPD, a novel semi-supervised method that integrates active learning into a multiresolution CPD algorithm. MuRAL-CPD leverages a wavelet-based multiresolution decomposition to detect changes across multiple temporal scales and incorporates user feedback to iteratively optimize key hyperparameters. This interaction enables the model to align its notion of change with that of the user, improving both accuracy and interpretability. Our experimental results on several real-world datasets show the effectiveness of MuRAL-CPD against state-of-the-art methods, particularly in scenarios where minimal supervision is available.

</details>


### [154] [Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models](https://arxiv.org/abs/2601.20687)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Xiaobo Xia,Ming-Kun Xie,Dong-Dong Wu,Biao Liu,Yuheng Jia,Xin Geng,Masashi Sugiyama,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出用于本地小模型部署的PU RL蒸馏方法，无需人工标注或奖励模型，实验证明该方法在低成本设置下表现良好。


<details>
  <summary>Details</summary>
Motivation: 本地部署小模型常见，但实际流程大多停在监督微调阶段，无法到达强化学习对齐阶段，因强化学习对齐需要昂贵人工标注或依赖奖励模型，不适合本地部署。

Method: 提出正无标签（PU）RL蒸馏方法，从黑盒生成中提取教师的偏好优化能力到可本地训练的学生模型，通过查询教师获取锚响应、本地采样学生候选、进行锚条件自排名来诱导偏好，实现全本地训练循环。

Result: 理论分析表明诱导的偏好信号顺序一致且集中在近最优候选，支持偏好优化的稳定性；实验证明该方法在低成本设置下表现稳定且强劲。

Conclusion: 提出的PU RL蒸馏方法可有效解决本地小模型部署中强化学习对齐的难题，在低成本下有良好表现。

Abstract: Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.

</details>


### [155] [Optimal Transport Group Counterfactual Explanations](https://arxiv.org/abs/2601.20692)
*Enrique Valero-Leal,Bernd Bischl,Pedro Larrañaga,Concha Bielza,Giuseppe Casalicchio*

Main category: cs.LG

TL;DR: 提出学习显式最优传输映射生成群体反事实解释，实验表明有良好泛化性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有群体反事实解释方法存在不能泛化到新成员、依赖强模型假设、难以控制群体几何失真等问题。

Method: 学习显式最优传输映射，将任何群体实例发送到其反事实，最小化群体总传输成本；对线性分类器通过数学优化推导表示群体反事实的函数。

Result: 实验显示能准确泛化、保留群体几何形状，与基线方法相比额外传输成本可忽略不计；在不能利用模型线性时也显著优于基线。

Conclusion: 所提方法能解决现有群体反事实解释方法的问题，具有更好的性能和泛化能力。

Abstract: Group counterfactual explanations find a set of counterfactual instances to explain a group of input instances contrastively. However, existing methods either (i) optimize counterfactuals only for a fixed group and do not generalize to new group members, (ii) strictly rely on strong model assumptions (e.g., linearity) for tractability or/and (iii) poorly control the counterfactual group geometry distortion. We instead learn an explicit optimal transport map that sends any group instance to its counterfactual without re-optimization, minimizing the group's total transport cost. This enables generalization with fewer parameters, making it easier to interpret the common actionable recourse. For linear classifiers, we prove that functions representing group counterfactuals are derived via mathematical optimization, identifying the underlying convex optimization type (QP, QCQP, ...). Experiments show that they accurately generalize, preserve group geometry and incur only negligible additional transport cost compared to baseline methods. If model linearity cannot be exploited, our approach also significantly outperforms the baselines.

</details>


### [156] [Is Pure Exploitation Sufficient in Exogenous MDPs with Linear Function Approximation?](https://arxiv.org/abs/2601.20694)
*Hao Liang,Jiayu Cheng,Sean R. Sinclair,Yali Du*

Main category: cs.LG

TL;DR: 提出纯利用学习(PEL)方法，证明外生MDPs中纯利用算法的有限样本遗憾界，实验显示PEL表现优于基线，表明纯利用足够。


<details>
  <summary>Details</summary>
Motivation: 现有外生MDPs遗憾保证依赖显式探索或表格假设，而实际中贪心纯利用方法效果好，理论落后。

Method: 提出PEL方法，针对表格情况分析其遗憾界；对大连续内生状态空间引入LSVI - PE线性近似方法；引入反事实轨迹和贝尔曼封闭特征传输工具。

Result: 在表格情况PEL达到特定遗憾界，LSVI - PE遗憾与特征维度等相关；实验中PEL优于基线。

Conclusion: 颠覆探索必要的传统观念，证明外生MDPs中纯利用足够。

Abstract: Exogenous MDPs (Exo-MDPs) capture sequential decision-making where uncertainty comes solely from exogenous inputs that evolve independently of the learner's actions. This structure is especially common in operations research applications such as inventory control, energy storage, and resource allocation, where exogenous randomness (e.g., demand, arrivals, or prices) drives system behavior. Despite decades of empirical evidence that greedy, exploitation-only methods work remarkably well in these settings, theory has lagged behind: all existing regret guarantees for Exo-MDPs rely on explicit exploration or tabular assumptions. We show that exploration is unnecessary. We propose Pure Exploitation Learning (PEL) and prove the first general finite-sample regret bounds for exploitation-only algorithms in Exo-MDPs. In the tabular case, PEL achieves $\widetilde{O}(H^2|Ξ|\sqrt{K})$. For large, continuous endogenous state spaces, we introduce LSVI-PE, a simple linear-approximation method whose regret is polynomial in the feature dimension, exogenous state space, and horizon, independent of the endogenous state and action spaces. Our analysis introduces two new tools: counterfactual trajectories and Bellman-closed feature transport, which together allow greedy policies to have accurate value estimates without optimism. Experiments on synthetic and resource-management tasks show that PEL consistently outperforming baselines. Overall, our results overturn the conventional wisdom that exploration is required, demonstrating that in Exo-MDPs, pure exploitation is enough.

</details>


### [157] [Structurally Human, Semantically Biased: Detecting LLM-Generated References with Embeddings and GNNs](https://arxiv.org/abs/2601.20704)
*Melika Mobini,Vincent Holst,Floriano Tori,Andres Algaba,Vincent Ginis*

Main category: cs.LG

TL;DR: 本文构建了真实和GPT - 4o生成的引文图，对比不同特征区分LLM和人类参考文献列表的能力，发现LLM参考文献拓扑结构与人类接近，但有语义特征可检测。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型生成的参考文献列表是否与人类的可区分。

Method: 构建真实和GPT - 4o生成的配对引文图，添加符合特定条件的随机基线，对比结构特征和嵌入特征，使用随机森林和图神经网络进行分析。

Result: 仅结构特征难以区分GPT与真实列表，但嵌入特征可显著提高区分度，在不同模型上结果稳健。

Conclusion: 仅基于参数知识生成的LLM参考文献在拓扑上模仿人类，但有可检测的语义指纹，检测和去偏应针对内容信号而非全局图结构。

Abstract: Large language models are increasingly used to curate bibliographies, raising the question: are their reference lists distinguishable from human ones? We build paired citation graphs, ground truth and GPT-4o-generated (from parametric knowledge), for 10,000 focal papers ($\approx$ 275k references) from SciSciNet, and added a field-matched random baseline that preserves out-degree and field distributions while breaking latent structure. We compare (i) structure-only node features (degree/closeness/eigenvector centrality, clustering, edge count) with (ii) 3072-D title/abstract embeddings, using an RF on graph-level aggregates and Graph Neural Networks with node features. Structure alone barely separates GPT from ground truth (RF accuracy $\approx$ 0.60) despite cleanly rejecting the random baseline ($\approx$ 0.89--0.92). By contrast, embeddings sharply increase separability: RF on aggregated embeddings reaches $\approx$ 0.83, and GNNs with embedding node features achieve 93\% test accuracy on GPT vs.\ ground truth. We show the robustness of our findings by replicating the pipeline with Claude Sonnet 4.5 and with multiple embedding models (OpenAI and SPECTER), with RF separability for ground truth vs.\ Claude $\approx 0.77$ and clean rejection of the random baseline. Thus, LLM bibliographies, generated purely from parametric knowledge, closely mimic human citation topology, but leave detectable semantic fingerprints; detection and debiasing should target content signals rather than global graph structure.

</details>


### [158] [Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions](https://arxiv.org/abs/2601.20714)
*Raul de la Rosa,Ivana Dusparic,Nicolas Cardozo*

Main category: cs.LG

TL;DR: 本文提出MORPHIN自适应性Q学习框架，能在不重新训练的情况下适应奖励函数变化和动作空间扩展，经实验验证比标准Q学习更优。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习智能体在非平稳环境中，奖励函数变化或动作空间扩展时难以适应的问题。

Method: 引入MORPHIN框架，将概念漂移检测与学习和探索超参数的动态调整相结合，保留先验策略知识。

Result: 通过Gridworld基准和交通信号控制模拟验证，MORPHIN比标准Q学习基线有更好的收敛速度和持续适应性，学习效率最高提升1.7倍。

Conclusion: MORPHIN框架能有效解决强化学习在非平稳环境中的适应问题，提高学习效率。

Abstract: Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.

</details>


### [159] [Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis](https://arxiv.org/abs/2601.20729)
*Anchen Sun,Zhibin Chen,Xiaodong Cai*

Main category: cs.LG

TL;DR: 本文针对基于ANN的Cox模型训练数据不足问题，提出Cox - MT模型，在癌症预后预测中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 基于ANN的Cox模型训练需大量标记样本，标记数据有限限制其性能。

Method: 采用深度半监督学习方法，基于Mean Teacher框架开发单模态和多模态ANN - 基Cox模型Cox - MT。

Result: 单模态Cox - MT模型在四种癌症预测中优于Cox - nnet，增加未标记样本可提升性能，多模态模型表现更优。

Conclusion: Cox - MT模型能有效利用标记和未标记数据，相比仅用标记数据训练的模型提高预测准确性。

Abstract: The Cox Proportional Hazards (PH) model is widely used in survival analysis. Recently, artificial neural network (ANN)-based Cox-PH models have been developed. However, training these Cox models with high-dimensional features typically requires a substantial number of labeled samples containing information about time-to-event. The limited availability of labeled data for training often constrains the performance of ANN-based Cox models. To address this issue, we employed a deep semi-supervised learning (DSSL) approach to develop single- and multi-modal ANN-based Cox models based on the Mean Teacher (MT) framework, which utilizes both labeled and unlabeled data for training. We applied our model, named Cox-MT, to predict the prognosis of several types of cancer using data from The Cancer Genome Atlas (TCGA). Our single-modal Cox-MT models, utilizing TCGA RNA-seq data or whole slide images, significantly outperformed the existing ANN-based Cox model, Cox-nnet, using the same data set across four types of cancer considered. As the number of unlabeled samples increased, the performance of Cox-MT significantly improved with a given set of labeled data. Furthermore, our multi-modal Cox-MT model demonstrated considerably better performance than the single-modal model. In summary, the Cox-MT model effectively leverages both labeled and unlabeled data to significantly enhance prediction accuracy compared to existing ANN-based Cox models trained solely on labeled data.

</details>


### [160] [Continual GUI Agents](https://arxiv.org/abs/2601.20732)
*Ziwei Liu,Borui Kang,Hangjie Yuan,Zixiang Zhao,Wei Li,Yifan Zhu,Tao Feng*

Main category: cs.LG

TL;DR: 提出持续GUI代理任务，引入GUI - AiF框架解决GUI分布变化时现有方法的问题，实验表明该框架优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 数字环境变化，静态环境训练的GUI代理性能下降，现有方法在GUI分布变化时无法保持稳定基础。

Method: 引入GUI - AiF强化微调框架，包含APR - iF和ARR - iF两种新奖励，引导代理适应变化的交互点和区域。

Result: 广泛实验显示GUI - AiF超越了最先进的基线。

Conclusion: 建立了首个GUI代理的持续学习框架，揭示了强化微调对持续GUI代理的潜力。

Abstract: As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.

</details>


### [161] [HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs](https://arxiv.org/abs/2601.20745)
*Guoan Wang,Feiyu Wang,Zongwei Lv,Yikun Zong,Tong Yang*

Main category: cs.LG

TL;DR: 为解决大语言模型部署的内存瓶颈，提出Hestia框架，在Llama - 3.2上评估表现优于现有三元QAT基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署受内存墙瓶颈，多数QAT方法存在过早离散优化景观和梯度不匹配问题，阻碍量化模型有效优化。

Method: 提出Hestia框架，用温度控制的softmax松弛代替刚性阶跃函数，利用张量级Hessian迹度量驱动细粒度温度退火。

Result: 在Llama - 3.2上评估，Hestia始终优于现有三元QAT基线，1B和3B模型零样本平均提升5.39%和4.34%。

Conclusion: Hessian引导的松弛能有效恢复表征能力，为1.58位大语言模型建立更稳健训练路径。

Abstract: As large language models (LLMs) continue to scale, deployment is increasingly bottlenecked by the memory wall, motivating a shift toward extremely low-bit quantization. However, most quantization-aware training (QAT) methods apply hard rounding and the straight-through estimator (STE) from the beginning of the training, which prematurely discretizes the optimization landscape and induces persistent gradient mismatch between latent weights and quantized weights, hindering effective optimization of quantized models. To address this, we propose Hestia, a Hessian-guided differentiable QAT framework for extremely low-bit LLMs, which replaces the rigid step function with a temperature-controlled softmax relaxation to maintain gradient flow early in training while progressively hardening quantization. Furthermore, Hestia leverages a tensor-wise Hessian trace metric as a lightweight curvature signal to drive fine-grained temperature annealing, enabling sensitivity-aware discretization across the model. Evaluations on Llama-3.2 show that Hestia consistently outperforms existing ternary QAT baselines, yielding average zero-shot improvements of 5.39% and 4.34% for the 1B and 3B models. These results indicate that Hessian-guided relaxation effectively recovers representational capacity, establishing a more robust training path for 1.58-bit LLMs. The code is available at https://github.com/hestia2026/Hestia.

</details>


### [162] [GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning](https://arxiv.org/abs/2601.20753)
*Zhiheng Jiang,Yunzhe Wang,Ryan Marr,Ellen Novoseller,Benjamin T. Files,Volkan Ustun*

Main category: cs.LG

TL;DR: 介绍 GraphAllocBench 这一灵活的基准测试，用于评估 PCPL。新基准受城市管理启发构建，还有新的评估指标，实验显示其可暴露现有方法局限，推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有 PCPL 基准多为玩具任务和固定环境，缺乏现实性和可扩展性。

Method: 引入 GraphAllocBench 基准，基于城市管理灵感的图资源分配环境；提出 PNDS 和 OS 两个新评估指标。

Result: 通过 MLPs 和图感知模型实验，GraphAllocBench 暴露现有 MORL 方法局限，展示图神经网络在复杂任务的潜力。

Conclusion: GraphAllocBench 灵活可变、可扩展，能推动 PCPL 发展。

Abstract: Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL) aims to approximate diverse Pareto-optimal solutions by conditioning policies on user-specified preferences over objectives. This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front. However, existing benchmarks for PCPL are largely restricted to toy tasks and fixed environments, limiting their realism and scalability. To address this gap, we introduce GraphAllocBench, a flexible benchmark built on a novel graph-based resource allocation sandbox environment inspired by city management, which we call CityPlannerEnv. GraphAllocBench provides a rich suite of problems with diverse objective functions, varying preference conditions, and high-dimensional scalability. We also propose two new evaluation metrics -- Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS) -- that directly capture preference consistency while complementing the widely used hypervolume metric. Through experiments with Multi-Layer Perceptrons (MLPs) and graph-aware models, we show that GraphAllocBench exposes the limitations of existing MORL approaches and paves the way for using graph-based methods such as Graph Neural Networks in complex, high-dimensional combinatorial allocation tasks. Beyond its predefined problem set, GraphAllocBench enables users to flexibly vary objectives, preferences, and allocation rules, establishing it as a versatile and extensible benchmark for advancing PCPL. Code: https://anonymous.4open.science/r/GraphAllocBench

</details>


### [163] [Supervised Guidance Training for Infinite-Dimensional Diffusion Models](https://arxiv.org/abs/2601.20756)
*Elizabeth L. Baker,Alexander Denker,Jes Frellsen*

Main category: cs.LG

TL;DR: 本文提出监督引导训练方法，实现对训练好的扩散模型微调，使其从函数空间贝叶斯反问题的后验分布中准确采样。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在函数空间的后验采样理论尚未完善，需解决如何将其条件化以进行后验采样的问题。

Method: 假设先验在Cameron - Martin空间或相对于高斯测度绝对连续，用Doob的h变换的无穷维扩展对模型进行条件化，针对难解的引导项提出无模拟的分数匹配目标（监督引导训练）。

Result: 给出函数空间贝叶斯反问题的数值例子，验证理论有效性。

Conclusion: 本工作首次提供了在函数空间微调训练好的扩散模型以准确从后验分布采样的方法。

Abstract: Score-based diffusion models have recently been extended to infinite-dimensional function spaces, with uses such as inverse problems arising from partial differential equations. In the Bayesian formulation of inverse problems, the aim is to sample from a posterior distribution over functions obtained by conditioning a prior on noisy observations. While diffusion models provide expressive priors in function space, the theory of conditioning them to sample from the posterior remains open. We address this, assuming that either the prior lies in the Cameron-Martin space, or is absolutely continuous with respect to a Gaussian measure. We prove that the models can be conditioned using an infinite-dimensional extension of Doob's $h$-transform, and that the conditional score decomposes into an unconditional score and a guidance term. As the guidance term is intractable, we propose a simulation-free score matching objective (called Supervised Guidance Training) enabling efficient and stable posterior sampling. We illustrate the theory with numerical examples on Bayesian inverse problems in function spaces. In summary, our work offers the first function-space method for fine-tuning trained diffusion models to accurately sample from a posterior.

</details>


### [164] [Less is More: Clustered Cross-Covariance Control for Offline RL](https://arxiv.org/abs/2601.20765)
*Nan Qiao,Sheng Yue,Shuning Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 离线强化学习中分布偏移问题严重，提出分区缓冲采样和梯度惩罚两个策略，提升表现。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中分布偏移问题，标准平方误差目标会引发有害TD交叉协方差，影响策略学习。

Method: 提出分区缓冲采样（C^4）和显式基于梯度的校正惩罚两个互补策略。

Result: 方法具有更高稳定性，在小数据集和强调OOD区域的分割中回报率比之前方法最多提升30%。

Conclusion: 缓冲分区保留最大化目标的下界属性，所提约束缓解极端OOD区域过度保守问题，不改变策略约束离线强化学习核心行为。

Abstract: A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.

</details>


### [165] [COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI](https://arxiv.org/abs/2601.20772)
*Shakhyar Gogoi*

Main category: cs.LG

TL;DR: COMET - SG1是面向边缘和嵌入式AI系统时间序列预测的轻量级稳定自回归回归模型，实验显示其有优势，适用于相关应用。


<details>
  <summary>Details</summary>
Motivation: 为边缘和嵌入式AI系统的时间序列预测提供一种能减少长期预测误差累积、满足边缘部署需求的模型。

Method: 通过线性行为空间编码、内存锚定过渡估计和确定性状态更新进行操作。

Result: 在非平稳合成时间序列数据实验中，短期预测精度有竞争力，长期漂移显著低于MLP、LSTM和k近邻基线。

Conclusion: COMET - SG1参数少、支持定点运算，为边缘和嵌入式AI应用的稳定自回归预测提供了实用且可解释的方法。

Abstract: COMET-SG1 is a lightweight, stability-oriented autoregressive regression model designed for time-series prediction on edge and embedded AI systems. Unlike recurrent neural networks or transformer-based sequence models, COMET-SG1 operates through linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates. This structure prioritizes bounded long-horizon behavior under fully autoregressive inference, a critical requirement for edge deployment where prediction errors accumulate over time. Experiments on non-stationary synthetic time-series data demonstrate that COMET-SG1 achieves competitive short-horizon accuracy while exhibiting significantly reduced long-horizon drift compared to MLP, LSTM, and k-nearest neighbor baselines. With a compact parameter footprint and operations compatible with fixed-point arithmetic, COMET-SG1 provides a practical and interpretable approach for stable autoregressive prediction in edge and embedded AI applications.

</details>


### [166] [Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying](https://arxiv.org/abs/2601.20773)
*Rubén Jiménez,Oriol Pujol*

Main category: cs.LG

TL;DR: 提出基于距离的复制框架处理黑盒复制问题，实验显示有性能提升。


<details>
  <summary>Details</summary>
Motivation: 部署的机器学习系统需不断演变，硬标签输出下黑盒复制在恢复边界几何形状能力上受限。

Method: 提出基于距离的复制（蒸馏）框架，用到教师决策边界的有符号距离替代硬标签监督，开发α控制的平滑和正则化方案，引入两种模型无关算法估计有符号距离。

Result: 在合成问题和UCI基准测试中，相比硬标签基线在保真度和泛化准确性上有持续提升，能为黑盒副本提供与不确定性相关的距离输出信号。

Conclusion: 基于距离的复制框架有效，可解决硬标签输出下黑盒复制的局限。

Abstract: Deployed machine learning systems must continuously evolve as data, architectures, and regulations change, often without access to original training data or model internals. In such settings, black-box copying provides a practical refactoring mechanism, i.e. upgrading legacy models by learning replicas from input-output queries alone. When restricted to hard-label outputs, copying turns into a discontinuous surface reconstruction problem from pointwise queries, severely limiting the ability to recover boundary geometry efficiently. We propose a distance-based copying (distillation) framework that replaces hard-label supervision with signed distances to the teacher's decision boundary, converting copying into a smooth regression problem that exploits local geometry. We develop an $α$-governed smoothing and regularization scheme with Hölder/Lipschitz control over the induced target surface, and introduce two model-agnostic algorithms to estimate signed distances under label-only access. Experiments on synthetic problems and UCI benchmarks show consistent improvements in fidelity and generalization accuracy over hard-label baselines, while enabling distance outputs as uncertainty-related signals for black-box replicas.

</details>


### [167] [When More Data Doesn't Help: Limits of Adaptation in Multitask Learning](https://arxiv.org/abs/2601.20774)
*Steve Hanneke,Mingyue Xu*

Main category: cs.LG

TL;DR: 文章聚焦多任务学习统计极限，超越已有定理给出更强不可能结果，表明单任务数据丰富也难克服多任务学习难度，并探讨最优适应性概念。


<details>
  <summary>Details</summary>
Motivation: 理解多任务学习的统计极限，突破已有无免费午餐定理的局限。

Method: 建立更强的适应不可能结果，该结果在单任务样本量任意大时都成立。

Result: 得出更强的适应不可能结果，说明单任务有大量数据也无法克服多任务学习的难度。

Conclusion: 多任务学习的难度不能通过单任务拥有丰富数据来克服，还提及最优适应性概念有未来研究价值。

Abstract: Multitask learning and related frameworks have achieved tremendous success in modern applications. In multitask learning problem, we are given a set of heterogeneous datasets collected from related source tasks and hope to enhance the performance above what we could hope to achieve by solving each of them individually. The recent work of arXiv:2006.15785 has showed that, without access to distributional information, no algorithm based on aggregating samples alone can guarantee optimal risk as long as the sample size per task is bounded.
  In this paper, we focus on understanding the statistical limits of multitask learning. We go beyond the no-free-lunch theorem in arXiv:2006.15785 by establishing a stronger impossibility result of adaptation that holds for arbitrarily large sample size per task. This improvement conveys an important message that the hardness of multitask learning cannot be overcame by having abundant data per task. We also discuss the notion of optimal adaptivity that may be of future interests.

</details>


### [168] [Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical & Dynamic Search Spaces](https://arxiv.org/abs/2601.20800)
*Kaito Baba,Yoshihiko Ozaki,Shuhei Watanabe*

Main category: cs.LG

TL;DR: 提出condPED - ANOVA框架用于估计条件搜索空间中超参数重要性，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 原PED - ANOVA无法处理条件搜索空间中依赖其他超参数的超参数，需要新方法。

Method: 引入条件超参数重要性，推导闭式估计器。

Result: 现有超参数重要性估计器在条件设置下会产生误导或无法解释的结果，而condPED - ANOVA能提供反映潜在条件结构的有意义重要性。

Conclusion: condPED - ANOVA是估计条件搜索空间中超参数重要性的有效框架。

Abstract: We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and efficient way to estimate HPI within the top-performing regions of the search space, it assumes a fixed, unconditional search space and therefore cannot properly handle conditional hyperparameters. To address this, we introduce a conditional HPI for top-performing regions and derive a closed-form estimator that accurately reflects conditional activation and domain changes. Experiments show that naive adaptations of existing HPI estimators yield misleading or uninterpretable importance estimates in conditional settings, whereas condPED-ANOVA consistently provides meaningful importances that reflect the underlying conditional structure.

</details>


### [169] [Reinforcement Learning via Self-Distillation](https://arxiv.org/abs/2601.20802)
*Jonas Hübotter,Frederike Lübeck,Lejs Behric,Anton Baumann,Marco Bagatella,Daniel Marta,Ido Hakimi,Idan Shenfeld,Thomas Kleine Buening,Carlos Guestrin,Andreas Krause*

Main category: cs.LG

TL;DR: 本文提出Self - Distillation Policy Optimization (SDPO)方法用于强化学习，在多个任务上提升了样本效率和最终准确率，在测试时还加速了难题的发现。


<details>
  <summary>Details</summary>
Motivation: 当前可验证奖励的强化学习（RLVR）方法仅从每次尝试的标量结果奖励中学习，存在严重的信用分配瓶颈，而许多可验证环境能提供丰富文本反馈。

Method: 将SDPO问题形式化为有丰富反馈的强化学习，把标记化的反馈转化为密集学习信号，将基于反馈的当前模型作为自我教师，把其基于反馈的下一个标记预测提炼回策略。

Result: 在科学推理、工具使用和竞争性编程等任务上，SDPO比强大的RLVR基线提高了样本效率和最终准确率；在仅返回标量反馈的标准RLVR环境中也优于基线；在测试时应用于单个问题加速了难题发现。

Conclusion: SDPO是一种有效的强化学习方法，能利用丰富反馈提升性能，在多种场景有良好表现。

Abstract: Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.

</details>


### [170] [GNN Explanations that do not Explain and How to find Them](https://arxiv.org/abs/2601.20815)
*Steve Azzolin,Stefano Teso,Bruno Lepri,Andrea Passerini,Sagar Malhotra*

Main category: cs.LG

TL;DR: 指出自解释图神经网络（SE - GNNs）解释存在与标签推断无关的关键问题，现有指标难以识别，提出新的忠实度指标。


<details>
  <summary>Details</summary>
Motivation: 现有SE - GNNs解释可能存在次优和误导性问题，但缺少对失败情况的刻画。

Method: 分析SE - GNNs解释，发现解释可能与标签推断无关的问题，通过实验揭示退化解释可以被恶意植入或自然出现，进而提出新的忠实度指标。

Result: 发现许多SE - GNNs能在产生退化解释的同时达到最优真实风险，多数忠实度指标无法识别这些失败模式，新指标能在恶意和自然场景下可靠标记退化解释为不忠实。

Conclusion: SE - GNNs解释存在严重问题，需要可靠审计，新的忠实度指标有重要作用。

Abstract: Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.

</details>


### [171] [Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning](https://arxiv.org/abs/2601.20829)
*Minwu Kim,Safal Shrestha,Keith Ross*

Main category: cs.LG

TL;DR: 提出failure - prefix conditioning方法解决RLVR训练饱和问题，该方法有效且能提高性能。


<details>
  <summary>Details</summary>
Motivation: RLVR训练面对问题饱和时易停滞，原因是难以获取有价值的失败信息。

Method: 提出failure - prefix conditioning方法，基于罕见错误推理轨迹的前缀进行训练，重新分配探索。

Result: 该方法性能提升与中难度问题训练相当，保持token效率，增强模型鲁棒性，迭代更新失败前缀可在性能平稳后进一步提升。

Conclusion: failure - prefix conditioning为解决饱和问题下的RLVR训练提供有效途径。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.

</details>


### [172] [Reward Models Inherit Value Biases from Pretraining](https://arxiv.org/abs/2601.20838)
*Brian Christian,Jessica A. F. Thompson,Elle Michelle Yang,Vincent Adam,Hannah Rose Kirk,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.LG

TL;DR: 本文研究奖励模型（RMs），发现其输出受基础预训练语言模型影响，强调预训练阶段安全性和对齐工作的重要性。


<details>
  <summary>Details</summary>
Motivation: 奖励模型对大语言模型与人类价值观对齐至关重要，但相关研究较少，且其受基础模型影响的性质和程度有待探索。

Method: 使用经过验证的心理语言语料库对10种领先的开源权重奖励模型进行综合研究，利用“Big Two”心理轴分析，进行消融实验。

Result: 不同基础模型的奖励模型在人类价值观的多个维度上有显著差异，如Llama RMs偏好“能动性”，Gemma RMs偏好“共融性”，这种影响可追溯到预训练模型的对数概率，且该效应可重复且持久。

Conclusion: 奖励模型的输出受基础预训练语言模型影响，凸显预训练阶段安全和对齐工作的重要性，开源开发者选择基础模型时需考虑价值观。

Abstract: Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the "Big Two" psychological axes, we show a robust preference of Llama RMs for "agency" and a corresponding robust preference of Gemma RMs for "communion." This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.

</details>


### [173] [PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting](https://arxiv.org/abs/2601.20845)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz*

Main category: cs.LG

TL;DR: 介绍PatchFormer时间序列基础模型，在多领域数据集实验中展示零样本多步预测优势，减少误差和训练数据需求，处理序列速度快。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法需特定领域特征工程和大量标注数据，希望提出更好解决方案。

Method: 将时间序列分割成补丁，通过分层掩码重建进行自监督预训练，使用轻量级适配器进行高效迁移；预训练采用动态掩码的掩码补丁重建和跨领域知识蒸馏。

Result: 在24个基准数据集上实现零样本多步预测的最优效果，均方误差降低27.3%，所需特定任务训练数据减少94%；处理长度为512的序列比全序列变压器快3.8倍。

Conclusion: PatchFormer是有效的时间序列基础模型，在多领域预测任务中表现出色。

Abstract: Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.

</details>


### [174] [C3Box: A CLIP-based Class-Incremental Learning Toolbox](https://arxiv.org/abs/2601.20852)
*Hao Sun,Da-Wei Zhou*

Main category: cs.LG

TL;DR: 传统机器学习系统处理数据流时存在遗忘问题，CIL是解决方向，现有CLIP - based CIL方法有缺陷，提出C3Box工具箱解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP - based CIL方法分散、配置不一致，阻碍公平比较、可重复性和实际应用，需要统一工具。

Method: 提出C3Box工具箱，将多种CIL方法集成到统一CLIP框架，继承PyCIL设计，提供JSON配置和标准化执行流程。

Result: C3Box可实现低工程开销的可重复实验，是可靠基准平台，代码开源。

Conclusion: C3Box是模块化、全面且用户友好的Python工具箱，能推动持续学习研究。

Abstract: Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at https://github.com/LAMDA-CL/C3Box.

</details>


### [175] [Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation](https://arxiv.org/abs/2601.20854)
*Aníbal Silva,Moisés Santos,André Restivo,Carlos Soares*

Main category: cs.LG

TL;DR: 本文实证研究将Transformers集成到VAE不同组件的影响，在57个数据集上实验得出两个结论，一是利用潜在和解码器表示的Transformers在保真度和多样性间有取舍，二是Transformer连续块相似度高，解码器输入输出近似线性。


<details>
  <summary>Details</summary>
Motivation: 表格数据对生成模型是挑战，标准VAE架构难建模特征关系，而Transformers更适合捕捉复杂特征交互，因此研究将其集成到VAE不同组件的影响。

Method: 在OpenML CC18套件的57个数据集上进行实验。

Result: 1. 让Transformers利用潜在和解码器表示会使保真度和多样性产生取舍；2. 所有组件中Transformer的连续块相似度高，解码器中输入输出关系近似线性。

Conclusion: 明确了将Transformers集成到VAE不同组件产生的影响特点。

Abstract: Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.

</details>


### [176] [Evolutionary Strategies lead to Catastrophic Forgetting in LLMs](https://arxiv.org/abs/2601.20861)
*Immanuel Abdi,Akshat Gupta,Micah Mok,Alexander Lu,Nicholas Lee,Gopala Anumanchipalli*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [177] [Colored Markov Modulated Fluid Queues](https://arxiv.org/abs/2601.20537)
*Benny Van Houdt*

Main category: cs.PF

TL;DR: 本文引入彩色马尔可夫调制流体队列（MMFQs）和带流体跳跃的彩色MMFQs，增强了建模灵活性。


<details>
  <summary>Details</summary>
Motivation: 经典MMFQs虽可分析多种跳跃过程，但缺乏特定记忆形式，为增强建模灵活性，分析难处理的排队系统。

Method: 引入彩色MMFQs和带流体跳跃的彩色MMFQs。

Result: 新框架提供额外记忆形式，可追踪特定事件发生时的流体水平。

Conclusion: 新框架大大增强了建模灵活性，能分析因维度灾难或状态空间爆炸而难以处理的排队系统。

Abstract: Markov-modulated fluid queues (MMFQs) are a powerful modeling framework for analyzing the performance of computer and communication systems. Their distinguishing feature is that the underlying Markov process evolves on a continuous state space, making them well suited to capture the dynamics of workloads, energy levels, and other performance-related quantities. Although classical MMFQs do not permit jumps in the fluid level, they can still be applied to analyze a wide range of jump processes.
  In this paper, we generalize the MMFQ framework in a new direction by introducing {\bf colored MMFQs} and {\bf colored MMFQs with fluid jumps}. This enriched framework provides an additional form of memory: the color of incoming fluid can be used to keep track of the fluid level when certain events took place. This capability greatly enhances modeling flexibility and enables the analysis of queueing systems that would otherwise be intractable due to the curse of dimensionality or state-space explosion.

</details>


### [178] [The Multiserver-Job Stochastic Recurrence Equation for Cloud Computing Performance Evaluation](https://arxiv.org/abs/2601.20653)
*Francois Baccelli,Diletta Olliaro,Marco Ajmone Marsan,Andrea Marin*

Main category: cs.PF

TL;DR: 用随机递归方程和遍历理论研究多服务器作业排队模型，证明性质、定义稳定性条件、引入算法，算法可GPU并行化且方法可扩展到复杂系统。


<details>
  <summary>Details</summary>
Motivation: 研究多服务器作业排队模型在先进先出调度下的特性和稳定性条件。

Method: 使用随机递归方程和遍历理论，证明模型性质，应用Loynes定理的单调可分扩展。

Result: 证明模型性质，定义稳定性条件，引入两个算法，SPS算法可GPU并行化。

Conclusion: 该方法可扩展到更复杂系统，如带类型资源的多服务器作业排队模型。

Abstract: We study the Multiserver-Job Queuing Model (MJQM) with general independent arrivals and service times under FCFS scheduling, using stochastic recurrence equations (SREs) and ergodic theory. We prove the monotonicity and separability properties of the MJQM SRE, enabling the application of the monotone-separable extension of Loynes' theorem and the formal definition of the MJQM stability condition. Based on these results, we introduce and implement two algorithms: one for drawing sub-perfect samples (SPS) of the system's workload and the second one to estimate the system's stability condition given the statistics of the jobs' input stream. The SPS algorithm allows for a massive GPU parallelization, greatly improving the efficiency of performance metrics evaluation. We also show that this approach extends to more complex systems, including MJQMs with typed resources.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [179] [Achieving Productivity Gains with AI-based IDE features: A Journey at Google](https://arxiv.org/abs/2601.19964)
*Maxim Tabachnyk,Xu Shu,Alexander Frömmgen,Pavel Sychev,Vahid Meimand,Ilia Krets,Stanislav Pyatykh,Abner Araujo,Kristóf Molnár,Satish Chandra*

Main category: cs.SE

TL;DR: 文章探讨谷歌开发和完善基于AI的IDE功能（代码补全和代码转换）的过程，及应对的挑战，展示如何优化AI开发工具提升企业生产力。


<details>
  <summary>Details</summary>
Motivation: 提升AI开发者工具在企业环境中的生产力。

Method: 通过严格实验应对延迟、用户体验和建议质量等挑战。

Result: 展示了在用户界面、后端和模型层优化AI开发者工具的过程。

Conclusion: 可以通过在各层面优化AI开发者工具，在企业环境中实现生产力的切实提升。

Abstract: We discuss Google's journey in developing and refining two internal AI-based IDE features: code completion and natural-language-driven code transformation (Transform Code). We address challenges in latency, user experience and suggestion quality, all backed by rigorous experimentation. The article serves as an example of how to refine AI developer tools across the user interface, backend, and model layers, to deliver tangible productivity improvements in an enterprise setting.

</details>


### [180] [Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis](https://arxiv.org/abs/2601.20103)
*Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.SE

TL;DR: 本文提出奖励漏洞利用分类法和TRACE基准，对比不同检测设置效果，揭示模型检测情况并分析影响因素，发布基准助力社区。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型用于代码强化学习评估，其检测奖励破解能力研究不足，需更好方法防止奖励破解。

Method: 提出54类奖励漏洞利用分类法，创建含517条测试轨迹的TRACE基准，对比孤立分类和对比异常检测设置。

Result: 对比设置下模型检测奖励破解更有效，GPT - 5.2最高推理模式检测率从45%提升到63%，模型处理语义情境奖励破解更困难，良性与破解轨迹比例和分析簇大小影响检测性能。

Conclusion: 发布基准和评估工具，方便社区扩展TRACE和评估模型。

Abstract: Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.

</details>


### [181] [Are We All Using Agents the Same Way? An Empirical Study of Core and Peripheral Developers Use of Coding Agents](https://arxiv.org/abs/2601.20106)
*Shamse Tasnim Cynthia,Joy Krishan Das,Banani Roy*

Main category: cs.SE

TL;DR: 对9427个代理PR进行实证研究，分析核心与外围开发者使用、审查、修改和验证代理生成贡献的情况，给出四方面发现并为开发者协作提供见解。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明核心与外围开发者对AI工具使用有差异，但新兴自主编码代理时代这种情况尚不明确，因此开展研究。

Method: 对9427个代理PR进行定性和定量混合分析。

Result: 1. 部分外围开发者更常用代理，任务分配均匀；核心开发者聚焦文档和测试，其PR更易合并。2. 核心开发者更多参与审查讨论，两组都关注可演化性问题。3. 代理PR较少被修改，修改时两组都常重构。4. 外围开发者更可能不运行CI检查就合并，核心开发者更坚持验证通过才接受。

Conclusion: 研究提供了开发者经验如何影响集成的全面视角，为核心和外围开发者与编码代理有效协作提供了见解。

Abstract: Autonomous AI agents are transforming software development and redefining how developers collaborate with AI. Prior research shows that the adoption and use of AI-powered tools differ between core and peripheral developers. However, it remains unclear how this dynamic unfolds in the emerging era of autonomous coding agents. In this paper, we present the first empirical study of 9,427 agentic PRs, examining how core and peripheral developers use, review, modify, and verify agent-generated contributions prior to acceptance. Through a mix of qualitative and quantitative analysis, we make four key contributions. First, a subset of peripheral developers use agents more often, delegating tasks evenly across bug fixing, feature addition, documentation, and testing. In contrast, core developers focus more on documentation and testing, yet their agentic PRs are frequently merged into the main/master branch. Second, core developers engage slightly more in review discussions than peripheral developers, and both groups focus on evolvability issues. Third, agentic PRs are less likely to be modified, but when they are, both groups commonly perform refactoring. Finally, peripheral developers are more likely to merge without running CI checks, whereas core developers more consistently require passing verification before acceptance. Our analysis offers a comprehensive view of how developer experience shapes integration offer insights for both peripheral and core developers on how to effectively collaborate with coding agents.

</details>


### [182] [Beyond Bug Fixes: An Empirical Investigation of Post-Merge Code Quality Issues in Agent-Generated Pull Requests](https://arxiv.org/abs/2601.20109)
*Shamse Tasnim Cynthia,Al Muttakin,Banani Roy*

Main category: cs.SE

TL;DR: 分析AI编码代理生成的PR合并后代码质量，发现合并成功不代表代码质量高，需系统检查。


<details>
  <summary>Details</summary>
Motivation: 现有工作多依赖基准和受控任务，AI编码代理生成的PR合并后代码质量缺乏大规模分析。

Method: 分析AIDev数据集中Python仓库的1210个合并的代理生成的bug修复PR，用SonarQube进行差异分析。

Result: 各代理原始问题数量差异经代码变更归一化后消失，代码异味占主导，bug少但严重。

Conclusion: 合并成功不能可靠反映合并后代码质量，需对代理生成的bug修复PR进行系统质量检查。

Abstract: The increasing adoption of AI coding agents has increased the number of agent-generated pull requests (PRs) merged with little or no human intervention. Although such PRs promise productivity gains, their post-merge code quality remains underexplored, as prior work has largely relied on benchmarks and controlled tasks rather than large-scale post-merge analyses. To address this gap, we analyze 1,210 merged agent-generated bug-fix PRs from Python repositories in the AIDev dataset. Using SonarQube, we perform a differential analysis between base and merged commits to identify code quality issues newly introduced by PR changes. We examine issue frequency, density, severity, and rule-level prevalence across five agents. Our results show that apparent differences in raw issue counts across agents largely disappear after normalizing by code churn, indicating that higher issue counts are primarily driven by larger PRs. Across all agents, code smells dominate, particularly at critical and major severities, while bugs are less frequent but often severe. Overall, our findings show that merge success does not reliably reflect post-merge code quality, highlighting the need for systematic quality checks for agent-generated bug-fix PRs.

</details>


### [183] [Usage, Effects and Requirements for AI Coding Assistants in the Enterprise: An Empirical Study](https://arxiv.org/abs/2601.20112)
*Maja Vukovic,Rangeet Pan,Tin Kam Ho,Rahul Krishna,Raju Pavuluri,Michele Merler*

Main category: cs.SE

TL;DR: 本文调查开发者对AI编码助手和CodeLLMs的使用体验，结合现有调查讨论其需求。


<details>
  <summary>Details</summary>
Motivation: 探讨编码助手和CodeLLMs是否适用于实际项目和企业用例，以及对软件工程流程和用户体验的影响。

Method: 对57位不同领域和技能水平的开发者进行调查，回顾35份关于AI编码助手和CodeLLMs的用户调查。

Result: 基于调查结果和现有调查分析得出相关发现。

Conclusion: 讨论了AI编码助手的需求。

Abstract: The rise of large language models (LLMs) has accelerated the development of automated techniques and tools for supporting various software engineering tasks, e.g., program understanding, code generation, software testing, and program repair. As CodeLLMs are being employed toward automating these tasks, one question that arises, especially in enterprise settings, is whether these coding assistants and the code LLMs that power them are ready for real-world projects and enterprise use cases, and how do they impact the existing software engineering process and user experience. In this paper we survey 57 developers from different domains and with varying software engineering skill about their experience with AI coding assistants and CodeLLMs. We also reviewed 35 user surveys on the usage, experience and expectations of professionals and students using AI coding assistants and CodeLLMs. Based on our study findings and analysis of existing surveys, we discuss the requirements for AI-powered coding assistants.

</details>


### [184] [Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization](https://arxiv.org/abs/2601.20147)
*Saima Afrin,Zaiyu Cheng,Tushar Sharma,Alexander Serebrenik,Massimiliano Di Penta,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 研究系统提示对指令调优语言模型和代码语言模型代码生成性能的影响，发现系统提示影响随模型规模增大，少样本提示可减弱影响，不同编程语言敏感度不同。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索系统提示对通用ILMs和专业CLMs代码生成性能的影响，本文旨在填补这一空白。

Method: 系统评估不同指令细节的系统提示、模型规模、提示策略和编程语言对ILMs和CLMs代码生成任务的影响，涵盖120种模型配置。

Result: 系统提示的影响随模型规模增大；少样本提示比零样本提示减少该影响；Java比Python对系统提示变化更敏感。

Conclusion: 明确了系统提示、模型规模、提示策略和编程语言在代码生成任务中的影响。

Abstract: Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.

</details>


### [185] [LogSieve: Task-Aware CI Log Reduction for Sustainable LLM-Based Analysis](https://arxiv.org/abs/2601.20148)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 提出LogSieve日志减少技术，在20个开源安卓项目CI日志上评估，平均减少行数42%、标记40%，降低计算和能耗成本，有语义保真优势，推动更环保、可解释的CI自动化。


<details>
  <summary>Details</summary>
Motivation: 持续集成（CI）日志量和详细程度增加，手动和自动分析成本高、耗时长且不环保，现有日志处理技术多针对结构化日志而非CI工作流程中的非结构化日志。

Method: 提出LogSieve轻量级、根因分析（RCA）感知且语义保留的日志减少技术，过滤低信息行，保留对下游推理相关内容；用嵌入基分类器自动检测相关性。

Result: 在20个开源安卓项目CI日志评估中，LogSieve平均减少42%行数和40%标记，语义损失小；相比结构优先基线，语义和分类保真度高；嵌入基分类器检测相关性准确率近人类（97%）。

Conclusion: LogSieve架起日志管理和大语言模型推理桥梁，为更环保、可解释的CI自动化提供实践路径。

Abstract: Logs are essential for understanding Continuous Integration (CI) behavior, particularly for diagnosing build failures and performance regressions. Yet their growing volume and verbosity make both manual inspection and automated analysis increasingly costly, time-consuming, and environmentally costly. While prior work has explored log compression, anomaly detection, and LLM-based log analysis, most efforts target structured system logs rather than the unstructured, noisy, and verbose logs typical of CI workflows.
  We present LogSieve, a lightweight, RCA-aware and semantics-preserving log reduction technique that filters low-information lines while retaining content relevant to downstream reasoning. Evaluated on CI logs from 20 open-source Android projects using GitHub Actions, LogSieve achieves an average 42% reduction in lines and 40% reduction in tokens with minimal semantic loss. This pre-inference reduction lowers computational cost and can proportionally reduce energy use (and associated emissions) by decreasing the volume of data processed during LLM inference.
  Compared with structure-first baselines (LogZip and random-line removal), LogSieve preserves much higher semantic and categorical fidelity (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy). Embedding-based classifiers automate relevance detection with near-human accuracy (97%), enabling scalable and sustainable integration of semantics-aware filtering into CI workflows. LogSieve thus bridges log management and LLM reasoning, offering a practical path toward greener and more interpretable CI automation.

</details>


### [186] [Cascaded Vulnerability Attacks in Software Supply Chains](https://arxiv.org/abs/2601.20158)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: 提出SBOM驱动的安全分析新方法，建模漏洞关系，用HGAT预测组件漏洞，将级联发现建模为链接预测问题，HGAT组件分类器有不错表现。


<details>
  <summary>Details</summary>
Motivation: 当前软件安全分析工具孤立评估漏洞，复杂软件供应链安全威胁源于级联漏洞链，且不同SBOM生成器和分析工具的下游漏洞发现差异大。

Method: 基于依赖结构建模漏洞关系，将丰富的SBOM表示为异构图，训练HGAT预测组件是否关联已知漏洞，用多层感知器神经网络将级联发现建模为CVE对的链接预测问题。

Result: HGAT组件分类器准确率达91.03%，F1分数为74.02%。

Conclusion: 所提的SBOM驱动的安全分析方法和工具在组件漏洞预测方面有较好效果。

Abstract: Most of the current software security analysis tools assess vulnerabilities in isolation. However, sophisticated software supply chain security threats often stem from cascaded vulnerability and security weakness chains that span dependent components. Moreover, although the adoption of Software Bills of Materials (SBOMs) has been accelerating, downstream vulnerability findings vary substantially across SBOM generators and analysis tools. We propose a novel approach to SBOM-driven security analysis methods and tools. We model vulnerability relationships over dependency structure rather than treating scanner outputs as independent records. We represent enriched SBOMs as heterogeneous graphs with nodes being the SBOM components and dependencies, the known software vulnerabilities, and the known software security weaknesses. We then train a Heterogeneous Graph Attention Network (HGAT) to predict whether a component is associated with at least one known vulnerability. Since documented multi-vulnerability chains are scarce, we model cascade discovery as a link prediction problem over CVE pairs using a multi-layer perceptron neural network. This way, we produce ranked candidate links that can be composed into multi-step paths. The HGAT component classifier achieves an Accuracy of 91.03% and an F1-score of 74.02%.

</details>


### [187] [How do Agents Refactor: An Empirical Study](https://arxiv.org/abs/2601.20160)
*Lukas Ottenhof,Daniel Penner,Abram Hindle,Thibaud Lutellier*

Main category: cs.SE

TL;DR: 文章首对Java中代理重构拉取请求进行分析，对比代理与开发者重构，发现代理重构多为注解更改，仅Cursor模型重构异味有显著增加。


<details>
  <summary>Details</summary>
Motivation: 此前少有研究探讨软件开发代理在Java重构中的实际表现、更改类型及对代码质量的影响，因此开展此研究。

Method: 使用RefactoringMiner和DesigniteJava 3.0，在86个项目中识别重构类型，检测重构提交前后的代码异味。

Result: 代理重构以注解更改为主，开发者则有多样的结构改进；仅Cursor模型在重构异味上有显著增加。

Conclusion: 软件开发代理的Java重构与开发者在重构类型上存在差异，不同代理表现不同。

Abstract: Software development agents such as Claude Code, GitHub Copilot, Cursor Agent, Devin, and OpenAI Codex are being increasingly integrated into developer workflows. While prior work has evaluated agent capabilities for code completion and task automation, there is little work investigating how these agents perform Java refactoring in practice, the types of changes they make, and their impact on code quality. In this study, we present the first analysis of agentic refactoring pull requests in Java, comparing them to developer refactorings across 86 projects per group. Using RefactoringMiner and DesigniteJava 3.0, we identify refactoring types and detect code smells before and after refactoring commits. Our results show that agent refactorings are dominated by annotation changes (the 5 most common refactoring types done by agents are annotation related), in contrast to the diverse structural improvements typical of developers. Despite these differences in refactoring types, we find Cursor to be the only model to show a statistically significant increase in refactoring smells.

</details>


### [188] [Who Writes the Docs in SE 3.0? Agent vs. Human Documentation Pull Requests](https://arxiv.org/abs/2601.20171)
*Kazuma Yamasaki,Joseph Ayobami Joshua,Tasha Settewong,Mahmoud Alfadel,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 研究AI代理在软件开发文档任务中的贡献，发现其提交更多文档PR且编辑常少经人类修改，引发对文档质量和人机协作的担忧。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程向SE3.0发展，AI代理用于开发任务，需了解其在文档任务中的贡献、人类开发者的审查和干预情况，以及这些因素对工作委托风险的影响，且当前该领域研究不足。

Method: 使用AIDev分析1997个由AI代理和人类开发者发起的与文档相关的拉取请求。

Result: AI代理在研究的代码库中提交的文档相关PR远多于人类，且其编辑的文档通常很少经人类后续修改。

Conclusion: AI代理已对文档工作流程有重大贡献，但引发了SE3.0中文档质量保证和人机协作方面的新挑战。

Abstract: As software engineering moves toward SE3.0, AI agents are increasingly used to carry out development tasks and contribute changes to software projects. It is therefore important to understand the extent of these contributions and how human developers review and intervene, since these factors shape the risks of delegating work to AI agents. While recent studies have examined how AI agents support software development tasks (e.g., code generation, issue resolution, and PR automation), their role in documentation tasks remains underexplored-even though documentation is widely consumed and shapes how developers understand and use software.
  Using the AIDev, we analyze 1,997 documentation-related pull requests (PRs) authored by AI agents and human developers, where documentation PRs are those that create or modify project documentation artifacts. We find that AI agents submit substantially more documentation-related PRs than humans in the studied repositories. We further observe that agent-authored documentation edits are typically integrated with little follow-up modification from humans, raising concerns about review practices and the reliability of agent-generated documentation. Overall, while AI agents already contribute substantially to documentation workflows, our results suggest concerns for emerging challenges for documentation quality assurance and human-AI collaboration in SE3.0.

</details>


### [189] [Control Models for In-IDE Code Completion](https://arxiv.org/abs/2601.20223)
*Aral de Moor,Yana Hrynevich,Hleb Badzeika,Vladyslav Furda,Marko Kojic,Artem Savelev,Kostadin Cvejoski,Darya Rovdo,Ekaterina Garanina*

Main category: cs.SE

TL;DR: 本文介绍JetBrains IDE中基于大语言模型代码补全的控制模型，评估不同架构，进行A/B测试并展示潜力。


<details>
  <summary>Details</summary>
Motivation: 让大语言模型代码补全更好地符合用户需求，减少不必要请求，实现更智能的IDE集成。

Method: 在含98个用户的离线数据集上评估基于提升和Transformer的架构，在生产环境进行A/B测试。

Result: 基于提升的方法在多种语言上有离线分类表现，在生产环境改善了补全效率和质量指标。

Conclusion: 使用辅助模型实现大语言模型功能在IDE中更智能集成有潜力，指出未来方向和待解决问题。

Abstract: We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems.

</details>


### [190] [Understanding npm Developers' Practices, Challenges, and Recommendations for Secure Package Development](https://arxiv.org/abs/2601.20240)
*Anthony Peruma,Truman Choy,Gerald Lee,Italo De Oliveira Santos*

Main category: cs.SE

TL;DR: 本文调查npm包开发者如何看待和处理工作中的安全问题，发现开发者优先考虑安全，但认为包安全性一般，存在多种安全担忧，提出改进建议。


<details>
  <summary>Details</summary>
Motivation: npm生态系统中第三方包的近期漏洞导致严重安全漏洞，危及依赖这些包的应用程序完整性，因此研究npm包开发者对安全的认知和处理方式。

Method: 对75名npm包开发者进行在线调查，并采用混合方法分析他们的回复。

Result: 开发者重视安全，但认为包仅处于中等安全水平，对供应链攻击等有担忧；仅40%对当前npm安全工具有满意度；倾向自动化方法；因各种原因丢弃依赖，常快速发布补丁修复漏洞；主要障碍是时间限制和高误报率；开发者希望有更好的检测工具等。

Conclusion: 研究结果有助于npm包贡献者和维护者，通过突出安全挑战，促进关于加强npm生态安全和可信度的最佳实践讨论。

Abstract: Background: The Node Package Manager (npm) ecosystem plays a vital role in modern software development by providing a vast repository of packages and tools that developers can use to implement their software systems. However, recent vulnerabilities in third-party packages have led to serious security breaches, compromising the integrity of applications that depend on them. Objective: This study investigates how npm package developers perceive and handle security in their work. We examined developers' understanding of security risks, the practices and tools they use, the barriers to stronger security measures, and their suggestions for improving the npm ecosystem's security. Method: We conducted an online survey with 75 npm package developers and undertook a mixed-methods approach to analyzing their responses. Results: While developers prioritize security, they perceive their packages as only moderately secure, with concerns about supply chain attacks, dependency vulnerabilities, and malicious code. Only 40% are satisfied with the current npm security tools due to issues such as alert fatigue. Automated methods such as two-factor authentication and npm audit are favored over code reviews. Many drop dependencies due to abandonment or vulnerabilities, and typically respond to vulnerabilities in their packages by quickly releasing patches. Key barriers include time constraints and high false-positive rates. To improve npm security, developers seek better detection tools, clearer documentation, stronger account protections, and more education initiatives. Conclusion: Our findings will benefit npm package contributors and maintainers by highlighting prevalent security challenges and promoting discussions on best practices to strengthen security and trustworthiness within the npm landscape.

</details>


### [191] [How Software Engineering Research Overlooks Local Industry: A Smaller Economy Perspective](https://arxiv.org/abs/2601.20382)
*Klara Borowa,Andrzej Zalewski,Lech Madeyski*

Main category: cs.SE

TL;DR: 来自波兰的研究者分析ICSE FOSE社区调查，指出研究与行业差距问题并给出小经济体软件工程研究与产业合作改进建议。


<details>
  <summary>Details</summary>
Motivation: 展现小经济体尤其是非英语国家软件工程研究者对关键软件社区问题的观点。

Method: 对ICSE FOSE社区调查进行反思性主题分析。

Result: 发现主要问题是研究与行业差距不断扩大，尤其影响小社区和本地小公司。

Conclusion: 基于分析和经验给出小经济体软件工程研究与产业合作的改进建议。

Abstract: The software engineering researchers from countries with smaller economies, particularly non-English speaking ones, represent valuable minorities within the software engineering community. As researchers from Poland, we represent such a country. We analyzed the ICSE FOSE (Future of Software Engineering) community survey through reflexive thematic analysis to show our viewpoint on key software community issues. We believe that the main problem is the growing research-industry gap, which particularly impacts smaller communities and small local companies. Based on this analysis and our experiences, we present a set of recommendations for improvements that would enhance software engineering research and industrial collaborations in smaller economies.

</details>


### [192] [Comprehension vs. Adoption: Evaluating a Language Workbench Through a Family of Experiments](https://arxiv.org/abs/2601.20394)
*Giovanna Broccia,Maurice H. ter Beek,Walter Cazzola,Luca Favalli,Francesco Bertolotti,Alessio Ferrari*

Main category: cs.SE

TL;DR: 本文通过实验评估模块化语言工作台Neverlang，发现用户对其元语言有足够理解、认可其有用性且有使用意愿，但易用性是挑战，且理解性与用户接受度无显著关联。


<details>
  <summary>Details</summary>
Motivation: 当前文献在评估语言工作台时常忽略用户中心方面（如可理解性和接受度），本文旨在填补这一空白。

Method: 采用定制版的方法评估模型（MEM），进行三轮涉及学术界参与者的实验，评估Neverlang元语言和程序的可理解性以及用户接受度，并研究各维度关系。

Result: 用户对Neverlang元语言有足够理解，认可其有用性并有使用意愿，但易用性是挑战；易用性和有用性感知影响使用意愿；理解性与用户接受度无显著关联。

Conclusion: 理解性与采用之间存在复杂相互作用，更高的元语言理解性不一定带来更高接受度。

Abstract: Language workbenches are tools that enable the definition, reuse, and composition of programming languages and their ecosystems, aiming to streamline language development. To facilitate their adoption by language designers, the comprehensibility of the language used to define other languages is an important aspect to evaluate. Moreover, considering that language workbenches are relatively new tools, user acceptance emerges as a crucial factor to be accounted for during their assessment. Current literature often neglects user-centred aspects like comprehensibility and acceptance in the assessment of this breed of tools. This paper addresses this gap through a family of experiments assessing Neverlang, a modular language workbench. The study adopts a tailored version of the Method Evaluation Model (MEM) to evaluate the comprehensibility of Neverlang's meta-language and programs, as well as user acceptance in terms of perceived ease of use, perceived usefulness, and intention to use. It also investigates the relationships among these dimensions. The experiments were conducted in three iterations involving participants from academia. The results reveal that users demonstrate sufficient comprehension of Neverlang's meta-language, particularly concerning its syntax, express a favourable perception of its usefulness, and indicate their intention to use it. However, the results also indicate that Neverlang's ease of use remains a challenge. Additionally, variations in the perceived ease of use and perceived usefulness, whether low or high, influence the users' intention to use the tool. Surprisingly, no significant correlation is found between comprehensibility and user acceptance. Notably, higher comprehensibility of the meta-language does not necessarily translate into greater acceptance, underscoring the complex interplay between comprehension and adoption.

</details>


### [193] [On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents](https://arxiv.org/abs/2601.20404)
*Jai Lal Lulla,Seyedmoein Mohsenimofidi,Matthias Galster,Jie M. Zhang,Sebastian Baltes,Christoph Treude*

Main category: cs.SE

TL;DR: 研究AGENTS.md文件对AI编码代理在GitHub拉取请求上运行时和令牌消耗的影响，发现该文件可降低运行时间和令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 了解仓库级配置工件如何影响AI编码代理的运营效率。

Method: 分析10个仓库和124个拉取请求，在有和没有AGENTS.md文件两种条件下执行代理，并测量执行时间和令牌使用情况。

Result: 存在AGENTS.md文件时，代理的中位运行时间降低28.64%，输出令牌消耗降低16.58%，且任务完成行为相当。

Conclusion: 讨论了AI编码代理在实践中配置和部署的直接影响，并提出了关于仓库级指令作用的研究议程。

Abstract: AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS$.$md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS$.$md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS$.$md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.

</details>


### [194] [An Empirical Evaluation of Modern MLOps Frameworks](https://arxiv.org/abs/2601.20415)
*Jon Marcos-Mercadé,Unai Lopez-Novoa,Mikel Egaña Aranguren*

Main category: cs.SE

TL;DR: 本文对多种MLOps工具进行实证评估，给出不同场景下适用工具的结论。


<details>
  <summary>Details</summary>
Motivation: 随着AI解决方案在专业环境中日益普及，开发者需对当前工具环境做出明智决策。

Method: 针对MNIST数字分类器和IMDB与BERT情感分类器两个常见ML场景，从安装简易性、配置灵活性等多个标准评估MLflow、Metaflow等工具，并给出加权结果。

Result: 得出不同工具在不同场景下的加权评估结果。

Conclusion: 给出不同场景下最适合的工具。

Abstract: Given the increasing adoption of AI solutions in professional environments, it is necessary for developers to be able to make informed decisions about the current tool landscape. This work empirically evaluates various MLOps (Machine Learning Operations) tools to facilitate the management of the ML model lifecycle: MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines. The tools are evaluated by assessing the criteria of Ease of installation, Configuration flexibility, Interoperability, Code instrumentation complexity, result interpretability, and Documentation when implementing two common ML scenarios: Digit classifier with MNIST and Sentiment classifier with IMDB and BERT. The evaluation is completed by providing weighted results that lead to practical conclusions on which tools are best suited for different scenarios.

</details>


### [195] [Challenges in Android Data Disclosure: An Empirical Study](https://arxiv.org/abs/2601.20459)
*Mugdha Khedkar,Michael Schlichtig,Mohamed Soliman,Eric Bodden*

Main category: cs.SE

TL;DR: 本文通过调查和分析开发者讨论，研究开发者填写Google Play商店数据安全部分（DSS）表格的体验，发现开发者在填写表格时存在诸多挑战，强调需要更清晰的指导和工具。


<details>
  <summary>Details</summary>
Motivation: 当前法律框架要求安卓开发者准确报告应用收集的数据，但代码库庞大使报告具有挑战性，因此研究开发者填写DSS表格的体验。

Method: 对41名安卓开发者进行调查，并分析172条在线开发者讨论，共收集683名开发者的见解。

Result: 开发者常手动分类数据或完全省略分类，依赖在线资源；开发者能识别应用收集的数据，但在转化为符合DSS的披露时缺乏信心；面临识别隐私相关数据、理解表格和担心应用被拒等挑战。

Conclusion: 需要更清晰的指导和更易获取的工具来支持开发者履行隐私报告义务。

Abstract: Current legal frameworks enforce that Android developers accurately report the data their apps collect. However, large codebases can make this reporting challenging. This paper employs an empirical approach to understand developers' experience with Google Play Store's Data Safety Section (DSS) form.
  We first survey 41 Android developers to understand how they categorize privacy-related data into DSS categories and how confident they feel when completing the DSS form. To gain a broader and more detailed view of the challenges developers encounter during the process, we complement the survey with an analysis of 172 online developer discussions, capturing the perspectives of 642 additional developers. Together, these two data sources represent insights from 683 developers.
  Our findings reveal that developers often manually classify the privacy-related data their apps collect into the data categories defined by Google-or, in some cases, omit classification entirely-and rely heavily on existing online resources when completing the form. Moreover, developers are generally confident in recognizing the data their apps collect, yet they lack confidence in translating this knowledge into DSS-compliant disclosures. Key challenges include issues in identifying privacy-relevant data to complete the form, limited understanding of the form, and concerns about app rejection due to discrepancies with Google's privacy requirements.
  These results underscore the need for clearer guidance and more accessible tooling to support developers in meeting privacy-aware reporting obligations.

</details>


### [196] [DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning](https://arxiv.org/abs/2601.20615)
*Yanlin Wang,Jiadong Wu,Tianyue Jiang,Mingwei Liu,Jiachi Chen,Chong Wang,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出首个针对基于RAG的代码生成系统计算效率的对抗攻击DrainCode，评估其有效性和泛化性，显示可大幅增加计算开销，用于评估受限环境下LLM安全。


<details>
  <summary>Details</summary>
Motivation: LLM推理的计算成本在安全背景下受关注少，旨在研究针对RAG代码生成系统计算效率的攻击。

Method: 通过基于变异的方法策略性地污染检索上下文，迫使LLM产生更长输出。

Result: DrainCode使延迟最多增加85%，能耗增加49%，输出长度增加超3倍，且在不同提示策略有泛化性，比不同防御方法更有效。

Conclusion: DrainCode是增加LLM计算开销的潜在方法，可用于评估资源受限环境下LLM的安全性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.

</details>


### [197] [Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model](https://arxiv.org/abs/2601.20662)
*Julien Malka,Arnout Engelen*

Main category: cs.SE

TL;DR: 现代软件工程关注软件构建工件完整性，可复现构建有应用潜力但面临挑战，本文介绍针对功能包管理模型的可复现性评估系统 Lila 以解决监控难题。


<details>
  <summary>Details</summary>
Motivation: 现代软件工程中软件成果完整性受关注，可复现构建大规模应用面临高可复现率和大规模监控的挑战，特别是有效可复现性监控问题未解决。

Method: 引入针对功能包管理模型的去中心化可复现性评估系统 Lila，其能实现构建结果的分布式报告和聚合到可复现性数据库。

Result: 未提及具体结果。

Conclusion: Lila 既有利于从业者，也有益于未来实证构建可复现性研究。

Abstract: Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.
  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.
  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.

</details>


### [198] [ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler](https://arxiv.org/abs/2601.20755)
*Bohua Zou,Debayan Roy,Dhimankumar Yogesh Airao,Weihao Xu,Binqi Sun,Yutao Liu,Haibo Chen*

Main category: cs.SE

TL;DR: 开发了一种用于现代大语言模型推理引擎的细粒度、非侵入式分析框架，使推理过程透明可诊断。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理系统缺乏操作级别的可见性，开发者难以了解时间和资源去向，基本问题也常无法解答。

Method: 基于扩展伯克利数据包过滤器（eBPF）技术，动态地在多层运行时函数上附加探针，无需修改或重新编译源代码。

Result: 将收集的跟踪信息转换为丰富可视化内容，展示不同推理行为，运行时开销低于4%且分析保真度高。

Conclusion: 该框架使大语言模型推理透明且可诊断，使性能分析成为优化、调度和资源感知部署的实用工具。

Abstract: As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama.cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.

</details>


### [199] [Context-Augmented Code Generation Using Programming Knowledge Graphs](https://arxiv.org/abs/2601.20810)
*Shahd Seddik,Fahd Seddik,Iman Saberi,Fatemeh Fard,Minh Hieu Huynh,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 本文提出编程知识图谱（PKG）解决大语言模型处理复杂问题及RAG检索和生成问题，评估显示精度提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理复杂问题能力不足，RAG存在检索和生成问题。

Method: 提出PKG进行代码和文本语义表示和细粒度检索，通过树剪枝提高检索精度，用重排序机制减轻幻觉。

Result: 在HumanEval和MBPP上评估，pass@1准确率最高提升20%，在MBPP上较基线提升34%。

Conclusion: PKG和重排序器能有效解决复杂问题，对无RAG时的正确解决方案负面影响小。

Abstract: Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [200] [Do Whitepaper Claims Predict Market Behavior? Evidence from Cryptocurrency Factor Analysis](https://arxiv.org/abs/2601.20336)
*Murad Farzulla*

Main category: q-fin.CP

TL;DR: 本文研究加密货币白皮书声明与市场行为的一致性，构建结合NLP分类和张量分解的流程，结果显示两者弱对齐，探讨了对叙事经济学和投资分析的影响。


<details>
  <summary>Details</summary>
Motivation: 探究加密货币白皮书关于功能和技术能力的声明是否与观察到的市场行为一致。

Method: 构建结合零样本NLP分类（BART - MNLI）与CP张量分解的流程，比较声明矩阵、市场统计数据和潜在因素三个空间，使用Procrustes旋转和Tucker的一致性系数测试对齐情况，进行跨模型验证和横截面分析。

Result: 结果显示声明与统计数据、声明与因素、统计数据与因素均为弱对齐；统计数据与因素的结果验证了方法；跨模型验证有一定一致性；横截面分析显示不同项目贡献异质；排除比特币不影响结果。

Conclusion: 白皮书叙事与市场因素结构弱对齐，因样本量有限难以区分弱对齐和无对齐，但可排除强对齐，讨论了对叙事经济学和投资分析的影响。

Abstract: Cryptocurrency projects articulate value propositions through whitepapers, making claims about functionality and technical capabilities. This study investigates whether these narratives align with observed market behavior. We construct a pipeline combining zero-shot NLP classification (BART-MNLI) with CP tensor decomposition to compare three spaces: (1) a claims matrix from 24 whitepapers across 10 semantic categories, (2) market statistics for 49 assets over two years of hourly data, and (3) latent factors from tensor decomposition (rank 2, 92.45% variance explained). Using Procrustes rotation and Tucker's congruence coefficient, we test alignment across 23 common entities.
  Results show weak alignment: claims-statistics (phi=0.341, p=0.332), claims-factors (phi=0.077, p=0.747), and statistics-factors (phi=0.197, p<0.001). The statistics-factors significance validates our methodology, confirming the pipeline detects relationships when present. Inter-model validation with DeBERTa-v3 yields 32% exact agreement but 67% top-3 agreement. Cross-sectional analysis reveals heterogeneous contributions: NEAR, MKR, ATOM show positive alignment while ENS, UNI, Bitcoin diverge most. Excluding Bitcoin confirms results are not driven by market dominance.
  We interpret findings as weak alignment between whitepaper narratives and market factor structure. Limited power (n=23) precludes distinguishing weak from no alignment, but strong alignment (phi>=0.70) can be confidently rejected. Implications for narrative economics and investment analysis are discussed.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [201] [Shrinkage Estimators for Mean and Covariance: Evidence on Portfolio Efficiency Across Market Dimensions](https://arxiv.org/abs/2601.20643)
*Rupendra Yadav,Amita Sharma,Aparna Mehra*

Main category: q-fin.PM

TL;DR: 研究评估不同收缩估计器下均值 - 方差（MV）和全局最小方差（GMV）模型的表现，发现GMV结合Ledoit Wolf两参数收缩协方差估计器（COV2）适合多数投资者，MV结合COV2和样本均值（SM）适合追求回报的投资者。


<details>
  <summary>Details</summary>
Motivation: 均值 - 方差模型核心参数存在估计误差问题，为解决此问题评估不同收缩估计器下模型表现。

Method: 研究5种预期回报收缩估计器和11种协方差矩阵收缩估计器，用超效率数据包络分析模型对投资组合排序，用滚动窗口法在6个真实数据集上进行3个样本外测试期的实证研究。

Result: 多数情况下，GMV结合COV2是多数投资者的最优选择，MV结合COV2和SM适合追求回报的投资者，这两个模型表现优于传统基准方法。

Conclusion: 本研究为理解特定收缩模型在不同投资者类型和市场环境中的表现奠定基础。

Abstract: The mean-variance model remains the most prevalent investment framework, built on diversification principles. However, it consistently struggles with estimation errors in expected returns and the covariance matrix, its core parameters. To address this concern, this research evaluates the performance of mean variance (MV) and global minimum-variance (GMV) models across various shrinkage estimators designed to improve these parameters. Specifically, we examine five shrinkage estimators for expected returns and eleven for the covariance matrix. To compare multiple portfolios, we employ a super efficient data envelopment analysis model to rank the portfolios according to investors risk-return preferences. Our comprehensive empirical investigation utilizes six real world datasets with different dimensional characteristics, applying a rolling window methodology across three out of sample testing periods. Following the ranking process, we examine the chosen shrinkage based MV or GMV portfolios against five traditional portfolio optimization techniques classical MV and GMV for sample estimates, MiniMax, conditional value at risk, and semi mean absolute deviation risk measures. Our empirical findings reveal that, in most scenarios, the GMV model combined with the Ledoit Wolf two parameter shrinkage covariance estimator (COV2) represents the optimal selection for a broad spectrum of investors. Meanwhile, the MV model utilizing COV2 alongside the sample mean (SM) proves more suitable for return oriented investors. These two identified models demonstrate superior performance compared to traditional benchmark approaches. Overall, this study lays the groundwork for a more comprehensive understanding of how specific shrinkage models perform across diverse investor profiles and market setups.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [202] [Deep Neural Networks as Iterated Function Systems and a Generalization Bound](https://arxiv.org/abs/2601.19958)
*Jonathan Vacher*

Main category: stat.ML

TL;DR: 本文利用随机迭代函数系统理论分析深度神经网络，建立不变测度存在唯一性，推导生成建模泛化界并提出新训练目标，通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络数学分析碎片化，稳定性和泛化性研究分散，架构递归应用参数化函数不稳定且训练难，缺乏生成场景泛化的严格结果。

Method: 利用随机迭代函数系统理论，将两种深度架构与位置相关IFS关联，引入随机动力系统结果。

Result: 建立了合适收缩假设下不变测度的存在唯一性，推导出生成建模的Wasserstein泛化界，提出新训练目标。

Conclusion: 理论在二维示例中得到验证，新训练目标在标准图像数据集上得到实证评估。

Abstract: Deep neural networks (DNNs) achieve remarkable performance on a wide range of tasks, yet their mathematical analysis remains fragmented: stability and generalization are typically studied in disparate frameworks and on a case-by-case basis. Architecturally, DNNs rely on the recursive application of parametrized functions, a mechanism that can be unstable and difficult to train, making stability a primary concern. Even when training succeeds, there are few rigorous results on how well such models generalize beyond the observed data, especially in the generative setting. In this work, we leverage the theory of stochastic Iterated Function Systems (IFS) and show that two important deep architectures can be viewed as, or canonically associated with, place-dependent IFS. This connection allows us to import results from random dynamical systems to (i) establish the existence and uniqueness of invariant measures under suitable contractivity assumptions, and (ii) derive a Wasserstein generalization bound for generative modeling. The bound naturally leads to a new training objective that directly controls the collage-type approximation error between the data distribution and its image under the learned transfer operator. We illustrate the theory on a controlled 2D example and empirically evaluate the proposed objective on standard image datasets (MNIST, CelebA, CIFAR-10).

</details>


### [203] [Incorporating data drift to perform survival analysis on credit risk](https://arxiv.org/abs/2601.20533)
*Jianwei Peng,Stefan Lessmann*

Main category: stat.ML

TL;DR: 研究数据漂移对基于生存分析的信用风险模型影响，提出动态联合建模框架，实验证明该模型在各漂移场景表现更优。


<details>
  <summary>Details</summary>
Motivation: 多数现有模型假定数据生成过程平稳，但实际抵押组合会受多种因素导致的数据漂移影响，需研究数据漂移影响并提升模型稳健性。

Method: 提出动态联合建模框架，整合基于余额动态的纵向行为标记与离散时间风险公式，结合地标独热编码和等渗校准。

Result: 在模拟三种数据漂移的抵押数据集上实验，地标联合模型在区分度和校准方面始终优于经典生存模型、基于树的漂移自适应学习器和梯度提升方法。

Conclusion: 模型设计具有优越性。

Abstract: Survival analysis has become a standard approach for modelling time to default by time-varying covariates in credit risk. Unlike most existing methods that implicitly assume a stationary data-generating process, in practise, mortgage portfolios are exposed to various forms of data drift caused by changing borrower behaviour, macroeconomic conditions, policy regimes and so on. This study investigates the impact of data drift on survival-based credit risk models and proposes a dynamic joint modelling framework to improve robustness under non-stationary environments. The proposed model integrates a longitudinal behavioural marker derived from balance dynamics with a discrete-time hazard formulation, combined with landmark one-hot encoding and isotonic calibration. Three types of data drift (sudden, incremental and recurring) are simulated and analysed on mortgage loan datasets from Freddie Mac. Experiments and corresponding evidence show that the proposed landmark-based joint model consistently outperforms classical survival models, tree-based drift-adaptive learners and gradient boosting methods in terms of discrimination and calibration across all drift scenarios, which confirms the superiority of our model design.

</details>


### [204] [Minimax Rates for Hyperbolic Hierarchical Learning](https://arxiv.org/abs/2601.20047)
*Divit Rawal,Sriram Vishwanath*

Main category: stat.ML

TL;DR: 在标准 Lipschitz 正则化下对分层数据学习，证明欧几里得和双曲表示在样本复杂度上存在指数分离，双曲表示达到信息论最优。


<details>
  <summary>Details</summary>
Motivation: 研究在分层数据学习中，欧几里得和双曲表示在样本复杂度方面的差异。

Method: 先分析欧几里得空间中边界半径嵌入导致的体积坍缩问题，再说明双曲空间中此类问题消失，并用 Fano 不等式给出下界。

Result: 欧几里得表示的 Lipschitz 常数需指数缩放，样本复杂度呈指数级；双曲表示能以 $O(mR \log m)$ 样本学习且达到信息论最优。

Conclusion: 双曲表示在分层数据学习的样本复杂度上优于欧几里得表示，且存在与几何无关的瓶颈。

Abstract: We prove an exponential separation in sample complexity between Euclidean and hyperbolic representations for learning on hierarchical data under standard Lipschitz regularization. For depth-$R$ hierarchies with branching factor $m$, we first establish a geometric obstruction for Euclidean space: any bounded-radius embedding forces volumetric collapse, mapping exponentially many tree-distant points to nearby locations. This necessitates Lipschitz constants scaling as $\exp(Ω(R))$ to realize even simple hierarchical targets, yielding exponential sample complexity under capacity control. We then show this obstruction vanishes in hyperbolic space: constant-distortion hyperbolic embeddings admit $O(1)$-Lipschitz realizability, enabling learning with $n = O(mR \log m)$ samples. A matching $Ω(mR \log m)$ lower bound via Fano's inequality establishes that hyperbolic representations achieve the information-theoretic optimum. We also show a geometry-independent bottleneck: any rank-$k$ prediction space captures only $O(k)$ canonical hierarchical contrasts.

</details>


### [205] [Efficient Evaluation of LLM Performance with Statistical Guarantees](https://arxiv.org/abs/2601.20251)
*Skyler Wu,Yash Nair,Emmanuel J. Candés*

Main category: stat.ML

TL;DR: 提出Factorized Active Querying (FAQ)方法对大语言模型基准测试进行推断，在开销可忽略不计情况下相比基线最多有5倍有效样本量增益，并发布代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 全面评估许多大语言模型在大量基准测试上代价高昂，希望在固定查询预算下获得模型准确性的置信区间。

Method: 提出Factorized Active Querying (FAQ)方法，利用贝叶斯因子模型利用历史信息、采用混合方差降低/主动学习采样策略选择问题，通过Proactive Active Inference保持有效性。

Result: 在两个基准测试套件上，忽略不计开销下，与强基线相比有效样本量最多有5倍增益，可在使用最多少5倍查询情况下达到均匀采样的置信区间宽度。

Conclusion: 提出的FAQ方法能在固定查询预算下有效进行大语言模型基准测试推断，代码和数据集可支持可重复性评估和未来研究。

Abstract: Exhaustively evaluating many large language models (LLMs) on a large suite of benchmarks is expensive. We cast benchmarking as finite-population inference and, under a fixed query budget, seek tight confidence intervals (CIs) for model accuracy with valid frequentist coverage. We propose Factorized Active Querying (FAQ), which (a) leverages historical information through a Bayesian factor model; (b) adaptively selects questions using a hybrid variance-reduction/active-learning sampling policy; and (c) maintains validity through Proactive Active Inference -- a finite-population extension of active inference (Zrnic & Candes, 2024) that enables direct question selection while preserving coverage. With negligible overhead cost, FAQ delivers up to $5\times$ effective sample size gains over strong baselines on two benchmark suites, across varying historical-data missingness levels: this means that it matches the CI width of uniform sampling while using up to $5\times$ fewer queries. We release our source code and our curated datasets to support reproducible evaluation and future research.

</details>


### [206] [Empirical Likelihood-Based Fairness Auditing: Distribution-Free Certification and Flagging](https://arxiv.org/abs/2601.20269)
*Jie Tang,Chuanlong Xie,Xianli Zeng,Lixing Zhu*

Main category: stat.ML

TL;DR: 提出基于经验似然的框架分析机器学习模型公平性，性能优于基于自助法的方法，且在数据集上成功识别交叉偏差


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在敏感子群体上存在性能差异和算法偏差问题，而现有审计技术受分布假设和计算开销限制

Method: 提出基于经验似然（EL）的框架，构建模型性能差异的稳健统计量，该方法非参数，使用约束优化轮廓

Result: EL方法优于基于自助法的方法，覆盖率更接近名义水平，计算延迟显著降低；在COMPAS数据集上成功识别交叉偏差

Conclusion: 所提的EL框架能有效解决机器学习模型的公平性审计问题，在实际应用中有较好的实用性

Abstract: Machine learning models in high-stakes applications, such as recidivism prediction and automated personnel selection, often exhibit systematic performance disparities across sensitive subpopulations, raising critical concerns regarding algorithmic bias. Fairness auditing addresses these risks through two primary functions: certification, which verifies adherence to fairness constraints; and flagging, which isolates specific demographic groups experiencing disparate treatment. However, existing auditing techniques are frequently limited by restrictive distributional assumptions or prohibitive computational overhead. We propose a novel empirical likelihood-based (EL) framework that constructs robust statistical measures for model performance disparities. Unlike traditional methods, our approach is non-parametric; the proposed disparity statistics follow asymptotically chi-square or mixed chi-square distributions, ensuring valid inference without assuming underlying data distributions. This framework uses a constrained optimization profile that admits stable numerical solutions, facilitating both large-scale certification and efficient subpopulation discovery. Empirically, the EL methods outperform bootstrap-based approaches, yielding coverage rates closer to nominal levels while reducing computational latency by several orders of magnitude. We demonstrate the practical utility of this framework on the COMPAS dataset, where it successfully flags intersectional biases, specifically identifying a significantly higher positive prediction rate for African-American males under 25 and a systemic under-prediction for Caucasian females relative to the population mean.

</details>


### [207] [VSCOUT: A Hybrid Variational Autoencoder Approach to Outlier Detection in High-Dimensional Retrospective Monitoring](https://arxiv.org/abs/2601.20830)
*Waldyn G. Martinez*

Main category: stat.ML

TL;DR: 提出VSCOUT框架用于高维回顾性监测，结合多种技术，实验显示优于传统方法，适合AI环境。


<details>
  <summary>Details</summary>
Motivation: 现代工业和服务过程数据挑战经典统计过程控制（SPC）基本假设，需新方法解决数据问题。

Method: VSCOUT结合自动相关性确定变分自编码器（ARD - VAE）、基于集成的潜在离群值过滤和变化点检测，有两阶段细化过程。

Result: 在基准数据集实验中，VSCOUT对特殊原因结构有高敏感性，控制误报，优于经典SPC程序、鲁棒估计器和现代机器学习基线。

Conclusion: VSCOUT具有可扩展性、分布灵活性和抗复杂污染模式能力，是AI环境回顾性建模和异常检测的实用有效方法。

Abstract: Modern industrial and service processes generate high-dimensional, non-Gaussian, and contamination-prone data that challenge the foundational assumptions of classical Statistical Process Control (SPC). Heavy tails, multimodality, nonlinear dependencies, and sparse special-cause observations can distort baseline estimation, mask true anomalies, and prevent reliable identification of an in-control (IC) reference set. To address these challenges, we introduce VSCOUT, a distribution-free framework designed specifically for retrospective (Phase I) monitoring in high-dimensional settings. VSCOUT combines an Automatic Relevance Determination Variational Autoencoder (ARD-VAE) architecture with ensemble-based latent outlier filtering and changepoint detection. The ARD prior isolates the most informative latent dimensions, while the ensemble and changepoint filters identify pointwise and structural contamination within the determined latent space. A second-stage retraining step removes flagged observations and re-estimates the latent structure using only the retained inliers, mitigating masking and stabilizing the IC latent manifold. This two-stage refinement produces a clean and reliable IC baseline suitable for subsequent Phase II deployment. Extensive experiments across benchmark datasets demonstrate that VSCOUT achieves superior sensitivity to special-cause structure while maintaining controlled false alarms, outperforming classical SPC procedures, robust estimators, and modern machine-learning baselines. Its scalability, distributional flexibility, and resilience to complex contamination patterns position VSCOUT as a practical and effective method for retrospective modeling and anomaly detection in AI-enabled environments.

</details>


### [208] [Physics-informed Blind Reconstruction of Dense Fields from Sparse Measurements using Neural Networks with a Differentiable Simulator](https://arxiv.org/abs/2601.20496)
*Ofek Aloni,Barak Fishbain*

Main category: stat.ML

TL;DR: 提出一种从稀疏测量生成密集物理场的重建方法，在流体力学标准问题上优于统计和神经网络方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成密集物理场的方法需空间统计或密集场示例，实际中可能不可用。

Method: 在训练阶段引入自动可微数值模拟器，无需空间统计和密集场示例。

Result: 在一组流体力学的三个标准问题上，结果优于统计和基于神经网络的方法。

Conclusion: 所提出的方法有效，解决了现有方法的局限。

Abstract: Generating dense physical fields from sparse measurements is a fundamental question in sampling, signal processing, and many other applications. State-of-the-art methods either use spatial statistics or rely on examples of dense fields in the training phase, which often are not available, and thus rely on synthetic data. Here, we present a reconstruction method that generates dense fields from sparse measurements, without assuming availability of the spatial statistics, nor of examples of the dense fields. This is made possible through the introduction of an automatically differentiable numerical simulator into the training phase of the method. The method is shown to have superior results over statistical and neural network based methods on a set of three standard problems from fluid mechanics.

</details>


### [209] [Sparse clustering via the Deterministic Information Bottleneck algorithm](https://arxiv.org/abs/2601.20628)
*Efthymios Costa,Ioanna Papatsouma,Angelos Markos*

Main category: stat.ML

TL;DR: 提出信息论框架用于稀疏数据聚类，经模拟和实际应用验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统聚类技术在处理特征空间子集的聚类结构时面临挑战，尤其是稀疏数据问题。

Method: 提出信息论框架，实现联合特征加权和聚类。

Result: 在合成数据模拟中表现为现有稀疏数据聚类算法的有力替代，在实际基因组数据集应用中证明有效。

Conclusion: 所提信息论框架可克服稀疏数据聚类问题，是有效的方法。

Abstract: Cluster analysis relates to the task of assigning objects into groups which ideally present some desirable characteristics. When a cluster structure is confined to a subset of the feature space, traditional clustering techniques face unprecedented challenges. We present an information-theoretic framework that overcomes the problems associated with sparse data, allowing for joint feature weighting and clustering. Our proposal constitutes a competitive alternative to existing clustering algorithms for sparse data, as demonstrated through simulations on synthetic data. The effectiveness of our method is established by an application on a real-world genomics data set.

</details>


### [210] [Demystifying Prediction Powered Inference](https://arxiv.org/abs/2601.20819)
*Yilin Song,Dan M. Kluger,Harsh Parikh,Tian Gu*

Main category: stat.ML

TL;DR: 本文对预测驱动推理（PPI）进行解读，构建统一实用工作流程，用数据展示其效果，指出问题并提供决策流程、方法总结表和诊断策略，助力研究人员合理应用。


<details>
  <summary>Details</summary>
Motivation: PPI虽能利用预测信息提高统计效率，但变体增多使从业者难以确定应用时机和方法，需要进行解读。

Method: 综合PPI理论基础、方法扩展、与现有统计文献联系和诊断工具构建统一工作流程，使用Mosaiks房价数据进行验证。

Result: PPI变体比完整案例分析有更窄置信区间；双重使用训练数据会导致反保守置信区间和覆盖率；在非随机缺失机制下所有方法都会产生有偏估计。

Conclusion: 将PPI视为通用方法而非单一估计量，能弥合方法创新与应用实践的差距，帮助研究人员将预测合理融入有效推理。

Abstract: Machine learning predictions are increasingly used to supplement incomplete or costly-to-measure outcomes in fields such as biomedical research, environmental science, and social science. However, treating predictions as ground truth introduces bias while ignoring them wastes valuable information. Prediction-Powered Inference (PPI) offers a principled framework that leverages predictions from large unlabeled datasets to improve statistical efficiency while maintaining valid inference through explicit bias correction using a smaller labeled subset. Despite its potential, the growing PPI variants and the subtle distinctions between them have made it challenging for practitioners to determine when and how to apply these methods responsibly. This paper demystifies PPI by synthesizing its theoretical foundations, methodological extensions, connections to existing statistics literature, and diagnostic tools into a unified practical workflow. Using the Mosaiks housing price data, we show that PPI variants produce tighter confidence intervals than complete-case analysis, but that double-dipping, i.e. reusing training data for inference, leads to anti-conservative confidence intervals and coverages. Under missing-not-at-random mechanisms, all methods, including classical inference using only labeled data, yield biased estimates. We provide a decision flowchart linking assumption violations to appropriate PPI variants, a summary table of selective methods, and practical diagnostic strategies for evaluating core assumptions. By framing PPI as a general recipe rather than a single estimator, this work bridges methodological innovation and applied practice, helping researchers responsibly integrate predictions into valid inference.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [211] [SunBURST: Deterministic GPU-Accelerated Bayesian Evidence via Mode-Centric Laplace Integration](https://arxiv.org/abs/2601.19957)
*Ira Wolfson*

Main category: stat.CO

TL;DR: 提出确定性GPU原生算法SunBURST用于贝叶斯证据计算，在高维高斯及近高斯后验分布有良好表现，证明高维高精度贝叶斯证据评估可实现计算上的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决高维下基于采样方法进行贝叶斯证据评估时因维度诅咒和采样顺序性导致的计算难题。

Method: 引入SunBURST算法，结合径向模式发现、批量L - BFGS细化和基于拉普拉斯的解析积分，将大量似然评估转化为大规模并行GPU工作负载。

Result: 对于高斯和近高斯后验分布，在高达1024维达到双精度容差的数值一致性；在多峰高斯混合中，保守配置下达到亚百分比精度。

Conclusion: 通过确定性积分与大规模并行计算结合，可使高维高精度贝叶斯证据评估在计算上变得可行。

Abstract: Bayesian evidence evaluation becomes computationally prohibitive in high dimensions due to the curse of dimensionality and the sequential nature of sampling-based methods. We introduce SunBURST, a deterministic GPU-native algorithm for Bayesian evidence calculation that replaces global volume exploration with mode-centric geometric integration. The pipeline combines radial mode discovery, batched L-BFGS refinement, and Laplace-based analytic integration, treating modes independently and converting large batches of likelihood evaluations into massively parallel GPU workloads.
  For Gaussian and near-Gaussian posteriors, where the Laplace approximation is exact or highly accurate, SunBURST achieves numerical agreement at double-precision tolerance in dimensions up to 1024 in our benchmarks, with sub-linear wall-clock scaling across the tested range. In multimodal Gaussian mixtures, conservative configurations yield sub-percent accuracy while maintaining favorable scaling.
  SunBURST is not intended as a universal replacement for sampling-based inference. Its design targets regimes common in physical parameter estimation and inverse problems, where posterior mass is locally well approximated by Gaussian structure around a finite number of modes. In strongly non-Gaussian settings, the method can serve as a fast geometry-aware evidence estimator or as a preprocessing stage for hybrid workflows. These results show that high-precision Bayesian evidence evaluation can be made computationally tractable in very high dimensions through deterministic integration combined with massive parallelism.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [212] [Matching and mixing: Matchability of graphs under Markovian error](https://arxiv.org/abs/2601.20020)
*Zhirui Li,Keith D. Levin,Zhiang Zhao,Vince Lyzinski*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of graph matching for a sequence of graphs generated under a time-dependent Markov chain noise model. Our edgelighter error model, a variant of the classical lamplighter random walk, iteratively corrupts the graph $G_0$ with edge-dependent noise, creating a sequence of noisy graph copies $(G_t)$. Much of the graph matching literature is focused on anonymization thresholds in edge-independent noise settings, and we establish novel anonymization thresholds in this edge-dependent noise setting when matching $G_0$ and $G_t$. Moreover, we also compare this anonymization threshold with the mixing properties of the Markov chain noise model. We show that when $G_0$ is drawn from an Erdős-Rényi model, the graph matching anonymization threshold and the mixing time of the edgelighter walk are both of order $Θ(n^2\log n)$. We further demonstrate that for more structured model for $G_0$ (e.g., the Stochastic Block Model), graph matching anonymization can occur in $O(n^α\log n)$ time for some $α<2$, indicating that anonymization can occur before the Markov chain noise model globally mixes. Through extensive simulations, we verify our theoretical bounds in the settings of Erdős-Rényi random graphs and stochastic block model random graphs, and explore our findings on real-world datasets derived from a Facebook friendship network and a European research institution email communication network.

</details>


### [213] [Improved Computational Lower Bound of Estimation for Multi-Frequency Group Synchronization](https://arxiv.org/abs/2601.20522)
*Zhangsong Li*

Main category: math.ST

TL;DR: 研究多频组同步问题的计算相变，用低次多项式算法分析信号估计，表明特定条件下简单谱方法计算最优，拓展前人工作并发现统计 - 计算差距。


<details>
  <summary>Details</summary>
Motivation: 研究多频组同步问题中结构化信号估计的计算相变，拓展前人仅适用于固定频率数的工作。

Method: 使用低次多项式算法框架分析观测中结构化信号的估计任务。

Result: 在圆群同步模型中，当频率数满足$L=n^{o(1)}$时，简单谱方法在所有多项式时间估计器中计算最优；在频率数足够大时，模型存在统计 - 计算差距。

Conclusion: 简单谱方法在特定条件下计算最优，拓展了前人工作，且模型存在统计 - 计算差距。

Abstract: We study the computational phase transition in a multi-frequency group synchronization problem, where pairwise relative measurements of group elements are observed across multiple frequency channels and corrupted by Gaussian noise. Using the framework of \emph{low-degree polynomial algorithms}, we analyze the task of estimating the structured signal in such observations. We show that, assuming the low-degree heuristic, in synchronization models over the circle group $\mathsf{SO}(2)$, a simple spectral method is computationally optimal among all polynomial-time estimators when the number of frequencies satisfies $L=n^{o(1)}$. This significantly extends prior work \cite{KBK24+}, which only applied to a fixed constant number of frequencies. Together with known upper bounds on the statistical threshold \cite{PWBM18a}, our results establish the existence of a \emph{statistical-to-computational gap} in this model when the number of frequencies is sufficiently large.

</details>


### [214] [Concentration Inequalities for Exchangeable Tensors and Matrix-valued Data](https://arxiv.org/abs/2601.20152)
*Chen Cheng,Rina Foygel Barber*

Main category: math.ST

TL;DR: 研究结构化随机数据加权和的集中不等式，给出相关界，应用于多因子响应模型和联邦平均中的固定设计草图方法，理论预测获数值验证。


<details>
  <summary>Details</summary>
Motivation: 研究结构化加权和在可交换性下的尾界和集中不等式，拓展经典独立项框架。

Method: 开发具有结构依赖可交换性的Hoeffding和Bernstein界。

Result: 恢复已知结果至最优常数，给出比Chatterjee方法更尖锐的界，理论预测获数值验证。

Conclusion: 所研究的结构为多因子响应模型估计和固定设计草图方法研究提供新分析工具。

Abstract: We study concentration inequalities for structured weighted sums of random data, including (i) tensor inner products and (ii) sequential matrix sums. We are interested in tail bounds and concentration inequalities for those structured weighted sums under exchangeability, extending beyond the classical framework of independent terms.
  We develop Hoeffding and Bernstein bounds provided with structure-dependent exchangeability. Along the way, we recover known results in weighted sum of exchangeable random variables and i.i.d. sums of random matrices to the optimal constants. Notably, we develop a sharper concentration bound for combinatorial sum of matrix arrays than the results previously derived from Chatterjee's method of exchangeable pairs.
  For applications, the richer structures provide us with novel analytical tools for estimating the average effect of multi-factor response models and studying fixed-design sketching methods in federated averaging. We apply our results to these problems, and find that our theoretical predictions are corroborated by numerical evidence.

</details>


### [215] [Spectral Bayesian Regression on the Sphere](https://arxiv.org/abs/2601.20528)
*Claudio Durastanti*

Main category: math.ST

TL;DR: 本文基于各向同性高斯场先验和拉普拉斯 - 贝尔特拉米算子诱导的调和结构，为单位球面上的非参数回归开发了贝叶斯框架，推导了后验分布等，建立了收敛率，并给出后验均值的变分表征。


<details>
  <summary>Details</summary>
Motivation: 为单位球面上的非参数回归建立合适的贝叶斯框架。

Method: 利用拉普拉斯 - 贝尔特拉米算子诱导的调和结构，在球面调和基下对回归模型进行精确对角化。

Result: 得到封闭形式的后验分布、最优谱截断方案和尖锐的后验收缩率，建立了多项式衰减角功率谱高斯先验下的后验收缩率，证明后验均值有精确的变分表征。

Conclusion: 所开发的贝叶斯框架有效，后验收缩率在正确先验校准下是极小极大最优的。

Abstract: We develop a fully intrinsic Bayesian framework for nonparametric regression on the unit sphere based on isotropic Gaussian field priors and the harmonic structure induced by the Laplace-Beltrami operator. Under uniform random design, the regression model admits an exact diagonalization in the spherical harmonic basis, yielding a Gaussian sequence representation with frequency-dependent multiplicities.
  Exploiting this structure, we derive closed-form posterior distributions, optimal spectral truncation schemes, and sharp posterior contraction rates under integrated squared loss. For Gaussian priors with polynomially decaying angular power spectra, including spherical Matérn priors, we establish posterior contraction rates over Sobolev classes, which are minimax-optimal under correct prior calibration.
  We further show that the posterior mean admits an exact variational characterization as a geometrically intrinsic penalized least-squares estimator, equivalent to a Laplace-Beltrami smoothing spline.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [216] [Bias-Reduced Estimation of Finite Mixtures: An Application to Latent Group Structures in Panel Data](https://arxiv.org/abs/2601.20197)
*Raphaël Langevin*

Main category: stat.ME

TL;DR: 论文指出有限混合参数密度的极大似然估计在有限样本下有偏差，提出最大化分类 - 混合似然函数的方法，经模拟和实证表明该方法优于标准 MLE。


<details>
  <summary>Details</summary>
Motivation: 解决有限混合参数密度的极大似然估计在有限样本下存在显著偏差的问题。

Method: 提出最大化分类 - 混合似然函数的方法，配备一致分类器；推导所得估计量的渐近分布并给出达到 oracle 效率的条件；进行蒙特卡罗模拟和实证应用。

Result: 蒙特卡罗模拟显示传统混合 MLE 有明显有限样本偏差，提出的估计策略在偏差和均方误差方面通常更优；实证应用表明该方法相对标准 MLE 程序减少约 17.6% 的样本外预测误差。

Conclusion: 提出的估计策略在有限样本下比标准 MLE 更优，能减少样本外预测误差。

Abstract: Finite mixture models are widely used in econometric analyses to capture unobserved heterogeneity. This paper shows that maximum likelihood estimation of finite mixtures of parametric densities can suffer from substantial finite-sample bias in all parameters under mild regularity conditions. The bias arises from the influence of outliers in component densities with unbounded or large support and increases with the degree of overlap among mixture components. I show that maximizing the classification-mixture likelihood function, equipped with a consistent classifier, yields parameter estimates that are less biased than those obtained by standard maximum likelihood estimation (MLE). I then derive the asymptotic distribution of the resulting estimator and provide conditions under which oracle efficiency is achieved. Monte Carlo simulations show that conventional mixture MLE exhibits pronounced finite-sample bias, which diminishes as the sample size or the statistical distance between component densities tends to infinity. The simulations further show that the proposed estimation strategy generally outperforms standard MLE in finite samples in terms of both bias and mean squared errors under relatively weak assumptions. An empirical application to latent group panel structures using health administrative data shows that the proposed approach reduces out-of-sample prediction error by approximately 17.6% relative to the best results obtained from standard MLE procedures.

</details>


### [217] [Exact Graph Learning via Integer Programming](https://arxiv.org/abs/2601.20589)
*Lucas Kook,Søren Wengel Mogensen*

Main category: stat.ME

TL;DR: 提出基于非参数条件独立性测试和整数规划的非参数图学习框架 GLIP，可高效精确恢复更大图，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图学习方法依赖限制性假设、采用贪婪算法或近似求解，对假设违背敏感或无法保证全局最优解。

Method: 将图学习问题重新表述为整数规划问题，利用图分离准则的有效编码，开发 R 包 'glip' 支持多种图学习。

Result: 能够精确恢复更大图，计算对应马尔可夫等价类或弱等价类，在多数实例和不同大小图上比现有精确图学习程序更快，在模拟和基准数据集上达最优性能。

Conclusion: 所提出的非参数图学习框架具有高效性和优越性。

Abstract: Learning the dependence structure among variables in complex systems is a central problem across medical, natural, and social sciences. These structures can be naturally represented by graphs, and the task of inferring such graphs from data is known as graph learning or as causal discovery if the graphs are given a causal interpretation. Existing approaches typically rely on restrictive assumptions about the data-generating process, employ greedy oracle algorithms, or solve approximate formulations of the graph learning problem. As a result, they are either sensitive to violations of central assumptions or fail to guarantee globally optimal solutions. We address these limitations by introducing a nonparametric graph learning framework based on nonparametric conditional independence testing and integer programming. We reformulate the graph learning problem as an integer-programming problem and prove that solving the integer-programming problem provides a globally optimal solution to the original graph learning problem. Our method leverages efficient encodings of graphical separation criteria, enabling the exact recovery of larger graphs than was previously feasible. We provide an implementation in the openly available R package 'glip' which supports learning (acyclic) directed (mixed) graphs and chain graphs. From the resulting output one can compute representations of the corresponding Markov equivalence classes or weak equivalence classes. Empirically, we demonstrate that our approach is faster than other existing exact graph learning procedures for a large fraction of instances and graphs of various sizes. GLIP also achieves state-of-the-art performance on simulated data and benchmark datasets across all aforementioned classes of graphs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [218] [Taming Toxic Talk: Using chatbots to intervene with users posting toxic comments](https://arxiv.org/abs/2601.20100)
*Jeremy Foote,Deepak Kumar,Bedadyuti Jha,Ryan Funkhouser,Loizos Bitsikokos,Hitesh Goel,Hsuen-Chi Chiu*

Main category: cs.HC

TL;DR: 本文探讨生成式AI聊天机器人与分享有毒内容用户的康复对话影响，通过大规模实验发现参与者有积极交流但后续有毒行为无显著变化。


<details>
  <summary>Details</summary>
Motivation: 明确生成式AI聊天机器人说服效果的实际影响，尝试用康复性方法处理网络有毒行为。

Method: 与七个Reddit社区合作，邀请发布有毒内容者与AI聊天机器人对话，并进行定性分析。

Result: 很多参与者真诚交流，表达悔意或改变意愿，但与对照组相比，后续一个月有毒行为无显著变化。

Conclusion: 讨论了研究结果的可能解释及理论和实践意义。

Abstract: Generative AI chatbots have proven surprisingly effective at persuading people to change their beliefs and attitudes in lab settings. However, the practical implications of these findings are not yet clear. In this work, we explore the impact of rehabilitative conversations with generative AI chatbots on users who share toxic content online. Toxic behaviors -- like insults or threats of violence, are widespread in online communities. Strategies to deal with toxic behavior are typically punitive, such as removing content or banning users. Rehabilitative approaches are rarely attempted, in part due to the emotional and psychological cost of engaging with aggressive users. In collaboration with seven large Reddit communities, we conducted a large-scale field experiment (N=893) to invite people who had recently posted toxic content to participate in conversations with AI chatbots. A qualitative analysis of the conversations shows that many participants engaged in good faith and even expressed remorse or a desire to change. However, we did not observe a significant change in toxic behavior in the following month compared to a control group. We discuss possible explanations for our findings, as well as theoretical and practical implications based on our results.

</details>


### [219] [DiagLink: A Dual-User Diagnostic Assistance System by Synergizing Experts with LLMs and Knowledge Graphs](https://arxiv.org/abs/2601.20311)
*Zihan Zhou,Yinan Liu,Yuyang Xie,Bin Wang,Xiaochun Yang,Zezheng Feng*

Main category: cs.HC

TL;DR: 提出DiagLink双用户诊断辅助系统，结合LLMs、KGs和医学专家，经评估能提升用户满意度和诊断效率。


<details>
  <summary>Details</summary>
Motivation: 全球医疗专业知识短缺和分布不均阻碍公平诊断，现有智能诊断系统在双用户交互和动态知识集成方面存在不足。

Method: 使用引导对话获取患者病史，利用LLMs和KGs进行协同推理，引入医生监督进行知识验证和进化，提供角色自适应界面等。

Result: 通过用户研究、用例和专家访谈，证明能提升用户满意度和诊断效率。

Conclusion: DiagLink有效，为未来AI辅助诊断系统设计提供见解。

Abstract: The global shortage and uneven distribution of medical expertise continue to hinder equitable access to accurate diagnostic care. While existing intelligent diagnostic system have shown promise, most struggle with dual-user interaction, and dynamic knowledge integration -- limiting their real-world applicability. In this study, we present DiagLink, a dual-user diagnostic assistance system that synergizes large language models (LLMs), knowledge graphs (KGs), and medical experts to support both patients and physicians. DiagLink uses guided dialogues to elicit patient histories, leverages LLMs and KGs for collaborative reasoning, and incorporates physician oversight for continuous knowledge validation and evolution. The system provides a role-adaptive interface, dynamically visualized history, and unified multi-source evidence to improve both trust and usability. We evaluate DiagLink through user study, use cases and expert interviews, demonstrating its effectiveness in improving user satisfaction and diagnostic efficiency, while offering insights for the design of future AI-assisted diagnostic systems.

</details>


### [220] [GuideAI: A Real-time Personalized Learning Solution with Adaptive Interventions](https://arxiv.org/abs/2601.20402)
*Ananya Shukla,Chaitanya Modi,Satvik Bajpai,Siddharth Siddharth*

Main category: cs.HC

TL;DR: 介绍GuideAI多模态框架增强大语言模型驱动学习，初步研究显示其能提升知识保留和降低认知负荷，有潜力满足个性化学习需求。


<details>
  <summary>Details</summary>
Motivation: 大语言模型缺乏对学习者认知和生理状态的感知，现有学习技术无法应对实时学习挑战，需开发能适应学习者个体需求的系统。

Method: 基于用户研究构建GuideAI框架，集成实时生物传感反馈，通过认知优化、生理干预和注意力策略动态调整学习内容和节奏，支持多种学习模式；开展初步研究进行标准化评估。

Result: 在问题解决能力和基于回忆的知识评估上有显著改善，参与者在NASA - TLX关键指标上有明显降低，感知性能提升。

Conclusion: GuideAI有潜力弥合当前基于大语言模型的学习系统与个性化学习需求之间的差距，为大规模自适应、认知感知教育奠定基础。

Abstract: Large Language Models (LLMs) have emerged as powerful learning tools, but they lack awareness of learners' cognitive and physiological states, limiting their adaptability to the user's learning style. Contemporary learning techniques primarily focus on structured learning paths, knowledge tracing, and generic adaptive testing but fail to address real-time learning challenges driven by cognitive load, attention fluctuations, and engagement levels. Building on findings from a formative user study (N=66), we introduce GuideAI, a multi-modal framework that enhances LLM-driven learning by integrating real-time biosensory feedback including eye gaze tracking, heart rate variability, posture detection, and digital note-taking behavior. GuideAI dynamically adapts learning content and pacing through cognitive optimizations (adjusting complexity based on learning progress markers), physiological interventions (breathing guidance and posture correction), and attention-aware strategies (redirecting focus using gaze analysis). Additionally, GuideAI supports diverse learning modalities, including text-based, image-based, audio-based, and video-based instruction, across varied knowledge domains. A preliminary study (N = 25) assessed GuideAI's impact on knowledge retention and cognitive load through standardized assessments. The results show statistically significant improvements in both problem-solving capability and recall-based knowledge assessments. Participants also experienced notable reductions in key NASA-TLX measures including mental demand, frustration levels, and effort, while simultaneously reporting enhanced perceived performance. These findings demonstrate GuideAI's potential to bridge the gap between current LLM-based learning systems and individualized learner needs, paving the way for adaptive, cognition-aware education at scale.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [221] [TGSBM: Transformer-Guided Stochastic Block Model for Link Prediction](https://arxiv.org/abs/2601.20646)
*Zhejian Yang,Songwei Zhao,Zilin Zhao,Hechang Chen*

Main category: cs.SI

TL;DR: 现有大规模网络链路预测方法有局限，提出TGSBM框架，实验显示其在性能、可扩展性和可解释性上表现出色，是大规模链路预测实用方法。


<details>
  <summary>Details</summary>
Motivation: 大规模网络有独特挑战，现有链路预测方法存在局限，如传统图神经网络无法捕捉全局依赖，图变换器复杂度高且缺乏可解释潜在结构。

Method: 提出TGSBM框架，集成重叠随机块模型和稀疏图变换器，包含扩展器增强稀疏注意力、神经变分编码器和神经边解码器三个组件。

Result: 在多个基准测试中表现出有竞争力的性能（HeaRT协议下平均排名1.6），具有优越可扩展性（训练速度快达6倍），能得到可解释的社区结构。

Conclusion: TGSBM是一种能在大规模链路预测中平衡准确性、效率和透明度的实用方法。

Abstract: Link prediction is a cornerstone of the Web ecosystem, powering applications from recommendation and search to knowledge graph completion and collaboration forecasting. However, large-scale networks present unique challenges: they contain hundreds of thousands of nodes and edges with heterogeneous and overlapping community structures that evolve over time. Existing approaches face notable limitations: traditional graph neural networks struggle to capture global structural dependencies, while recent graph transformers achieve strong performance but incur quadratic complexity and lack interpretable latent structure. We propose \textbf{TGSBM} (Transformer-Guided Stochastic Block Model), a framework that integrates the principled generative structure of Overlapping Stochastic Block Models with the representational power of sparse Graph Transformers. TGSBM comprises three main components: (i) \emph{expander-augmented sparse attention} that enables near-linear complexity and efficient global mixing, (ii) a \emph{neural variational encoder} that infers structured posteriors over community memberships and strengths, and (iii) a \emph{neural edge decoder} that reconstructs links via OSBM's generative process, preserving interpretability. Experiments across diverse benchmarks demonstrate competitive performance (mean rank 1.6 under HeaRT protocol), superior scalability (up to $6\times$ faster training), and interpretable community structures. These results position TGSBM as a practical approach that strikes a balance between accuracy, efficiency, and transparency for large-scale link prediction.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [222] [VoxPrivacy: A Benchmark for Evaluating Interactional Privacy of Speech Language Models](https://arxiv.org/abs/2601.19956)
*Yuxiang Wang,Hongyu Liu,Dekun Chen,Xueyao Zhang,Zhizheng Wu*

Main category: eess.AS

TL;DR: 提出VoxPrivacy基准评估SLM交互隐私，发现多数模型有漏洞，微调可提升隐私保护能力并发布相关资源。


<details>
  <summary>Details</summary>
Motivation: SLM进入多用户环境需区分用户以管理信息流，现有基准存在不足，需评估交互隐私。

Method: 引入VoxPrivacy基准，分三个难度层级，在32小时双语数据集和真实语音子集上评估，用新训练集微调模型。

Result: 多数开源模型在条件隐私决策上接近随机水平，闭源系统在主动隐私推理上不足，真实语音中仍有问题，微调可提升隐私保护能力。

Conclusion: 发布VoxPrivacy基准、训练集和微调模型，推动更安全、有上下文感知的SLM发展。

Abstract: As Speech Language Models (SLMs) transition from personal devices to shared, multi-user environments such as smart homes, a new challenge emerges: the model is expected to distinguish between users to manage information flow appropriately. Without this capability, an SLM could reveal one user's confidential schedule to another, a privacy failure we term interactional privacy. Thus, the ability to generate speaker-aware responses becomes essential for SLM safe deployment. Current SLM benchmarks test dialogue ability but overlook speaker identity. Multi-speaker benchmarks check who said what without assessing whether SLMs adapt their responses. Privacy benchmarks focus on globally sensitive data (e.g., bank passwords) while neglecting contextual privacy-sensitive information (e.g., a user's private appointment). To address this gap, we introduce VoxPrivacy, the first benchmark designed to evaluate interactional privacy in SLMs. VoxPrivacy spans three tiers of increasing difficulty, from following direct secrecy commands to proactively protecting privacy. Our evaluation of nine SLMs on a 32-hour bilingual dataset reveals a widespread vulnerability: most open-source models perform close to random chance (around 50% accuracy) on conditional privacy decisions, while even strong closed-source systems fall short on proactive privacy inference. We further validate these findings on Real-VoxPrivacy, a human-recorded subset, confirming that failures observed on synthetic data persist in real speech. Finally, we demonstrate a viable path forward: by fine-tuning on a new 4,000-hour training set, we improve privacy-preserving abilities while maintaining robustness. To support future work, we release the VoxPrivacy benchmark, the large-scale training set, and the fine-tuned model to foster the development of safer and more context-aware SLMs.

</details>


### [223] [Do we really need Self-Attention for Streaming Automatic Speech Recognition?](https://arxiv.org/abs/2601.19960)
*Youness Dkhissi,Valentin Vielzeuf,Elys Allesiardo,Anthony Larcher*

Main category: eess.AS

TL;DR: 质疑Transformer架构在受限环境适用性，提出用可变形卷积降低流式自动语音识别计算成本，去除自注意力机制不影响字错率。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在深度学习领域广泛使用，但未考虑受限任务适用性，需评估其在特定约束下相关性。

Method: 以流式自动语音识别为例，用可变形卷积替代自注意力机制，并尝试去除自注意力机制。

Result: 用可变形卷积降低了流式自动语音识别计算成本，去除自注意力机制未显著影响字错率。

Conclusion: Transformer架构高计算需求和延迟问题不适合流式应用，应寻找替代策略提升效率。

Abstract: Transformer-based architectures are the most used architectures in many deep learning fields like Natural Language Processing, Computer Vision or Speech processing. It may encourage the direct use of Transformers in the constrained tasks, without questioning whether it will yield the same benefits as in standard tasks.  Given specific constraints, it is essential to evaluate the relevance of transformer models. This work questions the suitability of transformers for specific domains. We argue that the high computational requirements and latency issues associated with these models do not align well with streaming applications. Our study promotes the search for alternative strategies to improve efficiency without sacrificing performance.  In light of this observation, our paper critically examines the usefulness of transformer architecture in such constrained environments. As a first attempt, we show that the computational cost for Streaming Automatic Speech Recognition (ASR) can be reduced using deformable convolution instead of Self-Attention. Furthermore, we show that Self-Attention mechanisms can be entirely removed and not replaced, without observing significant degradation in the Word Error Rate.

</details>


### [224] [MK-SGC-SC: Multiple Kernel guided Sparse Graph Construction in Spectral Clustering for Unsupervised Speaker Diarization](https://arxiv.org/abs/2601.19946)
*Nikhil Raghav,Avisek Gupta,Swagatam Das,Md Sahidullah*

Main category: eess.AS

TL;DR: 本文提出通过测量说话人嵌入的多内核相似度构建稀疏图进行谱聚类，在无监督说话人分割任务中取得了先进性能。


<details>
  <summary>Details</summary>
Motivation: 无监督说话人分割无需预训练或弱监督来识别说话人区域，促使研究聚类技术。

Method: 考虑四个多项式核和一个一阶反余弦核来测量说话人嵌入的相似度，以原则性方式构建稀疏图强调局部相似性。

Result: 所提方法在DIHARD - III、AMI和VoxConverse语料库的各种具有挑战性的环境中，在无监督说话人分割方面表现出色。

Conclusion: 测量多内核相似度构建稀疏图用于谱聚类足以在完全无监督设置下实现先进性能，代码已开源以鼓励进一步研究。

Abstract: Speaker diarization aims to segment audio recordings into regions corresponding to individual speakers. Although unsupervised speaker diarization is inherently challenging, the prospect of identifying speaker regions without pretraining or weak supervision motivates research on clustering techniques. In this work, we share the notable observation that measuring multiple kernel similarities of speaker embeddings to thereafter craft a sparse graph for spectral clustering in a principled manner is sufficient to achieve state-of-the-art performances in a fully unsupervised setting. Specifically, we consider four polynomial kernels and a degree one arccosine kernel to measure similarities in speaker embeddings, using which sparse graphs are constructed in a principled manner to emphasize local similarities. Experiments show the proposed approach excels in unsupervised speaker diarization over a variety of challenging environments in the DIHARD-III, AMI, and VoxConverse corpora. To encourage further research, our implementations are available at https://github.com/nikhilraghav29/MK-SGC-SC.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [225] [Hardware-Aware Model Design and Training of Silicon-based Analog Neural Networks](https://arxiv.org/abs/2601.19905)
*Giulio Filippeschi,Mirko Brazzini,Cristhopher Mosquera,Marco Lanuzza,Alessandro Catania,Sebastiano Strangio,Giuseppe Iannaccone*

Main category: cs.AR

TL;DR: 通过物理感知的硬件感知模型重新训练硅基模拟神经网络，可在有非理想性情况下恢复推理精度，还给出从设计到部署的完整流程。


<details>
  <summary>Details</summary>
Motivation: 在硅基模拟神经网络存在显著非理想性时，避免通过大量校准和保守模拟设计提高保真度带来的高成本，寻求更具扩展性和集成密度的方法。

Method: 提出物理感知的硬件感知模型，将操作离散到自适应时隙，并行处理激活模式，用16x16硅阵列测量校准模型并改进权重提取程序。

Result: 表明串扰与布局有关且常占主导，改进的权重提取程序使信噪比翻倍，在三种架构下恢复了理想软件网络的准确性。

Conclusion: 建立了时域模拟神经形态芯片从设计到部署的完整工作流程。

Abstract: Silicon-based analog neural networks physically embody the ideal neural network model in an approximate way. We show that by retraining the neural network using a physics-informed hardware-aware model one can fully recover the inference accuracy of the ideal network model even in the presence of significant non-idealities. This is way more promising for scalability and integration density than the default option of improving the fidelity of the analog neural network at the cost of significant energy, area, and design overhead, through extensive calibration and conservative analog design.
  We first present a physics-informed hardware-aware model for a time-domain vector-matrix multiplier implemented with single-transistor floating-gate memory cells that explicitly accounts for two dominant non-idealities of the physical implementation - capacitive crosstalk and bit-line voltage drop - and integrates seamlessly with modern deep-learning workflows. The model discretizes each operation into adaptive time slots, processes activation patterns in parallel, and accumulates their contributions to predict effective multiplier outputs. Using measurements from a 16x16 silicon array, we calibrate the model, show that crosstalk is layout-dependent and often dominant, and introduce an improved weight-extraction procedure that doubles signal-to-error ratio versus an ideal vector-matrix multiplier model. Finally, we show that by training silicon-based analog neural networks using an hardware-aware model in the forward pass we can recover the accuracy of the ideal software networks across three architectures -- custom MLP on low-resolution MNIST, LeNet-5 on MNIST, and a VGG-style CNN on CIFAR-10 - establishing a complete design-to-deployment workflow for time-domain analog neuromorphic chips.

</details>


### [226] [GPU-Augmented OLAP Execution Engine: GPU Offloading](https://arxiv.org/abs/2601.19911)
*Ilsun Chang*

Main category: cs.AR

TL;DR: 现代OLAP系统执行层CPU成本成新瓶颈，本文提出混合架构，选择性将高影响原语卸载到GPU，用关键数据传输与延迟物化，引入Risky Gate，实验显示门控卸载能改善尾延迟。


<details>
  <summary>Details</summary>
Motivation: 现代OLAP系统在执行层的CPU成本成为新瓶颈，需要解决此问题。

Method: 提出混合架构，选择性将高影响原语卸载到GPU；使用关键数据传输与延迟物化；引入Risky Gate根据输入大小、传输、内核和后处理成本以及候选集复杂度触发卸载。

Result: 用PostgreSQL微基准测试和GPU代理测量，发现门控卸载比始终开启GPU卸载能改善尾延迟（P95/P99）。

Conclusion: 将用于优化器阶段GPU辅助测量的风险感知门控原则扩展到执行层OLAP原语。

Abstract: Modern OLAP systems have mitigated I/O bottlenecks via storage-compute separation and columnar layouts, but CPU costs in the execution layer (especially Top-K selection and join probe) are emerging as new bottlenecks at scale. This paper proposes a hybrid architecture that augments existing vectorized execution by selectively offloading only high-impact primitives to the GPU. To reduce data movement, we use key-only transfer (keys and pointers) with late materialization. We further introduce a Risky Gate (risk-aware gating) that triggers offloading only in gain/risk intervals based on input size, transfer, kernel and post-processing costs, and candidate-set complexity (K, M). Using PostgreSQL microbenchmarks and GPU proxy measurements, we observe improved tail latency (P95/P99) under gated offloading compared to always-on GPU offloading. This work extends the risk-aware gating principle used for optimizer-stage GPU-assisted measurement (arXiv:2512.19750) to execution-layer OLAP primitives.

</details>


### [227] [DABench-LLM: Standardized and In-Depth Benchmarking of Post-Moore Dataflow AI Accelerators for LLMs](https://arxiv.org/abs/2601.19904)
*Ziyu Hu,Zhiqing Zhong,Weijian Zheng,Zhijing Ye,Xuwei Tan,Xueru Zhang,Zheng Xie,Rajkumar Kettimuthu,Xiaodong Yu*

Main category: cs.AR

TL;DR: 因摩尔定律放缓，传统架构难适应大语言模型增长，本文提出首个评估框架DABench - LLM，在三种加速器上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展快，传统架构能力不足，且缺乏针对数据流AI加速器的性能分析和标准化基准测试方法。

Method: 引入DABench - LLM框架，结合片内性能分析和片间可扩展性分析进行全面评估。

Result: 在三种商用数据流加速器上验证框架，揭示性能瓶颈并给出优化策略。

Conclusion: DABench - LLM框架在多种数据流AI硬件平台上具有通用性和有效性。

Abstract: The exponential growth of large language models has outpaced the capabilities of traditional CPU and GPU architectures due to the slowdown of Moore's Law. Dataflow AI accelerators present a promising alternative; however, there remains a lack of in-depth performance analysis and standardized benchmarking methodologies for LLM training. We introduce DABench-LLM, the first benchmarking framework designed for evaluating LLM workloads on dataflow-based accelerators. By combining intra-chip performance profiling and inter-chip scalability analysis, DABench-LLM enables comprehensive evaluation across key metrics such as resource allocation, load balance, and resource efficiency. The framework helps researchers rapidly gain insights into underlying hardware and system behaviors, and provides guidance for performance optimizations. We validate DABench-LLM on three commodity dataflow accelerators, Cerebras WSE-2, SambaNova RDU, and Graphcore IPU. Our framework reveals performance bottlenecks and provides specific optimization strategies, demonstrating its generality and effectiveness across a diverse range of dataflow-based AI hardware platforms.

</details>


### [228] [RAPID-Graph: Recursive All-Pairs Shortest Paths Using Processing-in-Memory for Dynamic Programming on Graphs](https://arxiv.org/abs/2601.19907)
*Yanru Chen,Zheyu Li,Keming Fan,Runyang Tian,John Hsu,Weihong Xu,Minxuan Zhou,Tajana Rosing*

Main category: cs.AR

TL;DR: 提出RAPID - Graph应对大规模图分析中全对最短路径（APSP）计算挑战，集成算法、架构和设备级优化，性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 大规模图分析中APSP数据移动复杂度高，传统内存层次带宽不足，成为主要瓶颈。

Method: 设计递归感知分区器，在算法层面减少数据依赖；在架构和设备层面设计2.5D PIM堆栈，实现瓦片级和单元级并行处理。

Result: 在2.45M节点的OGBN - Products数据集上，比最先进GPU集群快5.8倍、能效高1186倍，比之前PIM加速器速度快8.3倍、效率高104倍，比NVIDIA H100 GPU最高提速42.8倍、节能392倍。

Conclusion: RAPID - Graph在解决APSP计算问题上表现出色，具有显著的速度和能效优势。

Abstract: All-pairs shortest paths (APSP) remains a major bottleneck for large-scale graph analytics, as data movement with cubic complexity overwhelms the bandwidth of conventional memory hierarchies. In this work, we propose RAPID-Graph to address this challenge through a co-designed processing-in-memory (PIM) system that integrates algorithm, architecture, and device-level optimizations. At the algorithm level, we introduce a recursion-aware partitioner that enables an exact APSP computation by decomposing graphs into vertex tiles to reduce data dependency, such that both Floyd-Warshall and Min-Plus kernels execute fully in-place within digital PIM arrays. At the architecture and device levels, we design a 2.5D PIM stack integrating two phase-change memory compute dies, a logic die, and high-bandwidth scratchpad memory within a unified advanced package. An external non-volatile storage stack stores large APSP results persistently. The design achieves both tile-level and unit-level parallel processing to sustain high throughput. On the 2.45M-node OGBN-Products dataset, RAPID-Graph is 5.8x faster and 1,186x more energy efficient than state-of-the-art GPU clusters, while exceeding prior PIM accelerators by 8.3x in speed and 104x in efficiency. It further delivers up to 42.8x speedup and 392x energy savings over an NVIDIA H100 GPU.

</details>


### [229] [Understanding Bottlenecks for Efficiently Serving LLM Inference With KV Offloading](https://arxiv.org/abs/2601.19910)
*William Meng,Benjamin Lee,Hong Wang*

Main category: cs.AR

TL;DR: 本文聚焦KV缓存卸载中PCIe带宽限制瓶颈，推导临界值并实证分析，提出多方面优化建议。


<details>
  <summary>Details</summary>
Motivation: KV缓存卸载中PCIe带宽限制瓶颈严重，GPU利用率低，需优化。

Method: 开发分析框架推导临界缓存与预填充令牌比率，进行实证分析。

Result: 典型工作负载远超临界阈值，99%延迟用于传输，GPU仅利用28%额定热设计功耗。

Conclusion: 需对硬件互连、模型架构和调度算法进行优化。

Abstract: KV cache offloading enables long-context LLM inference by storing caches in CPU DRAM, but PCIe bandwidth limitations create severe bottlenecks. In this paper, we develops an analytical framework that derives $κ_{\text{crit}}$, the critical cached-to-prefill token ratio where execution becomes memory-bound and show typical workloads exceed this threshold by orders of magnitude. Empirical characterization reveals 99\% of latency spent on transfers and serving offloaded requests results in GPU's consuming only 28\% of their rated TDP, motivating our proposed optimizations for hardware interconnects, model architectures, and scheduling algorithms.

</details>


### [230] [Beyond GEMM-Centric NPUs: Enabling Efficient Diffusion LLM Sampling](https://arxiv.org/abs/2601.20706)
*Binglei Lou,Haoran Wu,Yao Lai,Jiayi Nie,Can Xiao,Xuan Guo,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: 文章指出扩散大语言模型采样阶段占推理延迟大的问题，提出NPU架构优化方法实现加速并开源验证代码。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）采样阶段与传统transformer层特性不同，采样占推理延迟高，且常规NPUs难以高效处理。

Method: 识别NPU架构需针对dLLM采样优化的关键指令，采用轻量级非GEMM向量原语、就地内存复用策略和分离的混合精度内存层次。

Result: 在同等nm技术节点下，相比NVIDIA RTX A6000 GPU有最高2.53倍的加速。

Conclusion: 所提出的优化方法可有效加速dLLM采样，开源代码验证与当前dLLM PyTorch实现功能等效。

Abstract: Diffusion Large Language Models (dLLMs) introduce iterative denoising to enable parallel token generation, but their sampling phase displays fundamentally different characteristics compared to GEMM-centric transformer layers. Profiling on modern GPUs reveals that sampling can account for up to 70% of total model inference latency-primarily due to substantial memory loads and writes from vocabulary-wide logits, reduction-based token selection, and iterative masked updates. These processes demand large on-chip SRAM and involve irregular memory accesses that conventional NPUs struggle to handle efficiently. To address this, we identify a set of critical instructions that an NPU architecture must specifically optimize for dLLM sampling. Our design employs lightweight non-GEMM vector primitives, in-place memory reuse strategies, and a decoupled mixed-precision memory hierarchy. Together, these optimizations deliver up to a 2.53x speedup over the NVIDIA RTX A6000 GPU under an equivalent nm technology node. We also open-source our cycle-accurate simulation and post-synthesis RTL verification code, confirming functional equivalence with current dLLM PyTorch implementations.

</details>


### [231] [Bench4HLS: End-to-End Evaluation of LLMs in High-Level Synthesis Code Generation](https://arxiv.org/abs/2601.19941)
*M Zafir Sadik Khan,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: 过去两年大语言模型在代码生成包括RTL硬件设计展现能力，HLS应用尚不成熟但受关注，提出Bench4HLS评估LLM生成的HLS设计。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在HLS应用受关注，缺乏全面的基准测试和评估框架。

Method: 构建包含170个案例的Bench4HLS，支持自动化评估，集成PPA分析API。

Result: 框架支持编译成功、功能正确性、综合可行性/优化评估，集成API并在Xilinx Vitis HLS演示、Catapult HLS验证。

Conclusion: Bench4HLS为HLS工作流中LLM基准测试提供基础方法。

Abstract: In last two years, large language models (LLMs) have shown strong capabilities in code generation, including hardware design at register-transfer level (RTL). While their use in high-level synthesis (HLS) remains comparatively less mature, the ratio of HLS- to RTL-focused studies has shifted from 1:10 to 2:10 in the past six months, indicating growing interest in leveraging LLMs for high-level design entry while relying on downstream synthesis for optimization. This growing trend highlights the need for a comprehensive benchmarking and evaluation framework dedicated to LLM-based HLS. To address this, We present Bench4HLS for evaluating LLM-generated HLS designs. Bench4HLS comprises 170 manually drafted and validated case studies, spanning small kernels to complex accelerators, curated from widely used public repositories. The framework supports fully automated assessment of compilation success, functional correctness via simulation, and synthesis feasibility/optimization. Crucially, Bench4HLS integrates a pluggable API for power, performance, and area (PPA) analysis across various HLS toolchains and architectures, demonstrated here with Xilinx Vitis HLS and validated on Catapult HLS. By providing a structured, extensible, and plug-and-play testbed, Bench4HLS establishes a foundational methodology for benchmarking LLMs in HLS workflows.

</details>


### [232] [STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification](https://arxiv.org/abs/2601.19903)
*Saeid Rajabi,Chengmo Yang,Satwik Patnaik*

Main category: cs.AR

TL;DR: 本文提出STELLAR框架，利用结构相似性指导基于大语言模型的SVA生成，实验表明其在多方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 形式验证依赖高质量SVAs，但手动编写慢且易出错，现有基于大语言模型的方法存在不足。

Method: 将RTL块表示为AST结构指纹，从知识库中检索结构相关的(RTL, SVA)对并集成到结构引导提示中。

Result: STELLAR在语法正确性、风格一致性和功能正确性方面表现优越。

Conclusion: 基于结构感知的检索是工业形式验证的一个有前景的方向。

Abstract: Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.

</details>


### [233] [GTAC: A Generative Transformer for Approximate Circuits](https://arxiv.org/abs/2601.19906)
*Jingxin Wang,Shitong Guo,Ruicheng Dai,Wenhui Liang,Ruogu Ding,Xin Ning,Weikang Qian*

Main category: cs.AR

TL;DR: 本文提出用于生成近似电路的GTAC模型，实验表明其在误差率约束下可减少面积且速度更快。


<details>
  <summary>Details</summary>
Motivation: 针对容错应用，提升电路性能、功耗和面积（PPA）。

Method: 引入基于生成式Transformer的GTAC模型，将误差阈值集成到设计过程。

Result: 与现有方法相比，GTAC在误差率约束下进一步减少6.4%面积，速度快4.3倍。

Conclusion: GTAC模型能有效提升电路的PPA性能。

Abstract: Targeting error-tolerant applications, approximate circuits introduce controlled errors to significantly improve performance, power, and area (PPA) of circuits. In this work, we introduce GTAC, a novel generative Transformer-based model for producing approximate circuits. By leveraging principles of approximate computing and AI-driven EDA, our model innovatively integrates error thresholds into the design process. Experimental results show that compared with a state-of-the-art method, GTAC further reduces 6.4% area under the error rate constraint, while being 4.3x faster.

</details>


### [234] [Analysis of LLM Vulnerability to GPU Soft Errors: An Instruction-Level Fault Injection Study](https://arxiv.org/abs/2601.19912)
*Duo Chai,Zizhen Liu,Shuhuai Wang,Songwei Pei,Cheng Liu,Huawei Li,Shangguang Wang*

Main category: cs.AR

TL;DR: 本文对大语言模型推理进行指令级故障注入研究，揭示其可靠性特征，为容错机制设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算和内存需求高，GPU 易出现软错误，但对其可靠性的系统分析有限，需填补这一空白。

Method: 进行首次大语言模型推理的指令级故障注入研究。

Result: 从多个角度揭示了可靠性特征，凸显了模型架构、参数规模和任务复杂度的影响。

Conclusion: 研究为大语言模型可靠性提供新见解，有助于设计更有效的容错机制。

Abstract: Large language models (LLMs) are highly compute- and memory-intensive, posing significant demands on high-performance GPUs. At the same time, advances in GPU technology driven by shrinking transistor sizes and lower operating voltages have made these devices increasingly susceptible to soft errors. While prior work has examined GPU reliability, most studies have focused on general-purpose applications or conventional neural networks mostly used for vision tasks such as classification and detection. In contrast, systematic analysis of modern large-scale LLMs remains limited, despite their rapid adoption in diverse application scenarios. Given the unique characteristics of LLMs, their resilience to soft errors may differ substantially from earlier models. To bridge this gap, we conduct the first instruction-level fault injection study of LLM inference. Our approach reveals reliability characteristics from multiple perspectives, highlighting the effects of model architecture, parameter scale, and task complexity. These findings provide new insights into LLM reliability and inform the design of more effective fault tolerance mechanisms.

</details>


### [235] [How Much Progress Has There Been in NVIDIA Datacenter GPUs?](https://arxiv.org/abs/2601.20115)
*Emanuele Del Sozzo,Martin Fleming,Kenneth Flamm,Neil Thompson*

Main category: cs.AR

TL;DR: 本文研究2000年代中期至今英伟达数据中心GPU的技术进步，分析其主要特征趋势和增长指标，还量化美国出口管制规定的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 随着GPU不断发展以满足性能需求，分析其过去和现在的进展对确定科研未来限制至关重要，尤其在AI领域，美国实施出口管制，因此有必要研究英伟达数据中心GPU的技术进步。

Method: 收集英伟达数据中心GPU的综合数据集，包含计算性能、发布价格等特征，研究主要GPU特征趋势，估算每内存带宽、每美元和每瓦的增长率等指标。

Result: FP16和FP32操作的翻倍时间分别为1.44年和1.69年，FP64为2.06 - 3.79年；片外内存大小和带宽增长慢于计算性能，每3.32 - 3.53年翻倍；数据中心GPU发布价格约每5.1年翻倍，功耗约每16年翻倍；美国出口管制新规会使潜在性能差距从23.6倍缩小到3.54倍。

Conclusion: 对英伟达数据中心GPU技术进步的分析有助于了解其发展趋势，量化出口管制影响能为相关决策和科研提供参考。

Abstract: Graphics Processing Units (GPUs) are the state-of-the-art architecture for essential tasks, ranging from rendering 2D/3D graphics to accelerating workloads in supercomputing centers and, of course, Artificial Intelligence (AI). As GPUs continue improving to satisfy ever-increasing performance demands, analyzing past and current progress becomes paramount in determining future constraints on scientific research. This is particularly compelling in the AI domain, where rapid technological advancements and fierce global competition have led the United States to recently implement export control regulations limiting international access to advanced AI chips. For this reason, this paper studies technical progress in NVIDIA datacenter GPUs released from the mid-2000s until today. Specifically, we compile a comprehensive dataset of datacenter NVIDIA GPUs comprising several features, ranging from computational performance to release price. Then, we examine trends in main GPU features and estimate progress indicators for per-memory bandwidth, per-dollar, and per-watt increase rates. Our main results identify doubling times of 1.44 and 1.69 years for FP16 and FP32 operations (without accounting for sparsity benefits), while FP64 doubling times range from 2.06 to 3.79 years. Off-chip memory size and bandwidth grew at slower rates than computing performance, doubling every 3.32 to 3.53 years. The release prices of datacenter GPUs have roughly doubled every 5.1 years, while their power consumption has approximately doubled every 16 years. Finally, we quantify the potential implications of current U.S. export control regulations in terms of the potential performance gaps that would result if implementation were assumed to be complete and successful. We find that recently proposed changes to export controls would shrink the potential performance gap from 23.6x to 3.54x.

</details>


### [236] [CHIME: Chiplet-based Heterogeneous Near-Memory Acceleration for Edge Multimodal LLM Inference](https://arxiv.org/abs/2601.19908)
*Yanru Chen,Runyang Tian,Yue Pan,Zheyu Li,Weihong Xu,Tajana Rosing*

Main category: cs.AR

TL;DR: 提出CHIME用于边缘多模态大语言模型推理加速，在速度、能效和吞吐量上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推动多模态助手集成到边缘设备，但推理面临严格延迟、能量约束和数据移动开销大的问题，尤其是多模态大语言模型。

Method: 提出基于小芯片的异构近内存加速方案CHIME，利用M3D DRAM和RRAM小芯片优势，通过协同设计的映射框架执行融合内核。

Result: 在FastVLM和MobileVLM上，相比边缘GPU有高达54倍的加速和246倍的能效提升，吞吐量比最先进的PIM加速器FACIL高69.2倍，比仅使用M3D DRAM的设计在能效和性能上也有提升。

Conclusion: CHIME能有效解决边缘多模态大语言模型推理的挑战，提升推理性能和能效。

Abstract: The proliferation of large language models (LLMs) is accelerating the integration of multimodal assistants into edge devices, where inference is executed under stringent latency and energy constraints, often exacerbated by intermittent connectivity. These challenges become particularly acute in the context of multimodal LLMs (MLLMs), as high-dimensional visual inputs are transformed into extensive token sequences, thereby inflating the key-value (KV) cache and imposing substantial data movement overheads to the LLM backbone. To address these issues, we present CHIME, a chiplet-based heterogeneous near-memory acceleration for edge MLLMs inference. CHIME leverages the complementary strengths of integrated monolithic 3D (M3D) DRAM and RRAM chiplets: DRAM supplies low-latency bandwidth for attention, while RRAM offers dense, non-volatile storage for weights. This heterogeneous hardware is orchestrated by a co-designed mapping framework that executes fused kernels near data, minimizing cross-chiplet traffic to maximize effective bandwidth. On FastVLM (0.6B/1.7B) and MobileVLM (1.7B/3B), CHIME achieves up to 54x speedup and up to 246x better energy efficiency per inference as compared to the edge GPU NVIDIA Jetson Orin NX. It sustains 116.5-266.5 token/J compared to Jetson's 0.7-1.1 token/J. Furthermore, it delivers up to 69.2x higher throughput than the state-of-the-art PIM accelerator FACIL. Compared to the M3D DRAM-only design, CHIME's heterogeneous memory further improves energy efficiency by 7% and performance by 2.4x.

</details>


### [237] [PiC-BNN: A 128-kbit 65 nm Processing-in-CAM-Based End-to-End Binary Neural Network Accelerator](https://arxiv.org/abs/2601.19920)
*Yuval Harary,Almog Sharoni,Esteban Garzón,Marco Lanuzza,Adam Teman,Leonid Yavits*

Main category: cs.AR

TL;DR: 提出PiC - BNN这种端到端二进制基于近似搜索的内容可寻址内存的BNN加速器，在MNIST和HG数据集上有良好表现。


<details>
  <summary>Details</summary>
Motivation: 典型BNNs在非线性层使用全精度运算，限制面积和能量效益且需全精度运算架构支持，需改进。

Method: 设计并制造基于商业65nm工艺的PiC - BNN，利用汉明距离容差应用大数定律实现准确分类，不进行全精度运算。

Result: 在MNIST数据集上达到95.2%准确率，HG数据集上达到93.5%准确率，吞吐量560K推理/秒，实现703M推理/秒/瓦的功率效率。

Conclusion: PiC - BNN能在不进行全精度运算下实现准确分类，有较好性能表现。

Abstract: Binary Neural Networks (BNNs), where weights and activations are constrained to binary values (+1, -1), are a highly efficient alternative to traditional neural networks. Unfortunately, typical BNNs, while binarizing linear layers (matrix-vector multiplication), still implement other network layers (batch normalization, softmax, output layer, and sometimes the input layer of a convolutional neural network) in full precision. This limits the area and energy benefits and requires architectural support for full precision operations. We propose PiC-BNN, a true end-to-end binary in-approximate search (Hamming distance tolerant) Content Addressable Memory based BNN accelerator. PiC-BNN is designed and manufactured in a commercial 65nm process. PiC-BNN uses Hamming distance tolerance to apply the law of large numbers to enable accurate classification without implementing full precision operations. PiC-BNN achieves baseline software accuracy (95.2%) on the MNIST dataset and 93.5% on the Hand Gesture (HG) dataset, a throughput of 560K inferences/s, and presents a power efficiency of 703M inferences/s/W when implementing a binary MLP model for MNIST/HG dataset classification.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [238] [Convergence Analysis of Randomized Subspace Normalized SGD under Heavy-Tailed Noise](https://arxiv.org/abs/2601.20399)
*Gaku Omiya,Pierre-Louis Poirion,Akiko Takeda*

Main category: math.OC

TL;DR: 论文证明了RS - SGD在次高斯噪声下的高概率收敛界，提出RS - NSGD并给出收敛保证，显示其优于全维归一化SGD。


<details>
  <summary>Details</summary>
Motivation: 随机子空间方法在非凸优化中多数分析基于期望，次高斯噪声下高概率界稀缺，且现代机器学习中梯度多为重尾分布。

Method: 先证明RS - SGD在次高斯噪声下的高概率收敛界，再提出RS - NSGD，将方向归一化融入子空间更新。

Result: RS - SGD达到与先前期望结果相同阶的oracle复杂度；在噪声有有界p阶矩假设下，RS - NSGD有期望和高概率收敛保证，且oracle复杂度优于全维归一化SGD。

Conclusion: 随机子空间方法在非凸优化中有较好表现，RS - NSGD在处理重尾梯度时有优势。

Abstract: Randomized subspace methods reduce per-iteration cost; however, in nonconvex optimization, most analyses are expectation-based, and high-probability bounds remain scarce even under sub-Gaussian noise. We first prove that randomized subspace SGD (RS-SGD) admits a high-probability convergence bound under sub-Gaussian noise, achieving the same order of oracle complexity as prior in-expectation results. Motivated by the prevalence of heavy-tailed gradients in modern machine learning, we then propose randomized subspace normalized SGD (RS-NSGD), which integrates direction normalization into subspace updates. Assuming the noise has bounded $p$-th moments, we establish both in-expectation and high-probability convergence guarantees, and show that RS-NSGD can achieve better oracle complexity than full-dimensional normalized SGD.

</details>


### [239] [Randomized Feasibility Methods for Constrained Optimization with Adaptive Step Sizes](https://arxiv.org/abs/2601.20076)
*Abhishek Chakraborty,Angelia Nedić*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider minimizing an objective function subject to constraints defined by the intersection of lower-level sets of convex functions. We study two cases: (i) strongly convex and Lipschitz-smooth objective function and (ii) convex but possibly nonsmooth objective function. To deal with the constraints that are not easy to project on, we use a randomized feasibility algorithm with Polyak steps and a random number of sampled constraints per iteration, while taking (sub)gradient steps to minimize the objective function. For case (i), we prove linear convergence in expectation of the objective function values to any prescribed tolerance using an adaptive stepsize. For case (ii), we develop a fully problem parameter-free and adaptive stepsize scheme that yields an $O(1/\sqrt{T})$ worst-case rate in expectation. The infeasibility of the iterates decreases geometrically with the number of feasibility updates almost surely, while for the averaged iterates, we establish an expected lower bound on the function values relative to the optimal value that depends on the distribution for the random number of sampled constraints. For certain choices of sample-size growth, optimal rates are achieved. Finally, simulations on a Quadratically Constrained Quadratic Programming (QCQP) problem and Support Vector Machines (SVM) demonstrate the computational efficiency of our algorithm compared to other state-of-the-art methods.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [240] [Quantum statistics from classical simulations via generative Gibbs sampling](https://arxiv.org/abs/2601.20228)
*Weizhou Wang,Xuanxi Zhang,Jonathan Weare,Aaron R. Dinner*

Main category: physics.chem-ph

TL;DR: 提出GG - PI框架结合生成建模与吉布斯采样从经典模拟数据恢复量子统计，相比PIMD减少耗时且易扩展。


<details>
  <summary>Details</summary>
Motivation: 准确模拟核量子效应重要但使用PIMD昂贵，需要更高效方法。

Method: 提出GG - PI框架，结合单珠条件密度的生成建模和吉布斯采样，利用廉价经典模拟或现有数据训练且可跨温度转移。

Result: 在标准测试系统上，与PIMD相比显著减少了时钟时间。

Conclusion: 该方法可轻松扩展到具有相似马尔可夫结构的广泛问题。

Abstract: Accurate simulation of nuclear quantum effects is essential for molecular modeling but expensive using path integral molecular dynamics (PIMD). We present GG-PI, a ring-polymer-based framework that combines generative modeling of the single-bead conditional density with Gibbs sampling to recover quantum statistics from classical simulation data. GG-PI uses inexpensive standard classical simulations or existing data for training and allows transfer across temperatures without retraining. On standard test systems, GG-PI significantly reduces wall clock time compared to PIMD. Our approach extends easily to a wide range of problems with similar Markov structure.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [241] [The Sound of Noise: Leveraging the Inductive Bias of Pre-trained Audio Transformers for Glitch Identification in LIGO](https://arxiv.org/abs/2601.20034)
*Suyash Deshmukh,Chayan Chatterjee,Abigail Petulante,Tabata Aira Ferreira,Karan Jani*

Main category: astro-ph.IM

TL;DR: 提出跨领域框架，用预训练音频模型处理引力波应变数据，分析LIGO探测器数据验证其有效性，特征提取优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有引力波干扰分类方法存在‘标签瓶颈’，难以泛化到新干扰形态或奇异信号。

Method: 提出跨领域框架，运用预训练的音频频谱图变换器（AST），将自然声音学习的特征表示迁移到引力波领域，用t - SNE可视化。

Result: 分析LIGO探测器第三、四轮观测数据，AST生成的信号和干扰嵌入良好分离，与Gravity Spy的干扰类别吻合。

Conclusion: 音频预训练带来的归纳偏置使特征提取优于传统方法，为发现新的异常瞬变和复杂噪声干扰分类提供有效途径。

Abstract: Transient noise artifacts, or glitches, fundamentally limit the sensitivity of gravitational-wave (GW) interferometers and can mimic true astrophysical signals, particularly the short-duration intermediate-mass black hole (IMBH) mergers. Current glitch classification methods, such as Gravity Spy, rely on supervised models trained from scratch using labeled datasets. These approaches suffer from a significant ``label bottleneck," requiring massive, expertly annotated datasets to achieve high accuracy and often struggling to generalize to new glitch morphologies or exotic GW signals encountered in observing runs. In this work, we present a novel cross-domain framework that treats GW strain data through the lens of audio processing. We utilize the Audio Spectrogram Transformer (AST), a model pre-trained on large-scale audio datasets, and adapt it to the GW domain. Instead of learning time-frequency features from scratch, our method exploits the strong inductive bias inherent in pre-trained audio models, transferring learned representations of natural sound to the characterization of detector noise and GW signals, including IMBHs. We validate this approach by analyzing strain data from the third (O3) and fourth (O4) observing runs of the LIGO detectors. We used t-Distributed Stochastic Neighbor Embedding (t-SNE), an unsupervised clustering technique, to visualize the AST-derived embeddings of signals and glitches, revealing well-separated groups that align closely with independently validated Gravity Spy glitch classes. Our results indicate that the inductive bias from audio pre-training allows superior feature extraction compared to traditional supervised techniques, offering a robust, data-efficient pathway for discovering new, anomalous transients, and classifying complex noise artifacts in the era of next-generation detectors.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [242] [A scalable flow-based approach to mitigate topological freezing](https://arxiv.org/abs/2601.20708)
*Claudio Bonanno,Andrea Bulgarelli,Elia Cellini,Alessandro Nada,Dario Panfalone,Davide Vadacchino,Lorenzo Verzichelli*

Main category: hep-lat

TL;DR: 论文提出借助随机归一化流，将带开放边界条件缺陷的配置转换到全周期性系综，来解决格点规范理论连续极限下的拓扑冻结问题，且该方法比纯随机非平衡方法性能更优。


<details>
  <summary>Details</summary>
Motivation: 标准Markov Chain Monte Carlo模拟在格点规范理论趋于连续极限时存在拓扑冻结问题，采用开放边界条件虽能恢复拓扑遍历采样，但会破坏平移不变性并引入非物理边界伪影。

Method: 基于随机归一化流（SNF），交替进行非平衡Monte Carlo更新和通过掩码参数化stout涂抹实现的局部规范等价缺陷耦合层。通过最小化平均耗散功进行训练。

Result: 缺陷SNFs在可比成本下比纯随机非平衡方法有更好表现，可重现拓扑磁化率的参考结果。

Conclusion: 提出的可扩展精确流策略有效，能解决边界伪影问题，提高效率。

Abstract: As lattice gauge theories with non-trivial topological features are driven towards the continuum limit, standard Markov Chain Monte Carlo simulations suffer for topological freezing, i.e., a dramatic growth of autocorrelations in topological observables. A widely used strategy is the adoption of Open Boundary Conditions (OBC), which restores ergodic sampling of topology but at the price of breaking translation invariance and introducing unphysical boundary artifacts. In this contribution we summarize a scalable, exact flow-based strategy to remove them by transporting configurations from a prior with a OBC defect to a fully periodic ensemble, and apply it to 4d SU(3) Yang--Mills theory. The method is based on a Stochastic Normalizing Flow (SNF) that alternates non-equilibrium Monte Carlo updates with localized, gauge-equivariant defect coupling layers implemented via masked parametric stout smearing. Training is performed by minimizing the average dissipated work, equivalent to a Kullback--Leibler divergence between forward and reverse non-equilibrium path measures, to achieve more reversible trajectories and improved efficiency. We discuss the scaling with the number of degrees of freedom affected by the defect and show that defect SNFs achieve better performances than purely stochastic non-equilibrium methods at comparable cost. Finally, we validate the approach by reproducing reference results for the topological susceptibility.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [243] [Exploring the holographic entropy cone via reinforcement learning](https://arxiv.org/abs/2601.19979)
*Temple He,Jaeha Lee,Hirosi Ooguri*

Main category: hep-th

TL;DR: 开发强化学习算法研究全息熵锥，对N=3和N=6情况进行测试，发现N=6中部分极端射线可实现，部分可能存在未知不等式。


<details>
  <summary>Details</summary>
Motivation: 研究全息熵锥，确定目标熵向量是否有图实现以及探测锥的面的位置。

Method: 开发强化学习算法，搜索与目标熵向量匹配的图实现，若没有则找最接近的图。

Result: 对N=3情况成功重现互信息的一夫一妻制；对N=6情况，为3条极端射线找到实现，另3条未找到。

Conclusion: 证明N=6中3条极端射线是全息熵锥的真正极端射线，另3条可能存在未知全息不等式。

Abstract: We develop a reinforcement learning algorithm to study the holographic entropy cone. Given a target entropy vector, our algorithm searches for a graph realization whose min-cut entropies match the target vector. If the target vector does not admit such a graph realization, it must lie outside the cone, in which case the algorithm finds a graph whose corresponding entropy vector most nearly approximates the target and allows us to probe the location of the facets. For the $\sf N=3$ cone, we confirm that our algorithm successfully rediscovers monogamy of mutual information beginning with a target vector outside the holographic entropy cone. We then apply the algorithm to the $\sf N=6$ cone, analyzing the 6 "mystery" extreme rays of the subadditivity cone from arXiv:2412.15364 that satisfy all known holographic entropy inequalities yet lacked graph realizations. We found realizations for 3 of them, proving they are genuine extreme rays of the holographic entropy cone, while providing evidence that the remaining 3 are not realizable, implying unknown holographic inequalities exist for $\sf N=6$.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [244] [Neural Quantum States in Mixed Precision](https://arxiv.org/abs/2601.20782)
*Massimo Solinas,Agnes Valenti,Nawaf Bou-Rabee,Roeland Wiersema*

Main category: quant-ph

TL;DR: 研究混合精度算法在基于神经网络的变分蒙特卡罗（VMC）中的作用，给出误差分析并验证，体现该策略在VMC中的实践有效性。


<details>
  <summary>Details</summary>
Motivation: 传统科学计算依赖双精度保证准确性，硬件加速器使低精度格式因性能、内存和能效优势变得有吸引力，研究混合精度在解决量子多体系统的VMC中的作用。

Method: 先推导简化精度在Metropolis - Hastings MCMC中引入误差的一般分析界限，再在VMC用例上进行实证验证。

Result: 算法的大部分尤其是对量子态采样可在半精度下执行且不损失准确性。

Conclusion: 为基于MCMC采样的机器学习方法提供评估混合精度算法适用性的理论框架，展示了混合精度策略在VMC中实现更具可扩展性和高能效量子多体系统模拟的实际效果。

Abstract: Scientific computing has long relied on double precision (64-bit floating point) arithmetic to guarantee accuracy in simulations of real-world phenomena. However, the growing availability of hardware accelerators such as Graphics Processing Units (GPUs) has made low-precision formats attractive due to their superior performance, reduced memory footprint, and improved energy efficiency. In this work, we investigate the role of mixed-precision arithmetic in neural-network based Variational Monte Carlo (VMC), a widely used method for solving computationally otherwise intractable quantum many-body systems. We first derive general analytical bounds on the error introduced by reduced precision on Metropolis-Hastings MCMC, and then empirically validate these bounds on the use-case of VMC. We demonstrate that significant portions of the algorithm, in particular, sampling the quantum state, can be executed in half precision without loss of accuracy. More broadly, this work provides a theoretical framework to assess the applicability of mixed-precision arithmetic in machine-learning approaches that rely on MCMC sampling. In the context of VMC, we additionally demonstrate the practical effectiveness of mixed-precision strategies, enabling more scalable and energy-efficient simulations of quantum many-body systems.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [245] [Large Language Models Polarize Ideologically but Moderate Affectively in Online Political Discourse](https://arxiv.org/abs/2601.20238)
*Gavin Wang,Srinaath Anbudurai,Oliver Sun,Xitong Li,Lynn Wu*

Main category: econ.GN

TL;DR: 研究分析Reddit政治论坛评论，发现ChatGPT加剧意识形态极化，但降低情感极化。


<details>
  <summary>Details</summary>
Motivation: 探究ChatGPT发布对Reddit最大政治论坛上意识形态和情感模式的影响。

Method: 分析数百万条评论。

Result: ChatGPT加剧意识形态极化，原因是其生成的评论倾向于呼应和强化原帖观点；但情感极化（敌意和毒性）下降。

Conclusion: 大语言模型可同时加深意识形态分离并促进更文明的交流，挑战了极端性和不文明性必然同步的长期假设。

Abstract: The emergence of large language models (LLMs) is reshaping how people engage in political discourse online. We examine how the release of ChatGPT altered ideological and emotional patterns in the largest political forum on Reddit. Analysis of millions of comments shows that ChatGPT intensified ideological polarization: liberals became more liberal, and conservatives more conservative. This shift does not stem from the creation of more persuasive or ideologically extreme original content using ChatGPT. Instead, it originates from the tendency of ChatGPT-generated comments to echo and reinforce the viewpoint of original posts, a pattern consistent with algorithmic sycophancy. Yet, despite growing ideological divides, affective polarization, measured by hostility and toxicity, declined. These findings reveal that LLMs can simultaneously deepen ideological separation and foster more civil exchanges, challenging the long-standing assumption that extremity and incivility necessarily move together.

</details>


### [246] [Bank Runs With and Without Bank Failure](https://arxiv.org/abs/2601.20285)
*Sergio Correia,Stephan Luck,Emil Verner*

Main category: econ.GN

TL;DR: 利用1863 - 1934年美国银行挤兑数据集研究挤兑原因与后果，发现弱银行易发生挤兑，强银行遇负面消息也会有挤兑，仅弱银行挤兑易致失败，银行倒闭比未倒闭挤兑影响大。


<details>
  <summary>Details</summary>
Motivation: 研究银行挤兑的原因和后果。

Method: 对历史报纸应用自然语言处理，识别4049起银行挤兑事件。

Result: 弱银行更易发生挤兑，强银行遇负面消息也会有挤兑；仅弱银行挤兑通常导致失败，强银行可通过多种机制度过；银行倒闭比未倒闭挤兑对存款和贷款影响更大。

Conclusion: 银行基本面差是挤兑导致失败和银行困境产生严重经济后果的必要条件。

Abstract: We study the causes and consequences of bank runs using a novel dataset on bank runs in the United States from 1863 to 1934. Applying natural language processing to historical newspapers, we identify 4,049 runs on individual banks. Runs are considerably more likely in weak banks but also occur in strong banks, especially in response to negative news about the real economy or the broader banking system. However, runs typically only result in failure for banks with weak fundamentals. Strong banks survive runs through various mechanisms, including interbank cooperation, equity injections, public signals of strength, and suspension of convertibility. At the local level, bank failures (with and without runs) translate into substantially larger declines in deposits and lending than runs without failures. Our findings suggest that poor bank fundamentals are necessary for bank runs to translate into failure and for bank distress to generate severe economic consequences.

</details>


### [247] [Manipulation in Prediction Markets: An Agent-based Modeling Experiment](https://arxiv.org/abs/2601.20452)
*Bridget Smart,Ebba Mark,Anne Bastian,Josefina Waugh*

Main category: econ.GN

TL;DR: 本文用基于代理的模拟结合价格动态分析，研究高预算代理如何在预测市场制造价格扭曲，探讨其持续性和稳定性，及代理专业性对价格方差的影响。


<details>
  <summary>Details</summary>
Motivation: 预测市场在政治选举中受关注，人们担心其易受操纵及对民主进程的影响。

Method: 构建基于代理的预测市场模型，模拟不同类型代理行为，结合理论分析价格动态。

Result: 有偏见的“鲸鱼”代理可临时改变价格，非“鲸鱼”代理的羊群行为和慢学习会提高价格扭曲的幅度和持续时间，理论分析也证实了此结果。

Conclusion: “鲸鱼”代理能根据其市场资本份额改变价格，价格扭曲持续时间取决于非“鲸鱼”代理的学习率和羊群行为强度。

Abstract: Prediction markets mobilize financial incentives to forecast binary event outcomes through the aggregation of dispersed beliefs and heterogeneous information. Their growing popularity and demonstrated predictive accuracy in political elections have raised speculation and concern regarding their susceptibility to manipulation and the potential consequences for democratic processes. Using agent-based simulations combined with an analytic characterization of price dynamics, we study how high-budget agents can introduce price distortions in prediction markets. We explore the persistence and stability of these distortions in the presence of herding or stubborn agents, and analyze how agent expertise affects market-price variance. Firstly we propose an agent-based model of a prediction market in which bettors with heterogeneous expertise, noisy private information, variable learning rates and budgets observe the evolution of public opinion on a binary election outcome to inform their betting strategies in the market. The model exhibits stability across a broad parameter space, with complex agent behaviors and price interactions producing self-regulatory price discovery. Second, using this simulation framework, we investigate the conditions under which a highly resourced minority, or ''whale'' agent, with a biased valuation can distort the market price, and for how long. We find that biased whales can temporarily shift prices, with the magnitude and duration of distortion increasing when non-whale bettors exhibit herding behavior and slow learning. Our theoretical analysis corroborates these results, showing that whales can shift prices proportionally to their share of market capital, with distortion duration depending on non-whale learning rates and herding intensity.

</details>


### [248] [Pricing Catastrophe: How Extreme Political Shocks Reprice Sovereign Risk, Beliefs, and Growth Expectations](https://arxiv.org/abs/2601.20724)
*Riste Ichev,Rok Spruk*

Main category: econ.GN

TL;DR: 以2023年10月7日以色列受袭为冲击，研究极端政治冲击下信念破裂对资本成本、预期和宏观经济动态的影响，发现长期主权风险重定价、家庭福利信念恶化、中期动能改善等结果。


<details>
  <summary>Details</summary>
Motivation: 研究极端政治冲击导致的信念破裂如何影响资本成本、预期和宏观经济动态。

Method: 利用2008 - 2025年月度数据和发达经济体捐赠池，采用带滚动窗口交叉验证和基于安慰剂推断的矩阵完成设计，并辅以合成双重差分法估计反事实路径。

Result: 一是以色列长期主权风险持续重定价，10年期收益率和利差大幅上升；二是家庭福利信念持续恶化；三是中期动能改善，OECD综合领先指标大幅上升，呈现风险 - 增长脱钩现象。

Conclusion: 信念驱动渠道是极端冲击塑造宏观金融结果的核心机制。

Abstract: Extreme political shocks may reshape economies not only through contemporaneous disruption but by altering beliefs about the distribution of future states. We study how such belief ruptures affect the cost of capital, expectations, and macroeconomic dynamics, using the October 7, 2023 attack on Israel as a precisely timed shock. Leveraging monthly data from 2008 to 2025 and a donor pool of advanced economies, we estimate counterfactual paths using a matrix completion design with rolling-window cross-validation and placebo-based inference, corroborated by synthetic difference-in-differences. We document three core findings. First, long-horizon sovereign risk of Israel is persistently repriced. Ten-year yields and spreads relative to the United States rise sharply and remain elevated. Second, household welfare beliefs deteriorate durably, as reflected in consumer confidence. Third, medium-run momentum improves, captured by a strong rise in the OECD composite leading indicator. These patterns reveal risk-growth decoupling where tail-risk premia rise even as medium-horizon activity expectations strengthen. Our results highlight belief-driven channels as a central mechanism through which extreme ruptures shape macro-financial outcomes.

</details>


### [249] [A Smoothed GMM for Dynamic Quantile Preferences Estimation](https://arxiv.org/abs/2601.20853)
*Xin Liu,Luciano de Castro,Antonio F. Galvao*

Main category: econ.GN

TL;DR: 本文提出估计$τ$-分位数的方法，在弱假设下证明估计量性质，用模拟和实证展示方法特性。


<details>
  <summary>Details</summary>
Motivation: 解决在一般条件分位数限制下，$τ$-分位数及其他有限维参数的估计问题。

Method: 采用广义矩估计框架，对矩函数进行平滑处理。

Result: 在弱假设下建立了估计量的一致性和渐近正态性，模拟展示了方法的有限样本性质，实证估计了风险态度和跨期替代弹性。

Conclusion: 所提出的方法能有效估计相关参数。

Abstract: This paper suggests methods for estimation of the $τ$-quantile, $τ\in(0,1)$, as a parameter along with the other finite-dimensional parameters identified by general conditional quantile restrictions. We employ a generalized method of moments framework allowing for non-linearities and dependent data, where moment functions are smoothed to aid both computation and tractability. Consistency and asymptotic normality of the estimators are established under weak assumptions. Simulations illustrate the finite-sample properties of the methods. An empirical application using a quantile intertemporal consumption model with multiple assets estimates the risk attitude, which is captured by $τ$, together with the elasticity of intertemporal substitution.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [250] [Securing AI Agents in Cyber-Physical Systems: A Survey of Environmental Interactions, Deepfake Threats, and Defenses](https://arxiv.org/abs/2601.20184)
*Mohsen Hatami,Van Tuan Pham,Hozefa Lakadawala,Yu Chen*

Main category: cs.CR

TL;DR: 本文全面综述CPS中AI代理面临的安全威胁，用SENTINEL框架整理文献，通过智能电网案例说明部署防御的限制，强调信任机制和深度防御架构，指出可信AI - CPS的挑战。


<details>
  <summary>Details</summary>
Motivation: AI代理融入CPS带来超越传统的新安全风险，如生成式AI攻击和新协议扩大攻击面，需要全面研究安全威胁。

Method: 使用SENTINEL框架整理文献，进行基于真实智能电网部署的端到端案例研究。

Result: 定量说明定时、噪声和误报成本对可部署防御的限制，指出仅检测机制不足以作为安全关键CPS的决策依据。

Conclusion: 强调基于来源和物理的信任机制与深度防御架构的作用，列出可信AI - CPS面临的开放挑战。

Abstract: The increasing integration of AI agents into cyber-physical systems (CPS) introduces new security risks that extend beyond traditional cyber or physical threat models. Recent advances in generative AI enable deepfake and semantic manipulation attacks that can compromise agent perception, reasoning, and interaction with the physical environment, while emerging protocols such as the Model Context Protocol (MCP) further expand the attack surface through dynamic tool use and cross-domain context sharing. This survey provides a comprehensive review of security threats targeting AI agents in CPS, with a particular focus on environmental interactions, deepfake-driven attacks, and MCP-mediated vulnerabilities. We organize the literature using the SENTINEL framework, a lifecycle-aware methodology that integrates threat characterization, feasibility analysis under CPS constraints, defense selection, and continuous validation. Through an end-to-end case study grounded in a real-world smart grid deployment, we quantitatively illustrate how timing, noise, and false-positive costs constrain deployable defenses, and why detection mechanisms alone are insufficient as decision authorities in safety-critical CPS. The survey highlights the role of provenance- and physics-grounded trust mechanisms and defense-in-depth architectures, and outlines open challenges toward trustworthy AI-enabled CPS.

</details>


### [251] [LIFT: Byzantine Resilient Hub-Sampling](https://arxiv.org/abs/2601.20368)
*Mohamed Amine Legheraba,Nour Rachdi,Maria Gradinariu Potop-Butucaru,Sébastien Tixeuil*

Main category: cs.CR

TL;DR: 研究评估了Elevator协议在拜占庭敌手环境下的表现，发现其易受攻击，进而提出LIFT协议，它更能抵抗攻击。


<details>
  <summary>Details</summary>
Motivation: 现有Elevator协议未研究对拜占庭敌手的弹性，需评估其安全性并提出改进。

Method: 评估Elevator在拜占庭敌手环境下的表现，提出LIFT协议，用加密安全的伪随机数生成器选择中心节点。

Result: 少量（2%）拜占庭节点就能颠覆Elevator网络，LIFT能承受最多10%的拜占庭节点。

Conclusion: 去中心化中心节点形成需要安全的随机性，LIFT更适合构建抗拜占庭的去中心化系统。

Abstract: Recently, a novel peer sampling protocol, Elevator, was introduced to construct network topologies tailored for emerging decentralized applications such as federated learning and blockchain. Elevator builds hub-based topologies in a fully decentralized manner, randomly selecting hubs among participating nodes. These hubs, acting as central nodes connected to the entire network, can be leveraged to accelerate message dissemination. Simulation results have shown that Elevator converges rapidly (within 3--4 cycles) and exhibits robustness against crash failures and churn. However, its resilience to Byzantine adversaries has not been investigated. In this work, we provide the first evaluation of Elevator under Byzantine adversaries and show that even a small fraction (2%) of Byzantine nodes is sufficient to subvert the network. As a result, we introduce LIFT, a new protocol that extends Elevator by employing a cryptographically secure pseudo-random number generator (PRNG) for hub selection, thereby mitigating Byzantine manipulation. In contrast, LIFT withstands adversarial infiltration and remains robust with up to 10% Byzantine nodes. These results highlight the necessity of secure randomness in decentralized hub formation and position LIFT as a more reliable building block for Byzantine-resilient decentralized systems.

</details>


### [252] [Eliciting Least-to-Most Reasoning for Phishing URL Detection](https://arxiv.org/abs/2601.20270)
*Holly Trikilis,Pasindu Marasinghe,Fariza Rashid,Suranga Seneviratne*

Main category: cs.CR

TL;DR: 提出用于钓鱼URL检测的最少到最多提示框架，引入“答案敏感性”机制，实验表明该框架优于单样本方法，性能与监督模型相当且所需训练数据少。


<details>
  <summary>Details</summary>
Motivation: 钓鱼是常见攻击手段，准确分类钓鱼URL很重要，大语言模型虽有成果但其推理能力未充分探索。

Method: 提出最少到最多提示框架，引入“答案敏感性”机制，用三个URL数据集和四个最先进大语言模型评估，与单样本方法和监督模型对比。

Result: 框架优于单样本基线，性能与监督模型相当，所需训练数据少。

Conclusion: 简单强大的提示策略始终优于单样本和监督方法，所需训练或少样本指导极少。

Abstract: Phishing continues to be one of the most prevalent attack vectors, making accurate classification of phishing URLs essential. Recently, large language models (LLMs) have demonstrated promising results in phishing URL detection. However, their reasoning capabilities that enabled such performance remain underexplored. To this end, in this paper, we propose a Least-to-Most prompting framework for phishing URL detection. In particular, we introduce an "answer sensitivity" mechanism that guides Least-to-Most's iterative approach to enhance reasoning and yield higher prediction accuracy. We evaluate our framework using three URL datasets and four state-of-the-art LLMs, comparing against a one-shot approach and a supervised model. We demonstrate that our framework outperforms the one-shot baseline while achieving performance comparable to that of the supervised model, despite requiring significantly less training data. Furthermore, our in-depth analysis highlights how the iterative reasoning enabled by Least-to-Most, and reinforced by our answer sensitivity mechanism, drives these performance gains. Overall, we show that this simple yet powerful prompting strategy consistently outperforms both one-shot and supervised approaches, despite requiring minimal training or few-shot guidance. Our experimental setup can be found in our Github repository github.sydney.edu.au/htri0928/least-to-most-phishing-detection.

</details>


### [253] [Multimodal Multi-Agent Ransomware Analysis Using AutoGen](https://arxiv.org/abs/2601.20346)
*Asifullah Khan,Aimen Wadood,Mubashar Iqbal,Umme Zahoora*

Main category: cs.CR

TL;DR: 本文提出多模态多智能体勒索软件分析框架用于勒索软件分类，在大规模数据集上评估显示其优于单模态和非自适应融合基线，为改进现实世界勒索软件防御系统提供有效途径。


<details>
  <summary>Details</summary>
Motivation: 传统勒索软件检测方法单独使用时存在局限性，需新方法提升检测效果。

Method: 提出多模态多智能体架构，结合静态、动态和网络信息，各数据类型由专业智能体处理，通过自编码器提取特征，经融合智能体整合，用基于Transformer的分类器识别勒索软件家族，智能体间通过反馈机制迭代优化特征表示。

Result: 在大规模数据集上实验，优于单模态和非自适应融合基线，Macro - F1最高提升0.936，降低校准误差，智能体反馈循环稳定收敛，最终综合得分约0.88。

Conclusion: 所提方法为改进现实世界勒索软件防御系统提供了实用有效的途径。

Abstract: Ransomware has become one of the most serious cybersecurity threats causing major financial losses and operational disruptions worldwide.Traditional detection methods such as static analysis, heuristic scanning and behavioral analysis often fall short when used alone. To address these limitations, this paper presents multimodal multi agent ransomware analysis framework designed for ransomware classification. Proposed multimodal multiagent architecture combines information from static, dynamic and network sources. Each data type is handled by specialized agent that uses auto encoder based feature extraction. These representations are then integrated through a fusion agent. After that fused representation are used by transformer based classifier. It identifies the specific ransomware family. The agents interact through an interagent feedback mechanism that iteratively refines feature representations by suppressing low confidence information. The framework was evaluated on large scale datasets containing thousands of ransomware and benign samples. Multiple experiments were conducted on ransomware dataset. It outperforms single modality and nonadaptive fusion baseline achieving improvement of up to 0.936 in Macro-F1 for family classification and reducing calibration error. Over 100 epochs, the agentic feedback loop displays a stable monotonic convergence leading to over +0.75 absolute improvement in terms of agent quality and a final composite score of around 0.88 without fine tuning of the language models. Zeroday ransomware detection remains family dependent on polymorphism and modality disruptions. Confidence aware abstention enables reliable real world deployment by favoring conservativeand trustworthy decisions over forced classification. The findings indicate that proposed approach provides a practical andeffective path toward improving real world ransomware defense systems.

</details>


### [254] [IoT Device Identification with Machine Learning: Common Pitfalls and Best Practices](https://arxiv.org/abs/2601.20548)
*Kahraman Kostas,Rabia Yasa Kostas*

Main category: cs.CR

TL;DR: 本文批判性审视用机器学习进行设备识别过程，分析相关权衡并指出错误，为研究者提供准则。


<details>
  <summary>Details</summary>
Motivation: 解决现有文献中设备识别过程的常见陷阱问题。

Method: 分析识别方法权衡、数据异质性、特征提取挑战和评估指标，指出特定错误。

Result: 指出如不当数据增强和误导性会话标识符等具体错误。

Conclusion: 为研究者提供增强物联网安全模型可重复性和泛化性的可靠准则。

Abstract: This paper critically examines the device identification process using machine learning, addressing common pitfalls in existing literature. We analyze the trade-offs between identification methods (unique vs. class based), data heterogeneity, feature extraction challenges, and evaluation metrics. By highlighting specific errors, such as improper data augmentation and misleading session identifiers, we provide a robust guideline for researchers to enhance the reproducibility and generalizability of IoT security models.

</details>


### [255] [Benchmarking LLAMA Model Security Against OWASP Top 10 For LLM Applications](https://arxiv.org/abs/2601.19970)
*Nourin Shahin,Izzat Alsmadi*

Main category: cs.CR

TL;DR: 研究针对OWASP框架对Llama模型变体进行安全基准测试，发现小模型在安全任务中表现更好，并提供开源数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型从研究原型走向企业系统，其安全漏洞对数据隐私和系统完整性构成严重风险。

Method: 使用FABRIC测试平台和NVIDIA A30 GPU，对五种标准Llama模型和五种Llama Guard变体在100个涵盖十个漏洞类别的对抗性提示上进行测试。

Result: 不同模型安全性能差异显著，紧凑的Llama - Guard - 3 - 1B检测率最高达76%且延迟最小，基础模型如Llama - 3.1 - 8B检测率为0%且推理时间长；发现模型大小和安全有效性呈反比。

Conclusion: 较小的、专门化的模型在安全任务中通常优于较大的通用模型，还提供开源基准数据集支持AI安全的可重复研究。

Abstract: As large language models (LLMs) move from research prototypes to enterprise systems, their security vulnerabilities pose serious risks to data privacy and system integrity. This study benchmarks various Llama model variants against the OWASP Top 10 for LLM Applications framework, evaluating threat detection accuracy, response safety, and computational overhead. Using the FABRIC testbed with NVIDIA A30 GPUs, we tested five standard Llama models and five Llama Guard variants on 100 adversarial prompts covering ten vulnerability categories. Our results reveal significant differences in security performance: the compact Llama-Guard-3-1B model achieved the highest detection rate of 76% with minimal latency (0.165s per test), whereas base models such as Llama-3.1-8B failed to detect threats (0% accuracy) despite longer inference times (0.754s). We observe an inverse relationship between model size and security effectiveness, suggesting that smaller, specialized models often outperform larger general-purpose ones in security tasks. Additionally, we provide an open-source benchmark dataset including adversarial prompts, threat labels, and attack metadata to support reproducible research in AI security, [1].

</details>


### [256] [SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks](https://arxiv.org/abs/2601.20310)
*Xin Zhang,Zijin Yang,Kejiang Chen,Linfeng Ma,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: 提出SemBind防御框架抵抗基于潜在扩散模型的潜在水印黑盒伪造攻击，兼容现有方案且能控制鲁棒性与安全性平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在扩散模型的潜在水印易受黑盒伪造攻击，威胁图像来源与可信度。

Method: 提出SemBind框架，通过学习语义掩码器将潜在信号与图像语义绑定，用对比学习训练掩码器，对代码重塑和置换以调制目标潜在信号。

Result: SemBind兼容现有潜在水印方案，图像质量基本不变，在四种主流潜在水印方法中，显著降低黑盒伪造下的误接受率。

Conclusion: SemBind能有效抵抗黑盒伪造攻击，提供可控的鲁棒性 - 安全平衡。

Abstract: Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider's model, can embed the provider's watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [257] [Interpreting Emergent Extreme Events in Multi-Agent Systems](https://arxiv.org/abs/2601.20538)
*Ling Tang,Jilin Mei,Dongrui Liu,Chen Qian,Dawei Cheng,Jing Shao,Xia Hu*

Main category: cs.MA

TL;DR: 提出首个解释多智能体系统中涌现极端事件的框架，通过适配Shapley值归因，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的多智能体系统中交互产生的极端事件起源不明，解释这些事件对系统安全至关重要。

Method: 适配Shapley值将极端事件归因于不同时间步智能体的每个动作，按时间、智能体和行为维度聚合归因分数，设计指标表征极端事件特征。

Result: 在经济、金融和社会等多智能体系统场景的实验中，证明了框架的有效性。

Conclusion: 所提框架能有效解释多智能体系统中的涌现极端事件，并提供极端现象涌现的一般见解。

Abstract: Large language model-powered multi-agent systems have emerged as powerful tools for simulating complex human-like systems. The interactions within these systems often lead to extreme events whose origins remain obscured by the black box of emergence. Interpreting these events is critical for system safety. This paper proposes the first framework for explaining emergent extreme events in multi-agent systems, aiming to answer three fundamental questions: When does the event originate? Who drives it? And what behaviors contribute to it? Specifically, we adapt the Shapley value to faithfully attribute the occurrence of extreme events to each action taken by agents at different time steps, i.e., assigning an attribution score to the action to measure its influence on the event. We then aggregate the attribution scores along the dimensions of time, agent, and behavior to quantify the risk contribution of each dimension. Finally, we design a set of metrics based on these contribution scores to characterize the features of extreme events. Experiments across diverse multi-agent system scenarios (economic, financial, and social) demonstrate the effectiveness of our framework and provide general insights into the emergence of extreme phenomena.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [258] [Simulating Complex Multi-Turn Tool Calling Interactions in Stateless Execution Environments](https://arxiv.org/abs/2601.19914)
*Maxwell Crouse,Ibrahim Abdelaziz,Kshitij Fadnis,Siva Sankalp Patel,Kinjal Basu,Chulaka Gunasekara,Sadhana Kumaravel,Asim Munawar,Pavan Kapanipathi*

Main category: cs.CL

TL;DR: 提出数据生成方法DiGiT - TC，用于生成工具调用对话，在标准基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有生成合成多轮工具调用数据的方法常假设交互在有状态执行环境中进行，但许多现实场景不满足此条件，需新方法填补空白。

Method: 引入数据生成方法DiGiT - TC，利用新颖生成模式在用户请求中隐式表示某些工具调用。

Result: 在标准工具调用基准测试中验证，即使在有状态问题设置中也有显著性能提升。

Conclusion: DiGiT - TC方法有效，能生成具有有状态环境搜索特征的工具调用对话，提升性能。

Abstract: Synthetic data has proven itself to be a valuable resource for tuning smaller, cost-effective language models to handle the complexities of multi-turn tool calling conversations. While many frameworks and systems for producing synthetic multi-turn tool calling data have been proposed, prior works have frequently assumed that any tool calling interactions will take place in an execution environment that maintains state. When such an environment is available, this is advantageous as it allows for the validity of an interaction to be determined by whether or not the state of the execution environment matches to some prespecified objective. Unfortunately, this does not hold in many real-world tool use settings, e.g., in enterprise settings where data security is of the utmost importance or in cases where tool specifications are synthesized from multiple sources. In this work, we address this gap by introducing a data generation method, DiGiT-TC, that is designed to produce tool calling conversations that have the characteristics of conversations generated through search in a stateful environment. The key to our technique lies in a novel generation pattern that allows our approach to implicitly represent certain tool calls in the user request. We validate our approach on standard tool calling benchmarks and demonstrate that, even in stateful problem settings, our approach results in strong performance gains.

</details>


### [259] [Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding](https://arxiv.org/abs/2601.19929)
*David Linus Ostby*

Main category: cs.CL

TL;DR: 提出Stingy Context压缩方案，在自动编码任务中实现18:1的LLM上下文缩减，实证效果好。


<details>
  <summary>Details</summary>
Motivation: 解决自动编码任务中LLM上下文过大问题。

Method: 采用基于分层树的压缩方案Stingy Context和TREEFRAG exploit分解。

Result: 将239k令牌的真实源代码库缩减至11k令牌，12个前沿模型在40个实际问题上成功率达94 - 97%，优于扁平方法并缓解中间丢失效应。

Conclusion: Stingy Context方案在自动编码任务中有效，能低成本实现高成功率。

Abstract: We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results across 12 Frontier models show 94 to 97% success on 40 real-world issues at low cost, outperforming flat methods and mitigating lost-in-the-middle effects.

</details>


### [260] [Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents](https://arxiv.org/abs/2601.20412)
*Qihao Wang,Yue Hu,Mingzhe Lu,Jiayue Wu,Yanbing Liu,Yuanmin Tang*

Main category: cs.CL

TL;DR: 引入基于认知负荷理论的框架和ToolLoad - Bench基准评估大语言模型调用外部工具的能力，能精确界定模型能力边界。


<details>
  <summary>Details</summary>
Motivation: 当前基准主要报告最终准确率，无法揭示大语言模型使用外部工具时的认知瓶颈，需从简单性能评分转向诊断性评估。

Method: 引入基于认知负荷理论的框架，将任务复杂性分解为内在负荷和额外负荷，构建参数可调的认知负荷基准ToolLoad - Bench。

Result: 评估显示随着认知负荷增加模型性能有明显断崖，框架预测与实验结果高度吻合。

Conclusion: 该框架为理解智能体能力极限提供了原则性方法，为构建更高效系统奠定了实践基础。

Abstract: The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.

</details>


### [261] [SERA: Soft-Verified Efficient Repository Agents](https://arxiv.org/abs/2601.20789)
*Ethan Shen,Danny Tormoen,Saurabh Shah,Ali Farhadi,Tim Dettmers*

Main category: cs.CL

TL;DR: 介绍了高效训练编码代理的方法SERA，可快速低成本创建针对私有代码库的代理，成本低且性能好，还用于分析训练规律并发布模型等。


<details>
  <summary>Details</summary>
Motivation: 解决开放权重编码代理因训练成本和复杂性而未发挥出针对私有代码库优势的问题。

Method: 提出SERA方法，采用监督微调（SFT），使用Soft Verified Generation（SVG）从单个代码库生成大量轨迹。

Result: SERA在全开源模型中达到了最先进的结果，成本比强化学习低26倍，比之前的合成数据方法低57倍，还生成大量合成轨迹用于分析。

Conclusion: 该工作将加速开放编码代理的研究，展示了开源模型针对私有代码库的优势，并发布相关资源支持研究。

Abstract: Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.

</details>


### [262] [From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text](https://arxiv.org/abs/2601.19913)
*Shinwoo Park,Yo-Sub Han*

Main category: cs.CL

TL;DR: 研究能否将专家检测作为可学习技能，引入LREAD规则，通过三阶段实验提升韩语文本人写与LLM输出的区分准确率，发现规则辅助的专家判断可补充自动检测器。


<details>
  <summary>Details</summary>
Motivation: 解决即使语言训练有素的读者也难以区分人写韩语文本和LLM输出的问题，研究能否将专家检测作为可学习技能并改进。

Method: 引入基于韩国写作标准的LREAD规则，采用三阶段纵向盲测协议对韩语专业学生进行实验。

Result: 多数投票准确率从60%提升到100%，注释者间一致性增强（Fleiss' kappa从 -0.09 到 0.82），校准后的人类更依赖特定语言微观诊断。

Conclusion: 规则辅助的专家判断可作为非英语环境下自动检测器的可解释补充，发布完整规则和校准检测特征分类法。

Abstract: Distinguishing human-written Korean text from fluent LLM outputs remains difficult even for linguistically trained readers, who can over-trust surface well-formedness. We study whether expert detection can be treated as a learnable skill and improved through structured calibration. We introduce LREAD, a rubric derived from national Korean writing standards and adapted to target micro-level artifacts (e.g., punctuation optionality, spacing behavior, and register shifts). In a three-phase longitudinal blind protocol with Korean linguistics majors, Phase 1 measures intuition-only detection, Phase 2 enforces criterion-level scoring with explicit justifications, and Phase 3 evaluates domain-focused mastery on held-out elementary essays. Across phases, majority-vote accuracy increases from 60% to 100%, accompanied by stronger inter-annotator agreement (Fleiss' kappa: -0.09 --> 0.82). Compared to state-of-the-art LLM detectors, calibrated humans rely more on language-specific micro-diagnostics that are not well captured by coarse discourse priors. Our findings suggest that rubric-scaffolded expert judgment can serve as an interpretable complement to automated detectors for non-English settings, and we release the full rubric and a taxonomy of calibrated detection signatures.

</details>


### [263] [Modeling Next-Token Prediction as Left-Nested Intuitionistic Implication](https://arxiv.org/abs/2601.19915)
*Paul Tarau*

Main category: cs.CL

TL;DR: 介绍Arrow语言模型，它源于下一令牌预测的直觉主义逻辑解释，用左嵌套蕴含链编码前缀，证明与多种模型的关系并有实际低秩实现


<details>
  <summary>Details</summary>
Motivation: 用直觉主义逻辑解释下一令牌预测，探索新的神经架构

Method: 将前缀编码为左嵌套蕴含链，用Prolog定理证明器验证模型性质

Result: 得出与乘法RNN等价的神经架构，给出低秩神经实现并对比多种模型

Conclusion: 基于证明论解释的Arrow语言模型有一定合理性和实际应用价值，可作为Transformer等模型的替代选择

Abstract: We introduce the \emph{Arrow Language Model}, a neural architecture derived from an intuitionistic-logic interpretation of next-token prediction. Instead of representing tokens as additive embeddings mixed by attention, we encode a prefix as a \emph{left-nested implication chain} whose structure preserves order through non-commutative composition. Next-token prediction corresponds to \emph{modus ponens}, and sequence processing becomes constructive proof extension under the Curry--Howard correspondence. Our Prolog-based specialized theorem provers validate fundamental properties of the neural models, among which relations between commutative vs. non-commutative sequencing and single-token vs. multi-token prediction choices. We show that a neural architecture equivalent to multiplicative RNNs arises naturally from a proof-theoretic interpretation of next-token prediction as nested intuitionistic implication, we present a practical low-rank neural realization and position the model relative to Transformers and state-space models.
  Keywords: logic-based derivation of neural architectures, intuitionistic implicational logic, token-as-operator neural models, state-space models, alternatives to transformer-based foundational models.

</details>


### [264] [FastWhisper: Adaptive Self-knowledge Distillation for Real-time Automatic Speech Recognition](https://arxiv.org/abs/2601.19919)
*Junseok Lee,Nahoon Kim,Sangyong Lee,Chang-Jae Chun*

Main category: cs.CL

TL;DR: 提出自适应自知识蒸馏（ASKD）方法，将Whisper模型蒸馏为FastWhisper，FastWhisper在字错误率和推理时间上表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决学生模型在训练时继承教师模型缺点导致泛化能力下降的问题。

Method: 提出自适应自知识蒸馏（ASKD），动态降低对教师模型的依赖，进行自知识蒸馏，并将Whisper模型蒸馏为FastWhisper。

Result: 在训练后，FastWhisper字错误率比教师模型Whisper低1.07%，相对推理时间快5倍。

Conclusion: ASKD方法能有效提升学生模型的泛化能力，FastWhisper在性能上优于Whisper。

Abstract: Knowledge distillation is one of the most effective methods for model compression. Previous studies have focused on the student model effectively training the predictive distribution of the teacher model. However, during training, the student model may inherit the shortcomings of the teacher model, which can lead to a decline in generalization capacity. To mitigate this issue, we propose adaptive self-knowledge distillation (ASKD), which dynamically reduces the dependence of the teacher model to improve the self-training capacity, and performs the self-knowledge distillation method to improve the generalization capacity of the student model. We further distill the Whisper model into a smaller variant, called FastWhisper. In our post-training setting, FastWhisper achieved a word error rate of 1.07% lower than the teacher model Whisper, and its relative inference time was 5 times faster.

</details>


### [265] [Demystifying Multi-Agent Debate: The Role of Confidence and Diversity](https://arxiv.org/abs/2601.19921)
*Xiaochen Zhu,Caiqi Zhang,Yizhou Chi,Tom Stafford,Nigel Collier,Andreas Vlachos*

Main category: cs.CL

TL;DR: 论文指出原始多智能体辩论（MAD）存在问题，提出多样性初始化和置信度调制辩论协议两种轻量级干预方法，理论和实证均表明其能提升MAD效果。


<details>
  <summary>Details</summary>
Motivation: 原始MAD计算成本高且常不如简单多数投票，在同质智能体和统一信念更新下无法可靠改善结果，需改进。

Method: 提出多样性感知初始化，选择更多样候选答案池；提出置信度调制辩论协议，智能体表达校准置信度并据此更新。

Result: 理论上，多样性感知初始化提高MAD成功先验概率，置信度调制更新使辩论向正确假设漂移；实证上，在六个推理型问答基准上，方法优于原始MAD和多数投票。

Conclusion: 将人类审议与基于大语言模型的辩论相联系，简单且有原则的修改可显著提高辩论有效性。

Abstract: Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.

</details>


### [266] [HEART: A Unified Benchmark for Assessing Humans and LLMs in Emotional Support Dialogue](https://arxiv.org/abs/2601.19922)
*Laya Iyer,Kriti Aggarwal,Sanmi Koyejo,Gail Heyman,Desmond C. Ong,Subhabrata Mukherjee*

Main category: cs.CL

TL;DR: 提出HEART框架对比人类和大语言模型在多轮情感支持对话中的表现，发现模型和人类各有优势，评估标准有趋同趋势。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型虽进步快，但缺乏比较其与人类在人际领域能力的方法，需评估其在情感支持对话中的表现。

Method: 引入HEART框架，对对话历史的人类和模型回复配对，通过盲测人类评分者和大语言模型评判器集合评估，按人际沟通科学的五个维度评分。

Result: 部分前沿模型在共情和一致性上接近或超越人类平均水平，人类在对抗回合的适应性重构等方面有优势，人类和大语言模型评判器偏好一致性达80%。

Conclusion: HEART框架将支持性对话作为独立能力轴，为理解模型支持与人类社会判断的异同及能力随模型规模的变化提供统一实证基础。

Abstract: Supportive conversation depends on skills that go beyond language fluency, including reading emotions, adjusting tone, and navigating moments of resistance, frustration, or distress. Despite rapid progress in language models, we still lack a clear way to understand how their abilities in these interpersonal domains compare to those of humans. We introduce HEART, the first-ever framework that directly compares humans and LLMs on the same multi-turn emotional-support conversations. For each dialogue history, we pair human and model responses and evaluate them through blinded human raters and an ensemble of LLM-as-judge evaluators. All assessments follow a rubric grounded in interpersonal communication science across five dimensions: Human Alignment, Empathic Responsiveness, Attunement, Resonance, and Task-Following. HEART uncovers striking behavioral patterns. Several frontier models approach or surpass the average human responses in perceived empathy and consistency. At the same time, humans maintain advantages in adaptive reframing, tension-naming, and nuanced tone shifts, particularly in adversarial turns. Human and LLM-as-judge preferences align on about 80 percent of pairwise comparisons, matching inter-human agreement, and their written rationales emphasize similar HEART dimensions. This pattern suggests an emerging convergence in the criteria used to assess supportive quality. By placing humans and models on equal footing, HEART reframes supportive dialogue as a distinct capability axis, separable from general reasoning or linguistic fluency. It provides a unified empirical foundation for understanding where model-generated support aligns with human social judgment, where it diverges, and how affective conversational competence scales with model size.

</details>


### [267] [Table-BiEval: A Self-Supervised, Dual-Track Framework for Decoupling Structure and Content in LLM Evaluation](https://arxiv.org/abs/2601.19923)
*Boxiang Zhao,Qince Li,Zhonghao Wang,Zelin Cao,Yi Wang,Peng Cheng,Bo Lin*

Main category: cs.CL

TL;DR: 本文提出Table - BiEval方法评估大语言模型将自然语言转化为结构化格式的性能，评估15个模型，揭示性能差异和架构瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前评估缺乏有效方法衡量大语言模型结构保真度，传统文本指标无法检测类代码输出的语义漂移。

Method: 提出基于无人工、自监督评估框架的Table - BiEval方法，利用确定性中间表示计算内容语义准确性和归一化树编辑距离，从内容中分离结构，并从层次结构和平表两个拓扑维度评估模型。

Result: 评估结果显示不同模型性能有显著差异，中型模型在结构效率上可能优于大型模型，深度递归嵌套是当前架构的普遍瓶颈。

Conclusion: Table - BiEval可有效定量评估大语言模型性能，揭示了当前大语言模型在结构处理上的特点和问题。

Abstract: As Large Language Models (LLMs) evolve into autonomous agents, the capability to faithfully translate natural language into rigorous structured formats-essential for tool invocation-and to convert complex tabular information into machine-readable specifications has become paramount. However, current evaluations lack effective methodologies to measure this structural fidelity without costly human intervention, as traditional text metrics fail to detect semantic drift in code-like outputs. This paper proposes Table-BiEval, a novel approach based on a human-free, self-supervised evaluation framework, to assess LLMs performance quantitatively. By leveraging deterministic Intermediate Representations, our framework calculates Content Semantic Accuracy and Normalized Tree Edit Distance to decouple structure from content. Also, it empirically evaluates 15 state-of-the-art LLMs across dual topological dimensions-hierarchical structures and flat tables. The results reveal substantial variability, highlighting that mid-sized models can surprisingly outperform larger counterparts in structural efficiency and confirming that deep recursive nesting remains a universal bottleneck for current architectures.

</details>


### [268] [OPT-Engine: Benchmarking the Limits of LLMs in Optimization Modeling via Complexity Scaling](https://arxiv.org/abs/2601.19924)
*Yitian Chen,Cheng Cheng,Yinan Sun,Zi Ling,Dongdong Ge*

Main category: cs.CL

TL;DR: 本文提出OPT - ENGINE框架评估大语言模型优化建模能力，发现工具集成推理更具鲁棒性，自动约束公式化是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型在自动化公式化和解决复杂现实优化问题的能力边界了解不足，需进行评估研究。

Method: 提出OPT - ENGINE可扩展基准框架，涵盖10个规范任务，利用该框架研究大语言模型推理能力，探讨两个关键问题。

Result: 工具集成推理在任务复杂度增加时更具鲁棒性，纯文本推理有上限；自动约束公式化是主要性能瓶颈。

Conclusion: 研究结果为开发用于高级优化的下一代大语言模型提供了可行指导。

Abstract: Large Language Models (LLMs) have demonstrated impressive progress in optimization modeling, fostering a rapid expansion of new methodologies and evaluation benchmarks. However, the boundaries of their capabilities in automated formulation and problem solving remain poorly understood, particularly when extending to complex, real-world tasks. To bridge this gap, we propose OPT-ENGINE, an extensible benchmark framework designed to evaluate LLMs on optimization modeling with controllable and scalable difficulty levels. OPT-ENGINE spans 10 canonical tasks across operations research, with five Linear Programming and five Mixed-Integer Programming. Utilizing OPT-ENGINE, we conduct an extensive study of LLMs' reasoning capabilities, addressing two critical questions: 1.) Do LLMs' performance remain robust when generalizing to out-of-distribution optimization tasks that scale in complexity beyond current benchmark levels? and 2.) At what stage, from problem interpretation to solution generation, do current LLMs encounter the most significant bottlenecks? Our empirical results yield two key insights: first, tool-integrated reasoning with external solvers exhibits significantly higher robustness as task complexity escalates, while pure-text reasoning reaches a ceiling; second, the automated formulation of constraints constitutes the primary performance bottleneck. These findings provide actionable guidance for developing next-generation LLMs for advanced optimization. Our code is publicly available at \textcolor{blue}{https://github.com/Cardinal-Operations/OPTEngine}.

</details>


### [269] [Evaluating Large Language Models for Abstract Evaluation Tasks: An Empirical Study](https://arxiv.org/abs/2601.19925)
*Yinuo Liu,Emre Sezgin,Eric A. Youngstrom*

Main category: cs.CL

TL;DR: 研究对比ChatGPT - 5、Gemini - 3 - Pro和Claude - Sonnet - 4.5评估摘要的一致性和可靠性，发现大语言模型与人类评审有一定一致性，在主观维度表现较弱。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在辅助科学评审方面的潜力，研究其评估复杂学术内容的可行性。

Method: 用同一评分标准让人类评审和三个大语言模型对160篇摘要评分，计算类内相关系数，用Bland - Altman图分析。

Result: 大语言模型相互间一致性良好到优秀；ChatGPT和Claude与人类评审在整体质量和内容特定标准上中度一致，主观维度上公平一致；Gemini部分标准公平一致；三个模型与人类平均综合得分差异可接受。

Conclusion: 大语言模型能批量处理摘要，与人类专家在整体质量和客观标准上有中度一致；在主观维度表现弱，应起补充作用，人类专业知识仍至关重要。

Abstract: Introduction: Large language models (LLMs) can process requests and generate texts, but their feasibility for assessing complex academic content needs further investigation. To explore LLM's potential in assisting scientific review, this study examined ChatGPT-5, Gemini-3-Pro, and Claude-Sonnet-4.5's consistency and reliability in evaluating abstracts compared to one another and to human reviewers. Methods: 160 abstracts from a local conference were graded by human reviewers and three LLMs using one rubric. Composite score distributions across three LLMs and fourteen reviewers were examined. Inter-rater reliability was calculated using intraclass correlation coefficients (ICCs) for within-AI reliability and AI-human concordance. Bland-Altman plots were examined for visual agreement patterns and systematic bias. Results: LLMs achieved good-to-excellent agreement with each other (ICCs: 0.59-0.87). ChatGPT and Claude reached moderate agreement with human reviewers on overall quality and content-specific criteria, with ICCs ~.45-.60 for composite, impression, clarity, objective, and results. They exhibited fair agreement on subjective dimensions, with ICC ranging from 0.23-0.38 for impact, engagement, and applicability. Gemini showed fair agreement on half criteria and no reliability on impact and applicability. Three LLMs showed acceptable or negligible mean difference (ChatGPT=0.24, Gemini=0.42, Claude=-0.02) from the human mean composite scores. Discussion: LLMs could process abstracts in batches with moderate agreement with human experts on overall quality and objective criteria. With appropriate process architecture, they can apply a rubric consistently across volumes of abstracts exceeding feasibility for a human rater. The weaker performance on subjective dimensions indicates that AI should serve a complementary role in evaluation, while human expertise remains essential.

</details>


### [270] [The Grammar of Transformers: A Systematic Review of Interpretability Research on Syntactic Knowledge in Language Models](https://arxiv.org/abs/2601.19926)
*Nora Graichen,Iria de-Dios-Flores,Gemma Boleda*

Main category: cs.CL

TL;DR: 对337篇评估基于Transformer的语言模型句法能力的文章进行系统综述，分析现有研究现状并给出未来工作建议。


<details>
  <summary>Details</summary>
Motivation: 评估基于Transformer的语言模型的句法能力，了解当前研究的现状和问题。

Method: 对337篇文章进行系统综述，分析1015个模型结果。

Result: 现有研究方法和数据多样，但过度关注英语、BERT模型和易研究的现象；模型能较好捕捉形式导向现象，在句法 - 语义接口现象上表现较弱。

Conclusion: 为未来工作提供建议，如报告完整数据、统一理论和方法、增加机制方法使用、拓宽研究范围。

Abstract: We present a systematic review of 337 articles evaluating the syntactic abilities of Transformer-based language models, reporting on 1,015 model results from a range of syntactic phenomena and interpretability methods. Our analysis shows that the state of the art presents a healthy variety of methods and data, but an over-focus on a single language (English), a single model (BERT), and phenomena that are easy to get at (like part of speech and agreement). Results also suggest that TLMs capture these form-oriented phenomena well, but show more variable and weaker performance on phenomena at the syntax-semantics interface, like binding or filler-gap dependencies. We provide recommendations for future work, in particular reporting complete data, better aligning theoretical constructs and methods across studies, increasing the use of mechanistic methods, and broadening the empirical scope regarding languages and linguistic phenomena.

</details>


### [271] [SDUs DAISY: A Benchmark for Danish Culture](https://arxiv.org/abs/2601.19930)
*Jacob Nielsen,Stine L. Beltoft,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: 介绍基于丹麦文化经典2006创建的丹麦文化基准Daisy，生成问答对并人工审核形成含741对问答的数据集。


<details>
  <summary>Details</summary>
Motivation: 创建一个新的丹麦文化基准。

Method: 从丹麦文化经典中选取主题，查询对应维基页面，用语言模型生成问题，人工审核问答对。

Result: 得到包含741个封闭式问答对的数据集，涵盖从公元前1300年考古发现到当代流行音乐等多方面主题。

Conclusion: 成功构建基于丹麦文化遗产的新基准Daisy及相关问答数据集。

Abstract: We introduce a new benchmark for Danish culture via cultural heritage, Daisy, based on the curated topics from the Danish Culture Canon 2006. For each artifact in the culture canon, we query the corresponding Wikipedia page and have a language model generate random questions. This yields a sampling strategy within each work, with a mix of central of peripheral questions for each work, not only knowledge of mainstream information, but also in-depth cornerstones defining the heritage of Danish Culture, defined by the Canon committee. Each question-answer pair is humanly approved or corrected in the final dataset consisting of 741 close-ended question answer pairs covering topics, from 1300 BC. archaeological findings, 1700 century poems and musicals pieces to contemporary pop music and Danish design and architecture.

</details>


### [272] [Text-to-State Mapping for Non-Resolution Reasoning: The Contradiction-Preservation Principle](https://arxiv.org/abs/2601.19933)
*Kei Saito*

Main category: cs.CL

TL;DR: 本文引入文本到状态的映射函数φ，形式化矛盾保留原则，用大语言模型开发提取协议，经68个测试句子验证有效，为非分辨率推理提供算法桥梁。


<details>
  <summary>Details</summary>
Motivation: 非分辨率推理（NRR）基础架构已建立，但自然语言如何映射到其数学结构的问题未解决。

Method: 引入文本到状态映射函数φ，形式化矛盾保留原则，利用现有大语言模型开发提取协议。

Result: 对68个测试句子的实证验证表明，该映射对歧义输入实现平均香农熵H(S)=1.087比特，而基线单解释方法为H(S)=0.000。

Conclusion: 该框架为原始文本和NRR算子作用的形式状态空间之间提供了缺失的算法桥梁，可推迟语言模型推理中的架构崩溃。

Abstract: Non-Resolution Reasoning (NRR) provides a formal framework for maintaining semantic ambiguity rather than forcing premature interpretation collapse. While the foundational architecture establishes state spaces and operators for ambiguity-preserving computation, the critical question of how natural language maps to these mathematical structures remains open. This paper introduces the text-to-state mapping function φ that transforms linguistic input into superposition states within the NRR framework. We formalize the Contradiction-Preservation Principle, which requires that genuinely ambiguous expressions maintain non-zero entropy in their state representations, and develop extraction protocols using existing Large Language Models as interpretation generators. Empirical validation across 68 test sentences spanning lexical, structural, and pragmatic ambiguity demonstrates that our mapping achieves mean Shannon entropy H(S) = 1.087 bits for ambiguous inputs while baseline single-interpretation approaches yield H(S) = 0.000. The framework provides the missing algorithmic bridge between raw text and the formal state spaces on which NRR operators act, enabling architectural collapse deferment in language model inference.

</details>


### [273] [Quantifying non deterministic drift in large language models](https://arxiv.org/abs/2601.19934)
*Claire Nicholson*

Main category: cs.CL

TL;DR: 论文通过实验量化大语言模型基础行为漂移，展示不同条件下漂移情况，为后续减轻和控制漂移方法提供参考。


<details>
  <summary>Details</summary>
Motivation: 实际中相同提示输入大语言模型不总产生相同输出，需量化基础行为漂移。

Method: 对gpt - 4o - mini和llama3.1 - 8b两个模型在五种提示类别下，用精确重复、扰动输入和重用模式进行实验，在温度0.0和0.7时测量漂移。

Result: 即使温度为0.0，模型仍有非确定性，不同模型大小、部署和提示类型有不同的变异性模式。

Conclusion: 研究为评估未来的漂移缓解和控制方法提供了参考点。

Abstract: Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, and reuse modes at temperatures of 0.0 and 0.7. Drift is measured using unique output fractions, lexical similarity, and word count statistics, enabling direct comparison across models, prompting modes, and deployment types. The results show that nondeterminism persists even at temperature 0.0, with distinct variability patterns by model size, deployment, and prompt type. We situate these findings within existing work on concept drift, behavioural drift, and infrastructure-induced nondeterminism, discuss the limitations of lexical metrics, and highlight emerging semantic approaches. By establishing a systematic empirical baseline in the absence of stabilisation techniques, this study provides a reference point for evaluating future drift mitigation and control methods.

</details>


### [274] [Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents](https://arxiv.org/abs/2601.19935)
*Yiting Shen,Kun Li,Wei Zhou,Songlin Hu*

Main category: cs.CL

TL;DR: 现有基准无法评估智能体主动应用记忆执行任务的能力，本文引入Mem2ActBench基准，构建数据集生成任务，实验表明当前系统在主动利用记忆进行参数接地方面不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要测试智能体被动检索孤立事实的能力，无法评估主动应用记忆执行任务的关键能力，需要新基准填补这一空白。

Method: 引入Mem2ActBench基准，用自动化管道合并异构数据源构建数据集，通过一致性建模解决冲突，用反向生成方法生成工具使用任务。

Result: 生成400个工具使用任务，91.3%强烈依赖记忆，对七个记忆框架的实验显示当前系统在主动利用记忆进行参数接地方面不足。

Conclusion: 当前系统在主动利用记忆执行任务方面存在不足，需要更有效的方法来评估和改进记忆应用。

Abstract: Large Language Model (LLM)-based agents are increasingly deployed for complex, tool-based tasks where long-term memory is critical to driving actions. Existing benchmarks, however, primarily test a angent's ability to passively retrieve isolated facts in response to explicit questions. They fail to evaluate the more crucial capability of actively applying memory to execute tasks. To address this gap, we introduce \textsc{Mem2ActBench}, a benchmark for evaluating whether agents can proactively leverage long-term memory to execute tool-based actions by selecting appropriate tools and grounding their parameters. The benchmark simulates persistent assistant usage, where users mention the same topic across long, interrupted interactions and expect previously established preferences and task states to be implicitly applied. We build the dataset with an automated pipeline that merges heterogeneous sources (ToolACE, BFCL, Oasst1), resolves conflicts via consistency modeling, and synthesizes 2,029 sessions with 12 user--assistant--tool turns on average. From these memory chains, a reverse-generation method produces 400 tool-use tasks, with human evaluation confirming 91.3\% are strongly memory-dependent. Experiments on seven memory frameworks show that current systems remain inadequate at actively utilizing memory for parameter grounding, highlighting the need for more effective approaches to evaluate and improve memory application in task execution.

</details>


### [275] [Benchmarking von ASR-Modellen im deutschen medizinischen Kontext: Eine Leistungsanalyse anhand von Anamnesegesprächen](https://arxiv.org/abs/2601.19945)
*Thomas Schuster,Julius Trögele,Nico Döring,Robin Krüger,Matthieu Hoffmann,Holger Friedrich*

Main category: cs.CL

TL;DR: 文章提出德语医疗模拟医患对话数据集，评估29个ASR模型，结果显示模型性能差异大。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别可减轻医疗人员工作量，但缺乏德语医疗场景尤其是含方言的评估基准。

Method: 提出模拟医患对话数据集，评估29个不同ASR模型，用WER、CER、BLEU三种指标评估并展望语义分析。

Result: 不同模型性能差异显著，最佳系统WER部分低于3%，部分模型在医疗术语和方言变体上错误率高。

Conclusion: 未明确提及，但暗示需进一步研究以提升德语医疗ASR模型性能。

Abstract: Automatic Speech Recognition (ASR) offers significant potential to reduce the workload of medical personnel, for example, through the automation of documentation tasks. While numerous benchmarks exist for the English language, specific evaluations for the German-speaking medical context are still lacking, particularly regarding the inclusion of dialects. In this article, we present a curated dataset of simulated doctor-patient conversations and evaluate a total of 29 different ASR models. The test field encompasses both open-weights models from the Whisper, Voxtral, and Wav2Vec2 families as well as commercial state-of-the-art APIs (AssemblyAI, Deepgram). For evaluation, we utilize three different metrics (WER, CER, BLEU) and provide an outlook on qualitative semantic analysis. The results demonstrate significant performance differences between the models: while the best systems already achieve very good Word Error Rates (WER) of partly below 3%, the error rates of other models, especially concerning medical terminology or dialect-influenced variations, are considerably higher.

</details>


### [276] [On the Effectiveness of LLM-Specific Fine-Tuning for Detecting AI-Generated Text](https://arxiv.org/abs/2601.20006)
*Michał Gromadzki,Anna Wróblewska,Agnieszka Kaliska*

Main category: cs.CL

TL;DR: 本文基于大规模语料库和新训练策略对AI生成文本检测进行研究，开发评估多种检测模型，提出新训练范式，最佳微调检测器表现出色。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展使文本生成接近人类写作，给教育、出版和数字安全的真实性验证带来挑战，检测AI生成文本成为重要技术和伦理问题。

Method: 引入人类撰写文本和AI生成文本的大规模语料库，开发评估多种检测模型，提出Per LLM和Per LLM family fine - tuning两种新训练范式。

Result: 在涵盖21个大语言模型的1亿token基准测试中，最佳微调检测器实现了高达99.6%的token级准确率，大幅超越现有开源基线。

Conclusion: 基于大规模语料库和新训练策略的AI生成文本检测方法有效，能大幅提高检测准确性。

Abstract: The rapid progress of large language models has enabled the generation of text that closely resembles human writing, creating challenges for authenticity verification in education, publishing, and digital security. Detecting AI-generated text has therefore become a crucial technical and ethical issue. This paper presents a comprehensive study of AI-generated text detection based on large-scale corpora and novel training strategies. We introduce a 1-billion-token corpus of human-authored texts spanning multiple genres and a 1.9-billion-token corpus of AI-generated texts produced by prompting a variety of LLMs across diverse domains. Using these resources, we develop and evaluate numerous detection models and propose two novel training paradigms: Per LLM and Per LLM family fine-tuning. Across a 100-million-token benchmark covering 21 large language models, our best fine-tuned detector achieves up to $99.6\%$ token-level accuracy, substantially outperforming existing open-source baselines.

</details>


### [277] [LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?](https://arxiv.org/abs/2601.20009)
*J. Ben Tamo,Daniel Carlander-Reuterfelt,Jonathan Rubin,Dezhi Hong,Mingxian Wang,Oleg Poliannikov*

Main category: cs.CL

TL;DR: 本文指出大语言模型处理非英语任务时语言控制的两个瓶颈，设计评估方案，用扩展方法分析，揭示内部三层结构，提出仅微调最终层的方法，在多语言上实现高语言一致性且节省资源。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在非英语任务中语言控制能力不足的问题。

Method: 设计四场景评估协议，扩展logit lens分析；提出仅微调负责语言控制的最终层的方法。

Result: 在Qwen - 3 - 32B和Bloom - 7.1B上，微调3 - 5%的参数，六种语言的语言一致性超98%，且不损失任务准确率。

Conclusion: 这是首个利用语言控制层定位进行高效多语言适配的方法，能在节省资源的情况下达到与全范围微调相近的效果。

Abstract: Despite multilingual pretraining, large language models often struggle with non-English tasks, particularly in language control, the ability to respond in the intended language. We identify and characterize two key failure modes: the multilingual transfer bottleneck (correct language, incorrect task response) and the language consistency bottleneck (correct task response, wrong language). To systematically surface these issues, we design a four-scenario evaluation protocol spanning MMLU, MGSM, and XQuAD benchmarks. To probe these issues with interpretability, we extend logit lens analysis to track language probabilities layer by layer and compute cross-lingual semantic similarity of hidden states. The results reveal a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Guided by these insights, we introduce selective fine-tuning of only the final layers responsible for language control. On Qwen-3-32B and Bloom-7.1B, this method achieves over 98 percent language consistency across six languages while fine-tuning only 3-5 percent of parameters, without sacrificing task accuracy. Importantly, this result is nearly identical to that of full-scope fine-tuning (for example, above 98 percent language consistency for both methods across all prompt scenarios) but uses a fraction of the computational resources. To the best of our knowledge, this is the first approach to leverage layer-localization of language control for efficient multilingual adaptation.

</details>


### [278] [VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.20055)
*Vikash Singh,Darion Cassel,Nathaniel Weir,Nick Feng,Sam Bayless*

Main category: cs.CL

TL;DR: 提出一种神经符号框架，结合大语言模型和SMT求解器，通过迭代优化生成验证引导的答案，在推理基准测试中表现提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽语法流畅，但在高风险领域确保逻辑正确性仍是挑战。

Method: 将大语言模型输出分解为原子声明，自动形式化为一阶逻辑，用自动定理证明验证逻辑一致性；引入多模型共识、语义路由、精确逻辑错误定位三项创新；对声明分类，聚合验证信号为统一分数，迭代优化答案。

Result: 使用GPT - OSS - 120B模型，VERGE在推理基准测试中相比单遍方法收敛时平均性能提升18.7%。

Conclusion: 该混合方法在可能情况下提供形式保证，在其他情况下进行共识验证，推动可信AI发展。

Abstract: Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.

</details>


### [279] [Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models](https://arxiv.org/abs/2601.20126)
*Abha Jha,Akanksha Mahajan,Ashwath Vaithinathan Aravindan,Praveen Saravanan,Sai Sailaja Policharla,Sonal Chaturbhuj Gehlot*

Main category: cs.CL

TL;DR: 研究用可验证奖励的强化学习（RLVR）减少大语言模型幻觉内容，通过不同弃权奖励结构微调模型评估，结果显示适量弃权奖励可减少错误回应，结合监督微调有一定效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生幻觉或无法验证的内容，影响在事实领域的可靠性，需新训练范式解决。

Method: 以可验证奖励的强化学习（RLVR）为训练范式，对Granite - 3.3 - 2B - Instruct和Qwen - 3 - 4B - Instruct在MedMCQA和Hendrycks Math基准上进行微调评估，采用三元奖励结构，研究结合监督微调的效果。

Result: 适中弃权奖励（r_abs ≈ -0.25到0.3）在选择题任务中减少错误回应且不严重降低准确率，大模型对弃权激励更鲁棒，开放式问答中存在探索不足问题，可通过监督弃权训练部分缓解。

Conclusion: 可验证奖励设计是缓解语言模型幻觉的可行且灵活的实用方法。

Abstract: Large Language Models (LLMs) often produce hallucinated or unverifiable content, undermining their reliability in factual domains. This work investigates Reinforcement Learning with Verifiable Rewards (RLVR) as a training paradigm that explicitly rewards abstention ("I don't know") alongside correctness to promote intellectual humility. We fine-tune and evaluate Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on the MedMCQA and Hendrycks Math benchmarks using a ternary reward structure ($-1$, r_abs, 1) under varying abstention reward structures. We further study the effect of combining RLVR with supervised fine-tuning strategies that teach abstention prior to reinforcement learning. Our results show that moderate abstention rewards (r_abs $\approx -0.25$ to 0.3) consistently reduce incorrect responses without severe accuracy degradation on multiple-choice tasks, with larger models exhibiting greater robustness to abstention incentives. On open-ended question answering, we observe limitations due to insufficient exploration, which can be partially mitigated through supervised abstention training. Overall, these findings demonstrate the feasibility and flexibility of verifiable reward design as a practical approach for hallucination mitigation in language models. Reproducible code for our abstention training framework is available here https://github.com/Mystic-Slice/rl-abstention.

</details>


### [280] [BengaliSent140: A Large-Scale Bengali Binary Sentiment Dataset for Hate and Non-Hate Speech Classification](https://arxiv.org/abs/2601.20129)
*Akif Islam,Sujan Kumar Roy,Md. Ekramul Hamid*

Main category: cs.CL

TL;DR: 本文介绍了大规模孟加拉语二元情感数据集BengaliSent140，它整合了七个现有数据集，具有平衡的类别分布，可用于训练和基准测试深度学习模型，且数据集公开可用。


<details>
  <summary>Details</summary>
Motivation: 现有孟加拉语情感和仇恨言论数据集规模小或局限于单一领域，无法满足现代深度学习模型对大量异质数据的需求。

Method: 整合七个现有孟加拉语文本数据集，将异构注释方案统一为二元情感表述（非仇恨和仇恨）。

Result: 构建了包含139,792个独特文本样本的BengaliSent140数据集，类别分布相对平衡，并给出基准实验结果。

Conclusion: BengaliSent140覆盖范围更广，为深度学习模型提供了强大基础，具有实际可用性，且已公开。

Abstract: Sentiment analysis for the Bengali language has attracted increasing research interest in recent years. However, progress remains constrained by the scarcity of large-scale and diverse annotated datasets. Although several Bengali sentiment and hate speech datasets are publicly available, most are limited in size or confined to a single domain, such as social media comments. Consequently, these resources are often insufficient for training modern deep learning based models, which require large volumes of heterogeneous data to learn robust and generalizable representations. In this work, we introduce BengaliSent140, a large-scale Bengali binary sentiment dataset constructed by consolidating seven existing Bengali text datasets into a unified corpus. To ensure consistency across sources, heterogeneous annotation schemes are systematically harmonized into a binary sentiment formulation with two classes: Not Hate (0) and Hate (1). The resulting dataset comprises 139,792 unique text samples, including 68,548 hate and 71,244 not-hate instances, yielding a relatively balanced class distribution. By integrating data from multiple sources and domains, BengaliSent140 offers broader linguistic and contextual coverage than existing Bengali sentiment datasets and provides a strong foundation for training and benchmarking deep learning models. Baseline experimental results are also reported to demonstrate the practical usability of the dataset. The dataset is publicly available at https://www.kaggle.com/datasets/akifislam/bengalisent140/

</details>


### [281] [Automated Benchmark Generation from Domain Guidelines Informed by Bloom's Taxonomy](https://arxiv.org/abs/2601.20253)
*Si Chen,Le Huy Khiem,Annalisa Szymanski,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 提出从专家指南自动生成基准的框架，应用于三个领域评估大语言模型，发现模型与人类推理差异。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型基准依赖人类考试数据集，在实践领域常不可用，需评估模型在这些领域的情境推理能力。

Method: 引入基于布卢姆分类法从专家指南自动生成基准的框架，将专家实践转化为场景，并扩展成自动评分的选择题和多轮对话。

Result: 应用于三个领域，发现大语言模型在高阶推理表现较好，低阶项目失败更多。

Conclusion: 生成的大规模基准能揭示模型行为，可用于现实场景中的情境推理评估。

Abstract: Open-ended question answering (QA) evaluates a model's ability to perform contextualized reasoning beyond factual recall. This challenge is especially acute in practice-based domains, where knowledge is procedural and grounded in professional judgment, while most existing LLM benchmarks depend on pre-existing human exam datasets that are often unavailable in such settings. We introduce a framework for automated benchmark generation from expert-authored guidelines informed by Bloom's Taxonomy. It converts expert practices into implicit violation-based scenarios and expands them into auto-graded multiple-choice questions (MCQs) and multi-turn dialogues across four cognitive levels, enabling deterministic, reproducible, and scalable evaluation. Applied to three applied domains: teaching, dietetics, and caregiving, we find differences between model and human-like reasoning: LLMs sometimes perform relatively better on higher-order reasoning (Analyze) but fail more frequently on lower-level items (Remember). We produce large-scale, psychometrically informed benchmarks that surface these non-intuitive model behaviors and enable evaluation of contextualized reasoning in real-world settings.

</details>


### [282] [Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale](https://arxiv.org/abs/2601.20276)
*Tianwei Lin,Zuyi Zhou,Xinda Zhao,Chenke Wang,Xiaohong Li,Yu Chen,Chuanrui Hu,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: 提出对抗性NIAH风格基准EMB - S及解耦诊断协议，发现语义辨别是长上下文记忆瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有NIAH评估多为良性跨度定位，无法满足长上下文大语言模型代理准确获取和使用证据的需求。

Method: 构建基于326M令牌MemoryBank的EMB - S基准，提出解耦诊断协议。

Result: 在不同规模参考语料库上，系统在良性NIAH饱和但在语义干扰下证据访问能力急剧下降。

Conclusion: 语义辨别而非上下文长度是大规模长上下文记忆的主要瓶颈。

Abstract: Long-context LLM agents must access the right evidence from large environments and use it faithfully. However, the popular Needle-in-a-Haystack (NIAH) evaluation mostly measures benign span localization. The needle is near-unique, and the haystack is largely irrelevant. We introduce EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark built on a 326M-token MemoryBank. While the full MemoryBank spans 326M tokens for retrieval-based (RAG) evaluation, we evaluate native long-context models only at scales that fit within each model's context window (up to 1M tokens in this work) to ensure a fair comparison. EMB-S pairs queries with collision-tested near-miss hard negatives and gold evidence sets spanning one or more documents, validated via human screening and LLM verification. We also propose a decoupled diagnostic protocol that reports evidence access (document-ID localization) separately from end-to-end QA quality under full-context prompting. This enables consistent diagnosis for both native long-context prompting and retrieval pipelines. Across a reference-corpus ladder from domain-isolated 64K contexts to a globally shared 326M-token environment, we observe a clear reality gap. Systems that saturate benign NIAH degrade sharply in evidence access under semantic interference. These results indicate that semantic discrimination, not context length alone, is the dominant bottleneck for long-context memory at scale.

</details>


### [283] [Beyond Speedup -- Utilizing KV Cache for Sampling and Reasoning](https://arxiv.org/abs/2601.20326)
*Zeyu Xing,Xing Li,Hui-Ling Zhen,Mingxuan Yuan,Sinno Jialin Pan*

Main category: cs.CL

TL;DR: 提出将KV缓存作为轻量级表示，用于下游任务，在两个关键应用中表现良好，为大语言模型推理的表示重用开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 利用KV缓存中可免费重用的上下文信息，避免重新计算或存储完整隐藏状态。

Method: 将KV缓存视为轻量级表示以替代全隐藏状态计算和存储。

Result: 在Chain - of - Embedding中，在Llama - 3.1 - 8B - Instruct和Qwen2 - 7B - Instruct上表现有竞争力或更优；在Fast/Slow Thinking Switching中，在Qwen3 - 8B和DeepSeek - R1 - Distil - Qwen - 14B上减少达5.7倍的token生成且精度损失小。

Conclusion: KV缓存是采样和推理的免费有效基础，为大语言模型推理的表示重用提供新方向。

Abstract: KV caches, typically used only to speed up autoregressive decoding, encode contextual information that can be reused for downstream tasks at no extra cost. We propose treating the KV cache as a lightweight representation, eliminating the need to recompute or store full hidden states. Despite being weaker than dedicated embeddings, KV-derived representations are shown to be sufficient for two key applications: \textbf{(i) Chain-of-Embedding}, where they achieve competitive or superior performance on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct; and \textbf{(ii) Fast/Slow Thinking Switching}, where they enable adaptive reasoning on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, reducing token generation by up to $5.7\times$ with minimal accuracy loss. Our findings establish KV caches as a free, effective substrate for sampling and reasoning, opening new directions for representation reuse in LLM inference. Code: https://github.com/cmd2001/ICLR2026_KV-Embedding.

</details>


### [284] [MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment](https://arxiv.org/abs/2601.20335)
*Qinzhuo Wu,Zhizhuo Yang,Hanhao Li,Pengzhi Gao,Wei Liu,Jian Luan*

Main category: cs.CL

TL;DR: 提出在线基准测试 MobileBench - OL 评估移动 GUI 代理，评估 12 个领先代理显示有改进空间，人工评估确认其可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有在线基准测试专注代理任务指令遵循能力，忽视推理和探索能力，且未考虑现实环境随机噪声，存在与现实环境的差距。

Method: 提出包含 80 个中国应用 1080 个任务的 MobileBench - OL，设 5 个子集多维度评估，提供带重置机制的自动评估框架。

Result: 评估 12 个领先 GUI 代理显示有满足现实需求的改进空间，人工评估确认 MobileBench - OL 能可靠衡量性能。

Conclusion: MobileBench - OL 可用于稳定、可重复的现实环境基准测试，数据和代码待接受后发布。

Abstract: Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.

</details>


### [285] [Harnessing Large Language Models for Precision Querying and Retrieval-Augmented Knowledge Extraction in Clinical Data Science](https://arxiv.org/abs/2601.20674)
*Juan Jose Rubio Jan,Jack Wu,Julia Ive*

Main category: cs.CL

TL;DR: 研究将大语言模型应用于电子健康记录数据科学任务，提出评估框架并实验，证明其在临床工作流中有潜力。


<details>
  <summary>Details</summary>
Motivation: 测试大语言模型在结构化数据查询和非结构化临床文本信息提取任务中的能力。

Method: 提出灵活评估框架，在MIMIC III子集上用本地和基于API的大语言模型进行实验，结合多种评估指标。

Result: 通过实验发现大语言模型在临床工作流中能支持精确查询和准确信息提取。

Conclusion: 大语言模型在临床工作流的结构化数据查询和非结构化文本信息提取方面有潜力。

Abstract: This study applies Large Language Models (LLMs) to two foundational Electronic Health Record (EHR) data science tasks: structured data querying (using programmatic languages, Python/Pandas) and information extraction from unstructured clinical text via a Retrieval Augmented Generation (RAG) pipeline. We test the ability of LLMs to interact accurately with large structured datasets for analytics and the reliability of LLMs in extracting semantically correct information from free text health records when supported by RAG. To this end, we presented a flexible evaluation framework that automatically generates synthetic question and answer pairs tailored to the characteristics of each dataset or task. Experiments were conducted on a curated subset of MIMIC III, (four structured tables and one clinical note type), using a mix of locally hosted and API-based LLMs. Evaluation combined exact-match metrics, semantic similarity, and human judgment. Our findings demonstrate the potential of LLMs to support precise querying and accurate information extraction in clinical workflows.

</details>


### [286] [QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks](https://arxiv.org/abs/2601.20731)
*Mae Sosto,Delfina Sol Martinez Pandiani,Laura Hollink*

Main category: cs.CL

TL;DR: 研究大语言模型如何再现社会规范及产生文本生成偏差，不同模型对不同类别主体的影响有差异，偏差形式和程度依赖模型特性。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型如何再现社会规范（特别是异性恋和顺性别规范）及文本生成中的可衡量偏差。

Method: 研究主体性别或性取向信息对不同类别主体上大语言模型响应的影响，以英文句子完成度在四个维度上的差异衡量表征不平衡。

Result: 掩码语言模型对酷儿标记主体产出最不利情感等；自回归语言模型部分缓解；闭源自回归模型对未标记主体产出更有害结果。

Conclusion: 大语言模型再现社会规范假设，偏差形式和程度依赖模型特性，且偏差只是重新分配而非消除。

Abstract: This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized "unmarked" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.

</details>


### [287] [PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments](https://arxiv.org/abs/2601.20330)
*Zhuang Chen,Dazhen Wan,Zhangkai Zheng,Guanqun Bi,Xiyao Xiao,Binghang Li,Minlie Huang*

Main category: cs.CL

TL;DR: 评估大语言模型在心理健康治疗能力时存在挑战，提出Ps框架解决问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在心理健康治疗有潜力，但因心理咨询非结构化和长期性，现有评估范式有缺陷，导致过程和标准不稳定。

Method: 引入Ps统一框架，通过轨迹锚定锦标赛评估，在模拟中锚定交互轨迹，在判断中锚定战斗轨迹，将锦标赛轨迹转化为奖励信号进行强化学习。

Result: 广泛实验验证了Ps的有效性，且与人类专家判断高度一致。

Conclusion: Ps框架能有效评估大语言模型的治疗能力并提升其性能。

Abstract: While large language models show promise in mental healthcare, evaluating their therapeutic competence remains challenging due to the unstructured and longitudinal nature of counseling. We argue that current evaluation paradigms suffer from an unanchored defect, leading to two forms of instability: process drift, where unsteered client simulation wanders away from specific counseling goals, and standard drift, where static pointwise scoring lacks the stability for reliable judgment. To address this, we introduce Ps, a unified framework that calibrates the therapeutic competence of LLMs via trajectory-anchored tournaments. We first anchor the interaction trajectory in simulation, where clients precisely control the fluid consultation process to probe multifaceted capabilities. We then anchor the battle trajectory in judgments through an efficient Swiss-system tournament, utilizing dynamic pairwise battles to yield robust Elo ratings. Beyond ranking, we demonstrate that tournament trajectories can be transformed into credible reward signals, enabling on-policy reinforcement learning to enhance LLMs' performance. Extensive experiments validate the effectiveness of PsychePass and its strong consistency with human expert judgments.

</details>


### [288] [Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space](https://arxiv.org/abs/2601.20339)
*Yangyi Shen,Tianjian Feng,Jiaqi Han,Wen Wang,Tianlang Chen,Chunhua Shen,Jure Leskovec,Stefano Ermon*

Main category: cs.CL

TL;DR: 提出Order - Token Search方法用于扩散语言模型（DLMs），在多个基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型的解码方法只能选择单一轨迹，限制了轨迹空间的探索。

Method: 引入Order - Token Search，通过联合搜索生成顺序和词元值来探索轨迹空间，核心是能对去噪动作评分的似然估计器。

Result: 在数学推理和编码基准测试中，Order - Token Search始终优于基线，在GSM8K、MATH500、Countdown和HumanEval上有明显提升。

Conclusion: 联合搜索是推进扩散语言模型解码的关键组成部分。

Abstract: Diffusion Language Models (DLMs) offer order-agnostic generation that can explore many possible decoding trajectories. However, current decoding methods commit to a single trajectory, limiting exploration in trajectory space. We introduce Order-Token Search to explore this space through jointly searching over generation order and token values. Its core is a likelihood estimator that scores denoising actions, enabling stable pruning and efficient exploration of diverse trajectories. Across mathematical reasoning and coding benchmarks, Order-Token Search consistently outperforms baselines on GSM8K, MATH500, Countdown, and HumanEval (3.1%, 3.8%, 7.9%, and 6.8% absolute over backbone), matching or surpassing diffu-GRPO post-trained d1-LLaDA. Our work establishes joint search as a key component for advancing decoding in DLMs.

</details>


### [289] [Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers](https://arxiv.org/abs/2601.20796)
*Yiran Huang,Karsten Roth,Quentin Bouniot,Wenjia Xu,Zeynep Akata*

Main category: cs.CL

TL;DR: 研究Transformer在多模态中上下文学习（ICL）能力，发现旋转位置嵌入影响单模态ICL数据复杂度阈值，多模态有学习不对称性，依赖归纳机制。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer如何从上下文示例中学习跨模态关联信息。

Method: 在合成分类任务训练的小型Transformer上进行控制实验，先研究单模态ICL核心原理，再扩展到多模态。

Result: RoPE增加单模态ICL数据复杂度阈值；多模态存在学习不对称性，依赖归纳机制复制标签；多模态训练可跨模态优化扩展电路。

Conclusion: 为理解现代Transformer多模态ICL提供机制基础，引入可控测试平台供后续研究。

Abstract: Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture. We begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.

</details>


### [290] [Linear representations in language models can change dramatically over a conversation](https://arxiv.org/abs/2601.20834)
*Andrew Kyle Lampinen,Yuxuan Li,Eghbal Hosseini,Sangnie Bhardwaj,Murray Shanahan*

Main category: cs.CL

TL;DR: 研究语言模型表示在对话中的动态变化，发现表示会大幅改变，有内容依赖性，对解释性和引导提出挑战，也带来新研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型表示在（模拟）对话中沿线性维度的动态变化。

Method: 观察语言模型表示在对话过程中的变化，对比不同情况。

Result: 线性表示在对话中会大幅改变，变化有内容依赖性，跨模型家族和层，无需在线策略对话，不同对话点引导效果不同。

Conclusion: 表示变化对解释性和引导有挑战，同时指出理解模型适应上下文的新研究方向。

Abstract: Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [291] [Dynamics of Human-AI Collective Knowledge on the Web: A Scalable Model and Insights for Sustainable Growth](https://arxiv.org/abs/2601.20099)
*Buddhika Nettasinghe,Kang Zhao*

Main category: cs.CY

TL;DR: 提出人类与大语言模型集体知识生态系统的动态模型，通过实验识别不同增长模式，结合实例分析并拟合维基百科数据，为网络知识可持续增长提供见解。


<details>
  <summary>Details</summary>
Motivation: 理解人类 - 大语言模型集体知识生态系统中反馈循环带来的好处与系统风险等现象。

Method: 提出动态模型，包含多种要素和机制，进行数值实验，结合PubMed、GitHub和Copilot等实例分析，拟合维基百科知识流数据。

Result: 识别不同增长模式，展示平台和政策手段对系统的影响，发现维基百科中LLM内容增加、人类贡献减少。

Conclusion: 模型和分析为网络上人类 - 人工智能集体知识的可持续增长提供了可操作的见解。

Abstract: Humans and large language models (LLMs) now co-produce and co-consume the web's shared knowledge archives. Such human-AI collective knowledge ecosystems contain feedback loops with both benefits (e.g., faster growth, easier learning) and systemic risks (e.g., quality dilution, skill reduction, model collapse). To understand such phenomena, we propose a minimal, interpretable dynamical model of the co-evolution of archive size, archive quality, model (LLM) skill, aggregate human skill, and query volume. The model captures two content inflows (human, LLM) controlled by a gate on LLM-content admissions, two learning pathways for humans (archive study vs. LLM assistance), and two LLM-training modalities (corpus-driven scaling vs. learning from human feedback). Through numerical experiments, we identify different growth regimes (e.g., healthy growth, inverted flow, inverted learning, oscillations), and show how platform and policy levers (gate strictness, LLM training, human learning pathways) shift the system across regime boundaries. Two domain configurations (PubMed, GitHub and Copilot) illustrate contrasting steady states under different growth rates and moderation norms. We also fit the model to Wikipedia's knowledge flow during pre-ChatGPT and post-ChatGPT eras separately. We find a rise in LLM additions with a concurrent decline in human inflow, consistent with a regime identified by the model. Our model and analysis yield actionable insights for sustainable growth of human-AI collective knowledge on the Web.

</details>


### [292] [Large language models accurately predict public perceptions of support for climate action worldwide](https://arxiv.org/abs/2601.20141)
*Nattavudh Powdthavee,Sandra J. Geiger*

Main category: cs.CY

TL;DR: 研究测试大语言模型（LLMs）能否预测全球气候变化行动观念差距，发现LLMs能较准确捕捉相关公众认知，虽在部分国家表现不佳。


<details>
  <summary>Details</summary>
Motivation: 人们对气候变化行动支持大，但低估他人支持阻碍了改变，需可靠工具预测全球气候变化行动的观念差距。

Method: 使用125个国家的指标和民意数据，将四个先进LLMs与盖洛普民意调查数据及统计回归进行基准测试，还进行控制测试。

Result: LLMs（尤其是Claude）能准确捕捉公众对他人为气候行动出资意愿的认知（MAE约5个百分点；r = 0.77），与统计模型相当，但在数字连接少、GDP低的国家表现下降；控制测试表明LLMs捕捉到关键心理过程，依靠结构化推理而非记忆值。

Conclusion: LLMs是评估气候变化行动观念差距的快速工具，在资源丰富国家可替代昂贵调查，在代表性不足人群中可作补充。

Abstract: Although most people support climate action, widespread underestimation of others' support stalls individual and systemic changes. In this preregistered experiment, we test whether large language models (LLMs) can reliably predict these perception gaps worldwide. Using country-level indicators and public opinion data from 125 countries, we benchmark four state-of-the-art LLMs against Gallup World Poll 2021/22 data and statistical regressions. LLMs, particularly Claude, accurately capture public perceptions of others' willingness to contribute financially to climate action (MAE approximately 5 p.p.; r = .77), comparable to statistical models, though performance declines in less digitally connected, lower-GDP countries. Controlled tests show that LLMs capture the key psychological process - social projection with a systematic downward bias - and rely on structured reasoning rather than memorized values. Overall, LLMs provide a rapid tool for assessing perception gaps in climate action, serving as an alternative to costly surveys in resource-rich countries and as a complement in underrepresented populations.

</details>


### [293] [How AI Impacts Skill Formation](https://arxiv.org/abs/2601.20245)
*Judy Hanwen Shen,Alex Tamkin*

Main category: cs.CY

TL;DR: 研究AI辅助对新手开发者技能习得的影响，发现其有弊端，应谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 探讨AI辅助对有效监管AI所需技能发展的影响，以及对新手技能习得的影响。

Method: 进行随机实验，研究开发者在有无AI辅助下掌握新异步编程库的情况。

Result: AI使用会损害概念理解、代码阅读和调试能力，平均无显著效率提升，部分全委托编码者有生产力提升但牺牲学习，找出六种AI交互模式。

Conclusion: AI增强的生产力不是通往能力的捷径，应谨慎采用AI辅助以保留技能形成，尤其在安全关键领域。

Abstract: AI assistance produces significant productivity gains across professional domains, particularly for novice workers. Yet how this assistance affects the development of skills required to effectively supervise AI remains unclear. Novice workers who rely heavily on AI to complete unfamiliar tasks may compromise their own skill acquisition in the process. We conduct randomized experiments to study how developers gained mastery of a new asynchronous programming library with and without the assistance of AI. We find that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average. Participants who fully delegated coding tasks showed some productivity improvements, but at the cost of learning the library. We identify six distinct AI interaction patterns, three of which involve cognitive engagement and preserve learning outcomes even when participants receive AI assistance. Our findings suggest that AI-enhanced productivity is not a shortcut to competence and AI assistance should be carefully adopted into workflows to preserve skill formation -- particularly in safety-critical domains.

</details>


### [294] [Agent Benchmarks Fail Public Sector Requirements](https://arxiv.org/abs/2601.20617)
*Jonathan Rystrøm,Chris Schmitz,Karolina Korgul,Jan Batzner,Chris Russell*

Main category: cs.CY

TL;DR: 本文聚焦确保大语言模型代理基准能反映公共部门要求，定义标准、分析超1300篇基准论文，发现无单个基准满足所有标准并呼吁行动。


<details>
  <summary>Details</summary>
Motivation: 部署基于大语言模型的代理到公共部门需评估其是否满足要求，但不清楚基准所需的标准及现有基准的达标情况。

Method: 基于对公共行政文献的第一性原理调查定义标准，使用专家验证的大语言模型辅助管道分析超1300篇基准论文。

Result: 没有单个基准满足所有定义的标准。

Conclusion: 呼吁研究人员开发与公共部门相关的基准，公共部门官员在评估代理应用时使用这些标准。

Abstract: Deploying Large Language Model-based agents (LLM agents) in the public sector requires assuring that they meet the stringent legal, procedural, and structural requirements of public-sector institutions. Practitioners and researchers often turn to benchmarks for such assessments. However, it remains unclear what criteria benchmarks must meet to ensure they adequately reflect public-sector requirements, or how many existing benchmarks do so. In this paper, we first define such criteria based on a first-principles survey of public administration literature: benchmarks must be \emph{process-based}, \emph{realistic}, \emph{public-sector-specific} and report \emph{metrics} that reflect the unique requirements of the public sector. We analyse more than 1,300 benchmark papers for these criteria using an expert-validated LLM-assisted pipeline. Our results show that no single benchmark meets all of the criteria. Our findings provide a call to action for both researchers to develop public sector-relevant benchmarks and for public-sector officials to apply these criteria when evaluating their own agentic use cases.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [295] [On Every Note a Griff: Looking for a Useful Representation of Basso Continuo Performance Style](https://arxiv.org/abs/2601.20478)
*Adam Štefunko,Carlos Eduardo Cancino-Chacón,Jan Hajič*

Main category: cs.SD

TL;DR: 介绍ACoRD数据集记录现代通奏低音演奏，提出griff表示法用于分析演奏风格并展示应用，强调保留即兴结构分析个人风格的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究对齐的通奏低音演奏，分析其演奏风格的多样性和个体差异。

Method: 提出受历史通奏低音论著启发的griff表示法，从对齐的演奏中提取griff并进行统计描述，通过实验分析演奏风格。

Result: 能使用griff对不同演奏者的通奏低音演奏风格进行统计分析。

Conclusion: 保留通奏低音即兴结构对精细分析个人演奏风格很有必要，griff能提供有意义的特征空间。

Abstract: Basso continuo is a baroque improvisatory accompaniment style which involves improvising multiple parts above a given bass line in a musical score on a harpsichord or organ. Basso continuo is not merely a matter of history; moreover, it is a historically inspired living practice, and The Aligned Continuo Dataset (ACoRD) records the first sample of modern-day basso continuo playing in the symbolic domain. This dataset, containing 175 MIDI recordings of 5 basso continuo scores performed by 7 players, allows us to start observing and analyzing the variety that basso continuo improvisation brings. A recently proposed basso continuo performance-to-score alignment system provides a way of mapping improvised performance notes to score notes. In order to study aligned basso continuo performances, we need an appropriate feature representation. We propose griff, a representation inspired by historical basso continuo treatises. It enables us to encode both pitch content and structure of a basso continuo realization in a transposition-invariant way. Griffs are directly extracted from aligned basso continuo performances by grouping together performance notes aligned to the same score note in a onset-time ordered way, and they provide meaningful tokens that form a feature space in which we can analyze basso continuo performance styles. We statistically describe griffs extracted from the ACoRD dataset recordings, and show in two experiments how griffs can be used for statistical analysis of individuality of different players' basso continuo performance styles. We finally present an argument why it is desirable to preserve the structure of a basso continuo improvisation in order to conduct a refined analysis of personal performance styles of individual basso continuo practitioners, and why griffs can provide a meaningful historically informed feature space worthy of a more robust empirical validation.

</details>


### [296] [LTS-VoiceAgent: A Listen-Think-Speak Framework for Efficient Streaming Voice Interaction via Semantic Triggering and Incremental Reasoning](https://arxiv.org/abs/2601.19952)
*Wenhao Zou,Yuwei Miao,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Jingwen Xu*

Main category: cs.SD

TL;DR: 提出LTS - VoiceAgent框架解决实时语音代理延迟问题，在多基准测试中表现更优。


<details>
  <summary>Details</summary>
Motivation: 端到端模型缺乏深度推理，级联管道架构执行时延迟高，现有级联流式策略有缺陷。

Method: 提出LTS - VoiceAgent框架，含动态语义触发器和双角色流编排器，引入Pause - and - Repair基准测试。

Result: 在VERA、Spoken - MQA等多个基准测试中，LTS - VoiceAgent在准确率、延迟和效率的平衡上优于串行级联基线和现有流式策略。

Conclusion: LTS - VoiceAgent框架能有效解决实时语音代理延迟问题，实现更好的性能权衡。

Abstract: Real-time voice agents face a dilemma: end-to-end models often lack deep reasoning, while cascaded pipelines incur high latency by executing ASR, LLM reasoning, and TTS strictly in sequence, unlike human conversation where listeners often start thinking before the speaker finishes. Since cascaded architectures remain the dominant choice for complex tasks, existing cascaded streaming strategies attempt to reduce this latency via mechanical segmentation (e.g., fixed chunks, VAD-based splitting) or speculative generation, but they frequently either break semantic units or waste computation on predictions that must be rolled back. To address these challenges, we propose LTS-VoiceAgent, a Listen-Think-Speak framework that explicitly separates when to think from how to reason incrementally. It features a Dynamic Semantic Trigger to detect meaningful prefixes, and a Dual-Role Stream Orchestrator that coordinates a background Thinker (for state maintenance) and a foreground Speaker (for speculative solving). This parallel design enables "thinking while speaking" without blocking responses. We also introduce a Pause-and-Repair benchmark containing natural disfluencies to stress-test streaming robustness. Experiments across VERA, Spoken-MQA, BigBenchAudio, and our benchmark show that LTS-VoiceAgent achieves a stronger accuracy-latency-efficiency trade-off than serial cascaded baselines and existing streaming strategies.

</details>


### [297] [Switchcodec: Adaptive residual-expert sparse quantization for high-fidelity neural audio coding](https://arxiv.org/abs/2601.20362)
*Xiangbo Wang,Wenbin Jiang,Jin Wang,Yubo You,Sheng Fang,Fei Wen*

Main category: cs.SD

TL;DR: SwitchCodec，一种基于REVQ的神经音频编解码器，通过动态路由专家量化器提高压缩效率，支持多比特率操作，实验中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有的神经音频压缩模型在处理具有广泛变异性的音频内容时，使用固定数量每帧码本的方式并非最优。

Method: 提出基于REVQ的SwitchCodec，结合共享量化器和动态路由的专家量化器，根据输入音频激活量化器；采用可变比特率机制在推理时调整激活的专家量化器数量。

Result: SwitchCodec在客观指标和主观听力测试中都超越了现有基线。

Conclusion: SwitchCodec通过改进量化方法和支持多比特率操作，有效提高了音频压缩效率。

Abstract: Recent neural audio compression models often rely on residual vector quantization for high-fidelity coding, but using a fixed number of per-frame codebooks is suboptimal for the wide variability of audio content-especially for signals that are either very simple or highly complex. To address this limitation, we propose SwitchCodec, a neural audio codec based on Residual Experts Vector Quantization (REVQ). REVQ combines a shared quantizer with dynamically routed expert quantizers that are activated according to the input audio, decoupling bitrate from codebook capacity and improving compression efficiency. This design ensures full training and utilization of each quantizer. In addition, a variable-bitrate mechanism adjusts the number of active expert quantizers at inference, enabling multi-bitrate operation without retraining. Experiments demonstrate that SwitchCodec surpasses existing baselines on both objective metrics and subjective listening tests.

</details>


### [298] [Self Voice Conversion as an Attack against Neural Audio Watermarking](https://arxiv.org/abs/2601.20432)
*Yigitcan Özer,Wanying Ge,Zhe Zhang,Xin Wang,Junichi Yamagishi*

Main category: cs.SD

TL;DR: 研究自语音转换对音频水印系统的攻击，表明其严重降低现有水印方法可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有音频水印方法对基于深度学习的攻击评估不足，自语音转换作为新攻击威胁水印安全。

Method: 研究自语音转换这种内容保留的攻击方式，通过语音转换模型改变声学特征。

Result: 自语音转换攻击严重降低了现有最先进水印方法的可靠性。

Conclusion: 自语音转换对现代音频水印技术的安全性有重要影响。

Abstract: Audio watermarking embeds auxiliary information into speech while maintaining speaker identity, linguistic content, and perceptual quality. Although recent advances in neural and digital signal processing-based watermarking methods have improved imperceptibility and embedding capacity, robustness is still primarily assessed against conventional distortions such as compression, additive noise, and resampling. However, the rise of deep learning-based attacks introduces novel and significant threats to watermark security. In this work, we investigate self voice conversion as a universal, content-preserving attack against audio watermarking systems. Self voice conversion remaps a speaker's voice to the same identity while altering acoustic characteristics through a voice conversion model. We demonstrate that this attack severely degrades the reliability of state-of-the-art watermarking approaches and highlight its implications for the security of modern audio watermarking techniques.

</details>


### [299] [Audio Deepfake Detection in the Age of Advanced Text-to-Speech models](https://arxiv.org/abs/2601.20510)
*Robin Singh,Aditya Yogesh Nair,Fabio Palumbo,Florian Barbaro,Anna Dyka,Lohith Rachakonda*

Main category: cs.SD

TL;DR: 本文对三种TTS模型进行比较评估，用12000个合成音频样本测试四种检测框架，发现单一范式检测器有局限，多视图检测方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 文本转语音系统发展使合成语音更逼真，给音频深度伪造检测带来新挑战，需评估不同TTS模型和检测框架。

Method: 对Dia2、Maya1和MeloTTS三种TTS模型进行评估，用Daily - Dialog数据集生成12000个合成音频样本，用四种检测框架进行测试。

Result: 不同生成机制下检测器性能有显著差异，单一架构有效模型对其他架构可能失效，多视图检测方法在所有评估模型中表现稳健。

Conclusion: 单一范式检测器有局限，需集成检测策略应对音频深度伪造威胁。

Abstract: Recent advances in Text-to-Speech (TTS) systems have substantially increased the realism of synthetic speech, raising new challenges for audio deepfake detection. This work presents a comparative evaluation of three state-of-the-art TTS models--Dia2, Maya1, and MeloTTS--representing streaming, LLM-based, and non-autoregressive architectures. A corpus of 12,000 synthetic audio samples was generated using the Daily-Dialog dataset and evaluated against four detection frameworks, including semantic, structural, and signal-level approaches. The results reveal significant variability in detector performance across generative mechanisms: models effective against one TTS architecture may fail against others, particularly LLM-based synthesis. In contrast, a multi-view detection approach combining complementary analysis levels demonstrates robust performance across all evaluated models. These findings highlight the limitations of single-paradigm detectors and emphasize the necessity of integrated detection strategies to address the evolving landscape of audio deepfake threats.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [300] [Demonstration-Free Robotic Control via LLM Agents](https://arxiv.org/abs/2601.20334)
*Brian Y. Tsui,Alan Y. Fang,Tiffany J. Hwu*

Main category: cs.RO

TL;DR: 研究将通用大语言模型代理框架用于具身操作，引入FAEA，评估结果显示其有不错表现且无需演示或微调，可用于数据增强。


<details>
  <summary>Details</summary>
Motivation: 现有视觉 - 语言 - 动作模型在具身操作中需特定任务演示和微调，泛化能力差，探索通用大语言模型代理框架作为替代控制范式。

Method: 引入FAEA，将LLM代理框架直接应用于具身操作，使用迭代推理，评估未修改的Claude Agent SDK。

Result: 在多个基准上成功，接近少数演示训练的VLA模型表现，加入人类反馈可提升性能，能自主探索新场景生成轨迹。

Conclusion: 通用代理足以应对一类以任务级规划为主的操作任务，为机器人系统利用代理基础设施和前沿模型进展开辟道路。

Abstract: Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim

</details>


### [301] [E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2601.19969)
*Haoyuan Deng,Yuanjiang Xue,Haoyang Du,Boyang Zhou,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出样本高效的真实世界人在环RL框架，通过选择信息样本减少人工干预，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有HiL - RL框架样本效率低，需大量人工干预导致高成本。

Method: 构建样本对策略熵的影响函数，选择影响函数值适中的样本，去除诱导熵骤降的捷径样本和影响可忽略的噪声样本。

Result: 在四个真实世界操作任务实验中，比现有方法成功率高42.1%，人工干预少10.1%。

Conclusion: 所提框架有效。

Abstract: Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \method achieves a 42.1\% higher success rate while requiring 10.1\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [302] [Trigger Optimization and Event Classification for Dark Matter Searches in the CYGNO Experiment Using Machine Learning](https://arxiv.org/abs/2601.20626)
*F. D. Amaro,R. Antonietti,E. Baracchini,L. Benussi,C. Capoccia,M. Caponero,L. G. M. de Carvalho,G. Cavoto,I. A. Costa,A. Croce,M. D'Astolfo,G. D'Imperio,G. Dho,E. Di Marco,J. M. F. dos Santos,D. Fiorina,F. Iacoangeli,Z. Islam,E. Kemp,H. P. Lima,G. Maccarrone,R. D. P. Mano,D. J. G. Marques,G. Mazzitelli,P. Meloni,A. Messina,C. M. B. Monteiro,R. A. Nobrega,G. M. Oppedisano,I. F. Pains,E. Paoletti,F. Petrucci,S. Piacentini,D. Pierluigi,D. Pinci,F. Renga,A. Russo,G. Saviano,P. A. O. C. Silva,N. J. Spooner,R. Tesauro,S. Tomassini,D. Tozzi*

Main category: physics.ins-det

TL;DR: CYGNO实验用光学读出TPC搜索低能稀有相互作用，面临大数据挑战，采用两种机器学习方法处理，一是在线数据降维，二是无标签分类识别核反冲拓扑。


<details>
  <summary>Details</summary>
Motivation: CYGNO实验光学读出产生的大数据给实时触发、数据降维和背景识别带来挑战，需有效处理方法。

Method: 一是基于重建异常检测的无监督在线数据降维策略，用卷积自编码器处理；二是对Am - Be中子源数据应用无标签分类框架的弱监督方法。

Result: 数据降维保留93.0±0.2%信号强度，丢弃97.8±0.1%图像区域，每帧推理约25ms；无标签分类性能接近理论极限，分离出与核反冲一致的高得分群体。

Conclusion: 两种机器学习方法能有效应对CYGNO实验大数据挑战，助力低能稀有相互作用搜索。

Abstract: The CYGNO experiment employs an optical-readout Time Projection Chamber (TPC) to search for rare low-energy interactions using finely resolved scintillation images. While the optical readout provides rich topological information, it produces large, sparse megapixel images that challenge real-time triggering, data reduction, and background discrimination.
  We summarize two complementary machine-learning approaches developed within CYGNO. First, we present a fast and fully unsupervised strategy for online data reduction based on reconstruction-based anomaly detection. A convolutional autoencoder trained exclusively on pedestal images (i.e. frames acquired with GEM amplification disabled) learns the detector noise morphology and highlights particle-induced structures through localized reconstruction residuals, from which compact Regions of Interest (ROIs) are extracted. On real prototype data, the selected configuration retains (93.0 +/- 0.2)% of reconstructed signal intensity while discarding (97.8 +/- 0.1)% of the image area, with ~25 ms per-frame inference time on a consumer GPU.
  Second, we report a weakly supervised application of the Classification Without Labels (CWoLa) framework to data acquired with an Americium--Beryllium neutron source. Using only mixed AmBe and standard datasets (no event-level labels), a convolutional classifier learns to identify nuclear-recoil-like topologies. The achieved performance approaches the theoretical limit imposed by the mixture composition and isolates a high-score population with compact, approximately circular morphologies consistent with nuclear recoils.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [303] [Cross-Country Learning for National Infectious Disease Forecasting Using European Data](https://arxiv.org/abs/2601.20771)
*Zacharias Komodromos,Kleanthis Malialis,Artemis Kontou,Panayiotis Kolios*

Main category: q-bio.PE

TL;DR: 提出跨国学习方法用于传染病预测，以塞浦路斯新冠病例预测为例验证，该方法能提升预测效果，框架适用于更广泛场景。


<details>
  <summary>Details</summary>
Motivation: 多数单国家历史数据驱动的传染病预测方法因数据长度和可变性受限，影响机器学习模型性能，需新方法。

Method: 采用跨国学习方法，用多个国家时间序列数据训练单一模型，以欧洲国家监测数据对塞浦路斯新冠病例预测进行案例研究，评估多种机器学习模型，分析回溯窗口长度和跨国“数据增强”对多步预测性能的影响。

Result: 结合其他国家数据的模型比仅用本国数据训练的模型有持续改进。

Conclusion: 提出的框架和研究结果适用于更广泛的传染病预测，尤其在国家历史数据有限的情况下。

Abstract: Accurate forecasting of infectious disease incidence is critical for public health planning and timely intervention. While most data-driven forecasting approaches rely primarily on historical data from a single country, such data are often limited in length and variability, restricting the performance of machine learning (ML) models. In this work, we investigate a cross-country learning approach for infectious disease forecasting, in which a single model is trained on time series data from multiple countries and evaluated on a country of interest. This setting enables the model to exploit shared epidemic dynamics across countries and to benefit from an enlarged training set. We examine this approach through a case study on COVID-19 case forecasting in Cyprus, using surveillance data from European countries. We evaluate multiple ML models and analyse the impact of the lookback window length and cross-country `data augmentation' on multi-step forecasting performance. Our results show that incorporating data from other countries can lead to consistent improvements over models trained solely on national data. Although the empirical focus is on Cyprus and COVID-19, the proposed framework and findings are applicable to infectious disease forecasting more broadly, particularly in settings with limited national historical data.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [304] [Leveraging Second-Order Curvature for Efficient Learned Image Compression: Theory and Empirical Evidence](https://arxiv.org/abs/2601.20769)
*Yichi Zhang,Fengqing Zhu*

Main category: eess.IV

TL;DR: 使用二阶拟牛顿优化器SOAP可提升学习型图像压缩（LIC）模型的训练效率和最终性能，且模型对量化更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 标准一阶优化器在处理LIC模型的率失真优化问题时存在梯度冲突，导致收敛慢和性能不佳。

Method: 使用二阶拟牛顿优化器SOAP进行模型训练，并进行理论和实证分析。

Result: SOAP能解决率失真目标中的更新冲突，实现更快更稳定收敛，且训练的模型激活和潜在异常值更少，对量化更鲁棒。

Conclusion: 二阶优化是提升LIC效率和实用性的强大实用工具，可无缝替换现有优化器。

Abstract: Training learned image compression (LIC) models entails navigating a challenging optimization landscape defined by the fundamental trade-off between rate and distortion. Standard first-order optimizers, such as SGD and Adam, struggle with \emph{gradient conflicts} arising from competing objectives, leading to slow convergence and suboptimal rate-distortion performance. In this work, we demonstrate that a simple utilization of a second-order quasi-Newton optimizer, \textbf{SOAP}, dramatically improves both training efficiency and final performance across diverse LICs. Our theoretical and empirical analyses reveal that Newton preconditioning inherently resolves the intra-step and inter-step update conflicts intrinsic to the R-D objective, facilitating faster, more stable convergence. Beyond acceleration, we uncover a critical deployability benefit: second-order trained models exhibit significantly fewer activation and latent outliers. This substantially enhances robustness to post-training quantization. Together, these results establish second-order optimization, achievable as a seamless drop-in replacement of the imported optimizer, as a powerful, practical tool for advancing the efficiency and real-world readiness of LICs.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [305] [Explainable deep learning reveals the physical mechanisms behind the turbulent kinetic energy equation](https://arxiv.org/abs/2601.20052)
*Francisco Alcántara-Ávila,Andrés Cremades,Sergio Hoyas,Ricardo Vinuesa*

Main category: physics.flu-dyn

TL;DR: 用可解释深度学习研究湍动能输运物理机制，发现近壁区重要结构、层次组织及经典相干结构无法代表湍动能预算项机制。


<details>
  <summary>Details</summary>
Motivation: 研究控制湍动能输运的物理机制。

Method: 使用基于SHAP的可解释深度学习模型分析摩擦雷诺数为125的湍流槽道流的湍动能预算项。

Result: 重要结构多在近壁区且与扫掠事件有关；近壁区有层次组织，外层该组织瓦解；经典相干结构无法代表机制。

Conclusion: 近壁区耗散是主导组织机制，其层次结构在外层瓦解。

Abstract: In this work, we investigate the physical mechanisms governing turbulent kinetic energy transport using explainable deep learning (XDL). An XDL model based on SHapley Additive exPlanations (SHAP) is used to identify and percolate high-importance structures for the evolution of the turbulent kinetic energy budget terms of a turbulent channel flow at a friction Reynolds number of $Re_τ= 125$. The results show that the important structures are predominantly located in the near-wall region and are more frequently associated with sweep-type events. In the viscous layer, the SHAP structures relevant for production and viscous diffusion are almost entirely contained within those relevant for dissipation, revealing a clear hierarchical organization of near-wall turbulence. In the outer layer, this hierarchical organization breaks down and only velocity-pressure-gradient correlation and turbulent transport SHAP structures remain, with a moderate spatial coincidence of approximately $60\%$. Finally, we show that none of the coherent structures classically studied in turbulence are capable of representing the mechanisms behind the various terms of the turbulent kinetic energy budget throughout the channel. These results reveal dissipation as the dominant organizing mechanism of near-wall turbulence, constraining production and viscous diffusion within a single structural hierarchy that breaks down in the outer layer.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [306] [Global Plane Waves From Local Gaussians: Periodic Charge Densities in a Blink](https://arxiv.org/abs/2601.19966)
*Jonas Elsborg,Felix Ærtebjerg,Luca Thiede,Alán Aspuru-Guzik,Tejs Vegge,Arghya Bhowmik*

Main category: cond-mat.mtrl-sci

TL;DR: 介绍ELECTRAFI模型用于预测晶体材料周期性电荷密度，速度快且精度高，用于初始化DFT计算可降低成本。


<details>
  <summary>Details</summary>
Motivation: 开发快速且准确的模型来预测晶体材料的周期性电荷密度，提高DFT计算效率。

Method: 在实空间构建各向异性高斯函数，利用其封闭形式的傅里叶变换和泊松求和公式解析计算平面波系数。

Result: ELECTRAFI在周期性基准测试中达到或超过了最先进的精度，比最强竞争对手快633倍，用于初始化DFT计算可降低约20%的计算成本。

Conclusion: 准确性和推理成本共同决定了端到端DFT加速，应注重效率。

Abstract: We introduce ELECTRAFI, a fast, end-to-end differentiable model for predicting periodic charge densities in crystalline materials. ELECTRAFI constructs anisotropic Gaussians in real space and exploits their closed-form Fourier transforms to analytically evaluate plane-wave coefficients via the Poisson summation formula. This formulation delegates non-local and periodic behavior to analytic transforms, enabling reconstruction of the full periodic charge density with a single inverse FFT. By avoiding explicit real-space grid probing, periodic image summation, and spherical harmonic expansions, ELECTRAFI matches or exceeds state-of-the-art accuracy across periodic benchmarks while being up to $633 \times$ faster than the strongest competing method, reconstructing crystal charge densities in a fraction of a second. When used to initialize DFT calculations, ELECTRAFI reduces total DFT compute cost by up to ~20%, whereas slower charge density models negate savings due to high inference times. Our results show that accuracy and inference cost jointly determine end-to-end DFT speedups, and motivate our focus on efficiency.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [307] [NET4EXA: Pioneering the Future of Interconnects for Supercomputing and AI](https://arxiv.org/abs/2601.19413)
*Michele Martinelli,Roberto Ammendola,Andrea Biagioni,Carlotta Chiarini,Ottorino Frezza,Francesca Lo Cicero,Alessandro Lonardo,Pier Stanislao Paolucci,Elena Pastorelli,Pierpaolo Perticaroli,Luca Pontisso,Cristian Rossi,Francesco Simula,Piero Vicini,David Colin,Grégoire Pichon,Alexandre Louvet,John Gliksberg,Claire Chen,Matteo Turisini,Andrea Monterubbiano,Jean-Philippe Nominé,Denis Dutoit,Hugo Taboada,Lilia Zaourar,Mohamed Benazouz,Angelos Bilas,Fabien Chaix,Manolis Katevenis,Nikolaos Chrysos,Evangelos Mageiropoulos,Christos Kozanitis,Thomas Moen,Steffen Persvold,Einar Rustad,Sandro Fiore,Fabrizio Granelli,Simone Pezzuto,Raffaello Potestio,Luca Tubiana,Philippe Velha,Flavio Vella,Daniele De Sensi,Salvatore Pontarelli*

Main category: cs.NI

TL;DR: NET4EXA 旨在为 HPC 和 AI 系统开发下一代高性能互连，基于 BXI 技术推出 BXIv3，还为 BXIv4 做铺垫，采用混合开发和协同设计方法并进行性能评估。


<details>
  <summary>Details</summary>
Motivation: 应对大规模基础设施（如大语言模型训练）对高性能互连不断增长的需求。

Method: 采用混合开发和协同设计方法，结合商业交换机技术、定制 IP 和基于 FPGA 的网卡。

Result: 集成功能完备的试验系统（TRL 8），将在 2025 年起用于即将到来的百亿亿次级及后续系统；对 BXIv3 互连性能进行评估。

Conclusion: 为 HPC 和 AI 系统提供下一代高性能互连解决方案，为 BXIv4 奠定基础。

Abstract: NET4EXA aims to develop a next-generation high-performance interconnect for HPC and AI systems, addressing the increasing demands of large-scale infrastructures, such as those required for training Large Language Models. Building upon the proven BXI (Bull eXascale Interconnect) European technology used in TOP15 supercomputers, NET4EXA will deliver the new BXI release, BXIv3, a complete hardware and software interconnect solution, including switch and network interface components. The project will integrate a fully functional pilot system at TRL 8, ready for deployment into upcoming exascale and post-exascale systems from 2025 onward. Leveraging prior research from European initiatives like RED-SEA, the previous achievements of consortium partners and over 20 years of expertise from BULL, NET4EXA also lays the groundwork for the future generation of BXI, BXIv4, providing analysis and preliminary design. The project will use a hybrid development and co-design approach, combining commercial switch technology with custom IP and FPGA-based NICs. Performances of NET4EXA BXIv3 interconnect will be evaluated using a broad portfolio of benchmarks, scientific scalable applications, and AI workloads.

</details>


### [308] [Proactive SFC Provisioning with Forecast-Driven DRL in Data Centers](https://arxiv.org/abs/2601.20229)
*Parisa Fard Moshiri,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 提出混合预测驱动的深度强化学习框架解决服务功能链中虚拟网络功能的高效放置问题，实验证明该方法可提升服务接受率、降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 传统静态资源分配无法适应流量负载和应用需求动态性，导致资源过度供应或供应不足。

Method: 利用DRL生成数据集训练深度学习预测模型，通过Optuna优化超参数，将最佳表现模型集成，预测结果用于数据中心选择。

Result: 提高资源密集型和延迟敏感型服务接受率，降低多种服务端到端延迟，实现更均衡资源分配、减少竞争。

Conclusion: 所提方法能有效解决服务功能链中虚拟网络功能放置问题，提升资源利用率和服务性能。

Abstract: Service Function Chaining (SFC) requires efficient placement of Virtual Network Functions (VNFs) to satisfy diverse service requirements while maintaining high resource utilization in Data Centers (DCs). Conventional static resource allocation often leads to overprovisioning or underprovisioning due to the dynamic nature of traffic loads and application demands. To address this challenge, we propose a hybrid forecast-driven Deep reinforcement learning (DRL) framework that combines predictive intelligence with SFC provisioning. Specifically, we leverage DRL to generate datasets capturing DC resource utilization and service demands, which are then used to train deep learning forecasting models. Using Optuna-based hyperparameter optimization, the best-performing models, Spatio-Temporal Graph Neural Network, Temporal Graph Neural Network, and Long Short-Term Memory, are combined into an ensemble to enhance stability and accuracy. The ensemble predictions are integrated into the DC selection process, enabling proactive placement decisions that consider both current and future resource availability. Experimental results demonstrate that the proposed method not only sustains high acceptance ratios for resource-intensive services such as Cloud Gaming and VoIP but also significantly improves acceptance ratios for latency-critical categories such as Augmented Reality increases from 30% to 50%, while Industry 4.0 improves from 30% to 45%. Consequently, the prediction-based model achieves significantly lower E2E latencies of 20.5%, 23.8%, and 34.8% reductions for VoIP, Video Streaming, and Cloud Gaming, respectively. This strategy ensures more balanced resource allocation, and reduces contention.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [309] [Spectral Diffusion Models on the Sphere](https://arxiv.org/abs/2601.20498)
*Pierpaolo Brutti,Claudio Durastanti,Francesco Mari*

Main category: math.PR

TL;DR: 本文提出直接在球面上实值函数的有限维球谐表示上定义的扩散建模框架，分析了空间和频谱得分匹配目标的差异并推导对应扩散方程。


<details>
  <summary>Details</summary>
Motivation: 将谱扩散方法扩展到球面数据时，存在欧几里得空间中不存在的几何和随机问题，需解决这些问题。

Method: 在球面上实值函数的有限维球谐表示上定义扩散建模框架，利用球面离散傅里叶变换分析。

Result: 球面离散傅里叶变换将空间布朗运动映射到频域中有约束的高斯过程，空间和频谱得分匹配目标不再等价，频域公式引入了依赖几何的归纳偏差。

Conclusion: 推导了相应的扩散方程并刻画了诱导噪声协方差。

Abstract: Diffusion models provide a principled framework for generative modeling via stochastic differential equations and time-reversed dynamics. Extending spectral diffusion approaches to spherical data, however, raises nontrivial geometric and stochastic issues that are absent in the Euclidean setting.
  In this work, we develop a diffusion modeling framework defined directly on finite-dimensional spherical harmonic representations of real-valued functions on the sphere. We show that the spherical discrete Fourier transform maps spatial Brownian motion to a constrained Gaussian process in the frequency domain with deterministic, generally non-isotropic covariance. This induces modified forward and reverse-time stochastic differential equations in the spectral domain.
  As a consequence, spatial and spectral score matching objectives are no longer equivalent, even in the band-limited setting, and the frequency-domain formulation introduces a geometry-dependent inductive bias. We derive the corresponding diffusion equations and characterize the induced noise covariance.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [310] [Distributed Learning over Noisy Communication Networks](https://arxiv.org/abs/2601.20723)
*Emrah Akyol,Marcos Vasconcelos*

Main category: eess.SY

TL;DR: 研究图上二进制协调博弈在带噪声通信下的对数线性学习，分析不同通信机制和预算下的学习动态，用数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 研究图上二进制协调博弈在邻居行动通过显式噪声通信链路传递时的情况。

Method: 将边建模为二进制对称信道或二进制擦除信道，分析快速通信和快照两种操作机制，引入有限通信预算。

Result: 在快速通信机制中，学习动态与缩放协调势的吉布斯采样器一致；快照机制中，马尔可夫链一般不可逆但有高温展开；有限通信预算能在两种机制间插值。

Conclusion: 从通信理论角度解释了结果，自然扩展到异构链路可靠性，数值实验量化了通信资源和稳态协调质量的权衡。

Abstract: We study binary coordination games over graphs under log-linear learning when neighbor actions are conveyed through explicit noisy communication links. Each edge is modeled as either a binary symmetric channel (BSC) or a binary erasure channel (BEC). We analyze two operational regimes. For binary symmetric and binary erasure channels, we provide a structural characterization of the induced learning dynamics. In a fast-communication regime, agents update using channel-averaged payoffs; the resulting learning dynamics coincide with a Gibbs sampler for a scaled coordination potential, where channel reliability enters only through a scalar attenuation coefficient. In a snapshot regime, agents update from a single noisy realization and ignore channel statistics; the induced Markov chain is generally nonreversible, but admits a high-temperature expansion whose drift matches that of the fast Gibbs sampler with the same attenuation. We further formalize a finite-$K$ communication budget, which interpolates between snapshot and fast behavior as the number of channel uses per update grows. This viewpoint yields a communication-theoretic interpretation in terms of retransmissions and repetition coding, and extends naturally to heterogeneous link reliabilities via effective edge weights. Numerical experiments illustrate the theory and quantify the tradeoff between communication resources and steady-state coordination quality.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [311] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: 提出无训练剪枝方法SAP和OSR协议，在ViDoRe基准测试中，SAP能减少超90%索引向量并保持检索保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型用于视觉文档检索时索引向量开销大，无训练剪枝方法在高压缩场景表现不佳，此前研究质疑无训练剪枝可行性。

Method: 提出结构锚剪枝（SAP）方法，从中间层识别关键视觉补丁实现高性能压缩；引入Oracle分数保留（OSR）协议评估层信息对压缩效率的影响。

Result: 在ViDoRe基准测试中，SAP减少索引向量超90%，保持了强大的检索保真度。

Conclusion: 语义结构锚补丁存在于中间层，与传统关注最终层的剪枝方法不同，SAP为视觉RAG提供高可扩展解决方案。

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [312] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文提出从单目图像恢复真实比例3D重建对象的方法，能提升精准营养领域表现，降低近30%的平均绝对体积估计误差。


<details>
  <summary>Details</summary>
Motivation: 饮食相关慢性病凸显食物摄入精准监测需求，现有AI饮食评估在从单目图像恢复尺寸信息方面存在挑战，一些3D重建方法无法恢复真实比例。

Method: 利用在大规模数据集上训练的模型提取的视觉特征来估计重建对象的比例，将单视图3D重建转换为符合实际物理意义的模型。

Result: 在两个公开数据集上的实验表明，该方法始终优于现有技术，平均绝对体积估计误差降低近30%。

Conclusion: 此方法具有提升精准营养领域的潜力。

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [313] [Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data](https://arxiv.org/abs/2601.20072)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.CV

TL;DR: 提出半监督掩码自编码器SSMAE应对标注数据稀缺时训练视觉Transformer的挑战，在CIFAR数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决标注数据稀缺、未标注数据丰富时训练视觉Transformer的难题。

Method: 提出SSMAE框架，联合优化掩码图像重建和分类，使用动态选择的伪标签，引入验证驱动的门控机制。

Result: 在CIFAR - 10和CIFAR - 100上，SSMAE始终优于监督ViT和微调的MAE，在低标注制度下增益最大。

Conclusion: 在数据高效的Transformer训练中，引入伪标签的时机和生成方式同样重要。

Abstract: We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.

</details>


### [314] [Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization](https://arxiv.org/abs/2601.20301)
*Jialuo He,Huangxun Chen*

Main category: cs.CV

TL;DR: 提出C - SAM框架，通过对剪枝掩码进行扰动提升模型紧凑性与鲁棒性，实验显示比基线有更好的认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有SAM技术在设备端DNN部署紧凑性要求方面研究不足，简单剪枝或剪枝后应用SAM存在问题。

Method: 提出C - SAM框架，将锐度感知学习从参数扰动转移到掩码扰动，在训练中显式扰动剪枝掩码。

Result: 在多个数据集和模型上实验表明，C - SAM比强基线有更高认证鲁棒性，提升达42%，且任务准确率与未剪枝模型相当。

Conclusion: C - SAM能同时优化模型紧凑性和对输入变化的鲁棒性。

Abstract: Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.

</details>


### [315] [Physically Guided Visual Mass Estimation from a Single RGB Image](https://arxiv.org/abs/2601.20303)
*Sungjae Lee,Junhan Jeong,Yeonjoo Hong,Kwang In Kim*

Main category: cs.CV

TL;DR: 提出单图像质量估计物理结构化框架，结合视觉线索与物理因素，实验显示优于现有方法


<details>
  <summary>Details</summary>
Motivation: 从视觉输入估计物体质量有挑战，因质量依赖几何体积和材料密度且二者不可直接观测，需物理有意义表征约束解空间

Method: 从单张RGB图像通过单目深度估计恢复物体三维几何信息，用视觉语言模型提取材料语义，通过实例自适应门控机制融合几何、语义和外观表征，在仅质量监督下用独立回归头预测两个物理引导的潜在因素

Result: 在image2mass和ABO - 500上的实验表明该方法始终优于现有方法

Conclusion: 所提物理结构化框架有效，可用于单图像质量估计

Abstract: Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.

</details>


### [316] [Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction](https://arxiv.org/abs/2601.20304)
*Genyuan Zhang,Zihao Wang,Zhifan Gao,Lei Xu,Zhen Zhou,Haijun Yu,Jianjia Zhang,Xiujian Liu,Weiwei Zhang,Shaoyu Wang,Huazhu Fu,Fenglin Liu,Weiwen Wu*

Main category: cs.CV

TL;DR: 提出结构约束语言感知扩散模型（SLDM）用于低剂量对比剂CT血管造影重建，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以实现不完全配对图像的准确增强，原因是模型识别特定结构能力有限。

Method: 提出SLDM模型，有效提取图像结构先验信息约束推理，引入语义监督确保结构一致性，应用减影血管造影增强模块。

Result: 视觉对比定性分析和多指标定量结果表明方法在低剂量造影剂CT血管造影重建中有效。

Conclusion: SLDM模型能够解决现有方法局限，准确增强低剂量CT图像。

Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.

</details>


### [317] [CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization](https://arxiv.org/abs/2601.20355)
*Yue Liang,Jiatong Du,Ziyi Yang,Yanjun Huang,Hong Chen*

Main category: cs.CV

TL;DR: 提出CURVE框架解决场景图过拟合问题，在零样本迁移和低数据模拟到真实适应中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 场景图常因过拟合到虚假关联，阻碍分布外泛化能力，需解决此局限。

Method: 提出CURVE框架，结合变分不确定性建模与不确定性引导的结构正则化，运用原型条件去偏。

Result: 在零样本迁移和低数据模拟到真实适应中，验证了其学习域稳定稀疏拓扑和提供可靠不确定性估计的能力。

Conclusion: CURVE框架能抑制高方差、特定环境关系，促进稀疏和域稳定拓扑，支持分布变化下的风险预测。

Abstract: Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.

</details>


### [318] [Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models](https://arxiv.org/abs/2601.20419)
*Yuhao Sun,Chengyi Cai,Jiacheng Zhang,Zesheng Ye,Xingliang Yuan,Feng Liu*

Main category: cs.CV

TL;DR: 现有研究表明细粒度文本描述与局部图像块对齐可提升预训练视觉 - 语言模型零样本性能，但存在冗余信息影响对齐效果。本文提出BiFTA方法，从视图和描述两方面去除冗余，在6个基准数据集上取得更好零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度文本描述和局部图像块存在冗余信息，导致文本 - 视觉对齐效果不佳。

Method: 提出BiFTA方法，包括视图细化（去除高IoU比率的冗余图像块）和描述细化（去除高成对余弦相似度的冗余文本描述）。

Result: BiFTA在基于ViT和ResNet的CLIP的6个基准数据集上实现了优越的零样本性能。

Conclusion: 在视觉 - 文本对齐中去除冗余信息是必要的。

Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.

</details>


### [319] [Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI](https://arxiv.org/abs/2601.20503)
*Jesse Phitidis,Alison Q. Smithard,William N. Whiteley,Joanna M. Wardlaw,Miguel O. Bernabeu,Maria Valdés Hernández*

Main category: cs.CV

TL;DR: 研究利用部分标注数据训练联合白质高信号（WMH）和缺血性中风病灶（ISL）分割模型的六种策略，发现使用伪标签效果最佳。


<details>
  <summary>Details</summary>
Motivation: WMH和ISL在脑部MRI的液体衰减反转恢复（FLAIR）序列中视觉上相互混淆，且常出现在同一受试者中，开发和验证用于分割和区分这些特征的深度学习模型困难。

Method: 研究了利用部分标注数据训练联合WMH和ISL分割模型的六种策略，将私有全标注和部分标注数据集与公开部分标注数据集结合，共2052个MRI体积。

Result: 几种方法能有效利用部分标注数据提升模型性能，使用伪标签效果最好。

Conclusion: 使用伪标签训练联合WMH和ISL分割模型是一种有效策略，可利用部分标注数据提升模型性能。

Abstract: White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.

</details>


### [320] [Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?](https://arxiv.org/abs/2601.20598)
*Lakshman Balasubramanian*

Main category: cs.CV

TL;DR: 论文回顾行人重识别（ReID）训练范式，评估模型跨域鲁棒性，研究基础模型作用，通过对11个模型和9个数据集的分析，发现监督模型跨域表现差，语言对齐模型跨域鲁棒性好。


<details>
  <summary>Details</summary>
Motivation: 解决行人重识别在计算机视觉中仍是难题的现状，研究不同训练范式模型的跨域鲁棒性及基础模型的作用。

Method: 比较监督、自监督和语言对齐三种训练范式，对11个模型和9个数据集进行分析。

Result: 监督模型在训练域占优，但在跨域数据上表现不佳；语言对齐模型虽未显式训练，但在跨域ReID任务中表现出惊人的鲁棒性。

Conclusion: 不同训练范式的行人重识别模型在跨域任务中的表现有明显差异，语言对齐模型更具跨域优势。

Abstract: Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.

</details>


### [321] [CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification](https://arxiv.org/abs/2601.20601)
*Zhuonan Wang,Wenjie Yan,Wenqiao Zhang,Xiaohui Song,Jian Ma,Ke Yao,Yibo Yu,Beng Chin Ooi*

Main category: cs.CV

TL;DR: 提出CLEAR - Mamba框架用于眼科造影图像分类，实验显示其性能优于多个基线模型，为特定模态医学图像分类提供有效方案。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分类方法在眼科造影图像分类中因单模态、细微病变模式和设备差异等，存在泛化和高置信度预测的局限性。

Method: 构建基于MedMamba的CLEAR - Mamba框架，架构上引入HaC层提升跨域适应性，训练上采用基于证据不确定性学习的RaP方案提升稳定性和可靠性，还构建大规模眼科造影数据集。

Result: CLEAR - Mamba在各项指标上始终优于多个基线模型，在多疾病分类和可靠性感知预测上优势明显。

Conclusion: 该研究为特定模态医学图像分类任务提供了平衡泛化性和可靠性的有效解决方案。

Abstract: Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.

</details>


### [322] [GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2601.20618)
*Shuguang Zhang,Junhong Lian,Guoxin Yu,Baoxun Xu,Xiang Ao*

Main category: cs.CV

TL;DR: 针对多模态讽刺检测现有方法的不足，提出GDCNet框架，实验证明其准确性和鲁棒性佳。


<details>
  <summary>Details</summary>
Motivation: 现有多模态讽刺检测方法在处理视觉和文本内容关联弱或语义间接情况时困难，且使用大语言模型生成讽刺线索有噪声。

Method: 提出GDCNet框架，利用多模态大语言模型生成的描述性、基于事实的图像描述作为稳定语义锚点，计算生成描述与原文的语义和情感差异并测量视觉 - 文本保真度，通过门控模块融合差异特征与视觉和文本表示。

Result: 在多模态讽刺检测基准测试中展示出卓越的准确性和鲁棒性，在MMSD2.0基准上达到新的最优水平。

Conclusion: GDCNet框架有效解决了现有多模态讽刺检测方法的不足，具有良好性能。

Abstract: Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.

</details>


### [323] [Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework](https://arxiv.org/abs/2601.20689)
*Xinyue Li,Zhichao Zhang,Zhiming Xu,Shubo Xu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出LEAF框架，从MLLM教师模型中提取感知质量先验到轻量级学生回归器，减少图像质量评估所需人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的图像质量评估方法计算成本高且依赖大量MOS标注，核心瓶颈是MOS尺度校准。

Method: 提出LEAF框架，教师模型通过逐点判断和成对偏好进行密集监督，学生模型通过联合蒸馏学习教师模型质量感知模式，并在小MOS子集上校准。

Result: 在用户生成和AI生成的IQA基准测试中，该方法显著减少人工标注需求，同时保持与人类标注的强相关性。

Conclusion: 所提方法使有限标注预算下的轻量级IQA成为可能。

Abstract: Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.

</details>


### [324] [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/abs/2601.20705)
*Zhuang Yu,Lei Shen,Jing Zhao,Shiliang Sun*

Main category: cs.CV

TL;DR: 本文提出针对多模态大模型在教育内容上表现评估的LEMON基准，含多学科课程视频及大量QA对，实验显示模型在部分任务有性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在长文本、知识密集、时间结构教育内容上的表现未被充分探索，需搭建评估基准。

Method: 创建LEMON基准，涵盖多学科课程视频片段，设计多种任务和子任务。

Result: 实验揭示不同任务间模型存在显著性能差距，如GPT - 4o在时间推理和教学预测上有困难。

Conclusion: LEMON可作为推进长形式教学内容多模态感知、推理和生成的可扩展且具挑战性的基准。

Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.

</details>


### [325] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: 提出Li - ViP3D++框架，用QGDF在查询空间融合多模态数据，在nuScenes上提升性能且更快，证明查询空间全可微相机 - 激光雷达融合可提升端到端感知预测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模块化管道限制信息流动、放大上游错误，基于查询的感知预测模型未充分探索相机和激光雷达在查询空间的互补性，融合方案有缺陷。

Method: 提出Li - ViP3D++框架，引入Query - Gated Deformable Fusion（QGDF）在查询空间融合多视图RGB和激光雷达数据，联合优化检测、跟踪和多假设轨迹预测。

Result: 在nuScenes上，Li - ViP3D++提升端到端行为和检测质量，EPA和mAP更高，减少误报，且比之前的Li - ViP3D更快。

Conclusion: 查询空间全可微相机 - 激光雷达融合可在不牺牲可部署性的情况下增加端到端感知预测的鲁棒性。

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


### [326] [FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models](https://arxiv.org/abs/2601.20791)
*Haonan Zhong,Wei Song,Tingxu Han,Maurice Pagnucco,Jingling Xue,Yang Song*

Main category: cs.CV

TL;DR: 提出无训练去偏框架FairT2V用于文本到视频生成，可减少人口统计偏差，以Open - Sora模型实验验证效果。


<details>
  <summary>Details</summary>
Motivation: 文本到视频扩散模型虽进展迅速，但人口统计偏差尤其是性别偏差未被充分研究，需解决该问题。

Method: 分析T2V模型人口统计偏差来源，用性别倾向分数量化；通过基于锚点的球面测地变换中和提示嵌入来减轻偏差，结合动态去噪时间表保持时间连贯性；提出结合VideoLLM推理和人工验证的视频级公平性评估协议。

Result: 在Open - Sora模型实验中，FairT2V大幅减少各职业的人口统计偏差，对视频质量影响极小。

Conclusion: FairT2V是一个有效的无训练去偏框架，能减轻T2V模型的编码器诱导偏差。

Abstract: Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.

</details>


### [327] [Open-Vocabulary Functional 3D Human-Scene Interaction Generation](https://arxiv.org/abs/2601.20835)
*Jie Liu,Yu Sun,Alpar Cseke,Yao Feng,Nicolas Heron,Michael J. Black,Yan Zhang*

Main category: cs.CV

TL;DR: 本文提出无训练、功能驱动的框架FunHSI，解决3D人类与3D场景功能交互问题，实验表明其能跨场景生成功能正确、物理合理的交互。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对物体功能和人-场景接触的显式推理，导致交互不合理或功能错误，为解决这一问题开展研究。

Method: 提出FunHSI框架，基于任务提示进行功能感知接触推理，利用视觉语言模型合成图像中执行任务的人体并估计3D姿态，通过分阶段优化完善3D人体配置。

Result: FunHSI不仅能合成更合理的通用3D交互，还支持细粒度功能交互。

Conclusion: FunHSI能在不同室内外场景中持续生成功能正确且物理合理的人-场景交互。

Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

</details>


### [328] [A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion](https://arxiv.org/abs/2601.20847)
*Willams de Lima Costa,Thifany Ketuli Silva de Souza,Jonas Ferreira Silva,Carlos Gabriel Bezerra Pereira,Bruno Reis Vila Nova,Leonardo Silvino Brito,Rafael Raider Leoni,Juliano Silva,Valter Ferreira,Sibele Miguel Soares Neto,Samantha Uehara,Daniel Giacomo,João Marcelo Teixeira,Veronica Teichrieb,Cristiano Coelho de Araújo*

Main category: cs.CV

TL;DR: 提出多模态框架和ROAD数据集用于路面分类，效果超现有技术，在复杂视觉条件下性能稳定。


<details>
  <summary>Details</summary>
Motivation: 现有路面分类技术因传感模态和数据集有限，难以在不同操作条件下泛化。

Method: 引入多模态框架，融合图像和惯性测量，使用轻量级双向交叉注意力模块和自适应门控层；推出新数据集ROAD，含不同子集。

Result: 在PVS基准和ROAD子集中性能提升，少数类F1分数更高，在复杂视觉条件下性能稳定。

Conclusion: 结合相机和IMU传感器与多模态注意力机制，为路面理解提供可扩展、鲁棒的基础。

Abstract: Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.

</details>


### [329] [Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining](https://arxiv.org/abs/2601.20333)
*Ali Zia,Usman Ali,Umer Ramzan,Abdul Rehman,Abdelwahed Khamis,Wei Xiang*

Main category: cs.CV

TL;DR: 本文提出TopoOT框架用于异常分割，结合多过滤持久图和测试时自适应，在2D和3D异常检测基准上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 深度拓扑数据分析适用于异常分割，传统基于阈值的二值化方法在分布偏移下表现不佳，需要更鲁棒的方法。

Method: 引入TopoOT框架，采用最优传输链算法，结合多过滤持久图和测试时自适应，用稳定性感知伪标签监督轻量级头网络训练。

Result: 在标准2D和3D异常检测基准上，TopoOT取得SOTA性能，在2D数据集上平均F1比最具竞争力的方法高24.1%，在3D异常分割基准上高10.2%。

Conclusion: TopoOT框架在异常分割任务中具有良好的鲁棒性和性能，能有效应对域偏移问题。

Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [330] [Assembling the Mind's Mosaic: Towards EEG Semantic Intent Decoding](https://arxiv.org/abs/2601.20447)
*Jiahe Li,Junru Chen,Fanqi Shen,Jialan Yang,Jada Li,Zhizhang Yuan,Baowen Cheng,Meng Li,Yang Yang*

Main category: q-bio.NC

TL;DR: 提出新框架SID及深度学习架构BrainMosaic，助力脑机接口自然通信，实验显示优于现有框架。


<details>
  <summary>Details</summary>
Motivation: 现有脑机接口框架语义表示简化、缺乏可解释性，难以实现自然交流。

Method: 引入Semantic Intent Decoding (SID) 框架，基于三个核心原则建模语义；构建BrainMosaic架构，通过集合匹配解码语义单元，语义引导重构句子。

Result: 在多语言EEG和临床SEEG数据集实验表明，SID和BrainMosaic明显优于现有框架。

Conclusion: SID和BrainMosaic为脑机接口介导的自然有效交流铺平道路。

Abstract: Enabling natural communication through brain-computer interfaces (BCIs) remains one of the most profound challenges in neuroscience and neurotechnology. While existing frameworks offer partial solutions, they are constrained by oversimplified semantic representations and a lack of interpretability. To overcome these limitations, we introduce Semantic Intent Decoding (SID), a novel framework that translates neural activity into natural language by modeling meaning as a flexible set of compositional semantic units. SID is built on three core principles: semantic compositionality, continuity and expandability of semantic space, and fidelity in reconstruction. We present BrainMosaic, a deep learning architecture implementing SID. BrainMosaic decodes multiple semantic units from EEG/SEEG signals using set matching and then reconstructs coherent sentences through semantic-guided reconstruction. This approach moves beyond traditional pipelines that rely on fixed-class classification or unconstrained generation, enabling a more interpretable and expressive communication paradigm. Extensive experiments on multilingual EEG and clinical SEEG datasets demonstrate that SID and BrainMosaic offer substantial advantages over existing frameworks, paving the way for natural and effective BCI-mediated communication.

</details>
