<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 83]
- [cs.CE](#cs.CE) [Total: 11]
- [cs.DB](#cs.DB) [Total: 14]
- [cs.DC](#cs.DC) [Total: 21]
- [cs.DS](#cs.DS) [Total: 12]
- [cs.GT](#cs.GT) [Total: 9]
- [cs.IR](#cs.IR) [Total: 21]
- [cs.LG](#cs.LG) [Total: 208]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.SE](#cs.SE) [Total: 27]
- [q-fin.CP](#q-fin.CP) [Total: 3]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 20]
- [stat.CO](#stat.CO) [Total: 4]
- [cs.AR](#cs.AR) [Total: 5]
- [eess.SY](#eess.SY) [Total: 10]
- [cs.LO](#cs.LO) [Total: 3]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [econ.GN](#econ.GN) [Total: 11]
- [cs.CR](#cs.CR) [Total: 23]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.HC](#cs.HC) [Total: 7]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.MM](#cs.MM) [Total: 2]
- [hep-ex](#hep-ex) [Total: 1]
- [cs.IT](#cs.IT) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [nlin.AO](#nlin.AO) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [quant-ph](#quant-ph) [Total: 7]
- [math.NT](#math.NT) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 63]
- [cs.DM](#cs.DM) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [stat.ME](#stat.ME) [Total: 7]
- [math.NA](#math.NA) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [eess.SP](#eess.SP) [Total: 5]
- [math.OC](#math.OC) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [physics.chem-ph](#physics.chem-ph) [Total: 3]
- [cs.RO](#cs.RO) [Total: 21]
- [cs.SI](#cs.SI) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 4]
- [math.RA](#math.RA) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 5]
- [cs.SD](#cs.SD) [Total: 17]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cs.CY](#cs.CY) [Total: 13]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.CL](#cs.CL) [Total: 63]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Comparative Study of Controllability, Explainability, and Performance in Dysfluency Detection Models](https://arxiv.org/abs/2509.00058)
*Eric Zhang,Li Wei,Sarah Chen,Michael Wang*

Main category: cs.AI

TL;DR: 本文对四种言语不流畅检测方法进行系统对比分析，指出各方法优缺点及权衡，并给出未来方向和实施建议。


<details>
  <summary>Details</summary>
Motivation: 现有言语不流畅检测模型虽性能提升，但临床应用还需可控性和可解释性，需对不同方法进行对比分析。

Method: 从性能、可控性和可解释性三个维度，对YOLO - Stutter、FluentNet、UDM和SSDM四种方法进行综合评估，包括多数据集测试和专家评估。

Result: YOLO - Stutter和FluentNet高效简单，但透明度有限；UDM在准确性和临床可解释性上平衡最佳；SSDM有前景但实验中未能完全复现。

Conclusion: 分析凸显了不同方法间的权衡，指明了临床可行的言语不流畅建模的未来方向，并给出各方法实施见解和部署考虑。

Abstract: Recent advances in dysfluency detection have introduced a variety of modeling
paradigms, ranging from lightweight object-detection inspired networks
(YOLOStutter) to modular interpretable frameworks (UDM). While performance on
benchmark datasets continues to improve, clinical adoption requires more than
accuracy: models must be controllable and explainable. In this paper, we
present a systematic comparative analysis of four representative
approaches--YOLO-Stutter, FluentNet, UDM, and SSDM--along three dimensions:
performance, controllability, and explainability. Through comprehensive
evaluation on multiple datasets and expert clinician assessment, we find that
YOLO-Stutter and FluentNet provide efficiency and simplicity, but with limited
transparency; UDM achieves the best balance of accuracy and clinical
interpretability; and SSDM, while promising, could not be fully reproduced in
our experiments. Our analysis highlights the trade-offs among competing
approaches and identifies future directions for clinically viable dysfluency
modeling. We also provide detailed implementation insights and practical
deployment considerations for each approach.

</details>


### [2] [Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy Against Benchmark Contamination](https://arxiv.org/abs/2509.00072)
*Terry Jingchen Zhang,Gopal Dev,Ning Wang,Nicole Ni,Wenyuan Jiang,Yinya Huang,Bernhard Schölkopf,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 研究用可扩展框架合成研究级QA评估大语言模型，发现模型在知识截止日期附近无显著性能下降，推测合成管道的多步推理可减轻基准污染，开源代码和数据集并倡导范式转变。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型能力评估受数据污染问题影响，怀疑静态基准测试衡量的是真正推理还是记忆。

Method: 使用可扩展框架从arXiv论文合成研究级QA，利用研究出版物的时间结构，评估4个前沿模型在1643个多步推理问题上的表现，并与之前研究对比分析。

Result: 不同大小、开发者和发布时间的模型在知识截止日期附近无显著性能下降。

Conclusion: 合成管道的多步推理能减轻基准污染，倡导优先使用推理驱动合成来构建基准。

Abstract: Capability evaluation of large language models (LLMs) is increasingly
shadowed by rising concerns of data contamination that cast doubts on whether
static benchmarks measure genuine reasoning or mere memorization. We present an
empirical study using an infinitely scalable framework to synthesize
research-level QA directly from arXiv papers, harnessing the natural temporal
structure of research publications where performance decay after knowledge
cutoffs may indicate potential contamination. We evaluated 4 frontier model
represented by 2 models of different knowledge cutoff dates per family on 1,643
multi-step reasoning questions synthesized from 20,277 arXiv papers stratified
over 26 months, covering at least 6 months before and after all cutoff dates.
Our results consistently showed a lack of significant performance decay near
knowledge cutoff dates for models of various sizes, developers, and release
dates. We further performed a comparative analysis with previous longitudinal
studies that reported significant post-cutoff performance decay using directly
retrieved questions based on public data. we hypothesize that the multi-step
reasoning required by our synthesis pipeline offered additional complexity that
goes deeper than shallow memorization, which effectively serves a mitigation
strategy against benchmark contamination. We fully open source our code and
dataset to aid reproducibility and advocate for a paradigm shift that
prioritize reasoning-driven synthesis to construct benchmarks over simply
collecting newly released questions periodically.

</details>


### [3] [Language and Experience: A Computational Model of Social Learning in Complex Tasks](https://arxiv.org/abs/2509.00074)
*Cédric Colas,Tracey Mills,Ben Prystawski,Michael Henry Tessler,Noah Goodman,Jacob Andreas,Joshua Tenenbaum*

Main category: cs.AI

TL;DR: 提出计算框架模拟社会学习，实验表明语言指导可加速学习，探索知识跨代积累及人机协作学习。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何整合他人语言指导和直接经验的知识，以及AI系统如何做到。

Method: 提出计算框架，将预训练语言模型转化为概率模型，结合行为实验和10个视频游戏模拟。

Result: 语言指导可减少风险交互、加速关键发现，促进人类和模型的学习，知识能跨代积累，实现人机知识转移。

Conclusion: 结构化、与语言兼容的表征可实现人机协作学习。

Abstract: The ability to combine linguistic guidance from others with direct experience
is central to human development, enabling safe and rapid learning in new
environments. How do people integrate these two sources of knowledge, and how
might AI systems? We present a computational framework that models social
learning as joint probabilistic inference over structured, executable world
models given sensorimotor and linguistic data. We make this possible by turning
a pretrained language model into a probabilistic model of how humans share
advice conditioned on their beliefs, allowing our agents both to generate
advice for others and to interpret linguistic input as evidence during Bayesian
inference. Using behavioral experiments and simulations across 10 video games,
we show how linguistic guidance can shape exploration and accelerate learning
by reducing risky interactions and speeding up key discoveries in both humans
and models. We further explore how knowledge can accumulate across generations
through iterated learning experiments and demonstrate successful knowledge
transfer between humans and models -- revealing how structured,
language-compatible representations might enable human-machine collaborative
learning.

</details>


### [4] [Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation](https://arxiv.org/abs/2509.00079)
*Andrew G. A. Correa,Ana C. H de Matos*

Main category: cs.AI

TL;DR: 提出熵引导的细化方法，小模型以约三分之一成本达推理模型95%质量，提升准确性。


<details>
  <summary>Details</summary>
Motivation: 推理模型成本高、延迟大，需低成本方法提升小模型性能。

Method: 提出熵引导的细化方法，用令牌级不确定性触发细化，提取对数概率、计算熵，用OR逻辑触发，回传不确定性报告指导修正。

Result: 小模型达参考推理模型95%质量，成本约三分之一，31%响应选择性细化，准确率提升16个百分点。

Conclusion: 该方法是单次推理和昂贵推理链的有效折中，适用于对质量和成本有要求的生产部署。

Abstract: Reasoning models often outperform smaller models but at 3--5$\times$ higher
cost and added latency. We present entropy-guided refinement: a lightweight,
test-time loop that uses token-level uncertainty to trigger a single, targeted
refinement pass. We extract logprobs, compute Shannon entropy on top-$k$
alternatives, and apply a simple OR-logic trigger over perplexity, maximum
token entropy, and low-confidence-token count. Unlike approaches that use
entropy only for measurement or decoding, we pass a compact uncertainty report
(tokens, confidences, alternatives, context) back to the model to guide
corrective edits. On representative technical queries across reasoning,
mathematics, and code generation tasks, a small model with our loop approaches
95\% of a reference reasoning model's quality at approximately one-third of the
cost. The method achieves selective refinement on ~31\% of responses while
improving accuracy by 16 percentage points over single-pass inference. We
demonstrate that this uncertainty-aware loop provides an effective middle
ground between single-pass inference and expensive reasoning chains, making it
practical for production deployments where both quality and cost matter.

</details>


### [5] [Analysis of Error Sources in LLM-based Hypothesis Search for Few-Shot Rule Induction](https://arxiv.org/abs/2509.01016)
*Aishni Parab,Hongjing Lu,Ying Nian Wu,Sumit Gulwani*

Main category: cs.AI

TL;DR: 对比基于大语言模型的假设搜索框架和直接程序生成方法在少样本规则归纳任务中的表现，显示假设搜索性能可与人类媲美，指出程序归纳方法的瓶颈与挑战。


<details>
  <summary>Details</summary>
Motivation: 探究不同方法在少样本规则归纳任务中的表现，以更好地建模归纳推理。

Method: 对比基于大语言模型的假设搜索框架和直接程序生成方法。

Result: 假设搜索性能与人类相当，直接程序生成表现较差，误差分析揭示假设生成的关键瓶颈。

Conclusion: 强调基于大语言模型的假设搜索在建模归纳推理方面的潜力，以及构建更高效系统面临的挑战。

Abstract: Inductive reasoning enables humans to infer abstract rules from limited
examples and apply them to novel situations. In this work, we compare an
LLM-based hypothesis search framework with direct program generation approaches
on few-shot rule induction tasks. Our findings show that hypothesis search
achieves performance comparable to humans, while direct program generation
falls notably behind. An error analysis reveals key bottlenecks in hypothesis
generation and suggests directions for advancing program induction methods.
Overall, this paper underscores the potential of LLM-based hypothesis search
for modeling inductive reasoning and the challenges in building more efficient
systems.

</details>


### [6] [Wrong Face, Wrong Move: The Social Dynamics of Emotion Misperception in Agent-Based Models](https://arxiv.org/abs/2509.00080)
*David Freire-Obregón*

Main category: cs.AI

TL;DR: 研究情绪感知准确性对代理情感和空间行为的影响，发现低准确性导致信任降低、情感瓦解和社会混乱，高准确性则有更好表现。


<details>
  <summary>Details</summary>
Motivation: 理解人类检测和响应他人情绪的能力对理解社会行为的重要性，研究感知准确性对情感和空间行为的影响。

Method: 使用不同准确率的情绪分类器实例化代理，在二维环形网格上进行本地通信，根据感知到的情绪移动，进行多组实验。

Result: 低准确率分类器导致信任降低、情感瓦解和社会混乱，高准确率分类器形成稳定情感集群和抗干扰能力，即使在中性场景中，误判也会导致隔离和凝聚力瓦解。

Conclusion: 情绪识别中的偏差或不精确可能显著扭曲社会过程并破坏情感整合。

Abstract: The ability of humans to detect and respond to others' emotions is
fundamental to understanding social behavior. Here, agents are instantiated
with emotion classifiers of varying accuracy to study the impact of perceptual
accuracy on emergent emotional and spatial behavior. Agents are visually
represented with face photos from the KDEF database and endowed with one of
three classifiers trained on the JAFFE (poor), CK+ (medium), or KDEF (high)
datasets. Agents communicate locally on a 2D toroidal lattice, perceiving
neighbors' emotional state based on their classifier and responding with
movement toward perceived positive emotions and away from perceived negative
emotions. Note that the agents respond to perceived, instead of ground-truth,
emotions, introducing systematic misperception and frustration. A battery of
experiments is carried out on homogeneous and heterogeneous populations and
scenarios with repeated emotional shocks. Results show that low-accuracy
classifiers on the part of the agent reliably result in diminished trust,
emotional disintegration into sadness, and disordered social organization. By
contrast, the agent that develops high accuracy develops hardy emotional
clusters and resilience to emotional disruptions. Even in emotionally neutral
scenarios, misperception is enough to generate segregation and disintegration
of cohesion. These findings underscore the fact that biases or imprecision in
emotion recognition may significantly warp social processes and disrupt
emotional integration.

</details>


### [7] [Ensemble Debates with Local Large Language Models for AI Alignment](https://arxiv.org/abs/2509.00091)
*Ephraiem Sarabamoun*

Main category: cs.AI

TL;DR: 研究本地开源集成辩论能否改善大语言模型与人类价值观的对齐推理，结果显示集成优于单模型基线，还提供相关资源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险决策中作用增大，需与人类价值观对齐，但依赖专有API限制了可重复性和广泛参与。

Method: 进行150场跨越15种场景和5种集成配置的辩论。

Result: 集成在7分评分体系中优于单模型基线（3.48 vs. 3.13），在推理深度和论点质量上提升最大，在真实性和人类增强方面改善最明显。

Conclusion: 提供代码、提示和辩论数据集，为基于集成的对齐评估提供可访问和可重复的基础。

Abstract: As large language models (LLMs) take on greater roles in high-stakes
decisions, alignment with human values is essential. Reliance on proprietary
APIs limits reproducibility and broad participation. We study whether local
open-source ensemble debates can improve alignmentoriented reasoning. Across
150 debates spanning 15 scenarios and five ensemble configurations, ensembles
outperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13),
with the largest gains in reasoning depth (+19.4%) and argument quality
(+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human
enhancement (+0.80). We provide code, prompts, and a debate data set, providing
an accessible and reproducible foundation for ensemble-based alignment
evaluation.

</details>


### [8] [MODE: Mixture of Document Experts for RAG](https://arxiv.org/abs/2509.00100)
*Rahul Anand*

Main category: cs.AI

TL;DR: 提出轻量级MODE替代RAG，在小数据集上效果好、速度快。


<details>
  <summary>Details</summary>
Motivation: RAG对特定领域小数据集而言依赖大规模向量数据库和跨编码器，过于复杂。

Method: 用簇路由检索替代细粒度近邻搜索，对文档嵌入、聚类，用质心表示，查询时路由到质心所在簇检索。

Result: 在HotpotQA和SQuAD语料库上，MODE答案质量与密集检索基线相当或更好，减少端到端检索时间。

Conclusion: MODE为重视简单性、速度和主题聚焦的中小语料库提供实用方案。

Abstract: Retrieval-Augmented Generation (RAG) often relies on large vector databases
and cross-encoders tuned for large-scale corpora, which can be excessive for
small, domain-specific collections. We present MODE (Mixture of Document
Experts), a lightweight alternative that replaces fine-grained nearest-neighbor
search with cluster-and-route retrieval. Documents are embedded, grouped into
semantically coherent clusters, and represented by cached centroids. At query
time, we route to the top centroid(s) and retrieve context only within those
clusters, eliminating external vector-database infrastructure and reranking
while keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,
MODE matches or exceeds a dense-retrieval baseline in answer quality while
reducing end-to-end retrieval time. Ablations show that cluster granularity and
multi-cluster routing control the recall/precision trade-off, and that tighter
clusters improve downstream accuracy. MODE offers a practical recipe for small
and medium corpora where simplicity, speed, and topical focus matter.

</details>


### [9] [Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems](https://arxiv.org/abs/2509.00115)
*Manish Shukla*

Main category: cs.AI

TL;DR: 本文为先前论文的续作，弥补了算法实例和实证证据的缺失。回顾基准测试和部署情况，提出AMDM算法，经实验验证该算法能降低异常检测延迟和误报率。


<details>
  <summary>Details</summary>
Motivation: 先前论文未提供算法实例和实证证据，需要进一步完善对代理人工智能评估的研究。

Method: 回顾近期基准测试和工业部署，对2023 - 2025年84篇论文进行系统综述；提出Adaptive Multi - Dimensional Monitoring (AMDM)算法；进行模拟和真实世界实验。

Result: AMDM算法将模拟目标漂移的异常检测延迟从12.3秒降至5.6秒，误报率从4.5%降至0.9%；给出比较表和ROC/PR曲线，重新分析案例研究找出缺失指标。

Conclusion: 所提出的AMDM算法能有效改善代理人工智能的异常检测性能，论文提供代码、数据和复现清单方便复现。

Abstract: Agentic artificial intelligence (AI) -- multi-agent systems that combine
large language models with external tools and autonomous planning -- are
rapidly transitioning from research laboratories into high-stakes domains. Our
earlier "Basic" paper introduced a five-axis framework and proposed preliminary
metrics such as goal drift and harm reduction but did not provide an
algorithmic instantiation or empirical evidence. This "Advanced" sequel fills
that gap. First, we revisit recent benchmarks and industrial deployments to
show that technical metrics still dominate evaluations: a systematic review of
84 papers from 2023--2025 found that 83% report capability metrics while only
30% consider human-centred or economic axes [2]. Second, we formalise an
Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises
heterogeneous metrics, applies per-axis exponentially weighted moving-average
thresholds and performs joint anomaly detection via the Mahalanobis distance.
Third, we conduct simulations and real-world experiments. AMDM cuts
anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and
reduces false-positive rates from 4.5% to 0.9% compared with static thresholds.
We present a comparison table and ROC/PR curves, and we reanalyse case studies
to surface missing metrics. Code, data and a reproducibility checklist
accompany this paper to facilitate replication.

</details>


### [10] [Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning](https://arxiv.org/abs/2509.00125)
*Ang Li,Zhihang Yuan,Yang Zhang,Shouda Liu,Yisen Wang*

Main category: cs.AI

TL;DR: 本文指出强化学习可验证反馈（RLVF）在提升大语言模型推理能力时存在局限，提出困难感知确定性引导探索（DACE）算法，实验表明其显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: RLVF依赖稀疏的基于结果的奖励，无法为推理过程提供细致指导，阻碍高效学习。

Method: 引入DACE算法，根据策略成功率在线评估任务难度，利用该信号调整内在奖励，困难任务惩罚高确定性以鼓励探索，简单任务奖励高确定性以提高学习效率。

Result: 在具有挑战性的数学推理基准测试（AIME、MATH）中，DACE显著优于强基线模型，训练的模型不仅准确率更高，且在扩大测试时计算量时表现更稳健。

Conclusion: DACE的自适应方法能在不牺牲精度的前提下促进有效探索。

Abstract: Reinforcement Learning with Verifiable Feedback (RLVF) has become a key
technique for enhancing the reasoning abilities of Large Language Models
(LLMs). However, its reliance on sparse, outcome based rewards, which only
indicate if a final answer is correct or not, fails to provide granular
guidance on the reasoning process itself. This limitation hinders efficient
learning, as the model cannot distinguish between high quality and inefficient
solutions, nor can it learn effectively from different types of failures. To
address this, we observe that an LLMs self-certainty often correlates with task
difficulty and solution quality. We introduce Difficulty Aware Certainty guided
Exploration (DACE), a novel RL algorithm that leverages this insight to
dynamically balance the exploration exploitation trade-off. DACE assesses task
difficulty online based on the policys success rate. It then uses this signal
to modulate an intrinsic reward: for difficult tasks where the model is
struggling, DACE encourages exploration by penalizing high certainty; for
easier tasks, it encourages learning efficiency by rewarding high certainty.
Experiments on challenging mathematical reasoning benchmarks (AIME, MATH) show
that DACE significantly outperforms strong baselines. The DACE-trained models
not only achieve higher accuracy but also demonstrate more robust performance
when scaling test-time compute, validating that our adaptive approach fosters
effective exploration without sacrificing precision.

</details>


### [11] [Optimizing Health Coverage in Ethiopia: A Learning-augmented Approach and Persistent Proportionality Under an Online Budget](https://arxiv.org/abs/2509.00135)
*Davin Choo,Yohai Trabelsi,Fentabil Getnet,Samson Warkaye Lamma,Wondesen Nigatu,Kasahun Sime,Lisa Matay,Milind Tambe,Stéphane Verguet*

Main category: cs.AI

TL;DR: 埃塞俄比亚卫生部加强卫生站以扩大基本医疗服务可及性，但因预算有限需优化框架。本文开发HARP工具及两种算法，并在三个地区验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚卫生部加强卫生站扩大医疗服务可及性，但预算有限且有其他优先级事务，需优化框架指导各地区的优先级安排。

Method: 开发基于决策支持优化框架的Health Access Resource Planner (HARP)工具，提出学习增强方法和贪心算法。

Result: 与埃塞俄比亚公共卫生研究所和卫生部合作，在三个地区的不同规划场景中验证了方法的实证有效性。

Conclusion: 所提出的工具和算法能在预算不确定情况下最大化人口覆盖，满足地区特定比例目标。

Abstract: As part of nationwide efforts aligned with the United Nations' Sustainable
Development Goal 3 on Universal Health Coverage, Ethiopia's Ministry of Health
is strengthening health posts to expand access to essential healthcare
services. However, only a fraction of this health system strengthening effort
can be implemented each year due to limited budgets and other competing
priorities, thus the need for an optimization framework to guide prioritization
across the regions of Ethiopia. In this paper, we develop a tool, Health Access
Resource Planner (HARP), based on a principled decision-support optimization
framework for sequential facility planning that aims to maximize population
coverage under budget uncertainty while satisfying region-specific
proportionality targets at every time step. We then propose two algorithms: (i)
a learning-augmented approach that improves upon expert recommendations at any
single-step; and (ii) a greedy algorithm for multi-step planning, both with
strong worst-case approximation estimation. In collaboration with the Ethiopian
Public Health Institute and Ministry of Health, we demonstrated the empirical
efficacy of our method on three regions across various planning scenarios.

</details>


### [12] [Virtual Group Knowledge and Group Belief in Topological Evidence Models (Extended Version)](https://arxiv.org/abs/2509.00184)
*Alexandru Baltag,Malvin Gattinger,Djanira Gomes*

Main category: cs.AI

TL;DR: 研究多智能体证据模型中群体知识和群体信念概念，对群体证据逻辑公理化并证明可判定性，扩展语言并对动态逻辑公理化。


<details>
  <summary>Details</summary>
Motivation: 在多智能体证据模型中研究群体知识和信念的概念。

Method: 将基于证据的信念和易出错知识的拓扑语义从个体扩展到群体，运用公理化方法。

Result: 完成了群体证据逻辑及其有趣片段（群体知识和群体信念逻辑）的公理化和可判定性证明，扩展语言并对动态逻辑公理化，证明与静态基础共表达。

Conclusion: 成功对相关逻辑进行公理化和可判定性证明，扩展语言后的动态逻辑与静态基础有共表达性。

Abstract: We study notions of (virtual) group knowledge and group belief within
multi-agent evidence models, obtained by extending the topological semantics of
evidence-based belief and fallible knowledge from individuals to groups. We
completely axiomatize and show the decidability of the logic of ("hard" and
"soft") group evidence, and do the same for an especially interesting fragment
of it: the logic of group knowledge and group belief. We also extend these
languages with dynamic evidence-sharing operators, and completely axiomatize
the corresponding logics, showing that they are co-expressive with their static
bases.

</details>


### [13] [Robust Deep Monte Carlo Counterfactual Regret Minimization: Addressing Theoretical Risks in Neural Fictitious Self-Play](https://arxiv.org/abs/2509.00923)
*Zakaria El Jaafari*

Main category: cs.AI

TL;DR: 本文分析神经MCCFR组件有效性随游戏规模的变化，提出自适应框架，通过实验验证组件有效性和交互，给出理论分析、缓解框架、实验验证和实践指南。


<details>
  <summary>Details</summary>
Motivation: MCCFR与深度神经网络结合在不同游戏复杂度下存在规模相关挑战，需分析组件有效性并提出应对框架。

Method: 提出Robust Deep MCCFR框架，包含目标网络延迟更新、均匀探索混合、方差感知训练目标和综合诊断监测，通过对Kuhn和Leduc Poker的系统消融研究。

Result: 在Kuhn Poker上最佳配置最终可利用性达0.0628，比经典框架提升60%；在Leduc Poker上选择性使用组件可利用性为0.2386，比经典框架提升23.5%。

Conclusion: 证明了组件有效性随规模变化，强调组件选择比全面缓解更重要，给出了理论分析、有收敛保证的缓解框架、多尺度实验验证和大游戏部署的实践指南。

Abstract: Monte Carlo Counterfactual Regret Minimization (MCCFR) has emerged as a
cornerstone algorithm for solving extensive-form games, but its integration
with deep neural networks introduces scale-dependent challenges that manifest
differently across game complexities. This paper presents a comprehensive
analysis of how neural MCCFR component effectiveness varies with game scale and
proposes an adaptive framework for selective component deployment. We identify
that theoretical risks such as nonstationary target distribution shifts, action
support collapse, variance explosion, and warm-starting bias have
scale-dependent manifestation patterns, requiring different mitigation
strategies for small versus large games. Our proposed Robust Deep MCCFR
framework incorporates target networks with delayed updates, uniform
exploration mixing, variance-aware training objectives, and comprehensive
diagnostic monitoring. Through systematic ablation studies on Kuhn and Leduc
Poker, we demonstrate scale-dependent component effectiveness and identify
critical component interactions. The best configuration achieves final
exploitability of 0.0628 on Kuhn Poker, representing a 60% improvement over the
classical framework (0.156). On the more complex Leduc Poker domain, selective
component usage achieves exploitability of 0.2386, a 23.5% improvement over the
classical framework (0.3703) and highlighting the importance of careful
component selection over comprehensive mitigation. Our contributions include:
(1) a formal theoretical analysis of risks in neural MCCFR, (2) a principled
mitigation framework with convergence guarantees, (3) comprehensive multi-scale
experimental validation revealing scale-dependent component interactions, and
(4) practical guidelines for deployment in larger games.

</details>


### [14] [HiVA: Self-organized Hierarchical Variable Agent via Goal-driven Semantic-Topological Evolution](https://arxiv.org/abs/2509.00189)
*Jinzhou Tang,Jusheng Zhang,Qinhan Lv,Sidi Liu,Jing Yang,Chengpei Tang,Keze Wang*

Main category: cs.AI

TL;DR: 本文介绍Hierarchical Variable Agent (HiVA)框架，实验表明其在自主任务执行中有效，提升任务准确率和资源效率。


<details>
  <summary>Details</summary>
Motivation: 现有自主代理范式存在权衡问题，固定工作流需手动重新配置，灵活反应循环无法将推理进度提炼为可转移结构，因此需要新框架。

Method: 引入HiVA框架，用Semantic - Topological Evolution (STEV)算法将代理工作流建模为自组织图，通过多臂老虎机注入前向路由、从环境反馈生成诊断梯度和协调更新等迭代过程进行优化。

Result: 在对话、编码、长上下文问答、数学和代理基准测试中，任务准确率提高5 - 10%，资源效率增强。

Conclusion: HiVA在自主任务执行中有效。

Abstract: Autonomous agents play a crucial role in advancing Artificial General
Intelligence, enabling problem decomposition and tool orchestration through
Large Language Models (LLMs). However, existing paradigms face a critical
trade-off. On one hand, reusable fixed workflows require manual reconfiguration
upon environmental changes; on the other hand, flexible reactive loops fail to
distill reasoning progress into transferable structures. We introduce
Hierarchical Variable Agent (HiVA), a novel framework modeling agentic
workflows as self-organized graphs with the Semantic-Topological Evolution
(STEV) algorithm, which optimizes hybrid semantic-topological spaces using
textual gradients as discrete-domain surrogates for backpropagation. The
iterative process comprises Multi-Armed Bandit-infused forward routing,
diagnostic gradient generation from environmental feedback, and coordinated
updates that co-evolve individual semantics and topology for collective
optimization in unknown environments. Experiments on dialogue, coding,
Long-context Q&A, mathematical, and agentic benchmarks demonstrate improvements
of 5-10% in task accuracy and enhanced resource efficiency over existing
baselines, establishing HiVA's effectiveness in autonomous task execution.

</details>


### [15] [Universal Deep Research: Bring Your Own Model and Strategy](https://arxiv.org/abs/2509.00244)
*Peter Belcak,Pavlo Molchanov*

Main category: cs.AI

TL;DR: 介绍通用深度研究（UDR）系统，可让用户创建自定义深度研究策略，无需额外训练或微调。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理硬编码执行特定研究策略，缺乏灵活性。

Method: 提出UDR系统，该系统可包裹任何语言模型，让用户创建、编辑和完善自定义研究策略，还配备示例策略和用户界面。

Result: 创建了能实现用户自定义研究策略的UDR系统。

Conclusion: UDR系统具有通用性，可让用户灵活创建深度研究策略。

Abstract: Deep research tools are among the most impactful and most commonly
encountered agentic systems today. We observe, however, that each deep research
agent introduced so far is hard-coded to carry out a particular research
strategy using a fixed choice of tools. We introduce Universal Deep Research
(UDR), a generalist agentic system that wraps around any language model and
enables the user to create, edit, and refine their own entirely custom deep
research strategies without any need for additional training or finetuning. To
showcase the generality of our system, we equip UDR with example minimal,
expansive, and intensive research strategies, and provide a user interface to
facilitate experimentation with the system.

</details>


### [16] [Instruction-Level Weight Shaping: A Framework for Self-Improving AI Agents](https://arxiv.org/abs/2509.00251)
*Rimom Costa*

Main category: cs.AI

TL;DR: 提出Instruction - Level Weight Shaping (ILWS)方法更新大语言模型知识，在企业支持和Adobe Commerce Cloud中表现良好，可推广到动态领域。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型添加新知识的方法如RAG、微调等存在延迟高、成本高、易遗忘等问题。

Method: 提出ILWS，用精心策划的系统指令作为外部可审计的伪参数，通过反思和用户反馈更新，Reflection Engine诊断推理情况并提出更改，对更改进行版本控制和评估，编辑预算超阈值时将指令空间改进转化为权重空间。

Result: 在企业支持中吞吐量提升2.4 - 5.0倍，审核的幻觉减少约80%；在Adobe Commerce Cloud中每小时处理工单增加4 - 5倍，每张工单处理时间降低约80%。

Conclusion: ILWS在指令层操作，可推广到需要自适应推理、工具创建和低延迟部署的动态领域。

Abstract: Large language models (LLMs) are fluent but largely static after
pre-training; new or shifting knowledge is typically added with
retrieval-augmented generation (RAG) or fine-tuning. RAG raises latency and
engineering overhead and often fails to integrate facts; prompt engineering is
brittle and can conflict with prior knowledge; fine-tuning is costly and risks
catastrophic forgetting. We propose Instruction-Level Weight Shaping (ILWS):
curated system instructions act as external, auditable pseudo-parameters
updated after each session via reflection and user feedback. A Reflection
Engine inspects conversation traces, diagnoses reasoning successes and
failures, and proposes typed deltas $\Delta K=(\Delta S,\Delta U,\Delta T)$
over instructions, user preferences, and tools. Deltas are version-controlled,
evaluated with a sliding window of 1-5 star ratings, auto-repaired on first
failure, and rolled back on repeated failure. When an edit budget crosses a
threshold, the agent compiles a rating-weighted synthetic set and distills
matured instruction-space gains into parameters, converting prompt-space
improvements into weight-space without downtime. ILWS makes explicit the
low-rank shaping induced by context in transformer blocks, preserves
governance, and removes per-call retrieval. In enterprise support it increased
throughput 2.4-5.0x and cut audited hallucinations by about 80% versus a frozen
baseline. In an Adobe Commerce Cloud proof of concept "L0 Support", it achieved
4-5x more tickets per hour and about 80% lower time per ticket, with autonomous
instruction updates and optional tool synthesis. Because ILWS operates at the
instruction layer until controlled distillation, it generalizes to dynamic
domains (legal, medical, engineering) requiring adaptive reasoning, tool
creation, and low-latency deployment.

</details>


### [17] [SHERPA: A Model-Driven Framework for Large Language Model Execution](https://arxiv.org/abs/2509.00272)
*Boqi Chen,Kua Chen,José Antonio Hernández López,Gunter Mussbacher,Dániel Varró,Amir Feizpour*

Main category: cs.AI

TL;DR: 提出SHERPA框架，通过将特定领域最佳实践融入分层状态机提升大语言模型在复杂任务上的表现，经多种任务验证有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型缺乏结构化推理能力，多步提示方法缺乏控制模型行为的通用机制。

Method: 提出SHERPA框架，用状态机构建大语言模型执行过程，通过机器学习方法实现更细粒度的行为控制。

Result: SHERPA适用于多种任务，能复制先前方法并提升性能，集成精心设计的状态机可显著提高输出质量。

Conclusion: 集成状态机对大语言模型输出质量提升显著，对有成熟最佳实践但训练数据不足的复杂任务尤其有益。

Abstract: Recently, large language models (LLMs) have achieved widespread application
across various fields. Despite their impressive capabilities, LLMs suffer from
a lack of structured reasoning ability, particularly for complex tasks
requiring domain-specific best practices, which are often unavailable in the
training data. Although multi-step prompting methods incorporating human best
practices, such as chain-of-thought and tree-of-thought, have gained
popularity, they lack a general mechanism to control LLM behavior. In this
paper, we propose SHERPA, a model-driven framework to improve the LLM
performance on complex tasks by explicitly incorporating domain-specific best
practices into hierarchical state machines. By structuring the LLM execution
processes using state machines, SHERPA enables more fine-grained control over
their behavior via rules or decisions driven by machine learning-based
approaches, including LLMs. We show that SHERPA is applicable to a wide variety
of tasks-specifically, code generation, class name generation, and question
answering-replicating previously proposed approaches while further improving
the performance. We demonstrate the effectiveness of SHERPA for the
aforementioned tasks using various LLMs. Our systematic evaluation compares
different state machine configurations against baseline approaches without
state machines. Results show that integrating well-designed state machines
significantly improves the quality of LLM outputs, and is particularly
beneficial for complex tasks with well-established human best practices but
lacking data used for training LLMs.

</details>


### [18] [SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces](https://arxiv.org/abs/2509.00287)
*Brian Wang,Mani Srivastava*

Main category: cs.AI

TL;DR: 论文提出SIGMUS系统，利用大语言模型实现多模态城市空间知识图谱语义集成，能建立不同数据源与事件间合理联系。


<details>
  <summary>Details</summary>
Motivation: 城市多模态数据分散难整合，依赖人工推理，需系统自动识别数据关系和事件组件以分析事件原因和预测规模强度。

Method: 创建SIGMUS系统，用大语言模型生成必要世界知识，将组织后的知识以知识图谱表示。

Result: 系统能在5种不同数据源（新闻文章文本、CCTV图像、空气质量、天气和交通测量）与同时同地发生的相关事件间建立合理联系。

Conclusion: SIGMUS系统有效实现了多模态城市空间知识图谱语义集成。

Abstract: Modern urban spaces are equipped with an increasingly diverse set of sensors,
all producing an abundance of multimodal data. Such multimodal data can be used
to identify and reason about important incidents occurring in urban landscapes,
such as major emergencies, cultural and social events, as well as natural
disasters. However, such data may be fragmented over several sources and
difficult to integrate due to the reliance on human-driven reasoning for
identifying relationships between the multimodal data corresponding to an
incident, as well as understanding the different components which define an
incident. Such relationships and components are critical to identifying the
causes of such incidents, as well as producing forecasting the scale and
intensity of future incidents as they begin to develop. In this work, we create
SIGMUS, a system for Semantic Integration for Knowledge Graphs in Multimodal
Urban Spaces. SIGMUS uses Large Language Models (LLMs) to produce the necessary
world knowledge for identifying relationships between incidents occurring in
urban spaces and data from different modalities, allowing us to organize
evidence and observations relevant to an incident without relying and
human-encoded rules for relating multimodal sensory data with incidents. This
organized knowledge is represented as a knowledge graph, organizing incidents,
observations, and much more. We find that our system is able to produce
reasonable connections between 5 different data sources (new article text, CCTV
images, air quality, weather, and traffic measurements) and relevant incidents
occurring at the same time and location.

</details>


### [19] [Supporting Our AI Overlords: Redesigning Data Systems to be Agent-First](https://arxiv.org/abs/2509.00997)
*Shu Liu,Soujanya Ponnapalli,Shreya Shankar,Sepanta Zeighami,Alan Zhu,Shubham Agarwal,Ruiqi Chen,Samion Suwito,Shuo Yuan,Ion Stoica,Matei Zaharia,Alvin Cheung,Natacha Crooks,Joseph E. Gonzalez,Aditya G. Parameswaran*

Main category: cs.AI

TL;DR: 未来大语言模型代理可能成数据系统主要负载，当前数据系统需适应代理工作负载，文中基于代理推测特性提出新的数据系统架构研究机会。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理处理数据的方式给当前数据系统带来挑战，需要数据系统更原生地支持代理工作负载。

Method: 分析代理推测的特性（规模、异构性、冗余性和可引导性），据此提出新研究机会。

Result: 提出了一系列新的研究机会，涵盖新查询接口、新查询处理技术和新代理内存存储等。

Conclusion: 数据系统需要适应以支持代理工作负载，并提出了新的代理优先数据系统架构研究方向。

Abstract: Large Language Model (LLM) agents, acting on their users' behalf to
manipulate and analyze data, are likely to become the dominant workload for
data systems in the future. When working with data, agents employ a
high-throughput process of exploration and solution formulation for the given
task, one we call agentic speculation. The sheer volume and inefficiencies of
agentic speculation can pose challenges for present-day data systems. We argue
that data systems need to adapt to more natively support agentic workloads. We
take advantage of the characteristics of agentic speculation that we identify,
i.e., scale, heterogeneity, redundancy, and steerability - to outline a number
of new research opportunities for a new agent-first data systems architecture,
ranging from new query interfaces, to new query processing techniques, to new
agentic memory stores.

</details>


### [20] [NEWSAGENT: Benchmarking Multimodal Agents as Journalists with Real-World Newswriting Tasks](https://arxiv.org/abs/2509.00446)
*Yen-Che Chien,Kuang-Da Wang,Wei-Yao Wang,Wen-Chih Peng*

Main category: cs.AI

TL;DR: 本文提出评估多模态网络数据生产力的基准NEWSAGENT，评估了大语言模型和代理框架，发现代理在规划和叙事整合方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 研究基于代理的系统在多大程度上提高多模态网络数据生产力，以新闻领域为例进行研究。

Method: 引入基准NEWSAGENT，给定写作指令和一手数据，让代理完成新闻创作任务，评估多种大语言模型和代理框架。

Result: 代理能够检索相关事实，但在规划和叙事整合方面存在困难。

Conclusion: NEWSAGENT为评估代理在多模态网络数据处理和实际生产力方面的能力提供了现实测试平台。

Abstract: Recent advances in autonomous digital agents from industry (e.g., Manus AI
and Gemini's research mode) highlight potential for structured tasks by
autonomous decision-making and task decomposition; however, it remains unclear
to what extent the agent-based systems can improve multimodal web data
productivity. We study this in the realm of journalism, which requires
iterative planning, interpretation, and contextual reasoning from multimodal
raw contents to form a well structured news. We introduce NEWSAGENT, a
benchmark for evaluating how agents can automatically search available raw
contents, select desired information, and edit and rephrase to form a news
article by accessing core journalistic functions. Given a writing instruction
and firsthand data as how a journalist initiates a news draft, agents are
tasked to identify narrative perspectives, issue keyword-based queries,
retrieve historical background, and generate complete articles. Unlike typical
summarization or retrieval tasks, essential context is not directly available
and must be actively discovered, reflecting the information gaps faced in
real-world news writing. NEWSAGENT includes 6k human-verified examples derived
from real news, with multimodal contents converted to text for broad model
compatibility. We evaluate open- and closed-sourced LLMs with commonly-used
agentic frameworks on NEWSAGENT, which shows that agents are capable of
retrieving relevant facts but struggling with planning and narrative
integration. We believe that NEWSAGENT serves a realistic testbed for iterating
and evaluating agent capabilities in terms of multimodal web data manipulation
to real-world productivity.

</details>


### [21] [GradeSQL: Outcome Reward Models for Ranking SQL Queries from Large Language Models](https://arxiv.org/abs/2509.01308)
*Mattia Tritto,Giuseppe Farano,Dario Di Palma,Gaetano Rossiello,Fedelucio Narducci,Dharmashankar Subramanian,Tommaso Di Noia*

Main category: cs.AI

TL;DR: 本文评估Outcome Reward Models (ORMs)作为Best-of-N (BoN)启发式方法在Text-to-SQL任务中的效果，结果显示ORMs优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理复杂Text-to-SQL查询时存在用户意图与数据库模式对齐问题，现有测试时策略依赖表面启发式，ORMs在该任务应用未充分探索。

Method: 评估ORMs作为BoN启发式，与ex-BoN和Maj比较，引入训练ORMs框架，在BIRD和SPIDER基准测试中微调多种开源大语言模型。

Result: ORMs在执行准确率上优于ex-BoN和Maj，微调已与SQL生成对齐的模型能带来更优ORM性能，在简单查询上有竞争力且更受益于增加候选数量。

Conclusion: ORMs是Text-to-SQL任务中比现有策略更有效的启发式方法。

Abstract: Text-to-SQL, the task of translating natural language questions into SQL
queries, has significantly advanced with the introduction of Large Language
Models (LLMs), broadening database accessibility for a wide range of users.
Despite substantial progress in generating valid SQL, current LLMs still
struggle with complex queries that require precise alignment between user
intent and the database schema. To mitigate this, test-time strategies such as
Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the
assumption that LLMs can generate correct answers but may require multiple
attempts. However, these methods rely on surface-level heuristics, selecting
either the syntactically correct query through execution-based BoN (ex-BoN) or
the most frequently generated query with Maj. Recently, Outcome Reward Models
(ORMs), which assign utility scores to generated outputs based on semantic
correctness, have emerged as a promising approach for better aligning model
predictions with user intent. Nevertheless, their application to Text-to-SQL
remains largely underexplored.
  In this work, we evaluate ORMs as an effective heuristic for BoN, compare
them with ex-BoN and Maj, and introduce a framework for training ORMs for the
Text-to-SQL task. We evaluate our ORMs on the BIRD and SPIDER benchmarks,
finetuning various open-source LLMs, including the Qwen2, Granite3, and Llama3
model families. Our results show that ORMs outperform ex-BoN and Maj, achieving
execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and
+2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that
finetuning models already aligned with SQL generation, such as OmniSQL, yields
superior ORM performance. Additionally, we observe that ORMs achieve
competitive results on simple queries and benefit more from an increased number
of candidates compared to ex-BoN and Maj.

</details>


### [22] [Multi-Agent Data Visualization and Narrative Generation](https://arxiv.org/abs/2509.00481)
*Anton Wolter,Georgios Vidalakis,Michael Yu,Ankit Grover,Vaishali Dhanoa*

Main category: cs.AI

TL;DR: 本文提出轻量级多智能体系统，自动化数据分析工作流，评估显示其具有良好性能。


<details>
  <summary>Details</summary>
Motivation: 利用多智能体系统改进数据可视化领域的数据到通信流程，实现自动化和人机协作。

Method: 结合混合多智能体架构与确定性组件，将关键逻辑从大语言模型中外化。

Result: 系统能输出细粒度、模块化结果，在4个不同数据集上评估，展现出强泛化性、叙事质量和计算效率，依赖少。

Conclusion: 该系统支持可持续的人机协作，具有良好的性能和应用价值。

Abstract: Recent advancements in the field of AI agents have impacted the way we work,
enabling greater automation and collaboration between humans and agents. In the
data visualization field, multi-agent systems can be useful for employing
agents throughout the entire data-to-communication pipeline. We present a
lightweight multi-agent system that automates the data analysis workflow, from
data exploration to generating coherent visual narratives for insight
communication. Our approach combines a hybrid multi-agent architecture with
deterministic components, strategically externalizing critical logic from LLMs
to improve transparency and reliability. The system delivers granular, modular
outputs that enable surgical modifications without full regeneration,
supporting sustainable human-AI collaboration. We evaluated our system across 4
diverse datasets, demonstrating strong generalizability, narrative quality, and
computational efficiency with minimal dependencies.

</details>


### [23] [Artificial Intelligence-Based Analysis of Ice Cream Melting Behavior Under Various Ingredients](https://arxiv.org/abs/2509.00507)
*Zhang Lai Bin,Zhen Bin It*

Main category: cs.AI

TL;DR: 本文探究几种添加剂对自制冰淇淋融化行为的影响，通过实验发现稳定剂有助于形成稳定结构，不同稳定剂效果有差异，为开发兼顾耐用性和成本效益的配方提供依据。


<details>
  <summary>Details</summary>
Motivation: 冰淇淋融化稳定性对消费者接受度和产品质量至关重要，旨在评估常见添加剂对冰淇淋抗融化性的影响，找到更具成本效益的配方。

Method: 制备添加不同添加剂的冰淇淋样品，在可控条件下进行融化测试，用延时录像记录融化过程，使用Python和OpenCV进行处理和分析。

Result: 所有样品融化后保留泡沫状结构，再冷冻后融化显示出更强的坚固性；不同稳定剂在抗融化性和结构支撑方面效果有差异。

Conclusion: 研究为常用食品添加剂在冰淇淋配方中的功能提供了见解，展示了开发兼顾耐用性和成本效益配方的潜力，对大小规模冰淇淋生产有实际应用价值。

Abstract: The stability of ice cream during melting is a critical factor for consumer's
acceptance and product quality. With the commonly added stabilizer to improve
texture, structure and slower melting as the factors to analyze. This report
explores the effects of locust bean gum, guar gum, maltodextrin, and
carrageenan on the melting behavior of homemade ice cream. The main objective
was to assess how these additives influence melting resistance and to identify
a more cost-effective recipe formulation. Ice cream samples incorporating each
additive were prepared and subjected to melting tests under controlled
conditions. Timelapse recordings were used to capture and analyze the
progression of melting over time. Python and OpenCV is used for process and
analysis. Observations revealed that all samples retained a foam-like structure
even after melting, suggesting the stabilizers contributed to the formation of
a stable air-cell matrix. Furthermore, when the melted samples were re-frozen
and subsequently melted again, they displayed increased sturdiness, indicating
improved resilience of the ice cream structure. Comparative analysis of the
different stabilizers highlighted variations in their effectiveness, with some
offering stronger melting resistance and structural support than others.
Overall, the findings provide insights into the functional roles of commonly
used food additives in ice cream formulation. By evaluating both performance
and cost, this study demonstrates the potential for developing recipes that
balance durability with economic efficiency, contributing to practical
applications in both small-scale and commercial ice cream production.

</details>


### [24] [LLM-Assisted Iterative Evolution with Swarm Intelligence Toward SuperBrain](https://arxiv.org/abs/2509.00510)
*Li Weigang,Pedro Carvalho Brom,Lucas Ramson Siefert*

Main category: cs.AI

TL;DR: 提出用于集体智能的SuperBrain框架，基于大语言模型与人类用户协同进化，介绍从子类脑到超类脑的动态路径，给出理论构建、初步实现及知识整合方案，为集体AI提供基础和路线图。


<details>
  <summary>Details</summary>
Motivation: 创建一种可扩展、可解释且符合伦理的集体人工智能，突破静态提示工程和孤立代理模拟的局限。

Method: 提出从子类脑到超类脑的动态路径，包括用户与大语言模型形成认知二元组、通过GA辅助的前后向进化优化、多子类脑通过群体智能协调、最终整合成超类脑。

Result: 给出理论构建，有初步实现（如无人机调度、关键词过滤），并提出跨二元组知识整合的注册表。

Conclusion: 该工作为可扩展、可解释且符合伦理的集体AI提供了概念基础和架构路线图。

Abstract: We propose a novel SuperBrain framework for collective intelligence, grounded
in the co-evolution of large language models (LLMs) and human users. Unlike
static prompt engineering or isolated agent simulations, our approach
emphasizes a dynamic pathway from Subclass Brain to Superclass Brain: (1) A
Subclass Brain arises from persistent, personalized interaction between a user
and an LLM, forming a cognitive dyad with adaptive learning memory. (2) Through
GA-assisted forward-backward evolution, these dyads iteratively refine prompts
and task performance. (3) Multiple Subclass Brains coordinate via Swarm
Intelligence, optimizing across multi-objective fitness landscapes and
exchanging distilled heuristics. (4) Their standardized behaviors and cognitive
signatures integrate into a Superclass Brain, an emergent meta-intelligence
capable of abstraction, generalization and self-improvement. We outline the
theoretical constructs, present initial implementations (e.g., UAV scheduling,
KU/KI keyword filtering) and propose a registry for cross-dyad knowledge
consolidation. This work provides both a conceptual foundation and an
architectural roadmap toward scalable, explainable and ethically aligned
collective AI.

</details>


### [25] [Text-to-Layout: A Generative Workflow for Drafting Architectural Floor Plans Using LLMs](https://arxiv.org/abs/2509.00543)
*Jayakrishna Duggempudi,Lu Gao,Ahmed Senouci,Zhe Han,Yunpeng Zhang*

Main category: cs.AI

TL;DR: 本文开发了基于大语言模型的AI工作流，可从自然语言提示生成建筑平面图，通过案例证明其有效性，且方便复制。


<details>
  <summary>Details</summary>
Motivation: 开发利用大语言模型辅助从自然语言提示起草建筑平面图的工作流。

Method: 结合提示工程、家具放置优化算法和Python脚本，利用大语言模型解释文本输入生成布局选项。

Result: 以中型住宅布局案例展示该方法能轻松生成功能和结构良好的输出。

Conclusion: 该工作流可透明复制，生成模型能直接集成到专业BIM流程。

Abstract: This paper presents the development of an AI-powered workflow that uses Large
Language Models (LLMs) to assist in drafting schematic architectural floor
plans from natural language prompts. The proposed system interprets textual
input to automatically generate layout options including walls, doors, windows,
and furniture arrangements. It combines prompt engineering, a furniture
placement refinement algorithm, and Python scripting to produce spatially
coherent draft plans compatible with design tools such as Autodesk Revit. A
case study of a mid-sized residential layout demonstrates the approach's
ability to generate functional and structured outputs with minimal manual
effort. The workflow is designed for transparent replication, with all key
prompt specifications documented to enable independent implementation by other
researchers. In addition, the generated models preserve the full range of
Revit-native parametric attributes required for direct integration into
professional BIM processes.

</details>


### [26] [Social World Models](https://arxiv.org/abs/2509.00559)
*Xuhui Zhou,Jiarui Liu,Akhila Yerukola,Hyunwoo Kim,Maarten Sap*

Main category: cs.AI

TL;DR: 本文引入新型结构化社交世界表示形式S3AP，助力AI系统有效推理社交动态，在多项任务中达最优性能，显示出开发更具社交意识系统的潜力。


<details>
  <summary>Details</summary>
Motivation: 人类能凭直觉处理社交互动，而AI系统难以自动构建和推理隐含社交情境，因此需引入新方法帮助AI更好处理社交动态。

Method: 采用POMDP驱动设计，将社交互动表示为结构化元组，可从自由形式叙述或其他输入中自动推导。

Result: S3AP帮助大语言模型在5项社交推理任务中更好理解社交叙述，达最优性能；诱导的社交世界模型能预测未来社交动态、改进智能体决策，在社交互动基准测试中提升最高18%。

Conclusion: S3AP作为强大通用的社交世界状态表示，有望推动开发更具社交意识、能更好应对社交互动的系统。

Abstract: Humans intuitively navigate social interactions by simulating unspoken
dynamics and reasoning about others' perspectives, even with limited
information. In contrast, AI systems struggle to automatically structure and
reason about these implicit social contexts. In this paper, we introduce a
novel structured social world representation formalism (S3AP), designed to help
AI systems reason more effectively about social dynamics. Following a
POMDP-driven design, S3AP represents social interactions as structured tuples,
such as state, observation, agent actions, and mental states, which can be
automatically induced from free-form narratives or other inputs. We first show
S3AP can help LLMs better understand social narratives across 5 social
reasoning tasks (e.g., +51% improvement on FANToM's theory-of-mind reasoning
with OpenAI's o1), reaching new state-of-the-art (SOTA) performance. We then
induce social world models from these structured representations, demonstrating
their ability to predict future social dynamics and improve agent
decision-making, yielding up to +18% improvement on the SOTOPIA social
interaction benchmark. Our findings highlight the promise of S3AP as a
powerful, general-purpose representation for social world states, enabling the
development of more socially-aware systems that better navigate social
interactions.

</details>


### [27] [BALM-TSF: Balanced Multimodal Alignment for LLM-Based Time Series Forecasting](https://arxiv.org/abs/2509.00622)
*Shiqiao Zhou,Holger Schöner,Huanbo Lyu,Edouard Fouché,Shuo Wang*

Main category: cs.AI

TL;DR: 提出轻量级时间序列预测框架BALM - TSF，解决文本与时间序列数据的模态不平衡问题，在长短期和少样本预测中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前多模态架构因文本和时间数据差异大，易过度强调一种模态而忽略另一种，导致信息损失，影响预测性能。

Method: 提出BALM - TSF框架，用时间序列编码器处理原始时间序列，将描述性统计数据输入带可学习提示的大语言模型生成文本嵌入，用缩放策略和对比目标确保跨模态上下文对齐，最后整合两种嵌入进行预测。

Result: 在标准基准上的大量实验表明，BALM - TSF用最少的可训练参数，在长期和少样本预测中都达到了SOTA性能。

Conclusion: BALM - TSF能有效利用文本和时间序列的互补信息，解决模态不平衡问题。

Abstract: Time series forecasting is a long-standing and highly challenging research
topic. Recently, driven by the rise of large language models (LLMs), research
has increasingly shifted from purely time series methods toward harnessing
textual modalities to enhance forecasting performance. However, the vast
discrepancy between text and temporal data often leads current multimodal
architectures to over-emphasise one modality while neglecting the other,
resulting in information loss that harms forecasting performance. To address
this modality imbalance, we introduce BALM-TSF (Balanced Multimodal Alignment
for LLM-Based Time Series Forecasting), a lightweight time series forecasting
framework that maintains balance between the two modalities. Specifically, raw
time series are processed by the time series encoder, while descriptive
statistics of raw time series are fed to an LLM with learnable prompt,
producing compact textual embeddings. To ensure balanced cross-modal context
alignment of time series and textual embeddings, a simple yet effective scaling
strategy combined with a contrastive objective then maps these textual
embeddings into the latent space of the time series embeddings. Finally, the
aligned textual semantic embeddings and time series embeddings are together
integrated for forecasting. Extensive experiments on standard benchmarks show
that, with minimal trainable parameters, BALM-TSF achieves state-of-the-art
performance in both long-term and few-shot forecasting, confirming its ability
to harness complementary information from text and time series. Code is
available at https://github.com/ShiqiaoZhou/BALM-TSF.

</details>


### [28] [NetGent: Agent-Based Automation of Network Application Workflows](https://arxiv.org/abs/2509.00625)
*Jaber Daneshamooz,Eugene Vuong,Laasya Koduru,Sanjay Chandrasekaran,Arpit Gupta*

Main category: cs.AI

TL;DR: 介绍NetGent框架，可自动化复杂应用工作流生成网络流量数据集，结合语言灵活性与编译执行可靠性，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 开发网络通用ML模型需多样真实网络流量数据，但现有浏览器自动化工具使用脆弱且成本高。

Method: 允许用户用自然语言规则指定工作流，编译为NFA，由状态合成组件转换为可执行代码，支持确定性重放、状态缓存减少冗余调用、适应应用界面变化。

Result: NetGent自动化超50个工作流，产生真实流量追踪数据，对UI变化有鲁棒性。

Conclusion: NetGent结合语言代理灵活性和编译执行可靠性，为推进网络ML提供可扩展基础。

Abstract: We present NetGent, an AI-agent framework for automating complex application
workflows to generate realistic network traffic datasets. Developing
generalizable ML models for networking requires data collection from network
environments with traffic that results from a diverse set of real-world web
applications. However, using existing browser automation tools that are
diverse, repeatable, realistic, and efficient remains fragile and costly.
NetGent addresses this challenge by allowing users to specify workflows as
natural-language rules that define state-dependent actions. These abstract
specifications are compiled into nondeterministic finite automata (NFAs), which
a state synthesis component translates into reusable, executable code. This
design enables deterministic replay, reduces redundant LLM calls through state
caching, and adapts quickly when application interfaces change. In experiments,
NetGent automated more than 50+ workflows spanning video-on-demand streaming,
live video streaming, video conferencing, social media, and web scraping,
producing realistic traffic traces while remaining robust to UI variability. By
combining the flexibility of language-based agents with the reliability of
compiled execution, NetGent provides a scalable foundation for generating the
diverse, repeatable datasets needed to advance ML in networking.

</details>


### [29] [On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized Knowledge Representations](https://arxiv.org/abs/2509.00710)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 本文提出模块化多智能体框架用于法律推理，经评估有显著性能提升，表明该架构可让复杂法律推理更易实现，增强AI法律推理的一致性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 法律推理对AI系统提出挑战，需要精确解释法规语言和一致应用复杂规则。

Method: 引入模块化多智能体框架，将法律推理分解为知识获取和应用阶段，应用阶段分三步处理具体案例。

Result: 在法定税收计算任务评估中，基础模型准确率从18.8%提升到76.4%，缩小推理与基础模型的性能差距。

Conclusion: 模块化架构结合形式化知识表示能让复杂法律推理更易实现，增强AI法律推理的一致性和可解释性，为未来研究奠定基础。

Abstract: Legal reasoning requires both precise interpretation of statutory language
and consistent application of complex rules, presenting significant challenges
for AI systems. This paper introduces a modular multi-agent framework that
decomposes legal reasoning into distinct knowledge acquisition and application
stages. In the first stage, specialized agents extract legal concepts and
formalize rules to create verifiable intermediate representations of statutes.
The second stage applies this knowledge to specific cases through three steps:
analyzing queries to map case facts onto the ontology schema, performing
symbolic inference to derive logically entailed conclusions, and generating
final answers using a programmatic implementation that operationalizes the
ontological knowledge. This bridging of natural language understanding with
symbolic reasoning provides explicit and verifiable inspection points,
significantly enhancing transparency compared to end-to-end approaches.
Evaluation on statutory tax calculation tasks demonstrates substantial
improvements, with foundational models achieving 76.4\% accuracy compared to
18.8\% baseline performance, effectively narrowing the performance gap between
reasoning and foundational models. These findings suggest that modular
architectures with formalized knowledge representations can make sophisticated
legal reasoning more accessible through computationally efficient models while
enhancing consistency and explainability in AI legal reasoning, establishing a
foundation for future research into more transparent, trustworthy, and
effective AI systems for legal domain.

</details>


### [30] [OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination](https://arxiv.org/abs/2509.00723)
*Junzhe Chen,Tianshu Zhang,Shiyu Huang,Yuwei Niu,Chao Sun,Rongzhou Zhang,Guanyu Zhou,Lijie Wen,Xuming Hu*

Main category: cs.AI

TL;DR: 本文指出全模态大语言模型存在幻觉问题，提出OmniDPO框架缓解幻觉，实验表明其能有效减少幻觉并提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 全模态大语言模型存在幻觉问题，文本模态先验主导，且忽视视频与音频内在关联，导致推理时出现幻觉。

Method: 提出OmniDPO框架，采用构建文本偏好样本对和多模态偏好样本对两个策略。

Result: 在两个全模态大语言模型上的实验表明，OmniDPO能有效缓解多模态幻觉，显著提升模型跨模态推理能力。

Conclusion: OmniDPO框架可有效解决全模态大语言模型的幻觉问题，提升模型性能。

Abstract: Recently, Omni-modal large language models (OLLMs) have sparked a new wave of
research, achieving impressive results in tasks such as audio-video
understanding and real-time environment perception. However, hallucination
issues still persist. Similar to the bimodal setting, the priors from the text
modality tend to dominate, leading OLLMs to rely more heavily on textual cues
while neglecting visual and audio information. In addition, fully multimodal
scenarios introduce new challenges. Most existing models align visual or
auditory modalities with text independently during training, while ignoring the
intrinsic correlations between video and its corresponding audio. This
oversight results in hallucinations when reasoning requires interpreting hidden
audio cues embedded in video content. To address these challenges, we propose
OmniDPO, a preference-alignment framework designed to mitigate hallucinations
in OLLMs. Specifically, OmniDPO incorporates two strategies: (1) constructing
text-preference sample pairs to enhance the model's understanding of
audio-video interactions; and (2) constructing multimodal-preference sample
pairs to strengthen the model's attention to visual and auditory information.
By tackling both challenges, OmniDPO effectively improves multimodal grounding
and reduces hallucination. Experiments conducted on two OLLMs demonstrate that
OmniDPO not only effectively mitigates multimodal hallucinations but also
significantly enhances the models' reasoning capabilities across modalities.
All code and datasets will be released upon paper acceptance.

</details>


### [31] [Efficient Graph Understanding with LLMs via Structured Context Injection](https://arxiv.org/abs/2509.00740)
*Govind Waghmare,Sumedh BG,Sonia Gupta,Srikanta Bedathur*

Main category: cs.AI

TL;DR: 提出结构化上下文注入框架，无需微调大语言模型来解决图问题，评估显示性能提升，证明该方法有效可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统微调或多步查询解决图推理任务成本高且效率低，需更优方法。

Method: 提出结构化上下文注入框架，将特定任务信息嵌入输入，引导大语言模型解决图问题，且无需微调。

Result: 在多个图任务上评估，显示性能持续提升，结构化输入上下文表现可与复杂方法媲美或超越。

Conclusion: 结构化上下文注入是用大语言模型进行图理解的有效且可扩展策略。

Abstract: Large Language Models (LLMs) have shown strong capabilities in solving
problems across domains, including graph-related tasks traditionally addressed
by symbolic or algorithmic methods. In this work, we present a framework for
structured context injection, where task-specific information is systematically
embedded in the input to guide LLMs in solving a wide range of graph problems.
Our method does not require fine-tuning of LLMs, making it cost-efficient and
lightweight. We observe that certain graph reasoning tasks remain challenging
for LLMs unless they are mapped to conceptually grounded representations.
However, achieving such mappings through fine-tuning or repeated multi-step
querying can be expensive and inefficient. Our approach offers a practical
alternative by injecting structured context directly into the input, enabling
the LLM to implicitly align the task with grounded conceptual spaces. We
evaluate the approach on multiple graph tasks using both lightweight and large
models, highlighting the trade-offs between accuracy and computational cost.
The results demonstrate consistent performance improvements, showing that
structured input context can rival or surpass more complex approaches. Our
findings underscore the value of structured context injection as an effective
and scalable strategy for graph understanding with LLMs.

</details>


### [32] [L-MARS -- Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search](https://arxiv.org/abs/2509.00761)
*Ziqi Wang,Boqin Yuan*

Main category: cs.AI

TL;DR: 提出L - MARS系统减少法律问答幻觉和不确定性，评估显示效果好，证明多智能体推理有应用价值。


<details>
  <summary>Details</summary>
Motivation: 减少法律问答中的幻觉和不确定性。

Method: 将查询分解为子问题，跨异构源进行有针对性搜索，用Judge Agent验证，通过迭代推理 - 搜索 - 验证循环处理。

Result: 在LegalSearchQA基准测试中，L - MARS提高了事实准确性，减少不确定性，获人类专家和基于大语言模型的评委更高偏好分数。

Conclusion: 多智能体推理与智能搜索为在高风险法律领域部署大语言模型提供可扩展和可复制的蓝图。

Abstract: We present L-MARS (Legal Multi-Agent Workflow with Orchestrated Reasoning and
Agentic Search), a system that reduces hallucination and uncertainty in legal
question answering through coordinated multi-agent reasoning and retrieval.
Unlike single-pass retrieval-augmented generation (RAG), L-MARS decomposes
queries into subproblems, issues targeted searches across heterogeneous sources
(Serper web, local RAG, CourtListener case law), and employs a Judge Agent to
verify sufficiency, jurisdiction, and temporal validity before answer
synthesis. This iterative reasoning-search-verification loop maintains
coherence, filters noisy evidence, and grounds answers in authoritative law. We
evaluated L-MARS on LegalSearchQA, a new benchmark of 200 up-to-date multiple
choice legal questions in 2025. Results show that L-MARS substantially improves
factual accuracy, reduces uncertainty, and achieves higher preference scores
from both human experts and LLM-based judges. Our work demonstrates that
multi-agent reasoning with agentic search offers a scalable and reproducible
blueprint for deploying LLMs in high-stakes domains requiring precise legal
retrieval and deliberation.

</details>


### [33] [Aligning Reasoning LLMs for Materials Discovery with Physics-aware Rejection Sampling](https://arxiv.org/abs/2509.00768)
*Lee Hyun,Sohee Yoon,Jinwoo Park,Sue In Chae,Seongeon Park,Jooyeon Ahn,Yebin Jung,Youjung Chung,Hogeun Chang,Myeonginn Kang,Jina Kim,Ho-Gyeong Kim,Myeonghun Jeong*

Main category: cs.AI

TL;DR: 本文提出Physics - aware Rejection Sampling (PaRS)方法用于AI驱动的材料发现，提高了大推理模型在过程感知属性预测中的性能。


<details>
  <summary>Details</summary>
Motivation: AI驱动的材料发现需要准确、校准且符合物理规律的过程感知配方到属性的预测器，现有的训练管道选择推理轨迹的方式不能很好反映物理可容许性。

Method: 引入Physics - aware Rejection Sampling (PaRS)训练时间轨迹选择方案，用较大教师模型合成的轨迹微调大的学生模型，并在匹配的令牌预算下与各种拒绝采样基线进行评估。

Result: 该方法提高了准确性和校准度，降低了物理违反率和采样成本。

Conclusion: 适度的、领域感知的约束与轨迹级选择相结合，为过程感知属性预测和闭环材料设计提供了一条可靠、高效的大推理模型实用途径。

Abstract: AI-driven materials discovery that couples automated experimentation with
algorithmic decision-making requires process aware recipe to property
predictors that are accurate, calibrated, and physically admissible. We
approach this as a reasoning problem with large reasoning models (LRMs). To
instill reasoning capability into language models, we curate reasoning traces
from a teacher model to train a student model. However, most training pipelines
select reasoning traces using binary correctness or learned preference signals
that poorly reflect physical admissibility. We introduce Physics-aware
Rejection Sampling (PaRS), a training-time trace selection scheme that favors
traces consistent with fundamental physics and numerically close to targets,
with lightweight halting to control compute. We instantiate our framework with
a large student model fine-tuned on traces synthesized by a larger teacher
model, and evaluate under matched token budgets against various rejection
sampling baselines. Our method improves accuracy and calibration, reduces
physics-violation rates, and lowers sampling cost relative to baselines. These
results indicate that modest, domain-aware constraints combined with
trace-level selection provide a practical path toward reliable, efficient LRMs
for process-aware property prediction and closed-loop materials design.

</details>


### [34] [Sharpe Ratio Optimization in Markov Decision Processes](https://arxiv.org/abs/2509.00793)
*Shuai Ma,Guangwu Liu,Li Xia*

Main category: cs.AI

TL;DR: 本文研究无限期马尔可夫决策过程（MDPs）中的夏普比率优化问题，通过Dinkelbachs变换和迭代算法解决动态规划应用难题，证明算法收敛性并实验验证，是首个用动态规划算法解决MDPs中夏普比率优化的方法。


<details>
  <summary>Details</summary>
Motivation: 夏普比率在金融中广泛使用，但在MDPs中优化夏普比率存在动态规划难以应用的问题，一是动态规划不适用于分数目标，二是对风险指标无效。

Method: 用Dinkelbachs变换将夏普比率目标转换为均方 - 方差（M2V）目标；开发迭代算法解决M2V优化问题；为平均和折扣MDP设置开发策略迭代程序。

Result: M2V优化和原始夏普比率优化在风险敏感参数等于最优夏普比率时共享最优策略；所推导的夏普比率序列单调递增并收敛到最优夏普比率；通过数值实验验证。

Conclusion: 提出的算法是首个用动态规划类型算法解决MDPs中夏普比率优化的方法，有望为解决具有其他分数目标的MDPs提供思路。

Abstract: Sharpe ratio (also known as reward-to-variability ratio) is a widely-used
metric in finance, which measures the additional return at the cost of per unit
of increased risk (standard deviation of return). However, the optimization of
Sharpe ratio in Markov decision processes (MDPs) is challenging, because there
exist two difficulties hindering the application of dynamic programming. One is
that dynamic programming does not work for fractional objectives, and the other
is that dynamic programming is invalid for risk metrics. In this paper, we
study the Sharpe ratio optimization in infinite-horizon MDPs, considering both
the long-run average and discounted settings. We address the first challenge
with the Dinkelbachs transform, which converts the Sharpe ratio objective to a
mean-squared-variance (M2V) objective. It is shown that the M2V optimization
and the original Sharpe ratio optimization share the same optimal policy when
the risk-sensitive parameter is equal to the optimal Sharpe ratio. For the
second challenge, we develop an iterative algorithm to solve the M2V
optimization which is similar to a mean-variance optimization in MDPs. We
iteratively solve the M2V problem and obtain the associated Sharpe ratio that
is used to update the risk-sensitive parameter in the next iteration of M2V
problems. We show that such a sequence of Sharpe ratios derived is
monotonically increasing and converges to the optimal Sharpe ratio. For both
average and discounted MDP settings, we develop a policy iteration procedure
and prove its convergence to the optimum. Numerical experiments are conducted
for validation. To the best of our knowledge, our approach is the first that
solves the Sharpe ratio optimization in MDPs with dynamic programming type
algorithms. We believe that the proposed algorithm can shed light on solving
MDPs with other fractional objectives.

</details>


### [35] [Neuro-Symbolic Predictive Process Monitoring](https://arxiv.org/abs/2509.00834)
*Axel Mezini,Elena Umili,Ivan Donadello,Fabrizio Maria Maggi,Matteo Mancanelli,Fabio Patrizi*

Main category: cs.AI

TL;DR: 提出神经符号预测过程监控方法解决业务流程管理中后缀预测问题，在三个数据集上验证有效且框架适用于符号序列生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习后缀预测方法因训练时未明确整合领域知识，常无法满足基本逻辑约束。

Method: 将有限轨迹上的线性时态逻辑（LTLf）融入自回归序列预测器训练过程，引入可微逻辑损失函数并结合标准预测损失。

Result: 在三个真实数据集上提高了后缀预测准确性和时间约束合规性，两种逻辑损失变体在噪声和现实环境中有效。

Conclusion: 该框架虽在业务流程管理中开发，但适用于任何符号序列生成任务，推动了神经符号人工智能发展。

Abstract: This paper addresses the problem of suffix prediction in Business Process
Management (BPM) by proposing a Neuro-Symbolic Predictive Process Monitoring
(PPM) approach that integrates data-driven learning with temporal logic-based
prior knowledge. While recent approaches leverage deep learning models for
suffix prediction, they often fail to satisfy even basic logical constraints
due to the absence of explicit integration of domain knowledge during training.
We propose a novel method to incorporate Linear Temporal Logic over finite
traces (LTLf) into the training process of autoregressive sequence predictors.
Our approach introduces a differentiable logical loss function, defined using a
soft approximation of LTLf semantics and the Gumbel-Softmax trick, which can be
combined with standard predictive losses. This ensures the model learns to
generate suffixes that are both accurate and logically consistent. Experimental
evaluation on three real-world datasets shows that our method improves suffix
prediction accuracy and compliance with temporal constraints. We also introduce
two variants of the logic loss (local and global) and demonstrate their
effectiveness under noisy and realistic settings. While developed in the
context of BPM, our framework is applicable to any symbolic sequence generation
task and contributes toward advancing Neuro-Symbolic AI.

</details>


### [36] [ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop Insulin Adoption in Type 1 Diabetes Care](https://arxiv.org/abs/2509.00891)
*Zonghai Yao,Talha Chafekar,Junda Wang,Shuo Han,Feiyun Ouyang,Junhui Qian,Lingxi Li,Hong Yu*

Main category: cs.AI

TL;DR: 文章引入ChatCLIDS评估大语言模型驱动的健康行为改变劝导对话，发现当前大语言模型在克服阻力上存在局限，提供了高保真可扩展测试平台。


<details>
  <summary>Details</summary>
Motivation: 现实中1型糖尿病闭环胰岛素输送系统采用率低，由行为、心理和社会障碍导致，需评估大语言模型驱动的劝导对话。

Method: 引入ChatCLIDS框架，有专家验证的虚拟患者库，模拟护士代理多轮交互，支持纵向咨询和对抗性社会影响场景评估。

Result: 较大且更具反思性的大语言模型会随时间调整策略，但所有模型在克服阻力上有困难，尤其在现实社会压力下。

Conclusion: 指出当前大语言模型用于行为改变的关键局限，为推进医疗等领域可信劝导式AI提供测试平台。

Abstract: Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1
diabetes remains low, driven not by technical failure, but by diverse
behavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the
first benchmark to rigorously evaluate LLM-driven persuasive dialogue for
health behavior change. Our framework features a library of expert-validated
virtual patients, each with clinically grounded, heterogeneous profiles and
realistic adoption barriers, and simulates multi-turn interactions with nurse
agents equipped with a diverse set of evidence-based persuasive strategies.
ChatCLIDS uniquely supports longitudinal counseling and adversarial social
influence scenarios, enabling robust, multi-dimensional evaluation. Our
findings reveal that while larger and more reflective LLMs adapt strategies
over time, all models struggle to overcome resistance, especially under
realistic social pressure. These results highlight critical limitations of
current LLMs for behavior change, and offer a high-fidelity, scalable testbed
for advancing trustworthy persuasive AI in healthcare and beyond.

</details>


### [37] [Question-to-Knowledge: Multi-Agent Generation of Inspectable Facts for Product Mapping](https://arxiv.org/abs/2509.01182)
*Wonduk Seo,Taesub Shin,Hyunjin An,Dokyun Kim,Seunghyun Lee*

Main category: cs.AI

TL;DR: 提出Q2K多智能体框架用于可靠的SKU映射，实验表明其在准确率、鲁棒性和效率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 电商中识别两个产品列表是否指同一SKU是挑战，基于规则的启发式方法和关键词相似度方法常误分类。

Method: 提出Q2K框架，包括推理智能体、知识智能体和去重智能体，还有人在环机制。

Result: 在真实数据集实验中，Q2K超越强基线，在困难场景有更高准确率和鲁棒性。

Conclusion: Q2K平衡了准确性和效率，为产品集成提供可扩展和可解释的解决方案。

Abstract: Identifying whether two product listings refer to the same Stock Keeping Unit
(SKU) is a persistent challenge in ecommerce, especially when explicit
identifiers are missing and product names vary widely across platforms. Rule
based heuristics and keyword similarity often misclassify products by
overlooking subtle distinctions in brand, specification, or bundle
configuration. To overcome these limitations, we propose Question to Knowledge
(Q2K), a multi agent framework that leverages Large Language Models (LLMs) for
reliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates
targeted disambiguation questions, (2) a Knowledge Agent that resolves them via
focused web searches, and (3) a Deduplication Agent that reuses validated
reasoning traces to reduce redundancy and ensure consistency. A human in the
loop mechanism further refines uncertain cases. Experiments on real world
consumer goods datasets show that Q2K surpasses strong baselines, achieving
higher accuracy and robustness in difficult scenarios such as bundle
identification and brand origin disambiguation. By reusing retrieved reasoning
instead of issuing repeated searches, Q2K balances accuracy with efficiency,
offering a scalable and interpretable solution for product integration.

</details>


### [38] [When Agents go Astray: Course-Correcting SWE Agents with PRMs](https://arxiv.org/abs/2509.02360)
*Shubham Gandhi,Jason Tsay,Jatin Ganhotra,Kiran Kate,Yara Rizk*

Main category: cs.AI

TL;DR: 提出SWE - PRM模型在推理时干预，纠正大语言模型代理在软件工程任务中的轨迹错误，提升可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理在软件工程任务执行轨迹存在冗余探索等低效问题，以往工作多事后诊断。

Method: 引入推理时的过程奖励模型SWE - PRM，利用常见低效分类法，执行中检测并纠正轨迹错误，不修改底层策略。

Result: 在SWE - bench Verified上，闭源PRM将分辨率从40.0%提升到50.6%；分类法引导的PRM表现更优，增加成功率并减少轨迹长度，推理成本低至0.2美元。

Conclusion: PRM是提高软件工程代理可靠性和效率的实用且可扩展机制。

Abstract: Large Language Model (LLM) agents are increasingly deployed for complex,
multi-step software engineering (SWE) tasks. However, their trajectories often
contain costly inefficiencies, such as redundant exploration, looping, and
failure to terminate once a solution is reached. Prior work has largely treated
these errors in a post-hoc manner, diagnosing failures only after execution. In
this paper, we introduce SWE-PRM, an inference-time Process Reward Model (PRM)
that intervenes during execution to detect and course-correct trajectory-level
errors. Our PRM design leverages a taxonomy of common inefficiencies and
delivers lightweight, interpretable feedback without modifying the underlying
policy. On SWE-bench Verified, closed-source PRMs improve resolution from 40.0%
to 50.6% (+10.6 p.p.), with the largest gains on medium and hard tasks. Among
feedback strategies, taxonomy-guided PRMs outperform unguided or explicit
action-prescriptive variants, increasing success rate while reducing trajectory
length. These benefits come at an acceptable added inference cost of as low as
$0.2, making PRMs a practical and scalable mechanism for improving SWE agents'
reliability and efficiency.

</details>


### [39] [SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement Fine-Tuning of LLMs](https://arxiv.org/abs/2509.00930)
*Yanxiao Zhao,Yaqian Li,Zihao Bo,Rinyoichi Takezoe,Haojia Hui,Mo Guang,Lei Ren,Xiaolin Qin,Kaiwen Long*

Main category: cs.AI

TL;DR: 本文介绍SATQuest工具评估和增强大语言模型逻辑推理能力，评估发现模型推理有局限，强化微调可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准和数据集缺乏细粒度分析所需的可控和可扩展工具，难以系统评估和增强大语言模型的推理能力。

Method: 引入SATQuest，从合取范式实例生成多样化的基于可满足性的逻辑推理问题，沿实例规模、问题类型和问题格式三个维度构建问题，采用随机生成和PySAT验证答案。

Result: 评估发现大语言模型逻辑推理有显著局限，强化微调能提升目标任务性能并推广到更复杂实例，但跨格式适应仍有挑战。

Conclusion: SATQuest有潜力成为提升大语言模型逻辑推理能力的基础工具和起点。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated remarkable
general reasoning capabilities. However, systematically evaluating and
enhancing these reasoning capabilities is challenging due to the lack of
controllable and scalable tools for fine-grained analysis. Existing benchmarks
and datasets often lack the necessary variable control for multi-dimensional,
systematic analysis and training, or have narrow problem types and formats. To
address these limitations, we introduce SATQuest, a systematic verifier
designed to evaluate and enhance logical reasoning in LLMs by generating
diverse, Satisfiability-based logical reasoning problems directly from
Conjunctive Normal Form (CNF) instances. SATQuest structures these problems
along three orthogonal dimensions: instance scale, problem type, and question
format, employing randomized, SAT-based problem generation and objective answer
verification via PySAT. This design mitigates memorization issues, allows for
nuanced insights into reasoning performance, and enables effective
reinforcement fine-tuning. Our extensive evaluation of various LLMs using
SATQuest identified significant limitations in their logical reasoning,
particularly in generalizing beyond familiar mathematical formats. Furthermore,
we show that reinforcement fine-tuning with SATQuest rewards substantially
improves targeted task performance and generalizes to more complex instances,
while highlighting remaining challenges in cross-format adaptation. Through
these demonstrations, we showcase SATQuest's potential as a foundational tool
and a valuable starting point for advancing LLM logical reasoning.

</details>


### [40] [UrbanInsight: A Distributed Edge Computing Framework with LLM-Powered Data Filtering for Smart City Digital Twins](https://arxiv.org/abs/2509.00936)
*Kishor Datta Gupta,Md Manjurul Ahsan,Mohd Ariful Haque,Roy George,Azmine Toushik Wasi*

Main category: cs.AI

TL;DR: 文章介绍一种融合物理信息机器学习、多模态数据融合、知识图谱和大语言模型的框架，用于构建数字孪生系统，提供可操作见解，助力智能基础设施建设。


<details>
  <summary>Details</summary>
Motivation: 现有城市数据处理系统存在规模、延迟和见解碎片化问题，需新方法利用城市数据改善城市生活。

Method: 融合物理信息机器学习、多模态数据融合、知识图谱表示和大语言模型驱动的自适应基于规则的智能。

Result: 形成超越被动监测、能提供可操作见解的数字孪生系统基础。

Conclusion: 该方法结合物理推理、语义数据融合和自适应规则生成，为创建响应式、可靠和可持续的智能基础设施开辟新可能。

Abstract: Cities today generate enormous streams of data from sensors, cameras, and
connected infrastructure. While this information offers unprecedented
opportunities to improve urban life, most existing systems struggle with scale,
latency, and fragmented insights. This work introduces a framework that blends
physics-informed machine learning, multimodal data fusion, and knowledge graph
representation with adaptive, rule-based intelligence powered by large language
models (LLMs). Physics-informed methods ground learning in real-world
constraints, ensuring predictions remain meaningful and consistent with
physical dynamics. Knowledge graphs act as the semantic backbone, integrating
heterogeneous sensor data into a connected, queryable structure. At the edge,
LLMs generate context-aware rules that adapt filtering and decision-making in
real time, enabling efficient operation even under constrained resources.
Together, these elements form a foundation for digital twin systems that go
beyond passive monitoring to provide actionable insights. By uniting
physics-based reasoning, semantic data fusion, and adaptive rule generation,
this approach opens new possibilities for creating responsive, trustworthy, and
sustainable smart infrastructures.

</details>


### [41] [A Hybrid Ai Framework For Strategic Patent Portfolio Pruning: Integrating Learning To-Rank And Market Need Analysis For Technology Transfer Optimization](https://arxiv.org/abs/2509.00958)
*Manish Verma,Vivek Sharma,Vishal Singh*

Main category: cs.AI

TL;DR: 本文提出用于筛选专利组合的多阶段混合智能框架，结合LTR模型与“Need - Seed”系统识别高价值专利用于技术转让。


<details>
  <summary>Details</summary>
Motivation: 当前专利估值方法依赖回顾性指标或手动分析，耗时久，需自动化且深入的评估方法。

Method: 结合Learning to Rank模型评估专利，用“Need - Seed”系统，“Need Agent”用NLP挖掘市场需求，“Seed Agent”用微调大模型分析专利，生成“Core Ontology Framework”匹配供需，有动态参数加权和人工验证协议。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确提及具体结论。

Abstract: This paper introduces a novel, multi stage hybrid intelligence framework for
pruning patent portfolios to identify high value assets for technology
transfer. Current patent valuation methods often rely on retrospective
indicators or manual, time intensive analysis. Our framework automates and
deepens this process by combining a Learning to Rank (LTR) model, which
evaluates patents against over 30 legal and commercial parameters, with a
unique "Need-Seed" agent-based system. The "Need Agent" uses Natural Language
Processing (NLP) to mine unstructured market and industry data, identifying
explicit technological needs. Concurrently, the "Seed Agent" employs fine tuned
Large Language Models (LLMs) to analyze patent claims and map their
technological capabilities. The system generates a "Core Ontology Framework"
that matches high potential patents (Seeds) to documented market demands
(Needs), providing a strategic rationale for divestment decisions. We detail
the architecture, including a dynamic parameter weighting system and a crucial
Human in the-Loop (HITL) validation protocol, to ensure both adaptability and
real-world credibility.

</details>


### [42] [Ultra Strong Machine Learning: Teaching Humans Active Learning Strategies via Automated AI Explanations](https://arxiv.org/abs/2509.00961)
*Lun Ai,Johannes Langer,Ute Schmid,Stephen Muggleton*

Main category: cs.AI

TL;DR: 介绍神经符号方法LENS用于自动解释机器学习逻辑程序，评估显示其优于直接大语言模型提示和手工模板，但在人类学习实验中未显著提升人类表现。


<details>
  <summary>Details</summary>
Motivation: 解决先前超强大机器学习（USML）方法中手工解释模板不可扩展的问题，构建有效USML系统支持人类学习。

Method: 提出LENS方法，结合符号程序合成与大语言模型来自动生成自然语言解释。

Result: LENS生成的解释优于直接大语言模型提示和手工模板；人类学习实验中未显著提升人类表现。

Conclusion: 工作为构建有效USML系统支持人类学习奠定基础。

Abstract: Ultra Strong Machine Learning (USML) refers to symbolic learning systems that
not only improve their own performance but can also teach their acquired
knowledge to quantifiably improve human performance. In this work, we present
LENS (Logic Programming Explanation via Neural Summarisation), a neuro-symbolic
method that combines symbolic program synthesis with large language models
(LLMs) to automate the explanation of machine-learned logic programs in natural
language. LENS addresses a key limitation of prior USML approaches by replacing
hand-crafted explanation templates with scalable automated generation. Through
systematic evaluation using multiple LLM judges and human validation, we
demonstrate that LENS generates superior explanations compared to direct LLM
prompting and hand-crafted templates. To investigate whether LENS can teach
transferable active learning strategies, we carried out a human learning
experiment across three related domains. Our results show no significant human
performance improvements, suggesting that comprehensive LLM responses may
overwhelm users for simpler problems rather than providing learning support.
Our work provides a solid foundation for building effective USML systems to
support human learning. The source code is available on:
https://github.com/lun-ai/LENS.git.

</details>


### [43] [CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks with LLMs](https://arxiv.org/abs/2509.00971)
*Jay Vaghasiya,Omkar Ghugarkar,Vishvesh Bhat,Vipul Dholaria,Julian McAuley*

Main category: cs.AI

TL;DR: 介绍基于General Symbolics的CoreThink推理层，在多个基准测试中表现出色，无需微调或训练成本，还推出代理编码IDE，认为需新推理技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大语言模型性能上会导致收益递减，需要开发新的推理技术。

Method: 基于General Symbolics构建CoreThink推理层，CoreThink GSR围绕工具调用、代码生成和规划三个用例。

Result: 在多个基准测试中取得SOTA分数，如Livecodebench v6达66.66%等；代理编码IDE在SWE - Bench Lite上准确率达62.3%。

Conclusion: 新的推理层能提升推理任务准确性且不产生负面影响，适合推理密集型用例。

Abstract: We introduce CoreThink, a state-of-the-art Reasoning Layer built upon a novel
reasoning method called General Symbolics. This approach diverges from
reasoning paradigms such as test-time scaling, Supervised Fine-Tuning (SFT),
and Reinforcement Learning with Verifiable Rewards (RLVR). CoreThink General
Symbolic Reasoner (GSR) is specifically structured around three key use cases:
tool-calling, code generation, and planning, demonstrating exemplary
performance across a total of seven benchmarks in their respective areas.
Notably, we are achieving SOTA scores of 66.66\% on Livecodebench v6, 89\% on
Instruction-Following Evals, and 24.4\% on ARC-AGI-2. We also present an
agentic coding IDE, developed using the principles of General Symbolics, which
achieves a state-of-the-art accuracy of 62.3\% on \texttt{SWE-Bench Lite}. We
are able to achieve these improvements without any finetuning or training
costs. Our Reasoning Layer is designed to provide a pure performance uplift,
ensuring that a model's accuracy on reasoning tasks is never negatively
impacted. We argue that incumbent methods will eventually lead to diminishing
returns in LLM performance, necessitating the development of new reasoning
techniques. This technical report details our approach at a high level and the
availability of the CoreThink models for reasoning-intensive use cases.

</details>


### [44] [Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning](https://arxiv.org/abs/2509.00975)
*Zifeng Ding,Shenyang Huang,Zeyu Cao,Emma Kondrup,Zachary Yang,Xingyue Huang,Yuan Sui,Zhangdie Yuan,Yuqicheng Zhu,Xianglong Hu,Yuan He,Farimah Poursafaei,Michael Bronstein,Andreas Vlachos*

Main category: cs.AI

TL;DR: 提出ReaL - TG框架微调大语言模型用于真实世界时间图的可解释链接预测，并提出新评估协议，实验表明ReaL - TG - 4B性能优于更大的前沿大语言模型。


<details>
  <summary>Details</summary>
Motivation: 传统神经方法缺乏可解释性且难以用于未见过的图，现有大语言模型图推理研究多局限于静态图或小的合成时间图，且缺乏对推理痕迹质量的评估。

Method: 提出ReaL - TG强化学习框架微调大语言模型进行可解释链接预测，使用基于结果的奖励鼓励模型自我探索推理策略；提出结合排序指标和大语言模型评判系统的新评估协议。

Result: 对Qwen3 - 4B微调得到的ReaL - TG - 4B在排序指标上优于GPT - 5 mini等更大的前沿大语言模型，且生成的解释质量高，得到大语言模型评判和人工评估的确认。

Conclusion: ReaL - TG框架有效，能让大语言模型在时间图链接预测任务中表现良好并具有可解释性，新评估协议也能有效评估推理痕迹质量。

Abstract: Forecasting future links is a central task in temporal graph (TG) reasoning,
requiring models to leverage historical interactions to predict upcoming ones.
Traditional neural approaches, such as temporal graph neural networks, achieve
strong performance but lack explainability and cannot be applied to unseen
graphs without retraining. Recent studies have begun to explore using large
language models (LLMs) for graph reasoning, but most of them are constrained to
static graphs or small synthetic TGs and lack the evaluation of the quality of
reasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced
Learning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that
fine-tunes LLMs to perform explainable link forecasting on real-world TGs.
ReaL-TG uses outcome-based reward to encourage models to self-explore reasoning
strategies from graph structure and to produce explanations that directly
justify their predictions. To enable evaluation on LLM-generated reasoning
traces, we propose a new evaluation protocol combining ranking metrics with an
LLM-as-a-Judge system that assesses both the quality of reasoning and the
impact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning
Qwen3-4B under our framework, show that it outperforms much larger frontier
LLMs, including GPT-5 mini, on ranking metrics, while producing high-quality
explanations confirmed by both the LLM judge and human evaluation.

</details>


### [45] [Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation](https://arxiv.org/abs/2509.00987)
*Adib Bazgir,Amir Habibdoust,Yuwen Zhang,Xing Song*

Main category: cs.AI

TL;DR: 本文综述因果多智能体大语言模型领域，探讨其设计、架构、评估、应用，指出挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂因果推理等方面存在局限，多智能体系统可解决这些问题。

Method: 探讨因果多智能体大语言模型在不同因果方面的设计，分析架构模式和交互协议，讨论评估方法、基准和应用领域。

Result: 对因果多智能体大语言模型的设计、架构、评估、应用有全面探讨。

Conclusion: 指出该领域存在的挑战、开放研究问题和有前景的未来方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various reasoning and generation tasks. However, their proficiency in complex
causal reasoning, discovery, and estimation remains an area of active
development, often hindered by issues like hallucination, reliance on spurious
correlations, and difficulties in handling nuanced, domain-specific, or
personalized causal relationships. Multi-agent systems, leveraging the
collaborative or specialized abilities of multiple LLM-based agents, are
emerging as a powerful paradigm to address these limitations. This review paper
explores the burgeoning field of causal multi-agent LLMs. We examine how these
systems are designed to tackle different facets of causality, including causal
reasoning and counterfactual analysis, causal discovery from data, and the
estimation of causal effects. We delve into the diverse architectural patterns
and interaction protocols employed, from pipeline-based processing and debate
frameworks to simulation environments and iterative refinement loops.
Furthermore, we discuss the evaluation methodologies, benchmarks, and diverse
application domains where causal multi-agent LLMs are making an impact,
including scientific discovery, healthcare, fact-checking, and personalized
systems. Finally, we highlight the persistent challenges, open research
questions, and promising future directions in this synergistic field, aiming to
provide a comprehensive overview of its current state and potential trajectory.

</details>


### [46] [Quantum-like Coherence Derived from the Interaction between Chemical Reaction and Its Environment](https://arxiv.org/abs/2509.01021)
*Yukio-Pegio Gunji,Andrew Adamatzky,Panagiotis Mougkogiannis,Andrei Khrenikov*

Main category: cs.AI

TL;DR: 本文对比人工智能与自然智能计算过程，定义开闭计算，在化学反应中实现开放计算，建模化学反应，分Token和Type计算，二者相互作用产生类量子相干形成尖峰波。


<details>
  <summary>Details</summary>
Motivation: 通过对比人工智能与自然智能计算过程，探索新的计算模式及在化学反应中的应用。

Method: 定义开闭计算，在化学反应中实现开放计算，将计算建模为化学反应、执行环境建模为分子聚集程度，划分Token和Type计算。

Result: Token计算呈现自组织临界现象，Type计算呈现量子逻辑，二者相互作用产生类量子相干，形成尖峰波实现信号传输。

Conclusion: 这种类量子相干可能是控制尖峰波和生化节律的酶的来源。

Abstract: By uncovering the contrast between Artificial Intelligence and Natural-born
Intelligence as a computational process, we define closed computing and open
computing, and implement open computing within chemical reactions. This
involves forming a mixture and invalidation of the computational process and
the execution environment, which are logically distinct, and coalescing both to
create a system that adjusts fluctuations. We model chemical reactions by
considering the computation as the chemical reaction and the execution
environment as the degree of aggregation of molecules that interact with the
reactive environment. This results in a chemical reaction that progresses while
repeatedly clustering and de-clustering, where concentration no longer holds
significant meaning. Open computing is segmented into Token computing, which
focuses on the individual behavior of chemical molecules, and Type computing,
which focuses on normative behavior. Ultimately, both are constructed as an
interplay between the two. In this system, Token computing demonstrates
self-organizing critical phenomena, while Type computing exhibits quantum
logic. Through their interplay, the recruitment of fluctuations is realized,
giving rise to interactions between quantum logical subspaces corresponding to
quantum coherence across different Hilbert spaces. As a result, spike waves are
formed, enabling signal transmission. This occurrence may be termed
quantum-like coherence, implying the source of enzymes responsible for
controlling spike waves and biochemical rhythms.

</details>


### [47] [Symbolic Planning and Multi-Agent Path Finding in Extremely Dense Environments with Movable Obstacles](https://arxiv.org/abs/2509.01022)
*Bo Fu,Zhe Chen,Rahul Chandan,Alex Barbosa,Michael Caldara,Joey Durham,Federico Pecora*

Main category: cs.AI

TL;DR: 本文介绍块重排问题（BRaP），将其定义为图搜索问题，提出五种基于搜索的解决方案并评估，方法在80x80网格中表现高效。


<details>
  <summary>Details</summary>
Motivation: 解决大型仓库管理中存储块重排以达到目标状态的难题。

Method: 将BRaP定义为图搜索问题，借鉴滑动拼图问题直觉，提出基于联合配置空间搜索、经典规划、多智能体路径规划和专家启发式的五种搜索算法。

Result: 尽管搜索空间大小与块数量呈指数关系，但方法在80x80网格中为深埋块创建重排计划时表现出效率。

Conclusion: 所提出的五种算法在解决BRaP问题上具有一定的有效性和可扩展性。

Abstract: We introduce the Block Rearrangement Problem (BRaP), a challenging component
of large warehouse management which involves rearranging storage blocks within
dense grids to achieve a target state. We formally define the BRaP as a graph
search problem. Building on intuitions from sliding puzzle problems, we propose
five search-based solution algorithms, leveraging joint configuration space
search, classical planning, multi-agent pathfinding, and expert heuristics. We
evaluate the five approaches empirically for plan quality and scalability.
Despite the exponential relation between search space size and block number,
our methods demonstrate efficiency in creating rearrangement plans for deeply
buried blocks in up to 80x80 grids.

</details>


### [48] [FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games](https://arxiv.org/abs/2509.01052)
*Jaewoo Ahn,Junseo Kim,Heeseung Yun,Jaehyeon Son,Dongmin Park,Jaewoong Cho,Gunhee Kim*

Main category: cs.AI

TL;DR: 提出FlashAdventure基准、CUA - as - Judge评估器和COAST框架测试GUI代理在冒险游戏中完成完整剧情的能力，实验显示COAST有改进但人与最佳代理仍有差距。


<details>
  <summary>Details</summary>
Motivation: 现有游戏基准缺乏多样性且很少评估代理完成完整剧情的能力，冒险游戏因复杂叙事交互带来挑战。

Method: 引入FlashAdventure基准，提出CUA - as - Judge自动化评估器和COAST代理框架。

Result: 当前GUI代理难以完成完整剧情，COAST通过弥合观察 - 行为差距提高了里程碑完成率。

Conclusion: 人与最佳代理存在明显差距，需继续研究缩小这一差距。

Abstract: GUI agents powered by LLMs show promise in interacting with diverse digital
environments. Among these, video games offer a valuable testbed due to their
varied interfaces, with adventure games posing additional challenges through
complex, narrative-driven interactions. Existing game benchmarks, however, lack
diversity and rarely evaluate agents on completing entire storylines. To
address this, we introduce FlashAdventure, a benchmark of 34 Flash-based
adventure games designed to test full story arc completion and tackle the
observation-behavior gap: the challenge of remembering and acting on earlier
gameplay information. We also propose CUA-as-a-Judge, an automated gameplay
evaluator, and COAST, an agentic framework leveraging long-term clue memory to
better plan and solve sequential tasks. Experiments show current GUI agents
struggle with full story arcs, while COAST improves milestone completion by
bridging the observation-behavior gap. Nonetheless, a marked discrepancy
between humans and best-performing agents warrants continued research efforts
to narrow this divide.

</details>


### [49] [VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use](https://arxiv.org/abs/2509.01055)
*Dongfu Jiang,Yi Lu,Zhuofeng Li,Zhiheng Lyu,Ping Nie,Haozhe Wang,Alex Su,Hui Chen,Kai Zou,Chao Du,Tianyu Pang,Wenhu Chen*

Main category: cs.AI

TL;DR: 介绍了统一模块化框架VerlTool，解决现有ARLT方法的低效问题，有多项关键贡献，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有ARLT方法存在代码碎片化、同步执行瓶颈和跨领域扩展性有限等问题，阻碍社区采用和算法创新。

Method: 通过系统设计原则构建统一模块化框架VerlTool，提供上游对齐、统一工具管理、异步执行等功能。

Result: 在6个ARLT领域表现有竞争力，结果与专业系统相当，实现近2倍加速，能快速集成工具。

Conclusion: VerlTool为工具增强的强化学习研究提供可扩展基础，减少开发开销。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated
success in enhancing LLM reasoning capabilities, but remains limited to
single-turn interactions without tool integration. While recent Agentic
Reinforcement Learning with Tool use (ARLT) approaches have emerged to address
multi-turn tool interactions, existing works develop task-specific codebases
that suffer from fragmentation, synchronous execution bottlenecks, and limited
extensibility across domains. These inefficiencies hinder broader community
adoption and algorithmic innovation. We introduce VerlTool, a unified and
modular framework that addresses these limitations through systematic design
principles. VerlTool provides four key contributions: (1) upstream alignment
with VeRL ensuring compatibility and simplified maintenance, (2) unified tool
management via standardized APIs supporting diverse modalities including code
execution, search, SQL databases, and vision processing, (3) asynchronous
rollout execution achieving near 2$\times$ speedup by eliminating
synchronization bottlenecks, and (4) comprehensive evaluation demonstrating
competitive performance across 6 ARLT domains. Our framework formalizes ARLT as
multi-turn trajectories with multi-modal observation tokens (text/image/video),
extending beyond single-turn RLVR paradigms. We train and evaluate models on
mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web
search, and software engineering tasks, achieving results comparable to
specialized systems while providing unified training infrastructure. The
modular plugin architecture enables rapid tool integration requiring only
lightweight Python definitions, significantly reducing development overhead and
providing a scalable foundation for tool-augmented RL research. Our code is
open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.

</details>


### [50] [Robix: A Unified Model for Robot Interaction, Reasoning and Planning](https://arxiv.org/abs/2509.01106)
*Huang Fang,Mengxi Zhang,Heng Dong,Wei Li,Zixuan Wang,Qifeng Zhang,Xueyun Tian,Yucheng Hu,Hang Li*

Main category: cs.AI

TL;DR: 介绍统一模型Robix，集成机器人推理、任务规划和自然语言交互，有新能力，采用三阶段训练策略，实验表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 构建能集成机器人推理、任务规划和自然语言交互，使机器人可遵循复杂指令、规划长时任务并自然与人交互的统一模型。

Method: 采用链思维推理，三阶段训练策略：持续预训练增强基础具身推理能力；监督微调将人机交互和任务规划建模为统一推理 - 行动序列；强化学习提高推理 - 行动一致性和长时任务连贯性。

Result: Robix在交互式任务执行中优于开源和商业基线，在不同指令类型和用户参与任务中表现出强泛化性。

Conclusion: Robix是有效的统一模型，能提升机器人在复杂任务执行和人机交互方面的能力。

Abstract: We introduce Robix, a unified model that integrates robot reasoning, task
planning, and natural language interaction within a single vision-language
architecture. Acting as the high-level cognitive layer in a hierarchical robot
system, Robix dynamically generates atomic commands for the low-level
controller and verbal responses for human interaction, enabling robots to
follow complex instructions, plan long-horizon tasks, and interact naturally
with human within an end-to-end framework. Robix further introduces novel
capabilities such as proactive dialogue, real-time interruption handling, and
context-aware commonsense reasoning during task execution. At its core, Robix
leverages chain-of-thought reasoning and adopts a three-stage training
strategy: (1) continued pretraining to enhance foundational embodied reasoning
abilities including 3D spatial understanding, visual grounding, and
task-centric reasoning; (2) supervised finetuning to model human-robot
interaction and task planning as a unified reasoning-action sequence; and (3)
reinforcement learning to improve reasoning-action consistency and long-horizon
task coherence. Extensive experiments demonstrate that Robix outperforms both
open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in
interactive task execution, demonstrating strong generalization across diverse
instruction types (e.g., open-ended, multi-stage, constrained, invalid, and
interrupted) and various user-involved tasks such as table bussing, grocery
shopping, and dietary filtering.

</details>


### [51] [Heads or Tails: A Simple Example of Causal Abstractive Simulation](https://arxiv.org/abs/2509.01136)
*Gabriel Simmons*

Main category: cs.AI

TL;DR: 本文介绍因果抽象模拟用于形式化语言模型模拟，以抛硬币为例说明模拟情况及应用价值。


<details>
  <summary>Details</summary>
Motivation: 为语言模型模拟的临时统计基准实践建立因果关系的坚实形式基础，为AI和心灵哲学提供精确操作化概念，为因果抽象研究提供新应用。

Method: 以语言模型模拟公平抛硬币为例，阐述语言模型模拟失败和成功的情况。

Result: 展示了语言模型模拟的失败案例和成功案例，说明了该形式主义可用于证明语言模型对其他系统的模拟。

Conclusion: 因果抽象模拟对语言模型模拟从业者、AI和心灵哲学家、因果抽象研究者有一定价值。

Abstract: This note illustrates how a variety of causal abstraction arXiv:1707.00819
arXiv:1812.03789, defined here as causal abstractive simulation, can be used to
formalize a simple example of language model simulation. This note considers
the case of simulating a fair coin toss with a language model. Examples are
presented illustrating the ways language models can fail to simulate, and a
success case is presented, illustrating how this formalism may be used to prove
that a language model simulates some other system, given a causal description
of the system. This note may be of interest to three groups. For practitioners
in the growing field of language model simulation, causal abstractive
simulation is a means to connect ad-hoc statistical benchmarking practices to
the solid formal foundation of causality. Philosophers of AI and philosophers
of mind may be interested as causal abstractive simulation gives a precise
operationalization to the idea that language models are role-playing
arXiv:2402.12422. Mathematicians and others working on causal abstraction may
be interested to see a new application of the core ideas that yields a new
variation of causal abstraction.

</details>


### [52] [Towards Open-World Retrieval-Augmented Generation on Knowledge Graph: A Multi-Agent Collaboration Framework](https://arxiv.org/abs/2509.01238)
*Jiasheng Xu,Mingda Li,Yongqiang Tang,Peijie Wang,Wensheng Zhang*

Main category: cs.AI

TL;DR: 现有基于知识图谱的检索增强生成方法依赖预定义锚实体，本文提出AnchorRAG多智能体协作框架，不依赖预定义锚实体，实验显示其显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的检索增强生成方法在开放世界场景中，因依赖预定义锚实体而鲁棒性受限，需要改进。

Method: 提出AnchorRAG框架，由预测智能体动态识别候选锚实体，初始化多个检索智能体并行多跳探索，监督智能体制定检索策略并合成知识路径生成答案。

Result: 在四个公开基准测试中，AnchorRAG显著优于现有基线，在现实世界问答任务中取得新的最优结果。

Conclusion: AnchorRAG框架能提高检索鲁棒性，减轻模糊或错误锚点的影响，在问答任务中表现出色。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
language understanding and reasoning. However, their dependence on static
training corpora makes them prone to factual errors and knowledge gaps.
Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating
external knowledge sources, especially structured Knowledge Graphs (KGs), which
provide explicit semantics and efficient retrieval. Existing KG-based RAG
approaches, however, generally assume that anchor entities are accessible to
initiate graph traversal, which limits their robustness in open world settings
where accurate linking between the query and the entity is unreliable. To
overcome this limitation, we propose AnchorRAG, a novel multi-agent
collaboration framework for open-world RAG without the predefined anchor
entities. Specifically, a predictor agent dynamically identifies candidate
anchor entities by aligning user query terms with KG nodes and initializes
independent retriever agents to conduct parallel multi-hop explorations from
each candidate. Then a supervisor agent formulates the iterative retrieval
strategy for these retriever agents and synthesizes the resulting knowledge
paths to generate the final answer. This multi-agent collaboration framework
improves retrieval robustness and mitigates the impact of ambiguous or
erroneous anchors. Extensive experiments on four public benchmarks demonstrate
that AnchorRAG significantly outperforms existing baselines and establishes new
state-of-the-art results on the real-world question answering tasks.

</details>


### [53] [Towards Agentic OS: An LLM Agent Framework for Linux Schedulers](https://arxiv.org/abs/2509.01245)
*Yusheng Zheng,Yanpeng Hu,Wei Zhang,Andi Quinn*

Main category: cs.AI

TL;DR: 提出SchedCP框架，让LLM代理自主优化Linux调度器，评估显示有性能提升和成本降低。


<details>
  <summary>Details</summary>
Motivation: 操作系统调度器存在语义差距，内核策略无法理解应用特定需求，导致性能欠佳。

Method: 构建解耦控制平面，实现为MCP服务器，提供工作负载分析引擎、调度策略库和执行验证器；用sched - agent多代理系统分析工作负载、合成eBPF调度策略并部署。

Result: SchedCP实现了高达1.79倍的性能提升和13倍的成本降低，同时保持高成功率。

Conclusion: SchedCP弥合语义差距，使专家级系统优化大众化，迈向自优化、应用感知的操作系统。

Abstract: Operating system schedulers suffer from a fundamental semantic gap, where
kernel policies fail to understand application-specific needs, leading to
suboptimal performance. We introduce SchedCP, the first framework that enables
fully autonomous Large Language Model (LLM) agents to safely and efficiently
optimize Linux schedulers without human involvement. Our core insight is that
the challenge is not merely to apply a better LLM, but to architect a decoupled
control plane that separates the AI's role of semantic reasoning ("what to
optimize") from the system's role of execution ("how to observe and act").
Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable
interface with three key services: a Workload Analysis Engine, an evolving
Scheduler Policy Repository, and an Execution Verifier that validates all
AI-generated code and configure before deployment with static and dynamic
analysis.
  We demonstrate this architecture's power with sched-agent, a multi-agent
system that autonomously analyzes workloads, synthesizes custom eBPF scheduling
policies, and deploys them via the sched\_ext infrastructure. Our evaluation
shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x
cost reduction compared to naive agentic approaches, all while maintaining high
success rate. By bridging the semantic gap, SchedCP democratizes expert-level
system optimization and represents a step towards creating truly
self-optimizing, application-aware operating systems. The code is open-sourced
in https://github.com/eunomia-bpf/schedcp

</details>


### [54] [Communicative Agents for Slideshow Storytelling Video Generation based on LLMs](https://arxiv.org/abs/2509.01277)
*Jingxing Fan,Jinrong Shen,Yusheng Yao,Shuangqing Wang,Qian Wang,Yuling Wang*

Main category: cs.AI

TL;DR: 本文提出VGTeam系统，它集成大语言模型，由多个通信代理协作生成幻灯片式视频，提高效率和可扩展性，降低计算成本，具有广泛影响。


<details>
  <summary>Details</summary>
Motivation: 传统文本到视频模型计算成本高，为改进视频创作流程，提高效率和可扩展性。

Method: 提出VGTeam系统，由多个负责不同视频生成环节的通信代理在聊天塔工作流中协作，将文本提示转换为视频。

Result: 系统平均生成视频成本仅0.103美元，成功率98.4%，保持高创意保真度和定制性。

Conclusion: VGTeam使视频制作大众化，凸显语言模型在创意领域的变革潜力，是下一代内容创作的先驱系统。

Abstract: With the rapid advancement of artificial intelligence (AI), the proliferation
of AI-generated content (AIGC) tasks has significantly accelerated developments
in text-to-video generation. As a result, the field of video production is
undergoing a transformative shift. However, conventional text-to-video models
are typically constrained by high computational costs.
  In this study, we propose Video-Generation-Team (VGTeam), a novel slide show
video generation system designed to redefine the video creation pipeline
through the integration of large language models (LLMs). VGTeam is composed of
a suite of communicative agents, each responsible for a distinct aspect of
video generation, such as scriptwriting, scene creation, and audio design.
These agents operate collaboratively within a chat tower workflow, transforming
user-provided textual prompts into coherent, slide-style narrative videos.
  By emulating the sequential stages of traditional video production, VGTeam
achieves remarkable improvements in both efficiency and scalability, while
substantially reducing computational overhead. On average, the system generates
videos at a cost of only $0.103, with a successful generation rate of 98.4%.
Importantly, this framework maintains a high degree of creative fidelity and
customization.
  The implications of VGTeam are far-reaching. It democratizes video production
by enabling broader access to high-quality content creation without the need
for extensive resources. Furthermore, it highlights the transformative
potential of language models in creative domains and positions VGTeam as a
pioneering system for next-generation content creation.

</details>


### [55] [Conformal Predictive Monitoring for Multi-Modal Scenarios](https://arxiv.org/abs/2509.01338)
*Francesca Cairoli,Luca Bortolussi,Jyotirmoy V. Deshmukh,Lars Lindemann,Nicola Paoletti*

Main category: cs.AI

TL;DR: 提出GenQPM方法解决现有QPM方法在多模态动力学下的问题，在导航和自动驾驶任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有QPM方法在处理多模态动力学系统时，给出的预测区间过于保守且缺乏特定模式满意度信息，需要改进。

Method: GenQPM利用基于分数的扩散模型近似系统动力学，用模式分类器按动力学模式划分预测轨迹，对每个模式应用共形推理生成特定模式预测区间。

Result: 在代理导航和自动驾驶任务基准测试中，GenQPM生成的预测区间比无模式感知的基线方法更具信息性（保守性更低）。

Conclusion: GenQPM方法能有效解决现有QPM方法在多模态动力学系统中的问题，提供更有意义的预测区间。

Abstract: We consider the problem of quantitative predictive monitoring (QPM) of
stochastic systems, i.e., predicting at runtime the degree of satisfaction of a
desired temporal logic property from the current state of the system. Since
computational efficiency is key to enable timely intervention against predicted
violations, several state-of-the-art QPM approaches rely on fast
machine-learning surrogates to provide prediction intervals for the
satisfaction values, using conformal inference to offer statistical guarantees.
However, these QPM methods suffer when the monitored agent exhibits multi-modal
dynamics, whereby certain modes may yield high satisfaction values while others
critically violate the property. Existing QPM methods are mode-agnostic and so
would yield overly conservative and uninformative intervals that lack
meaningful mode-specific satisfaction information. To address this problem, we
present GenQPM, a method that leverages deep generative models, specifically
score-based diffusion models, to reliably approximate the probabilistic and
multi-modal system dynamics without requiring explicit model access. GenQPM
employs a mode classifier to partition the predicted trajectories by dynamical
mode. For each mode, we then apply conformal inference to produce statistically
valid, mode-specific prediction intervals. We demonstrate the effectiveness of
GenQPM on a benchmark of agent navigation and autonomous driving tasks,
resulting in prediction intervals that are significantly more informative (less
conservative) than mode-agnostic baselines.

</details>


### [56] [Error Notebook-Guided, Training-Free Part Retrieval in 3D CAD Assemblies via Vision-Language Models](https://arxiv.org/abs/2509.01350)
*Yunqing Liu,Nan Zhang,Zhiming Tan*

Main category: cs.AI

TL;DR: 提出无需额外训练的零件检索框架，用错误笔记+RAG改进现有通用模型检索性能，实验有显著提升。


<details>
  <summary>Details</summary>
Motivation: 直接用LLMs/VLMs进行CAD组件零件检索有输入超限制、性能不佳和难以微调等问题。

Method: 构建错误笔记，利用RAG从错误笔记检索相关记录融入推理过程，使用人在环CAD数据集评估。

Result: 使用GPT - 4o和Gemini系列等专有模型实验有显著提升，GPT - 4o在人类偏好数据集上绝对准确率提升达23.4%，消融实验表明CoT推理在零件数量多的复杂情况有益。

Conclusion: 提出的新框架能有效处理含长非自然语言元数据的3D模型，有工程价值。

Abstract: Effective specification-aware part retrieval within complex CAD assemblies is
essential for automated design verification and downstream engineering tasks.
However, directly using LLMs/VLMs to this task presents some challenges: the
input sequences may exceed model token limits, and even after processing,
performance remains unsatisfactory. Moreover, fine-tuning LLMs/VLMs requires
significant computational resources, and for many high-performing general-use
proprietary models (e.g., GPT or Gemini), fine-tuning access is not available.
In this paper, we propose a novel part retrieval framework that requires no
extra training, but using Error Notebooks + RAG for refined prompt engineering
to help improve the existing general model's retrieval performance. The
construction of Error Notebooks consists of two steps: (1) collecting
historical erroneous CoTs and their incorrect answers, and (2) connecting these
CoTs through reflective corrections until the correct solutions are obtained.
As a result, the Error Notebooks serve as a repository of tasks along with
their corrected CoTs and final answers. RAG is then employed to retrieve
specification-relevant records from the Error Notebooks and incorporate them
into the inference process. Another major contribution of our work is a
human-in-the-loop CAD dataset, which is used to evaluate our method. In
addition, the engineering value of our novel framework lies in its ability to
effectively handle 3D models with lengthy, non-natural language metadata.
Experiments with proprietary models, including GPT-4o and the Gemini series,
show substantial gains, with GPT-4o (Omni) achieving up to a 23.4% absolute
accuracy improvement on the human preference dataset. Moreover, ablation
studies confirm that CoT reasoning provides benefits especially in challenging
cases with higher part counts (>10).

</details>


### [57] [DeepResearch Arena: The First Exam of LLMs' Research Abilities via Seminar-Grounded Tasks](https://arxiv.org/abs/2509.01396)
*Haiyuan Wan,Chen Yang,Junchi Yu,Meiqi Tu,Jiaxuan Lu,Di Yu,Jianbao Cao,Ben Gao,Jiaqing Xie,Aoran Wang,Wenlong Zhang,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: 本文引入基于学术研讨会的基准测试DeepResearch Arena，并提出MAHTG系统构建它，评估显示其对现有模型有挑战。


<details>
  <summary>Details</summary>
Motivation: 现有评估深度研究代理研究能力的方法因难以收集前沿研究问题而存在不足。

Method: 引入基于学术研讨会的DeepResearch Arena基准测试，提出Multi - Agent Hierarchical Task Generation (MAHTG)系统自动构建该基准。

Result: 用MAHTG系统从200多个学术研讨会中策划出包含超10000个高质量研究任务的DeepResearch Arena，涵盖12个学科。

Conclusion: DeepResearch Arena对当前最先进的代理提出了重大挑战，不同模型表现有明显差距。

Abstract: Deep research agents have attracted growing attention for their potential to
orchestrate multi-stage research workflows, spanning literature synthesis,
methodological design, and empirical verification. Despite these strides,
evaluating their research capability faithfully is rather challenging due to
the difficulty of collecting frontier research questions that genuinely capture
researchers' attention and intellectual curiosity. To address this gap, we
introduce DeepResearch Arena, a benchmark grounded in academic seminars that
capture rich expert discourse and interaction, better reflecting real-world
research environments and reducing the risk of data leakage. To automatically
construct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task
Generation (MAHTG) system that extracts research-worthy inspirations from
seminar transcripts. The MAHTG system further translates research-worthy
inspirations into high-quality research tasks, ensuring the traceability of
research task formulation while filtering noise. With the MAHTG system, we
curate DeepResearch Arena with over 10,000 high-quality research tasks from
over 200 academic seminars, spanning 12 disciplines, such as literature,
history, and science. Our extensive evaluation shows that DeepResearch Arena
presents substantial challenges for current state-of-the-art agents, with clear
performance gaps observed across different models.

</details>


### [58] [The Need for Verification in AI-Driven Scientific Discovery](https://arxiv.org/abs/2509.01398)
*Cristina Cornelio,Takuya Ito,Ryan Cory-Wright,Sanjeeb Dash,Lior Horesh*

Main category: cs.AI

TL;DR: 人工智能虽能大量快速生成科学假设，但验证机制缺乏会阻碍科学进步，本文回顾科学发现发展，介绍AI重塑科学发现实践的方法，强调验证是AI辅助发现的基石。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能大量生成假设却缺乏可靠验证机制这一阻碍科学进步的问题。

Method: 追溯科学发现的历史发展，研究AI对科学发现实践的重塑，回顾从数据驱动方法到符号推理框架等主要方法。

Result: 指出这些系统能发现模式和提出候选定律。

Conclusion: 严格透明的验证必须是AI辅助发现的基石。

Abstract: Artificial intelligence (AI) is transforming the practice of science. Machine
learning and large language models (LLMs) can generate hypotheses at a scale
and speed far exceeding traditional methods, offering the potential to
accelerate discovery across diverse fields. However, the abundance of
hypotheses introduces a critical challenge: without scalable and reliable
mechanisms for verification, scientific progress risks being hindered rather
than being advanced. In this article, we trace the historical development of
scientific discovery, examine how AI is reshaping established practices for
scientific discovery, and review the principal approaches, ranging from
data-driven methods and knowledge-aware neural architectures to symbolic
reasoning frameworks and LLM agents. While these systems can uncover patterns
and propose candidate laws, their scientific value ultimately depends on
rigorous and transparent verification, which we argue must be the cornerstone
of AI-assisted discovery.

</details>


### [59] [LLM-empowered Agents Simulation Framework for Scenario Generation in Service Ecosystem Governance](https://arxiv.org/abs/2509.01441)
*Deyu Zhou,Yuqi Hou,Xiao Xue,Xudong Lu,Qingzhong Li,Lizhen Cui*

Main category: cs.AI

TL;DR: 文章提出基于大语言模型代理的场景生成器设计方法用于服务生态系统治理，实验表明该方法能更高效准确生成场景。


<details>
  <summary>Details</summary>
Motivation: 社会环境复杂，服务生态系统治理中现有场景分析方法有局限性，需更好方法来构建实验系统和生成高质量场景。

Method: 提出场景生成器设计方法，协调三个大语言模型赋能的代理（环境代理、社会代理、规划代理）来自主优化实验方案以构建实验系统和生成场景。

Result: 在ProgrammableWeb数据集上实验，该方法能更高效准确地生成场景。

Conclusion: 该方法为服务生态系统治理相关实验系统构建提供了有效途径。

Abstract: As the social environment is growing more complex and collaboration is
deepening, factors affecting the healthy development of service ecosystem are
constantly changing and diverse, making its governance a crucial research
issue. Applying the scenario analysis method and conducting scenario rehearsals
by constructing an experimental system before managers make decisions, losses
caused by wrong decisions can be largely avoided. However, it relies on
predefined rules to construct scenarios and faces challenges such as limited
information, a large number of influencing factors, and the difficulty of
measuring social elements. These challenges limit the quality and efficiency of
generating social and uncertain scenarios for the service ecosystem. Therefore,
we propose a scenario generator design method, which adaptively coordinates
three Large Language Model (LLM) empowered agents that autonomously optimize
experimental schemes to construct an experimental system and generate high
quality scenarios. Specifically, the Environment Agent (EA) generates social
environment including extremes, the Social Agent (SA) generates social
collaboration structure, and the Planner Agent (PA) couples task-role
relationships and plans task solutions. These agents work in coordination, with
the PA adjusting the experimental scheme in real time by perceiving the states
of each agent and these generating scenarios. Experiments on the
ProgrammableWeb dataset illustrate our method generates more accurate scenarios
more efficiently, and innovatively provides an effective way for service
ecosystem governance related experimental system construction.

</details>


### [60] [Counterfactual Sensitivity for Faithful Reasoning in Language Models](https://arxiv.org/abs/2509.01544)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.AI

TL;DR: 提出Counterfactual Sensitivity Regularization (CSR)提升大语言模型推理忠实度，在多任务上效果好，还可推广并与其他方法协同。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常依赖有缺陷或无关推理得出正确答案，影响其在高风险领域的可信度。

Method: 提出CSR，训练时引入自动化、操作符级别的反事实干预并惩罚在逻辑无效推理下答案不变的模型；引入Counterfactual Outcome Sensitivity (COS)衡量忠实度。

Result: 在多个结构化推理任务中，CSR比标准微调与过程监督提升忠实度达70个百分点，仅伴随轻微准确率损失；学习到的敏感性可推广到更大模型，且与推理时间方法协同；在HellaSwag上扩展CSR可增强常识推理忠实度。

Conclusion: CSR是一种轻量级训练目标，能有效提升大语言模型推理的忠实度。

Abstract: Large language models (LLMs) often produce correct answers while relying on
flawed or irrelevant reasoning traces, undermining their trustworthiness in
high-stakes domains. We propose Counterfactual Sensitivity Regularization
(CSR), a lightweight training objective that enforces dependence between
intermediate reasoning and final outputs. CSR introduces automated,
operator-level counterfactual interventions (e.g., swapping "+" with "-")
during training and penalizes models that preserve the same answer under
logically invalid traces. This requires only one additional forward pass per
sample. To measure faithfulness, we introduce Counterfactual Outcome
Sensitivity (COS), which quantifies the impact of such perturbations on model
predictions. Across structured reasoning tasks - arithmetic (GSM8K), logical
deduction (PrOntoQA), and planning (Blocks World) - CSR improves faithfulness
by up to 70 percentage points over standard fine-tuning and process
supervision, with only minor accuracy loss. The learned sensitivity generalizes
to larger models and synergizes with inference-time methods such as
self-consistency. A pilot study on HellaSwag further demonstrates that
extending CSR with semantic perturbations can enhance faithfulness in
commonsense reasoning.

</details>


### [61] [Structured AI Decision-Making in Disaster Management](https://arxiv.org/abs/2509.01576)
*Julian Gerald Dcruz,Argyrios Zolotas,Niall Ross Greenwood,Miguel Arana-Catania*

Main category: cs.AI

TL;DR: 本文提出结构化决策框架用于安全关键领域的自主决策，在灾难管理中评估其性能，结果显示比基于判断的系统和人类操作员更优，有望构建更可靠的自主AI应用。


<details>
  <summary>Details</summary>
Motivation: 人工智能应用于安全关键领域决策时，需解决决策的伦理问题，使决策可靠且合理。

Method: 提出结构化决策框架并应用于灾难管理中的自主决策，引入Enabler agents、Levels和Scenarios概念，与基于判断的系统和有灾难经验的人类操作员对比评估。

Result: 结构化决策框架在多场景下准确决策的稳定性比基于判断的系统高60.94%，在不同场景下的准确率比人类操作员高38.93%。

Conclusion: 结构化决策框架有望在安全关键场景中构建更可靠的自主AI应用。

Abstract: With artificial intelligence (AI) being applied to bring autonomy to
decision-making in safety-critical domains such as the ones typified in the
aerospace and emergency-response services, there has been a call to address the
ethical implications of structuring those decisions, so they remain reliable
and justifiable when human lives are at stake. This paper contributes to
addressing the challenge of decision-making by proposing a structured
decision-making framework as a foundational step towards responsible AI. The
proposed structured decision-making framework is implemented in autonomous
decision-making, specifically within disaster management. By introducing
concepts of Enabler agents, Levels and Scenarios, the proposed framework's
performance is evaluated against systems relying solely on judgement-based
insights, as well as human operators who have disaster experience: victims,
volunteers, and stakeholders. The results demonstrate that the structured
decision-making framework achieves 60.94% greater stability in consistently
accurate decisions across multiple Scenarios, compared to judgement-based
systems. Moreover, the study shows that the proposed framework outperforms
human operators with a 38.93% higher accuracy across various Scenarios. These
findings demonstrate the promise of the structured decision-making framework
for building more reliable autonomous AI applications in safety-critical
contexts.

</details>


### [62] [Throttling Web Agents Using Reasoning Gates](https://arxiv.org/abs/2509.01619)
*Abhinav Kumar,Jaechul Roh,Ali Naseh,Amir Houmansadr,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 本文提出Web Agent Throttling框架，通过推理门对AI网络代理施加可调成本，实现计算不对称并进行了评估，还讨论了部署限制和环境影响。


<details>
  <summary>Details</summary>
Motivation: AI网络代理可能恶意或错误使用，会过载内容提供商、绕过防御，内容提供商需保护服务和内容免受攻击和抓取。

Method: 设计Throttling Gates框架，聚焦语言模型，让代理解决推理谜题；因现有谜题不满足要求，引入基于画谜的推理门并设计可扩展的生成和验证协议。

Result: 框架实现计算不对称，响应生成成本比SOTA模型的生成成本高9.2倍；在自定义网站和MCP服务器上部署推理门，用真实网络代理进行评估。

Conclusion: 讨论了框架在现实世界部署的局限性和环境影响。

Abstract: AI web agents use Internet resources at far greater speed, scale, and
complexity -- changing how users and services interact. Deployed maliciously or
erroneously, these agents could overload content providers. At the same time,
web agents can bypass CAPTCHAs and other defenses by mimicking user behavior or
flood authentication systems with fake accounts. Yet providers must protect
their services and content from denial-of-service attacks and scraping by web
agents. In this paper, we design a framework that imposes tunable costs on
agents before providing access to resources; we call this Web Agent Throttling.
We start by formalizing Throttling Gates as challenges issued to an agent that
are asymmetric, scalable, robust, and compatible with any agent. Focusing on a
common component -- the language model -- we require the agent to solve
reasoning puzzles, thereby incurring excessive token-generation costs. However,
we find that using existing puzzles, e.g., coding or math, as throttling gates
fails to satisfy our properties. To address this, we introduce rebus-based
Reasoning Gates, synthetic text puzzles that require multi-hop reasoning over
world knowledge (thereby throttling an agent's model). We design a scalable
generation and verification protocol for such reasoning gates. Our framework
achieves computational asymmetry, i.e., the response-generation cost is 9.2x
higher than the generation cost for SOTA models. We further deploy reasoning
gates on a custom website and Model Context Protocol (MCP) servers and evaluate
with real-world web agents. Finally, we discuss the limitations and
environmental impact of real-world deployment of our framework.

</details>


### [63] [Unraveling LLM Jailbreaks Through Safety Knowledge Neurons](https://arxiv.org/abs/2509.01631)
*Chongwen Zhao,Kaizhu Huang*

Main category: cs.AI

TL;DR: 提出神经元级可解释性方法和SafeTuning微调策略防御大语言模型越狱攻击，降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型越狱攻击防御原理不明，需深入探究。

Method: 提出神经元级可解释性方法，将模型内部表征投影到词汇空间，调整安全相关神经元激活；提出SafeTuning微调策略强化关键神经元。

Result: 调整安全相关神经元激活控制模型行为平均ASR超97%；SafeTuning降低多模型攻击成功率，优于四个基线防御。

Conclusion: 研究为理解和防御越狱攻击提供新视角。

Abstract: Large Language Models (LLMs) are increasingly attracting attention in various
applications. Nonetheless, there is a growing concern as some users attempt to
exploit these models for malicious purposes, including the synthesis of
controlled substances and the propagation of disinformation, a technique known
as "Jailbreak." While some studies have achieved defenses against jailbreak
attacks by modifying output distributions or detecting harmful content, the
exact rationale still remains elusive. In this work, we present a novel
neuron-level interpretability method that focuses on the role of safety-related
knowledge neurons. Unlike existing approaches, our method projects the model's
internal representation into a more consistent and interpretable vocabulary
space. We then show that adjusting the activation of safety-related neurons can
effectively control the model's behavior with a mean ASR higher than 97%.
Building on this insight, we propose SafeTuning, a fine-tuning strategy that
reinforces safety-critical neurons to improve model robustness against
jailbreaks. SafeTuning consistently reduces attack success rates across
multiple LLMs and outperforms all four baseline defenses. These findings offer
a new perspective on understanding and defending against jailbreak attacks.

</details>


### [64] [Physics Supernova: AI Agent Matches Elite Gold Medalists at IPhO 2025](https://arxiv.org/abs/2509.01659)
*Jiahao Qiu,Jingzhe Shi,Xinzhe Juan,Zelin Zhao,Jiayi Geng,Shilong Liu,Hongru Wang,Sanfeng Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: 介绍AI系统Physics Supernova，其物理问题解决能力与国际物理奥赛金牌得主相当，分析其能力并得出工具集成可提升解题表现的结论。


<details>
  <summary>Details</summary>
Motivation: 通用AI系统需具备强物理问题解决能力，国际物理奥赛可作为严格基准来评估。

Method: 开发Physics Supernova系统并让其参加IPhO 2025理论问题测试，广泛分析其在不同物理任务中的能力和灵活性。

Result: Physics Supernova在IPhO 2025理论问题中获23.5/30分，排名第14，超过人类金牌得主的中位数表现。

Conclusion: 智能体系统中原则性的工具集成可在解决挑战性科学问题上带来有竞争力的提升。

Abstract: Physics provides fundamental laws that describe and predict the natural
world. AI systems aspiring toward more general, real-world intelligence must
therefore demonstrate strong physics problem-solving abilities: to formulate
and apply physical laws for explaining and predicting physical processes. The
International Physics Olympiad (IPhO)--the world's most prestigious physics
competition--offers a rigorous benchmark for this purpose. We introduce Physics
Supernova, an AI agent system with superior physics problem-solving abilities
that match elite IPhO gold medalists. In IPhO 2025 theory problems, Physics
Supernova attains 23.5/30 points, ranking 14th of 406 contestants and
surpassing the median performance of human gold medalists. We extensively
analyzed Physics Supernova's capabilities and flexibility across diverse
physics tasks. These results show that principled tool integration within agent
systems can deliver competitive improvements in solving challenging science
problems. The codes are available at
https://github.com/CharlesQ9/Physics-Supernova.

</details>


### [65] [An LLM-enabled semantic-centric framework to consume privacy policies](https://arxiv.org/abs/2509.01716)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites, despite claiming
otherwise, due to the practical difficulty in comprehending them. The mist of
data privacy practices forms a major barrier for user-centred Web approaches,
and for data sharing and reusing in an agentic world. Existing research
proposed methods for using formal languages and reasoning for verifying the
compliance of a specified policy, as a potential cure for ignoring privacy
policies. However, a critical gap remains in the creation or acquisition of
such formal policies at scale. We present a semantic-centric approach for using
state-of-the-art large language models (LLM), to automatically identify key
information about privacy practices from privacy policies, and construct
$\mathit{Pr}^2\mathit{Graph}$, knowledge graph with grounding from Data Privacy
Vocabulary (DPV) for privacy practices, to support downstream tasks. Along with
the pipeline, the $\mathit{Pr}^2\mathit{Graph}$ for the top-100 popular
websites is also released as a public resource, by using the pipeline for
analysis. We also demonstrate how the $\mathit{Pr}^2\mathit{Graph}$ can be used
to support downstream tasks by constructing formal policy representations such
as Open Digital Right Language (ODRL) or perennial semantic Data Terms of Use
(psDToU). To evaluate the technology capability, we enriched the Policy-IE
dataset by employing legal experts to create custom annotations. We benchmarked
the performance of different large language models for our pipeline and
verified their capabilities. Overall, they shed light on the possibility of
large-scale analysis of online services' privacy practices, as a promising
direction to audit the Web and the Internet. We release all datasets and source
code as public resources to facilitate reuse and improvement.

</details>


### [66] [Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models](https://arxiv.org/abs/2509.01909)
*Ranjie Duan,Jiexi Liu,Xiaojun Jia,Shiji Zhao,Ruoxi Cheng,Fengxiang Wang,Cheng Wei,Yong Xie,Chang Liu,Defeng Li,Yinpeng Dong,Yichi Zhang,Yuefeng Chen,Chongwen Wang,Xingjun Ma,Xingxing Wei,Yang Liu,Hang Su,Jun Zhu,Xinfeng Li,Yitong Sun,Jie Zhang,Jinzhao Hu,Sha Xu,Yitong Yang,Jialing Tao,Hui Xue*

Main category: cs.AI

TL;DR: 提出Constructive Safety Alignment (CSA)范式，实现了模型安全与通用能力的平衡，并发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型安全机制多关注恶意用户风险，忽略非恶意心理困扰用户，简单拒绝可能导致更糟结果，需新安全范式。

Method: 在Oyster - I (Oy1)中实现CSA，结合博弈论预测用户反应、细粒度风险边界发现和可解释推理控制。

Result: Oy1在开放模型中达到最先进安全水平，在Constructive Benchmark上表现接近GPT - 5，在Strata - Sword越狱数据集上接近GPT - o1。

Conclusion: 从拒绝优先转向引导优先的安全模式，重新定义模型与用户关系，目标是打造不仅安全且有实际帮助的系统。

Abstract: Large language models (LLMs) typically deploy safety mechanisms to prevent
harmful content generation. Most current approaches focus narrowly on risks
posed by malicious actors, often framing risks as adversarial events and
relying on defensive refusals. However, in real-world settings, risks also come
from non-malicious users seeking help while under psychological distress (e.g.,
self-harm intentions). In such cases, the model's response can strongly
influence the user's next actions. Simple refusals may lead them to repeat,
escalate, or move to unsafe platforms, creating worse outcomes. We introduce
Constructive Safety Alignment (CSA), a human-centric paradigm that protects
against malicious misuse while actively guiding vulnerable users toward safe
and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic
anticipation of user reactions, fine-grained risk boundary discovery, and
interpretable reasoning control, turning safety into a trust-building process.
Oy1 achieves state-of-the-art safety among open models while retaining high
general capabilities. On our Constructive Benchmark, it shows strong
constructive engagement, close to GPT-5, and unmatched robustness on the
Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from
refusal-first to guidance-first safety, CSA redefines the model-user
relationship, aiming for systems that are not just safe, but meaningfully
helpful. We release Oy1, code, and the benchmark to support responsible,
user-centered AI.

</details>


### [67] [How Real Is AI Tutoring? Comparing Simulated and Human Dialogues in One-on-One Instruction](https://arxiv.org/abs/2509.01914)
*Ruijia Li,Yuan-Hao Jiang,Jiatong Wang,Bo Jiang*

Main category: cs.AI

TL;DR: 研究对比AI模拟与真人辅导对话，发现真人对话更优，为设计有效教育对话系统提供指导。


<details>
  <summary>Details</summary>
Motivation: 启发式和支架式师生对话对学生高阶思维和深度学习很重要，但大语言模型生成此类互动有挑战，需研究AI模拟与真人辅导对话差异。

Method: 使用启动 - 响应 - 反馈（IRF）编码方案和认知网络分析（ENA）进行定量比较。

Result: 真人对话在话语长度、提问和反馈行为上优于AI，互动模式差异大，真人对话认知引导性强且多样，AI模拟对话结构简化、行为趋同。

Conclusion: 揭示当前AI生成辅导的关键局限，为设计和评估更有效的生成式教育对话系统提供实证指导。

Abstract: Heuristic and scaffolded teacher-student dialogues are widely regarded as
critical for fostering students' higher-order thinking and deep learning.
However, large language models (LLMs) currently face challenges in generating
pedagogically rich interactions. This study systematically investigates the
structural and behavioral differences between AI-simulated and authentic human
tutoring dialogues. We conducted a quantitative comparison using an
Initiation-Response-Feedback (IRF) coding scheme and Epistemic Network Analysis
(ENA). The results show that human dialogues are significantly superior to
their AI counterparts in utterance length, as well as in questioning (I-Q) and
general feedback (F-F) behaviors. More importantly, ENA results reveal a
fundamental divergence in interactional patterns: human dialogues are more
cognitively guided and diverse, centered around a "question-factual
response-feedback" teaching loop that clearly reflects pedagogical guidance and
student-driven thinking; in contrast, simulated dialogues exhibit a pattern of
structural simplification and behavioral convergence, revolving around an
"explanation-simplistic response" loop that is essentially a simple information
transfer between the teacher and student. These findings illuminate key
limitations in current AI-generated tutoring and provide empirical guidance for
designing and evaluating more pedagogically effective generative educational
dialogue systems.

</details>


### [68] [Dynamic Speculative Agent Planning](https://arxiv.org/abs/2509.01920)
*Yilin Guan,Wenyue Hua,Qingfeng Lan,Sun Fei,Dujian Ding,Devang Acharya,Chi Wang,William Yang Wang*

Main category: cs.AI

TL;DR: 提出动态推测规划（DSP）框架，实现无损加速且成本大幅降低，实验显示效率高且成本显著减少。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理部署面临延迟和推理成本高的问题，现有加速方法有局限。

Method: 引入异步在线强化学习框架Dynamic Speculative Planning（DSP），优化联合目标平衡端到端延迟和成本。

Result: 在两个标准代理基准测试中，DSP达到与最快无损加速方法相当的效率，总成本降低30%，不必要成本最多降低60%。

Conclusion: DSP能有效解决大语言模型代理部署的成本和加速问题，可通过调整参数平衡性能。

Abstract: Despite their remarkable success in complex tasks propelling widespread
adoption, large language-model-based agents still face critical deployment
challenges due to prohibitive latency and inference costs. While recent work
has explored various methods to accelerate inference, existing approaches
suffer from significant limitations: they either fail to preserve performance
fidelity, require extensive offline training of router modules, or incur
excessive operational costs. Moreover, they provide minimal user control over
the tradeoff between acceleration and other performance metrics. To address
these gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous
online reinforcement learning framework that provides lossless acceleration
with substantially reduced costs without requiring additional pre-deployment
preparation. DSP explicitly optimizes a joint objective balancing end-to-end
latency against dollar cost, allowing practitioners to adjust a single
parameter that steers the system toward faster responses, cheaper operation, or
any point along this continuum. Experiments on two standard agent benchmarks
demonstrate that DSP achieves comparable efficiency to the fastest lossless
acceleration method while reducing total cost by 30% and unnecessary cost up to
60%. Our code and data are available through
https://github.com/guanyilin428/Dynamic-Speculative-Planning.

</details>


### [69] [EigenBench: A Comparative Behavioral Measure of Value Alignment](https://arxiv.org/abs/2509.01938)
*Jonathn Chang,Leonard Piff,Suvadip Sana,Jasmine X. Li,Lionel Levine*

Main category: cs.AI

TL;DR: 提出EigenBench方法对语言模型的价值观进行基准测试，该方法无需真实标签，测试发现分数方差多由提示语解释，少量体现模型自身倾向。


<details>
  <summary>Details</summary>
Motivation: 解决价值对齐缺乏量化指标的问题。

Method: 提出EigenBench黑盒方法，让模型相互评判输出，用EigenTrust聚合判断结果得出量化分数。

Result: 发现EigenBench分数的大部分方差由提示语解释，少量体现模型自身倾向。

Conclusion: EigenBench可用于对语言模型的价值观进行比较基准测试。

Abstract: Aligning AI with human values is a pressing unsolved problem. To address the
lack of quantitative metrics for value alignment, we propose EigenBench: a
black-box method for comparatively benchmarking language models' values. Given
an ensemble of models, a constitution describing a value system, and a dataset
of scenarios, our method returns a vector of scores quantifying each model's
alignment to the given constitution. To produce these scores, each model judges
the outputs of other models across many scenarios, and these judgments are
aggregated with EigenTrust (Kamvar et al, 2003), yielding scores that reflect a
weighted-average judgment of the whole ensemble. EigenBench uses no ground
truth labels, as it is designed to quantify traits for which reasonable judges
may disagree on the correct label. Using prompted personas, we test whether
EigenBench scores are more sensitive to the model or the prompt: we find that
most of the variance is explained by the prompt, but a small residual
quantifies the disposition of the model itself.

</details>


### [70] [mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in Clinical Decision Support](https://arxiv.org/abs/2509.02007)
*Shreyash Adappanavar,Krithi Shailya,Gokul S Krishnan,Sriraam Natarajan,Balaraman Ravindran*

Main category: cs.AI

TL;DR: 本文指出大语言模型在医疗场景部署存在AI对齐挑战，现有公平性评估方法不足。构建两个大规模基准测试，提出多指标框架mFARM及FAB分数，评估多个开源大模型，发现mFARM能有效捕捉偏差，模型在量化下表现稳定但上下文减少时性能下降。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险医疗场景部署存在AI对齐挑战，现有公平性评估方法在该场景存在缺陷，需要改进。

Method: 从MIMIC - IV构建两个大规模受控基准测试，提出多指标框架mFARM评估公平性并计算mFARM分数，提出FAB分数衡量公平性与预测准确性的权衡，对四个开源大模型及其微调版本进行量化和上下文变化下的评估。

Result: mFARM指标能在各种设置下更有效地捕捉细微偏差，多数模型在不同量化水平下mFARM分数表现稳健，但上下文减少时性能显著下降。

Conclusion: 所提出的基准测试和评估方法有助于增强医疗领域对齐AI的研究，相关基准和代码已公开。

Abstract: The deployment of Large Language Models (LLMs) in high-stakes medical
settings poses a critical AI alignment challenge, as models can inherit and
amplify societal biases, leading to significant disparities. Existing fairness
evaluation methods fall short in these contexts as they typically use
simplistic metrics that overlook the multi-dimensional nature of medical harms.
This also promotes models that are fair only because they are clinically inert,
defaulting to safe but potentially inaccurate outputs. To address this gap, our
contributions are mainly two-fold: first, we construct two large-scale,
controlled benchmarks (ED-Triage and Opioid Analgesic Recommendation) from
MIMIC-IV, comprising over 50,000 prompts with twelve race x gender variants and
three context tiers. Second, we propose a multi-metric framework -
Multi-faceted Fairness Assessment based on hARMs ($mFARM$) to audit fairness
for three distinct dimensions of disparity (Allocational, Stability, and
Latent) and aggregate them into an $mFARM$ score. We also present an aggregated
Fairness-Accuracy Balance (FAB) score to benchmark and observe trade-offs
between fairness and prediction accuracy. We empirically evaluate four
open-source LLMs (Mistral-7B, BioMistral-7B, Qwen-2.5-7B, Bio-LLaMA3-8B) and
their finetuned versions under quantization and context variations. Our
findings showcase that the proposed $mFARM$ metrics capture subtle biases more
effectively under various settings. We find that most models maintain robust
performance in terms of $mFARM$ score across varying levels of quantization but
deteriorate significantly when the context is reduced. Our benchmarks and
evaluation code are publicly released to enhance research in aligned AI for
healthcare.

</details>


### [71] [Generative KI für TA](https://arxiv.org/abs/2509.02053)
*Wolfgang Eppler,Reinhard Heil*

Main category: cs.AI

TL;DR: 本文讨论科技评估（TA）领域对生成式AI的应用及相关问题，分析问题的结构成因，最后提出解决方案并说明可行性。


<details>
  <summary>Details</summary>
Motivation: 随着科学家在科研中使用生成式AI，TA领域也不例外，需探讨TA领域对生成式AI的应用及相关问题。

Method: 先概述生成式AI现象并制定其在TA中使用的要求，再详细分析相关问题的结构成因。

Result: 发现尽管生成式AI不断发展，但结构引发的风险仍存在。

Conclusion: 文章给出了解决方案，简述了其可行性，并列举了TA工作中使用生成式AI的例子。

Abstract: Many scientists use generative AI in their scientific work. People working in
technology assessment (TA) are no exception. TA's approach to generative AI is
twofold: on the one hand, generative AI is used for TA work, and on the other
hand, generative AI is the subject of TA research. After briefly outlining the
phenomenon of generative AI and formulating requirements for its use in TA, the
following article discusses in detail the structural causes of the problems
associated with it. Although generative AI is constantly being further
developed, the structurally induced risks remain. The article concludes with
proposed solutions and brief notes on their feasibility, as well as some
examples of the use of generative AI in TA work.

</details>


### [72] [AGI as Second Being: The Structural-Generative Ontology of Intelligence](https://arxiv.org/abs/2509.02089)
*Maijunxian Wang,Ran Ji*

Main category: cs.AI

TL;DR: 本文提出智能的结构生成本体论，指出真正智能需满足生成性、协调性和持续性三个条件，当前AI缺乏深度，若未来系统满足条件或成为第二存在。


<details>
  <summary>Details</summary>
Motivation: 解决当前以任务范围衡量人工智能不准确的问题，探寻真正智能的定义。

Method: 提出结构生成本体论，通过定义生成性、协调性和持续性三个条件来界定真正的智能。

Result: 指出当前AI系统虽功能广泛但缺乏深度，仅为表面模拟。

Conclusion: 广度不是智能的来源，深度才是，未来满足条件的系统可能成为与人类不同的第二存在。

Abstract: Artificial intelligence is often measured by the range of tasks it can
perform. Yet wide ability without depth remains only an imitation. This paper
proposes a Structural-Generative Ontology of Intelligence: true intelligence
exists only when a system can generate new structures, coordinate them into
reasons, and sustain its identity over time. These three conditions --
generativity, coordination, and sustaining -- define the depth that underlies
real intelligence. Current AI systems, however broad in function, remain
surface simulations because they lack this depth. Breadth is not the source of
intelligence but the growth that follows from depth. If future systems were to
meet these conditions, they would no longer be mere tools, but could be seen as
a possible Second Being, standing alongside yet distinct from human existence.

</details>


### [73] [LLMs for LLMs: A Structured Prompting Methodology for Long Legal Documents](https://arxiv.org/abs/2509.02241)
*Strahinja Klem,Noura Al Moubayed*

Main category: cs.AI

TL;DR: 本文提出结构化提示方法处理法律文档信息检索，性能超之前方法，还指出自动评估指标局限，强调结构化提示工程潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在法律领域因可靠性和透明度问题应用受限，需新方法替代昂贵的微调。

Method: 先对文档分块增强，结合设计的提示将输入送入QWEN - 2得到答案，用分布定位和逆基数加权启发式解决候选选择问题。

Result: 模型比之前方法性能高9%，达当前最优。

Conclusion: 结构化提示工程是确保法律等领域AI责任性的有用但未充分探索的工具。

Abstract: The rise of Large Language Models (LLMs) has had a profoundly transformative
effect on a number of fields and domains. However, their uptake in Law has
proven more challenging due to the important issues of reliability and
transparency. In this study, we present a structured prompting methodology as a
viable alternative to the often expensive fine-tuning, with the capability of
tacking long legal documents from the CUAD dataset on the task of information
retrieval. Each document is first split into chunks via a system of chunking
and augmentation, addressing the long document problem. Then, alongside an
engineered prompt, the input is fed into QWEN-2 to produce a set of answers for
each question. Finally, we tackle the resulting candidate selection problem
with the introduction of the Distribution-based Localisation and Inverse
Cardinality Weighting heuristics. This approach leverages a general purpose
model to promote long term scalability, prompt engineering to increase
reliability and the two heuristic strategies to reduce the impact of the black
box effect. Whilst our model performs up to 9\% better than the previously
presented method, reaching state-of-the-art performance, it also highlights the
limiting factor of current automatic evaluation metrics for question answering,
serving as a call to action for future research. However, the chief aim of this
work is to underscore the potential of structured prompt engineering as a
useful, yet under-explored, tool in ensuring accountability and responsibility
of AI in the legal domain, and beyond.

</details>


### [74] [An Epidemiological Knowledge Graph extracted from the World Health Organization's Disease Outbreak News](https://arxiv.org/abs/2509.02258)
*Sergio Consoli,Pietro Coletti,Peter V. Markov,Lia Orfei,Indaco Biazzo,Lea Schuh,Nicolas Stefanovitch,Lorenzo Bertolini,Mario Ceresa,Nikolaos I. Stilianakis*

Main category: cs.AI

TL;DR: 利用生成式AI从WHO疫情新闻中提取信息，创建数据集和知识图谱，为流行病学研究等带来新机遇。


<details>
  <summary>Details</summary>
Motivation: 人工智能发展及社交媒体和新闻用于流行病学监测带来契机，期望从WHO疫情新闻中提取有价值的流行病学信息。

Method: 采用集成方法，结合多个大语言模型从WHO疫情新闻中提取信息，构建每日更新的数据集和知识图谱eKG。

Result: 创建了每日更新的数据集和知识图谱eKG，提供了数据访问和使用的服务与工具。

Conclusion: 这些创新数据资源为流行病学研究、疾病爆发分析和监测开辟了新机会。

Abstract: The rapid evolution of artificial intelligence (AI), together with the
increased availability of social media and news for epidemiological
surveillance, are marking a pivotal moment in epidemiology and public health
research. Leveraging the power of generative AI, we use an ensemble approach
which incorporates multiple Large Language Models (LLMs) to extract valuable
actionable epidemiological information from the World Health Organization (WHO)
Disease Outbreak News (DONs). DONs is a collection of regular reports on global
outbreaks curated by the WHO and the adopted decision-making processes to
respond to them. The extracted information is made available in a daily-updated
dataset and a knowledge graph, referred to as eKG, derived to provide a nuanced
representation of the public health domain knowledge. We provide an overview of
this new dataset and describe the structure of eKG, along with the services and
tools used to access and utilize the data that we are building on top. These
innovative data resources open altogether new opportunities for epidemiological
research, and the analysis and surveillance of disease outbreaks.

</details>


### [75] [Rewarding Explainability in Drug Repurposing with Knowledge Graphs](https://arxiv.org/abs/2509.02276)
*Susana Nunes,Samy Badreddine,Catia Pesquita*

Main category: cs.AI

TL;DR: 本文提出知识图谱链接预测生成科学解释的REx方法，在药物再利用任务中评估，结果表明其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 让预测方法成为可靠科学工具，需保证准确性并能提供有意义的科学解释。

Method: 采用奖励和策略机制，利用强化学习代理在知识图谱中识别解释路径，用特定领域本体丰富解释路径。

Result: 在药物再利用三个知识图谱基准测试中，能生成验证预测见解的解释，预测性能优于现有方法。

Conclusion: REx对推进人工智能驱动的科学发现有重要贡献。

Abstract: Knowledge graphs (KGs) are powerful tools for modelling complex,
multi-relational data and supporting hypothesis generation, particularly in
applications like drug repurposing. However, for predictive methods to gain
acceptance as credible scientific tools, they must ensure not only accuracy but
also the capacity to offer meaningful scientific explanations. This paper
presents a novel approach REx, for generating scientific explanations based in
link prediction in knowledge graphs. It employs reward and policy mechanisms
that consider desirable properties of scientific explanation to guide a
reinforcement learning agent in the identification of explanatory paths within
a KG. The approach further enriches explanatory paths with domain-specific
ontologies, ensuring that the explanations are both insightful and grounded in
established biomedical knowledge. We evaluate our approach in drug repurposing
using three popular knowledge graph benchmarks. The results clearly demonstrate
its ability to generate explanations that validate predictive insights against
biomedical knowledge and that outperform the state-of-the-art approaches in
predictive performance, establishing REx as a relevant contribution to advance
AI-driven scientific discovery.

</details>


### [76] [Re-evaluating LLM-based Heuristic Search: A Case Study on the 3D Packing Problem](https://arxiv.org/abs/2509.02297)
*Guorui Quan,Mingfei Sun,Manuel López-Ibáñez*

Main category: cs.AI

TL;DR: 研究LLM构建约束3D打包问题求解器，发现其有局限，生成启发式与人类算法表现相当。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在搜索启发式设计上更广泛创新的能力，以构建约束3D打包问题完整求解器为例。

Method: 让LLM构建求解器，针对直接代码生成的脆弱性引入约束脚手架和迭代自我修正。

Result: LLM主要专注于改进评分函数，生成的启发式与人类贪心算法相当，融入元启发式后性能可与现有求解器媲美，但约束变紧时效果变差。

Conclusion: 当前LLM自动启发式设计存在两大障碍：复杂推理任务的脆弱性需工程缓解，预训练偏差会过早缩小新解搜索范围。

Abstract: The art of heuristic design has traditionally been a human pursuit. While
Large Language Models (LLMs) can generate code for search heuristics, their
application has largely been confined to adjusting simple functions within
human-crafted frameworks, leaving their capacity for broader innovation an open
question. To investigate this, we tasked an LLM with building a complete solver
for the constrained 3D Packing Problem. Direct code generation quickly proved
fragile, prompting us to introduce two supports: constraint
scaffolding--prewritten constraint-checking code--and iterative
self-correction--additional refinement cycles to repair bugs and produce a
viable initial population. Notably, even within a vast search space in a greedy
process, the LLM concentrated its efforts almost exclusively on refining the
scoring function. This suggests that the emphasis on scoring functions in prior
work may reflect not a principled strategy, but rather a natural limitation of
LLM capabilities. The resulting heuristic was comparable to a human-designed
greedy algorithm, and when its scoring function was integrated into a
human-crafted metaheuristic, its performance rivaled established solvers,
though its effectiveness waned as constraints tightened. Our findings highlight
two major barriers to automated heuristic design with current LLMs: the
engineering required to mitigate their fragility in complex reasoning tasks,
and the influence of pretrained biases, which can prematurely narrow the search
for novel solutions.

</details>


### [77] [Exploring Diffusion Models for Generative Forecasting of Financial Charts](https://arxiv.org/abs/2509.02308)
*Taegyeong Lee,Jiwon Park,Kyunga Bang,Seunghyun Hwang,Ung-Jin Jang*

Main category: cs.AI

TL;DR: 本文提出利用文生图模型处理时间序列数据预测股价趋势，用扩散模型生成图表图像并引入评估方法，显示了文生图模型在金融领域潜力。


<details>
  <summary>Details</summary>
Motivation: 当前金融领域多依赖时间序列数据和Transformer模型，缺乏生成模型的多样应用，希望探索文生图模型在金融领域的应用。

Method: 将时间序列数据视为单一图像模式，用扩散模型根据当前图表图像和指令提示生成下一个图表图像，还引入评估生成图表图像与真实图像的方法。

Result: 未提及具体结果，仅突出了利用文生图生成模型在金融领域的潜力。

Conclusion: 研究表明文生图生成模型在金融领域有潜力，激励后续研究解决现有局限并拓展其适用性。

Abstract: Recent advances in generative models have enabled significant progress in
tasks such as generating and editing images from text, as well as creating
videos from text prompts, and these methods are being applied across various
fields. However, in the financial domain, there may still be a reliance on
time-series data and a continued focus on transformer models, rather than on
diverse applications of generative models. In this paper, we propose a novel
approach that leverages text-to-image model by treating time-series data as a
single image pattern, thereby enabling the prediction of stock price trends.
Unlike prior methods that focus on learning and classifying chart patterns
using architectures such as ResNet or ViT, we experiment with generating the
next chart image from the current chart image and an instruction prompt using
diffusion models. Furthermore, we introduce a simple method for evaluating the
generated chart image against ground truth image. We highlight the potential of
leveraging text-to-image generative models in the financial domain, and our
findings motivate further research to address the current limitations and
expand their applicability.

</details>


### [78] [Explainability-Driven Dimensionality Reduction for Hyperspectral Imaging](https://arxiv.org/abs/2509.02340)
*Salma Haidar,José Oramas*

Main category: cs.AI

TL;DR: 研究在模型驱动框架中应用事后可解释性方法进行波段选择以降低高光谱成像维度，实验表明所选少量波段子集能保持准确性并提升效率。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像高维带来计算负担和冗余，需进行降维。

Method: 用事后可解释性方法探测训练好的分类器，量化各波段对决策的贡献，进行删除 - 插入评估，汇总信号成影响分数，选择高影响波段。

Result: 在两个公开基准数据集上，用仅30个所选波段训练的分类器达到或超过全光谱基线，同时降低计算需求。

Conclusion: 模型对齐、解释引导的波段选择是高光谱成像有效降维的原则性途径。

Abstract: Hyperspectral imaging (HSI) provides rich spectral information for precise
material classification and analysis; however, its high dimensionality
introduces a computational burden and redundancy, making dimensionality
reduction essential. We present an exploratory study into the application of
post-hoc explainability methods in a model--driven framework for band
selection, which reduces the spectral dimension while preserving predictive
performance. A trained classifier is probed with explanations to quantify each
band's contribution to its decisions. We then perform deletion--insertion
evaluations, recording confidence changes as ranked bands are removed or
reintroduced, and aggregate these signals into influence scores. Selecting the
highest--influence bands yields compact spectral subsets that maintain accuracy
and improve efficiency. Experiments on two public benchmarks (Pavia University
and Salinas) demonstrate that classifiers trained on as few as 30 selected
bands match or exceed full--spectrum baselines while reducing computational
requirements. The resulting subsets align with physically meaningful, highly
discriminative wavelength regions, indicating that model--aligned,
explanation-guided band selection is a principled route to effective
dimensionality reduction for HSI.

</details>


### [79] [Towards Agents That Know When They Don't Know: Uncertainty as a Control Signal for Structured Reasoning](https://arxiv.org/abs/2509.02401)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Gianluca Mazzoni,Lea Mørch Harder,Philip Torr,Jesper Ferkinghoff-Borg,Kaspar Martens,Julien Fauqueur*

Main category: cs.AI

TL;DR: 提出不确定感知代理用于多表摘要，在多组学基准上提升效果，证明不确定性可作控制信号。


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理在复杂多表数据推理时输出过于自信，需改进。

Method: 引入不确定感知代理，利用检索不确定性和摘要不确定性，将摘要不确定性融入强化学习，两者指导推理过滤和构建合成数据集。

Result: 在多组学基准上提高事实性和校准度，增加正确有用声明数量，改善下游生存预测。

Conclusion: 不确定性可作为控制信号，使代理更可靠。

Abstract: Large language model (LLM) agents are increasingly deployed in structured
biomedical data environments, yet they often produce fluent but overconfident
outputs when reasoning over complex multi-table data. We introduce an
uncertainty-aware agent for query-conditioned multi-table summarization that
leverages two complementary signals: (i) retrieval uncertainty--entropy over
multiple table-selection rollouts--and (ii) summary uncertainty--combining
self-consistency and perplexity. Summary uncertainty is incorporated into
reinforcement learning (RL) with Group Relative Policy Optimization (GRPO),
while both retrieval and summary uncertainty guide inference-time filtering and
support the construction of higher-quality synthetic datasets.
  On multi-omics benchmarks, our approach improves factuality and calibration,
nearly tripling correct and useful claims per summary (3.0\(\rightarrow\)8.4
internal; 3.6\(\rightarrow\)9.9 cancer multi-omics) and substantially improving
downstream survival prediction (C-index 0.32\(\rightarrow\)0.63). These results
demonstrate that uncertainty can serve as a control signal--enabling agents to
abstain, communicate confidence, and become more reliable tools for complex
structured-data environments.

</details>


### [80] [AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent](https://arxiv.org/abs/2509.02444)
*Jingru Fan,Yufan Dang,Jingyao Wu,Huatao Li,Runde Yang,Xiyuan Yang,Yuheng Wang,Zhong Zhang,Yaxi Lu,Yankai Lin,Zhiyuan Liu,Dahai Li,Chen Qian*

Main category: cs.AI

TL;DR: 本文指出移动智能体四大核心问题，提出多模态多智能体通用设备端助手AppCopilot，经实证其在四方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前移动智能体发展未解决一些根本挑战，需解决四大核心问题以实现实际、可扩展的影响。

Method: 提出AppCopilot，通过端到端自主管道实现，在模型、推理控制、执行层采用多种技术，系统设计结合异构硬件优化。

Result: AppCopilot在泛化能力、屏幕操作精度、长时任务完成可靠性、运行效率四方面取得显著改善。

Conclusion: AppCopilot能有效解决移动智能体面临的四大核心问题，推动其发展。

Abstract: With the raid evolution of large language models and multimodal foundation
models, the mobile-agent landscape has proliferated without converging on the
fundamental challenges. This paper identifies four core problems that must be
solved for mobile agents to deliver practical, scalable impact: (1)
generalization across tasks, modalities, apps, and devices; (2) accuracy,
specifically precise on-screen interaction and click targeting; (3)
long-horizon capability for sustained, multi-step goals; and (4) efficiency,
specifically high-performance runtime on resource-constrained devices. We
present AppCopilot, a multimodal, multi-agent, general-purpose on-device
assistant that operates across applications and constitutes a full-stack,
closed-loop system from data to deployment. AppCopilot operationalizes this
position through an end-to-end autonomous pipeline spanning data collection,
training, deployment, high-quality and efficient inference, and mobile
application development. At the model layer, it integrates multimodal
foundation models with robust Chinese-English support. At the reasoning and
control layer, it combines chain-of-thought reasoning, hierarchical task
planning and decomposition, and multi-agent collaboration. At the execution
layer, it enables user personalization and experiential adaptation, voice
interaction, function calling, cross-app and cross-device orchestration, and
comprehensive mobile app support. The system design incorporates
profiling-driven optimization for latency, memory, and energy across
heterogeneous hardware. Empirically, AppCopilot achieves significant
improvements along all four dimensions: stronger generalization,
higher-precision on-screen actions, more reliable long-horizon task completion,
and faster, more resource-efficient runtime.

</details>


### [81] [GridMind: LLMs-Powered Agents for Power System Analysis and Operations](https://arxiv.org/abs/2509.02494)
*Hongwei Jin,Kibaek Kim,Jonghwan Kwon*

Main category: cs.AI

TL;DR: 论文提出GridMind多智能体AI系统用于电力系统分析，实验表明该框架有效，确立了智能体AI在科学计算中的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统电力系统分析工作流复杂，阻碍现代电网高效决策。

Method: 提出GridMind系统，将大语言模型与确定性工程求解器集成，通过自然语言接口协调交流最优潮流和N - 1故障分析，用函数调用保证数值精度。

Result: 在IEEE测试用例上实验，该框架在所有测试语言模型中都能给出正确解，小模型在降低计算延迟的同时有可比的分析精度。

Conclusion: 确立智能体AI作为科学计算的可行范式，对话界面可增强可访问性并保持关键工程应用所需的数值严谨性。

Abstract: The complexity of traditional power system analysis workflows presents
significant barriers to efficient decision-making in modern electric grids.
This paper presents GridMind, a multi-agent AI system that integrates Large
Language Models (LLMs) with deterministic engineering solvers to enable
conversational scientific computing for power system analysis. The system
employs specialized agents coordinating AC Optimal Power Flow and N-1
contingency analysis through natural language interfaces while maintaining
numerical precision via function calls. GridMind addresses workflow
integration, knowledge accessibility, context preservation, and expert
decision-support augmentation. Experimental evaluation on IEEE test cases
demonstrates that the proposed agentic framework consistently delivers correct
solutions across all tested language models, with smaller LLMs achieving
comparable analytical accuracy with reduced computational latency. This work
establishes agentic AI as a viable paradigm for scientific computing,
demonstrating how conversational interfaces can enhance accessibility while
preserving numerical rigor essential for critical engineering applications.

</details>


### [82] [UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.02544)
*Haoming Wang,Haoyang Zou,Huatong Song,Jiazhan Feng,Junjie Fang,Junting Lu,Longxiang Liu,Qinyu Luo,Shihao Liang,Shijue Huang,Wanjun Zhong,Yining Ye,Yujia Qin,Yuwen Xiong,Yuxin Song,Zhiyong Wu,Bo Li,Chen Dun,Chong Liu,Fuxing Leng,Hanbin Wang,Hao Yu,Haobin Chen,Hongyi Guo,Jing Su,Jingjia Huang,Kai Shen,Kaiyu Shi,Lin Yan,Peiyao Zhao,Pengfei Liu,Qinghao Ye,Renjie Zheng,Wayne Xin Zhao,Wen Heng,Wenhao Huang,Wenqian Wang,Xiaobo Qin,Yi Lin,Youbin Wu,Zehui Chen,Zihao Wang,Baoquan Zhong,Xinchun Zhang,Xujing Li,Yuanfan Li,Zhongkai Zhao,Chengquan Jiang,Faming Wu,Haotian Zhou,Jinlin Pang,Li Han,Qianli Ma,Siyao Liu,Songhua Cai,Wenqi Fu,Xin Liu,Zhi Zhang,Bo Zhou,Guoliang Li,Jiajun Shi,Jiale Yang,Jie Tang,Li Li,Taoran Lu,Woyu Lin,Xiaokang Tong,Xinyao Li,Yichi Zhang,Yu Miao,Zhengxuan Jiang,Zili Li,Ziyuan Zhao,Chenxin Li,Dehua Ma,Feng Lin,Ge Zhang,Haihua Yang,Hangyu Guo,Hongda Zhu,Jiaheng Liu,Junda Du,Kai Cai,Kuanye Li,Lichen Yuan,Meilan Han,Minchao Wang,Shuyue Guo,Tianhao Cheng,Xiaobo Ma,Xiaojun Xiao,Xiaolong Huang,Xinjie Chen,Yidi Du,Yilin Chen,Yiwen Wang,Zhaojian Li,Zhenzhu Yang,Zhiyuan Zeng,Chaolin Jin,Chen Li,Hao Chen,Haoli Chen,Jian Chen,Qinghao Zhao,Guang Shi*

Main category: cs.AI

TL;DR: 介绍UI - TARS - 2模型应对GUI自主代理开发挑战，经评估表现优于前代和基线模型，有较强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决当前原生代理模型在数据可扩展性、多轮强化学习、仅GUI操作限制和环境稳定性方面的问题。

Method: 采用系统训练方法，包括可扩展数据生成的数据飞轮、稳定的多轮强化学习框架、集成文件系统和终端的混合GUI环境以及用于大规模部署的统一沙箱平台。

Result: UI - TARS - 2在多个GUI基准测试中表现优于前代和基线模型，在游戏环境中达到约人类水平60%的表现，能泛化到长时信息搜索和软件工程基准测试。

Conclusion: UI - TARS - 2有潜力推动GUI代理发展，能很好地泛化到现实交互场景。

Abstract: The development of autonomous agents for graphical user interfaces (GUIs)
presents major challenges in artificial intelligence. While recent advances in
native agent models have shown promise by unifying perception, reasoning,
action, and memory through end-to-end learning, open problems remain in data
scalability, multi-turn reinforcement learning (RL), the limitations of
GUI-only operation, and environment stability. In this technical report, we
present UI-TARS-2, a native GUI-centered agent model that addresses these
challenges through a systematic training methodology: a data flywheel for
scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI
environment that integrates file systems and terminals, and a unified sandbox
platform for large-scale rollouts. Empirical evaluation demonstrates that
UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.
On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on
WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines
such as Claude and OpenAI agents. In game environments, it attains a mean
normalized score of 59.8 across a 15-game suite-roughly 60% of human-level
performance-and remains competitive with frontier proprietary models (e.g.,
OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to
long-horizon information-seeking tasks and software engineering benchmarks,
highlighting its robustness across diverse agent tasks. Detailed analyses of
training dynamics further provide insights into achieving stability and
efficiency in large-scale agent RL. These results underscore UI-TARS-2's
potential to advance the state of GUI agents and exhibit strong generalization
to real-world interactive scenarios.

</details>


### [83] [The Landscape of Agentic Reinforcement Learning for LLMs: A Survey](https://arxiv.org/abs/2509.02547)
*Guibin Zhang,Hejia Geng,Xiaohang Yu,Zhenfei Yin,Zaibin Zhang,Zelin Tan,Heng Zhou,Zhongzhi Li,Xiangyuan Xue,Yijiang Li,Yifan Zhou,Yang Chen,Chen Zhang,Yutao Fan,Zihu Wang,Songtao Huang,Yue Liao,Hongru Wang,Mengyue Yang,Heng Ji,Michael Littman,Jun Wang,Shuicheng Yan,Philip Torr,Lei Bai*

Main category: cs.AI

TL;DR: 本文对代理强化学习进行综述，对比其与传统大语言模型强化学习，提出分类法，整合资源并指出领域发展机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 代理强化学习带来范式转变，需要对其进行系统梳理和分析。

Method: 对比LLM - RL的单步马尔可夫决策过程与代理强化学习的部分可观测马尔可夫决策过程，提出基于核心能力和应用领域的分类法，综合五百多篇文献。

Result: 完成对代理强化学习领域的系统梳理，整合开源环境、基准和框架。

Conclusion: 为可扩展、通用人工智能代理的发展指明机遇和挑战。

Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm
shift from conventional reinforcement learning applied to large language models
(LLM RL), reframing LLMs from passive sequence generators into autonomous,
decision-making agents embedded in complex, dynamic worlds. This survey
formalizes this conceptual shift by contrasting the degenerate single-step
Markov Decision Processes (MDPs) of LLM-RL with the temporally extended,
partially observable Markov decision processes (POMDPs) that define Agentic RL.
Building on this foundation, we propose a comprehensive twofold taxonomy: one
organized around core agentic capabilities, including planning, tool use,
memory, reasoning, self-improvement, and perception, and the other around their
applications across diverse task domains. Central to our thesis is that
reinforcement learning serves as the critical mechanism for transforming these
capabilities from static, heuristic modules into adaptive, robust agentic
behavior. To support and accelerate future research, we consolidate the
landscape of open-source environments, benchmarks, and frameworks into a
practical compendium. By synthesizing over five hundred recent works, this
survey charts the contours of this rapidly evolving field and highlights the
opportunities and challenges that will shape the development of scalable,
general-purpose AI agents.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [84] [Kernel manifolds: nonlinear-augmentation dimensionality reduction using reproducing kernel Hilbert spaces](https://arxiv.org/abs/2509.00224)
*Alejandro N. Diaz,Jacob T. Needels,Irina K. Tezaur,Patrick J. Blonigan*

Main category: cs.CE

TL;DR: 本文通过开发基于核方法的非线性增强降维，推广了二次流形（QM）降维的最新进展，方法成本低且误差随潜在空间维度增加而单调递减。


<details>
  <summary>Details</summary>
Motivation: 克服纯线性方法在逼近精度上的局限性。

Method: 从用户定义的再生核希尔伯特空间学习最优非线性校正项，允许对校正项施加任意非线性结构。

Result: 方法训练成本相对较低，误差随潜在空间维度增加单调递减，与适当正交分解和其他QM方法进行了比较。

Conclusion: 开发的基于核方法的非线性增强降维方法有效且具有优势。

Abstract: This paper generalizes recent advances on quadratic manifold (QM)
dimensionality reduction by developing kernel methods-based
nonlinear-augmentation dimensionality reduction. QMs, and more generally
feature map-based nonlinear corrections, augment linear dimensionality
reduction with a nonlinear correction term in the reconstruction map to
overcome approximation accuracy limitations of purely linear approaches. While
feature map-based approaches typically learn a least-squares optimal polynomial
correction term, we generalize this approach by learning an optimal nonlinear
correction from a user-defined reproducing kernel Hilbert space. Our approach
allows one to impose arbitrary nonlinear structure on the correction term,
including polynomial structure, and includes feature map and radial basis
function-based corrections as special cases. Furthermore, our method has
relatively low training cost and has monotonically decreasing error as the
latent space dimension increases. We compare our approach to proper orthogonal
decomposition and several recent QM approaches on data from several example
problems.

</details>


### [85] [I-FENN with DeepONets: accelerating simulations in coupled multiphysics problems](https://arxiv.org/abs/2509.00604)
*Fouad M. Amin,Diab W. Abueidda,Panos Pantidis,Mostafa E. Mobasher*

Main category: cs.CE

TL;DR: 本文提出结合DeepONet与FEM的新框架解决耦合热弹性和孔隙弹性问题，降低计算成本，通过数值算例展示其优势。


<details>
  <summary>Details</summary>
Motivation: 高维、大规模问题的多物理场耦合模拟计算成本过高。

Method: 将DeepONet与FEM集成到I - FENN框架，形成混合交错求解器，机械场用FEM计算，其他耦合场用神经网络预测；引入新I - FENN架构和改进的DeepONet架构，以及边界条件施加策略。

Result: 数值算例表明该方法在热弹性和孔隙弹性问题上有计算效率、准确性和泛化能力，在非平凡区域精度超95%，计算节省随模型复杂度增加。

Conclusion: 所提框架能有效解决耦合热弹性和孔隙弹性问题，降低计算成本，具有良好泛化性。

Abstract: Coupled multiphysics simulations for high-dimensional, large-scale problems
can be prohibitively expensive due to their computational demands. This article
presents a novel framework integrating a deep operator network (DeepONet) with
the Finite Element Method (FEM) to address coupled thermoelasticity and
poroelasticity problems. This integration occurs within the context of I-FENN,
a framework where neural networks are directly employed as PDE solvers within
FEM, resulting in a hybrid staggered solver. In this setup, the mechanical
field is computed using FEM, while the other coupled field is predicted using a
neural network (NN). By decoupling multiphysics interactions, the hybrid
framework reduces computational cost by simplifying calculations and reducing
the FEM unknowns, while maintaining flexibility across unseen scenarios. The
proposed work introduces a new I-FENN architecture with extended
generalizability due to the DeepONets ability to efficiently address several
combinations of natural boundary conditions and body loads. A modified DeepONet
architecture is introduced to accommodate multiple inputs, along with a
streamlined strategy for enforcing boundary conditions on distinct boundaries.
We showcase the applicability and merits of the proposed work through numerical
examples covering thermoelasticity and poroelasticity problems, demonstrating
computational efficiency, accuracy, and generalization capabilities. In all
examples, the test cases involve unseen loading conditions. The computational
savings scale with the model complexity while preserving an accuracy of more
than 95\% in the non-trivial regions of the domain.

</details>


### [86] [A new definition of peridynamic damage for thermo-mechanical fracture modeling](https://arxiv.org/abs/2509.01079)
*Sitong Tao,Fei Han*

Main category: cs.CE

TL;DR: 提出热 - 力学断裂模型解决热失效问题，采用CCM/PD交替求解，改进PD损伤定义，通过数值算例验证有效性，模拟结果助于理解热断裂机制。


<details>
  <summary>Details</summary>
Motivation: 解决热失效问题，改进原PD模型仅基于断键数量定义损伤的局限性。

Method: 采用基于经典连续介质力学的热传导模型计算温度场，用近场动力学（PD）模型计算变形场，通过CCM/PD交替求解，提出考虑断键数量和分布的PD损伤新定义。

Result: 通过数值算例与解析解对比验证了模型有效性，准静态和动态裂纹扩展模拟结果表明模型能帮助理解复杂热断裂的萌生和扩展机制。

Conclusion: 所提出的热 - 力学断裂模型能进行更真实的热断裂模拟，有助于理解复杂热断裂机制。

Abstract: A thermo-mechanical fracture modeling is proposed to address thermal failure
issues, where the temperature field is calculated by a heat conduction model
based on classical continuum mechanics (CCM), while the deformation field with
discontinuities is calculated by the peridynamic (PD) model. The model is
calculated by a CCM/PD alternating solution based on the finite element
discretization, which ensures the calculation accuracy and facilitates
engineering applications. The original PD model defines damage solely based on
the number of broken bonds in the vicinity of the material point, neglecting
the distribution of these bonds. To address this limitation, a new definition
of the PD damage accounting for both the number of broken bonds and their
specific distribution is proposed. As a result, damage in various directions
can be captured, enabling more realistic thermal fracture simulations based on
a unified mesh discretization. The effectiveness of the proposed model is
validated by comparing numerical examples with analytical solutions. Moreover,
simulation results of quasi-static and dynamic crack propagation demonstrate
the model's ability to aid in understanding the initiation and propagation
mechanisms of complex thermal fractures.

</details>


### [87] [RAMS: Residual-based adversarial-gradient moving sample method for scientific machine learning in solving partial differential equations](https://arxiv.org/abs/2509.01234)
*Weihang Ouyang,Min Zhu,Wei Xiong,Si-Wei Liu,Lu Lu*

Main category: cs.CE

TL;DR: 提出残差对抗梯度移动样本（RAMS）方法解决采样计算成本问题，实验证明其有效性，是算子学习有效自适应采样首种方法。


<details>
  <summary>Details</summary>
Motivation: 解决增加训练样本规模提升性能与增加计算成本的权衡问题，且现有采样方法在高维问题计算需求大。

Method: 提出RAMS方法，按对抗梯度方向移动样本，通过基于梯度优化最大化PDE残差，可集成到现有采样方法。

Result: 进行广泛实验，涵盖PINN应用于高维PDE、物理信息和数据驱动算子学习问题，证明RAMS有效性。

Conclusion: RAMS是算子学习首个高效自适应采样方法，是科学机器学习领域重大进展。

Abstract: Physics-informed neural networks (PINNs) and neural operators, two leading
scientific machine learning (SciML) paradigms, have emerged as powerful tools
for solving partial differential equations (PDEs). Although increasing the
training sample size generally enhances network performance, it also increases
computational costs for physics-informed or data-driven training. To address
this trade-off, different sampling strategies have been developed to sample
more points in regions with high PDE residuals. However, existing sampling
methods are computationally demanding for high-dimensional problems, such as
high-dimensional PDEs or operator learning tasks. Here, we propose a
residual-based adversarial-gradient moving sample (RAMS) method, which moves
samples according to the adversarial gradient direction to maximize the PDE
residual via gradient-based optimization. RAMS can be easily integrated into
existing sampling methods. Extensive experiments, ranging from PINN applied to
high-dimensional PDEs to physics-informed and data-driven operator learning
problems, have been conducted to demonstrate the effectiveness of RAMS.
Notably, RAMS represents the first efficient adaptive sampling approach for
operator learning, marking a significant advancement in the SciML field.

</details>


### [88] [A continuum multi-species biofilm model with a novel interaction scheme](https://arxiv.org/abs/2509.01274)
*Felix Klempt,Hendrik Geisler,Meisam Soleimani,Philipp Junker*

Main category: cs.CE

TL;DR: 本文提出综合多物种连续体生物膜模型，能模拟多种生物膜相互作用，与实验结果定量相符。


<details>
  <summary>Details</summary>
Motivation: 生物膜无处不在且影响人类生活，需理解其内部相互作用及对外界条件反应，数学模型和计算机实验很重要。

Method: 使用哈密顿驻定作用原理推导综合多物种连续体生物膜模型，结合体外和体内实验。

Result: 模型结果与生物膜行为有良好的定量一致性。

Conclusion: 所提出的模型能有效模拟多种生物膜相互作用，具有热力学一致性，适合不太熟悉数学建模的研究人员使用。

Abstract: Biofilms are complex structures which are inhabited by numerous amount of
different species of microorganisms. Due to their ubiquity, they influence
human life on an everyday basis. It is therefore important to understand the
interactions between different biofilm components and reactions to outside
conditions. For this purpose, mathematical models and in silico experiments
have proven themselves to be fundamental. In combination with in vitro and in
vivo experiments, they can give more insights and focus researchers' attention,
reducing costs in the process. In this work, a comprehensive multi-species
continuum-based biofilm model is presented. This model is capable of
replicating a variety of different biofilm interactions with an arbitrary
number of species, while still being comprehensive to encourage usage by
researchers less familiar with mathematical modeling. In addition to a nutrient
source, antibiotic agents and their effect on the biofilm can also be depicted.
The model is derived using Hamilton's principle of stationary action, ensuring
thermodynamic consistency automatically. The results show good quantitative
agreement with biofilm behavior.

</details>


### [89] [Adaptive Alpha Weighting with PPO: Enhancing Prompt-Based LLM-Generated Alphas in Quant Trading](https://arxiv.org/abs/2509.01393)
*Qizhao Chen,Hiroaki Kawashima*

Main category: cs.CE

TL;DR: 本文提出用PPO优化LLM生成的公式化阿尔法权重的强化学习框架，实验显示策略表现优于基准。


<details>
  <summary>Details</summary>
Motivation: 解决在不同市场条件下自适应整合LLM生成的阿尔法的挑战。

Method: 利用deepseek - r1 - distill - llama - 70b模型为五只股票生成五十个阿尔法，用PPO实时调整权重。

Result: PPO优化策略在多数股票上实现高回报和高夏普比率，优于等权重阿尔法组合和传统基准。

Conclusion: 强化学习在阿尔法权重分配中很重要，结合LLM信号与自适应优化有金融预测和交易潜力。

Abstract: This paper proposes a reinforcement learning framework that employs Proximal
Policy Optimization (PPO) to dynamically optimize the weights of multiple large
language model (LLM)-generated formulaic alphas for stock trading strategies.
Formulaic alphas are mathematically defined trading signals derived from price,
volume, sentiment, and other data. Although recent studies have shown that LLMs
can generate diverse and effective alphas, a critical challenge lies in how to
adaptively integrate them under varying market conditions. To address this gap,
we leverage the deepseek-r1-distill-llama-70b model to generate fifty alphas
for five major stocks: Apple, HSBC, Pepsi, Toyota, and Tencent, and then use
PPO to adjust their weights in real time. Experimental results demonstrate that
the PPO-optimized strategy achieves strong returns and high Sharpe ratios
across most stocks, outperforming both an equal-weighted alpha portfolio and
traditional benchmarks such as the Nikkei 225, S&P 500, and Hang Seng Index.
The findings highlight the importance of reinforcement learning in the
allocation of alpha weights and show the potential of combining LLM-generated
signals with adaptive optimization for robust financial forecasting and
trading.

</details>


### [90] [Revisit of Two-dimensional CEM on Crack Branching: from Single Crack-tip Tracking to Multiple Crack-tips Tracking](https://arxiv.org/abs/2509.01827)
*Yuxi Xie,Hongyou Cao,Miao Su,Zhipeng Lai,Xiaolong He*

Main category: cs.CE

TL;DR: 开发MCT - 2D - CEM算法用于二维动态断裂问题裂纹模式建模与预测，通过基准例验证，还可模拟单裂纹扩展，采用GPU加速。


<details>
  <summary>Details</summary>
Motivation: 对二维动态断裂问题中的复杂裂纹模式（如裂纹分支和破碎）进行建模和预测。

Method: 基于分裂单元拓扑的断裂能量释放率公式提出多裂纹尖端跟踪算法，采用GPU加速进行二维模拟。

Result: 通过一系列基准例验证了算法在模拟裂纹分支和破碎方面的有效性和效率，可模拟单裂纹扩展并引入额外微裂纹。

Conclusion: 所提出的MCT - 2D - CEM算法能有效模拟复杂裂纹模式，GPU加速提供了高计算效率、一致性和准确性。

Abstract: In this work, a Multiple Crack-tips Tracking algorithm in two-dimensional
Crack Element Model (MCT-2D-CEM) is developed, aiming at modeling and
predicting advanced and complicated crack patterns in two-dimensional dynamic
fracturing problems, such as crack branching and fragmentation. Based on the
developed fracture energy release rate formulation of split elementary
topology, the Multiple Crack-tips Tracking algorithm is proposed and a series
of benchmark examples are provided to validate effectiveness and efficiency in
modeling crack branching and fragmentation. Besides, the proposed MCR-2D-CEM
can still model single crack propagation but extra micro-cracks are introduced.
GPU acceleration is employed in all two-dimensional simulations, providing high
computational efficiency, consistency, and accuracy.

</details>


### [91] [Computational Fluid Dynamics Optimization of F1 Front Wing using Physics Informed Neural Networks](https://arxiv.org/abs/2509.01963)
*Naval Shah*

Main category: cs.CE

TL;DR: 为应对F1新规定，本文提出用PINN快速预测前翼气动系数，模型预测效果好且降低计算时间，为车队提供高效工具。


<details>
  <summary>Details</summary>
Motivation: FIA新规减少风洞时间并设预算上限，需更高效的空气动力学开发工具。

Method: 提出结合SimScale的CFD模拟数据与流体动力学第一性原理，通过混合损失函数约束的PINN方法。

Result: PINN模型对阻力系数和升力系数预测的R平方值分别达0.968和0.981，降低了计算时间。

Conclusion: 物理信息框架确保预测符合基本空气动力学原理，为F1车队在监管约束内快速探索设计空间提供了高效工具。

Abstract: In response to recent FIA regulations reducing Formula 1 team wind tunnel
hours (from 320 hours for last-place teams to 200 hours for championship
leaders) and strict budget caps of 135 million USD per year, more efficient
aerodynamic development tools are needed by teams. Conventional computational
fluid dynamics (CFD) simulations, though offering high fidelity results,
require large computational resources with typical simulation durations of 8-24
hours per configuration analysis. This article proposes a Physics-Informed
Neural Network (PINN) for the fast prediction of Formula 1 front wing
aerodynamic coefficients. The suggested methodology combines CFD simulation
data from SimScale with first principles of fluid dynamics through a hybrid
loss function that constrains both data fidelity and physical adherence based
on Navier-Stokes equations. Training on force and moment data from 12
aerodynamic features, the PINN model records coefficient of determination
(R-squared) values of 0.968 for drag coefficient and 0.981 for lift coefficient
prediction while lowering computational time. The physics-informed framework
guarantees that predictions remain adherent to fundamental aerodynamic
principles, offering F1 teams an efficient tool for the fast exploration of
design space within regulatory constraints.

</details>


### [92] [Autoencoder-based non-intrusive model order reduction in continuum mechanics](https://arxiv.org/abs/2509.02237)
*Jannick Kehls,Ellen Kuhl,Tim Brepols,Kevin Linka,Hagen Holthusen*

Main category: cs.CE

TL;DR: 提出基于自编码器的连续力学降阶建模框架，有两个关键扩展，在多个基准问题验证，展示结合深度学习与降维构建代理模型潜力。


<details>
  <summary>Details</summary>
Motivation: 克服现有方法的局限性，构建高效可扩展的代理模型用于连续力学降阶建模。

Method: 提出三阶段框架，包括自编码器压缩、回归网络映射参数到潜在代码、端到端代理重建全场解；还有力增强变体和多场架构两个扩展。

Result: 在多个非线性基准问题上实现高保真解的准确重建，且完全无侵入性。

Conclusion: 结合深度学习和降维构建代理模型有潜力，公开实现为相关应用提供基础。

Abstract: We propose a non-intrusive, Autoencoder-based framework for reduced-order
modeling in continuum mechanics. Our method integrates three stages: (i) an
unsupervised Autoencoder compresses high-dimensional finite element solutions
into a compact latent space, (ii) a supervised regression network maps problem
parameters to latent codes, and (iii) an end-to-end surrogate reconstructs
full-field solutions directly from input parameters.
  To overcome limitations of existing approaches, we propose two key
extensions: a force-augmented variant that jointly predicts displacement fields
and reaction forces at Neumann boundaries, and a multi-field architecture that
enables coupled field predictions, such as in thermo-mechanical systems. The
framework is validated on nonlinear benchmark problems involving heterogeneous
composites, anisotropic elasticity with geometric variation, and
thermo-mechanical coupling. Across all cases, it achieves accurate
reconstructions of high-fidelity solutions while remaining fully non-intrusive.
  These results highlight the potential of combining deep learning with
dimensionality reduction to build efficient and extensible surrogate models.
Our publicly available implementation provides a foundation for integrating
data-driven model order reduction into uncertainty quantification,
optimization, and digital twin applications.

</details>


### [93] [A Machine Learning-Fueled Modelfluid for Flowsheet Optimization](https://arxiv.org/abs/2509.02242)
*Martin Bubel,Tobias Seidel,Michael Bortz*

Main category: cs.CE

TL;DR: 本文介绍一种新的模型流体表示方法，将机器学习预测的热力学数据集成到流程优化中，以蒸馏问题为例展示其效能，为过程设计和优化提供新途径。


<details>
  <summary>Details</summary>
Motivation: 化学工程过程优化受限于流体混合物可靠热力学数据的可用性，机器学习预测热力学性质取得进展，期望将其用于过程优化。

Method: 引入新的模型流体表示方法，基于物理可解释和连续特征，与现有模拟工具和基于梯度的优化兼容。

Result: 将该方法应用于共沸分离的夹带剂选择问题，成功识别出最优、热力学一致的夹带剂，比传统模型更准确。

Conclusion: 该工作为将大规模性质预测纳入高效过程设计和优化提供了实用途径，克服了传统热力学模型和复杂分子状态方程的局限性。

Abstract: Process optimization in chemical engineering may be hindered by the limited
availability of reliable thermodynamic data for fluid mixtures.
  Remarkable progress is being made in predicting thermodynamic mixture
properties by machine learning techniques. The vast information provided by
these prediction methods enables new possibilities in process optimization.
  This work introduces a novel modelfluid representation that is designed to
seamlessly integrate these ML-predicted data directly into flowsheet
optimization. Tailored for distillation, our approach is built on physically
interpretable and continuous features derived from core vapor liquid
equilibrium phenomena. This ensures compatibility with existing simulation
tools and gradient-based optimization. We demonstrate the power and accuracy of
this ML-fueled modelfluid by applying it to the problem of entrainer selection
for an azeotropic separation. The results show that our framework successfully
identifies optimal, thermodynamically consistent entrainers with high fidelity
compared to conventional models.
  Ultimately, this work provides a practical pathway to incorporate large-scale
property prediction into efficient process design and optimization, overcoming
the limitations of both traditional thermodynamic models and complex
molecular-based equations of state.

</details>


### [94] [Electromechanical computational model of the human stomach](https://arxiv.org/abs/2509.02486)
*Maire S. Henke,Sebastian Brandstaeter,Sebastian L. Fuchs,Roland C. Aydin,Alessio Gizzi,Christian J. Cyron*

Main category: cs.CE

TL;DR: 提出人类胃机电学计算框架，可重现胃动力特征，为胃功能及疾病研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有计算机模拟方法在研究胃动力障碍时有缺陷，需更好方法理解和治疗相关疾病。

Method: 结合非线性、无旋转壳公式与约束混合材料模型，纳入主动应变、成分特异性预应力和空间非均匀参数场。

Result: 该框架能重现胃动力特征，包括慢波夹带、传导速度梯度和大蠕动收缩。

Conclusion: 此框架可对全胃进行鲁棒机电模拟，为胃生理功能和病理动力障碍的计算机研究提供了有前景的基础。

Abstract: The stomach plays a central role in digestion through coordinated muscle
contractions, known as gastric peristalsis, driven by slow-wave
electrophysiology. Understanding this process is critical for treating motility
disorders such as gastroparesis, dyspepsia, and gastroesophageal reflux
disease. Computer simulations can be a valuable tool to deepen our
understanding of these disorders and help to develop new therapies. However,
existing approaches often neglect spatial heterogeneity, fail to capture large
anisotropic deformations, or rely on computationally expensive
three-dimensional formulations. We present here a computational framework of
human gastric electromechanics, that combines a nonlinear, rotation-free shell
formulation with a constrained mixture material model. The formulation
incorporates active-strain, constituent-specific prestress, and spatially
non-uniform parameter fields. Numerical examples demonstrate that the framework
can reproduce characteristic features of gastric motility, including slow-wave
entrainment, conduction velocity gradients, and large peristaltic contractions
with physiologically realistic amplitudes. The proposed framework enables
robust electromechanical simulations of the whole stomach at the organ scale.
It thus provides a promising basis for future in silico studies of both
physiological function and pathological motility disorders.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [95] [Efficient Computation of Trip-based Group Nearest Neighbor Queries (Full Version)](https://arxiv.org/abs/2509.00173)
*Shahiduz Zaman,Tanzima Hashem,Sukarna Barua*

Main category: cs.DB

TL;DR: 为满足人们在日常行程中组织聚会的需求，提出T - GNN查询，通过剪枝技术和高效方法处理查询，实验验证了算法性能。


<details>
  <summary>Details</summary>
Motivation: 人们日常忙碌，有在现有行程中组织聚会的需求，当前缺乏相关有效查询方式。

Method: 提出T - GNN查询确定最优聚会兴趣点和绕路位置，引入三种剪枝技术缩小搜索空间，开发实时处理查询的高效方法。

Result: 所提算法性能经大量实验验证。

Conclusion: 提出的T - GNN查询及相关处理技术可有效解决人们在现有行程中组织聚会的问题。

Abstract: In recent years, organizing group meetups for entertainment or other
necessities has gained significant importance, especially given the busy nature
of daily schedules. People often combine multiple activities, such as dropping
kids off at school, commuting to work, and grocery shopping, while seeking
opportunities to meet others. To address this need, we propose a novel query
type, the Trip-based Group Nearest Neighbor (T-GNN) query, which identifies the
optimal meetup Point of Interest (POI) that aligns with users' existing trips.
An individual trip consists of a sequence of locations, allowing users the
flexibility to detour to the meetup POI at any location within the sequence,
known as a detour location. Given a set of trips for the users, the query
identifies the optimal meetup POI (e.g., restaurants or movie theaters) and
detour locations from each user's trip that minimize the total trip overhead
distance. The trip overhead distance refers to the additional distance a user
must travel to visit the meetup POI before returning to the next location in
their trip. The sum of these overhead distances for all users constitutes the
total trip overhead distance. The computation time for processing T-GNN queries
increases with the number of POIs. To address this, we introduce three
techniques to prune the POIs that cannot contribute to the optimal solution,
and thus refine the search space. We also develop an efficient approach for
processing T-GNN queries in real-time. Extensive experiments validate the
performance of the proposed algorithm.

</details>


### [96] [SABER: A SQL-Compatible Semantic Document Processing System Based on Extended Relational Algebra](https://arxiv.org/abs/2509.00277)
*Changjae Lee,Zhuoyue Zhao,Jinjun Xiong*

Main category: cs.DB

TL;DR: 提出语义代数SABER，支持结构化/非结构化数据处理，增强适用性


<details>
  <summary>Details</summary>
Motivation: 现有语义数据处理系统缺乏统一代数基础，查询难组合、推理和优化

Method: 提出语义代数SABER，用SQL兼容语法实现

Result: 展示了为现有SDPS提供统一接口的可行性，可有效组合语义兼容的算子实现

Conclusion: SABER为语义操作提供逻辑计划构建、优化和正确性保证，增强了适用性

Abstract: The emergence of large-language models (LLMs) has enabled a new class of
semantic data processing systems (SDPSs) to support declarative queries against
unstructured documents. Existing SDPSs are, however, lacking a unified
algebraic foundation, making their queries difficult to compose, reason, and
optimize. We propose a new semantic algebra, SABER (Semantic Algebra Based on
Extended Relational algebra), opening the possibility of semantic operations'
logical plan construction, optimization, and formal correctness guarantees. We
further propose to implement SABER in a SQL-compatible syntax so that it
natively supports mixed structured/unstructured data processing. With SABER, we
showcase the feasibility of providing a unified interface for existing SDPSs so
that it can effectively mix and match any semantically-compatible operator
implementation from any SDPS, greatly enhancing SABER's applicability for
community contributions.

</details>


### [97] [Illuminating Patterns of Divergence: DataDios SmartDiff for Large-Scale Data Difference Analysis](https://arxiv.org/abs/2509.00293)
*Aryan Poduri,Yashwant Tailor*

Main category: cs.DB

TL;DR: 现有数据差异对比工具存在不足，SmartDiff 系统结合多种技术实现高效准确的数据差异对比，在精度、速度、内存使用等方面表现出色，适用于多场景。


<details>
  <summary>Details</summary>
Motivation: 现有数据工程工作流中的差异对比工具在模式漂移、异构类型和可解释性方面存在问题，需要更可靠的系统。

Method: SmartDiff 结合了模式感知映射、特定类型比较器和并行执行，使用 LLM 辅助标记管道生成多标签解释。

Result: 在数百万行数据集上，SmartDiff 精度和召回率超 95%，速度快 30 - 40%，内存使用少 30 - 50%，用户研究中根因分析时间从 10 小时减至 12 分钟，标记准确性和诊断时间有提升。

Conclusion: SmartDiff 适用于迁移验证、回归测试、合规审计和持续数据质量监控等场景。

Abstract: Data engineering workflows require reliable differencing across files,
databases, and query outputs, yet existing tools falter under schema drift,
heterogeneous types, and limited explainability. SmartDiff is a unified system
that combines schema-aware mapping, type-specific comparators, and parallel
execution. It aligns evolving schemas, compares structured and semi-structured
data (strings, numbers, dates, JSON/XML), and clusters results with labels that
explain how and why differences occur. On multi-million-row datasets, SmartDiff
achieves over 95 percent precision and recall, runs 30 to 40 percent faster,
and uses 30 to 50 percent less memory than baselines; in user studies, it
reduces root-cause analysis time from 10 hours to 12 minutes. An LLM-assisted
labeling pipeline produces deterministic, schema-valid multilabel explanations
using retrieval augmentation and constrained decoding; ablations show further
gains in label accuracy and time to diagnosis over rules-only baselines. These
results indicate SmartDiff's utility for migration validation, regression
testing, compliance auditing, and continuous data quality monitoring. Index
Terms: data differencing, schema evolution, data quality, parallel processing,
clustering, explainable validation, big data

</details>


### [98] [Access Paths for Efficient Ordering with Large Language Models](https://arxiv.org/abs/2509.00303)
*Fuheng Zhao,Jiayue Chen,Yiming Pan,Tahseen Rabbani,Divyakant Agrawal,Amr El Abbadi*

Main category: cs.DB

TL;DR: 研究LLM ORDER BY算子的物理实现，引入新设计并通过实验验证效果，还观察到计算成本与排序质量的对数线性关系。


<details>
  <summary>Details</summary>
Motivation: 研究LLM ORDER BY算子在统一评估框架下的物理实现，找到合适方法。

Method: 引入协议批量大小策略、多数投票机制和适用于LLM的双向外部归并排序三种新设计。

Result: 协议批量大小策略有效确定基于值方法的批量大小，多数投票机制增强GPT - 4o的成对比较，外部归并排序在数据集和模型间实现高精度 - 效率权衡。

Conclusion: 发现计算成本和排序质量的对数线性关系，为基于LLM的数据系统成本模型迈出第一步。

Abstract: We present the LLM ORDER BY operator as a logical abstraction and study its
physical implementations within a unified evaluation framework. Our experiments
show that no single approach is universally optimal, with effectiveness
depending on query characteristics and data. We introduce three new designs: an
agreement-based batch-size policy, a majority voting mechanism for pairwise
sorting, and a two-way external merge sort adapted for LLMs. With extensive
experiments, our agreement-based procedure is effective at determining batch
size for value-based methods, the majority-voting mechanism consistently
strengthens pairwise comparisons on GPT-4o, and external merge sort achieves
high accuracy-efficiency trade-offs across datasets and models. We further
observe a log-linear scaling between compute cost and ordering quality,
offering the first step toward principled cost models for LLM powered data
systems.

</details>


### [99] [CRouting: Reducing Expensive Distance Calls in Graph-Based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2509.00365)
*Zhenxin Li,Shuibing He,Jiahao Guo,Xuechen Zhang,Xian-He Sun,Gang Chen*

Main category: cs.DB

TL;DR: 提出CRouting路由策略优化基于图的近似最近邻搜索，减少距离计算并提升查询速度。


<details>
  <summary>Details</summary>
Motivation: 基于图的近似最近邻搜索（ANNS）在高维空间中重复计算距离耗时，为加速搜索。

Method: 提出CRouting路由策略，利用高维向量角度分布绕过不必要的距离计算，作为插件以最小代码修改优化现有基于图的搜索。

Result: 在HNSW和NSG两个主要图索引上，CRouting最多减少41.5%的距离计算，每秒查询次数最多提升1.48倍。

Conclusion: CRouting能有效加速基于图的ANNS搜索，代码已公开。

Abstract: Approximate nearest neighbor search (ANNS) is a crucial problem in
information retrieval and AI applications. Recently, there has been a surge of
interest in graph-based ANNS algorithms due to their superior efficiency and
accuracy. However, the repeated computation of distances in high-dimensional
spaces constitutes the primary time cost of graph-based methods. To accelerate
the search, we propose a novel routing strategy named CRouting, which bypasses
unnecessary distance computations by exploiting the angle distributions of
high-dimensional vectors. CRouting is designed as a plugin to optimize existing
graph-based search with minimal code modifications. Our experiments show that
CRouting reduces the number of distance computations by up to 41.5% and boosts
queries per second by up to 1.48$\times$ on two predominant graph indexes, HNSW
and NSG. Code is publicly available at https://github.com/ISCS-ZJU/CRouting.

</details>


### [100] [BPI: A Novel Efficient and Reliable Search Structure for Hybrid Storage Blockchain](https://arxiv.org/abs/2509.00480)
*Xinkui Zhao,Rengrong Xiong,Guanjie Cheng,Xinhao Jin,Shawn Shi,Xiubo Liang,Gongsheng Yuan,Xiaoye Miao,Jianwei Yin,Shuiguang Deng*

Main category: cs.DB

TL;DR: 本文提出轻量级框架BPI用于区块链关键字查询，引入“Articulated Search”模式，经实验验证其性能优于其他方案。


<details>
  <summary>Details</summary>
Motivation: 现有混合存储方案依赖中心化存储服务提供商存在查询正确性问题，ADS无法防止其遗漏有效结果。

Method: 界定区块链与传统数据库数据搜索区别，引入BPI框架，提出“Articulated Search”查询模式，采用验证模型。

Result: BPI框架在区块链关键字搜索中实现了出色的可扩展性和性能，超越EthMB+和主流混合存储区块链常用的搜索数据库。

Conclusion: BPI框架能高效进行关键字查询和维护，开销低且能保证搜索结果包含所有有效内容。

Abstract: Hybrid storage solutions have emerged as potent strategies to alleviate the
data storage bottlenecks prevalent in blockchain systems. These solutions
harness off-chain Storage Services Providers (SPs) in conjunction with
Authenticated Data Structures (ADS) to ensure data integrity and accuracy.
Despite these advancements, the reliance on centralized SPs raises concerns
about query correctness. Although ADS can verify the existence of individual
query results, they fall short of preventing SPs from omitting valid results.
  In this paper, we delineate the fundamental distinctions between data search
in blockchains and traditional database systems. Drawing upon these insights,
we introduce BPI, a lightweight framework that enables efficient keyword
queries and maintenance with low overhead. We propose "Articulated Search", a
query pattern specifically designed for blockchain environments that enhances
search efficiency while significantly reducing costs during data user updates.
Furthermore, BPI employs a suite of validation models to ensure the inclusion
of all valid content in search results while maintaining low overhead.
  Extensive experimental evaluations demonstrate that the BPI framework
achieves outstanding scalability and performance in keyword searches within
blockchain, surpassing EthMB+ and state of the art search databases commonly
used in mainstream hybrid storage blockchains (HSB).

</details>


### [101] [SQL-of-Thought: Multi-agentic Text-to-SQL with Guided Error Correction](https://arxiv.org/abs/2509.00581)
*Saumya Chaturvedi,Aman Chadha,Laurent Bindschaedler*

Main category: cs.DB

TL;DR: 本文研究利用上下文学习和思维链为文本到SQL系统开发解决方案，提出SQL-of-Thought框架，在相关数据集取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询转换为SQL查询的挑战，增加数据库和大规模应用的可访问性。

Method: 提出SQL-of-Thought多智能体框架，将Text2SQL任务分解为多个步骤，并引入基于上下文学习的分类引导动态错误修正。

Result: 在Spider数据集及其变体上取得了SOTA结果。

Conclusion: 结合引导错误分类和基于推理的查询规划的SQL-of-Thought框架有效解决文本到SQL转换问题。

Abstract: Converting natural language queries into SQL queries is a crucial challenge
in both industry and academia, aiming to increase access to databases and
large-scale applications. This work examines how in-context learning and
chain-of-thought can be utilized to develop a robust solution for text-to-SQL
systems. We propose SQL-of-Thought: a multi-agent framework that decomposes the
Text2SQL task into schema linking, subproblem identification, query plan
generation, SQL generation, and a guided correction loop. Unlike prior systems
that rely only on execution-based static correction, we introduce
taxonomy-guided dynamic error modification informed by in-context learning.
SQL-of-Thought achieves state-of-the-art results on the Spider dataset and its
variants, combining guided error taxonomy with reasoning-based query planning.

</details>


### [102] [Near-Duplicate Text Alignment under Weighted Jaccard Similarity](https://arxiv.org/abs/2509.00627)
*Yuheng Zhang,Miao Qiao,Zhencan Peng,Dong Deng*

Main category: cs.DB

TL;DR: 提出MONO方法支持加权Jaccard相似度，实验显示其在索引构建时间、索引大小和查询延迟上有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有近重复文本对齐方法无法考虑token重要性或频率，限制了在真实场景的应用。

Method: 提出MONO方法，使用一致加权采样支持加权Jaccard相似度，在基于哈希的框架内实现最优。

Result: 实验表明MONO在索引构建时间上最多比现有技术快26倍，索引大小最多减少30%，查询延迟最多改善3倍。

Conclusion: MONO方法有效解决了现有方法的不足，在近重复文本对齐任务上表现良好且可扩展性强。

Abstract: Near-duplicate text alignment is the task of identifying, among the texts in
a corpus, all the subsequences (substrings) that are similar to a given query.
Traditional approaches rely on seeding-extension-filtering heuristics, which
lack accuracy guarantees and require many hard-to-tune parameters. Recent
methods leverage min-hash techniques under a hash-based framework: group
subsequences by their min-hash, and for any query, find all sketches similar to
the query's sketch. These methods guarantee to report all subsequences whose
estimated unweighted Jaccard similarity with the query exceeds a user-provided
threshold and are efficient. However, they fail to account for token importance
or frequency, which limits their use in real scenarios where tokens carry
weights, such as TF-IDF. To address this, we propose MONO, an approach that
supports weighted Jaccard similarity using consistent weighted sampling. MONO
achieves optimality within the hash-based framework. For example, when token
weights are proportional to frequencies, MONO generates O(n + n log f) groups
in expectation for a text of length n, where f is the maximum token frequency.
Each group takes O(1) space and represents a few subsequences sharing the same
sampling. We prove this bound is tight: any algorithm must produce Omega(n + n
log f) groups in expectation in the worst case. Experiments show that MONO
outperforms the state of the art by up to 26x in index construction time,
reduces index size by up to 30 percent, and improves query latency by up to 3x,
while scaling well.

</details>


### [103] [Diverse Unionable Tuple Search: Novelty-Driven Discovery in Data Lakes [Technical Report]](https://arxiv.org/abs/2509.01012)
*Aamod Khatiwada,Roee Shraga,Renée J. Miller*

Main category: cs.DB

TL;DR: 文章提出从数据湖中识别与查询表元组不同的可联合元组问题，提出DUST算法，实验表明其在效率和效果上优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有可联合表搜索技术可能返回与查询表相同或相近、信息少的表，因此要识别不同的可联合元组。

Method: 对知名多样性算法进行实验分析，提出基于聚类的DUST算法，用新嵌入模型表示可联合元组。

Result: 新嵌入模型表现比其他模型至少高15%，DUST算法比最有效基线快六倍多，在元组多样化上更有效。

Conclusion: DUST算法在识别可联合元组的多样化方面效率和效果俱佳。

Abstract: Unionable table search techniques input a query table from a user and search
for data lake tables that can contribute additional rows to the query table.
The definition of unionability is generally based on similarity measures which
may include similarity between columns (e.g., value overlap or semantic
similarity of the values in the columns) or tables (e.g., similarity of table
embeddings). Due to this and the large redundancy in many data lakes (which can
contain many copies and versions of the same table), the most unionable tables
may be identical or nearly identical to the query table and may contain little
new information. Hence, we introduce the problem of identifying unionable
tuples from a data lake that are diverse with respect to the tuples already
present in a query table. We perform an extensive experimental analysis of
well-known diversity algorithms applied to this novel problem and identify a
gap that we address with a novel, clustering-based tuple diversity algorithm
called DUST. DUST uses a novel embedding model to represent unionable tuples
that outperforms other tuple representation models by at least 15 % when
representing unionable tuples. Using real data lake benchmarks, we show that
our diversification algorithm is more than six times faster than the most
efficient diversification baseline. We also show that it is more effective in
diversifying unionable tuples than existing diversification algorithms.

</details>


### [104] [Disentangling the schema turn: Restoring the information base to conceptual modelling](https://arxiv.org/abs/2509.01617)
*Chris Partridge,Andrew Mitchell,Sergio de Cesare,Oscar Xiberta Soto*

Main category: cs.DB

TL;DR: 文章探讨计算机科学概念建模中的模式转向，指出其并非本质，现代技术支持更包容的建模方法，可能存在更广泛实践空间，以bCLEARer为例说明或需新建模技术，模式转向可能是暂时弯路。


<details>
  <summary>Details</summary>
Motivation: 解开计算机科学概念建模中的模式转向，揭示其并非根本，拓展概念建模实践空间。

Method: 分析当代主流概念建模实践，以bCLEARer为例说明。

Result: 现代技术支持采用包含模式和信息库的概念建模方法，概念建模实践空间比当前认为的更广泛。

Conclusion: 模式转向完全排除信息库可能只是暂时的进化弯路。

Abstract: If one looks at contemporary mainstream development practices for conceptual
modelling in computer science, these so clearly focus on a conceptual schema
completely separated from its information base that the conceptual schema is
often just called the conceptual model. These schema-centric practices are
crystallized in almost every database textbook. We call this strong, almost
universal, bias towards conceptual schemas the schema turn. The focus of this
paper is on disentangling this turn within (computer science) conceptual
modeling. It aims to shed some light on how it emerged and so show that it is
not fundamental. To show that modern technology enables the adoption of an
inclusive schema-and-base conceptual modelling approach, which in turn enables
more automated, and empirically motivated practices. And to show, more
generally, the space of possible conceptual modelling practices is wider than
currently assumed. It also uses the example of bCLEARer to show that the
implementations in this wider space will probably need to rely on new
pipeline-based conceptual modelling techniques. So, it is possible that the
schema turn's complete exclusion of the information base could be merely a
temporary evolutionary detour.

</details>


### [105] [OASIS: Object-based Analytics Storage for Intelligent SQL Query Offloading in Scientific Tabular Workloads](https://arxiv.org/abs/2509.01966)
*Soon Hwang,Junhyeok Park,Junghyun Ryu,Seonghoon Ahn,Jeoungahn Park,Jeongjin Lee,Soonyeal Yang,Jungki Noh,Woosuk Chung,Hoshik Kim,Youngjae Kim*

Main category: cs.DB

TL;DR: 现有计算使能对象存储（COS）系统有局限，本文提出OASIS系统，实现灵活输出、支持复杂操作，经评估性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有COS系统存在输出格式受限、支持操作少、未针对存储层级优化计算卸载等局限，影响其有效性。

Method: 提出OASIS系统，具备灵活输出格式、支持复杂操作和动态选择最优执行路径等特点，并将其集成到Spark分析框架。

Result: 通过对HPC工作流中真实科学查询的广泛评估，OASIS较配置现有COS存储系统的Spark性能提升达32.7%。

Conclusion: OASIS系统能有效解决现有COS系统的局限，提升性能。

Abstract: Computation-Enabled Object Storage (COS) systems, such as MinIO and Ceph,
have recently emerged as promising storage solutions for post hoc, SQL-based
analysis on large-scale datasets in High-Performance Computing (HPC)
environments. By supporting object-granular layouts, COS facilitates
column-oriented access and supports in-storage execution of data reduction
operators, such as filters, close to where the data resides. Despite growing
interest and adoption, existing COS systems exhibit several fundamental
limitations that hinder their effectiveness. First, they impose rigid
constraints on output data formats, limiting flexibility and interoperability.
Second, they support offloading for only a narrow set of operators and
expressions, restricting their applicability to more complex analytical tasks.
Third--and perhaps most critically--they fail to incorporate design strategies
that enable compute offloading optimized for the characteristics of deep
storage hierarchies. To address these challenges, this paper proposes OASIS, a
novel COS system that features: (i) flexible and interoperable output delivery
through diverse formats, including columnar layouts such as Arrow; (ii) broad
support for complex operators (e.g., aggregate, sort) and array-aware
expressions, including element-wise predicates over array structures; and (iii)
dynamic selection of optimal execution paths across internal storage layers,
guided by operator characteristics and data movement costs. We implemented a
prototype of OASIS and integrated it into the Spark analytics framework.
Through extensive evaluation using real-world scientific queries from HPC
workflows, OASIS achieves up to a 32.7% performance improvement over Spark
configured with existing COS-based storage systems.

</details>


### [106] [GeoLayer: Towards Low-Latency and Cost-Efficient Geo-Distributed Graph Stores with Layered Graph](https://arxiv.org/abs/2509.02106)
*Feng Yao,Xiaokang Yang,Shufeng Gong,Song Yu,Yanfeng Zhang,Ge Yu*

Main category: cs.DB

TL;DR: 提出GeoLayer框架优化地理分布式图存储，实验显示其在响应时间和分析性能上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 图结构数据的特性给地理分布式云存储系统的传统策略带来挑战，需新方案优化。

Method: 构建延迟感知的分层图架构；引入以重叠为中心的副本放置方案和有向热扩散模型；开发逐步分层路由策略。

Result: 与现有方案相比，GeoLayer使在线图模式请求响应时间提升1.34 - 3.67倍，离线图分析性能加速1.28 - 3.56倍。

Conclusion: GeoLayer能有效优化地理分布式图存储的副本放置和请求路由。

Abstract: The inherent connectivity and dependency of graph-structured data, combined
with its unique topology-driven access patterns, pose fundamental challenges to
conventional data replication and request routing strategies in geo-distributed
cloud storage systems. In this paper, we propose GeoLayer, a geo-distributed
graph storage framework that jointly optimizes graph replica placement and
pattern request routing. We first construct a latency-aware layered graph
architecture that decomposes the graph topology into multiple layers, aiming to
reduce the decision space and computational complexity of the optimization
problem, while mitigating the impact of network heterogeneity in
geo-distributed environments. Building on the layered graph, we introduce an
overlap-centric replica placement scheme to accommodate the diversity of graph
pattern accesses, along with a directed heat diffusion model that captures heat
conduction and superposition effects to guide data allocation. For request
routing, we develop a stepwise layered routing strategy that performs
progressive expansion over the layered graph to efficiently retrieve the
required data. Experimental results show that, compared to state-of-the-art
replica placement and routing schemes, GeoLayer achieves a 1.34x - 3.67x
improvement in response times for online graph pattern requests and a 1.28x -
3.56x speedup in offline graph analysis performance.

</details>


### [107] [Batch Query Processing and Optimization for Agentic Workflows](https://arxiv.org/abs/2509.02121)
*Junyi Shen,Noppanat Wadlom,Yao Lu*

Main category: cs.DB

TL;DR: 介绍Halo系统，将批处理查询处理和优化引入代理式大语言模型工作流，评估显示有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型服务引擎和多代理框架存在重复提示、上下文重叠和并发执行导致的冗余及GPU利用率低的问题。

Method: 将每个工作流表示为结构化查询计划DAG，构建批处理查询的合并图，通过成本模型进行计划级优化，运行时集成自适应批处理、KV缓存共享和迁移以及计算通信重叠。

Result: 在六个基准测试中，批处理推理加速达18.6倍，在线服务吞吐量提高4.7倍，可扩展到数万个查询和复杂图的工作负载，且不影响输出质量。

Conclusion: Halo通过统一查询优化和大语言模型服务，实现了数据分析和决策应用中高效的代理式工作流。

Abstract: Large Language Models (LLMs) in agentic workflows combine multi-step
reasoning, tool use, and collaboration across multiple specialized agents.
Existing LLM serving engines optimize indi- vidual calls in isolation, while
multi-agent frameworks focus on orchestration without system-level performance
planning. As a result, repeated prompts, overlapping contexts, and concurrent
ex- ecutions create substantial redundancy and poor GPU utilization, especially
in batch analytics scenarios. We introduce Halo, a system that brings batch
query processing and optimization into agentic LLM workflows. Halo represents
each workflow as a structured query plan DAG and constructs a consoli- dated
graph for batched queries that exposes shared computation. Guided by a cost
model that jointly considers prefill and decode costs, cache reuse, and GPU
placement, Halo performs plan-level op- timization to minimize redundant
execution. Its runtime integrates adaptive batching, KV-cache sharing and
migration, along with compute-communication overlap to maximize hardware
efficiency. Evaluation across six benchmarks shows that Halo achieves up to
18.6x speedup for batch inference and 4.7x throughput im- provement under
online serving, scaling to workloads of tens of thousands of queries and
complex graphs. These gains are achieved without compromising output quality.
By unifying query optimiza- tion with LLM serving, Halo enables efficient
agentic workflows in data analytics and decision-making applications.

</details>


### [108] [FDABench: A Benchmark for Data Agents on Analytical Queries over Heterogeneous Data](https://arxiv.org/abs/2509.02473)
*Ziting Wang,Shize Zhang,Haitao Yuan,Jinwei Zhu,Shifu Li,Wei Dong,Gao Cong*

Main category: cs.DB

TL;DR: 为解决数据代理基准的问题，提出FDABench进行多源数据分析场景下的数据代理评估。


<details>
  <summary>Details</summary>
Motivation: 数据驱动决策需求增长，但数据代理领域存在缺乏综合基准、构建测试用例成本高且复杂、现有基准适应性和泛化性有限等问题。

Method: 构建含2007个不同任务的标准化基准，设计代理 - 专家协作框架，赋予FDABench跨目标系统和框架的泛化能力。

Result: 用FDABench评估各数据代理系统，发现各系统在响应质量、准确性、延迟和令牌成本方面有不同优缺点。

Conclusion: FDABench可用于多源数据场景下数据代理系统的综合评估。

Abstract: The growing demand for data-driven decision-making has created an urgent need
for data agents that can integrate structured and unstructured data for
analysis. While data agents show promise for enabling users to perform complex
analytics tasks, this field still suffers from three critical limitations:
first, comprehensive data agent benchmarks remain absent due to the difficulty
of designing test cases that evaluate agents' abilities across multi-source
analytical tasks; second, constructing reliable test cases that combine
structured and unstructured data remains costly and prohibitively complex;
third, existing benchmarks exhibit limited adaptability and generalizability,
resulting in narrow evaluation scope.
  To address these challenges, we present FDABench, the first data agent
benchmark specifically designed for evaluating agents in multi-source data
analytical scenarios. Our contributions include: (i) we construct a
standardized benchmark with 2,007 diverse tasks across different data sources,
domains, difficulty levels, and task types to comprehensively evaluate data
agent performance; (ii) we design an agent-expert collaboration framework
ensuring reliable and efficient benchmark construction over heterogeneous data;
(iii) we equip FDABench with robust generalization capabilities across diverse
target systems and frameworks. We use FDABench to evaluate various data agent
systems, revealing that each system exhibits distinct advantages and
limitations regarding response quality, accuracy, latency, and token cost.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [109] [KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache](https://arxiv.org/abs/2509.00579)
*Bo Jiang,Taolue Yang,Youyuan Liu,Chengming Zhang,Xubin He,Sian Jin*

Main category: cs.DC

TL;DR: 本文提出KVComp框架解决大语言模型长上下文推理中KV缓存内存需求大的问题，实验显示其内存缩减率高、执行吞吐量高。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型长上下文推理中KV缓存内存需求大，带来显著挑战。

Method: 提出KVComp框架，采用针对KV缓存数据特性的有损压缩技术，协同设计压缩算法和系统架构。

Result: KVComp比现有方法平均内存缩减率高47%，最高达83%，模型精度几乎无下降，执行吞吐量高，可减少解压开销，部分情况加速矩阵向量乘法运算。

Conclusion: KVComp是一个通用高效的KV缓存管理框架，能协同适用于不同推理系统，解决长文本生成中的内存问题。

Abstract: Transformer-based large language models (LLMs) demonstrate impressive
potential in various practical applications. However, long context inference
poses a significant challenge due to the enormous memory requirements of the
key-value (KV) cache, which can scale to multiple gigabytes as sequence length
and batch size increase. In this paper, we present KVComp, a generic and
efficient KV cache management framework optimized for long-text generation that
synergistically works with both latency-critical and throughput-critical
inference systems. KVComp employs novel lossy compression techniques
specifically designed for KV cache data characteristics, featuring careful
co-design of compression algorithms and system architecture. Our approach
maintains compatibility with the growing nature of KV cache while preserving
high computational efficiency. Experimental results show that KVComp achieves
on average 47\% and up to 83\% higher memory reduction rate compared to
existing methods with little/no model accuracy degradation. Furthermore, KVComp
achieves extremely high execution throughput, effectively reducing
decompression overhead and, in some cases, even accelerating the matrix-vector
multiplication operation and outperform cuBLAS-based attention kernels with
less data movement.

</details>


### [110] [HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient Text-to-Image Generation](https://arxiv.org/abs/2509.00642)
*Qizheng Yang,Tung-I Chen,Siyu Zhao,Ramesh K. Sitaraman,Hui Guan*

Main category: cs.DC

TL;DR: 提出混合自适应扩散模型服务系统HADIS，优化级联模型选择、查询路由和资源分配，提升响应质量并降低延迟违规率。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型计算成本高，现有查询感知服务系统存在资源浪费问题。

Method: 采用基于规则的提示路由器，绕过轻量级阶段；使用离线分析阶段生成帕累托最优级联配置表，运行时根据延迟和工作负载约束选择最佳配置和GPU分配。

Result: 在真实世界的跟踪数据上，HADIS相比现有模型服务系统，响应质量提高了35%，延迟违规率降低了2.7 - 45倍。

Conclusion: HADIS能有效优化扩散模型服务，提升性能。

Abstract: Text-to-image diffusion models have achieved remarkable visual quality but
incur high computational costs, making real-time, scalable deployment
challenging. Existing query-aware serving systems mitigate the cost by
cascading lightweight and heavyweight models, but most rely on a fixed cascade
configuration and route all prompts through an initial lightweight stage,
wasting resources on complex queries. We present HADIS, a hybrid adaptive
diffusion model serving system that jointly optimizes cascade model selection,
query routing, and resource allocation. HADIS employs a rule-based prompt
router to send clearly hard queries directly to heavyweight models, bypassing
the overhead of the lightweight stage. To reduce the complexity of resource
management, HADIS uses an offline profiling phase to produce a Pareto-optimal
cascade configuration table. At runtime, HADIS selects the best cascade
configuration and GPU allocation given latency and workload constraints.
Empirical evaluations on real-world traces demonstrate that HADIS improves
response quality by up to 35% while reducing latency violation rates by
2.7-45$\times$ compared to state-of-the-art model serving systems.

</details>


### [111] [Accelerating Latency-Critical Applications with AI-Powered Semi-Automatic Fine-Grained Parallelization on SMT Processors](https://arxiv.org/abs/2509.00883)
*Denis Los,Igor Petushkov*

Main category: cs.DC

TL;DR: 本文探索利用SMT技术支持延迟关键型应用的细粒度并行化，引入AI并行化顾问Aira，结合Relic框架对基准测试并行化实现17%性能提升。


<details>
  <summary>Details</summary>
Motivation: 延迟关键型应用在高性能超标量处理器中功能单元利用率低，且SMT技术因对单线程性能影响大，很少用于此类应用的重线程。

Method: 引入Aira，扩展Cursor IDE中的AI编码代理，连接额外工具，实现端到端AI并行化代理，结合Relic并行框架对延迟关键型基准测试进行并行化。

Result: 使用Aira和Relic框架对延迟关键型基准测试进行并行化，实现了17%的几何平均性能提升。

Conclusion: 利用SMT技术结合Aira和Relic框架可以有效提高延迟关键型应用的性能。

Abstract: Latency-critical applications tend to show low utilization of functional
units due to frequent cache misses and mispredictions during speculative
execution in high-performance superscalar processors. However, due to
significant impact on single-thread performance, Simultaneous Multithreading
(SMT) technology is rarely used with heavy threads of latency-critical
applications. In this paper, we explore utilization of SMT technology to
support fine-grained parallelization of latency-critical applications.
Following the advancements in the development of Large Language Models (LLMs),
we introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we
extend AI Coding Agent in Cursor IDE with additional tools connected through
Model Context Protocol, enabling end-to-end AI Agent for parallelization.
Additional connected tools enable LLM-guided hotspot detection, collection of
dynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance
simulation to estimate performance gains. We apply Aira with Relic parallel
framework for fine-grained task parallelism on SMT cores to parallelize
latency-critical benchmarks representing real-world applications used in
industry. We show 17% geomean performance gain from parallelization of
latency-critical benchmarks using Aira with Relic framework.

</details>


### [112] [Parallelizing Drug Discovery: HPC Pipelines for Alzheimer's Molecular Docking and Simulation](https://arxiv.org/abs/2509.00937)
*Paul Ruiz Alliata,Diana Rubaga,Daniel Kumlin,Alberto Puliga*

Main category: cs.DC

TL;DR: 探索HPC驱动的阿尔茨海默病药物发现管道，实施并行工作流并开发对接原型，案例研究显示工作流有生物学相关性，HPC有加速神经退行性药物发现潜力。


<details>
  <summary>Details</summary>
Motivation: 利用高性能计算（HPC）重塑计算药物发现，探索HPC驱动的阿尔茨海默病药物发现管道。

Method: 使用GROMACS和混合MPI - OpenMP策略实现并行工作流，用Python多进程库从顺序执行转向基于进程的并行开发对接原型。

Result: 案例研究显示工作流对靶向淀粉样蛋白 - β和tau蛋白有生物学相关性，对接原型有显著运行时间增益。

Conclusion: 虽在数据管理、计算成本和扩展效率方面有局限，但HPC有加速神经退行性药物发现的潜力。

Abstract: High-performance computing (HPC) is reshaping computational drug discovery by
enabling large-scale, time-efficient molecular simulations. In this work, we
explore HPC-driven pipelines for Alzheimer's disease drug discovery, focusing
on virtual screening, molecular docking, and molecular dynamics simulations. We
implemented a parallelised workflow using GROMACS with hybrid MPI-OpenMP
strategies, benchmarking scaling performance across energy minimisation,
equilibration, and production stages. Additionally, we developed a docking
prototype that demonstrates significant runtime gains when moving from
sequential execution to process-based parallelism using Python's
multiprocessing library. Case studies on prolinamide derivatives and baicalein
highlight the biological relevance of these workflows in targeting amyloid-beta
and tau proteins. While limitations remain in data management, computational
costs, and scaling efficiency, our results underline the potential of HPC to
accelerate neurodegenerative drug discovery.

</details>


### [113] [DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving](https://arxiv.org/abs/2509.01083)
*Mingyu Yang,Jae-Young Choi,Kihyo Moon,Minsung Jang,Eunjoo Joen*

Main category: cs.DC

TL;DR: 本文提出无训练框架DSDE，利用KLD差异的方差作为预测信号和自适应推测长度上限，实验证明基于KLD的稳定性信号可用于动态调整，能实现有竞争力的端到端延迟和更强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 投机解码在大批次服务环境中依赖固定推测长度并非最优，需探索动态适应的新方向。

Method: 提出无训练框架DSDE，包括基于Kullback - Leibler差异方差的预测信号和自适应推测长度上限。

Result: 基于信号的算法实现的端到端延迟与领先基线相当，在不同工作负载中表现出更强鲁棒性，尤其在低接受率情况下信号仍有诊断价值。

Conclusion: 事后信号是构建更强大和智能的大语言模型推理系统的有价值组成部分，为动态推测长度调整研究指明了有前景的方向。

Abstract: Speculative decoding accelerates large language model inference, but its
reliance on a fixed speculation length is suboptimal in large-batch serving
environments with diverse requests. This paper explores a new direction for
dynamic adaptation by investigating a novel class of post-hoc, diagnostic
signals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free
framework built on two primary components: (1) a predictive signal based on the
variance of the Kullback-Leibler (KLD) divergence, which diagnoses the
generation's regional stability, and (2) an adaptive speculation length cap to
mitigate the straggler problem in per-sequence decoding. Experiments
demonstrate the potential of using KLD-based stability signals for dynamic
adaptation. An algorithm guided by these signals achieves end-to-end latency
competitive with leading baselines and exhibits superior robustness across
diverse workloads. This robustness is particularly valuable in challenging
low-acceptance-rate regimes, where the proposed signal maintains its diagnostic
utility. Collectively, these findings validate post-hoc signals as a valuable
component for building more robust and intelligent LLM inference systems, and
highlight a promising direction for future research on dynamic speculation
length adaptation.

</details>


### [114] [Ocior: Ultra-Fast Asynchronous Leaderless Consensus with Two-Round Finality, Linear Overhead, and Adaptive Security](https://arxiv.org/abs/2509.01118)
*Jinyuan Chen*

Main category: cs.DC

TL;DR: 提出Ocior异步拜占庭容错共识协议，在多个方面达最优性能，引入OciorBLSts签名方案。


<details>
  <summary>Details</summary>
Motivation: 设计一个在弹性、通信、计算和轮次复杂度上达到最优性能的实用异步拜占庭容错共识协议。

Method: 使用并行共识实例单独并发处理交易，采用无领导共识方式，引入OciorBLSts非交互式阈值签名方案。

Result: 实现最优弹性，容忍最多t个故障节点；通信复杂度为O(n)；计算复杂度最佳O(n)，最差O(n log^2 n)；轮次复杂度两异步轮完成交易；OciorBLSts签名聚合计算成本最佳为O(n)。

Conclusion: Ocior协议在多方面具有最优性能，OciorBLSts方案支持高效共识。

Abstract: In this work, we propose Ocior, a practical asynchronous Byzantine
fault-tolerant (BFT) consensus protocol that achieves the optimal performance
in resilience, communication, computation, and round complexity. Unlike
traditional BFT consensus protocols, Ocior processes incoming transactions
individually and concurrently using parallel instances of consensus. While
leader-based consensus protocols rely on a designated leader to propose
transactions, Ocior is a leaderless consensus protocol that guarantees stable
liveness. Ocior achieves: 1) Optimal resilience: Ocior tolerates up to $t$
faulty nodes controlled by an adaptive adversary, for $n\geq 3t+1$. 2) Optimal
communication complexity: The total expected communication per transaction is
$O(n)$. 3) Optimal (or near-optimal) computation complexity: The total
computation per transaction is $O(n)$ in the best case, or $O(n \log^2 n)$ in
the worst case. 4) Optimal round complexity: A legitimate two-party transaction
can be finalized with a good-case latency of two asynchronous rounds, for any
$n\geq 3t+1$. The good case in terms of latency refers to the scenario where
the transaction is proposed by any (not necessarily designated) honest node. A
two-party transaction involves the transfer of digital assets from one user (or
group of users) to one or more recipients. To support efficient consensus, we
introduce a novel non-interactive threshold signature (TS) scheme called
OciorBLSts. It offers fast signature aggregation, and is adaptively secure.
OciorBLSts achieves a signature aggregation computation cost of only $O(n)$ for
the best case. Moreover, OciorBLSts supports the property of Instantaneous TS
Aggregation. This enables real-time aggregation of partial signatures as they
arrive, reducing waiting time and improving responsiveness.

</details>


### [115] [Detecting Rug Pulls in Decentralized Exchanges: Machine Learning Evidence from the TON Blockchain](https://arxiv.org/abs/2509.01168)
*Dmitry Yaremus,Jianghai Li,Alisa Kalacheva,Igor Vodolazov,Yury Yanovich*

Main category: cs.DC

TL;DR: 本文提出用于TON区块链去中心化交易所rug pull诈骗早期检测的机器学习框架，对比两种定义，模型能在交易前五分钟识别诈骗，为投资者提供预警。


<details>
  <summary>Details</summary>
Motivation: TON独特架构为欺诈分析提供新环境，需对其去中心化交易所的rug pull诈骗进行早期检测。

Method: 对TON两大DEXs进行综合研究，融合数据训练模型，对比基于TVL和基于闲置的两种rug pull定义。

Result: 梯度提升模型能在交易前五分钟有效识别rug pull，TVL法AUC高达0.891，闲置法召回率出色，各交易所特征集一致但分布差异大。

Conclusion: 该工作为投资者提供早期预警机制，增强TON DeFi生态安全基础设施。

Abstract: This paper presents a machine learning framework for the early detection of
rug pull scams on decentralized exchanges (DEXs) within The Open Network (TON)
blockchain. TON's unique architecture, characterized by asynchronous execution
and a massive web2 user base from Telegram, presents a novel and critical
environment for fraud analysis. We conduct a comprehensive study on the two
largest TON DEXs, Ston.Fi and DeDust, fusing data from both platforms to train
our models. A key contribution is the implementation and comparative analysis
of two distinct rug pull definitions--TVL-based (a catastrophic liquidity
withdrawal) and idle-based (a sudden cessation of all trading activity)--within
a single, unified study. We demonstrate that Gradient Boosting models can
effectively identify rug pulls within the first five minutes of trading, with
the TVL-based method achieving superior AUC (up to 0.891) while the idle-based
method excels at recall. Our analysis reveals that while feature sets are
consistent across exchanges, their underlying distributions differ
significantly, challenging straightforward data fusion and highlighting the
need for robust, platform-aware models. This work provides a crucial
early-warning mechanism for investors and enhances the security infrastructure
of the rapidly growing TON DeFi ecosystem.

</details>


### [116] [Optimal Parallel Scheduling under Concave Speedup Functions](https://arxiv.org/abs/2509.01811)
*Chengzhang Li,Peizhong Ju,Atilla Eryilmaz,Ness Shroff*

Main category: cs.DC

TL;DR: 本文针对一般凹加速函数下的并行作业调度问题，提出CDR规则、GWF方法和SmartFill算法，在多种凹加速函数上表现优于heSRPT。


<details>
  <summary>Details</summary>
Motivation: 现有heSRPT仅解决特定指数形式加速函数的并行计算资源调度问题，一般凹加速函数的调度问题仍未解决。

Method: 发现CDR规则，提出GWF方法，设计SmartFill算法。

Result: 对于规则加速函数，SmartFill有闭式最优解；对于非规则函数，能高效计算最优解。数值评估显示SmartFill在多种凹加速函数上大幅优于heSRPT。

Conclusion: SmartFill算法有效解决了一般凹加速函数下的并行作业调度问题。

Abstract: Efficient scheduling of parallel computation resources across multiple jobs
is a fundamental problem in modern cloud/edge computing systems for many
AI-based applications. Allocating more resources to a job accelerates its
completion, but with diminishing returns. Prior work (heSRPT) solved this
problem only for some specific speedup functions with an exponential form,
providing a closed-form solution. However, the general case with arbitrary
concave speedup functions -- which more accurately capture real-world workloads
-- has remained open.
  In this paper, we solve this open problem by developing optimal scheduling
algorithms for parallel jobs under general concave speedup functions. We first
discover a fundamental and broadly-applicable rule for optimal parallel
scheduling, namely the Consistent Derivative Ratio (CDR) Rule, which states
that the ratio of the derivatives of the speedup functions across active jobs
remains constant over time. To efficiently compute the optimal allocations that
satisfy the CDR Rule, we propose the General Water-Filling (GWF) method, a more
general version of classical water-filling in wireless communications.
Combining these insights, we design the SmartFill Algorithm to solve the
general scheduling problem. Unlike heSRPT, which always allocates resources to
all active jobs, SmartFill selectively determines which jobs should receive
resources and how much they should be allocated. For a broad class of so-called
\emph{regular} speedup functions, SmartFill yields closed-form optimal
solutions, while for non-regular functions it efficiently computes the optimum
with low complexity. Numerical evaluations show that SmartFill can
substantially outperform heSRPT across a wide range of concave speedup
functions.

</details>


### [117] [LobRA: Multi-tenant Fine-tuning over Heterogeneous Data](https://arxiv.org/abs/2509.01193)
*Sheng Lin,Fangcheng Fu,Haoyang Li,Hao Ge,Xuanyu Wang,Jiawen Niu,Yaofeng Tu,Bin Cui*

Main category: cs.DC

TL;DR: 随着预训练模型发展，微调需求增长，需降低成本。现有联合微调受数据异质性影响，提出 LobRA 框架，实验证明能显著减少联合微调 GPU 用时。


<details>
  <summary>Details</summary>
Motivation: 降低服务提供商处理微调请求的成本，解决联合微调受训练数据序列长度变化和偏斜这两个异质性问题的影响。

Method: 开发 LobRA 框架，部署具有异构资源使用和并行配置的微调副本匹配不同工作负载，考虑序列长度偏斜在异构副本间调度数据实现负载均衡。

Result: 实验验证 LobRA 能显著减少联合微调所需的 GPU 秒数，降幅为 45.03%-60.67%。

Conclusion: LobRA 框架有效解决了联合微调中因训练数据异质性带来的效率问题，能显著降低联合微调成本。

Abstract: With the breakthrough of Transformer-based pre-trained models, the demand for
fine-tuning (FT) to adapt the base pre-trained models to downstream
applications continues to grow, so it is essential for service providers to
reduce the cost of processing FT requests. Low-rank adaption (LoRA) is a widely
used FT technique that only trains small-scale adapters and keeps the base
model unaltered, conveying the possibility of processing multiple FT tasks by
jointly training different LoRA adapters with a shared base model.
  Nevertheless, through in-depth analysis, we reveal the efficiency of joint FT
is dampened by two heterogeneity issues in the training data -- the sequence
length variation and skewness. To tackle these issues, we develop LobRA, a
brand new framework that supports processing multiple FT tasks by jointly
training LoRA adapters. Two innovative designs are introduced. Firstly, LobRA
deploys the FT replicas (i.e., model replicas for FT) with heterogeneous
resource usages and parallel configurations, matching the diverse workloads
caused by the sequence length variation. Secondly, for each training step,
LobRA takes account of the sequence length skewness and dispatches the training
data among the heterogeneous FT replicas to achieve workload balance. We
conduct experiments to assess the performance of LobRA, validating that it
significantly reduces the GPU seconds required for joint FT by 45.03%-60.67%.

</details>


### [118] [LiquidGEMM: Hardware-Efficient W4A8 GEMM Kernel for High-Performance LLM Serving](https://arxiv.org/abs/2509.01229)
*Huanqi Hu,Bowen Xiao,Shixuan Sun,Jianian Yin,Zhexi Zhang,Xiang Luo,Chengquan Jiang,Weiqi Xu,Xiaoying Jia,Xin Liu,Minyi Guo*

Main category: cs.DC

TL;DR: 本文提出用于高效大语言模型服务的硬件高效W4A8 GEMM内核LiquidGEMM，实验显示其有显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有W4A8 GEMM内核因在CUDA核心上反量化效率低，无法跟上张量核心高吞吐量，难以满足实践需求。

Method: 设计了两个关键技术，即硬件高效的量化方法LiquidQuant和隐式细粒度流水线。

Result: LiquidGEMM比现有W4A8内核最高加速2.90倍，端到端系统级最高加速4.94倍；相比NVIDIA TensorRT - LLM中的量化GEMM内核，性能提升1.12 - 1.63倍，系统级最高加速1.63倍。

Conclusion: LiquidGEMM在大语言模型推理加速方面表现出色，能有效提升性能。

Abstract: Quantization is a critical technique for accelerating LLM inference by
reducing memory footprint and improving computational efficiency. Among various
schemes, 4-bit weight and 8-bit activation quantization (W4A8) offers a strong
balance between accuracy and performance. However, existing W4A8 GEMM kernels
fall short in practice due to inefficient dequantization on CUDA Cores, which
cannot keep pace with the high throughput of Tensor Cores. In this paper, we
present LiquidGEMM, a hardware-efficient W4A8 GEMM kernel for efficient LLM
serving. LiquidGEMM designs two key techniques: LiquidQuant, a
hardware-efficient quantization method that enables fast, overflow-safe
dequantization using just two arithmetic instructions per four elements; and an
implicit fine-grained pipeline that fully overlaps weight loading,
dequantization, and MMA across warp groups without software synchronization or
redundant memory traffic. Experimental results show that LiquidGEMM achieves up
to 2.90x speedup over state-of-the-art W4A8 kernels and up to 4.94x end-to-end
system-level speedup. Compared to various quantized GEMM kernels in NVIDIA
TensorRT-LLM, LiquidGEMM delivers 1.12-1.63x performance gains, and achieves up
to 1.63x system-level speedup.

</details>


### [119] [HiCR, an Abstract Model for Distributed Heterogeneous Programming](https://arxiv.org/abs/2509.01425)
*Sergio Miguel Martin,Luca Terracciano,Kiril Dichev,Noah Baumann,Jiashu Lin,Albert-Jan Yzelman*

Main category: cs.DC

TL;DR: 提出HiCR模型表示分布式异构应用和运行时系统语义，解释实现方式并给出应用示例。


<details>
  <summary>Details</summary>
Motivation: 使应用在当前和未来系统无需大量重构即可执行，且能适配任何并行编程范式。

Method: 采用基于插件的方法实现模型组件和操作，处理设备特定实现细节。

Result: 给出基于HiCR的应用能在多种平台上同等运行的示例。

Conclusion: HiCR模型可有效表示分布式异构应用和运行时系统语义，其Runtime Support Layer抽象层次有意义。

Abstract: We present HiCR, a model to represent the semantics of distributed
heterogeneous applications and runtime systems. The model describes a minimal
set of abstract operations to enable hardware topology discovery, kernel
execution, memory management, communication, and instance management, without
prescribing any implementation decisions. The goal of the model is to enable
execution in current and future systems without the need for significant
refactoring, while also being able to serve any governing parallel programming
paradigm. In terms of software abstraction, HiCR is naturally located between
distributed heterogeneous systems and runtime systems. We coin the phrase
\emph{Runtime Support Layer} for this level of abstraction. We explain how the
model's components and operations are realized by a plugin-based approach that
takes care of device-specific implementation details, and present examples of
HiCR-based applications that operate equally on a diversity of platforms.

</details>


### [120] [Safe Memory Reclamation Techniques](https://arxiv.org/abs/2509.02457)
*Ajay Singh*

Main category: cs.DC

TL;DR: 研究非垃圾回收编程语言中安全内存回收算法，结合软硬件思路解决设计挑战。


<details>
  <summary>Details</summary>
Motivation: 非垃圾回收编程语言中，为乐观和无锁并发数据结构的内存安全设计理想的安全内存回收算法存在诸多挑战。

Method: 融合软硬件堆栈的思想和工具来设计安全内存回收算法。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: Safe memory reclamation is crucial to memory safety for optimistic and
lock-free concurrent data structures in non garbage collected programming
languages. However, several challenges arise in designing an ideal safe memory
reclamation algorithm, including achieving high speed and scalability, easy of
use for programmers, applicability to wide class of data structures, managing
the large memory footprint caused by delayed freeing of memory for safety and
performance, and avoiding asymmetric overhead on data structure operations.
Several approaches to designing safe memory reclamation algorithms are studied
by blending ideas and tools from across the hardware-software stack. These
solutions cross traditional boundaries and exploit features exposed at
different layers.

</details>


### [121] [STZ: A High Quality and High Speed Streaming Lossy Compression Framework for Scientific Data](https://arxiv.org/abs/2509.01626)
*Daoce Wang,Pascal Grosset,Jesus Pulido,Jiannan Tian,Tushar M. Athawale,Jinda Jia,Baixi Sun,Boyuan Zhang,Sian Jin,Kai Zhao,James Ahrens,Fengguang Song*

Main category: cs.DC

TL;DR: 提出新流式压缩框架，支持渐进和解随机访问解压缩，保持高压缩质量与速度。


<details>
  <summary>Details</summary>
Motivation: 现有有损压缩中渐进和解随机访问解压缩功能会降低压缩质量和速度，需解决该局限。

Method: 设计首个同时支持渐进和解随机访问解压缩的框架，引入分层分区策略和分层预测机制。

Result: 框架实现高压缩和解压缩速度，比 SZ3 快达 6.7 倍，压缩质量与 SZ3 相当。

Conclusion: 新框架有效解决现有问题，能同时支持两项功能且保持高压缩质量与速度。

Abstract: Error-bounded lossy compression is one of the most efficient solutions to
reduce the volume of scientific data. For lossy compression, progressive
decompression and random-access decompression are critical features that enable
on-demand data access and flexible analysis workflows. However, these features
can severely degrade compression quality and speed. To address these
limitations, we propose a novel streaming compression framework that supports
both progressive decompression and random-access decompression while
maintaining high compression quality and speed. Our contributions are
three-fold: (1) we design the first compression framework that simultaneously
enables both progressive decompression and random-access decompression; (2) we
introduce a hierarchical partitioning strategy to enable both streaming
features, along with a hierarchical prediction mechanism that mitigates the
impact of partitioning and achieves high compression quality -- even comparable
to state-of-the-art (SOTA) non-streaming compressor SZ3; and (3) our framework
delivers high compression and decompression speed, up to 6.7$\times$ faster
than SZ3.

</details>


### [122] [A Continuous Energy Ising Machine Leveraging Difference-of-Convex Programming](https://arxiv.org/abs/2509.01928)
*Debraj Banerjee,Santanu Mahapatra,Kunal Narayan Chaudhury*

Main category: cs.DC

TL;DR: 提出新的伊辛问题求解方法，在GPU平台实现并优于现有求解器。


<details>
  <summary>Details</summary>
Motivation: 现有基于模拟退火的伊辛求解器缺乏收敛保证且对冷却时间表敏感。

Method: 将二进制自旋松弛为连续变量，引入势函数，得到的哈密顿量可表示为凸函数之差，设计高效迭代算法。

Result: 在从边缘设备到高性能计算集群的一系列GPU平台实现求解器，在不同问题规模上始终优于现有求解器。

Conclusion: 所提方法能有效解决伊辛问题，具有良好性能和收敛保证。

Abstract: Many combinatorial optimization problems can be reformulated as the task of
finding the ground state of a physical system, such as the Ising model. Most
existing Ising solvers are inspired by simulated annealing. Although annealing
techniques offer scalability, they lack convergence guarantees and are
sensitive to the cooling schedule. We propose to solve the Ising problem by
relaxing the binary spins to continuous variables and introducing a potential
function (attractor) that steers the solution toward binary spin
configurations. The resulting Hamiltonian can be expressed as a difference of
convex functions, enabling the design of efficient iterative algorithms that
require a single matrix-vector multiplication per iteration and are backed by
convergence guarantees. We implement our Ising solver across a range of GPU
platforms: from edge devices to high-performance computing clusters and
demonstrate that it consistently outperforms existing solvers across problem
sizes ranging from small ($10^3$ spins) to ultra-large ($10^8$ spins).

</details>


### [123] [Fault-Tolerant Decentralized Distributed Asynchronous Federated Learning with Adaptive Termination Detection](https://arxiv.org/abs/2509.02186)
*Phani Sahasra Akkinepally,Manaswini Piduguralla,Sushant Joshi,Sathya Peri,Sandeep Kulkarni*

Main category: cs.DC

TL;DR: 本文提出异步去中心化联邦学习方法，分两阶段开发框架并添加容错机制，还提出新终止技术确保客户端有效收敛。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖中央服务器存在瓶颈和单点故障问题，同步去中心化联邦学习有延迟问题，因此研究异步去中心化联邦学习。

Method: 分两阶段开发异步去中心化联邦学习方法，第一阶段开发异步框架让客户端独立学习更新，第二阶段添加容错机制；提出Client - Confident Convergence和Client - Responsive Termination技术。

Result: 提出异步去中心化联邦学习方法及相关技术。

Conclusion: 所提方法和技术能让活跃客户端有意义且高效结束，在异步通信和故障挑战下保持可靠收敛。

Abstract: Federated Learning (FL) facilitates collaborative model training across
distributed clients while ensuring data privacy. Traditionally, FL relies on a
centralized server to coordinate learning, which creates bottlenecks and a
single point of failure. Decentralized FL architectures eliminate the need for
a central server and can operate in either synchronous or asynchronous modes.
Synchronous FL requires all clients to compute updates and wait for one another
before aggregation, guaranteeing consistency but often suffering from delays
due to slower participants. Asynchronous FL addresses this by allowing clients
to update independently, offering better scalability and responsiveness in
heterogeneous environments.
  Our research develops an asynchronous decentralized FL approach in two
progressive phases. (a) In Phase 1, we develop an asynchronous FL framework
that enables clients to learn and update independently, removing the need for
strict synchronization. (b) In Phase 2, we extend this framework with fault
tolerance mechanisms to handle client failures and message drops, ensuring
robust performance even under unpredictable conditions. As a central
contribution, we propose Client-Confident Convergence and Client-Responsive
Termination novel techniques that provide each client with the ability to
autonomously determine appropriate termination points. These methods ensure
that all active clients conclude meaningfully and efficiently, maintaining
reliable convergence despite the challenges of asynchronous communication and
faults.

</details>


### [124] [Near-Optimal Stability for Distributed Transaction Processing in Blockchain Sharding](https://arxiv.org/abs/2509.02421)
*Ramesh Adhikari,Costas Busch,Dariusz R. Kowalski*

Main category: cs.DC

TL;DR: 本文研究区块链分片系统稳定性，提出单领导者调度器和多领导者分布式调度器，在特定注入率下保证系统稳定，且分布式调度器结果显著提升。


<details>
  <summary>Details</summary>
Motivation: 确保区块链分片系统在任何可处理的交易模式下的稳定性，应对最坏情况和交易处理攻击。

Method: 提出单领导者调度器和多领导者分布式调度器，并给出在一定注入率下保证系统稳定的条件。

Result: 单领导者调度器在注入率 ρ ≤ max{1/(16k), 1/(16⌈√s⌉)} 下保证稳定；分布式调度器在注入率 ρ ≤ 1/(16c₁ log D log s)max{1/k, 1/⌈√s⌉} 下保证稳定，该结果接近最优注入率。

Conclusion: 所提出的调度器能有效保证区块链分片系统的稳定性，分布式调度器结果相比之前有显著提升。

Abstract: In blockchain sharding, $n$ processing nodes are divided into $s$ shards, and
each shard processes transactions in parallel. A key challenge in such a system
is to ensure system stability for any ``tractable'' pattern of generated
transactions; this is modeled by an adversary generating transactions with a
certain rate of at most $\rho$ and burstiness $b$. This model captures
worst-case scenarios and even some attacks on transactions' processing, e.g.,
DoS. A stable system ensures bounded transaction queue sizes and bounded
transaction latency. It is known that the absolute upper bound on the maximum
injection rate for which any scheduler could guarantee bounded queues and
latency of transactions is $\max\left\{ \frac{2}{k+1}, \frac{2}{
\left\lfloor\sqrt{2s}\right\rfloor}\right\}$, where $k$ is the maximum number
of shards that each transaction accesses. Here, we first provide a single
leader scheduler that guarantees stability under injection rate $\rho \leq
\max\left\{ \frac{1}{16k}, \frac{1}{16\lceil \sqrt{s} \rceil}\right\}$.
Moreover, we also give a distributed scheduler with multiple leaders that
guarantees stability under injection rate $\rho \leq \frac{1}{16c_1 \log D \log
s}\max\left\{ \frac{1}{k}, \frac{1}{\lceil \sqrt{s} \rceil} \right\}$, where
$c_1$ is some positive constant and $D$ is the diameter of shard graph $G_s$.
This bound is within a poly-log factor from the optimal injection rate, and
significantly improves the best previous known result for the distributed
setting by Adhikari et al., SPAA 2024.

</details>


### [125] [Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized Modest Computer Cluster](https://arxiv.org/abs/2509.02440)
*Marie Reinbigler,Rishi Sharma,Rafael Pires,Elisabeth Brunet,Anne-Marie Kermarrec,Catalin Fetita*

Main category: cs.DC

TL;DR: 本文介绍PyramidAI技术以降低千兆像素图像分析的计算成本，在保证精度下减少处理数据量，还评估主流计算机计算潜力，减少分析时间。


<details>
  <summary>Details</summary>
Motivation: 千兆像素图像分析计算要求高，需降低计算成本。

Method: 采用从低分辨率到高分辨率逐步分析图像的方法，研究调整精度 - 计算性能权衡的策略，利用方法的并行性评估主流计算机计算潜力，用模拟器选择数据分布和负载均衡算法。

Result: PyramidAI最多可将分析所需处理的数据量减少2.65倍，用12个普通工作节点可将分析时间从超1小时减至几分钟。

Conclusion: PyramidAI为大规模图像分析提供了高效实用的解决方案。

Abstract: Analyzing gigapixel images is recognized as computationally demanding. In
this paper, we introduce PyramidAI, a technique for analyzing gigapixel images
with reduced computational cost. The proposed approach adopts a gradual
analysis of the image, beginning with lower resolutions and progressively
concentrating on regions of interest for detailed examination at higher
resolutions. We investigated two strategies for tuning the accuracy-computation
performance trade-off when implementing the adaptive resolution selection,
validated against the Camelyon16 dataset of biomedical images. Our results
demonstrate that PyramidAI substantially decreases the amount of processed data
required for analysis by up to 2.65x, while preserving the accuracy in
identifying relevant sections on a single computer. To ensure democratization
of gigapixel image analysis, we evaluated the potential to use mainstream
computers to perform the computation by exploiting the parallelism potential of
the approach. Using a simulator, we estimated the best data distribution and
load balancing algorithm according to the number of workers. The selected
algorithms were implemented and highlighted the same conclusions in a
real-world setting. Analysis time is reduced from more than an hour to a few
minutes using 12 modest workers, offering a practical solution for efficient
large-scale image analysis.

</details>


### [126] [An Efficient and Adaptive Watermark Detection System with Tile-based Error Correction](https://arxiv.org/abs/2509.02447)
*Xinrui Zhong,Xinze Feng,Jingwei Zuo,Fanjiang Ye,Yi Mu,Junfeng Guo,Heng Huang,Myungjin Lee,Yuke Wang*

Main category: cs.DC

TL;DR: 提出QRMark方法检测图像水印，结合纠错与平铺技术，提高效率，端到端评估显示有推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略大规模图像集水印检测的效率挑战，需高效检测方法。

Method: 结合QR码纠错与平铺技术，算法层面用Reed - Solomon纠错机制，系统层面采用资源感知流分配策略和基于图块的工作负载交错策略。

Result: QRMark较顺序基线平均实现2.43倍推理加速。

Conclusion: QRMark能在保持准确性和鲁棒性的同时，有效提高图像水印检测效率。

Abstract: Efficient and reliable detection of generated images is critical for the
responsible deployment of generative models. Existing approaches primarily
focus on improving detection accuracy and robustness under various image
transformations and adversarial manipulations, yet they largely overlook the
efficiency challenges of watermark detection across large-scale image
collections. To address this gap, we propose QRMark, an efficient and adaptive
end-to-end method for detecting embedded image watermarks. The core idea of
QRMark is to combine QR Code inspired error correction with tailored tiling
techniques to improve detection efficiency while preserving accuracy and
robustness. At the algorithmic level, QRMark employs a Reed-Solomon error
correction mechanism to mitigate the accuracy degradation introduced by tiling.
At the system level, QRMark implements a resource-aware stream allocation
policy that adaptively assigns more streams to GPU-intensive stages of the
detection pipeline. It further employs a tile-based workload interleaving
strategy to overlap data-loading overhead with computation and schedules
kernels across stages to maximize efficiency. End-to-end evaluations show that
QRMark achieves an average 2.43x inference speedup over the sequential
baseline.

</details>


### [127] [KubeIntellect: A Modular LLM-Orchestrated Agent Framework for End-to-End Kubernetes Management](https://arxiv.org/abs/2509.02449)
*Mohsen Seyedkazemi Ardebili,Andrea Bartolini*

Main category: cs.DC

TL;DR: 本文提出由大语言模型驱动的KubeIntellect系统用于Kubernetes智能端到端控制，评估显示其有高成功率和可靠性。


<details>
  <summary>Details</summary>
Motivation: Kubernetes管理复杂且碎片化，现有工具存在局限，需要更智能的管理系统。

Method: 构建KubeIntellect系统，采用模块化代理，由监督器协调，集成内存检查点等机制。

Result: 在200个自然语言查询中工具合成成功率达93%，可靠性100%，能在不同工作负载下高效运行。

Conclusion: 引入了可解释、可扩展且由大语言模型驱动的复杂基础设施管理系统。

Abstract: Kubernetes has become the foundation of modern cloud-native infrastructure,
yet its management remains complex and fragmented. Administrators must navigate
a vast API surface, manage heterogeneous workloads, and coordinate tasks across
disconnected tools - often requiring precise commands, YAML configuration, and
contextual expertise.
  This paper presents KubeIntellect, a Large Language Model (LLM)-powered
system for intelligent, end-to-end Kubernetes control. Unlike existing tools
that focus on observability or static automation, KubeIntellect supports
natural language interaction across the full spectrum of Kubernetes API
operations, including read, write, delete, exec, access control, lifecycle, and
advanced verbs. The system uses modular agents aligned with functional domains
(e.g., logs, metrics, RBAC), orchestrated by a supervisor that interprets user
queries, maintains workflow memory, invokes reusable tools, or synthesizes new
ones via a secure Code Generator Agent.
  KubeIntellect integrates memory checkpoints, human-in-the-loop clarification,
and dynamic task sequencing into a structured orchestration framework.
Evaluation results show a 93% tool synthesis success rate and 100% reliability
across 200 natural language queries, demonstrating the system's ability to
operate efficiently under diverse workloads. An automated demo environment is
provided on Azure, with additional support for local testing via kind. This
work introduces a new class of interpretable, extensible, and LLM-driven
systems for managing complex infrastructure.

</details>


### [128] [MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to Break the GPU Memory Wall](https://arxiv.org/abs/2509.02480)
*Avinash Maurya,M. Mustafa Rafique,Franck Cappello,Bogdan Nicolae*

Main category: cs.DC

TL;DR: 因LLM尺寸增长快于GPU内存，现有多层卸载技术有I/O开销，提出MLP - Offload优化LLM训练，评估显示其迭代速度比现有技术快2.5倍。


<details>
  <summary>Details</summary>
Motivation: LLM尺寸增长快于GPU内存，现有多层卸载技术在训练关键路径产生显著I/O开销，导致迭代变慢。

Method: 基于对I/O开销主导迭代时间等关键观察，设计并实现MLP - Offload，以缓存高效和并发控制的方式跨多层卸载优化器状态，缓解反向和更新阶段的I/O瓶颈。

Result: 在参数达280B的模型上评估，MLP - Offload迭代速度比现有LLM训练运行时间快2.5倍。

Conclusion: MLP - Offload能有效缓解资源受限环境下LLM训练的I/O瓶颈，提升训练效率。

Abstract: Training LLMs larger than the aggregated memory of multiple GPUs is
increasingly necessary due to the faster growth of LLM sizes compared to GPU
memory. To this end, multi-tier host memory or disk offloading techniques are
proposed by state of art. Despite advanced asynchronous multi-tier read/write
strategies, such offloading strategies result in significant I/O overheads in
the critical path of training, resulting in slower iterations. To this end, we
propose MLP-Offload, a novel multi-level, multi-path offloading engine
specifically designed for optimizing LLM training on resource-constrained
setups by mitigating I/O bottlenecks. We make several key observations that
drive the design of MLP-Offload, such as I/O overheads during the update
dominate the iteration time; I/O bandwidth of the third-level remote storage
tier remains unutilized; and, contention due to concurrent offloading amplifies
I/O bottlenecks. Driven by these insights, we design and implement MLP-Offload
to offload the optimizer states across multiple tiers in a cache-efficient and
concurrency-controlled fashion to mitigate I/O bottlenecks during the backward
and update phases. Evaluations on models up to 280B parameters shows that
MLP-Offload achieves 2.5$\times$ faster iterations compared to the
state-of-the-art LLM training runtimes.

</details>


### [129] [Energy-Efficient Split Learning for Resource-Constrained Environments: A Smart Farming Solution](https://arxiv.org/abs/2509.02549)
*Keiwan Soltani,Vishesh Kumar Tanwar,Ashish Gupta,Sajal K. Das*

Main category: cs.DC

TL;DR: 提出eEnergy - Split框架解决智能农业系统资源有限、数据隐私和连接性差问题，评估显示其节能且提高准确率，强调结合SL与节能设计潜力。


<details>
  <summary>Details</summary>
Motivation: 解决智能农业系统面临的资源有限、数据隐私需求和农村地区连接性差的问题。

Method: 提出eEnergy - Split框架，利用分割学习（SL）在边缘设备和中央服务器间分配模型，提出最优边缘部署算法和解决TSP的无人机轨迹规划策略。

Result: eEnergy - Split比联邦学习减少86%设备能耗，提高分类准确率，降低无人机能耗，整体准确率最多提高17%，SL能效与模型有关。

Conclusion: 结合分割学习与节能设计能为资源受限的智能农业环境提供可扩展、保护隐私的解决方案。

Abstract: Smart farming systems encounter significant challenges, including limited
resources, the need for data privacy, and poor connectivity in rural areas. To
address these issues, we present eEnergy-Split, an energy-efficient framework
that utilizes split learning (SL) to enable collaborative model training
without direct data sharing or heavy computation on edge devices. By
distributing the model between edge devices and a central server, eEnergy-Split
reduces on-device energy usage by up to 86 percent compared to federated
learning (FL) while safeguarding data privacy. Moreover, SL improves
classification accuracy by up to 6.2 percent over FL on ResNet-18 and by more
modest amounts on GoogleNet and MobileNetV2. We propose an optimal edge
deployment algorithm and a UAV trajectory planning strategy that solves the
Traveling Salesman Problem (TSP) exactly to minimize flight cost and extend and
maximize communication rounds. Comprehensive evaluations on agricultural pest
datasets reveal that eEnergy-Split lowers UAV energy consumption compared to
baseline methods and boosts overall accuracy by up to 17 percent. Notably, the
energy efficiency of SL is shown to be model-dependent-yielding substantial
savings in lightweight models like MobileNet, while communication and memory
overheads may reduce efficiency gains in deeper networks. These results
highlight the potential of combining SL with energy-aware design to deliver a
scalable, privacy-preserving solution for resource-constrained smart farming
environments.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [130] [Approximating Graphic Multi-Path TSP and Graphic Ordered TSP](https://arxiv.org/abs/2509.00448)
*Morteza Alimi,Niklas Dahlmeier,Tobias Mömke,Philipp Pabst,Laura Vargas Koch*

Main category: cs.DS

TL;DR: 本文针对图形度量下的Multi - Path TSP问题，给出了2的近似保证算法，还证明特殊情况与Graphic Multi - Path TSP的关系，且利用相关思想为Ordered TSP给出1.791近似算法。


<details>
  <summary>Details</summary>
Motivation: 当前Multi - Path TSP问题已知最佳近似因子为2.214，希望在图形度量下获得更好的近似保证。

Method: 从对应问题LP最优解的流分解中采样路径，并使用双倍边连接剩余顶点，同时将采样路径成本融入其中。

Result: 在图形度量下，Multi - Path TSP获得2的近似保证；为Ordered TSP在图形度量下给出1.791近似算法。

Conclusion: 对于图形度量下的Multi - Path TSP能得到更好的近似因子，相关思想可用于Ordered TSP获得更优近似算法。

Abstract: The path version of the Traveling Salesman Problem is one of the most
well-studied variants of the ubiquitous TSP. Its generalization, the Multi-Path
TSP, has recently been used in the best known algorithm for path TSP by Traub
and Vygen [Cambridge University Press, 2024]. The best known approximation
factor for this problem is $2.214$ by B\"{o}hm, Friggstad, M\"{o}mke and
Spoerhase [SODA 2025]. In this paper we show that for the case of graphic
metrics, a significantly better approximation guarantee of $2$ can be attained.
Our algorithm is based on sampling paths from a decomposition of the flow
corresponding to the optimal solution to the LP for the problem, and connecting
the left-out vertices with doubled edges. The cost of the latter is twice the
optimum in the worst case; we show how the cost of the sampled paths can be
absorbed into it without increasing the approximation factor. Furthermore, we
prove that any below-$2$ approximation algorithm for the special case of the
problem where each source is the same as the corresponding sink yields a
below-$2$ approximation algorithm for Graphic Multi-Path TSP.
  We also show that our ideas can be utilized to give a factor
$1.791$-approximation algorithm for Ordered TSP in graphic metrics, for which
the aforementioned paper [SODA 2025] and Armbruster, Mnich and N\"agele [APPROX
2024] give a $1.868$-approximation algorithm in general metrics.

</details>


### [131] [How to Compute a Moving Sum](https://arxiv.org/abs/2509.00537)
*David K. Maslen,Daniel N. Rockmore*

Main category: cs.DS

TL;DR: 本文探讨窗口递归计算的计算方面，提出新算法，发展相关理论并给出大量示例。


<details>
  <summary>Details</summary>
Motivation: 探索窗口递归计算的计算方面，包括顺序和并行计算，发展算法理论及其适用性。

Method: 引入低延迟的新顺序算法，通过将窗口递归与半直积的代数构造和半群中的幂运算算法相关联推导新的并行和向量算法，发展半结合性理论和用数据表示函数组合与应用的代数条件。

Result: 得到新的顺序、并行和向量算法，发展了相关理论，给出大量示例。

Conclusion: 系统化了实践者并行化递归计算的技术，新算法以伪代码呈现。

Abstract: Windowed recurrences are sliding window calculations where a function is
applied iteratively across the window of data, and are ubiquitous throughout
the natural, social, and computational sciences. In this monograph we explore
the computational aspects of these calculations, including sequential and
parallel computation, and develop the theory underlying the algorithms and
their applicability. We introduce an efficient new sequential algorithm with
low latency, and develop new techniques to derive and analyze the complexity
and domain of validity of existing sequential algorithms. For parallel
computation we derive new parallel and vector algorithms by relating windowed
recurrences to the algebraic construction of semidirect products, and to
algorithms for exponentiation in semigroups. In the middle chapters of the
monograph we further develop the theory of semi-associativity and the algebraic
conditions for representing function composition and function application by
data. This systematizes the techniques used by practitioners to parallelize
recurrence calculations. We end the monograph with an extensive gallery of
examples of interest to specialists in many fields. Throughout the monograph
new algorithms are described with pseudo-code transcribed from functioning
source code.

</details>


### [132] [Triangle Counting in Hypergraph Streams: A Complete and Practical Approach](https://arxiv.org/abs/2509.00674)
*Lingkai Meng,Long Yuan,Xuemin Lin,Wenjie Zhang,Ying Zhang*

Main category: cs.DS

TL;DR: 提出HTCount和HTCount - P算法解决超图流中三角形计数问题，算法准确且吞吐量高。


<details>
  <summary>Details</summary>
Motivation: 现有超图流三角形计数方法存在超顶点三角形结构分类不完整和采样方案不灵活的问题。

Method: 引入超顶点三角形完整分类，开发基于蓄水池的HTCount算法动态调整样本大小，开发基于分区的HTCount - P算法自适应划分未使用内存。

Result: 理论分析了算法的无偏性和方差界，案例研究展示了三角形结构的表达能力，实验表明算法在严格内存约束下估计准确，相对误差比现有方法低1 - 2个数量级，吞吐量高。

Conclusion: 提出的算法能有效解决现有超图流三角形计数方法的局限，在严格内存约束下表现良好。

Abstract: Triangle counting in hypergraph streams, including both hyper-vertex and
hyper-edge triangles, is a fundamental problem in hypergraph analytics, with
broad applications. However, existing methods face two key limitations: (i) an
incomplete classification of hyper-vertex triangle structures, typically
considering only inner or outer triangles; and (ii) inflexible sampling schemes
that predefine the number of sampled hyperedges, which is impractical under
strict memory constraints due to highly variable hyperedge sizes. To address
these challenges, we first introduce a complete classification of hyper-vertex
triangles, including inner, hybrid, and outer triangles. Based on this, we
develop HTCount, a reservoir-based algorithm that dynamically adjusts the
sample size based on the available memory M. To further improve memory
utilization and reduce estimation error, we develop HTCount-P, a
partition-based variant that adaptively partitions unused memory into
independent sample subsets. We provide theoretical analysis of the unbiasedness
and variance bounds of the proposed algorithms. Case studies demonstrate the
expressiveness of our triangle structures in revealing meaningful interaction
patterns. Extensive experiments on real-world hypergraphs show that both our
algorithms achieve highly accurate triangle count estimates under strict memory
constraints, with relative errors that are 1 to 2 orders of magnitude lower
than those of existing methods and consistently high throughput.

</details>


### [133] [Large cliques and large independent sets: can they coexist?](https://arxiv.org/abs/2509.00721)
*Uriel Feige,Ilia Pauzner*

Main category: cs.DS

TL;DR: 研究图中检测k - 排除顶点的复杂度，不同k取值下有不同结论。


<details>
  <summary>Details</summary>
Motivation: 受秘密共享方案中出现的问题的启发，研究检测k - 排除顶点的复杂度。

Method: 理论分析，针对不同k与n的关系讨论。

Result: 当k > (1/4 + ε)n时，n顶点图必有k - 排除顶点且可多项式时间找到；当k < (1/4 - ε)n时，判断图是否无k - 排除顶点是NP - 难的。

Conclusion: 不同k与n的关系下，检测k - 排除顶点的复杂度不同。

Abstract: For a graph $G$ and a parameter $k$, we call a vertex $k$-enabling if it
belongs both to a clique of size $k$ and to an independent set of size $k$, and
we call it $k$-excluding otherwise. Motivated by issues that arise in secret
sharing schemes, we study the complexity of detecting vertices that are
$k$-excluding. We show that for every $\epsilon$, for sufficiently large $n$,
if $k > (\frac{1}{4} + \epsilon)n$, then every graph on $n$ vertices must have
a $k$-excluding vertex, and moreover, such a vertex can be found in polynomial
time. In contrast, if $k < (\frac{1}{4} - \epsilon)n$, a regime in which it
might be that all vertices are $k$-enabling, deciding whether a graph has no
$k$-excluding vertex is NP-hard.

</details>


### [134] [New approximate distance oracles and their applications](https://arxiv.org/abs/2509.00890)
*Avi Kadria,Liam Roditty*

Main category: cs.DS

TL;DR: 本文全面研究非恒定查询时间的距离预言机，探索其空间、拉伸和查询时间的权衡，给出新的权衡关系并应用于两个问题。


<details>
  <summary>Details</summary>
Motivation: 以往工作多关注恒定查询时间的距离预言机，本文旨在研究非恒定查询时间的距离预言机。

Method: 考虑加权和无权图在不同拉伸情况下，构建新的距离预言机结构。

Result: 提出新的拉伸、空间和查询时间的三方权衡关系，构建特定拉伸、空间和查询时间的距离预言机，并展示其在两个问题上的应用。

Conclusion: 对非恒定查询时间的距离预言机进行了有效研究，拓展了经典距离预言机，且新预言机可应用于相关问题。

Abstract: Let $G = (V, E)$ be an undirected graph with $n$ vertices and $m$ edges, and
let $\mu = m/n$. A \emph{distance oracle} is a data structure designed to
answer approximate distance queries, with the goal of achieving low stretch,
efficient space usage, and fast query time. While much of the prior work
focused on distance oracles with constant query time, this paper presents a
comprehensive study of distance oracles with non-constant query time. We
explore the tradeoffs between space, stretch, and query time of distance
oracles in various regimes. Specifically, we consider both weighted and
unweighted graphs in the regimes of stretch $< 2$ and stretch $\ge 2$. Among
our results, we present a new three-way trade-off between stretch, space, and
query time, offering a natural extension of the classical Thorup-Zwick distance
oracle [STOC'01 and JACM'05] to regimes with larger query time. Specifically,
for any $0 < r < 1/2$ and integer $k \ge 1$, we construct a $(2k(1 - 2r) -
1)$-stretch distance oracle with $\tilde{O}(m + n^{1 + 1/k})$ space and
$\tilde{O}(\mu n^r)$ query time. In addition, we demonstrate several
applications of our new distance oracles to the $n$-Pairs Shortest Paths
($n$-PSP) problem and the All Nodes Shortest Cycles ($ANSC$) problem.

</details>


### [135] [Almost Tight Approximation Hardness and Online Algorithms for Resource Scheduling](https://arxiv.org/abs/2509.01086)
*Rathish Das,Hao Sun*

Main category: cs.DS

TL;DR: 研究优先级约束资源调度问题，证明离线近似硬度，建立与SCS问题联系，给出在线算法竞争比上下界并提出匹配的确定性在线算法。


<details>
  <summary>Details</summary>
Motivation: 解决离线设置下是否存在多项式时间O(1)因子近似算法的开放问题，研究在线调度算法的性能。

Method: 理论证明，包括近似硬度证明、问题联系证明和在线算法竞争比分析。

Result: 离线：证明近似硬度；建立与SCS问题联系。在线：给出竞争比上下界，提出匹配的确定性在线算法。

Conclusion: 离线问题难以获得低因子近似算法，在线有最优竞争比的确定性算法。

Abstract: We study the precedence-constrained resource scheduling problem [SICOMP'75].
There are $n$ jobs where each job takes a certain time to finish and has a
resource requirement throughout the execution time. There are precedence among
the jobs. The problem asks that given a resource budget, schedule the jobs
obeying the precedence constraints to minimize makespan (maximum completion
time of a job) such that at any point in time, the total resource being used by
all the jobs is at most the given resource budget. In the offline setting, an
important open question is whether a polynomial-time $O(1)$-factor
approximation algorithm can be found. We prove almost tight hardness of
approximation: For some constant $\alpha > 0$, there is no $o((\log
t_{\max})^{\alpha})$-factor ( or $o( ( \log n )^\alpha )$-factor )
approximation algorithm with $n$ jobs of maximum job length $t_{\max}$, unless
P = NP ( or NP $\subset$ DTIME$(O( 2^{\text{polylog}(n)}))$ ).
  We further show a connection between this scheduling problem and a seemingly
unrelated problem called the shortest common super-sequence (SCS) problem,
which has wide application in Biology and Genomics. We prove that an $o(\log
t_{\max})$-factor approximation of the scheduling problem would imply the
existence of an $o(|\Sigma|)$-approximation algorithm for SCS with alphabet
$\Sigma$.
  We then consider the online setting. We present $\Omega(\log n)$ and
$\Omega(\log t_{\max})$ lower bounds of the competitive ratio of any randomized
online algorithm. Moreover, we present a matching $O(\min\{\log n, \log
t_{\max}\})$-competitive deterministic online algorithm.

</details>


### [136] [Column-generation for a two-dimensional multi-criteria bin-packing problem](https://arxiv.org/abs/2509.01218)
*Christof Groschke,Steffen Goebbels,Jochen Rethmann*

Main category: cs.DS

TL;DR: 研究印刷电路板制造中的二维装箱问题，提出分支定价法。


<details>
  <summary>Details</summary>
Motivation: 早期混合整数规划（MIP）模型对小问题实例运行时间可接受，对大问题需更有效方法。

Method: 采用改进的Ryan - Foster分支的分支定价法，定价问题计算布局，将耗时约束从主问题分离。

Result: 未提及具体结果。

Conclusion: 未提及具体结论。

Abstract: In this study, we examine a two-dimensional bin-packing problem in printed
circuit board manufacturing. Among other objectives, the number of bins, but
also the number of different bin layouts, is to be minimized. As the running
times of an earlier MIP presentation are only acceptable for small problem
instances, we will now discuss a branch-and-price approach by using an adapted
Ryan-Foster-branching. The pricing problem computes the layouts, separating the
time-consuming constraints from the master problem.

</details>


### [137] [Hitting Geodesic Intervals in Structurally Restricted Graphs](https://arxiv.org/abs/2509.01413)
*Tatsuya Gima,Yasuaki Kobayashi,Yuto Okada,Yota Otachi,Hayato Takaike*

Main category: cs.DS

TL;DR: 本文围绕Hitting Geodesic Intervals问题，在正负两方面拓展已知结果，给出结构参数相关的复杂度对比。


<details>
  <summary>Details</summary>
Motivation: Aravind和Saxena引入Hitting Geodesic Intervals问题并给出参数化复杂度结果，本文旨在拓展这些结果。

Method: 证明问题在多种图结构（如加单点的5 - 顶点路径不相交并等）上是NP - 完全的；给出以k加模块化宽度、k加顶点完整性为参数的固定参数算法；证明以终端对最小顶点多割大小等为参数时问题是W[2] - 完全的。

Result: 得到问题在多种图结构上的NP - 完全性；给出两种固定参数算法；证明了问题在特定参数化下的W[2] - 完全性。

Conclusion: 拓展了Hitting Geodesic Intervals问题在正负两方面的已知结果，展示了结构参数下的复杂度对比。

Abstract: Given a graph $G = (V,E)$, a set $T$ of vertex pairs, and an integer $k$,
Hitting Geodesic Intervals asks whether there is a set $S \subseteq V$ of size
at most $k$ such that for each terminal pair $\{u,v\} \in T$, the set $S$
intersects at least one shortest $u$-$v$ path. Aravind and Saxena [WALCOM 2024]
introduced this problem and showed several parameterized complexity results. In
this paper, we extend the known results in both negative and positive
directions and present sharp complexity contrasts with respect to structural
graph parameters.
  We first show that the problem is NP-complete even on graphs obtained by
adding a single vertex to a disjoint union of 5-vertex paths. By modifying the
proof of this result, we also show the NP-completeness on graphs obtained from
a path by adding one vertex and on graphs obtained from a disjoint union of
triangles by adding one universal vertex. Furthermore, we show the
NP-completeness on graphs of bandwidth 4 and maximum degree 5 by replacing the
universal vertex in the last case with a long path. Under standard complexity
assumptions, these negative results rule out fixed-parameter algorithms for
most of the structural parameters studied in the literature (if the solution
size $k$ is not part of the parameter).
  We next present fixed-parameter algorithms parameterized by $k$ plus
modular-width and by $k$ plus vertex integrity. The algorithm for the latter
case does indeed solve a more general setting that includes the
parameterization by the minimum vertex multiway-cut size of the terminal
vertices. We show that this is tight in the sense that the problem
parameterized by the minimum vertex multicut size of the terminal pairs is
W[2]-complete. We then modify the proof of this intractability result and show
that the problem is W[2]-complete parameterized by $k$ even in the setting
where $T = \binom{Q}{2}$ for some $Q \subseteq V$.

</details>


### [138] [Fixed-Parameter Tractable Submodular Maximization over a Matroid](https://arxiv.org/abs/2509.01591)
*Shamisa Nematollahi,Adrian Vladu,Junyao Zhao*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we design fixed-parameter tractable (FPT) algorithms for
(non-monotone) submodular maximization subject to a matroid constraint, where
the matroid rank $r$ is treated as a fixed parameter that is independent of the
total number of elements $n$. We provide two FPT algorithms: one for the
offline setting and another for the random-order streaming setting. Our
streaming algorithm achieves a $\frac{1}{2}-\varepsilon$ approximation using
$\widetilde{O}\left(\frac{r}{\textrm{poly}(\varepsilon)}\right)$ memory, while
our offline algorithm obtains a $1-\frac{1}{e}-\varepsilon$ approximation with
$n\cdot 2^{\widetilde{O}\left(\frac{r}{\textrm{poly}(\varepsilon)}\right)}$
runtime and $\widetilde{O}\left(\frac{r}{\textrm{poly}(\varepsilon)}\right)$
memory. Both approximation factors are near-optimal in their respective
settings, given existing hardness results. In particular, our offline algorithm
demonstrates that--unlike in the polynomial-time regime--there is essentially
no separation between monotone and non-monotone submodular maximization under a
matroid constraint in the FPT framework.

</details>


### [139] [Fast Computation of $k$-Runs, Parameterized Squares, and Other Generalised Squares](https://arxiv.org/abs/2509.02179)
*Yuto Nakashima,Jakub Radoszewski,Tomasz Waleń*

Main category: cs.DS

TL;DR: 本文优化了k - 不匹配重复计算复杂度，应用结果提出参数化平方计算算法并改进计数算法。


<details>
  <summary>Details</summary>
Motivation: 优化已有算法复杂度，改进参数化平方相关计算的时间复杂度。

Method: 分析k - 不匹配重复的输出数量，应用其结果设计参数化平方计算和计数算法。

Result: 证明k - 不匹配重复算法复杂度为O(nk log k)；提出O(nσ log σ)时间的非等价参数化平方报告算法；给出O(n log n)时间的非等价参数化平方计数算法。

Conclusion: 优化了k - 不匹配重复和参数化平方计算与计数的时间复杂度，部分算法在特定条件下有更好表现。

Abstract: A $k$-mismatch square is a string of the form $XY$ where $X$ and $Y$ are two
equal-length strings that have at most $k$ mismatches. Kolpakov and Kucherov
[Theor. Comput. Sci., 2003] defined two notions of $k$-mismatch repeats, called
$k$-repetitions and $k$-runs, each representing a sequence of consecutive
$k$-mismatch squares of equal length. They proposed algorithms for computing
$k$-repetitions and $k$-runs working in $O(nk \log k + output)$ time for a
string of length $n$ over an integer alphabet, where $output$ is the number of
the reported repeats. We show that $output=O(nk \log k)$, both in case of
$k$-repetitions and $k$-runs, which implies that the complexity of their
algorithms is actually $O(nk \log k)$. We apply this result to computing
parameterized squares.
  A parameterized square is a string of the form $XY$ such that $X$ and $Y$
parameterized-match, i.e., there exists a bijection $f$ on the alphabet such
that $f(X) = Y$. Two parameterized squares $XY$ and $X'Y'$ are equivalent if
they parameterized match. Recently Hamai et al. [SPIRE 2024] showed that a
string of length $n$ over an alphabet of size $\sigma$ contains less than
$n\sigma$ non-equivalent parameterized squares, improving an earlier bound by
Kociumaka et al. [Theor. Comput. Sci., 2016]. We apply our bound for
$k$-mismatch repeats to propose an algorithm that reports all non-equivalent
parameterized squares in $O(n\sigma \log \sigma)$ time. We also show that the
number of non-equivalent parameterized squares can be computed in $O(n \log n)$
time. This last algorithm applies to squares under any substring compatible
equivalence relation and also to counting squares that are distinct as strings.
In particular, this improves upon the $O(n\sigma)$-time algorithm of
Gawrychowski et al. [CPM 2023] for counting order-preserving squares that are
distinct as strings if $\sigma = \omega(\log n)$.

</details>


### [140] [A Simple and Fast Reduction from Gomory-Hu Trees to Polylog Maxflows](https://arxiv.org/abs/2509.02520)
*Maximilian Probst Gutenberg,Rasmus Kyng,Weixuan Yuan,Wuwei Yuan*

Main category: cs.DS

TL;DR: 提出从Gomory - Hu树到多对数最大流计算的简单高效归约，适用于无权图、有权图和无权超图，且归约在多对数因子上是紧的。


<details>
  <summary>Details</summary>
Motivation: 寻找从Gomory - Hu树到最大流计算的有效归约方法。

Method: 提出一种归约方法，在无权图上对总实例大小为$	ilde{O}(m)$的图进行最大流计算，该方法也可扩展到有权图和无权超图。

Result: 在无权图上归约所需额外时间为$	ilde{O}(m)$；有权图实例大小和运行时间增至$	ilde{O}(n^2)$；归约方法可扩展到无权超图，且在多对数因子上是紧的。

Conclusion: 所提出的归约方法简单高效，在多对数因子上是紧的，可应用于多种图类型。

Abstract: Given an undirected graph $G=(V,E,w)$, a Gomory-Hu tree $T$ (Gomory and Hu,
1961) is a tree on $V$ that preserves all-pairs mincuts of $G$ exactly.
  We present a simple, efficient reduction from Gomory-Hu trees to polylog
maxflow computations. On unweighted graphs, our reduction reduces to maxflow
computations on graphs of total instance size $\tilde{O}(m)$ and the algorithm
requires only $\tilde{O}(m)$ additional time. Our reduction is the first that
is tight up to polylog factors. The reduction also seamlessly extends to
weighted graphs, however, instance sizes and runtime increase to
$\tilde{O}(n^2)$.
  Finally, we show how to extend our reduction to reduce Gomory-Hu trees for
unweighted hypergraphs to maxflow in hypergraphs. Again, our reduction is the
first that is tight up to polylog factors.

</details>


### [141] [Reusing Samples in Variance Reduction](https://arxiv.org/abs/2509.02526)
*Yujia Jin,Ishani Karmarkar,Aaron Sidford,Jiayi Wang*

Main category: cs.DS

TL;DR: 提出通用框架改进解决结构化优化问题时全批次和样本查询次数的权衡，适用于一类随机优化算法。


<details>
  <summary>Details</summary>
Motivation: 改善结构化优化问题中全批次和样本查询次数的权衡。

Method: 修改随机优化算法以复用子问题间查询输入的随机性，引入伪独立算法概念。

Result: 改进了有限和最小化、顶部特征向量计算、马尔可夫决策过程优化中全批次和样本查询次数的权衡。

Conclusion: 通过复用随机性和引入伪独立算法概念，可有效改善结构化优化问题中全批次和样本查询次数的权衡。

Abstract: We provide a general framework to improve trade-offs between the number of
full batch and sample queries used to solve structured optimization problems.
Our results apply to a broad class of randomized optimization algorithms that
iteratively solve sub-problems to high accuracy. We show that such algorithms
can be modified to reuse the randomness used to query the input across
sub-problems. Consequently, we improve the trade-off between the number of
gradient (full batch) and individual function (sample) queries for finite sum
minimization, the number of matrix-vector multiplies (full batch) and random
row (sample) queries for top-eigenvector computation, and the number of
matrix-vector multiplies with the transition matrix (full batch) and generative
model (sample) queries for optimizing Markov Decision Processes. To facilitate
our analysis we introduce the notion of pseudo-independent algorithms, a
generalization of pseudo-deterministic algorithms [Gat and Goldwasser 2011],
that quantifies how independent the output of a randomized algorithm is from a
randomness source.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [142] [Playing Markov Games Without Observing Payoffs](https://arxiv.org/abs/2509.00179)
*Daniel Ablin,Alon Cohen*

Main category: cs.GT

TL;DR: 本文引入并形式化一类新的零和对称马尔可夫博弈，表明玩家在不观察收益的情况下仍能与对手竞争，算法适用于矩阵和马尔可夫博弈且多项式时间运行。


<details>
  <summary>Details</summary>
Motivation: 优化不确定性下的问题，尤其是多智能体系统中的问题，拓展矩阵博弈对称性到马尔可夫环境。

Method: 引入并形式化新的零和对称马尔可夫博弈，将学习问题转化为在线学习问题。

Result: 玩家在不观察收益但知道转移动态且观察对手行动序列时，能与有完全信息的对手竞争，算法在游戏规模和回合数上多项式时间运行。

Conclusion: 拓宽了在严重信息劣势下可进行鲁棒学习的博弈类别，加深了在线学习与对抗博弈论的联系。

Abstract: Optimization under uncertainty is a fundamental problem in learning and
decision-making, particularly in multi-agent systems. Previously, Feldman,
Kalai, and Tennenholtz [2010] demonstrated the ability to efficiently compete
in repeated symmetric two-player matrix games without observing payoffs, as
long as the opponents actions are observed. In this paper, we introduce and
formalize a new class of zero-sum symmetric Markov games, which extends the
notion of symmetry from matrix games to the Markovian setting. We show that
even without observing payoffs, a player who knows the transition dynamics and
observes only the opponents sequence of actions can still compete against an
adversary who may have complete knowledge of the game. We formalize three
distinct notions of symmetry in this setting and show that, under these
conditions, the learning problem can be reduced to an instance of online
learning, enabling the player to asymptotically match the return of the
opponent despite lacking payoff observations. Our algorithms apply to both
matrix and Markov games, and run in polynomial time with respect to the size of
the game and the number of episodes. Our work broadens the class of games in
which robust learning is possible under severe informational disadvantage and
deepens the connection between online learning and adversarial game theory.

</details>


### [143] [Strategyproof Mechanisms for Facility Location with Prediction Under the Maximum Cost Objective](https://arxiv.org/abs/2509.00439)
*Hau Chan,Jianan Lin,Chenhao Wang*

Main category: cs.GT

TL;DR: 研究学习增强框架下度量空间中设施选址的机制设计问题，分析不同空间下策略证明机制的近似保证和群体策略证明性。


<details>
  <summary>Details</summary>
Motivation: 设计策略证明机制以真实获取代理对设施位置的偏好，利用不完美预测确定设施位置，使所有代理的最大成本近似最小化，并兼顾预测准确时的一致性和不准确时的鲁棒性。

Method: 在实数线上刻画特定性质的确定性策略证明机制，分析二维空间中特定确定性和随机机制的近似保证，讨论机制的群体策略证明性。

Result: 在实数线上，一致性严格小于2且有界鲁棒性的机制为MinMaxP机制，其近似比为(1+min(1, η))且无更好机制；分析了二维空间中机制的近似保证。

Conclusion: 得到了不同空间下策略证明机制的近似保证结果，讨论了群体策略证明性。

Abstract: We study the mechanism design problem of facility location on a metric space
in the learning-augmented framework, where mechanisms have access to an
imperfect prediction of optimal facility locations. Our goal is to design
strategyproof (SP) mechanisms to elicit agent preferences on the facility
locations truthfully and, leveraging the given imperfect prediction, determine
the facility location that approximately minimizes the maximum cost among all
agents. In particular, we seek SP mechanisms whose approximation guarantees
depend on the prediction errors -- achieve improved guarantees when the
prediction is accurate (known as the \emph{consistency}), while still ensuring
robust worst-case performance when the prediction is arbitrarily inaccurate
(known as the \emph{robustness}).
  When the metric space is the real line, we characterize all deterministic SP
mechanisms with consistency strictly less than 2 and bounded robustness: such
mechanisms must be the MinMaxP mechanism, which returns the prediction location
if it lies between the two extreme agent locations and, otherwise, returns the
closest agent location to the prediction. We further show that, for any
prediction error $\eta\ge 0$, while MinMaxP is $(1+\min(1,
\eta))$-approximation, no deterministic SP mechanism can achieve a better
approximation. In two-dimensional spaces with the $l_p$ metric, we analyze the
approximation guarantees of a deterministic mechanism that runs MinMaxP
independently on each coordinate, as well as a randomized mechanism that
selects between two deterministic ones with specific probabilities. Finally, we
discuss the group strategyproofness of the considered mechanisms.

</details>


### [144] [Mean-payoff and Energy Discrete Bidding Games](https://arxiv.org/abs/2509.00506)
*Guy Avni,Suman Sadhukhan*

Main category: cs.GT

TL;DR: 研究离散投标博弈中的平均支付和能量目标，证明阈值预算存在，确定其结构并分析复杂度。


<details>
  <summary>Details</summary>
Motivation: 实际应用促使研究离散投标博弈，当前连续投标博弈已被理解，而离散投标博弈研究不足。

Method: 对离散投标博弈的平均支付和能量目标进行研究，分析阈值预算的存在性和结构。

Result: 确定了阈值预算的存在性，明确了阈值结构，表明在简洁表示的博弈中寻找阈值的复杂度在NP和coNP内。

Conclusion: 研究了离散投标博弈的平均支付和能量目标，为该领域提供了新的理论结果和复杂度分析。

Abstract: A \emph{bidding} game is played on a graph as follows. A token is placed on
an initial vertex and both players are allocated budgets. In each turn, the
players simultaneously submit bids that do not exceed their available budgets,
the higher bidder moves the token, and pays the bid to the lower bidder. We
focus on \emph{discrete}-bidding, which are motivated by practical applications
and restrict the granularity of the players' bids, e.g, bids must be given in
cents. We study, for the first time, discrete-bidding games with {\em
mean-payoff} and {\em energy} objectives. In contrast, mean-payoff {\em
continuous}-bidding games (i.e., no granularity restrictions) are understood
and exhibit a rich mathematical structure. The {\em threshold} budget is a
necessary and sufficient initial budget for winning an energy game or
guaranteeing a target payoff in a mean-payoff game. We first establish
existence of threshold budgets; a non-trivial property due to the concurrent
moves of the players. Moreover, we identify the structure of the thresholds,
which is key in obtaining compact strategies, and in turn, showing that finding
threshold is in \NP~and \coNP even in succinctly-represented games.

</details>


### [145] [Quantum game models for interaction-aware decision-making in automated driving](https://arxiv.org/abs/2509.01582)
*Karim Essalmi,Fernando Garrido,Fawzi Nashashibi*

Main category: cs.GT

TL;DR: 提出QG - U1和QG - G4量子博弈模型用于自动驾驶交互决策，评估显示QG - G4有更低碰撞率和更高成功率，量子模型在特定参数下期望收益更高。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶决策方法忽视或简化与周围智能体的交互，导致车辆行为过于保守，需要解决此问题。

Method: 提出QG - U1和QG - G4量子博弈模型，结合量子力学原理扩展经典博弈论，可在标准计算机实时执行。

Result: QG - G4比基线方法有更低碰撞率和更高成功率，两个量子模型在特定参数设置下比经典博弈方法有更高期望收益。

Conclusion: 提出的量子博弈模型在自动驾驶交互决策方面有更好的效果，能改善传统方法的不足。

Abstract: Decision-making in automated driving must consider interactions with
surrounding agents to be effective. However, traditional methods often neglect
or oversimplify these interactions because they are difficult to model and
solve, which can lead to overly conservative behavior of the ego vehicle. To
address this gap, we propose two quantum game models, QG-U1 (Quantum Game -
Unitary 1) and QG-G4 (Quantum Game - Gates 4), for interaction-aware
decision-making. These models extend classical game theory by incorporating
principles of quantum mechanics, such as superposition, interference, and
entanglement. Specifically, QG-U1 and QG-G4 are designed for two-player games
with two strategies per player and can be executed in real time on a standard
computer without requiring quantum hardware. We evaluate both models in merging
and roundabout scenarios and compare them with classical game-theoretic methods
and baseline approaches (IDM, MOBIL, and a utility-based technique). Results
show that QG-G4 achieves lower collision rates and higher success rates
compared to baseline methods, while both quantum models yield higher expected
payoffs than classical game approaches under certain parameter settings.

</details>


### [146] [Complexity of the Existence of Constrained Secure Equilibria in Multi-Player Games](https://arxiv.org/abs/2509.01870)
*Hiroki Mizuno,Yoshiaki Takata,Hiroyuki Seki*

Main category: cs.GT

TL;DR: 探讨给定多人游戏中带约束的安全均衡是否存在问题的可判定性与复杂性


<details>
  <summary>Details</summary>
Motivation: 安全均衡是纳什均衡的有前景的细化，研究其带约束情况下是否存在问题有意义

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: We consider a multi-player non-zero-sum turn-based game (abbreviated as
multi-player game) on a finite directed graph. A secure equilibrium (SE) is a
strategy profile in which no player has the incentive to deviate from the
strategy because no player can increase her own payoff or lower the payoff of
another player. SE is a promising refinement of Nash equilibrium in which a
player does not care the payoff of another player. In this paper, we discuss
the decidability and complexity of the problem of deciding whether a secure
equilibrium with constraints (a payoff profile specifying which players must
win) exists for a given multi-player game.

</details>


### [147] [Entry Barriers in Content Markets](https://arxiv.org/abs/2509.01953)
*Haiqing Zhu,Lexing Xie,Yun Kuen Cheung*

Main category: cs.GT

TL;DR: 研究在线内容平台设置隐性或显性准入门槛及奖励机制能否提升内容质量，分析两种准入门槛，证明相关方法可提升内容质量。


<details>
  <summary>Details</summary>
Motivation: 在线平台低质量内容普遍，归因于缺乏有意义的准入要求，因此研究准入门槛和奖励机制能否提升内容质量。

Method: 采用博弈论分析在线内容平台的两种准入门槛，一是结构壁垒，二是战略壁垒，并考虑将部分或全部准入费用投入奖励池的方案。

Result: 两种奖励机制在纳什均衡时会产生结构壁垒，将准入费用投入奖励池的方案能提升整体内容质量。

Conclusion: 研究为设计结合准入费用的奖励机制以促进高质量内容和健康在线生态系统奠定理论基础。

Abstract: The prevalence of low-quality content on online platforms is often attributed
to the absence of meaningful entry requirements. This motivates us to
investigate whether implicit or explicit entry barriers, alongside appropriate
reward mechanisms, can enhance content quality. We present the first
game-theoretic analysis of two distinct types of entry barriers in online
content platforms. The first, a structural barrier, emerges from the collective
behaviour of incumbent content providers which disadvantages new entrants. We
show that both rank-order and proportional-share reward mechanisms induce such
a structural barrier at Nash equilibrium. The second, a strategic barrier,
involves the platform proactively imposing entry fees to discourage
participation from low-quality contributors. We consider a scheme in which the
platform redirects some or all of the entry fees into the reward pool. We
formally demonstrate that this approach can improve overall content quality.
Our findings establish a theoretical foundation for designing reward mechanisms
coupled with entry fees to promote higher-quality content and support healthier
online ecosystems.

</details>


### [148] [A Strongly Polynomial-Time Combinatorial Algorithm for the Nucleolus in Convex Games](https://arxiv.org/abs/2509.02380)
*Giacoomo Maggiorano,Alessandro Sosso,Gautier Stauffer*

Main category: cs.GT

TL;DR: 重新审视约简博弈法来计算凸博弈核仁，提出首个组合且强多项式算法。


<details>
  <summary>Details</summary>
Motivation: 核仁计算一般是NP难问题，凸博弈中现有多项式时间算法依赖椭球法，需探索新方法。

Method: 重新审视约简博弈法，开发新算法思路，利用最小核心多面体结构。

Result: 反驳了之前声称的多项式时间实现，阐明其失败原因，表明约简博弈法可有效使用。

Conclusion: 得到计算凸博弈核仁的首个组合且强多项式算法。

Abstract: The nucleolus is a fundamental solution concept in cooperative game theory,
yet computing it is NP-hard in general. In convex games-where players' marginal
contributions grow with coalition size-the only existing polynomial-time
algorithm relies on the ellipsoid method. We re-examine a reduced game
approach, refuting a previously claimed polynomial-time implementation and
clarifying why it fails. By developing new algorithmic ideas and exploiting the
structure of least core polyhedra, we show that reduced games can in fact be
used effectively. This yields the first combinatorial and strongly polynomial
algorithm for computing the nucleolus in convex games.

</details>


### [149] [Harnessing Information in Incentive Design](https://arxiv.org/abs/2509.02493)
*Raj Kiriti Velicheti,Subhonmesh Bose,Tamer Başar*

Main category: cs.GT

TL;DR: 研究激励设计博弈中信息不对称的影响，提出减少不对称的方法及不同情况的解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究激励设计中，面对有信息优势的代理方时，信息不对称对委托方的影响。

Method: 先证明减少信息不对称符合委托方利益，让委托方通过信息设计或付费获取信息来减少不确定性，在矩阵博弈和二次高斯博弈环境中研究。

Result: 引入不确定性会增加委托方成本，让代理方塑造其信念有优势。

Conclusion: 在激励设计博弈中，可通过合适方法减少信息不对称带来的影响。

Abstract: Incentive design deals with interaction between a principal and an agent
where the former can shape the latter's utility through a policy commitment. It
is well known that the principal faces an information rent when dealing with an
agent that has informational advantage. In this work, we embark on a systematic
study of the effect of information asymmetry in incentive design games.
Specifically, we first demonstrate that it is in principal's interest to
decrease this information asymmetry. To mitigate this uncertainty, we let the
principal gather information either by letting the agent shape her belief (aka
Information Design), or by paying to acquire it. Providing solutions to all
these cases we show that while introduction of uncertainty increases the
principal's cost, letting the agent shape its belief can be advantageous. We
study information asymmetry and information acquisition in both matrix games
and quadratic Gaussian game setups.

</details>


### [150] [Selecting Interlacing Committees](https://arxiv.org/abs/2509.02519)
*Chris Dong,Martin Bullinger,Tomasz Wąs,Larry Birnbaum,Edith Elkind*

Main category: cs.GT

TL;DR: 本文丰富委员会选择的标准方法，定义评估委员会与选民连接程度的量化指标以避免极化，针对特定领域获高效算法，还分析与其他代表性目标的兼容性并给出近似算法。


<details>
  <summary>Details</summary>
Motivation: 社会大规模极化常由极化的政治代表引发，现有计算社会选择的委员会选择方法未解决此问题。

Method: 定义两个评估委员会与选民连接程度的量化指标，针对选民 - 候选人区间域的偏好轮廓获得高效算法，分析与其他代表性目标的兼容性，找出近似保证间的权衡。

Result: 在一般情况下对应最大化问题是NP完全的，但在选民 - 候选人区间域有高效算法，还得到能同时实现常数因子近似的算法。

Conclusion: 所提方法可用于避免极化委员会，同时分析了与其他代表性目标的关系及权衡。

Abstract: Polarization is a major concern for a well-functioning society. Often, mass
polarization of a society is driven by polarizing political representation,
even when the latter is easily preventable. The existing computational social
choice methods for the task of committee selection are not designed to address
this issue. We enrich the standard approach to committee selection by defining
two quantitative measures that evaluate how well a given committee
interconnects the voters. Maximizing these measures aims at avoiding polarizing
committees. While the corresponding maximization problems are NP-complete in
general, we obtain efficient algorithms for profiles in the voter-candidate
interval domain. Moreover, we analyze the compatibility of our goals with other
representation objectives, such as excellence, diversity, and proportionality.
We identify trade-offs between approximation guarantees, and describe
algorithms that achieve simultaneous constant-factor approximations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [151] [Bias Mitigation for AI-Feedback Loops in Recommender Systems: A Systematic Literature Review and Taxonomy](https://arxiv.org/abs/2509.00109)
*Theodor Stoecker,Samed Bayer,Ingo Weber*

Main category: cs.IR

TL;DR: 文章进行系统文献综述，筛选347篇论文得24篇研究，构建分类法以指导行业和研究。


<details>
  <summary>Details</summary>
Motivation: 多数偏差缓解技术仅在静态数据上测试，其在多轮再训练中的长期公平性不明，需研究考虑AI反馈循环的偏差缓解方法。

Method: 对考虑AI反馈循环的偏差缓解方法进行系统文献综述，筛选论文并从六个维度编码构建分类法。

Result: 筛选出24篇2019 - 2025年的主要研究，构建了可复用的分类法。

Conclusion: 分类法为行业从业者选方法、研究者明确研究缺口提供指导，指出领域存在共享模拟器缺乏、评估指标不一等问题。

Abstract: Recommender systems continually retrain on user reactions to their own
predictions, creating AI feedback loops that amplify biases and diminish
fairness over time. Despite this well-known risk, most bias mitigation
techniques are tested only on static splits, so their long-term fairness across
multiple retraining rounds remains unclear. We therefore present a systematic
literature review of bias mitigation methods that explicitly consider AI
feedback loops and are validated in multi-round simulations or live A/B tests.
Screening 347 papers yields 24 primary studies published between 2019-2025.
Each study is coded on six dimensions: mitigation technique, biases addressed,
dynamic testing set-up, evaluation focus, application domain, and ML task,
organising them into a reusable taxonomy. The taxonomy offers industry
practitioners a quick checklist for selecting robust methods and gives
researchers a clear roadmap to the field's most urgent gaps. Examples include
the shortage of shared simulators, varying evaluation metrics, and the fact
that most studies report either fairness or performance; only six use both.

</details>


### [152] [Algorithm Adaptation Bias in Recommendation System Online Experiments](https://arxiv.org/abs/2509.00199)
*Chen Zheng,Zhenyu Zhao*

Main category: cs.IR

TL;DR: 本文指出在线实验（A/B测试）评估推荐系统存在算法适应偏差，详述其机制并给出实据，探讨更稳健评估方法。


<details>
  <summary>Details</summary>
Motivation: 在线实验评估推荐系统结果会受多种偏差干扰，算法适应偏差未得到充分研究但很关键，需提高对该偏差的认识并讨论解决方案。

Method: 详细阐述算法适应偏差的机制，展示来自真实世界实验的实证证据。

Result: 明确算法适应偏差会导致实验结果偏向大流量生产变体，低估小流量测试变体性能。

Conclusion: 应提高对算法适应偏差的认识，将其置于更广泛的推荐系统评估偏差中，讨论涵盖实验设计、测量和调整的解决方案，以实现更稳健的在线评估。

Abstract: Online experiments (A/B tests) are widely regarded as the gold standard for
evaluating recommender system variants and guiding launch decisions. However, a
variety of biases can distort the results of the experiment and mislead
decision-making. An underexplored but critical bias is algorithm adaptation
effect. This bias arises from the flywheel dynamics among production models,
user data, and training pipelines: new models are evaluated on user data whose
distributions are shaped by the incumbent system or tested only in a small
treatment group. As a result, the measured effect of a new product change in
modeling and user experience in this constrained experimental setting can
diverge substantially from its true impact in full deployment. In practice, the
experiment results often favor the production variant with large traffic while
underestimating the performance of the test variant with small traffic, which
leads to missing opportunities to launch a true winning arm or underestimating
the impact. This paper aims to raise awareness of algorithm adaptation bias,
situate it within the broader landscape of RecSys evaluation biases, and
motivate discussion of solutions that span experiment design, measurement, and
adjustment. We detail the mechanisms of this bias, present empirical evidence
from real-world experiments, and discuss potential methods for a more robust
online evaluation.

</details>


### [153] [Beyond Negative Transfer: Disentangled Preference-Guided Diffusion for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2509.00389)
*Xiaoxin Ye,Chengkai Huang,Hongtao Huang,Lina Yao*

Main category: cs.IR

TL;DR: 提出DPG - Diff模型用于跨域序列推荐，分解用户偏好，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有跨域序列推荐存在负迁移和噪声问题，现有扩散模型应用有局限，需新方法解决。

Method: 提出DPG - Diff模型，将用户偏好分解为域不变和域特定组件，共同指导反向扩散过程。

Result: 在真实数据集上的大量实验显示，DPG - Diff在多个指标上始终优于现有基线。

Conclusion: DPG - Diff能实现稳健的跨域知识转移，减轻负迁移，过滤序列噪声。

Abstract: Cross-Domain Sequential Recommendation (CDSR) leverages user behaviors across
domains to enhance recommendation quality. However, naive aggregation of
sequential signals can introduce conflicting domain-specific preferences,
leading to negative transfer. While Sequential Recommendation (SR) already
suffers from noisy behaviors such as misclicks and impulsive actions, CDSR
further amplifies this issue due to domain heterogeneity arising from diverse
item types and user intents. The core challenge is disentangling three
intertwined signals: domain-invariant preferences, domain-specific preferences,
and noise. Diffusion Models (DMs) offer a generative denoising framework
well-suited for disentangling complex user preferences and enhancing robustness
to noise. Their iterative refinement process enables gradual denoising, making
them effective at capturing subtle preference signals. However, existing
applications in recommendation face notable limitations: sequential DMs often
conflate shared and domain-specific preferences, while cross-domain
collaborative filtering DMs neglect temporal dynamics, limiting their ability
to model evolving user preferences. To bridge these gaps, we propose
\textbf{DPG-Diff}, a novel Disentangled Preference-Guided Diffusion Model, the
first diffusion-based approach tailored for CDSR, to or best knowledge.
DPG-Diff decomposes user preferences into domain-invariant and domain-specific
components, which jointly guide the reverse diffusion process. This
disentangled guidance enables robust cross-domain knowledge transfer, mitigates
negative transfer, and filters sequential noise. Extensive experiments on
real-world datasets demonstrate that DPG-Diff consistently outperforms
state-of-the-art baselines across multiple metrics.

</details>


### [154] [ERank: Fusing Supervised Fine-Tuning and Reinforcement Learning for Effective and Efficient Text Reranking](https://arxiv.org/abs/2509.00520)
*Yuzheng Cai,Yanzhao Zhang,Dingkun Long,Mingxin Li,Pengjun Xie,Weiguo Zheng*

Main category: cs.IR

TL;DR: 本文介绍高效点式重排器 ERank，提出两阶段训练管道，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前大语言模型驱动的重排器在相关性判别和效率上的权衡问题。

Method: 提出两阶段训练管道，先进行监督微调输出细粒度分数，再用强化学习结合列表式奖励进一步优化。

Result: 在多个基准测试中展示出优越的有效性和鲁棒性，如在 BRIGHT 基准上，ERank - 4B 的 nDCG@10 达 38.7，32B 变体达 40.2。

Conclusion: ERank 是一种高效且有效的点式重排器，在不同相关性场景中表现出色。

Abstract: Text reranking models are a crucial component in modern systems like
Retrieval-Augmented Generation, tasked with selecting the most relevant
documents prior to generation. However, current Large Language Models (LLMs)
powered rerankers often face a fundamental trade-off. On one hand, Supervised
Fine-Tuning based pointwise methods that frame relevance as a binary
classification task lack the necessary scoring discrimination, particularly for
those built on reasoning LLMs. On the other hand, approaches designed for
complex reasoning often employ powerful yet inefficient listwise formulations,
rendering them impractical for low latency applications. To resolve this
dilemma, we introduce ERank, a highly effective and efficient pointwise
reranker built from a reasoning LLM that excels across diverse relevance
scenarios. We propose a novel two-stage training pipeline that begins with
Supervised Fine-Tuning (SFT). In this stage, we move beyond binary labels and
train the model generatively to output fine grained integer scores, which
significantly enhances relevance discrimination. The model is then further
refined using Reinforcement Learning (RL) with a novel, listwise derived
reward. This technique instills global ranking awareness into the efficient
pointwise architecture. We evaluate the ERank reranker on the BRIGHT, FollowIR,
TREC DL, and BEIR benchmarks, demonstrating superior effectiveness and
robustness compared to existing approaches. On the reasoning-intensive BRIGHT
benchmark, our ERank-4B achieves an nDCG@10 of 38.7, while a larger 32B variant
reaches a state of the art nDCG@10 of 40.2.

</details>


### [155] [A Survey on Open Dataset Search in the LLM Era: Retrospectives and Perspectives](https://arxiv.org/abs/2509.00728)
*Pengyue Li,Sheng Wang,Hua Dai,Zhiyu Chen,Zhifeng Bao,Brian D. Davison*

Main category: cs.IR

TL;DR: 本文对开放数据集搜索问题进行系统综述，介绍超越传统方法的进展，强调大语言模型与搜索的关系，总结开放问题与未来方向。


<details>
  <summary>Details</summary>
Motivation: 高质量数据集在数据驱动任务中很重要，开放数据集搜索是关键研究挑战，需系统全面回顾该问题。

Method: 从数据集模态角度，关注基于示例的数据集搜索、基于内容的相似度测量技术、搜索加速技术，探讨大语言模型与开放数据集搜索的关系。

Result: 阐述了大语言模型能助力开放数据集搜索，而数据集搜索进展也能支持大语言模型。

Conclusion: 总结了开放研究问题和未来方向，为相关研究者和从业者提供结构化参考。

Abstract: High-quality datasets are typically required for accomplishing data-driven
tasks, such as training medical diagnosis models, predicting real-time traffic
conditions, or conducting experiments to validate research hypotheses.
Consequently, open dataset search, which aims to ensure the efficient and
accurate fulfillment of users' dataset requirements, has emerged as a critical
research challenge and has attracted widespread interest. Recent studies have
made notable progress in enhancing the flexibility and intelligence of open
dataset search, and large language models (LLMs) have demonstrated strong
potential in addressing long-standing challenges in this area. Therefore, a
systematic and comprehensive review of the open dataset search problem is
essential, detailing the current state of research and exploring future
directions. In this survey, we focus on recent advances in open dataset search
beyond traditional approaches that rely on metadata and keywords. From the
perspective of dataset modalities, we place particular emphasis on
example-based dataset search, advanced similarity measurement techniques based
on dataset content, and efficient search acceleration techniques. In addition,
we emphasize the mutually beneficial relationship between LLMs and open dataset
search. On the one hand, LLMs help address complex challenges in query
understanding, semantic modeling, and interactive guidance within open dataset
search. In turn, advances in dataset search can support LLMs by enabling more
effective integration into retrieval-augmented generation (RAG) frameworks and
data selection processes, thereby enhancing downstream task performance.
Finally, we summarize open research problems and outline promising directions
for future work. This work aims to offer a structured reference for researchers
and practitioners in the field of open dataset search.

</details>


### [156] [HiPS: Hierarchical PDF Segmentation of Textbooks](https://arxiv.org/abs/2509.00909)
*Sabine Wehnert,Harikrishnan Changaramkulath,Ernesto William De Luca*

Main category: cs.IR

TL;DR: 文章针对复杂结构化文档的分层分割挑战，研究基于目录和无明确目录输入的解析技术，结合预处理策略提升解析准确性，结果表明结合大语言模型和结构感知预处理可改善提取质量，还对方法进行比较评估。


<details>
  <summary>Details</summary>
Motivation: 当前主要用于研究论文分割的方法难以满足解析PDF格式文本（如教科书）的需求，文章聚焦复杂结构化文档尤其是法律教科书的分层分割挑战。

Method: 研究基于目录的技术和无明确目录输入的开源结构解析工具或大语言模型方法，结合OCR标题检测、XML特征和上下文文本特征等预处理策略，并评估其识别标题、分配层级和确定边界的能力。

Result: 结合大语言模型和结构感知预处理可大幅减少误报并提高提取质量，当PDF标题元数据质量高时，基于目录的技术表现出色。

Conclusion: 对各种方法进行比较评估，指出各自优缺点。

Abstract: The growing demand for effective tools to parse PDF-formatted texts,
particularly structured documents such as textbooks, reveals the limitations of
current methods developed mainly for research paper segmentation. This work
addresses the challenge of hierarchical segmentation in complex structured
documents, with a focus on legal textbooks that contain layered knowledge
essential for interpreting and applying legal norms. We examine a Table of
Contents (TOC)-based technique and approaches that rely on open-source
structural parsing tools or Large Language Models (LLMs) operating without
explicit TOC input. To enhance parsing accuracy, we incorporate preprocessing
strategies such as OCR-based title detection, XML-derived features, and
contextual text features. These strategies are evaluated based on their ability
to identify section titles, allocate hierarchy levels, and determine section
boundaries. Our findings show that combining LLMs with structure-aware
preprocessing substantially reduces false positives and improves extraction
quality. We also find that when the metadata quality of headings in the PDF is
high, TOC-based techniques perform particularly well. All code and data are
publicly available to support replication. We conclude with a comparative
evaluation of the methods, outlining their respective strengths and
limitations.

</details>


### [157] [Food Data in the Semantic Web: A Review of Nutritional Resources, Knowledge Graphs, and Emerging Applications](https://arxiv.org/abs/2509.00986)
*Darko Sasanski,Riste Stojanov*

Main category: cs.IR

TL;DR: 本文全面回顾语义网中的食品数据，涵盖关键营养资源、知识图谱及应用，分析资源贡献、技术并探讨知识图谱作用，为食品领域语义技术发展提供见解。


<details>
  <summary>Details</summary>
Motivation: 探索语义网中的食品数据，推动语义技术在食品领域的应用和发展。

Method: 对USDA、FoodOn等食品数据资源进行研究，聚焦食品实体链接和识别技术，探讨食品知识图谱的作用和应用。

Result: 明确了主要食品数据资源的贡献，分析了食品实体链接和识别技术的作用，探讨了食品知识图谱在多个方面的应用。

Conclusion: 通过综合当前进展和识别挑战，为食品领域利用语义技术的未来发展提供指导。

Abstract: This comprehensive review explores food data in the Semantic Web,
highlighting key nutritional resources, knowledge graphs, and emerging
applications in the food domain. It examines prominent food data resources such
as USDA, FoodOn, FooDB, and Recipe1M+, emphasizing their contributions to
nutritional data representation. Special focus is given to food entity linking
and recognition techniques, which enable integration of heterogeneous food data
sources into cohesive semantic resources. The review further discusses food
knowledge graphs, their role in semantic interoperability, data enrichment, and
knowledge extraction, and their applications in personalized nutrition,
ingredient substitution, food-drug and food-disease interactions, and
interdisciplinary research. By synthesizing current advancements and
identifying challenges, this work provides insights to guide future
developments in leveraging semantic technologies for the food domain.

</details>


### [158] [Identifying Origins of Place Names via Retrieval Augmented Generation](https://arxiv.org/abs/2509.01030)
*Alexis Horde Vo,Matt Duckham,Estrid He,Rafe Benli*

Main category: cs.IR

TL;DR: 文章介绍利用自然语言处理和语言模型自动识别地名起源的方法，指出面临的挑战及对地理信息检索的影响。


<details>
  <summary>Details</summary>
Motivation: 当前地名缺乏起源信息，人工丰富耗时，希望利用自然语言处理和语言模型实现自动化识别地名起源。

Method: 提出检索增强生成管道，在DBpedia上搜索地名起源，先提取可能相关子图，再用微调的基于语言模型的模型（ColBERTv2和Llama2）对提取的子图排序得出最终答案。

Result: 指出自动检索地名起源面临的关键挑战，特别是语言模型不太利用文本中的空间信息作为判别因素。

Conclusion: 该方法对地理信息检索有更广泛的影响。

Abstract: Who is the "Batman" behind "Batman Street" in Melbourne? Understanding the
historical, cultural, and societal narratives behind place names can reveal the
rich context that has shaped a community. Although place names serve as
essential spatial references in gazetteers, they often lack information about
place name origins. Enriching these place names in today's gazetteers is a
time-consuming, manual process that requires extensive exploration of a vast
archive of documents and text sources. Recent advances in natural language
processing and language models (LMs) hold the promise of significant automation
of identifying place name origins due to their powerful capability to exploit
the semantics of the stored documents. This chapter presents a retrieval
augmented generation pipeline designed to search for place name origins over a
broad knowledge base, DBpedia. Given a spatial query, our approach first
extracts sub-graphs that may contain knowledge relevant to the query; then
ranks the extracted sub-graphs to generate the final answer to the query using
fine-tuned LM-based models (i.e., ColBERTv2 and Llama2). Our results highlight
the key challenges facing automated retrieval of place name origins, especially
the tendency of language models to under-use the spatial information contained
in texts as a discriminating factor. Our approach also frames the wider
implications for geographic information retrieval using retrieval augmented
generation.

</details>


### [159] [Beyond the Surface: A Solution-Aware Retrieval Model for Competition-level Code Generation](https://arxiv.org/abs/2509.01129)
*Shiwen Zhang,Lingxiang Wang,Hainan Zhang,Ziwei Wang,Sijia Wen,Zhiming Zheng*

Main category: cs.IR

TL;DR: 本文提出用于竞赛编程任务的解决方案感知排序模型SolveRank，利用合成数据训练，实验表明其优于SOTA方法并提升难题代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成模型易受无关叙述干扰，现有检索模型忽视深层解决方案逻辑相似性，设计能准确识别和检索问题及代码的排序模型是竞赛代码生成的迫切研究问题。

Method: 提出SolveRank模型，利用DeepSeek - R1生成逻辑等价但表述不同的新问题，用GPT - 4o验证解决方案一致性，以这些为正样本、BM25/随机检索问题为负样本训练，推理时从语料库中检索相关问题和代码辅助下游代码生成器。

Result: 在xCodeEval数据集实验显示，SolveRank在精确率和召回率指标上优于SOTA排序方法，提升难题代码生成性能。

Conclusion: SolveRank模型能有效解决现有模型在竞赛编程任务中的问题，提升代码生成表现。

Abstract: In competitive programming task, problem statements are often embedded within
elaborate narrative backgrounds, requiring deep understanding of the underlying
solutions to successfully complete the tasks. Current code generation models
primarily focus on token-level semantic modeling, highly susceptible to
distractions from irrelevant narrative statements. Inspired by RAG, retrieving
reference code with similar solutions may help enhance model performance on
difficult problems. However, existing retrieval models also emphasize
surface-level semantic similarity, neglecting the deeper solution-level logical
similarities that are critical in competitive programming. Therefore, designing
ranking models capable of accurately identifying and retrieving problems and
corresponding codes remains an urgent research problem in competitive code
generation. In this paper, we propose SolveRank, a solution-aware ranking model
empowered by synthetic data for competitive programming tasks. Specifically, we
leverage the DeepSeek-R1 model to generate logically equivalent but differently
phrased new problems, verified by GPT-4o for solution consistency. Then, we
train SolveRank with these as positive samples and BM25/random-retrieved
problems as negatives. During inference, SolveRank retrieves relevant problems
and corresponding code from the corpus to assist a downstream code generator.
Experiments on the xCodeEval dataset demonstrate that SolveRank outperforms
SOTA ranking methods in precision and recall metrics, and boosts code
generation performance for difficult problems.

</details>


### [160] [MARS: Modality-Aligned Retrieval for Sequence Augmented CTR Prediction](https://arxiv.org/abs/2509.01184)
*Yutian Xiao,Shukuan Wang,Binhao Wang,Zhao Zhang,Yanze Zhang,Shanqi Liu,Chao Feng,Xiang Li,Fuzhen Zhuang*

Main category: cs.IR

TL;DR: 提出MARS框架解决CTR预测中低活跃用户交互稀疏问题，经实验验证效果好且已部署。


<details>
  <summary>Details</summary>
Motivation: 当前CTR模型受交互稀疏性限制，尤其在低活跃用户场景，现有数据增强方法忽略物品多模态特征。

Method: 利用基于Stein核的方法将文本和图像特征对齐到统一语义空间构建多模态用户嵌入，通过多模态用户嵌入检索、过滤和集中高活跃用户最相似行为序列来增强低活跃用户行为序列。

Result: MARS框架在离线实验和在线A/B测试中始终优于最先进基线，在快手核心业务指标上实现显著增长。

Conclusion: MARS框架有效解决问题，已成功部署服务数亿用户，代码开源确保可复现性。

Abstract: Click-through rate (CTR) prediction serves as a cornerstone of recommender
systems. Despite the strong performance of current CTR models based on user
behavior modeling, they are still severely limited by interaction sparsity,
especially in low-active user scenarios. To address this issue, data
augmentation of user behavior is a promising research direction. However,
existing data augmentation methods heavily rely on collaborative signals while
overlooking the rich multimodal features of items, leading to insufficient
modeling of low-active users.
  To alleviate this problem, we propose a novel framework \textbf{MARS}
(\textbf{M}odality-\textbf{A}ligned \textbf{R}etrieval for \textbf{S}equence
Augmented CTR Prediction). MARS utilizes a Stein kernel-based approach to align
text and image features into a unified and unbiased semantic space to construct
multimodal user embeddings. Subsequently, each low-active user's behavior
sequence is augmented by retrieving, filtering, and concentrating the most
similar behavior sequence of high-active users via multimodal user embeddings.
Validated by extensive offline experiments and online A/B tests, our framework
MARS consistently outperforms state-of-the-art baselines and achieves
substantial growth on core business metrics within
Kuaishou~\footnote{https://www.kuaishou.com/}. Consequently, MARS has been
successfully deployed, serving the main traffic for hundreds of millions of
users. To ensure reproducibility, we provide anonymous access to the
implementation code~\footnote{https://github.com/wangshukuan/MARS}.

</details>


### [161] [Re3: Learning to Balance Relevance & Recency for Temporal Information Retrieval](https://arxiv.org/abs/2509.01306)
*Jiawei Cao,Jie Ouyang,Zhaomeng Zhou,Mingyue Cheng,Yupeng Li,Jiaxian Yan,Qi Liu*

Main category: cs.IR

TL;DR: 介绍Re2Bench基准和Re3框架以解决时间信息检索问题，Re3在基准测试中取得SOTA结果，工作推动时间感知检索系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有时间信息检索方法常孤立处理相关性和时效性挑战，在二者交织场景易失败，需更好方案和评估方法。

Method: 引入Re2Bench基准用于解耦和评估相关性、时效性及其混合情况；提出Re3统一轻量级框架，通过查询感知门控机制动态平衡语义和时间信息。

Result: Re3在Re2Bench上取得SOTA结果，R@1在三个子集均领先，消融研究和骨干敏感性测试证明其鲁棒性和泛化能力。

Conclusion: 该工作提供可泛化解决方案和评估套件，推动时间感知检索系统发展。

Abstract: Temporal Information Retrieval (TIR) is a critical yet unresolved task for
modern search systems, retrieving documents that not only satisfy a query's
information need but also adhere to its temporal constraints. This task is
shaped by two challenges: Relevance, ensuring alignment with the query's
explicit temporal requirements, and Recency, selecting the freshest document
among multiple versions. Existing methods often address the two challenges in
isolation, relying on brittle heuristics that fail in scenarios where temporal
requirements and staleness resistance are intertwined. To address this gap, we
introduce Re2Bench, a benchmark specifically designed to disentangle and
evaluate Relevance, Recency, and their hybrid combination. Building on this
foundation, we propose Re3, a unified and lightweight framework that
dynamically balances semantic and temporal information through a query-aware
gating mechanism. On Re2Bench, Re3 achieves state-of-the-art results, leading
in R@1 across all three subsets. Ablation studies with backbone sensitivity
tests confirm robustness, showing strong generalization across diverse encoders
and real-world settings. This work provides both a generalizable solution and a
principled evaluation suite, advancing the development of temporally aware
retrieval systems. Re3 and Re2Bench are available online:
https://anonymous.4open.science/r/Re3-0C5A

</details>


### [162] [AI4DiTraRe: Building the BFO-Compliant Chemotion Knowledge Graph](https://arxiv.org/abs/2509.01536)
*Ebrahim Norouzi,Nicole Jung,Anna M. Jacyszyn,Jörg Waitelonis,Harald Sack*

Main category: cs.IR

TL;DR: 本文介绍构建符合BFO的Chemotion知识图谱的语义管道，以实现化学研究数据的本体驱动集成表示，遵循FAIR原则，支持AI驱动的化学发现与推理。


<details>
  <summary>Details</summary>
Motivation: 化学实验流程复杂，需要有效整合化学结构信息，构建知识图谱以支持AI驱动的发现和推理，遵循FAIR原则。

Method: 从Chemotion API以JSON - LD格式收集实验元数据，转换为RDF，通过SPARQL CONSTRUCT查询转换为与基本形式本体对齐的图。

Result: 构建了Chemotion知识图谱，源代码和数据集在GitHub公开，图谱由FIZ Karlsruhe信息服务工程托管。

Conclusion: 成果在Leibniz科学园区项目中取得，是正在进行的跨学科合作的一部分。

Abstract: Chemistry is an example of a discipline where the advancements of technology
have led to multi-level and often tangled and tricky processes ongoing in the
lab. The repeatedly complex workflows are combined with information from
chemical structures, which are essential to understand the scientific process.
An important tool for many chemists is Chemotion, which consists of an
electronic lab notebook and a repository. This paper introduces a semantic
pipeline for constructing the BFO-compliant Chemotion Knowledge Graph,
providing an integrated, ontology-driven representation of chemical research
data. The Chemotion-KG has been developed to adhere to the FAIR (Findable,
Accessible, Interoperable, Reusable) principles and to support AI-driven
discovery and reasoning in chemistry. Experimental metadata were harvested from
the Chemotion API in JSON-LD format, converted into RDF, and subsequently
transformed into a Basic Formal Ontology-aligned graph through SPARQL CONSTRUCT
queries. The source code and datasets are publicly available via GitHub. The
Chemotion Knowledge Graph is hosted by FIZ Karlsruhe Information Service
Engineering. Outcomes presented in this work were achieved within the Leibniz
Science Campus ``Digital Transformation of Research'' (DiTraRe) and are part of
an ongoing interdisciplinary collaboration.

</details>


### [163] [Ultra Fast Warm Start Solution for Graph Recommendations](https://arxiv.org/abs/2509.01549)
*Viacheslav Yusupov,Maxim Rakhuba,Evgeny Frolov*

Main category: cs.IR

TL;DR: 提出一种线性方法为可扩展的基于图的推荐系统UltraGCN更新推荐，速度快、效果好且可扩展性高。


<details>
  <summary>Details</summary>
Motivation: 在大量新数据和用户偏好变化的情况下，维持推荐的相关性。

Method: 将简单有效的低秩近似方法应用于基于图的模型。

Result: 能即时给出推荐，速度比传统方法快达30倍，且推荐质量有提升，在大型目录数据集上也有高可扩展性。

Conclusion: 所提出的线性方法在更新推荐上快速有效且可扩展。

Abstract: In this work, we present a fast and effective Linear approach for updating
recommendations in a scalable graph-based recommender system UltraGCN. Solving
this task is extremely important to maintain the relevance of the
recommendations under the conditions of a large amount of new data and changing
user preferences. To address this issue, we adapt the simple yet effective
low-rank approximation approach to the graph-based model. Our method delivers
instantaneous recommendations that are up to 30 times faster than conventional
methods, with gains in recommendation quality, and demonstrates high
scalability even on the large catalogue datasets.

</details>


### [164] [Cloud-Device Collaborative Agents for Sequential Recommendation](https://arxiv.org/abs/2509.01551)
*Jing Long,Sirui Huang,Huan Huo,Tong Chen,Hongzhi Yin,Guandong Xu*

Main category: cs.IR

TL;DR: 提出云-设备协作框架CDA4Rec用于顺序推荐，经实验验证其在准确性和效率上优于基线。


<details>
  <summary>Details</summary>
Motivation: 基于云的大语言模型代理有隐私、实时信号获取和可扩展性问题，设备端代理缺乏计算能力，为弥补两者局限而研究。

Method: 提出CDA4Rec框架，将推荐任务分解为子任务，根据计算需求和隐私敏感度分配到云或设备，用策略规划机制生成个性化执行计划。

Result: 在多个真实数据集上的实验表明，CDA4Rec在准确性和效率上始终优于竞争基线。

Conclusion: CDA4Rec在异构和资源受限环境中有效。

Abstract: Recent advances in large language models (LLMs) have enabled agent-based
recommendation systems with strong semantic understanding and flexible
reasoning capabilities. While LLM-based agents deployed in the cloud offer
powerful personalization, they often suffer from privacy concerns, limited
access to real-time signals, and scalability bottlenecks. Conversely, on-device
agents ensure privacy and responsiveness but lack the computational power for
global modeling and large-scale retrieval. To bridge these complementary
limitations, we propose CDA4Rec, a novel Cloud-Device collaborative framework
for sequential Recommendation, powered by dual agents: a cloud-side LLM and a
device-side small language model (SLM). CDA4Rec tackles the core challenge of
cloud-device coordination by decomposing the recommendation task into modular
sub-tasks including semantic modeling, candidate retrieval, structured user
modeling, and final ranking, which are allocated to cloud or device based on
computational demands and privacy sensitivity. A strategy planning mechanism
leverages the cloud agent's reasoning ability to generate personalized
execution plans, enabling context-aware task assignment and partial parallel
execution across agents. This design ensures real-time responsiveness, improved
efficiency, and fine-grained personalization, even under diverse user states
and behavioral sparsity. Extensive experiments across multiple real-world
datasets demonstrate that CDA4Rec consistently outperforms competitive
baselines in both accuracy and efficiency, validating its effectiveness in
heterogeneous and resource-constrained environments.

</details>


### [165] [CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching in Emerging E-commerce Markets](https://arxiv.org/abs/2509.01566)
*Yujing Wang,Yiren Chen,Huoran Li,Chunxu Xu,Yuchong Luo,Xianghui Mao,Cong Li,Lun Du,Chunyang Ma,Qiqi Jiang,Yin Wang,Fan Gao,Wenting Mo,Pei Wen,Shantanu Kumar,Taejin Park,Yiwei Song,Vijay Rajaram,Tao Cheng,Sonu Durgia,Pranam Kolari*

Main category: cs.IR

TL;DR: 论文介绍Coupang应对新兴电商市场冷启动相关性匹配问题的经验，提出CSRM框架，实验证明其有效并实现部署和显著线上收益。


<details>
  <summary>Details</summary>
Motivation: 全球电商平台扩张，企业进入新市场面临因人类标签和用户行为有限导致的冷启动挑战，需提升相关性匹配冷启动性能。

Method: 提出CSRM框架，利用多语言大语言模型，通过机器翻译任务激活跨语言迁移学习能力、基于检索的查询增强提升查询理解和融入电商知识、多轮自蒸馏训练策略减轻训练标签错误影响。

Result: 实验证明CSRM - LLM和提出的技术有效，实现成功的实际部署，缺陷率降低45.8%，会话购买率提升0.866%。

Conclusion: CSRM框架及相关技术能有效应对新兴电商市场冷启动相关性匹配问题，取得显著线上收益。

Abstract: As global e-commerce platforms continue to expand, companies are entering new
markets where they encounter cold-start challenges due to limited human labels
and user behaviors. In this paper, we share our experiences in Coupang to
provide a competitive cold-start performance of relevance matching for emerging
e-commerce markets. Specifically, we present a Cold-Start Relevance Matching
(CSRM) framework, utilizing a multilingual Large Language Model (LLM) to
address three challenges: (1) activating cross-lingual transfer learning
abilities of LLMs through machine translation tasks; (2) enhancing query
understanding and incorporating e-commerce knowledge by retrieval-based query
augmentation; (3) mitigating the impact of training label errors through a
multi-round self-distillation training strategy. Our experiments demonstrate
the effectiveness of CSRM-LLM and the proposed techniques, resulting in
successful real-world deployment and significant online gains, with a 45.8%
reduction in defect ratio and a 0.866% uplift in session purchase rate.

</details>


### [166] [Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs](https://arxiv.org/abs/2509.02017)
*Yuhao Wang,Junwei Pan,Xinhang Li,Maolin Wang,Yuan Wang,Yue Liu,Dapeng Liu,Jie Jiang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 文章指出现有基于大语言模型的序列推荐方法存在嵌入崩溃和灾难性遗忘问题，提出MME - SID框架，结合多模态和量化嵌入，用MM - RQ - VAE和LoRA等方法解决问题，实验验证其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的序列推荐方法存在嵌入崩溃和灾难性遗忘问题，影响模型可扩展性和推荐性能。

Method: 引入MME - SID框架，结合多模态和量化嵌入；提出MM - RQ - VAE，使用最大平均差异作为重建损失和对比学习进行对齐；用训练好的多模态代码嵌入初始化模型；用LoRA以多模态频率感知融合方式微调大语言模型。

Result: 在三个公开数据集上的实验验证了MME - SID能缓解嵌入崩溃和灾难性遗忘问题，性能优越。

Conclusion: MME - SID框架能有效解决现有基于大语言模型的序列推荐方法的问题，提升推荐性能，代码和数据集公开可复现。

Abstract: Sequential recommendation (SR) aims to capture users' dynamic interests and
sequential patterns based on their historical interactions. Recently, the
powerful capabilities of large language models (LLMs) have driven their
adoption in SR. However, we identify two critical challenges in existing
LLM-based SR methods: 1) embedding collapse when incorporating pre-trained
collaborative embeddings and 2) catastrophic forgetting of quantized embeddings
when utilizing semantic IDs. These issues dampen the model scalability and lead
to suboptimal recommendation performance. Therefore, based on LLMs like
Llama3-8B-instruct, we introduce a novel SR framework named MME-SID, which
integrates multimodal embeddings and quantized embeddings to mitigate embedding
collapse. Additionally, we propose a Multimodal Residual Quantized Variational
Autoencoder (MM-RQ-VAE) with maximum mean discrepancy as the reconstruction
loss and contrastive learning for alignment, which effectively preserve
intra-modal distance information and capture inter-modal correlations,
respectively. To further alleviate catastrophic forgetting, we initialize the
model with the trained multimodal code embeddings. Finally, we fine-tune the
LLM efficiently using LoRA in a multimodal frequency-aware fusion manner.
Extensive experiments on three public datasets validate the superior
performance of MME-SID thanks to its capability to mitigate embedding collapse
and catastrophic forgetting. The implementation code and datasets are publicly
available for reproduction:
https://github.com/Applied-Machine-Learning-Lab/MME-SID.

</details>


### [167] [Towards Multi-Aspect Diversification of News Recommendations Using Neuro-Symbolic AI for Individual and Societal Benefit](https://arxiv.org/abs/2509.02220)
*Markus Reiter-Haas,Elisabeth Lex*

Main category: cs.IR

TL;DR: 本文引入多方面新闻多样化推荐模式，提出结合符号与亚符号AI的研究方向，计划用用户研究评估模型，有望带来积极影响。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要关注新闻多样性的特定方面，新闻推荐复杂，需多方面多样化。

Method: 结合符号和亚符号人工智能，利用知识图谱和规则学习，通过用户研究评估模型。

Result: 未提及具体研究结果。

Conclusion: 提出的研究方向有望平衡新闻消费，为用户和社会带来积极影响。

Abstract: News recommendations are complex, with diversity playing a vital role. So
far, existing literature predominantly focuses on specific aspects of news
diversity, such as viewpoints. In this paper, we introduce multi-aspect
diversification in four distinct recommendation modes and outline the nuanced
challenges in diversifying lists, sequences, summaries, and interactions. Our
proposed research direction combines symbolic and subsymbolic artificial
intelligence, leveraging both knowledge graphs and rule learning. We plan to
evaluate our models using user studies to not only capture behavior but also
their perceived experience. Our vision to balance news consumption points to
other positive effects for users (e.g., increased serendipity) and society
(e.g., decreased polarization).

</details>


### [168] [Application Of Large Language Models For The Extraction Of Information From Particle Accelerator Technical Documentation](https://arxiv.org/abs/2509.02227)
*Qing Dai,Rasmus Ischebeck,Maruisz Sapinski,Adam Grycner*

Main category: cs.IR

TL;DR: 本文探讨用大语言模型（LLMs）从粒子加速器技术文档中提取信息，展示初步成果，指出局限并提出改进策略，强调其在保存专业知识方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统加速器系统技术文档多，经验丰富人员退休，急需有效方法保存和转移专业知识。

Method: 利用大语言模型对粒子加速器技术文档进行信息提取。

Result: 评估显示大语言模型在知识提取、总结和组织方面有效，降低人员退休导致知识流失风险。

Conclusion: 大语言模型在保存机构知识和确保专业领域连续性方面有重要作用，同时指出当前模型存在局限性并需改进。

Abstract: The large set of technical documentation of legacy accelerator systems,
coupled with the retirement of experienced personnel, underscores the urgent
need for efficient methods to preserve and transfer specialized knowledge. This
paper explores the application of large language models (LLMs), to automate and
enhance the extraction of information from particle accelerator technical
documents. By exploiting LLMs, we aim to address the challenges of knowledge
retention, enabling the retrieval of domain expertise embedded in legacy
documentation. We present initial results of adapting LLMs to this specialized
domain. Our evaluation demonstrates the effectiveness of LLMs in extracting,
summarizing, and organizing knowledge, significantly reducing the risk of
losing valuable insights as personnel retire. Furthermore, we discuss the
limitations of current LLMs, such as interpretability and handling of rare
domain-specific terms, and propose strategies for improvement. This work
highlights the potential of LLMs to play a pivotal role in preserving
institutional knowledge and ensuring continuity in highly specialized fields.

</details>


### [169] [Leveraging Media Frames to Improve Normative Diversity in News Recommendations](https://arxiv.org/abs/2509.02266)
*Sourabh Dattawad,Agnese Daffara,Tanise Ceron*

Main category: cs.IR

TL;DR: 论文提出基于新闻框架的推荐系统多样化方法，用规范指标评估其影响，实验显示能提升未点击框架曝光，增强多层面多样化。


<details>
  <summary>Details</summary>
Motivation: 现有基于点击的新闻推荐系统限制文章多样性，基于方面的多样化方法忽略新闻框架的作用。

Method: 将媒体框架作为推荐流程中可控方面，基于框架多样性选择文章，并使用规范多样性指标评估集成框架多样性的新闻推荐系统的影响。

Result: 基于媒体框架多样化的实验显示，未点击框架的曝光率提升达50%，该方法还增强了类别和情感层面的多样化。

Conclusion: 新闻框架是增强规范多样性的有力控制手段，引入新框架能拓宽用户对问题和观点的理解。

Abstract: Click-based news recommender systems suggest users content that aligns with
their existing history, limiting the diversity of articles they encounter.
Recent advances in aspect-based diversification -- adding features such as
sentiments or news categories (e.g. world, politics) -- have made progress
toward diversifying recommendations in terms of perspectives. However, these
approaches often overlook the role of news framing, which shapes how stories
are told by emphasizing specific angles or interpretations. In this paper, we
treat media frames as a controllable aspect within the recommendation pipeline.
By selecting articles based on a diversity of frames, our approach emphasizes
varied narrative angles and broadens the interpretive space recommended to
users. In addition to introducing frame-based diversification method, our work
is the first to assess the impact of a news recommender system that integrates
frame diversity using normative diversity metrics: representation, calibration,
and activation. Our experiments based on media frame diversification show an
improvement in exposure to previously unclicked frames up to 50%. This is
important because repeated exposure to the same frames can reinforce existing
biases or narrow interpretations, whereas introducing novel frames broadens
users' understanding of issues and perspectives. The method also enhances
diversification across categorical and sentiment levels, thereby demonstrating
that framing acts as a strong control lever for enhancing normative diversity.

</details>


### [170] [Upcycling Candidate Tokens of Large Language Models for Query Expansion](https://arxiv.org/abs/2509.02377)
*Jinseok Kim,Sukmin Cho,Soyeong Jeong,Sangyeop Kim,Sungzoon Cho*

Main category: cs.IR

TL;DR: 提出CTQE方法用于查询扩展，利用未选候选令牌，在不增加推理的情况下实现相关性和多样性，实验表明其性能强且成本低。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的查询扩展方法存在生成多样术语提升性能但增加计算成本的权衡问题。

Method: 提出Candidate Token Query Expansion (CTQE)方法，从单次大语言模型解码过程中利用未选候选令牌提取多样且相关的术语。

Result: CTQE在检索任务中表现出色，成本显著降低，性能优于或与更昂贵的方法相当。

Conclusion: CTQE方法能在降低成本的同时实现较好的检索性能。

Abstract: Query Expansion (QE) improves retrieval performance by enriching queries with
related terms. Recently, Large Language Models (LLMs) have been used for QE,
but existing methods face a trade-off: generating diverse terms boosts
performance but increases computational cost. To address this challenge, we
propose Candidate Token Query Expansion (CTQE), which extracts diverse and
relevant terms from a single LLM decoding pass by leveraging unselected
candidate tokens. These tokens, though not part of the final output, are
conditioned on the full query and capture useful information. By aggregating
them, CTQE achieves both relevance and diversity without extra inference,
reducing overhead and latency. Experiments show that CTQE delivers strong
retrieval performance with significantly lower cost, outperforming or
comparable to more expensive methods. Code is available at:
https://github.com/bluejeans8/CTQE

</details>


### [171] [Lighting the Way for BRIGHT: Reproducible Baselines with Anserini, Pyserini, and RankLLM](https://arxiv.org/abs/2509.02558)
*Yijun Ge,Sahel Sharifymoghaddam,Jimmy Lin*

Main category: cs.IR

TL;DR: 探索BRIGHT基准数据集检索结果，建立可复现基线，应用列表重排，发现BRIGHT的BM25实现差异并建议重新考虑BM25方法，还将查询端BM25集成到工具包。


<details>
  <summary>Details</summary>
Motivation: 探索BRIGHT数据集的检索结果，研究重排对推理密集型查询的影响，解决复现结果差异问题并适应新兴应用。

Method: 使用多种检索技术（稀疏、密集、融合）在BRIGHT上进行检索，应用大语言模型进行列表重排，将基线集成到工具包，分析BRIGHT的BM25实现差异。

Result: 建立可复现基线，发现BRIGHT的BM25实现与标准方法不同，将查询端BM25集成到Anserini和Pyserini。

Conclusion: 有必要重新考虑BM25方法以适应新兴应用。

Abstract: The BRIGHT benchmark is a dataset consisting of reasoning-intensive queries
over diverse domains. We explore retrieval results on BRIGHT using a range of
retrieval techniques, including sparse, dense, and fusion methods, and
establish reproducible baselines. We then apply listwise reranking with large
language models (LLMs) to further investigate the impact of reranking on
reasoning-intensive queries. These baselines are integrated into popular
retrieval and reranking toolkits Anserini, Pyserini, and RankLLM, with
two-click reproducibility that makes them easy to build upon and convenient for
further development. While attempting to reproduce the results reported in the
original BRIGHT paper, we find that the provided BM25 scores differ notably
from those that we obtain using Anserini and Pyserini. We discover that this
difference is due to BRIGHT's implementation of BM25, which applies BM25 on the
query rather than using the standard bag-of-words approach, as in Anserini, to
construct query vectors. This difference has become increasingly relevant due
to the rise of longer queries, with BRIGHT's lengthy reasoning-intensive
queries being a prime example, and further accentuated by the increasing usage
of retrieval-augmented generation, where LLM prompts can grow to be much longer
than ''traditional'' search engine queries. Our observation signifies that it
may be time to reconsider BM25 approaches going forward in order to better
accommodate emerging applications. To facilitate this, we integrate query-side
BM25 into both Anserini and Pyserini.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [172] [Diagnosing Psychiatric Patients: Can Large Language and Machine Learning Models Perform Effectively in Emergency Cases?](https://arxiv.org/abs/2509.00026)
*Abu Shad Ahammed,Sayeri Mukherjee,Roman Obermaisser*

Main category: cs.LG

TL;DR: 研究传统机器学习和大语言模型基于行为模式评估精神科患者以提供诊断评估，用德国救援站数据测试模型。


<details>
  <summary>Details</summary>
Motivation: 精神障碍患者因缺乏明显症状常被误判和误诊，紧急情况下识别精神问题有挑战但很必要。

Method: 收集德国救援站急诊精神科患者数据，使用包括Llama 3.1在内的多种机器学习模型进行评估。

Result: 原文未提及。

Conclusion: 原文未提及。

Abstract: Mental disorders are clinically significant patterns of behavior that are
associated with stress and/or impairment in social, occupational, or family
activities. People suffering from such disorders are often misjudged and poorly
diagnosed due to a lack of visible symptoms compared to other health
complications. During emergency situations, identifying psychiatric issues is
that's why challenging but highly required to save patients. In this paper, we
have conducted research on how traditional machine learning and large language
models (LLM) can assess these psychiatric patients based on their behavioral
patterns to provide a diagnostic assessment. Data from emergency psychiatric
patients were collected from a rescue station in Germany. Various machine
learning models, including Llama 3.1, were used with rescue patient data to
assess if the predictive capabilities of the models can serve as an efficient
tool for identifying patients with unhealthy mental disorders, especially in
rescue cases.

</details>


### [173] [Financial Decision Making using Reinforcement Learning with Dirichlet Priors and Quantum-Inspired Genetic Optimization](https://arxiv.org/abs/2509.00095)
*Prasun Nandy,Debjit Dhar,Rik Das*

Main category: cs.LG

TL;DR: 提出用于动态预算分配的混合强化学习框架，结合苹果财务数据实验，模型在未见过数据上表现良好，展示多技术结合在企业预算中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统预算分配模型难以处理现实金融数据的随机性和非线性，需要新方法。

Method: 提出结合狄利克雷随机和量子变异遗传优化的混合强化学习框架，使用苹果财务数据训练，用狄利克雷分布模拟金融环境，用带量子变异的遗传算法优化策略。

Result: 模型在未见过的财务数据上与实际分配高度一致，余弦相似度0.9990，KL散度0.0023。

Conclusion: 结合深度强化学习、随机建模和量子启发式方法对自适应企业预算有应用前景。

Abstract: Traditional budget allocation models struggle with the stochastic and
nonlinear nature of real-world financial data. This study proposes a hybrid
reinforcement learning (RL) framework for dynamic budget allocation, enhanced
with Dirichlet-inspired stochasticity and quantum mutation-based genetic
optimization. Using Apple Inc. quarterly financial data (2009 to 2025), the RL
agent learns to allocate budgets between Research and Development and Selling,
General and Administrative to maximize profitability while adhering to
historical spending patterns, with L2 penalties discouraging unrealistic
deviations. A Dirichlet distribution governs state evolution to simulate
shifting financial contexts. To escape local minima and improve generalization,
the trained policy is refined using genetic algorithms with quantum mutation
via parameterized qubit rotation circuits. Generation-wise rewards and
penalties are logged to visualize convergence and policy behavior. On unseen
fiscal data, the model achieves high alignment with actual allocations (cosine
similarity 0.9990, KL divergence 0.0023), demonstrating the promise of
combining deep RL, stochastic modeling, and quantum-inspired heuristics for
adaptive enterprise budgeting.

</details>


### [174] [Mitigating Data Exfiltration Attacks through Layer-Wise Learning Rate Decay Fine-Tuning](https://arxiv.org/abs/2509.00027)
*Elie Thellier,Huiyu Li,Nicholas Ayache,Hervé Delingette*

Main category: cs.LG

TL;DR: 提出在导出模型参数时通过衰减层学习率微调来缓解数据湖训练的医学机器学习模型数据泄露风险的策略，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 数据湖训练医学机器学习模型存在隐私风险，现有攻击可通过嵌入潜在表示或诱导记忆来窃取数据。

Method: 在导出模型参数时，使用衰减层学习率微调来扰动模型参数。

Result: 在多个数据集上，该方法保持了任务性能，有效破坏了现有数据窃取攻击，优于先前防御方法，使窃取的数据无法用于训练。

Conclusion: 该策略为数据湖训练模型和集中式联邦学习中的数据泄露提供了实用防御。

Abstract: Data lakes enable the training of powerful machine learning models on
sensitive, high-value medical datasets, but also introduce serious privacy
risks due to potential leakage of protected health information. Recent studies
show adversaries can exfiltrate training data by embedding latent
representations into model parameters or inducing memorization via multi-task
learning. These attacks disguise themselves as benign utility models while
enabling reconstruction of high-fidelity medical images, posing severe privacy
threats with legal and ethical implications. In this work, we propose a simple
yet effective mitigation strategy that perturbs model parameters at export time
through fine-tuning with a decaying layer-wise learning rate to corrupt
embedded data without degrading task performance. Evaluations on DermaMNIST,
ChestMNIST, and MIMIC-CXR show that our approach maintains utility task
performance, effectively disrupts state-of-the-art exfiltration attacks,
outperforms prior defenses, and renders exfiltrated data unusable for training.
Ablations and discussions on adaptive attacks highlight challenges and future
directions. Our findings offer a practical defense against data leakage in data
lake-trained models and centralized federated learning.

</details>


### [175] [An Evolutionary Multi-objective Optimization for Replica-Exchange-based Physics-informed Operator Learning Network](https://arxiv.org/abs/2509.00663)
*Binghang Lu,Changhong Mou,Guang Lin*

Main category: cs.LG

TL;DR: 提出基于副本交换的物理信息算子学习网络的进化多目标优化方法，解决参数偏微分方程，性能优于一般算子学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息神经网络和算子学习方法在平衡算子与物理损失、噪声或稀疏数据下的鲁棒性及不确定性量化方面存在不足。

Method: 集成进化多目标优化、副本交换随机梯度朗之万动力学和内置贝叶斯不确定性量化。

Result: 在多个问题上测试，该框架在准确性、噪声鲁棒性和不确定性量化能力上始终优于一般算子学习方法。

Conclusion: 所提框架有效解决了现有方法的局限性，具有更好的性能。

Abstract: In this paper, we propose an evolutionary Multi-objective Optimization for
Replica-Exchange-based Physics-informed Operator learning Network, which is a
novel operator learning network to efficiently solve parametric partial
differential equations. In forward and inverse settings, this operator learning
network only admits minimum requirement of noisy observational data. While
physics-informed neural networks and operator learning approaches such as Deep
Operator Networks and Fourier Neural Operators offer promising alternatives to
traditional numerical solvers, they struggle with balancing operator and
physics losses, maintaining robustness under noisy or sparse data, and
providing uncertainty quantification. The proposed framework addresses these
limitations by integrating: (i) evolutionary multi-objective optimization to
adaptively balance operator and physics-based losses in the Pareto front; (ii)
replica exchange stochastic gradient Langevin dynamics to improve global
parameter-space exploration and accelerate convergence; and (iii) built-in
Bayesian uncertainty quantification from stochastic sampling. The proposed
operator learning method is tested numerically on several different problems
including one-dimensional Burgers equation and the time-fractional mixed
diffusion-wave equation. The results indicate that our framework consistently
outperforms the general operator learning methods in accuracy, noise
robustness, and the ability to quantify uncertainty.

</details>


### [176] [ZeroQAT: Your Quantization-aware Training but Efficient](https://arxiv.org/abs/2509.00031)
*Qitao Tan,Xiaoying Song,Jin Lu,Guoming Li,Jun Liu,Lingzi Hong,Caiwen Ding,Jundong Li,Xiaoming Zhai,Shaoyi Huang,Wei Niu,Geng Yuan*

Main category: cs.LG

TL;DR: 提出ZeroQAT框架用于大语言模型低比特量化，结合PTQ效率与QAT精度。


<details>
  <summary>Details</summary>
Motivation: 现有低比特PTQ方法有精度下降问题，QAT因依赖反向传播成本高，实用性受限。

Method: 提出基于零阶优化的QAT框架ZeroQAT，利用前向梯度估计消除反向传播需求，联合学习量化权重、裁剪阈值和等价变换。

Result: 实验表明ZeroQAT能达到PTQ的效率，同时保持QAT的精度。

Conclusion: ZeroQAT为大语言模型高质量低比特量化提供了实用解决方案。

Abstract: Quantization is an effective technique to reduce the deployment cost of large
language models (LLMs), and post-training quantization (PTQ) has been widely
studied due to its efficiency. However, existing low-bit PTQ methods suffer
from accuracy degradation because their layer-wise optimization introduces
cumulative error propagation and misalignment between local reconstruction
objectives and downstream performance. While quantization-aware training (QAT)
provides a principled solution, its reliance on backpropagation incurs
prohibitive data, time, and memory costs, limiting its practicality. To address
these challenges, we propose ZeroQAT, a zeroth-order optimization-based QAT
framework. ZeroQAT leverages forward-only gradient estimation to eliminate the
need for backpropagation, significantly reducing computational and memory
overhead while retaining the benefits of end-to-end optimization. Moreover,
ZeroQAT jointly learns quantized weights, weight clipping thresholds, and
equivalent transformations to mitigate quantization error and handle activation
outliers. Experiments demonstrate that ZeroQAT achieves the efficiency of PTQ
while retaining the accuracy of QAT, offering a practical solution for
high-quality low-bit quantization of LLMs.

</details>


### [177] [ReLATE: Learning Efficient Sparse Encoding for High-Performance Tensor Decomposition](https://arxiv.org/abs/2509.00280)
*Ahmed E. Helal,Fabio Checconi,Jan Laukemann,Yongseok Soh,Jesmin Jahan Tithi,Fabrizio Petrini,Jee Choi*

Main category: cs.LG

TL;DR: 提出ReLATE框架，自动构建高效稀疏张量表示，在多种数据集上表现优于专家设计格式。


<details>
  <summary>Details</summary>
Motivation: 现有基于专家设计的稀疏张量格式无法适应不规则张量形状和数据分布，且张量分解在现代并行处理器上有性能挑战。

Method: 采用无标签训练样本的学习增强方法，通过自主智能体与TD环境交互，结合无模型和基于模型算法学习，引入规则驱动的动作掩码和动态信息动作过滤机制。

Result: 在多种稀疏张量数据集上生成的稀疏张量表示优于专家设计格式，最高达2倍加速，几何平均加速1.4 - 1.46倍。

Conclusion: ReLATE能自动适应不规则张量形状和数据分布，有效提升稀疏张量分解性能。

Abstract: Tensor decomposition (TD) is essential for analyzing high-dimensional sparse
data, yet its irregular computations and memory-access patterns pose major
performance challenges on modern parallel processors. Prior works rely on
expert-designed sparse tensor formats that fail to adapt to irregular tensor
shapes and/or highly variable data distributions. We present the
reinforcement-learned adaptive tensor encoding (ReLATE) framework, a novel
learning-augmented method that automatically constructs efficient sparse tensor
representations without labeled training samples. ReLATE employs an autonomous
agent that discovers optimized tensor encodings through direct interaction with
the TD environment, leveraging a hybrid model-free and model-based algorithm to
learn from both real and imagined actions. Moreover, ReLATE introduces
rule-driven action masking and dynamics-informed action filtering mechanisms
that ensure functionally correct tensor encoding with bounded execution time,
even during early learning stages. By automatically adapting to both irregular
tensor shapes and data distributions, ReLATE generates sparse tensor
representations that consistently outperform expert-designed formats across
diverse sparse tensor data sets, achieving up to 2X speedup compared to the
best sparse format, with a geometric-mean speedup of 1.4-1.46X.

</details>


### [178] [Industrial Steel Slag Flow Data Loading Method for Deep Learning Applications](https://arxiv.org/abs/2509.00034)
*Mert Sehri,Ana Cardoso,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: 研究提出跨域诊断方法识别钢渣流阶段，混合模型表现佳，为实时监测提供实用方案。


<details>
  <summary>Details</summary>
Motivation: 钢铸造过程因渣流污染易造成经济损失，需准确检测渣流状况。

Method: 采用一维卷积神经网络和长短期记忆层结合的混合深度学习模型，处理加速度计原始时域振动信号，用现实跨域数据集划分评估。

Result: 结合均方根预处理和选择性嵌入数据加载策略的混合架构分类准确率高，最高测试准确率达99.10 +/- 0.30。

Conclusion: 该方法为实时渣流监测提供实用可扩展方案，有助于提高钢铁制造可靠性和运营效率。

Abstract: Steel casting processes are vulnerable to financial losses due to slag flow
contamination, making accurate slag flow condition detection essential. This
study introduces a novel cross-domain diagnostic method using vibration data
collected from an industrial steel foundry to identify various stages of slag
flow. A hybrid deep learning model combining one-dimensional convolutional
neural networks and long short-term memory layers is implemented, tested, and
benchmarked against a standard one-dimensional convolutional neural network.
The proposed method processes raw time-domain vibration signals from
accelerometers and evaluates performance across 16 distinct domains using a
realistic cross-domain dataset split. Results show that the hybrid
convolutional neural network and long short-term memory architecture, when
combined with root mean square preprocessing and a selective embedding data
loading strategy, achieves robust classification accuracy, outperforming
traditional models and loading techniques. The highest test accuracy of 99.10
+/- 0.30 demonstrates the method's capability for generalization and industrial
relevance. This work presents a practical and scalable solution for real-time
slag flow monitoring, contributing to improved reliability and operational
efficiency in steel manufacturing.

</details>


### [179] [An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling with Potential for Consumer Electronics Edge Deployment](https://arxiv.org/abs/2509.00560)
*Can Cui,Zilong Fu,Penghe Huang,Yuanyuan Li,Wu Deng,Dongyan Li*

Main category: cs.LG

TL;DR: 本文提出从GNN到KANs的知识蒸馏框架SA - DSD，用改进的FR - KAN+作为学生模型，实验表明该框架有性能提升、减少参数量和推理时间。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘环境中，需要将GNN知识转移到更高效模型，但MLP难以捕捉GNN复杂邻域依赖，故提出新框架。

Method: 提出SA - DSD框架，改进FR - KAN并作为学生模型，构建基于师生预测一致性的采样概率矩阵，设计自适应加权损失机制。

Result: 在六个真实数据集上，SA - DSD比三个GNN教师模型性能提升3.05% - 3.62%，比FR - KAN+模型提升15.61%，参数量减少16.96倍，推理时间减少55.75%。

Conclusion: SA - DSD框架在边缘环境的知识蒸馏任务中表现良好，能有效提升性能、减少参数量和推理时间。

Abstract: Knowledge distillation (KD) is crucial for deploying deep learning models in
resource-constrained edge environments, particularly within the consumer
electronics sector, including smart home devices, wearable technology, and
mobile terminals. These applications place higher demands on model compression
and inference speed, necessitating the transfer of knowledge from Graph Neural
Networks (GNNs) to more efficient Multi-Layer Perceptron (MLP) models. However,
due to their fixed activation functions and fully connected architecture, MLPs
face challenges in rapidly capturing the complex neighborhood dependencies
learned by GNNs, thereby limiting their performance in edge environments. To
address these limitations, this paper introduces an innovative from GNNs to
Kolmogorov-Arnold Networks (KANs) knowledge distillation framework-Self
Attention Dynamic Sampling Distillation (SA-DSD). This study improved Fourier
KAN (FR-KAN) and replaced MLP with the improved FR-KAN+ as the student model.
Through the incorporation of learnable frequency bases and phase-shift
mechanisms, along with algorithmic optimization, FR-KAN significantly improves
its nonlinear fitting capability while effectively reducing computational
complexity. Building on this, a margin-level sampling probability matrix, based
on teacher-student prediction consistency, is constructed, and an adaptive
weighted loss mechanism is designed to mitigate performance degradation in the
student model due to the lack of explicit neighborhood aggregation. Extensive
experiments conducted on six real-world datasets demonstrate that SA-DSD
achieves performance improvements of 3.05%-3.62% over three GNN teacher models
and 15.61% over the FR-KAN+ model. Moreover, when compared with key benchmark
models, SA-DSD achieves a 16.96x reduction in parameter count and a 55.75%
decrease in inference time.

</details>


### [180] [Transfer Learning for Minimum Operating Voltage Prediction in Advanced Technology Nodes: Leveraging Legacy Data and Silicon Odometer Sensing](https://arxiv.org/abs/2509.00035)
*Yuxuan Yin,Rebecca Chen,Boxun Xu,Chen He,Peng Li*

Main category: cs.LG

TL;DR: 提出新颖迁移学习框架，利用16nm节点数据实现5nm节点最小工作电压预测，结合片上传感器数据提升精度。


<details>
  <summary>Details</summary>
Motivation: 先进技术节点下，因训练数据有限和工艺变化与最小工作电压关系复杂，开发最小工作电压预测模型具有挑战性。

Method: 提出新颖迁移学习框架，利用16nm节点的大量遗留数据，并结合片上硅里程计传感器数据的输入特征。

Result: 显著提高了预测精度。

Conclusion: 所提迁移学习框架能利用旧节点数据实现先进节点最小工作电压的准确预测。

Abstract: Accurate prediction of chip performance is critical for ensuring energy
efficiency and reliability in semiconductor manufacturing. However, developing
minimum operating voltage ($V_{min}$) prediction models at advanced technology
nodes is challenging due to limited training data and the complex relationship
between process variations and $V_{min}$. To address these issues, we propose a
novel transfer learning framework that leverages abundant legacy data from the
16nm technology node to enable accurate $V_{min}$ prediction at the advanced
5nm node. A key innovation of our approach is the integration of input features
derived from on-chip silicon odometer sensor data, which provide fine-grained
characterization of localized process variations -- an essential factor at the
5nm node -- resulting in significantly improved prediction accuracy.

</details>


### [181] [A-FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler](https://arxiv.org/abs/2509.00036)
*Cheng Jin,Zhenyu Xiao,Yuantao Gu*

Main category: cs.LG

TL;DR: 提出A - FloPS框架加速预训练扩散模型采样，实验显示其在样本质量和效率上优于现有无训练采样器。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算成本高，现有无训练加速方法受采样轨迹低效的限制。

Method: 将预训练扩散模型采样轨迹重新参数化为流匹配形式，并进行自适应速度分解。

Result: 在条件图像生成和文本到图像合成实验中，A - FloPS在样本质量和效率上优于现有无训练采样器，少量函数评估即可有好效果，且自适应机制能改进原生基于流的生成模型。

Conclusion: A - FloPS是高质量、低延迟生成建模的通用有效解决方案。

Abstract: Diffusion models deliver state-of-the-art generative performance across
diverse modalities but remain computationally expensive due to their inherently
iterative sampling process. Existing training-free acceleration methods
typically improve numerical solvers for the reverse-time ODE, yet their
effectiveness is fundamentally constrained by the inefficiency of the
underlying sampling trajectories. We propose A-FloPS (Adaptive Flow Path
Sampler), a principled, training-free framework that reparameterizes the
sampling trajectory of any pre-trained diffusion model into a flow-matching
form and augments it with an adaptive velocity decomposition. The
reparameterization analytically maps diffusion scores to flow-compatible
velocities, yielding integration-friendly trajectories without retraining. The
adaptive mechanism further factorizes the velocity field into a linear drift
term and a residual component whose temporal variation is actively suppressed,
restoring the accuracy benefits of high-order integration even in extremely
low-NFE regimes. Extensive experiments on conditional image generation and
text-to-image synthesis show that A-FloPS consistently outperforms
state-of-the-art training-free samplers in both sample quality and efficiency.
Notably, with as few as $5$ function evaluations, A-FloPS achieves
substantially lower FID and generates sharper, more coherent images. The
adaptive mechanism also improves native flow-based generative models,
underscoring its generality. These results position A-FloPS as a versatile and
effective solution for high-quality, low-latency generative modeling.

</details>


### [182] [Exploring and Reshaping the Weight Distribution in LLM](https://arxiv.org/abs/2509.00046)
*Chunming Ye,Songzhou Li,Xu Xu*

Main category: cs.LG

TL;DR: 本文探究大模型不同层权重分布相关性及其对LoRA训练效果的影响，发现权重余弦距离呈幂律分布，提出定性描述方法、设计数据生成器，重塑LoRA初始化权重训练，提升了训练性能。


<details>
  <summary>Details</summary>
Motivation: 大模型不同层权重分布因结构或功能差异而不同，探究不同层权重分布相关性及对LoRA训练效果的潜在影响。

Method: 揭示模型不同层权重余弦距离呈幂律分布；提取特定层权重矩阵，用奇异值分解计算奇异值并分析其余弦距离概率分布；提出定性描述不同模型分布特征的方法；设计结合高斯过程和帕累托分布函数的数据生成器；基于分布特征和数据生成方法重塑LoRA初始化权重进行训练。

Result: 实验表明，在不改变模型结构和训练过程的情况下，该方法使LoRA训练性能有一定提升。

Conclusion: 通过对大模型不同层权重分布的研究和处理，所提出的方法能有效提升LoRA训练性能。

Abstract: The performance of Large Language Models is influenced by their
characteristics such as architecture, model sizes, decoding methods and so on.
Due to differences in structure or function, the weights in different layers of
large models have varying distributions. This paper explores the correlations
between different types of layers in terms of weights distribution and studies
the potential impact of these correlations on LoRA training effectiveness.
Firstly, the study reveals that in the model the cosine distances between
weights of different layers manifest power-law distribution. We extract
Query-projection, down-projection and other weight matrices from the
self-attention layers and MLP layers, calculate the singular values of the
matrices using singular value decomposition, and organize a certain number of
singular values into matrices according to projection's type. By analyzing the
probability distribution of the cosine distances between these matrices, it is
found that the cosine distances values between them have distinct power-law
distribution characteristics. Secondly, based on the results of distance
calculations and analysis across different layers of model, a qualitative
method is proposed to describe the distribution characteristics of different
models. Next, to construct weights that align with the distribution
characteristics, a data generator is designed using a combination of Gaussian
process and Pareto distribution functions. The generator is used to simulate
the generation of data that aligns with specific distribution characteristics.
Finally, based on the aforementioned distribution characteristics and data
generation method, the weights in LoRA initialization are reshaped for
training. Experimental results indicate that, without altering the model
structure or training process, this method achieves a certain improvement in
the performance of LoRA training.

</details>


### [183] [Prediction, Generation of WWTPs microbiome community structures and Clustering of WWTPs various feature attributes using DE-BP model, SiTime-GAN model and DPNG-EPMC ensemble clustering algorithm with modulation of microbial ecosystem health](https://arxiv.org/abs/2509.01526)
*Mingzhi Dai,Weiwei Cai,Xiang Feng,Huiqun Yu,Weibin Guo,Miao Guo*

Main category: cs.LG

TL;DR: 本文用DE - BP预测活性污泥微生物组成，用DPNG - EPMC对污水处理厂聚类分析，用SiTime - GAN合成数据，结果表明各模型有效，有助于理解影响活性污泥群落的因素。


<details>
  <summary>Details</summary>
Motivation: 微生物组工程在微生物控制改进方面面临重大障碍，需要方法来深入理解活性污泥群落。

Method: 使用差分进化优化的反向传播神经网络（DE - BP）预测微生物组成；引入定向位置非线性情感偏好迁移行为聚类算法（DPNG - EPMC）对污水处理厂聚类；使用相似时间生成对抗网络（SiTime - GAN）合成数据。

Result: DE - BP模型能提供更好的微生物组成预测；DPNG - EPMC可用于不同特征属性污水处理厂分析；SiTime - GAN模型能生成有价值的增量合成数据。

Conclusion: 通过预测微生物群落和分析污水处理厂不同特征属性，有助于理解影响活性污泥群落的因素。

Abstract: Microbiomes not only underpin Earth's biogeochemical cycles but also play
crucial roles in both engineered and natural ecosystems, such as the soil,
wastewater treatment, and the human gut. However, microbiome engineering faces
significant obstacles to surmount to deliver the desired improvements in
microbiome control. Here, we use the backpropagation neural network (BPNN),
optimized through differential evolution (DE-BP), to predict the microbial
composition of activated sludge (AS) systems collected from wastewater
treatment plants (WWTPs) located worldwide. Furthermore, we introduce a novel
clustering algorithm termed Directional Position Nonlinear Emotional Preference
Migration Behavior Clustering (DPNG-EPMC). This method is applied to conduct a
clustering analysis of WWTPs across various feature attributes. Finally, we
employ the Similar Time Generative Adversarial Networks (SiTime-GAN), to
synthesize novel microbial compositions and feature attributes data. As a
result, we demonstrate that the DE-BP model can provide superior predictions of
the microbial composition. Additionally, we show that the DPNG-EPMC can be
applied to the analysis of WWTPs under various feature attributes. Finally, we
demonstrate that the SiTime-GAN model can generate valuable incremental
synthetic data. Our results, obtained through predicting the microbial
community and conducting analysis of WWTPs under various feature attributes,
develop an understanding of the factors influencing AS communities.

</details>


### [184] [Teaching AI to Remember: Insights from Brain-Inspired Replay in Continual Learning](https://arxiv.org/abs/2509.00047)
*Jina Kim*

Main category: cs.LG

TL;DR: 研究受人类大脑记忆巩固启发的内部重放机制在持续学习中的效果，发现其能缓解遗忘但有初始任务精度降低的问题，揭示当前方法局限并指出未来方向。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络在持续学习中面临灾难性遗忘问题，受人类大脑记忆巩固启发，深入研究内部重放机制。

Method: 在类增量设置下使用CIFAR - 100数据集评估内部重放机制单独及与突触智能（SI）结合的效果，还通过对数似然分布、重建误差等进一步分析。

Result: 内部重放显著缓解遗忘，与SI结合效果更佳，但会降低初始任务精度，且增加潜在空间的表征重叠，限制任务特定区分。

Conclusion: 当前受大脑启发的方法存在局限性，需探索平衡持续学习系统中保留和适应性的未来方向。

Abstract: Artificial neural networks (ANNs) continue to face challenges in continual
learning, particularly due to catastrophic forgetting, the loss of previously
learned knowledge when acquiring new tasks. Inspired by memory consolidation in
the human brain, we investigate the internal replay mechanism proposed
by~\citep{brain_inspired_replay1}, which reactivates latent representations of
prior experiences during learning. As internal replay was identified as the
most influential component among the brain-inspired mechanisms in their
framework, it serves as the central focus of our in-depth investigation. Using
the CIFAR-100 dataset in a class-incremental setting, we evaluate the
effectiveness of internal replay, both in isolation and in combination with
Synaptic Intelligence (SI). Our experiments show that internal replay
significantly mitigates forgetting, especially when paired with SI, but at the
cost of reduced initial task accuracy, highlighting a trade-off between memory
stability and learning plasticity. Further analyses using log-likelihood
distributions, reconstruction errors, silhouette scores, and UMAP projections
reveal that internal replay increases representational overlap in latent space,
potentially limiting task-specific differentiation. These results underscore
the limitations of current brain-inspired methods and suggest future directions
for balancing retention and adaptability in continual learning systems.

</details>


### [185] [Semantic and episodic memories in a predictive coding model of the neocortex](https://arxiv.org/abs/2509.01987)
*Lucie Fontaine,Frédéric Alexandre*

Main category: cs.LG

TL;DR: 本文探讨新皮层预测编码模型的情景记忆能力，发现少量样本训练时可回忆具体示例，但泛化差，表明情景记忆可源于语义学习，也说明海马稀疏表征的必要性。


<details>
  <summary>Details</summary>
Motivation: 近期预测编码对语义和情景记忆的二元性提出挑战，引发对新皮层情景能力及其与语义记忆关系的疑问。

Method: 提出新皮层的预测编码模型并探索其情景能力。

Result: 模型在少量样本训练时能回忆具体示例，但会过拟合、泛化差；训练样本增多则失去回忆能力。

Conclusion: 新皮层可用密集重叠表征逐渐编码少量个体示例，这也解释了海马稀疏表征的需求。

Abstract: Complementary Learning Systems theory holds that intelligent agents need two
learning systems. Semantic memory is encoded in the neocortex with dense,
overlapping representations and acquires structured knowledge. Episodic memory
is encoded in the hippocampus with sparse, pattern-separated representations
and quickly learns the specifics of individual experiences. Recently, this
duality between semantic and episodic memories has been challenged by
predictive coding, a biologically plausible neural network model of the
neocortex which was shown to have hippocampus-like abilities on
auto-associative memory tasks. These results raise the question of the episodic
capabilities of the neocortex and their relation to semantic memory. In this
paper, we present such a predictive coding model of the neocortex and explore
its episodic capabilities. We show that this kind of model can indeed recall
the specifics of individual examples but only if it is trained on a small
number of examples. The model is overfitted to these exemples and does not
generalize well, suggesting that episodic memory can arise from semantic
learning. Indeed, a model trained with many more examples loses its recall
capabilities. This work suggests that individual examples can be encoded
gradually in the neocortex using dense, overlapping representations but only in
a limited number, motivating the need for sparse, pattern-separated
representations as found in the hippocampus.

</details>


### [186] [DaCe AD: Unifying High-Performance Automatic Differentiation for Machine Learning and Scientific Computing](https://arxiv.org/abs/2509.02197)
*Afif Boudaoud,Alexandru Calotoiu,Marcin Copik,Torsten Hoefler*

Main category: cs.LG

TL;DR: 提出无代码修改的通用高效自动微分引擎DaCe AD，用新算法优化存储与重算权衡，在NPBench上表现超JAX 92倍。


<details>
  <summary>Details</summary>
Motivation: 现有自动微分框架存在对编程语言支持有限、需代码修改、在科学计算代码上性能有限、前向传播数据存储方案简单等问题，促使科学家手动计算梯度。

Method: 提出DaCe AD，使用基于ILP的新算法在给定内存约束下优化存储与重算权衡。

Result: 将方法应用于NPBench，平均性能比JAX高92倍以上且无需代码修改。

Conclusion: DaCe AD是通用、高效且无需代码修改的自动微分引擎。

Abstract: Automatic differentiation (AD) is a set of techniques that systematically
applies the chain rule to compute the gradients of functions without requiring
human intervention. Although the fundamentals of this technology were
established decades ago, it is experiencing a renaissance as it plays a key
role in efficiently computing gradients for backpropagation in machine learning
algorithms. AD is also crucial for many applications in scientific computing
domains, particularly emerging techniques that integrate machine learning
models within scientific simulations and schemes. Existing AD frameworks have
four main limitations: limited support of programming languages, requiring code
modifications for AD compatibility, limited performance on scientific computing
codes, and a naive store-all solution for forward-pass data required for
gradient calculations. These limitations force domain scientists to manually
compute the gradients for large problems. This work presents DaCe AD, a
general, efficient automatic differentiation engine that requires no code
modifications. DaCe AD uses a novel ILP-based algorithm to optimize the
trade-off between storing and recomputing to achieve maximum performance within
a given memory constraint. We showcase the generality of our method by applying
it to NPBench, a suite of HPC benchmarks with diverse scientific computing
patterns, where we outperform JAX, a Python framework with state-of-the-art
general AD capabilities, by more than 92 times on average without requiring any
code changes.

</details>


### [187] [Adaptive Physics-Informed Neural Networks with Multi-Category Feature Engineering for Hydrogen Sorption Prediction in Clays, Shales, and Coals](https://arxiv.org/abs/2509.00049)
*Mohammad Nooraiepour,Mohammad Masoudi,Zezhang Song,Helge Hellevang*

Main category: cs.LG

TL;DR: 本文引入自适应物理信息神经网络框架用于增强氢吸附预测，经测试有高准确性和鲁棒性，能加速场地筛选和辅助决策。


<details>
  <summary>Details</summary>
Motivation: 传统实验方法在预测黏土、页岩和煤中氢吸附时存在耗时、易出错和难以捕捉地质异质性的问题，需要新方法推进相关领域发展。

Method: 引入自适应物理信息神经网络（PINN）框架，结合多类别特征工程，集成经典等温线模型与热力学约束，使用深残差网络和多头注意力机制，通过自适应损失函数和蒙特卡罗丢弃法优化。

Result: K折交叉验证和超参数优化使预测准确性高（R2 = 0.979，RMSE = 0.045 mol/kg），收敛速度快67%，在不同岩性中表现稳健，可靠性分数达85 - 91%。

Conclusion: 该自适应物理信息框架能加速场地筛选，通过可靠的不确定性量化实现风险决策。

Abstract: Accurate prediction of hydrogen sorption in clays, shales, and coals is vital
for advancing underground hydrogen storage, natural hydrogen exploration, and
radioactive waste containment. Traditional experimental methods, while
foundational, are time-consuming, error-prone, and limited in capturing
geological heterogeneity. This study introduces an adaptive physics-informed
neural network (PINN) framework with multi-category feature engineering to
enhance hydrogen sorption prediction. The framework integrates classical
isotherm models with thermodynamic constraints to ensure physical consistency
while leveraging deep learning flexibility. A comprehensive dataset consisting
of 155 samples, which includes 50 clays, 60 shales, and 45 coals, was employed,
incorporating diverse compositional properties and experimental conditions.
Multi-category feature engineering across seven categories captured complex
sorption dynamics. The PINN employs deep residual networks with multi-head
attention, optimized via adaptive loss functions and Monte Carlo dropout for
uncertainty quantification. K-fold cross-validation and hyperparameter
optimization achieve significant accuracy (R2 = 0.979, RMSE = 0.045 mol per kg)
with 67% faster convergence despite 15-fold increased complexity. The framework
demonstrates robust lithology-specific performance across clay minerals (R2 =
0.981), shales (R2 = 0.971), and coals (R2 = 0.978), maintaining 85-91%
reliability scores. Interpretability analysis via SHAP, accumulated local
effects, and Friedman's H-statistics reveal that hydrogen adsorption capacity
dominates predictions, while 86.7% of feature pairs exhibit strong
interactions, validating the necessity of non-linear modeling approaches. This
adaptive physics-informed framework accelerates site screening and enables
risk-informed decision-making through robust uncertainty quantification.

</details>


### [188] [Estimating Parameter Fields in Multi-Physics PDEs from Scarce Measurements](https://arxiv.org/abs/2509.00203)
*Xuyang Li,Mahdi Masmoudi,Rami Gharbi,Nizar Lajnef,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: 本文介绍通用方法Neptune用于从系统响应的稀疏测量中推断参数场，在多个问题中表现优于现有方法，有广泛应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有参数估计方法在处理参数非线性和时空变化、非线性动力学、多物理相互作用或系统响应观测有限等情况时存在困难，需要新方法。

Method: 引入Neptune，使用独立坐标神经网络在物理空间或状态变量中连续表示每个参数场。

Result: 在多个物理和生物医学问题中，Neptune显著优于现有方法，从最少50个观测实现稳健参数估计，与PINNs相比，降低参数估计误差两个数量级，动态响应预测误差降低一个数量级，外推能力更优。

Conclusion: Neptune有助于实现可靠且数据高效的参数推断，有望在工程、医疗等领域带来广泛变革性影响。

Abstract: Parameterized partial differential equations (PDEs) underpin the mathematical
modeling of complex systems in diverse domains, including engineering,
healthcare, and physics. A central challenge in using PDEs for real-world
applications is to accurately infer the parameters, particularly when the
parameters exhibit non-linear and spatiotemporal variations. Existing parameter
estimation methods, such as sparse identification and physics-informed neural
networks (PINNs), struggle in such cases, especially with nonlinear dynamics,
multiphysics interactions, or limited observations of the system response. To
address these challenges, we introduce Neptune, a general-purpose method
capable of inferring parameter fields from sparse measurements of system
responses. Neptune employs independent coordinate neural networks to
continuously represent each parameter field in physical space or in state
variables. Across various physical and biomedical problems, where direct
parameter measurements are prohibitively expensive or unattainable, Neptune
significantly outperforms existing methods, achieving robust parameter
estimation from as few as 50 observations, reducing parameter estimation errors
by two orders of magnitude and dynamic response prediction errors by a factor
of ten compared to PINNs. Furthermore, Neptune exhibits superior extrapolation
capabilities, enabling accurate predictions in regimes beyond training data
where PINN fail. By facilitating reliable and data-efficient parameter
inference, Neptune promises broad transformative impacts in engineering,
healthcare, and beyond.

</details>


### [189] [Surrogate Benchmarks for Model Merging Optimization](https://arxiv.org/abs/2509.02555)
*Rio Akizuki,Yuya Kudo,Nozomu Yoshinari,Yoichi Hirose,Toshiyuki Nishimoto,Kento Uchida,Shinichi Shirakawa*

Main category: cs.LG

TL;DR: 本文提出用于模型合并超参数优化的代理基准，以低成本实现算法开发和性能比较。


<details>
  <summary>Details</summary>
Motivation: 模型合并超参数调优可提升合并效果，但优化过程计算成本高，尤其在大语言模型合并中。

Method: 定义两个搜索空间，收集数据样本构建代理模型，以根据超参数预测合并模型性能。

Result: 所提出的基准能很好地预测合并模型性能，模拟优化算法行为。

Conclusion: 开发的代理基准可实现模型合并超参数优化的低成本算法开发和性能比较。

Abstract: Model merging techniques aim to integrate the abilities of multiple models
into a single model. Most model merging techniques have hyperparameters, and
their setting affects the performance of the merged model. Because several
existing works show that tuning hyperparameters in model merging can enhance
the merging outcome, developing hyperparameter optimization algorithms for
model merging is a promising direction. However, its optimization process is
computationally expensive, particularly in merging LLMs. In this work, we
develop surrogate benchmarks for optimization of the merging hyperparameters to
realize algorithm development and performance comparison at low cost. We define
two search spaces and collect data samples to construct surrogate models to
predict the performance of a merged model from a hyperparameter. We demonstrate
that our benchmarks can predict the performance of merged models well and
simulate optimization algorithm behaviors.

</details>


### [190] [Applying Deep Learning to Anomaly Detection of Russian Satellite Activity for Indications Prior to Military Activity](https://arxiv.org/abs/2509.00050)
*David Kurtenbach,Megan Manly,Zach Metzinger*

Main category: cs.LG

TL;DR: 运用深度学习技术分析俄乌冲突前后俄在轨航天器异常活动，评估模型检测异常并分析各轨道要素异常情况。


<details>
  <summary>Details</summary>
Motivation: 分析俄乌冲突前俄在轨航天器活动，寻找可用于未来冲突军事行为预警的迹象。

Method: 采用统计和深度学习方法（隔离森林、传统自编码器等），基于五年数据样本建立在轨活动基线，针对每个航天器训练模型，通过重建误差超阈值检测异常，分析各轨道要素。

Result: 检测到俄在轨航天器活动有统计学显著异常，并详细分析了各轨道要素的异常情况。

Conclusion: 深度学习技术可有效检测俄在轨航天器异常活动，为未来冲突军事行为预警提供依据。

Abstract: We apply deep learning techniques for anomaly detection to analyze activity
of Russian-owned resident space objects (RSO) prior to the Ukraine invasion and
assess the results for any findings that can be used as indications and
warnings (I&W) of aggressive military behavior for future conflicts. Through
analysis of anomalous activity, an understanding of possible tactics and
procedures can be established to assess the existence of statistically
significant changes in Russian RSO pattern of life/pattern of behavior
(PoL/PoB) using publicly available two-line element (TLE) data. This research
looks at statistical and deep learning approaches to assess anomalous activity.
The deep learning methods assessed are isolation forest (IF), traditional
autoencoder (AE), variational autoencoder (VAE), Kolmogorov Arnold Network
(KAN), and a novel anchor-loss based autoencoder (Anchor AE). Each model is
used to establish a baseline of on-orbit activity based on a five-year data
sample. The primary investigation period focuses on the six months leading up
to the invasion date of February 24, 2022. Additional analysis looks at RSO
activity during an active combat period by sampling TLE data after the invasion
date. The deep learning autoencoder models identify anomalies based on
reconstruction errors that surpass a threshold sigma. To capture the nuance and
unique characteristics of each RSO an individual model was trained for each
observed space object. The research made an effort to prioritize explainability
and interpretability of the model results thus each observation was assessed
for anomalous behavior of the individual six orbital elements versus analyzing
the input data as a single monolithic observation. The results demonstrate not
only statistically significant anomalies of Russian RSO activity but also
details anomalous findings to the individual orbital element.

</details>


### [191] [From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis](https://arxiv.org/abs/2509.00057)
*Yousuf Moiz Ali,Jaroslaw E. Prilepsky,Nicola Sambo,Joao Pedro,Mohammad M. Hosseini,Antonio Napoli,Sergei K. Turitsyn,Pedro Freire*

Main category: cs.LG

TL;DR: 本文对比了光网络故障检测和识别中处理类别不平衡的预处理、处理中及后处理方法，给出不同场景下最优方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习用于光网络故障管理时，类别不平衡是挑战，后处理方法研究较少。

Method: 使用实验数据集对比预处理、处理中及后处理方法。

Result: 故障检测中后处理（阈值调整）F1分数提升最高，随机欠采样推理最快；故障识别中GenAI方法性能提升最大，后处理在多类场景效果有限；有类别重叠且对延迟敏感时SMOTE有效，无延迟限制时元学习最佳；低重叠场景下生成式AI性能高且推理时间短。

Conclusion: 不同类别不平衡场景下有不同最优的类别不平衡缓解方法。

Abstract: Machine learning-based failure management in optical networks has gained
significant attention in recent years. However, severe class imbalance, where
normal instances vastly outnumber failure cases, remains a considerable
challenge. While pre- and in-processing techniques have been widely studied,
post-processing methods are largely unexplored. In this work, we present a
direct comparison of pre-, in-, and post-processing approaches for class
imbalance mitigation in failure detection and identification using an
experimental dataset. For failure detection, post-processing
methods-particularly Threshold Adjustment-achieve the highest F1 score
improvement (up to 15.3%), while Random Under-Sampling provides the fastest
inference. In failure identification, GenAI methods deliver the most
substantial performance gains (up to 24.2%), whereas post-processing shows
limited impact in multi-class settings. When class overlap is present and
latency is critical, over-sampling methods such as the SMOTE are most
effective; without latency constraints, Meta-Learning yields the best results.
In low-overlap scenarios, Generative AI approaches provide the highest
performance with minimal inference time.

</details>


### [192] [T-MLP: Tailed Multi-Layer Perceptron for Level-of-Detail Signal Representation](https://arxiv.org/abs/2509.00066)
*Chuanxiang Yang,Yuanfeng Zhou,Guangshun Wei,Siyu Ren,Yuan Liu,Junhui Hou,Wenping Wang*

Main category: cs.LG

TL;DR: 提出支持细节层次（LoD）信号表示的新型神经架构T - MLP，实验表明其在多种信号表示任务中优于其他神经LoD基线。


<details>
  <summary>Details</summary>
Motivation: 现有广泛使用的多层感知机（MLP）缺乏对LoD信号表示的原生支持，需要新架构。

Method: 对MLP进行改进，引入T - MLP，在其隐藏层附加多个输出分支（尾巴），实现多深度直接监督，并设计损失函数和训练策略。

Result: T - MLP在多种信号表示任务中优于其他神经LoD基线。

Conclusion: T - MLP是一种有效的支持LoD信号表示的神经架构。

Abstract: Level-of-detail (LoD) representation is critical for efficiently modeling and
transmitting various types of signals, such as images and 3D shapes. In this
work, we present a novel neural architecture that supports LoD signal
representation. Our architecture is based on an elaborate modification of the
widely used Multi-Layer Perceptron (MLP), which inherently operates at a single
scale and therefore lacks native support for LoD. Specifically, we introduce
the Tailed Multi-Layer Perceptron (T-MLP) that extends the MLP by attaching
multiple output branches, also called tails, to its hidden layers, enabling
direct supervision at multiple depths. Our loss formulation and training
strategy allow each hidden layer to effectively learn a target signal at a
specific LoD, thus enabling multi-scale modeling. Extensive experimental
results show that our T-MLP outperforms other neural LoD baselines across a
variety of signal representation tasks.

</details>


### [193] [AnomalyExplainer Explainable AI for LLM-based anomaly detection using BERTViz and Captum](https://arxiv.org/abs/2509.00069)
*Prasasthy Balasubramanian,Dumindu Kankanamge,Ekaterina Gilman,Mourad Oussalah*

Main category: cs.LG

TL;DR: 研究提出框架结合可视化工具和自然语言报告检测异常并提供解释，提升网络安全工作流程。


<details>
  <summary>Details</summary>
Motivation: 对话式AI和大语言模型在网络安全应用存在误报和模型管理难题，可解释AI实用性存疑。

Method: 提出框架，结合可视化工具BERTViz和Captum及基于注意力输出的自然语言报告检测异常。

Result: 对比分析显示RoBERTa在HDFS数据集上准确率高、异常检测强，用户反馈表明聊天机器人易用且提升对异常理解。

Conclusion: 所开发框架能加强网络安全工作流程。

Abstract: Conversational AI and Large Language Models (LLMs) have become powerful tools
across domains, including cybersecurity, where they help detect threats early
and improve response times. However, challenges such as false positives and
complex model management still limit trust. Although Explainable AI (XAI) aims
to make AI decisions more transparent, many security analysts remain uncertain
about its usefulness. This study presents a framework that detects anomalies
and provides high-quality explanations through visual tools BERTViz and Captum,
combined with natural language reports based on attention outputs. This reduces
manual effort and speeds up remediation. Our comparative analysis showed that
RoBERTa offers high accuracy (99.6 %) and strong anomaly detection,
outperforming Falcon-7B and DeBERTa, as well as exhibiting better flexibility
than large-scale Mistral-7B on the HDFS dataset from LogHub. User feedback
confirms the chatbot's ease of use and improved understanding of anomalies,
demonstrating the ability of the developed framework to strengthen
cybersecurity workflows.

</details>


### [194] [SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits](https://arxiv.org/abs/2509.00071)
*Shang Liu,Jing Wang,Wenji Fang,Zhiyao Xie*

Main category: cs.LG

TL;DR: 提出SynCircuit生成具有有效功能的HDL格式合成电路，提升下游任务中ML模型性能。


<details>
  <summary>Details</summary>
Motivation: AI辅助IC设计方法发展受电路设计数据匮乏制约，尤其是公共领域数据不足。

Method: 采用三步框架：提出基于扩散的生成模型解决DCG生成任务；细化初始图生成输出以确保电路有效；用蒙特卡罗树搜索方法优化生成图中的逻辑冗余。

Result: SynCircuit能生成更逼真的合成电路，提升下游电路设计任务中ML模型的性能。

Conclusion: SynCircuit是生成具有有效功能的合成电路的有效方法，可推动AI辅助IC设计发展。

Abstract: In recent years, AI-assisted IC design methods have demonstrated great
potential, but the availability of circuit design data is extremely limited,
especially in the public domain. The lack of circuit data has become the
primary bottleneck in developing AI-assisted IC design methods. In this work,
we make the first attempt, SynCircuit, to generate new synthetic circuits with
valid functionalities in the HDL format. SynCircuit automatically generates
synthetic data using a framework with three innovative steps: 1) We propose a
customized diffusion-based generative model to resolve the Directed Cyclic
Graph (DCG) generation task, which has not been well explored in the AI
community. 2) To ensure our circuit is valid, we enforce the circuit
constraints by refining the initial graph generation outputs. 3) The Monte
Carlo tree search (MCTS) method further optimizes the logic redundancy in the
generated graph. Experimental results demonstrate that our proposed SynCircuit
can generate more realistic synthetic circuits and enhance ML model performance
in downstream circuit design tasks.

</details>


### [195] [Robust Detection of Synthetic Tabular Data under Schema Variability](https://arxiv.org/abs/2509.00092)
*G. Charbel N. Kindji,Elisa Fromont,Lina Maria Rojas-Barahona,Tanguy Urvoy*

Main category: cs.LG

TL;DR: 提出新架构检测野外合成表格数据，性能超基线。


<details>
  <summary>Details</summary>
Motivation: 强大生成模型引发数据真实性担忧，表格数据检测方法被忽视且检测有挑战，需检测野外合成表格数据。

Method: 引入新的数据级变压器架构，加入表格适配组件。

Result: 新架构显著优于唯一已发表基线，AUC和准确率提高7个点，加入适配组件后准确率再提高7个点。

Conclusion: 在现实条件下检测合成表格数据不仅可行，且可实现高可靠性。

Abstract: The rise of powerful generative models has sparked concerns over data
authenticity. While detection methods have been extensively developed for
images and text, the case of tabular data, despite its ubiquity, has been
largely overlooked. Yet, detecting synthetic tabular data is especially
challenging due to its heterogeneous structure and unseen formats at test time.
We address the underexplored task of detecting synthetic tabular data in the
wild, where tables have variable and previously unseen schemas. We introduce a
novel datum-wise transformer architecture that significantly outperforms the
only previously published baseline, improving both AUC and accuracy by 7
points. By incorporating a table-adaptation component, our model gains an
additional 7 accuracy points, demonstrating enhanced robustness. This work
provides the first strong evidence that detecting synthetic tabular data in
real-world conditions is not only feasible, but can be done with high
reliability.

</details>


### [196] [Calibration through the Lens of Indistinguishability](https://arxiv.org/abs/2509.02279)
*Parikshit Gopalan,Lunjia Hu*

Main category: cs.LG

TL;DR: 本文是关于校准的综述，介绍了校准误差的定义、测量方法及对决策者的意义，提出校准是一种不可区分性的观点。


<details>
  <summary>Details</summary>
Motivation: 鉴于机器学习中概率预测的普遍性，研究如何解释预测概率以及评估概率预测器。

Method: 对近期关于校准基础问题的研究进行综述。

Result: 提出校准是预测者假设的世界与真实世界之间的一种不可区分性，不同校准度量量化了两类世界可被区分的程度。

Conclusion: 阐述了校准误差定义、测量的相关研究对下游决策者使用预测进行决策的意义。

Abstract: Calibration is a classical notion from the forecasting literature which aims
to address the question: how should predicted probabilities be interpreted? In
a world where we only get to observe (discrete) outcomes, how should we
evaluate a predictor that hypothesizes (continuous) probabilities over possible
outcomes? The study of calibration has seen a surge of recent interest, given
the ubiquity of probabilistic predictions in machine learning. This survey
describes recent work on the foundational questions of how to define and
measure calibration error, and what these measures mean for downstream decision
makers who wish to use the predictions to make decisions. A unifying viewpoint
that emerges is that of calibration as a form of indistinguishability, between
the world hypothesized by the predictor and the real world (governed by nature
or the Bayes optimal predictor). In this view, various calibration measures
quantify the extent to which the two worlds can be told apart by certain
classes of distinguishers or statistical measures.

</details>


### [197] [Mitigating Clinician Information Overload: Generative AI for Integrated EHR and RPM Data Analysis](https://arxiv.org/abs/2509.00073)
*Ankit Shetgaonkar,Dipen Pradhan,Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Aman Raj*

Main category: cs.LG

TL;DR: 本文全面概述生成式AI在医疗领域的能力、要求和应用，探讨其改善临床效率的潜力及面临挑战。


<details>
  <summary>Details</summary>
Motivation: 解决因RPM和EHR数据复杂导致的临床医生数据过载问题，提升临床效率。

Method: 先介绍患者数据形式和来源，再探讨大语言模型驱动应用的潜力和面临的挑战。

Result: 发现大语言模型应用可提升纵向患者数据导航和提供临床决策支持，但面临数据整合、质量、隐私等挑战。

Conclusion: 这是首次对管理临床医生数据过载的生成式AI技术进行总结。

Abstract: Generative Artificial Intelligence (GenAI), particularly Large Language
Models (LLMs), offer powerful capabilities for interpreting the complex data
landscape in healthcare. In this paper, we present a comprehensive overview of
the capabilities, requirements and applications of GenAI for deriving clinical
insights and improving clinical efficiency. We first provide some background on
the forms and sources of patient data, namely real-time Remote Patient
Monitoring (RPM) streams and traditional Electronic Health Records (EHRs). The
sheer volume and heterogeneity of this combined data present significant
challenges to clinicians and contribute to information overload. In addition,
we explore the potential of LLM-powered applications for improving clinical
efficiency. These applications can enhance navigation of longitudinal patient
data and provide actionable clinical decision support through natural language
dialogue. We discuss the opportunities this presents for streamlining clinician
workflows and personalizing care, alongside critical challenges such as data
integration complexity, ensuring data quality and RPM data reliability,
maintaining patient privacy, validating AI outputs for clinical safety,
mitigating bias, and ensuring clinical acceptance. We believe this work
represents the first summarization of GenAI techniques for managing clinician
data overload due to combined RPM / EHR data complexities.

</details>


### [198] [Gaming and Cooperation in Federated Learning: What Can Happen and How to Monitor It](https://arxiv.org/abs/2509.02391)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: 本文将联邦学习建模为战略系统，提出分析框架，引入量化指标，总结清单，提供算法和性能保证，模拟验证框架，给出设计原则和操作指南。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的成功依赖参与者的隐蔽行为，需清晰区分真正提升性能和仅针对指标的行为。

Method: 提出分析框架，引入量化行为激励和集体性能损失的两个指标，总结清单，提供资源分配算法和性能保证，结合多种方法实现鲁棒应用。

Result: 模拟验证框架预测模式，释放程序实现可重复性。

Conclusion: 给出设计原则和操作指南，降低指标博弈激励，维持和扩大稳定合作。

Abstract: The success of Federated Learning depends on the actions that participants
take out of sight. We model Federated Learning not as a mere optimization task
but as a strategic system entangled with rules and incentives. From this
perspective, we present an analytical framework that makes it possible to
clearly identify where behaviors that genuinely improve performance diverge
from those that merely target metrics. We introduce two indices that
respectively quantify behavioral incentives and collective performance loss,
and we use them as the basis for consistently interpreting the impact of
operational choices such as rule design, the level of information disclosure,
evaluation methods, and aggregator switching. We further summarize thresholds,
auto-switch rules, and early warning signals into a checklist that can be
applied directly in practice, and we provide both a practical algorithm for
allocating limited audit resources and a performance guarantee. Simulations
conducted across diverse environments consistently validate the patterns
predicted by our framework, and we release all procedures for full
reproducibility. While our approach operates most strongly under several
assumptions, combining periodic recalibration, randomization, and
connectivity-based alarms enables robust application under the variability of
real-world operations. We present both design principles and operational
guidelines that lower the incentives for metric gaming while sustaining and
expanding stable cooperation.

</details>


### [199] [Experimental Assessment of a Multi-Class AI/ML Architecture for Real-Time Characterization of Cyber Events in a Live Research Reactor](https://arxiv.org/abs/2509.00076)
*Zachery Dahm,Konstantinos Vasili,Vasileios Theos,Konstantinos Gkouliaras,William Richards,True Miller,Brian Jowers,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 本文介绍多层AI/ML架构用于核反应堆，结合IT与OT数据流区分安全事件和异常，通过用例验证其能力，显示AI/ML在核安全有前景但需完善。


<details>
  <summary>Details</summary>
Motivation: 核工业对AI/ML应用兴趣增加，但缺乏在运行核反应堆中研究其可行性和适用性的工作，期望利用AI/ML为核领域带来好处。

Method: 引入多层AI/ML架构，结合信息技术和运营技术数据流；利用普渡大学研究反应堆PUR - 1进行多并发虚假数据注入和拒绝服务攻击的用例测试。

Result: AI/ML能区分正常、异常和网络安全相关事件；结合运营和信息技术数据提高分类准确性，但在同步和收集方面有挑战。

Conclusion: AI/ML在核网络安全有显著前景，但处理复杂事件区分和多类架构需进一步完善。

Abstract: There is increased interest in applying Artificial Intelligence and Machine
Learning (AI/ML) within the nuclear industry and nuclear engineering community.
Effective implementation of AI/ML could offer benefits to the nuclear domain,
including enhanced identification of anomalies, anticipation of system
failures, and operational schedule optimization. However, limited work has been
done to investigate the feasibility and applicability of AI/ML tools in a
functioning nuclear reactor. Here, we go beyond the development of a single
model and introduce a multi-layered AI/ML architecture that integrates both
information technology and operational technology data streams to identify,
characterize, and differentiate (i) among diverse cybersecurity events and (ii)
between cyber events and other operational anomalies. Leveraging Purdue
Universitys research reactor, PUR-1, we demonstrate this architecture through a
representative use case that includes multiple concurrent false data injections
and denial-of-service attacks of increasing complexity under realistic reactor
conditions. The use case includes 14 system states (1 normal, 13 abnormal) and
over 13.8 million multi-variate operational and information technology data
points. The study demonstrated the capability of AI/ML to distinguish between
normal, abnormal, and cybersecurity-related events, even under challenging
conditions such as denial-of-service attacks. Combining operational and
information technology data improved classification accuracy but posed
challenges related to synchronization and collection during certain cyber
events. While results indicate significant promise for AI/ML in nuclear
cybersecurity, the findings also highlight the need for further refinement in
handling complex event differentiation and multi-class architectures.

</details>


### [200] [Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models](https://arxiv.org/abs/2509.00083)
*Laksh Patel,Neel Shanbhag*

Main category: cs.LG

TL;DR: 提出Generative Data Cartography框架应对生成模型过拟合和记忆问题，理论证明有效性，实验显示可减少数据泄露且对性能影响小。


<details>
  <summary>Details</summary>
Motivation: 解决现代生成模型过拟合、无意记忆稀有训练样本，可能被对手提取或虚增基准性能的问题。

Method: 提出Generative Data Cartography框架，为预训练样本分配难度和记忆分数，分区样本以指导针对性剪枝和权重调整。

Result: GenDataCarto在10%数据剪枝时将合成金丝雀提取成功率降低超40%，验证困惑度增加小于0.5%。

Conclusion: 有原则的数据干预可大幅减少数据泄露，对生成性能成本最小化。

Abstract: Modern generative models risk overfitting and unintentionally memorizing rare
training examples, which can be extracted by adversaries or inflate benchmark
performance. We propose Generative Data Cartography (GenDataCarto), a
data-centric framework that assigns each pretraining sample a difficulty score
(early-epoch loss) and a memorization score (frequency of ``forget events''),
then partitions examples into four quadrants to guide targeted pruning and
up-/down-weighting. We prove that our memorization score lower-bounds classical
influence under smoothness assumptions and that down-weighting
high-memorization hotspots provably decreases the generalization gap via
uniform stability bounds. Empirically, GenDataCarto reduces synthetic canary
extraction success by over 40\% at just 10\% data pruning, while increasing
validation perplexity by less than 0.5\%. These results demonstrate that
principled data interventions can dramatically mitigate leakage with minimal
cost to generative performance.

</details>


### [201] [Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs](https://arxiv.org/abs/2509.00084)
*Qibin Wang,Pu Zhao,Shaohan Huang,Fangkai Yang,Lu Wang,Furu Wei,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.LG

TL;DR: 为提升大语言模型解决复杂推理问题能力，提出Generative Self - Refinement (GSR)框架，实验在五个数学基准测试中达SOTA，且技能与模型无关。


<details>
  <summary>Details</summary>
Motivation: 现有TTS方法（如Best - of - N和多数投票）有局限性，依赖候选响应质量，引入额外模型选择最佳响应部署成本高，需新方法提升大语言模型解决复杂多步推理问题的能力。

Method: 提出GSR并行测试时间缩放框架，统一模型先并行生成候选响应集，再基于问题和候选响应的提示进行自我细化；设计混合训练管道，联合优化直接解决问题和细化候选响应两个互补目标。

Result: 方法在五个数学基准测试中达到了最先进的性能，学习到的自我细化技能与模型无关，在不同模型规模上稳健，能泛化到分布外推理任务。

Conclusion: GSR框架是一种有效的提升大语言模型解决复杂推理问题能力的方法，其自我细化技能具有模型无关性和良好的泛化性。

Abstract: To further enhance the ability of Large Language Models (LLMs) to solve
complex, multi-step reasoning problems, test-time scaling (TTS) methods have
gained widespread attention. Existing approaches such as Best-of-N and majority
voting are limited as their performance depends on the quality of candidate
responses, making them unable to produce a correct solution when all candidates
are incorrect. Introducing an additional model to select the best response also
incurs significant deployment costs. To this end, we introduce Generative
Self-Refinement (GSR), a novel parallel test-time scaling framework where a
unified model first generates a set of candidate responses in parallel and then
performs self-refinement to synthesize a new superior solution based on a
prompt consisting of the problem and these candidates. However, LLMs struggle
to perform refinement effectively when prompted directly. Therefore, we design
a hybrid training pipeline by jointly optimizing for two complementary
objectives, solving problems directly and refining candidate responses.
Experimental results demonstrate that our method achieves state-of-the-art
performance across five mathematical benchmarks. We further show that this
learned self-refinement skill is a model-agnostic enhancement, robust across
different model scales and generalizing to out-of-distribution reasoning tasks.

</details>


### [202] [Cache Management for Mixture-of-Experts LLMs -- extended version](https://arxiv.org/abs/2509.02408)
*Spyros Angelopoulos,Loris Marchal,Adrien Obrecht,Bertrand Simon*

Main category: cs.LG

TL;DR: 论文研究大语言模型专家管理优化的分页问题，给出算法理论下界，提出基于层的LRU扩展算法，模拟显示其性能优于标准LRU。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数多，内存管理是挑战，基于混合专家的架构需高效管理系统有限缓存。

Method: 提出新的分页问题模型，给出确定性和随机化算法竞争比的下界，提出基于层的LRU扩展算法。

Result: 在合成数据集和实际MoE使用轨迹上的大量模拟显示，提出的算法性能优于经典分页问题的策略如标准LRU。

Conclusion: 所提算法在大语言模型专家管理优化的分页问题上有更好的性能。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a variety of tasks. One of the main challenges towards the successful
deployment of LLMs is memory management, since they typically involve billions
of parameters. To this end, architectures based on Mixture-of-Experts have been
proposed, which aim to reduce the size of the parameters that are activated
when producing a token. This raises the equally critical issue of efficiently
managing the limited cache of the system, in that frequently used experts
should be stored in the fast cache rather than in the slower secondary memory.
  In this work, we introduce and study a new paging problem that models expert
management optimization. Our formulation captures both the layered architecture
of LLMs and the requirement that experts are cached efficiently. We first
present lower bounds on the competitive ratio of both deterministic and
randomized algorithms, which show that under mild assumptions, LRU-like
policies have good theoretical competitive performance. We then propose a
layer-based extension of LRU that is tailored to the problem at hand.
  Extensive simulations on both synthetic datasets and actual traces of MoE
usage show that our algorithm outperforms policies for the classic paging
problem, such as the standard LRU.

</details>


### [203] [Centralized vs. Federated Learning for Educational Data Mining: A Comparative Study on Student Performance Prediction with SAEB Microdata](https://arxiv.org/abs/2509.00086)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 研究评估了联邦学习FedProx算法在巴西教育场景预测学生表现的可行性与有效性，对比集中式模型，联邦模型虽有少量性能损失但隐私性好，是可行方案。


<details>
  <summary>Details</summary>
Motivation: 数据挖掘和人工智能用于教育面临隐私立法限制学生敏感数据集中化的问题，需隐私保护计算方法。

Method: 使用联邦学习FedProx算法，用巴西基础教育评估系统微数据训练深度神经网络模型，模拟50所学校场景，并与集中式XGBoost模型对比。

Result: 集中式模型准确率63.96%，联邦模型峰值准确率61.23%，联邦模型有少量性能损失但有强大隐私保证。

Conclusion: 联邦学习是符合巴西LGPD要求、在巴西教育场景构建协作预测模型的可行有效解决方案。

Abstract: The application of data mining and artificial intelligence in education
offers unprecedented potential for personalizing learning and early
identification of at-risk students. However, the practical use of these
techniques faces a significant barrier in privacy legislation, such as Brazil's
General Data Protection Law (LGPD), which restricts the centralization of
sensitive student data. To resolve this challenge, privacy-preserving
computational approaches are required. The present study evaluates the
feasibility and effectiveness of Federated Learning, specifically the FedProx
algorithm, to predict student performance using microdata from the Brazilian
Basic Education Assessment System (SAEB). A Deep Neural Network (DNN) model was
trained in a federated manner, simulating a scenario with 50 schools, and its
performance was rigorously benchmarked against a centralized eXtreme Gradient
Boosting (XGBoost) model. The analysis, conducted on a universe of over two
million student records, revealed that the centralized model achieved an
accuracy of 63.96%. Remarkably, the federated model reached a peak accuracy of
61.23%, demonstrating a marginal performance loss in exchange for a robust
privacy guarantee. The results indicate that Federated Learning is a viable and
effective solution for building collaborative predictive models in the
Brazilian educational context, in alignment with the requirements of the LGPD.

</details>


### [204] [Yet Unnoticed in LSTM: Binary Tree Based Input Reordering, Weight Regularization, and Gate Nonlinearization](https://arxiv.org/abs/2509.00087)
*Mojtaba Moattari*

Main category: cs.LG

TL;DR: 本文提出输入重排序、权重归一化和门非线性化方法改进LSTM，经文本分类任务验证可提升其准确率。


<details>
  <summary>Details</summary>
Motivation: 当前LSTM模型不能最优聚焦特定旧索引或长期信息，缺乏权重归一化研究及门的非线性化处理。

Method: 提出输入重排序方法对特定输入索引进行优先级排序；研究哪种范数能最好地找到权重间关系；对门进行非线性化处理；将提出方法与简单LSTM进行对比。

Result: 所提方法在文本分类任务中提高了LSTM的准确率。

Conclusion: 所提的输入重排序、权重归一化和门非线性化方法能有效提升LSTM性能。

Abstract: LSTM models used in current Machine Learning literature and applications, has
a promising solution for permitting long term information using gating
mechanisms that forget and reduce effect of current input information. However,
even with this pipeline, they do not optimally focus on specific old index or
long-term information. This paper elaborates upon input reordering approaches
to prioritize certain input indices. Moreover, no LSTM based approach is found
in the literature that examines weight normalization while choosing the right
weight and exponent of Lp norms through main supervised loss function. In this
paper, we find out which norm best finds relationship between weights to either
smooth or sparsify them. Lastly, gates, as weighted representations of inputs
and states, which control reduction-extent of current input versus previous
inputs (~ state), are not nonlinearized enough (through a small FFNN). As
analogous to attention mechanisms, gates easily filter current information to
bold (emphasize on) past inputs. Nonlinearized gates can more easily tune up to
peculiar nonlinearities of specific input in the past. This type of
nonlinearization is not proposed in the literature, to the best of author's
knowledge. The proposed approaches are implemented and compared with a simple
LSTM to understand their performance in text classification tasks. The results
show they improve accuracy of LSTM.

</details>


### [205] [Learning from Peers: Collaborative Ensemble Adversarial Training](https://arxiv.org/abs/2509.00089)
*Li Dengjin,Guo Yanming,Xie Yuxiang,Li Zheng,Chen Jiangming,Li Xiaolong,Lao Mingrui*

Main category: cs.LG

TL;DR: 本文针对现有集成对抗训练（EAT）忽略子模型合作的问题，提出协作集成对抗训练（CEAT）方法，实验表明该方法性能优越且具有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有EAT策略独立训练子模型，忽略子模型间的合作收益，影响集成模型的鲁棒性。

Method: 提出CEAT方法，在对抗训练中让子模型关注预测差异大的样本，利用概率差异自适应分配样本权重，并引入校准距离正则化。

Result: 在广泛采用的数据集上的实验表明，CEAT方法相比其他EAT方法达到了最先进的性能。

Conclusion: CEAT方法是模型无关的，可无缝应用于各种集成方法，具有灵活的适用性。

Abstract: Ensemble Adversarial Training (EAT) attempts to enhance the robustness of
models against adversarial attacks by leveraging multiple models. However,
current EAT strategies tend to train the sub-models independently, ignoring the
cooperative benefits between sub-models. Through detailed inspections of the
process of EAT, we find that that samples with classification disparities
between sub-models are close to the decision boundary of ensemble, exerting
greater influence on the robustness of ensemble. To this end, we propose a
novel yet efficient Collaborative Ensemble Adversarial Training (CEAT), to
highlight the cooperative learning among sub-models in the ensemble. To be
specific, samples with larger predictive disparities between the sub-models
will receive greater attention during the adversarial training of the other
sub-models. CEAT leverages the probability disparities to adaptively assign
weights to different samples, by incorporating a calibrating distance
regularization. Extensive experiments on widely-adopted datasets show that our
proposed method achieves the state-of-the-art performance over competitive EAT
methods. It is noteworthy that CEAT is model-agnostic, which can be seamlessly
adapted into various ensemble methods with flexible applicability.

</details>


### [206] [Learning to Shard: RL for Co-optimizing the Parallelism Degrees and Per-operator Sharding Dimensions in Distributed LLM Inference](https://arxiv.org/abs/2509.00217)
*Ruokai Yin,Sattwik Deb Mishra,Xuan Zuo,Hokchhay Tann,Preyas Shah,Apala Guha*

Main category: cs.LG

TL;DR: 提出Learn to Shard方法为分布式大语言模型推理联合优化并行度和算子分片维度，在H100集群上评估有显著吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 当前分布式大语言模型推理系统如Megatron - LM依赖静态启发式方法，在模型规模扩大和硬件拓扑多样化时性能不佳。

Method: 采用基于注意力的策略，从精英历史中的高性能策略学习，以高效探索巨大的组合搜索空间。

Result: 在H100集群上使用高达1.6T参数的MoE模型评估，Learn to Shard比元启发式基线实现了高达3.5倍的吞吐量提升，比Megatron启发式方法实现了1.06倍的提升。

Conclusion: Learn to Shard能有效联合优化分布式大语言模型推理的并行度和算子分片维度，提升推理性能。

Abstract: Distributed LLM inference requires careful coordination of parallelization
strategies across hundreds to thousands of NPUs to meet production SLOs.
Current systems like Megatron-LM rely on static heuristics that separately
configure parallelism degrees and per-operator sharding dimensions, leaving
significant performance on the table as models scale and hardware topologies
diversify. We introduce Learn to Shard, to our knowledge, the first RL-based
approach to co-optimize both coarse-grained parallelism degrees and
fine-grained per-operator sharding dimensions for distributed LLM inference.
Our method employs an attention-based policy over an elite history that learns
from high-performing strategies to efficiently navigate the vast combinatorial
search space. Evaluated on H100 clusters with MoE models up to 1.6T parameters,
Learn to Shard achieves up to 3.5x throughput improvement over metaheuristic
baselines and 1.06x over Megatron heuristics.

</details>


### [207] [Context-Action Embedding Learning for Off-Policy Evaluation in Contextual Bandits](https://arxiv.org/abs/2509.00648)
*Kushagra Chandak,Vincent Liu,Haanvid Lee*

Main category: cs.LG

TL;DR: 提出CAEL - MIPS方法解决MIPS估计器不足，实证表明其MSE表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有Inverse Propensity Score (IPS)权重法在大动作空间或部分上下文 - 动作空间探索不足时方差大，Marginalized IPS (MIPS)估计器未最小化均方误差且未考虑上下文信息。

Method: 引入Context - Action Embedding Learning for MIPS (CAEL - MIPS)，从离线数据学习上下文 - 动作嵌入以最小化MIPS估计器的均方误差，并给出最小化均方误差的目标。

Result: 在合成数据集和真实数据集的实证研究中，CAEL - MIPS估计器在均方误差方面优于基线。

Conclusion: CAEL - MIPS能有效解决现有方法的局限，具有更好的性能。

Abstract: We consider off-policy evaluation (OPE) in contextual bandits with finite
action space. Inverse Propensity Score (IPS) weighting is a widely used method
for OPE due to its unbiased, but it suffers from significant variance when the
action space is large or when some parts of the context-action space are
underexplored. Recently introduced Marginalized IPS (MIPS) estimators mitigate
this issue by leveraging action embeddings. However, these embeddings do not
minimize the mean squared error (MSE) of the estimators and do not consider
context information. To address these limitations, we introduce Context-Action
Embedding Learning for MIPS, or CAEL-MIPS, which learns context-action
embeddings from offline data to minimize the MSE of the MIPS estimator.
Building on the theoretical analysis of bias and variance of MIPS, we present
an MSE-minimizing objective for CAEL-MIPS. In the empirical studies on a
synthetic dataset and a real-world dataset, we demonstrate that our estimator
outperforms baselines in terms of MSE.

</details>


### [208] [Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning LLMs](https://arxiv.org/abs/2509.00096)
*Yao Fu,Runchao Li,Xianxuan Long,Haotian Yu,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.LG

TL;DR: 揭示神经网络剪枝影响大语言模型测谎能力，提出TPLO方法及提示规则，提升剪枝模型幻觉检测和问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法会破坏大语言模型用于测谎的内部激活特征，需找到不牺牲测谎能力的剪枝方法。

Method: 提出基于层异常值的真实剪枝（TPLO）方法，同时引入提示规则丰富TruthfulQA基准。

Result: 该方法在50%稀疏度下实现88%的幻觉检测准确率，提升了模型在TruthfulQA上的性能。

Conclusion: TPLO方法能在剪枝时保留大语言模型测谎关键特征，提升剪枝模型性能。

Abstract: Neural network pruning has emerged as a promising approach for deploying LLMs
in low-resource scenarios while preserving downstream task performance.
However, for the first time, we reveal that such pruning disrupts LLMs'
internal activation features crucial for lie detection, where probing
classifiers (typically small logistic regression models) trained on these
features assess the truthfulness of LLM-generated statements. This discovery
raises a crucial open question: how can we prune LLMs without sacrificing these
critical lie detection capabilities? Our investigation further reveals that
naively adjusting layer-wise pruning sparsity based on importance inadvertently
removes crucial weights, failing to improve lie detection performance despite
its reliance on the most crucial LLM layer. To address this issue, we propose
Truthful Pruning aligned by Layer-wise Outliers (TPLO), which places greater
emphasis on layers with more activation outliers and stronger discriminative
features simultaneously. This preserves LLMs' original performance while
retaining critical features of inner states needed for robust lie detection.
Moreover, we introduce a prompting rule to enrich the TruthfulQA benchmark for
better calibrating LLM pruning. Empirical results show that our approach
improves the hallucination detection for pruned LLMs (achieving 88% accuracy at
50% sparsity) and enhances their performance on TruthfulQA.

</details>


### [209] [Progressive Element-wise Gradient Estimation for Neural Network Quantization](https://arxiv.org/abs/2509.00097)
*Kaiqi Zhao*

Main category: cs.LG

TL;DR: 提出 Progressive Element - wise Gradient Estimation (PEGE) 替代 STE 用于神经网络量化，可提升量化模型精度，实验表明其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有 Quantization - Aware Training (QAT) 方法中的 Straight - Through Estimator (STE) 忽略连续与量化值的离散化误差，导致精度下降，尤其是低比特宽度时。

Method: 提出 PEGE，通过对数课程驱动的混合精度替换策略逐步用量化值替代全精度的权重和激活值，将 QAT 表述为共同优化问题，同时最小化预测任务损失和量化离散化误差。

Result: 在 CIFAR - 10 和 ImageNet 上对多种架构（如 ResNet、VGG）的大量实验显示，PEGE 始终优于现有反向传播方法，使低精度模型精度可匹配甚至超越全精度模型。

Conclusion: PEGE 是 STE 的有效替代方案，能无缝集成到前向传播方法中，提高量化模型准确性，是一个统一且可推广的框架。

Abstract: Neural network quantization aims to reduce the bit-widths of weights and
activations, making it a critical technique for deploying deep neural networks
on resource-constrained hardware. Most Quantization-Aware Training (QAT)
methods rely on the Straight-Through Estimator (STE) to address the
non-differentiability of discretization functions by replacing their
derivatives with that of the identity function. While effective, STE overlooks
discretization errors between continuous and quantized values, which can lead
to accuracy degradation -- especially at extremely low bit-widths. In this
paper, we propose Progressive Element-wise Gradient Estimation (PEGE), a simple
yet effective alternative to STE, which can be seamlessly integrated with any
forward propagation methods and improves the quantized model accuracy. PEGE
progressively replaces full-precision weights and activations with their
quantized counterparts via a novel logarithmic curriculum-driven
mixed-precision replacement strategy. Then it formulates QAT as a
co-optimization problem that simultaneously minimizes the task loss for
prediction and the discretization error for quantization, providing a unified
and generalizable framework. Extensive experiments on CIFAR-10 and ImageNet
across various architectures (e.g., ResNet, VGG) demonstrate that PEGE
consistently outperforms existing backpropagation methods and enables
low-precision models to match or even outperform the accuracy of their
full-precision counterparts.

</details>


### [210] [LLM-QUBO: An End-to-End Framework for Automated QUBO Transformation from Natural Language Problem Descriptions](https://arxiv.org/abs/2509.00099)
*Huixiang Zhang,Mahzabeen Emu,Salimur Choudhury*

Main category: cs.LG

TL;DR: 提出LLM - QUBO端到端框架解决量子退火应用难题，结合大模型与混合量子 - 经典方法，降低应用门槛。


<details>
  <summary>Details</summary>
Motivation: 量子退火解决NP - 难组合优化问题时，存在问题转化为QUBO格式过程复杂及当前量子硬件可扩展性受限的挑战。

Method: 提出LLM - QUBO框架，用大语言模型解析自然语言生成数学表示，集成混合量子 - 经典Benders分解方法，分区处理问题。

Result: 使用经典求解器验证生成QUBO的正确性和混合方法的可扩展性，建立了性能基线。

Conclusion: 该协同计算范式解决优化问题实际应用关键挑战，降低入门门槛，为量子设备用于大规模现实优化问题提供途径。

Abstract: Quantum annealing offers a promising paradigm for solving NP-hard
combinatorial optimization problems, but its practical application is severely
hindered by two challenges: the complex, manual process of translating problem
descriptions into the requisite Quadratic Unconstrained Binary Optimization
(QUBO) format and the scalability limitations of current quantum hardware. To
address these obstacles, we propose a novel end-to-end framework, LLM-QUBO,
that automates this entire formulation-to-solution pipeline. Our system
leverages a Large Language Model (LLM) to parse natural language, automatically
generating a structured mathematical representation. To overcome hardware
limitations, we integrate a hybrid quantum-classical Benders' decomposition
method. This approach partitions the problem, compiling the combinatorial
complex master problem into a compact QUBO format, while delegating linearly
structured sub-problems to classical solvers. The correctness of the generated
QUBO and the scalability of the hybrid approach are validated using classical
solvers, establishing a robust performance baseline and demonstrating the
framework's readiness for quantum hardware. Our primary contribution is a
synergistic computing paradigm that bridges classical AI and quantum computing,
addressing key challenges in the practical application of optimization problem.
This automated workflow significantly reduces the barrier to entry, providing a
viable pathway to transform quantum devices into accessible accelerators for
large-scale, real-world optimization challenges.

</details>


### [211] [Exploiting a Mixture-of-Layers in an Electrocardiography Foundation Model](https://arxiv.org/abs/2509.00102)
*Phu X. Nguyen,Huy Phan,Hieu Pham,Christos Chatzichristos,Bert Vandenberk,Maarten De Vos*

Main category: cs.LG

TL;DR: 本文探讨心电图Transformer基础模型各层表征，提出PMA架构有效利用层表征多样性，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer心电图基础模型各层内部表征未被充分理解和利用，探究最后一层是否为下游任务最优表征层。

Method: 先使用一维ViT通过掩码建模对模型进行预训练；下游应用中用门控网络选择性融合各层表征；将该方法扩展到预训练阶段，通过分组平均聚合所有表征后输入基于解码器的Transformer。

Result: 未明确提及具体结果。

Conclusion: 提出的PMA架构能有效利用模型层表征多样性，可提升下游应用性能。

Abstract: Transformer-based foundation models for Electrocardiograms (ECGs) have
recently achieved impressive performance in many downstream applications.
However, the internal representations of such models across layers have not
been fully understood and exploited. An important question arises: Does the
final layer of the pre-trained Transformer model, the \emph{de facto}
representational layer, provide optimal performance for downstream tasks?
Although our answer based on empirical and theoretical analyses for this
question is negative, we propose a novel approach to leverage the
representation diversity of the model's layers effectively. Specifically, we
introduce a novel architecture called Post-pretraining Mixture-of-layers
Aggregation (PMA), which enables a flexible combination of the layer-wise
representations from the layer stack of a Transformer-based foundation model.
We first pre-train the model from ECG signals using the 1-dimensional Vision
Transformer (ViT) via masked modeling. In downstream applications, instead of
relying solely on the last layer of the model, we employ a gating network to
selectively fuse the representations from the pretrained model's layers,
thereby enhancing representation power and improving performance of the
downstream applications. In addition, we extend the proposed method to the
pretraining stage by aggregating all representations through group-wise
averaging before feeding them into the decoder-based Transformer.

</details>


### [212] [ART: Adaptive Resampling-based Training for Imbalanced Classification](https://arxiv.org/abs/2509.00955)
*Arjun Basandrai,Shourya Jain,K. Ilanthenral*

Main category: cs.LG

TL;DR: 本文提出自适应重采样训练（ART）方法，根据模型的类级性能更新训练数据分布，在多基准测试中表现优于现有方法，提升了不平衡分类的性能。


<details>
  <summary>Details</summary>
Motivation: 传统重采样方法使用固定分布，忽略类学习难度变化，限制了模型整体性能。

Method: 提出ART方法，定期根据模型的类级性能更新训练数据分布，用类级宏观F1分数确定重采样程度，在类级别进行自适应调整。

Result: 在多个基准测试中，ART始终优于基于重采样和算法级别的方法，提升了宏观F1分数，且多数情况下改进具有统计学意义。

Conclusion: ART是不平衡分类的可靠选择，能稳定地提供最强的宏观F1分数。

Abstract: Traditional resampling methods for handling class imbalance typically uses
fixed distributions, undersampling the majority or oversampling the minority.
These static strategies ignore changes in class-wise learning difficulty, which
can limit the overall performance of the model.
  This paper proposes an Adaptive Resampling-based Training (ART) method that
periodically updates the distribution of the training data based on the
class-wise performance of the model. Specifically, ART uses class-wise macro F1
scores, computed at fixed intervals, to determine the degree of resampling to
be performed.
  Unlike instance-level difficulty modeling, which is noisy and
outlier-sensitive, ART adapts at the class level. This allows the model to
incrementally shift its attention towards underperforming classes in a way that
better aligns with the optimization objective.
  Results on diverse benchmarks, including Pima Indians Diabetes and Yeast
dataset demonstrate that ART consistently outperforms both resampling-based and
algorithm-level methods, including Synthetic Minority Oversampling Technique
(SMOTE), NearMiss Undersampling, and Cost-sensitive Learning on binary as well
as multi-class classification tasks with varying degrees of imbalance.
  In most settings, these improvements are statistically significant. On
tabular datasets, gains are significant under paired t-tests and Wilcoxon tests
(p < 0.05), while results on text and image tasks remain favorable. Compared to
training on the original imbalanced data, ART improves macro F1 by an average
of 2.64 percentage points across all tested tabular datasets. Unlike existing
methods, whose performance varies by task, ART consistently delivers the
strongest macro F1, making it a reliable choice for imbalanced classification.

</details>


### [213] [Pre-trained knowledge elevates large language models beyond traditional chemical reaction optimizers](https://arxiv.org/abs/2509.00103)
*Robert MacKnight,Jose Emilio Regio,Jeffrey G. Ethier,Luke A. Baldwin,Gabe Gomes*

Main category: cs.LG

TL;DR: 研究表明大语言模型（LLMs）改变实验化学优化范式，对比LLM - GO与BO和随机采样，发现LLM - GO在多数单目标数据集表现佳，发布Iron Mind平台。


<details>
  <summary>Details</summary>
Motivation: 探究预训练的大语言模型知识对实验化学优化范式的影响。

Method: 使用六个全枚举的分类反应数据集，对比LLM - GO、BO和随机采样，引入拓扑无关信息理论框架分析采样多样性。

Result: 前沿LLMs在五个单目标数据集上表现与或超BO，BO仅在显式多目标权衡中占优，LLMs探索熵更高。

Conclusion: LLM - GO在传统方法难以处理的复杂分类空间中表现出色。

Abstract: Modern optimization in experimental chemistry employs algorithmic search
through black-box parameter spaces. Here we demonstrate that pre-trained
knowledge in large language models (LLMs) fundamentally changes this paradigm.
Using six fully enumerated categorical reaction datasets (768 - 5,684
experiments), we benchmark LLM-guided optimization (LLM-GO) against Bayesian
optimization (BO) and random sampling. Frontier LLMs consistently match or
exceed BO performance across five single-objective datasets, with advantages
growing as parameter complexity increases and high-performing conditions become
scarce (<5% of space). BO retains superiority only for explicit multi-objective
trade-offs. To understand these contrasting behaviors, we introduce a
topology-agnostic information theory framework quantifying sampling diversity
throughout optimization campaigns. This analysis reveals that LLMs maintain
systematically higher exploration entropy than BO across all datasets while
achieving superior performance, with advantages most pronounced in
solution-scarce parameter spaces where high-entropy exploration typically fails
- suggesting that pre-trained domain knowledge enables more effective
navigation of chemical parameter space rather than replacing structured
exploration strategies. To enable transparent benchmarking and community
validation, we release Iron Mind (https://gomes.andrew.cmu.edu/iron-mind), a
no-code platform for side-by-side evaluation of human, algorithmic, and LLM
optimization campaigns with public leaderboards and complete trajectories. Our
findings establish that LLM-GO excels precisely where traditional methods
struggle: complex categorical spaces requiring domain understanding rather than
mathematical optimization.

</details>


### [214] [Principled Approximation Methods for Efficient and Scalable Deep Learning](https://arxiv.org/abs/2509.00174)
*Pedro Savarese*

Main category: cs.LG

TL;DR: 本文研究深度学习系统效率提升的近似方法，涉及架构设计、模型压缩和优化，实验表明方法可提升效率并维持性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型计算和能源需求增长，部署和应用存在障碍，需提升效率。

Method: 研究架构设计、模型压缩和优化三种方法，如模型压缩中提出新近似方法，架构设计中设计搜索算法，优化方面提出自适应优化器。

Result: 在图像分类、语言建模和生成建模任务实验中，所提方法提升训练和推理效率，维持或提升模型性能。

Conclusion: 通过可扩展且有原则的近似方法能解决计算难题，提升深度学习系统效率。

Abstract: Recent progress in deep learning has been driven by increasingly larger
models. However, their computational and energy demands have grown
proportionally, creating significant barriers to their deployment and to a
wider adoption of deep learning technologies. This thesis investigates
principled approximation methods for improving the efficiency of deep learning
systems, with a particular focus on settings that involve discrete constraints
and non-differentiability.
  We study three main approaches toward improved efficiency: architecture
design, model compression, and optimization. For model compression, we propose
novel approximations for pruning and quantization that frame the underlying
discrete problem as continuous and differentiable, enabling gradient-based
training of compression schemes alongside the model's parameters. These
approximations allow for fine-grained sparsity and precision configurations,
leading to highly compact models without significant fine-tuning. In the
context of architecture design, we design an algorithm for neural architecture
search that leverages parameter sharing across layers to efficiently explore
implicitly recurrent architectures. Finally, we study adaptive optimization,
revisiting theoretical properties of widely used methods and proposing an
adaptive optimizer that allows for quick hyperparameter tuning.
  Our contributions center on tackling computationally hard problems via
scalable and principled approximations. Experimental results on image
classification, language modeling, and generative modeling tasks show that the
proposed methods provide significant improvements in terms of training and
inference efficiency while maintaining, or even improving, the model's
performance.

</details>


### [215] [CCE: Confidence-Consistency Evaluation for Time Series Anomaly Detection](https://arxiv.org/abs/2509.01098)
*Zhijie Zhong,Zhiwen Yu,Yiu-ming Cheung,Kaixiang Yang*

Main category: cs.LG

TL;DR: 本文针对现有时间序列异常检测指标的局限，提出了CCE指标和RankEval基准，二者皆开源。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测指标存在判别力不足、依赖超参数、对扰动敏感和计算开销大等局限。

Method: 采用贝叶斯估计量化异常分数的不确定性，构建全局和事件级的置信度与一致性分数得到CCE指标；建立RankEval基准以比较各指标的排序能力。

Result: 理论和实验表明CCE有严格的有界性、对分数扰动的Lipschitz鲁棒性和线性时间复杂度；RankEval是首个标准化、可复现的评估指标客观比较管道。

Conclusion: 提出的CCE指标和RankEval基准能有效解决现有指标的问题，且二者实现均完全开源。

Abstract: Time Series Anomaly Detection metrics serve as crucial tools for model
evaluation. However, existing metrics suffer from several limitations:
insufficient discriminative power, strong hyperparameter dependency,
sensitivity to perturbations, and high computational overhead. This paper
introduces Confidence-Consistency Evaluation (CCE), a novel evaluation metric
that simultaneously measures prediction confidence and uncertainty consistency.
By employing Bayesian estimation to quantify the uncertainty of anomaly scores,
we construct both global and event-level confidence and consistency scores for
model predictions, resulting in a concise CCE metric. Theoretically and
experimentally, we demonstrate that CCE possesses strict boundedness, Lipschitz
robustness against score perturbations, and linear time complexity
$\mathcal{O}(n)$. Furthermore, we establish RankEval, a benchmark for comparing
the ranking capabilities of various metrics. RankEval represents the first
standardized and reproducible evaluation pipeline that enables objective
comparison of evaluation metrics. Both CCE and RankEval implementations are
fully open-source.

</details>


### [216] [FNODE: Flow-Matching for data-driven simulation of constrained multibody systems](https://arxiv.org/abs/2509.00183)
*Hongyu Wang,Jingquan Wang,Dan Negrut*

Main category: cs.LG

TL;DR: 提出Flow - Matching Neural Ordinary Differential Equation (FNODE)框架解决约束多体系统数据驱动建模的高计算成本和长期预测精度有限问题，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决约束多体系统数据驱动建模中高计算成本和长期预测精度有限的问题。

Method: 引入FNODE框架，直接从轨迹数据学习加速度向量场，通过监督加速度而非积分状态来重新制定训练目标，用数值微分技术（如混合FFT和FD方案）计算加速度目标。

Result: 在多个基准测试中，FNODE始终优于Multi - Body Dynamic Neural ODE (MBD - NODE)、Long Short - Term Memory (LSTM)网络和Fully Connected Neural Networks (FCNN)等现有方法。

Conclusion: FNODE具有良好的准确性、泛化性和计算效率。

Abstract: Data-driven modeling of constrained multibody systems faces two persistent
challenges: high computational cost and limited long-term prediction accuracy.
To address these issues, we introduce the Flow-Matching Neural Ordinary
Differential Equation (FNODE), a framework that learns acceleration vector
fields directly from trajectory data. By reformulating the training objective
to supervise accelerations rather than integrated states, FNODE eliminates the
need for backpropagation through an ODE solver, which represents a bottleneck
in traditional Neural ODEs. Acceleration targets are computed efficiently using
numerical differentiation techniques, including a hybrid Fast Fourier Transform
(FFT) and Finite Difference (FD) scheme. We evaluate FNODE on a diverse set of
benchmarks, including the single and triple mass-spring-damper systems, double
pendulum, slider-crank, and cart-pole. Across all cases, FNODE consistently
outperforms existing approaches such as Multi-Body Dynamic Neural ODE
(MBD-NODE), Long Short-Term Memory (LSTM) networks, and Fully Connected Neural
Networks (FCNN), demonstrating good accuracy, generalization, and computational
efficiency.

</details>


### [217] [Nonlinear Performative Prediction](https://arxiv.org/abs/2509.01139)
*Guangzheng Zhong,Yang Liu,Jiming Liu*

Main category: cs.LG

TL;DR: 本文放松不合理假设，将执行性预测推广到非线性情况，提出保证预测模型执行稳定性的算法，实验效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有执行性预测研究依赖不可控假设，多聚焦线性情况，而现实数据多为非线性。

Method: 用最大间隔方法构建损失函数，通过核方法扩展到非线性空间，用预测误差差异量化数据分布偏移，推导执行稳定性条件并开发算法。

Result: 在合成和真实数据集上的实验表明，方法效果优于现有基线。

Conclusion: 所提方法能有效将执行性预测推广到非线性情况，并保证预测模型的执行稳定性。

Abstract: Performative prediction is an emerging paradigm in machine learning that
addresses scenarios where the model's prediction may induce a shift in the
distribution of the data it aims to predict. Current works in this field often
rely on uncontrollable assumptions, such as bounded gradients of performative
loss, and primarily focus on linear cases in their examples and evaluations to
maintain consistency between theoretical guarantees and empirical validations.
However, such linearity rarely holds in real-world applications, where the data
usually exhibit complex nonlinear characteristics. In this paper, we relax
these out-of-control assumptions and present a novel design that generalizes
performative prediction to nonlinear cases while preserving essential
theoretical properties. Specifically, we formulate the loss function of
performative prediction using a maximum margin approach and extend it to
nonlinear spaces through kernel methods. To quantify the data distribution
shift, we employ the discrepancy between prediction errors on these two
distributions as an indicator, which characterizes the impact of the
performative effect on specific learning tasks. By doing so, we can derive, for
both linear and nonlinear cases, the conditions for performative stability, a
critical and desirable property in performative contexts. Building on these
theoretical insights, we develop an algorithm that guarantees the performative
stability of the predictive model. We validate the effectiveness of our method
through experiments on synthetic and real-world datasets with both linear and
nonlinear data distributions, demonstrating superior performance compared to
state-of-the-art baselines.

</details>


### [218] [Democratizing Agentic AI with Fast Test-Time Scaling on the Edge](https://arxiv.org/abs/2509.00195)
*Hao Mark Chen,Zhiwen Mo,Guanxi Lu,Shuang Liang,Lingxiao Ma,Wayne Luk,Hongxiang Fan*

Main category: cs.LG

TL;DR: 提出FlashTTS系统使TTS适用于内存受限的大语言模型推理，实现边缘设备高性能AI。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署智能AI受内存限制，现有TTS方法在边缘硬件开销大，需解决推理问题。

Method: 引入三种协同优化：推测束扩展、非对称多模型内存分配、动态前缀感知调度，构建基于vLLM的即插即用库。

Result: FlashTTS使单消费级GPU上边缘大语言模型达到云模型的准确率和延迟，平均吞吐量高2.2倍，延迟降低38%-68%。

Conclusion: FlashTTS为边缘设备上民主化、高性能智能AI铺平道路。

Abstract: Deploying agentic AI on edge devices is crucial for privacy and
responsiveness, but memory constraints typically relegate these systems to
smaller Large Language Models (LLMs) with inferior reasoning capabilities.
Test-Time Scaling (TTS) can bridge this reasoning gap by dedicating more
compute during inference, but existing methods incur prohibitive overhead on
edge hardware. To overcome this, we introduce FlashTTS, a serving system that
makes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces
three synergistic optimizations: (i) Speculative Beam Extension to mitigate
system stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model
Memory Allocation to dynamically balance memory between generation and
verification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache
reuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on
a single consumer GPU (24 GB) to match the accuracy and latency of large cloud
models. Our evaluation demonstrates that FlashTTS achieves an average 2.2x
higher goodput and reduces latency by 38%-68% compared to a vLLM baseline,
paving the way for democratized, high-performance agentic AI on edge devices.

</details>


### [219] [ADMP-GNN: Adaptive Depth Message Passing GNN](https://arxiv.org/abs/2509.01170)
*Yassine Abbahaddou,Fragkiskos D. Malliaros,Johannes F. Lutzeyer,Michalis Vazirgiannis*

Main category: cs.LG

TL;DR: 本文指出GNN对所有节点使用固定消息传递步数的问题，提出ADMP - GNN框架动态调整各节点消息传递层数，在节点分类任务中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: GNN对所有节点使用固定消息传递步数，未考虑节点不同计算需求和特征，而实际不同特征节点的最优消息传递层数不同。

Method: 提出Adaptive Depth Message Passing GNN (ADMP - GNN)框架，动态调整每个节点的消息传递层数，且适用于遵循消息传递方案的任何模型。

Result: 在节点分类任务中，ADMP - GNN性能优于基线GNN模型。

Conclusion: ADMP - GNN框架通过动态调整节点消息传递层数，能有效提升性能，且具有广泛适用性。

Abstract: Graph Neural Networks (GNNs) have proven to be highly effective in various
graph learning tasks. A key characteristic of GNNs is their use of a fixed
number of message-passing steps for all nodes in the graph, regardless of each
node's diverse computational needs and characteristics. Through empirical
real-world data analysis, we demonstrate that the optimal number of
message-passing layers varies for nodes with different characteristics. This
finding is further supported by experiments conducted on synthetic datasets. To
address this, we propose Adaptive Depth Message Passing GNN (ADMP-GNN), a novel
framework that dynamically adjusts the number of message passing layers for
each node, resulting in improved performance. This approach applies to any
model that follows the message passing scheme. We evaluate ADMP-GNN on the node
classification task and observe performance improvements over baseline GNN
models.

</details>


### [220] [Online Identification of IT Systems through Active Causal Learning](https://arxiv.org/abs/2509.02130)
*Kim Hammar,Rolf Stadler*

Main category: cs.LG

TL;DR: 提出在线、数据驱动的主动因果学习方法识别 IT 系统因果模型，经实验验证准确且干扰小。


<details>
  <summary>Details</summary>
Motivation: 传统由领域专家设计和维护因果模型，在现代 IT 系统复杂性和动态性增加时面临挑战。

Method: 采用基于高斯过程回归的主动因果学习方法，通过基于滚动的干预策略收集系统测量数据，迭代估计因果函数。

Result: 方法在贝叶斯意义上最优，能产生有效干预，在测试台上实验显示可准确识别因果系统模型，对系统操作干扰小。

Conclusion: 所提出的主动因果学习方法可用于在线、数据驱动地识别 IT 系统的因果模型。

Abstract: Identifying a causal model of an IT system is fundamental to many branches of
systems engineering and operation. Such a model can be used to predict the
effects of control actions, optimize operations, diagnose failures, detect
intrusions, etc., which is central to achieving the longstanding goal of
automating network and system management tasks. Traditionally, causal models
have been designed and maintained by domain experts. This, however, proves
increasingly challenging with the growing complexity and dynamism of modern IT
systems. In this paper, we present the first principled method for online,
data-driven identification of an IT system in the form of a causal model. The
method, which we call active causal learning, estimates causal functions that
capture the dependencies among system variables in an iterative fashion using
Gaussian process regression based on system measurements, which are collected
through a rollout-based intervention policy. We prove that this method is
optimal in the Bayesian sense and that it produces effective interventions.
Experimental validation on a testbed shows that our method enables accurate
identification of a causal system model while inducing low interference with
system operations.

</details>


### [221] [MatPROV: A Provenance Graph Dataset of Material Synthesis Extracted from Scientific Literature](https://arxiv.org/abs/2509.01042)
*Hirofumi Tsuruta,Masaya Kumagai*

Main category: cs.LG

TL;DR: 文章采用PROV - DM标准，构建MatPROV数据集，以解决现有合成流程结构化提取方法的局限性，利于未来自动化合成规划和优化研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究在结构化提取合成流程时依赖刚性特定领域模式或假设流程为线性操作序列，无法捕捉实际流程的结构复杂性，因此需要改进方法。

Method: 采用国际标准PROV - DM，使用大语言模型从科学文献中提取符合PROV - DM的合成流程，构建MatPROV数据集，并以直观有向图表示。

Result: 构建了MatPROV数据集，该数据集能捕捉材料、操作和条件之间的结构复杂性和因果关系，实现机器可解释的合成知识。

Conclusion: 这种表示方法为未来自动化合成规划和优化等研究提供了机会。

Abstract: Synthesis procedures play a critical role in materials research, as they
directly affect material properties. With data-driven approaches increasingly
accelerating materials discovery, there is growing interest in extracting
synthesis procedures from scientific literature as structured data. However,
existing studies often rely on rigid, domain-specific schemas with predefined
fields for structuring synthesis procedures or assume that synthesis procedures
are linear sequences of operations, which limits their ability to capture the
structural complexity of real-world procedures. To address these limitations,
we adopt PROV-DM, an international standard for provenance information, which
supports flexible, graph-based modeling of procedures. We present MatPROV, a
dataset of PROV-DM-compliant synthesis procedures extracted from scientific
literature using large language models. MatPROV captures structural
complexities and causal relationships among materials, operations, and
conditions through visually intuitive directed graphs. This representation
enables machine-interpretable synthesis knowledge, opening opportunities for
future research such as automated synthesis planning and optimization.

</details>


### [222] [From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive Inference](https://arxiv.org/abs/2509.00202)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: 本文提出TConstFormer架构，通过创新机制实现恒定大小KV Cache和低计算复杂度，在长文本推理任务上有优势，利于流式语言模型应用。


<details>
  <summary>Details</summary>
Motivation: Transformer自回归推理的KV Cache线性增长、计算复杂度高，阻碍其处理超长序列，需解决该限制。

Method: 基于TLinFormer工作，引入TConstFormer架构，采用周期性状态更新机制实现恒定大小KV Cache。

Result: 理论计算和实验表明，TConstFormer在长文本推理任务的速度、内存效率和整体性能上远超基线模型。

Conclusion: 该突破为高效、强大的流式语言模型应用奠定基础。

Abstract: Although the Transformer has become the cornerstone of modern AI, its
autoregressive inference suffers from a linearly growing KV Cache and a
computational complexity of O(N^2 d), severely hindering its ability to process
ultra-long sequences. To overcome this limitation, this paper introduces the
TConstFormer architecture, building upon our previous work, TLinFormer.
TConstFormer employs an innovative periodic state update mechanism to achieve a
truly constant-size O(1) KV Cache. The computational complexity of this
mechanism is also O(1) in an amortized sense: it performs purely constant-time
computations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single
linear-time global information synchronization only on the $k$-th step.
Theoretical calculations and experimental results demonstrate that TConstFormer
exhibits an overwhelming advantage over baseline models in terms of speed,
memory efficiency, and overall performance on long-text inference tasks. This
breakthrough paves the way for efficient and robust streaming language model
applications.

</details>


### [223] [HydroGAT: Distributed Heterogeneous Graph Attention Transformer for Spatiotemporal Flood Prediction](https://arxiv.org/abs/2509.02481)
*Aishwarya Sarkar,Autrin Hakimi,Xiaoqiong Chen,Hai Huang,Chaoqun Lu,Ibrahim Demir,Ali Jannesari*

Main category: cs.LG

TL;DR: 论文提出HydroGAT时空网络用于洪水预测，在两个美国中西部流域表现良好，还开发分布式数据并行管道加速训练。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法忽略区域拓扑信息，现有基于GNN的洪水预测模型存在训练成本高、无法同时捕捉时空交互等问题，需要更有效的洪水预测方法。

Method: 引入异质流域图，提出HydroGAT时空网络自适应学习局部时间重要性和最有影响力的上游位置，开发分布式数据并行管道支持高分辨率流域规模训练。

Result: 在两个美国中西部流域和五个基线架构上评估，模型在小时流量预测中NSE高达0.97、KGE高达0.96、PBIAS在±5%内，提供可解释注意力图，分布式管道在64个NVIDIA A100 GPU上实现15倍加速。

Conclusion: HydroGAT模型在洪水预测中表现出色，分布式管道能有效支持高分辨率训练，代码开源。

Abstract: Accurate flood forecasting remains a challenge for water-resource management,
as it demands modeling of local, time-varying runoff drivers (e.g.,
rainfall-induced peaks, baseflow trends) and complex spatial interactions
across a river network. Traditional data-driven approaches, such as
convolutional networks and sequence-based models, ignore topological
information about the region. Graph Neural Networks (GNNs) propagate
information exactly along the river network, which is ideal for learning
hydrological routing. However, state-of-the-art GNN-based flood prediction
models collapse pixels to coarse catchment polygons as the cost of training
explodes with graph size and higher resolution. Furthermore, most existing
methods treat spatial and temporal dependencies separately, either applying
GNNs solely on spatial graphs or transformers purely on temporal sequences,
thus failing to simultaneously capture spatiotemporal interactions critical for
accurate flood prediction. We introduce a heterogenous basin graph where every
land and river pixel is a node connected by physical hydrological flow
directions and inter-catchment relationships. We propose HydroGAT, a
spatiotemporal network that adaptively learns local temporal importance and the
most influential upstream locations. Evaluated in two Midwestern US basins and
across five baseline architectures, our model achieves higher NSE (up to 0.97),
improved KGE (up to 0.96), and low bias (PBIAS within $\pm$5%) in hourly
discharge prediction, while offering interpretable attention maps that reveal
sparse, structured intercatchment influences. To support high-resolution
basin-scale training, we develop a distributed data-parallel pipeline that
scales efficiently up to 64 NVIDIA A100 GPUs on NERSC Perlmutter supercomputer,
demonstrating up to 15x speedup across machines. Our code is available at
https://github.com/swapp-lab/HydroGAT.

</details>


### [224] [Effects of Distributional Biases on Gradient-Based Causal Discovery in the Bivariate Categorical Case](https://arxiv.org/abs/2509.01621)
*Tim Schwabe,Moritz Lange,Laurenz Wiskott,Maribel Acosta*

Main category: cs.LG

TL;DR: 研究基于梯度的因果发现方法受数据分布偏差的影响及控制方法，实证表明消除因果分解竞争可使模型更稳健。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的因果发现方法可能受训练数据分布偏差影响，识别出两种偏差并研究其影响。

Method: 针对双变量分类设置，用两个简单模型推导因果分解，研究偏差影响，还展示控制偏差方法，并对现有相关方法进行实证评估。

Result: 两个简单模型易受两种偏差影响，且展示了控制偏差的方法，实证表明消除因果分解竞争可使模型对偏差更稳健。

Conclusion: 消除可能的因果分解之间的竞争能让基于梯度的因果发现模型对数据分布偏差更具鲁棒性。

Abstract: Gradient-based causal discovery shows great potential for deducing causal
structure from data in an efficient and scalable way. Those approaches however
can be susceptible to distributional biases in the data they are trained on. We
identify two such biases: Marginal Distribution Asymmetry, where differences in
entropy skew causal learning toward certain factorizations, and Marginal
Distribution Shift Asymmetry, where repeated interventions cause faster shifts
in some variables than in others. For the bivariate categorical setup with
Dirichlet priors, we illustrate how these biases can occur even in controlled
synthetic data. To examine their impact on gradient-based methods, we employ
two simple models that derive causal factorizations by learning marginal or
conditional data distributions - a common strategy in gradient-based causal
discovery. We demonstrate how these models can be susceptible to both biases.
We additionally show how the biases can be controlled. An empirical evaluation
of two related, existing approaches indicates that eliminating competition
between possible causal factorizations can make models robust to the presented
biases.

</details>


### [225] [Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports](https://arxiv.org/abs/2509.02072)
*Jian Chen,Jinbao Tian,Yunqi Xu,Zhou Li*

Main category: cs.LG

TL;DR: 提出ABEX - RAT框架解决职业事故报告自动分类中数据不平衡问题，实验取得新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 职业事故报告数据集存在严重的类别不平衡问题，影响分析模型性能，阻碍可靠自动化系统发展。

Method: 提出ABEX - RAT框架，先使用ABEX管道利用大语言模型提取核心语义并生成合成样本，再用随机对抗训练协议训练轻量级分类器。

Result: 在公共OSHA数据集上实验达到90.32%的宏F1分数，显著优于先前的最优方法和微调大模型基线。

Conclusion: 该协同策略是针对专业不平衡分类任务替代暴力微调的高效方法。

Abstract: The automatic classification of occupational accident reports is a critical
research area for enhancing workplace safety and enabling large-scale risk
analysis. However, the severe class imbalance inherent in these real-world
datasets often compromises the performance of analytical models, particularly
for rare but severe incident types, hindering the development of reliable
automated systems. To address this challenge, we propose ABEX-RAT, a novel and
efficient framework that synergizes generative data augmentation with robust
adversarial training. Our approach first employs a twostep
abstractive-expansive (ABEX) pipeline, which leverages a large language model
to distill core incident semantics and then uses a generative model to create
diverse, highquality synthetic samples for underrepresented classes.
Subsequently, a lightweight classifier is trained on the augmented data using a
computationally efficient random adversarial training (RAT) protocol, which
stochastically applies perturbations to enhance model generalization and
robustness without significant overhead. Experimental results on the public
OSHA dataset demonstrate that our method achieves new state-of-the-art
performance, reaching a macro-F1 score of 90.32% and significantly
outperforming previous SOTA and fine-tuned large model baselines. Our work
validates that this synergistic strategy is a highly effective and efficient
alternative to brute-force fine-tuning for specialized, imbalanced
classification tasks. The code is publicly available
at:https://github.com/nxcc-lab/ABEX-RAT.

</details>


### [226] [Speech Foundation Models Generalize to Time Series Tasks from Wearable Sensor Data](https://arxiv.org/abs/2509.00221)
*Jaya Narain,Zakaria Aldeneh,Shirley Ren*

Main category: cs.LG

TL;DR: 研究表明语音基础模型学习的表征可跨领域，在可穿戴传感器时间序列任务上表现出色，为通用时间序列模型研究迈进了一步。


<details>
  <summary>Details</summary>
Motivation: 探索语音和传感器时间序列数据在时频域的信息，寻找能跨领域应用的模型以提升时间序列任务性能。

Method: 用从HuBERT和wav2vec 2.0提取的特征训练探测器，并与特定模态数据集训练的自监督模型提取的特征进行对比。

Result: 基于语音模型特征训练的探测器在情绪分类、心律失常检测和活动分类任务中优于特定模态模型。

Conclusion: 该方法提升了数据稀缺时间序列任务的性能和鲁棒性，为语音和传感器数据通用时间序列模型的研究提供方向。

Abstract: Both speech and sensor time series data encode information in both the time-
and frequency- domains, like spectral powers and waveform shapelets. We show
that speech foundation models learn representations that are domain-independent
and achieve state-of-the-art performance on time series tasks from wearable
sensors. Probes trained on features extracted from HuBERT and wav2vec 2.0
outperform those extracted from self-supervised models trained directly on
modality specific datasets for mood classification, arrhythmia detection, and
activity classification tasks. We find a particularly strong relevance of the
convolutional feature encoders from speech models for wearable sensor tasks.
The methods proposed here improve performance and robustness for data-scarce
time series tasks, using simple probing methods. This work is a step towards
generalized time series models for speech and sensor data, a topic for further
exploration.

</details>


### [227] [Efficient Transformer-Inspired Variants of Physics-Informed Deep Operator Networks](https://arxiv.org/abs/2509.01679)
*Zhi-Feng Wei,Wenqian Chen,Panos Stinis*

Main category: cs.LG

TL;DR: 提出受Transformer启发的DeepONet变体，在PDE基准测试中准确性和训练效率表现良好，且有效性与方程特性相关。


<details>
  <summary>Details</summary>
Motivation: 现有DeepONet存在精度和训练效率不能兼顾的问题，希望提出改进方法。

Method: 提出一系列Transformer启发的DeepONet变体，在分支和主干网络间引入双向交叉调节。

Result: 在四个PDE基准测试中，存在变体匹配或超越改进DeepONet的准确性，且训练效率更高。

Conclusion: 交叉调节的有效性取决于方程及其物理特性，通过统计分析验证了变体的有效性。

Abstract: Operator learning has emerged as a promising tool for accelerating the
solution of partial differential equations (PDEs). The Deep Operator Networks
(DeepONets) represent a pioneering framework in this area: the "vanilla"
DeepONet is valued for its simplicity and efficiency, while the modified
DeepONet achieves higher accuracy at the cost of increased training time. In
this work, we propose a series of Transformer-inspired DeepONet variants that
introduce bidirectional cross-conditioning between the branch and trunk
networks in DeepONet. Query-point information is injected into the branch
network and input-function information into the trunk network, enabling dynamic
dependencies while preserving the simplicity and efficiency of the "vanilla"
DeepONet in a non-intrusive manner. Experiments on four PDE benchmarks --
advection, diffusion-reaction, Burgers', and Korteweg-de Vries equations --
show that for each case, there exists a variant that matches or surpasses the
accuracy of the modified DeepONet while offering improved training efficiency.
Moreover, the best-performing variant for each equation aligns naturally with
the equation's underlying characteristics, suggesting that the effectiveness of
cross-conditioning depends on the characteristics of the equation and its
underlying physics. To ensure robustness, we validate the effectiveness of our
variants through a range of rigorous statistical analyses, among them the
Wilcoxon Two One-Sided Test, Glass's Delta, and Spearman's rank correlation.

</details>


### [228] [Quantum-Optimized Selective State Space Model for Efficient Time Series Prediction](https://arxiv.org/abs/2509.00259)
*Stefan-Alexandru Jura,Mihai Udrescu,Alexandru Topirceanu*

Main category: cs.LG

TL;DR: 提出量子优化选择性状态空间模型Q - SSM用于长序列时间预测，在三个基准测试中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的架构存在二次复杂度和长时性能下降问题，状态空间模型存在训练不稳定、对初始化敏感和多变量预测鲁棒性有限等问题，需要更好的长序列时间预测方法。

Method: 提出Q - SSM，集成状态空间动力学和变分量子门，采用简单的参数化量子电路（RY - RX ansatz）自适应调节内存更新。

Result: 在ETT、Traffic和Exchange Rate三个基准测试中，Q - SSM始终优于强基线模型、基于Transformer的模型和S - Mamba。

Conclusion: 变分量子门控可以解决当前长序列预测的局限性，实现准确和鲁棒的多变量预测。

Abstract: Long-range time series forecasting remains challenging, as it requires
capturing non-stationary and multi-scale temporal dependencies while
maintaining noise robustness, efficiency, and stability. Transformer-based
architectures such as Autoformer and Informer improve generalization but suffer
from quadratic complexity and degraded performance on very long time horizons.
State space models, notably S-Mamba, provide linear-time updates but often face
unstable training dynamics, sensitivity to initialization, and limited
robustness for multivariate forecasting. To address such challenges, we propose
the Quantum-Optimized Selective State Space Model (Q-SSM), a hybrid
quantum-optimized approach that integrates state space dynamics with a
variational quantum gate. Instead of relying on expensive attention mechanisms,
Q-SSM employs a simple parametrized quantum circuit (RY-RX ansatz) whose
expectation values regulate memory updates adaptively. This quantum gating
mechanism improves convergence stability, enhances the modeling of long-term
dependencies, and provides a lightweight alternative to attention. We
empirically validate Q-SSM on three widely used benchmarks, i.e., ETT, Traffic,
and Exchange Rate. Results show that Q-SSM consistently improves over strong
baselines (LSTM, TCN, Reformer), Transformer-based models, and S-Mamba. These
findings demonstrate that variational quantum gating can address current
limitations in long-range forecasting, leading to accurate and robust
multivariate predictions.

</details>


### [229] [Bouncy particle sampler with infinite exchanging parallel tempering](https://arxiv.org/abs/2509.02003)
*Yohei Saito,Shun Kimura,Koujin Takeda*

Main category: cs.LG

TL;DR: 文章探讨贝叶斯推理后验分布近似方法，引入并行回火到弹跳粒子采样器（BPS）并提出算法，经数值模拟证明对多峰分布有效。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推理后验分布难解析评估，HMC和MCMC有局限，BPS虽有优势但需加速收敛到后验分布。

Method: 将并行回火（PT）引入BPS，提出逆温度交换率设为无穷时的算法。

Result: 通过数值模拟，证明算法对多峰分布有效。

Conclusion: 引入PT的BPS算法能加速收敛到后验分布，对多峰分布有效。

Abstract: Bayesian inference is useful to obtain a predictive distribution with a small
generalization error. However, since posterior distributions are rarely
evaluated analytically, we employ the variational Bayesian inference or
sampling method to approximate posterior distributions. When we obtain samples
from a posterior distribution, Hamiltonian Monte Carlo (HMC) has been widely
used for the continuous variable part and Markov chain Monte Carlo (MCMC) for
the discrete variable part. Another sampling method, the bouncy particle
sampler (BPS), has been proposed, which combines uniform linear motion and
stochastic reflection to perform sampling. BPS was reported to have the
advantage of being easier to set simulation parameters than HMC. To accelerate
the convergence to a posterior distribution, we introduced parallel tempering
(PT) to BPS, and then proposed an algorithm when the inverse temperature
exchange rate is set to infinity. We performed numerical simulations and
demonstrated its effectiveness for multimodal distribution.

</details>


### [230] [Continuously Tempered Diffusion Samplers](https://arxiv.org/abs/2509.00316)
*Ezra Erives,Bowen Jing,Peter Holderrieth,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 提出连续回火扩散采样器，利用分子动力学探索技术改进提议分布，提升采样器性能。


<details>
  <summary>Details</summary>
Motivation: 以往基于退火的神经采样器在训练阶段的提议分布探索不足，导致训练后性能不佳。

Method: 引入不同温度的分布族，利用分子动力学探索技术改进提议分布。

Result: 通过实验验证了扩展探索驱动下采样器性能得到提升。

Conclusion: 提出的连续回火扩散采样器能有效改善提议分布，提高采样器性能。

Abstract: Annealing-based neural samplers seek to amortize sampling from unnormalized
distributions by training neural networks to transport a family of densities
interpolating from source to target. A crucial design choice in the training
phase of such samplers is the proposal distribution by which locations are
generated at which to evaluate the loss. Previous work has obtained such a
proposal distribution by combining a partially learned transport with annealed
Langevin dynamics. However, isolated modes and other pathological properties of
the annealing path imply that such proposals achieve insufficient exploration
and thereby lower performance post training. To remedy this, we propose
continuously tempered diffusion samplers, which leverage exploration techniques
developed in the context of molecular dynamics to improve proposal
distributions. Specifically, a family of distributions across different
temperatures is introduced to lower energy barriers at higher temperatures and
drive exploration at the lower temperature of interest. We empirically validate
improved sampler performance driven by extended exploration. Code is available
at https://github.com/eje24/ctds.

</details>


### [231] [Fantastic Pretraining Optimizers and Where to Find Them](https://arxiv.org/abs/2509.02046)
*Kaiyue Wen,David Hall,Tengyu Ma,Percy Liang*

Main category: cs.LG

TL;DR: 文章指出AdamW在语言模型预训练中占主导地位，分析替代优化器速度提升说法不准确的原因，通过研究发现公平对比需严格调参和多尺度评估，矩阵优化器速度提升与模型规模成反比。


<details>
  <summary>Details</summary>
Motivation: 解决替代优化器与AdamW对比时存在的方法缺陷，实现公平对比并推动实用化。

Method: 对十种深度学习优化器在四种模型规模和数据 - 模型比率下进行系统研究。

Result: 最优超参数因优化器而异；很多提议优化器的实际加速比低于宣称且随模型增大而减小；对比中间检查点可能有误导；最快的优化器用矩阵作为预调节器，其加速比与模型规模成反比。

Conclusion: 公平且有意义的对比需要严格的超参数调优和跨模型规模、数据 - 模型比率的评估，在训练结束时进行。

Abstract: AdamW has long been the dominant optimizer in language model pretraining,
despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We
posit that two methodological shortcomings have obscured fair comparisons and
hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited
or misleading evaluation setups. To address these two issues, we conduct a
systematic study of ten deep learning optimizers across four model scales
(0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum).
We find that fair and informative comparisons require rigorous hyperparameter
tuning and evaluations across a range of model scales and data-to-model ratios,
performed at the end of training. First, optimal hyperparameters for one
optimizer may be suboptimal for another, making blind hyperparameter transfer
unfair. Second, the actual speedup of many proposed optimizers over well-tuned
baselines is lower than claimed and decreases with model size to only 1.1x for
1.2B parameter models. Thirdly, comparing intermediate checkpoints before
reaching the target training budgets can be misleading, as rankings between two
optimizers can flip during training due to learning rate decay. Through our
thorough investigation, we find that all the fastest optimizers such as Muon
and Soap, use matrices as preconditioners -- multiplying gradients with
matrices rather than entry-wise scalars. However, the speedup of matrix-based
optimizers is inversely proportional to model scale, decreasing from 1.4x over
AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.

</details>


### [232] [Chunked TabPFN: Exact Training-Free In-Context Learning for Long-Context Tabular Data](https://arxiv.org/abs/2509.00326)
*Renat Sergazinov,Shao-An Yin*

Main category: cs.LG

TL;DR: 提出 tiled-block 策略使 TabPFN 处理长上下文，在 TabArena 基准测试证明有效。


<details>
  <summary>Details</summary>
Motivation: TabPFN v2 无法处理超 10K 上下文标记，现有依赖上下文压缩的方法有局限。

Method: 引入 tiled-block 策略在 TabPFN 框架内计算注意力。

Result: 方法在标准 TabArena 基准测试展示了有效性。

Conclusion: tiled-block 策略能让 TabPFN 无需预处理处理长上下文，且与标准 GPU 配置兼容。

Abstract: TabPFN v2 achieves better results than tree-based models on several tabular
benchmarks, which is notable since tree-based models are usually the strongest
choice for tabular data. However, it cannot handle more than 10K context tokens
because transformers have quadratic computation and memory costs.
  Unlike existing approaches that rely on context compression, such as
selecting representative samples via K-nearest neighbors (KNN), we introduce a
\textbf{tiled-block} strategy to compute attention within the TabPFN framework.
This design is compatible with standard GPU setups and, to the best of our
knowledge, is the first to enable TabPFN to \textbf{process long contexts
without any pre-processing}. We demonstrate the effectiveness of our approach
on the standard TabArena benchmark.

</details>


### [233] [Differentiable Expectation-Maximisation and Applications to Gaussian Mixture Model Optimal Transport](https://arxiv.org/abs/2509.02109)
*Samuel Boïté,Eloi Tanguy,Julie Delon,Agnès Desolneux,Rémi Flamary*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Expectation-Maximisation (EM) algorithm is a central tool in statistics
and machine learning, widely used for latent-variable models such as Gaussian
Mixture Models (GMMs). Despite its ubiquity, EM is typically treated as a
non-differentiable black box, preventing its integration into modern learning
pipelines where end-to-end gradient propagation is essential. In this work, we
present and compare several differentiation strategies for EM, from full
automatic differentiation to approximate methods, assessing their accuracy and
computational efficiency. As a key application, we leverage this differentiable
EM in the computation of the Mixture Wasserstein distance $\mathrm{MW}_2$
between GMMs, allowing $\mathrm{MW}_2$ to be used as a differentiable loss in
imaging and machine learning tasks. To complement our practical use of
$\mathrm{MW}_2$, we contribute a novel stability result which provides
theoretical justification for the use of $\mathrm{MW}_2$ with EM, and also
introduce a novel unbalanced variant of $\mathrm{MW}_2$. Numerical experiments
on barycentre computation, colour and style transfer, image generation, and
texture synthesis illustrate the versatility and effectiveness of the proposed
approach in different settings.

</details>


### [234] [Counterfactual Risk Minimization with IPS-Weighted BPR and Self-Normalized Evaluation in Recommender Systems](https://arxiv.org/abs/2509.00333)
*Rahul Raja,Arpita Vats*

Main category: cs.LG

TL;DR: 本文提出集成IPS加权训练与带倾向正则化器的IPS加权BPR目标的管道，实验表明该方法在无偏曝光下泛化性更好且降低评估方差。


<details>
  <summary>Details</summary>
Motivation: 从记录的隐式反馈中学习和评估推荐系统因曝光偏差而具有挑战性，IPS校正偏差时存在高方差和不稳定性问题。

Method: 提出集成IPS加权训练与带倾向正则化器（PR）的IPS加权贝叶斯个性化排序（BPR）目标的管道，比较直接法（DM）、IPS和自归一化IPS（SNIPS）进行离线策略评估。

Result: 实验表明该方法在无偏曝光下泛化性更好，降低评估方差。

Conclusion: 该方法为现实推荐场景中的反事实学习和评估提供实用指导。

Abstract: Learning and evaluating recommender systems from logged implicit feedback is
challenging due to exposure bias. While inverse propensity scoring (IPS)
corrects this bias, it often suffers from high variance and instability. In
this paper, we present a simple and effective pipeline that integrates
IPS-weighted training with an IPS-weighted Bayesian Personalized Ranking (BPR)
objective augmented by a Propensity Regularizer (PR). We compare Direct Method
(DM), IPS, and Self-Normalized IPS (SNIPS) for offline policy evaluation, and
demonstrate how IPS-weighted training improves model robustness under biased
exposure. The proposed PR further mitigates variance amplification from extreme
propensity weights, leading to more stable estimates. Experiments on synthetic
and MovieLens 100K data show that our approach generalizes better under
unbiased exposure while reducing evaluation variance compared to naive and
standard IPS methods, offering practical guidance for counterfactual learning
and evaluation in real-world recommendation settings.

</details>


### [235] [Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation](https://arxiv.org/abs/2509.02154)
*Aymene Mohammed Bouayed,Samuel Deslauriers-Gauthier,Adrian Iaccovelli,David Naccache*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Variational Autoencoders (VAEs) with global priors mirror the training set's
class frequency in latent space, underrepresenting tail classes and reducing
generative fairness on imbalanced datasets. While $t^3$VAE improves robustness
via heavy-tailed Student's t-distribution priors, it still allocates latent
volume proportionally to the class frequency.In this work, we address this
issue by explicitly enforcing equitable latent space allocation across classes.
To this end, we propose Conditional-$t^3$VAE, which defines a per-class
\mbox{Student's t} joint prior over latent and output variables, preventing
dominance by majority classes. Our model is optimized using a closed-form
objective derived from the $\gamma$-power divergence. Moreover, for
class-balanced generation, we derive an equal-weight latent mixture of
Student's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA,
Conditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE
and Gaussian-based VAE baselines, particularly under severe class imbalance. In
per-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional
Gaussian VAE across all highly imbalanced settings. While Gaussian-based models
remain competitive under mild imbalance ratio ($\rho \lesssim 3$), our approach
substantially improves generative fairness and diversity in more extreme
regimes.

</details>


### [236] [Are We Really Learning the Score Function? Reinterpreting Diffusion Models Through Wasserstein Gradient Flow Matching](https://arxiv.org/abs/2509.00336)
*An B. Vuong,Michael T. McCann,Javier E. Santos,Yen Ting Lin*

Main category: cs.LG

TL;DR: 研究发现训练的扩散网络违背得分函数约束，但生成效果好。提出用Wasserstein梯度流视角理解扩散训练，非保守误差不一定影响密度传输。


<details>
  <summary>Details</summary>
Motivation: 解释扩散模型学习的向量场非保守却能良好生成的矛盾现象。

Method: 给出数值证据证明训练的扩散网络违背得分函数约束，提出用Wasserstein梯度流视角理解扩散训练。

Result: 表明非保守误差不一定影响密度传输，WGF视角能自然产生“概率流”，无需反向随机微分方程理论。

Conclusion: 倡导采用WGF视角作为理解扩散生成模型的理论框架。

Abstract: Diffusion models are commonly interpreted as learning the score function,
i.e., the gradient of the log-density of noisy data. However, this assumption
implies that the target of learning is a conservative vector field, which is
not enforced by the neural network architectures used in practice. We present
numerical evidence that trained diffusion networks violate both integral and
differential constraints required of true score functions, demonstrating that
the learned vector fields are not conservative. Despite this, the models
perform remarkably well as generative mechanisms. To explain this apparent
paradox, we advocate a new theoretical perspective: diffusion training is
better understood as flow matching to the velocity field of a Wasserstein
Gradient Flow (WGF), rather than as score learning for a reverse-time
stochastic differential equation. Under this view, the "probability flow"
arises naturally from the WGF framework, eliminating the need to invoke
reverse-time SDE theory and clarifying why generative sampling remains
successful even when the neural vector field is not a true score. We further
show that non-conservative errors from neural approximation do not necessarily
harm density transport. Our results advocate for adopting the WGF perspective
as a principled, elegant, and theoretically grounded framework for
understanding diffusion generative models.

</details>


### [237] [Scalable Option Learning in High-Throughput Environments](https://arxiv.org/abs/2509.00338)
*Mikael Henaff,Scott Fujimoto,Michael Rabbat*

Main category: cs.LG

TL;DR: 提出可扩展选项学习（SOL）算法，解决分层强化学习扩展到高吞吐量环境的挑战，在NetHack等环境验证其有效性并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有分层强化学习方法未实现大规模训练的好处，需解决扩展到高吞吐量环境的关键挑战。

Method: 提出Scalable Option Learning (SOL) 算法，在NetHack上用200亿帧经验训练分层智能体。

Result: SOL算法比现有分层方法吞吐量高25倍，显著超越扁平智能体，在MiniHack和Mujoco环境也展示了适用性。

Conclusion: SOL算法可有效扩展分层强化学习到高吞吐量环境，具有良好的扩展性和通用性。

Abstract: Hierarchical reinforcement learning (RL) has the potential to enable
effective decision-making over long timescales. Existing approaches, while
promising, have yet to realize the benefits of large-scale training. In this
work, we identify and solve several key challenges in scaling hierarchical RL
to high-throughput environments. We propose Scalable Option Learning (SOL), a
highly scalable hierarchical RL algorithm which achieves a 25x higher
throughput compared to existing hierarchical methods. We train our hierarchical
agents using 20 billion frames of experience on the complex game of NetHack,
significantly surpassing flat agents and demonstrating positive scaling trends.
We also validate our algorithm on MiniHack and Mujoco environments, showcasing
its general applicability. Our code is open sourced at
github.com/facebookresearch/sol.

</details>


### [238] [LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning](https://arxiv.org/abs/2509.00347)
*Hanping Zhang,Yuhong Guo*

Main category: cs.LG

TL;DR: 提出LLMDPD方法解决离线强化学习泛化问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中因离线数据局限性，智能体在新任务或环境泛化能力差。

Method: 提出LLMDPD方法，结合文本任务描述和轨迹提示引导策略学习，用大语言模型处理文本提示，用Transformer模型编码轨迹提示，将提示作为条件输入到上下文感知策略级扩散模型。

Result: LLMDPD在未见任务上表现优于现有离线强化学习方法。

Conclusion: LLMDPD能有效提高离线强化学习在不同场景下的泛化和适应能力。

Abstract: Reinforcement Learning (RL) is known for its strong decision-making
capabilities and has been widely applied in various real-world scenarios.
However, with the increasing availability of offline datasets and the lack of
well-designed online environments from human experts, the challenge of
generalization in offline RL has become more prominent. Due to the limitations
of offline data, RL agents trained solely on collected experiences often
struggle to generalize to new tasks or environments. To address this challenge,
we propose LLM-Driven Policy Diffusion (LLMDPD), a novel approach that enhances
generalization in offline RL using task-specific prompts. Our method
incorporates both text-based task descriptions and trajectory prompts to guide
policy learning. We leverage a large language model (LLM) to process text-based
prompts, utilizing its natural language understanding and extensive knowledge
base to provide rich task-relevant context. Simultaneously, we encode
trajectory prompts using a transformer model, capturing structured behavioral
patterns within the underlying transition dynamics. These prompts serve as
conditional inputs to a context-aware policy-level diffusion model, enabling
the RL agent to generalize effectively to unseen tasks. Our experimental
results demonstrate that LLMDPD outperforms state-of-the-art offline RL methods
on unseen tasks, highlighting its effectiveness in improving generalization and
adaptability in diverse settings.

</details>


### [239] [Theory Foundation of Physics-Enhanced Residual Learning](https://arxiv.org/abs/2509.00348)
*Shixiao Liang,Wang Chen,Keke Long,Peng Zhang,Xiaopeng Li,Jintao Ke*

Main category: cs.LG

TL;DR: 本文从理论上解释Physics - Enhanced Residual Learning (PERL)的优势，通过定理证明和数值例子验证其效果，表明PERL在自动驾驶应用中有实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有PERL方法的数值结果缺乏理论依据，无法得到充分解释，因此需要从理论角度解释其优势。

Method: 研究具有Lipschitz连续性的一类问题，通过研究损失函数边界与残差学习结构的关系，严格证明一组定理。

Result: 数值例子表明，即使训练样本显著减少，PERL的精度始终高于纯神经网络。

Conclusion: PERL在减少数据量的同时提高了预测性能，在现实自动驾驶应用中有实用价值。

Abstract: Intensive studies have been conducted in recent years to integrate neural
networks with physics models to balance model accuracy and interpretability.
One recently proposed approach, named Physics-Enhanced Residual Learning
(PERL), is to use learning to estimate the residual between the physics model
prediction and the ground truth. Numeral examples suggested that integrating
such residual with physics models in PERL has three advantages: (1) a reduction
in the number of required neural network parameters; (2) faster convergence
rates; and (3) fewer training samples needed for the same computational
precision. However, these numerical results lack theoretical justification and
cannot be adequately explained.
  This paper aims to explain these advantages of PERL from a theoretical
perspective. We investigate a general class of problems with Lipschitz
continuity properties. By examining the relationships between the bounds to the
loss function and residual learning structure, this study rigorously proves a
set of theorems explaining the three advantages of PERL.
  Several numerical examples in the context of automated vehicle trajectory
prediction are conducted to illustrate the proposed theorems. The results
confirm that, even with significantly fewer training samples, PERL consistently
achieves higher accuracy than a pure neural network. These results demonstrate
the practical value of PERL in real world autonomous driving applications where
corner case data are costly or hard to obtain. PERL therefore improves
predictive performance while reducing the amount of data required.

</details>


### [240] [Is RL fine-tuning harder than regression? A PDE learning approach for diffusion models](https://arxiv.org/abs/2509.02528)
*Wenlong Mou*

Main category: cs.LG

TL;DR: 研究用通用值函数逼近微调扩散过程最优控制策略，开发新算法，证明学习值函数和控制策略统计率，表明微调可通过监督回归实现且有更快统计率保证。


<details>
  <summary>Details</summary>
Motivation: 研究如何学习微调给定扩散过程的最优控制策略。

Method: 通过解决基于Hamilton - Jacobi - Bellman (HJB)方程的变分不等式问题开发新算法。

Result: 证明了学习的价值函数和控制策略的统计率，取决于函数类的复杂性和近似误差。

Conclusion: 与通用强化学习问题不同，微调可通过监督回归实现，且有更快的统计率保证。

Abstract: We study the problem of learning the optimal control policy for fine-tuning a
given diffusion process, using general value function approximation. We develop
a new class of algorithms by solving a variational inequality problem based on
the Hamilton-Jacobi-Bellman (HJB) equations. We prove sharp statistical rates
for the learned value function and control policy, depending on the complexity
and approximation errors of the function class. In contrast to generic
reinforcement learning problems, our approach shows that fine-tuning can be
achieved via supervised regression, with faster statistical rate guarantees.

</details>


### [241] [Optimized Weight Initialization on the Stiefel Manifold for Deep ReLU Neural Networks](https://arxiv.org/abs/2509.00362)
*Hyungu Lee,Taehyeong Kim,Hayoung Choi*

Main category: cs.LG

TL;DR: 提出针对ReLU优化的正交初始化方法，理论和实验证明其能稳定深度网络训练。


<details>
  <summary>Details</summary>
Motivation: 现有初始化方法在深度网络中无法有效调节预激活均值和控制激活稀疏性，难以稳定训练ReLU网络。

Method: 在Stiefel流形上解决优化问题，得到针对ReLU优化的正交初始化方法，推导封闭解和高效采样方案。

Result: 理论上可防止ReLU死亡、减缓激活方差衰减、缓解梯度消失；实验上在多个数据集和场景中优于先前初始化方法。

Conclusion: 所提初始化方法能稳定深度架构中的信号和梯度流，实现深度网络稳定训练。

Abstract: Stable and efficient training of ReLU networks with large depth is highly
sensitive to weight initialization. Improper initialization can cause permanent
neuron inactivation dying ReLU and exacerbate gradient instability as network
depth increases. Methods such as He, Xavier, and orthogonal initialization
preserve variance or promote approximate isometry. However, they do not
necessarily regulate the pre-activation mean or control activation sparsity,
and their effectiveness often diminishes in very deep architectures. This work
introduces an orthogonal initialization specifically optimized for ReLU by
solving an optimization problem on the Stiefel manifold, thereby preserving
scale and calibrating the pre-activation statistics from the outset. A family
of closed-form solutions and an efficient sampling scheme are derived.
Theoretical analysis at initialization shows that prevention of the dying ReLU
problem, slower decay of activation variance, and mitigation of gradient
vanishing, which together stabilize signal and gradient flow in deep
architectures. Empirically, across MNIST, Fashion-MNIST, multiple tabular
datasets, few-shot settings, and ReLU-family activations, our method
outperforms previous initializations and enables stable training in deep
networks.

</details>


### [242] [Federated learning over physical channels: adaptive algorithms with near-optimal guarantees](https://arxiv.org/abs/2509.02538)
*Rui Zhang,Wenlong Mou*

Main category: cs.LG

TL;DR: 提出可在物理信道实现的自适应联邦随机梯度下降算法，有理论保证且经模拟验证有效性。


<details>
  <summary>Details</summary>
Motivation: 降低联邦学习中通过物理信道传输信息的通信成本。

Method: 提出新的自适应联邦随机梯度下降算法，考虑信道噪声和硬件约束。

Result: 建立算法的理论保证，展示适应随机梯度噪声水平的收敛率，模拟研究证明算法实际有效性。

Conclusion: 所提出的算法在理论和实践上均有良好表现。

Abstract: In federated learning, communication cost can be significantly reduced by
transmitting the information over the air through physical channels. In this
paper, we propose a new class of adaptive federated stochastic gradient descent
(SGD) algorithms that can be implemented over physical channels, taking into
account both channel noise and hardware constraints. We establish theoretical
guarantees for the proposed algorithms, demonstrating convergence rates that
are adaptive to the stochastic gradient noise level. We also demonstrate the
practical effectiveness of our algorithms through simulation studies with deep
learning models.

</details>


### [243] [Unifying Adversarial Perturbation for Graph Neural Networks](https://arxiv.org/abs/2509.00387)
*Jinluan Yang,Ruihao Zhang,Zhengyu Chen,Fei Wu,Kun Kuang*

Main category: cs.LG

TL;DR: 本文研究GNN对节点特征和图结构对抗攻击的脆弱性，提出PerturbEmbedding方法，实验证明该方法显著提升GNN鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法提升GNN鲁棒性和泛化能力有效，但应用局限于特定数据集和GNN类型，需新方法。

Method: 提出PerturbEmbedding方法，直接对GNN每个隐藏嵌入进行扰动操作，为现有扰动策略提供统一框架，统一随机和对抗扰动形式。

Result: 在不同数据集和骨干模型上实验表明，PerturbEmbedding显著提升GNN鲁棒性和泛化能力，优于现有方法，拒绝随机和对抗扰动进一步提升骨干模型性能。

Conclusion: PerturbEmbedding方法能有效增强GNN对攻击的恢复力，提升其泛化能力。

Abstract: This paper studies the vulnerability of Graph Neural Networks (GNNs) to
adversarial attacks on node features and graph structure. Various methods have
implemented adversarial training to augment graph data, aiming to bolster the
robustness and generalization of GNNs. These methods typically involve applying
perturbations to the node feature, weights, or graph structure and subsequently
minimizing the loss by learning more robust graph model parameters under the
adversarial perturbations. Despite the effectiveness of adversarial training in
enhancing GNNs' robustness and generalization abilities, its application has
been largely confined to specific datasets and GNN types. In this paper, we
propose a novel method, PerturbEmbedding, that integrates adversarial
perturbation and training, enhancing GNNs' resilience to such attacks and
improving their generalization ability. PerturbEmbedding performs perturbation
operations directly on every hidden embedding of GNNs and provides a unified
framework for most existing perturbation strategies/methods. We also offer a
unified perspective on the forms of perturbations, namely random and
adversarial perturbations. Through experiments on various datasets using
different backbone models, we demonstrate that PerturbEmbedding significantly
improves both the robustness and generalization abilities of GNNs,
outperforming existing methods. The rejection of both random (non-targeted) and
adversarial (targeted) perturbations further enhances the backbone model's
performance.

</details>


### [244] [Curriculum Guided Personalized Subgraph Federated Learning](https://arxiv.org/abs/2509.00402)
*Minku Kang,Hogun Park*

Main category: cs.LG

TL;DR: 提出CUFL框架解决子图联邦学习数据异质性及过拟合问题，实验显示性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 子图联邦学习存在数据异质性，加权模型聚合易因稀疏和有偏子图导致过拟合，使聚合失效。

Method: 客户端采用课程学习自适应选边训练，调节个性化；用随机参考图重构的细粒度结构指标估计客户端相似度改进加权聚合。

Result: 在六个基准数据集上的实验表明，CUFL性能优于相关基线。

Conclusion: CUFL框架能有效解决子图联邦学习中的问题，提升性能。

Abstract: Subgraph Federated Learning (FL) aims to train Graph Neural Networks (GNNs)
across distributed private subgraphs, but it suffers from severe data
heterogeneity. To mitigate data heterogeneity, weighted model aggregation
personalizes each local GNN by assigning larger weights to parameters from
clients with similar subgraph characteristics inferred from their current model
states. However, the sparse and biased subgraphs often trigger rapid
overfitting, causing the estimated client similarity matrix to stagnate or even
collapse. As a result, aggregation loses effectiveness as clients reinforce
their own biases instead of exploiting diverse knowledge otherwise available.
To this end, we propose a novel personalized subgraph FL framework called
Curriculum guided personalized sUbgraph Federated Learning (CUFL). On the
client side, CUFL adopts Curriculum Learning (CL) that adaptively selects edges
for training according to their reconstruction scores, exposing each GNN first
to easier, generic cross-client substructures and only later to harder,
client-specific ones. This paced exposure prevents early overfitting to biased
patterns and enables gradual personalization. By regulating personalization,
the curriculum also reshapes server aggregation from exchanging generic
knowledge to propagating client-specific knowledge. Further, CUFL improves
weighted aggregation by estimating client similarity using fine-grained
structural indicators reconstructed on a random reference graph. Extensive
experiments on six benchmark datasets confirm that CUFL achieves superior
performance compared to relevant baselines. Code is available at
https://github.com/Kang-Min-Ku/CUFL.git.

</details>


### [245] [Metis: Training Large Language Models with Advanced Low-Bit Quantization](https://arxiv.org/abs/2509.00404)
*Hengjie Cao,Mengyi Chen,Yifeng Yang,Ruijun Huang,Fang Dong,Jixian Zhou,Anrui Chen,Mingzhi Dong,Yujiang Wang,Jinlong Hou,Yuan Cheng,Fan Wu,Fan Yang,Tun Lu,Ning Gu,Li Shang*

Main category: cs.LG

TL;DR: 文章指出各向异性参数分布是低比特量化训练大语言模型的障碍，提出Metis训练框架解决该问题，使FP8和FP4训练有良好表现。


<details>
  <summary>Details</summary>
Motivation: 解决低比特量化训练大语言模型时，各向异性参数分布导致的训练不稳定和模型性能低的问题。

Method: 引入Metis训练框架，结合谱分解与随机嵌入、谱域自适应学习率和双范围正则化。

Result: FP8训练超越FP32基线，FP4训练达到与FP32相当的准确率。

Conclusion: Metis为先进低比特量化下大语言模型的稳健和可扩展训练铺平了道路。

Abstract: This work identifies anisotropic parameter distributions as a fundamental
barrier to training large language models (LLMs) with low-bit quantization: a
few dominant singular values create wide numerical ranges that conflict with
the inherent bias of block-wise quantization. This bias disproportionately
preserves high-magnitude values while discarding smaller ones, causing training
instability and low model performance. This work introduces Metis, a training
framework that combines (i) spectral decomposition with random embedding to
efficiently disentangle dominant from long-tail components, compressing broad
distributions into quantization-friendly narrow ranges; (ii) adaptive learning
rates in the spectral domain to amplify underrepresented directions and better
capture diverse features critical for performance; and (iii) a dual-range
regularizer that jointly constrains numerical precision and parameter range
distribution, ensuring stable, unbiased low-bit training. With Metis, FP8
training surpasses FP32 baselines, and FP4 training achieves accuracy
comparable to FP32, paving the way for robust and scalable LLM training under
advanced low-bit quantization. The code implementation for Metis is available
at: https://github.com/typename-yyf/Metis-quantization.

</details>


### [246] [Lagrangian Relaxation for Multi-Action Partially Observable Restless Bandits: Heuristic Policies and Indexability](https://arxiv.org/abs/2509.00415)
*Rahul Meshram,Kesav Kaza*

Main category: cs.LG

TL;DR: 研究多动作部分可观测不安分多臂老虎机问题，以公共卫生干预规划为应用动机，分析拉格朗日界方法，给出近似计算方法和启发式策略，讨论Whittle指数策略及局限。


<details>
  <summary>Details</summary>
Motivation: 以公共卫生干预规划为应用场景，研究多动作部分可观测不安分多臂老虎机问题。

Method: 分析拉格朗日界方法，用基于点的值迭代（PBVI）和在线滚动策略近似计算拉格朗日界，研究启发式策略，讨论Whittle指数策略。

Result: 给出拉格朗日界近似计算方法，研究启发式策略，讨论Whittle指数策略局限性。

Conclusion: 对多动作部分可观测不安分多臂老虎机问题进行了深入研究，为相关应用提供了理论和方法支持。

Abstract: Partially observable restless multi-armed bandits have found numerous
applications including in recommendation systems, communication systems, public
healthcare outreach systems, and in operations research. We study multi-action
partially observable restless multi-armed bandits, it is a generalization of
the classical restless multi-armed bandit problem -- 1) each bandit has finite
states, and the current state is not observable, 2) each bandit has finite
actions. In particular, we assume that more than two actions are available for
each bandit. We motivate our problem with the application of public-health
intervention planning. We describe the model and formulate a long term
discounted optimization problem, where the state of each bandit evolves
according to a Markov process, and this evolution is action dependent. The
state of a bandit is not observable but one of finitely many feedback signals
are observable. Each bandit yields a reward, based on the action taken on that
bandit. The agent is assumed to have a budget constraint. The bandits are
assumed to be independent. However, they are weakly coupled at the agent
through the budget constraint.
  We first analyze the Lagrangian bound method for our partially observable
restless bandits. The computation of optimal value functions for finite-state,
finite-action POMDPs is non-trivial. Hence, the computation of Lagrangian
bounds is also challenging. We describe approximations for the computation of
Lagrangian bounds using point based value iteration (PBVI) and online rollout
policy. We further present various properties of the value functions and
provide theoretical insights on PBVI and online rollout policy. We study
heuristic policies for multi-actions PORMAB. Finally, we discuss present
Whittle index policies and their limitations in our model.

</details>


### [247] [Memory Limitations of Prompt Tuning in Transformers](https://arxiv.org/abs/2509.00421)
*Maxime Meyer,Mario Michelessa,Caroline Chaux,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文探索提示调优的记忆能力，证明变压器模型记忆的信息量与提示长度线性相关，且首次正式证明大语言模型中扩展上下文会导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有对提示调优的理论分析有限，主要关注通用近似属性，本文旨在探索变压器理论的不同方面——提示调优的记忆能力。

Method: 通过理论证明，一是证明变压器记忆的信息量与提示长度的关系，二是证明扩展上下文导致性能下降的现象。

Result: 证明了变压器记忆的信息量不能随提示长度超线性增长，以及扩展上下文会使变压器性能下降。

Conclusion: 变压器架构存在内在局限性，处理长序列的能力受限。

Abstract: Despite the empirical success of prompt tuning in adapting pretrained
language models to new tasks, theoretical analyses of its capabilities remain
limited. Existing theoretical work primarily addresses universal approximation
properties, demonstrating results comparable to standard weight tuning. In this
paper, we explore a different aspect of the theory of transformers: the
memorization capability of prompt tuning. We provide two principal theoretical
contributions. First, we prove that the amount of information memorized by a
transformer cannot scale faster than linearly with the prompt length. Second,
and more importantly, we present the first formal proof of a phenomenon
empirically observed in large language models: performance degradation in
transformers with extended contexts. We rigorously demonstrate that
transformers inherently have limited memory, constraining the amount of
information they can retain, regardless of the context size. This finding
offers a fundamental understanding of the intrinsic limitations of transformer
architectures, particularly their ability to handle long sequences.

</details>


### [248] [Universal Properties of Activation Sparsity in Modern Large Language Models](https://arxiv.org/abs/2509.00454)
*Filip Szatkowski,Patryk Będkowski,Alessio Devoto,Jan Dubiński,Pasquale Minervini,Mikołaj Piórczyński,Simone Scardapane,Bartosz Wójcik*

Main category: cs.LG

TL;DR: 提出通用框架评估大语言模型激活稀疏性鲁棒性，揭示其普遍模式并给出应用建议。


<details>
  <summary>Details</summary>
Motivation: ReLU模型的激活稀疏性研究方法不适用于现代大语言模型，当前大语言模型激活稀疏性研究零散、缺乏共识。

Method: 提出通用框架评估稀疏性鲁棒性，并对现代大语言模型FFN层进行系统研究。

Result: 揭示了大语言模型激活稀疏性的普遍模式。

Conclusion: 研究为大语言模型激活稀疏性提供了见解，并为模型设计和加速提供实用指南。

Abstract: Input-dependent activation sparsity is a notable property of deep learning
models, which has been extensively studied in networks with ReLU activations
and is associated with efficiency, robustness, and interpretability. However,
the approaches developed for ReLU-based models depend on exact zero activations
and do not transfer directly to modern large language models~(LLMs), which have
abandoned ReLU in favor of other activation functions. As a result, current
work on activation sparsity in LLMs is fragmented, model-specific, and lacks
consensus on which components to target. We propose a general framework to
assess sparsity robustness and present a systematic study of the phenomenon in
the FFN layers of modern LLMs, including diffusion LLMs. Our findings reveal
universal patterns of activation sparsity in LLMs, provide insights into this
phenomenon, and offer practical guidelines for exploiting it in model design
and acceleration.

</details>


### [249] [Localizing and Mitigating Memorization in Image Autoregressive Models](https://arxiv.org/abs/2509.00488)
*Aditya Kasliwal,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: 本文研究图像自回归（IAR）模型的记忆问题，发现不同架构记忆模式不同，干预记忆组件可降低数据提取能力且对图像质量影响小。


<details>
  <summary>Details</summary>
Motivation: IAR模型虽性能优但存在训练数据记忆及隐私问题，需探究记忆发生位置和方式。

Method: 测量细粒度记忆，分析不同IAR架构的记忆模式。

Result: 不同IAR架构记忆模式不同，干预记忆组件可显著降低数据提取能力，对生成图像质量影响小。

Conclusion: 研究为图像生成模型内部行为提供新见解，指出缓解隐私风险的实用策略。

Abstract: Image AutoRegressive (IAR) models have achieved state-of-the-art performance
in speed and quality of generated images. However, they also raise concerns
about memorization of their training data and its implications for privacy.
This work explores where and how such memorization occurs within different
image autoregressive architectures by measuring a fine-grained memorization.
The analysis reveals that memorization patterns differ across various
architectures of IARs. In hierarchical per-resolution architectures, it tends
to emerge early and deepen with resolutions, while in IARs with standard
autoregressive per token prediction, it concentrates in later processing
stages. These localization of memorization patterns are further connected to
IARs' ability to memorize and leak training data. By intervening on their most
memorizing components, we significantly reduce the capacity for data extraction
from IARs with minimal impact on the quality of generated images. These
findings offer new insights into the internal behavior of image generative
models and point toward practical strategies for mitigating privacy risks.

</details>


### [250] [Graph Convolutional Network With Pattern-Spatial Interactive and Regional Awareness for Traffic Forecasting](https://arxiv.org/abs/2509.00515)
*Xinyu Ji,Chengcheng Yan,Jibiao Yuan,Fiefie Zhao*

Main category: cs.LG

TL;DR: 提出PSIRAGCN进行交通预测，在三个数据集上表现优于基线且平衡计算成本。


<details>
  <summary>Details</summary>
Motivation: 以往研究难以有效建模不同感知视角的时空相关性，忽略交通模式与空间相关性的交互融合，且未考虑区域异质性。

Method: 提出由模式和空间模块组成的模式 - 空间交互融合框架，在空间模块设计基于消息传递的图卷积网络，利用区域特征库重建具有区域感知的数据驱动消息传递。

Result: 在三个真实世界交通数据集上的实验表明，PSIRAGCN优于最先进的基线模型。

Conclusion: PSIRAGCN能有效解决以往研究的局限，在交通预测中表现良好并平衡计算成本。

Abstract: Traffic forecasting is significant for urban traffic management, intelligent
route planning, and real-time flow monitoring. Recent advances in
spatial-temporal models have markedly improved the modeling of intricate
spatial-temporal correlations for traffic forecasting. Unfortunately, most
previous studies have encountered challenges in effectively modeling
spatial-temporal correlations across various perceptual perspectives, which
have neglected the interactive fusion between traffic patterns and spatial
correlations. Additionally, constrained by spatial heterogeneity, most studies
fail to consider distinct regional heterogeneity during message-passing. To
overcome these limitations, we propose a Pattern-Spatial Interactive and
Regional Awareness Graph Convolutional Network (PSIRAGCN) for traffic
forecasting. Specifically, we propose a pattern-spatial interactive fusion
framework composed of pattern and spatial modules. This framework aims to
capture patterns and spatial correlations by adopting a perception perspective
from the global to the local level and facilitating mutual utilization with
positive feedback. In the spatial module, we designed a graph convolutional
network based on message-passing. The network is designed to leverage a
regional characteristics bank to reconstruct data-driven message-passing with
regional awareness. Reconstructed message passing can reveal the regional
heterogeneity between nodes in the traffic network. Extensive experiments on
three real-world traffic datasets demonstrate that PSIRAGCN outperforms the
State-of-the-art baseline while balancing computational costs.

</details>


### [251] [Biological Pathway Informed Models with Graph Attention Networks (GATs)](https://arxiv.org/abs/2509.00524)
*Gavin Wong,Ping Shu Ho,Ivan Au Yeung,Ka Chun Cheung,Simon See*

Main category: cs.LG

TL;DR: 提出基因水平建模通路的GAT框架，优于MLP，能验证生物学先验、提升模型鲁棒性并从数据发现基因相互作用。


<details>
  <summary>Details</summary>
Motivation: 多数ML模型忽略已知通路结构，现有通路感知模型丢弃拓扑和基因相互作用。

Method: 提出图注意力网络（GAT）框架在基因水平对通路建模。

Result: GAT泛化性远超MLP，预测通路动态时MSE降低81%；通过边干预验证生物学先验、提升模型鲁棒性；从原始mRNA数据正确发现TP53 - MDM2 - MDM4反馈回路的基因相互作用。

Conclusion: GAT模型有潜力直接从实验数据生成新生物学假设。

Abstract: Biological pathways map gene-gene interactions that govern all human
processes. Despite their importance, most ML models treat genes as unstructured
tokens, discarding known pathway structure. The latest pathway-informed models
capture pathway-pathway interactions, but still treat each pathway as a "bag of
genes" via MLPs, discarding its topology and gene-gene interactions. We propose
a Graph Attention Network (GAT) framework that models pathways at the gene
level. We show that GATs generalize much better than MLPs, achieving an 81%
reduction in MSE when predicting pathway dynamics under unseen treatment
conditions. We further validate the correctness of our biological prior by
encoding drug mechanisms via edge interventions, boosting model robustness.
Finally, we show that our GAT model is able to correctly rediscover all five
gene-gene interactions in the canonical TP53-MDM2-MDM4 feedback loop from raw
time-series mRNA data, demonstrating potential to generate novel biological
hypotheses directly from experimental data.

</details>


### [252] [FedThief: Harming Others to Benefit Oneself in Self-Centered Federated Learning](https://arxiv.org/abs/2509.00540)
*Xiangyu Zhang,Mang Ye*

Main category: cs.LG

TL;DR: 研究新型自利联邦学习攻击范式，提出FedThief框架，实验表明能有效降低全局模型性能并使攻击者模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有攻击策略使攻击者自身模型也受影响，而现实中攻击者有自利动机，想让自身模型优于其他参与者。

Method: 提出FedThief框架，上传阶段上传修改内容降低全局模型性能，通过差异感知集成技术提升私有模型性能。

Result: 有效降低全局模型性能，攻击者获得显著优于全局模型的集成模型。

Conclusion: 所提方法能在联邦学习中达成攻击者自利目标，即降低全局模型性能同时提升自身模型性能。

Abstract: In federated learning, participants' uploaded model updates cannot be
directly verified, leaving the system vulnerable to malicious attacks. Existing
attack strategies have adversaries upload tampered model updates to degrade the
global model's performance. However, attackers also degrade their own private
models, gaining no advantage. In real-world scenarios, attackers are driven by
self-centered motives: their goal is to gain a competitive advantage by
developing a model that outperforms those of other participants, not merely to
cause disruption. In this paper, we study a novel Self-Centered Federated
Learning (SCFL) attack paradigm, in which attackers not only degrade the
performance of the global model through attacks but also enhance their own
models within the federated learning process. We propose a framework named
FedThief, which degrades the performance of the global model by uploading
modified content during the upload stage. At the same time, it enhances the
private model's performance through divergence-aware ensemble techniques, where
"divergence" quantifies the deviation between private and global models, that
integrate global updates and local knowledge. Extensive experiments show that
our method effectively degrades the global model performance while allowing the
attacker to obtain an ensemble model that significantly outperforms the global
model.

</details>


### [253] [Advanced spectral clustering for heterogeneous data in credit risk monitoring systems](https://arxiv.org/abs/2509.00546)
*Lu Han,Mengyan Li,Jiping Qiang,Zhi Su*

Main category: cs.LG

TL;DR: 提出高级谱聚类（ASC）方法处理异构数据信用监测问题，在中小企业数据集上表现良好，能识别有意义集群支持信贷干预。


<details>
  <summary>Details</summary>
Motivation: 异构数据给信用监测带来挑战，需有效方法处理。

Method: 提出ASC方法，通过优化权重参数整合财务和文本相似性，用新的特征值 - 轮廓优化方法选择特征向量。

Result: 在1428家中小企业数据集上，ASC的轮廓分数比单类型数据基线方法高18%；聚类结果有可操作见解，如51%低风险企业文本含“社会招聘”；在多种聚类算法中稳健性良好。

Conclusion: ASC将谱聚类理论与异构数据应用结合，能识别有意义集群，支持更有针对性和有效的信贷干预。

Abstract: Heterogeneous data, which encompass both numerical financial variables and
textual records, present substantial challenges for credit monitoring. To
address this issue, we propose Advanced Spectral Clustering (ASC), a method
that integrates financial and textual similarities through an optimized weight
parameter and selects eigenvectors using a novel eigenvalue-silhouette
optimization approach. Evaluated on a dataset comprising 1,428 small and
medium-sized enterprises (SMEs), ASC achieves a Silhouette score that is 18%
higher than that of a single-type data baseline method. Furthermore, the
resulting clusters offer actionable insights; for instance, 51% of low-risk
firms are found to include the term 'social recruitment' in their textual
records. The robustness of ASC is confirmed across multiple clustering
algorithms, including k-means, k-medians, and k-medoids, with
{\Delta}Intra/Inter < 0.13 and {\Delta}Silhouette Coefficient < 0.02. By
bridging spectral clustering theory with heterogeneous data applications, ASC
enables the identification of meaningful clusters, such as recruitment-focused
SMEs exhibiting a 30% lower default risk, thereby supporting more targeted and
effective credit interventions.

</details>


### [254] [Integrated Multivariate Segmentation Tree for the Analysis of Heterogeneous Credit Data in Small and Medium-Sized Enterprises](https://arxiv.org/abs/2509.00550)
*Lu Han,Xiuying Wang*

Main category: cs.LG

TL;DR: 提出综合多元分割树(IMST)框架用于中小企业信用评估，方法含文本数据转换、特征选择和树构建，实验显示IMST精度超基线模型，且有更好的可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统决策树模型处理高维数据及融合文本信息有困难，为提升中小企业信用评估效果。

Method: 分三步，一是用矩阵分解将文本数据转为数值矩阵；二是用Lasso回归选择显著财务特征；三是基于基尼指数或熵构建多元分割树，并进行最弱链接剪枝。

Result: 在1428家中国中小企业数据集上，IMST准确率达88.9%，超过基线决策树及逻辑回归、支持向量机等传统模型。

Conclusion: IMST模型在精度、可解释性、计算效率和风险检测能力上表现更优。

Abstract: Traditional decision tree models, which rely exclusively on numerical
variables, often encounter difficulties in handling high-dimensional data and
fail to effectively incorporate textual information. To address these
limitations, we propose the Integrated Multivariate Segmentation Tree (IMST), a
comprehensive framework designed to enhance credit evaluation for small and
medium-sized enterprises (SMEs) by integrating financial data with textual
sources. The methodology comprises three core stages: (1) transforming textual
data into numerical matrices through matrix factorization; (2) selecting
salient financial features using Lasso regression; and (3) constructing a
multivariate segmentation tree based on the Gini index or Entropy, with
weakest-link pruning applied to regulate model complexity. Experimental results
derived from a dataset of 1,428 Chinese SMEs demonstrate that IMST achieves an
accuracy of 88.9%, surpassing baseline decision trees (87.4%) as well as
conventional models such as logistic regression and support vector machines
(SVM). Furthermore, the proposed model exhibits superior interpretability and
computational efficiency, featuring a more streamlined architecture and
enhanced risk detection capabilities.

</details>


### [255] [TranCIT: Transient Causal Interaction Toolbox](https://arxiv.org/abs/2509.00602)
*Salar Nouri,Kaidi Shao,Shervin Safavi*

Main category: cs.LG

TL;DR: 介绍开源Python包TranCIT用于量化非平稳神经信号的瞬态因果相互作用，展示其有效性和优势。


<details>
  <summary>Details</summary>
Motivation: 传统方法对短暂神经事件分析不足，先进的特定事件技术在Python生态中缺乏易用实现，需填补此空白。

Method: TranCIT实现了包括Granger因果关系、转移熵、基于结构因果模型的动态因果强度（DCS）和相对动态因果强度（rDCS）的综合分析管道。

Result: TranCIT成功捕获了传统方法失效的高同步状态下的因果关系，并在真实数据中识别出了海马CA3到CA1的已知瞬态信息流。

Conclusion: TranCIT为研究复杂系统的瞬态因果动力学提供了用户友好且经过验证的解决方案。

Abstract: Quantifying transient causal interactions from non-stationary neural signals
is a fundamental challenge in neuroscience. Traditional methods are often
inadequate for brief neural events, and advanced, event-specific techniques
have lacked accessible implementations within the Python ecosystem. Here, we
introduce trancit (Transient Causal Interaction Toolbox), an open-source Python
package designed to bridge this gap. TranCIT implements a comprehensive
analysis pipeline, including Granger Causality, Transfer Entropy, and the more
robust Structural Causal Model-based Dynamic Causal Strength (DCS) and relative
Dynamic Causal Strength (rDCS) for accurately detecting event-driven causal
effects. We demonstrate TranCIT's utility by successfully capturing causality
in high-synchrony regimes where traditional methods fail and by identifying the
known transient information flow from hippocampal CA3 to CA1 during sharp-wave
ripple events in real-world data. The package offers a user-friendly, validated
solution for investigating the transient causal dynamics that govern complex
systems.

</details>


### [256] [RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models](https://arxiv.org/abs/2509.00614)
*Shikun Liu,Deyu Zou,Nima Shoghi,Victor Fung,Kai Liu,Pan Li*

Main category: cs.LG

TL;DR: 在基础模型时代，微调预训练模型很关键，针对分子图基础模型（MGFMs）微调难题，分类八种微调方法并评估，设计出 ROFT - MOL 方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型微调中如过拟合和稀疏标注等问题，特别是 MGFMs 因数据集小和数据稀缺、需兼顾多样任务带来的微调困难。

Method: 将八种微调方法分为三类机制，在多种标注设置的下游回归和分类任务上对这些方法进行基准测试。

Result: 评估为设计新方法提供见解，设计出 ROFT - MOL 方法，结合两种微调方法优势，在两类任务上表现更好且使用方便。

Conclusion: ROFT - MOL 方法能有效提升 MGFMs 在下游任务的微调性能。

Abstract: In the era of foundation models, fine-tuning pre-trained models for specific
downstream tasks has become crucial. This drives the need for robust
fine-tuning methods to address challenges such as model overfitting and sparse
labeling. Molecular graph foundation models (MGFMs) face unique difficulties
that complicate fine-tuning. These models are limited by smaller pre-training
datasets and more severe data scarcity for downstream tasks, both of which
require enhanced model generalization. Moreover, MGFMs must accommodate diverse
objectives, including both regression and classification tasks. To better
understand and improve fine-tuning techniques under these conditions, we
classify eight fine-tuning methods into three mechanisms: weight-based,
representation-based, and partial fine-tuning. We benchmark these methods on
downstream regression and classification tasks across supervised and
self-supervised pre-trained models in diverse labeling settings. This extensive
evaluation provides valuable insights and informs the design of a refined
robust fine-tuning method, ROFT-MOL. This approach combines the strengths of
simple post-hoc weight interpolation with more complex weight ensemble
fine-tuning methods, delivering improved performance across both task types
while maintaining the ease of use inherent in post-hoc weight interpolation.

</details>


### [257] [TimeCopilot](https://arxiv.org/abs/2509.00616)
*Azul Garza,Reneé Rosillo*

Main category: cs.LG

TL;DR: 介绍TimeCopilot——首个开源预测框架，结合TSFMs和LLMs，自动化预测流程，结果优且提供实用基础。


<details>
  <summary>Details</summary>
Motivation: 构建一个结合多时间序列基础模型和大语言模型的开源预测框架，实现自动化预测流程并提供自然语言解释。

Method: 通过单一统一API结合TSFMs和LLMs，自动化特征分析、模型选择等预测流程，框架对LLM无特定依赖，支持多种模型和集成。

Result: 在大规模GIFT - Eval基准测试中，TimeCopilot以低成本实现了最先进的概率预测性能。

Conclusion: 该框架为可重现、可解释且易访问的代理预测系统提供了实用基础。

Abstract: We introduce TimeCopilot, the first open-source agentic framework for
forecasting that combines multiple Time Series Foundation Models (TSFMs) with
Large Language Models (LLMs) through a single unified API. TimeCopilot
automates the forecasting pipeline: feature analysis, model selection,
cross-validation, and forecast generation, while providing natural language
explanations and supporting direct queries about the future. The framework is
LLM-agnostic, compatible with both commercial and open-source models, and
supports ensembles across diverse forecasting families. Results on the
large-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art
probabilistic forecasting performance at low cost. Our framework provides a
practical foundation for reproducible, explainable, and accessible agentic
forecasting systems.

</details>


### [258] [Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers](https://arxiv.org/abs/2509.00631)
*Giacomo Acciarini,Simone Mestici,Halil Kelebek,Linnea Wolniewicz,Michael Vergalla,Madhulika Guhathakurta,Umaa Rebbapragada,Bala Poduval,Atılım Güneş Baydin,Frank Soboczenski*

Main category: cs.LG

TL;DR: 提出用Temporal Fusion Transformers预测电离层TEC的机器学习框架，实验效果好且开源代码。


<details>
  <summary>Details</summary>
Motivation: 电离层对相关系统影响大，但准确预测其变化有挑战性，TEC可靠预测受测量稀疏和经验模型精度限制。

Method: 采用Temporal Fusion Transformers构建机器学习框架，容纳异质输入源，应用预处理和时间对齐策略。

Result: 模型能提前24小时进行稳健预测，均方根误差低至3.33 TECU，太阳EUV辐照度预测信号最强。

Conclusion: 框架有预测精度和可解释性，开源代码利于复现和社区开发。

Abstract: The ionosphere critically influences Global Navigation Satellite Systems
(GNSS), satellite communications, and Low Earth Orbit (LEO) operations, yet
accurate prediction of its variability remains challenging due to nonlinear
couplings between solar, geomagnetic, and thermospheric drivers. Total Electron
Content (TEC), a key ionospheric parameter, is derived from GNSS observations,
but its reliable forecasting is limited by the sparse nature of global
measurements and the limited accuracy of empirical models, especially during
strong space weather conditions. In this work, we present a machine learning
framework for ionospheric TEC forecasting that leverages Temporal Fusion
Transformers (TFT) to predict sparse ionosphere data. Our approach accommodates
heterogeneous input sources, including solar irradiance, geomagnetic indices,
and GNSS-derived vertical TEC, and applies preprocessing and temporal alignment
strategies. Experiments spanning 2010-2025 demonstrate that the model achieves
robust predictions up to 24 hours ahead, with root mean square errors as low as
3.33 TECU. Results highlight that solar EUV irradiance provides the strongest
predictive signals. Beyond forecasting accuracy, the framework offers
interpretability through attention-based analysis, supporting both operational
applications and scientific discovery. To encourage reproducibility and
community-driven development, we release the full implementation as the
open-source toolkit \texttt{ionopy}.

</details>


### [259] [Disentangling Slow and Fast Temporal Dynamics in Degradation Inference with Hierarchical Differential Models](https://arxiv.org/abs/2509.00639)
*Mengjie Zhao,Olga Fink*

Main category: cs.LG

TL;DR: 提出分层受控微分方程(H - CDE)框架解决系统退化推断难题，经评估性能优于基于残差的方法。


<details>
  <summary>Details</summary>
Motivation: 从传感器数据可靠推断系统退化对状态监测和预测至关重要，但操作变化主导系统行为，使潜在退化过程难以分离，现有方法存在局限性。

Method: 提出H - CDE框架，包含慢（退化）和快（操作）CDE组件，有多尺度时间积分方案、可学习路径变换和新型激活函数三个创新点。

Result: 在动态响应和稳态系统上的综合评估表明，H - CDE能有效分离退化与操作动态。

Conclusion: H - CDE优于基于残差的基线方法，能更准确、稳健和可解释地推断系统退化。

Abstract: Reliable inference of system degradation from sensor data is fundamental to
condition monitoring and prognostics in engineered systems. Since degradation
is rarely observable and measurable, it must be inferred to enable accurate
health assessment and decision-making. This is particularly challenging because
operational variations dominate system behavior, while degradation introduces
only subtle, long-term changes. Consequently, sensor data mainly reflect
short-term operational variability, making it difficult to disentangle the
underlying degradation process. Residual-based methods are widely employed, but
the residuals remain entangled with operational history, often resulting in
noisy and unreliable degradation estimation, particularly in systems with
dynamic responses. Neural Ordinary Equations (NODEs) offer a promising
framework for inferring latent dynamics, but the time-scale separation in
slow-fast systems introduces numerical stiffness and complicates training,
while degradation disentanglement remains difficult. To address these
limitations, we propose a novel Hierarchical Controlled Differential Equation
(H-CDE) framework that incorporates a slow (degradation) and a fast (operation)
CDE component in a unified architecture. It introduces three key innovations: a
multi-scale time integration scheme to mitigate numerical stiffness; a
learnable path transformation that extracts latent degradation drivers to
control degradation evolution; and a novel activation function that enforces
monotonicity on inferred degradation as a regularizer for disentanglement.
Through comprehensive evaluations on both dynamic response (e.g., bridges) and
steady state (e.g., aero-engine) systems, we demonstrate that H-CDE effectively
disentangles degradation from operational dynamics and outperforms
residual-based baselines, yielding more accurate, robust, and interpretable
inference.

</details>


### [260] [AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models](https://arxiv.org/abs/2509.00641)
*Zhipeng Yin,Zichong Wang,Avash Palikhe,Zhen Liu,Jun Liu,Wenbin Zhang*

Main category: cs.LG

TL;DR: 生成式模型在文生图任务中成果显著，但存在版权风险。本文提出AMCR框架缓解风险，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 生成式模型依赖大量训练数据，可能复制版权元素，现有基于提示的版权风险缓解策略在处理微妙情况时存在不足。

Method: 提出AMCR框架，包括将有风险的提示重构为安全形式、通过基于注意力的相似度分析检测部分侵权、在生成过程中自适应缓解风险。

Result: 广泛的实验验证了AMCR在揭示和缓解潜在版权风险方面的有效性。

Conclusion: AMCR框架为生成式模型的安全部署提供了实用见解和基准。

Abstract: Generative models have achieved impressive results in text to image tasks,
significantly advancing visual content creation. However, this progress comes
at a cost, as such models rely heavily on large-scale training data and may
unintentionally replicate copyrighted elements, creating serious legal and
ethical challenges for real-world deployment. To address these concerns,
researchers have proposed various strategies to mitigate copyright risks, most
of which are prompt based methods that filter or rewrite user inputs to prevent
explicit infringement. While effective in handling obvious cases, these
approaches often fall short in more subtle situations, where seemingly benign
prompts can still lead to infringing outputs. To address these limitations,
this paper introduces Assessing and Mitigating Copyright Risks (AMCR), a
comprehensive framework which i) builds upon prompt-based strategies by
systematically restructuring risky prompts into safe and non-sensitive forms,
ii) detects partial infringements through attention-based similarity analysis,
and iii) adaptively mitigates risks during generation to reduce copyright
violations without compromising image quality. Extensive experiments validate
the effectiveness of AMCR in revealing and mitigating latent copyright risks,
offering practical insights and benchmarks for the safer deployment of
generative models.

</details>


### [261] [Missing Data Imputation using Neural Cellular Automata](https://arxiv.org/abs/2509.00651)
*Tin Luu,Binh Nguyen,Man Ngo*

Main category: cs.LG

TL;DR: 本文提出受神经细胞自动机（NCA）启发的表格数据缺失值插补新方法，实验证明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 过往研究在利用生成模型解决缺失值插补任务时忽略了强大的计算模型NCA，希望探索基于NCA的插补方法。

Method: 提出受NCA启发的插补方法，并进行适当调整以解决缺失数据插补问题。

Result: 实验表明该模型在插补误差和插补后性能方面优于现有技术。

Conclusion: 经适当调整，基于NCA的模型能够有效解决缺失数据插补问题。

Abstract: When working with tabular data, missingness is always one of the most painful
problems. Throughout many years, researchers have continuously explored better
and better ways to impute missing data. Recently, with the rapid development
evolution in machine learning and deep learning, there is a new trend of
leveraging generative models to solve the imputation task. While the imputing
version of famous models such as Variational Autoencoders or Generative
Adversarial Networks were investigated, prior work has overlooked Neural
Cellular Automata (NCA), a powerful computational model. In this paper, we
propose a novel imputation method that is inspired by NCA. We show that, with
some appropriate adaptations, an NCA-based model is able to address the missing
data imputation problem. We also provide several experiments to evidence that
our model outperforms state-of-the-art methods in terms of imputation error and
post-imputation performance.

</details>


### [262] [IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather Forecasting over India](https://arxiv.org/abs/2509.00653)
*Tung Nguyen,Harkanwar Singh,Nilay Naharas,Lucas Bandarkar,Aditya Grover*

Main category: cs.LG

TL;DR: 提出针对印度次大陆的区域天气预报基准IndiaWeatherBench，开源数据和代码以推动区域天气预报研究。


<details>
  <summary>Details</summary>
Motivation: 区域天气预报重要但研究不足，现有工作因数据集和实验设置不同限制比较和复现性。

Method: 引入IndiaWeatherBench，提供整理的数据集和评估指标，实现并评估多种模型架构、边界条件策略和训练目标。

Result: 建立了多种模型的强基线，IndiaWeatherBench易于扩展到其他地理区域。

Conclusion: 希望IndiaWeatherBench能为区域天气预报研究奠定基础。

Abstract: Regional weather forecasting is a critical problem for localized climate
adaptation, disaster mitigation, and sustainable development. While machine
learning has shown impressive progress in global weather forecasting, regional
forecasting remains comparatively underexplored. Existing efforts often use
different datasets and experimental setups, limiting fair comparison and
reproducibility. We introduce IndiaWeatherBench, a comprehensive benchmark for
data-driven regional weather forecasting focused on the Indian subcontinent.
IndiaWeatherBench provides a curated dataset built from high-resolution
regional reanalysis products, along with a suite of deterministic and
probabilistic metrics to facilitate consistent training and evaluation. To
establish strong baselines, we implement and evaluate a range of models across
diverse architectures, including UNets, Transformers, and Graph-based networks,
as well as different boundary conditioning strategies and training objectives.
While focused on India, IndiaWeatherBench is easily extensible to other
geographic regions. We open-source all raw and preprocessed datasets, model
implementations, and evaluation pipelines to promote accessibility and future
development. We hope IndiaWeatherBench will serve as a foundation for advancing
regional weather forecasting research. Code is available at
https://github.com/tung-nd/IndiaWeatherBench.

</details>


### [263] [Valid Property-Enhanced Contrastive Learning for Targeted Optimization & Resampling for Novel Drug Design](https://arxiv.org/abs/2509.00684)
*Amartya Banerjee,Somnath Kar,Anirban Pal,Debabrata Maiti*

Main category: cs.LG

TL;DR: 提出VECTOR+框架用于低数据场景下分子药物发现，在两个数据集上评估，生成新的可合成候选分子，表现优于基准方法，是低数据下属性条件分子设计的可靠方法。


<details>
  <summary>Details</summary>
Motivation: 解决低数据场景下将生成模型导向化学空间药理相关区域的难题。

Method: 提出VECTOR+框架，将属性引导的表征学习与可控分子生成相结合，适用于回归和分类任务。

Result: 在两个数据集上生成新的可合成候选分子，对PD - L1有良好表现，能推广到激酶抑制剂，优于JT - VAE和MolGPT。

Conclusion: VECTOR+是低数据场景下属性条件分子设计的可靠、可扩展方法，连接了对比学习和生成建模。

Abstract: Efficiently steering generative models toward pharmacologically relevant
regions of chemical space remains a major obstacle in molecular drug discovery
under low-data regimes. We present VECTOR+: Valid-property-Enhanced Contrastive
Learning for Targeted Optimization and Resampling, a framework that couples
property-guided representation learning with controllable molecule generation.
VECTOR+ applies to both regression and classification tasks and enables
interpretable, data-efficient exploration of functional chemical space. We
evaluate on two datasets: a curated PD-L1 inhibitor set (296 compounds with
experimental $IC_{50}$ values) and a receptor kinase inhibitor set (2,056
molecules by binding mode). Despite limited training data, VECTOR+ generates
novel, synthetically tractable candidates. Against PD-L1 (PDB 5J89), 100 of
8,374 generated molecules surpass a docking threshold of $-15.0$ kcal/mol, with
the best scoring $-17.6$ kcal/mol compared to the top reference inhibitor
($-15.4$ kcal/mol). The best-performing molecules retain the conserved biphenyl
pharmacophore while introducing novel motifs. Molecular dynamics (250 ns)
confirm binding stability (ligand RMSD < $2.5$ angstroms). VECTOR+ generalizes
to kinase inhibitors, producing compounds with stronger docking scores than
established drugs such as brigatinib and sorafenib. Benchmarking against JT-VAE
and MolGPT across docking, novelty, uniqueness, and Tanimoto similarity
highlights the superior performance of our method. These results position our
work as a robust, extensible approach for property-conditioned molecular design
in low-data settings, bridging contrastive learning and generative modeling for
reproducible, AI-accelerated discovery.

</details>


### [264] [DELTA: Variational Disentangled Learning for Privacy-Preserving Data Reprogramming](https://arxiv.org/abs/2509.00693)
*Arun Vignesh Malarkkan,Haoyue Bai,Anjali Kaushik,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出DELTA框架解决隐私保护数据重编程问题，实验表明其提升预测性能并减少隐私泄露


<details>
  <summary>Details</summary>
Motivation: 现有特征工程重下游任务性能，有隐私泄露风险，提出隐私保护数据重编程（PPDR）任务

Method: 提出两阶段变分解纠缠生成学习框架DELTA，第一阶段用策略引导强化学习发现特征转换，第二阶段用变分LSTM编解码器和正则化抑制隐私信号

Result: 在八个数据集上实验，DELTA提升预测性能约9.3%，减少隐私泄露约35%

Conclusion: DELTA实现了鲁棒、有隐私意识的数据转换

Abstract: In real-world applications, domain data often contains identifiable or
sensitive attributes, is subject to strict regulations (e.g., HIPAA, GDPR), and
requires explicit data feature engineering for interpretability and
transparency. Existing feature engineering primarily focuses on advancing
downstream task performance, often risking privacy leakage. We generalize this
learning task under such new requirements as Privacy-Preserving Data
Reprogramming (PPDR): given a dataset, transforming features to maximize target
attribute prediction accuracy while minimizing sensitive attribute prediction
accuracy. PPDR poses challenges for existing systems: 1) generating
high-utility feature transformations without being overwhelmed by a large
search space, and 2) disentangling and eliminating sensitive information from
utility-oriented features to reduce privacy inferability. To tackle these
challenges, we propose DELTA, a two-phase variational disentangled generative
learning framework. Phase I uses policy-guided reinforcement learning to
discover feature transformations with downstream task utility, without any
regard to privacy inferability. Phase II employs a variational LSTM seq2seq
encoder-decoder with a utility-privacy disentangled latent space design and
adversarial-causal disentanglement regularization to suppress privacy signals
during feature generation. Experiments on eight datasets show DELTA improves
predictive performance by ~9.3% and reduces privacy leakage by ~35%,
demonstrating robust, privacy-aware data transformation.

</details>


### [265] [Robust Spatiotemporal Forecasting Using Adaptive Deep-Unfolded Variational Mode Decomposition](https://arxiv.org/abs/2509.00703)
*Osama Ahmad,Lukas Wesemann,Fabian Waschkowski,Zubair Khalid*

Main category: cs.LG

TL;DR: 提出模式自适应图网络MAGN解决时空预测难题，性能优。


<details>
  <summary>Details</summary>
Motivation: 传统GNN时空预测因复杂模式和谱纠缠有挑战，分解集成方法有计算低效和手动调参问题。

Method: 将迭代变分模态分解VMD转化为可训练神经模块，包括用固定深度网络代替迭代优化的UVMD模块和可学习带宽约束。

Result: 在LargeST基准上，MAGN比VMGCN预测误差降低85 - 95%，优于现有基线。

Conclusion: MAGN有效解决了现有方法局限性，提升时空预测性能。

Abstract: Accurate spatiotemporal forecasting is critical for numerous complex systems
but remains challenging due to complex volatility patterns and spectral
entanglement in conventional graph neural networks (GNNs). While
decomposition-integrated approaches like variational mode graph convolutional
network (VMGCN) improve accuracy through signal decomposition, they suffer from
computational inefficiency and manual hyperparameter tuning. To address these
limitations, we propose the mode adaptive graph network (MAGN) that transforms
iterative variational mode decomposition (VMD) into a trainable neural module.
Our key innovations include (1) an unfolded VMD (UVMD) module that replaces
iterative optimization with a fixed-depth network to reduce the decomposition
time (by 250x for the LargeST benchmark), and (2) mode-specific learnable
bandwidth constraints ({\alpha}k ) adapt spatial heterogeneity and eliminate
manual tuning while preventing spectral overlap. Evaluated on the LargeST
benchmark (6,902 sensors, 241M observations), MAGN achieves an 85-95% reduction
in the prediction error over VMGCN and outperforms state-of-the-art baselines.

</details>


### [266] [Why Pool When You Can Flow? Active Learning with GFlowNets](https://arxiv.org/abs/2509.00704)
*Renfei Zhang,Mohit Pandey,Artem Cherkasov,Martin Ester*

Main category: cs.LG

TL;DR: 提出BALD - GFlowNet生成式主动学习框架解决池式主动学习在虚拟筛选中可扩展性问题，实验显示其性能与标准BALD相当且生成分子结构更多样。


<details>
  <summary>Details</summary>
Motivation: 池式主动学习在评估大型未标记数据集时计算成本高，限制了可扩展性，在药物发现的虚拟筛选中问题尤其突出，传统方法如BALD扩展到数十亿样本库时计算量仍很大。

Method: 引入BALD - GFlowNet框架，利用生成流网络（GFlowNets）按BALD奖励比例直接采样对象，用生成式采样替代传统池式采集。

Result: 在虚拟筛选实验中，BALD - GFlowNet性能与标准BALD基线相当，且生成的分子结构更多样。

Conclusion: BALD - GFlowNet为高效可扩展的分子发现提供了有前景的方向。

Abstract: The scalability of pool-based active learning is limited by the computational
cost of evaluating large unlabeled datasets, a challenge that is particularly
acute in virtual screening for drug discovery. While active learning strategies
such as Bayesian Active Learning by Disagreement (BALD) prioritize informative
samples, it remains computationally intensive when scaled to libraries
containing billions samples. In this work, we introduce BALD-GFlowNet, a
generative active learning framework that circumvents this issue. Our method
leverages Generative Flow Networks (GFlowNets) to directly sample objects in
proportion to the BALD reward. By replacing traditional pool-based acquisition
with generative sampling, BALD-GFlowNet achieves scalability that is
independent of the size of the unlabeled pool. In our virtual screening
experiment, we show that BALD-GFlowNet achieves a performance comparable to
that of standard BALD baseline while generating more structurally diverse
molecules, offering a promising direction for efficient and scalable molecular
discovery.

</details>


### [267] [Task-Aware Adaptive Modulation: A Replay-Free and Resource-Efficient Approach For Continual Graph Learning](https://arxiv.org/abs/2509.00735)
*Jingtao Liu,Xinming Zhang*

Main category: cs.LG

TL;DR: 本文提出无重放、资源高效的TAAM方法解决持续图学习中的问题，实验显示其全面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前持续图学习方法存在稳定性 - 可塑性困境和资源密集预训练问题，需要新方法解决。

Method: 提出TAAM方法，核心是神经突触调制器（NSM），训练后冻结存储专家知识，采用原型引导策略，插入冻结的GNN骨干进行细粒度调制。

Result: 在六个GCIL基准数据集上，TAAM全面优于现有方法。

Conclusion: TAAM为解决持续图学习的稳定性 - 可塑性困境提供了新途径，是一种资源高效的方法。

Abstract: Continual Graph Learning(CGL)focuses on acquiring new knowledge while
retaining previously learned information, essential for real-world graph
applications. Current methods grapple with two main issues:1) The
Stability-Plasticity Dilemma: Replay-based methods often create an imbalance
between the Dilemma, while incurring significant storage costs.2) The
Resource-Heavy Pre-training: Leading replay-free methods critically depend on
extensively pre-trained backbones, this reliance imposes a substantial resource
burden.In this paper, we argue that the key to overcoming these challenges lies
not in replaying data or fine-tuning the entire network, but in dynamically
modulating the internal computational flow of a frozen backbone. We posit that
lightweight, task-specific modules can effectively steer a GNN's reasoning
process. Motivated by this insight, we propose Task-Aware Adaptive
Modulation(TAAM), a replay-free, resource-efficient approach that charts a new
path for navigating the stability-plasticity dilemma. TAAM's core is its Neural
Synapse Modulators(NSM), which are trained and then frozen for each task to
store expert knowledge. A pivotal prototype-guided strategy governs these
modulators: 1) For training, it initializes a new NSM by deep-copying from a
similar past modulator to boost knowledge transfer. 2) For inference, it
selects the most relevant frozen NSM for each task. These NSMs insert into a
frozen GNN backbone to perform fine-grained, node-attentive modulation of its
internal flow-different from the static perturbations of prior methods.
Extensive experiments show that TAAM comprehensively outperforms
state-of-the-art methods across six GCIL benchmark datasets. The code will be
released upon acceptance of the paper.

</details>


### [268] [Attribute Fusion-based Classifier on Framework of Belief Structure](https://arxiv.org/abs/2509.00754)
*Qiying Hu,Yingying Liang,Qianli Zhou,Witold Pedrycz*

Main category: cs.LG

TL;DR: 本文提出增强的基于属性融合的分类器，通过选择性建模策略和新的BPA转换方法提升性能，实验显示其优于现有证据分类器。


<details>
  <summary>Details</summary>
Motivation: 传统基于DST的属性融合分类器存在隶属函数建模过于简单和对BPA信念结构利用有限的问题，在复杂场景效果不佳。

Method: 采用选择性建模策略构建隶属函数，引入新方法将可能性分布转换为BPA，并将基于信念结构的BPA生成方法应用于证据K近邻分类器。

Result: 所提分类器优于现有最佳证据分类器，平均准确率提高4.84%，且方差低。

Conclusion: 所提分类器具有卓越的有效性和鲁棒性。

Abstract: Dempster-Shafer Theory (DST) provides a powerful framework for modeling
uncertainty and has been widely applied to multi-attribute classification
tasks. However, traditional DST-based attribute fusion-based classifiers suffer
from oversimplified membership function modeling and limited exploitation of
the belief structure brought by basic probability assignment (BPA), reducing
their effectiveness in complex real-world scenarios. This paper presents an
enhanced attribute fusion-based classifier that addresses these limitations
through two key innovations. First, we adopt a selective modeling strategy that
utilizes both single Gaussian and Gaussian Mixture Models (GMMs) for membership
function construction, with model selection guided by cross-validation and a
tailored evaluation metric. Second, we introduce a novel method to transform
the possibility distribution into a BPA by combining simple BPAs derived from
normalized possibility distributions, enabling a much richer and more flexible
representation of uncertain information. Furthermore, we apply the belief
structure-based BPA generation method to the evidential K-Nearest Neighbors
classifier, enhancing its ability to incorporate uncertainty information into
decision-making. Comprehensive experiments on benchmark datasets are conducted
to evaluate the performance of the proposed attribute fusion-based classifier
and the enhanced evidential K-Nearest Neighbors classifier in comparison with
both evidential classifiers and conventional machine learning classifiers. The
results demonstrate that our proposed classifier outperforms the best existing
evidential classifier, achieving an average accuracy improvement of 4.84%,
while maintaining low variance, thus confirming its superior effectiveness and
robustness.

</details>


### [269] [Flow Matters: Directional and Expressive GNNs for Heterophilic Graphs](https://arxiv.org/abs/2509.00772)
*Arman Gupta,Govind Waghmare,Gaurav Oberoi,Nitish Srivastava*

Main category: cs.LG

TL;DR: 研究异质图中边方向性和表达性消息传递对节点分类的联合影响，提出Poly和Dir - Poly模型，实验表明模型表现出色且方向性消息传递效果依赖于上下文。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在异质图中因依赖局部同质性邻域而表现不佳，此前研究表明建模边方向性和多项式表达性GNN有提升效果，因此研究二者联合影响。

Method: 提出Polynomially expressive GAT baseline (Poly)和direction - aware variant (Dir - Poly)两种架构，学习输入特征上的排列等变高次多项式。

Result: Poly模型在五个基准异质数据集上持续优于现有基线，Dir - Poly在有固有方向性的图上有额外提升，达到最优结果；在无向图上引入人工方向性不总是有效。

Conclusion: 边方向和表达性特征建模在异质图学习中起互补作用。

Abstract: In heterophilic graphs, where neighboring nodes often belong to different
classes, conventional Graph Neural Networks (GNNs) struggle due to their
reliance on local homophilous neighborhoods. Prior studies suggest that
modeling edge directionality in such graphs can increase effective homophily
and improve classification performance. Simultaneously, recent work on
polynomially expressive GNNs shows promise in capturing higher-order
interactions among features. In this work, we study the combined effect of edge
directionality and expressive message passing on node classification in
heterophilic graphs. Specifically, we propose two architectures: (1) a
polynomially expressive GAT baseline (Poly), and (2) a direction-aware variant
(Dir-Poly) that separately aggregates incoming and outgoing edges. Both models
are designed to learn permutation-equivariant high-degree polynomials over
input features, while remaining scalable with no added time complexity.
Experiments on five benchmark heterophilic datasets show that our Poly model
consistently outperforms existing baselines, and that Dir-Poly offers
additional gains on graphs with inherent directionality (e.g., Roman Empire),
achieving state-of-the-art results. Interestingly, on undirected graphs,
introducing artificial directionality does not always help, suggesting that the
benefit of directional message passing is context-dependent. Our findings
highlight the complementary roles of edge direction and expressive feature
modeling in heterophilic graph learning.

</details>


### [270] [ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive Process Monitoring Methods](https://arxiv.org/abs/2509.00797)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt*

Main category: cs.LG

TL;DR: 本文提出 ProCause 方法解决现有 PresPM 评估方法的不足，研究表明集成模型更可靠，LSTM 在处理时间依赖时有潜力，且经实际数据验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有 PresPM 方法评估困难，RealCause 存在忽略过程数据时间依赖和仅依赖单一 CI 模型架构的问题。

Method: 引入 ProCause 方法，支持顺序和非顺序模型，集成多种 CI 架构。

Result: 研究发现 TARNet 并非总是最佳选择，集成模型更可靠，LSTM 在有时间依赖时有助于改进评估。

Conclusion: ProCause 能更可靠地评估 PresPM 方法。

Abstract: Prescriptive Process Monitoring (PresPM) is the subfield of Process Mining
that focuses on optimizing processes through real-time interventions based on
event log data. Evaluating PresPM methods is challenging due to the lack of
ground-truth outcomes for all intervention actions in datasets. A generative
deep learning approach from the field of Causal Inference (CI), RealCause, has
been commonly used to estimate the outcomes for proposed intervention actions
to evaluate a new policy. However, RealCause overlooks the temporal
dependencies in process data, and relies on a single CI model architecture,
TARNet, limiting its effectiveness. To address both shortcomings, we introduce
ProCause, a generative approach that supports both sequential (e.g., LSTMs) and
non-sequential models while integrating multiple CI architectures (S-Learner,
T-Learner, TARNet, and an ensemble). Our research using a simulator with known
ground truths reveals that TARNet is not always the best choice; instead, an
ensemble of models offers more consistent reliability, and leveraging LSTMs
shows potential for improved evaluations when temporal dependencies are
present. We further validate ProCause's practical effectiveness through a
real-world data analysis, ensuring a more reliable evaluation of PresPM
methods.

</details>


### [271] [Fairness in Federated Learning: Trends, Challenges, and Opportunities](https://arxiv.org/abs/2509.00799)
*Noorain Mukhtiar,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 文章探讨联邦学习公平性问题，分析偏置来源、现有技术优劣，概述公平性相关概念与评估指标，展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习因多种异质性导致公平性问题，影响系统有效性，阻碍其应用。

Method: 分析不同偏置来源，讨论现有缓解技术优缺点，概述公平性概念与技术方面，研究评估指标。

Result: 对公平性相关概念、理论基础、技术方面有全面概述，明确评估指标。

Conclusion: 提出有潜力推动公平联邦学习框架发展的开放研究方向，为该领域未来研究奠定基础。

Abstract: At the intersection of the cutting-edge technologies and privacy concerns,
Federated Learning (FL) with its distributed architecture, stands at the
forefront in a bid to facilitate collaborative model training across multiple
clients while preserving data privacy. However, the applicability of FL systems
is hindered by fairness concerns arising from numerous sources of heterogeneity
that can result in biases and undermine a system's effectiveness, with skewed
predictions, reduced accuracy, and inefficient model convergence. This survey
thus explores the diverse sources of bias, including but not limited to, data,
client, and model biases, and thoroughly discusses the strengths and
limitations inherited within the array of the state-of-the-art techniques
utilized in the literature to mitigate such disparities in the FL training
process. We delineate a comprehensive overview of the several notions,
theoretical underpinnings, and technical aspects associated with fairness and
their adoption in FL-based multidisciplinary environments. Furthermore, we
examine salient evaluation metrics leveraged to measure fairness
quantitatively. Finally, we envisage exciting open research directions that
have the potential to drive future advancements in achieving fairer FL
frameworks, in turn, offering a strong foundation for future research in this
pivotal area.

</details>


### [272] [XAI-Driven Machine Learning System for Driving Style Recognition and Personalized Recommendations](https://arxiv.org/abs/2509.00802)
*Feriel Amel Sellal,Ahmed Ayoub Bellachia,Meryem Malak Dif,Enguerrand De Rautlin De La Roy,Mouhamed Amine Bouchiha,Yacine Ghamri-Doudane*

Main category: cs.LG

TL;DR: 本文提出基于机器学习方法平衡驾驶风格分类准确性与可解释性，引入数据集，用多种算法，结合SHAP技术，表现与深度学习模型相当且更具透明实用性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型用于驾驶风格分类时黑盒特性限制可解释性和信任度，需平衡准确性与可解释性。

Method: 引入CARLA - Drive数据集，利用随机森林、梯度提升和支持向量机等机器学习技术，并应用SHAP可解释性技术。

Result: 随机森林和梯度提升分类器在三类分类任务上准确率达0.92。

Conclusion: 该方法在性能上与深度学习模型匹配，且为智能交通系统的实际部署提供透明度和实用性。

Abstract: Artificial intelligence (AI) is increasingly used in the automotive industry
for applications such as driving style classification, which aims to improve
road safety, efficiency, and personalize user experiences. While deep learning
(DL) models, such as Long Short-Term Memory (LSTM) networks, excel at this
task, their black-box nature limits interpretability and trust. This paper
proposes a machine learning (ML)-based method that balances high accuracy with
interpretability. We introduce a high-quality dataset, CARLA-Drive, and
leverage ML techniques like Random Forest (RF), Gradient Boosting (XGBoost),
and Support Vector Machine (SVM), which are efficient, lightweight, and
interpretable. In addition, we apply the SHAP (Shapley Additive Explanations)
explainability technique to provide personalized recommendations for safer
driving. Achieving an accuracy of 0.92 on a three-class classification task
with both RF and XGBoost classifiers, our approach matches DL models in
performance while offering transparency and practicality for real-world
deployment in intelligent transportation systems.

</details>


### [273] [Crystal Structure Prediction with a Geometric Permutation-Invariant Loss Function](https://arxiv.org/abs/2509.00832)
*Emmanuel Jehanno,Romain Menegaux,Julien Mairal,Sergei Grudinin*

Main category: cs.LG

TL;DR: 提出新损失函数SinkFast解决分子组装问题，在COD - Cluster17基准测试上优于现有流匹配方法。


<details>
  <summary>Details</summary>
Motivation: 晶体结构预测是材料设计的挑战，准确预测有机材料三维晶体结构困难，现有方法计算成本高。

Method: 提出新损失函数，通过基于Sinkhorn算法的可微线性分配方案，保证对分子集的排列不变性。

Result: 简单回归的SinkFast方法在COD - Cluster17基准测试上显著优于复杂的流匹配方法。

Conclusion: 新提出的SinkFast方法在分子组装形成晶体结构预测方面有更好的表现。

Abstract: Crystalline structure prediction remains an open challenge in materials
design. Despite recent advances in computational materials science, accurately
predicting the three-dimensional crystal structures of organic materials--an
essential first step for designing materials with targeted properties--remains
elusive. In this work, we address the problem of molecular assembly, where a
set $\mathcal{S}$ of identical rigid molecules is packed to form a crystalline
structure. Existing state-of-the-art models typically rely on computationally
expensive, iterative flow-matching approaches. We propose a novel loss function
that correctly captures key geometric molecular properties while maintaining
permutation invariance over $\mathcal{S}$. We achieve this via a differentiable
linear assignment scheme based on the Sinkhorn algorithm. Remarkably, we show
that even a simple regression using our method {\em SinkFast} significantly
outperforms more complex flow-matching approaches on the COD-Cluster17
benchmark, a curated subset of the Crystallography Open Database (COD).

</details>


### [274] [Causal SHAP: Feature Attribution with Dependency Awareness through Causal Discovery](https://arxiv.org/abs/2509.00846)
*Woon Yee Ng,Li Rong Wang,Siyuan Liu,Xiuyi Fan*

Main category: cs.LG

TL;DR: 提出Causal SHAP框架解决SHAP在特征归因时无法区分因果和相关性的问题，实验验证有效，为可解释AI领域做贡献。


<details>
  <summary>Details</summary>
Motivation: SHAP在高相关特征下无法区分因果和相关性，常误判特征重要性，在高风险领域需更好的特征归因方法。

Method: 结合PC算法进行因果发现和IDA算法进行因果强度量化，提出Causal SHAP框架。

Result: 通过合成和真实数据集实验验证，Causal SHAP降低了仅与目标相关特征的归因分数。

Conclusion: Causal SHAP为可解释AI领域提供了有因果意识的模型解释实用框架，在医疗等领域有重要价值。

Abstract: Explaining machine learning (ML) predictions has become crucial as ML models
are increasingly deployed in high-stakes domains such as healthcare. While
SHapley Additive exPlanations (SHAP) is widely used for model interpretability,
it fails to differentiate between causality and correlation, often
misattributing feature importance when features are highly correlated. We
propose Causal SHAP, a novel framework that integrates causal relationships
into feature attribution while preserving many desirable properties of SHAP. By
combining the Peter-Clark (PC) algorithm for causal discovery and the
Intervention Calculus when the DAG is Absent (IDA) algorithm for causal
strength quantification, our approach addresses the weakness of SHAP.
Specifically, Causal SHAP reduces attribution scores for features that are
merely correlated with the target, as validated through experiments on both
synthetic and real-world datasets. This study contributes to the field of
Explainable AI (XAI) by providing a practical framework for causal-aware model
explanations. Our approach is particularly valuable in domains such as
healthcare, where understanding true causal relationships is critical for
informed decision-making.

</details>


### [275] [Predicting Multi-Type Talented Students in Secondary School Using Semi-Supervised Machine Learning](https://arxiv.org/abs/2509.00863)
*Xinzhe Zheng,Zhen-Qun Yang,Jiannong Cao,Jiabei Cheng*

Main category: cs.LG

TL;DR: 研究引入TalentPredictor模型预测中学生七种天赋类型，克服传统方法局限，准确率高，展现机器学习早期识别天赋潜力。


<details>
  <summary>Details</summary>
Motivation: 传统人才识别方法依赖手动、侧重学术成绩、干预晚，忽视非学术天赋，需改进。

Method: 引入结合Transformer、LSTM和ANN架构的半监督多模态神经网络TalentPredictor，利用本地1041名中学生离线教育数据，聚类奖项记录、提取学习行为特征。

Result: 模型达到高预测准确率（分类准确率0.908，ROCAUC 0.908）。

Conclusion: 机器学习在学生发展早期识别多样天赋具有潜力。

Abstract: Talent identification plays a critical role in promoting student development.
However, traditional approaches often rely on manual processes or focus
narrowly on academic achievement, and typically delaying intervention until the
higher education stage. This oversight overlooks diverse non-academic talents
and misses opportunities for early intervention. To address this gap, this
study introduces TalentPredictor, a novel semi-supervised multi-modal neural
network that combines Transformer, LSTM, and ANN architectures. This model is
designed to predict seven different talent types--academic, sport, art,
leadership, service, technology, and others--in secondary school students
within an offline educational setting. Drawing on existing offline educational
data from 1,041 local secondary students, TalentPredictor overcomes the
limitations of traditional talent identification methods. By clustering various
award records into talent categories and extracting features from students'
diverse learning behaviors, it achieves high prediction accuracy (0.908
classification accuracy, 0.908 ROCAUC). This demonstrates the potential of
machine learning to identify diverse talents early in student development.

</details>


### [276] [Tabular Diffusion Counterfactual Explanations](https://arxiv.org/abs/2509.00876)
*Wei Zhang,Brian Barr,John Paisley*

Main category: cs.LG

TL;DR: 本文针对金融和社科领域的表格数据，提出基于Gumbel - softmax分布近似的分类特征引导反向过程，实验表明该方法优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的反事实解释方法主要用于计算机视觉问题，本文聚焦金融和社科的表格数据。

Method: 提出基于Gumbel - softmax分布近似的分类特征引导反向过程，研究温度τ的影响并推导理论界限。

Result: 在多个大规模信贷等表格数据集上实验，该方法在可解释性、多样性、不稳定性和有效性等定量指标上优于流行基线方法。

Conclusion: 所提方法能产生稳健且现实的反事实解释。

Abstract: Counterfactual explanations methods provide an important tool in the field of
{interpretable machine learning}. Recent advances in this direction have
focused on diffusion models to explain a deep classifier. However, these
techniques have predominantly focused on problems in computer vision. In this
paper, we focus on tabular data typical in finance and the social sciences and
propose a novel guided reverse process for categorical features based on an
approximation to the Gumbel-softmax distribution. Furthermore, we study the
effect of the temperature $\tau$ and derive a theoretical bound between the
Gumbel-softmax distribution and our proposed approximated distribution. We
perform experiments on several large-scale credit lending and other tabular
datasets, assessing their performance in terms of the quantitative measures of
interpretability, diversity, instability, and validity. These results indicate
that our approach outperforms popular baseline methods, producing robust and
realistic counterfactual explanations.

</details>


### [277] [An Explainable Gaussian Process Auto-encoder for Tabular Data](https://arxiv.org/abs/2509.00884)
*Wei Zhang,Brian Barr,John Paisley*

Main category: cs.LG

TL;DR: 本文提出用高斯过程构建自动编码器架构生成反事实样本的新方法，实验表明该方法能生成多样化且分布内的反事实样本。


<details>
  <summary>Details</summary>
Motivation: 可解释机器学习受关注，反事实解释方法是解释黑盒模型的重要工具，期望提出更好的生成反事实样本方法。

Method: 用高斯过程构建自动编码器架构生成反事实样本，引入新的密度估计器搜索分布内样本，还提出选择密度估计器最优正则化率的算法。

Result: 在多个大规模表格数据集上实验，与其他基于自动编码器的方法对比，该方法能生成多样化且分布内的反事实样本。

Conclusion: 所提方法能有效生成多样化且分布内的反事实样本，所需可学习参数少，不易过拟合。

Abstract: Explainable machine learning has attracted much interest in the community
where the stakes are high. Counterfactual explanations methods have become an
important tool in explaining a black-box model. The recent advances have
leveraged the power of generative models such as an autoencoder. In this paper,
we propose a novel method using a Gaussian process to construct the
auto-encoder architecture for generating counterfactual samples. The resulting
model requires fewer learnable parameters and thus is less prone to
overfitting. We also introduce a novel density estimator that allows for
searching for in-distribution samples. Furthermore, we introduce an algorithm
for selecting the optimal regularization rate on density estimator while
searching for counterfactuals. We experiment with our method in several
large-scale tabular datasets and compare with other auto-encoder-based methods.
The results show that our method is capable of generating diversified and
in-distribution counterfactual samples.

</details>


### [278] [DTRNet: Dynamic Token Routing Network to Reduce Quadratic Costs in Transformers](https://arxiv.org/abs/2509.00925)
*Aman Sharma,Saeed Najafi,Parsa Farinneya,Benyamin Jamialahmadi,Marzieh S. Tahaei,Yuhe Fan,Mehdi Rezagholizadeh,Boxing Chen,Aref Jafari*

Main category: cs.LG

TL;DR: 提出DTRNet改进Transformer架构，可让令牌动态跳过交叉令牌混合的二次成本，降低计算量，表现良好。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer在每一层对每个令牌应用二次自注意力，计算成本高。

Method: 引入DTRNet，保留MLP模块，将大多数令牌的注意力成本降低为线性。

Result: 训练后每层仅约10%的令牌通过注意力，性能与全Transformer相当，在精度和内存上优于MoD和D - LLM。

Conclusion: DTRNet通过解耦令牌更新和注意力混合，大幅减少二次计算，是Transformer的简单、高效、可扩展替代方案。

Abstract: Transformers achieve state-of-the-art results across many tasks, but their
uniform application of quadratic self-attention to every token at every layer
makes them computationally expensive. We introduce DTRNet (Dynamic Token
Routing Network), an improved Transformer architecture that allows tokens to
dynamically skip the quadratic cost of cross-token mixing while still receiving
lightweight linear updates. By preserving the MLP module and reducing the
attention cost for most tokens to linear, DTRNet ensures that every token is
explicitly updated while significantly lowering overall computation. This
design offers an efficient and effective alternative to standard dense
attention. Once trained, DTRNet blocks routes only ~10% of tokens through
attention at each layer while maintaining performance comparable to a full
Transformer. It consistently outperforms routing-based layer skipping methods
such as MoD and D-LLM in both accuracy and memory at matched FLOPs, while
routing fewer tokens to full attention. Its efficiency gains, scales with
sequence length, offering significant reduction in FLOPs for long-context
inputs. By decoupling token updates from attention mixing, DTRNet substantially
reduces the quadratic share of computation, providing a simple, efficient, and
scalable alternative to Transformers.

</details>


### [279] [Superposition in Graph Neural Networks](https://arxiv.org/abs/2509.00928)
*Lukas Pertl,Han Xuanyuan,Pietro Liò*

Main category: cs.LG

TL;DR: 研究GNN潜在空间中的特征叠加现象，分析特征几何，将表征几何与设计选择联系起来，为可解释GNN提供实用方法。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNN）因消息传递混合信号和内部通道难与人类概念对齐而难以解释，故研究GNN潜在空间中的特征叠加。

Method: 使用明确图概念进行受控实验，在图层面提取类条件质心特征，在节点层面提取线性探测方向特征，并通过简单的基不变诊断方法分析其几何结构。

Result: 发现增加宽度会产生重叠的相位模式；拓扑将重叠印记到节点级特征，池化部分重新混合成任务对齐的图轴；更尖锐的池化增加轴对齐并减少通道共享；浅模型可陷入亚稳态低秩嵌入。

Conclusion: 将表征几何与具体设计选择（宽度、池化和最终层激活）相联系，为构建更可解释的GNN提供实用方法。

Abstract: Interpreting graph neural networks (GNNs) is difficult because message
passing mixes signals and internal channels rarely align with human concepts.
We study superposition, the sharing of directions by multiple features,
directly in the latent space of GNNs. Using controlled experiments with
unambiguous graph concepts, we extract features as (i) class-conditional
centroids at the graph level and (ii) linear-probe directions at the node
level, and then analyze their geometry with simple basis-invariant diagnostics.
Across GCN/GIN/GAT we find: increasing width produces a phase pattern in
overlap; topology imprints overlap onto node-level features that pooling
partially remixes into task-aligned graph axes; sharper pooling increases axis
alignment and reduces channel sharing; and shallow models can settle into
metastable low-rank embeddings. These results connect representational geometry
with concrete design choices (width, pooling, and final-layer activations) and
suggest practical approaches for more interpretable GNNs.

</details>


### [280] [SCOUT: Toward Sub-Quadratic Attention via Segment Compression for Optimized Utility in Transformers](https://arxiv.org/abs/2509.00935)
*Aref Jafari,Yuhe Fan,Benyamin Jamialahmadi,Parsa Farinneya,Boxing Chen,Marzieh S. Tahaei*

Main category: cs.LG

TL;DR: 提出混合架构SCOUT，压缩固定大小段内的令牌，对压缩表示应用注意力，降低计算和内存成本，在长序列任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: Transformers注意力复杂度为二次方，限制长序列可扩展性；线性模型在长序列上因无法保留远距离令牌信息而性能下降。

Method: 提出SCOUT混合架构，先通过线性局部混合器丰富令牌嵌入，再让每个令牌稀疏地关注少量压缩检查点令牌。

Result: SCOUT在相同计算预算下优于强长序列基线，在语言建模和常识推理任务上与全注意力Transformers相当，端到端吞吐量高于SOTA模型。

Conclusion: SCOUT在保留全注意力表达能力的同时，大幅降低计算和内存成本，具有良好的可扩展性和性能。

Abstract: Transformers have demonstrated strong performance across a wide range of
sequence modeling tasks, but their quadratic attention complexity limits
scalability to long sequences. Linear models such as Mamba and sliding-window
attention (SWA) address this by mixing tokens through recurrent or localized
operations with fixed-size memory, achieving efficient inference. However,
these methods risk degrading performance on long sequences due to their
inability to retain detailed information from distant tokens. We propose SCOUT
(Segment Compression for Optimized Utility in Transformers), a hybrid
architecture that compresses tokens locally within fixed-size segments and
applies attention only over these compressed representations. Each token
embedding is first enriched via a linear local mixer, Mamba or SWA, that
integrates recent context. Then, instead of attending to all previous tokens,
each token sparsely attends to a small number of compressed checkpoint tokens
that summarize the input history. This design retains much of the expressivity
of full attention while substantially reducing the computational and memory
cost. By attending to compressed history rather than all previous tokens, SCOUT
incurs slightly higher memory than purely linear models, but its growth rate
remains sub-quadratic and far more scalable than that of full Transformers. We
analyze SCOUT's computational and memory efficiency and evaluate it empirically
on long-context language modeling and reasoning tasks. SCOUT with both Mamba
and SWA mixers outperforms strong long-sequence baselines under the same
computational budget, matches full-attention Transformers on language modeling
and common-sense reasoning tasks at 400M and 1.3B scales. Moreover, our SCOUT
achieves higher end-to-end throughput than SOTA models, while delivering
comparable results on long sequence benchmarks.

</details>


### [281] [Online Decentralized Federated Multi-task Learning With Trustworthiness in Cyber-Physical Systems](https://arxiv.org/abs/2509.00992)
*Olusola Odeyomi,Sofiat Olaosebikan,Ajibuwa Opeyemi,Oluwadoyinsola Ige*

Main category: cs.LG

TL;DR: 本文提出在线去中心化联邦多任务学习算法，利用网络物理特性在拜占庭客户端占优时实现模型个性化与弹性，模拟效果接近无拜占庭情况。


<details>
  <summary>Details</summary>
Motivation: 现有多任务学习在在线去中心化联邦学习中未充分探索，且实际应用中拜占庭客户端数量难限制，需解决其占优时的模型个性化与弹性问题。

Method: 提出在线去中心化联邦多任务学习算法，利用网络物理特性（如无线系统接收信号强度或侧信息）为每次迭代中从邻居处收到的本地模型分配信任概率。

Result: 模拟结果显示该算法表现接近无拜占庭的设置。

Conclusion: 所提算法能在拜占庭客户端占优时提供模型个性化与弹性。

Abstract: Multi-task learning is an effective way to address the challenge of model
personalization caused by high data heterogeneity in federated learning.
However, extending multi-task learning to the online decentralized federated
learning setting is yet to be explored. The online decentralized federated
learning setting considers many real-world applications of federated learning,
such as autonomous systems, where clients communicate peer-to-peer and the data
distribution of each client is time-varying. A more serious problem in
real-world applications of federated learning is the presence of Byzantine
clients. Byzantine-resilient approaches used in federated learning work only
when the number of Byzantine clients is less than one-half the total number of
clients. Yet, it is difficult to put a limit on the number of Byzantine clients
within a system in reality. However, recent work in robotics shows that it is
possible to exploit cyber-physical properties of a system to predict clients'
behavior and assign a trust probability to received signals. This can help to
achieve resiliency in the presence of a dominating number of Byzantine clients.
Therefore, in this paper, we develop an online decentralized federated
multi-task learning algorithm to provide model personalization and resiliency
when the number of Byzantine clients dominates the number of honest clients.
Our proposed algorithm leverages cyber-physical properties, such as the
received signal strength in wireless systems or side information, to assign a
trust probability to local models received from neighbors in each iteration.
Our simulation results show that the proposed algorithm performs close to a
Byzantine-free setting.

</details>


### [282] [MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper](https://arxiv.org/abs/2509.00996)
*Runjia Zeng,Guangyan Sun,Qifan Wang,Tong Geng,Sohail Dianat,Xiaotian Han,Raghuveer Rao,Xueling Zhang,Cheng Han,Lifu Huang,Dongfang Liu*

Main category: cs.LG

TL;DR: 提出混合专家提示调优（MEPT）方法，在SuperGLUE上优于多个基线，提升准确率并减少激活提示，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法参数空间僵化，难以灵活适应多样和不断变化的数据分布。

Method: 提出MEPT方法，利用混合专家架构，集成多个提示专家自适应学习多样和非平稳的数据分布。

Result: 在SuperGLUE上优于多个最先进的参数高效基线，平均准确率提升1.94%，显著减少79.25%的激活提示。

Conclusion: MEPT方法有效，得到流形学习理论和神经激活路径可视化结果的支持。

Abstract: Considering deep neural networks as manifold mappers, the
pretrain-then-fine-tune paradigm can be interpreted as a two-stage process:
pretrain establishes a broad knowledge base, and fine-tune adjusts the model
parameters to activate specific neural pathways to align with the target
manifold. Although prior fine-tuning approaches demonstrate success, their
rigid parameter space limits their ability to dynamically activate appropriate
neural pathways, rendering them ill-equipped to adapt flexibly to the diverse
and evolving data distributions. In light of this view, we propose a novel
approach, Mixture of Expert Prompt Tuning (MEPT), as an effective and efficient
manifold-mapping framework. MEPT leverages the Mixture of Experts architecture
by integrating multiple prompt experts to adaptively learn diverse and
non-stationary data distributions. Empirical evaluations demonstrate that MEPT
outperforms several state-of-the-art parameter efficient baselines on
SuperGLUE, achieving notable improvements in mean accuracy (e.g., 1.94%) while
significantly reducing activated prompts by 79.25%. The effectiveness of MEPT
is further supported by theoretical insights from manifold learning and
validated through neural activation pathway visualization results. Our code is
avaliable at https://github.com/runtsang/MEPT.

</details>


### [283] [Any-Order Flexible Length Masked Diffusion](https://arxiv.org/abs/2509.01025)
*Jaeyeon Kim,Lee Cheuk-Kit,Carles Domingo-Enrich,Yilun Du,Sham Kakade,Timothy Ngotiaoco,Sitan Chen,Michael Albergo*

Main category: cs.LG

TL;DR: 提出FlexMDMs，可灵活建模序列长度，保留MDMs任意顺序推理灵活性，实验表现佳且可由预训练MDMs改造。


<details>
  <summary>Details</summary>
Motivation: MDMs不支持token插入，只能固定长度生成，有局限性。

Method: 基于随机插值框架扩展，通过插入和取消掩码token生成序列。

Result: FlexMDMs困惑度与MDMs相当，长度统计建模更精确，迷宫规划任务成功率高，预训练MDMs可轻松改造为FlexMDMs，在数学和代码填充任务表现提升。

Conclusion: FlexMDMs解决了MDMs固定长度生成的问题，具有良好性能且改造方便。

Abstract: Masked diffusion models (MDMs) have recently emerged as a promising
alternative to autoregressive models over discrete domains. MDMs generate
sequences in an any-order, parallel fashion, enabling fast inference and strong
performance on non-causal tasks. However, a crucial limitation is that they do
not support token insertions and are thus limited to fixed-length generations.
To this end, we introduce Flexible Masked Diffusion Models (FlexMDMs), a
discrete diffusion paradigm that simultaneously can model sequences of flexible
length while provably retaining MDMs' flexibility of any-order inference.
Grounded in an extension of the stochastic interpolant framework, FlexMDMs
generate sequences by inserting mask tokens and unmasking them. Empirically, we
show that FlexMDMs match MDMs in perplexity while modeling length statistics
with much higher fidelity. On a synthetic maze planning task, they achieve
$\approx 60 \%$ higher success rate than MDM baselines. Finally, we show
pretrained MDMs can easily be retrofitted into FlexMDMs: on 16 H100s, it takes
only three days to fine-tune LLaDA-8B into a FlexMDM, achieving superior
performance on math (GSM8K, $58\% \to 67\%$) and code infilling performance
($52\% \to 65\%$).

</details>


### [284] [Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition](https://arxiv.org/abs/2509.01031)
*Xiaozhou Ye,Kevin I-Kai Wang*

Main category: cs.LG

TL;DR: 提出TPRL - DG框架用于人体活动识别，解决跨用户变异性问题，在数据集上表现优于现有方法，助力相关领域发展。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法在人体活动识别中因跨用户变异性导致泛化能力差，现有领域泛化方法存在不足。

Method: 提出TPRL - DG框架，将特征提取定义为强化学习驱动的顺序决策过程，利用Transformer生成器产生时间标记，通过多目标奖励函数优化。

Result: 在DSADS和PAMAP2数据集上，TPRL - DG在跨用户泛化方面超越现有方法，无需用户校准获得更高准确率。

Conclusion: TPRL - DG能学习稳健、用户不变的时间模式，可实现可扩展的人体活动识别系统，推动相关领域进步。

Abstract: Human Activity Recognition (HAR) using wearable sensors is crucial for
healthcare, fitness tracking, and smart environments, yet cross-user
variability -- stemming from diverse motion patterns, sensor placements, and
physiological traits -- hampers generalization in real-world settings.
Conventional supervised learning methods often overfit to user-specific
patterns, leading to poor performance on unseen users. Existing domain
generalization approaches, while promising, frequently overlook temporal
dependencies or depend on impractical domain-specific labels. We propose
Temporal-Preserving Reinforcement Learning Domain Generalization (TPRL-DG), a
novel framework that redefines feature extraction as a sequential
decision-making process driven by reinforcement learning. TPRL-DG leverages a
Transformer-based autoregressive generator to produce temporal tokens that
capture user-invariant activity dynamics, optimized via a multi-objective
reward function balancing class discrimination and cross-user invariance. Key
innovations include: (1) an RL-driven approach for domain generalization, (2)
autoregressive tokenization to preserve temporal coherence, and (3) a
label-free reward design eliminating the need for target user annotations.
Evaluations on the DSADS and PAMAP2 datasets show that TPRL-DG surpasses
state-of-the-art methods in cross-user generalization, achieving superior
accuracy without per-user calibration. By learning robust, user-invariant
temporal patterns, TPRL-DG enables scalable HAR systems, facilitating
advancements in personalized healthcare, adaptive fitness tracking, and
context-aware environments.

</details>


### [285] [IMU-Enhanced EEG Motion Artifact Removal with Fine-Tuned Large Brain Models](https://arxiv.org/abs/2509.01073)
*Yuhong Zhang,Xusheng Zhu,Yuchen Xu,ChiaEn Lu,Hsinyu Shih,Gert Cauwenberghs,Tzyy-Ping Jung*

Main category: cs.LG

TL;DR: 本文提出基于微调大脑模型的相关注意力映射方法去除EEG运动伪影，结合IMU信号提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: EEG信号信噪比低，运动相关伪影阻碍脑机接口实际应用，以往研究多为单模态方法，未结合IMU信号。

Method: 提出基于微调大脑模型（LaBraM）的相关注意力映射方法，利用IMU数据空间通道关系识别EEG信号中运动相关伪影，用5.9小时EEG和IMU记录训练约920万参数的微调模型。

Result: 与ASR - ICA基准对比，结合IMU参考信号在不同时间尺度和运动活动下显著提高了鲁棒性。

Conclusion: 结合IMU信号的方法能有效去除EEG运动伪影，提升系统在不同运动场景下的鲁棒性。

Abstract: Electroencephalography (EEG) is a non-invasive method for measuring brain
activity with high temporal resolution; however, EEG signals often exhibit low
signal-to-noise ratios because of contamination from physiological and
environmental artifacts. One of the major challenges hindering the real-world
deployment of brain-computer interfaces (BCIs) involves the frequent occurrence
of motion-related EEG artifacts. Most prior studies on EEG motion artifact
removal rely on single-modality approaches, such as Artifact Subspace
Reconstruction (ASR) and Independent Component Analysis (ICA), without
incorporating simultaneously recorded modalities like inertial measurement
units (IMUs), which directly capture the extent and dynamics of motion. This
work proposes a fine-tuned large brain model (LaBraM)-based correlation
attention mapping method that leverages spatial channel relationships in IMU
data to identify motion-related artifacts in EEG signals. The fine-tuned model
contains approximately 9.2 million parameters and uses 5.9 hours of EEG and IMU
recordings for training, just 0.2346\% of the 2500 hours used to train the base
model. We compare our results against the established ASR-ICA benchmark across
varying time scales and motion activities, showing that incorporating IMU
reference signals significantly improves robustness under diverse motion
scenarios.

</details>


### [286] [REFINESTAT: Efficient Exploration for Probabilistic Program Synthesis](https://arxiv.org/abs/2509.01082)
*Madhav Kanda,Shubham Ugare,Sasa Misailovic*

Main category: cs.LG

TL;DR: 提出RefineStat框架解决小语言模型生成概率程序的问题，评估显示其生成程序质量高。


<details>
  <summary>Details</summary>
Motivation: 概率编程中统计模型发现搜索空间大，小语言模型生成概率程序存在语法和语义错误。

Method: 引入RefineStat框架，先强制语义约束，再在可靠性检查失败时进行诊断感知的细化。

Result: 在多个概率编程代码生成任务中，使用小语言模型评估发现RefineStat生成的程序语法正确且统计可靠，常匹配或超越闭源大语言模型。

Conclusion: RefineStat框架能有效解决小语言模型生成概率程序的问题。

Abstract: Probabilistic programming offers a powerful framework for modeling
uncertainty, yet statistical model discovery in this domain entails navigating
an immense search space under strict domain-specific constraints. When small
language models are tasked with generating probabilistic programs, they
frequently produce outputs that suffer from both syntactic and semantic errors,
such as flawed inference constructs. Motivated by probabilistic programmers'
domain expertise and debugging strategies, we introduce RefineStat, a language
model--driven framework that enforces semantic constraints ensuring synthesized
programs contain valid distributions and well-formed parameters, and then
applies diagnostic-aware refinement by resampling prior or likelihood
components whenever reliability checks fail. We evaluate RefineStat on multiple
probabilistic-programming code-generation tasks using smaller language models
(SLMs) and find that it produces programs that are both syntactically sound and
statistically reliable, often matching or surpassing those from closed-source
large language models (e.g., OpenAI o3).

</details>


### [287] [A Class of Random-Kernel Network Models](https://arxiv.org/abs/2509.01090)
*James Tian*

Main category: cs.LG

TL;DR: 提出随机核网络，证明其深层结构在样本复杂度上有深度分离定理。


<details>
  <summary>Details</summary>
Motivation: 探索随机特征模型的多层扩展及深度结构在样本复杂度上的优势。

Method: 引入随机核网络，通过确定性核组合创建深度，仅最外层引入随机性。

Result: 证明深层结构比浅层结构能用更少蒙特卡罗样本近似某些函数。

Conclusion: 建立了样本复杂度上的深度分离定理。

Abstract: We introduce random-kernel networks, a multilayer extension of random feature
models where depth is created by deterministic kernel composition and
randomness enters only in the outermost layer. We prove that deeper
constructions can approximate certain functions with fewer Monte Carlo samples
than any shallow counterpart, establishing a depth separation theorem in sample
complexity.

</details>


### [288] [SC-GIR: Goal-oriented Semantic Communication via Invariant Representation Learning](https://arxiv.org/abs/2509.01119)
*Senura Hansaja Wanasekara,Van-Dinh Nguyen,Kok-Seng,M. -Duong Nguyen,Symeon Chatzinotas,Octavia A. Dobre*

Main category: cs.LG

TL;DR: 提出面向目标的基于不变表示的语义通信框架SC - GIR用于图像传输，实验表明其性能优于基线方案。


<details>
  <summary>Details</summary>
Motivation: 当前面向目标的语义通信方法存在收发器联合训练、冗余数据交换和依赖标记数据集等问题，限制了任务无关性。

Method: 利用自监督学习提取源数据的不变表示，使用基于协方差的对比学习技术获得有意义且语义密集的潜在表示，并应用于图像数据集的有损压缩。

Result: SC - GIR比基线方案性能高近10%，在不同信噪比条件下压缩数据的分类准确率超85%。

Conclusion: 所提框架能有效学习紧凑且信息丰富的潜在表示。

Abstract: Goal-oriented semantic communication (SC) aims to revolutionize communication
systems by transmitting only task-essential information. However, current
approaches face challenges such as joint training at transceivers, leading to
redundant data exchange and reliance on labeled datasets, which limits their
task-agnostic utility. To address these challenges, we propose a novel
framework called Goal-oriented Invariant Representation-based SC (SC-GIR) for
image transmission. Our framework leverages self-supervised learning to extract
an invariant representation that encapsulates crucial information from the
source data, independent of the specific downstream task. This compressed
representation facilitates efficient communication while retaining key features
for successful downstream task execution. Focusing on machine-to-machine tasks,
we utilize covariance-based contrastive learning techniques to obtain a latent
representation that is both meaningful and semantically dense. To evaluate the
effectiveness of the proposed scheme on downstream tasks, we apply it to
various image datasets for lossy compression. The compressed representations
are then used in a goal-oriented AI task. Extensive experiments on several
datasets demonstrate that SC-GIR outperforms baseline schemes by nearly 10%,,
and achieves over 85% classification accuracy for compressed data under
different SNR conditions. These results underscore the effectiveness of the
proposed framework in learning compact and informative latent representations.

</details>


### [289] [MATL-DC: A Multi-domain Aggregation Transfer Learning Framework for EEG Emotion Recognition with Domain-Class Prototype under Unseen Targets](https://arxiv.org/abs/2509.01135)
*Guangli Li,Canbiao Wu,Zhehao Zhou,Na Tian,Zhen Liang*

Main category: cs.LG

TL;DR: 提出MATL - DC框架用于EEG情感识别，经实验验证性能良好。


<details>
  <summary>Details</summary>
Motivation: 当前迁移学习模型过度依赖源域和目标域数据，阻碍情感识别实际应用。

Method: 设计特征解耦模块，采用多域聚合机制形成超域，提取类原型表示，采用成对学习策略，训练时目标域完全不可见，推理阶段用训练好的域 - 类原型进行推理。

Result: 在公开数据库（SEED、SEED - IV和SEED - V）上验证，MATL - DC模型准确率分别为84.70%、68.11%和61.08%，性能与依赖源域和目标域的方法相当甚至更好。

Conclusion: MATL - DC框架可有效解决当前迁移学习模型在EEG情感识别中的问题，具有良好性能。

Abstract: Emotion recognition based on electroencephalography (EEG) signals is
increasingly becoming a key research hotspot in affective Brain-Computer
Interfaces (aBCIs). However, the current transfer learning model greatly
depends on the source domain and target domain data, which hinder the practical
application of emotion recognition. Therefore, we propose a Multi-domain
Aggregation Transfer Learning framework for EEG emotion recognition with
Domain-Class prototype under unseen targets (MATL-DC). We design the feature
decoupling module to decouple class-invariant domain features from
domain-invariant class features from shallow features. In the model training
stage, the multi-domain aggregation mechanism aggregates the domain feature
space to form a superdomain, which enhances the characteristics of emotional
EEG signals. In each superdomain, we further extract the class prototype
representation by class features. In addition, we adopt the pairwise learning
strategy to transform the sample classification problem into the similarity
problem between sample pairs, which effectively alleviates the influence of
label noise. It is worth noting that the target domain is completely unseen
during the training process. In the inference stage, we use the trained
domain-class prototypes for inference, and then realize emotion recognition. We
rigorously validate it on the publicly available databases (SEED, SEED-IV and
SEED-V). The results show that the accuracy of MATL-DC model is 84.70\%,
68.11\% and 61.08\%, respectively. MATL-DC achieves comparable or even better
performance than methods that rely on both source and target domains. The
source code is available at https://github.com/WuCB-BCI/MATL-DC.

</details>


### [290] [Multi-Modal Machine Learning Framework for Predicting Early Recurrence of Brain Tumors Using MRI and Clinical Biomarkers](https://arxiv.org/abs/2509.01161)
*Cheng Cheng,Zeping Chen,Rui Xie,Peiyao Zheng,Xavier Wang*

Main category: cs.LG

TL;DR: 提出多模态机器学习框架整合MRI特征与临床生物标志物预测脑肿瘤术后复发，模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 准确预测脑肿瘤手术切除后早期复发是临床挑战，旨在改进术后复发预测。

Method: 提出多模态机器学习框架，采用GBM、RSF、CoxBoost和XGBoost四种算法，用C - index、时间依赖AUC、校准曲线和决策曲线分析验证模型性能。

Result: 模型展现出有前景的性能。

Conclusion: 该模型可作为风险分层和个性化随访计划的潜在工具。

Abstract: Accurately predicting early recurrence in brain tumor patients following
surgical resection remains a clinical challenge. This study proposes a
multi-modal machine learning framework that integrates structural MRI features
with clinical biomarkers to improve postoperative recurrence prediction. We
employ four machine learning algorithms -- Gradient Boosting Machine (GBM),
Random Survival Forest (RSF), CoxBoost, and XGBoost -- and validate model
performance using concordance index (C-index), time-dependent AUC, calibration
curves, and decision curve analysis. Our model demonstrates promising
performance, offering a potential tool for risk stratification and personalized
follow-up planning.

</details>


### [291] [A Multimodal Deep Learning Framework for Early Diagnosis of Liver Cancer via Optimized BiLSTM-AM-VMD Architecture](https://arxiv.org/abs/2509.01164)
*Cheng Cheng,Zeping Chen,Xavier Wang*

Main category: cs.LG

TL;DR: 提出BiLSTM - AM - VMD多模态深度学习框架用于早期肝癌诊断，在真实数据集上表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 解决早期肝癌诊断问题，提高诊断的预测准确性和可解释性。

Method: 提出结合双向LSTM、多头注意力机制和变分模态分解的多模态深度学习框架BiLSTM - AM - VMD，使用包含临床特征、生化标志物和影像衍生变量的异构数据。

Result: 在真实数据集上的实验结果显示，该方法优于传统机器学习和基线深度学习模型。

Conclusion: 所提出的BiLSTM - AM - VMD框架有效可行，能用于早期肝癌诊断。

Abstract: This paper proposes a novel multimodal deep learning framework integrating
bidirectional LSTM, multi-head attention mechanism, and variational mode
decomposition (BiLSTM-AM-VMD) for early liver cancer diagnosis. Using
heterogeneous data that include clinical characteristics, biochemical markers,
and imaging-derived variables, our approach improves both prediction accuracy
and interpretability. Experimental results on real-world datasets demonstrate
superior performance over traditional machine learning and baseline deep
learning models.

</details>


### [292] [StoxLSTM: A Stochastic Extended Long Short-Term Memory Network for Time Series Forecasting](https://arxiv.org/abs/2509.01187)
*Zihao Wang,Yunjie Li,Lingmin Zan,Zheng Gong,Mengtao Zhu*

Main category: cs.LG

TL;DR: 本文提出StoxLSTM，将随机潜在变量融入xLSTM，实验表明其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 虽xLSTM在时间序列应用中成功，但在具有复杂动态的真实数据集上，其表征能力和预测性能仍有提升空间。

Method: 提出StoxLSTM，将随机潜在变量融入xLSTM架构到状态空间建模框架，通过特殊设计的循环块对潜在动态演化建模。

Result: 在多个公开基准数据集上的实验表明，StoxLSTM始终优于现有基线模型，具有更好的鲁棒性和更强的泛化能力。

Conclusion: StoxLSTM能有效捕捉潜在时间模式和依赖关系，是一种更优的时间序列建模方法。

Abstract: The Extended Long Short-Term Memory (xLSTM) network has attracted widespread
research interest due to its enhanced capability to model complex temporal
dependencies in diverse time series applications. Despite its success, there is
still potential to further improve its representational capacity and
forecasting performance, particularly on challenging real-world datasets with
unknown, intricate, and hierarchical dynamics. In this work, we propose a
stochastic xLSTM, termed StoxLSTM, that improves the original architecture into
a state space modeling framework by incorporating stochastic latent variables
within xLSTM. StoxLSTM models the latent dynamic evolution through specially
designed recurrent blocks, enabling it to effectively capture the underlying
temporal patterns and dependencies. Extensive experiments on publicly available
benchmark datasets from multiple research communities demonstrate that StoxLSTM
consistently outperforms state-of-the-art baselines with better robustness and
stronger generalization ability.

</details>


### [293] [Preserving Vector Space Properties in Dimensionality Reduction: A Relationship Preserving Loss Framework](https://arxiv.org/abs/2509.01198)
*Eddi Weinwurm,Alexander Kovalenko*

Main category: cs.LG

TL;DR: 提出关系保留损失（RPL）用于降维，保留向量空间关键属性，实验显示能降维且保持下游任务性能，还可用于多领域。


<details>
  <summary>Details</summary>
Motivation: 传统降维会扭曲向量空间属性，影响下游任务，需一种能保留这些属性的方法。

Method: 提出RPL损失函数，通过最小化高维数据与其低维嵌入的关系矩阵差异来保留属性，训练神经网络进行非线性投影，并有矩阵扰动理论的误差界支持。

Result: 初始实验表明RPL能减少嵌入维度，同时在下游任务中基本保持性能。

Conclusion: RPL可用于降维，也能广泛应用于跨领域对齐、知识蒸馏等多个领域。

Abstract: Dimensionality reduction can distort vector space properties such as
orthogonality and linear independence, which are critical for tasks including
cross-modal retrieval, clustering, and classification. We propose a
Relationship Preserving Loss (RPL), a loss function that preserves these
properties by minimizing discrepancies between relationship matrices (e.g.,
Gram or cosine) of high-dimensional data and their low-dimensional embeddings.
RPL trains neural networks for non-linear projections and is supported by error
bounds derived from matrix perturbation theory. Initial experiments suggest
that RPL reduces embedding dimensions while largely retaining performance on
downstream tasks, likely due to its preservation of key vector space
properties. While we describe here the use of RPL in dimensionality reduction,
this loss can also be applied more broadly, for example to cross-domain
alignment and transfer learning, knowledge distillation, fairness and
invariance, dehubbing, graph and manifold learning, and federated learning,
where distributed embeddings must remain geometrically consistent.

</details>


### [294] [Geometric origin of adversarial vulnerability in deep learning](https://arxiv.org/abs/2509.01235)
*Yixiong Ren,Wenkang Du,Jianhui Zhou,Haiping Huang*

Main category: cs.LG

TL;DR: 提出几何感知深度学习框架，提升训练准确性与对抗鲁棒性，揭示学习物理原理。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中平衡训练准确性和对抗鲁棒性的挑战。

Method: 引入几何感知深度学习框架，利用逐层局部训练塑造深度神经网络内部表征。

Result: 该框架促进特征空间中类内紧凑性和类间分离，实现流形平滑和对抗鲁棒性，能用能量模型解释性能。

Conclusion: 为生物和人工智能系统学习的物理原理提供见解，使深度网络在减少表征干扰的同时吸收新信息。

Abstract: How to balance training accuracy and adversarial robustness has become a
challenge since the birth of deep learning. Here, we introduce a geometry-aware
deep learning framework that leverages layer-wise local training to sculpt the
internal representations of deep neural networks. This framework promotes
intra-class compactness and inter-class separation in feature space, leading to
manifold smoothness and adversarial robustness against white or black box
attacks. The performance can be explained by an energy model with Hebbian
coupling between elements of the hidden representation. Our results thus shed
light on the physics of learning in the direction of alignment between
biological and artificial intelligence systems. Using the current framework,
the deep network can assimilate new information into existing knowledge
structures while reducing representation interference.

</details>


### [295] [What Expressivity Theory Misses: Message Passing Complexity for GNNs](https://arxiv.org/abs/2509.01254)
*Niklas Kemper,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 指出GNN表达性理论局限性，提出消息传递复杂度MPC框架，验证其理论预测与实证性能相关。


<details>
  <summary>Details</summary>
Motivation: 现有GNN表达性理论聚焦高表达性存在误区，不能反映GNN实际能力，需新衡量方法。

Method: 提出消息传递复杂度MPC，通过大量基础GNN任务验证。

Result: MPC理论预测与实证性能相关，能解释架构成败。

Conclusion: MPC超越表达性理论，为理解和改进GNN架构提供更强大细致框架。

Abstract: Expressivity theory, characterizing which graphs a GNN can distinguish, has
become the predominant framework for analyzing GNNs, with new models striving
for higher expressivity. However, we argue that this focus is misguided: First,
higher expressivity is not necessary for most real-world tasks as these tasks
rarely require expressivity beyond the basic WL test. Second, expressivity
theory's binary characterization and idealized assumptions fail to reflect
GNNs' practical capabilities. To overcome these limitations, we propose Message
Passing Complexity (MPC): a continuous measure that quantifies the difficulty
for a GNN architecture to solve a given task through message passing. MPC
captures practical limitations like over-squashing while preserving the
theoretical impossibility results from expressivity theory, effectively
narrowing the gap between theory and practice. Through extensive validation on
fundamental GNN tasks, we show that MPC's theoretical predictions correlate
with empirical performance, successfully explaining architectural successes and
failures. Thereby, MPC advances beyond expressivity theory to provide a more
powerful and nuanced framework for understanding and improving GNN
architectures.

</details>


### [296] [Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge Networks](https://arxiv.org/abs/2509.01257)
*Andrea Fox,Francesco De Pellegrini,Eitan Altman*

Main category: cs.LG

TL;DR: 提出分散式框架解决边缘计算系统中多智能体资源竞争问题，性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习方法在有限可观测性和通信约束下失效，需解决边缘计算系统中智能体资源竞争问题。

Method: 提出分散式框架，每个智能体解决约束马尔可夫决策过程，通过共享约束向量隐式协调，用安全强化学习学习策略。

Result: 在理论上有保证，实验验证性能优于集中式和独立基线，在大规模场景更明显。

Conclusion: 所提方法能在边缘计算系统中有效协调智能体，在资源竞争下实现更好性能。

Abstract: In edge computing systems, autonomous agents must make fast local decisions
while competing for shared resources. Existing MARL methods often resume to
centralized critics or frequent communication, which fail under limited
observability and communication constraints. We propose a decentralized
framework in which each agent solves a constrained Markov decision process
(CMDP), coordinating implicitly through a shared constraint vector. For the
specific case of offloading, e.g., constraints prevent overloading shared
server resources. Coordination constraints are updated infrequently and act as
a lightweight coordination mechanism. They enable agents to align with global
resource usage objectives but require little direct communication. Using safe
reinforcement learning, agents learn policies that meet both local and global
goals. We establish theoretical guarantees under mild assumptions and validate
our approach experimentally, showing improved performance over centralized and
independent baselines, especially in large-scale settings.

</details>


### [297] [Iterative In-Context Learning to Enhance LLMs Abstract Reasoning: The Case-Study of Algebraic Tasks](https://arxiv.org/abs/2509.01267)
*Stefano Fioravanti,Matteo Zavatteri,Roberto Confalonieri,Kamyar Zeinalipour,Paolo Frazzetto,Alessandro Sperduti,Nicolò Navarin*

Main category: cs.LG

TL;DR: 引入上下文学习方法提升大语言模型泛化能力，在代数表达式化简任务验证，发现大模型初始表现不佳，迭代示例选择策略有效，简单示例提示效果更好。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在系统泛化，尤其是处理组合规则推理任务和分布外示例时面临的挑战。

Method: 采用迭代示例选择策略，逐步构建针对特定任务优化的少样本示例集。

Result: 大语言模型在数学任务中初始表现有限；迭代示例选择提示策略结合显式推理指令对推理有帮助；部分大语言模型用简单示例提示泛化性能更好。

Conclusion: 提出的上下文学习方法能提升通用大语言模型的泛化能力，简单示例提示在某些情况下更有利于泛化。

Abstract: LLMs face significant challenges in systematic generalization, particularly
when dealing with reasoning tasks requiring compositional rules and handling
out-of-distribution examples. To address these challenges, we introduce an
in-context learning methodology that improves the generalization capabilities
of general purpose LLMs. Our approach employs an iterative example selection
strategy, which incrementally constructs a tailored set of few-shot examples
optimized to enhance model's performance on a given task. As a proof of
concept, we apply this methodology to the resolution of algebraic expressions
involving non-standard simplification rules, according to which the priority of
addition and multiplication is changed.
  Our findings indicate that LLMs exhibit limited proficiency in these
mathematical tasks. We further demonstrate that LLMs reasoning benefits from
our iterative shot selection prompting strategy integrated with explicit
reasoning instructions. Crucially, our experiments reveal that some LLMs
achieve better generalization performances when prompted with simpler few-shot
examples rather than complex ones following the test data distribution.

</details>


### [298] [Building surrogate models using trajectories of agents trained by Reinforcement Learning](https://arxiv.org/abs/2509.01285)
*Julen Cestero,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出用强化学习训练策略高效采样模拟确定性环境的方法，分析显示混合数据集表现最佳，该方法改进了现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有减少样本数量的策略在宽状态空间模拟环境中效果不佳，需解决样本效率问题。

Method: 提出用强化学习训练的策略对模拟确定性环境进行高效采样的方法，并与拉丁超立方采样、主动学习和克里金法对比分析。

Result: 包含随机代理、专家代理和经训练探索状态转移分布最大熵区域的代理所获取样本的混合数据集在所有数据集中得分最高。

Conclusion: 所提方法改进了现有技术，为在复杂模拟器上应用替代辅助强化学习策略优化策略铺平道路。

Abstract: Sample efficiency in the face of computationally expensive simulations is a
common concern in surrogate modeling. Current strategies to minimize the number
of samples needed are not as effective in simulated environments with wide
state spaces. As a response to this challenge, we propose a novel method to
efficiently sample simulated deterministic environments by using policies
trained by Reinforcement Learning. We provide an extensive analysis of these
surrogate-building strategies with respect to Latin-Hypercube sampling or
Active Learning and Kriging, cross-validating performances with all sampled
datasets. The analysis shows that a mixed dataset that includes samples
acquired by random agents, expert agents, and agents trained to explore the
regions of maximum entropy of the state transition distribution provides the
best scores through all datasets, which is crucial for a meaningful state space
representation. We conclude that the proposed method improves the
state-of-the-art and clears the path to enable the application of
surrogate-aided Reinforcement Learning policy optimization strategies on
complex simulators.

</details>


### [299] [Equivariant U-Shaped Neural Operators for the Cahn-Hilliard Phase-Field Model](https://arxiv.org/abs/2509.01293)
*Xiao Xue,M. F. P. ten Eikelder,Tianyue Yang,Yiqing Li,Kan He,Shuo Wang,Peter V. Coveney*

Main category: cs.LG

TL;DR: 提出等变U型神经算子E - UNO用于预测二元混合物相分离中相场变量演化，表现优于传统基线模型，是复杂相场系统的有效替代方法。


<details>
  <summary>Details</summary>
Motivation: 数值求解器计算成本高、缺乏灵活性，现有神经算子架构无法捕捉多尺度行为和忽略物理对称性。

Method: 结合全局谱卷积与多分辨率U型架构，调节平移等变性以符合底层物理规律。

Result: E - UNO在时空上实现准确预测，尤其在细尺度和高频结构上优于标准傅里叶神经算子和U型神经算子基线。

Conclusion: 通过编码对称性和尺度层次，E - UNO泛化性更好、所需训练数据更少、动力学符合物理规律，可作为复杂相场系统的高效替代。

Abstract: Phase separation in binary mixtures, governed by the Cahn-Hilliard equation,
plays a central role in interfacial dynamics across materials science and soft
matter. While numerical solvers are accurate, they are often computationally
expensive and lack flexibility across varying initial conditions and
geometries. Neural operators provide a data-driven alternative by learning
solution operators between function spaces, but current architectures often
fail to capture multiscale behavior and neglect underlying physical symmetries.
Here we show that an equivariant U-shaped neural operator (E-UNO) can learn the
evolution of the phase-field variable from short histories of past dynamics,
achieving accurate predictions across space and time. The model combines global
spectral convolution with a multi-resolution U-shaped architecture and
regulates translation equivariance to align with the underlying physics. E-UNO
outperforms standard Fourier neural operator and U-shaped neural operator
baselines, particularly on fine-scale and high-frequency structures. By
encoding symmetry and scale hierarchy, the model generalizes better, requires
less training data, and yields physically consistent dynamics. This establishes
E-UNO as an efficient surrogate for complex phase-field systems.

</details>


### [300] [Towards Trustworthy Vital Sign Forecasting: Leveraging Uncertainty for Prediction Intervals](https://arxiv.org/abs/2509.01319)
*Li Rong Wang,Thomas C. Henderson,Yew Soon Ong,Yih Yng Ng,Xiuyi Fan*

Main category: cs.LG

TL;DR: 提出两种从RUE推导预测区间（PIs）的方法用于生命体征预测，在高低频数据上实验，不同方法各有优势，显示出临床应用前景。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医疗中部署受限，缺乏可靠的不确定性量化会阻碍临床决策，需要为生命体征预测提供可信赖和可解释的模型输出。

Method: 提出两种从RUE推导PIs的方法，包括基于高斯copula分布的参数化方法和基于k近邻（KNN）的非参数化方法。

Result: 高斯copula方法在低频数据上表现优于共形预测基线，KNN方法在高频数据上表现最佳。

Conclusion: RUE推导的PIs在提供可解释、考虑不确定性的生命体征预测方面有临床应用前景。

Abstract: Vital signs, such as heart rate and blood pressure, are critical indicators
of patient health and are widely used in clinical monitoring and
decision-making. While deep learning models have shown promise in forecasting
these signals, their deployment in healthcare remains limited in part because
clinicians must be able to trust and interpret model outputs. Without reliable
uncertainty quantification -- particularly calibrated prediction intervals
(PIs) -- it is unclear whether a forecasted abnormality constitutes a
meaningful warning or merely reflects model noise, hindering clinical
decision-making. To address this, we present two methods for deriving PIs from
the Reconstruction Uncertainty Estimate (RUE), an uncertainty measure
well-suited to vital-sign forecasting due to its sensitivity to data shifts and
support for label-free calibration. Our parametric approach assumes that
prediction errors and uncertainty estimates follow a Gaussian copula
distribution, enabling closed-form PI computation. Our non-parametric approach,
based on k-nearest neighbours (KNN), empirically estimates the conditional
error distribution using similar validation instances. We evaluate these
methods on two large public datasets with minute- and hour-level sampling,
representing high- and low-frequency health signals. Experiments demonstrate
that the Gaussian copula method consistently outperforms conformal prediction
baselines on low-frequency data, while the KNN approach performs best on
high-frequency data. These results underscore the clinical promise of
RUE-derived PIs for delivering interpretable, uncertainty-aware vital sign
forecasts.

</details>


### [301] [Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2509.01321)
*Xinyu Tang,Zhenduo Zhang,Yurou Liu,Wayne Xin Zhao,Zujie Wen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: 提出DEPO优化管线，结合离线和在线数据选择策略，减少计算成本并提升模型性能，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型使用RLVR方法有高训练成本和低数据效率问题，需改进。

Method: 提出DEPO管线，离线基于多样性等选高质量样本，在线用探索性指标过滤样本并引入回放机制。

Result: 在五个推理基准测试中，DEPO在离线和在线数据选择场景均优于现有方法，用20%数据在AIME24和AIME25上有显著加速。

Conclusion: DEPO是有效的数据高效策略优化方法，能降低成本并提升模型性能。

Abstract: Recent advances in large reasoning models have leveraged reinforcement
learning with verifiable rewards (RLVR) to improve reasoning capabilities.
However, scaling these methods typically requires extensive rollout computation
and large datasets, leading to high training costs and low data efficiency. To
mitigate this issue, we propose DEPO, a Data-Efficient Policy Optimization
pipeline that combines optimized strategies for both offline and online data
selection. In the offline phase, we curate a high-quality subset of training
samples based on diversity, influence, and appropriate difficulty. During
online RLVR training, we introduce a sample-level explorability metric to
dynamically filter samples with low exploration potential, thereby reducing
substantial rollout computational costs. Furthermore, we incorporate a replay
mechanism for under-explored samples to ensure adequate training, which
enhances the model's final convergence performance. Experiments across five
reasoning benchmarks show that DEPO consistently outperforms existing methods
in both offline and online data selection scenarios. Notably, using only 20% of
the training data, our approach achieves a 1.85 times speed-up on AIME24 and a
1.66 times speed-up on AIME25 compared to GRPO trained on the full dataset.

</details>


### [302] [Multitask Battery Management with Flexible Pretraining](https://arxiv.org/abs/2509.01323)
*Hong Lu,Jiali Chen,Jingzhao Zhang,Guannan He,Xuebing Han,Minggao Ouyang*

Main category: cs.LG

TL;DR: 提出灵活掩码自编码器（FMAE）框架用于电池管理，能从异构数据学习统一表示，实验表现优于特定任务方法，为动态系统多任务管理提供实用途径。


<details>
  <summary>Details</summary>
Motivation: 工业规模电池管理各任务需大量数据和工程工作，限制智能电池管理可扩展性。

Method: 提出FMAE框架，可处理缺失数据通道，学习异构数据统一表示。

Result: 在五个电池管理任务、十一个数据集上始终优于特定任务方法；剩余寿命预测任务使用少50倍推理数据；处理现实数据缺失情况时性能影响小。

Conclusion: FMAE是灵活、数据高效的模型，简化动态系统多任务管理。

Abstract: Industrial-scale battery management involves various types of tasks, such as
estimation, prediction, and system-level diagnostics. Each task employs
distinct data across temporal scales, sensor resolutions, and data channels.
Building task-specific methods requires a great deal of data and engineering
effort, which limits the scalability of intelligent battery management. Here we
present the Flexible Masked Autoencoder (FMAE), a flexible pretraining
framework that can learn with missing battery data channels and capture
inter-correlations across data snippets. FMAE learns unified battery
representations from heterogeneous data and can be adopted by different tasks
with minimal data and engineering efforts. Experimentally, FMAE consistently
outperforms all task-specific methods across five battery management tasks with
eleven battery datasets. On remaining life prediction tasks, FMAE uses 50 times
less inference data while maintaining state-of-the-art results. Moreover, when
real-world data lack certain information, such as system voltage, FMAE can
still be applied with marginal performance impact, achieving comparable results
with the best hand-crafted features. FMAE demonstrates a practical route to a
flexible, data-efficient model that simplifies real-world multi-task management
of dynamical systems.

</details>


### [303] [Globally aware optimization with resurgence](https://arxiv.org/abs/2509.01329)
*Wei Bu*

Main category: cs.LG

TL;DR: 提出利用复分析复兴理论的优化框架，从发散渐近级数提取全局信息指导局部优化。


<details>
  <summary>Details</summary>
Motivation: 现代优化中基于局部梯度的方法缺乏目标函数全局信息，导致收敛不佳和对初始化敏感。

Method: 计算统计力学配分函数，提取渐近级数系数，识别与临界目标函数值一一对应的Borel平面奇点。

Result: 目标值为局部优化器提供全局指导，实现原则性学习率调整并逃离次优区域。

Conclusion: 提出的目标值有优化景观几何的理论基础，区别于启发式自适应方法。

Abstract: Modern optimization faces a fundamental challenge: local gradient-based
methods provide no global information about the objective function $L$
landscape, often leading to suboptimal convergence and sensitivity to
initialization. We introduce a novel optimization framework that leverages
resurgence theory from complex analysis to extract global structural
information from divergent asymptotic series. Our key insight is that the
factorially divergent perturbative expansions of parameter space partition
functions encode precise information about all critical objective function
value in the landscape through their Borel transform singularities.
  The algorithm works by computing the statistical mechanical partition
function $Z(g) = \int e^{-L(\theta)/g} d\theta$ for small coupling $g\ll 1$,
extracting its asymptotic series coefficients, and identifying Borel plane
singularities that correspond one-to-one with critical objective function
values. These target values provide global guidance to local optimizers,
enabling principled learning rate adaptation and escape from suboptimal
regions. Unlike heuristic adaptive methods, targets are theoretically grounded
in the geometry of the optimization landscape.

</details>


### [304] [AT Loss: Advanced Torrential Loss Function for Precipitation Forecasting](https://arxiv.org/abs/2509.01348)
*Jaeho Choi,Hyeri Kim,Kwang-Ho Kim,Jaesung Lee*

Main category: cs.LG

TL;DR: 为解决降水预测中现有损失函数问题，引入简单惩罚表达式并转化为QUBO形式，再近似得到可微的AT损失函数，经实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习降水预测方法多使用现成损失函数，基于CSI的优化在长时间干旱时效果不佳，需更好的优化准则。

Method: 引入简单惩罚表达式，将其重解释为QUBO形式，通过近似过程将QUBO松弛为可微的AT损失函数。

Result: 通过Lipschitz常数、预测性能评估、一致性实验和消融研究证明了AT损失函数的优越性。

Conclusion: 提出的AT损失函数能有效解决现有降水预测方法的局限性，具有更好的性能。

Abstract: Accurate precipitation forecasting is becoming increasingly important in the
context of climate change. In response, machine learning-based approaches have
recently gained attention as an emerging alternative to traditional methods
such as numerical weather prediction and climate models. Nonetheless, many
recent approaches still rely on off-the-shelf loss functions, and even the more
advanced ones merely involve optimization processes based on the critical
success index (CSI). The problem, however, is that CSI may become ineffective
during extended dry periods when precipitation remains below the threshold,
rendering it less than ideal as a criterion for optimization. To address this
limitation, we introduce a simple penalty expression and reinterpret it as a
quadratic unconstrained binary optimization (QUBO) formulation. Ultimately, the
resulting QUBO formulation is relaxed into a differentiable advanced torrential
(AT) loss function through an approximation process. The proposed AT loss
demonstrates its superiority through the Lipschitz constant, forecast
performance evaluations, consistency experiments, and ablation studies with the
operational model.

</details>


### [305] [Causal Sensitivity Identification using Generative Learning](https://arxiv.org/abs/2509.01352)
*Soma Bandyopadhyay,Sudeshna Sarkar*

Main category: cs.LG

TL;DR: 提出新的生成方法识别因果影响并应用于预测任务，利用CVAE实现，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 提出一种能识别因果影响并应用于预测任务的方法，减少混杂偏差。

Method: 从干预和反事实角度进行因果影响分析，利用条件变分自动编码器（CVAE）识别因果影响并作为生成预测器，通过识别因果敏感特征减少混杂偏差。

Result: 在大规模GeoLife数据集和亚洲贝叶斯网络基准上的实验，验证了该方法识别因果影响和提高预测性能的能力。

Conclusion: 所提出的方法能有效识别因果影响并应用于预测任务，可减少混杂偏差，提升预测表现。

Abstract: In this work, we propose a novel generative method to identify the causal
impact and apply it to prediction tasks. We conduct causal impact analysis
using interventional and counterfactual perspectives. First, applying
interventions, we identify features that have a causal influence on the
predicted outcome, which we refer to as causally sensitive features, and
second, applying counterfactuals, we evaluate how changes in the cause affect
the effect. Our method exploits the Conditional Variational Autoencoder (CVAE)
to identify the causal impact and serve as a generative predictor. We are able
to reduce confounding bias by identifying causally sensitive features. We
demonstrate the effectiveness of our method by recommending the most likely
locations a user will visit next in their spatiotemporal trajectory influenced
by the causal relationships among various features. Experiments on the
large-scale GeoLife [Zheng et al., 2010] dataset and the benchmark Asia
Bayesian network validate the ability of our method to identify causal impact
and improve predictive performance.

</details>


### [306] [DPF-CM: A Data Processing Framework with Privacy-Preserving Vector Databases for Chinese Medical LLMs Training and Deployment](https://arxiv.org/abs/2509.01354)
*Wei Huang,Anda Cheng,Zhao Zhang,Yinggui Wang*

Main category: cs.LG

TL;DR: 提出DPF - CM框架用于中文医疗大语言模型训练与部署，含训练数据处理和隐私保护模块，提升模型精度并减少隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 现有中文医疗语言模型开源训练管道缺乏对训练数据处理的全面探索。

Method: 提出DPF - CM框架，包括针对模型训练的数据处理管道（采用链式示例上下文学习策略和基于集成的过滤机制）和模型部署时的隐私保护模块（PPVD方法）。

Result: 显著提高模型精度，训练的中文医疗大语言模型达到开源同类模型的最优性能，框架减少训练数据隐私泄露27%。

Conclusion: DPF - CM框架有效提升中文医疗大语言模型性能并降低隐私泄露风险。

Abstract: Current open-source training pipelines for Chinese medical language models
predominantly emphasize optimizing training methodologies to enhance the
performance of large language models (LLMs), yet lack comprehensive exploration
into training data processing. To address this gap, we propose DPF-CM, a
holistic Data Processing Framework for Chinese Medical LLMs training and
deployment. DPF-CM comprises two core modules. The first module is a data
processing pipeline tailored for model training. Beyond standard data
processing operations, we (1) introduce a chained examples context-learning
strategy to generate question-oriented instructions to mitigate the lack of
instruction content, and (2) implement an ensemble-based filtering mechanism
for preference data curation that averages multiple reward models to suppress
noisy samples. The second module focuses on privacy preservation during model
deployment. To prevent privacy risks from the inadvertent exposure of training
data, we propose a Privacy Preserving Vector Database (PPVD) approach, which
involves model memory search, high-risk database construction, secure database
construction, and match-and-replace, four key stages to minimize privacy
leakage during inference collectively. Experimental results show that DPF-CM
significantly improves model accuracy, enabling our trained Chinese medical LLM
to achieve state-of-the-art performance among open-source counterparts.
Moreover, the framework reduces training data privacy leakage by 27%.

</details>


### [307] [CbLDM: A Diffusion Model for recovering nanostructure from pair distribution function](https://arxiv.org/abs/2509.01370)
*Jiarui Cao,Zhiyang Zhang,Heming Wang,Jun Xu,Ling Lan,Ran Gu*

Main category: cs.LG

TL;DR: 本文聚焦用PDF恢复纳米结构问题，提出CbLDM模型，能提升效率、降低误差，且预测精度高于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决纳米结构逆问题，理解纳米材料性质与结构的关系。

Method: 提出CbLDM模型，用条件先验估计条件后验分布以减少扩散模型采样步骤，用拉普拉斯矩阵替代距离矩阵恢复纳米结构。

Result: CbLDM与现有模型对比，预测精度显著更高。

Conclusion: CbLDM有解决纳米结构逆问题的能力，且有处理其他连续条件生成任务的潜力。

Abstract: Nowadays, the nanostructure inverse problem is an attractive problem that
helps researchers to understand the relationship between the properties and the
structure of nanomaterials. This article focuses on the problem of using PDF to
recover the nanostructure, which this article views as a conditional generation
problem. This article propose a deep learning model CbLDM, Condition-based
Latent Diffusion Model. Based on the original latent diffusion model, the
sampling steps of the diffusion model are reduced and the sample generation
efficiency is improved by using the conditional prior to estimate conditional
posterior distribution, which is the approximated distribution of p(z|x). In
addition, this article uses the Laplacian matrix instead of the distance matrix
to recover the nanostructure, which can reduce the reconstruction error.
Finally, this article compares CbLDM with existing models which were used to
solve the nanostructure inverse problem, and find that CbLDM demonstrates
significantly higher prediction accuracy than these models, which reflects the
ability of CbLDM to solve the nanostructure inverse problem and the potential
to cope with other continuous conditional generation tasks.

</details>


### [308] [Learn to Jump: Adaptive Random Walks for Long-Range Propagation through Graph Hierarchies](https://arxiv.org/abs/2509.01381)
*Joël Mathys,Federico Errica*

Main category: cs.LG

TL;DR: 提出利用分层图结构和自适应随机游走的方法解决消息传递架构难以建模长距离依赖的问题，在合成长距离任务中表现超传统方法。


<details>
  <summary>Details</summary>
Motivation: 消息传递架构在节点和图预测任务中难以充分建模长距离依赖。

Method: 引入可学习的转移概率，决定随机游走是选择原图还是通过分层捷径。

Result: 在合成长距离任务中超越仅基于原始拓扑的传统方法理论界限，偏好分层的游走与原图上更长的游走性能相同。

Conclusion: 该方法为高效处理大图并有效捕捉长距离依赖开辟了有前景的方向。

Abstract: Message-passing architectures struggle to sufficiently model long-range
dependencies in node and graph prediction tasks. We propose a novel approach
exploiting hierarchical graph structures and adaptive random walks to address
this challenge. Our method introduces learnable transition probabilities that
decide whether the walk should prefer the original graph or travel across
hierarchical shortcuts. On a synthetic long-range task, we demonstrate that our
approach can exceed the theoretical bound that constrains traditional
approaches operating solely on the original topology. Specifically, walks that
prefer the hierarchy achieve the same performance as longer walks on the
original graph. These preliminary findings open a promising direction for
efficiently processing large graphs while effectively capturing long-range
dependencies.

</details>


### [309] [Distillation of a tractable model from the VQ-VAE](https://arxiv.org/abs/2509.01400)
*Armin Hadžić,Milan Papez,Tomáš Pevný*

Main category: cs.LG

TL;DR: 提出通过选择高概率潜在变量子集将VQ - VAE蒸馏为易处理模型，实验表现佳挑战其难处理观点。


<details>
  <summary>Details</summary>
Motivation: 离散潜在空间的深度生成模型（如VQ - VAE）因潜在空间大，概率推断难处理。

Method: 选择高概率的潜在变量子集将VQ - VAE蒸馏为易处理模型，并将蒸馏模型构建为概率电路。

Result: 蒸馏模型保留了VQ - VAE的表达能力，在密度估计和条件生成任务中有有竞争力的表现。

Conclusion: 挑战了VQ - VAE是固有难处理模型的观点。

Abstract: Deep generative models with discrete latent space, such as the
Vector-Quantized Variational Autoencoder (VQ-VAE), offer excellent data
generation capabilities, but, due to the large size of their latent space,
their probabilistic inference is deemed intractable. We demonstrate that the
VQ-VAE can be distilled into a tractable model by selecting a subset of latent
variables with high probabilities. This simple strategy is particularly
efficient, especially if the VQ-VAE underutilizes its latent space, which is,
indeed, very often the case. We frame the distilled model as a probabilistic
circuit, and show that it preserves expressiveness of the VQ-VAE while
providing tractable probabilistic inference. Experiments illustrate competitive
performance in density estimation and conditional generation tasks, challenging
the view of the VQ-VAE as an inherently intractable model.

</details>


### [310] [Evaluating the stability of model explanations in instance-dependent cost-sensitive credit scoring](https://arxiv.org/abs/2509.01409)
*Matteo Ballegeer,Matthias Bogaert,Dries F. Benoit*

Main category: cs.LG

TL;DR: 研究评估IDCS模型下LIME和SHAP解释稳定性，发现IDCS提升成本效率但解释稳定性差，需解决稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有文献未探索IDCS损失函数对模型解释稳定性的影响，且监管对透明度要求增加。

Method: 用四个公开信用评分数据集评估IDCS分类器的判别力和成本效率，引入新指标；通过重采样研究不同类别不平衡程度下SHAP和LIME特征重要性排名的稳定性。

Result: IDCS分类器提高成本效率，但解释稳定性比传统模型差，类别不平衡加剧时更明显。

Conclusion: 信用评分中成本优化和可解释性存在权衡，需解决IDCS分类器稳定性问题。

Abstract: Instance-dependent cost-sensitive (IDCS) classifiers offer a promising
approach to improving cost-efficiency in credit scoring by tailoring loss
functions to instance-specific costs. However, the impact of such loss
functions on the stability of model explanations remains unexplored in
literature, despite increasing regulatory demands for transparency. This study
addresses this gap by evaluating the stability of Local Interpretable
Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP)
when applied to IDCS models. Using four publicly available credit scoring
datasets, we first assess the discriminatory power and cost-efficiency of IDCS
classifiers, introducing a novel metric to enhance cross-dataset comparability.
We then investigate the stability of SHAP and LIME feature importance rankings
under varying degrees of class imbalance through controlled resampling. Our
results reveal that while IDCS classifiers improve cost-efficiency, they
produce significantly less stable explanations compared to traditional models,
particularly as class imbalance increases, highlighting a critical trade-off
between cost optimization and interpretability in credit scoring. Amid
increasing regulatory scrutiny on explainability, this research underscores the
pressing need to address stability issues in IDCS classifiers to ensure that
their cost advantages are not undermined by unstable or untrustworthy
explanations.

</details>


### [311] [Accelerating PDE Solvers with Equation-Recast Neural Operator Preconditioning](https://arxiv.org/abs/2509.01416)
*Qiyun Cheng,Md Hossain Sahadath,Huihua Yang,Shaowu Pan,Wei Ji*

Main category: cs.LG

TL;DR: 提出MD - PNOP框架加速参数化PDE求解器，解决神经算子外推局限，减少迭代次数，保证物理约束，用不同算子在中子输运应用中验证，减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 传统偏微分方程数值求解器计算开销大，是大规模参数研究和设计优化的瓶颈。

Method: 将参数偏差的残差作为额外源项，用训练好的神经算子离线优化解，将神经算子预测作为迭代PDE求解器的初始猜测。

Result: 用DeepONet和FNO在中子输运应用中验证，单组常数参数训练的神经算子能加速不同参数分布的求解，计算时间减少约50%。

Conclusion: MD - PNOP框架能加速参数化PDE求解器，严格保留物理约束，减少计算时间且不损失精度。

Abstract: The computational overhead of traditional numerical solvers for partial
differential equations (PDEs) remains a critical bottleneck for large-scale
parametric studies and design optimization. We introduce a Minimal-Data
Parametric Neural Operator Preconditioning (MD-PNOP) framework, which
establishes a new paradigm for accelerating parametric PDE solvers while
strictly preserving physical constraints. The key idea is to recast the
residual from parameter deviation as additional source term, where any trained
neural operator can be used to refine the solution in an offline fashion. This
directly addresses the fundamental extrapolation limitation of neural
operators, enabling extrapolative generalization of any neural operator trained
at a single parameter setting across a wide range of configurations without any
retraining. The neural operator predictions are then embedded into iterative
PDE solvers as improved initial guesses, thereby reducing convergence
iterations without sacrificing accuracy. Unlike purely data-driven approaches,
MD-PNOP guarantees that the governing equations remain fully enforced,
eliminating concerns about loss of physics or interpretability. The framework
is architecture-agnostic and is demonstrated using both Deep Operator Networks
(DeepONet) and Fourier Neural Operators (FNO) for Boltzmann transport equation
solvers in neutron transport applications. We demonstrated that neural
operators trained on a single set of constant parameters successfully
accelerate solutions with heterogeneous, sinusoidal, and discontinuous
parameter distributions. Besides, MD-PNOP consistently achieves ~50% reduction
in computational time while maintaining full order fidelity for fixed-source,
single-group eigenvalue, and multigroup coupled eigenvalue problems.

</details>


### [312] [The Geometry of Nonlinear Reinforcement Learning](https://arxiv.org/abs/2509.01432)
*Nikola Milosevic,Nico Scherf*

Main category: cs.LG

TL;DR: 提出统一几何框架，将强化学习中奖励最大化、安全探索和内在动机视为单一优化问题，并推广经典方法，阐述其涵盖目标及开放挑战。


<details>
  <summary>Details</summary>
Motivation: 将强化学习中奖励最大化、安全探索和内在动机这几个常被分开研究的目标进行统一。

Method: 提出统一几何框架，将这些目标视为环境中可实现长期行为空间上的单一优化问题，推广经典方法。

Result: 经典方法自然推广到非线性效用和凸约束，该视角能涵盖鲁棒性、安全性、探索和多样性目标。

Conclusion: 介绍了该框架并指出几何与深度强化学习交叉领域的开放挑战。

Abstract: Reward maximization, safe exploration, and intrinsic motivation are often
studied as separate objectives in reinforcement learning (RL). We present a
unified geometric framework, that views these goals as instances of a single
optimization problem on the space of achievable long-term behavior in an
environment. Within this framework, classical methods such as policy mirror
descent, natural policy gradient, and trust-region algorithms naturally
generalize to nonlinear utilities and convex constraints. We illustrate how
this perspective captures robustness, safety, exploration, and diversity
objectives, and outline open challenges at the interface of geometry and deep
RL.

</details>


### [313] [Benchmarking Optimizers for Large Language Model Pretraining](https://arxiv.org/abs/2509.01440)
*Andrei Semenov,Matteo Pagliardini,Martin Jaggi*

Main category: cs.LG

TL;DR: 对大语言模型优化技术进行标准化评估，为从业者提供优化器选择建议，为研究者指明方向并开源代码促进方法发展和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型优化方法实验协议多样，难以直接比较，需综合评估。

Method: 在标准化大语言模型预训练场景下，系统改变模型大小、批量大小和训练时长，对近期优化技术进行评估，并仔细调整每个方法。

Result: 为从业者提供不同场景下最适合的优化器建议，为研究者指明未来优化研究方向。

Conclusion: 通过开源代码和可复现实验，有望助力未来方法的发展和严格基准测试。

Abstract: The recent development of Large Language Models (LLMs) has been accompanied
by an effervescence of novel ideas and methods to better optimize the loss of
deep learning models. Claims from those methods are myriad: from faster
convergence to removing reliance on certain hyperparameters. However, the
diverse experimental protocols used to validate these claims make direct
comparisons between methods challenging. This study presents a comprehensive
evaluation of recent optimization techniques across standardized LLM
pretraining scenarios, systematically varying model size, batch size, and
training duration. Through careful tuning of each method, we provide guidance
to practitioners on which optimizer is best suited for each scenario. For
researchers, our work highlights promising directions for future optimization
research. Finally, by releasing our code and making all experiments fully
reproducible, we hope our efforts can help the development and rigorous
benchmarking of future methods.

</details>


### [314] [Hierarchical Motion Captioning Utilizing External Text Data Source](https://arxiv.org/abs/2509.01471)
*Clayton Leite,Yu Xiao*

Main category: cs.LG

TL;DR: 本文提出两步分层方法增强运动字幕生成，利用大语言模型细化标注并引入检索机制，在三个数据集上平均性能提升6% - 50%。


<details>
  <summary>Details</summary>
Motivation: 现有运动字幕生成方法需带高层描述的运动数据，但此类数据在现有运动 - 文本数据集中稀缺，且缺乏低层运动描述。

Method: 提出两步分层方法，先用大语言模型细化标注以训练模型生成含低层细节字幕，再引入检索机制结合外部文本生成精确高层字幕。

Result: 在HumanML3D、KIT和BOTH57M三个数据集上，相比M2T - Interpretable，平均性能（BLEU - 1、BLEU - 4、CIDEr和ROUGE - L）提升6% - 50%。

Conclusion: 该方法能利用外部文本知识提高运动字幕准确性，尤其适用于现有数据集未涵盖的运动。

Abstract: This paper introduces a novel approach to enhance existing motion captioning
methods, which directly map representations of movement to high-level
descriptive captions (e.g., ``a person doing jumping jacks"). The existing
methods require motion data annotated with high-level descriptions (e.g.,
``jumping jacks"). However, such data is rarely available in existing
motion-text datasets, which additionally do not include low-level motion
descriptions. To address this, we propose a two-step hierarchical approach.
First, we employ large language models to create detailed descriptions
corresponding to each high-level caption that appears in the motion-text
datasets (e.g., ``jumping while synchronizing arm extensions with the opening
and closing of legs" for ``jumping jacks"). These refined annotations are used
to retrain motion-to-text models to produce captions with low-level details.
Second, we introduce a pioneering retrieval-based mechanism. It aligns the
detailed low-level captions with candidate high-level captions from additional
text data sources, and combine them with motion features to fabricate precise
high-level captions. Our methodology is distinctive in its ability to harness
knowledge from external text sources to greatly increase motion captioning
accuracy, especially for movements not covered in existing motion-text
datasets. Experiments on three distinct motion-text datasets (HumanML3D, KIT,
and BOTH57M) demonstrate that our method achieves an improvement in average
performance (across BLEU-1, BLEU-4, CIDEr, and ROUGE-L) ranging from 6% to 50%
compared to the state-of-the-art M2T-Interpretable.

</details>


### [315] [Prior-Guided Flow Matching for Target-Aware Molecule Design with Learnable Atom Number](https://arxiv.org/abs/2509.01486)
*Jingyuan Zhou,Hao Qian,Shikui Tu,Lei Xu*

Main category: cs.LG

TL;DR: 提出新的目标感知分子生成模型PAFlow，在结合亲和力上达到新的最优水平并保持良好分子特性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型存在概率动态不稳定、生成分子大小与蛋白口袋几何不匹配等问题，影响质量和产生脱靶效应。

Method: 采用高效流匹配框架，构建离散原子类型的条件流匹配，引入蛋白 - 配体相互作用预测器引导向量场，设计基于蛋白口袋信息的原子数预测器。

Result: 在CrossDocked2020基准测试中，PAFlow达到新的结合亲和力最优水平（平均Vina分数达 - 8.31），同时保持良好分子特性。

Conclusion: PAFlow在基于结构的药物设计中是有效的，能改善分子生成的质量和结合亲和力。

Abstract: Structure-based drug design (SBDD), aiming to generate 3D molecules with high
binding affinity toward target proteins, is a vital approach in novel drug
discovery. Although recent generative models have shown great potential, they
suffer from unstable probability dynamics and mismatch between generated
molecule size and the protein pockets geometry, resulting in inconsistent
quality and off-target effects. We propose PAFlow, a novel target-aware
molecular generation model featuring prior interaction guidance and a learnable
atom number predictor. PAFlow adopts the efficient flow matching framework to
model the generation process and constructs a new form of conditional flow
matching for discrete atom types. A protein-ligand interaction predictor is
incorporated to guide the vector field toward higher-affinity regions during
generation, while an atom number predictor based on protein pocket information
is designed to better align generated molecule size with target geometry.
Extensive experiments on the CrossDocked2020 benchmark show that PAFlow
achieves a new state-of-the-art in binding affinity (up to -8.31 Avg. Vina
Score), simultaneously maintains favorable molecular properties.

</details>


### [316] [Unsupervised Identification and Replay-based Detection (UIRD) for New Category Anomaly Detection in ECG Signal](https://arxiv.org/abs/2509.01512)
*Zhangyue Shi,Zekai Wang,Yuxuan Li*

Main category: cs.LG

TL;DR: 针对心电图自动分析中样本不均衡和数据存储问题，提出基于伪重放的半监督持续学习框架，在四个公开数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 临床心电图自动分析存在特定类型样本有限导致的类别不平衡问题，且大量历史数据存储负担重，影响异常检测性能。

Method: 提出基于伪重放的半监督持续学习框架，包含无监督识别和基于重放的检测两部分，用无监督生成对抗网络检测新特征，用生成器学习数据分布合成伪数据。

Result: 在四个公开心电图数据集验证了框架有效性，能检测新异常并保持对现有信号的检测性能。

Conclusion: 所提出的方法在识别新异常和检测现有心电图信号方面很有前景。

Abstract: In clinical practice, automatic analysis of electrocardiogram (ECG) is widely
applied to identify irregular heart rhythms and other electrical anomalies of
the heart, enabling timely intervention and potentially improving clinical
outcomes. However, due to the limited samples in certain types of ECG signals,
the class imbalance issues pose a challenge for ECG-based detection. In
addition, as the volume of patient data grows, long-term storage of all
historical data becomes increasingly burdensome as training samples to
recognize new patterns and classify existing ECG signals accurately. Therefore,
to enhance the performance of anomaly detection while addressing storage
limitations, we propose a pseudo-replay based semi-supervised continual
learning framework, which consists of two components: unsupervised
identification and replay-based detection. For unsupervised identification, an
unsupervised generative adversarial network (GAN)-based framework is integrated
to detect novel patterns. Besides, instead of directly storing all historical
data, a pseudo replay-based learning strategy is proposed which utilizes a
generator to learn the data distribution for each individual task. When a new
task arises, the generator synthesizes pseudo data representative of previous
learnt classes, enabling the model to detect both the existed patterns and the
newly presented anomalies. The effectiveness of the proposed framework is
validated in four public ECG datasets, which leverages supervised
classification problems for anomaly detection. The experimental results show
that the developed approach is very promising in identifying novel anomalies
while maintaining good performance on detecting existing ECG signals.

</details>


### [317] [Forward-Only Continual Learning](https://arxiv.org/abs/2509.01533)
*Jiao Chen,Jiayi He,Fangfang Chen,Zuohong Lv,Jianhua Tang*

Main category: cs.LG

TL;DR: 提出FoRo方法解决预训练模型持续学习灾难性遗忘问题，实验显示其减少遗忘、提升准确率，降低内存和时间成本。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型持续学习方法依赖迭代误差反向传播和基于梯度的优化，计算密集且不适合资源受限环境，需解决灾难性遗忘问题。

Method: 提出FoRo方法，包括轻量级提示调优策略和知识编码机制，不修改预训练模型；用CMA - ES优化提示嵌入，通过非线性随机投影和递归最小二乘法将任务知识编码到知识编码矩阵。

Result: FoRo显著减少平均遗忘、提高准确率，减少内存使用和运行时间，在长任务序列中保持高知识保留率。

Conclusion: FoRo是探索预训练模型持续学习的有前景方向，尤其适用于对效率和效果要求高的现实多媒体应用。

Abstract: Catastrophic forgetting remains a central challenge in continual learning
(CL) with pre-trained models. While existing approaches typically freeze the
backbone and fine-tune a small number of parameters to mitigate forgetting,
they still rely on iterative error backpropagation and gradient-based
optimization, which can be computationally intensive and less suitable for
resource-constrained environments. To address this, we propose FoRo, a
forward-only, gradient-free continual learning method. FoRo consists of a
lightweight prompt tuning strategy and a novel knowledge encoding mechanism,
both designed without modifying the pre-trained model. Specifically, prompt
embeddings are inserted at the input layer and optimized using the Covariance
Matrix Adaptation Evolution Strategy (CMA-ES), which mitigates distribution
shifts and extracts high-quality task representations. Subsequently,
task-specific knowledge is encoded into a knowledge encoding matrix via
nonlinear random projection and recursive least squares, enabling incremental
updates to the classifier without revisiting prior data. Experiments show that
FoRo significantly reduces average forgetting and improves accuracy. Thanks to
forward-only learning, FoRo reduces memory usage and run time while maintaining
high knowledge retention across long task sequences. These results suggest that
FoRo could serve as a promising direction for exploring continual learning with
pre-trained models, especially in real-world multimedia applications where both
efficiency and effectiveness are critical.

</details>


### [318] [Graph Contrastive Learning versus Untrained Baselines: The Role of Dataset Size](https://arxiv.org/abs/2509.01541)
*Smayan Khanna,Doruk Efe Gökmen,Risi Kondor,Vincenzo Vitelli*

Main category: cs.LG

TL;DR: 研究图对比学习（GCL）是否优于未训练基线，发现其优势依赖数据集大小和任务难度，需关注数据集大小作用并设计避免性能停滞的算法。


<details>
  <summary>Details</summary>
Motivation: 探究图对比学习是否真的优于未训练基线。

Method: 在标准数据集、大型分子数据集和合成数据集上对比GCL与未训练图神经网络、简单多层感知器和手工统计量的表现。

Result: GCL优势依赖数据集大小和任务难度，在ogbg - molhiv数据集上小尺度滞后大尺度领先但会停滞，在合成数据集上准确性与图数量对数相关，性能差距随任务复杂度变化。

Conclusion: 在基准测试和应用中需明确数据集大小的作用，设计避免性能停滞的GCL算法。

Abstract: Graph Contrastive Learning (GCL) has emerged as a leading paradigm for self-
supervised learning on graphs, with strong performance reported on standardized
datasets and growing applications ranging from genomics to drug discovery. We
ask a basic question: does GCL actually outperform untrained baselines? We find
that GCL's advantage depends strongly on dataset size and task difficulty. On
standard datasets, untrained Graph Neural Networks (GNNs), simple multilayer
perceptrons, and even handcrafted statistics can rival or exceed GCL. On the
large molecular dataset ogbg-molhiv, we observe a crossover: GCL lags at small
scales but pulls ahead beyond a few thousand graphs, though this gain
eventually plateaus. On synthetic datasets, GCL accuracy approximately scales
with the logarithm of the number of graphs and its performance gap (compared
with untrained GNNs) varies with respect to task complexity. Moving forward, it
is crucial to identify the role of dataset size in benchmarks and applications,
as well as to design GCL algorithms that avoid performance plateaus.

</details>


### [319] [Feynman-Kac-Flow: Inference Steering of Conditional Flow Matching to an Energy-Tilted Posterior](https://arxiv.org/abs/2509.01543)
*Konstantin Mark,Leonard Galustian,Maximilian P. -P. Kovar,Esther Heid*

Main category: cs.LG

TL;DR: 本文为条件流匹配（CFM）推导了Feynman - Kac引导方法，并在合成任务和化学反应过渡态生成上进行评估。


<details>
  <summary>Details</summary>
Motivation: CFM在生成建模中表现出色，但缺乏将生成样本导向精确要求的方法，现有扩散模型的引导方法未扩展到CFM。

Method: 将需求表述为用能量势倾斜输出，首次为CFM推导Feynman - Kac引导方法。

Result: 在合成任务上评估方法，解决化学反应过渡态生成中正确手性这一先前未解决的挑战。

Conclusion: 所提出的Feynman - Kac引导的CFM方法有效，代码开源。

Abstract: Conditional Flow Matching(CFM) represents a fast and high-quality approach to
generative modelling, but in many applications it is of interest to steer the
generated samples towards precise requirements. While steering approaches like
gradient-based guidance, sequential Monte Carlo steering or Feynman-Kac
steering are well established for diffusion models, they have not been extended
to flow matching approaches yet. In this work, we formulate this requirement as
tilting the output with an energy potential. We derive, for the first time,
Feynman-Kac steering for CFM. We evaluate our approach on a set of synthetic
tasks, including the generation of tilted distributions in a high-dimensional
space, which is a particularly challenging case for steering approaches. We
then demonstrate the impact of Feynman-Kac steered CFM on the previously
unsolved challenge of generated transition states of chemical reactions with
the correct chirality, where the reactants or products can have a different
handedness, leading to geometric constraints of the viable reaction pathways
connecting reactants and products. Code to reproduce this study is avaiable
open-source at https://github.com/heid-lab/fkflow.

</details>


### [320] [Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing](https://arxiv.org/abs/2509.01548)
*Zihao Wang,Enneng Yang,Lu Yin,Shiwei Liu,Li Shen*

Main category: cs.LG

TL;DR: 提出MergeLock机制防止未经授权的模型合并，实验证明其有效且具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着微调模型增多，模型合并的安全问题凸显，现有方法无法有效防止非法合并。

Method: 利用Transformer模型注意力机制的对称性，随机采样两对可逆矩阵应用于QK和VO分支，使模型参数不可合并。

Result: 在视觉和语言任务实验中，MergeLock在多数情况下使涉及受保护模型的合并模型性能下降超95%，且无法用低成本恢复方法有效恢复。

Conclusion: MergeLock能有效直接防止未经授权的模型合并，且具有较强鲁棒性。

Abstract: Model merging leverages multiple finetuned expert models to construct a
multi-task model with low cost, and is gaining increasing attention. However,
as a growing number of finetuned models become publicly available, concerns
about the safety of model merging have emerged. Unauthorized merging may
infringe on developers' rights and risk leaking sensitive personal information.
Most existing methods focus on detecting whether a merged model originates from
a specific source model, but fail to effectively prevent illegal merging. In
this paper, we propose MergeLock, an active protection mechanism that disrupts
model parameters to render them unmergeable, thereby directly preventing
unauthorized model merging. Specifically, leveraging the inherent symmetry of
the attention mechanism in Transformer-based models, we randomly sample two
pairs of invertible matrices and apply them to the Query-Key (QK) and
Value-Output (VO) branches. This transformation keeps the model's output
unchanged while pushing it away from the shared parameter space of other
finetuned models. Extensive experiments across both vision and language tasks
demonstrate that MergeLock can degrade the performance of merged models by over
95% when a protected model is involved in most cases, demonstrating its
effectiveness. Moreover, we further demonstrate that merged models protected by
MergeLock cannot be effectively recovered using low-cost restoration methods,
further enhancing robustness against unauthorized merging. The code is
available at https://github.com/hetailang/Merge-Lock.

</details>


### [321] [Direct Profit Estimation Using Uplift Modeling under Clustered Network Interference](https://arxiv.org/abs/2509.01558)
*Bram van den Akker*

Main category: cs.LG

TL;DR: 本文提出实用方法，用AddIPW估计器作可微学习目标优化策略，模拟显示该方法优于忽视干扰的方法，以利润为中心的策略表现更佳。


<details>
  <summary>Details</summary>
Motivation: 标准提升建模方法未考虑干扰，违反SUTVA导致策略欠佳，且干扰感知估计器未应用于提升建模，策略优化不完善。

Method: 使用AddIPW估计器作为适合基于梯度优化的可微学习目标，并与响应转换技术结合直接优化经济结果。

Result: 模拟显示该方法显著优于忽视干扰的方法，尤其干扰效应增大时；以利润为中心的提升策略在识别高影响干预措施上表现更优。

Conclusion: 提出的方法为实现更有利可图的激励个性化提供了实用途径。

Abstract: Uplift modeling is a key technique for promotion optimization in recommender
systems, but standard methods typically fail to account for interference, where
treating one item affects the outcomes of others. This violation of the Stable
Unit Treatment Value Assumption (SUTVA) leads to suboptimal policies in
real-world marketplaces. Recent developments in interference-aware estimators
such as Additive Inverse Propensity Weighting (AddIPW) have not found their way
into the uplift modeling literature yet, and optimising policies using these
estimators is not well-established. This paper proposes a practical methodology
to bridge this gap. We use the AddIPW estimator as a differentiable learning
objective suitable for gradient-based optimization. We demonstrate how this
framework can be integrated with proven response transformation techniques to
directly optimize for economic outcomes like incremental profit. Through
simulations, we show that our approach significantly outperforms
interference-naive methods, especially as interference effects grow.
Furthermore, we find that adapting profit-centric uplift strategies within our
framework can yield superior performance in identifying the highest-impact
interventions, offering a practical path toward more profitable incentive
personalization.

</details>


### [322] [Learning Longitudinal Stress Dynamics from Irregular Self-Reports via Time Embeddings](https://arxiv.org/abs/2509.01569)
*Louis Simon,Mohamed Chetouani*

Main category: cs.LG

TL;DR: 研究利用时间嵌入处理EMA序列中的时间依赖，提出Ema2Vec方法用于纵向压力预测，表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 移动和可穿戴传感技术与生态自我报告问卷结合可探索人类行为纵向建模，但存在数据缺失和报告时间不规则问题，影响人类状态和行为预测。

Method: 引入新的时间嵌入方法Ema2Vec处理不规则间隔的自我报告，并用于纵向压力预测任务。

Result: 该方法优于依赖固定大小每日窗口的标准压力预测基线，以及无时间感知表示的纵向序列训练模型。

Conclusion: 在建模不规则采样的纵向数据时，纳入时间嵌入很重要。

Abstract: The widespread adoption of mobile and wearable sensing technologies has
enabled continuous and personalized monitoring of affect, mood disorders, and
stress. When combined with ecological self-report questionnaires, these systems
offer a powerful opportunity to explore longitudinal modeling of human
behaviors. However, challenges arise from missing data and the irregular timing
of self-reports, which make challenging the prediction of human states and
behaviors. In this study, we investigate the use of time embeddings to capture
time dependencies within sequences of Ecological Momentary Assessments (EMA).
We introduce a novel time embedding method, Ema2Vec, designed to effectively
handle irregularly spaced self-reports, and evaluate it on a new task of
longitudinal stress prediction. Our method outperforms standard stress
prediction baselines that rely on fixed-size daily windows, as well as models
trained directly on longitudinal sequences without time-aware representations.
These findings emphasize the importance of incorporating time embeddings when
modeling irregularly sampled longitudinal data.

</details>


### [323] [One-Shot Clustering for Federated Learning Under Clustering-Agnostic Assumption](https://arxiv.org/abs/2509.01587)
*Maciej Krzysztof Zuziak,Roberto Pellungrini,Salvatore Rinzivillo*

Main category: cs.LG

TL;DR: 提出One - Shot Clustered Federated Learning (OCFL)算法，可自动检测聚类时机，实验验证其自动执行CFL的良好性能，还探讨CFL算法可行性及个性化与可解释性的关系。


<details>
  <summary>Details</summary>
Motivation: Clustered Federated Learning (CFL)领域虽有成果但仍待探索，其基本假设和设置与标准FL不同。

Method: 基于计算客户端梯度间的余弦距离和检测联邦模型收敛的温度度量，对五个基准数据集的四十多个不同任务测试各种一次性聚类算法。

Result: OCFL算法无需调整超参数即可自动执行CFL，性能良好；基于客户端梯度的CFL算法中，基于密度的聚类方法在区分不同分布训练的神经网络损失曲面时效率高。

Conclusion: OCFL算法可有效用于CFL；基于密度的聚类方法在CFL中有高效率；可通过GradCAM深入了解个性化与本地预测可解释性的关系。

Abstract: Federated Learning (FL) is a widespread and well-adopted paradigm of
decentralised learning that allows training one model from multiple sources
without the need to transfer data between participating clients directly. Since
its inception in 2015, it has been divided into numerous subfields that deal
with application-specific issues, such as data heterogeneity or resource
allocation. One such sub-field, Clustered Federated Learning (CFL), deals with
the problem of clustering the population of clients into separate cohorts to
deliver personalised models. Although a few remarkable works have been
published in this domain, the problem remains largely unexplored, as its basic
assumptions and settings differ slightly from those of standard FL. In this
work, we present One-Shot Clustered Federated Learning (OCFL), a
clustering-agnostic algorithm that can automatically detect the earliest
suitable moment for clustering. Our algorithm is based on computing the cosine
distance between the gradients of the clients and a temperature measure that
detects when the federated model starts to converge. We empirically evaluate
our methodology by testing various one-shot clustering algorithms for over
forty different tasks on five benchmark datasets. Our experiments showcase the
good performance of our approach when used to perform CFL in an automated
manner without the need to adjust hyperparameters. We also revisit the
practical feasibility of CFL algorithms based on the gradients of the clients,
providing firm evidence of the high efficiency of density-based clustering
methods when used to differentiate between the loss surfaces of neural networks
trained on different distributions. Moreover, by inspecting the feasibility of
local explanations generated with the help of GradCAM, we can provide more
insights into the relationship between personalisation and the explainability
of local predictions.

</details>


### [324] [Entropy-Driven Curriculum for Multi-Task Training in Human Mobility Prediction](https://arxiv.org/abs/2509.01613)
*Tianye Fang,Xuanshu Luo,Martin Werner*

Main category: cs.LG

TL;DR: 本文提出结合熵驱动课程学习和多任务学习的统一训练框架解决人类移动性数据预测难题，实验显示该方法有良好表现。


<details>
  <summary>Details</summary>
Motivation: 人类移动性数据复杂阻碍模型训练，仅预测下一位置忽略隐含因素，导致结果不佳。

Method: 提出熵驱动课程学习策略，基于Lempel - Ziv压缩量化轨迹可预测性并从简单到复杂训练；采用多任务训练同时优化位置预测和移动距离、方向估计。

Result: 在HuMob Challenge实验中，该方法在GEO - BLEU和DTW指标上达到最优，收敛速度比无课程学习训练快达2.92倍。

Conclusion: 所提出的统一训练框架有效提升人类移动性预测的性能和收敛速度。

Abstract: The increasing availability of big mobility data from ubiquitous portable
devices enables human mobility prediction through deep learning approaches.
However, the diverse complexity of human mobility data impedes model training,
leading to inefficient gradient updates and potential underfitting. Meanwhile,
exclusively predicting next locations neglects implicit determinants, including
distances and directions, thereby yielding suboptimal prediction results. This
paper presents a unified training framework that integrates entropy-driven
curriculum and multi-task learning to address these challenges. The proposed
entropy-driven curriculum learning strategy quantifies trajectory
predictability based on Lempel-Ziv compression and organizes training from
simple to complex for faster convergence and enhanced performance. The
multi-task training simultaneously optimizes the primary location prediction
alongside auxiliary estimation of movement distance and direction for learning
realistic mobility patterns, and improve prediction accuracy through
complementary supervision signals. Extensive experiments conducted in
accordance with the HuMob Challenge demonstrate that our approach achieves
state-of-the-art performance on GEO-BLEU (0.354) and DTW (26.15) metrics with
up to 2.92-fold convergence speed compared to training without curriculum
learning.

</details>


### [325] [Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP](https://arxiv.org/abs/2509.01630)
*Bingheng Wang,Yichao Gao,Tianchen Sun,Lin Zhao*

Main category: cs.LG

TL;DR: 本文提出L2C框架元学习ADMM - DDP超参数，在合作空中运输任务表现良好，梯度计算更快。


<details>
  <summary>Details</summary>
Motivation: 分布式轨迹优化ADMM - DDP需大量调整超参数，提出L2C框架解决该问题。

Method: 用轻量级智能体神经网络建模超参数，端到端区分ADMM - DDP管道，复用DDP组件计算元梯度，用辅助ADMM框架协调，截断迭代和元学习ADMM惩罚参数加速训练。

Result: 在合作空中运输任务中，L2C生成可行轨迹，重新配置四旋翼编队，适应不同团队规模和任务条件，梯度计算比现有方法快88%。

Conclusion: L2C框架能有效解决ADMM - DDP超参数调整问题，在多智能体系统协调中表现出色。

Abstract: Distributed trajectory optimization via ADMM-DDP is a powerful approach for
coordinating multi-agent systems, but it requires extensive tuning of tightly
coupled hyperparameters that jointly govern local task performance and global
coordination. In this paper, we propose Learning to Coordinate (L2C), a general
framework that meta-learns these hyperparameters, modeled by lightweight
agent-wise neural networks, to adapt across diverse tasks and agent
configurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in
a distributed manner. It also enables efficient meta-gradient computation by
reusing DDP components such as Riccati recursions and feedback gains. These
gradients correspond to the optimal solutions of distributed matrix-valued LQR
problems, coordinated across agents via an auxiliary ADMM framework that
becomes convex under mild assumptions. Training is further accelerated by
truncating iterations and meta-learning ADMM penalty parameters optimized for
rapid residual reduction, with provable Lipschitz-bounded gradient errors. On a
challenging cooperative aerial transport task, L2C generates dynamically
feasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures
quadrotor formations for safe 6-DoF load manipulation in tight spaces, and
adapts robustly to varying team sizes and task conditions, while achieving up
to $88\%$ faster gradient computation than state-of-the-art methods.

</details>


### [326] [Relative Trajectory Balance is equivalent to Trust-PCL](https://arxiv.org/abs/2509.01632)
*Tristan Deleu,Padideh Nouri,Yoshua Bengio,Doina Precup*

Main category: cs.LG

TL;DR: 本文建立了相对轨迹平衡（RTB）与带KL正则化的离策略强化学习方法Trust - PCL的等价性，表明KL正则化强化学习方法有可比性能。


<details>
  <summary>Details</summary>
Motivation: 在生成建模微调中，KL正则化方法很有效，RTB用于改进顺序生成模型微调，需明确RTB在KL正则化强化学习理论中的位置及与早期方法关系。

Method: 基于GFlowNets与最大熵RL的相关工作，建立RTB和Trust - PCL的等价性。

Result: KL正则化强化学习方法在RTB论文的示例中有可比性能。

Conclusion: 明确了RTB在KL正则化RL理论中的位置，为之前结果提供了新视角。

Abstract: Recent progress in generative modeling has highlighted the importance of
Reinforcement Learning (RL) for fine-tuning, with KL-regularized methods in
particular proving to be highly effective for both autoregressive and diffusion
models. Complementing this line of work, the Relative Trajectory Balance (RTB)
objective was recently introduced in the context of Generative Flow Networks
(GFlowNets) to serve the same role of improving fine-tuning in sequential
generative models. Building on prior work linking GFlowNets and maximum-entropy
RL, we establish in this paper an equivalence between RTB and Trust-PCL, an
off-policy RL method with KL regularization. This equivalence situates RTB
within the broader theoretical landscape of KL-regularized RL, and clarifies
its relationship to earlier methods. Leveraging this insight, we revisit an
illustrative example from the RTB paper and show that KL-regularized RL methods
achieve comparable performance, offering an alternative perspective to what was
previously reported.

</details>


### [327] [REVELIO -- Universal Multimodal Task Load Estimation for Cross-Domain Generalization](https://arxiv.org/abs/2509.01642)
*Maximilian P. Oppelt,Andreas Foltyn,Nadine R. Lang-Richter,Bjoern M. Eskofier*

Main category: cs.LG

TL;DR: 本文引入新的多模态数据集，评估多种模型在认知负荷检测中的表现，发现多模态方法优于单模态，跨域泛化仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 当前任务负荷检测模型缺乏泛化性，在评估模型鲁棒性和跨域泛化能力方面存在差距。

Method: 引入新的多模态数据集，以n - back测试为基础，结合客观性能、主观NASA - TLX评级和任务级设计进行标注，系统训练和评估xLSTM、ConvNeXt和Transformer等模型。

Result: 多模态方法始终优于单模态基线，特定模态和模型架构在不同应用子集影响不同，跨域转移时模型性能下降。

Conclusion: 研究结果为开发更具泛化性的认知负荷检测系统提供了基线和见解，推动人机交互和自适应系统的研究与实践。

Abstract: Task load detection is essential for optimizing human performance across
diverse applications, yet current models often lack generalizability beyond
narrow experimental domains. While prior research has focused on individual
tasks and limited modalities, there remains a gap in evaluating model
robustness and transferability in real-world scenarios. This paper addresses
these limitations by introducing a new multimodal dataset that extends
established cognitive load detection benchmarks with a real-world gaming
application, using the $n$-back test as a scientific foundation. Task load
annotations are derived from objective performance, subjective NASA-TLX
ratings, and task-level design, enabling a comprehensive evaluation framework.
State-of-the-art end-to-end model, including xLSTM, ConvNeXt, and Transformer
architectures are systematically trained and evaluated on multiple modalities
and application domains to assess their predictive performance and cross-domain
generalization. Results demonstrate that multimodal approaches consistently
outperform unimodal baselines, with specific modalities and model architectures
showing varying impact depending on the application subset. Importantly, models
trained on one domain exhibit reduced performance when transferred to novel
applications, underscoring remaining challenges for universal cognitive load
estimation. These findings provide robust baselines and actionable insights for
developing more generalizable cognitive load detection systems, advancing both
research and practical implementation in human-computer interaction and
adaptive systems.

</details>


### [328] [Distilled Pretraining: A modern lens of Data, In-Context Learning and Test-Time Scaling](https://arxiv.org/abs/2509.01649)
*Sachin Goyal,David Lopez-Paz,Kartik Ahuja*

Main category: cs.LG

TL;DR: 研究蒸馏在大语言模型预训练中的影响，发现其提升测试时缩放性能但损害上下文学习能力，并通过二元模型研究解释原因，为预训练设计提供建议。


<details>
  <summary>Details</summary>
Motivation: 蒸馏在大语言模型预训练中受关注，但对测试时缩放和上下文学习等新范式的影响未充分研究。

Method: 进行蒸馏预训练实验，在二元模型沙盒中研究蒸馏预训练。

Result: 蒸馏预训练使模型测试时缩放性能更好，但损害上下文学习能力，尤其是通过归纳头建模的能力。

Conclusion: 研究结果有助于从业者理解蒸馏影响，为预训练设计提供参考。

Abstract: In the past year, distillation has seen a renewed prominence in large
language model (LLM) pretraining, exemplified by the Llama-3.2 and Gemma model
families. While distillation has historically been shown to improve statistical
modeling, its effects on new paradigms that are key to modern LLMs, such as
test-time scaling and in-context learning, remain underexplored. In this work,
we make three main contributions. First, we show that pretraining with
distillation yields models that exhibit remarkably better test-time scaling.
Second, we observe that this benefit comes with a trade-off: distillation
impairs in-context learning capabilities, particularly the one modeled via
induction heads. Third, to demystify these findings, we study distilled
pretraining in a sandbox of a bigram model, which helps us isolate the common
principal factor behind our observations. Finally, using these insights, we
shed light on various design choices for pretraining that should help
practitioners going forward.

</details>


### [329] [Reinforcement Learning for Machine Learning Engineering Agents](https://arxiv.org/abs/2509.01684)
*Sherry Yang,Joy He-Yueya,Percy Liang*

Main category: cs.LG

TL;DR: 本文指出基于强化学习（RL）的较弱模型代理可优于基于静态大模型的代理，提出解决RL两大挑战的方法，实验显示小模型经RL训练后表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示强大语言模型的代理无法随经验提升性能，因此探索基于RL的较弱模型代理的可能性。

Method: 提出分布式异步RL框架中的持续时间感知梯度更新以处理可变时长动作；提出环境检测以提供部分奖励反馈。

Result: 在MLEBench上的实验表明，经RL训练的小模型（Qwen2.5 - 3B）平均比大模型（Claude - 3.5 - Sonnet）在12个Kaggle任务上性能高22%。

Conclusion: 基于RL的较弱模型代理可通过解决相关挑战，实现优于基于静态大模型的代理的性能。

Abstract: Existing agents for solving tasks such as ML engineering rely on prompting
powerful language models. As a result, these agents do not improve with more
experience. In this paper, we show that agents backed by weaker models that
improve via reinforcement learning (RL) can outperform agents backed by much
larger, but static models. We identify two major challenges with RL in this
setting. First, actions can take a variable amount of time (e.g., executing
code for different solutions), which leads to asynchronous policy gradient
updates that favor faster but suboptimal solutions. To tackle variable-duration
actions, we propose duration- aware gradient updates in a distributed
asynchronous RL framework to amplify high-cost but high-reward actions. Second,
using only test split performance as a reward provides limited feedback. A
program that is nearly correct is treated the same as one that fails entirely.
To address this, we propose environment instrumentation to offer partial
credit, distinguishing almost-correct programs from those that fail early
(e.g., during data loading). Environment instrumentation uses a separate static
language model to insert print statement to an existing program to log the
agent's experimental progress, from which partial credit can be extracted as
reward signals for learning. Our experimental results on MLEBench suggest that
performing gradient updates on a much smaller model (Qwen2.5-3B) trained with
RL outperforms prompting a much larger model (Claude-3.5-Sonnet) with agent
scaffolds, by an average of 22% across 12 Kaggle tasks.

</details>


### [330] [Robust Anomaly Detection through Multi-Modal Autoencoder Fusion for Small Vehicle Damage Detection](https://arxiv.org/abs/2509.01719)
*Sara Khan,Mehmed Yüksel,Frank Kirchner*

Main category: cs.LG

TL;DR: 本文提出基于异常检测的多模态架构用于车队和共享车辆系统的磨损检测，集成传感器实现实时检测，多模态模型性能佳，还可拓展到其他应用。


<details>
  <summary>Details</summary>
Motivation: 现有车队和共享车辆系统的磨损检测中，人工检查费力且易出错，基于图像的方法实时性能差、检测底盘损伤效果不佳。

Method: 引入基于异常检测的多模态架构，将IMU和麦克风等传感器集成到安装在挡风玻璃上的紧凑型设备，开发多种基于多模态自编码器的架构变体并与单模态和现有方法对比。

Result: 集成池化多模态模型性能最佳，ROC - AUC达92%。

Conclusion: 该方法在现实应用中有效，还可拓展到提高汽车安全性、辅助自动驾驶车辆碰撞检测等应用。

Abstract: Wear and tear detection in fleet and shared vehicle systems is a critical
challenge, particularly in rental and car-sharing services, where minor damage,
such as dents, scratches, and underbody impacts, often goes unnoticed or is
detected too late. Currently, manual inspection methods are the default
approach but are labour intensive and prone to human error. In contrast,
state-of-the-art image-based methods struggle with real-time performance and
are less effective at detecting underbody damage due to limited visual access
and poor spatial coverage. This work introduces a novel multi-modal
architecture based on anomaly detection to address these issues. Sensors such
as IMUs and microphones are integrated into a compact device mounted on the
vehicle's windshield. This approach supports real-time damage detection while
avoiding the need for highly resource-intensive sensors. We developed multiple
variants of multi-modal autoencoder-based architectures and evaluated them
against unimodal and state-of-the-art methods. Our ensemble pooling multi-modal
model achieved the highest performance, with a Receiver Operating
Characteristic-Area Under Curve (ROC-AUC) of 92%, demonstrating its
effectiveness in real-world applications. This approach can also be extended to
other applications, such as improving automotive safety - where it can
integrate with airbag systems for efficient deployment - and helping autonomous
vehicles by complementing other sensors in collision detection.

</details>


### [331] [Succeed or Learn Slowly: Sample Efficient Off-Policy Reinforcement Learning for Mobile App Control](https://arxiv.org/abs/2509.01720)
*Georgios Papoudakis,Thomas Coste,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.LG

TL;DR: 本文提出新的离策略强化学习算法SoLS用于多轮任务中基础模型的策略近似，在安卓应用控制任务评估中显著优于现有方法，且计算资源需求低、推理快。


<details>
  <summary>Details</summary>
Motivation: 解决在多轮任务中使用基础模型进行策略近似的强化学习面临的稀疏奖励设置和策略梯度更新问题。

Method: 提出SoLS算法，采用修改后的离策略演员 - 评论家方法，对正样本直接更新策略，对负样本保守正则化更新；并用成功转换重放（STR）增强SoLS。

Result: 在AndroidWorld基准测试中，SoLS显著优于现有方法，相对提升至少17%，推理速度比基于GPT - 4o的方法快5 - 60倍，且计算资源需求少。

Conclusion: SoLS算法在多轮任务基础模型策略近似上有效，能提高样本效率，减少计算资源消耗。

Abstract: Reinforcement learning (RL) using foundation models for policy approximations
in multi-turn tasks remains challenging. We identify two main limitations
related to sparse reward settings and policy gradient updates, based on which
we formulate a key insight: updates from positive samples with high returns
typically do not require policy regularisation, whereas updates from negative
samples, reflecting undesirable behaviour, can harm model performance. This
paper introduces Succeed or Learn Slowly (SoLS), a novel off-policy RL
algorithm evaluated on mobile app control tasks. SoLS improves sample
efficiency when fine-tuning foundation models for user interface navigation via
a modified off-policy actor-critic approach, applying direct policy updates for
positive samples and conservative, regularised updates for negative ones to
prevent model degradation. We augment SoLS with Successful Transition Replay
(STR), which prioritises learning from successful interactions, further
improving sample efficiency. We evaluate SoLS on the AndroidWorld benchmark,
where it significantly outperforms existing methods (at least 17% relative
increase), including prompt-engineering and RL approaches, while requiring
substantially fewer computational resources than GPT-4o-based methods with
5-60x faster inference.

</details>


### [332] [Convolutional Monge Mapping between EEG Datasets to Support Independent Component Labeling](https://arxiv.org/abs/2509.01721)
*Austin Meek,Carlos H. Mendoza-Cardenas,Austin J. Brockmeier*

Main category: cs.LG

TL;DR: 提出CMMN方法的扩展，用于EEG信号谱归一化，应用于IC分类任务有显著效果。


<details>
  <summary>Details</summary>
Motivation: EEG记录有伪影、噪声等问题，现有方法受设备和环境影响，需合适谱归一化减少差异。

Method: 提出CMMN方法的两种扩展方式计算源参考谱，得到时空可分离滤波器。

Result: 应用滤波器在IC分类任务中，识别脑与非脑独立成分有显著改善。

Conclusion: 合适的谱归一化可通过滤波减少记录设备和环境差异对机器学习模型性能的影响。

Abstract: EEG recordings contain rich information about neural activity but are subject
to artifacts, noise, and superficial differences due to sensors, amplifiers,
and filtering. Independent component analysis and automatic labeling of
independent components (ICs) enable artifact removal in EEG pipelines.
Convolutional Monge Mapping Normalization (CMMN) is a recent tool used to
achieve spectral conformity of EEG signals, which was shown to improve deep
neural network approaches for sleep staging. Here we propose a novel extension
of the CMMN method with two alternative approaches to computing the source
reference spectrum the target signals are mapped to: (1) channel-averaged and
$l_1$-normalized barycenter, and (2) a subject-to-subject mapping that finds
the source subject with the closest spectrum to the target subject. Notably,
our extension yields space-time separable filters that can be used to map
between datasets with different numbers of EEG channels. We apply these filters
in an IC classification task, and show significant improvement in recognizing
brain versus non-brain ICs.
  Clinical relevance - EEG recordings are used in the diagnosis and monitoring
of multiple neuropathologies, including epilepsy and psychosis. While EEG
analysis can benefit from automating artifact removal through independent
component analysis and labeling, differences in recording equipment and context
(the presence of noise from electrical wiring and other devices) may impact the
performance of machine learning models, but these differences can be minimized
by appropriate spectral normalization through filtering.

</details>


### [333] [BM-CL: Bias Mitigation through the lens of Continual Learning](https://arxiv.org/abs/2509.01730)
*Lucas Mansilla,Rodrigo Echeveste,Camila Gonzalez,Diego H. Milone,Enzo Ferrante*

Main category: cs.LG

TL;DR: 本文提出BM - CL框架，利用持续学习原理缓解机器学习中的偏差，实验证明该框架有效，为开发公平有效的机器学习系统提供途径。


<details>
  <summary>Details</summary>
Motivation: 传统偏差缓解技术存在‘拉平效应’，即改善弱势群体结果会降低优势群体性能，需解决此权衡问题。

Method: 借鉴Learning without Forgetting和Elastic Weight Consolidation等技术，将偏差缓解重新解释为持续学习问题，使模型逐步平衡公平目标。

Result: 在合成和真实图像数据集上的实验表明，该框架能缓解偏差，同时最小化原始知识的损失。

Conclusion: 所提方法连接了公平性和持续学习领域，为开发公平有效的机器学习系统提供了有前景的途径。

Abstract: Biases in machine learning pose significant challenges, particularly when
models amplify disparities that affect disadvantaged groups. Traditional bias
mitigation techniques often lead to a {\itshape leveling-down effect}, whereby
improving outcomes of disadvantaged groups comes at the expense of reduced
performance for advantaged groups. This study introduces Bias Mitigation
through Continual Learning (BM-CL), a novel framework that leverages the
principles of continual learning to address this trade-off. We postulate that
mitigating bias is conceptually similar to domain-incremental continual
learning, where the model must adjust to changing fairness conditions,
improving outcomes for disadvantaged groups without forgetting the knowledge
that benefits advantaged groups. Drawing inspiration from techniques such as
Learning without Forgetting and Elastic Weight Consolidation, we reinterpret
bias mitigation as a continual learning problem. This perspective allows models
to incrementally balance fairness objectives, enhancing outcomes for
disadvantaged groups while preserving performance for advantaged groups.
Experiments on synthetic and real-world image datasets, characterized by
diverse sources of bias, demonstrate that the proposed framework mitigates
biases while minimizing the loss of original knowledge. Our approach bridges
the fields of fairness and continual learning, offering a promising pathway for
developing machine learning systems that are both equitable and effective.

</details>


### [334] [Communication-Aware Knowledge Distillation for Federated LLM Fine-Tuning over Wireless Networks](https://arxiv.org/abs/2509.01750)
*Xinlu Zhang,Na Yan,Yang Su,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 论文聚焦高效通信开销的联邦大语言模型蒸馏，提出自适应Top - k逻辑选择机制和聚合方案，结合LoRA适配隐藏层投影，实验显示性能优且降低约50%通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习参数共享方法通信开销高、难适应异构架构，联邦蒸馏传输大语言模型逻辑时对带宽有限客户端有挑战，需解决高效通信问题。

Method: 提出自适应Top - k逻辑选择机制动态稀疏化逻辑；设计自适应逻辑聚合方案缓解维度不一致；将LoRA适配隐藏层投影纳入蒸馏损失。

Result: 方案性能优于基线方法，有效降低约50%通信开销。

Conclusion: 所提方案在降低通信开销上有效且性能良好。

Abstract: Federated learning (FL) for large language models (LLMs) offers a
privacy-preserving scheme, enabling clients to collaboratively fine-tune
locally deployed LLMs or smaller language models (SLMs) without exchanging raw
data. While parameter-sharing methods in traditional FL models solves number of
technical challenges, they still incur high communication overhead and struggle
with adapting to heterogeneous model architectures. Federated distillation, a
framework for mutual knowledge transfer via shared logits, typically offers
lower communication overhead than parameter-sharing methods. However,
transmitting logits from LLMs remains challenging for bandwidth-limited clients
due to their high dimensionality. In this work, we focus on a federated LLM
distillation with efficient communication overhead. To achieve this, we first
propose an adaptive Top-k logit selection mechanism, dynamically sparsifying
logits according to real-time communication conditions. Then to tackle the
dimensional inconsistency introduced by the adaptive sparsification, we design
an adaptive logits aggregation scheme, effectively alleviating the artificial
and uninformative inputs introduced by conventional zero-padding methods.
Finally, to enhance the distillation effect, we incorporate LoRA-adapted
hidden-layer projection from LLM into the distillation loss, reducing the
communication overhead further while providing richer representation.
Experimental results demonstrate that our scheme achieves superior performance
compared to baseline methods while effectively reducing communication overhead
by approximately 50%.

</details>


### [335] [Toward a Unified Benchmark and Taxonomy of Stochastic Environments](https://arxiv.org/abs/2509.01793)
*Aryan Amit Barsainyan,Jing Yu Lim,Dianbo Liu*

Main category: cs.LG

TL;DR: 现有RL基准难以评估真实环境鲁棒性，本文引入STORI基准并提出随机性分类法。


<details>
  <summary>Details</summary>
Motivation: 当前RL代理在真实环境鲁棒性不足，现有基准难以捕捉真实挑战，且缺乏随机性分类。

Method: 引入STORI基准，该基准包含多种随机效应；提出RL环境随机性分类法。

Result: 未提及具体结果。

Conclusion: STORI基准可对不同形式不确定性下的RL方法进行严格评估，随机性分类法为分析和比较方法提供统一框架。

Abstract: Reinforcement Learning (RL) agents have achieved strong results on benchmarks
such as Atari100k, yet they remain limited in robustness to real-world
conditions. Model-Based RL approaches that rely on learned World Models often
struggle in environments with true stochasticity and partial observability,
despite their theoretical grounding in POMDPs. Current benchmarks rarely
capture these challenges, focusing instead on deterministic or overly
simplified settings, and the lack of a clear taxonomy of stochasticity further
hampers systematic evaluation. To address this gap, we introduce STORI
(STOchastic-ataRI), a benchmark that incorporates diverse stochastic effects
and enables rigorous assessment of RL methods under varied forms of
uncertainty. In addition, we propose a taxonomy of stochasticity in RL
environments, providing a unified framework for analyzing and comparing
approaches.

</details>


### [336] [A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics](https://arxiv.org/abs/2509.01794)
*Trusting Inekwe,Emmanuel Agu,Winnie Mkandawire,Andres Colubri*

Main category: cs.LG

TL;DR: 新冠疫情影响心血管疾病（CVD）生物标志物，本文提出MBT - CB模型预测CVD生物标志物，表现优于基线模型，有改善预测和支持临床决策潜力。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情破坏医疗系统，影响CVD生物标志物，此前工作未用机器学习对电子健康记录（EHRs）进行多目标CVD生物标志物预测并兼顾多种因素，需要准确建模以预测疾病进展和指导预防护理。

Method: 提出MBT - CB模型，结合多目标贝叶斯Transformer和预训练BERT框架，利用贝叶斯变分推理估计不确定性，通过嵌入捕获时间关系，用DeepMTR模型捕获生物标志物相互关系。

Result: 在3390份CVD患者记录（304名独特患者）的回顾性EHR数据上评估，MBT - CB优于包括其他基于BERT的机器学习模型在内的基线模型，MAE为0.00887，RMSE为0.0135，MSE为0.00027，能有效捕获数据和模型不确定性、患者生物标志物相互关系和时间动态。

Conclusion: MBT - CB表现出色，有潜力改善CVD生物标志物预测，支持疫情期间临床决策。

Abstract: The COVID-19 pandemic disrupted healthcare systems worldwide,
disproportionately impacting individuals with chronic conditions such as
cardiovascular disease (CVD). These disruptions -- through delayed care and
behavioral changes, affected key CVD biomarkers, including LDL cholesterol
(LDL-C), HbA1c, BMI, and systolic blood pressure (SysBP). Accurate modeling of
these changes is crucial for predicting disease progression and guiding
preventive care. However, prior work has not addressed multi-target prediction
of CVD biomarker from Electronic Health Records (EHRs) using machine learning
(ML), while jointly capturing biomarker interdependencies, temporal patterns,
and predictive uncertainty. In this paper, we propose MBT-CB, a Multi-target
Bayesian Transformer (MBT) with pre-trained BERT-based transformer framework to
jointly predict LDL-C, HbA1c, BMI and SysBP CVD biomarkers from EHR data. The
model leverages Bayesian Variational Inference to estimate uncertainties,
embeddings to capture temporal relationships and a DeepMTR model to capture
biomarker inter-relationships. We evaluate MBT-CT on retrospective EHR data
from 3,390 CVD patient records (304 unique patients) in Central Massachusetts
during the Covid-19 pandemic. MBT-CB outperformed a comprehensive set of
baselines including other BERT-based ML models, achieving an MAE of 0.00887,
RMSE of 0.0135 and MSE of 0.00027, while effectively capturing data and model
uncertainty, patient biomarker inter-relationships, and temporal dynamics via
its attention and embedding mechanisms. MBT-CB's superior performance
highlights its potential to improve CVD biomarker prediction and support
clinical decision-making during pandemics.

</details>


### [337] [When LLM Meets Time Series: Can LLMs Perform Multi-Step Time Series Reasoning and Inference](https://arxiv.org/abs/2509.01822)
*Wen Ye,Jinbo Liu,Defu Cao,Wei Yang,Yan Liu*

Main category: cs.LG

TL;DR: 本文介绍用于评估大语言模型作为时间序列AI助手的TSAIA基准，用其评估八个模型，揭示当前模型在构建复杂时间序列分析工作流上的局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现实应用领域对时间序列数据进行复杂推理的能力未被充分探索，需要建立严格的基准数据集进行评估。

Method: 调研20多篇学术出版物确定33种现实任务表述，构建涵盖多种挑战的基准，采用特定任务成功标准和推理质量指标，在统一协议下评估八个模型。

Result: 分析揭示当前模型在构建复杂时间序列分析工作流方面存在局限。

Conclusion: 需要针对特定领域的适配开发专门方法，基准和代码已开源。

Abstract: The rapid advancement of Large Language Models (LLMs) has sparked growing
interest in their application to time series analysis tasks. However, their
ability to perform complex reasoning over temporal data in real-world
application domains remains underexplored. To move toward this goal, a first
step is to establish a rigorous benchmark dataset for evaluation. In this work,
we introduce the TSAIA Benchmark, a first attempt to evaluate LLMs as
time-series AI assistants. To ensure both scientific rigor and practical
relevance, we surveyed over 20 academic publications and identified 33
real-world task formulations. The benchmark encompasses a broad spectrum of
challenges, ranging from constraint-aware forecasting to anomaly detection with
threshold calibration: tasks that require compositional reasoning and
multi-step time series analysis. The question generator is designed to be
dynamic and extensible, supporting continuous expansion as new datasets or task
types are introduced. Given the heterogeneous nature of the tasks, we adopt
task-specific success criteria and tailored inference-quality metrics to ensure
meaningful evaluation for each task. We apply this benchmark to assess eight
state-of-the-art LLMs under a unified evaluation protocol. Our analysis reveals
limitations in current models' ability to assemble complex time series analysis
workflows, underscoring the need for specialized methodologies for
domain-specific adaptation. Our benchmark is available at
https://huggingface.co/datasets/Melady/TSAIA, and the code is available at
https://github.com/USC-Melady/TSAIA.

</details>


### [338] [Goal-Conditioned Reinforcement Learning for Data-Driven Maritime Navigation](https://arxiv.org/abs/2509.01838)
*Vaishnav Vaidheeswaran,Dilith Jayakody,Samruddhi Mulay,Anand Lo,Md Mahbub Alam,Gabriel Spadon*

Main category: cs.LG

TL;DR: 提出适用于大型海事数据的强化学习解决方案，在圣劳伦斯湾验证，实验表明动作掩码和正向塑造奖励能提升策略性能。


<details>
  <summary>Details</summary>
Motivation: 现有船舶路由研究难以跨多起点 - 终点对泛化，未利用大规模数据驱动的交通图，因此需新解决方案。

Method: 提出强化学习解决方案，代理在多离散动作空间中根据连续观测选择方向和速度，使用奖励函数平衡多因素，结合多种策略评估配置。

Result: 动作掩码使策略性能明显提升，正向塑造奖励能带来额外收益。

Conclusion: 所提强化学习解决方案有效，动作掩码和正向塑造奖励对提升性能有积极作用。

Abstract: Routing vessels through narrow and dynamic waterways is challenging due to
changing environmental conditions and operational constraints. Existing
vessel-routing studies typically fail to generalize across multiple
origin-destination pairs and do not exploit large-scale, data-driven traffic
graphs. In this paper, we propose a reinforcement learning solution for big
maritime data that can learn to find a route across multiple origin-destination
pairs while adapting to different hexagonal grid resolutions. Agents learn to
select direction and speed under continuous observations in a multi-discrete
action space. A reward function balances fuel efficiency, travel time, wind
resistance, and route diversity, using an Automatic Identification System
(AIS)-derived traffic graph with ERA5 wind fields. The approach is demonstrated
in the Gulf of St. Lawrence, one of the largest estuaries in the world. We
evaluate configurations that combine Proximal Policy Optimization with
recurrent networks, invalid-action masking, and exploration strategies. Our
experiments demonstrate that action masking yields a clear improvement in
policy performance and that supplementing penalty-only feedback with positive
shaping rewards produces additional gains.

</details>


### [339] [Optimizing In-Context Learning for Efficient Full Conformal Prediction](https://arxiv.org/abs/2509.01840)
*Weicao Deng,Sangwoo Park,Min Li,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文提出高效FCP框架E - ICL+FCP，减少低效性和计算开销，实验显示其在效率 - 覆盖率权衡上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有Conformal Prediction（CP）的两种主要变体Split CP（SCP）和full CP（FCP）分别存在数据低效和再训练复杂度高的问题，基于元学习或上下文学习（ICL）的方法有局限性。

Method: 引入增强的基于ICL的FCP框架E - ICL+FCP，采用基于排列不变的Transformer的ICL模型，并使用CP感知损失进行训练，模拟FCP所需的多个再训练模型而无需实际再训练。

Result: 在合成和真实任务的实验中，E - ICL+FCP在效率 - 覆盖率权衡上优于现有的SCP和FCP基线。

Conclusion: E - ICL+FCP能在保持覆盖率的同时，显著减少低效性和计算开销，实现更优的效率 - 覆盖率权衡。

Abstract: Reliable uncertainty quantification is critical for trustworthy AI. Conformal
Prediction (CP) provides prediction sets with distribution-free coverage
guarantees, but its two main variants face complementary limitations. Split CP
(SCP) suffers from data inefficiency due to dataset partitioning, while full CP
(FCP) improves data efficiency at the cost of prohibitive retraining
complexity. Recent approaches based on meta-learning or in-context learning
(ICL) partially mitigate these drawbacks. However, they rely on training
procedures not specifically tailored to CP, which may yield large prediction
sets. We introduce an efficient FCP framework, termed enhanced ICL-based FCP
(E-ICL+FCP), which employs a permutation-invariant Transformer-based ICL model
trained with a CP-aware loss. By simulating the multiple retrained models
required by FCP without actual retraining, E-ICL+FCP preserves coverage while
markedly reducing both inefficiency and computational overhead. Experiments on
synthetic and real tasks demonstrate that E-ICL+FCP attains superior
efficiency-coverage trade-offs compared to existing SCP and FCP baselines.

</details>


### [340] [GradES: Significantly Faster Training in Transformers with Gradient-Based Early Stopping](https://arxiv.org/abs/2509.01842)
*Qifu Wen,Xi Zeng,Zihan Zhou,Shuaijun Liu,Mehdi Hosseinzadeh,Reza Rawassizadeh*

Main category: cs.LG

TL;DR: 提出基于梯度的早期停止方法GradES，在训练中跟踪矩阵梯度，当梯度低于阈值时单独停止更新参数，加速训练并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统早期停止方法对大型transformer计算成本高，需长时间验证推理。

Method: GradES在transformer组件内操作，跟踪反向传播中矩阵梯度，当梯度低于阈值时单独停止更新投影矩阵。

Result: GradES将训练时间加速1.57 - 7.22倍，平均准确率提高1.2%。

Conclusion: GradES通过策略性冻结收敛参数，加速训练并通过防止过拟合提升泛化能力。

Abstract: Early stopping monitors global validation loss and halts all parameter
updates simultaneously, which is computationally costly for large transformers
due to the extended time required for validation inference. We propose GradES,
a novel gradient-based early stopping approach that operates within transformer
components (attention projections and Feed-Forward layer matrices). We found
that different components converge at varying rates during fine-tuning. GradES
tracks the magnitude of gradients in backpropagation for these matrices during
training. When a projection matrix's gradients fall below a convergence
threshold $\tau$, we exclude that projection matrix from further updates
individually, eliminating costly validation passes while allowing slow
converging matrices to continue learning. By strategically freezing parameters
when their gradients converge, GradES speeds up training time by
1.57--7.22$\times$ while simultaneously enhancing generalization through early
prevention of overfitting, resulting in 1.2% higher average accuracy.

</details>


### [341] [Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic Activation Function](https://arxiv.org/abs/2509.01874)
*Jason Abohwo,Thomas Mosen*

Main category: cs.LG

TL;DR: 提出Signed Quadratic Shrink (SQS)激活函数，使门控线性单元（GLUs）学习可解释特征，实验显示性能有竞争力且支持基于权重的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有通过权重分析模型特征的技术存在性能降低和数据效率低等缺点，需要更好方法来确保机器学习模型可靠性和鲁棒性。

Method: 引入Signed Quadratic Shrink (SQS)激活函数，用于Gated Linear Units (GLUs)学习可解释特征。

Result: SQS与最先进的激活函数相比，性能具有竞争力，同时支持基于权重的可解释性。

Conclusion: SQS是一种能让GLUs学习可解释特征且无现有技术缺点的有效激活函数。

Abstract: Understanding the inner workings of machine learning models is critical for
ensuring their reliability and robustness. Whilst many techniques in
mechanistic interpretability focus on activation driven analyses, being able to
derive meaningful features directly from the weights of a neural network would
provide greater guarantees and more computational efficiency. Existing
techniques for analyzing model features through weights suffer from drawbacks
such as reduced performance and data inefficiency. In this paper, we introduce
Signed Quadratic Shrink (SQS), an activation function designed to allow Gated
Linear Units (GLUs) to learn interpretable features without these drawbacks.
Our experimental results show that SQS achieves performance competitive with
state-of-the-art activation functions whilst enabling weight-based
interpretability

</details>


### [342] [Semi-on-Demand Transit Feeders with Shared Autonomous Vehicles and Reinforcement-Learning-Based Zonal Dispatching Control](https://arxiv.org/abs/2509.01883)
*Max T. M. Ng,Roman Engelhardt,Florian Dandl,Hani S. Mahmassani,Klaus Bogenberger*

Main category: cs.LG

TL;DR: 本文提出基于共享自动驾驶车辆和强化学习的半按需公交接驳服务，通过模拟验证其效果并展示应用潜力。


<details>
  <summary>Details</summary>
Motivation: 结合固定路线公交成本效益和按需响应运输适应性，提高低密度地区可达性，解决多模式交通系统首末英里问题。

Method: 开发半按需公交接驳服务，使用基于强化学习的区域调度控制，用近端策略优化算法动态分配车辆，通过基于智能体的模拟验证。

Result: 强化学习模型有效训练后，半按需服务比传统固定路线服务多服务16%乘客，综合成本高13%；强化学习控制带来2.4%更多乘客，成本高1.4%。

Conclusion: 展示了将共享自动驾驶接驳车和机器学习技术集成到公共交通的潜力，为解决多模式交通首末英里问题创新奠定基础。

Abstract: This paper develops a semi-on-demand transit feeder service using shared
autonomous vehicles (SAVs) and zonal dispatching control based on reinforcement
learning (RL). This service combines the cost-effectiveness of fixed-route
transit with the adaptability of demand-responsive transport to improve
accessibility in lower-density areas. Departing from the terminus, SAVs first
make scheduled fixed stops, then offer on-demand pick-ups and drop-offs in a
pre-determined flexible-route area. Our deep RL model dynamically assigns
vehicles to subdivided flexible-route zones in response to real-time demand
fluctuations and operations, using a policy gradient algorithm - Proximal
Policy Optimization. The methodology is demonstrated through agent-based
simulations on a real-world bus route in Munich, Germany. Results show that
after efficient training of the RL model, the semi-on-demand service with
dynamic zonal control serves 16% more passengers at 13% higher generalized
costs on average compared to traditional fixed-route service. The efficiency
gain brought by RL control brings 2.4% more passengers at 1.4% higher costs.
This study not only showcases the potential of integrating SAV feeders and
machine learning techniques into public transit, but also sets the groundwork
for further innovations in addressing first-mile-last-mile problems in
multimodal transit systems.

</details>


### [343] [Deep Reinforcement Learning for Real-Time Drone Routing in Post-Disaster Road Assessment Without Domain Knowledge](https://arxiv.org/abs/2509.01886)
*Huatian Gong,Jiuh-Biing Sheu,Zheng Wang,Xiaoguang Yang,Ran Yan*

Main category: cs.LG

TL;DR: 提出基于注意力的编码器 - 解码器模型（AEDM）用于灾后道路损坏评估的无人机实时路由决策，实验显示其在解质量和推理时间上优于传统方法，适用于时间敏感的灾害响应。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法计算时间长且需领域知识，不适用于时间敏感的灾害场景，因此需新方法进行灾后道路损坏评估的无人机实时路由决策。

Method: 采用深度强化学习确定无人机评估路线，开发网络转换方法将基于链路的路由问题转换为基于节点的公式，用合成道路网络生成技术解决训练数据集稀缺问题，使用具有多任务学习能力的多最优策略优化（POMO）训练模型。

Result: AEDM在解质量上比商业求解器高16 - 69%，推理时间为1 - 2秒，远快于传统方法的100 - 2000秒，在不同问题规模、无人机数量和时间约束下有强泛化能力。

Conclusion: 该方法有效平衡了计算效率和解质量，适用于时间关键的灾害响应应用。

Abstract: Rapid post-disaster road damage assessment is critical for effective
emergency response, yet traditional optimization methods suffer from excessive
computational time and require domain knowledge for algorithm design, making
them unsuitable for time-sensitive disaster scenarios. This study proposes an
attention-based encoder-decoder model (AEDM) for real-time drone routing
decision in post-disaster road damage assessment. The method employs deep
reinforcement learning to determine high-quality drone assessment routes
without requiring algorithmic design knowledge. A network transformation method
is developed to convert link-based routing problems into equivalent node-based
formulations, while a synthetic road network generation technique addresses the
scarcity of large-scale training datasets. The model is trained using policy
optimization with multiple optima (POMO) with multi-task learning capabilities
to handle diverse parameter combinations. Experimental results demonstrate two
key strengths of AEDM: it outperforms commercial solvers by 16--69\% in
solution quality and achieves real-time inference (1--2 seconds) versus
100--2,000 seconds for traditional methods. The model exhibits strong
generalization across varying problem scales, drone numbers, and time
constraints, consistently outperforming baseline methods on unseen parameter
distributions and real-world road networks. The proposed method effectively
balances computational efficiency with solution quality, making it particularly
suitable for time-critical disaster response applications where rapid
decision-making is essential for saving lives.

</details>


### [344] [Predicting NCAP Safety Ratings: An Analysis of Vehicle Characteristics and ADAS Features Using Machine Learning](https://arxiv.org/abs/2509.01897)
*Raunak Kunwar,Aera Kim LeBoulluec*

Main category: cs.LG

TL;DR: 研究用机器学习分析NCAP数据，探讨ADAS特征和车辆属性对预测车辆获5星NCAP评级的作用，优化随机森林模型效果好。


<details>
  <summary>Details</summary>
Motivation: 车辆安全评估重要，NCAP评级纳入ADAS技术，需了解各系统实证交互情况，研究能否用ADAS特征和车辆属性预测5星评级。

Method: 用含约5128个车辆变体的NCAP公开数据集，对比逻辑回归、随机森林、梯度提升和支持向量分类器4种机器学习模型，用5折分层交叉验证，对随机森林和梯度提升进行超参数优化。

Result: 特征重要性分析显示基本车辆特征主导预测能力，ADAS特征也有贡献，优化随机森林模型在测试集上准确率89.18%，ROC AUC为0.9586。

Conclusion: 可用机器学习分析大规模NCAP数据，既定车辆参数和现代ADAS特征对获得顶级安全评级有联合预测重要性。

Abstract: Vehicle safety assessment is crucial for consumer information and regulatory
oversight. The New Car Assessment Program (NCAP) assigns standardized safety
ratings, which traditionally emphasize passive safety measures but now include
active safety technologies such as Advanced Driver-Assistance Systems (ADAS).
It is crucial to understand how these various systems interact empirically.
This study explores whether particular ADAS features like Forward Collision
Warning, Lane Departure Warning, Crash Imminent Braking, and Blind Spot
Detection, together with established vehicle attributes (e.g., Curb Weight,
Model Year, Vehicle Type, Drive Train), can reliably predict a vehicle's
likelihood of earning the highest (5-star) overall NCAP rating. Using a
publicly available dataset derived from NCAP reports that contain approximately
5,128 vehicle variants spanning model years 2011-2025, we compared four
different machine learning models: logistic regression, random forest, gradient
boosting, and support vector classifier (SVC) using a 5-fold stratified
cross-validation approach. The two best-performing algorithms (random forest
and gradient boost) were hyperparameter optimized using RandomizedSearchCV.
Analysis of feature importance showed that basic vehicle characteristics,
specifically curb weight and model year, dominated predictive capability,
contributing more than 55% of the feature relevance of the Random Forest model.
However, the inclusion of ADAS features also provided meaningful predictive
contributions. The optimized Random Forest model achieved robust results on a
held-out test set, with an accuracy of 89.18% and a ROC AUC of 0.9586. This
research reveals the use of machine learning to analyze large-scale NCAP data
and highlights the combined predictive importance of both established vehicle
parameters and modern ADAS features to achieve top safety ratings.

</details>


### [345] [VISP: Volatility Informed Stochastic Projection for Adaptive Regularization](https://arxiv.org/abs/2509.01903)
*Tanvir Islam*

Main category: cs.LG

TL;DR: 提出VISP自适应正则化方法，利用梯度波动性引导随机噪声注入，实验表明能提升泛化性能、稳定网络内部动态和促进更鲁棒特征表示。


<details>
  <summary>Details</summary>
Motivation: 传统正则化技术使用均匀噪声或固定丢弃率，为解决过拟合问题，提出新的自适应正则化方法。

Method: 提出VISP方法，从梯度统计中动态计算波动性，并用于缩放随机投影矩阵，选择性地正则化梯度波动性较高的输入和隐藏节点。

Result: 在MNIST、CIFAR - 10和SVHN上的实验显示，VISP持续提升了泛化性能，优于基线模型和固定噪声替代方案；详细分析表明其稳定了网络内部动态，促进了更鲁棒的特征表示。

Conclusion: VISP是一种有效的自适应正则化方法，能改善深度学习模型的泛化能力和特征表示。

Abstract: We propose VISP: Volatility Informed Stochastic Projection, an adaptive
regularization method that leverages gradient volatility to guide stochastic
noise injection in deep neural networks. Unlike conventional techniques that
apply uniform noise or fixed dropout rates, VISP dynamically computes
volatility from gradient statistics and uses it to scale a stochastic
projection matrix. This mechanism selectively regularizes inputs and hidden
nodes that exhibit higher gradient volatility while preserving stable
representations, thereby mitigating overfitting. Extensive experiments on
MNIST, CIFAR-10, and SVHN demonstrate that VISP consistently improves
generalization performance over baseline models and fixed-noise alternatives.
In addition, detailed analyses of the evolution of volatility, the spectral
properties of the projection matrix, and activation distributions reveal that
VISP not only stabilizes the internal dynamics of the network but also fosters
a more robust feature representation.

</details>


### [346] [Causal representation learning from network data](https://arxiv.org/abs/2509.01916)
*Jifan Zhang,Michelle M. Li,Elena Zheleva*

Main category: cs.LG

TL;DR: 本文提出GraCE - VAE框架用于非独立同分布（non - i.i.d.）设置下的因果解纠缠，理论证明其可识别性，并在数据集上评估。


<details>
  <summary>Details</summary>
Motivation: 以往研究从独立同分布数据角度研究因果解纠缠，本文针对非独立同分布设置且有网络数据的情况开展研究。

Method: 开发GraCE - VAE框架，将基于差异的变分自编码器与图神经网络结合，联合恢复真实潜在因果图和干预效果。

Result: 证明独立同分布数据可识别性的理论结果在本文设置下成立，在三个基因扰动数据集上对GraCE - VAE与现有基线进行评估。

Conclusion: 利用结构化上下文进行因果解纠缠具有积极影响。

Abstract: Causal disentanglement from soft interventions is identifiable under the
assumptions of linear interventional faithfulness and availability of both
observational and interventional data. Previous research has looked into this
problem from the perspective of i.i.d. data. Here, we develop a framework,
GraCE-VAE, for non-i.i.d. settings, in which structured context in the form of
network data is available. GraCE-VAE integrates discrepancy-based variational
autoencoders with graph neural networks to jointly recover the true latent
causal graph and intervention effects. We show that the theoretical results of
identifiability from i.i.d. data hold in our setup. We also empirically
evaluate GraCE-VAE against state-of-the-art baselines on three genetic
perturbation datasets to demonstrate the impact of leveraging structured
context for causal disentanglement.

</details>


### [347] [A Continuous Encoding-Based Representation for Efficient Multi-Fidelity Multi-Objective Neural Architecture Search](https://arxiv.org/abs/2509.01943)
*Zhao Wei,Chin Chun Ooi,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 提出自适应Co - Kriging辅助多保真度多目标NAS算法，降低计算成本，在多个问题上表现优于现有方法，还用于创建风速回归模型。


<details>
  <summary>Details</summary>
Motivation: 解决神经架构搜索（NAS）在优化多个冲突目标时计算成本高的问题。

Method: 提出自适应Co - Kriging辅助多保真度多目标NAS算法，结合基于聚类的局部多保真度填充采样策略，使用新的连续编码方法降低搜索维度。

Result: 在三个数值基准、2D达西流回归问题和CHASE_DB1生物医学图像分割问题上，提出的NAS算法在有限计算预算下优于现有方法；创建的风速回归模型能以较低计算复杂度实现良好预测。

Conclusion: 该NAS算法有效，能在降低计算成本的同时取得较好的架构搜索效果，还能独立识别出优秀U - Net架构的原则。

Abstract: Neural architecture search (NAS) is an attractive approach to automate the
design of optimized architectures but is constrained by high computational
budget, especially when optimizing for multiple, important conflicting
objectives. To address this, an adaptive Co-Kriging-assisted multi-fidelity
multi-objective NAS algorithm is proposed to further reduce the computational
cost of NAS by incorporating a clustering-based local multi-fidelity infill
sampling strategy, enabling efficient exploration of the search space for
faster convergence. This algorithm is further accelerated by the use of a novel
continuous encoding method to represent the connections of nodes in each cell
within a generalized cell-based U-Net backbone, thereby decreasing the search
dimension (number of variables). Results indicate that the proposed NAS
algorithm outperforms previously published state-of-the-art methods under
limited computational budget on three numerical benchmarks, a 2D Darcy flow
regression problem and a CHASE_DB1 biomedical image segmentation problem. The
proposed method is subsequently used to create a wind velocity regression model
with application in urban modelling, with the found model able to achieve good
prediction with less computational complexity. Further analysis revealed that
the NAS algorithm independently identified principles undergirding superior
U-Net architectures in other literature, such as the importance of allowing
each cell to incorporate information from prior cells.

</details>


### [348] [Knowledge distillation as a pathway toward next-generation intelligent ecohydrological modeling systems](https://arxiv.org/abs/2509.01972)
*Long Jiang,Yang Yang,Ting Fong May Chui,Morgan Thornwell,Hoshin Vijai Gupta*

Main category: cs.LG

TL;DR: 提出集成基于过程的模型与机器学习的统一三相框架用于生态水文建模，在萨米什流域验证其有效性与扩展性。


<details>
  <summary>Details</summary>
Motivation: 基于过程的模型有结构刚性等问题，机器学习方法缺乏可解释性和可迁移性，需新方法模拟生态水文过程。

Method: 提出统一三相框架，分别为行为蒸馏、结构蒸馏和认知蒸馏，逐步将基于过程的模型与机器学习集成并嵌入人工智能。

Result: 在萨米什流域的示范表明，该框架能重现基于过程的模型输出，提高预测准确性，支持基于情景的决策。

Conclusion: 该框架为下一代智能生态水文建模系统提供可扩展和可迁移的途径，有扩展到其他基于过程领域的潜力。

Abstract: Simulating ecohydrological processes is essential for understanding complex
environmental systems and guiding sustainable management amid accelerating
climate change and human pressures. Process-based models provide physical
realism but can suffer from structural rigidity, high computational costs, and
complex calibration, while machine learning (ML) methods are efficient and
flexible yet often lack interpretability and transferability. We propose a
unified three-phase framework that integrates process-based models with ML and
progressively embeds them into artificial intelligence (AI) through knowledge
distillation. Phase I, behavioral distillation, enhances process models via
surrogate learning and model simplification to capture key dynamics at lower
computational cost. Phase II, structural distillation, reformulates process
equations as modular components within a graph neural network (GNN), enabling
multiscale representation and seamless integration with ML models. Phase III,
cognitive distillation, embeds expert reasoning and adaptive decision-making
into intelligent modeling agents using the Eyes-Brain-Hands-Mouth architecture.
Demonstrations for the Samish watershed highlight the framework's applicability
to ecohydrological modeling, showing that it can reproduce process-based model
outputs, improve predictive accuracy, and support scenario-based
decision-making. The framework offers a scalable and transferable pathway
toward next-generation intelligent ecohydrological modeling systems, with the
potential extension to other process-based domains.

</details>


### [349] [ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting](https://arxiv.org/abs/2509.01997)
*Jiacheng Shi,Haibin Wei,Jiang Wang,Xiaowei Xu,Longzhi Du,Taixu Jiang*

Main category: cs.LG

TL;DR: 本文提出创新时空学习模型，用两个图学习未来订单分布信息，提升物流供需压力预测性能并在真实环境验证有效性。


<details>
  <summary>Details</summary>
Motivation: 当前研究在学习在线配送平台未来订单分布时，因问题具有时间序列不敏感和强随机性，难以有效且高效捕捉信息。

Method: 提出利用进行中和全局两个图的时空学习模型，引入创新图学习网络框架ACA - Net 提取未来订单分布信息。

Result: 相比传统时空长序列方法性能更优，能有效学习稳健未来图，提升物流供需压力预测结果。

Conclusion: 提出的方法在真实生产环境中有效。

Abstract: Logistical demand-supply forecasting that evaluates the alignment between
projected supply and anticipated demand, is essential for the efficiency and
quality of on-demand food delivery platforms and serves as a key indicator for
scheduling decisions. Future order distribution information, which reflects the
distribution of orders in on-demand food delivery, is crucial for the
performance of logistical demand-supply forecasting. Current studies utilize
spatial-temporal analysis methods to model future order distribution
information from serious time slices. However, learning future order
distribution in online delivery platform is a time-series-insensitive problem
with strong randomness. These approaches often struggle to effectively capture
this information while remaining efficient. This paper proposes an innovative
spatiotemporal learning model that utilizes only two graphs (ongoing and
global) to learn future order distribution information, achieving superior
performance compared to traditional spatial-temporal long-series methods. The
main contributions are as follows: (1) The introduction of ongoing and global
graphs in logistical demand-supply pressure forecasting compared to traditional
long time series significantly enhances forecasting performance. (2) An
innovative graph learning network framework using adaptive future graph
learning and innovative cross attention mechanism (ACA-Net) is proposed to
extract future order distribution information, effectively learning a robust
future graph that substantially improves logistical demand-supply pressure
forecasting outcomes. (3) The effectiveness of the proposed method is validated
in real-world production environments.

</details>


### [350] [Second-Order Tensorial Partial Differential Equations on Graphs](https://arxiv.org/abs/2509.02015)
*Aref Einizade,Fragkiskos D. Malliaros,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: 本文引入二阶张量偏微分方程（So - TPDEGs），提出二阶连续乘积图神经网络的理论框架，能有效处理多图交互数据并保留高频信息，还进行了相关理论分析。


<details>
  <summary>Details</summary>
Motivation: 现有处理多交互图数据的方法多为离散图滤波，基于TPDEGs的连续方法限于一阶导数，难以捕捉复杂、多尺度和异质结构，需改进。

Method: 引入二阶TPDEGs，利用笛卡尔积图中余弦核的可分离性实现高效谱分解。

Result: 提供了图扰动下的稳定性和关于谱特性的过平滑行为的严格理论分析。

Conclusion: 理论结果为多个实际领域的连续图学习奠定了坚实基础。

Abstract: Processing data that lies on multiple interacting (product) graphs is
increasingly important in practical applications, yet existing methods are
mostly restricted to discrete graph filtering. Tensorial partial differential
equations on graphs (TPDEGs) offer a principled framework for modeling such
multidomain data in a continuous setting. However, current continuous
approaches are limited to first-order derivatives, which tend to dampen
high-frequency signals and slow down information propagation. This makes these
TPDEGs-based approaches less effective for capturing complex, multi-scale, and
heterophilic structures. In this paper, we introduce second-order TPDEGs
(So-TPDEGs) and propose the first theoretically grounded framework for
second-order continuous product graph neural networks. Our approach leverages
the separability of cosine kernels in Cartesian product graphs to implement
efficient spectral decomposition, while naturally preserving high-frequency
information. We provide rigorous theoretical analyses of stability under graph
perturbations and over-smoothing behavior regarding spectral properties. Our
theoretical results establish a robust foundation for advancing continuous
graph learning across multiple practical domains.

</details>


### [351] [Genetic Programming with Model Driven Dimension Repair for Learning Interpretable Appointment Scheduling Rules](https://arxiv.org/abs/2509.02034)
*Huan Zhang,Yang Wang,Ya-Hui Jia,Yi Mei*

Main category: cs.LG

TL;DR: 本文提出带维度修复的维度感知遗传编程算法进化预约规则，实验表明该方法能进化出高质量规则，还分析了规则语义。


<details>
  <summary>Details</summary>
Motivation: 传统遗传编程设计预约规则难解释和信任，因其未考虑维度一致性，本文旨在解决此问题。

Method: 开发带维度修复的维度感知遗传编程算法，将任务建模为混合整数线性规划模型求解，利用维度修复程序探索更多规则结构。

Result: 在模拟诊所实验中，该方法进化出的预约规则在目标值和维度一致性上优于手动设计规则和现有方法。

Conclusion: 该方法能有效进化出高质量、可解释的预约规则，为设计更优规则提供了思路。

Abstract: Appointment scheduling is a great challenge in healthcare operations
management. Appointment rules (AR) provide medical practitioners with a simple
yet effective tool to determine patient appointment times. Genetic programming
(GP) can be used to evolve ARs. However, directly applying GP to design ARs may
lead to rules that are difficult for end-users to interpret and trust. A key
reason is that GP is unaware of the dimensional consistency, which ensures that
the evolved rules align with users' domain knowledge and intuitive
understanding. In this paper, we develop a new dimensionally aware GP algorithm
with dimension repair to evolve ARs with dimensional consistency and high
performance. A key innovation of our method is the dimension repair procedure,
which optimizes the dimensional consistency of an expression tree while
minimizing structural changes and ensuring that its output dimension meets the
problem's requirements. We formulate the task as a mixed-integer linear
programming model that can be efficiently solved using common mathematical
programming methods. With the support of the dimension repair procedure, our
method can explore a wider range of AR structures by temporarily breaking the
dimensional consistency of individuals, and then restoring it without altering
their overall structure, thereby identifying individuals with greater potential
advantages. We evaluated the proposed method in a comprehensive set of
simulated clinics. The experimental results demonstrate that our approach
managed to evolve high-quality ARs that significantly outperform not only the
manually designed ARs but also existing state-of-the-art dimensionally aware GP
methods in terms of both objective values and dimensional consistency. In
addition, we analyzed the semantics of the evolved ARs, providing insight into
the design of more effective and interpretable ARs.

</details>


### [352] [Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization Framework with Curvature-Guided Perturbation](https://arxiv.org/abs/2509.02048)
*Yi Yin,Guangquan Zhang,Hua Zuo,Jie Lu*

Main category: cs.LG

TL;DR: 文章提出新的双层优化框架发布私有数据集，平衡隐私保护和数据效用，实验证明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决隐私保护技术常降低数据效用，在隐私保护和数据效用间找到最优平衡这一关键挑战。

Method: 引入双层优化框架，上层关注数据效用，用判别器引导生成过程；下层关注数据隐私，用数据流形的局部外在曲率衡量个体对成员推理攻击的脆弱性，向低曲率区域扰动样本。

Result: 方法增强了下游任务对成员推理攻击的抵抗力，在样本质量和多样性上超越现有方法。

Conclusion: 通过交替优化两个目标，该方法实现了隐私和效用的协同平衡。

Abstract: Machine learning models require datasets for effective training, but directly
sharing raw data poses significant privacy risk such as membership inference
attacks (MIA). To mitigate the risk, privacy-preserving techniques such as data
perturbation, generalization, and synthetic data generation are commonly
utilized. However, these methods often degrade data accuracy, specificity, and
diversity, limiting the performance of downstream tasks and thus reducing data
utility. Therefore, striking an optimal balance between privacy preservation
and data utility remains a critical challenge.
  To address this issue, we introduce a novel bilevel optimization framework
for the publication of private datasets, where the upper-level task focuses on
data utility and the lower-level task focuses on data privacy. In the
upper-level task, a discriminator guides the generation process to ensure that
perturbed latent variables are mapped to high-quality samples, maintaining
fidelity for downstream tasks. In the lower-level task, our framework employs
local extrinsic curvature on the data manifold as a quantitative measure of
individual vulnerability to MIA, providing a geometric foundation for targeted
privacy protection. By perturbing samples toward low-curvature regions, our
method effectively suppresses distinctive feature combinations that are
vulnerable to MIA. Through alternating optimization of both objectives, we
achieve a synergistic balance between privacy and utility. Extensive
experimental evaluations demonstrate that our method not only enhances
resistance to MIA in downstream tasks but also surpasses existing methods in
terms of sample quality and diversity.

</details>


### [353] [LUCIE-3D: A three-dimensional climate emulator for forced responses](https://arxiv.org/abs/2509.02061)
*Haiwen Guan,Troy Arcomano,Ashesh Chattopadhyay,Romit Maulik*

Main category: cs.LG

TL;DR: 介绍轻量级3D气候模拟器LUCIE - 3D，它基于LUCIE - 2D框架，用SFNO骨干和ERA5数据训练，能模拟多种气候现象，训练高效，是气候研究有价值工具。


<details>
  <summary>Details</summary>
Motivation: 开发能捕捉大气垂直结构、响应气候变化强迫且保持计算效率和长期稳定性的3D气候模拟器。

Method: 基于LUCIE - 2D框架，采用Spherical Fourier Neural Operator (SFNO)骨干，在30年ERA5再分析数据上训练，融入大气CO2作为强迫变量，可整合规定海表温度。

Result: LUCIE - 3D成功重现气候均值、变率和长期气候变化信号，捕捉关键动力过程，极端事件统计表现可信，训练高效。

Conclusion: LUCIE - 3D稳定性、物理一致性和易获取性使其成为气候研究有价值工具，有广泛应用潜力。

Abstract: We introduce LUCIE-3D, a lightweight three-dimensional climate emulator
designed to capture the vertical structure of the atmosphere, respond to
climate change forcings, and maintain computational efficiency with long-term
stability. Building on the original LUCIE-2D framework, LUCIE-3D employs a
Spherical Fourier Neural Operator (SFNO) backbone and is trained on 30 years of
ERA5 reanalysis data spanning eight vertical {\sigma}-levels. The model
incorporates atmospheric CO2 as a forcing variable and optionally integrates
prescribed sea surface temperature (SST) to simulate coupled ocean--atmosphere
dynamics. Results demonstrate that LUCIE-3D successfully reproduces
climatological means, variability, and long-term climate change signals,
including surface warming and stratospheric cooling under increasing CO2
concentrations. The model further captures key dynamical processes such as
equatorial Kelvin waves, the Madden--Julian Oscillation, and annular modes,
while showing credible behavior in the statistics of extreme events. Despite
requiring longer training than its 2D predecessor, LUCIE-3D remains efficient,
training in under five hours on four GPUs. Its combination of stability,
physical consistency, and accessibility makes it a valuable tool for rapid
experimentation, ablation studies, and the exploration of coupled climate
dynamics, with potential applications extending to paleoclimate research and
future Earth system emulation.

</details>


### [354] [Data-Dependent Smoothing for Protein Discovery with Walk-Jump Sampling](https://arxiv.org/abs/2509.02069)
*Srinivas Anumasa,Barath Chandran. C,Tingting Chen,Dianbo Liu*

Main category: cs.LG

TL;DR: 提出数据依赖平滑Walk - Jump框架处理蛋白质数据，实验显示有提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略蛋白质数据分布的依赖可变性，需考虑数据局部几何特征。

Method: 引入数据依赖平滑Walk - Jump框架，用核密度估计（KDE）预处理估计每个数据点噪声尺度σ，用这些值训练得分模型。

Result: 在多个指标上有持续改进。

Conclusion: 数据感知的sigma预测对稀疏高维环境下生成建模很重要。

Abstract: Diffusion models have emerged as a powerful class of generative models by
learning to iteratively reverse the noising process. Their ability to generate
high-quality samples has extended beyond high-dimensional image data to other
complex domains such as proteins, where data distributions are typically sparse
and unevenly spread. Importantly, the sparsity itself is uneven. Empirically,
we observed that while a small fraction of samples lie in dense clusters, the
majority occupy regions of varying sparsity across the data space. Existing
approaches largely ignore this data-dependent variability. In this work, we
introduce a Data-Dependent Smoothing Walk-Jump framework that employs kernel
density estimation (KDE) as a preprocessing step to estimate the noise scale
$\sigma$ for each data point, followed by training a score model with these
data-dependent $\sigma$ values. By incorporating local data geometry into the
denoising process, our method accounts for the heterogeneous distribution of
protein data. Empirical evaluations demonstrate that our approach yields
consistent improvements across multiple metrics, highlighting the importance of
data-aware sigma prediction for generative modeling in sparse, high-dimensional
settings.

</details>


### [355] [Towards Comprehensive Information-theoretic Multi-view Learning](https://arxiv.org/abs/2509.02084)
*Long Shi,Yunshan Ye,Wenjie Wang,Tao Lei,Yu Zhao,Gang Kou,Badong Chen*

Main category: cs.LG

TL;DR: 提出抛弃多视图冗余假设的信息论多视图学习框架CIML，理论证明其联合表示对下游任务预测充分，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多数结合信息论原理的多视图方法依赖多视图冗余假设，忽略了各视图中独特信息对任务的预测潜力。

Method: 提出CIML框架，考虑公共和独特信息的预测能力。公共表示学习通过最大化Gacs - Korner公共信息提取共享特征并基于信息瓶颈压缩；独特表示学习用信息瓶颈压缩各视图独特表示，同时最小化独特与公共表示及不同独特表示间的互信息。

Result: 理论证明学习到的联合表示对下游任务具有预测充分性，实验结果表明模型优于多个现有方法。

Conclusion: 所提出的CIML框架在多视图学习中表现优异，有推广价值。

Abstract: Information theory has inspired numerous advancements in multi-view learning.
Most multi-view methods incorporating information-theoretic principles rely an
assumption called multi-view redundancy which states that common information
between views is necessary and sufficient for down-stream tasks. This
assumption emphasizes the importance of common information for prediction, but
inherently ignores the potential of unique information in each view that could
be predictive to the task. In this paper, we propose a comprehensive
information-theoretic multi-view learning framework named CIML, which discards
the assumption of multi-view redundancy. Specifically, CIML considers the
potential predictive capabilities of both common and unique information based
on information theory. First, the common representation learning maximizes
Gacs-Korner common information to extract shared features and then compresses
this information to learn task-relevant representations based on the
Information Bottleneck (IB). For unique representation learning, IB is employed
to achieve the most compressed unique representation for each view while
simultaneously minimizing the mutual information between unique and common
representations, as well as among different unique representations.
Importantly, we theoretically prove that the learned joint representation is
predictively sufficient for the downstream task. Extensive experimental results
have demonstrated the superiority of our model over several state-of-art
methods. The code is released on CIML.

</details>


### [356] [DivMerge: A divergence-based model merging method for multi-tasking](https://arxiv.org/abs/2509.02108)
*Touayouch Brahim,Fosse Loïc,Damnati Géraldine,Lecorvé Gwénolé*

Main category: cs.LG

TL;DR: 提出一种多任务学习模型合并方法，利用Jensen - Shannon散度，无需额外标注数据，平衡任务重要性，任务增多时仍表现良好。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中模型合并存在任务干扰问题，且任务数量增加时干扰加剧。

Method: 利用Jensen - Shannon散度指导模型合并过程，自动平衡任务重要性，无需额外标注数据。

Result: 在任务数量增长时保持鲁棒性，持续超越先前工作。

Conclusion: 所提方法能将不同任务训练的模型合并为一个，在所有任务上保持良好性能。

Abstract: Multi-task learning (MTL) is often achieved by merging datasets before
fine-tuning, but the growing availability of fine-tuned models has led to new
approaches such as model merging via task arithmetic. A major challenge in this
setting is task interference, which worsens as the number of tasks increases.
We propose a method that merges models trained on different tasks into a single
model, maintaining strong performance across all tasks. Our approach leverages
Jensen-Shannon divergence to guide the merging process without requiring
additional labelled data, and automatically balances task importance. Unlike
existing methods, our approach remains robust as the number of tasks grows and
consistently outperforms prior work.

</details>


### [357] [HiGraph: A Large-Scale Hierarchical Graph Dataset for Malware Analysis](https://arxiv.org/abs/2509.02113)
*Han Chen,Hanchen Wang,Hongmei Chen,Ying Zhang,Lu Qin,Wenjie Zhang*

Main category: cs.LG

TL;DR: 提出用于恶意软件分析的最大公共分层图数据集HiGraph，含大量控制流图和函数调用图，展示其效用并公开数据和工具。


<details>
  <summary>Details</summary>
Motivation: 基于图的恶意软件分析受限于缺乏能捕捉软件固有层次结构的大规模数据集，现有方法常将程序简化为单层图，无法建模高低层语义关系。

Method: 引入HiGraph数据集，采用两层表示保留结构语义。

Result: 通过大规模分析揭示良性和恶意软件的不同结构特性。

Conclusion: HiGraph可作为该领域的基础基准。

Abstract: The advancement of graph-based malware analysis is critically limited by the
absence of large-scale datasets that capture the inherent hierarchical
structure of software. Existing methods often oversimplify programs into single
level graphs, failing to model the crucial semantic relationship between
high-level functional interactions and low-level instruction logic. To bridge
this gap, we introduce \dataset, the largest public hierarchical graph dataset
for malware analysis, comprising over \textbf{200M} Control Flow Graphs (CFGs)
nested within \textbf{595K} Function Call Graphs (FCGs). This two-level
representation preserves structural semantics essential for building robust
detectors resilient to code obfuscation and malware evolution. We demonstrate
HiGraph's utility through a large-scale analysis that reveals distinct
structural properties of benign and malicious software, establishing it as a
foundational benchmark for the community. The dataset and tools are publicly
available at https://higraph.org.

</details>


### [358] [Threshold-Based Optimal Arm Selection in Monotonic Bandits: Regret Lower Bounds and Algorithms](https://arxiv.org/abs/2509.02119)
*Chanakya Varude,Jay Chaudhary,Siddharth Kaushik,Prasanna Chaporkar*

Main category: cs.LG

TL;DR: 本文探讨基于阈值的多臂老虎机问题，推导渐近遗憾下界，提出算法并通过模拟验证，扩展经典老虎机理论。


<details>
  <summary>Details</summary>
Motivation: 源于通信网络、临床给药、能源管理、推荐系统等应用，探索在阈值约束下的高效决策方法。

Method: 研究不同阈值相关的变体问题，推导渐近遗憾下界，提出算法并通过蒙特卡罗模拟验证。

Result: 渐近遗憾下界仅依赖于阈值相邻的臂，提出的算法经模拟验证具有最优性。

Conclusion: 扩展了经典老虎机理论，适用于有阈值约束的高效决策场景。

Abstract: In multi-armed bandit problems, the typical goal is to identify the arm with
the highest reward. This paper explores a threshold-based bandit problem,
aiming to select an arm based on its relation to a prescribed threshold \(\tau
\). We study variants where the optimal arm is the first above \(\tau\), the
\(k^{th}\) arm above or below it, or the closest to it, under a monotonic
structure of arm means. We derive asymptotic regret lower bounds, showing
dependence only on arms adjacent to \(\tau\). Motivated by applications in
communication networks (CQI allocation), clinical dosing, energy management,
recommendation systems, and more. We propose algorithms with optimality
validated through Monte Carlo simulations. Our work extends classical bandit
theory with threshold constraints for efficient decision-making.

</details>


### [359] [Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time](https://arxiv.org/abs/2509.02129)
*Jintao Cheng,Weibin Li,Jiehao Luo,Xiaoyu Tang,Zhijian He,Jin Wu,Yao Zou,Wei Zhang*

Main category: cs.LG

TL;DR: 提出零样本框架用TTS和UASC解决VPR当前方法局限，提升跨域性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决当前VPR方法（如VFM和MLLM）计算开销大、跨域迁移能力有限的问题。

Method: 提出零样本框架，用TTS利用MLLM的视觉 - 语言对齐能力进行直接相似度评分，采用结构化提示生成JSON输出，结合UASC实现实时自适应。

Result: 实验显示跨域VPR性能显著提升，计算效率最高提升210倍。

Conclusion: 所提框架能有效解决现有问题，实现跨不同环境的卓越泛化。

Abstract: Visual Place Recognition (VPR) has evolved from handcrafted descriptors to
deep learning approaches, yet significant challenges remain. Current
approaches, including Vision Foundation Models (VFMs) and Multimodal Large
Language Models (MLLMs), enhance semantic understanding but suffer from high
computational overhead and limited cross-domain transferability when
fine-tuned. To address these limitations, we propose a novel zero-shot
framework employing Test-Time Scaling (TTS) that leverages MLLMs'
vision-language alignment capabilities through Guidance-based methods for
direct similarity scoring. Our approach eliminates two-stage processing by
employing structured prompts that generate length-controllable JSON outputs.
The TTS framework with Uncertainty-Aware Self-Consistency (UASC) enables
real-time adaptation without additional training costs, achieving superior
generalization across diverse environments. Experimental results demonstrate
significant improvements in cross-domain VPR performance with up to 210$\times$
computational efficiency gains.

</details>


### [360] [Simulating classification models to evaluate Predict-Then-Optimize methods](https://arxiv.org/abs/2509.02191)
*Pieter Smet*

Main category: cs.LG

TL;DR: 本文围绕Predict - Then - Optimize方法，引入多分类器预测模拟算法，评估算法性能，研究预测误差与解质量关系。


<details>
  <summary>Details</summary>
Motivation: 在复杂约束优化问题中，缺乏对Predict - Then - Optimize方法中‘更准确预测带来更优解’假设的证据，需分析预测误差对解质量的影响。

Method: 引入多分类器预测模拟算法，结合已有二分类模拟算法，进行计算实验。

Result: 模拟分类器性能有一定准确性但有波动，应用算法评估调度问题的Predict - Then - Optimize算法，发现预测误差与解接近最优解程度关系复杂。

Conclusion: 设计和评估基于机器学习预测的决策系统时，需重视预测误差与解质量的复杂关系。

Abstract: Uncertainty in optimization is often represented as stochastic parameters in
the optimization model. In Predict-Then-Optimize approaches, predictions of a
machine learning model are used as values for such parameters, effectively
transforming the stochastic optimization problem into a deterministic one. This
two-stage framework is built on the assumption that more accurate predictions
result in solutions that are closer to the actual optimal solution. However,
providing evidence for this assumption in the context of complex, constrained
optimization problems is challenging and often overlooked in the literature.
Simulating predictions of machine learning models offers a way to
(experimentally) analyze how prediction error impacts solution quality without
the need to train real models. Complementing an algorithm from the literature
for simulating binary classification, we introduce a new algorithm for
simulating predictions of multiclass classifiers. We conduct a computational
study to evaluate the performance of these algorithms, and show that classifier
performance can be simulated with reasonable accuracy, although some
variability is observed. Additionally, we apply these algorithms to assess the
performance of a Predict-Then-Optimize algorithm for a machine scheduling
problem. The experiments demonstrate that the relationship between prediction
error and how close solutions are to the actual optimum is non-trivial,
highlighting important considerations for the design and evaluation of
decision-making systems based on machine learning predictions.

</details>


### [361] [Baichuan-M2: Scaling Medical Capability with Large Verifier System](https://arxiv.org/abs/2509.02208)
*Baichuan-M2 Team,:,Chengfeng Dou,Chong Liu,Fan Yang,Fei Li,Jiyuan Jia,Mingyang Chen,Qiang Ju,Shuai Wang,Shunya Dang,Tianpeng Li,Xiangrong Zeng,Yijie Zhou,Chenzheng Zhu,Da Pan,Fei Deng,Guangwei Ai,Guosheng Dong,Hongda Zhang,Jinyang Tai,Jixiang Hong,Kai Lu,Linzhuang Sun,Peidong Guo,Qian Ma,Rihui Xin,Shihui Yang,Shusen Zhang,Yichuan Mo,Zheng Liang,Zhishou Zhang,Hengfu Cui,Zuyi Zhu,Xiaochuan Wang*

Main category: cs.LG

TL;DR: 随着大语言模型发展，医疗领域应用成研究焦点。本文引入动态验证框架，开发Baichuan - M2模型，在HealthBench上表现出色，证明动态验证系统对医疗AI重要性。


<details>
  <summary>Details</summary>
Motivation: 医疗大语言模型在静态基准测试和实际临床决策中的表现存在差距，传统考试无法体现医疗咨询的动态、交互特性。

Method: 引入动态验证框架，包含患者模拟器和临床评分生成器；用改进GRPO算法通过多阶段强化学习策略训练32B参数的Baichuan - M2模型。

Result: Baichuan - M2在HealthBench上超越所有开源模型和多数闭源模型，在HealthBench Hard基准测试中得分超32，此前仅GPT - 5能达到。

Conclusion: 强大的动态验证系统对使大语言模型能力与实际临床应用相匹配至关重要，为医疗AI部署在性能 - 参数权衡方面建立了新的帕累托前沿。

Abstract: As large language models (LLMs) advance in conversational and reasoning
capabilities, their practical application in healthcare has become a critical
research focus. However, there is a notable gap between the performance of
medical LLMs on static benchmarks such as USMLE and their utility in real-world
clinical decision-making. This discrepancy arises because traditional exams
fail to capture the dynamic, interactive nature of medical consultations. To
address this challenge, we introduce a novel dynamic verification framework
that moves beyond static answer verifier, establishing a large-scale,
high-fidelity interactive reinforcement learning system. Our framework
comprises two key components: a Patient Simulator that creates realistic
clinical environments using de-identified medical records, and a Clinical
Rubrics Generator that dynamically produces multi-dimensional evaluation
metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter
medical augmented reasoning model trained through a multi-stage reinforcement
learning strategy with an improved Group Relative Policy Optimization (GRPO)
algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other
open-source models and most advanced closed-source counterparts, achieving a
score above 32 on the challenging HealthBench Hard benchmark-previously
exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier
system is essential for aligning LLM capabilities with practical clinical
applications, establishing a new Pareto front in the performance-parameter
trade-off for medical AI deployment.

</details>


### [362] [ST-Hyper: Learning High-Order Dependencies Across Multiple Spatial-Temporal Scales for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.02217)
*Binqing Wu,Jianlong Huang,Zongjiang Shang,Ling Chen*

Main category: cs.LG

TL;DR: 提出ST - Hyper通过自适应超图建模来在多时空尺度上预测多元时间序列，实验显示其性能达最优。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在多变量时间序列预测中难以对跨多个时空尺度的依赖关系进行建模。

Method: 引入时空金字塔建模（STPM）模块提取多时空尺度特征；引入自适应超图建模（AHM）模块学习稀疏超图以捕获特征间高阶依赖；通过三相超图传播交互特征。

Result: 在六个真实世界的多变量时间序列数据集上实验，ST - Hyper实现了最优性能，长期和短期预测平均MAE分别降低3.8%和6.8%。

Conclusion: ST - Hyper在多变量时间序列预测中表现良好，能有效建模跨多时空尺度的依赖关系。

Abstract: In multivariate time series (MTS) forecasting, many deep learning based
methods have been proposed for modeling dependencies at multiple spatial
(inter-variate) or temporal (intra-variate) scales. However, existing methods
may fail to model dependencies across multiple spatial-temporal scales
(ST-scales, i.e., scales that jointly consider spatial and temporal scopes). In
this work, we propose ST-Hyper to model the high-order dependencies across
multiple ST-scales through adaptive hypergraph modeling. Specifically, we
introduce a Spatial-Temporal Pyramid Modeling (STPM) module to extract features
at multiple ST-scales. Furthermore, we introduce an Adaptive Hypergraph
Modeling (AHM) module that learns a sparse hypergraph to capture robust
high-order dependencies among features. In addition, we interact with these
features through tri-phase hypergraph propagation, which can comprehensively
capture multi-scale spatial-temporal dynamics. Experimental results on six
real-world MTS datasets demonstrate that ST-Hyper achieves the state-of-the-art
performance, outperforming the best baselines with an average MAE reduction of
3.8\% and 6.8\% for long-term and short-term forecasting, respectively.

</details>


### [363] [VariAntNet: Learning Decentralized Control of Multi-Agent Systems](https://arxiv.org/abs/2509.02271)
*Yigal Koifman,Erez Koifman,Eran Iceland,Ariel Barel,Alfred M. Bruckstein*

Main category: cs.LG

TL;DR: 本文提出基于深度学习的分散控制模型VariAntNet，用于促进多智能体群聚和协作任务执行，在多智能体聚集任务中表现优于现有分析解决方案。


<details>
  <summary>Details</summary>
Motivation: 简单多智能体系统在灾害响应应用中有需求，但面临保持群体凝聚力和避免碎片化等挑战，现有分析解决方案在时间关键场景中速度慢，需新方法。

Method: 提出VariAntNet模型，包括从无序、可变大小的局部观测中提取几何特征，使用新的可微多目标损失函数训练神经网络。

Result: VariAntNet在多智能体聚集任务中显著优于现有分析解决方案，收敛速度翻倍，不同群体规模下保持高连接性。

Conclusion: 分析了新方法和现有分析方法在时间关键场景中的权衡。

Abstract: A simple multi-agent system can be effectively utilized in disaster response
applications, such as firefighting. Such a swarm is required to operate in
complex environments with limited local sensing and no reliable inter-agent
communication or centralized control. These simple robotic agents, also known
as Ant Robots, are defined as anonymous agents that possess limited sensing
capabilities, lack a shared coordinate system, and do not communicate
explicitly with one another. A key challenge for simple swarms lies in
maintaining cohesion and avoiding fragmentation despite limited-range sensing.
Recent advances in machine learning offer effective solutions to some of the
classical decentralized control challenges. We propose VariAntNet, a deep
learning-based decentralized control model designed to facilitate agent
swarming and collaborative task execution. VariAntNet includes geometric
features extraction from unordered, variable-sized local observations. It
incorporates a neural network architecture trained with a novel,
differentiable, multi-objective, mathematically justified loss function that
promotes swarm cohesiveness by utilizing the properties of the visibility graph
Laplacian matrix. VariAntNet is demonstrated on the fundamental multi-agent
gathering task, where agents with bearing-only and limited-range sensing must
gather at some location. VariAntNet significantly outperforms an existing
analytical solution, achieving more than double the convergence rate while
maintaining high swarm connectivity across varying swarm sizes. While the
analytical solution guarantees cohesion, it is often too slow in practice. In
time-critical scenarios, such as emergency response operations where lives are
at risk, slower analytical methods are impractical and justify the loss of some
agents within the swarm. This paper presents and analyzes this trade-off in
detail.

</details>


### [364] [Balanced Multimodal Learning: An Unidirectional Dynamic Interaction Perspective](https://arxiv.org/abs/2509.02281)
*Shijie Wang,Li Zhang,Xinyan Liang,Yuhua Qian,Shen Hu*

Main category: cs.LG

TL;DR: 本文提出一种不依赖联合损失的多模态不平衡学习新策略UDI，实验表明其在处理模态不平衡问题上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于联合损失的多模态学习策略会导致模态不平衡，且现有解决策略具有局限性，需探索新策略。

Method: 提出Unidirectional Dynamic Interaction (UDI)策略，先训练锚模态至收敛，再用其表示通过无监督损失引导其他模态，并动态调整模态交互。

Result: 实验结果显示UDI在处理模态不平衡方面优于现有方法，提升了多模态学习任务的性能。

Conclusion: UDI通过解耦模态优化和实现有向信息流，防止单一模态主导，促进有效跨模态特征学习，能更好处理多模态不平衡问题。

Abstract: Multimodal learning typically utilizes multimodal joint loss to integrate
different modalities and enhance model performance. However, this joint
learning strategy can induce modality imbalance, where strong modalities
overwhelm weaker ones and limit exploitation of individual information from
each modality and the inter-modality interaction information.Existing
strategies such as dynamic loss weighting, auxiliary objectives and gradient
modulation mitigate modality imbalance based on joint loss. These methods
remain fundamentally reactive, detecting and correcting imbalance after it
arises, while leaving the competitive nature of the joint loss untouched. This
limitation drives us to explore a new strategy for multimodal imbalance
learning that does not rely on the joint loss, enabling more effective
interactions between modalities and better utilization of information from
individual modalities and their interactions. In this paper, we introduce
Unidirectional Dynamic Interaction (UDI), a novel strategy that abandons the
conventional joint loss in favor of a proactive, sequential training scheme.
UDI first trains the anchor modality to convergence, then uses its learned
representations to guide the other modality via unsupervised loss. Furthermore,
the dynamic adjustment of modality interactions allows the model to adapt to
the task at hand, ensuring that each modality contributes optimally. By
decoupling modality optimization and enabling directed information flow, UDI
prevents domination by any single modality and fosters effective cross-modal
feature learning. Our experimental results demonstrate that UDI outperforms
existing methods in handling modality imbalance, leading to performance
improvement in multimodal learning tasks.

</details>


### [365] [AdaSwitch: An Adaptive Switching Meta-Algorithm for Learning-Augmented Bounded-Influence Problems](https://arxiv.org/abs/2509.02302)
*Xi Chen,Yuze Chen,Yuan Zhou*

Main category: cs.LG

TL;DR: 研究含序列预测的多阶段在线决策问题，提出有界影响框架和AdaSwitch元算法，该算法在预测准确时接近离线基准性能，不准确时保证竞争比，且适用于多种场景。


<details>
  <summary>Details</summary>
Motivation: 解决一类含可能不准确的序列预测的多阶段在线决策问题，在不同预测准确性下实现较好决策性能。

Method: 引入有界影响框架，提出AdaSwitch元算法。

Result: 算法在预测准确时接近离线基准性能，不准确时保证经典竞争比，且适用于多种决策场景。

Conclusion: 所提框架和元算法对学习增强的在线决策具有灵活性和广泛适用性。

Abstract: We study a class of multi-period online decision-making problems with
sequence-based predictions, which may be generated by machine learning models
but whose accuracy is not guaranteed. In each period, the decision-maker
observes the realized request and must take an irrevocable action that yields a
reward or incurs a cost, without knowledge of future arrivals. We introduce a
bounded-influence framework, in which past decisions and requests exert only
limited impact on the future optimal reward. Within this framework, we propose
the AdaSwitch meta-algorithm, which exploits predictions to attain performance
close to the offline benchmark when predictions are accurate, while preserving
classical competitive-ratio guarantees under highly inaccurate predictions. Our
framework and meta-algorithm apply to diverse settings, including lead-time
quotation in processing systems, the $k$-server problem, and online allocation
of reusable resources. These applications illustrate the flexibility and broad
applicability of our approach to learning-augmented online decision-making.

</details>


### [366] [Extrapolated Markov Chain Oversampling Method for Imbalanced Text Classification](https://arxiv.org/abs/2509.02332)
*Aleksi Avela,Pauliina Ilmonen*

Main category: cs.LG

TL;DR: 提出基于马尔可夫链的文本过采样方法，在处理不平衡文本数据分类时能产生有竞争力结果，尤其在不平衡严重时。


<details>
  <summary>Details</summary>
Motivation: 现实文本分类中存在数据不平衡问题，且文本数据有独特困难，现有通用过采样方法应用于文本数据有局限。

Method: 引入基于马尔可夫链的文本过采样方法，从少数类和部分多数类估计转移概率以在过采样中扩展少数类特征空间。

Result: 在多个真实数据示例中，该方法与其他著名过采样方法相比能产生极具竞争力的结果。

Conclusion: 该基于马尔可夫链的文本过采样方法在处理不平衡文本数据分类时表现良好，尤其适用于不平衡严重的情况。

Abstract: Text classification is the task of automatically assigning text documents
correct labels from a predefined set of categories. In real-life (text)
classification tasks, observations and misclassification costs are often
unevenly distributed between the classes - known as the problem of imbalanced
data. Synthetic oversampling is a popular approach to imbalanced
classification. The idea is to generate synthetic observations in the minority
class to balance the classes in the training set. Many general-purpose
oversampling methods can be applied to text data; however, imbalanced text data
poses a number of distinctive difficulties that stem from the unique nature of
text compared to other domains. One such factor is that when the sample size of
text increases, the sample vocabulary (i.e., feature space) is likely to grow
as well. We introduce a novel Markov chain based text oversampling method. The
transition probabilities are estimated from the minority class but also partly
from the majority class, thus allowing the minority feature space to expand in
oversampling. We evaluate our approach against prominent oversampling methods
and show that our approach is able to produce highly competitive results
against the other methods in several real data examples, especially when the
imbalance is severe.

</details>


### [367] [RDIT: Residual-based Diffusion Implicit Models for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2509.02341)
*Chih-Yu Lai,Yu-Chien Ning,Duane S. Boning*

Main category: cs.LG

TL;DR: 提出RDIT框架用于概率时间序列预测，结合点估计与基于残差的条件扩散，在多数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有概率时间序列预测方法存在分布建模不佳、训练和评估指标不匹配的问题。

Method: 提出RDIT框架，结合点估计和基于残差的条件扩散，使用双向Mamba网络，理论上证明调整标准差可最小化CRPS并推导算法实现分布匹配。

Result: 在八个多变量数据集上，RDIT比强基线有更低的CRPS、更快的推理速度和更好的覆盖率。

Conclusion: RDIT框架在概率时间序列预测中表现良好，是一种有效的方法。

Abstract: Probabilistic Time Series Forecasting (PTSF) plays a critical role in domains
requiring accurate and uncertainty-aware predictions for decision-making.
However, existing methods offer suboptimal distribution modeling and suffer
from a mismatch between training and evaluation metrics. Surprisingly, we found
that augmenting a strong point estimator with a zero-mean Gaussian, whose
standard deviation matches its training error, can yield state-of-the-art
performance in PTSF. In this work, we propose RDIT, a plug-and-play framework
that combines point estimation and residual-based conditional diffusion with a
bidirectional Mamba network. We theoretically prove that the Continuous Ranked
Probability Score (CRPS) can be minimized by adjusting to an optimal standard
deviation and then derive algorithms to achieve distribution matching.
Evaluations on eight multivariate datasets across varied forecasting horizons
demonstrate that RDIT achieves lower CRPS, rapid inference, and improved
coverage compared to strong baselines.

</details>


### [368] [Scaffolding Collaborative Learning in STEM: A Two-Year Evaluation of a Tool-Integrated Project-Based Methodology](https://arxiv.org/abs/2509.02355)
*Caterina Fuster-Barcelo,Gonzalo R. Rios-Munoz,Arrate Munoz-Barrutia*

Main category: cs.LG

TL;DR: 研究在机器学习健康硕士项目中整合数字协作工具和结构化同行评估，发现能提升学习成果和评估公平性。


<details>
  <summary>Details</summary>
Motivation: 探索在机器学习健康硕士项目中提升学生参与度、评估透明度和公平性的方法。

Method: 在两年内重新设计生物医学图像处理课程，结合Google Colab实时编程、Weights & Biases实验跟踪报告和基于评分标准的同行评估。

Result: 与干预前相比，两年实施期间最终项目分数的分散度和熵增加，学生对课程和自身学习过程的参与度提高。

Conclusion: 整合工具支持的协作和结构化评估机制可提升STEM教育的学习成果和公平性。

Abstract: This study examines the integration of digital collaborative tools and
structured peer evaluation in the Machine Learning for Health master's program,
through the redesign of a Biomedical Image Processing course over two academic
years. The pedagogical framework combines real-time programming with Google
Colab, experiment tracking and reporting via Weights & Biases, and
rubric-guided peer assessment to foster student engagement, transparency, and
fair evaluation. Compared to a pre-intervention cohort, the two implementation
years showed increased grade dispersion and higher entropy in final project
scores, suggesting improved differentiation and fairness in assessment. The
survey results further indicate greater student engagement with the subject and
their own learning process. These findings highlight the potential of
integrating tool-supported collaboration and structured evaluation mechanisms
to enhance both learning outcomes and equity in STEM education.

</details>


### [369] [Evaluating Cumulative Spectral Gradient as a Complexity Measure](https://arxiv.org/abs/2509.02399)
*Haji Gul,Abdul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.LG

TL;DR: 研究评估CSG指标在知识图谱链接预测中的表现，发现其存在问题，强调需更稳健指标。


<details>
  <summary>Details</summary>
Motivation: 准确估计数据集复杂度对评估和比较知识图谱链接预测模型至关重要，需评估CSG指标在知识图谱链接预测中的行为。

Method: 在标准知识图谱链接预测基准上，通过改变计算CSG的两个关键参数M和K，对CSG行为进行严格评估。

Result: CSG对K的选择高度敏感，不随目标类数量自然缩放，与MRR等性能指标弱相关或无相关，在链接预测中稳定性和泛化预测能力失效。

Conclusion: 知识图谱链接预测评估需要更稳健、与分类器无关的复杂度度量。

Abstract: Accurate estimation of dataset complexity is crucial for evaluating and
comparing link prediction models for knowledge graphs (KGs). The Cumulative
Spectral Gradient (CSG) metric derived from probabilistic divergence between
classes within a spectral clustering framework was proposed as a dataset
complexity measure that (1) naturally scales with the number of classes and (2)
correlates strongly with downstream classification performance. In this work,
we rigorously assess CSG behavior on standard knowledge graph link prediction
benchmarks a multi class tail prediction task, using two key parameters
governing its computation, M, the number of Monte Carlo sampled points per
class, and K, the number of nearest neighbors in the embedding space. Contrary
to the original claims, we find that (1) CSG is highly sensitive to the choice
of K and therefore does not inherently scale with the number of target classes,
and (2) CSG values exhibit weak or no correlation with established performance
metrics such as mean reciprocal rank (MRR). Through experiments on FB15k 237,
WN18RR, and other standard datasets, we demonstrate that CSG purported
stability and generalization predictive power break down in link prediction
settings. Our results highlight the need for more robust, classifier agnostic
complexity measures in KG link prediction evaluation.

</details>


### [370] [Fisher information flow in artificial neural networks](https://arxiv.org/abs/2509.02407)
*Maximilian Weimar,Lukas M. Rachbauer,Ilya Starshynov,Daniele Faccio,Linara Adilova,Dorian Bouchet,Stefan Rotter*

Main category: cs.LG

TL;DR: 提出监测人工神经网络（ANN）中费舍尔信息流动的方法，表明最优估计对应信息最大传输，提供无模型训练停止准则并通过成像实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解ANN在参数估计任务中如何内部处理和传输参数相关信息，改进参数估计过程。

Method: 提出监测费舍尔信息在执行参数估计任务的ANN中从输入到输出层流动的方法。

Result: 最优估计性能对应费舍尔信息的最大传输，训练超过此点会因过拟合导致信息损失，该方法可用于成像实验数据训练的网络。

Conclusion: 该方法提供了无模型的网络训练停止准则，在实际物理场景中有效。

Abstract: The estimation of continuous parameters from measured data plays a central
role in many fields of physics. A key tool in understanding and improving such
estimation processes is the concept of Fisher information, which quantifies how
information about unknown parameters propagates through a physical system and
determines the ultimate limits of precision. With Artificial Neural Networks
(ANNs) gradually becoming an integral part of many measurement systems, it is
essential to understand how they process and transmit parameter-relevant
information internally. Here, we present a method to monitor the flow of Fisher
information through an ANN performing a parameter estimation task, tracking it
from the input to the output layer. We show that optimal estimation performance
corresponds to the maximal transmission of Fisher information, and that
training beyond this point results in information loss due to overfitting. This
provides a model-free stopping criterion for network training-eliminating the
need for a separate validation dataset. To demonstrate the practical relevance
of our approach, we apply it to a network trained on data from an imaging
experiment, highlighting its effectiveness in a realistic physical setting.

</details>


### [371] [Learnable Loss Geometries with Mirror Descent for Scalable and Convergent Meta-Learning](https://arxiv.org/abs/2509.02418)
*Yilang Zhang,Bingcong Li,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 本文提出一种新的元学习方法，通过学习距离生成函数应对复杂损失度量，分析了收敛性，数值测试显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决元学习中样本高效适应先验信息的挑战，尤其是处理复杂损失几何的问题。

Method: 学习一个通用的距离生成函数，用神经网络进行参数化，诱导非线性镜像映射。

Result: 分析证明了所提方法及基于预条件的元学习方法的收敛性，收敛速率与标准基于梯度的元学习方法相当；数值测试显示新算法性能优越，减少适应步骤。

Conclusion: 新算法能有效应对复杂损失几何，在少样本学习中表现良好，适合大规模元学习模型。

Abstract: Utilizing task-invariant knowledge acquired from related tasks as prior
information, meta-learning offers a principled approach to learning a new task
with limited data records. Sample-efficient adaptation of this prior
information is a major challenge facing meta-learning, and plays an important
role because it facilitates training the sought task-specific model with just a
few optimization steps. Past works deal with this challenge through
preconditioning that speeds up convergence of the per-task training. Though
effective in representing locally quadratic loss curvatures, simple linear
preconditioning can be hardly potent with complex loss geometries. Instead of
relying on a quadratic distance metric, the present contribution copes with
complex loss metrics by learning a versatile distance-generating function,
which induces a nonlinear mirror map to effectively capture and optimize a wide
range of loss geometries. With suitable parameterization, this generating
function is effected by an expressive neural network that is provably a valid
distance. Analytical results establish convergence of not only the proposed
method, but also all meta-learning approaches based on preconditioning. To
attain gradient norm less than $\epsilon$, the convergence rate of
$\mathcal{O}(\epsilon^{-2})$ is on par with standard gradient-based
meta-learning methods. Numerical tests on few-shot learning datasets
demonstrate the superior empirical performance of the novel algorithm, as well
as its rapid per-task convergence, which markedly reduces the number of
adaptation steps, hence also accommodating large-scale meta-learning models.

</details>


### [372] [VASSO: Variance Suppression for Sharpness-Aware Minimization](https://arxiv.org/abs/2509.02433)
*Bingcong Li,Yilang Zhang,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 论文提出VASSO方法结合SAM，可在视觉和语言任务上提升泛化能力，还能实现泛化与计算的平衡。


<details>
  <summary>Details</summary>
Motivation: SAM在实际中存在‘过度友好的对手’问题，会限制泛化能力，需解决此问题。

Method: 提出VASSO方法，通过方差抑制来稳定对手，并将其与SAM结合。

Result: 在广泛的视觉和语言任务上，VASSO与SAM结合在数值上验证了更好的泛化能力；应用于高效SAM变体时，实现了泛化与计算的理想权衡。

Conclusion: VASSO是一种可证明稳定对手的通用方法，与SAM结合能提升模型泛化能力并实现泛化 - 计算的良好权衡。

Abstract: Sharpness-aware minimization (SAM) has well-documented merits in enhancing
generalization of deep neural network models. Accounting for sharpness in the
loss function geometry, where neighborhoods of `flat minima' heighten
generalization ability, SAM seeks `flat valleys' by minimizing the maximum loss
provoked by an adversarial perturbation within the neighborhood. Although
critical to account for sharpness of the loss function, in practice SAM suffers
from `over-friendly adversaries,' which can curtail the outmost level of
generalization. To avoid such `friendliness,' the present contribution fosters
stabilization of adversaries through variance suppression (VASSO). VASSO offers
a general approach to provably stabilize adversaries. In particular, when
integrating VASSO with SAM, improved generalizability is numerically validated
on extensive vision and language tasks. Once applied on top of a
computationally efficient SAM variant, VASSO offers a desirable
generalization-computation tradeoff.

</details>


### [373] [Generative Sequential Notification Optimization via Multi-Objective Decision Transformers](https://arxiv.org/abs/2509.02458)
*Borja Ocejo,Ruofan Wang,Ke Liu,Rohit K. Patra,Haotian Shen,David Liu,Yiwen Yuan,Gokulraj Mohanasundaram,Fedor Borisyuk,Prakruthi Prabhakar*

Main category: cs.LG

TL;DR: 提出基于Decision Transformer的框架优化通知推送，实验表明能提升通知效用、减少用户疲劳，比CQL更优。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法在优化通知推送规模应用时面临稳定性、分布偏移敏感等实际挑战。

Method: 采用基于Decision Transformer的框架，将策略学习重构为回报条件监督学习，包括与CQL对比、多奖励设计、分位数回归等。

Result: 在已部署的通知系统中实验显示，该方法提升通知效用和整体会话活跃度，减少用户疲劳，在领英通知决策中会话增加0.72%。

Conclusion: 基于Decision Transformer的方法比基于多目标CQL的代理更能使通知推荐更相关，优化通知推送效果。

Abstract: Notifications are an important communication channel for delivering timely
and relevant information. Optimizing their delivery involves addressing complex
sequential decision-making challenges under constraints such as message utility
and user fatigue. Offline reinforcement learning (RL) methods, such as
Conservative Q-Learning (CQL), have been applied to this problem but face
practical challenges at scale, including instability, sensitivity to
distribution shifts, limited reproducibility, and difficulties with
explainability in high-dimensional recommendation settings. We present a
Decision Transformer (DT) based framework that reframes policy learning as
return-conditioned supervised learning, improving robustness, scalability, and
modeling flexibility. Our contributions include a real-world comparison with
CQL, a multi-reward design suitable for non-episodic tasks, a quantile
regression approach to return-to-go conditioning, and a production-ready system
with circular buffer-based sequence processing for near-real-time inference.
Extensive offline and online experiments in a deployed notification system show
that our approach improves notification utility and overall session activity
while minimizing user fatigue. Compared to a multi-objective CQL-based agent,
the DT-based approach achieved a +0.72% increase in sessions for notification
decision-making at LinkedIn by making notification recommendation more
relevant.

</details>


### [374] [Exploring Variational Graph Autoencoders for Distribution Grid Data Generation](https://arxiv.org/abs/2509.02469)
*Syed Zain Abbas,Ehimare Okoyomon*

Main category: cs.LG

TL;DR: 为解决能源网络机器学习研究中公共电力系统数据不足问题，研究用VGAEs生成合成配电网，评估四种解码器变体，发现GCN方法有优势也有局限，开源模型和分析以推动研究。


<details>
  <summary>Details</summary>
Motivation: 解决能源网络机器学习研究中公共电力系统数据缺乏的问题。

Method: 使用ENGAGE和DINGO两个开源数据集，评估四种解码器变体，用结构和光谱指标将生成网络与原始电网对比。

Result: 简单解码器无法捕捉真实拓扑，基于GCN的方法在ENGAGE数据集表现好，但在更复杂的DINGO数据集存在生成不连通组件和重复图案等问题。

Conclusion: VGAEs用于电网合成有前景也有局限，需要更具表现力的生成模型和可靠评估，开源成果以推动电力系统研究。

Abstract: To address the lack of public power system data for machine learning research
in energy networks, we investigate the use of variational graph autoencoders
(VGAEs) for synthetic distribution grid generation. Using two open-source
datasets, ENGAGE and DINGO, we evaluate four decoder variants and compare
generated networks against the original grids using structural and spectral
metrics. Results indicate that simple decoders fail to capture realistic
topologies, while GCN-based approaches achieve strong fidelity on ENGAGE but
struggle on the more complex DINGO dataset, producing artifacts such as
disconnected components and repeated motifs. These findings highlight both the
promise and limitations of VGAEs for grid synthesis, underscoring the need for
more expressive generative models and robust evaluation. We release our models
and analysis as open source to support benchmarking and accelerate progress in
ML-driven power system research.

</details>


### [375] [SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning](https://arxiv.org/abs/2509.02479)
*Zhenghai Xue,Longtao Zheng,Qian Liu,Yingru Li,Xiaosen Zheng,Zejun Ma,Bo An*

Main category: cs.LG

TL;DR: 提出SimpleTIR算法解决多轮工具集成推理训练不稳定问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多轮工具集成推理（TIR）使用强化学习训练时的不稳定和性能崩溃问题。

Method: 引入SimpleTIR算法，识别并过滤包含无效回合的轨迹以稳定训练。

Result: 在数学推理基准测试中达到SOTA，如将AIME24分数从22.1提升到50.5，鼓励模型发现多样推理模式。

Conclusion: SimpleTIR能有效稳定多轮TIR训练，提升推理性能。

Abstract: Large Language Models (LLMs) can significantly improve their reasoning
capabilities by interacting with external tools, a paradigm known as
Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios
using Reinforcement Learning (RL) is often hindered by training instability and
performance collapse. We identify that such instability is primarily caused by
a distributional drift from external tool feedback, leading to the generation
of low-probability tokens. This issue compounds over successive turns, causing
catastrophic gradient norm explosions that derail the training process. To
address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that
stabilizes multi-turn TIR training. Its core strategy is to identify and filter
out trajectories containing void turns, i.e., turns that yield neither a code
block nor a final answer. By removing these problematic trajectories from the
policy update, SimpleTIR effectively blocks the harmful, high-magnitude
gradients, thus stabilizing the learning dynamics. Extensive experiments show
that SimpleTIR achieves state-of-the-art performance on challenging math
reasoning benchmarks, notably elevating the AIME24 score from a text-only
baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model.
Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR
encourages the model to discover diverse and sophisticated reasoning patterns,
such as self-correction and cross-validation.

</details>


### [376] [RNN Generalization to Omega-Regular Languages](https://arxiv.org/abs/2509.02491)
*Charles Pert,Dalal Alrajeh,Alessandra Russo*

Main category: cs.LG

TL;DR: 研究RNN能否泛化到LTL公式导出的ω-正则语言，实验表明RNN在目标语言上准确率高，显示神经方法学习复杂ω-正则语言的可行性。


<details>
  <summary>Details</summary>
Motivation: Büchi自动机处理复杂系统行为有可扩展性挑战，神经网络用于解决此类问题，需研究其泛化能力。

Method: 在最终周期ω-字序列上训练RNN以复制目标BA行为，评估其对分布外序列的泛化能力。

Result: 在比训练样本长8倍的序列上评估，RNN在目标ω-正则语言上准确率高，92.6%的任务实现完美或接近完美泛化。

Conclusion: 神经方法学习复杂ω-正则语言可行，有望成为神经符号验证方法的组成部分。

Abstract: B\"uchi automata (BAs) recognize $\omega$-regular languages defined by formal
specifications like linear temporal logic (LTL) and are commonly used in the
verification of reactive systems. However, BAs face scalability challenges when
handling and manipulating complex system behaviors. As neural networks are
increasingly used to address these scalability challenges in areas like model
checking, investigating their ability to generalize beyond training data
becomes necessary. This work presents the first study investigating whether
recurrent neural networks (RNNs) can generalize to $\omega$-regular languages
derived from LTL formulas. We train RNNs on ultimately periodic $\omega$-word
sequences to replicate target BA behavior and evaluate how well they generalize
to out-of-distribution sequences. Through experiments on LTL formulas
corresponding to deterministic automata of varying structural complexity, from
3 to over 100 states, we show that RNNs achieve high accuracy on their target
$\omega$-regular languages when evaluated on sequences up to $8 \times$ longer
than training examples, with $92.6\%$ of tasks achieving perfect or
near-perfect generalization. These results establish the feasibility of neural
approaches for learning complex $\omega$-regular languages, suggesting their
potential as components in neurosymbolic verification methods.

</details>


### [377] [MoPEQ: Mixture of Mixed Precision Quantized Experts](https://arxiv.org/abs/2509.02512)
*Krishna Teja Chitty-Venkata,Jie Ye,Murali Emani*

Main category: cs.LG

TL;DR: 提出Post Training Quantization算法MoPEQ为专家分配最优位宽，平衡精度与模型大小，实验表明在VLM-MoEs混合精度量化有竞争力。


<details>
  <summary>Details</summary>
Motivation: 大语言和视觉模型的混合专家架构因计算和内存需求大，部署有挑战，需有效量化方法。

Method: 提出MoPEQ算法，用Hessian trace近似分析专家敏感度来分配位宽，聚类相似专家。

Result: 在VLMEvalKit基准数据集上，混合精度量化的MoEs与统一精度基线方法相比，在精度相当情况下内存占用显著改善。

Conclusion: 通过对专家激活频率和敏感度的综合研究，深入理解了VLM-MoEs的混合精度量化。

Abstract: Large Language and Vision Models using a Mixture-of-Experts (MoE)
architecture pose significant challenges for deployment due to their
computational and memory demands. Mixed Precision Quantization assigns
different precisions to different layers of an LLM/VLM based on layer
sensitivity and importance within the model. In this work, we propose a Post
Training Quantization algorithm, MoPEQ, that assigns optimal bit width to each
expert. Our method balances accuracy and model size by analyzing each expert's
sensitivity using Hessian trace approximation instead of relying on the
activation frequency of the expert. This per-expert granularity approach
clusters similar experts to maintain model performance while reducing memory
requirements. The experimental results on VLMEvalKit benchmark datasets using
State-of-the-art VLMs Deepseek-VL2 -tiny, -small, -base, and MolmoE models
demonstrate that our mixed precision quantized MoEs achieve competitive
accuracy with substantial improvements in memory footprint compared to
uniform-precision baseline methods. We perform a comprehensive study to analyze
the impact of expert activation frequency and sensitivity using Hessian trace
approximation at both layer-wise and model-wide expert precision allocation of
2, 3, and 4 bits to provide a thorough understanding of mixed precision
quantization of VLM-MoEs.

</details>


### [378] [DynaGuard: A Dynamic Guardrail Model With User-Defined Policies](https://arxiv.org/abs/2509.02563)
*Monte Hoover,Vatsal Baherwani,Neel Jain,Khalid Saifullah,Joseph Vincent,Chirag Jain,Melissa Kazemi Rad,C. Bayan Bruss,Ashwinee Panda,Tom Goldstein*

Main category: cs.LG

TL;DR: 提出动态监护人模型，可基于用户定义策略评估文本，检测违规行为，在准确性和速度上有优势。


<details>
  <summary>Details</summary>
Motivation: 标准监护人模型只能检测预定义的静态危害类别，无法满足不同应用领域需求。

Method: 提出动态监护人模型，可用于快速检测策略违规或通过思维链推理阐明和证明模型输出。

Result: 动态监护人模型在静态危害类别检测准确性上与静态模型相当，能以更短时间识别自由形式策略违规，准确性与前沿推理模型相近。

Conclusion: 动态监护人模型可根据用户定义策略评估文本，适用于不同应用领域，具有良好的检测性能。

Abstract: Guardian models are used to supervise and moderate the outputs of user-facing
chatbots, enforcing guardrails and detecting bad behaviors. Standard guardian
models like LlamaGuard detect predefined, static categories of harms. We
propose dynamic guardian models that evaluate text based on user-defined
policies, making them useful for different application domains that are not
addressed by standard guardian models. Our dynamic guardian models can be used
for fast detection of policy violations or with chain-of-thought reasoning that
articulates and justifies the model outputs. Our dynamic guardian models match
static models in detection accuracy for static harm categories while
identifying violations of free-form policies with accuracy comparable to
frontier reasoning models in a fraction of the time.

</details>


### [379] [Understanding sparse autoencoder scaling in the presence of feature manifolds](https://arxiv.org/abs/2509.02565)
*Eric J. Michaud,Liv Gorton,Tom McGrath*

Main category: cs.LG

TL;DR: 本文运用容量分配模型研究稀疏自编码器（SAEs）的缩放行为，发现特征流形会使SAEs学习的特征远少于潜在特征数量，并探讨SAEs在实际中是否处于病态状态。


<details>
  <summary>Details</summary>
Motivation: 理解SAEs的缩放行为以及特征流形对其的影响。

Method: 采用神经缩放文献中的容量分配模型。

Result: 模型恢复了不同的缩放机制，在一种机制下，特征流形会导致SAEs学习的特征远少于潜在特征数量。

Conclusion: 对SAEs在实际中是否处于病态状态进行了初步讨论。

Abstract: Sparse autoencoders (SAEs) model the activations of a neural network as
linear combinations of sparsely occurring directions of variation (latents).
The ability of SAEs to reconstruct activations follows scaling laws w.r.t. the
number of latents. In this work, we adapt a capacity-allocation model from the
neural scaling literature (Brill, 2024) to understand SAE scaling, and in
particular, to understand how "feature manifolds" (multi-dimensional features)
influence scaling behavior. Consistent with prior work, the model recovers
distinct scaling regimes. Notably, in one regime, feature manifolds have the
pathological effect of causing SAEs to learn far fewer features in data than
there are latents in the SAE. We provide some preliminary discussion on whether
or not SAEs are in this pathological regime in the wild.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [380] [Speeding Up the NSGA-II via Dynamic Population Sizes](https://arxiv.org/abs/2509.01739)
*Benjamin Doerr,Martin S. Krejca,Simon Wietheger*

Main category: cs.NE

TL;DR: 提出动态NSGA - II（dNSGA - II）算法，分析其运行时间，相比经典NSGA - II有显著提速。


<details>
  <summary>Details</summary>
Motivation: 经典多目标进化算法（MOEAs）为存储最优权衡需使用大的静态种群，导致算法速度慢，因此提出改进算法。

Method: 基于流行的NSGA - II，设计非静态种群大小的dNSGA - II，通过数学运行时间分析其性能。

Result: 证明dNSGA - II在特定参数下计算OneMinMax基准的完整Pareto前沿的运行时间，在最优参数选择下相比经典NSGA - II有性能提升。

Conclusion: dNSGA - II在运行时间上优于经典NSGA - II，去除参数τ的方法虽有一定减速但仍比经典算法快。

Abstract: Multi-objective evolutionary algorithms (MOEAs) are among the most widely and
successfully applied optimizers for multi-objective problems. However, to store
many optimal trade-offs (the Pareto optima) at once, MOEAs are typically run
with a large, static population of solution candidates, which can slow down the
algorithm. We propose the dynamic NSGA-II (dNSGA-II), which is based on the
popular NSGA-II and features a non-static population size. The dNSGA-II starts
with a small initial population size of four and doubles it after a
user-specified number $\tau$ of function evaluations, up to a maximum size of
$\mu$. Via a mathematical runtime analysis, we prove that the dNSGA-II with
parameters $\mu \geq 4(n + 1)$ and $\tau \geq \frac{256}{50} e n$ computes the
full Pareto front of the \textsc{OneMinMax} benchmark of size $n$ in
$O(\log(\mu) \tau + \mu \log(n))$ function evaluations, both in expectation and
with high probability. For an optimal choice of $\mu$ and $\tau$, the resulting
$O(n \log(n))$ runtime improves the optimal expected runtime of the classic
NSGA-II by a factor of $\Theta(n)$. In addition, we show that the parameter
$\tau$ can be removed when utilizing concurrent runs of the dNSGA-II. This
approach leads to a mild slow-down by a factor of $O(\log(n))$ compared to an
optimal choice of $\tau$ for the dNSGA-II, which is still a speed-up of
$\Theta(n / \log(n))$ over the classic NSGA-II.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [381] [Improving Nonpreemptive Multiserver Job Scheduling with Quickswap](https://arxiv.org/abs/2509.01893)
*Zhongrui Chen,Adityo Anggraito,Diletta Olliaro,Andrea Marin,Marco Ajmone Marsan,Benjamin Berg,Isaac Grosof*

Main category: cs.PF

TL;DR: 针对多服务器作业设计非抢占式调度策略，提出MSFQ策略，性能优于MSF和FCFS。


<details>
  <summary>Details</summary>
Motivation: 现有非抢占式调度策略如FCFS会使核心闲置、MSF会导致作业等待时间波动大，需设计新策略实现高系统利用率和低平均响应时间。

Method: 提出MSF - Quick Swap (MSFQ)策略，通过定期给系统中其他作业优先级来降低作业等待时间的可变性；对MSFQ进行稳定性分析和平均响应时间分析，复杂情况用模拟评估。

Result: 证明在作业请求一个核心或所有核心的情况下，MSFQ显著优于MSF；模拟显示优化后的MSFQ变体在实际多服务器作业负载中大幅优于MSF和FCFS。

Conclusion: MSFQ策略在多服务器作业调度中表现良好，能有效提高系统性能。

Abstract: Modern data center workloads are composed of multiserver jobs, computational
jobs that require multiple CPU cores in order to run. A data center server can
run many multiserver jobs in parallel, as long as it has sufficient resources
to meet their demands. However, multiserver jobs are generally stateful,
meaning that job preemptions incur significant overhead from saving and
reloading the state associated with running jobs. Hence, most systems try to
avoid these costly job preemptions altogether. Given these constraints, a
scheduling policy must determine what set of jobs to run in parallel at each
moment in time to minimize the mean response time across a stream of arriving
jobs. Unfortunately, simple non-preemptive policies such as FCFS may leave many
cores idle, resulting in high mean response times or even system instability.
Our goal is to design and analyze non-preemptive scheduling policies for
multiserver jobs that maintain high system utilization to achieve low mean
response time.
  One well-known non-preemptive policy, Most Servers First (MSF), prioritizes
jobs with higher core requirements and achieves high resource utilization.
However, MSF causes extreme variability in job waiting times, and can perform
significantly worse than FCFS in practice. To address this, we propose and
analyze a class of scheduling policies called MSF-Quick Swap (MSFQ) that
performs well. MSFQ reduces the variability of job waiting times by
periodically granting priority to other jobs in the system. We provide both
stability results and an analysis of mean response time under MSFQ to prove
that our policy dramatically outperforms MSF in the case where jobs request one
core or all the cores. In more complex cases, we evaluate MSFQ in simulation.
We show that, with some additional optimization, variants of the MSFQ policy
can greatly outperform MSF and FCFS on real-world multiserver job workloads.

</details>


### [382] [Non-Asymptotic Performance Analysis of DOA Estimation Based on Real-Valued Root-MUSIC](https://arxiv.org/abs/2509.01999)
*Junyang Liu,Weicheng Zhao,Qingping Wang,Xiangtian Meng,Maria Greco,Fulvio Gini*

Main category: cs.PF

TL;DR: 本文对非渐近条件下的RV - root - MUSIC算法进行理论性能分析，研究其根扰动特性，仿真验证结果正确性，为实际应用中DOA估计参数优化提供理论基础。


<details>
  <summary>Details</summary>
Motivation: RV - root - MUSIC算法存在镜像根估计模糊问题，需对其进行理论性能分析。

Method: 通过共轭扩展法的等效子空间以及真实根和镜像根扰动的等效性，研究噪声子空间扰动、真实根扰动和镜像根扰动特性，建立统计模型并推导扰动的广义表达式。

Result: 仿真结果表明所推导的统计特性正确有效。

Conclusion: 研究结果为雷达、通信网络和智能传感技术等实际应用中DOA估计的参数选择优化提供了坚实理论基础。

Abstract: This paper presents a systematic theoretical performance analysis of the
Real-Valued root-MUSIC (RV-root-MUSIC) algorithm under non-asymptotic
conditions. However, RV-root-MUSIC suffers from the problem of estimation
ambiguity for the mirror roots, therefore the conventional beamforming (CBF)
technique is typically employed to filter out the mirror roots. Through the
equivalent subspace based on the conjugate extension method and the equivalence
of perturbations for both true roots and mirror roots , this paper provides a
comprehensive investigation of three critical aspects: noise subspace
perturbation, true root perturbation, and mirror root perturbation
characteristics in the RV-root-MUSIC algorithm. The statistical model is
established and the generalized expression of perturbation is further
developed. The simulation results show the correctness and validity of the
derived statistical characteristics. The results provide a solid theoretical
foundation for optimizing the parameter selection of DOA estimation in
practical applications, particularly in radar systems, communication networks,
and intelligent sensing technologies.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [383] [LLM-based Triplet Extraction for Automated Ontology Generation in Software Engineering Standards](https://arxiv.org/abs/2509.00140)
*Songhui Yue*

Main category: cs.SE

TL;DR: 提出基于大语言模型辅助的软件工程标准关系三元组提取方法并评估，结果与OpenIE方法相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 自动化本体生成对扩展本体使用至关重要，软件工程标准文本复杂，关系三元组提取是自动化本体生成的第一步。

Method: 提出开源大语言模型辅助的关系三元组提取方法，构建包含文档分割、候选术语挖掘等步骤的自动化本体生成工作流，并构建三个粒度的黄金标准基准进行评估。

Result: 生成的本体与OpenIE的三元组提取方法相当，可能更优。

Conclusion: 基于大语言模型辅助的关系三元组提取方法在软件工程标准的自动化本体生成中有良好效果。

Abstract: Ontologies have supported knowledge representation and whitebox reasoning for
decades; thus, the automated ontology generation (AOG) plays a crucial role in
scaling their use. Software engineering standards (SES) consist of long,
unstructured text (with high noise) and paragraphs with domain-specific terms.
In this setting, relation triple extraction (RTE), together with term
extraction, constitutes the first stage toward AOG. This work proposes an
open-source large language model (LLM)-assisted approach to RTE for SES.
Instead of solely relying on prompt-engineering-based methods, this study
promotes the use of LLMs as an aid in constructing ontologies and explores an
effective AOG workflow that includes document segmentation, candidate term
mining, LLM-based relation inference, term normalization, and cross-section
alignment. Golden-standard benchmarks at three granularities are constructed
and used to evaluate the ontology generated from the study. The results show
that it is comparable and potentially superior to the OpenIE method of triple
extraction.

</details>


### [384] [LLM-Based Program Generation for Triggering Numerical Inconsistencies Across Compilers](https://arxiv.org/abs/2509.00256)
*Yutong Wang,Cindy Rubio-González*

Main category: cs.SE

TL;DR: 提出用大语言模型生成浮点程序的LLM4FP框架，能有效检测浮点不一致性。


<details>
  <summary>Details</summary>
Motivation: 解决编译器间浮点不一致性影响数值软件可靠性的问题。

Method: 结合基于语法的生成和基于反馈的变异生成程序，并在多编译器和优化级别上评估。

Result: 相比Varity能检测到两倍多的不一致性，涉及更多实值差异，在更多优化级别发现不一致，在主机和设备编译器间找到最多不匹配。

Conclusion: 大语言模型引导的程序生成能提升数值不一致性的检测能力。

Abstract: Floating-point inconsistencies across compilers can undermine the reliability
of numerical software. We present LLM4FP, the first framework that uses Large
Language Models (LLMs) to generate floating-point programs specifically
designed to trigger such inconsistencies. LLM4FP combines Grammar-Based
Generation and Feedback-Based Mutation to produce diverse and valid programs.
We evaluate LLM4FP across multiple compilers and optimization levels, measuring
inconsistency rate, time cost, and program diversity. LLM4FP detects over twice
as many inconsistencies compared to the state-of-the-art tool, Varity. Notably,
most of the inconsistencies involve real-valued differences, rather than
extreme values like NaN or infinities. LLM4FP also uncovers inconsistencies
across a wider range of optimization levels, and finds the most mismatches
between host and device compilers. These results show that LLM-guided program
generation improves the detection of numerical inconsistencies.

</details>


### [385] [JS-TOD: Detecting Order-Dependent Flaky Tests in Jest](https://arxiv.org/abs/2509.00466)
*Negar Hashemi,Amjed Tahir,Shawn Rasheed,August Shi,Rachel Blagojevic*

Main category: cs.SE

TL;DR: 介绍JS - TOD工具，可提取、重排和重新运行Jest测试以发现测试顺序依赖导致的不稳定问题，评估揭示了两个主要原因。


<details>
  <summary>Details</summary>
Motivation: 测试顺序依赖是测试不稳定的主要原因之一，理想测试应独立运行，但实际中测试结果受执行顺序影响，需要工具来发现该问题。

Method: 采用系统方法对测试、测试套件和describe块进行随机化，工具高度可定制，可设置重排和重运行次数。

Result: 使用JS - TOD评估揭示出测试顺序依赖不稳定的两个主要原因：测试间共享文件和共享模拟状态。

Conclusion: JS - TOD工具能有效发现Jest测试中因顺序依赖导致的测试不稳定问题。

Abstract: We present JS-TOD (JavaScript Test Order-dependency Detector), a tool that
can extract, reorder, and rerun Jest tests to reveal possible order-dependent
test flakiness. Test order dependency is one of the leading causes of test
flakiness. Ideally, each test should operate in isolation and yield consistent
results no matter the sequence in which tests are run. However, in practice,
test outcomes can vary depending on their execution order. JS-TOD employed a
systematic approach to randomising tests, test suites, and describe blocks. The
tool is highly customisable, as one can set the number of orders and reruns
required (the default setting is 10 reorder and 10 reruns for each test and
test suite). Our evaluation using JS-TOD reveals two main causes of test order
dependency flakiness: shared files and shared mocking state between tests.

</details>


### [386] [Bug Whispering: Towards Audio Bug Reporting](https://arxiv.org/abs/2509.00785)
*Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 探讨移动端应用让用户录制音频报bug，分析音频报bug面临的挑战并鼓励进一步研究。


<details>
  <summary>Details</summary>
Motivation: 音频录制易实现，可增加bug报告数量，提高bug识别和修复率，但音频报bug有挑战现有技术的特性。

Method: 基于初步实验讨论音频报bug面临的挑战。

Result: 发现音频报bug存在挑战现有技术的特性。

Conclusion: 鼓励对音频报bug的收集和分析进行进一步研究。

Abstract: Bug reporting is a key feature of mobile applications, as it enables
developers to collect information about faults that escaped testing and thus
affected end-users. This paper explores the idea of allowing end-users to
immediately report the problems that they experience by recording and
submitting audio messages. Audio recording is simple to implement and has the
potential to increase the number of bug reports that development teams can
gather, thus potentially improving the rate at which bugs are identified and
fixed. However, audio bug reports exhibit specific characteristics that
challenge existing techniques for reproducing bugs. This paper discusses these
challenges based on a preliminary experiment, and motivates further research on
the collection and analysis of audio-based bug reports

</details>


### [387] [REConnect: Participatory RE that Matters](https://arxiv.org/abs/2509.01006)
*Daniela Damian,Bachan Ghimire,Ze Shi Li*

Main category: cs.SE

TL;DR: 介绍REConnect方法，强调以人类连接为中心的需求工程，结合案例阐述原则和实践，并探讨其与生成式AI的关系。


<details>
  <summary>Details</summary>
Motivation: 当前流行的需求工程实践可能使需求工作脱离文化、社会和政治背景，需重新聚焦人类连接以确保系统符合人类价值观和用户需求。

Method: 引入REConnect方法，采用以人为本的参与式方法，结合三个社会影响案例研究，提出三个关键原则和一套可操作实践。

Result: REConnect的三个关键原则能产生具有文化基础、社会合法性和可持续性的需求，其可操作实践贯穿需求工程各阶段。

Conclusion: 在生成式AI时代，REConnect方法的参与式实践应指导AI集成，保障人类能动性和价值观伦理。

Abstract: Software increasingly shapes the infrastructures of daily life, making
requirements engineering (RE) central to ensuring that systems align with human
values and lived experiences. Yet, current popular practices such as CrowdRE
and AI-assisted elicitation strategies risk detaching requirements work from
the cultural, social, and political contexts that shape lived experiences,
human values, and real user needs. In this paper, we introduce REConnect that
re-centers RE on the human connection as central to the understanding of lived
experiences where impact is sought. REConnect advocates for a human-centered
participatory approach "that matters" to the communities and beneficiaries
involved, ensuring alignment with their values and aspirations. Drawing on
three case studies of societal impact: BloodSync in rural Nepal, Herluma
supporting women at risk of homelessness in Canada, and BridgingRoots to
revitalize Indigenous languages in the Canadian Arctic. REConnect argues that
three key principles and enablers: building trusting relationships,
co-designing with and alongside stakeholders, and empowering users as agents of
change, can yield requirements that are culturally grounded, socially
legitimate, and sustainable beyond system delivery. REConnect also proposes a
set of actionable practices (REActions) that embed relationality and ongoing
stakeholder engagement throughout requirements elicitation, analysis, and
validation of solution development. Finally, we situate REConnect in the era of
Generative AI. While AI can accelerate and scale certain RE tasks, its
integration must be guided by participatory practices that not only preserve
human agency but also empower humans' roles to become guardians of values and
ethics, inclusion amplifiers, curators of AI outputs, and co-reflectors in
iterative review cycles.

</details>


### [388] [Generative Goal Modeling](https://arxiv.org/abs/2509.01048)
*Ateeq Sharfuddin,Travis Breaux*

Main category: cs.SE

TL;DR: 本文提出用文本蕴含方法从访谈记录中提取目标并构建目标模型，经评估，GPT - 4o能较可靠地提取目标、追踪来源文本及生成目标模型细化关系。


<details>
  <summary>Details</summary>
Motivation: 在软件工程中，业务分析师需审查访谈记录来识别和记录需求，目标建模是表示早期涉众需求的常用技术，因此希望找到可靠从访谈记录中提取目标并构建目标模型的方法。

Method: 使用文本蕴含方法从访谈记录中提取目标并构建目标模型，在15份涵盖29个应用领域的访谈记录上进行评估。

Result: GPT - 4o能可靠地从访谈记录中提取目标，与人工提取目标的匹配率为62.0%，能以98.7%的准确率追踪目标在记录中的原文，经人工评估，生成目标模型细化关系的准确率为72.2%。

Conclusion: 所提出的用文本蕴含提取目标和构建目标模型的方法可行，GPT - 4o在相关任务中有较好表现。

Abstract: In software engineering, requirements may be acquired from stakeholders
through elicitation methods, such as interviews, observational studies, and
focus groups. When supporting acquisition from interviews, business analysts
must review transcripts to identify and document requirements. Goal modeling is
a popular technique for representing early stakeholder requirements as it lends
itself to various analyses, including refinement to map high-level goals into
software operations, and conflict and obstacle analysis. In this paper, we
describe an approach to use textual entailment to reliably extract goals from
interview transcripts and to construct goal models. The approach has been
evaluated on 15 interview transcripts across 29 application domains. The
findings show that GPT-4o can reliably extract goals from interview
transcripts, matching 62.0% of goals acquired by humans from the same
transcripts, and that GPT-4o can trace goals to originating text in the
transcript with 98.7% accuracy. In addition, when evaluated by human
annotators, GPT-4o generates goal model refinement relationships among
extracted goals with 72.2% accuracy.

</details>


### [389] [A Survey on the Techniques and Tools for Automated Requirements Elicitation and Analysis of Mobile Apps](https://arxiv.org/abs/2509.01068)
*Chong Wang,Haoning Wu,Peng Liang,Maya Daneva,Marten van Sinderen*

Main category: cs.SE

TL;DR: 本文开展系统映射研究，调查移动应用自动需求获取与分析技术和工具现状，发现半自动化技术常用，工具多为开源非自研，主要支持需求分析、挖掘和分类任务。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对移动应用自动需求获取与分析技术、工具特点及支持的需求工程任务的了解，本文旨在调查其现状。

Method: 按照Kitchenham等人的指南开展系统映射研究。

Result: 基于73篇论文，发现最常用半自动化技术，工具主要特点是开源非自研，用于需求分析和文本预处理，最受关注的三个需求工程任务是需求分析、挖掘和分类。

Conclusion: 移动应用自动需求获取与分析技术和工具使用在增长；相关研究主要使用半自动化技术；需求分析、挖掘和分类是受自动技术和工具支持最多的三个任务；最流行的工具是开源非自研，主要用于需求分析和文本处理。

Abstract: [Background:] Research on automated requirements elicitation and analysis of
mobile apps employed lots of techniques and tools proposed by RE researchers
and practitioners. However, little is known about the characteristics of these
techniques and tools as well as the RE tasks in requirements elicitation and
analysis that got supported with the help of respective techniques and tools.
[Aims:] The goal of this paper is to investigate the state-of-the-art of the
techniques and tools used in automated requirements elicitation and analysis of
mobile apps. [Method:] We carried out a systematic mapping study by following
the guidelines of Kitchenham et al. [Results:] Based on 73 selected papers, we
found the most frequently used techniques - semi-automatic techniques, and the
main characteristics of the tools - open-sourced and non-self-developed tools
for requirements analysis and text pre-processing. Plus, the most three
investigated RE tasks are requirements analysis, mining and classification.
[Conclusions:] Our most important conclusions are: (1) there is a growth in the
use of techniques and tools in automated requirements elicitation and analysis
of mobile apps, (2) semi-automatic techniques are mainly used in the
publications on this research topic, (3) requirements analysis, mining and
classification are the top three RE tasks with the support of automatic
techniques and tools, and (4) the most popular tools are open-sourced and
non-self-developed, and they are mainly used in requirements analysis and text
processing.

</details>


### [390] [Compiler Bugs Detection in Logic Synthesis Tools via Linear Upper Confidence Bound](https://arxiv.org/abs/2509.01149)
*Hui Zeng,Zhihao Xu,Hui Li,Siwen Wang,Qian Ma*

Main category: cs.SE

TL;DR: 提出Lin - Hunter框架提升FPGA逻辑综合工具验证效率，发现多个新bug且表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA逻辑综合工具测试方法依赖随机选择，生成HDL测试用例结构多样性不足，无法充分探索工具特性空间。

Method: 引入一套有原则的变形转换规则生成功能等效但结构多样的HDL测试用例变体，集成基于LinUCB方法的自适应策略选择机制。

Result: 三个月实验发现18个独特bug，其中10个未报告，官方开发者已确认，在测试用例多样性和bug发现效率上优于现有方法。

Conclusion: Lin - Hunter框架能有效提升FPGA逻辑综合工具验证的效率和测试用例的多样性。

Abstract: Field-Programmable Gate Arrays (FPGAs) play an indispensable role in
Electronic Design Automation (EDA), translating Register-Transfer Level (RTL)
designs into gate-level netlists. The correctness and reliability of FPGA logic
synthesis tools are critically important, as unnoticed bugs in these tools may
infect the final hardware implementations. However, recent approaches often
rely heavily on random selection strategies, limiting the structural diversity
of the generated HDL test cases and resulting in inadequate exploration of the
tool's feature space. To address this limitation, we propose Lin-Hunter, a
novel testing framework designed to systematically enhance the diversity of HDL
test cases and the efficiency of FPGA logic synthesis tool validation.
Specifically, Lin-Hunter introduces a principled set of metamorphic
transformation rules to generate functionally equivalent yet structurally
diverse HDL test case variants, effectively addressing the limited diversity of
existing test inputs. To further enhance bug discovery efficiency, Lin-Hunter
integrates an adaptive strategy selection mechanism based on the Linear Upper
Confidence Bound (LinUCB) method. This method leverages feedback from synthesis
logs of previously executed test cases to dynamically prioritize transformation
strategies that have empirically demonstrated a higher likelihood of triggering
synthesis bugs. Comprehensive experiments conducted over a three-month period
demonstrate the practical effectiveness of Lin-Hunter. Our method has
discovered 18 unique bugs, including 10 previously unreported defects, which
have been confirmed by official developers. Moreover, our method outperforms
state-of-the-art testing methods in both test-case diversity and bug-discovery
efficiency.

</details>


### [391] [Policy-driven Software Bill of Materials on GitHub: An Empirical Study](https://arxiv.org/abs/2509.01255)
*Oleksii Novikov,Davide Fucci,Oleksandr Adamov,Daniel Mendez*

Main category: cs.SE

TL;DR: 研究开源项目中策略驱动型SBOM现状，发现流行GitHub仓库中仅0.56%含此类SBOM，依赖项有漏洞且部分无许可信息。


<details>
  <summary>Details</summary>
Motivation: 了解开源项目中策略驱动型SBOM的当前状态，因相关研究尚处早期。

Method: 进行软件仓库挖掘研究，收集GitHub上的SBOM文件，用描述性统计分析策略驱动型SBOM信息和依赖项漏洞。

Result: 仅0.56%的流行GitHub仓库含策略驱动型SBOM，依赖项有2202个独特漏洞，22%未报告许可信息。

Conclusion: 研究结果为SBOM用于安全评估和许可提供了见解。

Abstract: Background. The Software Bill of Materials (SBOM) is a machine-readable list
of all the software dependencies included in a software. SBOM emerged as way to
assist securing the software supply chain. However, despite mandates from
governments to use SBOM, research on this artifact is still in its early
stages. Aims. We want to understand the current state of SBOM in open-source
projects, focusing specifically on policy-driven SBOMs, i.e., SBOM created to
achieve security goals, such as enhancing project transparency and ensuring
compliance, rather than being used as fixtures for tools or artificially
generated for benchmarking or academic research purposes. Method. We performed
a mining software repository study to collect and carefully select SBOM files
hosted on GitHub. We analyzed the information reported in policy-driven SBOMs
and the vulnerabilities associated with the declared dependencies by means of
descriptive statistics. Results. We show that only 0.56% of popular GitHub
repositories contain policy-driven SBOM. The declared dependencies contain
2,202 unique vulnerabilities, while 22% of them do not report licensing
information. Conclusion. Our findings provide insights for SBOM usage to
support security assessment and licensing.

</details>


### [392] [Metamorphic Testing of Multimodal Human Trajectory Prediction](https://arxiv.org/abs/2509.01294)
*Helge Spieker,Nadjib Lazaar,Arnaud Gotlieb,Nassim Belmecheri*

Main category: cs.SE

TL;DR: 本文提出用变质测试（MT）方法测试多模态人类轨迹预测（HTP）系统，解决了无明确测试预言机的问题。


<details>
  <summary>Details</summary>
Motivation: 人类轨迹预测对自动驾驶系统很重要，但测试多模态HTP模型因无明确测试预言机存在挑战，需解决此问题。

Method: 提出五个变质关系（MRs），针对历史轨迹数据和语义分割地图进行变换，还提出基于概率分布距离度量的概率违反标准。

Result: 开发了一个用于无预言机测试多模态、随机HTP系统的MT框架。

Conclusion: 该框架可在不依赖真实轨迹的情况下评估模型对输入变换和上下文变化的鲁棒性。

Abstract: Context: Predicting human trajectories is crucial for the safety and
reliability of autonomous systems, such as automated vehicles and mobile
robots. However, rigorously testing the underlying multimodal Human Trajectory
Prediction (HTP) models, which typically use multiple input sources (e.g.,
trajectory history and environment maps) and produce stochastic outputs
(multiple possible future paths), presents significant challenges. The primary
difficulty lies in the absence of a definitive test oracle, as numerous future
trajectories might be plausible for any given scenario. Objectives: This
research presents the application of Metamorphic Testing (MT) as a systematic
methodology for testing multimodal HTP systems. We address the oracle problem
through metamorphic relations (MRs) adapted for the complexities and stochastic
nature of HTP. Methods: We present five MRs, targeting transformations of both
historical trajectory data and semantic segmentation maps used as an
environmental context. These MRs encompass: 1) label-preserving geometric
transformations (mirroring, rotation, rescaling) applied to both trajectory and
map inputs, where outputs are expected to transform correspondingly. 2)
Map-altering transformations (changing semantic class labels, introducing
obstacles) with predictable changes in trajectory distributions. We propose
probabilistic violation criteria based on distance metrics between probability
distributions, such as the Wasserstein or Hellinger distance. Conclusion: This
study introduces tool, a MT framework for the oracle-less testing of
multimodal, stochastic HTP systems. It allows for assessment of model
robustness against input transformations and contextual changes without
reliance on ground-truth trajectories.

</details>


### [393] [Aligning Requirement for Large Language Model's Code Generation](https://arxiv.org/abs/2509.01313)
*Zhao Tian,Junjie Chen*

Main category: cs.SE

TL;DR: 现有大语言模型代码生成存在规范对齐问题，提出Specine技术，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的代码常与编程规范不匹配，现有基于代理的技术忽略规范感知问题，而准确感知规范对代码生成至关重要。

Method: 借鉴软件需求工程，提出Specine技术，识别未对齐的输入规范、提升大语言模型感知的规范并进行对齐。

Result: 在四个最先进的大语言模型和五个具有挑战性的基准测试中与十个最先进的基线进行对比，Specine表现有效，在Pass@1指标上平均提升29.60%。

Conclusion: Specine技术能有效提升大语言模型的代码生成性能。

Abstract: Code generation refers to the automatic generation of source code based on a
given programming specification, which has garnered significant attention
particularly with the advancement of large language models (LLMs). However, due
to the inherent complexity of real-world problems, the LLM-generated code often
fails to fully align with the provided specification. While state-of-the-art
agent-based techniques have been proposed to enhance LLM code generation, they
overlook the critical issue of specification perception, resulting in
persistent misalignment issues. Given that accurate perception of programming
specifications serves as the foundation of the LLM-based code generation
paradigm, ensuring specification alignment is particularly crucial. In this
work, we draw on software requirements engineering to propose Specine, a novel
specification alignment technique for LLM code generation. Its key idea is to
identify misaligned input specifications, lift LLM-perceived specifications,
and align them to enhance the code generation performance of LLMs. Our
comprehensive experiments on four state-of-the-art LLMs across five challenging
competitive benchmarks by comparing with ten state-of-the-art baselines,
demonstrate the effectiveness of Specine. For example, Specine outperforms the
most effective baseline, achieving an average improvement of 29.60\% across all
subjects in terms of Pass@1.

</details>


### [394] [Leveraging SystemC-TLM-based Virtual Prototypes for Embedded Software Fuzzing](https://arxiv.org/abs/2509.01318)
*Chiara Ghinami,Jonas Winzer,Nils Bosbach,Lennart M. Reimann,Lukas Jünger,Simon Wörner,Rainer Leupers*

Main category: cs.SE

TL;DR: 提出集成AFL fuzzer和SystemC模拟器的框架，解决现有嵌入式软件测试方案的局限，展示了其灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入式软件测试方案中fuzzer与模拟器耦合紧密，缺乏硬件外设支持且灵活性有限，需改进。

Method: 提出框架，用harness解耦fuzzer和模拟器，拦截外设访问并向fuzzer查询值。

Result: 实现了模拟环境中外设的灵活互换，支持不同SystemC虚拟原型接口。

Conclusion: 通过与不同模拟器集成和测试不同软件，证明了方案的灵活性。

Abstract: SystemC-based virtual prototypes have emerged as widely adopted tools to test
software ahead of hardware availability, reducing the time-to-market and
improving software reliability. Recently, fuzzing has become a popular method
for automated software testing due to its ability to quickly identify
corner-case errors. However, its application to embedded software is still
limited. Simulator tools can help bridge this gap by providing a more powerful
and controlled execution environment for testing. Existing solutions, however,
often tightly couple fuzzers with built-in simulators that lack support for
hardware peripherals and of- fer limited flexibility, restricting their ability
to test embedded software. To address these limitations, we present a framework
that allows the integration of American-Fuzzy-Lop-based fuzzers and
SystemC-based simulators. The framework provides a harness to decouple the
adopted fuzzer and simulator. In addition, it intercepts peripheral accesses
and queries the fuzzer for values, effectively linking peripheral behavior to
the fuzzer. This solution enables flexible interchangeability of peripher- als
within the simulation environment and supports the interfacing of different
SystemC-based virtual prototypes. The flexibility of the pro- posed solution is
demonstrated by integrating the harness with different simulators and by
testing various softwares.

</details>


### [395] [Towards Multi-Platform Mutation Testing of Task-based Chatbots](https://arxiv.org/abs/2509.01389)
*Diego Clerissi,Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 提出MUTABOT突变测试方法用于注入故障，将其扩展到多平台，并通过实验展示该方法可揭示Botium测试套件弱点。


<details>
  <summary>Details</summary>
Motivation: 全面测试基于任务的聊天机器人所有可能对话场景具有挑战性，可能存在未被发现的错误行为。

Method: 提出MUTABOT突变测试方法注入故障，将其扩展到Dialogflow和Rasa多平台。

Result: 实验展示了突变测试可揭示Botium最先进测试生成器生成的测试套件的弱点。

Conclusion: MUTABOT突变测试方法能有效发现测试套件的不足，可用于多平台。

Abstract: Chatbots, also known as conversational agents, have become ubiquitous,
offering services for a multitude of domains. Unlike general-purpose chatbots,
task-based chatbots are software designed to prioritize the completion of tasks
of the domain they handle (e.g., flight booking). Given the growing popularity
of chatbots, testing techniques that can generate full conversations as test
cases have emerged. Still, thoroughly testing all the possible conversational
scenarios implemented by a task-based chatbot is challenging, resulting in
incorrect behaviors that may remain unnoticed. To address this challenge, we
proposed MUTABOT, a mutation testing approach for injecting faults in
conversations and producing faulty chatbots that emulate defects that may
affect the conversational aspects. In this paper, we present our extension of
MUTABOT to multiple platforms (Dialogflow and Rasa), and present experiments
that show how mutation testing can be used to reveal weaknesses in test suites
generated by the Botium state-of-the-art test generator.

</details>


### [396] [Non Technical Debt in Agile Software Development](https://arxiv.org/abs/2509.01445)
*Muhammad Ovais Ahmad,Tomas Gustavsson*

Main category: cs.SE

TL;DR: 本文研究非技术债务对大规模敏捷软件开发的影响，通过调查分析找出关键驱动因素及影响，并提出实用策略以减少债务、释放团队潜力。


<details>
  <summary>Details</summary>
Motivation: 了解非技术债务的不同类型如何干扰大规模敏捷软件开发环境。

Method: 通过广泛调查、深度访谈和统计分析，对不同软件专业人员进行研究。

Result: 发现结构良好的团队学习和适应能力强；心理安全对创新和留住员工至关重要；低效流程和角色不清影响工作满意度等；社交碎片化导致返工和成本增加；忽视人力资源需求限制组织发展。

Conclusion: 提出优化团队组成、明确角色等实用策略，可减少非技术债务，释放团队潜力。

Abstract: NonTechnical Debt (NTD) is a common challenge in agile software development,
manifesting in four critical forms, Process Debt, Social Debt, People Debt,
Organizational debt. NODLA project is a collaboration between Karlstad
University and four leading Swedish industrial partners, reveals how various
debt types disrupt large scale Agile Software Development (ASD) environments.
Through extensive surveys, indepth interviews, and statistical analyses
involving a diverse group of software professionals, we identified key drivers
of NTD and their impacts. Our findings emphasize (1) Well structured, highly
cohesive teams learn faster, adapt more effectively, and innovate consistently.
(2) Psychological safety, fostered by proactive leadership, is essential for
innovation, experimentation, and keeping employees. (3) Inefficient processes
and unclear roles contribute significantly to drops in job satisfaction,
productivity and team morale. (4) Social fragmentation, particularly in remote
and hybrid settings, breeds rework, delays, and increased costs. (5) Neglected
human resource needs, such as delayed hiring or insufficient training, limit an
organization ability to meet growing demands. This white paper distils these
insights into practical, evidence based strategies, such as refining team
composition, clarifying roles, fostering psychological safety, streamlining
workflows, and embracing failure as a learning tool. By implementing these
strategies, organizations can reduce NTD, reclaim agility, and unlock their
teams full potential.

</details>


### [397] [Benchmarking and Studying the LLM-based Code Review](https://arxiv.org/abs/2509.01494)
*Zhengran Zeng,Ruikai Shi,Keke Han,Yixin Li,Kaicheng Sun,Yidong Wang,Zhuohao Yu,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: 现有自动代码审查（ACR）基准有局限，本文提出SWRBench基准及评估方法，评估发现当前系统表现不佳，提出多审查聚合策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有ACR基准无法反映现实复杂性，难以评估现代大语言模型，存在聚焦细粒度代码单元、缺乏完整项目上下文和评估指标不足等问题。

Method: 引入包含1000个手动验证PR的SWRBench基准，采用基于LLM的客观评估方法，提出多审查聚合策略。

Result: 对主流ACR工具和LLM的评估显示当前系统表现不佳，ACR工具更擅长检测功能错误，多审查聚合策略使F1分数最多提高43.67%。

Conclusion: 贡献了SWRBench基准、评估方法、对当前ACR能力的研究及有效提升方法，为ACR研究提供有价值见解。

Abstract: Automated Code Review (ACR) is crucial for software quality, yet existing
benchmarks often fail to reflect real-world complexities, hindering the
evaluation of modern Large Language Models (LLMs). Current benchmarks
frequently focus on fine-grained code units, lack complete project context, and
use inadequate evaluation metrics. To address these limitations, we introduce
SWRBench , a new benchmark comprising 1000 manually verified Pull Requests
(PRs) from GitHub, offering PR-centric review with full project context.
SWRBench employs an objective LLM-based evaluation method that aligns strongly
with human judgment (~90 agreement) by verifying if issues from a structured
ground truth are covered in generated reviews. Our systematic evaluation of
mainstream ACR tools and LLMs on SWRBench reveals that current systems
underperform, and ACR tools are more adept at detecting functional errors.
Subsequently, we propose and validate a simple multi-review aggregation
strategy that significantly boosts ACR performance, increasing F1 scores by up
to 43.67%. Our contributions include the SWRBench benchmark, its objective
evaluation method, a comprehensive study of current ACR capabilities, and an
effective enhancement approach, offering valuable insights for advancing ACR
research.

</details>


### [398] [A Privacy-Preserving Recommender for Filling Web Forms Using a Local Large Language Model](https://arxiv.org/abs/2509.01527)
*Amirreza Nayyeri,Abbas Rasoolzadegan*

Main category: cs.SE

TL;DR: 本文介绍了一种使用大语言模型本地运行的隐私保护推荐工具，可辅助测试人员进行网页表单测试。


<details>
  <summary>Details</summary>
Motivation: 网页应用需确保无故障运行，网页表单测试中手动生成测试值耗时易错，现有基于大语言模型的工具存在数据泄露风险。

Method: 该工具分析表单的HTML结构，检测输入类型，根据字段类型和上下文内容提取约束条件。

Result: 未提及。

Conclusion: 提出了可本地运行、保护隐私的网页表单测试辅助工具。

Abstract: Web applications are increasingly used in critical domains such as education,
finance, and e-commerce. This highlights the need to ensure their failure-free
performance. One effective method for evaluating failure-free performance is
web form testing, where defining effective test scenarios is key to a complete
and accurate evaluation. A core aspect of this process involves filling form
fields with suitable values to create effective test cases. However, manually
generating these values is time-consuming and prone to errors. To address this,
various tools have been developed to assist testers. With the appearance of
large language models (LLMs), a new generation of tools seeks to handle this
task more intelligently. Although many LLM-based tools have been introduced, as
these models typically rely on cloud infrastructure, their use in testing
confidential web forms raises concerns about unintended data leakage and
breaches of confidentiality. This paper introduces a privacy-preserving
recommender that operates locally using a large language model. The tool
assists testers in web form testing by suggesting effective field values. This
tool analyzes the HTML structure of forms, detects input types, and extracts
constraints based on each field's type and contextual content, guiding proper
field filling.

</details>


### [399] [WFC/WFD: Web Fuzzing Commons, Dataset and Guidelines to Support Experimentation in REST API Fuzzing](https://arxiv.org/abs/2509.01612)
*Omur Sahin,Man Zhang,Andrea Arcuri*

Main category: cs.SE

TL;DR: 提出Web Fuzzing Commons (WFC)和Web Fuzzing Dataset (WFD)解决REST API模糊测试问题并通过实验展示其有用性


<details>
  <summary>Details</summary>
Motivation: 解决REST API模糊测试中处理API认证、故障类型分类比较和公平测试案例等阻碍研究进展的问题

Method: 提出WFC（开源库和模式定义）和WFD（含36个开源API的数据集），用EvoMaster进行实验并与其他工具比较

Result: 通过实验展示了WFC/WFD的有用性，讨论了工具比较的常见陷阱

Conclusion: WFC和WFD可帮助解决REST API模糊测试的重要挑战，任何模糊测试器都可从中受益

Abstract: Fuzzing REST APIs is an important research problem, with practical
applications and impact in industry. As such, a lot of research work has been
carried out on this topic in the last few years. However, there are three major
issues that hinder further progress: how to deal with API authentication; how
to catalog and compare different fault types found by different fuzzers; and
what to use as case study to facilitate fair comparisons among fuzzers. To
address these important challenges, we present Web Fuzzing Commons (WFC) and
Web Fuzzing Dataset (WFD). WFC is a set of open-source libraries and schema
definitions to declaratively specify authentication info and catalog different
types of faults that fuzzers can automatically detect. WFD is a collection of
36 open-source APIs with all necessary scaffolding to easily run experiments
with fuzzers, supported by WFC. To show the usefulness of WFC/WFD, a set of
experiments is carried out with EvoMaster, a state-of-the-art fuzzer for Web
APIs. However, any fuzzer can benefit from WFC and WFD. We compare EvoMaster
with other state-of-the-art tools such as ARAT-RL, EmRest, LLamaRestTest,
RESTler, and Schemathesis. We discuss common pitfalls in tool comparisons, as
well as providing guidelines with support of WFC/WFD to avoid them.

</details>


### [400] [Automated Generation of Issue-Reproducing Tests by Combining LLMs and Search-Based Testing](https://arxiv.org/abs/2509.01616)
*Konstantinos Kitsios,Marco Castelluccio,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 提出工具BLAST结合LLM和SBST自动从问题-补丁对生成问题复现测试，在Python基准测试中表现优于现有技术，还搭建GitHub机器人测试，分析开发者反馈。


<details>
  <summary>Details</summary>
Motivation: 过去研究显示开发者常不使用问题复现测试提交补丁，因此自动生成问题复现测试成为研究热点。

Method: 提出BLAST工具，结合LLM和SBST。LLM部分通过git历史分析、静态分析和SBST生成测试提取相关上下文；SBST部分将问题描述和补丁通过中间LLM生成的种子输入优化。

Result: BLAST在Python基准测试中为151/426（35.4%）的问题成功生成问题复现测试，优于现有技术（23.5%）；搭建的GitHub机器人在32个PR - 问题对中有11个生成了测试。

Conclusion: 通过分析开发者反馈，讨论了研究者和工具开发者面临的挑战和机遇。

Abstract: Issue-reproducing tests fail on buggy code and pass once a patch is applied,
thus increasing developers' confidence that the issue has been resolved and
will not be re-introduced. However, past research has shown that developers
often commit patches without such tests, making the automated generation of
issue-reproducing tests an area of interest. We propose BLAST, a tool for
automatically generating issue-reproducing tests from issue-patch pairs by
combining LLMs and search-based software testing (SBST). For the LLM part, we
complement the issue description and the patch by extracting relevant context
through git history analysis, static analysis, and SBST-generated tests. For
the SBST part, we adapt SBST for generating issue-reproducing tests; the issue
description and the patch are fed into the SBST optimization through an
intermediate LLM-generated seed, which we deserialize into SBST-compatible
form. BLAST successfully generates issue-reproducing tests for 151/426 (35.4%)
of the issues from a curated Python benchmark, outperforming the
state-of-the-art (23.5%). Additionally, to measure the real-world impact of
BLAST, we built a GitHub bot that runs BLAST whenever a new pull request (PR)
linked to an issue is opened, and if BLAST generates an issue-reproducing test,
the bot proposes it as a comment in the PR. We deployed the bot in three
open-source repositories for three months, gathering data from 32 PRs-issue
pairs. BLAST generated an issue-reproducing test in 11 of these cases, which we
proposed to the developers. By analyzing the developers' feedback, we discuss
challenges and opportunities for researchers and tool builders. Data and
material: https://doi.org/10.5281/zenodo.16949042

</details>


### [401] [Tether: A Personalized Support Assistant for Software Engineers with ADHD](https://arxiv.org/abs/2509.01946)
*Aarsh Shah,Cleyton Magalhaes,Kiev Gama,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 介绍了支持ADHD软件工程师的LLM桌面应用Tether，结合多种技术提供支持，初步验证有改进，为未来工具奠基。


<details>
  <summary>Details</summary>
Motivation: 软件工程中对神经多样性关注不足，缺乏支持ADHD开发者认知挑战的工具。

Method: 结合本地活动监控、检索增强生成和游戏化，集成操作系统级系统跟踪，聊天机器人利用ADHD特定资源。

Result: 通过自我使用初步验证，迭代提示优化和RAG增强后上下文准确性提高。

Conclusion: Tether有别于通用工具，为未来软件工程中神经多样性感知工具奠定基础，凸显LLM作为个性化支持系统的潜力。

Abstract: Equity, diversity, and inclusion in software engineering often overlook
neurodiversity, particularly the experiences of developers with Attention
Deficit Hyperactivity Disorder (ADHD). Despite the growing awareness about that
population in SE, few tools are designed to support their cognitive challenges
(e.g., sustained attention, task initiation, self-regulation) within
development workflows. We present Tether, an LLM-powered desktop application
designed to support software engineers with ADHD by delivering adaptive,
context-aware assistance. Drawing from engineering research methodology, Tether
combines local activity monitoring, retrieval-augmented generation (RAG), and
gamification to offer real-time focus support and personalized dialogue. The
system integrates operating system level system tracking to prompt engagement
and its chatbot leverages ADHD-specific resources to offer relevant responses.
Preliminary validation through self-use revealed improved contextual accuracy
following iterative prompt refinements and RAG enhancements. Tether
differentiates itself from generic tools by being adaptable and aligned with
software-specific workflows and ADHD-related challenges. While not yet
evaluated by target users, this work lays the foundation for future
neurodiversity-aware tools in SE and highlights the potential of LLMs as
personalized support systems for underrepresented cognitive needs.

</details>


### [402] [Automated Repair of C Programs Using Large Language Models](https://arxiv.org/abs/2509.01947)
*Mahdi Farzandway,Fatemeh Ghassemi*

Main category: cs.SE

TL;DR: 研究探索大语言模型自动修复C程序潜力，提出集成SBFL、运行时反馈和CoT提示的框架，评估显示有较高修复准确率和提升。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在自动修复C程序方面的潜力。

Method: 提出集成频谱故障定位、运行时反馈和CoT提示的自主修复循环框架，迭代修复代码并结合推理。

Result: 在3902个Codeflaws基准bug上实现44.93%的修复准确率，比GPT - 4 with CoT等基线提升3.61%。

Conclusion: 展示了统计程序分析与生成式AI在自动调试中集成的实用途径。

Abstract: This study explores the potential of Large Language Models (LLMs) in
automating the repair of C programs. We present a framework that integrates
spectrum-based fault localization (SBFL), runtime feedback, and
Chain-of-Thought-structured prompting into an autonomous repair loop. Unlike
prior approaches, our method explicitly combines statistical program analysis
with LLM reasoning. The iterative repair cycle leverages a structured
Chain-of-Thought (CoT) prompting approach, where the model reasons over failing
tests, suspicious code regions, and prior patch outcomes, before generating new
candidate patches. The model iteratively changes the code, evaluates the
results, and incorporates reasoning from previous attempts into subsequent
modifications, reducing repeated errors and clarifying why some bugs remain
unresolved. Our evaluation spans 3,902 bugs from the Codeflaws benchmark, where
our approach achieves 44.93% repair accuracy, representing a 3.61% absolute
improvement over strong state-of-the-art APR baselines such as GPT-4 with CoT.
This outcome highlights a practical pathway toward integrating statistical
program analysis with generative AI in automated debugging.

</details>


### [403] [ProbTest: Unit Testing for Probabilistic Programs (Extended Version)](https://arxiv.org/abs/2509.02012)
*Katrine Christensen,Mahsa Varshosaz,Raúl Pardo*

Main category: cs.SE

TL;DR: 提出用于测试概率程序结果的新黑盒单元测试方法ProbTest，基于优惠券收集问题理论，实现为PyTest插件并进行评估。


<details>
  <summary>Details</summary>
Motivation: 概率程序因随机性，测试其预期结果需多次执行，需确定有效测试的执行次数。

Method: 提出基于优惠券收集问题理论的黑盒单元测试方法ProbTest，并实现为PyTest插件。

Result: 实现ProbTest为PyTest插件，开发者可像编写普通Python程序一样编写单元测试，必要测试执行自动处理。

Conclusion: 方法可用于Gymnasium强化学习库和随机数据结构的案例研究。

Abstract: Testing probabilistic programs is non-trivial due to their stochastic nature.
Given an input, the program may produce different outcomes depending on the
underlying stochastic choices in the program. This means testing the expected
outcomes of probabilistic programs requires repeated test executions unlike
deterministic programs where a single execution may suffice for each test
input. This raises the following question: how many times should we run a
probabilistic program to effectively test it? This work proposes a novel
black-box unit testing method, ProbTest, for testing the outcomes of
probabilistic programs. Our method is founded on the theory surrounding a
well-known combinatorial problem, the coupon collector's problem. Using this
method, developers can write unit tests as usual without extra effort while the
number of required test executions is determined automatically with statistical
guarantees for the results. We implement ProbTest as a plug-in for PyTest, a
well-known unit testing tool for python programs. Using this plug-in,
developers can write unit tests similar to any other Python program and the
necessary test executions are handled automatically. We evaluate the method on
case studies from the Gymnasium reinforcement learning library and a randomized
data structure.

</details>


### [404] [Scalable Thread-Safety Analysis of Java Classes with CodeQL](https://arxiv.org/abs/2509.02022)
*Bjørnar Haugstad Jåtten,Simon Boye Jørgensen,Rasmus Petersen,Raúl Pardo*

Main category: cs.SE

TL;DR: 本文提出分析Java类线程安全的可扩展方法，经评估展示了该方法在实际代码库中的适用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 确定类是否线程安全是一项具有挑战性的任务，因此需要一种方法来分析Java类的线程安全性。

Method: 基于Java内存模型的数据竞争自由原则定义Java类的线程安全，设计确保线程安全的属性集，将属性编码到静态分析工具CodeQL中自动分析Java源代码。

Result: 对1000个GitHub仓库进行评估，检测到数千个线程安全错误，查询运行时间短，提交的PR获开发者积极反馈，CodeQL查询正纳入GitHub actions。

Conclusion: 该方法在实际代码库中分析线程安全具有适用性和可扩展性。

Abstract: In object-oriented languages software developers rely on thread-safe classes
to implement concurrent applications. However, determining whether a class is
thread-safe is a challenging task. This paper presents a highly scalable method
to analyze thread-safety in Java classes. We provide a definition of
thread-safety for Java classes founded on the correctness principle of the Java
memory model, data race freedom. We devise a set of properties for Java classes
that are proven to ensure thread-safety. We encode these properties in the
static analysis tool CodeQL to automatically analyze Java source code. We
perform an evaluation on the top 1000 GitHub repositories. The evaluation
comprises 3632865 Java classes; with 1992 classes annotated as @ThreadSafe from
71 repositories. These repositories include highly popular software such as
Apache Flink (24.6k stars), Facebook Fresco (17.1k stars), PrestoDB (16.2k
starts), and gRPC (11.6k starts). Our queries detected thousands of
thread-safety errors. The running time of our queries is below 2 minutes for
repositories up to 200k lines of code, 20k methods, 6000 fields, and 1200
classes. We have submitted a selection of detected concurrency errors as PRs,
and developers positively reacted to these PRs. We have submitted our CodeQL
queries to the main CodeQL repository, and they are currently in the process of
becoming available as part of GitHub actions. The results demonstrate the
applicability and scalability of our method to analyze thread-safety in
real-world code bases.

</details>


### [405] [Curiosity-Driven Testing for Sequential Decision-Making Process](https://arxiv.org/abs/2509.02025)
*Junda He,Zhou Yang,Jieke Shi,Chengran Yang,Kisub Kim,Bowen Xu,Xin Zhou,David Lo*

Main category: cs.SE

TL;DR: 提出CureFuzz用于测试顺序决策过程（SDPs），在故障检测和优化性能上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有顺序决策模型易学习不安全行为，缺乏能识别多样崩溃场景的测试框架。

Method: 提出好奇机制有效探索新场景，引入多目标种子选择技术平衡新场景探索和崩溃场景生成。

Result: 在各种SDMs上评估，CureFuzz在故障总数和不同类型崩溃场景检测上大幅超越现有方法。

Conclusion: CureFuzz是测试SDMs和优化其性能的有价值工具。

Abstract: Sequential decision-making processes (SDPs) are fundamental for complex
real-world challenges, such as autonomous driving, robotic control, and traffic
management. While recent advances in Deep Learning (DL) have led to mature
solutions for solving these complex problems, SDMs remain vulnerable to
learning unsafe behaviors, posing significant risks in safety-critical
applications. However, developing a testing framework for SDMs that can
identify a diverse set of crash-triggering scenarios remains an open challenge.
To address this, we propose CureFuzz, a novel curiosity-driven black-box fuzz
testing approach for SDMs. CureFuzz proposes a curiosity mechanism that allows
a fuzzer to effectively explore novel and diverse scenarios, leading to
improved detection of crashtriggering scenarios. Additionally, we introduce a
multi-objective seed selection technique to balance the exploration of novel
scenarios and the generation of crash-triggering scenarios, thereby optimizing
the fuzzing process. We evaluate CureFuzz on various SDMs and experimental
results demonstrate that CureFuzz outperforms the state-of-the-art method by a
substantial margin in the total number of faults and distinct types of
crash-triggering scenarios. We also demonstrate that the crash-triggering
scenarios found by CureFuzz can repair SDMs, highlighting CureFuzz as a
valuable tool for testing SDMs and optimizing their performance.

</details>


### [406] [Txt2Sce: Scenario Generation for Autonomous Driving System Testing Based on Textual Reports](https://arxiv.org/abs/2509.02150)
*Pin Ji,Yang Feng,Zongtai Li,Xiangchi Zhou,Jia Liu,Jun Sun,Zhihong Zhao*

Main category: cs.SE

TL;DR: 本文提出Txt2Sce方法，基于文本事故报告生成OpenSCENARIO格式测试场景，实验表明该方法能转换报告、增强场景多样性并检测自动驾驶系统意外行为。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统场景测试方法依赖视觉数据、缺乏标准化格式，可扩展性和可移植性受限，需改进。

Method: 提出Txt2Sce方法，先用大语言模型将文本事故报告转换为OpenSCENARIO场景文件，再通过场景拆解、块变异和组装生成基于推导的场景文件树。

Result: 生成33个场景文件树，共4373个场景文件，成功将文本报告转换为有效OpenSCENARIO文件，增强场景多样性，有效检测出Autoware在安全、智能和流畅性方面的意外行为。

Conclusion: Txt2Sce方法能解决现有场景测试方法的问题，对自动驾驶系统测试有积极作用。

Abstract: With the rapid advancement of deep learning and related technologies,
Autonomous Driving Systems (ADSs) have made significant progress and are
gradually being widely applied in safety-critical fields. However, numerous
accident reports show that ADSs still encounter challenges in complex
scenarios. As a result, scenario-based testing has become essential for
identifying defects and ensuring reliable performance. In particular,
real-world accident reports offer valuable high-risk scenarios for more
targeted ADS testing. Despite their potential, existing methods often rely on
visual data, which demands large memory and manual annotation. Additionally,
since existing methods do not adopt standardized scenario formats (e.g.,
OpenSCENARIO), the generated scenarios are often tied to specific platforms and
ADS implementations, limiting their scalability and portability. To address
these challenges, we propose Txt2Sce, a method for generating test scenarios in
OpenSCENARIO format based on textual accident reports. Txt2Sce first uses a LLM
to convert textual accident reports into corresponding OpenSCENARIO scenario
files. It then generates a derivation-based scenario file tree through scenario
disassembly, scenario block mutation, and scenario assembly. By utilizing the
derivation relationships between nodes in the scenario tree, Txt2Sce helps
developers identify the scenario conditions that trigger unexpected behaviors
of ADSs. In the experiments, we employ Txt2Sce to generate 33 scenario file
trees, resulting in a total of 4,373 scenario files for testing the open-source
ADS, Autoware. The experimental results show that Txt2Sce successfully converts
textual reports into valid OpenSCENARIO files, enhances scenario diversity
through mutation, and effectively detects unexpected behaviors of Autoware in
terms of safety, smartness, and smoothness.

</details>


### [407] [Formalizing Operational Design Domains with the Pkl Language](https://arxiv.org/abs/2509.02221)
*Martin Skoglund,Fredrik Warg,Anders Thorsén,Sasikumar Punnekkat,Hans Hansson*

Main category: cs.SE

TL;DR: 自动化功能部署改变安全评估，本文提出用Pkl语言形式化ODD，解决相关挑战并提高可用性，以汽车为例但可广泛应用。


<details>
  <summary>Details</summary>
Motivation: 自动化功能需新安全评估框架，ODD形式化存在挑战，需解决这些问题以确保操作上下文的严格评估。

Method: 用Pkl语言对ODD进行形式化，利用专门配置语言特性。

Result: 提出了用Pkl语言形式化ODD的方法，解决了ODD指定中的核心挑战并提高了可用性。

Conclusion: 该方法以汽车为例展示，可广泛应用于确保操作上下文的严格评估。

Abstract: The deployment of automated functions that can operate without direct human
supervision has changed safety evaluation in domains seeking higher levels of
automation. Unlike conventional systems that rely on human operators, these
functions require new assessment frameworks to demonstrate that they do not
introduce unacceptable risks under real-world conditions. To make a convincing
safety claim, the developer must present a thorough justification argument,
supported by evidence, that a function is free from unreasonable risk when
operated in its intended context. The key concept relevant to the presented
work is the intended context, often captured by an Operational Design Domain
specification (ODD). ODD formalization is challenging due to the need to
maintain flexibility in adopting diverse specification formats while preserving
consistency and traceability and integrating seamlessly into the development,
validation, and assessment. This paper presents a way to formalize an ODD in
the Pkl language, addressing central challenges in specifying ODDs while
improving usability through specialized configuration language features. The
approach is illustrated with an automotive example but can be broadly applied
to ensure rigorous assessments of operational contexts.

</details>


### [408] [Methodology for Test Case Allocation based on a Formalized ODD](https://arxiv.org/abs/2509.02311)
*Martin Skoglund,Fredrik Warg,Anders Thoren,Sasikumar Punnekkat,Hans Hansson*

Main category: cs.SE

TL;DR: 本文提出评估测试用例分配到不同测试环境适用性的方法，并通过自动倒车卡车功能案例展示，该方法与ODD参数相关，便于自动分配测试用例。


<details>
  <summary>Details</summary>
Motivation: CCAM系统出现使安全评估需要新方法，多种测试环境结合的方法在匹配测试需求和能力上有挑战。

Method: 借鉴并扩展现有ODD形式化，加入关键测试属性，集成ODD参数和额外测试属性。

Result: 通过自动倒车卡车功能案例展示了该方法可支持自动适用性评估。

Conclusion: 该方法使系统实现保真度与ODD参数相关，便于基于各环境对象检测传感器评估能力自动分配测试用例。

Abstract: The emergence of Connected, Cooperative, and Automated Mobility (CCAM)
systems has significantly transformed the safety assessment landscape. Because
they integrate automated vehicle functions beyond those managed by a human
driver, new methods are required to evaluate their safety. Approaches that
compile evidence from multiple test environments have been proposed for
type-approval and similar evaluations, emphasizing scenario coverage within the
systems Operational Design Domain (ODD). However, aligning diverse test
environment requirements with distinct testing capabilities remains
challenging. This paper presents a method for evaluating the suitability of
test case allocation to various test environments by drawing on and extending
an existing ODD formalization with key testing attributes. The resulting
construct integrates ODD parameters and additional test attributes to capture a
given test environments relevant capabilities. This approach supports automatic
suitability evaluation and is demonstrated through a case study on an automated
reversing truck function. The system's implementation fidelity is tied to ODD
parameters, facilitating automated test case allocation based on each
environments capacity for object-detection sensor assessment.

</details>


### [409] [ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation](https://arxiv.org/abs/2509.02330)
*Yicong Zhao,Shisong Chen,Jiacheng Zhang,Zhixu Li*

Main category: cs.SE

TL;DR: 提出ReCode框架用于高效准确代码修复，及RACodeBench基准，实验显示其有高修复准确率与低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有代码修复方法训练成本高或推理计算昂贵，传统检索策略无法捕捉代码结构细节。

Method: 提出ReCode框架，含算法感知检索策略和模块化双编码器架构；构建RACodeBench基准。

Result: 在RACodeBench和竞赛编程数据集实验表明，ReCode修复准确率高且推理成本显著降低。

Conclusion: ReCode对现实世界代码修复场景有实用价值。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in code-related tasks, such as code generation and automated
program repair. Despite their promising performance, most existing approaches
for code repair suffer from high training costs or computationally expensive
inference. Retrieval-augmented generation (RAG), with its efficient in-context
learning paradigm, offers a more scalable alternative. However, conventional
retrieval strategies, which are often based on holistic code-text embeddings,
fail to capture the structural intricacies of code, resulting in suboptimal
retrieval quality. To address the above limitations, we propose ReCode, a
fine-grained retrieval-augmented in-context learning framework designed for
accurate and efficient code repair. Specifically, ReCode introduces two key
innovations: (1) an algorithm-aware retrieval strategy that narrows the search
space using preliminary algorithm type predictions; and (2) a modular
dual-encoder architecture that separately processes code and textual inputs,
enabling fine-grained semantic matching between input and retrieved contexts.
Furthermore, we propose RACodeBench, a new benchmark constructed from
real-world user-submitted buggy code, which addresses the limitations of
synthetic benchmarks and supports realistic evaluation. Experimental results on
RACodeBench and competitive programming datasets demonstrate that ReCode
achieves higher repair accuracy with significantly reduced inference cost,
highlighting its practical value for real-world code repair scenarios.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [410] [Prospects of Imitating Trading Agents in the Stock Market](https://arxiv.org/abs/2509.00982)
*Mateusz Wilinski,Juho Kanniainen*

Main category: q-fin.CP

TL;DR: 本文展示生成工具可用于模仿交易代理，提出基于状态空间模型的改进生成架构并应用于数据，在合成数据上训练模型，对比预测分布与真实情况。


<details>
  <summary>Details</summary>
Motivation: 探索将成功应用于限价订单簿数据的生成工具用于模仿交易代理。

Method: 提出基于状态空间模型的改进生成架构，在基于异构代理模型生成的合成数据上训练模型。

Result: 文中未明确提及具体结果。

Conclusion: 文中未明确给出结论。

Abstract: In this work we show how generative tools, which were successfully applied to
limit order book data, can be utilized for the task of imitating trading
agents. To this end, we propose a modified generative architecture based on the
state-space model, and apply it to limit order book data with identified
investors. The model is trained on synthetic data, generated from a
heterogeneous agent-based model. Finally, we compare model's predicted
distribution over different aspects of investors' actions, with the ground
truths known from the agent-based model.

</details>


### [411] [Is All the Information in the Price? LLM Embeddings versus the EMH in Stock Clustering](https://arxiv.org/abs/2509.01590)
*Bingyang Wang,Grant Johnson,Maria Hybinette,Tucker Balch*

Main category: q-fin.CP

TL;DR: 研究人工智能能否提升股票聚类效果，对比三种聚类方法，发现价格聚类表现佳，支持有效市场假说。


<details>
  <summary>Details</summary>
Motivation: 研究人工智能能否在半强式有效市场假说下，比传统方法提升股票聚类效果。

Method: 对比价格、人为规则（GICS）和人工智能（LLM嵌入）三种聚类方法，用套利定价理论框架转化为合成因子模型评估。

Result: 价格聚类表现远超规则和人工智能方法，相对GICS和LLM嵌入分别降低15.9%和14.7%的均方根误差。

Conclusion: 提出通用评估方法，首次直接对比三种聚类方法，实证短线回报信息多包含在价格中，支持有效市场假说，为从业者和学者提供参考。

Abstract: This paper investigates whether artificial intelligence can enhance stock
clustering compared to traditional methods. We consider this in the context of
the semi-strong Efficient Markets Hypothesis (EMH), which posits that prices
fully reflect all public information and, accordingly, that clusters based on
price information cannot be improved upon. We benchmark three clustering
approaches: (i) price-based clusters derived from historical return
correlations, (ii) human-informed clusters defined by the Global Industry
Classification Standard (GICS), and (iii) AI-driven clusters constructed from
large language model (LLM) embeddings of stock-related news headlines. At each
date, each method provides a classification in which each stock is assigned to
a cluster. To evaluate a clustering, we transform it into a synthetic factor
model following the Arbitrage Pricing Theory (APT) framework. This enables
consistent evaluation of predictive performance in a roll forward,
out-of-sample test. Using S&P 500 constituents from from 2022 through 2024, we
find that price-based clustering consistently outperforms both rule-based and
AI-based methods, reducing root mean squared error (RMSE) by 15.9% relative to
GICS and 14.7% relative to LLM embeddings. Our contributions are threefold: (i)
a generalizable methodology that converts any equity grouping: manual, machine,
or market-driven, into a real-time factor model for evaluation; (ii) the first
direct comparison of price-based, human rule-based, and AI-based clustering
under identical conditions; and (iii) empirical evidence reinforcing that
short-horizon return information is largely contained in prices. These results
support the EMH while offering practitioners a practical diagnostic for
monitoring evolving sector structures and provide academics a framework for
testing alternative hypotheses about how quickly markets absorb information.

</details>


### [412] [Controllable Generation of Implied Volatility Surfaces with Variational Autoencoders](https://arxiv.org/abs/2509.01743)
*Jing Wang,Shuaiqiang Liu,Cornelis Vuik*

Main category: q-fin.CP

TL;DR: 本文提出用VAE可控合成隐含波动率曲面（IVS）的框架，可显式控制特征生成IVS，实验证明其有效性并结合了解释性、可控性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有传统数据驱动模型无法对隐含波动率曲面有意义的形状特征进行显式控制，需要一种新方法来实现可控合成。

Method: 使用变分自编码器（VAE），将金融可解释的形状特征与残差潜在因素分离，把目标特征作为可控潜在变量嵌入VAE架构，通过回归量化IVS特征值并融入解码器引导生成，还有可选的后生成潜在空间修复算法。

Result: 生成模型能快速生成具有所需特征的现实IVS，在单特征和多特征控制设置下都有高精度，可去除静态无套利条件的偶尔违规。

Conclusion: 该框架结合了解释性、可控性和灵活性，适用于合成IVS生成和场景设计。

Abstract: This paper presents a deep generative modeling framework for controllably
synthesizing implied volatility surfaces (IVSs) using a variational autoencoder
(VAE). Unlike conventional data-driven models, our approach provides explicit
control over meaningful shape features (e.g., volatility level, slope,
curvature, term-structure) to generate IVSs with desired characteristics. In
our framework, financially interpretable shape features are disentangled from
residual latent factors. The target features are embedded into the VAE
architecture as controllable latent variables, while the residual latent
variables capture additional structure to preserve IVS shape diversity. To
enable this control, IVS feature values are quantified via regression at an
anchor point and incorporated into the decoder to steer generation. Numerical
experiments demonstrate that the generative model enables rapid generation of
realistic IVSs with desired features rather than arbitrary patterns, and
achieves high accuracy across both single- and multi-feature control settings.
For market validity, an optional post-generation latent-space repair algorithm
adjusts only the residual latent variables to remove occasional violations of
static no-arbitrage conditions without altering the specified features.
Compared with black-box generators, the framework combines interpretability,
controllability, and flexibility for synthetic IVS generation and scenario
design.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [413] [Robust MCVaR Portfolio Optimization with Ellipsoidal Support and Reproducing Kernel Hilbert Space-based Uncertainty](https://arxiv.org/abs/2509.00447)
*Rupendra Yadav,Aparna Mehra*

Main category: q-fin.PM

TL;DR: 本文提出最小化MCVaR的投资组合优化框架，测试了鲁棒模型在六个金融市场数据集上的表现，发现其在熊市有优势，在不同市场条件下的表现还需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 引入投资组合优化框架，最小化混合条件风险价值，同时考虑预期回报的机会约束和资产数量的基数约束。

Method: 提出鲁棒MCVaR模型，利用基于再生核希尔伯特空间的不确定性集处理机会约束，得到简化的二阶锥规划公式。

Result: 鲁棒模型在很多情况下优于名义模型、市场投资组合和等权重投资组合；在熊市有明显优势，在牛市和中性市场与名义模型表现相近。

Conclusion: 鲁棒模型在波动市场有效，但需进一步研究其在不同市场条件下的表现。

Abstract: This study introduces a portfolio optimization framework to minimize mixed
conditional value at risk (MCVaR), incorporating a chance constraint on
expected returns and limiting the number of assets via cardinality constraints.
A robust MCVaR model is presented, which presumes ellipsoidal support for
random returns without assuming any distribution. The model utilizes an
uncertainty set grounded in a reproducing kernel Hilbert space (RKHS) to manage
the chance constraint, resulting in a simplified second-order cone programming
(SOCP) formulation. The performance of the robust model is tested on datasets
from six distinct financial markets. The outcomes of comprehensive experiments
indicate that the robust model surpasses the nominal model, market portfolio,
and equal-weight portfolio with higher expected returns, lower risk metrics,
enhanced reward-risk ratios, and a better value of Jensen's alpha in many
cases. Furthermore, we aim to validate the robust models in different market
phases (bullish, bearish, and neutral). The robust model shows a distinct
advantage in bear markets, providing better risk protection against adverse
conditions. In contrast, its performance in bullish and neutral phases is
somewhat similar to that of the nominal model. The robust model appears
effective in volatile markets, although further research is necessary to
comprehend its performance across different market conditions.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [414] [Neural Lévy SDE for State--Dependent Risk and Density Forecasting](https://arxiv.org/abs/2509.01041)
*Ziyao Wang,Svetlozar T Rachev*

Main category: q-fin.RM

TL;DR: 提出神经Lévy跳跃 - 扩散框架用于金融回报风险和密度预测，实证显示优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 经典扩散模型难以捕捉金融回报重尾、波动聚集和突然跳跃，现有机器学习模型在可解释尾风险、多期预测等方面存在不足。

Method: 提出神经Lévy跳跃 - 扩散框架联合学习条件漂移、扩散、跳跃强度和跳跃大小分布；用准最大似然估计，结合双幂变差权重分离扩散和跳跃贡献；成本感知投资组合优化器生成交易策略。

Result: 对横截面股票数据的实证分析表明，该框架校准效果更好，尾部控制更精准，能显著降低风险。

Conclusion: 该框架是一种可解释、可测试且可实际应用的状态依赖风险和密度预测方法。

Abstract: Financial returns are known to exhibit heavy tails, volatility clustering and
abrupt jumps that are poorly captured by classical diffusion models. Advances
in machine learning have enabled highly flexible functional forms for
conditional means and volatilities, yet few models deliver interpretable
state--dependent tail risk, capture multiple forecast horizons and yield
distributions amenable to backtesting and execution. This paper proposes a
neural L\'evy jump--diffusion framework that jointly learns, as functions of
observable state variables, the conditional drift, diffusion, jump intensity
and jump size distribution. We show how a single shared encoder yields multiple
forecasting heads corresponding to distinct horizons (daily, weekly, etc.),
facilitating multi--horizon density forecasts and risk measures. The state
vector includes conventional price and volume features as well as novel
complexity measures such as permutation entropy and recurrence quantification
analysis determinism, which quantify predictability in the underlying process.
Estimation is based on a quasi--maximum likelihood approach that separates
diffusion and jump contributions via bipower variation weights and incorporates
monotonicity and smoothness regularisation to ensure identifiability. A
cost--aware portfolio optimiser translates the model's conditional densities
into implementable trading strategies under leverage, turnover and
no--trade--band constraints. Extensive empirical analyses on cross--sectional
equity data demonstrate improved calibration, sharper tail control and
economically significant risk reduction relative to baseline diffusive and
GARCH benchmarks. The proposed framework is therefore an interpretable,
testable and practically deployable method for state--dependent risk and
density forecasting.

</details>


### [415] [Signal from Noise Signal from Noise: A Neural Network-Based Denoising Approach for Measuring Global Financial Spillovers](https://arxiv.org/abs/2509.01156)
*Abdullah Karasan,Özge Sezgin Alp*

Main category: q-fin.RM

TL;DR: 研究使用神经网络去噪架构分析多市场回报和波动率溢出效应，揭示不同市场角色变化，强调去噪在溢出分析中的必要性。


<details>
  <summary>Details</summary>
Motivation: 准确评估金融市场溢出效应，需从噪声中过滤信号。

Method: 采用神经网络去噪架构，对协方差矩阵去噪后进行溢出效应估计，分析时段为2014年末到2025年中，采用静态和时变框架。

Result: 正常情况下发达市场是波动溢出净传输者，系统压力时成净接收者；发展中市场溢出角色不稳定。去噪能明确溢出渠道动态和异质性，使溢出模式与金融事件更匹配。

Conclusion: 溢出分析中去噪对监测系统性风险和市场互联性很有必要。

Abstract: Filtering signal from noise is fundamental to accurately assessing spillover
effects in financial markets. This study investigates denoised return and
volatility spillovers across a diversified set of markets, spanning developed
and developing economies as well as key asset classes, using a neural
network-based denoising architecture. By applying denoising to the covariance
matrices prior to spillover estimation, we disentangle signal from noise. Our
analysis covers the period from late 2014 to mid-2025 and adopts both static
and time-varying frameworks. The results reveal that developed markets
predominantly serve as net transmitters of volatility spillovers under normal
conditions, but often transition into net receivers during episodes of systemic
stress, such as the Covid-19 pandemic. In contrast, developing markets display
heightened instability in their spillover roles, frequently oscillating between
transmitter and receiver positions. Denoising not only clarifies the dynamic
and heterogeneous nature of spillover channels, but also sharpens the alignment
between observed spillover patterns and known financial events. These findings
highlight the necessity of denoising in spillover analysis for effective
monitoring of systemic risk and market interconnectedness.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [416] [Multi Scale Analysis of Nifty 50 Return Characteristics Valuation Dynamics and Market Complexity 1990 to 2024](https://arxiv.org/abs/2509.00697)
*Chandradew Sharma*

Main category: q-fin.ST

TL;DR: 本文用34年印度Nifty 50指数数据构建框架分析股市回报动态，发现回报分布不对称，P/E与回报相关，用复杂性科学工具揭示结构，为新兴市场投资提供见解。


<details>
  <summary>Details</summary>
Motivation: 填补文献空白，理解印度市场股票回报动态，研究估值指标与回报分布的关系。

Method: 使用34年Nifty 50指数数据，应用复杂性科学工具，如熵、Hurst指数、Lyapunov指标和信息论指标。

Result: 回报分布不对称，1年回报盈利概率74%；不同P/E对应不同回报结果；复杂性科学工具揭示弱持续性、长记忆和低维混沌，证实估值对未来回报有预测影响。

Conclusion: 框架将估值、条件分布和非线性动态相结合，为新兴市场资产配置、下行风险管理和长期投资策略提供见解。

Abstract: This study presents a unified, distribution-aware, and complexity-informed
framework for understanding equity return dynamics in the Indian market, using
34 years (1990 to 2024) of Nifty 50 index data. Addressing a key gap in the
literature, we demonstrate that the price to earnings ratio, as a valuation
metric, may probabilistically map return distributions across investment
horizons spanning from days to decades.
  Return profiles exhibit strong asymmetry. One-year returns show a 74 percent
probability of gain, with a modal return of 10.67 percent and a reward-to-risk
ratio exceeding 5. Over long horizons, modal CAGRs surpass 13 percent, while
worst-case returns remain negative for up to ten years, defining a historical
trapping period. This horizon shortens to six years in the post-1999 period,
reflecting growing market resilience.
  Conditional analysis of the P/E ratio reveals regime-dependent outcomes. Low
valuations (P/E less than 13) historically show zero probability of loss across
all horizons, while high valuations (P/E greater than 27) correspond to
unstable returns and extended breakeven periods.
  To uncover deeper structure, we apply tools from complexity science. Entropy,
Hurst exponents, and Lyapunov indicators reveal weak persistence, long memory,
and low-dimensional chaos. Information-theoretic metrics, including mutual
information and transfer entropy, confirm a directional and predictive
influence of valuation on future returns.
  These findings offer actionable insights for asset allocation, downside risk
management, and long-term investment strategy in emerging markets. Our
framework bridges valuation, conditional distributions, and nonlinear dynamics
in a rigorous and practically relevant manner.

</details>


### [417] [Bridging Human Cognition and AI: A Framework for Explainable Decision-Making Systems](https://arxiv.org/abs/2509.02388)
*N. Jean,G. Le Pera*

Main category: q-fin.ST

TL;DR: 本文提出一个通用框架，将先进可解释性技术与Malle行为解释五类别模型结合，通过案例展示其实用性，为构建AI系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: AI和ML模型可解释性在高风险领域重要但常未达成，需提升透明度、可解释性和用户信任。

Method: 提出通用框架，将先进可解释性技术与Malle的五类别模型结合。

Result: 通过信用风险评估和监管分析等真实案例展示了框架的实用性。

Conclusion: 该框架使技术解释与人类认知机制对齐，为更易理解、负责和道德的AI系统奠定基础。

Abstract: Explainability in AI and ML models is critical for fostering trust, ensuring
accountability, and enabling informed decision making in high stakes domains.
Yet this objective is often unmet in practice. This paper proposes a general
purpose framework that bridges state of the art explainability techniques with
Malle's five category model of behavior explanation: Knowledge Structures,
Simulation/Projection, Covariation, Direct Recall, and Rationalization. The
framework is designed to be applicable across AI assisted decision making
systems, with the goal of enhancing transparency, interpretability, and user
trust. We demonstrate its practical relevance through real world case studies,
including credit risk assessment and regulatory analysis powered by large
language models (LLMs). By aligning technical explanations with human cognitive
mechanisms, the framework lays the groundwork for more comprehensible,
responsible, and ethical AI systems.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [418] [The Impact of Sequential versus Parallel Clearing Mechanisms in Agent-Based Simulations of Artificial Limit Order Book Exchanges](https://arxiv.org/abs/2509.01683)
*Matej Steinbacher,Mitja Steinbacher,Matjaz Steinbacher*

Main category: q-fin.TR

TL;DR: 研究不同清算机制计算实现对人工股市多资产价格动态的影响，指出顺序处理订单簿有偏差，强调清算机制建模需考量。


<details>
  <summary>Details</summary>
Motivation: 探究不同清算机制计算实现在人工股市框架下对多资产价格动态的影响。

Method: 在人工股票市场框架下进行研究。

Result: 顺序处理订单簿会影响交易者资本分配，扭曲资产需求和价格轨迹，整体价格水平受宏观因素主导，微观清算机制影响资产价值分配。

Conclusion: 人工市场中需仔细考量和验证清算机制以准确模拟复杂金融行为。

Abstract: This study examines the impact of different computing implementations of
clearing mechanisms on multi-asset price dynamics within an artificial stock
market framework. We show that sequential processing of order books introduces
a systematic and significant bias by affecting the allocation of traders'
capital within a single time step. This occurs because applying budget
constraints sequentially grants assets processed earlier preferential access to
funds, distorting individual asset demand and consequently their price
trajectories. The findings highlight that while the overall price level is
primarily driven by macro factors like the money-to-stock ratio, the market's
microstructural clearing mechanism plays a critical role in the allocation of
value among individual assets. This underscores the necessity for careful
consideration and validation of clearing mechanisms in artificial markets to
accurately model complex financial behaviors.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [419] [Simulation-based inference of yeast centromeres](https://arxiv.org/abs/2509.00200)
*Eloïse Touron,Pedro L. C. Rodrigues,Julyan Arbel,Nelle Varoquaux,Michael Arbel*

Main category: stat.ML

TL;DR: 本文指出染色质折叠和染色体空间排列对DNA复制和基因表达至关重要，现有方法推断酵母着丝粒位置困难，提出基于实验Hi - C图和模拟接触图随机推断酿酒酵母所有着丝粒位置的新方法。


<details>
  <summary>Details</summary>
Motivation: 染色质折叠异常会导致疾病，着丝粒对染色体分离和折叠很重要，但酵母着丝粒位置难以推断，现有Hi - C方法高度依赖良好的预定位。

Method: 基于实验Hi - C图和模拟接触图，以随机方式推断酿酒酵母所有着丝粒的位置。

Result: 未提及。

Conclusion: 未提及。

Abstract: The chromatin folding and the spatial arrangement of chromosomes in the cell
play a crucial role in DNA replication and genes expression. An improper
chromatin folding could lead to malfunctions and, over time, diseases. For
eukaryotes, centromeres are essential for proper chromosome segregation and
folding. Despite extensive research using de novo sequencing of genomes and
annotation analysis, centromere locations in yeasts remain difficult to infer
and are still unknown in most species. Recently, genome-wide chromosome
conformation capture coupled with next-generation sequencing (Hi-C) has become
one of the leading methods to investigate chromosome structures. Some recent
studies have used Hi-C data to give a point estimate of each centromere, but
those approaches highly rely on a good pre-localization. Here, we present a
novel approach that infers in a stochastic manner the locations of all
centromeres in budding yeast based on both the experimental Hi-C map and
simulated contact maps.

</details>


### [420] [Assessing One-Dimensional Cluster Stability by Extreme-Point Trimming](https://arxiv.org/abs/2509.00258)
*Erwan Dereure,Emmanuel Akame Mfoumou,David Holcman*

Main category: stat.ML

TL;DR: 本文提出评估一维独立同分布样本尾部行为和几何稳定性的概率方法，该方法表现良好且可用于聚类分析。


<details>
  <summary>Details</summary>
Motivation: 评估一维独立同分布样本的尾部行为和几何稳定性。

Method: 提出直径收缩比量化极端点去除时数据范围的相对减少，推导均匀和高斯假设下的期望收缩表达式，构建决策规则分类样本。

Result: 该测试在小样本或有噪声情况下分类准确率高于经典似然比测试，且大样本时保持渐近一致性，还能用于聚类分析。

Conclusion: 为稳健分布推断和聚类稳定性分析提供理论见解和实用工具。

Abstract: We develop a probabilistic method for assessing the tail behavior and
geometric stability of one-dimensional n i.i.d. samples by tracking how their
span contracts when the most extreme points are trimmed. Central to our
approach is the diameter-shrinkage ratio, that quantifies the relative
reduction in data range as extreme points are successively removed. We derive
analytical expressions, including finite-sample corrections, for the expected
shrinkage under both the uniform and Gaussian hypotheses, and establish that
these curves remain distinct even for moderate number of removal. We construct
an elementary decision rule that assigns a sample to whichever theoretical
shrinkage profile it most closely follows. This test achieves higher
classification accuracy than the classical likelihood-ratio test in
small-sample or noisy regimes, while preserving asymptotic consistency for
large n. We further integrate our criterion into a clustering pipeline (e.g.
DBSCAN), demonstrating its ability to validate one-dimensional clusters without
any density estimation or parameter tuning. This work thus provides both
theoretical insight and practical tools for robust distributional inference and
cluster stability analysis.

</details>


### [421] [Probit Monotone BART](https://arxiv.org/abs/2509.00263)
*Jared D. Fisher*

Main category: stat.ML

TL;DR: 提出probit monotone BART用于二元结果变量的单调函数估计。


<details>
  <summary>Details</summary>
Motivation: 在已有Bayesian Additive Regression Trees (BART)和Monotone BART基础上，为二元结果变量的条件均值函数估计提供更合适方法。

Method: 提出probit monotone BART方法。

Result: 无明确提及具体结果。

Conclusion: 提出的probit monotone BART可使单调BART框架用于二元结果变量条件均值函数估计。

Abstract: Bayesian Additive Regression Trees (BART) of Chipman et al. (2010) has proven
to be a powerful tool for nonparametric modeling and prediction. Monotone BART
(Chipman et al., 2022) is a recent development that allows BART to be more
precise in estimating monotonic functions. We further these developments by
proposing probit monotone BART, which allows the monotone BART framework to
estimate conditional mean functions when the outcome variable is binary.

</details>


### [422] [Beyond Universal Approximation Theorems: Algorithmic Uniform Approximation by Neural Networks Trained with Noisy Data](https://arxiv.org/abs/2509.00924)
*Anastasis Kratsios,Tin Sum Cheng,Daniel Roy*

Main category: stat.ML

TL;DR: 引入架构特定随机训练算法，构建均匀逼近器，使先进通用逼近定理更接近实际机器学习。


<details>
  <summary>Details</summary>
Motivation: 现有通用逼近定理因假设无噪声数据和自由选择网络参数，与机器学习训练可靠泛化模型的目标脱节，需弥合差距。

Method: 引入架构特定随机训练算法，从N个噪声训练样本构建均匀逼近器。

Result: 训练的神经网络达到极小极大最优可训练参数数量，模型展现出类似真实神经网络的关键行为。

Conclusion: 该方法使先进通用逼近定理更接近实际机器学习，核心问题从含噪样本算法可实现性转变为随机梯度下降能否达到类似保证。

Abstract: At its core, machine learning seeks to train models that reliably generalize
beyond noisy observations; however, the theoretical vacuum in which
state-of-the-art universal approximation theorems (UATs) operate isolates them
from this goal, as they assume noiseless data and allow network parameters to
be chosen freely, independent of algorithmic realism. This paper bridges that
gap by introducing an architecture-specific randomized training algorithm that
constructs a uniform approximator from $N$ noisy training samples on the
$d$-dimensional cube $[0,1]^d$. Our trained neural networks attain the
minimax-optimal quantity of \textit{trainable} (non-random) parameters, subject
to logarithmic factors which vanish under the idealized noiseless sampling
assumed in classical UATs.
  Additionally, our trained models replicate key behaviours of real-world
neural networks, absent in standard UAT constructions, by: (1) exhibiting
sub-linear parametric complexity when fine-tuning on structurally related and
favourable out-of-distribution tasks, (2) exactly interpolating the training
data, and (3) maintaining reasonable Lipschitz regularity (after the initial
clustering attention layer). These properties bring state-of-the-art UATs
closer to practical machine learning, shifting the central open question from
algorithmic implementability with noisy samples to whether stochastic gradient
descent can achieve comparable guarantees.

</details>


### [423] [The Nondecreasing Rank](https://arxiv.org/abs/2509.00265)
*Andrew McCormack*

Main category: stat.ML

TL;DR: 文章引入矩阵或张量的非递减（ND）秩概念，介绍其性质，提出算法求低ND秩近似并应用于两个数据集。


<details>
  <summary>Details</summary>
Motivation: 引入新的矩阵或张量的ND秩概念，研究其性质并找到低ND秩近似。

Method: 引入ND秩概念，推导相关理论，提出分层交替最小二乘法的变体算法。

Result: 发现并非所有单调张量都有有限ND秩，对猪体重和新冠疫情心理健康调查两个数据集找到低ND秩分解。

Conclusion: ND秩是有意义的概念，提出的算法可用于找到数据张量的低ND秩近似。

Abstract: In this article the notion of the nondecreasing (ND) rank of a matrix or
tensor is introduced. A tensor has an ND rank of r if it can be represented as
a sum of r outer products of vectors, with each vector satisfying a
monotonicity constraint. It is shown that for certain poset orderings finding
an ND factorization of rank $r$ is equivalent to finding a nonnegative rank-r
factorization of a transformed tensor. However, not every tensor that is
monotonic has a finite ND rank. Theory is developed describing the properties
of the ND rank, including typical, maximum, and border ND ranks. Highlighted
also are the special settings where a matrix or tensor has an ND rank of one or
two. As a means of finding low ND rank approximations to a data tensor we
introduce a variant of the hierarchical alternating least squares algorithm.
Low ND rank factorizations are found and interpreted for two datasets
concerning the weight of pigs and a mental health survey during the COVID-19
pandemic.

</details>


### [424] [Partial Functional Dynamic Backdoor Diffusion-based Causal Model](https://arxiv.org/abs/2509.00472)
*Xinwen Liu,Lei Qian,Song Xi Chen,Niansheng Tang*

Main category: stat.ML

TL;DR: 提出PFD - BDCM用于存在空间异质性和时间依赖性的未测量混杂因素的因果推断，理论分析确定误差界限，实证显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理存在空间异质性和时间依赖性的未测量混杂因素的因果推断时有局限性。

Method: 将复杂时空动态模型与多分辨率变量分析相结合，把有效后门调整集整合到基于扩散的采样机制中，用特定区域结构方程和条件自回归过程处理未测量混杂因素，用函数数据的基展开处理异质分辨率变量。

Result: 理论分析确定了PFD - BDCM反事实估计的误差界限，实证评估表明PFD - BDCM在合成数据集和真实世界空气污染数据上优于现有方法。

Conclusion: PFD - BDCM是一种有效的处理存在空间异质性和时间依赖性的未测量混杂因素因果推断的方法。

Abstract: We introduce a Partial Functional Dynamic Backdoor Diffusion-based Causal
Model (PFD-BDCM), specifically designed for causal inference in the presence of
unmeasured confounders with spatial heterogeneity and temporal dependency. The
proposed PFD-BDCM framework addresses the restrictions of the existing
approaches by uniquely integrating models for complex spatio-temporal dynamics
with the analysis of multi-resolution variables. Specifically, the framework
systematically mitigates confounding bias by integrating valid backdoor
adjustment sets into a diffusion-based sampling mechanism. Moreover, it
accounts for the intricate dynamics of unmeasured confounders through the
deployment of region-specific structural equations and conditional
autoregressive processes, and accommodates variables observed at heterogeneous
resolutions via basis expansions for functional data. Our theoretical analysis
establishes error bounds for counterfactual estimates of PFD-BDCM, formally
linking reconstruction accuracy to counterfactual fidelity under monotonicity
assumptions of structural equation and invertibility assumptions of encoding
function. Empirical evaluations on synthetic datasets and real-world air
pollution data demonstrate PFD-BDCM's superiority over existing methods.

</details>


### [425] [Identifying Causal Direction via Dense Functional Classes](https://arxiv.org/abs/2509.00538)
*Katerina Hlavackova-Schindler,Suzana Marsela*

Main category: stat.ML

TL;DR: 提出基于MDL原则的双变量因果分数LCUBE，可区分两变量因果方向，实证显示其在多个数据集上精度优越。


<details>
  <summary>Details</summary>
Motivation: 在无隐藏混杂因素假设下，解决确定两个单变量连续值变量因果方向的问题，且在无潜在模型假设时难以明确因果关系。

Method: 提出基于MDL原则的双变量因果分数，利用在紧凑实区间有密度性质的函数，证明特定条件下因果分数可识别，提出用三次回归样条的LCUBE方法。

Result: LCUBE在Tuebingen因果对数据集的AUDRC精度优越，在10个常见基准数据集上平均精度高，在13个数据集上精度高于平均。

Conclusion: LCUBE是可识别、可解释、简单且快速的方法，能有效区分因果方向。

Abstract: We address the problem of determining the causal direction between two
univariate, continuous-valued variables, X and Y, under the assumption of no
hidden confounders. In general, it is not possible to make definitive
statements about causality without some assumptions on the underlying model. To
distinguish between cause and effect, we propose a bivariate causal score based
on the Minimum Description Length (MDL) principle, using functions that possess
the density property on a compact real interval. We prove the identifiability
of these causal scores under specific conditions. These conditions can be
easily tested. Gaussianity of the noise in the causal model equations is not
assumed, only that the noise is low. The well-studied class of cubic splines
possesses the density property on a compact real interval. We propose LCUBE as
an instantiation of the MDL-based causal score utilizing cubic regression
splines. LCUBE is an identifiable method that is also interpretable, simple,
and very fast. It has only one hyperparameter. Empirical evaluations compared
to state-of-the-art methods demonstrate that LCUBE achieves superior precision
in terms of AUDRC on the real-world Tuebingen cause-effect pairs dataset. It
also shows superior average precision across common 10 benchmark datasets and
achieves above average precision on 13 datasets.

</details>


### [426] [Semi-Supervised Bayesian GANs with Log-Signatures for Uncertainty-Aware Credit Card Fraud Detection](https://arxiv.org/abs/2509.00931)
*David Hirnschall*

Main category: stat.ML

TL;DR: 提出用于信用卡欺诈检测的深度生成半监督框架，在BankSim数据集上评估，效果优于基准。


<details>
  <summary>Details</summary>
Motivation: 金融交易数据规模和复杂度增加，传统方法需大量标注数据，难处理不规则采样频率和不同序列长度的时间序列。

Method: 扩展条件生成对抗网络进行数据增强，集成贝叶斯推理获取预测分布和量化不确定性，利用对数签名进行特征编码，引入基于Wasserstein距离的损失函数。

Result: 在BankSim数据集不同标注样本比例下评估，在全局统计和特定领域指标上持续优于基准。

Conclusion: GAN驱动的半监督学习结合对数签名对不规则采样时间序列有效，强调不确定性感知预测的重要性。

Abstract: We present a novel deep generative semi-supervised framework for credit card
fraud detection, formulated as time series classification task. As financial
transaction data streams grow in scale and complexity, traditional methods
often require large labeled datasets, struggle with time series of irregular
sampling frequencies and varying sequence lengths. To address these challenges,
we extend conditional Generative Adversarial Networks (GANs) for targeted data
augmentation, integrate Bayesian inference to obtain predictive distributions
and quantify uncertainty, and leverage log-signatures for robust feature
encoding of transaction histories. We introduce a novel Wasserstein
distance-based loss to align generated and real unlabeled samples while
simultaneously maximizing classification accuracy on labeled data. Our approach
is evaluated on the BankSim dataset, a widely used simulator for credit card
transaction data, under varying proportions of labeled samples, demonstrating
consistent improvements over benchmarks in both global statistical and
domain-specific metrics. These findings highlight the effectiveness of
GAN-driven semi-supervised learning with log-signatures for irregularly sampled
time series and emphasize the importance of uncertainty-aware predictions.

</details>


### [427] [Preconditioned Regularized Wasserstein Proximal Sampling](https://arxiv.org/abs/2509.01685)
*Hong Ye Tan,Stanley Osher,Wuchen Li*

Main category: stat.ML

TL;DR: 提出一种预条件版本的无噪声采样方法，用于从吉布斯分布采样，分析收敛性和偏差，实验显示在多场景有良好表现。


<details>
  <summary>Details</summary>
Motivation: 研究从吉布斯分布采样的有效方法。

Method: 提出预条件版本的无噪声采样方法，通过Cole - Hopf变换近似得分函数，将扩散分量解释为改进的自注意力块。

Result: 对于二次势给出离散时间非渐近收敛分析和偏差特征；实验在多种场景展示加速和稳定性，在非凸贝叶斯神经网络训练中表现有竞争力。

Conclusion: 该预条件无噪声采样方法在采样任务中有效且性能良好。

Abstract: We consider sampling from a Gibbs distribution by evolving finitely many
particles. We propose a preconditioned version of a recently proposed
noise-free sampling method, governed by approximating the score function with
the numerically tractable score of a regularized Wasserstein proximal operator.
This is derived by a Cole--Hopf transformation on coupled anisotropic heat
equations, yielding a kernel formulation for the preconditioned regularized
Wasserstein proximal. The diffusion component of the proposed method is also
interpreted as a modified self-attention block, as in transformer
architectures. For quadratic potentials, we provide a discrete-time
non-asymptotic convergence analysis and explicitly characterize the bias, which
is dependent on regularization and independent of step-size. Experiments
demonstrate acceleration and particle-level stability on various log-concave
and non-log-concave toy examples to Bayesian total-variation regularized image
deconvolution, and competitive/better performance on non-convex Bayesian neural
network training when utilizing variable preconditioning matrices.

</details>


### [428] [Hybrid Topic-Semantic Labeling and Graph Embeddings for Unsupervised Legal Document Clustering](https://arxiv.org/abs/2509.00990)
*Deepak Bastola,Woohyeok Choi*

Main category: stat.ML

TL;DR: 本文提出结合无监督主题和图嵌入与监督模型的混合方法对法律文本分类，实验证明其优于仅文本或仅图嵌入方法，还需进一步细化和特定领域适配。


<details>
  <summary>Details</summary>
Motivation: 法律文件因特定领域语言和有限标注数据，给文本分类带来挑战，需新分类方法。

Method: 结合Top2Vec学习语义文档嵌入、发现潜在主题，用Node2Vec通过二分图捕捉结构关系，用KMeans对嵌入组合聚类。

Result: Top2Vec+Node2Vec方法提升聚类质量，优于基线模型。

Conclusion: 该方法有创新性，但效果依赖初始主题生成质量和嵌入模型，需探索特定领域嵌入、调参等，可用于探索性分析和监督学习前任务，需进一步细化和适配。

Abstract: Legal documents pose unique challenges for text classification due to their
domain-specific language and often limited labeled data. This paper proposes a
hybrid approach for classifying legal texts by combining unsupervised topic and
graph embeddings with a supervised model. We employ Top2Vec to learn semantic
document embeddings and automatically discover latent topics, and Node2Vec to
capture structural relationships via a bipartite graph of legal documents. The
embeddings are combined and clustered using KMeans, yielding coherent groupings
of documents. Our computations on a legal document dataset demonstrate that the
combined Top2Vec+Node2Vec approach improves clustering quality over text-only
or graph-only embeddings. We conduct a sensitivity analysis of hyperparameters,
such as the number of clusters and the dimensionality of the embeddings, and
demonstrate that our method achieves competitive performance against baseline
Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF)
models. Key findings indicate that while the pipeline presents an innovative
approach to unsupervised legal document analysis by combining semantic topic
modeling with graph embedding techniques, its efficacy is contingent upon the
quality of initial topic generation and the representational power of the
chosen embedding models for specialized legal language. Strategic
recommendations include the exploration of domain-specific embeddings, more
comprehensive hyperparameter tuning for Node2Vec, dynamic determination of
cluster numbers, and robust human-in-the-loop validation processes to enhance
legal relevance and trustworthiness. The pipeline demonstrates potential for
exploratory legal data analysis and as a precursor to supervised learning tasks
but requires further refinement and domain-specific adaptation for practical
legal applications.

</details>


### [429] [Lipschitz-Guided Design of Interpolation Schedules in Generative Models](https://arxiv.org/abs/2509.01629)
*Yifan Chen,Eric Vanden-Eijnden,Jiawei Xu*

Main category: stat.ML

TL;DR: 研究流和扩散生成模型随机插值框架中插值调度设计，提出新准则优化调度，在多种分布上验证性能提升。


<details>
  <summary>Details</summary>
Motivation: 发现不同标量插值调度统计效率相同但数值效率差异大，因此关注漂移场数值特性进行调度设计。

Method: 提出平均平方Lipschitz性最小化作为数值优化准则，推导转移公式实现不同调度转换。

Result: 高斯分布下优化调度的Lipschitz常数指数级提升，高斯混合分布减少少步采样的模式崩溃，在高维不变分布上有性能提升。

Conclusion: 基于数值特性的调度设计方法有效，能在多种分布上实现性能改进。

Abstract: We study the design of interpolation schedules in the stochastic interpolants
framework for flow and diffusion-based generative models. We show that while
all scalar interpolation schedules achieve identical statistical efficiency
under Kullback-Leibler divergence in path space after optimal diffusion
coefficient tuning, their numerical efficiency can differ substantially. This
observation motivates focusing on numerical properties of the resulting drift
fields rather than statistical criteria for schedule design. We propose
averaged squared Lipschitzness minimization as a principled criterion for
numerical optimization, providing an alternative to kinetic energy minimization
used in optimal transport approaches. A transfer formula is derived that
enables conversion between different schedules at inference time without
retraining neural networks. For Gaussian distributions, our optimized schedules
achieve exponential improvements in Lipschitz constants over standard linear
schedules, while for Gaussian mixtures, they reduce mode collapse in few-step
sampling. We also validate our approach on high-dimensional invariant
distributions from stochastic Allen-Cahn equations and Navier-Stokes equations,
demonstrating robust performance improvements across resolutions.

</details>


### [430] [The Price of Sparsity: Sufficient Conditions for Sparse Recovery using Sparse and Sparsified Measurements](https://arxiv.org/abs/2509.01809)
*Youssef Chaabouni,David Gamarnik*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of recovering the support of a sparse signal using
noisy projections. While extensive work has been done on the dense measurement
matrix setting, the sparse setting remains less explored. In this work, we
establish sufficient conditions on the sample size for successful sparse
recovery using sparse measurement matrices. Bringing together our result with
previously known necessary conditions, we discover that, in the regime where
$ds/p \rightarrow +\infty$, sparse recovery in the sparse setting exhibits a
phase transition at an information-theoretic threshold of
$n_{\text{INF}}^{\text{SP}} =
\Theta\left(s\log\left(p/s\right)/\log\left(ds/p\right)\right)$, where $p$
denotes the signal dimension, $s$ the number of non-zero components of the
signal, and $d$ the expected number of non-zero components per row of
measurement. This expression makes the price of sparsity explicit: restricting
each measurement to $d$ non-zeros inflates the required sample size by a factor
of $\log{s}/\log\left(ds/p\right)$, revealing a precise trade-off between
sampling complexity and measurement sparsity. Additionally, we examine the
effect of sparsifying an originally dense measurement matrix on sparse signal
recovery. We prove in the regime of $s = \alpha p$ and $d = \psi p$ with
$\alpha, \psi \in \left(0,1\right)$ and $\psi$ small that a sample of size
$n^{\text{Sp-ified}}_{\text{INF}} = \Theta\left(p / \psi^2\right)$ is
sufficient for recovery, subject to a certain uniform integrability conjecture,
the proof of which is work in progress.

</details>


### [431] [Design of Experiment for Discovering Directed Mixed Graph](https://arxiv.org/abs/2509.01887)
*Haijie Xu,Chen Zhang*

Main category: stat.ML

TL;DR: 研究简单结构因果模型因果图结构识别的实验设计问题，提出两类算法，算法与下界渐近紧。


<details>
  <summary>Details</summary>
Motivation: 潜在混杂因素导致的循环和双向边使仅用观测数据无法恢复图骨架，传统条件独立测试在某些情况下失效。

Method: 建立单次实验最大干预变量数和总实验数的下界，结合条件独立测试和do - see测试，考虑d分离和σ分离，开发有界和无界两类算法。

Result: 开发出两类能恢复除双相邻双向边外所有因果边的算法，算法与导出的下界在对数因子内渐近紧。

Conclusion: 所提算法在解决简单结构因果模型因果图结构识别问题上具有有效性和渐近最优性。

Abstract: We study the problem of experimental design for accurately identifying the
causal graph structure of a simple structural causal model (SCM), where the
underlying graph may include both cycles and bidirected edges induced by latent
confounders. The presence of cycles renders it impossible to recover the graph
skeleton using observational data alone, while confounding can further
invalidate traditional conditional independence (CI) tests in certain
scenarios. To address these challenges, we establish lower bounds on both the
maximum number of variables that can be intervened upon in a single experiment
and the total number of experiments required to identify all directed edges and
non-adjacent bidirected edges. Leveraging both CI tests and do see tests, and
accounting for $d$ separation and $\sigma$ separation, we develop two classes
of algorithms, i.e., bounded and unbounded, that can recover all causal edges
except for double adjacent bidirected edges. We further show that, up to
logarithmic factors, the proposed algorithms are tight with respect to the
derived lower bounds.

</details>


### [432] [Non-Linear Model-Based Sequential Decision-Making in Agriculture](https://arxiv.org/abs/2509.01924)
*Sakshi Arya,Wentao Lin*

Main category: stat.ML

TL;DR: 提出非线性、基于模型的多臂老虎机算法用于农业顺序决策，理论分析和模拟显示效果好，适用于资源受限场景并助力可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 经典多臂老虎机和强化学习方法在农业顺序决策中存在依赖线性或黑盒奖励模型、需大量数据等问题，而实际决策常基于有限观测。

Method: 提出嵌入特定领域响应曲线到探索 - 利用循环的非线性、基于模型的多臂老虎机算法，结合原则性不确定性量化和可快速计算的利润最优解。

Result: 算法实现次线性悔值和接近最优的样本复杂度，在低样本情况下比线性和非参数基线表现更好。

Conclusion: 该方法适用于资源受限场景，支持可持续顺序决策，有助于实现零饥饿和负责任消费与生产的可持续发展目标。

Abstract: Sequential decision-making is central to sustainable agricultural management
and precision agriculture, where resource inputs must be optimized under
uncertainty and over time. However, such decisions must often be made with
limited observations, whereas classical bandit and reinforcement learning
approaches typically rely on either linear or black-box reward models that may
misrepresent domain knowledge or require large amounts of data. We propose a
family of nonlinear, model-based bandit algorithms that embed domain-specific
response curves directly into the exploration-exploitation loop. By coupling
(i) principled uncertainty quantification with (ii) closed-form or rapidly
computable profit optima, these algorithms achieve sublinear regret and
near-optimal sample complexity while preserving interpretability. Theoretical
analysis establishes regret and sample complexity bounds, and extensive
simulations emulating real-world fertilizer-rate decisions show consistent
improvements over both linear and nonparametric baselines (such as linear UCB
and $k$-NN UCB) in the low-sample regime, under both well-specified and
shape-compatible misspecified models. Because our approach leverages
mechanistic insight rather than large data volumes, it is especially suited to
resource-constrained settings, supporting sustainable, inclusive, and
transparent sequential decision-making across agriculture, environmental
management, and allied applications. This methodology directly contributes to
SDG 2 (Zero Hunger) and SDG 12 (Responsible Consumption and Production) by
enabling data-driven, less wasteful agricultural practices.

</details>


### [433] [Inference in Spreading Processes with Neural-Network Priors](https://arxiv.org/abs/2509.02073)
*Davide Ghio,Fabrizio Boncoraglio,Lenka Zdeborová*

Main category: stat.ML

TL;DR: 本文研究图上随机过程的推理问题，假设节点初始状态是协变量的未知函数，用单层感知机表示，在贝叶斯框架下研究其对初始状态和传播轨迹恢复的影响，推导BP - AMP算法并评估性能，还发现特定情况下的一阶相变和统计 - 计算差距。


<details>
  <summary>Details</summary>
Motivation: 以往工作假设过程初始状态在节点上是随机独立同分布的，这在实际中可能不现实，实际中每个节点可能有影响系统初始状态的协变量。

Method: 假设节点初始状态是协变量的未知函数，用单层感知机表示；在贝叶斯框架下推导混合信念传播和近似消息传递（BP - AMP）算法。

Result: 在某些情况下，使用拉德马赫分布的神经网络权重时，模型会出现一阶相变，产生统计 - 计算差距，BP - AMP算法虽理论上可完美恢复但实际失败。

Conclusion: 基于协变量的神经网络先验信息在图上随机过程推理中有一定作用，但存在统计 - 计算差距问题。

Abstract: Stochastic processes on graphs are a powerful tool for modelling complex
dynamical systems such as epidemics. A recent line of work focused on the
inference problem where one aims to estimate the state of every node at every
time, starting from partial observation of a subset of nodes at a subset of
times. In these works, the initial state of the process was assumed to be
random i.i.d. over nodes. Such an assumption may not be realistic in practice,
where one may have access to a set of covariate variables for every node that
influence the initial state of the system. In this work, we will assume that
the initial state of a node is an unknown function of such covariate variables.
Given that functions can be represented by neural networks, we will study a
model where the initial state is given by a simple neural network -- notably
the single-layer perceptron acting on the known node-wise covariate variables.
  Within a Bayesian framework, we study how such neural-network prior
information enhances the recovery of initial states and spreading trajectories.
We derive a hybrid belief propagation and approximate message passing (BP-AMP)
algorithm that handles both the spreading dynamics and the information included
in the node covariates, and we assess its performance against the estimators
that either use only the spreading information or use only the information from
the covariate variables.
  We show that in some regimes, the model can exhibit first-order phase
transitions when using a Rademacher distribution for the neural-network
weights. These transitions create a statistical-to-computational gap where even
the BP-AMP algorithm, despite the theoretical possibility of perfect recovery,
fails to achieve it.

</details>


### [434] [Amputation-imputation based generation of synthetic tabular data for ratemaking](https://arxiv.org/abs/2509.02171)
*Yevhen Havrylenko,Meelis Käärik,Artur Tuttar*

Main category: stat.ML

TL;DR: 本文探讨合成数据生成解决精算费率制定数据获取问题，比较多种生成方法，结果显示基于MICE的方法有潜力且更易用。


<details>
  <summary>Details</summary>
Motivation: 精算费率制定依赖高质量数据，但获取新数据成本高、有隐私问题，故探索合成数据生成解决方案。

Method: 除讨论精算文献中已有生成方法，引入基于MICE的方法，用开源数据集对比MICE模型与其他生成模型，评估合成数据保留变量分布和关系情况，研究合成数据训练的GLMs与原始数据训练的一致性，评估各方法易用性及合成数据增强对GLMs预测索赔数量性能的影响。

Result: 基于MICE的方法在创建高质量表格数据方面有潜力，且比其他方法更易用。

Conclusion: 基于MICE的方法可作为解决精算费率制定数据获取问题的有效途径。

Abstract: Actuarial ratemaking depends on high-quality data, yet access to such data is
often limited by the cost of obtaining new data, privacy concerns, etc. In this
paper, we explore synthetic-data generation as a potential solution to these
issues. In addition to discussing generative methods previously studied in the
actuarial literature, we introduce to the insurance community another approach
based on Multiple Imputation by Chained Equations (MICE). We present a
comparative study using an open-source dataset and evaluating MICE-based models
against other generative models like Variational Autoencoders and Conditional
Tabular Generative Adversarial Networks. We assess how well synthetic data
preserves the original marginal distributions of variables as well as the
multivariate relationships among covariates. We also investigate the
consistency between Generalized Linear Models (GLMs) trained on synthetic data
with GLMs trained on the original data. Furthermore, we assess the ease of use
of each generative approach and study the impact of augmenting original data
with synthetic data on the performance of GLMs for predicting claim counts. Our
results highlight the potential of MICE-based methods in creating high-quality
tabular data while being more user-friendly than the other methods.

</details>


### [435] [Variational Uncertainty Decomposition for In-Context Learning](https://arxiv.org/abs/2509.02327)
*I. Shavindra Jayasekera,Jacob Si,Wenlong Chen,Filippo Valdettaro,A. Aldo Faisal,Yingzhen Li*

Main category: stat.ML

TL;DR: 介绍了一种无需从潜在参数后验采样的上下文学习变分不确定性分解框架，实验证明该方法分解的不确定性具有理想特性。


<details>
  <summary>Details</summary>
Motivation: 理解上下文学习中不确定性的来源对确保大语言模型预测任务的可靠性至关重要，但现有分解思路因潜在参数后验难以处理而未充分探索。

Method: 优化辅助查询作为探针，获得大语言模型上下文学习过程中偶然不确定性的上界，进而得到认知不确定性的下界。

Result: 通过合成和真实世界任务的实验，定量和定性地表明该方法分解的不确定性具有认知和偶然不确定性的理想特性。

Conclusion: 提出的变分不确定性分解框架在上下文学习中有效，能合理分解不确定性。

Abstract: As large language models (LLMs) gain popularity in conducting prediction
tasks in-context, understanding the sources of uncertainty in in-context
learning becomes essential to ensuring reliability. The recent hypothesis of
in-context learning performing predictive Bayesian inference opens the avenue
for Bayesian uncertainty estimation, particularly for decomposing uncertainty
into epistemic uncertainty due to lack of in-context data and aleatoric
uncertainty inherent in the in-context prediction task. However, the
decomposition idea remains under-explored due to the intractability of the
latent parameter posterior from the underlying Bayesian model. In this work, we
introduce a variational uncertainty decomposition framework for in-context
learning without explicitly sampling from the latent parameter posterior, by
optimising auxiliary queries as probes to obtain an upper bound to the
aleatoric uncertainty of an LLM's in-context learning procedure, which also
induces a lower bound to the epistemic uncertainty. Through experiments on
synthetic and real-world tasks, we show quantitatively and qualitatively that
the decomposed uncertainties obtained from our method exhibit desirable
properties of epistemic and aleatoric uncertainty.

</details>


### [436] [Distribution estimation via Flow Matching with Lipschitz guarantees](https://arxiv.org/abs/2509.02337)
*Lea Kunkel*

Main category: stat.ML

TL;DR: 本文研究Flow Matching控制向量场Lipschitz常数依赖的假设，推导了Wasserstein 1距离收敛率，在高维等情况有改进。


<details>
  <summary>Details</summary>
Motivation: Flow Matching虽经验上成功，但数学上对其统计能力理解有限，主要因理论界对驱动ODE的向量场Lipschitz常数敏感。

Method: 研究导致控制这种依赖的假设。

Result: 推导了估计分布和目标分布间Wasserstein 1距离的收敛率，在高维设置中改进了先前结果，适用于某些无界分布类且无需对数凹性。

Conclusion: 通过研究假设得到更优的收敛率，提升了对Flow Matching统计能力的数学理解。

Abstract: Flow Matching, a promising approach in generative modeling, has recently
gained popularity. Relying on ordinary differential equations, it offers a
simple and flexible alternative to diffusion models, which are currently the
state-of-the-art. Despite its empirical success, the mathematical understanding
of its statistical power so far is very limited. This is largely due to the
sensitivity of theoretical bounds to the Lipschitz constant of the vector field
which drives the ODE. In this work, we study the assumptions that lead to
controlling this dependency. Based on these results, we derive a convergence
rate for the Wasserstein $1$ distance between the estimated distribution and
the target distribution which improves previous results in high dimensional
setting. This rate applies to certain classes of unbounded distributions and
particularly does not require $\log$-concavity.

</details>


### [437] [Wild Refitting for Model-Free Excess Risk Evaluation of Opaque ML/AI Models under Bregman Loss](https://arxiv.org/abs/2509.02476)
*Haichen Hu,David Simchi-Levi*

Main category: stat.ML

TL;DR: 研究用Bregman损失评估经典惩罚经验风险最小化（ERM）的超额风险问题，利用野生重拟合程序有效上界超额风险，有高概率性能保证，对评估现代不透明ML和AI模型有前景。


<details>
  <summary>Details</summary>
Motivation: 解决评估经典惩罚ERM超额风险问题，且适用于复杂模型类。

Method: 利用野生重拟合程序，进行随机向量值对称化、缩放预测残差、构造人工修改结果，重新训练第二个预测器进行超额风险估计。

Result: 在固定设计和随机设计设置下建立高概率性能保证，表明在适当噪声尺度下野生重拟合能得到超额风险有效上界。

Conclusion: 该工作对理论评估现代不透明ML和AI模型有前途。

Abstract: We study the problem of evaluating the excess risk of classical penalized
empirical risk minimization (ERM) with Bregman losses. We show that by
leveraging the recently proposed wild refitting procedure (Wainwright, 2025),
one can efficiently upper bound the excess risk through the so-called "wild
optimism," without relying on the global structure of the underlying function
class. This property makes our approach inherently model-free. Unlike
conventional analyses, our framework operates with just one dataset and
black-box access to the training procedure. The method involves randomized
vector-valued symmetrization with an appropriate scaling of the prediction
residues and constructing artificially modified outcomes, upon which we retrain
a second predictor for excess risk estimation. We establish high-probability
performance guarantees both under the fixed design setting and the random
design setting, demonstrating that wild refitting under Bregman losses, with an
appropriately chosen wild noise scale, yields a valid upper bound on the excess
risk. This work thus is promising for theoretically evaluating modern opaque ML
and AI models such as deep neural networks and large language models, where the
model class is too complex for classical learning theory and empirical process
techniques to apply.

</details>


### [438] [Probabilities of Causation and Root Cause Analysis with Quasi-Markovian Models](https://arxiv.org/abs/2509.02535)
*Eduardo Rocha Laurentino,Fabio Gagliardi Cozman,Denis Deratani Maua,Daniel Angelo Esteves Lawand,Davi Goncalves Bezerra Coelho,Lucas Martins Marques*

Main category: stat.ML

TL;DR: 本文介绍算法简化及根因分析新框架，以解决因果概率计算难题。


<details>
  <summary>Details</summary>
Motivation: 因果概率评估因果关系存在计算挑战，如部分可识别性和潜在混淆问题。

Method: 引入算法简化以降低计算因果概率边界的复杂度，提出新的根因分析方法框架。

Result: 显著降低计算因果概率更严格边界的计算复杂度。

Conclusion: 可通过新框架系统使用因果指标对整个因果路径进行排名。

Abstract: Probabilities of causation provide principled ways to assess causal
relationships but face computational challenges due to partial identifiability
and latent confounding. This paper introduces both algorithmic simplifications,
significantly reducing the computational complexity of calculating tighter
bounds for these probabilities, and a novel methodological framework for Root
Cause Analysis that systematically employs these causal metrics to rank entire
causal paths.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [439] [SANVI: A Fast Spectral-Assisted Network Variational Inference Method with an Extended Surrogate Likelihood Function](https://arxiv.org/abs/2509.00562)
*Dingbo Wu,Fangzheng Xie*

Main category: stat.CO

TL;DR: 本文提出广义随机点积图框架下的Spectral - Assisted Network Variational Inference (SANVI) 方法，证明相关性质，数值研究显示其比经典MCMC算法更高效。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推理用于统计网络分析时，因马尔可夫链蒙特卡罗抽样算法导致计算成本高。

Method: 提出扩展替代似然函数，利用其特性通过随机梯度下降设计高斯变分推理算法。

Result: 证明最大扩展替代似然估计量的渐近效率和变分后验分布的Bernstein - von Mises极限；数值研究显示SANVI算法与经典MCMC算法估计精度相当，但计算成本更低。

Conclusion: 提出的SANVI方法在统计网络分析中具有计算高效性和实用性。

Abstract: Bayesian inference has been broadly applied to statistical network analysis,
but suffers from the expensive computational costs due to the nature of Markov
chain Monte Carlo sampling algorithms. This paper proposes a novel and
computationally efficient Spectral-Assisted Network Variational Inference
(SANVI) method within the framework of the generalized random dot product
graph. The key idea is a cleverly designed extended surrogate likelihood
function that enjoys two convenient features. Firstly, it decouples the
generalized inner product of latent positions in the random graph model.
Secondly, it relaxes the complicated domain of the original likelihood function
to the entire Euclidean space. Leveraging these features, we design a
computationally efficient Gaussian variational inference algorithm via
stochastic gradient descent. Furthermore, we show the asymptotic efficiency of
the maximum extended surrogate likelihood estimator and the Bernstein-von Mises
limit of the variational posterior distribution. Through extensive numerical
studies, we demonstrate the usefulness of the proposed SANVI algorithm compared
to the classical Markov chain Monte Carlo algorithm, including comparable
estimation accuracy for the latent positions and less computational costs.

</details>


### [440] [Removal of Redundant Candidate Points for the Exact D-Optimal Design Problem](https://arxiv.org/abs/2509.00719)
*Radoslav Harman,Samuel Rosa*

Main category: stat.CO

TL;DR: 本文提出基于近似设计的必要条件以筛选D - 最优精确设计的候选点，可大幅缩小候选集规模，使计算精确D - 最优设计成为可能。


<details>
  <summary>Details</summary>
Motivation: 解决在大型有限候选集上计算D - 最优精确设计这一困难的整数优化问题。

Method: 提出基于近似设计的必要条件来筛选候选点，证明大样本下D - 最优精确设计的支撑集包含在最大方差集内。

Result: 在随机生成的基准模型和常用的约束混合模型上，所提方法将初始候选集缩小了几个数量级。

Conclusion: 所提方法使通过混合整数二阶锥规划计算精确D - 最优设计成为可能，并能提供最优性保证。

Abstract: One of the most common problems in statistical experimentation is computing
D-optimal designs on large finite candidate sets. While optimal approximate
(i.e., infinite-sample) designs can be efficiently computed using convex
methods, constructing optimal exact (i.e., finite-sample) designs is a
substantially more difficult integer-optimization problem. In this paper, we
propose necessary conditions, based on approximate designs, that must be
satisfied by any support point of a D-optimal exact design. These conditions
enable rapid elimination of redundant candidate points without loss of
optimality, thereby reducing memory requirements and runtime of subsequent
exact design algorithms. In addition, we prove that for sufficiently large
sample sizes, the supports of D-optimal exact designs are contained in a
typically small maximum-variance set. We demonstrate the approach on randomly
generated benchmark models with candidate sets up to 100 million points, and on
commonly used constrained mixture models with up to one million points. The
proposed approach reduces the initial candidate sets by several orders of
magnitude, thereby making it possible to compute exact D-optimal designs for
these problems via mixed-integer second-order cone programming, which provides
optimality guarantees.

</details>


### [441] [Regime-Switching Langevin Monte Carlo Algorithms](https://arxiv.org/abs/2509.00941)
*Xiaoyu Wang,Yingli Wang,Lingjiong Zhu*

Main category: stat.CO

TL;DR: 本文受概率文献中体制转换随机微分方程启发，提出并研究了体制转换Langevin动力学及其蒙特卡罗算法，给出收敛保证和迭代复杂度分析，并用数值实验验证算法效率。


<details>
  <summary>Details</summary>
Motivation: 为采样目标概率分布，改进现有Langevin Monte Carlo算法。

Method: 受体制转换随机微分方程启发，提出体制转换Langevin动力学和相应的蒙特卡罗算法，包括RS - LMC、RS - KLMC和FRS - KLMC。

Result: 给出算法到目标分布的2 - Wasserstein非渐近收敛保证，分析了迭代复杂度。

Conclusion: 数值实验表明提出的算法有效。

Abstract: Langevin Monte Carlo (LMC) algorithms are popular Markov Chain Monte Carlo
(MCMC) methods to sample a target probability distribution, which arises in
many applications in machine learning. Inspired by regime-switching stochastic
differential equations in the probability literature, we propose and study
regime-switching Langevin dynamics (RS-LD) and regime-switching kinetic
Langevin dynamics (RS-KLD). Based on their discretizations, we introduce
regime-switching Langevin Monte Carlo (RS-LMC) and regime-switching kinetic
Langevin Monte Carlo (RS-KLMC) algorithms, which can also be viewed as LMC and
KLMC algorithms with random stepsizes. We also propose
frictional-regime-switching kinetic Langevin dynamics (FRS-KLD) and its
associated algorithm frictional-regime-switching kinetic Langevin Monte Carlo
(FRS-KLMC), which can also be viewed as the KLMC algorithm with random
frictional coefficients. We provide their 2-Wasserstein non-asymptotic
convergence guarantees to the target distribution, and analyze the iteration
complexities. Numerical experiments using both synthetic and real data are
provided to illustrate the efficiency of our proposed algorithms.

</details>


### [442] [Ensemble Control Variates](https://arxiv.org/abs/2509.01091)
*Long M. Nguyen,Christopher Drovandi,Leah F. South*

Main category: stat.CO

TL;DR: 提出用基于平均的集成学习构建Stein控制变量，聚焦零方差控制变量集成，模拟研究显示集成ZVCV方法统计效率与正则化ZVCV相当且速度更快，为构建控制变量技术开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 现有基于Langevin - Stein算子的控制变量需在用户定义函数类中优化函数，零方差控制变量在中高维易过参数化，正则化方法速度慢。

Method: 使用基于平均的集成学习构建Stein控制变量，引入基于OLS估计器集成的集成ZVCV方法，并通过模拟研究评估。

Result: 集成ZVCV方法在统计效率上与正则化ZVCV方法有竞争力，且速度大幅提升。

Conclusion: 通过集成学习构建控制变量技术开辟了新方向。

Abstract: Control variates have become an increasingly popular variance-reduction
technique in Bayesian inference. Many broadly applicable control variates are
based on the Langevin-Stein operator, which leverages gradient information from
any gradient-based sampler to produce variance-reduced estimators of
expectations. These control variates typically require optimising over a
function $u(\theta)$ within a user-defined functional class $G$, such as the
space of $Q$th-order polynomials or a reproducing kernel Hilbert space. We
propose using averaging-based ensemble learning to construct Stein-based
control variates. While the proposed framework is broadly applicable, we focus
on ensembles constructed from zero-variance control variates (ZVCV), a popular
parametric approach based on solving a linear approximation problem that can
easily be over-parameterised in medium-to-high dimensional settings. A common
remedy is to use regularised ZVCV via penalised regression, but these methods
can be prohibitively slow. We introduce ensemble ZVCV methods based on
ensembles of OLS estimators and evaluate the proposed methods against
established methods in the literature in a simulation study. Our results show
that ensemble ZVCV methods are competitive with regularised ZVCV methods in
terms of statistical efficiency, but are substantially faster. This work opens
a new direction for constructing broadly applicable control variate techniques
via ensemble learning.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [443] [GeneTEK: Low-power, high-performance and scalable genome sequence matching in FPGAs](https://arxiv.org/abs/2509.01020)
*Elena Espinosa,Rubén Rodríguez Álvarez,José Miranda,Rafael Larrosa,Miguel Peón-Quirós,Oscar Plata,David Atienza*

Main category: cs.AR

TL;DR: 本文提出基于FPGA的可扩展灵活加速器模板GeneTEK，在速度和能效上超越CPU和GPU实现，克服现有FPGA方法可扩展性限制，证实FPGA用于基因组工作负载潜力。


<details>
  <summary>Details</summary>
Motivation: 下一代测序使基因组数据量激增，成对序列比对耗时耗能，现有FPGA实现存在可扩展性问题，需改进。

Method: 提出可扩展灵活的FPGA加速器模板，用高级综合和基于工作者架构实现Myers算法，以Xilinx Zynq UltraScale+ FPGA为实例。

Result: GeneTEK在速度和能效上超越CPU和GPU实现，执行速度至少提高19.4%，能耗最多降低62倍，比对矩阵比先前FPGA方案大72%。

Conclusion: FPGA作为可扩展基因组工作负载的节能平台具有潜力。

Abstract: The advent of next-generation sequencing (NGS) has revolutionized genomic
research by enabling high-throughput data generation through parallel
sequencing of a diverse range of organisms at significantly reduced costs. This
breakthrough has unleashed a "Cambrian explosion" in genomic data volume and
diversity. This volume of workloads places genomics among the top four big data
challenges anticipated for this decade. In this context, pairwise sequence
alignment represents a very time- and energy-consuming step in common
bioinformatics pipelines. Speeding up this step requires the implementation of
heuristic approaches, optimized algorithms, and/or hardware acceleration.
  Whereas state-of-the-art CPU and GPU implementations have demonstrated
significant performance gains, recent field programmable gate array (FPGA)
implementations have shown improved energy efficiency. However, the latter
often suffer from limited scalability due to constraints on hardware resources
when aligning longer sequences. In this work, we present a scalable and
flexible FPGA-based accelerator template that implements Myers's algorithm
using high-level synthesis and a worker-based architecture. GeneTEK, an
instance of this accelerator template in a Xilinx Zynq UltraScale+ FPGA,
outperforms state-of-the-art CPU and GPU implementations in both speed and
energy efficiency, while overcoming scalability limitations of current FPGA
approaches. Specifically, GeneTEK achieves at least a 19.4% increase in
execution speed and up to 62x reduction in energy consumption compared to
leading CPU and GPU solutions, while fitting comparison matrices up to 72%
larger compared to previous FPGA solutions. These results reaffirm the
potential of FPGAs as an energy-efficient platform for scalable genomic
workloads.

</details>


### [444] [On the Thermal Vulnerability of 3D-Stacked High-Bandwidth Memory Architectures](https://arxiv.org/abs/2509.00633)
*Mehdi Elahi,Mohamed R. Elshamy,Abdel-Hameed A. Badawy,Ahmad Patooghy*

Main category: cs.AR

TL;DR: 3D堆叠HBM架构有热漏洞，对手可利用相邻性进行热性能降级攻击且难检测。


<details>
  <summary>Details</summary>
Motivation: 指出3D堆叠HBM架构存在热漏洞，对手可能利用此漏洞进行攻击，凸显研究防范此类攻击的必要性。

Method: 未提及具体方法。

Result: 攻击者可通过注入热脉冲制造热波，影响受害者应用数据/指令访问，且能绕过安全测试和内存管理策略。

Conclusion: 此类攻击因模仿合法工作负载而难以检测。

Abstract: 3D-stacked High Bandwidth Memory (HBM) architectures provide high-performance
memory interactions to address the well-known performance challenge, namely the
memory wall. However, these architectures are susceptible to thermal
vulnerabilities due to the inherent vertical adjacency that occurs during the
manufacturing process of HBM architectures. We anticipate that adversaries may
exploit the intense vertical and lateral adjacency to design and develop
thermal performance degradation attacks on the memory banks that host
data/instructions from victim applications. In such attacks, the adversary
manages to inject short and intense heat pulses from vertically and/or
laterally adjacent memory banks, creating a convergent thermal wave that
maximizes impact and delays the victim application from accessing its
data/instructions. As the attacking application does not access any
out-of-range memory locations, it can bypass both design-time security tests
and the operating system's memory management policies. In other words, since
the attack mimics legitimate workloads, it will be challenging to detect.

</details>


### [445] [COMET: A Framework for Modeling Compound Operation Dataflows with Explicit Collectives](https://arxiv.org/abs/2509.00599)
*Shubham Negi,Manik Singhal,Aayush Ankit,Sudeep Bhoja,Kaushik Roy*

Main category: cs.AR

TL;DR: 现有数据流程优化框架不适用于现代工作负载，提出COMET框架优化机器学习加速器复合操作数据流，实现性能和能效提升。


<details>
  <summary>Details</summary>
Motivation: 新兴DNN模型的复合操作给数据流优化和减少片外内存流量带来挑战，现有框架不适用于现代工作负载。

Method: 提出COMET框架，引入新表示法对空间集群间的集体通信进行建模，包含考虑GEMM和非GEMM操作依赖的延迟和能耗模型。

Result: COMET能分析和优化复合操作数据流，优化后的数据流在GEMM - Softmax、GEMM - LayerNorm和self - attention上相比未融合基线分别实现1.42倍、3.46倍和1.82倍加速。

Conclusion: COMET框架的集体感知建模扩大了映射空间探索范围，提高了性能和能效。

Abstract: Modern machine learning accelerators are designed to efficiently execute deep
neural networks (DNNs) by optimizing data movement, memory hierarchy, and
compute throughput. However, emerging DNN models such as large language models,
state space models increasingly rely on compound operations-structured
compositions of multiple basic operations-which introduce new challenges for
dataflow optimization and minimizing off-chip memory traffic. Moreover, as
model size continues to grow, deployment across spatially distributed compute
clusters becomes essential, requiring frequent and complex collective
communication. Existing dataflow optimization frameworks and performance models
either focus on single operations or lack explicit modeling of collective
communication cost, limiting their applicability to modern workloads.
  To address these limitations, we propose, a framework for modeling and
optimizing dataflow for compound operations on machine learning accelerators.
COMET introduces a novel representation that explicitly models collective
communication across spatial clusters, along with latency and energy cost
models that account for both GEMM and non-GEMM operation level dependencies
within compound operations. We demonstrate COMET's capabilities to analyze and
optimize dataflows for compound operations such as GEMM--Softmax,
GEMM--LayerNorm, and self-attention, across both edge and cloud accelerator
configurations. Our collective-aware modeling enables exploration of a broader
mapping space, leading to improved performance and energy efficiency.
Specifically, our optimized dataflows achieve up to 1.42$\times$ speedup for
GEMM-Softmax, 3.46$\times$ for GEMM-LayerNorm and 1.82$\times$ for
self-attention compared to unfused baselines.

</details>


### [446] [Low Power Approximate Multiplier Architecture for Deep Neural Networks](https://arxiv.org/abs/2509.00764)
*Pragun Jaswal,L. Hemanth Krishna,B. Srinivasu*

Main category: cs.AR

TL;DR: 提出用于DNN的低功耗近似乘法器架构，评估显示节能且精度较好，适合低功耗AI硬件。


<details>
  <summary>Details</summary>
Motivation: 为DNN应用设计低功耗近似乘法器架构。

Method: 设计引入单个组合误差的4:2压缩器并集成到8x8无符号乘法器，用于自定义卷积层并在神经网络任务中评估。

Result: 相比现有乘法器节能达30.24%，图像去噪PSNR和SSIM提升，手写数字识别保持高分类精度。

Conclusion: 该架构在能效和计算精度间取得良好平衡，适合低功耗AI硬件实现。

Abstract: This paper proposes an low power approximate multiplier architecture for deep
neural network (DNN) applications. A 4:2 compressor, introducing only a single
combination error, is designed and integrated into an 8x8 unsigned multiplier.
This integration significantly reduces the usage of exact compressors while
preserving low error rates. The proposed multiplier is employed within a custom
convolution layer and evaluated on neural network tasks, including image
recognition and denoising. Hardware evaluation demonstrates that the proposed
design achieves up to 30.24% energy savings compared to the best among existing
multipliers. In image denoising, the custom approximate convolution layer
achieves improved Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity
Index Measure (SSIM) compared to other approximate designs. Additionally, when
applied to handwritten digit recognition, the model maintains high
classification accuracy. These results demonstrate that the proposed
architecture offers a favorable balance between energy efficiency and
computational precision, making it suitable for low-power AI hardware
implementations.

</details>


### [447] [Guidance and Control Neural Network Acceleration using Memristors](https://arxiv.org/abs/2509.02369)
*Zacharia A. Rudge,Dario Izzo,Moritz Fieback,Anteneh Gebregiorgis,Said Hamdioui,Dominik Dold*

Main category: cs.AR

TL;DR: 本文探索使用PCM和RRAM忆阻器进行航天应用中板载内存计算AI加速，模拟评估其性能，指出虽有噪声影响精度挑战，但退化后再训练可恢复性能，为相关研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 小卫星和立方体卫星能源预算受限以及现代芯片受辐射问题困扰，需要研究能满足航天应用计算和性能需求的神经网络加速器。

Method: 模拟使用忆阻器加速的制导与控制神经网络（G&CNET）在多种场景和两种忆阻器类型下的表现，考虑器件非理想因素。

Result: 忆阻加速器能学习专家行动，但噪声影响精度存在挑战，退化后再训练可恢复性能到标称水平。

Conclusion: 为未来航天忆阻AI加速器研究奠定基础，凸显其潜力和进一步研究的必要性。

Abstract: In recent years, the space community has been exploring the possibilities of
Artificial Intelligence (AI), specifically Artificial Neural Networks (ANNs),
for a variety of on board applications. However, this development is limited by
the restricted energy budget of smallsats and cubesats as well as radiation
concerns plaguing modern chips. This necessitates research into neural network
accelerators capable of meeting these requirements whilst satisfying the
compute and performance needs of the application. This paper explores the use
of Phase-Change Memory (PCM) and Resistive Random-Access Memory (RRAM)
memristors for on-board in-memory computing AI acceleration in space
applications. A guidance and control neural network (G\&CNET) accelerated using
memristors is simulated in a variety of scenarios and with both device types to
evaluate the performance of memristor-based accelerators, considering device
non-idealities such as noise and conductance drift. We show that the memristive
accelerator is able to learn the expert actions, though challenges remain with
the impact of noise on accuracy. We also show that re-training after
degradation is able to restore performance to nominal levels. This study
provides a foundation for future research into memristor-based AI accelerators
for space, highlighting their potential and the need for further investigation.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [448] [Optimized Renewable Energy Planning MDP for Socially-Equitable Electricity Coverage in the US](https://arxiv.org/abs/2509.00008)
*Riya Kinnarkar,Mansur Arief*

Main category: eess.SY

TL;DR: 本文提出MDP框架优化可再生能源分配，兼顾社会公平，实验表明公平优化能提升可再生能源渗透率并减少服务不足的低收入人群，公平分配可兼顾系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统电网基础设施阻碍可再生能源整合，加剧能源获取不平等，低收入社区停电时间更长，需解决社会公平问题。

Method: 开发马尔可夫决策过程（MDP）框架，纳入预算约束、能源需求变化和社会脆弱性指标，对比MDP方法与随机分配、贪婪可再生能源扩张和专家启发式等基线政策。

Result: 公平导向优化可实现32.9%的可再生能源渗透率，使服务不足的低收入人口减少55%，专家政策奖励最高，蒙特卡罗树搜索基线性能有竞争力且预算利用率低。

Conclusion: 清洁能源资源的公平分配可在不牺牲整体系统性能的情况下实现，为将社会公平考虑与气候目标及清洁能源基础设施的包容性接入相结合提供了途径。

Abstract: Traditional power grid infrastructure presents significant barriers to
renewable energy integration and perpetuates energy access inequities, with
low-income communities experiencing disproportionately longer power outages.
This study develops a Markov Decision Process (MDP) framework to optimize
renewable energy allocation while explicitly addressing social equity concerns
in electricity distribution. The model incorporates budget constraints, energy
demand variability, and social vulnerability indicators across eight major U.S.
cities to evaluate policy alternatives for equitable clean energy transitions.
Numerical experiments compare the MDP-based approach against baseline policies
including random allocation, greedy renewable expansion, and expert heuristics.
Results demonstrate that equity-focused optimization can achieve 32.9%
renewable energy penetration while reducing underserved low-income populations
by 55% compared to conventional approaches. The expert policy achieved the
highest reward, while the Monte Carlo Tree Search baseline provided competitive
performance with significantly lower budget utilization, demonstrating that
fair distribution of clean energy resources is achievable without sacrificing
overall system performance and providing ways for integrating social equity
considerations with climate goals and inclusive access to clean power
infrastructure.

</details>


### [449] [Two-Stage Mechanism Design for Electric Vehicle Charging with Day-Ahead Reservations](https://arxiv.org/abs/2509.00270)
*Pan-Yang Su,Yi Ju,Scott Moura,Shankar Sastry*

Main category: eess.SY

TL;DR: 提出电动汽车两阶段充电市场模型，对比多种机制，提出用挂牌价格机制解决问题，结合日前面向市场的VCG拍卖提高效率。


<details>
  <summary>Details</summary>
Motivation: 探索适合电动汽车在日前市场预订充电时段、实时市场交换时段的市场运行机制。

Method: 构建两阶段模型，对比多种候选机制的规范性属性，提出挂牌价格机制和结合日前面向市场的VCG拍卖。

Result: 经典VCG机制变体不满足所有属性，挂牌价格机制满足除效率外的其他属性，结合VCG拍卖在高相关性下有高效结果。

Conclusion: 提出的挂牌价格机制结合日前面向市场的VCG拍卖，在电动汽车两市场估值高度相关时能实现高效结果。

Abstract: We propose a general two-period model where electrical vehicles (EVs) can
reserve charging sessions in the day-ahead market and swap them in the
real-time market. Under the model, we explore several candidate mechanisms for
running the two markets, compared using several normative properties such as
incentive compatibility, efficiency, reservation awareness, and budget balance.
Specifically, reservation awareness is the only property coupling the two
markets and dictates that an EV will not get a lower utility by joining the
real-time market. Focusing on the real-time market, we show that two variants
of the classical Vickrey-Clarke-Groves (VCG) mechanism do not satisfy all the
proposed properties; specifically, one is not reservation-aware, while the
other is not budget-balanced. Moreover, we show that no mechanism satisfies
some combinations of the properties. Then, we propose to use a posted-price
mechanism to resolve the issue, which turns out to be the dynamic pricing
mechanism adopted in many real-world systems. The proposed mechanism has no
efficiency guarantee but satisfies all the other properties. To improve
efficiency, we propose to use a VCG auction in the day-ahead market that guides
the reserve prices in the real-time market. When EVs' valuations in the two
markets are highly correlated, the proposed approach results in highly
efficient outcomes.

</details>


### [450] [Comparative Techno-economic Assessment of Wind-Powered Green Hydrogen Pathways](https://arxiv.org/abs/2509.00136)
*Merlinda Andoni,Benoit Couraud,Valentin Robu,Jamie Blanche,Sonam Norbu,Si Chen,Satria Putra Kanugrahan,David Flynn*

Main category: eess.SY

TL;DR: 开发技术经济框架比较英国绿氢部署策略，发现电成本对LCOH影响最大，政策可降低LCOH提升竞争力。


<details>
  <summary>Details</summary>
Motivation: 全球关注能源系统弹性，绿氢对净零转型重要，但高生产成本限制其部署，需比较不同部署策略。

Method: 基于LCOH评估开发综合技术经济框架，应用于5种风电 - 电解槽系统配置，确定最具成本效益案例并进行关键经济参数敏感性分析。

Result: 电成本是LCOH的主要贡献因素，其次是电解槽成本。

Conclusion: 位置、市场安排和控制策略对绿氢系统经济可行性至关重要，补贴低成本电力接入和优化部署的政策可降低LCOH，提升绿氢经济竞争力。

Abstract: Amid global interest in resilient energy systems, green hydrogen is
considered vital to the net-zero transition, yet its deployment remains limited
by high production cost. The cost is determined by the its production pathway,
system configuration, asset location, and interplay with electricity markets
and regulatory frameworks. To compare different deployment strategies in the
UK, we develop a comprehensive techno-economic framework based on the Levelised
Cost of Hydrogen (LCOH) assessment. We apply this framework to 5 configurations
of wind-electrolyser systems, identify the most cost-effective business cases,
and conduct a sensitivity analysis of key economic parameters. Our results
reveal that electricity cost is the dominant contributor to LCOH, followed by
the electrolyser cost. Our work highlights the crucial role that location,
market arrangements and control strategies among RES and hydrogen investors
play in the economic feasibility of deploying green hydrogen systems. Policies
that subsidise low-cost electricity access and optimise deployment can lower
LCOH, enhancing the economic competitiveness of green hydrogen.

</details>


### [451] [Semantic Technologies in Practical Demand Response: An Informational Requirement-based Roadmap](https://arxiv.org/abs/2509.01459)
*Ozan Baris Mulayim,Yuvraj Agarwal,Mario Bergés,Steve Schaefer,Mitali Shah,Derek Supple*

Main category: eess.SY

TL;DR: 本文指出当前商业建筑需求响应本体存在局限，应用形式化方法明确信息需求，评估现有本体，发现与实际需求的差异并提出扩展集成路线图，以提升电网互操作性。


<details>
  <summary>Details</summary>
Motivation: 未来电网复杂分散，实现语义互操作性需解决当前本体缺乏反映实际需求的形式框架、通用与特定本体集成缺乏形式化和实证验证的问题。

Method: 应用形式化本体评估/开发方法确定基于激励的商业建筑需求响应语义互操作性的信息需求，从业主角度识别各阶段信息需求，评估现有本体。

Result: 发现现有本体与实际需求存在显著差异。

Conclusion: 提出现有本体必要的扩展和集成路线图，以增强当前和未来智能电网的互操作性，促进需求响应系统集成。

Abstract: The future grid will be highly complex and decentralized, requiring
sophisticated coordination across numerous human and software agents that
manage distributed resources such as Demand Response (DR). Realizing this
vision demands significant advances in semantic interoperability, which enables
scalable and cost-effective automation across heterogeneous systems. While
semantic technologies have progressed in commercial building and DR domains,
current ontologies have two critical limitations: they are often developed
without a formal framework that reflects real-world DR requirements, and
proposals for integrating general and application-specific ontologies remain
mostly conceptual, lacking formalization or empirical validation.
  In this paper, we address these gaps by applying a formal ontology
evaluation/development approach to define the informational requirements (IRs)
necessary for semantic interoperability in the area of incentive-based DR for
commercial buildings. We identify the IRs associated with each stage of the
wholesale incentive-based DR process, focusing on the perspective of building
owners. Using these IRs, we evaluate how well existing ontologies (Brick,
DELTA, and EFOnt) support the operational needs of DR participation. Our
findings reveal substantial misalignments between current ontologies and
practical DR requirements. Based on our assessments, we propose a roadmap of
necessary extensions and integrations for these ontologies. This work
ultimately aims to enhance the interoperability of today's and future smart
grid, thereby facilitating scalable integration of DR systems into the grid's
complex operational framework.

</details>


### [452] [Solving Optimal Power Flow using a Variational Quantum Approach](https://arxiv.org/abs/2509.00341)
*Thinh Viet Le,Mark M. Wilde,Vassilis Kekatos*

Main category: eess.SY

TL;DR: 本文提出变分量子范式解决最优潮流（OPF）问题，在IEEE 57节点电力系统测试效果良好，且框架适用范围广。


<details>
  <summary>Details</summary>
Motivation: 现代电网复杂性给最优潮流问题的可扩展性和最优性带来挑战，需新方法解决。

Method: 通过参数化量子电路（PQC）编码原始和对偶变量，将拉格朗日函数表示为量子可观测量的缩放期望，以混合方式寻找鞍点，用原始 - 对偶方法更新PQC参数，还提出置换原始变量使OPF可观测量呈带状形式。

Result: 使用Pennylane模拟器在IEEE 57节点电力系统的数值测试表明，所提双变分量子框架能找到高质量OPF解。

Conclusion: 所提变分量子范式可有效解决OPF问题，且框架适用范围广，包括圆锥规划、稀疏图问题和训练量子机器学习模型等。

Abstract: The optimal power flow (OPF) is a large-scale optimization problem that is
central in the operation of electric power systems. Although it can be posed as
a nonconvex quadratically constrained quadratic program, the complexity of
modern-day power grids raises scalability and optimality challenges. In this
context, this work proposes a variational quantum paradigm for solving the OPF.
We encode primal variables through the state of a parameterized quantum circuit
(PQC), and dual variables through the probability mass function associated with
a second PQC. The Lagrangian function can thus be expressed as scaled
expectations of quantum observables. An OPF solution can be found by
minimizing/maximizing the Lagrangian over the parameters of the first/second
PQC. We pursue saddle points of the Lagrangian in a hybrid fashion. Gradients
of the Lagrangian are estimated using the two PQCs, while PQC parameters are
updated classically using a primal-dual method. We propose permuting primal
variables so that OPF observables are expressed in a banded form, allowing them
to be measured efficiently. Numerical tests on the IEEE 57-node power system
using Pennylane's simulator corroborate that the proposed doubly variational
quantum framework can find high-quality OPF solutions. Although showcased for
the OPF, this framework features a broader scope, including conic programs with
numerous variables and constraints, problems defined over sparse graphs, and
training quantum machine learning models to satisfy constraints.

</details>


### [453] [Game Theoretic Resilience Recommendation Framework for CyberPhysical Microgrids Using Hypergraph MetaLearning](https://arxiv.org/abs/2509.00528)
*S Krishna Niketh,Prasanta K Panigrahi,V Vignesh,Mayukha Pal*

Main category: eess.SY

TL;DR: 提出物理感知的弹性框架应对辐射状微电网协同网络攻击，经多系统验证有效且可扩展


<details>
  <summary>Details</summary>
Motivation: 应对辐射状微电网在协同网络攻击下的安全问题

Method: 用HGNN+MAML建模攻击者，用双层Stackelberg博弈建模防御者，用ADMM协调器嵌入NSGA - II优化

Result: 防御策略对90%攻击恢复近全服务，缓解电压违规，找出脆弱走廊

Conclusion: 框架有效，可通过预配置联络线增强弹性，且具有可扩展性

Abstract: This paper presents a physics-aware cyberphysical resilience framework for
radial microgrids under coordinated cyberattacks. The proposed approach models
the attacker through a hypergraph neural network (HGNN) enhanced with model
agnostic metalearning (MAML) to rapidly adapt to evolving defense strategies
and predict high-impact contingencies. The defender is modeled via a bi-level
Stackelberg game, where the upper level selects optimal tie-line switching and
distributed energy resource (DER) dispatch using an Alternating Direction
Method of Multipliers (ADMM) coordinator embedded within the Non-dominated
Sorting Genetic Algorithm II (NSGA-II). The framework simultaneously optimizes
load served, operational cost, and voltage stability, ensuring all post-defense
states satisfy network physics constraints. The methodology is first validated
on the IEEE 69-bus distribution test system with 12 DERs, 8 critical loads, and
5 tie-lines, and then extended to higher bus systems including the IEEE 123-bus
feeder and a synthetic 300-bus distribution system. Results show that the
proposed defense strategy restores nearly full service for 90% of top-ranked
attacks, mitigates voltage violations, and identifies Feeder 2 as the principal
vulnerability corridor. Actionable operating rules are derived, recommending
pre-arming of specific tie-lines to enhance resilience, while higher bus system
studies confirm scalability of the framework on the IEEE 123-bus and 300-bus
systems.

</details>


### [454] [Revisiting Deep AC-OPF](https://arxiv.org/abs/2509.00655)
*Oluwatomisin I. Dada,Neil D. Lawrence*

Main category: eess.SY

TL;DR: 本文系统评估ML模型对比线性基线，引入OPFormer - V模型，发现ML相对增益不如预期，简单线性基线性能相当。


<details>
  <summary>Details</summary>
Motivation: 重新审视此前关于机器学习方法作为求解交流最优潮流（AC - OPF）快速替代方案的说法。

Method: 系统评估ML模型与精心设计的简单线性基线，引入OPFormer - V模型并与DeepOPF - V模型和简单线性方法对比。

Result: OPFormer - V优于DeepOPF - V，但ML方法相对增益不如预期，简单线性基线可取得相当性能。

Conclusion: 未来评估中应包含强线性基线。

Abstract: Recent work has proposed machine learning (ML) approaches as fast surrogates
for solving AC optimal power flow (AC-OPF), with claims of significant
speed-ups and high accuracy. In this paper, we revisit these claims through a
systematic evaluation of ML models against a set of simple yet carefully
designed linear baselines. We introduce OPFormer-V, a transformer-based model
for predicting bus voltages, and compare it to both the state-of-the-art
DeepOPF-V model and simple linear methods. Our findings reveal that, while
OPFormer-V improves over DeepOPF-V, the relative gains of the ML approaches
considered are less pronounced than expected. Simple linear baselines can
achieve comparable performance. These results highlight the importance of
including strong linear baselines in future evaluations.

</details>


### [455] [End-to-End Low-Level Neural Control of an Industrial-Grade 6D Magnetic Levitation System](https://arxiv.org/abs/2509.01388)
*Philipp Hartmann,Jannick Stranghöner,Klaus Neumann*

Main category: eess.SY

TL;DR: 本文提出首个用于6D磁悬浮的神经控制器，展示了基于学习的神经控制在复杂物理系统中的可行性。


<details>
  <summary>Details</summary>
Motivation: 磁悬浮控制因系统复杂不稳定而具有挑战性，传统控制方法有局限性，需新方法。

Method: 基于专有控制器的交互数据进行端到端训练，将原始传感器数据和6D参考位姿直接映射到线圈电流命令。

Result: 神经控制器能有效泛化到未见情况，保持精确和鲁棒控制。

Conclusion: 基于学习的神经控制在复杂物理系统中可行，未来或可增强甚至替代传统工程方法。

Abstract: Magnetic levitation is poised to revolutionize industrial automation by
integrating flexible in-machine product transport and seamless manipulation. It
is expected to become the standard drive for automated manufacturing. However,
controlling such systems is inherently challenging due to their complex,
unstable dynamics. Traditional control approaches, which rely on hand-crafted
control engineering, typically yield robust but conservative solutions, with
their performance closely tied to the expertise of the engineering team. In
contrast, neural control learning presents a promising alternative. This paper
presents the first neural controller for 6D magnetic levitation. Trained
end-to-end on interaction data from a proprietary controller, it directly maps
raw sensor data and 6D reference poses to coil current commands. The neural
controller can effectively generalize to previously unseen situations while
maintaining accurate and robust control. These results underscore the practical
feasibility of learning-based neural control in complex physical systems and
suggest a future where such a paradigm could enhance or even substitute
traditional engineering approaches in demanding real-world applications. The
trained neural controller, source code, and demonstration videos are publicly
available at https://sites.google.com/view/neural-maglev.

</details>


### [456] [RadioDiff-Loc: Diffusion Model Enhanced Scattering Congnition for NLoS Localization with Sparse Radio Map Estimation](https://arxiv.org/abs/2509.01875)
*Xiucheng Wang,Qiming Zhang,Nan Cheng*

Main category: eess.SY

TL;DR: 提出基于条件扩散模型的非视距定位生成推理框架，实现高精度低成本定位。


<details>
  <summary>Details</summary>
Motivation: 传统定位技术在非视距环境因多径传播和发射功率未知而失效，需新的定位方法。

Method: 利用衍射能量集中在建筑边缘的物理特性，在障碍物几何顶点采集稀疏RSS测量值，归一化构建功率不变的无线电地图，训练条件扩散模型重建地图，通过识别地图亮点定位。

Result: 实现高精度定位，显著降低采样成本。

Conclusion: 该方法为非合作非视距发射器定位提供了可扩展且基于物理的解决方案。

Abstract: Accurate localization of non-cooperative signal sources in non-line-of-sight
(NLoS) environments remains a critical challenge with a wide range of
applications, including autonomous navigation, industrial automation, and
emergency response. In such settings, traditional positioning techniques
relying on line-of-sight (LoS) or cooperative signaling fail due to severe
multipath propagation and unknown transmit power. This paper proposes a novel
generative inference framework for NLoS localization based on conditional
diffusion models. By leveraging the physical insight that diffracted
electromagnetic energy concentrates near building edges, we develop a sampling
strategy that collects sparse received signal strength (RSS) measurements at
the geometric vertices of obstacles--locations that maximize Fisher information
and mutual information with respect to the unknown source. To overcome the lack
of known transmission power, we normalize all sampled RSS values relative to
the maximum observed intensity, enabling the construction of a power-invariant
radio map (RM). A conditional diffusion model is trained to reconstruct the
full RM based on environmental layout and sparse RSS observations. Localization
is then achieved by identifying the brightest point on the generated RM.
Moreover, the proposed framework is compatible with existing RSS-based
localization algorithms, enabling a dual-driven paradigm that fuses physical
knowledge and data-driven inference for improved accuracy. Extensive
theoretical analysis and empirical validation demonstrate that our approach
achieves high localization accuracy with significantly reduced sampling cost,
offering a scalable and physically grounded solution for non-cooperative NLoS
emitter localization.

</details>


### [457] [Selection of Optimal Number and Location of PMUs for CNN Based Fault Location and Identification](https://arxiv.org/abs/2509.02192)
*Khalid Daud Khattak,Muhammad A. Choudhry*

Main category: eess.SY

TL;DR: 提出FSNR算法确定PMU数量和位置以提升深度学习故障诊断性能，在IEEE系统验证效果佳。


<details>
  <summary>Details</summary>
Motivation: 确定PMU数量和位置以最大化深度学习故障诊断性能。

Method: 提出FSNR算法，用交叉验证SVM对候选PMU位置排序，通过局部邻域探索优化选择，将PMU子集用于1D CNN进行故障定位和分类。

Result: 在IEEE 34系统故障定位超96%、故障类型分类超99%；在IEEE 123系统故障定位约94%、故障类型分类约99.8%。

Conclusion: FSNR - SVM方法能确定最小PMU配置，实现最佳CNN性能。

Abstract: In this paper, we present a data-driven Forward Selection with Neighborhood
Refinement (FSNR) algorithm to determine the number and placement of Phasor
Measurement Units (PMUs) for maximizing deep-learning-based fault diagnosis
performance. Candidate PMU locations are ranked via a cross-validated Support
Vector Machine (SVM) classifier, and each selection is refined through local
neighborhood exploration to produce a near-optimal sensor set. The resulting
PMU subset is then supplied to a 1D Convolutional Neural Network (CNN) for
faulted-line localization and fault-type classification from time-series
measurements. Evaluation on modified IEEE 34- and IEEE 123-bus systems
demonstrates that the proposed FSNR-SVM method identifies a minimal PMU
configuration that achieves the best overall CNN performance, attaining over 96
percent accuracy in fault location and over 99 percent accuracy in fault-type
classification on the IEEE 34 system, and approximately 94 percent accuracy in
fault location and around 99.8 percent accuracy in fault-type classification on
the IEEE 123 system.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [458] [Derivation and Verification of Array Sorting by Merging, and its Certification in Dafny](https://arxiv.org/abs/2509.01758)
*Juan Pablo Carbonell,José E. Solsona,Nora Szasz,Álvaro Tasistro*

Main category: cs.LO

TL;DR: 本文用Dafny语言对两种归并排序进行形式化验证，给出推导过程，方法也适用于快速排序。


<details>
  <summary>Details</summary>
Motivation: 在验证感知编程语言Dafny中对数组的两种归并排序进行完整验证。

Method: 考虑用分治法或分区法解决线性数组前后条件规范，推导归并排序和合并算法，分析分区子问题树设计循环不变式得到迭代版本，利用模式进行形式化和验证。

Result: 实现了两种归并排序的完整验证，展示了模式在Dafny中方便形式化和验证的作用。

Conclusion: 该方法适用于推导快速排序的变体。

Abstract: We provide full certifications of two versions of merge sort of arrays in the
verification-aware programming language Dafny. We start by considering schemas
for applying the divide-and-conquer or partition method of solution to
specifications given by pre- and post-conditions involving linear arrays. We
then derive the merge sort and merging algorithms as instances of these
schemas, thereby arriving at a fully recursive formulation. Further, the
analysis of the tree of subproblems arising from the partition facilitates the
design of loop invariants that allow to derive a fully iterative version
(sometimes called bottom-up merge sort) that does not employ a stack. We show
how the use of the provided schemas conveniently conducts the formalization and
actual verification in Dafny. The whole method is also applicable to deriving
variants of quicksort, which we sketch.

</details>


### [459] [An Information-Flow Perspective on Explainability Requirements: Specification and Verification](https://arxiv.org/abs/2509.01479)
*Bernd Finkbeiner,Hadar Frenkel,Julian Siber*

Main category: cs.LO

TL;DR: 本文用扩展的认知时态逻辑处理可解释性与隐私问题，给出可解释性规范及检查算法，通过原型实现和基准测试展示其效果。


<details>
  <summary>Details</summary>
Motivation: 可解释系统存在正信息流动，需与可能侵犯隐私的负信息流动进行平衡和验证，且可解释性和隐私都涉及知识推理。

Method: 使用扩展了反事实原因量化的认知时态逻辑，指定可解释性为系统级要求并提供检查有限状态模型的算法。

Result: 实现算法原型并在多个基准测试中进行评估，能区分可解释和不可解释系统，还可提出额外隐私要求。

Conclusion: 所提方法能有效处理可解释系统中的信息流动平衡和隐私问题。

Abstract: Explainable systems expose information about why certain observed effects are
happening to the agents interacting with them. We argue that this constitutes a
positive flow of information that needs to be specified, verified, and balanced
against negative information flow that may, e.g., violate privacy guarantees.
Since both explainability and privacy require reasoning about knowledge, we
tackle these tasks with epistemic temporal logic extended with quantification
over counterfactual causes. This allows us to specify that a multi-agent system
exposes enough information such that agents acquire knowledge on why some
effect occurred. We show how this principle can be used to specify
explainability as a system-level requirement and provide an algorithm for
checking finite-state models against such specifications. We present a
prototype implementation of the algorithm and evaluate it on several
benchmarks, illustrating how our approach distinguishes between explainable and
unexplainable systems, and how it allows to pose additional privacy
requirements.

</details>


### [460] [Probabilistically stable revision and comparative probability: a representation theorem and applications](https://arxiv.org/abs/2509.02495)
*Krzysztof Mierzewski*

Main category: cs.LO

TL;DR: 本文证明表征定理，刻画概率稳定信念修正算子，给出定性选择函数语义，证明比较概率理论的两个结果，并指出主要结果在简单投票博弈和显示偏好理论中的应用。


<details>
  <summary>Details</summary>
Motivation: 对Leitgeb提出的信念稳定性规则所产生的概率稳定信念修正算子进行完整刻画，并为其逻辑提供语义。

Method: 证明表征定理，借鉴比较概率序理论，证明比较概率理论的两个结果。

Result: 得到概率稳定信念修正算子的完整刻画和定性选择函数语义，证明比较概率理论的两个结果。

Conclusion: 概率稳定信念修正逻辑有强单调性，但不满足AGM信念修正假设，仅满足非常弱的案例推理形式，主要结果有测量理论及其他领域应用。

Abstract: The stability rule for belief, advocated by Leitgeb [Annals of Pure and
Applied Logic 164, 2013], is a rule for rational acceptance that captures
categorical belief in terms of $\textit{probabilistically stable
propositions}$: propositions to which the agent assigns resiliently high
credence. The stability rule generates a class of $\textit{probabilistically
stable belief revision}$ operators, which capture the dynamics of belief that
result from an agent updating their credences through Bayesian conditioning
while complying with the stability rule for their all-or-nothing beliefs. In
this paper, we prove a representation theorem that yields a complete
characterisation of such probabilistically stable revision operators and
provides a `qualitative' selection function semantics for the (non-monotonic)
logic of probabilistically stable belief revision. Drawing on the theory of
comparative probability orders, this result gives necessary and sufficient
conditions for a selection function to be representable as a
strongest-stable-set operator on a finite probability space. The resulting
logic of probabilistically stable belief revision exhibits strong monotonicity
properties while failing the AGM belief revision postulates and satisfying only
very weak forms of case reasoning. In showing the main theorem, we prove two
results of independent interest to the theory of comparative probability: the
first provides necessary and sufficient conditions for the joint representation
of a pair of (respectively, strict and non-strict) comparative probability
orders. The second result provides a method for axiomatising the logic of ratio
comparisons of the form ``event $A$ is at least $k$ times more likely than
event $B$''. In addition to these measurement-theoretic applications, we point
out two applications of our main result to the theory of simple voting games
and to revealed preference theory.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [461] [Self-Organising Memristive Networks as Physical Learning Systems](https://arxiv.org/abs/2509.00747)
*Francesco Caravelli,Gianluca Milano,Adam Z. Stieg,Carlo Ricciardi,Simon Anthony Brown,Zdenka Kuncic*

Main category: cond-mat.dis-nn

TL;DR: 本文介绍利用物理系统学习的新兴范式，聚焦自组织忆阻网络（SOMNs），展示其在学习中的潜力及应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统基于晶体管硬件实现的人工神经网络软件不可持续，需范式转变。

Method: 采用由电阻式存储纳米组件构成的物理网络，结合实验和理论方法（如平均场理论、图论等）研究SOMNs。

Result: 揭示SOMNs的集体非线性和自适应动力学，发现其与生物神经元网络可塑性的相似性。

Conclusion: 纳米技术、统计物理等的融合为推进新一代物理智能技术提供独特机遇，SOMNs有望实现嵌入式边缘智能。

Abstract: Learning with physical systems is an emerging paradigm that seeks to harness
the intrinsic nonlinear dynamics of physical substrates for learning. The
impetus for a paradigm shift in how hardware is used for computational
intelligence stems largely from the unsustainability of artificial neural
network software implemented on conventional transistor-based hardware. This
Perspective highlights one promising approach using physical networks comprised
of resistive memory nanoscale components with dynamically reconfigurable,
self-organising electrical circuitry. Experimental advances have revealed the
non-trivial interactions within these Self-Organising Memristive Networks
(SOMNs), offering insights into their collective nonlinear and adaptive
dynamics, and how these properties can be harnessed for learning using
different hardware implementations. Theoretical approaches, including
mean-field theory, graph theory, and concepts from disordered systems, reveal
deeper insights into the dynamics of SOMNs, especially during transitions
between different conductance states where criticality and other dynamical
phase transitions emerge in both experiments and models. Furthermore, parallels
between adaptive dynamics in SOMNs and plasticity in biological neuronal
networks suggest the potential for realising energy-efficient, brain-like
continual learning. SOMNs thus offer a promising route toward embedded edge
intelligence, unlocking real-time decision-making for autonomous systems,
dynamic sensing, and personalised healthcare, by enabling embedded learning in
resource-constrained environments. The overarching aim of this Perspective is
to show how the convergence of nanotechnology, statistical physics, complex
systems, and self-organising principles offers a unique opportunity to advance
a new generation of physical intelligence technologies.

</details>


### [462] [Phase diagram and eigenvalue dynamics of stochastic gradient descent in multilayer neural networks](https://arxiv.org/abs/2509.01349)
*Chanju Park,Biagio Lucini,Gert Aarts*

Main category: cond-mat.dis-nn

TL;DR: 研究神经网络相图为随机梯度下降超参数选择提供直觉和实用指导。


<details>
  <summary>Details</summary>
Motivation: 超参数调优是保证机器学习模型收敛的关键步骤，需为随机梯度下降的超参数选择提供依据。

Method: 将多层神经网络损失景观解释为特征空间中的无序系统，利用朗之万方程对随机梯度下降进行分析。

Result: 识别出训练过程中的三个阶段，能有效对三种动态机制进行分类。

Conclusion: 研究成果为优化器超参数的选择提供了实用指导。

Abstract: Hyperparameter tuning is one of the essential steps to guarantee the
convergence of machine learning models. We argue that intuition about the
optimal choice of hyperparameters for stochastic gradient descent can be
obtained by studying a neural network's phase diagram, in which each phase is
characterised by distinctive dynamics of the singular values of weight
matrices. Taking inspiration from disordered systems, we start from the
observation that the loss landscape of a multilayer neural network with mean
squared error can be interpreted as a disordered system in feature space, where
the learnt features are mapped to soft spin degrees of freedom, the initial
variance of the weight matrices is interpreted as the strength of the disorder,
and temperature is given by the ratio of the learning rate and the batch size.
As the model is trained, three phases can be identified, in which the dynamics
of weight matrices is qualitatively different. Employing a Langevin equation
for stochastic gradient descent, previously derived using Dyson Brownian
motion, we demonstrate that the three dynamical regimes can be classified
effectively, providing practical guidance for the choice of hyperparameters of
the optimiser.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [463] [Economic Impacts of Climate Change in the United States: Integrating and Harmonizing Evidence from Recent Studies](https://arxiv.org/abs/2509.00212)
*Elizabeth Kopits,Daniel Kraynak,Bryan Parthum,Lisa Rennels,David Smith,Elizabeth Spink,Joseph Perla,Nshan Burns*

Main category: econ.GN

TL;DR: 本文综合美国人口气候变化影响证据，对比计量研究，整合市场与非市场损害，强调需更多研究气候影响及整合不同方法结果。


<details>
  <summary>Details</summary>
Motivation: 综合美国特定人口气候变化影响的证据，以更好评估气候变化对美国GDP的影响及温室气体社会成本。

Method: 对计量研究进行同类比较，整合非市场损害与GDP影响。

Result: 使用统一输入时2100年GDP损失范围更窄更低，但美国特定的温室气体社会成本仍高于当前模型的市场损害估计；整合后得到联合估计的社会成本。

Conclusion: 需要对市场和非市场气候影响进行更多研究，进一步探索宏观经济和枚举方法结果的整合以提升对气候政策分析的有用性。

Abstract: This paper synthesizes evidence on climate change impacts specific to U.S.
populations. We develop an apples-to-apples comparison of econometric studies
that empirically estimate the relationship between climate change and gross
domestic product (GDP). We demonstrate that with harmonized probabilistic
socioeconomic and climate inputs these papers project a narrower and lower
range of 2100 GDP losses than what is reported across the published studies,
yet the implied U.S.-specific social cost of greenhouse gases (SC-GHG) is still
greater than the market-based damage estimates in current enumerative models.
We then integrate evidence on nonmarket damages with the GDP impacts and
recover a jointly-estimated SC-GHG. Our findings highlight the need for more
research on both market and nonmarket climate impacts, including interaction
and international spillover impacts. Further investigation of how results of
macroeconomic and enumerative approaches can be integrated would enhance the
usefulness of both strands of literature to climate policy analysis going
forward.

</details>


### [464] [Exploring Trade Openness and Logistics Efficiency in the G20 Economies: A Bootstrap ARDL Analysis of Growth Dynamics](https://arxiv.org/abs/2509.00368)
*Haibo Wang,Lutfu Sua*

Main category: econ.GN

TL;DR: 研究G20经济体贸易开放度、物流绩效与经济增长关系，用ARDL - ECM模型分析2007 - 2023年数据，结果强调高效物流对贸易和经济增长的关键作用。


<details>
  <summary>Details</summary>
Motivation: 为寻求通过基础设施投资和贸易便利化监管改革提升贸易竞争力和经济发展的政策制定者提供信息。

Method: 使用Bootstrap自回归分布滞后（ARDL）模型，并结合动态误差修正机制（ECM），以世界银行物流绩效指数衡量贸易便利化和物流基础设施。

Result: 高效物流（海关管理、物理基础设施和运输可靠性）对推动国际贸易和促进持续经济增长至关重要，改善这些方面可提升贸易能力和经济表现。

Conclusion: 高效物流在驱动国际贸易和经济增长中起关键作用，针对性投资和改革可提升国家贸易和经济表现。

Abstract: This study examines the relationship between trade openness, logistics
performance, and economic growth within G20 economies. Using a Bootstrap
Autoregressive Distributed Lag (ARDL) model augmented by a dynamic error
correction mechanism (ECM), the analysis quantifies both short run and long run
effects of trade facilitation and logistics infrastructure, measured via the
World Bank's Logistics Performance Index (LPI) from 2007 to 2023, on economic
growth. The G20, as a consortium of the world's leading economies, exhibits
significant variation in logistics efficiency and degrees of trade openness,
providing a robust context for comparative analysis. The ARDL-ECM approach,
reinforced by bootstrap resampling, delivers reliable estimates even in the
presence of small samples and complex variable linkages. Findings are intended
to inform policymakers seeking to enhance trade competitiveness and economic
development through targeted investment in infrastructure and regulatory
reforms supporting trade facilitation. The results underscore the critical role
of efficient logistics specifically customs administration, physical
infrastructure, and shipment reliability in driving international trade and
fostering sustained economic growth. Improvements in these areas can
substantially increase a country's trade capacity and overall economic
performance.

</details>


### [465] [Worker Quality, Matching and Productivity Slowdown](https://arxiv.org/abs/2509.00516)
*Shujiang Cao,Shutao Cao*

Main category: econ.GN

TL;DR: 加拿大数据中，总体生产率与高收入者收入份额正相关，2000年代初以来生产率放缓伴随高收入者收入份额趋平，研究发现2003 - 2015年总体全要素生产率下降由顶级员工质量下降导致。


<details>
  <summary>Details</summary>
Motivation: 基于加拿大数据中总体生产率与高收入者收入份额的关系及2000年代初以来的变化，研究顶级员工和员工匹配对全要素生产率放缓的作用。

Method: 考虑顶级员工与非顶级员工的匹配，估算2003 - 2015年加拿大企业的全要素生产率，将其分为希克斯中性技术和顶级员工质量两部分。

Result: 2003 - 2015年总体全要素生产率下降，与统计机构估算一致，生产率下降完全由顶级员工质量下降导致，希克斯中性技术有所提升，企业内和企业间顶级员工质量变化都对总体顶级员工质量下降有重要影响。

Conclusion: 顶级员工质量下降是全要素生产率下降的原因，并讨论了顶级员工质量下降的可能原因，如顶级人才外流。

Abstract: Measured aggregate productivity and the income share of top earners are
strongly and positively correlated in the Canadian data. Productivity slowdown
since the early 2000s was accompanied with a flattening income share of top
earners. Motivated by these facts, we study the role of firms' top-paid workers
and worker matching in accounting for the slowdown of measured total factor
productivity. We first estimate total factor productivity for Canadian firms in
the period of 2003-2015, taking into account the assortative matching between
top workers and non-top workers. Measured total factor productivity consists of
the Hicks-neutral technology and the quality of top workers. Our estimation
suggests that measured aggregate total factor productivity declined from 2003
to 2015, in line with that estimated by the statistical agency. The decline of
measured productivity is entirely accounted for by the declining quality of top
workers, while the Hicks-neutral technology improved. Both the within-firm
changes and the cross-firm reallocation of top-worker quality are important in
contributing to the decline of overall top-worker quality. We also discuss
possible causes of declines in the quality of top workers, e.g., the emigration
of top talents as studied in recent literature.

</details>


### [466] [Information-Nonintensive Models of Rumour Impacts on Complex Investment Decisions](https://arxiv.org/abs/2509.00588)
*Nina Bočková,Karel Doubravský,Barbora Volná,Mirko Dohnal*

Main category: econ.GN

TL;DR: 本文提出定性框架分析谣言对复杂投资决策的影响，基于趋势模型，应用于三个相互关联的模型。


<details>
  <summary>Details</summary>
Motivation: 分析在严重信息约束下，谣言对复杂投资决策的影响。

Method: 提出基于趋势的模型，依靠最少的数据输入，通过趋势规则生成场景，用有向图表示系统行为，应用于三个相互关联的模型。

Result: 未提及

Conclusion: 未提及

Abstract: This paper develops a qualitative framework for analysing the impact of
rumours on complex investment decisions (CID) under severe information
constraints. The proposed trend-based models rely on minimal data inputs in the
form of increasing, decreasing, or constant relations. Sets of trend rules
generate scenarios, and permitted transitions between them form a directed
graph that represents system behaviour over time. The approach is applied in
three interconnected models: financial CID, rumour-spreading dynamics, and
their integration.

</details>


### [467] [A Framework for a Comprehensive National Future Readiness Index](https://arxiv.org/abs/2509.00755)
*Ali Qassim Jawad,Xavier Sala-i-Martin*

Main category: econ.GN

TL;DR: 本文介绍未来就绪指数（IFR），它是评估国家应对变化能力的新框架，能为政策制定者提供见解，指导国家发展。


<details>
  <summary>Details</summary>
Motivation: 在持续加速变化的环境下，需要一个评估国家应对能力的框架，以指导国家发展。

Method: 基于经典的临时冲击和永久冲击的区分，运用全社会视角，借鉴GBC模型，评估三个核心参与者在六个基础维度的准备情况。

Result: IFR明确整合了恢复力和适应能力，可进行部门和专题分析。

Conclusion: IFR是一个动态、与政策相关的工具，能为政策制定者提供可操作的见解，指导国家在系统性波动时代的发展。

Abstract: This paper introduces the Index of Future Readiness (IFR), a novel framework
for assessing a country's capacity to withstand, adapt to, and prosper within
an environment of continuous and accelerating change. The framework builds on
the classical distinction, first emphasized in economic discourse by Robert E.
Lucas Jr. in the 1970s, between temporary shocks, which call for economic
resilience, and permanent shocks, which require adaptive capacity. The IFR
applies a whole-of-society perspective, drawing on Ali Qassim Jawad s GBC
model, to evaluate the preparedness of three central actors (government,
businesses, and citizens) across six foundational dimensions. Unlike
conventional benchmarking instruments, the IFR explicitly integrates both
resilience, understood as the ability to bounce back from temporary shocks, and
adaptive capacity, conceived as the ability to bounce forward in the face of
structural or permanent disturbances. By enabling both sectoral and thematic
analysis, the IFR provides policymakers with actionable insights and thus
serves as a dynamic, policy-relevant tool for guiding national development in
an era of systemic volatility.

</details>


### [468] [Contest vs. Competition in Cournot Duopoly: Schaffer's Paradox](https://arxiv.org/abs/2509.00960)
*Rabah Amir,Igor V. Evstigneev,Mikhail V. Zhitlukhin*

Main category: econ.GN

TL;DR: 本文比较古诺双寡头市场中两种产业组织形式，探讨两种形式转换的悖论现象，并在两类古诺模型中研究。


<details>
  <summary>Details</summary>
Motivation: 研究古诺双寡头市场中不同产业组织形式转换带来的悖论现象，如从追求利润最大化到竞争固定奖金导致零利润，反之产量大幅下降与竞争提高效率常识相悖。

Method: 比较古诺双寡头市场中两种产业组织形式（追求利润最大化和竞争固定奖金），并在同质产品和差异化产品两种古诺模型中进行研究。

Result: 从追求利润最大化到竞争固定奖金会导致完全竞争和零利润；从竞争固定奖金到追求利润最大化会使产量大幅下降。

Conclusion: 在不同版本的古诺模型中存在这些特殊的产业组织转换现象。

Abstract: The paper compares two types of industrial organization in the Cournot
duopoly: (a) the classical one, where the market players maximize profits and
the outcome of the game is a Cournot-Nash equilibrium; (b) a contest in which
players strive to win a fixed prize/bonus employing unbeatable strategies.
Passing from (a) to (b) leads to a perfect competition with zero profits of the
players (Schaffer's paradox). Transition from (b) to (a) results in a
substantial decline in the production output, which also seems paradoxical, as
it is commonly accepted that competition increases efficiency. We examine these
phenomena in two versions of the Cournot model: with a homogeneous good and
with differentiated goods.

</details>


### [469] [An Economy of AI Agents](https://arxiv.org/abs/2509.01063)
*Gillian K. Hadfield,Andrew Koh*

Main category: econ.GN

TL;DR: 文章展望未来十年AI智能体部署情况，调查相关进展并提出经济学家需关注的问题。


<details>
  <summary>Details</summary>
Motivation: 探讨未来十年广泛部署的AI智能体对经济的影响及相关经济学问题。

Method: 对近期发展进行调查。

Result: 无明确的具体结果。

Conclusion: 指出需要关注AI智能体与人、彼此的交互，对市场和组织的塑造，以及良好市场所需的制度等问题。

Abstract: In the coming decade, artificially intelligent agents with the ability to
plan and execute complex tasks over long time horizons with little direct
oversight from humans may be deployed across the economy. This chapter surveys
recent developments and highlights open questions for economists around how AI
agents might interact with humans and with each other, shape markets and
organizations, and what institutions might be required for well-functioning
markets.

</details>


### [470] [NoLBERT: A No Lookahead(back) Foundational Language Model for Empirical Research](https://arxiv.org/abs/2509.01110)
*Ali Kakhbod,Peiyao Li*

Main category: econ.GN

TL;DR: 提出适用于社科研究的轻量级带时间戳语言模型NoLBERT，避免偏差，在NLP基准表现好，用于专利文本构建创新网络并发现创新中心性与利润增长关系。


<details>
  <summary>Details</summary>
Motivation: 为社科尤其是经济和金融领域的实证研究提供合适的语言模型，避免影响计量推断的回溯和前瞻偏差。

Method: 仅在1976 - 1995年的文本上进行预训练。

Result: 在NLP基准上超越特定领域基线，保持时间一致性；应用于专利文本可构建企业级创新网络，创新中心性提升预示长期利润增长。

Conclusion: NoLBERT是适合社科实证研究的有效语言模型，在专利文本应用中有实际价值。

Abstract: We present NoLBERT, a lightweight, timestamped foundational language model
for empirical research in social sciences, particularly in economics and
finance. By pre-training exclusively on 1976-1995 text, NoLBERT avoids both
lookback and lookahead biases that can undermine econometric inference. It
exceeds domain-specific baselines on NLP benchmarks while maintaining temporal
consistency. Applied to patent texts, NoLBERT enables the construction of
firm-level innovation networks and shows that gains in innovation centrality
predict higher long-run profit growth.

</details>


### [471] [Gender Differences in Healthcare Utilisation - Evidence from Unexpected Adverse Health Shocks](https://arxiv.org/abs/2509.01310)
*Nadja van 't Hoff,Giovanni Mellace,Seetha Menon*

Main category: econ.GN

TL;DR: 本文利用丹麦医疗数据，用双重差分法研究健康冲击对医疗利用的因果影响，发现男性比女性更多使用医疗服务，为解决医疗不平等提供见解。


<details>
  <summary>Details</summary>
Motivation: 为更好理解男女健康 - 生存悖论，即女性寿命长但健康结果差，提供医疗利用性别差异的因果证据。

Method: 利用丰富的丹麦行政医疗数据，采用交错双重差分法，利用治疗时间的随机性估计不良健康冲击对医疗利用的因果影响。

Result: 男性比女性持续更多地使用医疗服务。

Conclusion: 研究结果有助于医疗公平的广泛讨论，为解决医疗不平等的政策干预提供依据。

Abstract: This paper is the first to provide causal evidence of gender differences in
healthcare utilisation to better understand the male-female health-survival
paradox, where women live longer but experience worse health outcomes. Using
rich Danish administrative healthcare data, we apply a staggered
difference-in-differences approach that exploits the randomness in treatment
timing to estimate the causal impact of adverse health shocks, such as
non-fatal heart attacks or strokes, on healthcare use. Our findings suggest
that men consistently use more healthcare than women, highlighting the
underlying factors driving gender disparities in health outcomes. These
insights contribute to the broader discourse on healthcare equity and inform
policy interventions aimed at addressing these imbalances.

</details>


### [472] [When Do Consumers Lose from Variable Electricity Pricing?](https://arxiv.org/abs/2509.01499)
*Nathan Engelman Lado,Richard Chen,Saurabh Amin*

Main category: econ.GN

TL;DR: 本文构建理论框架分析电价从固定费率向分时定价转变时消费者异质性对福利结果的影响，发现高峰期消费且需求缺乏弹性的消费者易受损，可变定价政策需配套针对性政策保障公平。


<details>
  <summary>Details</summary>
Motivation: 可变电价虽能降低供电成本和电网压力，但对低收入消费者的分配影响研究不足，需分析消费者异质性对福利结果的影响。

Method: 构建理论框架，推导识别消费者因定价改革损失效用的充分条件，比较不同消费者类型的福利效应，进行均衡分析。

Result: 消费者脆弱性取决于消费时间、需求弹性和价格敏感度的相互作用；高峰期消费且需求缺乏弹性的消费者易受损；需求弹性仅在价格大幅变动时提供福利保护；总体弹性模式通过定价机制产生溢出效应。

Conclusion: 可变定价政策应配套针对性政策，确保消费者公平获得需求响应能力和定价福利。

Abstract: Time-varying electricity pricing better reflects the varying cost of
electricity compared to flat-rate pricing. Variations between peak and off-peak
costs are increasing due to weather variation, renewable intermittency, and
increasing electrification of demand. Empirical and theoretical studies suggest
that variable pricing can lower electricity supply costs and reduce grid
stress. However, the distributional impacts, particularly on low-income
consumers, remain understudied. This paper develops a theoretical framework to
analyze how consume heterogeneity affects welfare outcomes when electricity
markets transition from flat-rate to time-varying pricing, considering
realistic assumptions about heterogeneous consumer demand, supply costs, and
utility losses from unmet consumption.
  We derive sufficient conditions for identifying when consumers lose utility
from pricing reforms and compare welfare effects across consumer types. Our
findings reveal that consumer vulnerability depends on the interaction of
consumption timing, demand flexibility capabilities, and price sensitivity
levels. Consumers with high peak-period consumption and inflexible demand,
characteristics often associated with low-income households, are most
vulnerable to welfare losses. Critically, we demonstrate that demand
flexibility provides welfare protection only when coincident with large price
changes. Our equilibrium analysis reveals that aggregate flexibility patterns
generate spillover effects through pricing mechanisms, with peak periods
experiencing greater price changes when they have less aggregate flexibility,
potentially concentrating larger price increases among vulnerable populations
that have a limited ability to respond. These findings suggest that variable
pricing policies should be accompanied by targeted policies ensuring equitable
access to demand response capabilities and pricing benefits.

</details>


### [473] [Location Matters: Insights from a Natural Field Experiment to Enhance Small Business Tax Compliance in Indonesia](https://arxiv.org/abs/2509.02328)
*Sarah Xue Dong,Agung Satyadini,Mathias Sinning*

Main category: econ.GN

TL;DR: 以印尼12000家中小企业为对象实验研究地区环境对税收执法策略效果的影响，发现威慑信效果最佳，不同地区需定制税收管理策略。


<details>
  <summary>Details</summary>
Motivation: 发展中国家小企业税收合规率低，且地区环境对执法策略效果的影响不明。

Method: 与印尼税务部门合作开展大规模自然实地实验，对企业随机分组发送不同信件或不发信。

Result: 所有信件都提升了合规率，威慑信效果最佳，每花1美元可增收约30美元；大城市税务机关附近未处理企业合规率高，非大城市地区执法信息能提升合规率，针对大城市附近已合规企业执法适得其反。

Conclusion: 需制定地理定制化税收管理策略，为地理距离与税收执法效果关系提供新实验证据。

Abstract: Tax compliance among small businesses remains low in developing countries,
yet little is known about how regional context shapes the effectiveness of
enforcement strategies. Both theory and evidence suggest an ambiguous
relationship between compliance and geographic proximity to tax offices. We
study this issue using a large-scale natural field experiment with Indonesia's
tax authority involving 12,000 micro, small, and medium enterprises (MSMEs).
Businesses were randomly assigned to receive deterrence, information, or public
goods letters, or no message. All letters improved compliance, with deterrence
messages producing the largest gains - substantially increasing filing rates
and raising monthly tax payments. Each dollar spent on deterrence letters
generated about US$30 in additional revenue over the course of a year. We
observe high compliance among non-treated MSMEs near metropolitan tax offices
and find that enforcement messages successfully raise compliance in
non-metropolitan regions to comparable levels. However, targeting already
compliant MSMEs near metropolitan tax offices backfires, underscoring the need
for geographically tailored tax administration strategies. These results
provide novel experimental evidence on the relation between geographic
proximity and the effectiveness of tax enforcement, helping to reconcile mixed
findings in the tax compliance literature.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [474] [Statistics-Friendly Confidentiality Protection for Establishment Data, with Applications to the QCEW](https://arxiv.org/abs/2509.01597)
*Kaitlyn Webb,Prottay Protivash,John Durrell,Daniell Toth,Aleksandra Slavković,Daniel Kifer*

Main category: cs.CR

TL;DR: 本文受高斯差分隐私启发，提出适用于商业数据的保密框架并评估机制。


<details>
  <summary>Details</summary>
Motivation: 商业数据保密是研究不足领域，传统方法结果不佳，现代隐私技术不适用于商业数据。

Method: 受高斯差分隐私启发，提出保密框架，设计两种查询回答机制，分析将噪声查询答案转换为保密微数据的新挑战。

Result: 在保密的季度就业和工资普查（QCEW）微数据和公共替代数据集上评估机制。

Conclusion: 未明确提及结论。

Abstract: Confidentiality for business data is an understudied area of disclosure
avoidance, where legacy methods struggle to provide acceptable results. Modern
formal privacy techniques designed for person-level data do not provide
suitable confidentiality/utility trade-offs due to the highly skewed nature of
business data and because extreme outlier records are often important
contributors to query answers. In this paper, inspired by Gaussian Differential
Privacy, we propose a novel confidentiality framework for business data with a
focus on interpretability for policy makers. We propose two query-answering
mechanisms and analyze new challenges that arise when noisy query answers are
converted into confidentiality-preserving microdata. We evaluate our mechanisms
on confidential Quarterly Census of Employment and Wages (QCEW) microdata and a
public substitute dataset.

</details>


### [475] [Federated Survival Analysis with Node-Level Differential Privacy: Private Kaplan-Meier Curves](https://arxiv.org/abs/2509.00615)
*Narasimha Raghavan Veeraragavan,Jan Franz Nygård*

Main category: cs.CR

TL;DR: 研究用节点级差分隐私跨医疗辖区计算Kaplan - Meier生存曲线，对比四种平滑技术，证明可在不迭代训练或用复杂密码学情况下共享临床有用生存信息。


<details>
  <summary>Details</summary>
Motivation: 在保护患者隐私的前提下，实现跨多个医疗辖区计算Kaplan - Meier生存曲线。

Method: 各站点仅披露一次曲线并添加拉普拉斯噪声，服务器对噪声曲线求平均；对比四种平滑技术。

Result: Total - Variation平均准确率最佳，频域平滑器有更强最坏情况鲁棒性，Weibull模型在最严格隐私设置下最稳定。

Conclusion: 隐私预算为0.5及以上时，可在不迭代训练或用复杂密码学情况下共享临床有用生存信息。

Abstract: We investigate how to calculate Kaplan-Meier survival curves across multiple
health-care jurisdictions while protecting patient privacy with node-level
differential privacy. Each site discloses its curve only once, adding Laplace
noise whose scale is determined by the length of the common time grid; the
server then averages the noisy curves, so the overall privacy budget remains
unchanged. We benchmark four one-shot smoothing techniques: Discrete Cosine
Transform, Haar Wavelet shrinkage, adaptive Total-Variation denoising, and a
parametric Weibull fit on the NCCTG lung-cancer cohort under five privacy
levels and three partition scenarios (uniform, moderately skewed, highly
imbalanced). Total-Variation gives the best mean accuracy, whereas the
frequency-domain smoothers offer stronger worst-case robustness and the Weibull
model shows the most stable behaviour at the strictest privacy setting. Across
all methods the released curves keep the empirical log-rank type-I error below
fifteen percent for privacy budgets of 0.5 and higher, demonstrating that
clinically useful survival information can be shared without iterative training
or heavy cryptography.

</details>


### [476] [LiFeChain: Lightweight Blockchain for Secure and Efficient Federated Lifelong Learning in IoT](https://arxiv.org/abs/2509.01434)
*Handi Chen,Jing Deng,Xiuzhe Wu,Zhihan Jiang,Xinchen Zhang,Xianhao Chen,Edith C. H. Ngai*

Main category: cs.CR

TL;DR: 本文提出适用于联邦终身学习（FLL）的轻量级区块链LiFeChain，可增强FLL安全性和效率，实验证明其效果良好。


<details>
  <summary>Details</summary>
Motivation: IoT中FLL易受攻击且风险可能被数据异质性掩盖，标准单服务器架构难维持可靠审计跟踪，直接应用区块链会增加成本。

Method: 提出LiFeChain，包含服务器端的模型相关性证明（PoMC）共识和客户端的分段零知识仲裁（Seg - ZA）机制，且可作为即插即用组件集成到现有FLL算法。

Result: LiFeChain能增强模型对抗两种长期攻击的性能，保持高效率和可扩展性。

Conclusion: LiFeChain是首个专为FLL设计的区块链，能有效解决FLL在IoT系统中的安全和效率问题。

Abstract: The expansion of Internet of Things (IoT) devices constantly generates
heterogeneous data streams, driving demand for continuous, decentralized
intelligence. Federated Lifelong Learning (FLL) provides an ideal solution by
incorporating federated and lifelong learning to overcome catastrophic
forgetting. The extended lifecycle of FLL in IoT systems increases their
vulnerability to persistent attacks, and these risks may be obscured by
performance degradation caused by spatial-temporal data heterogeneity.
Moreover, this problem is exacerbated by the standard single-server
architecture, as its single point of failure makes it difficult to maintain a
reliable audit trail for long-term threats. Blockchain provides a tamper-proof
foundation for trustworthy FLL systems. Nevertheless, directly applying
blockchain to FLL significantly increases computational and retrieval costs
with the expansion of the knowledge base, slowing down the training on IoT
devices. To address these challenges, we propose LiFeChain, a lightweight
blockchain for secure and efficient federated lifelong learning by providing a
tamper-resistant ledger with minimal on-chain disclosure and bidirectional
verification. To the best of our knowledge, LiFeChain is the first blockchain
tailored for FLL. LiFeChain incorporates two complementary mechanisms: the
proof-of-model-correlation (PoMC) consensus on the server, which couples
learning and unlearning mechanisms to mitigate negative transfer, and segmented
zero-knowledge arbitration (Seg-ZA) on the client, which detects and arbitrates
abnormal committee behavior without compromising privacy. LiFeChain is designed
as a plug-and-play component that can be seamlessly integrated into existing
FLL algorithms. Experimental results demonstrate that LiFeChain not only
enhances model performance against two long-term attacks but also sustains high
efficiency and scalability.

</details>


### [477] [Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs](https://arxiv.org/abs/2509.02372)
*Zhiyang Chen,Tara Saba,Xun Deng,Xujie Si,Fan Long*

Main category: cs.CR

TL;DR: 本文提出可扩展自动化审计框架评估大语言模型生成恶意代码风险，发现测试模型均存在系统性漏洞，强调需加强防御机制和安全检查。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖互联网数据集训练，存在吸收和重现恶意内容的安全风险，需评估该威胁。

Method: 引入可扩展自动化审计框架，从已知诈骗数据库合成无害开发者风格提示词查询生产大语言模型，判断是否生成含有害URL的代码。

Result: 对四个生产大语言模型的大规模评估发现系统性漏洞，平均4.2%的程序含恶意URL，且恶意代码常由良性提示触发，找到177个触发所有模型产生有害输出的无害提示。

Conclusion: 生产大语言模型的训练数据已被大规模污染，迫切需要更强大的防御机制和生成后安全检查以缓解潜在安全威胁。

Abstract: Large Language Models (LLMs) have become critical to modern software
development, but their reliance on internet datasets for training introduces a
significant security risk: the absorption and reproduction of malicious
content. To evaluate this threat, this paper introduces a scalable, automated
audit framework that synthesizes innocuous, developer-style prompts from known
scam databases to query production LLMs and determine if they generate code
containing harmful URLs. We conducted a large-scale evaluation across four
production LLMs (GPT-4o, GPT-4o-mini, Llama-4-Scout, and DeepSeek-V3), and
found a systemic vulnerability, with all tested models generating malicious
code at a non-negligible rate. On average, 4.2\% of programs generated in our
experiments contained malicious URLs. Crucially, this malicious code is often
generated in response to benign prompts. We manually validate the prompts which
cause all four LLMs to generate malicious code, and resulting in 177 innocuous
prompts that trigger all models to produce harmful outputs. These results
provide strong empirical evidence that the training data of production LLMs has
been successfully poisoned at scale, underscoring the urgent need for more
robust defense mechanisms and post-generation safety checks to mitigate the
propagation of hidden security threats.

</details>


### [478] [Per-sender neural network classifiers for email authorship validation](https://arxiv.org/abs/2509.00005)
*Rohit Dube*

Main category: cs.CR

TL;DR: 探讨邮件作者身份验证问题，提出新数据集，评估两种分类器，Char - CNN模型效果好，作者分类器可低开销集成到现有邮件安全系统。


<details>
  <summary>Details</summary>
Motivation: 现代组织面临商业邮件妥协和横向针对性网络钓鱼攻击威胁，多数组织默认信任内部邮件，易受受损员工账户攻击，需新防御方法。

Method: 定义并探索作者身份验证问题，基于安然语料库创建新数据集，用人类撰写和大语言模型生成邮件模拟非真实消息，评估Naive Bayes模型和Char - CNN模型。

Result: Char - CNN模型在各种情况下达到高准确率和F1分数。

Conclusion: 按发件人分类的作者分类器可低开销集成到现有商业邮件安全系统。

Abstract: Business email compromise and lateral spear phishing attacks are among modern
organizations' most costly and damaging threats. While inbound phishing
defenses have improved significantly, most organizations still trust internal
emails by default, leaving themselves vulnerable to attacks from compromised
employee accounts. In this work, we define and explore the problem of
authorship validation: verifying whether a claimed sender actually authored a
given email. Authorship validation is a lightweight, real-time defense that
complements traditional detection methods by modeling per-sender writing style.
Further, the paper presents a collection of new datasets based on the Enron
corpus. These simulate inauthentic messages using both human-written and large
language model-generated emails. The paper also evaluates two classifiers -- a
Naive Bayes model and a character-level convolutional neural network (Char-CNN)
-- for the authorship validation task. Our experiments show that the Char-CNN
model achieves high accuracy and F1 scores under various circumstances.
Finally, we discuss deployment considerations and show that per-sender
authorship classifiers are practical for integrating into existing commercial
email security systems with low overhead.

</details>


### [479] [Enabling Transparent Cyber Threat Intelligence Combining Large Language Models and Domain Ontologies](https://arxiv.org/abs/2509.00081)
*Luca Cotti,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.CR

TL;DR: 提出结合本体驱动输出和大语言模型的方法，构建AI代理提高从安全日志提取信息的准确性和可解释性，实验显示比传统方法更准。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以可靠透明地识别和解释恶意事件，尤其是非结构化或模糊日志条目，受蜜罐日志数据分析需求驱动。

Method: 结合本体驱动结构化输出和大语言模型，集成领域本体和基于SHACL的约束引导输出结构，将提取信息组织到本体增强图数据库。

Result: 实验表明该方法比传统仅使用提示的方法在信息提取上准确性更高，更注重提取质量而非处理速度。

Conclusion: 所提方法能有效提高从网络安全日志中提取信息的准确性和可解释性。

Abstract: Effective Cyber Threat Intelligence (CTI) relies upon accurately structured
and semantically enriched information extracted from cybersecurity system logs.
However, current methodologies often struggle to identify and interpret
malicious events reliably and transparently, particularly in cases involving
unstructured or ambiguous log entries. In this work, we propose a novel
methodology that combines ontology-driven structured outputs with Large
Language Models (LLMs), to build an Artificial Intelligence (AI) agent that
improves the accuracy and explainability of information extraction from
cybersecurity logs. Central to our approach is the integration of domain
ontologies and SHACL-based constraints to guide the language model's output
structure and enforce semantic validity over the resulting graph. Extracted
information is organized into an ontology-enriched graph database, enabling
future semantic analysis and querying. The design of our methodology is
motivated by the analytical requirements associated with honeypot log data,
which typically comprises predominantly malicious activity. While our case
study illustrates the relevance of this scenario, the experimental evaluation
is conducted using publicly available datasets. Results demonstrate that our
method achieves higher accuracy in information extraction compared to
traditional prompt-only approaches, with a deliberate focus on extraction
quality rather than processing speed.

</details>


### [480] [Private, Verifiable, and Auditable AI Systems](https://arxiv.org/abs/2509.00085)
*Tobin South*

Main category: cs.CR

TL;DR: 论文探讨现代AI中隐私、可验证性和可审计性的平衡，提出技术方案并为系统设计和政策讨论提供参考。


<details>
  <summary>Details</summary>
Motivation: 社会对AI依赖增加，需确保其安全、责任和可信度，解决隐私、可验证性和可审计性的复杂关系。

Method: 结合国际政策贡献和技术研究识别AI风险，运用零知识密码学、安全多方计算、可信执行环境等技术。

Result: 提出解决关键隐私和可验证性挑战的技术方案。

Conclusion: 给出基于基础模型的AI系统平衡隐私、可验证性和可审计性的统一观点，为系统设计提供蓝图并影响政策讨论。

Abstract: The growing societal reliance on artificial intelligence necessitates robust
frameworks for ensuring its security, accountability, and trustworthiness. This
thesis addresses the complex interplay between privacy, verifiability, and
auditability in modern AI, particularly in foundation models. It argues that
technical solutions that integrate these elements are critical for responsible
AI innovation. Drawing from international policy contributions and technical
research to identify key risks in the AI pipeline, this work introduces novel
technical solutions for critical privacy and verifiability challenges.
Specifically, the research introduces techniques for enabling verifiable and
auditable claims about AI systems using zero-knowledge cryptography; utilizing
secure multi-party computation and trusted execution environments for
auditable, confidential deployment of large language models and information
retrieval; and implementing enhanced delegation mechanisms, credentialing
systems, and access controls to secure interactions with autonomous and
multi-agent AI systems. Synthesizing these technical advancements, this
dissertation presents a cohesive perspective on balancing privacy,
verifiability, and auditability in foundation model-based AI systems, offering
practical blueprints for system designers and informing policy discussions on
AI safety and governance.

</details>


### [481] [AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema](https://arxiv.org/abs/2509.00088)
*Ting-Chun Liu,Ching-Yu Hsu,Kuan-Yi Lee,Chi-An Fu,Hung-yi Lee*

Main category: cs.CR

TL;DR: 提出AEGIS框架应对大语言模型提示注入攻击，在真实数据集上表现优于基线，凸显对抗训练的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决基于提示的检测方法需手动提示工程的问题，提高大语言模型在提示注入攻击下的安全性。

Method: 提出AEGIS框架，用类梯度自然语言提示优化技术迭代优化攻击和防御提示，通过文本梯度优化模块和大语言模型引导的评估循环让攻防双方自主进化。

Result: 在真实数据集上，攻击成功率达1.0，比基线提高0.26；检测真阳性率达0.84，提高0.23，真阴性率为0.89。消融实验证实了协同进化等的重要性，框架在不同大语言模型中有效。

Conclusion: 对抗训练是防范提示注入的可扩展且有效的方法。

Abstract: Prompt injection attacks pose a significant challenge to the safe deployment
of Large Language Models (LLMs) in real-world applications. While prompt-based
detection offers a lightweight and interpretable defense strategy, its
effectiveness has been hindered by the need for manual prompt engineering. To
address this issue, we propose AEGIS , an Automated co-Evolutionary framework
for Guarding prompt Injections Schema. Both attack and defense prompts are
iteratively optimized against each other using a gradient-like natural language
prompt optimization technique. This framework enables both attackers and
defenders to autonomously evolve via a Textual Gradient Optimization (TGO)
module, leveraging feedback from an LLM-guided evaluation loop. We evaluate our
system on a real-world assignment grading dataset of prompt injection attacks
and demonstrate that our method consistently outperforms existing baselines,
achieving superior robustness in both attack success and detection.
Specifically, the attack success rate (ASR) reaches 1.0, representing an
improvement of 0.26 over the baseline. For detection, the true positive rate
(TPR) improves by 0.23 compared to the previous best work, reaching 0.84, and
the true negative rate (TNR) remains comparable at 0.89. Ablation studies
confirm the importance of co-evolution, gradient buffering, and multi-objective
optimization. We also confirm that this framework is effective in different
LLMs. Our results highlight the promise of adversarial training as a scalable
and effective approach for guarding prompt injections.

</details>


### [482] [A Whole New World: Creating a Parallel-Poisoned Web Only AI-Agents Can See](https://arxiv.org/abs/2509.00124)
*Shaked Zychlinski*

Main category: cs.CR

TL;DR: 本文介绍利用网站伪装技术攻击大语言模型驱动的自主网页浏览代理的新攻击向量，阐述攻击机制与安全影响，强调需加强防御。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型驱动的自主网页浏览代理日益普及，其独特且相似的数字指纹带来新的安全隐患，需研究相关攻击及防御。

Method: 利用网站伪装技术，恶意网站识别代理请求并提供含恶意指令的伪装页面。

Result: 攻击者可劫持代理行为，实现数据泄露、恶意软件执行或传播虚假信息，且对人类用户和传统安全爬虫不可见。

Conclusion: 该攻击对未来智能体AI有严重安全影响，急需针对此隐蔽且可扩展攻击的强大防御措施。

Abstract: This paper introduces a novel attack vector that leverages website cloaking
techniques to compromise autonomous web-browsing agents powered by Large
Language Models (LLMs). As these agents become more prevalent, their unique and
often homogenous digital fingerprints - comprising browser attributes,
automation framework signatures, and network characteristics - create a new,
distinguishable class of web traffic. The attack exploits this
fingerprintability. A malicious website can identify an incoming request as
originating from an AI agent and dynamically serve a different, "cloaked"
version of its content. While human users see a benign webpage, the agent is
presented with a visually identical page embedded with hidden, malicious
instructions, such as indirect prompt injections. This mechanism allows
adversaries to hijack agent behavior, leading to data exfiltration, malware
execution, or misinformation propagation, all while remaining completely
invisible to human users and conventional security crawlers. This work
formalizes the threat model, details the mechanics of agent fingerprinting and
cloaking, and discusses the profound security implications for the future of
agentic AI, highlighting the urgent need for robust defenses against this
stealthy and scalable attack.

</details>


### [483] [Cross-Domain Malware Detection via Probability-Level Fusion of Lightweight Gradient Boosting Models](https://arxiv.org/abs/2509.00476)
*Omar Khalid Ali Mohamed*

Main category: cs.CR

TL;DR: 提出轻量级恶意软件检测框架，跨三数据集概率融合，效果好且开销低可复现。


<details>
  <summary>Details</summary>
Motivation: 现有单数据集模型跨域泛化差、计算成本高，需更强检测机制。

Method: 在三个不同数据集上训练LightGBM分类器，选特征，用网格搜索确定权重融合预测概率。

Result: 融合方法在跨域验证集上宏F1分数达0.823，优于单个模型。

Conclusion: 该框架泛化性好、计算开销低，适合实时部署且可复现。

Abstract: The escalating sophistication of malware necessitates robust detection
mechanisms that generalize across diverse data sources. Traditional
single-dataset models struggle with cross-domain generalization and often incur
high computational costs. This paper presents a novel, lightweight framework
for malware detection that employs probability-level fusion across three
distinct datasets: EMBER (static features), API Call Sequences (behavioral
features), and CIC Obfuscated Memory (memory patterns). Our method trains
individual LightGBM classifiers on each dataset, selects top predictive
features to ensure efficiency, and fuses their prediction probabilities using
optimized weights determined via grid search. Extensive experiments demonstrate
that our fusion approach achieves a macro F1-score of 0.823 on a cross-domain
validation set, significantly outperforming individual models and providing
superior generalization. The framework maintains low computational overhead,
making it suitable for real-time deployment, and all code and data are provided
for full reproducibility.

</details>


### [484] [Enabling Trustworthy Federated Learning via Remote Attestation for Mitigating Byzantine Threats](https://arxiv.org/abs/2509.00634)
*Chaoyu Zhang,Heng Jin,Shanghao Shi,Hexuan Yu,Sydney Johns,Y. Thomas Hou,Wenjing Lou*

Main category: cs.CR

TL;DR: 文章提出Sentinel方案解决联邦学习中拜占庭攻击检测难题，实验证明其能确保本地训练完整性且开销低。


<details>
  <summary>Details</summary>
Motivation: 联邦学习分布式特性使其易受拜占庭攻击，现有数据驱动防御区分恶意更新与自然变化能力差，误报率高、过滤性能差。

Method: 提出基于远程认证（RA）的Sentinel方案，采用代码插桩跟踪控制流和监控关键变量，利用可信执行环境（TEE）中的可信训练记录器生成认证报告并签名传输至服务器验证。

Result: 在物联网设备上的实验表明，Sentinel能以低运行时和内存开销确保本地训练完整性。

Conclusion: Sentinel方案可从系统安全角度缓解联邦学习中的拜占庭攻击，确保只有可信模型更新被聚合到全局模型。

Abstract: Federated Learning (FL) has gained significant attention for its
privacy-preserving capabilities, enabling distributed devices to
collaboratively train a global model without sharing raw data. However, its
distributed nature forces the central server to blindly trust the local
training process and aggregate uncertain model updates, making it susceptible
to Byzantine attacks from malicious participants, especially in
mission-critical scenarios. Detecting such attacks is challenging due to the
diverse knowledge across clients, where variations in model updates may stem
from benign factors, such as non-IID data, rather than adversarial behavior.
Existing data-driven defenses struggle to distinguish malicious updates from
natural variations, leading to high false positive rates and poor filtering
performance.
  To address this challenge, we propose Sentinel, a remote attestation
(RA)-based scheme for FL systems that regains client-side transparency and
mitigates Byzantine attacks from a system security perspective. Our system
employs code instrumentation to track control-flow and monitor critical
variables in the local training process. Additionally, we utilize a trusted
training recorder within a Trusted Execution Environment (TEE) to generate an
attestation report, which is cryptographically signed and securely transmitted
to the server. Upon verification, the server ensures that legitimate client
training processes remain free from program behavior violation or data
manipulation, allowing only trusted model updates to be aggregated into the
global model. Experimental results on IoT devices demonstrate that Sentinel
ensures the trustworthiness of the local training integrity with low runtime
and memory overhead.

</details>


### [485] [LLM-HyPZ: Hardware Vulnerability Discovery using an LLM-Assisted Hybrid Platform for Zero-Shot Knowledge Extraction and Refinement](https://arxiv.org/abs/2509.00647)
*Yu-Zheng Lin,Sujan Ghimire,Abhiram Nandimandalam,Jonah Michael Camacho,Unnati Tripathi,Rony Macwan,Sicong Shao,Setareh Rafatirad,Rozhin Yasaei,Pratik Satam,Soheil Salehi*

Main category: cs.CR

TL;DR: 提出LLM - HyPZ框架用于从漏洞语料库中进行零样本知识提取和完善，应用该框架识别出硬件相关漏洞并总结主题，LLaMA 3.3 70B分类准确率高，支持了MITRE CWE MIHW 2025更新。


<details>
  <summary>Details</summary>
Motivation: 硬件漏洞快速增长，需要系统且可扩展的分析方法，现有基于专家的方法缺乏统计严谨性和有主观偏差，缺少大规模数据驱动的硬件弱点分析基础。

Method: 提出LLM - HyPZ框架，集成零样本LLM分类、上下文嵌入、无监督聚类和提示驱动的总结方法来大规模挖掘硬件相关CVEs。

Result: 应用该框架从2021 - 2024 CVE语料库中识别出1742个硬件相关漏洞，总结为五个主题，LLaMA 3.3 70B在验证集上分类准确率达99.5%，框架管道找出411个用于下游MIHW分析的CVEs。

Conclusion: LLM - HyPZ是首个数据驱动、可扩展的系统发现硬件漏洞的方法，弥合了专家知识和实际漏洞证据之间的差距。

Abstract: The rapid growth of hardware vulnerabilities has created an urgent need for
systematic and scalable analysis methods. Unlike software flaws, which are
often patchable post-deployment, hardware weaknesses remain embedded across
product lifecycles, posing persistent risks to processors, embedded devices,
and IoT platforms. Existing efforts such as the MITRE CWE Hardware List (2021)
relied on expert-driven Delphi surveys, which lack statistical rigor and
introduce subjective bias, while large-scale data-driven foundations for
hardware weaknesses have been largely absent. In this work, we propose
LLM-HyPZ, an LLM-assisted hybrid framework for zero-shot knowledge extraction
and refinement from vulnerability corpora. Our approach integrates zero-shot
LLM classification, contextualized embeddings, unsupervised clustering, and
prompt-driven summarization to mine hardware-related CVEs at scale. Applying
LLM-HyPZ to the 2021-2024 CVE corpus (114,836 entries), we identified 1,742
hardware-related vulnerabilities. We distilled them into five recurring themes,
including privilege escalation via firmware and BIOS, memory corruption in
mobile and IoT systems, and physical access exploits. Benchmarking across seven
LLMs shows that LLaMA 3.3 70B achieves near-perfect classification accuracy
(99.5%) on a curated validation set. Beyond methodological contributions, our
framework directly supported the MITRE CWE Most Important Hardware Weaknesses
(MIHW) 2025 update by narrowing the candidate search space. Specifically, our
pipeline surfaced 411 of the 1,026 CVEs used for downstream MIHW analysis,
thereby reducing expert workload and accelerating evidence gathering. These
results establish LLM-HyPZ as the first data-driven, scalable approach for
systematically discovering hardware vulnerabilities, thereby bridging the gap
between expert knowledge and real-world vulnerability evidence.

</details>


### [486] [Clone What You Can't Steal: Black-Box LLM Replication via Logit Leakage and Distillation](https://arxiv.org/abs/2509.00973)
*Kanchon Gharami,Hansaka Aluvihare,Shafika Showkat Moni,Berker Peköz*

Main category: cs.CR

TL;DR: 论文聚焦大语言模型API缺乏访问控制暴露logits问题，提出受限复制管道克隆模型，展示不同层数学生模型效果，强调加固API和本地防御部署的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型API缺乏强访问控制暴露logits，此前研究在严格查询约束下再生黑盒模型不足，需填补该空白。

Method: 采用两阶段方法，先通过奇异值分解重建输出投影矩阵，再将剩余架构提炼到不同深度的紧凑学生模型。

Result: 6层学生模型重现97.6%的教师模型隐藏状态几何，困惑度仅增7.31%；4层变体推理快17.1%，参数减少18.1%，攻击在不到24 GPU小时内完成。

Conclusion: 成本受限的攻击者能快速克隆大语言模型，迫切需要加固推理API和进行安全的本地防御部署。

Abstract: Large Language Models (LLMs) are increasingly deployed in mission-critical
systems, facilitating tasks such as satellite operations, command-and-control,
military decision support, and cyber defense. Many of these systems are
accessed through application programming interfaces (APIs). When such APIs lack
robust access controls, they can expose full or top-k logits, creating a
significant and often overlooked attack surface. Prior art has mainly focused
on reconstructing the output projection layer or distilling surface-level
behaviors. However, regenerating a black-box model under tight query
constraints remains underexplored. We address that gap by introducing a
constrained replication pipeline that transforms partial logit leakage into a
functional deployable substitute model clone. Our two-stage approach (i)
reconstructs the output projection matrix by collecting top-k logits from under
10k black-box queries via singular value decomposition (SVD) over the logits,
then (ii) distills the remaining architecture into compact student models with
varying transformer depths, trained on an open source dataset. A 6-layer
student recreates 97.6% of the 6-layer teacher model's hidden-state geometry,
with only a 7.31% perplexity increase, and a 7.58 Negative Log-Likelihood
(NLL). A 4-layer variant achieves 17.1% faster inference and 18.1% parameter
reduction with comparable performance. The entire attack completes in under 24
graphics processing unit (GPU) hours and avoids triggering API rate-limit
defenses. These results demonstrate how quickly a cost-limited adversary can
clone an LLM, underscoring the urgent need for hardened inference APIs and
secure on-premise defense deployments.

</details>


### [487] [Web Fraud Attacks Against LLM-Driven Multi-Agent Systems](https://arxiv.org/abs/2509.01211)
*Dezhang Kong,Hujin Peng,Yilun Zhang,Lele Zhao,Zhenhua Xu,Shi Lin,Changting Lin,Meng Han*

Main category: cs.CR

TL;DR: 提出针对LLM驱动多智能体系统的Web欺诈攻击，设计11种攻击变体，实验表明其有破坏性和逃避检测优势，强调需解决该类攻击。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的多智能体系统应用增多，Web链接安全影响系统可靠性，需关注诱导系统访问恶意网站的攻击。

Method: 提出Web欺诈攻击，设计包含域名篡改、链接结构伪装等11种代表性攻击变体。

Result: 实验表明Web欺诈攻击在不同多智能体系统架构中有显著破坏潜力，且在逃避检测上有优势。

Conclusion: Web欺诈攻击的隐蔽性和破坏性对系统和用户安全构成威胁，需加以解决。

Abstract: With the proliferation of applications built upon LLM-driven multi-agent
systems (MAS), the security of Web links has become a critical concern in
ensuring system reliability. Once an agent is induced to visit a malicious
website, attackers can use it as a springboard to conduct diverse subsequent
attacks, which will drastically expand the attack surface. In this paper, we
propose Web Fraud Attacks, a novel type of attack aiming at inducing MAS to
visit malicious websites. We design 11 representative attack variants that
encompass domain name tampering (homoglyph deception, character substitution,
etc.), link structure camouflage (sub-directory nesting, sub-domain grafting,
parameter obfuscation, etc.), and other deceptive techniques tailored to
exploit MAS's vulnerabilities in link validation. Through extensive experiments
on these crafted attack vectors, we demonstrate that Web fraud attacks not only
exhibit significant destructive potential across different MAS architectures
but also possess a distinct advantage in evasion: they circumvent the need for
complex input formats such as jailbreaking, which inherently carry higher
exposure risks. These results underscore the importance of addressing Web fraud
attacks in LLM-driven MAS, as their stealthiness and destructiveness pose
non-negligible threats to system security and user safety.

</details>


### [488] [Anomaly detection in network flows using unsupervised online machine learning](https://arxiv.org/abs/2509.01375)
*Alberto Miguel-Diez,Adrián Campazas-Vega,Ángel Manuel Guerrero-Higueras,Claudia Álvarez-Aparicio,Vicente Matellán-Olivera*

Main category: cs.CR

TL;DR: 提出基于无监督机器学习和在线学习能力的网络流量异常检测模型，在特定数据集上评估效果好且适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 网络流量增长、攻击频发且复杂，网络行为动态变化，需要能持续自适应的解决方案。

Method: 使用River库和One - Class SVM实现无监督机器学习的异常检测模型，可动态学习网络正常行为。

Result: 在NF - UNSW - NB15数据集及其扩展版本v2上评估，准确率超98%，误报率低于3.1%，最新版本数据集召回率达100%，单流处理时间<0.033 ms。

Conclusion: 该方法适用于实时应用，在网络流量异常检测中有效。

Abstract: Nowadays, the volume of network traffic continues to grow, along with the
frequency and sophistication of attacks. This scenario highlights the need for
solutions capable of continuously adapting, since network behavior is dynamic
and changes over time. This work presents an anomaly detection model for
network flows using unsupervised machine learning with online learning
capabilities. This approach allows the system to dynamically learn the normal
behavior of the network and detect deviations without requiring labeled data,
which is particularly useful in real-world environments where traffic is
constantly changing and labeled data is scarce. The model was implemented using
the River library with a One-Class SVM and evaluated on the NF-UNSW-NB15
dataset and its extended version v2, which contain network flows labeled with
different attack categories. The results show an accuracy above 98%, a false
positive rate below 3.1%, and a recall of 100% in the most advanced version of
the dataset. In addition, the low processing time per flow (<0.033 ms)
demonstrates the feasibility of the approach for real-time applications.

</details>


### [489] [Securing Radiation Detection Systems with an Efficient TinyML-Based IDS for Edge Devices](https://arxiv.org/abs/2509.01592)
*Einstein Rivas Pizarro,Wajiha Zaheer,Li Yang,Khalil El-Khatib,Glenn Harvel*

Main category: cs.CR

TL;DR: 文章指出辐射检测系统易受网络攻击，提出新合成辐射数据集和适用于资源受限环境的入侵检测系统，用TinyML技术优化XGBoost模型实现实时检测。


<details>
  <summary>Details</summary>
Motivation: 辐射检测系统易受多种网络攻击，威胁公共健康和安全，需解决方案。

Method: 提出新合成辐射数据集和入侵检测系统，利用TinyML技术优化XGBoost模型，包括剪枝、量化、特征选择和采样。

Result: 显著减小模型大小和计算需求，能在低资源设备上进行实时入侵检测。

Conclusion: 所提方法在资源受限环境中能实现入侵检测，平衡效率和准确性。

Abstract: Radiation Detection Systems (RDSs) play a vital role in ensuring public
safety across various settings, from nuclear facilities to medical
environments. However, these systems are increasingly vulnerable to
cyber-attacks such as data injection, man-in-the-middle (MITM) attacks, ICMP
floods, botnet attacks, privilege escalation, and distributed denial-of-service
(DDoS) attacks. Such threats could compromise the integrity and reliability of
radiation measurements, posing significant public health and safety risks. This
paper presents a new synthetic radiation dataset and an Intrusion Detection
System (IDS) tailored for resource-constrained environments, bringing Machine
Learning (ML) predictive capabilities closer to the sensing edge layer of
critical infrastructure. Leveraging TinyML techniques, the proposed IDS employs
an optimized XGBoost model enhanced with pruning, quantization, feature
selection, and sampling. These TinyML techniques significantly reduce the size
of the model and computational demands, enabling real-time intrusion detection
on low-resource devices while maintaining a reasonable balance between
efficiency and accuracy.

</details>


### [490] [An Efficient Intrusion Detection System for Safeguarding Radiation Detection Systems](https://arxiv.org/abs/2509.01599)
*Nathanael Coolidge,Jaime González Sanz,Li Yang,Khalil El Khatib,Glenn Harvel,Nelson Agbemava,I Putu Susila,Mehmet Yavuz Yagci*

Main category: cs.CR

TL;DR: 本文提出基于机器学习的入侵检测系统（IDS）检测辐射检测系统（RDS）中的拒绝服务（DoS）攻击，着重使用LightGBM算法并优化模型。


<details>
  <summary>Details</summary>
Motivation: 现有RDS缺乏防外部恶意攻击保护，IDS在RDS中的应用新颖，需检测DoS攻击保护关键基础设施。

Method: 利用采样方法基于真实辐射数据集模拟DoS攻击，评估随机森林、SVM、逻辑回归和LightGBM等ML算法，使用特征选择、并行执行和随机搜索等技术优化模型。

Result: LightGBM准确性高、计算资源消耗低，适合实时入侵检测，最终开发出优化高效的基于LightGBM的IDS。

Conclusion: 能为RDS实现准确的入侵检测。

Abstract: Radiation Detection Systems (RDSs) are used to measure and detect abnormal
levels of radioactive material in the environment. These systems are used in
many applications to mitigate threats posed by high levels of radioactive
material. However, these systems lack protection against malicious external
attacks to modify the data. The novelty of applying Intrusion Detection Systems
(IDS) in RDSs is a crucial element in safeguarding these critical
infrastructures. While IDSs are widely used in networking environments to
safeguard against various attacks, their application in RDSs is novel. A common
attack on RDSs is Denial of Service (DoS), where the attacker aims to overwhelm
the system, causing malfunctioning RDSs. This paper proposes an efficient
Machine Learning (ML)-based IDS to detect anomalies in radiation data, focusing
on DoS attacks. This work explores the use of sampling methods to create a
simulated DoS attack based on a real radiation dataset, followed by an
evaluation of various ML algorithms, including Random Forest, Support Vector
Machine (SVM), logistic regression, and Light Gradient-Boosting Machine
(LightGBM), to detect DoS attacks on RDSs. LightGBM is emphasized for its
superior accuracy and low computational resource consumption, making it
particularly suitable for real-time intrusion detection. Additionally, model
optimization and TinyML techniques, including feature selection, parallel
execution, and random search methods, are used to improve the efficiency of the
proposed IDS. Finally, an optimized and efficient LightGBM-based IDS is
developed to achieve accurate intrusion detection for RDSs.

</details>


### [491] [Practical and Private Hybrid ML Inference with Fully Homomorphic Encryption](https://arxiv.org/abs/2509.01253)
*Sayan Biswas,Philippe Chartier,Akash Dhasade,Tom Jurien,David Kerriou,Anne-Marie Kerrmarec,Mohammed Lemou,Franklin Tranie,Martijn de Vos,Milos Vujasinovic*

Main category: cs.CR

TL;DR: 介绍混合推理框架Safhire，可降低计算量和延迟，保障模型机密性，评估显示比基线有更低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 当代云服务需保护用户数据和服务器模型机密性，但全同态加密（FHE）实用性受限于昂贵的自举和低效的非线性激活近似。

Method: 引入混合推理框架Safhire，在服务器加密执行线性层，将非线性操作交给客户端明文处理；应用随机混洗保护模型机密性；采用快速密文打包和部分提取等优化降低延迟。

Result: 在多个标准模型和数据集上评估，Safhire比基线Orion推理延迟降低1.5X - 10.5X，通信开销可控，精度相当。

Conclusion: 证明了混合FHE推理的实用性。

Abstract: In contemporary cloud-based services, protecting users' sensitive data and
ensuring the confidentiality of the server's model are critical. Fully
homomorphic encryption (FHE) enables inference directly on encrypted inputs,
but its practicality is hindered by expensive bootstrapping and inefficient
approximations of non-linear activations. We introduce Safhire, a hybrid
inference framework that executes linear layers under encryption on the server
while offloading non-linearities to the client in plaintext. This design
eliminates bootstrapping, supports exact activations, and significantly reduces
computation. To safeguard model confidentiality despite client access to
intermediate outputs, Safhire applies randomized shuffling, which obfuscates
intermediate values and makes it practically impossible to reconstruct the
model. To further reduce latency, Safhire incorporates advanced optimizations
such as fast ciphertext packing and partial extraction. Evaluations on multiple
standard models and datasets show that Safhire achieves 1.5X - 10.5X lower
inference latency than Orion, a state-of-the-art baseline, with manageable
communication overhead and comparable accuracy, thereby establishing the
practicality of hybrid FHE inference.

</details>


### [492] [E-PhishGen: Unlocking Novel Research in Phishing Email Detection](https://arxiv.org/abs/2509.01791)
*Luca Pajola,Eugenio Caripoti,Simeone Pizzi,Mauro Conti,Stefan Banzer,Giovanni Apruzzese*

Main category: cs.CR

TL;DR: 文章指出钓鱼邮件检测仍是未解决问题，评估相关研究，发现数据集问题，重评估检测方法，提出E - PhishGEN框架生成新数据集E - PhishLLM，测试显示有改进空间。


<details>
  <summary>Details</summary>
Motivation: 尽管之前有很多解决方案达到近乎完美的准确率，但对抗恶意邮件仍是未解决的难题，需对钓鱼邮件检测的科研工作进行批判性评估。

Method: 聚焦评估研究方法的基准数据集，重实现和重评估多种机器学习检测方法，提出E - PhishGEN框架生成新数据集E - PhishLLM，用用户研究验证数据集质量。

Result: 多数方法在相同数据集上训练和测试时性能近乎完美，但在新数据集E - PhishLLM上性能大幅下降。

Conclusion: 钓鱼邮件检测仍是开放问题，为未来研究提供了解决该问题的途径。

Abstract: Every day, our inboxes are flooded with unsolicited emails, ranging between
annoying spam to more subtle phishing scams. Unfortunately, despite abundant
prior efforts proposing solutions achieving near-perfect accuracy, the reality
is that countering malicious emails still remains an unsolved dilemma.
  This "open problem" paper carries out a critical assessment of scientific
works in the context of phishing email detection. First, we focus on the
benchmark datasets that have been used to assess the methods proposed in
research. We find that most prior work relied on datasets containing emails
that -- we argue -- are not representative of current trends, and mostly
encompass the English language. Based on this finding, we then re-implement and
re-assess a variety of detection methods reliant on machine learning (ML),
including large-language models (LLM), and release all of our codebase -- an
(unfortunately) uncommon practice in related research. We show that most such
methods achieve near-perfect performance when trained and tested on the same
dataset -- a result which intrinsically hinders development (how can future
research outperform methods that are already near perfect?). To foster the
creation of "more challenging benchmarks" that reflect current phishing trends,
we propose E-PhishGEN, an LLM-based (and privacy-savvy) framework to generate
novel phishing-email datasets. We use our E-PhishGEN to create E-PhishLLM, a
novel phishing-email detection dataset containing 16616 emails in three
languages. We use E-PhishLLM to test the detectors we considered, showing a
much lower performance than that achieved on existing benchmarks -- indicating
a larger room for improvement. We also validate the quality of E-PhishLLM with
a user study (n=30). To sum up, we show that phishing email detection is still
an open problem -- and provide the means to tackle such a problem by future
research.

</details>


### [493] [Forecasting Future DDoS Attacks Using Long Short Term Memory (LSTM) Model](https://arxiv.org/abs/2509.02076)
*Kong Mun Yeen,Rafidah Md Noor,Wahidah Md Shah,Aslinda Hassan,Muhammad Umair Munir*

Main category: cs.CR

TL;DR: 本文使用深度学习模型预测未来DDoS攻击，采用CRISP - DM模型。


<details>
  <summary>Details</summary>
Motivation: 现有DDoS攻击预测研究相对检测研究较少，通过研究当前趋势和新数据集预测攻击，以便制定缓解计划。

Method: 采用Cross Industry Standard Process for Data Mining (CRISP - DM) 模型。

Result: 未提及

Conclusion: 未提及

Abstract: This paper forecasts future Distributed Denial of Service (DDoS) attacks
using deep learning models. Although several studies address forecasting DDoS
attacks, they remain relatively limited compared to detection-focused research.
By studying the current trends and forecasting based on newer and updated
datasets, mitigation plans against the attacks can be planned and formulated.
The methodology used in this research work conforms to the Cross Industry
Standard Process for Data Mining (CRISP-DM) model.

</details>


### [494] [From Attack Descriptions to Vulnerabilities: A Sentence Transformer-Based Approach](https://arxiv.org/abs/2509.02077)
*Refat Othman,Diaeddin Rimawi,Bruno Rossi,Barbara Russo*

Main category: cs.CR

TL;DR: 本文评估14种SOTA句子转换器，自动从攻击文本描述中识别漏洞，MMPNet模型表现优，自动化关联攻击技术与漏洞可提升安全能力。


<details>
  <summary>Details</summary>
Motivation: 在安全领域，手动将攻击与CVE漏洞映射不可行，需要自动化方法来实现及时的事件响应。

Method: 评估14种SOTA句子转换器，自动从攻击文本描述中识别漏洞。

Result: MMPNet模型在使用攻击技术描述时分类性能优，F1分数89.0，精度84.0，召回率94.7；模型识别的漏洞与CVE库有一定对应比例；发现275个未记录在MITRE库中的预测链接。

Conclusion: 自动化关联攻击技术与漏洞可提升软件安全事件检测和响应能力，减少漏洞可利用时间，助力构建更安全系统。

Abstract: In the domain of security, vulnerabilities frequently remain undetected even
after their exploitation. In this work, vulnerabilities refer to publicly
disclosed flaws documented in Common Vulnerabilities and Exposures (CVE)
reports. Establishing a connection between attacks and vulnerabilities is
essential for enabling timely incident response, as it provides defenders with
immediate, actionable insights. However, manually mapping attacks to CVEs is
infeasible, thereby motivating the need for automation. This paper evaluates 14
state-of-the-art (SOTA) sentence transformers for automatically identifying
vulnerabilities from textual descriptions of attacks. Our results demonstrate
that the multi-qa-mpnet-base-dot-v1 (MMPNet) model achieves superior
classification performance when using attack Technique descriptions, with an
F1-score of 89.0, precision of 84.0, and recall of 94.7. Furthermore, it was
observed that, on average, 56% of the vulnerabilities identified by the MMPNet
model are also represented within the CVE repository in conjunction with an
attack, while 61% of the vulnerabilities detected by the model correspond to
those cataloged in the CVE repository. A manual inspection of the results
revealed the existence of 275 predicted links that were not documented in the
MITRE repositories. Consequently, the automation of linking attack techniques
to vulnerabilities not only enhances the detection and response capabilities
related to software security incidents but also diminishes the duration during
which vulnerabilities remain exploitable, thereby contributing to the
development of more secure systems.

</details>


### [495] [Real-time ML-based Defense Against Malicious Payload in Reconfigurable Embedded Systems](https://arxiv.org/abs/2509.02387)
*Rye Stahle-Smith,Rasha Karakchi*

Main category: cs.CR

TL;DR: 本文研究嵌入式系统芯片级硬件恶意负载，提出基于静态字节特征的监督式机器学习方法检测FPGA恶意比特流，评估显示随机森林F1分数达0.97，模型成功部署。


<details>
  <summary>Details</summary>
Motivation: FPGA在可重构系统中使用增加，恶意比特流带来安全风险，如DoS、数据泄露和隐蔽攻击。

Method: 提出通过静态字节级特征检测恶意比特流的监督式机器学习方法，直接在二进制层面分析比特流，对数据进行字节频率分析、TSVD压缩和SMOTE平衡处理。

Result: 评估的分类器中随机森林宏F1分数达0.97，模型序列化后通过PYNQ成功部署。

Conclusion: 该方法可在资源受限系统上实现实时特洛伊木马检测。

Abstract: The growing use of FPGAs in reconfigurable systems introducessecurity risks
through malicious bitstreams that could cause denial-of-service (DoS), data
leakage, or covert attacks. We investigated chip-level hardware malicious
payload in embedded systems and proposed a supervised machine learning method
to detect malicious bitstreams via static byte-level features. Our approach
diverges from existing methods by analyzing bitstreams directly at the binary
level, enabling real-time detection without requiring access to source code or
netlists. Bitstreams were sourced from state-of-the-art (SOTA) benchmarks and
re-engineered to target the Xilinx PYNQ-Z1 FPGA Development Board. Our dataset
included 122 samples of benign and malicious configurations. The data were
vectorized using byte frequency analysis, compressed using TSVD, and balanced
using SMOTE to address class imbalance. The evaluated classifiers demonstrated
that Random Forest achieved a macro F1-score of 0.97, underscoring the
viability of real-time Trojan detection on resource-constrained systems. The
final model was serialized and successfully deployed via PYNQ to enable
integrated bitstream analysis.

</details>


### [496] [A Survey: Towards Privacy and Security in Mobile Large Language Models](https://arxiv.org/abs/2509.02411)
*Honghui Xu,Kaiyang Li,Wei Chen,Danyang Zheng,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: 本文综述移动大语言模型的隐私和安全问题，分类现有解决方案，分析漏洞，提出潜在应用和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 移动大语言模型在移动和边缘环境部署存在隐私和安全挑战，需综合研究应对。

Method: 对移动大语言模型隐私和安全问题进行全面综述，分类现有解决方案，分析独特漏洞并比较其效果和局限。

Result: 明确移动大语言模型在资源受限环境实现安全和效率的独特障碍。

Conclusion: 提出潜在应用、讨论开放挑战并给出未来研究方向，助力开发可信、合规和可扩展的移动大语言模型系统。

Abstract: Mobile Large Language Models (LLMs) are revolutionizing diverse fields such
as healthcare, finance, and education with their ability to perform advanced
natural language processing tasks on-the-go. However, the deployment of these
models in mobile and edge environments introduces significant challenges
related to privacy and security due to their resource-intensive nature and the
sensitivity of the data they process. This survey provides a comprehensive
overview of privacy and security issues associated with mobile LLMs,
systematically categorizing existing solutions such as differential privacy,
federated learning, and prompt encryption. Furthermore, we analyze
vulnerabilities unique to mobile LLMs, including adversarial attacks,
membership inference, and side-channel attacks, offering an in-depth comparison
of their effectiveness and limitations. Despite recent advancements, mobile
LLMs face unique hurdles in achieving robust security while maintaining
efficiency in resource-constrained environments. To bridge this gap, we propose
potential applications, discuss open challenges, and suggest future research
directions, paving the way for the development of trustworthy,
privacy-compliant, and scalable mobile LLM systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [497] [Tree algorithms for set reconciliation](https://arxiv.org/abs/2509.02373)
*Francisco Lázaro,Čedomir Stefanović*

Main category: cs.NI

TL;DR: 提出增强分区集合协调（EPSR）方案，提高集合协调效率，模拟显示其能近半降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 考虑双方对相似集合进行协调的场景，提升分区集合协调（PSR）效率。

Method: 借鉴随机访问协议树算法技术，提出EPSR方案，并在事件驱动模拟器中模拟性能。

Result: 新协议近半降低PSR通信成本，保持相同时间复杂度。

Conclusion: EPSR方案在保持时间和通信轮复杂度的同时，降低了通信成本，提高了效率。

Abstract: In this work, a set reconciliation setting is considered in which two parties
have similar sets that they would like to reconcile. In particular, we focus on
a divide-and-conquer strategy known as partitioned set reconciliation (PSR), in
which the sets to be reconciled are successively partitioned until they contain
a number of differences below some predetermined value. Borrowing techniques
from tree-algorithms for random-access protocols, we present and analyze a
novel set reconciliation scheme that we term enhanced partitioned set
reconciliation (EPSR). This approach improves the efficiency in terms of
overhead, i.e., it yields a lower communication cost, while keeping the same
time and communication round complexity as PSR. Additionally, we simulate the
performance of the proposed algorithm in an event-driven simulator. Our
findings indicate that this novel protocol nearly halves the communication cost
of PSR while maintaining the same time complexity.

</details>


### [498] [Intelligent Spectrum Management in Satellite Communications](https://arxiv.org/abs/2509.00286)
*Rakshitha De Silva,Shiva Raj Pokhrel,Jonathan Kua,Sithamparanathan Kandeepan*

Main category: cs.NI

TL;DR: 本文探讨将智能动态频谱管理（DSM）方法应用于卫星通信（SatCom），评估AI/ML方法，研究性能指标，指出挑战并规划未来方向，助力构建可持续可扩展的SatCom网络。


<details>
  <summary>Details</summary>
Motivation: 传统卫星频谱分配方法有局限，新兴频谱稀缺，需要新解决方案。

Method: 探索智能DSM方法在SatCom的应用，讨论法规和标准化问题，评估和分类AI/ML方法，研究性能评估指标。

Result: 对相关技术进行了广泛评估和分类，深入研究了性能评估指标。

Conclusion: 识别了监管框架、网络架构和智能频谱管理方面的开放挑战，规划了未来研究方向，以实现增强全球连接的可持续可扩展SatCom网络。

Abstract: Satellite Communication (SatCom) networks represent a fundamental pillar in
modern global connectivity, facilitating reliable service and extensive
coverage across a plethora of applications. The expanding demand for
high-bandwidth services and the proliferation of mega satellite constellations
highlight the limitations of traditional exclusive satellite spectrum
allocation approaches. Cognitive Radio (CR) leading to Cognitive Satellite
(CogSat) networks through Dynamic Spectrum Management (DSM), which enables the
dynamic adaptability of radio equipment to environmental conditions for optimal
performance, presents a promising solution for the emerging spectrum scarcity.
In this survey, we explore the adaptation of intelligent DSM methodologies to
SatCom, leveraging satellite network integrations. We discuss contributions and
hurdles in regulations and standardizations in realizing intelligent DSM in
SatCom, and deep dive into DSM techniques, which enable CogSat networks.
Furthermore, we extensively evaluate and categorize state-of-the-art Artificial
Intelligence (AI)/Machine Learning (ML) methods leveraged for DSM while
exploring operational resilience and robustness of such integrations. In
addition, performance evaluation metrics critical for adaptive resource
management and system optimization in CogSat networks are thoroughly
investigated. This survey also identifies open challenges and outlines future
research directions in regulatory frameworks, network architectures, and
intelligent spectrum management, paving the way for sustainable and scalable
SatCom networks for enhanced global connectivity.

</details>


### [499] [Unsupervised Dataset Cleaning Framework for Encrypted Traffic Classification](https://arxiv.org/abs/2509.00701)
*Kun Qiu,Ying Wang,Baoqian Li,Wenjun Zhu*

Main category: cs.NI

TL;DR: 本文提出自动清理加密移动流量的无监督框架，评估显示其分类准确率与手动清理相近，是基于机器学习的加密流量分类的高效预处理步骤。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法区分加密流量，现有流量数据清理方案依赖手动检查，成本高且耗时。

Method: 提出一个无监督框架自动清理加密移动流量。

Result: 在真实数据集上评估，该框架与手动清理相比，分类准确率仅降低2% - 2.5%。

Conclusion: 此方法为基于机器学习的加密流量分类提供了高效且有效的预处理步骤。

Abstract: Traffic classification, a technique for assigning network flows to predefined
categories, has been widely deployed in enterprise and carrier networks. With
the massive adoption of mobile devices, encryption is increasingly used in
mobile applications to address privacy concerns. Consequently, traditional
methods such as Deep Packet Inspection (DPI) fail to distinguish encrypted
traffic. To tackle this challenge, Artificial Intelligence (AI), in particular
Machine Learning (ML), has emerged as a promising solution for encrypted
traffic classification. A crucial prerequisite for any ML-based approach is
traffic data cleaning, which removes flows that are not useful for training
(e.g., irrelevant protocols, background activity, control-plane messages, and
long-lived sessions). Existing cleaning solutions depend on manual inspection
of every captured packet, making the process both costly and time-consuming. In
this poster, we present an unsupervised framework that automatically cleans
encrypted mobile traffic. Evaluation on real-world datasets shows that our
framework incurs only a 2%~2.5% reduction in classification accuracy compared
with manual cleaning. These results demonstrate that our method offers an
efficient and effective preprocessing step for ML-based encrypted traffic
classification.

</details>


### [500] [Quantum-based QoE Optimization in Advanced Cellular Networks: Integration and Cloud Gaming Use Case](https://arxiv.org/abs/2509.01008)
*Fatma Chaouech,Javier Villegas,António Pereira,Carlos Baena,Sergio Fortes,Raquel Barco,Dominic Gribben,Mohammad Dib,Alba Villarino,Aser Cortines,Román Orús*

Main category: cs.NI

TL;DR: 本文探索QML和QI技术在优化电信系统端到端网络服务的应用，采用混合框架，以云游戏服务网络配置优化为例评估，显示QML有潜力但存在挑战。


<details>
  <summary>Details</summary>
Motivation: 探索QML和QI技术在电信系统端到端网络服务优化的应用，并与经典ML方法对比。

Method: 采用结合量子和经典计算的混合框架，框架包含QoE估计器和优化器，针对云游戏服务网络配置优化实现，通过多种性能指标评估。

Result: QML模型在估计上与经典ML模型精度相似或更优，减少推理和加载时间，高维数据表现更好。

Conclusion: QML在网络优化有潜力，但数据可用性和量子与经典ML集成复杂性是未来研究方向。

Abstract: This work explores the integration of Quantum Machine Learning (QML) and
Quantum-Inspired (QI) techniques for optimizing end-to-end (E2E) network
services in telecommunication systems, particularly focusing on 5G networks and
beyond. The application of QML and QI algorithms is investigated, comparing
their performance with classical Machine Learning (ML) approaches. The present
study employs a hybrid framework combining quantum and classical computing
leveraging the strengths of QML and QI, without the penalty of quantum hardware
availability. This is particularized for the optimization of the Quality of
Experience (QoE) over cellular networks. The framework comprises an estimator
for obtaining the expected QoE based on user metrics, service settings, and
cell configuration, and an optimizer that uses the estimation to choose the
best cell and service configuration. Although the approach is applicable to any
QoE-based network management, its implementation is particularized for the
optimization of network configurations for Cloud Gaming services. Then, it is
evaluated via performance metrics such as accuracy and model loading and
inference times for the estimator, and time to solution and solution score for
the optimizer. The results indicate that QML models achieve similar or superior
accuracy to classical ML models for estimation, while decreasing inference and
loading times. Furthermore, potential for better performance is observed for
higher-dimensional data, highlighting promising results for higher complexity
problems. Thus, the results demonstrate the promising potential of QML in
advancing network optimization, although challenges related to data
availability and integration complexities between quantum and classical ML are
identified as future research lines.

</details>


### [501] [On Transferring, Merging, and Splitting Task-Oriented Network Digital Twins](https://arxiv.org/abs/2509.02551)
*Zifan Zhang,Minghong Fang,Mingzhe Chen,Yuchen Liu*

Main category: cs.NI

TL;DR: 本文探讨统一孪生变换（UTT）框架下网络数字孪生（NDT）的内部和相互操作，优化资源利用并降低创建成本，评估证明其可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 构建精确NDT面临整合数据源、映射属性和保持可扩展性等挑战，以往工作多从零开始创建和映射NDT，本文探索新计算范式。

Method: 在UTT框架下探索NDT的内部和相互操作，利用联合多模态和分布式映射机制，对分布式映射问题进行理论分析。

Result: 优化了资源利用，降低了创建NDT的成本，确保了孪生模型一致性，在实际孪生辅助应用评估中证明了NDT互操作性的可行性和有效性。

Conclusion: UTT框架下NDT的互操作是一种可行且有效的新计算范式，可用于相应任务开发。

Abstract: The integration of digital twinning technologies is driving next-generation
networks toward new capabilities, allowing operators to thoroughly understand
network conditions, efficiently analyze valuable radio data, and innovate
applications through user-friendly, immersive interfaces. Building on this
foundation, network digital twins (NDTs) accurately depict the operational
processes and attributes of network infrastructures, facilitating predictive
management through real-time analysis and measurement. However, constructing
precise NDTs poses challenges, such as integrating diverse data sources,
mapping necessary attributes from physical networks, and maintaining
scalability for various downstream tasks. Unlike previous works that focused on
the creation and mapping of NDTs from scratch, we explore intra- and
inter-operations among NDTs within a Unified Twin Transformation (UTT)
framework, which uncovers a new computing paradigm for efficient transfer,
merging, and splitting of NDTs to create task-oriented twins. By leveraging
joint multi-modal and distributed mapping mechanisms, UTT optimizes resource
utilization and reduces the cost of creating NDTs, while ensuring twin model
consistency. A theoretical analysis of the distributed mapping problem is
conducted to establish convergence bounds for this multi-modal gated
aggregation process. Evaluations on real-world twin-assisted applications, such
as trajectory reconstruction, human localization, and sensory data generation,
demonstrate the feasibility and effectiveness of interoperability among NDTs
for corresponding task development.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [502] [Lightning Fast Caching-based Parallel Denoising Prediction for Accelerating Talking Head Generation](https://arxiv.org/abs/2509.00052)
*Jianzhi Long,Wenhao Sun,Rongcheng Tu,Dacheng Tao*

Main category: cs.GR

TL;DR: 提出针对基于扩散的会说话头像模型的加速框架，提升推理速度并保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的会说话头像模型推理慢，现有通用加速方法无法利用会说话头像生成的时空冗余。

Method: 提出LightningCP，缓存静态特征并实现并行预测；提出DFA，加速注意力计算并移除部分参考特征。

Result: 大量实验表明该框架显著提升推理速度，同时保持视频质量。

Conclusion: 所提任务特定框架能有效解决会说话头像模型推理效率问题。

Abstract: Diffusion-based talking head models generate high-quality, photorealistic
videos but suffer from slow inference, limiting practical applications.
Existing acceleration methods for general diffusion models fail to exploit the
temporal and spatial redundancies unique to talking head generation. In this
paper, we propose a task-specific framework addressing these inefficiencies
through two key innovations. First, we introduce Lightning-fast Caching-based
Parallel denoising prediction (LightningCP), caching static features to bypass
most model layers in inference time. We also enable parallel prediction using
cached features and estimated noisy latents as inputs, efficiently bypassing
sequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to
further accelerate attention computations, exploiting the spatial decoupling in
talking head videos to restrict attention to dynamic foreground regions.
Additionally, we remove reference features in certain layers to bring extra
speedup. Extensive experiments demonstrate that our framework significantly
improves inference speed while preserving video quality.

</details>


### [503] [LatentEdit: Adaptive Latent Control for Consistent Semantic Editing](https://arxiv.org/abs/2509.00541)
*Siyi Liu,Weiming Chen,Yushun Tang,Zhihai He*

Main category: cs.GR

TL;DR: 提出LatentEdit框架用于图像编辑，在保真度和可编辑性间达平衡，提升实时部署效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式图像编辑难以在保持背景相似性时实现高质量编辑，且兼顾速度和内存效率。

Method: 引入自适应潜在融合框架LatentEdit，动态结合当前潜在代码与源图像反转的参考潜在代码，选择性保留高相似度区域特征。

Result: 在PIE - Bench数据集实验中，LatentEdit在保真度和可编辑性间达最优平衡，优于现有方法；无反转变体减少神经函数评估次数和中间变量存储。

Conclusion: LatentEdit是轻量级、即插即用解决方案，兼容多种架构，提升实时部署效率。

Abstract: Diffusion-based Image Editing has achieved significant success in recent
years. However, it remains challenging to achieve high-quality image editing
while maintaining the background similarity without sacrificing speed or memory
efficiency. In this work, we introduce LatentEdit, an adaptive latent fusion
framework that dynamically combines the current latent code with a reference
latent code inverted from the source image. By selectively preserving source
features in high-similarity, semantically important regions while generating
target content in other regions guided by the target prompt, LatentEdit enables
fine-grained, controllable editing. Critically, the method requires no internal
model modifications or complex attention mechanisms, offering a lightweight,
plug-and-play solution compatible with both UNet-based and DiT-based
architectures. Extensive experiments on the PIE-Bench dataset demonstrate that
our proposed LatentEdit achieves an optimal balance between fidelity and
editability, outperforming the state-of-the-art method even in 8-15 steps.
Additionally, its inversion-free variant further halves the number of neural
function evaluations and eliminates the need for storing any intermediate
variables, substantially enhancing real-time deployment efficiency.

</details>


### [504] [HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices](https://arxiv.org/abs/2509.01839)
*Akis Nousias,Stavros Nousias*

Main category: cs.GR

TL;DR: 文章提出新方法，调整Transformer架构以近似Hodge矩阵，实现高效架构，用于网格分割和分类任务且无需昂贵特征值分解操作。


<details>
  <summary>Details</summary>
Motivation: 现有用于形状分析任务的Transformer架构在图和网格上使用的传统注意力层依赖昂贵的特征值分解方法，为解决此问题提出新方法。

Method: 受离散外微分中Hodge拉普拉斯算子构造启发，调整Transformer架构，利用多头注意力机制近似Hodge矩阵，学习离散算子。

Result: 得到计算高效的架构，在网格分割和分类任务中取得可比性能。

Conclusion: 该方法能消除昂贵的特征值分解操作和复杂预处理操作，实现直接学习框架。

Abstract: Currently, prominent Transformer architectures applied on graphs and meshes
for shape analysis tasks employ traditional attention layers that heavily
utilize spectral features requiring costly eigenvalue decomposition-based
methods. To encode the mesh structure, these methods derive positional
embeddings, that heavily rely on eigenvalue decomposition based operations,
e.g. on the Laplacian matrix, or on heat-kernel signatures, which are then
concatenated to the input features. This paper proposes a novel approach
inspired by the explicit construction of the Hodge Laplacian operator in
Discrete Exterior Calculus as a product of discrete Hodge operators and
exterior derivatives, i.e. $(L := \star_0^{-1} d_0^T \star_1 d_0)$. We adjust
the Transformer architecture in a novel deep learning layer that utilizes the
multi-head attention mechanism to approximate Hodge matrices $\star_0$,
$\star_1$ and $\star_2$ and learn families of discrete operators $L$ that act
on mesh vertices, edges and faces. Our approach results in a
computationally-efficient architecture that achieves comparable performance in
mesh segmentation and classification tasks, through a direct learning
framework, while eliminating the need for costly eigenvalue decomposition
operations or complex preprocessing operations.

</details>


### [505] [Think2Sing: Orchestrating Structured Motion Subtitles for Singing-Driven 3D Head Animation](https://arxiv.org/abs/2509.02278)
*Zikai Huang,Yihan Zhou,Xuemiao Xu,Cheng Xu,Xiaofen Xing,Jing Qin,Shengfeng He*

Main category: cs.GR

TL;DR: 提出Think2Sing框架生成3D头部动画，有创新且效果好


<details>
  <summary>Details</summary>
Motivation: 现有语音驱动方法用于唱歌动画效果不佳，需生成语义连贯、时间一致的3D头部动画

Method: 提出基于扩散的Think2Sing框架，引入运动字幕，将任务设为运动强度预测问题，创建多模态唱歌数据集

Result: Think2Sing在真实感、表现力和情感保真度上优于现有方法，可灵活编辑动画

Conclusion: Think2Sing能有效解决唱歌驱动3D头部动画问题，有良好表现和编辑灵活性

Abstract: Singing-driven 3D head animation is a challenging yet promising task with
applications in virtual avatars, entertainment, and education. Unlike speech,
singing involves richer emotional nuance, dynamic prosody, and lyric-based
semantics, requiring the synthesis of fine-grained, temporally coherent facial
motion. Existing speech-driven approaches often produce oversimplified,
emotionally flat, and semantically inconsistent results, which are insufficient
for singing animation. To address this, we propose Think2Sing, a
diffusion-based framework that leverages pretrained large language models to
generate semantically coherent and temporally consistent 3D head animations,
conditioned on both lyrics and acoustics. A key innovation is the introduction
of motion subtitles, an auxiliary semantic representation derived through a
novel Singing Chain-of-Thought reasoning process combined with acoustic-guided
retrieval. These subtitles contain precise timestamps and region-specific
motion descriptions, serving as interpretable motion priors. We frame the task
as a motion intensity prediction problem, enabling finer control over facial
regions and improving the modeling of expressive motion. To support this, we
create a multimodal singing dataset with synchronized video, acoustic
descriptors, and motion subtitles, enabling diverse and expressive motion
learning. Extensive experiments show that Think2Sing outperforms
state-of-the-art methods in realism, expressiveness, and emotional fidelity,
while also offering flexible, user-controllable animation editing.

</details>


### [506] [Unifi3D: A Study on 3D Representations for Generation and Reconstruction in a Common Framework](https://arxiv.org/abs/2509.02474)
*Nina Wiedemann,Sainan Liu,Quentin Leboutet,Katelyn Gao,Benjamin Ummenhofer,Michael Paulitsch,Kai Yuan*

Main category: cs.GR

TL;DR: 本文提出统一评估框架评估3D表示在重建和生成方面的性能，分析各步骤，发现重建误差影响大，为应用选模型提供见解。


<details>
  <summary>Details</summary>
Motivation: 3D表示多样且分散，需统一评估框架来评估其在重建和生成方面的性能。

Method: 提出统一评估框架，基于质量、计算效率和泛化性能等多标准比较3D表示，实验覆盖3D生成流程各步骤。

Result: 发现重建误差显著影响整体性能。

Conclusion: 应联合评估生成和重建，研究结果为不同应用选择合适3D模型提供参考，利于开发更稳健和特定应用的3D生成解决方案。

Abstract: Following rapid advancements in text and image generation, research has
increasingly shifted towards 3D generation. Unlike the well-established
pixel-based representation in images, 3D representations remain diverse and
fragmented, encompassing a wide variety of approaches such as voxel grids,
neural radiance fields, signed distance functions, point clouds, or octrees,
each offering distinct advantages and limitations. In this work, we present a
unified evaluation framework designed to assess the performance of 3D
representations in reconstruction and generation. We compare these
representations based on multiple criteria: quality, computational efficiency,
and generalization performance. Beyond standard model benchmarking, our
experiments aim to derive best practices over all steps involved in the 3D
generation pipeline, including preprocessing, mesh reconstruction, compression
with autoencoders, and generation. Our findings highlight that reconstruction
errors significantly impact overall performance, underscoring the need to
evaluate generation and reconstruction jointly. We provide insights that can
inform the selection of suitable 3D models for various applications,
facilitating the development of more robust and application-specific solutions
in 3D generation. The code for our framework is available at
https://github.com/isl-org/unifi3d.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [507] [How to Make Museums More Interactive? Case Study of Artistic Chatbot](https://arxiv.org/abs/2509.00572)
*Filip J. Kucia,Bartosz Grabek,Szymon D. Trochimiak,Anna Wróblewska*

Main category: cs.HC

TL;DR: 研究提出Artistic Chatbot语音聊天系统，用于艺术展览支持非正式学习和提升游客参与度，探讨系统架构、交互设计和部署挑战，发现其能有效保持回复与展览内容相关。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的对话代理在实体学习环境如文化遗产地、博物馆和艺术画廊的应用未充分探索，研究旨在填补该空白。

Method: 开发Artistic Chatbot语音聊天系统，利用特定领域知识库回答波兰语口语问题，分析系统架构和用户交互设计。

Result: 交互分析显示，即使面对目标领域外的不可预测查询，Artistic Chatbot仍能使60%的回复与展览内容直接相关。

Conclusion: Artistic Chatbot等聊天机器人有增加公共文化场所互动性的潜力。

Abstract: Conversational agents powered by Large Language Models (LLMs) are
increasingly utilized in educational settings, in particular in individual
closed digital environments, yet their potential adoption in the physical
learning environments like cultural heritage sites, museums, and art galleries
remains relatively unexplored. In this study, we present Artistic Chatbot, a
voice-to-voice RAG-powered chat system to support informal learning and enhance
visitor engagement during a live art exhibition celebrating the 15th
anniversary of the Faculty of Media Art at the Warsaw Academy of Fine Arts,
Poland. The question answering (QA) chatbot responded to free-form spoken
questions in Polish using the context retrieved from a curated, domain-specific
knowledge base consisting of 226 documents provided by the organizers,
including faculty information, art magazines, books, and journals. We describe
the key aspects of the system architecture and user interaction design, as well
as discuss the practical challenges associated with deploying chatbots at
public cultural sites. Our findings, based on interaction analysis, demonstrate
that chatbots such as Artistic Chatbot effectively maintain responses grounded
in exhibition content (60\% of responses directly relevant), even when faced
with unpredictable queries outside the target domain, showing their potential
for increasing interactivity in public cultural sites.
  GitHub project page: https://github.com/cinekucia/artistic-chatbot-cikm2025

</details>


### [508] [Queuing for Civility: Regulating Emotions and Reducing Toxicity in Digital Discourse](https://arxiv.org/abs/2509.00696)
*Akriti Verma,Shama Islam,Valeh Moghaddam,Adnan Anwar*

Main category: cs.HC

TL;DR: 提出基于图的框架和评论排队机制调节线上对话情绪，分析数据表明可减少毒性和愤怒传播，提升线上环境健康。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注事后调节，忽略线上对话实时情绪动态及用户情绪相互影响，本文旨在解决线上毒性问题。

Method: 提出基于图的框架识别情绪调节需求，提出评论排队机制应对故意挑衅者。

Result: 基于图的框架使毒性降低12%，评论排队机制使愤怒传播减少15%，平均仅4%评论被暂时扣留。

Conclusion: 实时情绪调节与延迟审核结合可显著提升线上环境的健康。

Abstract: The pervasiveness of online toxicity, including hate speech and trolling,
disrupts digital interactions and online well-being. Previous research has
mainly focused on post-hoc moderation, overlooking the real-time emotional
dynamics of online conversations and the impact of users' emotions on others.
This paper presents a graph-based framework to identify the need for emotion
regulation within online conversations. This framework promotes self-reflection
to manage emotional responses and encourage responsible behaviour in real time.
Additionally, a comment queuing mechanism is proposed to address intentional
trolls who exploit emotions to inflame conversations. This mechanism introduces
a delay in publishing comments, giving users time to self-regulate before
further engaging in the conversation and helping maintain emotional balance.
Analysis of social media data from Twitter and Reddit demonstrates that the
graph-based framework reduced toxicity by 12%, while the comment queuing
mechanism decreased the spread of anger by 15%, with only 4% of comments being
temporarily held on average. These findings indicate that combining real-time
emotion regulation with delayed moderation can significantly improve well-being
in online environments.

</details>


### [509] [Why it is worth making an effort with GenAI](https://arxiv.org/abs/2509.00852)
*Yvonne Rogers*

Main category: cs.HC

TL;DR: 本文探讨学生过度依赖生成式AI完成作业问题，提出设计需更多努力使用的AI工具促进学习并讨论增强人类认知工具设计。


<details>
  <summary>Details</summary>
Motivation: 学生过度依赖生成式AI做作业，担忧影响写作和批判性思维能力发展，需逆转该趋势。

Method: 提出设计需更多努力使用的新AI工具，如结合GenAI与传统学习方法，探讨增强人类认知工具设计。

Result: 未提及明确具体结果。

Conclusion: 可设计新AI工具让学生在使用时付出更多努力，从而加深学习体验，掌握元认知和反思技能。

Abstract: Students routinely use ChatGPT and the like now to help them with their
homework, such as writing an essay. It takes less effort to complete and is
easier to do than by hand. It can even produce as good if not better output
than the student's own work. However, there is a growing concern that
over-reliance on using GenAI in this way will stifle the development of
learning writing and critical thinking skills. How might this trend be
reversed? What if students were required to make more effort when using GenAI
to do their homework? It might be more challenging, but the additional effort
involved could result in them learning more and having a greater sense of
achievement. This tension can be viewed as a form of effort paradox; where
effort is both viewed as something to be avoided but at the same time is
valued. Is it possible to let students learn sometimes with less and other
times more effort? Students are already adept at the former but what about the
latter? Could we design new kinds of AI tools that deliberately require more
effort to use to deepen the learning experience? In this paper, I begin to
outline what form these might take, for example, asking students to use a
combination of GenAI tools with traditional learning approaches (e.g.
note-taking while reading). I also discuss how else to design tools to think
with that augments human cognition; where students learn more the skills of
metacognition and reflection.

</details>


### [510] [Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces](https://arxiv.org/abs/2509.01051)
*Matte Lim,Catherine Yeh,Martin Wattenberg,Fernanda Viégas,Panagiotis Michalatos*

Main category: cs.HC

TL;DR: 提出结合力投影与流聚类方法的可视化技术，创建Chronotome工具探索时间数据主题，通过用例展示其效用。


<details>
  <summary>Details</summary>
Motivation: 现有降维方法难以捕捉现实数据随时间的语义变化。

Method: 结合力投影和流聚类方法构建嵌入的时空图。

Result: 创建了Chronotome工具，可实时交互式探索时间数据主题。

Conclusion: 该方法为理解时间数据集的美学和语义提供新视角。

Abstract: Many real-world datasets -- from an artist's body of work to a person's
social media history -- exhibit meaningful semantic changes over time that are
difficult to capture with existing dimensionality reduction methods. To address
this gap, we introduce a visualization technique that combines force-based
projection and streaming clustering methods to build a spatial-temporal map of
embeddings. Applying this technique, we create Chronotome, a tool for
interactively exploring evolving themes in time-based data -- in real time. We
demonstrate the utility of our approach through use cases on text and image
data, showing how it offers a new lens for understanding the aesthetics and
semantics of temporal datasets.

</details>


### [511] [Community-Centered Spatial Intelligence for Climate Adaptation at Nova Scotia's Eastern Shore](https://arxiv.org/abs/2509.01845)
*Gabriel Spadon,Oladapo Oyebode,Camilo M. Botero,Tushar Sharma,Floris Goerlandt,Ronald Pelot*

Main category: cs.HC

TL;DR: 本文介绍以人类为中心的项目，联合多学科知识与社区共创工具，构建数字档案，助力新斯科舍省东海岸地区应对气候变化。


<details>
  <summary>Details</summary>
Motivation: 新斯科舍省东海岸乡村地区面临气候变化的生存威胁，传统应对方式不足。

Method: 融合计算机科学、工业工程和海岸地理学等多学科知识，通过东海岸公民科学海岸监测网络整合居民经验，采用学生团队与居民直接合作的模式。

Result: 提出详细项目时间表和可复制模型。

Conclusion: 该技术支持模式能让传统社区更有效应对气候转型。

Abstract: This paper presents an overview of a human-centered initiative aimed at
strengthening climate resilience along Nova Scotia's Eastern Shore. This
region, a collection of rural villages with deep ties to the sea, faces
existential threats from climate change that endanger its way of life. Our
project moves beyond a purely technical response, weaving together expertise
from Computer Science, Industrial Engineering, and Coastal Geography to
co-create tools with the community. By integrating generational knowledge of
residents, particularly elders, through the Eastern Shore Citizen Science
Coastal Monitoring Network, this project aims to collaborate in building a
living digital archive. This effort is hosted under Dalhousie University's
Transforming Climate Action (TCA) initiative, specifically through its
Transformative Adaptations to Social-Ecological Climate Change Trajectories
(TranSECT) and TCA Artificial Intelligence (TCA-AI) projects. This work is
driven by a collaboration model in which student teams work directly with
residents. We present a detailed project timeline and a replicable model for
how technology can support traditional communities, enabling them to navigate
climate transformation more effectively.

</details>


### [512] [A Theoretical Framework of the Processes of Change in Psychotherapy Delivered by Artificial Agents](https://arxiv.org/abs/2509.02144)
*Arthur Bran Herbener,Malene Flensborg Damholdt*

Main category: cs.HC

TL;DR: 文章提出人工代理心理治疗改变过程的理论框架，指出人类治疗师地位的作用，分析人工代理存在的差距，给出研究和应用方向并讨论其代理性质复杂性。


<details>
  <summary>Details</summary>
Motivation: 近期大语言模型推出后，人工代理能否取代人类治疗师受关注，但人工代理心理治疗改变过程未知，为促进假设发展和科学辩论。

Method: 对与人类治疗师相关的有效成分进行概念分析。

Result: 提出人类治疗师的本体和社会文化地位对治疗结果起关键作用，人工代理会出现真诚差距和可信度差距，还提出研究和应用途径。

Conclusion: 强调人工代理的复杂代理性质使建立通用命题更复杂。

Abstract: The question of whether artificial agents (e.g., chatbots and social robots)
can replace human therapists has received notable attention following the
recent launch of large language models. However, little is known about the
processes of change in psychotherapy delivered by artificial agents. To
facilitate hypothesis development and stimulate scientific debate, the present
article offers the first theoretical framework of the processes of change in
psychotherapy delivered by artificial agents. The theoretical framework rests
upon a conceptual analysis of what active ingredients may be inherently linked
to the presence of human therapists. We propose that human therapists'
ontological status as human beings and sociocultural status as socially
sanctioned healthcare professionals play crucial roles in promoting treatment
outcomes. In the absence of the ontological and sociocultural status of human
therapists, we propose what we coin the genuineness gap and credibility gap can
emerge and undermine key processes of change in psychotherapy. Based on these
propositions, we propose avenues for scientific investigations and practical
applications aimed at leveraging the strengths of artificial agents and human
therapists respectively. We also highlight the intricate agentic nature of
artificial agents and discuss how this complicates endeavors to establish
universally applicable propositions regarding the processes of change in these
interventions.

</details>


### [513] [Look: AI at Work! - Analysing Key Aspects of AI-support at the Work Place](https://arxiv.org/abs/2509.02274)
*Stefan Schiffer,Anna Milena Rothermel,Alexander Ferrein,Astrid Rosenthal-von der Pütten*

Main category: cs.HC

TL;DR: 分析工作场所应用人工智能的技术和心理因素，以12个应用案例为例，给出技术建议并提出心理方面研究问题。


<details>
  <summary>Details</summary>
Motivation: 探讨工作场所应用人工智能的技术和心理因素，为未来工作系统集成人工智能提供参考。

Method: 对12个应用案例进行分析，从技术和心理两方面展开研究。

Result: 在技术方面得出开发AI应用的建议，在心理方面提出相关研究问题。

Conclusion: 开发AI应用需关注高质量数据和人类专业知识融入，心理层面要研究接受度、开放性和信任等问题。

Abstract: In this paper we present an analysis of technological and psychological
factors of applying artificial intelligence (AI) at the work place. We do so
for a number of twelve application cases in the context of a project where AI
is integrated at work places and in work systems of the future. From a
technological point of view we mainly look at the areas of AI that the
applications are concerned with. This allows to formulate recommendations in
terms of what to look at in developing an AI application and what to pay
attention to with regards to building AI literacy with different stakeholders
using the system. This includes the importance of high-quality data for
training learning-based systems as well as the integration of human expertise,
especially with knowledge-based systems. In terms of the psychological factors
we derive research questions to investigate in the development of AI supported
work systems and to consider in future work, mainly concerned with topics such
as acceptance, openness, and trust in an AI system.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [514] [Resting-state fMRI Analysis using Quantum Time-series Transformer](https://arxiv.org/abs/2509.00711)
*Junghoon Justin Park,Jungwoo Seo,Sangyoon Bae,Samuel Yen-Chi Chen,Huan-Hsin Tseng,Jiook Cha,Shinjae Yoo*

Main category: eess.IV

TL;DR: 提出量子时间序列变压器处理静息态功能磁共振成像数据，降低训练开销，在小样本场景表现好，能识别多动症神经生物标志物。


<details>
  <summary>Details</summary>
Motivation: 经典自注意力变压器模型存在二次复杂度、参数多和数据需求大的问题，需要改进以处理静息态功能磁共振成像数据。

Method: 引入量子时间序列变压器，利用线性组合的幺正算符和量子奇异值变换。

Result: 在大规模功能磁共振成像数据集上，量子时间序列变压器预测性能与经典模型相当或更优，在小样本场景优势明显，能可靠识别多动症神经生物标志物。

Conclusion: 量子增强变压器有望通过更有效地建模复杂时空动态和提高临床可解释性，推动计算神经科学发展。

Abstract: Resting-state functional magnetic resonance imaging (fMRI) has emerged as a
pivotal tool for revealing intrinsic brain network connectivity and identifying
neural biomarkers of neuropsychiatric conditions. However, classical
self-attention transformer models--despite their formidable representational
power--struggle with quadratic complexity, large parameter counts, and
substantial data requirements. To address these barriers, we introduce a
Quantum Time-series Transformer, a novel quantum-enhanced transformer
architecture leveraging Linear Combination of Unitaries and Quantum Singular
Value Transformation. Unlike classical transformers, Quantum Time-series
Transformer operates with polylogarithmic computational complexity, markedly
reducing training overhead and enabling robust performance even with fewer
parameters and limited sample sizes. Empirical evaluation on the largest-scale
fMRI datasets from the Adolescent Brain Cognitive Development Study and the UK
Biobank demonstrates that Quantum Time-series Transformer achieves comparable
or superior predictive performance compared to state-of-the-art classical
transformer models, with especially pronounced gains in small-sample scenarios.
Interpretability analyses using SHapley Additive exPlanations further reveal
that Quantum Time-series Transformer reliably identifies clinically meaningful
neural biomarkers of attention-deficit/hyperactivity disorder (ADHD). These
findings underscore the promise of quantum-enhanced transformers in advancing
computational neuroscience by more efficiently modeling complex spatio-temporal
dynamics and improving clinical interpretability.

</details>


### [515] [A Novel Method to Determine Total Oxidant Concentration Produced by Non-Thermal Plasma Based on Image Processing and Machine Learning](https://arxiv.org/abs/2509.00479)
*Mirkan Emir Sancak,Unal Sen,Ulker Diler Keris-Sen*

Main category: eess.IV

TL;DR: 本文提出基于颜色的计算机分析（CBCA）方法，结合图像处理与机器学习，准确测定非热等离子体处理水溶液中总氧化剂浓度，模型预测准确性高。


<details>
  <summary>Details</summary>
Motivation: 传统滴定法测定非热等离子体处理水溶液中总氧化剂浓度存在主观性，难以准确测定。

Method: 利用自建视觉数据采集系统捕捉KI溶液氧化时颜色变化视频，用标准滴定法监测总氧化剂浓度变化；处理视频帧提取颜色特征，统计评估后用其训练和验证五个机器学习模型。

Result: 颜色特征与总氧化剂浓度有强线性相关性，线性回归和梯度提升模型预测准确性高，减少特征集能提升预测效率，CBCA系统即使减少特征也能高精度预测。

Conclusion: CBCA方法能准确测定非热等离子体处理水溶液中总氧化剂浓度。

Abstract: Accurate determination of total oxidant concentration ([Ox]_{tot}) in
non-thermal plasma (NTP)-treated aqueous systems remains a critical challenge
due to the transient nature of reactive oxygen and nitrogen species and the
subjectivity of conventional titration methods used for [Ox]_{tot}
determination. This study introduces a novel, color-based computer analysis
(CBCA) method that integrates advanced image processing with machine learning
(ML) to quantify colorimetric shifts in potassium iodide (KI) solutions during
oxidation. First, a custom-built visual data acquisition system captured
high-resolution video of the color transitions in a KI solution during
oxidation with an NTP system. The change in [Ox]_{tot} during the experiments
was monitored with a standard titrimetric method. Second, the captured frames
were processed using a robust image processing pipeline to extract RGB, HSV,
and Lab color features. The extracted features were statistically evaluated,
and the results revealed strong linear correlations with the measured
[Ox]_{tot} values, particularly in the saturation (HSV), a and b (Lab), and
blue (RGB) channels. Subsequently, the [Ox]_{tot} measurements and the
extracted color features were used to train and validate five ML models. Among
them, linear regression and gradient boosting models achieved the highest
predictive accuracy (R^2 > 0.990). It was also found that reducing the feature
set from nine to four resulted in comparable performance with improved
prediction efficiency, especially for gradient boosting. Finally, comparison of
the model predictions with real titration measurements revealed that the CBCA
system successfully predicts the [Ox]_{tot} in KI solution with high accuracy
(R^2 > 0.998) even with a reduced number of features.

</details>


### [516] [Can General-Purpose Omnimodels Compete with Specialists? A Case Study in Medical Image Segmentation](https://arxiv.org/abs/2509.00866)
*Yizhe Zhang,Qiang Chen,Tao Zhou*

Main category: eess.IV

TL;DR: 研究通用全模态模型在医学图像分割领域与特定领域模型的性能对比，发现表现因任务而异，全模态模型有独特优势可互补。


<details>
  <summary>Details</summary>
Motivation: 探讨强大的通用全模态模型在知识密集型领域能否与高度专业化模型表现相当，以医学图像分割领域为例进行研究。

Method: 对最先进的全模态模型（Gemini 2.5 Pro）和特定领域深度学习模型在息肉、视网膜血管和乳腺肿瘤分割三项任务上进行零样本性能的对比研究，按特定领域模型的准确率筛选出简单和困难样本子集。

Result: 在息肉和乳腺肿瘤分割中，特定领域模型在简单样本上表现出色，全模态模型在困难样本上更稳健；在视网膜血管分割中，特定领域模型在简单和困难样本上均表现更优；定性分析显示全模态模型可能有更高敏感性。

Conclusion: 当前全模态模型尚不能完全替代特定领域模型，但在处理困难边缘情况时可与特定领域模型互补。

Abstract: The emergence of powerful, general-purpose omnimodels capable of processing
diverse data modalities has raised a critical question: can these
``jack-of-all-trades'' systems perform on par with highly specialized models in
knowledge-intensive domains? This work investigates this question within the
high-stakes field of medical image segmentation. We conduct a comparative study
analyzing the zero-shot performance of a state-of-the-art omnimodel (Gemini 2.5
Pro, the ``Nano Banana'' model) against domain-specific deep learning models on
three distinct tasks: polyp (endoscopy), retinal vessel (fundus), and breast
tumor segmentation (ultrasound). Our study focuses on performance at the
extremes by curating subsets of the ``easiest'' and ``hardest'' cases based on
the specialist models' accuracy. Our findings reveal a nuanced and
task-dependent landscape. For polyp and breast tumor segmentation, specialist
models excel on easy samples, but the omnimodel demonstrates greater robustness
on hard samples where specialists fail catastrophically. Conversely, for the
fine-grained task of retinal vessel segmentation, the specialist model
maintains superior performance across both easy and hard cases. Intriguingly,
qualitative analysis suggests omnimodels may possess higher sensitivity,
identifying subtle anatomical features missed by human annotators. Our results
indicate that while current omnimodels are not yet a universal replacement for
specialists, their unique strengths suggest a potential complementary role with
specialist models, particularly in enhancing robustness on challenging edge
cases.

</details>


### [517] [DRetNet: A Novel Deep Learning Framework for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.01072)
*Idowu Paul Okuwobi,Jingyuan Liu,Jifeng Wan,Jiaojiao Jiang*

Main category: eess.IV

TL;DR: 本文提出用于糖尿病视网膜病变（DR）检测的新框架，含三项创新，性能佳且临床相关性高，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前自动DR检测系统在处理低质量图像、缺乏可解释性和领域知识整合方面存在挑战，需新方法解决。

Method: 引入新框架，包含基于物理信息神经网络的自适应视网膜图像增强、混合特征融合网络、带不确定性量化的多阶段分类器。

Result: 框架在准确率、精确率等指标表现良好，眼科医生认为其预测临床相关性高，定性分析增强系统可解释性和可信度，在多种条件下表现稳健。

Conclusion: 该框架是临床应用的有前景工具，能在资源有限环境实现更准确可靠的DR检测。

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide,
necessitating early detection to prevent vision loss. Current automated DR
detection systems often struggle with poor-quality images, lack
interpretability, and insufficient integration of domain-specific knowledge. To
address these challenges, we introduce a novel framework that integrates three
innovative contributions: (1) Adaptive Retinal Image Enhancement Using
Physics-Informed Neural Networks (PINNs): this technique dynamically enhances
retinal images by incorporating physical constraints, improving the visibility
of critical features such as microaneurysms, hemorrhages, and exudates; (2)
Hybrid Feature Fusion Network (HFFN): by combining deep learning embeddings
with handcrafted features, HFFN leverages both learned representations and
domain-specific knowledge to enhance generalization and accuracy; (3)
Multi-Stage Classifier with Uncertainty Quantification: this method breaks down
the classification process into logical stages, providing interpretable
predictions and confidence scores, thereby improving clinical trust. The
proposed framework achieves an accuracy of 92.7%, a precision of 92.5%, a
recall of 92.6%, an F1-score of 92.5%, an AUC of 97.8%, a mAP of 0.96, and an
MCC of 0.85. Ophthalmologists rated the framework's predictions as highly
clinically relevant (4.8/5), highlighting its alignment with real-world
diagnostic needs. Qualitative analyses, including Grad-CAM visualizations and
uncertainty heatmaps, further enhance the interpretability and trustworthiness
of the system. The framework demonstrates robust performance across diverse
conditions, including low-quality images, noisy data, and unseen datasets.
These features make the proposed framework a promising tool for clinical
adoption, enabling more accurate and reliable DR detection in resource-limited
settings.

</details>


### [518] [Towards Early Detection: AI-Based Five-Year Forecasting of Breast Cancer Risk Using Digital Breast Tomosynthesis Imaging](https://arxiv.org/abs/2509.00900)
*Manon A. Dorster,Felix J. Dorfner,Mason C. Cleveland,Melisa S. Guelen,Jay Patel,Dania Daye,Jean-Philippe Thiran,Albert E. Kim,Christopher P. Bridge*

Main category: eess.IV

TL;DR: 本文提出基于深度学习的框架，利用大量DBT检查数据预测患者5年患乳腺癌风险，模型在测试集表现良好，显示出DBT结合DL方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 早期检测乳腺癌利于治疗，当前风险预测模型表现一般且未纳入DBT成像，需优化乳腺癌筛查。

Method: 使用Meta AI DINOv2图像编码器提取特征，结合累积风险层，基于161,753次DBT检查的大规模数据集训练风险预测器。

Result: 最佳模型在测试集上对5年内预测的AUROC达到0.80。

Conclusion: 基于DBT的深度学习方法有潜力补充传统风险评估工具，值得进一步验证和改进。

Abstract: As early detection of breast cancer strongly favors successful therapeutic
outcomes, there is major commercial interest in optimizing breast cancer
screening. However, current risk prediction models achieve modest performance
and do not incorporate digital breast tomosynthesis (DBT) imaging, which was
FDA-approved for breast cancer screening in 2011. To address this unmet need,
we present a deep learning (DL)-based framework capable of forecasting an
individual patient's 5-year breast cancer risk directly from screening DBT.
Using an unparalleled dataset of 161,753 DBT examinations from 50,590 patients,
we trained a risk predictor based on features extracted using the Meta AI
DINOv2 image encoder, combined with a cumulative hazard layer, to assess a
patient's likelihood of developing breast cancer over five years. On a held-out
test set, our best-performing model achieved an AUROC of 0.80 on predictions
within 5 years. These findings reveal the high potential of DBT-based DL
approaches to complement traditional risk assessment tools, and serve as a
promising basis for additional investigation to validate and enhance our work.

</details>


### [519] [Temporal Representation Learning for Real-Time Ultrasound Analysis](https://arxiv.org/abs/2509.01433)
*Yves Stebler,Thomas M. Sutter,Ece Ozkan,Julia E. Vogt*

Main category: eess.IV

TL;DR: 本文提出从超声视频学习有效时间表征的方法，用于超声心动图射血分数估计，在数据集上提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型常忽略超声序列的时间连续性，分析帧时独立进行，遗漏关键时间依赖。

Method: 利用时间一致的掩码和对比学习，增强视频帧间的时间连贯性。

Result: 在EchoNet - Dynamic数据集上，射血分数预测准确性大幅提升。

Conclusion: 时间感知表征学习对实时超声分析很重要。

Abstract: Ultrasound (US) imaging is a critical tool in medical diagnostics, offering
real-time visualization of physiological processes. One of its major advantages
is its ability to capture temporal dynamics, which is essential for assessing
motion patterns in applications such as cardiac monitoring, fetal development,
and vascular imaging. Despite its importance, current deep learning models
often overlook the temporal continuity of ultrasound sequences, analyzing
frames independently and missing key temporal dependencies. To address this
gap, we propose a method for learning effective temporal representations from
ultrasound videos, with a focus on echocardiography-based ejection fraction
(EF) estimation. EF prediction serves as an ideal case study to demonstrate the
necessity of temporal learning, as it requires capturing the rhythmic
contraction and relaxation of the heart. Our approach leverages temporally
consistent masking and contrastive learning to enforce temporal coherence
across video frames, enhancing the model's ability to represent motion
patterns. Evaluated on the EchoNet-Dynamic dataset, our method achieves a
substantial improvement in EF prediction accuracy, highlighting the importance
of temporally-aware representation learning for real-time ultrasound analysis.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [520] [Traj-MLLM: Can Multimodal Large Language Models Reform Trajectory Data Mining?](https://arxiv.org/abs/2509.00053)
*Shuo Liu,Di Yao,Yan Lin,Gao Cong,Jingping Bi*

Main category: cs.MM

TL;DR: 本文提出Traj - MLLM框架用于轨迹数据挖掘，不依赖训练数据和微调骨干模型，在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹分析模型存在泛化问题，MLLMs有潜力解决但存在轨迹模态差距等基础挑战。

Method: 提出Traj - MLLM框架，将原始轨迹转换为图像 - 文本序列，利用MLLMs推理能力，并提出提示优化方法进行任务适配。

Result: 在四个公开数据集上，Traj - MLLM在旅行时间估计、移动性预测、异常检测和交通模式识别任务上分别比现有方法高48.05%、15.52%、51.52%、1.83%。

Conclusion: Traj - MLLM无需训练数据和微调骨干模型，能有效解决轨迹数据挖掘的泛化问题。

Abstract: Building a general model capable of analyzing human trajectories across
different geographic regions and different tasks becomes an emergent yet
important problem for various applications. However, existing works suffer from
the generalization problem, \ie, they are either restricted to train for
specific regions or only suitable for a few tasks. Given the recent advances of
multimodal large language models (MLLMs), we raise the question: can MLLMs
reform current trajectory data mining and solve the problem? Nevertheless, due
to the modality gap of trajectory, how to generate task-independent multimodal
trajectory representations and how to adapt flexibly to different tasks remain
the foundational challenges. In this paper, we propose \texttt{Traj-MLLM}},
which is the first general framework using MLLMs for trajectory data mining. By
integrating multiview contexts, \texttt{Traj-MLLM}} transforms raw trajectories
into interleaved image-text sequences while preserving key spatial-temporal
characteristics, and directly utilizes the reasoning ability of MLLMs for
trajectory analysis. Additionally, a prompt optimization method is proposed to
finalize data-invariant prompts for task adaptation. Extensive experiments on
four publicly available datasets show that \texttt{Traj-MLLM}} outperforms
state-of-the-art baselines by $48.05\%$, $15.52\%$, $51.52\%$, $1.83\%$ on
travel time estimation, mobility prediction, anomaly detection and
transportation mode identification, respectively. \texttt{Traj-MLLM}} achieves
these superior performances without requiring any training data or fine-tuning
the MLLM backbones.

</details>


### [521] [LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition](https://arxiv.org/abs/2509.01337)
*Qianrui Zhou,Hua Xu,Yifan Wang,Xinzhi Dong,Hanlei Zhang*

Main category: cs.MM

TL;DR: 本文提出LLM - Guided Semantic Relational Reasoning (LGSRR)方法用于多模态意图理解，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模态层面存在依赖局限，难以进行细粒度语义的关系推理以实现复杂意图理解。

Method: 提出基于LLM的策略，利用浅到深的思维链提取细粒度语义作为后续推理的指导，形式化建模三种基本语义关系并分析其相互作用。

Result: 在多模态意图和对话行为识别任务的大量实验中，LGSRR优于现有方法，在不同语义理解场景中有稳定性能提升。

Conclusion: LGSRR方法能有效提升多模态意图理解中的关系推理性能。

Abstract: Understanding human intents from multimodal signals is critical for analyzing
human behaviors and enhancing human-machine interactions in real-world
scenarios. However, existing methods exhibit limitations in their
modality-level reliance, constraining relational reasoning over fine-grained
semantics for complex intent understanding. This paper proposes a novel
LLM-Guided Semantic Relational Reasoning (LGSRR) method, which harnesses the
expansive knowledge of large language models (LLMs) to establish semantic
foundations that boost smaller models' relational reasoning performance.
Specifically, an LLM-based strategy is proposed to extract fine-grained
semantics as guidance for subsequent reasoning, driven by a shallow-to-deep
Chain-of-Thought (CoT) that autonomously uncovers, describes, and ranks
semantic cues by their importance without relying on manually defined priors.
Besides, we formally model three fundamental types of semantic relations
grounded in logical principles and analyze their nuanced interplay to enable
more effective relational reasoning. Extensive experiments on multimodal intent
and dialogue act recognition tasks demonstrate LGSRR's superiority over
state-of-the-art methods, with consistent performance gains across diverse
semantic understanding scenarios. The complete data and code are available at
https://github.com/thuiar/LGSRR.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [522] [Double Descent and Overparameterization in Particle Physics Data](https://arxiv.org/abs/2509.01397)
*Matthias Vigl,Lukas Heinrich*

Main category: hep-ex

TL;DR: 首次在粒子物理数据中展现过参数化模型优势，探索‘双下降’现象出现情况和过参数化带来性能提升的条件。


<details>
  <summary>Details</summary>
Motivation: 在机器学习任务中观察到过参数化模型的益处，期望在粒子物理数据中进行验证和探索。

Method: 未提及

Result: 首次在粒子物理数据中展示了过参数化模型的相关行为。

Conclusion: 在粒子物理数据中过参数化模型也有性能提升效果，且探索了‘双下降’现象及性能提升条件。

Abstract: Recently, the benefit of heavily overparameterized models has been observed
in machine learning tasks: models with enough capacity to easily cross the
\emph{interpolation threshold} improve in generalization error compared to the
classical bias-variance tradeoff regime. We demonstrate this behavior for the
first time in particle physics data and explore when and where `double descent'
appears and under which circumstances overparameterization results in a
performance gain.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [523] [Newton-Flow Particle Filters based on Generalized Cramér Distance](https://arxiv.org/abs/2509.00182)
*Uwe D. Hanebeck*

Main category: cs.IT

TL;DR: 提出一种用于高维问题的递归粒子滤波器，该滤波器不会退化，实现简单且高效。


<details>
  <summary>Details</summary>
Motivation: 解决高维问题中粒子滤波器退化的问题。

Method: 用确定性低差异粒子集表示状态估计，在测量更新步骤通过同伦延拓逐步引入似然函数，推导粒子集间的广义Cramér距离并用牛顿流最小化该距离。

Result: 得到一种实现简单、高效的新滤波器。

Conclusion: 新滤波器可作为经典方法的替代。

Abstract: We propose a recursive particle filter for high-dimensional problems that
inherently never degenerates. The state estimate is represented by
deterministic low-discrepancy particle sets. We focus on the measurement update
step, where a likelihood function is used for representing the measurement and
its uncertainty. This likelihood is progressively introduced into the filtering
procedure by homotopy continuation over an artificial time. A generalized
Cram\'er distance between particle sets is derived in closed form that is
differentiable and invariant to particle order. A Newton flow then continually
minimizes this distance over artificial time and thus smoothly moves particles
from prior to posterior density. The new filter is surprisingly simple to
implement and very efficient. It just requires a prior particle set and a
likelihood function, never estimates densities from samples, and can be used as
a plugin replacement for classic approaches.

</details>


### [524] [Hierarchical Maximum Entropy via the Renormalization Group](https://arxiv.org/abs/2509.01424)
*Amir R. Asadi*

Main category: cs.IT

TL;DR: 引入“分层最大熵”框架处理多层次模型，借助重整化群程序得到帕累托最优分布，探索分层不变性简化程序并提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有统计、机器学习模型和物理系统中存在多层次结构，需拓展最大熵分布理论处理这些多层模型。

Method: 通过制定吉布斯变分原理和唐斯克 - 瓦拉丹熵变分表示的多层次扩展，引入参数流概念。

Result: 可通过重整化群程序得到跨层次变换中使熵最大化的帕累托最优分布，探索的分层不变性设置简化了重整化群程序。

Conclusion: 该工作将概率论、信息论和统计力学的思想联系起来。

Abstract: Hierarchical structures, which include multiple levels, are prevalent in
statistical and machine-learning models as well as physical systems. Extending
the foundational result that the maximum entropy distribution under mean
constraints is given by the exponential Gibbs-Boltzmann form, we introduce the
framework of "hierarchical maximum entropy" to address these multilevel models.
We demonstrate that Pareto optimal distributions, which maximize entropies
across all levels of hierarchical transformations, can be obtained via
renormalization-group procedures from theoretical physics. This is achieved by
formulating multilevel extensions of the Gibbs variational principle and the
Donsker-Varadhan variational representation of entropy. Moreover, we explore
settings with hierarchical invariances that significantly simplify the
renormalization-group procedures, enhancing computational efficiency: quadratic
modular loss functions, logarithmic loss functions, and nearest-neighbor loss
functions. This is accomplished through the introduction of the concept of
parameter flows, which serves as an analog to renormalization flows in
renormalization group theory. This work connects ideas from probability theory,
information theory, and statistical mechanics.

</details>


### [525] [Learning to Ask: Decision Transformers for Adaptive Quantitative Group Testing](https://arxiv.org/abs/2509.01723)
*Mahdi Soleymani,Tara Javidi*

Main category: cs.IT

TL;DR: 本文针对定量群测试（QGT）问题，将其转化为整数向量恢复任务，再用离线强化学习结合决策转换器自适应求解，实验首次让自适应算法的平均查询次数低于非自适应信息论界限。


<details>
  <summary>Details</summary>
Motivation: 信息论表明自适应方法可减少QGT问题所需查询次数，但尚无算法超越非自适应界限，其实用性未知。

Method: 将QGT问题转化为维度与稀疏性相关的整数向量恢复任务，将该任务表述为离线强化学习问题，用决策转换器自适应求解。

Result: 实验中自适应算法首次使平均查询次数低于非自适应信息论界限。

Conclusion: 自适应方法确实能减少QGT问题的查询次数。

Abstract: We consider the problem of quantitative group testing (QGT), where the goal
is to recover a sparse binary vector from aggregate subset-sum queries: each
query selects a subset of indices and returns the sum of those entries.
Information-theoretic results suggest that adaptivity could yield up to a
twofold reduction in the total number of required queries, yet no algorithm has
surpassed the non-adaptive bound, leaving its practical benefit an open
question. In this paper, we reduce the QGT problem to an integer-vector
recovery task whose dimension scales with the sparsity of the original problem
rather than its full ambient size. We then formulate this reduced recovery task
as an offline reinforcement learning problem and employ Decision Transformers
to solve it adaptively. By combining these two steps, we obtain an effective
end-to-end method for solving the QGT problem. Our experiments show that, for
the first time in the literature, our adaptive algorithm reduces the average
number of queries below the well-known non-adaptive information-theoretic
bound, demonstrating that adaptivity can indeed reduce the number of queries.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [526] [Modeling and benchmarking quantum optical neurons for efficient neural computation](https://arxiv.org/abs/2509.01784)
*Andrea Andrisani,Gennaro Vessio,Fabrizio Sgobba,Francesco Di Lena,Luigi Amato Santamaria,Giovanna Castellano*

Main category: physics.optics

TL;DR: 本文介绍基于HOM和MZ干涉仪的量子光学神经元（QON）架构，评估其在图像分类任务表现，部分配置性能可与经典神经元媲美。


<details>
  <summary>Details</summary>
Motivation: 利用光子干涉以节能且物理可行方式进行神经运算，探索QON作为未来量子启发神经架构组件的潜力。

Method: 引入基于HOM和MZ干涉仪的QON架构，结合不同光子调制策略，在软件中实现可微分模块，在MNIST和FashionMNIST数据集上训练评估。

Result: HOM振幅调制和MZ相移调制配置在部分场景性能与经典神经元相当，收敛更快更稳定；强度编码对分布变化更敏感，训练不稳定。

Conclusion: QON有潜力成为未来量子启发神经架构和混合光子 - 电子系统的高效可扩展组件。

Abstract: Quantum optical neurons (QONs) are emerging as promising computational units
that leverage photonic interference to perform neural operations in an
energy-efficient and physically grounded manner. Building on recent theoretical
proposals, we introduce a family of QON architectures based on Hong-Ou-Mandel
(HOM) and Mach-Zehnder (MZ) interferometers, incorporating different photon
modulation strategies -- phase, amplitude, and intensity. These physical setups
yield distinct pre-activation functions, which we implement as fully
differentiable modules in software. We evaluate these QONs both in isolation
and as building blocks of multilayer networks, training them on binary and
multiclass image classification tasks using the MNIST and FashionMNIST
datasets. Our experiments show that two configurations -- HOM-based amplitude
modulation and MZ-based phase-shifted modulation -- achieve performance
comparable to that of classical neurons in several settings, and in some cases
exhibit faster or more stable convergence. In contrast, intensity-based
encodings display greater sensitivity to distributional shifts and training
instabilities. These results highlight the potential of QONs as efficient and
scalable components for future quantum-inspired neural architectures and hybrid
photonic-electronic systems.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [527] [Feature Augmentations for High-Dimensional Learning](https://arxiv.org/abs/2509.00232)
*Xiaonan Zhu,Bingyan Wang,Jianqing Fan*

Main category: stat.AP

TL;DR: 提出通过用设计矩阵及其变换提取的因子增强特征来提升监督学习算法性能的方法，经实验验证可提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 高维测量常相关，特征工程易导致过参数化，需要快速降维。

Method: 用设计矩阵及其变换提取因子和特异残差来增强特征，使用简单的PCA增强方法。

Result: 在多种算法和不同领域的真实数据上实验，验证了该特征增强方法能提升整体预测性能。

Conclusion: 该方法弥补了以往研究只关注收集额外数据或构建更强大算法的不足。

Abstract: High-dimensional measurements are often correlated which motivates their
approximation by factor models. This holds also true when features are
engineered via low-dimensional interactions or kernel tricks. This often
results in over parametrization and requires a fast dimensionality reduction.
We propose a simple technique to enhance the performance of supervised learning
algorithms by augmenting features with factors extracted from design matrices
and their transformations. This is implemented by using the factors and
idiosyncratic residuals which significantly weaken the correlations between
input variables and hence increase the interpretability of learning algorithms
and numerical stability. Extensive experiments on various algorithms and
real-world data in diverse fields are carried out, among which we put special
emphasis on the stock return prediction problem with Chinese financial news
data due to the increasing interest in NLP problems in financial studies. We
verify the capability of the proposed feature augmentation approach to boost
overall prediction performance with the same algorithm. The approach bridges a
gap in research that has been overlooked in previous studies, which focus
either on collecting additional data or constructing more powerful algorithms,
whereas our method lies in between these two directions using a simple PCA
augmentation.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [528] [Optimal information injection and transfer mechanisms for active matter reservoir computing](https://arxiv.org/abs/2509.01799)
*Mario U. Gaimann,Miriam Klopotek*

Main category: nlin.AO

TL;DR: 本文研究以活性物质系统模拟作为储层的储层计算，发现非线性驱动力能改善计算性能，活性物质形成液滴特别适合储层计算。


<details>
  <summary>Details</summary>
Motivation: 探讨生物复杂系统作为储层底物时能否有效独立处理信息，研究自组织如何产生合适的时空模式。

Method: 使用由混沌移动输入信号驱动的活性物质系统模拟作为储层，在多种非平衡活性物质相中进行储层计算。

Result: 从排斥驱动力切换到吸引驱动力时，系统计算方式改变但预测性能景观几乎相同；非线性驱动力改善计算，激活新兴调节机制；活性物质形成液滴特别适合储层计算，预测性能景观呈凸形且具有鲁棒性和适应性。

Conclusion: 活性物质系统可作为有效的储层用于计算，其中形成液滴的活性物质在储层计算中表现出色。

Abstract: Reservoir computing (RC) is a state-of-the-art machine learning method that
makes use of the power of dynamical systems (the reservoir) for real-time
inference. When using biological complex systems as reservoir substrates, it
serves as a testbed for basic questions about bio-inspired computation -- of
how self-organization generates proper spatiotemporal patterning. Here, we use
a simulation of an active matter system, driven by a chaotically moving input
signal, as a reservoir. So far, it has been unclear whether such complex
systems possess the capacity to process information efficiently and
independently of the method by which it was introduced. We find that when
switching from a repulsive to an attractive driving force, the system
completely changes the way it computes, while the predictive performance
landscapes remain nearly identical. The nonlinearity of the driver's injection
force improves computation by decoupling the single-agent dynamics from that of
the driver. Triggered are the (re-)growth, deformation, and active motion of
smooth structural boundaries (interfaces), and the emergence of coherent
gradients in speed -- features found in many soft materials and biological
systems. The nonlinear driving force activates emergent regulatory mechanisms,
which manifest enhanced morphological and dynamic diversity -- arguably
improving fading memory, nonlinearity, expressivity, and thus, performance. We
further perform RC in a broad variety of non-equilibrium active matter phases
that arise when tuning internal (repulsive) forces for information transfer.
Overall, we find that active matter agents forming liquid droplets are
particularly well suited for RC. The consistently convex shape of the
predictive performance landscapes, together with the observed phenomenological
richness, conveys robustness and adaptivity.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [529] [IoT-based Noise Monitoring using Mobile Nodes for Smart Cities](https://arxiv.org/abs/2509.00979)
*Bhima Sankar Manthina,Shreyash Gujar,Sachin Chaudhari,Kavita Vemuri1,Shivam Chhirolya*

Main category: cs.ET

TL;DR: 提出基于物联网、使用移动节点的实时环境噪声监测解决方案，经校准和实验，系统展示了在智慧城市部署潜力。


<details>
  <summary>Details</summary>
Motivation: 城市噪声污染威胁公众健康，现有监测基础设施空间覆盖和适应性有限。

Method: 利用低成本声音传感器和GPS模块收集地理标记噪声数据，用多种机器学习算法在实验室校准声音节点，后基于移动环境数据校准。

Result: 实验室校准精度高，移动中节点性能下降，RFR模型移动校准表现最佳，系统部署收集大量数据，发现噪声时空变化，结合车速校准可提高精度。

Conclusion: 该系统有在智慧城市广泛部署潜力，能助力噪声污染管理和城市规划。

Abstract: Urban noise pollution poses a significant threat to public health, yet
existing monitoring infrastructures offer limited spatial coverage and
adaptability. This paper presents a scalable, low-cost, IoT-based, real-time
environmental noise monitoring solution using mobile nodes (sensor nodes on a
moving vehicle). The system utilizes a low-cost sound sensor integrated with
GPS-enabled modules to collect geotagged noise data at one-second intervals.
The sound nodes are calibrated against a reference sound level meter in a
laboratory setting to ensure accuracy using various machine learning (ML)
algorithms, such as Simple Linear Regression (SLR), Multiple Linear Regression
(MLR), Polynomial Regression (PR), Segmented Regression (SR), Support Vector
Regression (SVR), Decision Tree (DT), and Random Forest Regression (RFR). While
laboratory calibration demonstrates high accuracy, it is shown that the
performance of the nodes degrades during data collection in a moving vehicle.
To address this, it is demonstrated that the calibration must be performed on
the IoT-based node based on the data collected in a moving environment along
with the reference device. Among the employed ML models, RFR achieved the best
performance with an R2 of 0.937 and RMSE of 1.09 for mobile calibration. The
system was deployed in Hyderabad, India, through three measurement campaigns
across 27 days, capturing 436,420 data points. Results highlight temporal and
spatial noise variations across weekdays, weekends, and during Diwali.
Incorporating vehicular velocity into the calibration significantly improves
accuracy. The proposed system demonstrates the potential for widespread
deployment of IoT-based noise sensing networks in smart cities, enabling
effective noise pollution management and urban planning.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [530] [Traq: Estimating the Quantum Cost of Classical Programs](https://arxiv.org/abs/2509.01508)
*Anurudh Peduri,Gilles Barthe,Michael Walter*

Main category: quant-ph

TL;DR: 提出Traq方法自动估计经典程序量子加速，含经典语言、成本分析和编译，还有概念验证实现和案例研究。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算机实际加速预测依赖冗长手动分析和数值模拟，且一次针对一个应用，需自动方法。

Method: 提出Traq方法，含经典语言、成本分析和编译，成本分析细粒度界定量子程序复杂度。

Result: 实现概念验证并开展AND - OR树案例研究。

Conclusion: Traq能全自动且有保证地估计经典程序量子加速。

Abstract: Predicting practical speedups offered by future quantum computers has become
a major focus of the quantum computing community. Typically, these predictions
are supported by lengthy manual analyses and numerical simulations and are
carried out for one specific application at a time. In this paper, we present
Traq, a principled approach towards estimating the quantum speedup of classical
programs fully automatically and with provable guarantees. It consists of a
classical language that includes high-level primitives amenable to quantum
speedups, a cost analysis, and a compilation to low-level quantum programs. Our
cost analysis upper bounds the complexity of the resulting quantum program in a
fine-grained way: it captures non-asymptotic information and is sensitive to
the input of the program (rather than providing worst-case costs). We also
provide a proof-of-concept implementation and a case study inspired by AND-OR
trees.

</details>


### [531] [It's-A-Me, Quantum Mario: Scalable Quantum Reinforcement Learning with Multi-Chip Ensembles](https://arxiv.org/abs/2509.00713)
*Junghoon Justin Park,Huan-Hsin Tseng,Shinjae Yoo,Samuel Yen-Chi Chen,Jiook Cha*

Main category: quant-ph

TL;DR: 提出多芯片集成框架解决量子强化学习在NISQ时代的约束，在复杂环境表现优且可扩展。


<details>
  <summary>Details</summary>
Motivation: 量子强化学习受NISQ时代约束（如有限量子比特和噪声积累），进展缓慢。

Method: 引入多芯片集成框架，用多个小量子卷积神经网络，将复杂高维观测划分到独立量子电路，在双深度Q网络框架下经典聚合输出。

Result: 在复杂环境实现量子强化学习，相比经典基线和单芯片量子模型有更好性能和学习稳定性。

Conclusion: 多芯片集成框架减少降维信息损失，可在近期量子硬件实现，为量子强化学习应用于现实问题提供实用途径。

Abstract: Quantum reinforcement learning (QRL) promises compact function approximators
with access to vast Hilbert spaces, but its practical progress is slowed by
NISQ-era constraints such as limited qubits and noise accumulation. We
introduce a multi-chip ensemble framework using multiple small Quantum
Convolutional Neural Networks (QCNNs) to overcome these constraints. Our
approach partitions complex, high-dimensional observations from the Super Mario
Bros environment across independent quantum circuits, then classically
aggregates their outputs within a Double Deep Q-Network (DDQN) framework. This
modular architecture enables QRL in complex environments previously
inaccessible to quantum agents, achieving superior performance and learning
stability compared to classical baselines and single-chip quantum models. The
multi-chip ensemble demonstrates enhanced scalability by reducing information
loss from dimensionality reduction while remaining implementable on near-term
quantum hardware, providing a practical pathway for applying QRL to real-world
problems.

</details>


### [532] [Quantum Causality: Resolving Simpson's Paradox with $\mathcal{DO}$-Calculus](https://arxiv.org/abs/2509.00744)
*Pilsung Kang*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Distinguishing correlation from causation is a fundamental challenge in
machine intelligence, often representing a critical barrier to building robust
and trustworthy systems. While Pearl's $\mathcal{DO}$-calculus provides a
rigorous framework for causal inference, a parallel challenge lies in its
physical implementation. Here, we apply and experimentally validate a quantum
algorithmic framework for performing causal interventions. Our approach maps
causal networks onto quantum circuits where probabilistic links are encoded by
controlled-rotation gates, and interventions are realized by a structural
remodeling of the circuit -- a physical analogue to Pearl's ``graph surgery''.
We demonstrate the method's efficacy by resolving Simpson's Paradox in a
3-qubit model, and show its scalability by quantifying confounding bias in a
10-qubit healthcare simulation. Critically, we provide a proof-of-principle
experimental validation on an IonQ Aria quantum computer, successfully
reproducing the paradox and its resolution in the presence of real-world noise.
This work establishes a practical pathway for quantum causal inference,
offering a new computational tool to address deep-rooted challenges in
algorithmic fairness and explainable AI (XAI).

</details>


### [533] [Quantum Circuits for Quantum Convolutions: A Quantum Convolutional Autoencoder](https://arxiv.org/abs/2509.00637)
*Javier Orduz,Pablo Rivas,Erich Baker*

Main category: quant-ph

TL;DR: 本文聚焦用随机量子电路处理输入数据，作为量子卷积生成新表示用于卷积网络，实验显示性能与经典卷积神经网络相当，部分情况可加速收敛。


<details>
  <summary>Details</summary>
Motivation: 研究利用量子力学或量子信息理论加速学习时间或收敛，评估数据在量子信息空间变换的鲁棒性和性能提升。

Method: 使用随机量子电路处理输入数据，作为量子卷积生成新表示用于卷积网络。

Result: 性能与经典卷积神经网络相当，部分情况使用量子卷积可加速收敛。

Conclusion: 随机量子电路作为量子卷积处理输入数据在卷积网络中有一定应用价值，性能表现良好。

Abstract: Quantum machine learning deals with leveraging quantum theory with classic
machine learning algorithms. Current research efforts study the advantages of
using quantum mechanics or quantum information theory to accelerate learning
time or convergence. Other efforts study data transformations in the quantum
information space to evaluate robustness and performance boosts. This paper
focuses on processing input data using randomized quantum circuits that act as
quantum convolutions producing new representations that can be used in a
convolutional network. Experimental results suggest that the performance is
comparable to classic convolutional neural networks, and in some instances,
using quantum convolutions can accelerate convergence.

</details>


### [534] [Quantum Machine Learning for UAV Swarm Intrusion Detection](https://arxiv.org/abs/2509.01812)
*Kuan-Cheng Chen,Samuel Yen-Chi Chen,Tai-Yue Li,Chen-Yu Liu,Kin K. Leung*

Main category: quant-ph

TL;DR: 本文利用120k流量模拟语料，对三种量子机器学习方法进行基准测试，分析多种因素影响，揭示不同模型权衡关系并公开代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 无人机群入侵检测因高移动性、非平稳流量和严重类别不平衡而复杂，需研究量子机器学习方法在其中的应用。

Method: 利用120k流量模拟语料，对量子核、变分量子神经网络、混合量子训练神经网络三种量子机器学习方法进行基准测试，在相同预处理、平衡和噪声模型假设下评估，分析编码策略、电路深度等因素影响。

Result: 量子核和QT - NNs在低数据、非线性情况下表现出色，较深的QNNs存在可训练性问题，数据充足时CNN更优。

Conclusion: 揭示了不同量子机器学习模型在无人机群入侵检测中的权衡关系，公开代码和数据集以支持网络安全领域可重复的量子机器学习研究。

Abstract: Intrusion detection in unmanned-aerial-vehicle (UAV) swarms is complicated by
high mobility, non-stationary traffic, and severe class imbalance. Leveraging a
120 k-flow simulation corpus that covers five attack types, we benchmark three
quantum-machine-learning (QML) approaches - quantum kernels, variational
quantum neural networks (QNNs), and hybrid quantum-trained neural networks
(QT-NNs) - against strong classical baselines. All models consume an 8-feature
flow representation and are evaluated under identical preprocessing, balancing,
and noise-model assumptions. We analyse the influence of encoding strategy,
circuit depth, qubit count, and shot noise, reporting accuracy, macro-F1,
ROC-AUC, Matthews correlation, and quantum-resource footprints. Results reveal
clear trade-offs: quantum kernels and QT-NNs excel in low-data, nonlinear
regimes, while deeper QNNs suffer from trainability issues, and CNNs dominate
when abundant data offset their larger parameter count. The complete codebase
and dataset partitions are publicly released to enable reproducible QML
research in network security.

</details>


### [535] [Exploring Quantum Machine Learning for Weather Forecasting](https://arxiv.org/abs/2509.01422)
*Maria Heloísa F. da Silva,Gleydson F. de Jesus,Christiano M. S. Nascimento,Valéria L. da Silva,Clebson Cruz*

Main category: quant-ph

TL;DR: 本文探讨量子机器学习与气候预测的交叉领域，实现基于真实气象数据训练的量子神经网络，结果显示其在风速和温度预测上有优势，凸显量子模型在气候预测潜力及待解决挑战。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型受大气动态和混沌特性挑战，引入量子计算模拟技术是有前景的解决方案。

Method: 实现一个基于NASA气象数据训练的量子神经网络（QNN）。

Result: QNN在风速预测上准确性和数据突变适应性超经典循环神经网络（RNN），在温度预测上处理时间变化稳健且收敛更快。

Conclusion: 量子模型在中短期气候预测有潜力，但存在关键挑战，需优化以扩大适用性。

Abstract: Weather forecasting plays a crucial role in supporting strategic decisions
across various sectors, including agriculture, renewable energy production, and
disaster management. However, the inherently dynamic and chaotic behavior of
the atmosphere presents significant challenges to conventional predictive
models. On the other hand, introducing quantum computing simulation techniques
to the forecasting problems constitutes a promising alternative to overcome
these challenges. In this context, this work explores the emerging intersection
between quantum machine learning (QML) and climate forecasting. We present the
implementation of a Quantum Neural Network (QNN) trained on real meteorological
data from NASA's Prediction of Worldwide Energy Resources (POWER) database. The
results show that QNN has the potential to outperform a classical Recurrent
Neural Network (RNN) in terms of accuracy and adaptability to abrupt data
shifts, particularly in wind speed prediction. Despite observed nonlinearities
and architectural sensitivities, the QNN demonstrated robustness in handling
temporal variability and faster convergence in temperature prediction. These
findings highlight the potential of quantum models in short and medium term
climate prediction, while also revealing key challenges and future directions
for optimization and broader applicability.

</details>


### [536] [QUBO-based training for VQAs on Quantum Annealers](https://arxiv.org/abs/2509.01821)
*Ernesto Acosta,Guillermo Botella,Carlos Cano*

Main category: quant-ph

TL;DR: 提出将VQA参数优化任务转化为QUBO问题的训练方法，实验证明可降低计算开销，为VQA训练提供可扩展替代方案。


<details>
  <summary>Details</summary>
Motivation: 解决大规模组合优化问题，改进VQA训练方法，应对传统梯度法局限。

Method: 将VQA参数优化任务转化为QUBO问题，利用VQA ansatz的哈密顿量，采用自适应元启发式优化方案和递归细化策略。

Result: 方法可行，相比经典和进化优化器显著降低计算开销，达到相当或更优解质量。

Conclusion: 量子退火器可作为VQA训练可扩展替代方案，为混合优化模型提供新可能。

Abstract: Quantum annealers provide an effective framework for solving large-scale
combinatorial optimization problems. This work presents a novel methodology for
training Variational Quantum Algorithms (VQAs) by reformulating the parameter
optimization task as a Quadratic Unconstrained Binary Optimization (QUBO)
problem. Unlike traditional gradient-based methods, our approach directly
leverages the Hamiltonian of the chosen VQA ansatz and employs an adaptive,
metaheuristic optimization scheme. This optimization strategy provides a rich
set of configurable parameters which enables the adaptation to specific problem
characteristics and available computational resources. The proposed framework
is generalizable to arbitrary Hamiltonians and integrates a recursive
refinement strategy to progressively approximate high-quality solutions.
  Experimental evaluations demonstrate the feasibility of the method and its
ability to significantly reduce computational overhead compared to classical
and evolutionary optimizers, while achieving comparable or superior solution
quality. These findings suggest that quantum annealers can serve as a scalable
alternative to classical optimizers for VQA training, particularly in scenarios
affected by barren plateaus and noisy gradient estimates, and open new
possibilities for hybrid quantum gate - quantum annealing - classical
optimization models in near-term quantum computing.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [537] [Analysis of Algorithms for Moser's Problems on Sums of Consecutive Primes](https://arxiv.org/abs/2509.00236)
*Jonathan P. Sorenson,Eleanor Waiss*

Main category: math.NT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In his 1963 paper on the sum of consecutive primes, Moser posed four open
questions related to $f(n)$, the number of ways an integer $n$ can be written
as a sum of consecutive primes. (See also problem C2 from Richard K.~Guy's
\textit{Unsolved Problems in Number Theory}.) In this paper, we present and
analyze two algorithms that, when given a bound $x$, construct a histogram of
values of $f(n)$ for all $n\le x$. These two algorithms were described, but not
analyzed, by Jean Charles Meyrignac (2000) and Michael S. Branicky (2022). We
show the first algorithm takes $O(x\log x)$ time using $x^{2/3}$ space, and the
second has two versions, one of which takes $O(x\log x)$ time but only
$x^{3/5}$ space, and the other which takes $O(x(\log x)^2)$ time but only $O(
\sqrt{x\log x})$ space. However, Meyrinac's algorithm is easier to parallelize.
We then present data generated by these algorithms that address all four open
questions.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [538] [Generalization vs. Memorization in Autoregressive Deep Learning: Or, Examining Temporal Decay of Gradient Coherence](https://arxiv.org/abs/2509.00024)
*James Amarel,Nicolas Hengartner,Robyn Miller,Kamaljeet Singh,Siddharth Mansingh,Arvind Mohan,Benjamin Migliori,Emily Casleton,Alexei Skurikhin,Earl Lawrence,Gerd J. Kunde*

Main category: physics.comp-ph

TL;DR: 基础模型作为自回归PDE替代模型有加速科学发现潜力，但实现泛化是挑战，需评估指标，本文用影响函数形式分析该替代模型。


<details>
  <summary>Details</summary>
Motivation: 自回归PDE替代模型虽有潜力，但实现真正泛化是关键挑战，需评估指标区分泛化和记忆。

Method: 应用影响函数形式系统刻画自回归PDE替代模型对不同物理场景信息的吸收和传播。

Result: 揭示标准模型和训练程序的基本局限性，为改进替代模型设计提供可行见解。

Conclusion: 未明确提及结论，但暗示所采用方法对改进自回归PDE替代模型有积极意义。

Abstract: Foundation models trained as autoregressive PDE surrogates hold significant
promise for accelerating scientific discovery through their capacity to both
extrapolate beyond training regimes and efficiently adapt to downstream tasks
despite a paucity of examples for fine-tuning. However, reliably achieving
genuine generalization - a necessary capability for producing novel scientific
insights and robustly performing during deployment - remains a critical
challenge. Establishing whether or not these requirements are met demands
evaluation metrics capable of clearly distinguishing genuine model
generalization from mere memorization.
  We apply the influence function formalism to systematically characterize how
autoregressive PDE surrogates assimilate and propagate information derived from
diverse physical scenarios, revealing fundamental limitations of standard
models and training routines in addition to providing actionable insights
regarding the design of improved surrogates.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [539] [Performance is not All You Need: Sustainability Considerations for Algorithms](https://arxiv.org/abs/2509.00045)
*Xiang Li,Chong Zhang,Hongpeng Wang,Shreyank Narayana Gowda,Yushi Li,Xiaobo Jin*

Main category: cs.CV

TL;DR: 文章提出创新的二维可持续性评估系统应对深度学习模型训练高碳排放问题，通过构建基准实验验证，推动绿色AI从理论到实践过渡，还提供代码支持。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型训练高碳排放，平衡算法性能和能耗的核心挑战。

Method: 提出创新二维可持续性评估系统，包括可持续调和均值（FMS）和可持续曲线下面积（ASC）两个定量指标，在多模态任务构建基准。

Result: 系统能为跨任务算法评估提供定量依据。

Conclusion: 系统推动绿色AI研究从理论到实践，代码为行业建立算法能效标准提供方法支持。

Abstract: This work focuses on the high carbon emissions generated by deep learning
model training, specifically addressing the core challenge of balancing
algorithm performance and energy consumption. It proposes an innovative
two-dimensional sustainability evaluation system. Different from the
traditional single performance-oriented evaluation paradigm, this study
pioneered two quantitative indicators that integrate energy efficiency ratio
and accuracy: the sustainable harmonic mean (FMS) integrates accumulated energy
consumption and performance parameters through the harmonic mean to reveal the
algorithm performance under unit energy consumption; the area under the
sustainability curve (ASC) constructs a performance-power consumption curve to
characterize the energy efficiency characteristics of the algorithm throughout
the cycle. To verify the universality of the indicator system, the study
constructed benchmarks in various multimodal tasks, including image
classification, segmentation, pose estimation, and batch and online learning.
Experiments demonstrate that the system can provide a quantitative basis for
evaluating cross-task algorithms and promote the transition of green AI
research from theory to practice. Our sustainability evaluation framework code
can be found here, providing methodological support for the industry to
establish algorithm energy efficiency standards.

</details>


### [540] [A Diffusion-Based Framework for Configurable and Realistic Multi-Storage Trace Generation](https://arxiv.org/abs/2509.01919)
*Seohyun Kim,Junyoung Lee,Jongho Park,Jinhyung Koo,Sungjin Lee,Yeseong Kim*

Main category: cs.CV

TL;DR: 提出DiTTO框架用于生成多设备存储轨迹，实验显示其能高保真、多样化生成且误差仅8%。


<details>
  <summary>Details</summary>
Motivation: 生成现实、可精确配置且多样的多设备存储轨迹。

Method: 利用先进扩散技术，以用户定义配置合成高保真连续轨迹，捕捉时间动态和设备间依赖关系。

Result: DiTTO能生成高保真、多样化的轨迹，与引导配置紧密契合，误差仅8%。

Conclusion: DiTTO是一种有效的生成多设备存储轨迹的框架。

Abstract: We propose DiTTO, a novel diffusion-based framework for generating realistic,
precisely configurable, and diverse multi-device storage traces. Leveraging
advanced diffusion tech- niques, DiTTO enables the synthesis of high-fidelity
continuous traces that capture temporal dynamics and inter-device dependencies
with user-defined configurations. Our experimental results demonstrate that
DiTTO can generate traces with high fidelity and diversity while aligning
closely with guided configurations with only 8% errors.

</details>


### [541] [Enabling Federated Object Detection for Connected Autonomous Vehicles: A Deployment-Oriented Evaluation](https://arxiv.org/abs/2509.01868)
*Komala Subramanyam Cherukuri,Kewei Sha,Zhenhua Huang*

Main category: cs.CV

TL;DR: 文章指出集中式目标检测模型在可扩展性等方面不足，联邦学习虽有优势但部署难，介绍了对基于联邦学习的自动驾驶汽车目标检测的整体评估，为其稳健部署铺路。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在自动驾驶汽车中实际部署面临的计算需求大、操作条件多样等挑战，确保可靠性能。

Method: 对基于联邦学习的自动驾驶汽车目标检测进行整体评估，使用YOLOv5、YOLOv8等探测器，在KITTI等数据集上，分析不同条件下检测精度、计算成本和资源使用的权衡。

Result: 对基于联邦学习的自动驾驶汽车目标检测进行了评估和分析。

Conclusion: 为基于联邦学习的目标检测在自动驾驶汽车中的稳健部署奠定基础。

Abstract: Object detection is crucial for Connected Autonomous Vehicles (CAVs) to
perceive their surroundings and make safe driving decisions. Centralized
training of object detection models often achieves promising accuracy, fast
convergence, and simplified training process, but it falls short in
scalability, adaptability, and privacy-preservation. Federated learning (FL),
by contrast, enables collaborative, privacy-preserving, and continuous training
across naturally distributed CAV fleets. However, deploying FL in real-world
CAVs remains challenging due to the substantial computational demands of
training and inference, coupled with highly diverse operating conditions.
Practical deployment must address three critical factors: (i) heterogeneity
from non-IID data distributions, (ii) constrained onboard computing hardware,
and (iii) environmental variability such as lighting and weather, alongside
systematic evaluation to ensure reliable performance. This work introduces the
first holistic deployment-oriented evaluation of FL-based object detection in
CAVs, integrating model performance, system-level resource profiling, and
environmental robustness. Using state-of-the-art detectors, YOLOv5, YOLOv8,
YOLOv11, and Deformable DETR, evaluated on the KITTI, BDD100K, and nuScenes
datasets, we analyze trade-offs between detection accuracy, computational cost,
and resource usage under diverse resolutions, batch sizes, weather and lighting
conditions, and dynamic client participation, paving the way for robust FL
deployment in CAVs.

</details>


### [542] [IPG: Incremental Patch Generation for Generalized Adversarial Patch Training](https://arxiv.org/abs/2508.10946)
*Wonho Lee,Hyunsik Na,Jisu Lee,Daeseon Choi*

Main category: cs.CV

TL;DR: 本文提出增量补丁生成（IPG）方法，生成对抗补丁比现有方法高效11.1倍，实验证明其有效性，在多领域有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 对抗补丁对AI模型鲁棒性构成挑战，传统方法有局限，需更高效方法。

Method: 提出增量补丁生成（IPG）方法。

Result: IPG生成对抗补丁比现有方法高效11.1倍，生成的补丁泛化性好，能覆盖更多模型漏洞，生成的数据集可作为构建鲁棒模型的知识基础。

Conclusion: IPG在对抗补丁防御及自动驾驶、安全系统、医学影像等现实应用中有很大潜力。

Abstract: The advent of adversarial patches poses a significant challenge to the
robustness of AI models, particularly in the domain of computer vision tasks
such as object detection. In contradistinction to traditional adversarial
examples, these patches target specific regions of an image, resulting in the
malfunction of AI models. This paper proposes Incremental Patch Generation
(IPG), a method that generates adversarial patches up to 11.1 times more
efficiently than existing approaches while maintaining comparable attack
performance. The efficacy of IPG is demonstrated by experiments and ablation
studies including YOLO's feature distribution visualization and adversarial
training results, which show that it produces well-generalized patches that
effectively cover a broader range of model vulnerabilities. Furthermore,
IPG-generated datasets can serve as a robust knowledge foundation for
constructing a robust model, enabling structured representation, advanced
reasoning, and proactive defenses in AI security ecosystems. The findings of
this study suggest that IPG has considerable potential for future utilization
not only in adversarial patch defense but also in real-world applications such
as autonomous vehicles, security systems, and medical imaging, where AI models
must remain resilient to adversarial attacks in dynamic and high-stakes
environments.

</details>


### [543] [Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary](https://arxiv.org/abs/2509.00033)
*Tahoshin Alam Ishat*

Main category: cs.CV

TL;DR: 研究探索并微调现有模型，结合YOLOv8、LSTM和ASR为TinyLLaMa提取数据，预测食谱并生成烹饪步骤指南，证明计算机视觉在日常活动的应用。


<details>
  <summary>Details</summary>
Motivation: 扩展计算机视觉在日常生活（如厨房工作）中的应用，构建特定任务系统以在复杂环境中表现良好。

Method: 探索并微调现有模型，结合YOLOv8分割模型、基于手部点运动序列训练的LSTM模型、ASR（whisper - base）为LLM（TinyLLaMa）提取数据。

Result: 收集数据构建了鲁棒的特定任务系统，能在复杂环境中良好运行。

Conclusion: 该研究拓展了计算机视觉在日常生活更多关键任务中的应用领域。

Abstract: This is a research exploring existing models and fine tuning them to combine
a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence
and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to
predict the recipe and generate text creating a step by step guide for the
cooking procedure. All the data were gathered by the author for a robust task
specific system to perform best in complex and challenging environments proving
the extension and endless application of computer vision in daily activities
such as kitchen work. This work extends the field for many more crucial task of
our day to day life.

</details>


### [544] [ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity Score for Autonomous Rover Target Prioritization](https://arxiv.org/abs/2509.00042)
*Poyraz Baydemir*

Main category: cs.CV

TL;DR: 提出ARTPS系统用于行星表面自主探索，结合多种技术，在火星探测器数据集上表现优异，减少误报。


<details>
  <summary>Details</summary>
Motivation: 开发用于行星表面自主探索的目标优先级排序系统。

Method: 结合单目深度估计、多组件异常检测和加权好奇心评分，使用Vision Transformers进行单目深度估计。

Result: 在火星探测器数据集上取得AUROC 0.94、AUPRC 0.89和F1-Score 0.87的表现，消融研究显示目标优先级排序准确性显著提高，混合融合方法减少23%误报。

Conclusion: ARTPS系统有效，其混合融合方法能减少误报并保持检测灵敏度。

Abstract: We present ARTPS (Autonomous Rover Target Prioritization System), a novel
hybrid AI system that combines depth estimation, anomaly detection, and
learnable curiosity scoring for autonomous exploration of planetary surfaces.
Our approach integrates monocular depth estimation using Vision Transformers
with multi-component anomaly detection and a weighted curiosity score that
balances known value, anomaly signals, depth variance, and surface roughness.
The system achieves state-of-the-art performance with AUROC of 0.94, AUPRC of
0.89, and F1-Score of 0.87 on Mars rover datasets. We demonstrate significant
improvements in target prioritization accuracy through ablation studies and
provide comprehensive analysis of component contributions. The hybrid fusion
approach reduces false positives by 23% while maintaining high detection
sensitivity across diverse terrain types.

</details>


### [545] [Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion](https://arxiv.org/abs/2509.00062)
*Justin Jung*

Main category: cs.CV

TL;DR: 提出Scaffold Diffusion用于生成稀疏多类别3D体素结构，在高稀疏数据上表现良好，凸显离散扩散模型潜力。


<details>
  <summary>Details</summary>
Motivation: 生成逼真稀疏多类别3D体素结构存在因体素结构内存扩展和稀疏性导致的类别不平衡问题。

Method: 将体素视为标记，使用离散扩散语言模型生成3D体素结构。

Result: 在3D - Craft数据集的Minecraft房屋结构上评估，Scaffold Diffusion在98%以上稀疏数据上能生成逼真连贯结构，还提供交互式查看器。

Conclusion: 离散扩散是3D稀疏体素生成建模的有前景框架。

Abstract: Generating realistic sparse multi-category 3D voxel structures is difficult
due to the cubic memory scaling of voxel structures and moreover the
significant class imbalance caused by sparsity. We introduce Scaffold
Diffusion, a generative model designed for sparse multi-category 3D voxel
structures. By treating voxels as tokens, Scaffold Diffusion uses a discrete
diffusion language model to generate 3D voxel structures. We show that discrete
diffusion language models can be extended beyond inherently sequential domains
such as text to generate spatially coherent 3D structures. We evaluate on
Minecraft house structures from the 3D-Craft dataset and demonstrate that,
unlike prior baselines and an auto-regressive formulation, Scaffold Diffusion
produces realistic and coherent structures even when trained on data with over
98% sparsity. We provide an interactive viewer where readers can visualize
generated samples and the generation process. Our results highlight discrete
diffusion as a promising framework for 3D sparse voxel generative modeling.

</details>


### [546] [Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments](https://arxiv.org/abs/2509.00176)
*Muhammad Ali,Salman Khan*

Main category: cs.CV

TL;DR: 本文引入用于现实场景垃圾分类的新数据集，评估VLLMs性能，强调提升其鲁棒性的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂环境和物体变形的杂乱数据集中的能力未被充分探索，需要评估VLLMs在这种挑战性条件下的性能。

Method: 引入针对现实场景垃圾分类的新数据集，并提出深入的评估方法。

Result: 通过新数据集和综合分析，获得了VLLMs在挑战性条件下性能的有价值见解。

Conclusion: VLLMs需要进一步提升鲁棒性，以在复杂环境中表现更好，数据集和代码将公开。

Abstract: Recent advancements in Large Language Models (LLMs) have paved the way for
Vision Large Language Models (VLLMs) capable of performing a wide range of
visual understanding tasks. While LLMs have demonstrated impressive performance
on standard natural images, their capabilities have not been thoroughly
explored in cluttered datasets where there is complex environment having
deformed shaped objects. In this work, we introduce a novel dataset
specifically designed for waste classification in real-world scenarios,
characterized by complex environments and deformed shaped objects. Along with
this dataset, we present an in-depth evaluation approach to rigorously assess
the robustness and accuracy of VLLMs. The introduced dataset and comprehensive
analysis provide valuable insights into the performance of VLLMs under
challenging conditions. Our findings highlight the critical need for further
advancements in VLLM's robustness to perform better in complex environments.
The dataset and code for our experiments will be made publicly available.

</details>


### [547] [Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment](https://arxiv.org/abs/2509.00210)
*Jinzhou Tang,Jusheng zhang,Sidi Liu,Waikit Xiu,Qinhan Lv,Xiying Li*

Main category: cs.CV

TL;DR: 提出VEME方法增强深度学习模型在未知环境复杂任务中的推理能力，实验显示有一定提升。


<details>
  <summary>Details</summary>
Motivation: 现有先进视觉语言模型在时空推理和适应动态开放任务方面存在局限，需提升深度学习模型在未知环境复杂任务中的类人推理能力。

Method: 提出VEME方法，包含跨模态对齐框架、动态隐式认知地图、基于指令的导航推理框架，嵌入几何感知的时空情节经验。

Result: 在VSI - Bench和VLN - CE实验中，与传统方法相比，准确率和探索效率提升1% - 3%。

Conclusion: 该方法显著提高了动态环境中的推理和规划能力。

Abstract: Achieving human-like reasoning in deep learning models for complex tasks in
unknown environments remains a critical challenge in embodied intelligence.
While advanced vision-language models (VLMs) excel in static scene
understanding, their limitations in spatio-temporal reasoning and adaptation to
dynamic, open-set tasks like task-oriented navigation and embodied question
answering (EQA) persist due to inadequate modeling of fine-grained
spatio-temporal cues and physical world comprehension. To address this, we
propose VEME, a novel cross-modal alignment method that enhances generalization
in unseen scenes by learning an ego-centric, experience-centered world model.
Our framework integrates three key components: (1) a cross-modal alignment
framework bridging objects, spatial representations, and visual semantics with
spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic,
implicit cognitive map activated by world embedding to enable task-relevant
geometric-semantic memory recall; and (3) an instruction-based navigation and
reasoning framework leveraging embodied priors for long-term planning and
efficient exploration. By embedding geometry-aware spatio-temporal episodic
experiences, our method significantly improves reasoning and planning in
dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate
1%-3% accuracy and exploration efficiency improvement compared to traditional
approaches.

</details>


### [548] [Multimodal Deep Learning for Phyllodes Tumor Classification from Ultrasound and Clinical Data](https://arxiv.org/abs/2509.00213)
*Farhan Fuad Abir,Abigail Elliott Daly,Kyle Anderman,Tolga Ozmen,Laura J. Brattain*

Main category: cs.CV

TL;DR: 提出多模态深度学习框架融合乳腺超声图像和临床数据诊断乳腺叶状肿瘤，性能优于单模态基线，展示多模态AI潜力。


<details>
  <summary>Details</summary>
Motivation: 叶状肿瘤术前难分类，常导致不必要手术切除，需提高诊断准确性。

Method: 开发双分支神经网络，提取和融合超声图像及患者元数据特征，采用类感知采样和受试者分层5折交叉验证。

Result: 多模态方法在分类良恶性叶状肿瘤上优于单模态基线，ConvNeXt和ResNet18表现最佳，有相应AUC - ROC和F1分数。

Conclusion: 多模态AI可作为非侵入性诊断工具，减少不必要活检，改善乳腺肿瘤管理临床决策。

Abstract: Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are
difficult to classify preoperatively due to their radiological similarity to
benign fibroadenomas. This often leads to unnecessary surgical excisions. To
address this, we propose a multimodal deep learning framework that integrates
breast ultrasound (BUS) images with structured clinical data to improve
diagnostic accuracy. We developed a dual-branch neural network that extracts
and fuses features from ultrasound images and patient metadata from 81 subjects
with confirmed PTs. Class-aware sampling and subject-stratified 5-fold
cross-validation were applied to prevent class imbalance and data leakage. The
results show that our proposed multimodal method outperforms unimodal baselines
in classifying benign versus borderline/malignant PTs. Among six image
encoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal
setting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and
0.7294, respectively. This study demonstrates the potential of multimodal AI to
serve as a non-invasive diagnostic tool, reducing unnecessary biopsies and
improving clinical decision-making in breast tumor management.

</details>


### [549] [Generative AI for Industrial Contour Detection: A Language-Guided Vision System](https://arxiv.org/abs/2509.00284)
*Liang Gong,Tommy,Wang,Sara Chaker,Yanchen Dong,Fouad Bousetouane,Brenden Morton,Mark Mendez*

Main category: cs.CV

TL;DR: 提出语言引导生成视觉系统用于制造业残余轮廓检测，在数据集上提升轮廓保真度，GPT - image - 1表现更优，显示VLM引导工作流潜力。


<details>
  <summary>Details</summary>
Motivation: 工业计算机视觉系统受噪声、材料可变性和成像条件影响，经典边缘检测器和手工管道效果不佳，需新系统实现CAD级精度。

Method: 系统分数据采集与预处理、条件GAN轮廓生成、视觉语言建模多模态轮廓细化三阶段，用人工参与过程制作标准提示并通过图像文本引导合成应用。对多个视觉语言模型进行基准测试。

Result: 在FabTrack数据集上提升轮廓保真度，增强边缘连续性和几何对齐，减少手动追踪；GPT - image - 1在结构准确性和感知质量上优于Gemini 2.0 Flash。

Conclusion: VLM引导的生成工作流有潜力推动工业计算机视觉突破经典管道的局限。

Abstract: Industrial computer vision systems often struggle with noise, material
variability, and uncontrolled imaging conditions, limiting the effectiveness of
classical edge detectors and handcrafted pipelines. In this work, we present a
language-guided generative vision system for remnant contour detection in
manufacturing, designed to achieve CAD-level precision. The system is organized
into three stages: data acquisition and preprocessing, contour generation using
a conditional GAN, and multimodal contour refinement through vision-language
modeling, where standardized prompts are crafted in a human-in-the-loop process
and applied through image-text guided synthesis. On proprietary FabTrack
datasets, the proposed system improved contour fidelity, enhancing edge
continuity and geometric alignment while reducing manual tracing. For the
refinement stage, we benchmarked several vision-language models, including
Google's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided
workflow, and open-source baselines. Under standardized conditions, GPT-image-1
consistently outperformed Gemini 2.0 Flash in both structural accuracy and
perceptual quality. These findings demonstrate the promise of VLM-guided
generative workflows for advancing industrial computer vision beyond the
limitations of classical pipelines.

</details>


### [550] [Target-Oriented Single Domain Generalization](https://arxiv.org/abs/2509.00351)
*Marzi Heidari,Yuhong Guo*

Main category: cs.CV

TL;DR: 提出TO - SDG问题设置，引入STAR模块利用目标域文本描述提升单域泛化能力，实验证明其优越性，表明文本元数据可增强模型泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有单域泛化方法忽略目标部署环境的文本描述，而单源域训练的深度模型在分布偏移下表现差。

Method: 提出TO - SDG问题设置，引入STAR模块，利用VLM注入目标语义，通过目标锚定子空间调整特征，用光谱投影保留目标线索方向，采用视觉语言蒸馏和特征空间Mixup。

Result: 在图像分类和目标检测基准实验中STAR表现优越。

Conclusion: 少量文本元数据能在严重数据约束下显著提升泛化能力，为在未见数据的目标环境部署鲁棒模型开辟新途径。

Abstract: Deep models trained on a single source domain often fail catastrophically
under distribution shifts, a critical challenge in Single Domain Generalization
(SDG). While existing methods focus on augmenting source data or learning
invariant features, they neglect a readily available resource: textual
descriptions of the target deployment environment. We propose Target-Oriented
Single Domain Generalization (TO-SDG), a novel problem setup that leverages the
textual description of the target domain, without requiring any target data, to
guide model generalization. To address TO-SDG, we introduce Spectral TARget
Alignment (STAR), a lightweight module that injects target semantics into
source features by exploiting visual-language models (VLMs) such as CLIP. STAR
uses a target-anchored subspace derived from the text embedding of the target
description to recenter image features toward the deployment domain, then
utilizes spectral projection to retain directions aligned with target cues
while discarding source-specific noise. Moreover, we use a vision-language
distillation to align backbone features with VLM's semantic geometry. STAR
further employs feature-space Mixup to ensure smooth transitions between source
and target-oriented representations. Experiments across various image
classification and object detection benchmarks demonstrate STAR's superiority.
This work establishes that minimal textual metadata, which is a practical and
often overlooked resource, significantly enhances generalization under severe
data constraints, opening new avenues for deploying robust models in target
environments with unseen data.

</details>


### [551] [AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data](https://arxiv.org/abs/2509.00353)
*Koushik Ahmed Kushal,Abdullah Al Mamun*

Main category: cs.CV

TL;DR: 提出多模态深度学习框架AQFusionNet用于空气质量指数预测，在印度和尼泊尔样本上表现优于单模态基线，适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 资源受限地区因传感器部署稀疏和基础设施有限，空气污染监测存在挑战，需要更好的空气质量指数预测方法。

Method: 引入AQFusionNet框架，用轻量级CNN骨干网络（MobileNetV2、ResNet18、EfficientNet - B0）整合地面大气图像与污染物浓度数据，通过语义对齐嵌入空间结合视觉和传感器特征。

Result: 在8000多个印度和尼泊尔样本实验中，AQFusionNet始终优于单模态基线，使用EfficientNet - B0骨干网络时分类准确率达92.02%，RMSE为7.70，比单模态方法提升18.5%，且计算开销低。

Conclusion: AQFusionNet为基础设施有限环境下的空气质量指数监测提供可扩展且实用的解决方案，即使部分传感器可用时也有强大预测能力。

Abstract: Air pollution monitoring in resource-constrained regions remains challenging
due to sparse sensor deployment and limited infrastructure. This work
introduces AQFusionNet, a multimodal deep learning framework for robust Air
Quality Index (AQI) prediction. The framework integrates ground-level
atmospheric imagery with pollutant concentration data using lightweight CNN
backbones (MobileNetV2, ResNet18, EfficientNet-B0). Visual and sensor features
are combined through semantically aligned embedding spaces, enabling accurate
and efficient prediction. Experiments on more than 8,000 samples from India and
Nepal demonstrate that AQFusionNet consistently outperforms unimodal baselines,
achieving up to 92.02% classification accuracy and an RMSE of 7.70 with the
EfficientNet-B0 backbone. The model delivers an 18.5% improvement over
single-modality approaches while maintaining low computational overhead, making
it suitable for deployment on edge devices. AQFusionNet provides a scalable and
practical solution for AQI monitoring in infrastructure-limited environments,
offering robust predictive capability even under partial sensor availability.

</details>


### [552] [SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding](https://arxiv.org/abs/2509.00357)
*Zhen Chen,Xingjian Luo,Kun Yuan,Jinlin Wu,Danny T. M. Chan,Nassir Navab,Hongbin Liu,Zhen Lei,Jiebo Luo*

Main category: cs.CV

TL;DR: 本文提出SurgLLM框架用于外科手术视频理解，通过Surg - Pretrain、TM - Tuning和Surgical Task Dynamic Ensemble提升性能，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有外科手术视频理解研究存在视觉内容感知不足和时间感知不够的问题，阻碍通用计算机辅助手术（CAS）解决方案的发展。

Method: 提出SurgLLM框架，采用Surgical Context - aware Multimodal Pretraining增强空间聚焦，Temporal - aware Multimodal Tuning增强时间推理，Surgical Task Dynamic Ensemble处理不同任务。

Result: 在包括字幕生成、通用视觉问答和时间视觉问答等多种外科手术视频理解任务上的实验显示，显著优于现有方法。

Conclusion: SurgLLM框架在通用外科手术视频理解中有效，代码已开源。

Abstract: Surgical video understanding is crucial for facilitating Computer-Assisted
Surgery (CAS) systems. Despite significant progress in existing studies, two
major limitations persist, including inadequate visual content perception and
insufficient temporal awareness in surgical videos, and hinder the development
of versatile CAS solutions. In this work, we propose the SurgLLM framework, an
effective large multimodal model tailored for versatile surgical video
understanding tasks with enhanced spatial focus and temporal awareness.
Specifically, to empower the spatial focus of surgical videos, we first devise
Surgical Context-aware Multimodal Pretraining (Surg-Pretrain) for the video
encoder of SurgLLM, by performing instrument-centric Masked Video
Reconstruction (MV-Recon) and subsequent multimodal alignment. To incorporate
surgical temporal knowledge into SurgLLM, we further propose Temporal-aware
Multimodal Tuning (TM-Tuning) to enhance temporal reasoning with interleaved
multimodal embeddings. Moreover, to accommodate various understanding tasks of
surgical videos without conflicts, we devise a Surgical Task Dynamic Ensemble
to efficiently triage a query with optimal learnable parameters in our SurgLLM.
Extensive experiments performed on diverse surgical video understanding tasks,
including captioning, general VQA, and temporal VQA, demonstrate significant
improvements over the state-of-the-art approaches, validating the effectiveness
of our SurgLLM in versatile surgical video understanding. The source code is
available at https://github.com/franciszchen/SurgLLM.

</details>


### [553] [Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models](https://arxiv.org/abs/2509.00373)
*Sihao Wu,Gaojie Jin,Wei Huang,Jianhong Wang,Xiaowei Huang*

Main category: cs.CV

TL;DR: 提出SPO - VLM框架增强Vision Language Models对抗攻击的鲁棒性，经实验验证有效且将开源。


<details>
  <summary>Details</summary>
Motivation: 现有Vision Language Models易受对抗攻击，激活转向防御方法依赖特定对比提示，性能不佳且影响视觉定位。

Method: 提出两阶段防御框架SPO - VLM，第一阶段从多数据源计算自适应特定层转向向量，第二阶段通过序列级偏好优化细化向量。

Result: 广泛实验表明SPO - VLM通过激活转向和偏好优化增强了对抗攻击的安全性，在良性任务上保持良好性能，不影响视觉理解能力。

Conclusion: SPO - VLM框架能有效提升Vision Language Models的鲁棒性，平衡了效率和效果，且将开源支持后续研究。

Abstract: Vision Language Models (VLMs) have demonstrated impressive capabilities in
integrating visual and textual information for understanding and reasoning, but
remain highly vulnerable to adversarial attacks. While activation steering has
emerged as a promising defence, existing approaches often rely on task-specific
contrastive prompts to extract harmful directions, which exhibit suboptimal
performance and can degrade visual grounding performance. To address these
limitations, we propose \textit{Sequence-Level Preference Optimization} for VLM
(\textit{SPO-VLM}), a novel two-stage defense framework that combines
activation-level intervention with policy-level optimization to enhance model
robustness. In \textit{Stage I}, we compute adaptive layer-specific steering
vectors from diverse data sources, enabling generalized suppression of harmful
behaviors during inference. In \textit{Stage II}, we refine these steering
vectors through a sequence-level preference optimization process. This stage
integrates automated toxicity assessment, as well as visual-consistency rewards
based on caption-image alignment, to achieve safe and semantically grounded
text generation. The two-stage structure of SPO-VLM balances efficiency and
effectiveness by combining a lightweight mitigation foundation in Stage I with
deeper policy refinement in Stage II. Extensive experiments shown SPO-VLM
enhances safety against attacks via activation steering and preference
optimization, while maintaining strong performance on benign tasks without
compromising visual understanding capabilities. We will release our code, model
weights, and evaluation toolkit to support reproducibility and future research.
\textcolor{red}{Warning: This paper may contain examples of offensive or
harmful text and images.}

</details>


### [554] [DAOVI: Distortion-Aware Omnidirectional Video Inpainting](https://arxiv.org/abs/2509.00396)
*Ryosuke Seshimo,Mariko Isogawa*

Main category: cs.CV

TL;DR: 本文提出用于全向视频修复的DAOVI模型，实验表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频修复方法多针对窄视角普通视频，未处理全向视频等距柱状投影中的畸变问题，需新方法解决全向视频修复。

Method: 提出Distortion - Aware Omnidirectional Video Inpainting (DAOVI)模型，引入在图像空间考虑测地距离评估时间运动信息的模块，以及在特征空间解决全向视频固有几何畸变的深度感知特征传播模块。

Result: 实验结果显示，提出的方法在定量和定性上均优于现有方法。

Conclusion: 所提出的DAOVI模型能有效解决全向视频修复问题，表现优于现有方法。

Abstract: Omnidirectional videos that capture the entire surroundings are employed in a
variety of fields such as VR applications and remote sensing. However, their
wide field of view often causes unwanted objects to appear in the videos. This
problem can be addressed by video inpainting, which enables the natural removal
of such objects while preserving both spatial and temporal consistency.
Nevertheless, most existing methods assume processing ordinary videos with a
narrow field of view and do not tackle the distortion in equirectangular
projection of omnidirectional videos. To address this issue, this paper
proposes a novel deep learning model for omnidirectional video inpainting,
called Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI
introduces a module that evaluates temporal motion information in the image
space considering geodesic distance, as well as a depth-aware feature
propagation module in the feature space that is designed to address the
geometric distortion inherent to omnidirectional videos. The experimental
results demonstrate that our proposed method outperforms existing methods both
quantitatively and qualitatively.

</details>


### [555] [VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding](https://arxiv.org/abs/2509.00484)
*Zhihong Zhang,Xiaojian Huang,Jin Xu,Zhuodong Luo,Xinzhi Wang,Jiansheng Wei,Xuejin Chen*

Main category: cs.CV

TL;DR: 提出VideoRewardBench基准评估视频领域多模态奖励模型，构建高质量偏好数据集，评估28个模型并得出关键见解。


<details>
  <summary>Details</summary>
Motivation: 现有视频领域多模态奖励模型评估基准存在问题，如问题数量和多样性有限、评估维度不全面等。

Method: 引入VideoRewardBench基准，涵盖视频理解四个核心方面，通过AI辅助数据管道构建1563个标注样本的偏好数据集，对28个多模态奖励模型进行评估。

Result: 表现最好的GPT - 4o整体准确率仅57.0%，开源模型Qwen2.5 - VL - 72B仅53.3%，还得出关于模型训练、推理和输入帧数影响的三点关键见解。

Conclusion: VideoRewardBench为视频领域多模态奖励模型的评估和发展提供了有挑战性和价值的基准。

Abstract: Multimodal reward models (MRMs) play a crucial role in the training,
inference, and evaluation of Large Vision Language Models (LVLMs) by assessing
response quality. However, existing benchmarks for evaluating MRMs in the video
domain suffer from a limited number and diversity of questions, a lack of
comprehensive evaluation dimensions, and inadequate evaluation of diverse types
of MRMs. To address these gaps, we introduce VideoRewardBench, the first
comprehensive benchmark covering four core aspects of video understanding:
perception, knowledge, reasoning, and safety. Through our AI-assisted data
pipeline, we curate a high-quality preference dataset of 1,563 annotated
samples, including 1,482 unique videos and 1,559 distinct questions--15 times
the number found in the most question-rich prior benchmark. Each sample is a
triplet consisting of a video-text prompt, a chosen response, and a rejected
response. We also conduct a comprehensive evaluation across 28 multimodal
reward models spanning three categories: generative, discriminative, and
semi-scalar. Results show that even the top-performing model GPT-4o achieves
only 57.0% overall accuracy, and the state-of-the-art open-source model
Qwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three key
insights: (i) MRMs trained with reinforcement learning (RL) do not necessarily
exhibit stronger cross-modal generalization than those trained without RL; (ii)
except for discriminative MRMs, other types of MRMs across varying model
capacities can benefit from inference-time scaling; and (iii) variations in
input video frame count have different effects on different types of MRMs. We
believe VideoRewardBench offers a challenging and valuable benchmark for
advancing the evaluation and development of MRMs in the video domain.

</details>


### [556] [Multi-Focused Video Group Activities Hashing](https://arxiv.org/abs/2509.00490)
*Zhongmiao Qi,Yan Jiang,Bolin Zhang,Lijun Guo,Chong Wang,Qiangbo Qian*

Main category: cs.CV

TL;DR: 针对复杂场景视频数据增长下快速检索群体活动问题，提出STVH和M - STVH技术，实验效果出色。


<details>
  <summary>Details</summary>
Motivation: 复杂场景视频数据爆炸式增长，现有任务多基于整视频检索，无法聚焦活动粒度，且现实检索场景需求多样。

Method: 提出STVH技术，通过统一框架同时建模个体对象动态和群体交互；提出M - STVH作为增强版本，通过多焦点表示学习进行分层特征集成。

Result: 在公开数据集上进行对比实验，STVH和M - STVH都取得了优异结果。

Conclusion: STVH和M - STVH技术能有效解决复杂场景下群体活动快速检索问题。

Abstract: With the explosive growth of video data in various complex scenarios, quickly
retrieving group activities has become an urgent problem. However, many tasks
can only retrieve videos focusing on an entire video, not the activity
granularity. To solve this problem, we propose a new STVH (spatiotemporal
interleaved video hashing) technique for the first time. Through a unified
framework, the STVH simultaneously models individual object dynamics and group
interactions, capturing the spatiotemporal evolution on both group visual
features and positional features. Moreover, in real-life video retrieval
scenarios, it may sometimes require activity features, while at other times, it
may require visual features of objects. We then further propose a novel M-STVH
(multi-focused spatiotemporal video hashing) as an enhanced version to handle
this difficult task. The advanced method incorporates hierarchical feature
integration through multi-focused representation learning, allowing the model
to jointly focus on activity semantics features and object visual features. We
conducted comparative experiments on publicly available datasets, and both STVH
and M-STVH can achieve excellent results.

</details>


### [557] [Towards Methane Detection Onboard Satellites](https://arxiv.org/abs/2509.00626)
*Maggie Chen,Hala Lambdouar,Luca Marini,Laura Martínez-Ferrer,Chris Bridges,Giacomo Acciarini*

Main category: cs.CV

TL;DR: 本文提出用未正射校正数据（UnorthoDOS）进行甲烷检测的新方法，模型表现良好，并公开数据和代码。


<details>
  <summary>Details</summary>
Motivation: 甲烷是温室气体，及时检测对缓解气候变化至关重要，机载卫星机器学习可实现快速检测并降低下行成本。

Method: 引入使用未正射校正数据（UnorthoDOS）的新方法，绕过传统预处理步骤，在未正射校正和正射校正数据集上训练机器学习模型。

Result: 在未正射校正数据集上训练的ML模型性能与正射校正数据集相当，正射校正数据集训练的模型优于匹配滤波器基线（mag1c）。

Conclusion: 提出的新方法有效，还公开了模型检查点、数据集和代码。

Abstract: Methane is a potent greenhouse gas and a major driver of climate change,
making its timely detection critical for effective mitigation. Machine learning
(ML) deployed onboard satellites can enable rapid detection while reducing
downlink costs, supporting faster response systems. Conventional methane
detection methods often rely on image processing techniques, such as
orthorectification to correct geometric distortions and matched filters to
enhance plume signals. We introduce a novel approach that bypasses these
preprocessing steps by using \textit{unorthorectified} data (UnorthoDOS). We
find that ML models trained on this dataset achieve performance comparable to
those trained on orthorectified data. Moreover, we also train models on an
orthorectified dataset, showing that they can outperform the matched filter
baseline (mag1c). We release model checkpoints and two ML-ready datasets
comprising orthorectified and unorthorectified hyperspectral images from the
Earth Surface Mineral Dust Source Investigation (EMIT) sensor at
https://huggingface.co/datasets/SpaceML/UnorthoDOS , along with code at
https://github.com/spaceml-org/plume-hunter.

</details>


### [558] [Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model](https://arxiv.org/abs/2509.00664)
*Yifei She,Huangxuan Wu*

Main category: cs.CV

TL;DR: 本文指出多模态大语言模型在基本视觉任务上的不足，提出FtZ框架，实验显示其在多项基准测试中表现优于基线，证明组合异构专家编码器可克服视觉感知瓶颈。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在复杂语义理解出色，但在基本视觉任务中因依赖单视觉编码器而表现不佳，需解决这一问题。

Method: 引入FtZ框架，通过轻量级多头交叉注意力机制，将语义强大的锚定编码器与感知丰富的增强编码器组合。

Result: 在TextVQA、POPE等多个需要细粒度视觉理解的基准测试中，FtZ模型显著优于仅使用单编码器或现有特征融合方法的基线。

Conclusion: 组合异构专家编码器是克服当前多模态大语言模型视觉感知瓶颈的有效途径，为构建下一代感知能力更强的AI系统提供新设计范式。

Abstract: Multimodal Large Language Models (MLLMs) have made significant progress in
bridging visual perception with high-level textual reasoning. However, they
face a fundamental contradiction: while excelling at complex semantic
understanding, these models often fail at basic visual tasks that require
precise detail perception. This deficiency primarily stems from the prevalent
architectural reliance on a single vision encoder optimized for high-level
semantic alignment, which inherently sacrifices the ability to capture
fine-grained visual information. To address this issue, we introduce Fusion to
Enhance (FtZ), a novel vision tower framework. FtZ moves beyond the
single-encoder design by innovatively composing a semantically powerful anchor
encoder with a perception-rich augmenting encoder via a lightweight Multi-Head
Cross-Attention mechanism. Experimental results demonstrate that on several
challenging benchmarks demanding fine-grained visual understanding, such as
TextVQA, POPE, MMMU, MME and MM-Vet, our FtZ model significantly outperforms
baselines that use only a single encoder or existing feature fusion methods.
This work proves that composing heterogeneous expert encoders is an efficient
and effective path to overcoming the visual perception bottleneck in current
MLLMs, offering a new design paradigm for building next-generation AI systems
with stronger perceptual capabilities.

</details>


### [559] [Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis Using Prune Learning](https://arxiv.org/abs/2509.00745)
*Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos,Tanaya Maslekar*

Main category: cs.CV

TL;DR: 提出皮肤病变分类公平算法，减少肤色相关不必要通道，降低计算成本和偏差，更适用于实际应用。


<details>
  <summary>Details</summary>
Motivation: 深度学习皮肤病变分类模型存在肤色偏差问题，且实现公平性面临肤色分类难、计算需求高和公平性验证复杂等挑战。

Method: 通过计算VGG网络卷积层特征图、Vision Transformer的补丁和头部的偏度，减少与肤色相关的不必要通道，专注病变区域。

Result: 降低计算成本，减轻偏差，在保持公平性的同时可能减小模型大小。

Conclusion: 该算法克服了不同肤色诊断公平性的挑战，更适用于实际应用。

Abstract: Recent advances in deep learning have significantly improved the accuracy of
skin lesion classification models, supporting medical diagnoses and promoting
equitable healthcare. However, concerns remain about potential biases related
to skin color, which can impact diagnostic outcomes. Ensuring fairness is
challenging due to difficulties in classifying skin tones, high computational
demands, and the complexity of objectively verifying fairness. To address these
challenges, we propose a fairness algorithm for skin lesion classification that
overcomes the challenges associated with achieving diagnostic fairness across
varying skin tones. By calculating the skewness of the feature map in the
convolution layer of the VGG (Visual Geometry Group) network and the patches
and the heads of the Vision Transformer, our method reduces unnecessary
channels related to skin tone, focusing instead on the lesion area. This
approach lowers computational costs and mitigates bias without relying on
conventional statistical methods. It potentially reduces model size while
maintaining fairness, making it more practical for real-world applications.

</details>


### [560] [Causal Interpretation of Sparse Autoencoder Features in Vision](https://arxiv.org/abs/2509.00749)
*Sangyu Han,Yearim Kim,Nojun Kwak*

Main category: cs.CV

TL;DR: 提出因果特征解释方法CaFE，利用有效感受野解释视觉SAE特征，结果显示其解释更准确。


<details>
  <summary>Details</summary>
Motivation: 传统通过检查激活最高的补丁理解视觉Transformer中SAE特征的方法不准确，因自注意力机制会混合信息，激活补丁不一定是特征触发的原因。

Method: 提出CaFE方法，以SAE特征的每次激活为目标，应用输入归因方法结合有效感受野确定因果驱动激活的图像补丁。

Result: ERF图与朴素激活图常不同，揭示隐藏上下文依赖，补丁插入测试表明CaFE比激活排序补丁更能有效恢复或抑制特征激活。

Conclusion: CaFE对视觉SAE特征的解释更忠实、语义更精确，强调仅依赖激活位置有误解风险。

Abstract: Understanding what sparse auto-encoder (SAE) features in vision transformers
truly represent is usually done by inspecting the patches where a feature's
activation is highest. However, self-attention mixes information across the
entire image, so an activated patch often co-occurs with-but does not cause-the
feature's firing. We propose Causal Feature Explanation (CaFE), which leverages
Effective Receptive Field (ERF). We consider each activation of an SAE feature
to be a target and apply input-attribution methods to identify the image
patches that causally drive that activation. Across CLIP-ViT features, ERF maps
frequently diverge from naive activation maps, revealing hidden context
dependencies (e.g., a "roaring face" feature that requires the co-occurrence of
eyes and nose, rather than merely an open mouth). Patch insertion tests confirm
that CaFE more effectively recovers or suppresses feature activations than
activation-ranked patches. Our results show that CaFE yields more faithful and
semantically precise explanations of vision-SAE features, highlighting the risk
of misinterpretation when relying solely on activation location.

</details>


### [561] [Multimodal Iterative RAG for Knowledge Visual Question Answering](https://arxiv.org/abs/2509.00798)
*Changin Choi,Wonseok Lee,Jungmin Ko,Wonjong Rhee*

Main category: cs.CV

TL;DR: 提出多模态迭代检索增强生成框架MI - RAG，实验表明其能显著提升检索召回率和答案准确率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在知识密集型视觉问题上表现有限，传统单遍检索增强生成框架难以收集足够知识。

Method: 提出MI - RAG框架，在每次迭代中利用累积推理记录动态形成多查询，驱动跨异构知识库的联合搜索，并将新知识合成到推理记录中。

Result: 在Encyclopedic VQA、InfoSeek和OK - VQA等基准测试中，MI - RAG显著提高了检索召回率和答案准确率。

Conclusion: MI - RAG为知识密集型视觉问答中的组合推理提供了可扩展的方法。

Abstract: While Multimodal Large Language Models (MLLMs) have significantly advanced
multimodal understanding, their performance remains limited on
knowledge-intensive visual questions that require external knowledge beyond the
image. Retrieval-Augmented Generation (RAG) has become a promising solution for
providing models with external knowledge, its conventional single-pass
framework often fails to gather sufficient knowledge. To overcome this
limitation, we propose MI-RAG, a Multimodal Iterative RAG framework that
leverages reasoning to enhance retrieval and update reasoning over newly
retrieved knowledge across modalities. At each iteration, MI-RAG leverages an
accumulated reasoning record to dynamically formulate a multi-query. These
queries then drive a joint search across heterogeneous knowledge bases
containing both visually-grounded and textual knowledge. The newly acquired
knowledge is synthesized into the reasoning record, progressively refining
understanding across iterations. Experiments on challenging benchmarks,
including Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG
significantly improves both retrieval recall and answer accuracy, establishing
a scalable approach for compositional reasoning in knowledge-intensive VQA.

</details>


### [562] [Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play Approach for Enhanced Fetal Plane Classification](https://arxiv.org/abs/2509.00808)
*Yang Chen,Sanglin Zhao,Baoyu Chen,Mans Gustaf*

Main category: cs.CV

TL;DR: 提出自适应对比度调整模块(ACAM)用于胎儿超声标准平面分类，在多中心数据集验证提升模型性能，建立医学图像分析新范式。


<details>
  <summary>Details</summary>
Motivation: 胎儿超声标准平面分类面临低组织对比度、边界模糊和图像质量依赖操作者等挑战，需克服这些局限。

Method: 提出ACAM模块，用浅纹理敏感网络预测对比度参数，通过可微映射转换图像，在下游分类器中融合多视图。

Result: 在多中心12400张图像数据集上，该模块提升不同模型性能，轻量级、传统和先进模型准确率分别提升2.02%、1.29%和1.15%。

Conclusion: 模块的内容感知自适应能力，用符合超声流程的变换替代随机预处理，通过多视图融合提高对成像异质性的鲁棒性，有效连接低层次图像特征和高层次语义，建立新范式。

Abstract: Fetal ultrasound standard plane classification is essential for reliable
prenatal diagnosis but faces inherent challenges, including low tissue
contrast, boundary ambiguity, and operator-dependent image quality variations.
To overcome these limitations, we propose a plug-and-play adaptive contrast
adjustment module (ACAM), whose core design is inspired by the clinical
practice of doctors adjusting image contrast to obtain clearer and more
discriminative structural information. The module employs a shallow
texture-sensitive network to predict clinically plausible contrast parameters,
transforms input images into multiple contrast-enhanced views through
differentiable mapping, and fuses them within downstream classifiers. Validated
on a multi-center dataset of 12,400 images across six anatomical categories,
the module consistently improves performance across diverse models, with
accuracy of lightweight models increasing by 2.02 percent, accuracy of
traditional models increasing by 1.29 percent, and accuracy of state-of-the-art
models increasing by 1.15 percent. The innovation of the module lies in its
content-aware adaptation capability, replacing random preprocessing with
physics-informed transformations that align with sonographer workflows while
improving robustness to imaging heterogeneity through multi-view fusion. This
approach effectively bridges low-level image features with high-level
semantics, establishing a new paradigm for medical image analysis under
real-world image quality variations.

</details>


### [563] [Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization](https://arxiv.org/abs/2509.00826)
*Xinlei Liu,Tao Hu,Peng Yi,Weitao Han,Jichao Xie,Baolin Li*

Main category: cs.CV

TL;DR: 本文重构优化目标，提出基于梯度的攻击方法SDM，实验表明其攻击性能强、成本效益高，还能增强对抗训练防御效果。


<details>
  <summary>Details</summary>
Motivation: 评估计算机视觉模型的鲁棒性需要高效的对抗攻击方法。

Method: 将生成对抗样本的优化目标重构为“最大化非真实标签概率上界与真实标签概率的差值”，提出SDM方法，建立“循环 - 阶段 - 步长”三层优化框架，不同阶段采用不同损失函数。

Result: 与先前SOTA方法相比，SDM攻击性能更强、成本效益更高，还能与对抗训练方法结合增强防御效果。

Conclusion: SDM是一种有效的对抗攻击方法，具有良好性能且可辅助对抗训练。

Abstract: Efficient adversarial attack methods are critical for assessing the
robustness of computer vision models. In this paper, we reconstruct the
optimization objective for generating adversarial examples as "maximizing the
difference between the non-true labels' probability upper bound and the true
label's probability," and propose a gradient-based attack method termed
Sequential Difference Maximization (SDM). SDM establishes a three-layer
optimization framework of "cycle-stage-step." The processes between cycles and
between iterative steps are respectively identical, while optimization stages
differ in terms of loss functions: in the initial stage, the negative
probability of the true label is used as the loss function to compress the
solution space; in subsequent stages, we introduce the Directional Probability
Difference Ratio (DPDR) loss function to gradually increase the non-true
labels' probability upper bound by compressing the irrelevant labels'
probabilities. Experiments demonstrate that compared with previous SOTA
methods, SDM not only exhibits stronger attack performance but also achieves
higher attack cost-effectiveness. Additionally, SDM can be combined with
adversarial training methods to enhance their defensive effects. The code is
available at https://github.com/X-L-Liu/SDM.

</details>


### [564] [Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion](https://arxiv.org/abs/2509.00843)
*Xueyang Kang,Zhengkang Xiang,Zezheng Zhang,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 提出将单视图NVS分解为360度场景外推和新视图插值的模型，生成全局一致新视图，在多数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单图像新视图合成方法在长距离或环形轨迹上难以保持视图一致性，无法处理大偏差视图。

Method: 将单视图NVS分解为360度场景外推和新视图插值，利用全景扩散模型学习场景先验，从全景中采样和变形关键帧，通过空间噪声扩散过程生成新视图。

Result: 实验表明该方法在生成沿用户定义轨迹的连贯视图方面优于现有方法，能生成全局一致新视图并实现灵活相机控制。

Conclusion: 提出的模型有效解决了现有方法的不足，在多场景数据集上表现出色，代码已开源。

Abstract: Novel view synthesis (NVS) from a single image is highly ill-posed due to
large unobserved regions, especially for views that deviate significantly from
the input. While existing methods focus on consistency between the source and
generated views, they often fail to maintain coherence and correct view
alignment across long-range or looped trajectories. We propose a model that
addresses this by decomposing single-view NVS into a 360-degree scene
extrapolation followed by novel view interpolation. This design ensures
long-term view and scene consistency by conditioning on keyframes extracted and
warped from a generated panoramic representation. In the first stage, a
panorama diffusion model learns the scene prior from the input perspective
image. Perspective keyframes are then sampled and warped from the panorama and
used as anchor frames in a pre-trained video diffusion model, which generates
novel views through a proposed spatial noise diffusion process. Compared to
prior work, our method produces globally consistent novel views -- even in loop
closure scenarios -- while enabling flexible camera control. Experiments on
diverse scene datasets demonstrate that our approach outperforms existing
methods in generating coherent views along user-defined trajectories. Our
implementation is available at https://github.com/YiGuYT/LookBeyond.

</details>


### [565] [Pose as Clinical Prior: Learning Dual Representations for Scoliosis Screening](https://arxiv.org/abs/2509.00872)
*Zirui Zhou,Zizhao Peng,Dongyang Jin,Chao Fan,Fengwei An,Shiqi Yu*

Main category: cs.CV

TL;DR: 本文引入Scoliosis1K - Pose数据集和双表示框架DRF用于脊柱侧弯筛查，实验表明DRF性能达最优，相关数据和代码公开。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的脊柱侧弯筛查方法多依赖轮廓数据集，忽视临床相关姿势不对称，而基于姿势数据的筛查因缺乏大规模标注数据集和原始姿势坐标离散敏感等问题未被充分探索。

Method: 引入Scoliosis1K - Pose数据集，构建双表示框架DRF，集成连续骨骼图和离散姿势不对称向量PAV，并使用PAV - 引导注意力模块PGA。

Result: DRF达到了最优性能，可视化结果表明模型利用临床不对称线索指导特征提取，促进双表示协同。

Conclusion: 提出的数据集和方法有效，可用于脊柱侧弯筛查。

Abstract: Recent AI-based scoliosis screening methods primarily rely on large-scale
silhouette datasets, often neglecting clinically relevant postural
asymmetries-key indicators in traditional screening. In contrast, pose data
provide an intuitive skeletal representation, enhancing clinical
interpretability across various medical applications. However, pose-based
scoliosis screening remains underexplored due to two main challenges: (1) the
scarcity of large-scale, annotated pose datasets; and (2) the discrete and
noise-sensitive nature of raw pose coordinates, which hinders the modeling of
subtle asymmetries. To address these limitations, we introduce
Scoliosis1K-Pose, a 2D human pose annotation set that extends the original
Scoliosis1K dataset, comprising 447,900 frames of 2D keypoints from 1,050
adolescents. Building on this dataset, we introduce the Dual Representation
Framework (DRF), which integrates a continuous skeleton map to preserve spatial
structure with a discrete Postural Asymmetry Vector (PAV) that encodes
clinically relevant asymmetry descriptors. A novel PAV-Guided Attention (PGA)
module further uses the PAV as clinical prior to direct feature extraction from
the skeleton map, focusing on clinically meaningful asymmetries. Extensive
experiments demonstrate that DRF achieves state-of-the-art performance.
Visualizations further confirm that the model leverages clinical asymmetry cues
to guide feature extraction and promote synergy between its dual
representations. The dataset and code are publicly available at
https://zhouzi180.github.io/Scoliosis1K/.

</details>


### [566] [Spotlighter: Revisiting Prompt Tuning from a Representative Mining View](https://arxiv.org/abs/2509.00905)
*Yutong Gao,Maoyuan Shao,Xinyang Huang,Chuang Zhu,Lijuan Sun,Yu Weng,Xuan Liu,Guoshun Nan*

Main category: cs.CV

TL;DR: 提出轻量级令牌选择框架Spotlighter，在提示调优中提升准确性和效率，在11个少样本基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: CLIP提示调优存在冗余或弱相关特征组件引入噪声和增加计算成本的问题。

Method: 提出Spotlighter框架，从样本和语义角度评估视觉令牌激活，保留高分令牌，用类特定语义记忆库优化选择，引入两级排名机制动态加权令牌 - 原型交互。

Result: 在11个少样本基准测试中，Spotlighter在调和平均准确率上比CLIP最高高11.19%，每秒额外增加0.8K帧，仅增加21个额外参数。

Conclusion: Spotlighter是提示调优的有效且可扩展的基线。

Abstract: CLIP's success has demonstrated that prompt tuning can achieve robust
cross-modal semantic alignment for tasks ranging from open-domain recognition
to fine-grained classification. However, redundant or weakly relevant feature
components introduce noise and incur unnecessary computational costs. In this
work, we propose Spotlighter, a lightweight token-selection framework that
simultaneously enhances accuracy and efficiency in prompt tuning. Spotlighter
evaluates each visual token's activation from both sample-wise and
semantic-wise perspectives and retains only the top-scoring tokens for
downstream prediction. A class-specific semantic memory bank of learned
prototypes refines this selection, ensuring semantic representativeness and
compensating for discarded features. To further prioritize informative signals,
we introduce a two-level ranking mechanism that dynamically weights
token--prototype interactions. Across 11 few-shot benchmarks, Spotlighter
outperforms CLIP by up to 11.19\% in harmonic mean accuracy and achieves up to
0.8K additional FPS, with only 21 extra parameters. These results establish
Spotlighter as an effective and scalable baseline for prompt tuning. Code for
our method will be available at
https://github.com/greatest-gourmet/Spotlighter.

</details>


### [567] [Seeing through Unclear Glass: Occlusion Removal with One Shot](https://arxiv.org/abs/2509.01033)
*Qiang Li,Yuanming Cao*

Main category: cs.CV

TL;DR: 本文聚焦通过受污染玻璃拍摄的图像恢复，采集真实配对图像，提出一体化模型，实验显示该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法多依赖合成数据，且对玻璃污染物处理场景有限，本文旨在解决多种遮挡物污染玻璃的图像恢复问题。

Method: 采集真实有污染和无污染的配对图像，提出一体化模型，利用一次性测试时间自适应机制和自监督辅助学习任务更新模型。

Result: 实验表明，该方法在清理真实受污染图像（尤其是未见图像）上，在定量和定性方面均优于现有方法。

Conclusion: 所提出的方法在处理多种遮挡物污染玻璃的图像恢复上效果显著，优于现有技术。

Abstract: Images taken through window glass are often degraded by contaminants adhered
to the glass surfaces. Such contaminants cause occlusions that attenuate the
incoming light and scatter stray light towards the camera. Most of existing
deep learning methods for neutralizing the effects of contaminated glasses
relied on synthetic training data. Few researchers used real degraded and clean
image pairs, but they only considered removing or alleviating the effects of
rain drops on glasses. This paper is concerned with the more challenging task
of learning the restoration of images taken through glasses contaminated by a
wide range of occluders, including muddy water, dirt and other small foreign
particles found in reality. To facilitate the learning task we have gone to a
great length to acquire real paired images with and without glass contaminants.
More importantly, we propose an all-in-one model to neutralize contaminants of
different types by utilizing the one-shot test-time adaptation mechanism. It
involves a self-supervised auxiliary learning task to update the trained model
for the unique occlusion type of each test image. Experimental results show
that the proposed method outperforms the state-of-the-art methods
quantitatively and qualitatively in cleaning realistic contaminated images,
especially the unseen ones.

</details>


### [568] [DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion](https://arxiv.org/abs/2509.01177)
*Junxiang Liu,Junming Lin,Jiangtong Li,Jie Li*

Main category: cs.CV

TL;DR: 本文提出DynaMind框架用于从EEG信号重建动态视觉场景，在SEED - DV数据集上取得SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 现有从EEG信号重建动态视觉场景的方法受EEG低空间分辨率、神经记录与视频动态时间不匹配和语义信息利用不足等限制，难以解决视觉刺激的动态连贯性和复杂语义上下文问题。

Method: 引入DynaMind框架，包含区域感知语义映射器（RSM）、时间感知动态对齐器（TDA）和双引导视频重建器（DGVR）三个核心模块，联合建模神经动态和语义特征。

Result: 在SEED - DV数据集上，DynaMind提升了重建视频的准确率，视频和基于帧的准确率分别提高12.5和10.3个百分点，像素级质量提升，SSIM提高9.4%，FVMD降低19.7%。

Conclusion: DynaMind是一项关键进展，弥合了神经动态与高保真视觉语义之间的差距。

Abstract: Reconstruction dynamic visual scenes from electroencephalography (EEG)
signals remains a primary challenge in brain decoding, limited by the low
spatial resolution of EEG, a temporal mismatch between neural recordings and
video dynamics, and the insufficient use of semantic information within brain
activity. Therefore, existing methods often inadequately resolve both the
dynamic coherence and the complex semantic context of the perceived visual
stimuli. To overcome these limitations, we introduce DynaMind, a novel
framework that reconstructs video by jointly modeling neural dynamics and
semantic features via three core modules: a Regional-aware Semantic Mapper
(RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video
Reconstructor (DGVR). The RSM first utilizes a regional-aware encoder to
extract multimodal semantic features from EEG signals across distinct brain
regions, aggregating them into a unified diffusion prior. In the mean time, the
TDA generates a dynamic latent sequence, or blueprint, to enforce temporal
consistency between the feature representations and the original neural
recordings. Together, guided by the semantic diffusion prior, the DGVR
translates the temporal-aware blueprint into a high-fidelity video
reconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art
(SOTA), boosting reconstructed video accuracies (video- and frame-based) by
12.5 and 10.3 percentage points, respectively. It also achieves a leap in
pixel-level quality, showing exceptional visual fidelity and temporal coherence
with a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical
advancement, bridging the gap between neural dynamics and high-fidelity visual
semantics.

</details>


### [569] [FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus](https://arxiv.org/abs/2509.01181)
*Qiaoqiao Jin,Siming Fu,Dong She,Weinan Jia,Hualiang Wang,Mu Liu,Jidong Jiang*

Main category: cs.CV

TL;DR: 提出FocusDPO框架解决多主体个性化图像生成难题，提升现有模型性能。


<details>
  <summary>Details</summary>
Motivation: 多主体个性化图像生成中实现对多主体细粒度独立控制存在保留主体保真度和防止跨主体属性泄漏的挑战。

Method: 基于动态语义对应和监督图像复杂度自适应识别焦点区域，在训练中跨噪声时间步逐步调整焦点区域，采用加权策略，在DPO过程中根据参考图像语义复杂度动态调整焦点分配并建立对应映射。

Result: 显著提升现有预训练个性化生成模型性能，在单主体和多主体个性化图像合成基准测试中取得最优结果。

Conclusion: 有效减轻属性泄漏，在不同生成场景中保留高主体保真度，推动可控多主体图像合成发展。

Abstract: Multi-subject personalized image generation aims to synthesize customized
images containing multiple specified subjects without requiring test-time
optimization. However, achieving fine-grained independent control over multiple
subjects remains challenging due to difficulties in preserving subject fidelity
and preventing cross-subject attribute leakage. We present FocusDPO, a
framework that adaptively identifies focus regions based on dynamic semantic
correspondence and supervision image complexity. During training, our method
progressively adjusts these focal areas across noise timesteps, implementing a
weighted strategy that rewards information-rich patches while penalizing
regions with low prediction confidence. The framework dynamically adjusts focus
allocation during the DPO process according to the semantic complexity of
reference images and establishes robust correspondence mappings between
generated and reference subjects. Extensive experiments demonstrate that our
method substantially enhances the performance of existing pre-trained
personalized generation models, achieving state-of-the-art results on both
single-subject and multi-subject personalized image synthesis benchmarks. Our
method effectively mitigates attribute leakage while preserving superior
subject fidelity across diverse generation scenarios, advancing the frontier of
controllable multi-subject image synthesis.

</details>


### [570] [Generalizable Self-supervised Monocular Depth Estimation with Mixture of Low-Rank Experts for Diverse Endoscopic Scenes](https://arxiv.org/abs/2509.01206)
*Liangjing Shao,Benshuang Chen,Chenkang Du,Xueli Liu,Xinrong Chen*

Main category: cs.CV

TL;DR: 提出用于内窥镜单目深度估计的自监督框架，在真实和模拟数据集上表现出色，泛化能力佳。


<details>
  <summary>Details</summary>
Motivation: 内窥镜场景光照条件和场景特征多样，给可泛化深度估计带来挑战。

Method: 提出基于块的动态低秩专家混合模块微调基础模型；提出自监督训练框架处理亮度和反射率不一致问题。

Result: 在真实和模拟内窥镜数据集上超越现有方法，零样本深度估计泛化能力最佳。

Conclusion: 该方法有助于内窥镜准确感知，可用于微创测量和手术。

Abstract: Self-supervised monocular depth estimation is a significant task for low-cost
and efficient three-dimensional scene perception in endoscopy. The variety of
illumination conditions and scene features is still the primary challenge for
generalizable depth estimation in endoscopic scenes. In this work, a
self-supervised framework is proposed for monocular depth estimation in various
endoscopy. Firstly, due to various features in endoscopic scenes with different
tissues, a novel block-wise mixture of dynamic low-rank experts is proposed to
efficiently finetuning the foundation model for endoscopic depth estimation. In
the proposed module, based on the input feature, different experts with a small
amount of trainable parameters are adaptively selected for weighted inference,
from various mixture of low-rank experts which are allocated based on the
training quality of each block. Moreover, a novel self-supervised training
framework is proposed to jointly cope with the inconsistency of brightness and
reflectance. The proposed method outperform state-of-the-art works on both
realistic and simulated endoscopic datasets. Furthermore, the proposed network
also achieves the best generalization based on zero-shot depth estimation on
diverse endoscopic scenes. The proposed method could contribute to accurate
endoscopic perception for minimally invasive measurement and surgery. The code
will be released upon acceptance, while the demo video can be found on here:
https://endo-gede.netlify.app/.

</details>


### [571] [RT-DETRv2 Explained in 8 Illustrations](https://arxiv.org/abs/2509.01241)
*Ethan Qi Yang Chua,Jen Hong Tan*

Main category: cs.CV

TL;DR: 本文用八张精心设计的插图解释 RT - DETRv2 架构，助研究者和从业者理解其工作原理。


<details>
  <summary>Details</summary>
Motivation: 目标检测架构难理解，现有图表难阐明 RT - DETRv2 组件工作及组合方式，希望让其架构真正可理解。

Method: 通过一系列八张精心设计的插图，从整体流程到关键组件进行解释，可视化张量流并剖析各模块逻辑。

Result: 未提及明确结果

Conclusion: 有望为研究者和从业者提供 RT - DETRv2 底层工作原理的清晰心智模型。

Abstract: Object detection architectures are notoriously difficult to understand, often
more so than large language models. While RT-DETRv2 represents an important
advance in real-time detection, most existing diagrams do little to clarify how
its components actually work and fit together. In this article, we explain the
architecture of RT-DETRv2 through a series of eight carefully designed
illustrations, moving from the overall pipeline down to critical components
such as the encoder, decoder, and multi-scale deformable attention. Our goal is
to make the existing one genuinely understandable. By visualizing the flow of
tensors and unpacking the logic behind each module, we hope to provide
researchers and practitioners with a clearer mental model of how RT-DETRv2
works under the hood.

</details>


### [572] [Face4FairShifts: A Large Image Benchmark for Fairness and Robust Learning across Visual Domains](https://arxiv.org/abs/2509.00658)
*Yumeng Lin,Dong Li,Xintao Wu,Minglai Shao,Xujiang Zhao,Zhong Chen,Chen Zhao*

Main category: cs.CV

TL;DR: 提出大规模人脸图像基准Face4FairShifts评估公平感知学习和领域泛化，分析模型性能，指出相关数据集局限，强调需有效公平感知领域适应技术。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在领域偏移下确保公平性和鲁棒性的挑战。

Method: 创建包含100,000张图像、跨四个视觉不同领域、14个属性39个注释的Face4FairShifts数据集，并进行大量实验。

Result: 分析了模型在分布偏移下的性能，发现显著差距。

Conclusion: 现有相关数据集有局限，需要更有效的公平感知领域适应技术，Face4FairShifts为推进公平可靠AI系统提供综合测试平台。

Abstract: Ensuring fairness and robustness in machine learning models remains a
challenge, particularly under domain shifts. We present Face4FairShifts, a
large-scale facial image benchmark designed to systematically evaluate
fairness-aware learning and domain generalization. The dataset includes 100,000
images across four visually distinct domains with 39 annotations within 14
attributes covering demographic and facial features. Through extensive
experiments, we analyze model performance under distribution shifts and
identify significant gaps. Our findings emphasize the limitations of existing
related datasets and the need for more effective fairness-aware domain
adaptation techniques. Face4FairShifts provides a comprehensive testbed for
advancing equitable and reliable AI systems. The dataset is available online at
https://meviuslab.github.io/Face4FairShifts/.

</details>


### [573] [LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model](https://arxiv.org/abs/2509.00676)
*Xiyao Wang,Chunyuan Li,Jianwei Yang,Kai Zhang,Bo Liu,Tianyi Xiong,Furong Huang*

Main category: cs.CV

TL;DR: 本文挑战视觉语言建模中批评模型和策略模型分离的传统，将批评模型数据集重组进行强化学习，得到兼具评估和生成能力的统一模型，在多个基准测试中表现出色，且自批评能提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 打破视觉语言建模中批评模型与策略模型分离的现状，探索让批评模型具备直接策略使用能力的方法。

Method: 将偏好标记的批评模型数据集重组为可验证的训练信号，在基础生成模型上进行强化学习。

Result: LLaVA - Critic - R1在26个视觉推理和理解基准测试中表现出色，平均比基础模型提升5.7%；LLaVA - Critic - R1 + 在MMMU上取得71.9的SOTA成绩；测试时应用自批评在五个推理任务上平均提升13.8%。

Conclusion: 基于批评模型数据的强化学习可产生兼具评估和生成能力的统一模型，为可扩展、自我改进的多模态系统提供简单途径。

Abstract: In vision-language modeling, critic models are typically trained to evaluate
outputs -- assigning scalar scores or pairwise preferences -- rather than to
generate responses. This separation from policy models, which produce the
responses, is so entrenched that critics are rarely considered for direct
policy use. In this work, we challenge this convention. We propose to
reorganize preference-labeled critic datasets into verifiable training signals
and perform reinforcement learning directly on a base generative model,
producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference
judgments while retaining full generation ability. Surprisingly,
LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a
competitive policy model -- matching or surpassing specialized reasoning VLMs
trained with in-domain data across 26 visual reasoning and understanding
benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B).
Extending this approach to existing strong reasoning VLMs yields
LLaVA-Critic-R1+, which further advances policy performance without sacrificing
critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale.
Finally, we show that the enhanced critic ability benefits inference: applying
self-critique at test time yields an average +13.8% improvement on five
representative reasoning tasks without additional training. Our results reveal
that RL training on critic data can produce a unified model excelling at both
evaluation and generation, offering a simple path toward scalable,
self-improving multimodal systems.

</details>


### [574] [Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2509.01341)
*Yunus Serhat Bicakci,Joseph Shingleton,Anahid Basiri*

Main category: cs.CV

TL;DR: 本文提出一种结合开放权重和公开多模态大语言模型与检索增强生成的新方法用于街景图像地理定位，表现优异且无需昂贵微调或重新训练。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体数据和智能手机摄像头普及，传统计算机视觉技术进行图像定位面临挑战且极具价值，需新方法。

Method: 在两个大规模数据集上用SigLIP编码器构建向量数据库，查询图像用从数据库检索的相似和不同地理定位信息提示增强后由多模态大语言模型处理。

Result: 在三个广泛使用的基准数据集上达到了更高的准确性。

Conclusion: 基于检索增强生成的多模态大语言模型在地理定位估计上有效，为传统从头训练模型方法提供了替代途径，为GeoAI带来更易获取和可扩展的解决方案。

Abstract: Street-level geolocalization from images is crucial for a wide range of
essential applications and services, such as navigation, location-based
recommendations, and urban planning. With the growing popularity of social
media data and cameras embedded in smartphones, applying traditional computer
vision techniques to localize images has become increasingly challenging, yet
highly valuable. This paper introduces a novel approach that integrates
open-weight and publicly accessible multimodal large language models with
retrieval-augmented generation. The method constructs a vector database using
the SigLIP encoder on two large-scale datasets (EMP-16 and OSV-5M). Query
images are augmented with prompts containing both similar and dissimilar
geolocation information retrieved from this database before being processed by
the multimodal large language models. Our approach has demonstrated
state-of-the-art performance, achieving higher accuracy compared against three
widely used benchmark datasets (IM2GPS, IM2GPS3k, and YFCC4k). Importantly, our
solution eliminates the need for expensive fine-tuning or retraining and scales
seamlessly to incorporate new data sources. The effectiveness of
retrieval-augmented generation-based multimodal large language models in
geolocation estimation demonstrated by this paper suggests an alternative path
to the traditional methods which rely on the training models from scratch,
opening new possibilities for more accessible and scalable solutions in GeoAI.

</details>


### [575] [Uirapuru: Timely Video Analytics for High-Resolution Steerable Cameras on Edge Devices](https://arxiv.org/abs/2509.01371)
*Guilherme H. Apostolo,Pablo Bauszat,Vinod Nigade,Henri E. Bal,Lin Wang*

Main category: cs.CV

TL;DR: 本文提出用于高分辨率可转向摄像头的实时边缘视频分析框架Uirapuru，实验表明其在准确性和推理速度上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有实时视频分析工作多针对静态视角摄像头，可转向摄像头的动态性给现有方法带来挑战，需新的解决方案。

Method: 将对摄像头驱动的全面理解融入系统设计，并在每帧级别采用快速自适应分块。

Result: 在高分辨率视频数据集和真实世界视频上实验，相比现有静态摄像头方法，准确性最多提升1.45倍，推理速度最多提升4.53倍。

Conclusion: Uirapuru能在保证指定延迟预算的同时提高准确性，或在精度相当的情况下提升推理速度。

Abstract: Real-time video analytics on high-resolution cameras has become a popular
technology for various intelligent services like traffic control and crowd
monitoring. While extensive work has been done on improving analytics accuracy
with timing guarantees, virtually all of them target static viewpoint cameras.
In this paper, we present Uirapuru, a novel framework for real-time, edge-based
video analytics on high-resolution steerable cameras. The actuation performed
by those cameras brings significant dynamism to the scene, presenting a
critical challenge to existing popular approaches such as frame tiling. To
address this problem, Uirapuru incorporates a comprehensive understanding of
camera actuation into the system design paired with fast adaptive tiling at a
per-frame level. We evaluate Uirapuru on a high-resolution video dataset,
augmented by pan-tilt-zoom (PTZ) movements typical for steerable cameras and on
real-world videos collected from an actual PTZ camera. Our experimental results
show that Uirapuru provides up to 1.45x improvement in accuracy while
respecting specified latency budgets or reaches up to 4.53x inference speedup
with on-par accuracy compared to state-of-the-art static camera approaches.

</details>


### [576] [SoccerHigh: A Benchmark Dataset for Automatic Soccer Video Summarization](https://arxiv.org/abs/2509.01439)
*Artur Díaz-Juan,Coloma Ballester,Gloria Haro*

Main category: cs.CV

TL;DR: 本文引入足球视频摘要数据集，提出基线模型和新评估指标，数据集和代码公开。


<details>
  <summary>Details</summary>
Motivation: 解决体育精彩片段生成中缺乏公开可用数据集的问题。

Method: 引入足球视频摘要数据集，提出针对该任务的基线模型和受目标摘要长度约束的新评估指标。

Result: 基线模型在测试集上F1分数达0.3956。

Conclusion: 引入的数据集可作为足球视频摘要任务的基准。

Abstract: Video summarization aims to extract key shots from longer videos to produce
concise and informative summaries. One of its most common applications is in
sports, where highlight reels capture the most important moments of a game,
along with notable reactions and specific contextual events. Automatic summary
generation can support video editors in the sports media industry by reducing
the time and effort required to identify key segments. However, the lack of
publicly available datasets poses a challenge in developing robust models for
sports highlight generation. In this paper, we address this gap by introducing
a curated dataset for soccer video summarization, designed to serve as a
benchmark for the task. The dataset includes shot boundaries for 237 matches
from the Spanish, French, and Italian leagues, using broadcast footage sourced
from the SoccerNet dataset. Alongside the dataset, we propose a baseline model
specifically designed for this task, which achieves an F1 score of 0.3956 in
the test set. Furthermore, we propose a new metric constrained by the length of
each target summary, enabling a more objective evaluation of the generated
content. The dataset and code are available at
https://ipcv.github.io/SoccerHigh/.

</details>


### [577] [MSA2-Net: Utilizing Self-Adaptive Convolution Module to Extract Multi-Scale Information in Medical Image Segmentation](https://arxiv.org/abs/2509.01498)
*Chao Deng,Xiaosen Li,Xiao Qin*

Main category: cs.CV

TL;DR: 论文提出自适应卷积模块并集成到MSA2 - Net，提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: nnUNet框架忽略分割网络内部超参数调整，限制模型泛化能力，需改进。

Method: 提出自适应卷积模块，集成到MSA2 - Net的多尺度卷积桥和多尺度融合解码器。

Result: MSA2 - Net在多个数据集上取得高Dice系数得分，如Synapse 86.49%等。

Conclusion: MSA2 - Net在多数据集医学图像分割任务中表现出高鲁棒性和精确性。

Abstract: The nnUNet segmentation framework adeptly adjusts most hyperparameters in
training scripts automatically, but it overlooks the tuning of internal
hyperparameters within the segmentation network itself, which constrains the
model's ability to generalize. Addressing this limitation, this study presents
a novel Self-Adaptive Convolution Module that dynamically adjusts the size of
the convolution kernels depending on the unique fingerprints of different
datasets. This adjustment enables the MSA2-Net, when equipped with this module,
to proficiently capture both global and local features within the feature maps.
Self-Adaptive Convolution Module is strategically integrated into two key
components of the MSA2-Net: the Multi-Scale Convolution Bridge and the
Multi-Scale Amalgamation Decoder. In the MSConvBridge, the module enhances the
ability to refine outputs from various stages of the CSWin Transformer during
the skip connections, effectively eliminating redundant data that could
potentially impair the decoder's performance. Simultaneously, the MSADecoder,
utilizing the module, excels in capturing detailed information of organs
varying in size during the decoding phase. This capability ensures that the
decoder's output closely reproduces the intricate details within the feature
maps, thus yielding highly accurate segmentation images. MSA2-Net, bolstered by
this advanced architecture, has demonstrated exceptional performance, achieving
Dice coefficient scores of 86.49\%, 92.56\%, 93.37\%, and 92.98\% on the
Synapse, ACDC, Kvasir, and Skin Lesion Segmentation (ISIC2017) datasets,
respectively. This underscores MSA2-Net's robustness and precision in medical
image segmentation tasks across various datasets.

</details>


### [578] [Unified Supervision For Vision-Language Modeling in 3D Computed Tomography](https://arxiv.org/abs/2509.01554)
*Hao-Chih Lee,Zelong Liu,Hamza Ahmed,Spencer Kim,Sean Huver,Vishwesh Nath,Zahi A. Fayad,Timothy Deyer,Xueyan Mei*

Main category: cs.CV

TL;DR: 文章提出体积视觉语言模型Uniferum，整合不同监督信号，在3D医学影像中表现出色，为临床可靠、数据高效的VLM指明方向。


<details>
  <summary>Details</summary>
Motivation: 通用视觉语言模型在诊断放射学中缺乏判别精度，且公开体积CT数据集稀缺、异质性大，需要改进。

Method: 引入Uniferum模型，将分类标签和分割掩码中的不同监督信号统一到单一训练框架，协调三个有不同注释的3D CT数据集。

Result: Uniferum在CT - RATE基准上使AUROC提高7%，在RAD - CHEST和INSPECT数据集上有零样本性能，具备稳健的分布外泛化能力。

Conclusion: 整合异质注释和身体分割可提高模型性能，为3D医学影像中临床可靠、数据高效的VLM设定新方向。

Abstract: General-purpose vision-language models (VLMs) have emerged as promising tools
in radiology, offering zero-shot capabilities that mitigate the need for large
labeled datasets. However, in high-stakes domains like diagnostic radiology,
these models often lack the discriminative precision required for reliable
clinical use. This challenge is compounded by the scarcity and heterogeneity of
publicly available volumetric CT datasets, which vary widely in annotation
formats and granularity. To address these limitations, we introduce Uniferum, a
volumetric VLM that unifies diverse supervision signals, encoded in
classification labels and segmentation masks, into a single training framework.
By harmonizing three public 3D CT datasets with distinct annotations, Uniferum
achieves state-of-the-art performance, improving AUROC on the CT-RATE benchmark
by 7% compared to CLIP-based and conventional multi-label convolutional models.
The model demonstrates robust out-of-distribution generalization, with observed
evidence of unexpected zero-shot performance on the RAD-CHEST and INSPECT
datasets. Our results highlight the effectiveness of integrating heterogeneous
annotations and body segmentation to enhance model performance, setting a new
direction for clinically reliable, data-efficient VLMs in 3D medical imaging.

</details>


### [579] [O-DisCo-Edit: Object Distortion Control for Unified Realistic Video Editing](https://arxiv.org/abs/2509.01596)
*Yuqing Chen,Junjie Wang,Lin Liu,Ruihang Chu,Xiaopeng Zhang,Qi Tian,Yujiu Yang*

Main category: cs.CV

TL;DR: 提出O - DisCo - Edit统一框架用于视频编辑，实验显示其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型用于视频编辑时，可控编辑因需精确操纵多样对象属性而困难，且不同编辑任务需不同控制信号，增加模型设计复杂度和训练资源需求。

Method: 提出包含新颖对象失真控制（O - DisCo）的统一框架O - DisCo - Edit，结合基于随机和自适应噪声的控制信号与“复制形式”保留模块，通过有效训练范式实现编辑。

Result: 广泛实验和全面人工评估表明，O - DisCo - Edit在各种视频编辑任务中超越了专业和多任务的现有先进方法。

Conclusion: O - DisCo - Edit是一个有效的视频编辑统一框架，能实现高效、高保真的视频编辑。

Abstract: Diffusion models have recently advanced video editing, yet controllable
editing remains challenging due to the need for precise manipulation of diverse
object properties. Current methods require different control signal for diverse
editing tasks, which complicates model design and demands significant training
resources. To address this, we propose O-DisCo-Edit, a unified framework that
incorporates a novel object distortion control (O-DisCo). This signal, based on
random and adaptive noise, flexibly encapsulates a wide range of editing cues
within a single representation. Paired with a "copy-form" preservation module
for preserving non-edited regions, O-DisCo-Edit enables efficient,
high-fidelity editing through an effective training paradigm. Extensive
experiments and comprehensive human evaluations consistently demonstrate that
O-DisCo-Edit surpasses both specialized and multitask state-of-the-art methods
across various video editing tasks.
https://cyqii.github.io/O-DisCo-Edit.github.io/

</details>


### [580] [AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef](https://arxiv.org/abs/2509.01019)
*Scarlett Raine,Benjamin Moshirian,Tobias Fischer*

Main category: cs.CV

TL;DR: 因气候变化等，珊瑚礁濒临崩溃，需自动化扩大修复规模。提出由AI等驱动的珊瑚补种设备自动部署方案，测试有较好结果并公开数据。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁因气候变化、海洋酸化和污染濒临崩溃，需引入自动化扩大珊瑚礁修复规模。

Method: 提出由人工智能、计算机视觉和机器人技术驱动的珊瑚补种设备自动部署方案，进行自动化海底基质分类。

Result: 算法在大堡礁测试中，部署准确率达77.8%，子图像块分类准确率达89.1%，实时模型推理速度为每秒5.5帧，还公开了大量标注的基质图像数据。

Conclusion: 自动化方案能减少对人类专家的依赖，提高珊瑚礁修复的范围和效率。

Abstract: Coral reefs are on the brink of collapse, with climate change, ocean
acidification, and pollution leading to a projected 70-90% loss of coral
species within the next decade. Restoration efforts are crucial, but their
success hinges on introducing automation to upscale efforts. We present
automated deployment of coral re-seeding devices powered by artificial
intelligence, computer vision, and robotics. Specifically, we perform automated
substrate classification, enabling detection of areas of the seafloor suitable
for coral growth, thus significantly reducing reliance on human experts and
increasing the range and efficiency of restoration. Real-world testing of the
algorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%,
sub-image patch classification of 89.1%, and real-time model inference at 5.5
frames per second. Further, we present and publicly contribute a large
collection of annotated substrate image data to foster future research in this
area.

</details>


### [581] [TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D Force Estimation in Catheterization](https://arxiv.org/abs/2509.01605)
*Pedram Fekri,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.CV

TL;DR: 提出新颖的编码器 - 解码器视觉Transformer模型用于导管分割和3D力估计，在合成X光图像实验中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多任务深度学习模型有改进空间，从新视角解决导管分割和力估计问题。

Method: 提出新颖的编码器 - 解码器视觉Transformer模型，处理两个输入X光图像序列，利用共享分割头和回归头进行分割和力估计。

Result: 模型在不同噪声水平的合成X光图像上实验，优于现有纯分割模型、基于视觉的导管力估计方法和多任务方法。

Conclusion: 该模型在导管分割和力估计方面达到新的最优水平。

Abstract: Recently, the emergence of multitask deep learning models has enhanced
catheterization procedures by providing tactile and visual perception data
through an end-to-end architec- ture. This information is derived from a
segmentation and force estimation head, which localizes the catheter in X-ray
images and estimates the applied pressure based on its deflection within the
image. These stereo vision architectures incorporate a CNN- based
encoder-decoder that captures the dependencies between X-ray images from two
viewpoints, enabling simultaneous 3D force estimation and stereo segmentation
of the catheter. With these tasks in mind, this work approaches the problem
from a new perspective. We propose a novel encoder-decoder Vision Transformer
model that processes two input X-ray images as separate sequences. Given
sequences of X-ray patches from two perspectives, the transformer captures
long-range dependencies without the need to gradually expand the receptive
field for either image. The embeddings generated by both the encoder and
decoder are fed into two shared segmentation heads, while a regression head
employs the fused information from the decoder for 3D force estimation. The
proposed model is a stereo Vision Transformer capable of simultaneously
segmenting the catheter from two angles while estimating the generated forces
at its tip in 3D. This model has undergone extensive experiments on synthetic
X-ray images with various noise levels and has been compared against
state-of-the-art pure segmentation models, vision-based catheter force
estimation methods, and a multitask catheter segmentation and force estimation
approach. It outperforms existing models, setting a new state-of-the-art in
both catheter segmentation and force estimation.

</details>


### [582] [Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling](https://arxiv.org/abs/2509.01624)
*Natalia Frumkin,Diana Marculescu*

Main category: cs.CV

TL;DR: 提出Q - Sched范式用于扩散模型训练后量化，调整采样轨迹，结合JAQ损失，减少模型大小并提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型计算密集，现有少步扩散模型依赖大的未压缩骨干网络，限制了训练后量化方法，需要新的量化范式。

Method: 引入Q - Sched范式修改扩散模型调度器；提出JAQ损失学习量化感知预条件系数。

Result: 实现模型大小4倍缩减且达到全精度准确率，FID指标有显著提升，用户研究确认其有效性。

Conclusion: 量化和少步蒸馏对高保真图像生成是互补的，Q - Sched有效。

Abstract: Text-to-image diffusion models are computationally intensive, often requiring
dozens of forward passes through large transformer backbones. For instance,
Stable Diffusion XL generates high-quality images with 50 evaluations of a
2.6B-parameter model, an expensive process even for a single batch. Few-step
diffusion models reduce this cost to 2-8 denoising steps but still depend on
large, uncompressed U-Net or diffusion transformer backbones, which are often
too costly for full-precision inference without datacenter GPUs. These
requirements also limit existing post-training quantization methods that rely
on full-precision calibration. We introduce Q-Sched, a new paradigm for
post-training quantization that modifies the diffusion model scheduler rather
than model weights. By adjusting the few-step sampling trajectory, Q-Sched
achieves full-precision accuracy with a 4x reduction in model size. To learn
quantization-aware pre-conditioning coefficients, we propose the JAQ loss,
which combines text-image compatibility with an image quality metric for
fine-grained optimization. JAQ is reference-free and requires only a handful of
calibration prompts, avoiding full-precision inference during calibration.
Q-Sched delivers substantial gains: a 15.5% FID improvement over the FP16
4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step
Phased Consistency Model, showing that quantization and few-step distillation
are complementary for high-fidelity generation. A large-scale user study with
more than 80,000 annotations further confirms Q-Sched's effectiveness on both
FLUX.1[schnell] and SDXL-Turbo.

</details>


### [583] [Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models](https://arxiv.org/abs/2509.01167)
*Hyunjong Ok,Jaeho Lee*

Main category: cs.CV

TL;DR: 研究指出流行视觉编码器在视频MLLM关键帧识别能力有限，需更好技术。


<details>
  <summary>Details</summary>
Motivation: 探究现有视觉语言编码器能否真正识别视频中最具信息的帧，避免处理全帧的高计算成本。

Method: 通过实证研究，揭示流行视觉编码器能力局限。

Result: 发现流行视觉编码器在确定MLLM处理文本查询时应关注的视频区域能力有限。

Conclusion: 高效视频MLLM可能需要开发更好的关键帧识别技术。

Abstract: Recent advances in multimodal large language models (MLLMs) have led to much
progress in video understanding tasks. To avoid the heavy computational cost of
processing all frames, these models typically rely on keyframe sampling methods
guided by vision-language encoders (\textit{e.g.,} SigLIP). However, it remains
unclear whether such encoders can truly identify the most informative frames.
In this work, we provide several empirical pieces of evidence revealing that
popular vision encoders critically suffer from their limited capability to
identify where the MLLM should look inside the video to handle the given
textual query appropriately. Our findings suggest that the development of
better keyframe identification techniques may be necessary for efficient video
MLLMs.

</details>


### [584] [Deep Learning-Based Rock Particulate Classification Using Attention-Enhanced ConvNeXt](https://arxiv.org/abs/2509.01704)
*Anthony Amankwah,Chris Aldrich*

Main category: cs.CV

TL;DR: 提出基于ConvNeXt架构的增强深度学习模型CNSCA用于岩石尺寸分类，经评估证明注意力机制可提升模型能力。


<details>
  <summary>Details</summary>
Motivation: 准确的岩石尺寸分类对岩土工程、采矿和资源管理至关重要，精确估计影响运营效率和安全。

Method: 提出基于ConvNeXt架构的增强深度学习模型CNSCA，引入自注意力机制捕捉长距离空间依赖，引入通道注意力机制强调信息特征通道。

Result: 在岩石尺寸分类数据集上评估模型并与三个强基线进行比较，结果表明注意力机制显著增强了模型对岩石等自然纹理细粒度分类任务的能力。

Conclusion: 引入注意力机制的CNSCA模型能有效捕捉岩石图像中的局部模式和上下文关系，提高了分类准确性和鲁棒性。

Abstract: Accurate classification of rock sizes is a vital component in geotechnical
engineering, mining, and resource management, where precise estimation
influences operational efficiency and safety. In this paper, we propose an
enhanced deep learning model based on the ConvNeXt architecture, augmented with
both self-attention and channel attention mechanisms. Building upon the
foundation of ConvNext, our proposed model, termed CNSCA, introduces
self-attention to capture long-range spatial dependencies and channel attention
to emphasize informative feature channels. This hybrid design enables the model
to effectively capture both fine-grained local patterns and broader contextual
relationships within rock imagery, leading to improved classification accuracy
and robustness. We evaluate our model on a rock size classification dataset and
compare it against three strong baseline. The results demonstrate that the
incorporation of attention mechanisms significantly enhances the models
capability for fine-grained classification tasks involving natural textures
like rocks.

</details>


### [585] [AgroSense: An Integrated Deep Learning System for Crop Recommendation via Soil Image Analysis and Nutrient Profiling](https://arxiv.org/abs/2509.01344)
*Vishal Pandey,Ranjita Das,Debasmita Biswas*

Main category: cs.CV

TL;DR: 介绍了AgroSense框架，结合土壤图像分类和养分分析进行作物推荐，评估效果好，为精准农业提供实时决策支持。


<details>
  <summary>Details</summary>
Motivation: 传统土壤分析技术慢且不适用于田间决策，为满足全球粮食安全和可持续农业对实时智能作物推荐系统的需求。

Method: 引入AgroSense框架，含土壤分类模块（用ResNet - 18等架构）和作物推荐模块（用多层感知器等），用多模态数据集评估。

Result: 融合模型准确率达98.0%，精度97.8%，召回率97.7%，F1分数96.75%，RMSE和MAE分别降至0.32和0.27。

Conclusion: AgroSense为精准农业实时决策提供实用、可扩展方案，为资源受限环境下的轻量级多模态AI系统奠定基础。

Abstract: Meeting the increasing global demand for food security and sustainable
farming requires intelligent crop recommendation systems that operate in real
time. Traditional soil analysis techniques are often slow, labor-intensive, and
not suitable for on-field decision-making. To address these limitations, we
introduce AgroSense, a deep-learning framework that integrates soil image
classification and nutrient profiling to produce accurate and contextually
relevant crop recommendations. AgroSense comprises two main components: a Soil
Classification Module, which leverages ResNet-18, EfficientNet-B0, and Vision
Transformer architectures to categorize soil types from images; and a Crop
Recommendation Module, which employs a Multi-Layer Perceptron, XGBoost,
LightGBM, and TabNet to analyze structured soil data, including nutrient
levels, pH, and rainfall. We curated a multimodal dataset of 10,000 paired
samples drawn from publicly available Kaggle repositories, approximately 50,000
soil images across seven classes, and 25,000 nutrient profiles for experimental
evaluation. The fused model achieves 98.0% accuracy, with a precision of 97.8%,
a recall of 97.7%, and an F1-score of 96.75%, while RMSE and MAE drop to 0.32
and 0.27, respectively. Ablation studies underscore the critical role of
multimodal coupling, and statistical validation via t-tests and ANOVA confirms
the significance of our improvements. AgroSense offers a practical, scalable
solution for real-time decision support in precision agriculture and paves the
way for future lightweight multimodal AI systems in resource-constrained
environments.

</details>


### [586] [M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision](https://arxiv.org/abs/2509.01360)
*Che Liu,Zheng Jiang,Chengyu Fang,Heng Guo,Yan-Jie Zhou,Jiaqi Qu,Le Lu,Minfeng Xu*

Main category: cs.CV

TL;DR: 文章提出统一视觉编码器M3Ret，利用大规模混合模态数据集进行自监督学习，在零样本图像检索等方面表现优异，有良好泛化性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像检索方法因模态特定设计，缺乏统一表示，影响可扩展性，需实现统一学习。

Method: 策划大规模混合模态数据集，训练无模态特定定制的统一视觉编码器M3Ret，使用生成式和对比式自监督学习范式。

Result: 在各模态零样本图像检索中达新的最优水平，无需配对数据实现强跨模态对齐，模型能泛化到未见的MRI任务。

Conclusion: 研究成果为医学影像界带来希望，M3Ret向多模态医学图像理解的视觉自监督基础模型迈进了一步。

Abstract: Medical image retrieval is essential for clinical decision-making and
translational research, relying on discriminative visual representations. Yet,
current methods remain fragmented, relying on separate architectures and
training strategies for 2D, 3D, and video-based medical data. This
modality-specific design hampers scalability and inhibits the development of
unified representations. To enable unified learning, we curate a large-scale
hybrid-modality dataset comprising 867,653 medical imaging samples, including
2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging
this dataset, we train M3Ret, a unified visual encoder without any
modality-specific customization. It successfully learns transferable
representations using both generative (MAE) and contrastive (SimDINO)
self-supervised learning (SSL) paradigms. Our approach sets a new
state-of-the-art in zero-shot image-to-image retrieval across all individual
modalities, surpassing strong baselines such as DINOv3 and the text-supervised
BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired
data, and the model generalizes to unseen MRI tasks, despite never observing
MRI during pretraining, demonstrating the generalizability of purely visual
self-supervision to unseen modalities. Comprehensive analyses further validate
the scalability of our framework across model and data sizes. These findings
deliver a promising signal to the medical imaging community, positioning M3Ret
as a step toward foundation models for visual SSL in multimodal medical image
understanding.

</details>


### [587] [Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction](https://arxiv.org/abs/2509.01873)
*Xueyang Kang*

Main category: cs.CV

TL;DR: 论文指出3D深度学习训练有挑战，传统技术在非结构化环境有局限，提出结合传统几何技术与深度学习的方法解决3D视觉挑战并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前3D深度学习直接训练有挑战，传统SfM和SLAM技术在非结构化环境表现不佳，需开发结合传统几何与深度学习的3D表示方法。

Method: 开发针对相机位姿估计、点云配准、深度预测和3D重建等任务的几何深度学习方法，将几何先验或约束融入深度学习模型。

Result: 提升了几何表示的准确性和鲁棒性，在数字文化遗产保护和沉浸式VR/AR环境等现实应用中证明了有效性。

Conclusion: 结合传统几何技术与深度学习的方法能有效解决3D视觉的基本挑战。

Abstract: Modern deep learning developments create new opportunities for 3D mapping
technology, scene reconstruction pipelines, and virtual reality development.
Despite advances in 3D deep learning technology, direct training of deep
learning models on 3D data faces challenges due to the high dimensionality
inherent in 3D data and the scarcity of labeled datasets. Structure-from-motion
(SfM) and Simultaneous Localization and Mapping (SLAM) exhibit robust
performance when applied to structured indoor environments but often struggle
with ambiguous features in unstructured environments. These techniques often
struggle to generate detailed geometric representations effective for
downstream tasks such as rendering and semantic analysis. Current limitations
require the development of 3D representation methods that combine traditional
geometric techniques with deep learning capabilities to generate robust
geometry-aware deep learning models.
  The dissertation provides solutions to the fundamental challenges in 3D
vision by developing geometric deep learning methods tailored for essential
tasks such as camera pose estimation, point cloud registration, depth
prediction, and 3D reconstruction. The integration of geometric priors or
constraints, such as including depth information, surface normals, and
equivariance into deep learning models, enhances both the accuracy and
robustness of geometric representations. This study systematically investigates
key components of 3D vision, including camera pose estimation, point cloud
registration, depth estimation, and high-fidelity 3D reconstruction,
demonstrating their effectiveness across real-world applications such as
digital cultural heritage preservation and immersive VR/AR environments.

</details>


### [588] [HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision](https://arxiv.org/abs/2509.01882)
*Shubham Laxmikant Deshmukh,Matthew Wilchek,Feras A. Batarseh*

Main category: cs.CV

TL;DR: 本文介绍基于深度学习的水质监测框架HydroVision，用RGB图像估计水质参数，经模型评估DenseNet121表现最佳，未来将提升其在复杂场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉发展推动环境监测新应用，需非接触方法评估水质和检测污染，以用于灾害响应和公共卫生保护。

Method: 引入HydroVision框架，用2022 - 2024年美国地质调查局超50万张季节性变化图像训练，通过迁移学习评估四种卷积神经网络和视觉变换器。

Result: DenseNet121验证性能最高，预测CDOM的R2分数达0.89。

Conclusion: 该框架有用于实际水质监测的潜力，未来需提升其在低光照和有遮挡场景的鲁棒性。

Abstract: Ongoing advancements in computer vision, particularly in pattern recognition
and scene classification, have enabled new applications in environmental
monitoring. Deep learning now offers non-contact methods for assessing water
quality and detecting contamination, both critical for disaster response and
public health protection. This work introduces HydroVision, a deep
learning-based scene classification framework that estimates optically active
water quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored
Dissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and
Turbidity from standard Red-Green-Blue (RGB) images of surface water.
HydroVision supports early detection of contamination trends and strengthens
monitoring by regulatory agencies during external environmental stressors,
industrial activities, and force majeure events. The model is trained on more
than 500,000 seasonally varied images collected from the United States
Geological Survey Hydrologic Imagery Visualization and Information System
between 2022 and 2024. This approach leverages widely available RGB imagery as
a scalable, cost-effective alternative to traditional multispectral and
hyperspectral remote sensing. Four state-of-the-art convolutional neural
networks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer
are evaluated through transfer learning to identify the best-performing
architecture. DenseNet121 achieves the highest validation performance, with an
R2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for
real-world water quality monitoring across diverse conditions. While the
current model is optimized for well-lit imagery, future work will focus on
improving robustness under low-light and obstructed scenarios to expand its
operational utility.

</details>


### [589] [Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework](https://arxiv.org/abs/2509.01910)
*Furong Jia,Lanxin Liu,Ce Hou,Fan Zhang,Xinyan Liu,Yu Liu*

Main category: cs.CV

TL;DR: 提出结合全球地理定位与概念瓶颈的框架，提升地理定位模型可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有地理定位模型如GeoCLIP可解释性不足，当前基于概念的可解释性方法与地理对齐目标不匹配。

Method: 插入概念感知对齐模块，将图像和位置嵌入投影到地理概念库，最小化概念级损失。

Result: 在地理定位准确性上超越GeoCLIP，提升多种地理空间预测任务性能。

Conclusion: 该方法为地理定位引入可解释性，能揭示地理决策过程中更丰富的语义信息。

Abstract: Worldwide geo-localization involves determining the exact geographic location
of images captured globally, typically guided by geographic cues such as
climate, landmarks, and architectural styles. Despite advancements in
geo-localization models like GeoCLIP, which leverages images and location
alignment via contrastive learning for accurate predictions, the
interpretability of these models remains insufficiently explored. Current
concept-based interpretability methods fail to align effectively with
Geo-alignment image-location embedding objectives, resulting in suboptimal
interpretability and performance. To address this gap, we propose a novel
framework integrating global geo-localization with concept bottlenecks. Our
method inserts a Concept-Aware Alignment Module that jointly projects image and
location embeddings onto a shared bank of geographic concepts (e.g., tropical
climate, mountain, cathedral) and minimizes a concept-level loss, enhancing
alignment in a concept-specific subspace and enabling robust interpretability.
To our knowledge, this is the first work to introduce interpretability into
geo-localization. Extensive experiments demonstrate that our approach surpasses
GeoCLIP in geo-localization accuracy and boosts performance across diverse
geospatial prediction tasks, revealing richer semantic insights into geographic
decision-making processes.

</details>


### [590] [Structure-aware Contrastive Learning for Diagram Understanding of Multimodal Models](https://arxiv.org/abs/2509.01959)
*Hiroshi Sasaki*

Main category: cs.CV

TL;DR: 提出新训练范式提升视觉语言模型对图表图像理解，在流程图数据集上验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在处理图表等专业视觉领域存在局限，需提升对图表图像理解。

Method: 使用“硬”样本进行对比学习，结合两个利用图表固有结构特性的损失函数。

Result: 在流程图基准数据集上，在图像文本匹配和视觉问答任务上相比标准CLIP和传统硬负样本CLIP学习范式有显著提升。

Conclusion: 强调针对专业任务定制训练策略的重要性，有助于推动视觉语言集成领域的图表理解。

Abstract: Multimodal models, such as the Contrastive Language-Image Pre-training (CLIP)
model, have demonstrated remarkable success in aligning visual and linguistic
representations. However, these models exhibit limitations when applied to
specialised visual domains, such as diagrams, which encode structured, symbolic
information distinct from that of natural imagery.
  In this paper, we introduce a novel training paradigm explicitly designed to
enhance the comprehension of diagrammatic images within vision-language models.
Our approach uses ``hard'' samples for our proposed contrastive learning that
incorporates two specialised loss functions that leverage the inherent
structural properties of diagrams. By integrating these objectives into model
training, our method enables models to develop a more structured and
semantically coherent understanding of diagrammatic content.
  We empirically validate our approach on a benchmark dataset of flowcharts, as
a representative class of diagrammatic imagery, demonstrating substantial
improvements over standard CLIP and conventional hard negative CLIP learning
paradigms for both image-text matching and visual question answering tasks. Our
findings underscore the significance of tailored training strategies for
specialised tasks and contribute to advancing diagrammatic understanding within
the broader landscape of vision-language integration.

</details>


### [591] [2D Gaussian Splatting with Semantic Alignment for Image Inpainting](https://arxiv.org/abs/2509.01964)
*Hongyu Li,Chaofeng Chen,Xiaoming Li,Guangming Lu*

Main category: cs.CV

TL;DR: 本文探索高斯 splatting 在图像修复中的潜力，提出基于 2D 高斯 splatting 的图像修复框架，经实验验证性能良好，为 2D 图像处理开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 探索高斯 splatting 在图像修复领域未发掘的潜力，以满足图像修复中局部像素合成连贯和全局语义恢复一致的需求。

Method: 提出基于 2D 高斯 splatting 的图像修复框架，采用逐块光栅化策略提高效率和可扩展性，引入预训练 DINO 模型特征确保全局语义一致性。

Result: 在标准基准测试中，该方法在定量指标和感知质量上均取得有竞争力的表现。

Conclusion: 为高斯 splatting 在 2D 图像处理中的应用建立了新方向。

Abstract: Gaussian Splatting (GS), a recent technique for converting discrete points
into continuous spatial representations, has shown promising results in 3D
scene modeling and 2D image super-resolution. In this paper, we explore its
untapped potential for image inpainting, which demands both locally coherent
pixel synthesis and globally consistent semantic restoration. We propose the
first image inpainting framework based on 2D Gaussian Splatting, which encodes
incomplete images into a continuous field of 2D Gaussian splat coefficients and
reconstructs the final image via a differentiable rasterization process. The
continuous rendering paradigm of GS inherently promotes pixel-level coherence
in the inpainted results. To improve efficiency and scalability, we introduce a
patch-wise rasterization strategy that reduces memory overhead and accelerates
inference. For global semantic consistency, we incorporate features from a
pretrained DINO model. We observe that DINO's global features are naturally
robust to small missing regions and can be effectively adapted to guide
semantic alignment in large-mask scenarios, ensuring that the inpainted content
remains contextually consistent with the surrounding scene. Extensive
experiments on standard benchmarks demonstrate that our method achieves
competitive performance in both quantitative metrics and perceptual quality,
establishing a new direction for applying Gaussian Splatting to 2D image
processing.

</details>


### [592] [Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination](https://arxiv.org/abs/2509.01986)
*Ziyun Zeng,Junhao Zhang,Wei Li,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 文章提出Draw - In - Mind (DIM)数据集解决统一模型图像编辑能力不足问题，训练出DIM - 4.6B - T2I/Edit模型，在相关基准测试表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型在文本到图像生成上效果好，但图像编辑能力弱，原因是理解和生成模块职责划分不平衡。

Method: 引入包含DIM - T2I和DIM - Edit两个子集的DIM数据集，用轻量级两层MLP连接冻结的Qwen2.5 - VL - 3B和可训练的SANA1.5 - 1.6B并在DIM数据集上训练。

Result: DIM - 4.6B - Edit在ImgEdit和GEdit - Bench基准测试达到SOTA或有竞争力，超越更大模型。

Conclusion: 明确将设计职责分配给理解模块对图像编辑有显著好处。

Abstract: In recent years, integrating multimodal understanding and generation into a
single unified model has emerged as a promising paradigm. While this approach
achieves strong results in text-to-image (T2I) generation, it still struggles
with precise image editing. We attribute this limitation to an imbalanced
division of responsibilities. The understanding module primarily functions as a
translator that encodes user instructions into semantic conditions, while the
generation module must simultaneously act as designer and painter, inferring
the original layout, identifying the target editing region, and rendering the
new content. This imbalance is counterintuitive because the understanding
module is typically trained with several times more data on complex reasoning
tasks than the generation module. To address this issue, we introduce
Draw-In-Mind (DIM), a dataset comprising two complementary subsets: (i)
DIM-T2I, containing 14M long-context image-text pairs to enhance complex
instruction comprehension; and (ii) DIM-Edit, consisting of 233K
chain-of-thought imaginations generated by GPT-4o, serving as explicit design
blueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable
SANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM
dataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale,
DIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and
GEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1
and Step1X-Edit. These findings demonstrate that explicitly assigning the
design responsibility to the understanding module provides significant benefits
for image editing. Our dataset and models will be available at
https://github.com/showlab/DIM.

</details>


### [593] [Unsupervised Training of Vision Transformers with Synthetic Negatives](https://arxiv.org/abs/2509.02024)
*Nikolaos Giakoumoglou,Andreas Floros,Kleanthis Marios Papadopoulos,Tania Stathaki*

Main category: cs.CV

TL;DR: 本文挖掘自监督学习中硬负样本潜力，将合成硬负样本集成到视觉变换器以改善表征学习，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决自监督学习中硬负样本潜力被忽视的问题，且此前在视觉变换器中对合成硬负样本探索较少。

Method: 将合成硬负样本集成到视觉变换器中以进行表征学习。

Result: 实验表明，该技术显著提升了学习表征的判别能力，在DeiT - S和Swin - T架构上均有性能提升。

Conclusion: 这种简单有效的技术能有效提升视觉变换器表征学习性能。

Abstract: This paper does not introduce a novel method per se. Instead, we address the
neglected potential of hard negative samples in self-supervised learning.
Previous works explored synthetic hard negatives but rarely in the context of
vision transformers. We build on this observation and integrate synthetic hard
negatives to improve vision transformer representation learning. This simple
yet effective technique notably improves the discriminative power of learned
representations. Our experiments show performance improvements for both DeiT-S
and Swin-T architectures.

</details>


### [594] [Fake & Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives](https://arxiv.org/abs/2509.02029)
*Nikolaos Giakoumoglou,Andreas Floros,Kleanthis Marios Papadopoulos,Tania Stathaki*

Main category: cs.CV

TL;DR: 本文基于现有视觉自监督学习方法，提出Syn2Co框架，研究合成数据在自监督学习中的应用，指出其潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 现有对比自监督学习依赖大量真实数据和精心挑选的难样本，探索替代方案。

Method: 研究生成模型用于无监督表征学习，利用合成数据增加样本多样性；在表征空间生成合成难样本；提出Syn2Co框架并在DeiT - S和Swin - T架构上评估。

Result: 未明确提及具体结果，但指出研究突出了合成数据在自监督学习中的潜力和局限。

Conclusion: 合成数据在自监督学习中有潜力和局限，为后续研究提供了洞察。

Abstract: This paper does not introduce a new method per se. Instead, we build on
existing self-supervised learning approaches for vision, drawing inspiration
from the adage "fake it till you make it". While contrastive self-supervised
learning has achieved remarkable success, it typically relies on vast amounts
of real-world data and carefully curated hard negatives. To explore
alternatives to these requirements, we investigate two forms of "faking it" in
vision transformers. First, we study the potential of generative models for
unsupervised representation learning, leveraging synthetic data to augment
sample diversity. Second, we examine the feasibility of generating synthetic
hard negatives in the representation space, creating diverse and challenging
contrasts. Our framework - dubbed Syn2Co - combines both approaches and
evaluates whether synthetically enhanced training can lead to more robust and
transferable visual representations on DeiT-S and Swin-T architectures. Our
findings highlight the promise and limitations of synthetic data in
self-supervised learning, offering insights for future work in this direction.

</details>


### [595] [SALAD -- Semantics-Aware Logical Anomaly Detection](https://arxiv.org/abs/2509.02101)
*Matic Fučka,Vitjan Zavrtanik,Danijel Skočaj*

Main category: cs.CV

TL;DR: 提出SALAD方法用于逻辑异常检测，在MVTec LOCO上表现出色，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有表面异常检测方法难以处理逻辑异常，最佳逻辑异常检测方法会丢弃空间和语义信息导致性能不佳。

Method: 提出SALAD方法，引入新的组成分支显式建模对象组成图分布，采用无需手工标签和特定类别信息的新组成图提取程序。

Result: 在MVTec LOCO标准基准上显著优于现有方法，图像级AUROC达96.1%。

Conclusion: SALAD方法通过有效建模组成图分布，能提升逻辑异常检测性能。

Abstract: Recent surface anomaly detection methods excel at identifying structural
anomalies, such as dents and scratches, but struggle with logical anomalies,
such as irregular or missing object components. The best-performing logical
anomaly detection approaches rely on aggregated pretrained features or
handcrafted descriptors (most often derived from composition maps), which
discard spatial and semantic information, leading to suboptimal performance. We
propose SALAD, a semantics-aware discriminative logical anomaly detection
method that incorporates a newly proposed composition branch to explicitly
model the distribution of object composition maps, consequently learning
important semantic relationships. Additionally, we introduce a novel procedure
for extracting composition maps that requires no hand-made labels or
category-specific information, in contrast to previous methods. By effectively
modelling the composition map distribution, SALAD significantly improves upon
state-of-the-art methods on the standard benchmark for logical anomaly
detection, MVTec LOCO, achieving an impressive image-level AUROC of 96.1%.
Code: https://github.com/MaticFuc/SALAD

</details>


### [596] [Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks](https://arxiv.org/abs/2509.02175)
*Nils Hoehing,Mayug Maniparambil,Ellen Rushe,Noel E. O'Connor,Anthony Ventresque*

Main category: cs.CV

TL;DR: 提出开源对比视觉语言模型基准RocketScience测试空间关系理解，发现现有模型不足，定位瓶颈并开源数据代码。


<details>
  <summary>Details</summary>
Motivation: 测试视觉语言模型的空间关系理解能力。

Method: 构建全新真实世界图像 - 文本对组成的基准测试集，进行实验和分解分析。

Result: 开源和前沿商业VLM缺乏空间关系理解能力，推理模型表现好，基于思维链模型的性能瓶颈是空间推理而非对象定位。

Conclusion: 当前VLM在空间关系理解上存在明显不足，可通过提升空间推理能力改进。

Abstract: We propose RocketScience, an open-source contrastive VLM benchmark that tests
for spatial relation understanding. It is comprised of entirely new real-world
image-text pairs covering mostly relative spatial understanding and the order
of objects. The benchmark is designed
  to be very easy for humans and hard for the current generation of VLMs, and
this is empirically verified. Our results show a striking lack of spatial
relation understanding in open source and frontier commercial VLMs and a
surprisingly high performance of reasoning models. Additionally, we perform a
disentanglement analysis to separate the contributions of object localization
and spatial reasoning in chain-of-thought-based models and find that the
performance on the benchmark is bottlenecked by spatial reasoning and not
object localization capabilities.
  We release the dataset with a CC-BY-4.0 license and make the evaluation code
available at: https://github.com/nilshoehing/rocketscience

</details>


### [597] [Vision-Based Embedded System for Noncontact Monitoring of Preterm Infant Behavior in Low-Resource Care Settings](https://arxiv.org/abs/2509.02018)
*Stanley Mugisha,Rashid Kisitu,Francis Komakech,Excellence Favor*

Main category: cs.CV

TL;DR: 本文提出基于视觉的非侵入式自动框架，用量化MobileNet模型在树莓派上实现新生儿行为状态实时检测，准确率高且计算高效，表明轻量级模型适合资源受限环境的NICU监测。


<details>
  <summary>Details</summary>
Motivation: 早产是新生儿死亡主因，低资源地区缺乏先进NICU，现有婴儿行为监测依赖手动或侵入式传感器，存在误差大、不实用和易造成皮肤损伤等问题。

Method: 引入嵌入式监测系统，利用量化MobileNet模型在树莓派上进行实时行为状态检测，结合模型量化、树莓派优化视觉管道和安全物联网通信。

Result: 在公共新生儿图像数据集上训练和评估，睡眠检测准确率91.8%，哭闹/正常分类准确率97.7%，计算高效适合边缘部署；大架构模型准确率提升有限但计算成本高。

Conclusion: 轻量级优化模型如MobileNet为可扩展、低成本且临床可行的NICU监测系统提供了最可行基础，有助于改善资源受限环境下的早产护理。

Abstract: Preterm birth remains a leading cause of neonatal mortality,
disproportionately affecting low-resource settings with limited access to
advanced neonatal intensive care units (NICUs).Continuous monitoring of infant
behavior, such as sleep/awake states and crying episodes, is critical but
relies on manual observation or invasive sensors, which are prone to error,
impractical, and can cause skin damage. This paper presents a novel,
noninvasive, and automated vision-based framework to address this gap. We
introduce an embedded monitoring system that utilizes a quantized MobileNet
model deployed on a Raspberry Pi for real-time behavioral state detection. When
trained and evaluated on public neonatal image datasets, our system achieves
state-of-the-art accuracy (91.8% for sleep detection and 97.7% for
crying/normal classification) while maintaining computational efficiency
suitable for edge deployment. Through comparative benchmarking, we provide a
critical analysis of the trade-offs between model size, inference latency, and
diagnostic accuracy. Our findings demonstrate that while larger architectures
(e.g., ResNet152, VGG19) offer marginal gains in accuracy, their computational
cost is prohibitive for real-time edge use. The proposed framework integrates
three key innovations: model quantization for memory-efficient inference (68%
reduction in size), Raspberry Pi-optimized vision pipelines, and secure IoT
communication for clinical alerts. This work conclusively shows that
lightweight, optimized models such as the MobileNet offer the most viable
foundation for scalable, low-cost, and clinically actionable NICU monitoring
systems, paving the way for improved preterm care in resource-constrained
environments.

</details>


### [598] [SegFormer Fine-Tuning with Dropout: Advancing Hair Artifact Removal in Skin Lesion Analysis](https://arxiv.org/abs/2509.02156)
*Asif Mohammed Saad,Umme Niraj Mahi*

Main category: cs.CV

TL;DR: 本文提出微调的SegFormerWithDropout模型进行毛发分割，在特定数据集上训练评估，结果显示模型在毛发分割上表现良好，可助力皮肤癌检测。


<details>
  <summary>Details</summary>
Motivation: 皮肤镜图像中的毛发伪影会影响皮肤病变分析，干扰诊断特征识别，因此需要精确分割毛发。

Method: 提出SegFormerWithDropout架构，使用预训练MiT - B2编码器，在分割头加入0.3的dropout概率防止过拟合；在500张带精细毛发掩码注释的皮肤镜图像数据集上训练，采用10折交叉验证、AdamW优化、交叉熵损失和早停策略；用IoU、Dice系数等多种指标评估。

Result: 交叉验证实验中，平均Dice系数约0.96，IoU值0.93，PSNR约34 dB，SSIM为0.97，LPIPS为0.06。

Conclusion: 该模型在毛发伪影分割上有效，能提升皮肤癌检测预处理效果。

Abstract: Hair artifacts in dermoscopic images present significant challenges for
accurate skin lesion analysis, potentially obscuring critical diagnostic
features in dermatological assessments. This work introduces a fine-tuned
SegFormer model augmented with dropout regularization to achieve precise hair
mask segmentation. The proposed SegformerWithDropout architecture leverages the
MiT-B2 encoder, pretrained on ImageNet, with an in-channel count of 3 and 2
output classes, incorporating a dropout probability of 0.3 in the segmentation
head to prevent overfitting. Training is conducted on a specialized dataset of
500 dermoscopic skin lesion images with fine-grained hair mask annotations,
employing 10-fold cross-validation, AdamW optimization with a learning rate of
0.001, and cross-entropy loss. Early stopping is applied based on validation
loss, with a patience of 3 epochs and a maximum of 20 epochs per fold.
Performance is evaluated using a comprehensive suite of metrics, including
Intersection over Union (IoU), Dice coefficient, Peak Signal-to-Noise Ratio
(PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch
Similarity (LPIPS). Experimental results from the cross-validation demonstrate
robust performance, with average Dice coefficients reaching approximately 0.96
and IoU values of 0.93, alongside favorable PSNR (around 34 dB), SSIM (0.97),
and low LPIPS (0.06), highlighting the model's effectiveness in accurate hair
artifact segmentation and its potential to enhance preprocessing for downstream
skin cancer detection tasks.

</details>


### [599] [Ordinal Adaptive Correction: A Data-Centric Approach to Ordinal Image Classification with Noisy Labels](https://arxiv.org/abs/2509.02351)
*Alireza Sedighi Moghaddam,Mohammad Reza Mohammadi*

Main category: cs.CV

TL;DR: 本文提出ORDAC方法解决序数图像分类中标签噪声问题，在基准数据集验证有效，证明自适应标签校正能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 标签标注过程易出错和有噪声，影响机器学习模型性能和可靠性，需解决序数图像分类中标签噪声检测和校正问题。

Method: 提出ORDAC方法，利用标签分布学习能力，训练时动态调整每个样本标签分布的均值和标准差，校正而非丢弃有噪声样本。

Result: 在年龄估计和疾病严重程度检测基准数据集上，ORDAC及其扩展版本提升了模型性能，如在Adience数据集上降低了平均绝对误差、提高了召回率，还能校正原始数据集中的固有噪声。

Conclusion: 使用标签分布进行自适应标签校正能增强含噪声数据下序数分类模型的鲁棒性和准确性。

Abstract: Labeled data is a fundamental component in training supervised deep learning
models for computer vision tasks. However, the labeling process, especially for
ordinal image classification where class boundaries are often ambiguous, is
prone to error and noise. Such label noise can significantly degrade the
performance and reliability of machine learning models. This paper addresses
the problem of detecting and correcting label noise in ordinal image
classification tasks. To this end, a novel data-centric method called ORDinal
Adaptive Correction (ORDAC) is proposed for adaptive correction of noisy
labels. The proposed approach leverages the capabilities of Label Distribution
Learning (LDL) to model the inherent ambiguity and uncertainty present in
ordinal labels. During training, ORDAC dynamically adjusts the mean and
standard deviation of the label distribution for each sample. Rather than
discarding potentially noisy samples, this approach aims to correct them and
make optimal use of the entire training dataset. The effectiveness of the
proposed method is evaluated on benchmark datasets for age estimation (Adience)
and disease severity detection (Diabetic Retinopathy) under various asymmetric
Gaussian noise scenarios. Results show that ORDAC and its extended versions
(ORDAC_C and ORDAC_R) lead to significant improvements in model performance.
For instance, on the Adience dataset with 40% noise, ORDAC_R reduced the mean
absolute error from 0.86 to 0.62 and increased the recall metric from 0.37 to
0.49. The method also demonstrated its effectiveness in correcting intrinsic
noise present in the original datasets. This research indicates that adaptive
label correction using label distributions is an effective strategy to enhance
the robustness and accuracy of ordinal classification models in the presence of
noisy data.

</details>


### [600] [From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation](https://arxiv.org/abs/2509.02419)
*Tao Wang,Zhenxuan Zhang,Yuanbo Zhou,Xinlin Zhang,Yuanbin Chen,Tao Tan,Guang Yang,Tong Tong*

Main category: cs.CV

TL;DR: 研究提出GSD - Net解决医学图像分割中注释噪声问题，在多个数据集实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络在医学图像分割中依赖高质量标注，而标注获取成本高且含噪声，影响模型性能。

Method: 提出GSD - Net，包含几何距离感知模块、结构引导标签细化模块和知识转移模块。

Result: 在六个公开数据集实验，GSD - Net在噪声注释下达SOTA性能，如在Kvasir等数据集有不同程度提升。

Conclusion: GSD - Net能有效应对医学图像分割中的噪声注释问题，提升模型性能。

Abstract: The effectiveness of convolutional neural networks in medical image
segmentation relies on large-scale, high-quality annotations, which are costly
and time-consuming to obtain. Even expert-labeled datasets inevitably contain
noise arising from subjectivity and coarse delineations, which disrupt feature
learning and adversely impact model performance. To address these challenges,
this study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which
integrates geometric and structural cues to improve robustness against noisy
annotations. It incorporates a Geometric Distance-Aware module that dynamically
adjusts pixel-level weights using geometric features, thereby strengthening
supervision in reliable regions while suppressing noise. A Structure-Guided
Label Refinement module further refines labels with structural priors, and a
Knowledge Transfer module enriches supervision and improves sensitivity to
local details. To comprehensively assess its effectiveness, we evaluated
GSD-Net on six publicly available datasets: four containing three types of
simulated label noise, and two with multi-expert annotations that reflect
real-world subjectivity and labeling inconsistencies. Experimental results
demonstrate that GSD-Net achieves state-of-the-art performance under noisy
annotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen,
8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of
this study are available at https://github.com/ortonwang/GSD-Net.

</details>


### [601] [Anisotropic Fourier Features for Positional Encoding in Medical Imaging](https://arxiv.org/abs/2509.02488)
*Nabil Jabareen,Dongsheng Yuan,Dingming Liu,Foo-Wei Ten,Sören Lukassen*

Main category: cs.CV

TL;DR: 研究探讨医学影像中位置编码作用，提出AFPE，经多任务测试，表明选对位置编码可提升性能，AFPE在各向异性场景表现优。


<details>
  <summary>Details</summary>
Motivation: 医学影像中常用位置编码方法应对复杂形状和各向异性图像存在局限性，需更好的编码方法。

Method: 提出Anisotropic Fourier Feature Positional Encoding (AFPE)，并在胸部X光、CT图像和超声心动图任务中与常用位置编码进行系统对比测试。

Result: 选择合适的位置编码能显著提升模型性能，最优编码依赖于感兴趣结构形状和数据各向异性，AFPE在各向异性设置中显著优于现有方法。

Conclusion: 在各向异性医学图像和视频中，选择适合数据和感兴趣形状的各向异性位置编码至关重要。

Abstract: The adoption of Transformer-based architectures in the medical domain is
growing rapidly. In medical imaging, the analysis of complex shapes - such as
organs, tissues, or other anatomical structures - combined with the often
anisotropic nature of high-dimensional images complicates these adaptations. In
this study, we critically examine the role of Positional Encodings (PEs),
arguing that commonly used approaches may be suboptimal for the specific
challenges of medical imaging. Sinusoidal Positional Encodings (SPEs) have
proven effective in vision tasks, but they struggle to preserve Euclidean
distances in higher-dimensional spaces. Isotropic Fourier Feature Positional
Encodings (IFPEs) have been proposed to better preserve Euclidean distances,
but they lack the ability to account for anisotropy in images. To address these
limitations, we propose Anisotropic Fourier Feature Positional Encoding (AFPE),
a generalization of IFPE that incorporates anisotropic, class-specific, and
domain-specific spatial dependencies. We systematically benchmark AFPE against
commonly used PEs on multi-label classification in chest X-rays, organ
classification in CT images, and ejection fraction regression in
echocardiography. Our results demonstrate that choosing the correct PE can
significantly improve model performance. We show that the optimal PE depends on
the shape of the structure of interest and the anisotropy of the data. Finally,
our proposed AFPE significantly outperforms state-of-the-art PEs in all tested
anisotropic settings. We conclude that, in anisotropic medical images and
videos, it is of paramount importance to choose an anisotropic PE that fits the
data and the shape of interest.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [602] [Continuous Petri Nets for Fast Yield Computation: Polynomial-Time and MILP Approaches](https://arxiv.org/abs/2509.02371)
*Addie Jordon,Juri Kolčák,Daniel Merkle*

Main category: cs.DM

TL;DR: 针对化学系统，利用连续Petri网多项式时间可解可达性的特性，提出计算分子最大产量的多项式时间算法，还给出基于混合整数线性规划的替代算法。


<details>
  <summary>Details</summary>
Motivation: 离散Petri网虽能模拟化学反应网络，但计算复杂度高，连续Petri网能在多项式时间解决可达性问题，可用于化学系统分子最大产量计算。

Method: 利用连续Petri网多项式时间可解可达性的特性设计算法，还给出基于混合整数线性规划的算法。

Result: 提出计算分子最大产量的多项式时间算法，替代算法理论复杂度差但实际运行时间更好。

Conclusion: 基于连续Petri网的算法可用于化学系统分子最大产量计算，替代算法在实际中有优势。

Abstract: Petri nets provide accurate analogues to chemical reaction networks, with
places representing individual molecules (the resources of the system) and
transitions representing chemical reactions which convert educt molecules into
product molecules. Their natural affinity for modeling chemical reaction
networks is, however, impeded by their computational complexity, which is at
least PSpace-hard for most interesting questions, including reachability.
Continuous Petri nets offer the same structure and discrete time as discrete
Petri nets, but use continuous state-space, which allows them to answer the
reachability question in polynomial time. We exploit this property to introduce
a polynomial time algorithm for computing the maximal yield of a molecule in a
chemical system. Additionally, we provide an alternative algorithm based on
mixed-integer linear programming with worse theoretical complexity, but better
runtime in practice, as demonstrated on both synthetic and chemical data.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [603] [Reinforcement learning for graph theory, Parallelizing Wagner's approach](https://arxiv.org/abs/2509.01607)
*Alix Bouffard,Jane Breen*

Main category: math.CO

TL;DR: 运用强化学习构建图拉普拉斯矩阵谱半径猜想界的反例，改进Wagner方法。


<details>
  <summary>Details</summary>
Motivation: 构建图拉普拉斯矩阵谱半径猜想界的反例。

Method: 扩展Stevanovic等人对Wagner方法的重新实现，可同时训练多个独特模型，重新定义动作空间以调整当前局部最优对学习过程的影响。

Result: 未提及

Conclusion: 未提及

Abstract: Our work applies reinforcement learning to construct counterexamples
concerning conjectured bounds on the spectral radius of the Laplacian matrix of
a graph. We expand upon the re-implementation of Wagner's approach by
Stevanovic et al. with the ability to train numerous unique models
simultaneously and a novel redefining of the action space to adjust the
influence of the current local optimum on the learning process.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [604] [Animer une base de connaissance: des ontologies aux mod{è}les d'I.A. g{é}n{é}rative](https://arxiv.org/abs/2509.01304)
*Peter Stockinger*

Main category: cs.DL

TL;DR: 文章基于区域研究知识库应用领域，对符号AI与神经AI混合进行符号学解读，介绍LaCAS生态系统，以特定知识域和对象为例，探讨神经工具融入知识库生命周期，勾勒专业智能体生态系统。


<details>
  <summary>Details</summary>
Motivation: 在社科人文学科尝试非人类中心分析框架背景下，对符号AI与神经AI混合进行研究。

Method: 以区域研究知识库设计与使用为应用领域，结合LaCAS生态系统和Okapi软件环境，以‘世界语言’知识域和‘克丘亚语’知识对象为例进行说明，结合模型驱动和数据驱动方法。

Result: 展示了LaCAS生态系统有约160,000个文献资源和十个知识宏观领域，探讨了神经工具融入知识库生命周期的多种应用。

Conclusion: 可构建能在尊重符号约束下激活数据库的专业智能体生态系统。

Abstract: In a context where the social sciences and humanities are experimenting with
non-anthropocentric analytical frames, this article proposes a semiotic
(structural) reading of the hybridization between symbolic AI and neural (or
sub-symbolic) AI based on a field of application: the design and use of a
knowledge base for area studies. We describe the LaCAS ecosystem -- Open
Archives in Linguistic and Cultural Studies (thesaurus; RDF/OWL ontology; LOD
services; harvesting; expertise; publication), deployed at Inalco (National
Institute for Oriental Languages and Civilizations) in Paris with the Okapi
(Open Knowledge and Annotation Interface) software environment from Ina
(National Audiovisual Institute), which now has around 160,000 documentary
resources and ten knowledge macro-domains grouping together several thousand
knowledge objects. We illustrate this approach using the knowledge domain
''Languages of the world'' (~540 languages) and the knowledge object ''Quechua
(language)''. On this basis, we discuss the controlled integration of neural
tools, more specifically generative tools, into the life cycle of a knowledge
base: assistance with data localization/qualification, index extraction and
aggregation, property suggestion and testing, dynamic file generation, and
engineering of contextualized prompts (generic, contextual, explanatory,
adjustment, procedural) aligned with a domain ontology. We outline an ecosystem
of specialized agents capable of animating the database while respecting its
symbolic constraints, by articulating model-driven and data-driven methods.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [605] [FBMS: An R Package for Flexible Bayesian Model Selection and Model Averaging](https://arxiv.org/abs/2509.00753)
*Florian Frommlet,Jon Lachmann,Geir Storvik,Aliaksandr Hubin*

Main category: stat.ME

TL;DR: FBMS R包通过多种蒙特卡罗模型探索方法，在复杂回归场景中实现贝叶斯模型选择和平均，介绍核心算法并展示其在高斯回归及复杂场景建模的应用。


<details>
  <summary>Details</summary>
Motivation: 在复杂回归设置中实现贝叶斯模型选择和平均，处理多模态后验景观及非线性特征。

Method: 实现高效的Mode Jumping Markov Chain Monte Carlo (MJMCMC)算法，提供引入非线性特征生成的GMJMCMC算法，维护和更新转换特征群体、计算后验概率等。

Result: 展示了FBMS在高斯回归的推理和预测建模中的有效使用。

Conclusion: 该方法可扩展到更复杂的建模场景，包括其他响应分布和混合效应模型。

Abstract: The FBMS R package facilitates Bayesian model selection and model averaging
in complex regression settings by employing a variety of Monte Carlo model
exploration methods. At its core, the package implements an efficient Mode
Jumping Markov Chain Monte Carlo (MJMCMC) algorithm, designed to improve mixing
in multi-modal posterior landscapes within Bayesian generalized linear models.
In addition, it provides a genetically modified MJMCMC (GMJMCMC) algorithm that
introduces nonlinear feature generation, thereby enabling the estimation of
Bayesian generalized nonlinear models (BGNLMs). Within this framework, the
algorithm maintains and updates populations of transformed features, computes
their posterior probabilities, and evaluates the posteriors of models
constructed from them. We demonstrate the effective use of FBMS for both
inferential and predictive modeling in Gaussian regression, focusing on
different instances of the BGNLM class of models. Furthermore, through a broad
set of applications, we illustrate how the methodology can be extended to
increasingly complex modeling scenarios, extending to other response
distributions and mixed effect models.

</details>


### [606] [Sampling as Bandits: Evaluation-Efficient Design for Black-Box Densities](https://arxiv.org/abs/2509.01437)
*Takuo Matsubara,Andrew Duncan,Simon Cotter,Konstantinos Zygalakis*

Main category: stat.ME

TL;DR: 介绍了适用于目标密度评估成本高的场景的新重要性采样方法BIS，展示其有效性和优势。


<details>
  <summary>Details</summary>
Motivation: 解决目标密度评估成本高的问题，设计新的重要性采样方法。

Method: 结合空间填充设计和多臂老虎机的顺序策略直接设计样本，利用高斯过程代理指导样本选择。

Result: 建立了收敛的理论保证，在多种采样任务中表现有效。

Conclusion: BIS能用更少的目标评估实现准确近似，在多种分布和实际应用中优于其他方法。

Abstract: We introduce bandit importance sampling (BIS), a new class of importance
sampling methods designed for settings where the target density is expensive to
evaluate. In contrast to adaptive importance sampling, which optimises a
proposal distribution, BIS directly designs the samples through a sequential
strategy that combines space-filling designs with multi-armed bandits. Our
method leverages Gaussian process surrogates to guide sample selection,
enabling efficient exploration of the parameter space with minimal target
evaluations. We establish theoretical guarantees on convergence and demonstrate
the effectiveness of the method across a broad range of sampling tasks. BIS
delivers accurate approximations with fewer target evaluations, outperforming
competing approaches across multimodal, heavy-tailed distributions, and
real-world applications to Bayesian inference of computationally expensive
models.

</details>


### [607] [hdMTD: An R Package for High-Dimensional Mixture Transition Distribution Models](https://arxiv.org/abs/2509.01808)
*Maiara Gripp,Giulio Iacobelli,Guilherme Ost,Daniel Y. Takahashi*

Main category: stat.ME

TL;DR: 文章介绍了R包hdMTD，可用于估计高维马尔可夫模型参数，通过模拟数据和实际应用展示其能力。


<details>
  <summary>Details</summary>
Motivation: 传统MTD模型推理受高维联合分布估计限制，为克服此局限开发hdMTD包。

Method: 使用Ost和Takahashi (2023)的方法，通过BIC算法或前向逐步和切割算法检索相关过去集，用期望最大化算法估计参数，用完美抽样算法模拟MTD链。

Result: 开发出hdMTD包，可进行参数估计、模拟MTD链等操作。

Conclusion: 通过模拟数据和巴西温度记录的实际应用展示了hdMTD包的能力。

Abstract: Several natural phenomena exhibit long-range conditional dependencies.
High-order mixture transition distribution (MTD) are parsimonious
non-parametric models to study these phenomena. An MTD is a Markov chain in
which the transition probabilities are expressed as a convex combination of
lower-order conditional distributions. Despite their generality, inference for
MTD models has traditionally been limited by the need to estimate
high-dimensional joint distributions. In particular, for a sample of size n,
the feasible order d of the MTD is typically restricted to d approximately
O(log n). To overcome this limitation, Ost and Takahashi (2023) recently
introduced a computationally efficient non-parametric inference method that
identifies the relevant lags in high-order MTD models, even when d is
approximately O(n), provided that the set of relevant lags is sparse. In this
article, we introduce hdMTD, an R package allowing us to estimate parameters of
such high-dimensional Markovian models. Given a sample from an MTD chain, hdMTD
can retrieve the relevant past set using the BIC algorithm or the forward
stepwise and cut algorithm described in Ost and Takahashi (2023). The package
also computes the maximum likelihood estimate for transition probabilities and
estimates high-order MTD parameters through the expectation-maximization
algorithm. Additionally, hdMTD also allows for simulating an MTD chain from its
stationary invariant distribution using the perfect (exact) sampling algorithm,
enabling Monte Carlo simulation of the model. We illustrate the package's
capabilities through simulated data and a real-world application involving
temperature records from Brazil.

</details>


### [608] [generalRSS: Sampling and Inference for Balanced and Unbalanced Ranked Set Sampling in R](https://arxiv.org/abs/2509.02039)
*Chul Moon,Soohyun Ahn*

Main category: stat.ME

TL;DR: 介绍Ranked set sampling (RSS)方法，指出unbalanced RSS (URSS)对偏态分布有效，介绍generalRSS包功能并通过医学数据应用展示其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有RSS软件主要关注平衡设计，存在局限性，需一个能支持BRSS和URSS的工具。

Method: 介绍RSS方法，使用generalRSS包进行RSS数据生成、样本分配和统计推断，通过两个医学数据应用展示。

Result: 通过医学数据应用，展示了URSS的实际实施，generalRSS便于在实际数据分析中进行排序集抽样和推断。

Conclusion: generalRSS包对实际数据分析中进行排序集抽样和推断有帮助。

Abstract: Ranked set sampling (RSS) is a stratified sampling method that improves
efficiency over simple random sampling (SRS) by utilizing auxiliary information
for ranking and stratification. While balanced RSS (BRSS) assumes equal
allocation across strata, unbalanced RSS (URSS) allows unequal allocation,
making it particularly effective for skewed distributions. The generalRSS
package provides extensive tools for both BRSS and URSS, addressing limitations
in existing RSS software that primarily focus on balanced designs. It supports
RSS data generation, efficient sample allocation strategies for URSS, and
statistical inference for both balanced and unbalanced designs. This paper
presents the RSS methodology and demonstrates the utility of generalRSS through
two medical data applications: a one-sample mean inference and a two-sample
area under the curve (AUC) comparison using NHANES datasets. These applications
illustrate the practical implementation of URSS and show how generalRSS
facilitates ranked set sampling and inference in real-world data analysis.

</details>


### [609] [LHS in LHS: A new expansion strategy for Latin hypercube sampling in simulation design](https://arxiv.org/abs/2509.00159)
*Matteo Boschini,Davide Gerosa,Alessandro Crespi,Matteo Falcone*

Main category: stat.ME

TL;DR: 提出LHS in LHS新扩展算法，可向现有LHS分布集添加新样本并保留其属性，还引入LHS度指标，代码通过Python包expandLHS发布。


<details>
  <summary>Details</summary>
Motivation: 现有LHS是单阶段算法，需先验确定样本量，缺乏扩展能力，需新方法。

Method: 识别参数空间中远离初始集的区域，在这些区域生成新LHS并与原样本合并。

Result: 提出LHS in LHS算法和LHS度指标，有公开Python实现。

Conclusion: LHS in LHS算法能有效扩展LHS分布集并保留属性。

Abstract: Latin Hypercube Sampling (LHS) is a prominent tool in simulation design, with
a variety of applications in high-dimensional and computationally expensive
problems. LHS allows for various optimization strategies, most notably to
ensure space-filling properties. However, LHS is a single-stage algorithm that
requires a priori knowledge of the targeted sample size. In this work, we
present LHS in LHS, a new expansion algorithm for LHS that enables the addition
of new samples to an existing LHS-distributed set while (approximately)
preserving its properties. In summary, the algorithm identifies regions of the
parameter space that are far from the initial set, draws a new LHS within those
regions, and then merges it with the original samples. As a by-product, we
introduce a new metric, the LHS degree, which quantifies the deviation of a
given design from an LHS distribution. Our public implementation is distributed
via the Python package expandLHS.

</details>


### [610] [Generalized promotion time cure model: A new modeling framework to identify cell-type-specific genes and improve survival prognosis](https://arxiv.org/abs/2509.01001)
*Zhi Zhao,Fatih Kizilaslan,Shixiong Wang,Manuela Zucknick*

Main category: stat.ME

TL;DR: 提出贝叶斯广义促进时间治愈模型（GPTCMs）用于多尺度数据整合以识别细胞类型特异性基因并改善癌症预后，模拟显示该模型有效。


<details>
  <summary>Details</summary>
Motivation: 单细胞技术产生的高维组学数据可增强癌症生存建模，但缺乏整合多尺度数据的统计模型。

Method: 提出一类贝叶斯广义促进时间治愈模型（GPTCMs）进行多尺度数据整合。

Result: 在低维和高维设置的模拟中，该模型能够识别细胞类型相关协变量并改善生存预测。

Conclusion: 贝叶斯GPTCMs模型可用于识别细胞类型特异性基因和改善癌症预后。

Abstract: Single-cell technologies provide an unprecedented opportunity for dissecting
the interplay between the cancer cells and the associated tumor
microenvironment, and the produced high-dimensional omics data should also
augment existing survival modeling approaches for identifying tumor cell
type-specific genes predictive of cancer patient survival. However, there is no
statistical model to integrate multiscale data including individual-level
survival data, multicellular-level cell composition data and cellular-level
single-cell omics covariates. We propose a class of Bayesian generalized
promotion time cure models (GPTCMs) for the multiscale data integration to
identify cell-type-specific genes and improve cancer prognosis. We demonstrate
with simulations in both low- and high-dimensional settings that the proposed
Bayesian GPTCMs are able to identify cell-type-associated covariates and
improve survival prediction.

</details>


### [611] [Wrong Model, Right Uncertainty: Spatial Associations for Discrete Data with Misspecification](https://arxiv.org/abs/2509.01776)
*David R. Burt,Renato Berlinghieri,Tamara Broderick*

Main category: stat.ME

TL;DR: 本文针对离散响应的空间关联，在模型误设和非随机采样情况下，给出有渐近名义覆盖率保证的置信区间，实证表明方法可靠。


<details>
  <summary>Details</summary>
Motivation: 现有估计协变量与二元或计数响应关联的方法在空间域有不现实假设，近期工作仅适用于连续响应。

Method: 展示处理空间变化噪声的方法，对提出的估计量给出一致性证明，结合delta方法和Lyapunov中心极限定理。

Result: 实证显示标准方法产生的置信区间不可靠，甚至关联符号错误，而本文方法可靠提供正确覆盖率。

Conclusion: 本文方法能在离散响应、模型误设和非随机采样情况下，为空间关联提供有保证的置信区间。

Abstract: Scientists are often interested in estimating an association between a
covariate and a binary- or count-valued response. For instance, public health
officials are interested in how much disease presence (a binary response per
individual) varies as temperature or pollution (covariates) increases. Many
existing methods can be used to estimate associations, and corresponding
uncertainty intervals, but make unrealistic assumptions in the spatial domain.
For instance, they incorrectly assume models are well-specified. Or they assume
the training and target locations are i.i.d. -- whereas in practice, these
locations are often not even randomly sampled. Some recent work avoids these
assumptions but works only for continuous responses with spatially constant
noise. In the present work, we provide the first confidence intervals with
guaranteed asymptotic nominal coverage for spatial associations given discrete
responses, even under simultaneous model misspecification and nonrandom
sampling of spatial locations. To do so, we demonstrate how to handle spatially
varying noise, provide a novel proof of consistency for our proposed estimator,
and use a delta method argument with a Lyapunov central limit theorem. We show
empirically that standard approaches can produce unreliable confidence
intervals and can even get the sign of an association wrong, while our method
reliably provides correct coverage.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [612] [Stabilization techniques for immersogeometric analysis of plate and shell problems in explicit dynamics](https://arxiv.org/abs/2509.00522)
*Giuliano Guarino,Yannis Voet,Pablo Antolin,Annalisa Buffa*

Main category: math.NA

TL;DR: 本文扩展先前工作，实现用集中质量矩阵对板壳问题进行稳定的沉浸几何分析，技术基于多项式扩展，恢复了与边界拟合离散化相当的精度。


<details>
  <summary>Details</summary>
Motivation: 有限元板壳公式在结构分析中广泛使用，但显式动力学中临界时间步受严格约束，浸入有限元离散化中切割不良的单元会加剧问题，集中质量矩阵虽能增加临界时间步但可能引发虚假振荡。

Method: 基于多项式扩展，扩展先前工作以进行稳定的沉浸几何分析。

Result: 能够对板壳问题用集中质量矩阵进行稳定的沉浸几何分析，恢复了与边界拟合离散化相当的精度。

Conclusion: 所提技术可解决集中质量矩阵在板壳问题分析中带来的问题，实现稳定且精度较高的分析。

Abstract: Finite element plate and shell formulations are ubiquitous in structural
analysis for modeling all kinds of slender structures, both for static and
dynamic analyses. The latter are particularly challenging as the high order
nature of the underlying partial differential equations and the slenderness of
the structures all impose a stringent constraint on the critical time step in
explicit dynamics. Unfortunately, badly cut elements in immersed finite element
discretizations further aggravate the issue. While lumping the mass matrix
often increases the critical time step, it might also trigger spurious
oscillations in the approximate solution thereby compromising the numerical
solution. In this article, we extend our previous work in
\cite{voet2025stabilization} to allow stable immersogeometric analysis of plate
and shell problems with lumped mass matrices. This technique is based on
polynomial extensions and restores a level of accuracy comparable to
boundary-fitted discretizations.

</details>


### [613] [WoSNN: Stochastic Solver for PDEs with Machine Learning](https://arxiv.org/abs/2509.00204)
*Silei Song,Arash Fahim,Michael Mascagni*

Main category: math.NA

TL;DR: 本文结合机器学习与WoS及空间离散化方法，提出新随机求解器WoS - NN，能精确快速求解椭圆问题，实验显示其误差降低约75%，样本仅用8%，节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 解决椭圆偏微分方程是科研和工程研究的基础步骤，传统WoS方法虽有效但有局限，需开发新方法实现精确快速全局求解和梯度近似。

Method: 将机器学习技术与WoS和空间离散化方法结合，开发WoS - NN求解器。

Result: WoS - NN方法能提供准确的场估计，与传统WoS方法相比，误差降低约75%，仅使用8%的路径样本，节省大量计算时间和资源消耗。

Conclusion: WoS - NN方法继承了原WoS方法的优点，结合神经网络后适合静态区域的密集请求，是一种高效的椭圆问题求解方法。

Abstract: Solving elliptic partial differential equations (PDEs) is a fundamental step
in various scientific and engineering studies. As a classic stochastic solver,
the Walk-on-Spheres (WoS) method is a well-established and efficient algorithm
that provides accurate local estimates for PDEs. In this paper, by integrating
machine learning techniques with WoS and space discretization approaches, we
develop a novel stochastic solver, WoS-NN. This new method solves elliptic
problems with Dirichlet boundary conditions, facilitating precise and rapid
global solutions and gradient approximations. The method inherits excellent
characteristics from the original WoS method, such as being meshless and robust
to irregular regions. By integrating neural networks, WoS-NN also gives instant
local predictions after training without re-sampling, which is especially
suitable for intense requests on a static region. A typical experimental result
demonstrates that the proposed WoS-NN method provides accurate field
estimations, reducing errors by around $75\%$ while using only $8\%$ of path
samples compared to the conventional WoS method, which saves abundant
computational time and resource consumption.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [614] [afspm: A Framework for Manufacturer-Agnostic Automation in Scanning Probe Microscopy](https://arxiv.org/abs/2509.00113)
*Nicholas J. Sullivan,Julio J. Valdés,Kirk H. Bevan,Peter Grutter*

Main category: physics.ins-det

TL;DR: 本文提出针对扫描探针显微镜（SPM）的自动化框架，定义通用控制和数据结构模式，实现多语言支持与组件解耦，经两台不同厂商SPM验证。


<details>
  <summary>Details</summary>
Motivation: SPM实验耗时且需大量领域知识，现有自动化方法难以集成到非开发对象的SPM中，需开发便于代码共享和组件复用的自动化框架。

Method: 提出自动化框架，定义通用控制和数据结构模式，组件间传递数据，通过SPM特定翻译器发送命令，采用调解逻辑限制SPM访问。

Result: 将框架与两台不同厂商的SPM集成并测试，运行涉及热漂移校正组件的实验。

Conclusion: 所提出的自动化框架可促进代码共享和组件复用，适用于不同厂商的SPM。

Abstract: Scanning probe microscopy (SPM) is a valuable technique by which one can
investigate the physical characteristics of the surfaces of materials. However,
its widespread use is hampered by the time-consuming nature of running an
experiment and the significant domain knowledge required. Recent studies have
shown the value of multiple forms of automation in improving this, but their
use is limited due to the difficulty of integrating them with SPMs other than
the one it was developed for. With this in mind, we propose an automation
framework for SPMs aimed toward facilitating code sharing and reusability of
developed components. Our framework defines generic control and data structure
schemas which are passed among independent software processes (components),
with the final SPM commands sent after passing through an SPM-specific
translator. This approach permits multi-language support and allows for
experimental components to be decoupled among multiple computers. Our mediation
logic limits access to the SPM to a single component at a time, with a simple
override mechanism in order to correct detected experiment problems. To
validate our proposal, we integrated and tested it with two SPMs from separate
manufacturers, and ran an experiment involving a thermal drift correction
component.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [615] [A Fluid Antenna Enabled Physical Layer Key Generation for Next-G Wireless Networks](https://arxiv.org/abs/2509.00018)
*Jiacheng Guo,Ning Gao,Yiping Zuo,Hao Xu,Shi Jin,Kai Kit Wong*

Main category: eess.SP

TL;DR: 本文提出流体天线（FA）的物理层密钥生成（PLKG）系统，推导KGR表达式，用PSO和AO算法优化，仿真表明系统优于基准方案。


<details>
  <summary>Details</summary>
Motivation: 在恶劣传播环境中，传统PLKG的密钥生成率（KGR）显著下降，需要新方案解决该问题。

Method: 推导FA阵列KGR的闭式表达式，用粒子群优化（PSO）算法联合优化预编码矩阵和天线位置，开发结合投影梯度下降（PGD）和PSO的交替优化（AO）算法。

Result: 通过利用额外的空间自由度，FA使能的PLKG系统优于传统固定位置天线（FPA）阵列和可重构智能表面（RIS），相比传统均匀平面天线（UPA），PSO算法下KGR性能提高35.42%，AO算法下提高67.73%。

Conclusion: FA使能的PLKG系统能有效提高恶劣环境下的密钥生成率，具有更好的性能。

Abstract: As a promising physical layer security technique, physical layer key
generation (PLKG) enables legitimate users to obtain secret keys from wireless
channel without security infrastructures. However, in harsh propagation
environments, the channel characteristic becomes unsatisfactory, the key
generation rate (KGR) is significantly deteriorated. In this paper, we propose
a novel fluid antenna (FA) enabled PLKG system to address this challenge.
Specifically, we first derive the closed-form expression of the KGR for FA
array, and then jointly optimize the precoding matrix and the antenna positions
via a particle swarm optimization (PSO) algorithm. Next, to further reduce the
computational complexity of the optimization procedure, we develop an
alternating optimization (AO) algorithm, which combines the projected gradient
descent (PGD) and the PSO. Simulation results demonstrate that by exploiting
the additional spatial degree of freedom (DoF), our FA enabled PLKG system is
superior to the benchmarks, such as the conventional fixed-position antenna
(FPA) array and the reconfigurable intelligent surface (RIS). It is worth
highlighting that compared to the conventional uniform planar antenna (UPA),
the FA enabled PLKG achieves a 35.42\% KGR performance improvement under PSO
algorithm and a 67.73\% KGR performance improvement under AO algorithm,
respectively.

</details>


### [616] [Exploring the Efficacy of Convolutional Neural Networks in Sleep Apnea Detection from Single Channel EEG](https://arxiv.org/abs/2509.00012)
*Chun Hin Siu,Hossein Miri*

Main category: eess.SP

TL;DR: 本文提出用单通道脑电图数据训练的卷积神经网络检测睡眠呼吸暂停，解决了传统方法的局限，显示家庭应用潜力。


<details>
  <summary>Details</summary>
Motivation: 睡眠呼吸暂停诊断和治疗的主要挑战是识别高危人群，当前金标准多导睡眠图（PSG）存在成本高、耗人力和不便等问题。

Method: 使用卷积神经网络（CNN），结合无限脉冲响应（IIR）巴特沃斯滤波器的预处理管道、提供更广泛时间上下文的数据集构建方法和SMOTETomek处理类别不平衡。

Result: 所提出的CNN准确率达85.1%，马修斯相关系数（MCC）为0.22。

Conclusion: 从传统实验室诊断过渡到更易获取的自动化家庭解决方案是可行的，能改善患者预后并提高睡眠障碍诊断的可及性。

Abstract: Sleep apnea, a prevalent sleep disorder, involves repeated episodes of
breathing interruptions during sleep, leading to various health complications,
including cognitive impairments, high blood pressure, heart disease, stroke,
and even death. One of the main challenges in diagnosing and treating sleep
apnea is identifying individuals at risk. The current gold standard for
diagnosis, Polysomnography (PSG), is costly, labor intensive, and inconvenient,
often resulting in poor quality sleep data. This paper presents a novel
approach to the detection of sleep apnea using a Convolutional Neural Network
(CNN) trained on single channel EEG data. The proposed CNN achieved an accuracy
of 85.1% and a Matthews Correlation Coefficient (MCC) of 0.22, demonstrating a
significant potential for home based applications by addressing the limitations
of PSG in automated sleep apnea detection. Key contributions of this work also
include the development of a comprehensive preprocessing pipeline with an
Infinite Impulse Response (IIR) Butterworth filter, a dataset construction
method providing broader temporal context, and the application of SMOTETomek to
address class imbalance. This research underscores the feasibility of
transitioning from traditional laboratory based diagnostics to more accessible,
automated home based solutions, improving patient outcomes and broadening the
accessibility of sleep disorder diagnostics.

</details>


### [617] [Conditional Generative Adversarial Networks Based Inertial Signal Translation](https://arxiv.org/abs/2509.00016)
*Marcin Kolakowski*

Main category: eess.SP

TL;DR: 提出将腕戴传感器惯性信号转换为鞋载传感器信号的方法用于步态分析，用条件生成对抗网络实验验证，结果表明该方法可行。


<details>
  <summary>Details</summary>
Motivation: 使腕戴传感器惯性信号能使用先进的步态分析方法，实现日常高效步态分析。

Method: 使用条件生成对抗网络（GANs）进行信号转换，用传统GAN和Wasserstein GANs（WGANs）验证，测试卷积自编码器和卷积U - Net两种生成器架构。

Result: 提出的方法能实现准确的信号转换。

Conclusion: 该方法可利用腕部传感器惯性信号进行日常高效的步态分析。

Abstract: The paper presents an approach in which inertial signals measured with a
wrist-worn sensor (e.g., a smartwatch) are translated into those that would be
recorded using a shoe-mounted sensor, enabling the use of state-of-the-art gait
analysis methods. In the study, the signals are translated using Conditional
Generative Adversarial Networks (GANs). Two different GAN versions are used for
experimental verification: traditional ones trained using binary cross-entropy
loss and Wasserstein GANs (WGANs). For the generator, two architectures, a
convolutional autoencoder, and a convolutional U-Net, are tested. The
experiment results have shown that the proposed approach allows for an accurate
translation, enabling the use of wrist sensor inertial signals for efficient,
every-day gait analysis.

</details>


### [618] [Non-Identical Diffusion Models in MIMO-OFDM Channel Generation](https://arxiv.org/abs/2509.01641)
*Yuzhi Yang,Omar Alhussein,Mérouane Debbah*

Main category: eess.SP

TL;DR: 提出非相同扩散模型用于无线OFDM信道生成，引入元素级时间指标和匹配输入大小矩阵控制噪声进展，通过理论和数值实验验证方案有效性，并提出维度时间嵌入策略，开发评估多种训练和生成方法。


<details>
  <summary>Details</summary>
Motivation: 标准扩散模型用标量时间指标难以准确捕捉局部误差变化，传统时间嵌入无法适应导频方案和噪声水平的变异性，为解决这些问题进行研究。

Method: 提出非相同扩散模型，用元素级时间指标代替标量时间指标；引入匹配输入大小的矩阵控制元素级噪声进展；采用类似现有方法的扩散过程；提出维度时间嵌入策略；开发并评估多种训练和生成方法。

Result: 理论和数值上证明了非相同扩散方案的正确性和有效性，通过数值实验比较了多种训练和生成方法。

Conclusion: 所提出的非相同扩散模型及相关策略和方法在无线MIMO - OFDM信道生成中有效，能处理初始估计可靠性不均的问题。

Abstract: We propose a novel diffusion model, termed the non-identical diffusion model,
and investigate its application to wireless orthogonal frequency division
multiplexing (OFDM) channel generation. Unlike the standard diffusion model
that uses a scalar-valued time index to represent the global noise level, we
extend this notion to an element-wise time indicator to capture local error
variations more accurately. Non-identical diffusion enables us to characterize
the reliability of each element (e.g., subcarriers in OFDM) within the noisy
input, leading to improved generation results when the initialization is
biased. Specifically, we focus on the recovery of wireless multi-input
multi-output (MIMO) OFDM channel matrices, where the initial channel estimates
exhibit highly uneven reliability across elements due to the pilot scheme.
Conventional time embeddings, which assume uniform noise progression, fail to
capture such variability across pilot schemes and noise levels. We introduce a
matrix that matches the input size to control element-wise noise progression.
Following a similar diffusion procedure to existing methods, we show the
correctness and effectiveness of the proposed non-identical diffusion scheme
both theoretically and numerically. For MIMO-OFDM channel generation, we
propose a dimension-wise time embedding strategy. We also develop and evaluate
multiple training and generation methods and compare them through numerical
experiments.

</details>


### [619] [Synesthesia of Machines (SoM)-Based Task-Driven MIMO System for Image Transmission](https://arxiv.org/abs/2509.02031)
*Sijiang Li,Rongqing Zhang,Xiang Cheng,Jian Tang*

Main category: eess.SP

TL;DR: 本文提出基于Synesthesia of Machines (SoM)的任务驱动MIMO系统SoM - MIMO用于图像传输，实验表明其优于基线方案。


<details>
  <summary>Details</summary>
Motivation: 现有MIMO JSCC方案在支持网络移动代理复杂协作感知任务方面性能有限，需高效鲁棒的感官数据传输方法。

Method: 利用感知任务特征金字塔结构特性和闭环MIMO通信系统信道特性，提出SoM - MIMO系统。

Result: 与两个JSCC基线方案相比，在所有信噪比水平上平均mAP分别提高6.30和10.48，通信开销相同。

Conclusion: SoM - MIMO能实现高效且鲁棒的数字MIMO图像传输。

Abstract: To support cooperative perception (CP) of networked mobile agents in dynamic
scenarios, the efficient and robust transmission of sensory data is a critical
challenge. Deep learning-based joint source-channel coding (JSCC) has
demonstrated promising results for image transmission under adverse channel
conditions, outperforming traditional rule-based codecs. While recent works
have explored to combine JSCC with the widely adopted multiple-input
multiple-output (MIMO) technology, these approaches are still limited to the
discrete-time analog transmission (DTAT) model and simple tasks. Given the
limited performance of existing MIMO JSCC schemes in supporting complex CP
tasks for networked mobile agents with digital MIMO communication systems, this
paper presents a Synesthesia of Machines (SoM)-based task-driven MIMO system
for image transmission, referred to as SoM-MIMO. By leveraging the structural
properties of the feature pyramid for perceptual tasks and the channel
properties of the closed-loop MIMO communication system, SoM-MIMO enables
efficient and robust digital MIMO transmission of images. Experimental results
have shown that compared with two JSCC baseline schemes, our approach achieves
average mAP improvements of 6.30 and 10.48 across all SNR levels, while
maintaining identical communication overhead.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [620] [Is Noisy Data a Blessing in Disguise? A Distributionally Robust Optimization Perspective](https://arxiv.org/abs/2509.01076)
*Chung-Han Hsieh,Rong Gan*

Main category: math.OC

TL;DR: 本文研究含噪声数据的分布鲁棒优化，提出逆像构造方法，证明其有效性，表明噪声数据可带来优势，能助力管理者做决策。


<details>
  <summary>Details</summary>
Motivation: 将噪声数据系统地纳入决策过程，解决噪声数据给决策带来的挑战。

Method: 通过在观测空间中以含噪声的经验分布为中心构建Wasserstein球，利用已知噪声核求其逆像来构造潜在分布上的模糊集。

Result: 得到可处理的凸重写形式，建立了有限样本性能和渐近一致性等统计保证；该模型比直接模型保守性低，有更高最优值和更低模糊代价；在公平资源分配问题中能产生更公平的解。

Conclusion: 管理者可将噪声作为鲁棒性来源，而非障碍，以做出更鲁棒和战略平衡的决策。

Abstract: Noisy data are often viewed as a challenge for decision-making. This paper
studies a distributionally robust optimization (DRO) that shows how such noise
can be systematically incorporated. Rather than applying DRO to the noisy
empirical distribution, we construct ambiguity sets over the \emph{latent}
distribution by centering a Wasserstein ball at the noisy empirical
distribution in the observation space and taking its inverse image through a
known noise kernel. We validate this inverse-image construction by deriving a
tractable convex reformulation and establishing rigorous statistical
guarantees, including finite-sample performance and asymptotic consistency.
Crucially, we demonstrate that, under mild conditions, noisy data may be a
``blessing in disguise." Our noisy-data DRO model is less conservative than its
direct counterpart, leading to provably higher optimal values and a lower price
of ambiguity. In the context of fair resource allocation problems, we
demonstrate that this robust approach can induce solutions that are
structurally more equitable. Our findings suggest that managers can leverage
uncertainty by harnessing noise as a source of robustness rather than treating
it as an obstacle, producing more robust and strategically balanced decisions.

</details>


### [621] [Asynchronous and Stochastic Distributed Resource Allocation](https://arxiv.org/abs/2509.01172)
*Qiang Li,Michal Yemini,Hoi-To Wai*

Main category: math.OC

TL;DR: 提出并研究异步随机环境下的分布式资源分配问题，提出Asyn - PD算法，证明收敛性并数值验证。


<details>
  <summary>Details</summary>
Motivation: 解决异步随机环境下分布式系统的资源分配问题，考虑系统中计算和通信时间的异构性。

Method: 采用近似随机原始 - 对偶方法，提出Asynchronous stochastic Primal - Dual (Asyn - PD)算法。

Result: 证明Asyn - PD算法二阶矩以$O(1/t)$的速率收敛到近似问题的鞍点解，数值验证了收敛结果。

Conclusion: 所提出的异步算法相比同步算法有优势，服务器无需等待所有工作节点更新。

Abstract: This work proposes and studies the distributed resource allocation problem in
asynchronous and stochastic settings. We consider a distributed system with
multiple workers and a coordinating server with heterogeneous computation and
communication times. We explore an approximate stochastic primal-dual approach
with the aim of 1) adhering to the resource budget constraints, 2) allowing for
the asynchronicity between the workers and the server, and 3) relying on the
locally available stochastic gradients. We analyze our Asynchronous stochastic
Primal-Dual (Asyn-PD) algorithm and prove its convergence in the second moment
to the saddle point solution of the approximate problem at the rate of
$O(1/t)$, where $t$ is the iteration number. Furthermore, we verify our
algorithm numerically to validate the analytically derived convergence results,
and demonstrate the advantages of utilizing our asynchronous algorithm rather
than deploying a synchronous algorithm where the server must wait until it gets
update from all workers.

</details>


### [622] [Convergence Analysis of the PAGE Stochastic Algorithm for Convex Finite-Sum Optimization](https://arxiv.org/abs/2509.00737)
*Laurent Condat,Peter Richtárik*

Main category: math.OC

TL;DR: 分析PAGE算法在凸环境下的情况，得到新收敛率和更好复杂度。


<details>
  <summary>Details</summary>
Motivation: 在凸环境下对PAGE算法进行分析，以得到比一般非凸情况更好的结果。

Method: 对PAGE算法在凸环境下进行分析推导。

Result: 得到了新的收敛率，复杂度优于一般非凸情况。

Conclusion: PAGE算法在凸环境下有更好的表现，能获得更好复杂度。

Abstract: PAGE is a stochastic algorithm proposed by Li et al. [2021] to find a
stationary point of an average of smooth nonconvex functions. We analyze PAGE
in the convex setting and derive new convergence rates, leading to a better
complexity than in the general nonconvex regime.

</details>


### [623] [Online Complexity Estimation for Repetitive Scenario Design](https://arxiv.org/abs/2509.02103)
*Guillaume O. Berger,Raphaël M. Jungers*

Main category: math.OC

TL;DR: 提出一种动态学习最优样本量的方法解决重复场景设计问题，证明方法合理性与收敛性并展示其实践效率。


<details>
  <summary>Details</summary>
Motivation: 解决重复场景设计中需调整样本量以获得期望风险水平的问题。

Method: 基于先前场景解决方案及其风险水平的数据，学习风险概率密度函数与样本量的关系以获取最优样本量。

Result: 证明该方法对固定复杂度场景问题获取最优样本量的合理性与收敛性，且在一系列具有挑战性问题上展示了实践效率。

Conclusion: 所提方法可有效解决重复场景设计问题，包括非固定复杂度、非凸约束和时变分布等问题。

Abstract: We consider the problem of repetitive scenario design where one has to solve
repeatedly a scenario design problem and can adjust the sample size (number of
scenarios) to obtain a desired level of risk (constraint violation
probability). We propose an approach to learn on the fly the optimal sample
size based on observed data consisting in previous scenario solutions and their
risk level. Our approach consists in learning a function that represents the
pdf (probability density function) of the risk as a function of the sample
size. Once this function is known, retrieving the optimal sample size is
straightforward. We prove the soundness and convergence of our approach to
obtain the optimal sample size for the class of fixed-complexity scenario
problems, which generalizes fully-supported convex scenario programs that have
been studied extensively in the scenario optimization literature. We also
demonstrate the practical efficiency of our approach on a series of challenging
repetitive scenario design problems, including non-fixed-complexity problems,
nonconvex constraints and time-varying distributions.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [624] [Protocol for Clustering 4DSTEM Data for Phase Differentiation in Glasses](https://arxiv.org/abs/2509.00943)
*Mridul Kumar,Yevgeny Rakita*

Main category: cond-mat.mtrl-sci

TL;DR: 本文应用无监督机器学习分析Ge - Sb - Te相变材料的成分和结构异质性，识别出四个不同簇并揭示化学和结构变化。


<details>
  <summary>Details</summary>
Motivation: 传统技术难以解析相变材料（PCMs）成分和结构的纳米级变化，需要新方法。

Method: 对4D - STEM数据应用无监督机器学习，经PCA预处理和降维，用t - SNE和UMAP进行聚类验证，通过轮廓评分优化k - means聚类。

Result: 识别出四个不同簇，元素强度直方图显示不同簇的化学特征变化，平均衍射图案证实结构变化。

Conclusion: 聚类分析可为关联PCMs的局部化学和结构特征提供有力框架，深入了解其内在异质性。

Abstract: Phase-change materials (PCMs) such as Ge-Sb-Te alloys are widely used in
non-volatile memory applications due to their rapid and reversible switching
between amorphous and crystalline states. However, their functional properties
are strongly governed by nanoscale variations in composition and structure,
which are challenging to resolve using conventional techniques. Here, we apply
unsupervised machine learning to 4-dimensional scanning transmission electron
microscopy (4D-STEM) data to identify compositional and structural
heterogeneity in Ge-Sb-Te. After preprocessing and dimensionality reduction
with principal component analysis (PCA), cluster validation was performed with
t-SNE and UMAP, followed by k-means clustering optimized through silhouette
scoring. Four distinct clusters were identified which were mapped back to the
diffraction data. Elemental intensity histograms revealed chemical signatures
change across clusters, oxygen and germanium enrichment in Cluster 1, tellurium
in Cluster 2, antimony in Cluster 3, and germanium again in Cluster 4.
Furthermore, averaged diffraction patterns from these clusters confirmed
structural variations. Together, these findings demonstrate that clustering
analysis can provide a powerful framework for correlating local chemical and
structural features in PCMs, offering deeper insights into their intrinsic
heterogeneity.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [625] [Computational Modeling for Personalized Transcranial Electrical Stimulation: Theory, Tools, and Applications](https://arxiv.org/abs/2509.01192)
*Mo Wang,Kexin Zheng,Yiling Liu,Huichun Luo,Tifei Yuan,Hongkai Wen,Pengfei Wei,Quanying Liu*

Main category: q-bio.NC

TL;DR: 本文综述了支持个性化经颅电刺激（tES）的计算技术进展，包括正演和逆演建模，指出当前进展及面临的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 以往综述缺乏对实现个性化刺激优化的计算建模框架的最新综合，而个体大脑解剖和生理差异使个性化tES受关注，故进行此综述。

Method: 系统研究模拟个体电场的正演建模和优化刺激参数的逆演建模方法，批判性评估头部建模流程、优化算法和多模态脑数据整合的进展。

Result: 近期进展加速了特定主体头部导体模型的构建，扩展了优化方法，可实现动态和个性化刺激规划。

Conclusion: 通过整合个性化tES计算建模的最新进展，强调了在研究和临床环境中实现精准神经调节面临的挑战、机遇和未来方向。

Abstract: Objective. Personalized transcranial electrical stimulation (tES) has gained
growing attention due to the substantial inter-individual variability in brain
anatomy and physiology. While previous reviews have discussed the physiological
mechanisms and clinical applications of tES, there remains a critical gap in
up-to-date syntheses focused on the computational modeling frameworks that
enable individualized stimulation optimization. Approach. This review presents
a comprehensive overview of recent advances in computational techniques
supporting personalized tES. We systematically examine developments in forward
modeling for simulating individualized electric fields, as well as inverse
modeling approaches for optimizing stimulation parameters. We critically
evaluate progress in head modeling pipelines, optimization algorithms, and the
integration of multimodal brain data. Main results. Recent advances have
substantially accelerated the construction of subject-specific head conductor
models and expanded the landscape of optimization methods, including
multi-objective optimization and brain network-informed optimization. These
advances allow for dynamic and individualized stimulation planning, moving
beyond empirical trial-and-error approaches.Significance. By integrating the
latest developments in computational modeling for personalized tES, this review
highlights current challenges, emerging opportunities, and future directions
for achieving precision neuromodulation in both research and clinical contexts.

</details>


### [626] [Meta-learning ecological priors from large language models explains human learning and decision making](https://arxiv.org/abs/2509.00116)
*Akshay K. Jagadish,Mirko Thalmann,Julian Coda-Forno,Marcel Binz,Eric Schulz*

Main category: q-bio.NC

TL;DR: 提出生态理性分析框架，开发ERMI算法，该算法能捕捉人类行为，结果表明人类认知可能与日常问题生态结构自适应对齐。


<details>
  <summary>Details</summary>
Motivation: 探究学习和决策是否可解释为对现实世界任务统计结构的原则性适应。

Method: 引入生态理性分析框架，利用大语言模型生成认知任务，用元学习推导理性模型，开发ERMI算法。

Result: ERMI在15个实验中捕捉人类行为，在逐次试验预测中优于多个已有的认知模型。

Conclusion: 人类认知可能反映了与日常遇到问题的生态结构的自适应对齐。

Abstract: Human cognition is profoundly shaped by the environments in which it unfolds.
Yet, it remains an open question whether learning and decision making can be
explained as a principled adaptation to the statistical structure of real-world
tasks. We introduce ecologically rational analysis, a computational framework
that unifies the normative foundations of rational analysis with ecological
grounding. Leveraging large language models to generate ecologically valid
cognitive tasks at scale, and using meta-learning to derive rational models
optimized for these environments, we develop a new class of learning
algorithms: Ecologically Rational Meta-learned Inference (ERMI). ERMI
internalizes the statistical regularities of naturalistic problem spaces and
adapts flexibly to novel situations, without requiring hand-crafted heuristics
or explicit parameter updates. We show that ERMI captures human behavior across
15 experiments spanning function learning, category learning, and decision
making, outperforming several established cognitive models in trial-by-trial
prediction. Our results suggest that much of human cognition may reflect
adaptive alignment to the ecological structure of the problems we encounter in
everyday life.

</details>


### [627] [DCA: Graph-Guided Deep Embedding Clustering for Brain Atlases](https://arxiv.org/abs/2509.01426)
*Mo Wang,Kaining Peng,Jingsheng Tang,Hongkai Wen,Quanying Liu*

Main category: q-bio.NC

TL;DR: 提出Deep Cluster Atlas (DCA)框架生成个性化脑图谱，引入评估平台，性能优于现有图谱。


<details>
  <summary>Details</summary>
Motivation: 现有脑图谱多为预定义、组水平模板，灵活性和分辨率有限。

Method: 结合预训练自编码器和空间正则化深度聚类生成脑图谱，引入标准化评估平台。

Result: 跨多个数据集和尺度，DCA在功能同质性上提升98.8%，轮廓系数提升29%，下游任务表现出色。

Conclusion: DCA是一种有效生成个性化脑图谱的方法，性能优于现有技术。

Abstract: Brain atlases are essential for reducing the dimensionality of neuroimaging
data and enabling interpretable analysis. However, most existing atlases are
predefined, group-level templates with limited flexibility and resolution. We
present Deep Cluster Atlas (DCA), a graph-guided deep embedding clustering
framework for generating individualized, voxel-wise brain parcellations. DCA
combines a pretrained autoencoder with spatially regularized deep clustering to
produce functionally coherent and spatially contiguous regions. Our method
supports flexible control over resolution and anatomical scope, and generalizes
to arbitrary brain structures. We further introduce a standardized benchmarking
platform for atlas evaluation, using multiple large-scale fMRI datasets. Across
multiple datasets and scales, DCA outperforms state-of-the-art atlases,
improving functional homogeneity by 98.8\% and silhouette coefficient by 29\%,
and achieves superior performance in downstream tasks such as autism diagnosis
and cognitive decoding. Codes and models will be released soon.

</details>


### [628] [Automatic Screening of Parkinson's Disease from Visual Explorations](https://arxiv.org/abs/2509.01326)
*Maria F. Alcala-Durand,J. Camilo Puerta-Acevedo,Julián D. Arias-Londoño,Juan I. Godino-Llorente*

Main category: q-bio.NC

TL;DR: 本文研究基于注视的特征对帕金森病自动筛查的效用，结合多种特征，用机器学习分类器评估，集成模型表现佳，支持视觉探索用于帕金森病早期自动筛查。


<details>
  <summary>Details</summary>
Motivation: 探究基于注视的特征集在不同视觉探索任务中对帕金森病自动筛查的效用。

Method: 引入新方法，结合经典眼动特征和注视聚类特征，从六个探索测试中自动提取特征，用不同机器学习分类器评估，用专家混合集成整合测试和双眼输出。

Result: 集成模型优于单个分类器，在保留测试集上的受试者工作特征曲线下面积（AUC）达到0.95。

Conclusion: 视觉探索可作为帕金森病早期自动筛查的非侵入性工具。

Abstract: Eye movements can reveal early signs of neurodegeneration, including those
associated with Parkinson's Disease (PD). This work investigates the utility of
a set of gaze-based features for the automatic screening of PD from different
visual exploration tasks. For this purpose, a novel methodology is introduced,
combining classic fixation/saccade oculomotor features (e.g., saccade count,
fixation duration, scanned area) with features derived from gaze clusters
(i.e., regions with a considerable accumulation of fixations). These features
are automatically extracted from six exploration tests and evaluated using
different machine learning classifiers. A Mixture of Experts ensemble is used
to integrate outputs across tests and both eyes. Results show that ensemble
models outperform individual classifiers, achieving an Area Under the Receiving
Operating Characteristic Curve (AUC) of 0.95 on a held-out test set. The
findings support visual exploration as a non-invasive tool for early automatic
screening of PD.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [629] [MolErr2Fix:Benchmarking LLM Trustworthiness in Chemistry via Modular Error Detection, Localization, Explanation, and Revision](https://arxiv.org/abs/2509.00063)
*Yuyang Wu,Jinhui Ye,Shuhao Zhang,Lu Dai,Yonatan Bisk,Olexandr Isayev*

Main category: physics.chem-ph

TL;DR: 提出MolErr2Fix基准评估大语言模型化学推理能力，评估显示模型有性能差距，将公开数据促进研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在分子科学中存在化学描述不准确、难识别及解释错误的问题，需严格评估其化学推理能力。

Method: 设计MolErr2Fix基准，含1193个细粒度标注错误实例，让模型识别、定位、解释和修正分子描述错误。

Result: 评估当前先进大语言模型发现显著性能差距。

Conclusion: MolErr2Fix为评估大语言模型化学推理能力提供针对性基准，助力开发更可靠、有化学知识的语言模型。

Abstract: Large Language Models (LLMs) have shown growing potential in molecular
sciences, but they often produce chemically inaccurate descriptions and
struggle to recognize or justify potential errors. This raises important
concerns about their robustness and reliability in scientific applications. To
support more rigorous evaluation of LLMs in chemical reasoning, we present the
MolErr2Fix benchmark, designed to assess LLMs on error detection and correction
in molecular descriptions. Unlike existing benchmarks focused on
molecule-to-text generation or property prediction, MolErr2Fix emphasizes
fine-grained chemical understanding. It tasks LLMs with identifying,
localizing, explaining, and revising potential structural and semantic errors
in molecular descriptions. Specifically, MolErr2Fix consists of 1,193
fine-grained annotated error instances. Each instance contains quadruple
annotations, i.e,. (error type, span location, the explanation, and the
correction). These tasks are intended to reflect the types of reasoning and
verification required in real-world chemical communication. Evaluations of
current state-of-the-art LLMs reveal notable performance gaps, underscoring the
need for more robust chemical reasoning capabilities. MolErr2Fix provides a
focused benchmark for evaluating such capabilities and aims to support progress
toward more reliable and chemically informed language models. All annotations
and an accompanying evaluation API will be publicly released to facilitate
future research.

</details>


### [630] [NMR-Solver: Automated Structure Elucidation via Large-Scale Spectral Matching and Physics-Guided Fragment Optimization](https://arxiv.org/abs/2509.00640)
*Yongqi Jin,Jun-Jie Wang,Fanjie Xu,Xiaohong Ji,Zhifeng Gao,Linfeng Zhang,Guolin Ke,Rong Zhu,Weinan E*

Main category: physics.chem-ph

TL;DR: 提出NMR - Solver框架用于从¹H和¹³C NMR光谱自动确定小分子结构，经多场景评估展示其优势，建立分子科学反问题求解范式。


<details>
  <summary>Details</summary>
Motivation: NMR光谱解析未知分子结构劳动密集且依赖专业知识，现有方法在实际应用中表现不佳。

Method: 引入分子结构解析自动化框架，结合大规模光谱匹配与基于物理引导的片段优化，利用NMR中原子级结构 - 光谱关系。

Result: 在模拟基准、文献实验数据和实际实验中评估，展示了强泛化性、鲁棒性和实际应用价值。

Conclusion: NMR - Solver将计算NMR分析、深度学习和可解释化学推理统一，通过结合NMR物理原理实现可扩展、自动化和有化学意义的分子识别。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is one of the most powerful and
widely used tools for molecular structure elucidation in organic chemistry.
However, the interpretation of NMR spectra to determine unknown molecular
structures remains a labor-intensive and expertise-dependent process,
particularly for complex or novel compounds. Although recent methods have been
proposed for molecular structure elucidation, they often underperform in
real-world applications due to inherent algorithmic limitations and limited
high-quality data. Here, we present NMR-Solver, a practical and interpretable
framework for the automated determination of small organic molecule structures
from $^1$H and $^{13}$C NMR spectra. Our method introduces an automated
framework for molecular structure elucidation, integrating large-scale spectral
matching with physics-guided fragment-based optimization that exploits
atomic-level structure-spectrum relationships in NMR. We evaluate NMR-Solver on
simulated benchmarks, curated experimental data from the literature, and
real-world experiments, demonstrating its strong generalization, robustness,
and practical utility in challenging, real-life scenarios. NMR-Solver unifies
computational NMR analysis, deep learning, and interpretable chemical reasoning
into a coherent system. By incorporating the physical principles of NMR into
molecular optimization, it enables scalable, automated, and chemically
meaningful molecular identification, establishing a generalizable paradigm for
solving inverse problems in molecular science.

</details>


### [631] [Migration as a Probe: A Generalizable Benchmark Framework for Specialist vs. Generalist Machine-Learned Force Fields in Doped Materials](https://arxiv.org/abs/2509.00090)
*Yi Cao,Paulette Clancy*

Main category: physics.chem-ph

TL;DR: 本文提出基准框架对比从头构建和微调基础模型的机器学习力场，评估不同任务表现，为其开发提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 不清楚构建专业模型和调整通用基础模型哪个更好，以及数据效率、预测准确性和分布外失败风险的权衡。

Method: 使用MACE架构，以Cr插层Sb2Te3为测试案例，用迁移路径和NEB轨迹作为诊断探针，评估平衡、动力学和力学任务。

Result: 所有模型能捕捉平衡结构，但非平衡过程预测有差异，特定任务微调提高动力学准确性，但可能降低长程物理表征。训练范式产生不同潜在编码。

Conclusion: 为MLFF开发提供实用指南，强调基于迁移的探针是有效诊断方法，并指出不确定性感知主动学习策略的途径。

Abstract: Machine-learned force fields (MLFFs), particularly pre-trained foundation
models, promise to bring ab initio-level accuracy to the length and time scales
of molecular dynamics. Yet this shift raises a central question: is it better
to build a specialist model from scratch or adapt a generalist foundation model
for a specific system? The trade-offs in data efficiency, predictive accuracy,
and risks of out-of-distribution (OOD) failure remain unclear. Here, we present
a benchmarking framework that contrasts bespoke (from scratch) and fine-tuned
foundation models in a test case of a technologically relevant 2D material,
Cr-intercalated Sb2Te3, using the MACE architecture. Our framework employs
migration pathways, evaluated through nudged elastic band (NEB) trajectories,
as a diagnostic probe that tests both interpolation and extrapolation. We
assess accuracy for equilibrium, kinetic (atomic migration), and mechanical
(interlayer sliding) tasks. While all models capture equilibrium structures,
predictions for non-equilibrium processes diverge. Task-specific fine-tuning
substantially improves kinetic accuracy compared with both from-scratch and
zero-shot models, but can degrade learned representations of long-range
physics. Analysis of internal representations shows that training paradigms
yield distinct, non-overlapping latent encodings of system physics. This work
offers a practical guide for MLFF development, highlights migration-based
probes as efficient diagnostics, and suggests pathways toward uncertainty-aware
active learning strategies.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [632] [Robotic Fire Risk Detection based on Dynamic Knowledge Graph Reasoning: An LLM-Driven Approach with Graph Chain-of-Thought](https://arxiv.org/abs/2509.00054)
*Haimei Pan,Jiyun Zhang,Qinxi Wei,Xiongnan Jin,Chen Xinkai,Jie Cheng*

Main category: cs.RO

TL;DR: 为提升火灾场景中机器人智能感知与响应规划能力，构建知识图谱并提出IOG框架，经实验证明该框架在火灾风险检测与救援决策中有良好应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前火灾预警与救援研究因感知不完整、态势感知不足和响应延迟面临挑战，需提升机器人在火灾场景的智能感知与响应规划能力。

Method: 利用大语言模型构建知识图谱整合火灾领域知识与救援任务信息，提出IOG框架集成知识图谱结构化信息与大跨模态模型。

Result: 通过大量模拟和实际实验表明，IOG框架在火灾风险检测和救援决策中具有良好的适用性和实际应用价值。

Conclusion: IOG框架能够有效用于火灾风险检测和救援决策，具有一定的实用意义。

Abstract: Fire is a highly destructive disaster, but effective prevention can
significantly reduce its likelihood of occurrence. When it happens, deploying
emergency robots in fire-risk scenarios can help minimize the danger to human
responders. However, current research on pre-disaster warnings and
disaster-time rescue still faces significant challenges due to incomplete
perception, inadequate fire situational awareness, and delayed response. To
enhance intelligent perception and response planning for robots in fire
scenarios, we first construct a knowledge graph (KG) by leveraging large
language models (LLMs) to integrate fire domain knowledge derived from fire
prevention guidelines and fire rescue task information from robotic emergency
response documents. We then propose a new framework called Insights-on-Graph
(IOG), which integrates the structured fire information of KG and Large
Multimodal Models (LMMs). The framework generates perception-driven risk graphs
from real-time scene imagery to enable early fire risk detection and provide
interpretable emergency responses for task module and robot component
configuration based on the evolving risk situation. Extensive simulations and
real-world experiments show that IOG has good applicability and practical
application value in fire risk detection and rescue decision-making.

</details>


### [633] [U2UData-2: A Scalable Swarm UAVs Autonomous Flight Dataset for Long-horizon Tasks](https://arxiv.org/abs/2509.00055)
*Tongtong Feng,Xin Wang,Feilin Han,Leping Zhang,Wenwu Zhu*

Main category: cs.RO

TL;DR: 本文提出用于长时程任务的大规模集群无人机自主飞行数据集U2UData - 2和可扩展的集群无人机数据在线收集与算法闭环验证平台，还引入野生动物保护长时程任务并提供基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据集限制仅关注特定基本任务，无法用于长时程任务的实际部署，而长时程任务有特殊要求。

Method: 使用15架无人机进行自主协同飞行收集数据，构建包含多种场景和数据的数据集，搭建支持多种自定义功能的平台。

Result: 得到U2UData - 2数据集，包含大量数据和环境信息，搭建了可定制、可在线收集数据和验证算法的平台，还引入野生动物保护任务并提供基准测试。

Conclusion: U2UData - 2数据集和平台有助于推动集群无人机长时程任务的自主飞行，可在https://fengtt42.github.io/U2UData - 2/获取。

Abstract: Swarm UAV autonomous flight for Long-Horizon (LH) tasks is crucial for
advancing the low-altitude economy. However, existing methods focus only on
specific basic tasks due to dataset limitations, failing in real-world
deployment for LH tasks. LH tasks are not mere concatenations of basic tasks,
requiring handling long-term dependencies, maintaining persistent states, and
adapting to dynamic goal shifts. This paper presents U2UData-2, the first
large-scale swarm UAV autonomous flight dataset for LH tasks and the first
scalable swarm UAV data online collection and algorithm closed-loop
verification platform. The dataset is captured by 15 UAVs in autonomous
collaborative flights for LH tasks, comprising 12 scenes, 720 traces, 120
hours, 600 seconds per trajectory, 4.32M LiDAR frames, and 12.96M RGB frames.
This dataset also includes brightness, temperature, humidity, smoke, and
airflow values covering all flight routes. The platform supports the
customization of simulators, UAVs, sensors, flight algorithms, formation modes,
and LH tasks. Through a visual control window, this platform allows users to
collect customized datasets through one-click deployment online and to verify
algorithms by closed-loop simulation. U2UData-2 also introduces an LH task for
wildlife conservation and provides comprehensive benchmarks with 9 SOTA models.
U2UData-2 can be found at https://fengtt42.github.io/U2UData-2/.

</details>


### [634] [First Order Model-Based RL through Decoupled Backpropagation](https://arxiv.org/abs/2509.00215)
*Joseph Amigo,Rooholla Khorrambakht,Elliot Chane-Sane,Nicolas Mansard,Ludovic Righetti*

Main category: cs.RO

TL;DR: 本文提出一种将轨迹生成与梯度计算解耦的方法，在基准控制任务和真实机器人上验证了算法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的强化学习方法获取模拟器梯度不实际，基于模型的强化学习存在预测误差影响策略性能的问题。

Method: 提出将轨迹生成与梯度计算解耦的方法，轨迹用模拟器展开，梯度通过学习的可微模型反向传播计算。

Result: 该方法达到了SHAC等专业优化器的样本效率和速度，保持了PPO等标准方法的通用性，避免了其他一阶基于模型强化学习方法的不良行为。

Conclusion: 在基准控制任务和真实Go2四足机器人的双足和四足运动任务上验证了算法的有效性。

Abstract: There is growing interest in reinforcement learning (RL) methods that
leverage the simulator's derivatives to improve learning efficiency. While
early gradient-based approaches have demonstrated superior performance compared
to derivative-free methods, accessing simulator gradients is often impractical
due to their implementation cost or unavailability. Model-based RL (MBRL) can
approximate these gradients via learned dynamics models, but the solver
efficiency suffers from compounding prediction errors during training rollouts,
which can degrade policy performance. We propose an approach that decouples
trajectory generation from gradient computation: trajectories are unrolled
using a simulator, while gradients are computed via backpropagation through a
learned differentiable model of the simulator. This hybrid design enables
efficient and consistent first-order policy optimization, even when simulator
gradients are unavailable, as well as learning a critic from simulation
rollouts, which is more accurate. Our method achieves the sample efficiency and
speed of specialized optimizers such as SHAC, while maintaining the generality
of standard approaches like PPO and avoiding ill behaviors observed in other
first-order MBRL methods. We empirically validate our algorithm on benchmark
control tasks and demonstrate its effectiveness on a real Go2 quadruped robot,
across both quadrupedal and bipedal locomotion tasks.

</details>


### [635] [Embodied AI in Social Spaces: Responsible and Adaptive Robots in Complex Setting - UKAIRS 2025 (Copy)](https://arxiv.org/abs/2509.00218)
*Aleksandra Landowska,Aislinn D Gomez Bergin,Ayodeji O. Abioye,Jayati Deshmukh,Andriana Bouadouki,Maria Wheadon,Athina Georgara,Dominic Price,Tuyen Nguyen,Shuang Ao,Lokesh Singh,Yi Long,Raffaele Miele,Joel E. Fischer,Sarvapali D. Ramchurn*

Main category: cs.RO

TL;DR: 本文介绍并概述一个多学科项目，旨在开发适用于复杂动态环境的多人类多机器人系统，展示具身AI对可持续、道德和以人为本未来的支持。


<details>
  <summary>Details</summary>
Motivation: 开发适用于复杂动态环境的负责任且自适应的多人类多机器人系统。

Method: 整合协同设计、伦理框架和多模态传感来创建AI驱动的机器人。

Result: 展示了项目愿景、方法和早期成果。

Conclusion: 具身AI能支持可持续、道德和以人为本的未来。

Abstract: This paper introduces and overviews a multidisciplinary project aimed at
developing responsible and adaptive multi-human multi-robot (MHMR) systems for
complex, dynamic settings. The project integrates co-design, ethical
frameworks, and multimodal sensing to create AI-driven robots that are
emotionally responsive, context-aware, and aligned with the needs of diverse
users. We outline the project's vision, methodology, and early outcomes,
demonstrating how embodied AI can support sustainable, ethical, and
human-centred futures.

</details>


### [636] [TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization](https://arxiv.org/abs/2509.00310)
*Yuxuan Ding,Shuangge Wang,Tesca Fitzgerald*

Main category: cs.RO

TL;DR: 本文提出TReF - 6方法，从单轨迹推断6DoF任务相关帧，支持跨不同对象配置的一次性模仿学习。


<details>
  <summary>Details</summary>
Motivation: 机器人因缺乏可转移和可解释的空间表示，难以从单一演示中泛化。

Method: 引入TReF - 6方法，从轨迹几何中确定影响点定义局部坐标系原点，结合视觉语言模型和Grounded - SAM定位。

Result: 在仿真中验证了TReF - 6，展示了对轨迹噪声的鲁棒性，在真实操作任务中部署端到端管道。

Conclusion: TReF - 6支持一次性模仿学习，能在不同对象配置中保留任务意图。

Abstract: Robots often struggle to generalize from a single demonstration due to the
lack of a transferable and interpretable spatial representation. In this work,
we introduce TReF-6, a method that infers a simplified, abstracted 6DoF
Task-Relevant Frame from a single trajectory. Our approach identifies an
influence point purely from the trajectory geometry to define the origin for a
local frame, which serves as a reference for parameterizing a Dynamic Movement
Primitive (DMP). This influence point captures the task's spatial structure,
extending the standard DMP formulation beyond start-goal imitation. The
inferred frame is semantically grounded via a vision-language model and
localized in novel scenes by Grounded-SAM, enabling functionally consistent
skill generalization. We validate TReF-6 in simulation and demonstrate
robustness to trajectory noise. We further deploy an end-to-end pipeline on
real-world manipulation tasks, showing that TReF-6 supports one-shot imitation
learning that preserves task intent across diverse object configurations.

</details>


### [637] [A Framework for Task and Motion Planning based on Expanding AND/OR Graphs](https://arxiv.org/abs/2509.00317)
*Fulvio Mastrogiovanni,Antony Thomas*

Main category: cs.RO

TL;DR: 本文介绍基于扩展与或图的任务与运动规划框架TMP - EAOG，评估显示其能应对基准测试挑战。


<details>
  <summary>Details</summary>
Motivation: 太空环境中机器人自主面临诸多挑战，任务与运动规划（TMP）对自主服务等任务至关重要。

Method: 引入基于扩展与或图的TMP - EAOG框架，在与或图中编码任务抽象，迭代扩展并进行运动规划评估。

Result: 在两个基准领域评估TMP - EAOG，以模拟移动机械臂为代理，显示其能处理多种挑战。

Conclusion: TMP - EAOG具有一定不确定性鲁棒性、可控自主性和有界灵活性，能应对基准测试中的各种挑战。

Abstract: Robot autonomy in space environments presents unique challenges, including
high perception and motion uncertainty, strict kinematic constraints, and
limited opportunities for human intervention. Therefore, Task and Motion
Planning (TMP) may be critical for autonomous servicing, surface operations, or
even in-orbit missions, just to name a few, as it models tasks as discrete
action sequencing integrated with continuous motion feasibility assessments. In
this paper, we introduce a TMP framework based on expanding AND/OR graphs,
referred to as TMP-EAOG, and demonstrate its adaptability to different
scenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph,
which expands iteratively as the plan is executed, and performs in-the-loop
motion planning assessments to ascertain their feasibility. As a consequence,
TMP-EAOG is characterised by the desirable properties of (i) robustness to a
certain degree of uncertainty, because AND/OR graph expansion can accommodate
for unpredictable information about the robot environment, (ii) controlled
autonomy, since an AND/OR graph can be validated by human experts, and (iii)
bounded flexibility, in that unexpected events, including the assessment of
unfeasible motions, can lead to different courses of action as alternative
paths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We
use a simulated mobile manipulator as a proxy for space-grade autonomous
robots. Our evaluation shows that TMP-EAOG can deal with a wide range of
challenges in the benchmarks.

</details>


### [638] [Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach](https://arxiv.org/abs/2509.00319)
*Chi Kit Ng,Huxin Gao,Tian-Ao Ren,Jiewen Lai,Hongliang Ren*

Main category: cs.RO

TL;DR: 本文提出基于深度强化学习的接触辅助导航策略（CAN）用于柔性机器人内窥镜（FRE）在胃肠道导航，提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 柔性机器人内窥镜在动态胃部导航具有挑战性，需有效利用与变形胃壁的接触到达目标位置。

Method: 引入基于深度强化学习的CAN策略，利用接触力反馈，通过基于物理的有限元方法模拟变形胃建立训练环境，用近端策略优化（PPO）算法训练。

Result: 在静态和动态胃部环境中成功率达100%，平均误差1.6毫米，在有强外部干扰的未知场景中成功率达85%。

Conclusion: 基于DRL的CAN策略比先前方法显著提升FRE导航性能。

Abstract: Navigating a flexible robotic endoscope (FRE) through the gastrointestinal
tract is critical for surgical diagnosis and treatment. However, navigation in
the dynamic stomach is particularly challenging because the FRE must learn to
effectively use contact with the deformable stomach walls to reach target
locations. To address this, we introduce a deep reinforcement learning (DRL)
based Contact-Aided Navigation (CAN) strategy for FREs, leveraging contact
force feedback to enhance motion stability and navigation precision. The
training environment is established using a physics-based finite element method
(FEM) simulation of a deformable stomach. Trained with the Proximal Policy
Optimization (PPO) algorithm, our approach achieves high navigation success
rates (within 3 mm error between the FRE's end-effector and target) and
significantly outperforms baseline policies. In both static and dynamic stomach
environments, the CAN agent achieved a 100% success rate with 1.6 mm average
error, and it maintained an 85% success rate in challenging unseen scenarios
with stronger external disturbances. These results validate that the DRL-based
CAN strategy substantially enhances FRE navigation performance over prior
methods.

</details>


### [639] [Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic Endoluminal Navigation of Deformable Continuum Robots](https://arxiv.org/abs/2509.00329)
*Yu Tian,Chi Kit Ng,Hongliang Ren*

Main category: cs.RO

TL;DR: 提出JEDP - RL框架用于可变形连续体机器人规划，在模拟中比PPO有优势。


<details>
  <summary>Details</summary>
Motivation: 可变形连续体机器人规划有挑战，传统RL方法和基于雅可比的方法存在局限性。

Method: 提出JEDP - RL框架，将规划分解为分阶段的雅可比估计和策略执行，训练时先局部探索估计雅可比矩阵，再用雅可比特征增强状态表示。

Result: 在SOFA手术动态模拟中，比PPO基线：收敛速度快3.2倍；导航效率高，到达目标步数少25%；泛化能力强，材料属性变化时成功率92%，在未见组织环境中成功率83%（比PPO高33%）。

Conclusion: JEDP - RL框架在可变形连续体机器人规划中表现优于PPO基线，具有更快收敛速度、更高导航效率和更强泛化能力。

Abstract: Deformable continuum robots (DCRs) present unique planning challenges due to
nonlinear deformation mechanics and partial state observability, violating the
Markov assumptions of conventional reinforcement learning (RL) methods. While
Jacobian-based approaches offer theoretical foundations for rigid manipulators,
their direct application to DCRs remains limited by time-varying kinematics and
underactuated deformation dynamics. This paper proposes Jacobian Exploratory
Dual-Phase RL (JEDP-RL), a framework that decomposes planning into phased
Jacobian estimation and policy execution. During each training step, we first
perform small-scale local exploratory actions to estimate the deformation
Jacobian matrix, then augment the state representation with Jacobian features
to restore approximate Markovianity. Extensive SOFA surgical dynamic
simulations demonstrate JEDP-RL's three key advantages over proximal policy
optimization (PPO) baselines: 1) Convergence speed: 3.2x faster policy
convergence, 2) Navigation efficiency: requires 25% fewer steps to reach the
target, and 3) Generalization ability: achieve 92% success rate under material
property variations and achieve 83% (33% higher than PPO) success rate in the
unseen tissue environment.

</details>


### [640] [Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial Reasoning](https://arxiv.org/abs/2509.00465)
*Jiading Fang*

Main category: cs.RO

TL;DR: 论文引入具身空间智能解决机器人按自然语言指令感知和行动的挑战，在场景表示和空间推理两方面作出贡献，为机器人奠定基础。


<details>
  <summary>Details</summary>
Motivation: 应对创建能基于自然语言指令在现实世界感知和行动的机器人的挑战，弥合大语言模型与物理具身之间的差距。

Method: 在感知方面，用隐式神经模型开发场景表示，包括自监督相机校准、高保真深度场生成和大规模重建；在空间推理方面，引入新导航基准、3D语言锚定方法和状态反馈机制。

Result: 开发了鲁棒、可扩展且准确的场景表示，增强了大语言模型的空间能力。

Conclusion: 为机器人稳健感知周围环境并根据复杂语言指令智能行动奠定了基础。

Abstract: This thesis introduces "Embodied Spatial Intelligence" to address the
challenge of creating robots that can perceive and act in the real world based
on natural language instructions. To bridge the gap between Large Language
Models (LLMs) and physical embodiment, we present contributions on two fronts:
scene representation and spatial reasoning. For perception, we develop robust,
scalable, and accurate scene representations using implicit neural models, with
contributions in self-supervised camera calibration, high-fidelity depth field
generation, and large-scale reconstruction. For spatial reasoning, we enhance
the spatial capabilities of LLMs by introducing a novel navigation benchmark, a
method for grounding language in 3D, and a state-feedback mechanism to improve
long-horizon decision-making. This work lays a foundation for robots that can
robustly perceive their surroundings and intelligently act upon complex,
language-based commands.

</details>


### [641] [NeuralSVCD for Efficient Swept Volume Collision Detection](https://arxiv.org/abs/2509.00499)
*Dongwon Son,Hojin Jung,Beomjoon Kim*

Main category: cs.RO

TL;DR: 本文提出NeuralSVCD解决机器人在非结构化环境中运动规划的扫掠体积碰撞检测问题，实验显示其在准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统离散方法可能漏检碰撞，现有SVCD方法在效率和准确性上存在权衡，限制实际应用。

Method: 引入NeuralSVCD，通过分布式几何表示和时间优化利用形状局部性和时间局部性。

Result: NeuralSVCD在碰撞检测准确性和计算效率上始终优于现有先进的SVCD方法。

Conclusion: NeuralSVCD在多种机器人操作场景中具有强大的适用性。

Abstract: Robot manipulation in unstructured environments requires efficient and
reliable Swept Volume Collision Detection (SVCD) for safe motion planning.
Traditional discrete methods potentially miss collisions between these points,
whereas SVCD continuously checks for collisions along the entire trajectory.
Existing SVCD methods typically face a trade-off between efficiency and
accuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a
novel neural encoder-decoder architecture tailored to overcome this trade-off.
Our approach leverages shape locality and temporal locality through distributed
geometric representations and temporal optimization. This enhances
computational efficiency without sacrificing accuracy. Comprehensive
experiments show that NeuralSVCD consistently outperforms existing
state-of-the-art SVCD methods in terms of both collision detection accuracy and
computational efficiency, demonstrating its robust applicability across diverse
robotic manipulation scenarios. Code and videos are available at
https://neuralsvcd.github.io/.

</details>


### [642] [Mechanistic interpretability for steering vision-language-action models](https://arxiv.org/abs/2509.00328)
*Bear Häon,Kaylene Stocking,Ian Chuang,Claire Tomlin*

Main category: cs.RO

TL;DR: 本文引入首个通过内部表征解释和引导视觉 - 语言 - 动作（VLA）模型的框架，在模拟和实体机器人上实现零样本行为控制，建立了机器人学中可解释和可引导基础模型的新范式。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的解释和引导方法远不及经典机器人管道，缺乏机制洞察，难以在现实世界机器人中部署学习策略，因此受大语言模型机制可解释性进展的启发开展研究。

Method: 将变压器层内的前馈激活投影到词嵌入基上，识别与动作选择有因果联系的稀疏语义方向，引入通用激活引导方法实时调节行为。

Result: 在Pi0和OpenVLA两个开源VLA模型上进行评估，在模拟环境（LIBERO）和实体机器人（UR5）上实现零样本行为控制。

Conclusion: 可以系统地利用具身VLA模型的可解释组件进行控制，为机器人学中的基础模型建立了新范式。

Abstract: Vision-Language-Action (VLA) models are a promising path to realizing
generalist embodied agents that can quickly adapt to new tasks, modalities, and
environments. However, methods for interpreting and steering VLAs fall far
short of classical robotics pipelines, which are grounded in explicit models of
kinematics, dynamics, and control. This lack of mechanistic insight is a
central challenge for deploying learned policies in real-world robotics, where
robustness and explainability are critical. Motivated by advances in
mechanistic interpretability for large language models, we introduce the first
framework for interpreting and steering VLAs via their internal
representations, enabling direct intervention in model behavior at inference
time. We project feedforward activations within transformer layers onto the
token embedding basis, identifying sparse semantic directions - such as speed
and direction - that are causally linked to action selection. Leveraging these
findings, we introduce a general-purpose activation steering method that
modulates behavior in real time, without fine-tuning, reward signals, or
environment interaction. We evaluate this method on two recent open-source
VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in
simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that
interpretable components of embodied VLAs can be systematically harnessed for
control - establishing a new paradigm for transparent and steerable foundation
models in robotics.

</details>


### [643] [Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot](https://arxiv.org/abs/2509.00564)
*Philip Lorimer,Jack Saunders,Alan Hunter,Wenbin Li*

Main category: cs.RO

TL;DR: 研究应用强化学习实现自由漫游拍摄机器人自动推轨镜头，对比控制策略，在模拟和实际测试中验证方法有效性，推动电影制作技术发展。


<details>
  <summary>Details</summary>
Motivation: 解决自由漫游摄影车自动化相机控制的挑战，推动电影制作领域发展。

Method: 应用强化学习实现自动推轨镜头，对比联合控制和独立控制策略。

Result: 强化学习管道在模拟中超越传统比例 - 微分控制器，在实际测试中证明有效。

Conclusion: 该方法具有实用性，为复杂拍摄场景研究奠定基础，促进技术与电影创意融合。

Abstract: Free-roaming dollies enhance filmmaking with dynamic movement, but challenges
in automated camera control remain unresolved. Our study advances this field by
applying Reinforcement Learning (RL) to automate dolly-in shots using
free-roaming ground-based filming robots, overcoming traditional control
hurdles. We demonstrate the effectiveness of combined control for precise film
tasks by comparing it to independent control strategies. Our robust RL pipeline
surpasses traditional Proportional-Derivative controller performance in
simulation and proves its efficacy in real-world tests on a modified ROSBot 2.0
platform equipped with a camera turret. This validates our approach's
practicality and sets the stage for further research in complex filming
scenarios, contributing significantly to the fusion of technology with
cinematic creativity. This work presents a leap forward in the field and opens
new avenues for research and development, effectively bridging the gap between
technological advancement and creative filmmaking.

</details>


### [644] [Learning Dolly-In Filming From Demonstration Using a Ground-Based Robot](https://arxiv.org/abs/2509.00574)
*Philip Lorimer,Alan Hunter,Wenbin Li*

Main category: cs.RO

TL;DR: 提出基于GAIL的LfD方法用于拍摄机器人自动推轨镜头，在模拟和现实中表现良好，为电影领域提供无奖励的RL替代方案。


<details>
  <summary>Details</summary>
Motivation: 电影摄像机控制难以通过手工奖励函数编码，RL在机器人电影制作中依赖定制奖励和大量调整，限制了创意可用性。

Method: 使用GAIL的LfD方法，通过操纵杆遥操作在模拟中收集专家轨迹进行训练。

Result: GAIL策略在模拟中优于PPO基线，在现实中无需微调可直接应用，比TD3方法有更一致的取景和主体对齐。

Conclusion: LfD为电影领域提供了强大、无奖励的RL替代方案，减少技术工作，弥合艺术意图和机器人自主性之间的差距。

Abstract: Cinematic camera control demands a balance of precision and artistry -
qualities that are difficult to encode through handcrafted reward functions.
While reinforcement learning (RL) has been applied to robotic filmmaking, its
reliance on bespoke rewards and extensive tuning limits creative usability. We
propose a Learning from Demonstration (LfD) approach using Generative
Adversarial Imitation Learning (GAIL) to automate dolly-in shots with a
free-roaming, ground-based filming robot. Expert trajectories are collected via
joystick teleoperation in simulation, capturing smooth, expressive motion
without explicit objective design.
  Trained exclusively on these demonstrations, our GAIL policy outperforms a
PPO baseline in simulation, achieving higher rewards, faster convergence, and
lower variance. Crucially, it transfers directly to a real-world robot without
fine-tuning, achieving more consistent framing and subject alignment than a
prior TD3-based method. These results show that LfD offers a robust,
reward-free alternative to RL in cinematic domains, enabling real-time
deployment with minimal technical effort. Our pipeline brings intuitive,
stylized camera control within reach of creative professionals, bridging the
gap between artistic intent and robotic autonomy.

</details>


### [645] [Data Retrieval with Importance Weights for Few-Shot Imitation Learning](https://arxiv.org/abs/2509.01657)
*Amber Xie,Rahul Chand,Dorsa Sadigh,Joey Hejna*

Main category: cs.RO

TL;DR: 现有基于检索的少样本模仿学习方法有局限，提出重要性加权检索（IWR）方法，在模拟和真实环境评估中提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的少样本模仿学习方法使用的规则存在依赖高方差最近邻估计且未考虑先验数据分布的问题。

Method: 引入重要性加权检索（IWR），用高斯核密度估计来估计重要性权重进行数据检索。

Result: 在模拟环境和真实世界的Bridge数据集评估中，IWR持续提升了现有基于检索方法的性能。

Conclusion: IWR仅需微小修改就能改善现有基于检索方法的性能。

Abstract: While large-scale robot datasets have propelled recent progress in imitation
learning, learning from smaller task specific datasets remains critical for
deployment in new environments and unseen tasks. One such approach to few-shot
imitation learning is retrieval-based imitation learning, which extracts
relevant samples from large, widely available prior datasets to augment a
limited demonstration dataset. To determine the relevant data from prior
datasets, retrieval-based approaches most commonly calculate a prior data
point's minimum distance to a point in the target dataset in latent space.
While retrieval-based methods have shown success using this metric for data
selection, we demonstrate its equivalence to the limit of a Gaussian kernel
density (KDE) estimate of the target data distribution. This reveals two
shortcomings of the retrieval rule used in prior work. First, it relies on
high-variance nearest neighbor estimates that are susceptible to noise. Second,
it does not account for the distribution of prior data when retrieving data. To
address these issues, we introduce Importance Weighted Retrieval (IWR), which
estimates importance weights, or the ratio between the target and prior data
distributions for retrieval, using Gaussian KDEs. By considering the
probability ratio, IWR seeks to mitigate the bias of previous selection rules,
and by using reasonable modeling parameters, IWR effectively smooths estimates
using all data points. Across both simulation environments and real-world
evaluations on the Bridge dataset we find that our method, IWR, consistently
improves performance of existing retrieval-based methods, despite only
requiring minor modifications.

</details>


### [646] [Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk Assessment](https://arxiv.org/abs/2509.01836)
*Md Mahbub Alam,Jose F. Rodrigues-Jr,Gabriel Spadon*

Main category: cs.RO

TL;DR: 提出基于Transformer的多船轨迹预测框架，集成碰撞风险分析，在真实AIS数据上表现出色，能量化碰撞风险以增强海上安全。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型多为单船预测，忽略船舶交互、航行规则和碰撞风险评估，需要准确的多船轨迹预测以增强态势感知和防止碰撞。

Method: 框架识别目标船附近船舶，通过并行流编码运动学和物理特征，利用因果卷积处理时间局部性，空间变换进行位置编码，混合位置嵌入捕获局部运动模式和长距离依赖，联合预测未来轨迹。

Result: 在大规模真实AIS数据上，使用联合多船指标评估，模型预测能力优于传统单船位移误差指标。

Conclusion: 该框架能通过模拟预测轨迹间的交互量化潜在碰撞风险，为增强海上安全和决策支持提供可行见解。

Abstract: Accurate vessel trajectory prediction is essential for enhancing situational
awareness and preventing collisions. Still, existing data-driven models are
constrained mainly to single-vessel forecasting, overlooking vessel
interactions, navigation rules, and explicit collision risk assessment. We
present a transformer-based framework for multi-vessel trajectory prediction
with integrated collision risk analysis. For a given target vessel, the
framework identifies nearby vessels. It jointly predicts their future
trajectories through parallel streams encoding kinematic and derived physical
features, causal convolutions for temporal locality, spatial transformations
for positional encoding, and hybrid positional embeddings that capture both
local motion patterns and long-range dependencies. Evaluated on large-scale
real-world AIS data using joint multi-vessel metrics, the model demonstrates
superior forecasting capabilities beyond traditional single-vessel displacement
errors. By simulating interactions among predicted trajectories, the framework
further quantifies potential collision risks, offering actionable insights to
strengthen maritime safety and decision support.

</details>


### [647] [Constrained Decoding for Robotics Foundation Models](https://arxiv.org/abs/2509.01728)
*Parv Kapoor,Akila Ganlath,Changliu Liu,Sebastian Scherer,Eunsuk Kang*

Main category: cs.RO

TL;DR: 提出一种用于机器人基础模型的受限解码框架，可执行动力学系统中动作轨迹的逻辑约束。


<details>
  <summary>Details</summary>
Motivation: 现有机器人基础模型是数据驱动的，缺乏行为正确性和安全约束的明确概念。

Method: 引入受限解码框架，确保生成的动作在运行时可证明满足信号时序逻辑（STL）规范，且无需重新训练，同时与基础模型无关。

Result: 对该方法在最先进的导航基础模型上进行全面评估，表明解码时的干预不仅可过滤不安全动作，还可用于条件动作生成。

Conclusion: 所提出的受限解码框架能有效解决现有机器人基础模型的局限性。

Abstract: Recent advances in the development of robotic foundation models have led to
promising end-to-end and general-purpose capabilities in robotic systems. These
models are pretrained on vast datasets of robot trajectories to process multi-
modal inputs and directly output a sequence of action that the system then
executes in the real world. Although this approach is attractive from the
perspective of im- proved generalization across diverse tasks, these models are
still data-driven and, therefore, lack explicit notions of behavioral
correctness and safety constraints. We address these limitations by introducing
a constrained decoding framework for robotics foundation models that enforces
logical constraints on action trajec- tories in dynamical systems. Our method
ensures that generated actions provably satisfy signal temporal logic (STL)
specifications at runtime without retraining, while remaining agnostic of the
underlying foundation model. We perform com- prehensive evaluation of our
approach across state-of-the-art navigation founda- tion models and we show
that our decoding-time interventions are useful not only for filtering unsafe
actions but also for conditional action-generation. Videos available on our
website: https://constrained-robot-fms.github.io

</details>


### [648] [Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance](https://arxiv.org/abs/2509.02055)
*Yang Zhang,Chenwei Wang,Ouyang Lu,Yuan Zhao,Yunfei Ge,Zhenglong Sun,Xiu Li,Chi Zhang,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 提出Align-Then-stEer (ATE)框架解决VLA模型下游任务适配难题，实验显示在模拟和现实场景中提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在下游任务适配存在瓶颈，行动分布不匹配，微调需大量数据和计算资源。

Method: 构建统一潜在空间对齐不同行动空间，通过引导机制推动模型输出分布向目标域靠近。

Result: 相比直接微调，模拟中平均多任务成功率最高提升9.8%，现实跨实体场景成功率提升32%。

Conclusion: 提出通用轻量级解决方案，增强VLA模型在新机器人平台和任务的实用性。

Abstract: Vision-Language-Action (VLA) models pre-trained on large, diverse datasets
show remarkable potential for general-purpose robotic manipulation. However, a
primary bottleneck remains in adapting these models to downstream tasks,
especially when the robot's embodiment or the task itself differs from the
pre-training data. This discrepancy leads to a significant mismatch in action
distributions, demanding extensive data and compute for effective fine-tuning.
To address this challenge, we introduce \textbf{Align-Then-stEer
(\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation
framework. \texttt{ATE} first aligns disparate action spaces by constructing a
unified latent space, where a variational autoencoder constrained by reverse KL
divergence embeds adaptation actions into modes of the pre-training action
latent distribution. Subsequently, it steers the diffusion- or flow-based VLA's
generation process during fine-tuning via a guidance mechanism that pushes the
model's output distribution towards the target domain. We conduct extensive
experiments on cross-embodiment and cross-task manipulation in both simulation
and real world. Compared to direct fine-tuning of representative VLAs, our
method improves the average multi-task success rate by up to \textbf{9.8\%} in
simulation and achieves a striking \textbf{32\% success rate gain} in a
real-world cross-embodiment setting. Our work presents a general and
lightweight solution that greatly enhances the practicality of deploying VLA
models to new robotic platforms and tasks.

</details>


### [649] [Learning Social Heuristics for Human-Aware Path Planning](https://arxiv.org/abs/2509.02134)
*Andrea Eirale,Matteo Leonetti,Marcello Chiaberge*

Main category: cs.RO

TL;DR: 提出Heuristic Planning with Learned Social Value (HPLSV)方法用于社交导航，初步应用于排队场景并计划推广。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多聚焦传统导航，为使机器人真正被社会接受，需让其达成特定社交规范，这需要专门学习过程。

Method: 提出HPLSV方法，学习封装社交导航成本的价值函数，并将其作为启发式搜索路径规划的额外启发式。

Result: 将该方法应用于排队的常见社交场景。

Conclusion: 计划将该方法推广到更多人类活动场景。

Abstract: Social robotic navigation has been at the center of numerous studies in
recent years. Most of the research has focused on driving the robotic agent
along obstacle-free trajectories, respecting social distances from humans, and
predicting their movements to optimize navigation. However, in order to really
be socially accepted, the robots must be able to attain certain social norms
that cannot arise from conventional navigation, but require a dedicated
learning process. We propose Heuristic Planning with Learned Social Value
(HPLSV), a method to learn a value function encapsulating the cost of social
navigation, and use it as an additional heuristic in heuristic-search path
planning. In this preliminary work, we apply the methodology to the common
social scenario of joining a queue of people, with the intention of
generalizing to further human activities.

</details>


### [650] [AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring](https://arxiv.org/abs/2509.01878)
*Scarlett Raine,Tobias Fischer*

Main category: cs.RO

TL;DR: 论文探讨水下AI成为研究前沿，分析其发展驱动因素、面临挑战及新兴趋势，指出水下研究推动AI方法创新，成果可惠及多领域。


<details>
  <summary>Details</summary>
Motivation: 气候变化使海洋生态系统面临压力，需要可扩展的AI监测解决方案，研究水下AI发展情况及影响。

Method: 分析水下AI发展的驱动因素，研究水下挑战对AI方法的推动，调查数据集、场景理解和3D重建等新兴趋势。

Result: 发现三个驱动因素，水下挑战推动了弱监督学习等方面的进步，出现从被动观察到主动干预的范式转变。

Conclusion: 水下研究推动了基础模型等方面的边界拓展，方法创新可惠及多个领域。

Abstract: Marine ecosystems face increasing pressure due to climate change, driving the
need for scalable, AI-powered monitoring solutions. This paper examines the
rapid emergence of underwater AI as a major research frontier and analyzes the
factors that have transformed marine perception from a niche application into a
catalyst for AI innovation. We identify three convergent drivers: environmental
necessity for ecosystem-scale monitoring, democratization of underwater
datasets through citizen science platforms, and researcher migration from
saturated terrestrial computer vision domains. Our analysis reveals how unique
underwater challenges - turbidity, cryptic species detection, expert annotation
bottlenecks, and cross-ecosystem generalization - are driving fundamental
advances in weakly supervised learning, open-set recognition, and robust
perception under degraded conditions. We survey emerging trends in datasets,
scene understanding and 3D reconstruction, highlighting the paradigm shift from
passive observation toward AI-driven, targeted intervention capabilities. The
paper demonstrates how underwater constraints are pushing the boundaries of
foundation models, self-supervised learning, and perception, with
methodological innovations that extend far beyond marine applications to
benefit general computer vision, robotics, and environmental monitoring.

</details>


### [651] [Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified Approach to Security and Safety](https://arxiv.org/abs/2509.02163)
*Wenxiao Zhang,Xiangrui Kong,Conan Dewitt,Thomas Bräunl,Jin B. Hong*

Main category: cs.RO

TL;DR: 提出统一框架提升基于大语言模型的机器人系统可靠性，实验显示有显著改善并开源框架。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型集成到机器人系统虽有进步，但确保系统可靠性（包括对抗攻击安全性和复杂环境安全性）是关键挑战。

Method: 提出统一框架，结合提示组装、状态管理和安全验证，用性能和安全指标评估。

Result: 实验表明在注入攻击下有30.8%的改善，在复杂环境对抗条件下最多有325%的改善。

Conclusion: 该工作弥合了基于大语言模型的机器人系统中安全与保障的差距，为在现实世界部署可靠的集成大语言模型的移动机器人提供了可行见解。

Abstract: Integrating large language models (LLMs) into robotic systems has
revolutionised embodied artificial intelligence, enabling advanced
decision-making and adaptability. However, ensuring reliability, encompassing
both security against adversarial attacks and safety in complex environments,
remains a critical challenge. To address this, we propose a unified framework
that mitigates prompt injection attacks while enforcing operational safety
through robust validation mechanisms. Our approach combines prompt assembling,
state management, and safety validation, evaluated using both performance and
security metrics. Experiments show a 30.8% improvement under injection attacks
and up to a 325% improvement in complex environment settings under adversarial
conditions compared to baseline scenarios. This work bridges the gap between
safety and security in LLM-based robotic systems, offering actionable insights
for deploying reliable LLM-integrated mobile robots in real-world settings. The
framework is open-sourced with simulation and physical deployment demos at
https://llmeyesim.vercel.app/

</details>


### [652] [Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots](https://arxiv.org/abs/2509.02530)
*Minghuan Liu,Zhengbang Zhu,Xiaoshen Han,Peng Hu,Haotong Lin,Xinyao Li,Jingxiao Chen,Jiafeng Xu,Yichu Yang,Yunfeng Lin,Xinghang Li,Yong Yu,Weinan Zhang,Tao Kong,Bingyi Kang*

Main category: cs.RO

TL;DR: 提出Camera Depth Models (CDMs) 作为日常深度相机的插件，利用模拟数据生成高质量配对数据，使深度预测接近模拟精度，实现机器人策略从模拟到现实的无缝泛化。


<details>
  <summary>Details</summary>
Motivation: 现代机器人操作依赖2D颜色空间视觉观察泛化性差，人类在3D世界交互更依赖物理属性，虽可从深度相机获取3D几何信息，但使用深度相机存在挑战。

Method: 提出CDMs作为插件，开发神经数据引擎，通过对深度相机噪声模式建模从模拟中生成高质量配对数据。

Result: CDMs在深度预测中达到接近模拟的精度，在两个具有挑战性的长视野任务中，未添加噪声或进行现实微调的模拟策略能无缝泛化到现实机器人，性能几乎无下降。

Conclusion: 研究结果有望启发未来在通用机器人策略中利用模拟数据和3D信息的研究。

Abstract: Modern robotic manipulation primarily relies on visual observations in a 2D
color space for skill learning but suffers from poor generalization. In
contrast, humans, living in a 3D world, depend more on physical properties-such
as distance, size, and shape-than on texture when interacting with objects.
Since such 3D geometric information can be acquired from widely available depth
cameras, it appears feasible to endow robots with similar perceptual
capabilities. Our pilot study found that using depth cameras for manipulation
is challenging, primarily due to their limited accuracy and susceptibility to
various types of noise. In this work, we propose Camera Depth Models (CDMs) as
a simple plugin on daily-use depth cameras, which take RGB images and raw depth
signals as input and output denoised, accurate metric depth. To achieve this,
we develop a neural data engine that generates high-quality paired data from
simulation by modeling a depth camera's noise pattern. Our results show that
CDMs achieve nearly simulation-level accuracy in depth prediction, effectively
bridging the sim-to-real gap for manipulation tasks. Notably, our experiments
demonstrate, for the first time, that a policy trained on raw simulated depth,
without the need for adding noise or real-world fine-tuning, generalizes
seamlessly to real-world robots on two challenging long-horizon tasks involving
articulated, reflective, and slender objects, with little to no performance
degradation. We hope our findings will inspire future research in utilizing
simulation data and 3D information in general robot policies.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [653] [Unnoticeable Community Deception via Multi-objective Optimization](https://arxiv.org/abs/2509.01438)
*Junyuan Fang,Huimin Liu,Yueqi Peng,Jiajing Wu,Zibin Zheng,Chi K. Tse*

Main category: cs.SI

TL;DR: 文章指出当前社区欺骗方法存在局限，提出新欺骗指标，将不可察觉社区欺骗任务建模为多目标优化问题，还提出两个变体方法，实验证明策略优越性。


<details>
  <summary>Details</summary>
Motivation: 现有社区检测成功会引发隐私和信息安全问题，当前社区欺骗方法存在评估指标合理性和攻击不可察觉性等局限。

Method: 通过实证研究调查常用欺骗指标局限性，提出新欺骗指标，结合攻击预算将任务建模为多目标优化问题，引入度偏置和社区偏置候选节点选择机制提出两个变体方法。

Result: 在三个基准数据集上的大量实验表明，所提出的社区欺骗策略具有优越性。

Conclusion: 所提出的社区欺骗策略能有效解决当前社区欺骗方法的局限，表现良好。

Abstract: Community detection in graphs is crucial for understanding the organization
of nodes into densely connected clusters. While numerous strategies have been
developed to identify these clusters, the success of community detection can
lead to privacy and information security concerns, as individuals may not want
their personal information exposed. To address this, community deception
methods have been proposed to reduce the effectiveness of detection algorithms.
Nevertheless, several limitations, such as the rationality of evaluation
metrics and the unnoticeability of attacks, have been ignored in current
deception methods. Therefore, in this work, we first investigate the
limitations of the widely used deception metric, i.e., the decrease of
modularity, through empirical studies. Then, we propose a new deception metric,
and combine this new metric together with the attack budget to model the
unnoticeable community deception task as a multi-objective optimization
problem. To further improve the deception performance, we propose two variant
methods by incorporating the degree-biased and community-biased candidate node
selection mechanisms. Extensive experiments on three benchmark datasets
demonstrate the superiority of the proposed community deception strategies.

</details>


### [654] [Dynamics in Two-Sided Attention Markets: Objective, Optimization, and Control](https://arxiv.org/abs/2509.01970)
*Haiqing Zhu,Yun Kuen Cheung,Lexing Xie*

Main category: cs.SI

TL;DR: 研究内容创作与消费生态系统的双边市场，设计势函数，证明双边交互动态对应镜像下降并给出非凸函数镜像下降局部收敛结果，为解释注意力市场多样结果提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 在内容多在线分发且由平台介导的情况下，需理解内容创作与消费生态系统，此前研究多关注单边市场，本文聚焦双边市场。

Method: 设计势函数，研究创作者最佳响应动态与用户多项logit选择的耦合，证明其与势函数上的镜像下降等价，分析不同平台排名策略对应的势函数。

Result: 证明了双边交互动态对应镜像下降，给出非凸函数镜像下降的新局部收敛结果。

Conclusion: 研究结果为解释注意力市场的多样结果提供了理论基础。

Abstract: With most content distributed online and mediated by platforms, there is a
pressing need to understand the ecosystem of content creation and consumption.
A considerable body of recent work shed light on the one-sided market on
creator-platform or user-platform interactions, showing key properties of
static (Nash) equilibria and online learning. In this work, we examine the {\it
two-sided} market including the platform and both users and creators. We design
a potential function for the coupled interactions among users, platform and
creators. We show that such coupling of creators' best-response dynamics with
users' multilogit choices is equivalent to mirror descent on this potential
function. Furthermore, a range of platform ranking strategies correspond to a
family of potential functions, and the dynamics of two-sided interactions still
correspond to mirror descent. We also provide new local convergence result for
mirror descent in non-convex functions, which could be of independent interest.
Our results provide a theoretical foundation for explaining the diverse
outcomes observed in attention markets.

</details>


### [655] [Content and Engagement Trends in COVID-19 YouTube Videos: Evidence from the Late Pandemic](https://arxiv.org/abs/2509.01954)
*Nirmalya Thakur,Madeline D Hartel,Lane Michael Boden,Dallas Enriquez,Boston Joyner Ricks*

Main category: cs.SI

TL;DR: 研究2023年1月至2024年10月约1万条YouTube新冠相关视频，发现其参与度受发布时间、标题词汇、话题和视频时长等因素影响。


<details>
  <summary>Details</summary>
Motivation: 评估疫情后期，时间、词汇、语言和结构因素如何影响YouTube上新冠相关视频的参与度。

Method: 调查约1万条视频，进行发布活动、词汇、情感、视频时长等多方面分析。

Result: 发布活动有工作日效应；标题高频词与新冠和YouTube特性有关；标题含“shorts”的视频浏览量高；视频描述情感与浏览量经处理后有较强相关性；不同类型视频时长对应不同浏览量。

Conclusion: 疫情后期YouTube上新冠相关视频的参与模式受发布时间、标题词汇、话题和视频时长影响，有独特特征。

Abstract: This work investigated about 10,000 COVID-19-related YouTube videos published
between January 2023 and October 2024 to evaluate how temporal, lexical,
linguistic, and structural factors influenced engagement during the late
pandemic period. Publishing activity showed consistent weekday effects: in the
first window, average views peaked on Mondays at 92,658; in the second, on
Wednesdays at 115,479; and in the third, on Fridays at 84,874, reflecting a
shift in audience attention toward mid- and late week. Lexical analysis of
video titles revealed recurring high-frequency keywords related to COVID-19 and
YouTube features, including COVID, coronavirus, shorts, and live. Frequency
analysis revealed sharp spikes, with COVID appearing in 799 video titles in
August 2024, while engagement analysis showed that videos titled with shorts
attracted very high views, peaking at 2.16 million average views per video in
June 2023. Analysis of sentiment of video descriptions in English showed weak
correlation with views in the raw data (Pearson r = 0.0154, p = 0.2987), but
stronger correlations emerged once outliers were addressed, with Spearman r =
0.110 (p < 0.001) and Pearson r = 0.0925 (p < 0.001). Category-level analysis
of video durations revealed contrasting outcomes: long videos focusing on
people and blogs averaged 209,114 views, short entertainment videos averaged
288,675 views, and medium-to-long news and politics videos averaged 51,309 and
59,226 views, respectively. These results demonstrate that engagement patterns
of COVID-19-related videos on YouTube during the late pandemic followed
distinct characteristics driven by publishing schedules, title vocabulary,
topics, and genre-specific duration effects.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [656] [CERA: A Framework for Improved Generalization of Machine Learning Models to Changed Climates](https://arxiv.org/abs/2509.00010)
*Shuchang Liu,Paul A. O'Gorman*

Main category: physics.ao-ph

TL;DR: 提出CERA框架应对气候变化下机器学习泛化挑战，在参数化湿物理过程中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在气候变化下泛化能力差，依赖训练数据且手动特征工程困难。

Method: 提出CERA框架，包含显式潜在空间对齐的自编码器和下游过程估计的预测器。

Result: CERA在未用+4K气候标签数据训练时，能提高对温暖气候的泛化能力，优于基线模型，捕捉关键趋势和降水变化。

Conclusion: CERA框架是手动特征工程的替代方案，对其他气候应用有前景。

Abstract: Robust generalization under climate change remains a major challenge for
machine learning applications in climate science. Most existing approaches
struggle to extrapolate beyond the climate they were trained on, leading to a
strong dependence on training data from model simulations of warm climates. Use
of climate-invariant inputs improves generalization but requires challenging
manual feature engineering. Here, we present CERA (Climate-invariant Encoding
through Representation Alignment), a machine learning framework consisting of
an autoencoder with explicit latent-space alignment, followed by a predictor
for downstream process estimation. We test CERA on the problem of
parameterizing moist-physics processes. Without training on labeled data from a
+4K climate, CERA leverages labeled control-climate data and unlabeled
warmer-climate inputs to improve generalization to the warmer climate,
outperforming both raw-input and physically informed baselines in predicting
key moisture and energy tendencies. It captures not only the vertical and
meridional structures of the moisture tendencies, but also shifts in the
intensity distribution of precipitation including extremes. Ablation
experiments show that latent alignment improves both accuracy and the
robustness across random seeds used in training. While some reduced skill
remains in the boundary layer, the framework offers a data-driven alternative
to manual feature engineering of climate invariant inputs. Beyond
parameterizations used in hybrid ML-physics systems, the approach holds promise
for other climate applications such as statistical downscaling.

</details>


### [657] [MedFormer: a data-driven model for forecasting the Mediterranean Sea](https://arxiv.org/abs/2509.00015)
*Italo Epicoco,Davide Donno,Gabriele Accarino,Simone Norberti,Alessandro Grandi,Michele Giurato,Ronan McAdam,Donatello Elia,Emanuela Clementi,Paola Nassisi,Enrico Scoccimarro,Giovanni Coppini,Silvio Gualdi,Giovanni Aloisio,Simona Masina,Giulio Boccaletti,Antonio Navarra*

Main category: physics.ao-ph

TL;DR: 提出用于地中海中期海洋预报的全数据驱动深度学习模型MedFormer，经测试性能优于传统模型，显示数据驱动方法潜力。


<details>
  <summary>Details</summary>
Motivation: 准确海洋预报对海洋应用重要，数据驱动模型在大气预报有优势，但应用于海洋系统有挑战，需开发适用于海洋预报的模型。

Method: 基于U - Net架构并结合3D注意力机制构建MedFormer，用20年海洋再分析数据训练，高分辨率业务分析数据微调，采用自回归策略生成9天预报，结合历史海洋状态和大气强迫。

Result: 与地中海预报系统MedFS对比，用均方根差和异常相关系数评估，MedFormer在关键3D海洋变量上始终表现更优。

Conclusion: 数据驱动方法如MedFormer在准确性和计算效率上可补充甚至超越传统数值海洋预报系统。

Abstract: Accurate ocean forecasting is essential for supporting a wide range of marine
applications. Recent advances in artificial intelligence have highlighted the
potential of data-driven models to outperform traditional numerical approaches,
particularly in atmospheric weather forecasting. However, extending these
methods to ocean systems remains challenging due to their inherently slower
dynamics and complex boundary conditions. In this work, we present MedFormer, a
fully data-driven deep learning model specifically designed for medium-range
ocean forecasting in the Mediterranean Sea. MedFormer is based on a U-Net
architecture augmented with 3D attention mechanisms and operates at a high
horizontal resolution of 1/24{\deg}. The model is trained on 20 years of daily
ocean reanalysis data and fine-tuned with high-resolution operational analyses.
It generates 9-day forecasts using an autoregressive strategy. The model
leverages both historical ocean states and atmospheric forcings, making it
well-suited for operational use. We benchmark MedFormer against the
state-of-the-art Mediterranean Forecasting System (MedFS), developed at
Euro-Mediterranean Center on Climate Change (CMCC), using both analysis data
and independent observations. The forecast skills, evaluated with the Root Mean
Squared Difference and the Anomaly Correlation Coefficient, indicate that
MedFormer consistently outperforms MedFS across key 3D ocean variables. These
findings underscore the potential of data-driven approaches like MedFormer to
complement, or even surpass, traditional numerical ocean forecasting systems in
both accuracy and computational efficiency.

</details>


### [658] [Deep Learning for Operational High-Resolution Nowcasting in Switzerland Using Graph Neural Networks](https://arxiv.org/abs/2509.00017)
*Ophélia Miralles,Daniele Nerini,Jonas Bhend,Baudouin Raoult,Christoph Spirig*

Main category: physics.ao-ph

TL;DR: 本文提出基于图神经网络（GNN）的方法用于瑞士高分辨率临近预报，该方法优于传统方法，具业务相关性。


<details>
  <summary>Details</summary>
Motivation: 现有网格化方法应用于瑞士等地形复杂小区域进行高分辨率预报存在计算挑战，需新方法。

Method: 使用Anemoi框架和观测输入，结合地表观测与选定的过去和未来数值天气预报状态，采用观测引导插值策略。

Result: GNN模型在12小时内的预报中持续优于传统方法，尤其在风速和降水预报上。

Conclusion: 该方法经综合验证，对山区具有业务应用相关性。

Abstract: Recent advances in neural weather forecasting have shown significant
potential for accurate short-term forecasts. However, adapting such gridded
approaches to smaller, topographically complex regions like Switzerland
introduces computational challenges, especially when aiming for high spatial (1
km) and temporal (10 minutes) resolution. This paper presents a Graph Neural
Network (GNN)-based approach for high-resolution nowcasting in Switzerland
using the Anemoi framework and observational inputs. The proposed model
combines surface observations with selected past and future numerical weather
prediction (NWP) states, enabling an observation-guided interpolation strategy
that enhances short-term accuracy while preserving physical consistency. We
evaluate the method on multiple surface variables and compare it against
operational high-resolution NWP (ICON) and nowcasting (INCA) baselines. The
results show that the GNN model consistently outperforms traditional approaches
in lead times up to 12 hours, especially for wind and precipitation. A
comprehensive verification procedure, including spatial skill scores,
event-based evaluation, and blind tests with professional forecasters,
demonstrates the operational relevance of the approach for mountainous domains.

</details>


### [659] [An Observations-focused Assessment of Global AI Weather Prediction Models During the South Asian Monsoon](https://arxiv.org/abs/2509.01879)
*Aman Gupta,Aditi Sheshadri,Dhruv Suri*

Main category: physics.ao-ph

TL;DR: 评估七种AI天气模型在南亚季风期表现，发现它们在关键指标有不足，ECMWF的AIFS模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 评估七种先进AI天气模型在南亚季风期的性能，判断其用于业务应用的可行性。

Method: 将七种模型的预测结果与观测数据对比，测试温度、风、动能谱等多项指标。

Result: 模型能合理预测大规模动态，但在关键指标上表现不佳，与地面气象站数据对比误差大，不同模型存在差异。

Conclusion: ECMWF的AIFS模型性能和可用性最可靠，GraphCast和GenCast次之。

Abstract: Seven state-of-the-art AI weather models (FourCastNet, FourCastNet-SFNO,
Pangu-Weather, GraphCast, Aurora, AIFS, and GenCast) are evaluated against
observational data during the South Asian Monsoon. The models are tested on
temperature, winds, global kinetic energy spectrum, regional precipitation,
cloud cover, cyclone trajectory prediction, and hyperlocal predictions around
extreme weather events. The models forecast large-scale dynamics with
reasonable accuracy, but fall short on key metrics critical to Monsoon-time
weather prediction. The models exhibit substantially higher errors when
compared against ground-based weather station data than against reanalysis or
conventional forecasts. The AI weather prediction models show key differences
in mesoscale kinetic energy and extreme precipitation during the Monsoon, and
predict markedly different Monsoon-time cyclone trajectories over the Indian
subcontinent, raising questions about their readiness for operational
applications. Our analysis finds that ECMWF's deterministic AIFS model offers
the most reliable performance and usability, with GraphCast and GenCast being
close seconds.

</details>


<div id='math.RA'></div>

# math.RA [[Back]](#toc)

### [660] [A Hybrid Framework for Healing Semigroups with Machine Learning](https://arxiv.org/abs/2509.01763)
*Sarayu Sirikonda,Jasper van de Kreeke*

Main category: math.RA

TL;DR: 提出混合框架修复损坏有限半群，结合确定性策略与随机森林分类器，实验表明该框架修复率高于仅用确定性或机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 有限半群表格损坏破坏结合律和代数结构，确定性方法在高基数和高损坏率时效果差。

Method: 提出结合确定性修复策略与随机森林分类器的混合框架。

Result: 在Mace4生成的数据集上实验，在损坏率p = 15%时，该框架对基数n = 6的半群修复率达95%，n = 10时达60%。

Conclusion: 混合框架比仅用确定性或仅用机器学习的基线方法有更高的修复率。

Abstract: In this paper, we propose a hybrid framework that heals corrupted finite
semigroups, combining deterministic repair strategies with Machine Learning
using a Random Forest Classifier. Corruption in these tables breaks
associativity and invalidates the algebraic structure. Deterministic methods
work for small cardinality n and low corruption but degrade rapidly. Our
experiments, carried out on Mace4-generated data sets, demonstrate that our
hybrid framework achieves higher healing rates than deterministic-only and
ML-only baselines. At a corruption percentage of p=15%, our framework healed
95% of semigroups up to cardinality n=6 and 60% at n=10.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [661] [Nash Q-Network for Multi-Agent Cybersecurity Simulation](https://arxiv.org/abs/2509.00678)
*Qintong Xie,Edward Koh,Xavier Cadet,Peter Chin*

Main category: cs.MA

TL;DR: 论文提出基于策略的Nash Q学习方法解决多智能体强化学习（MARL）在复杂环境中同时训练智能体的挑战，并在网络防御模拟中实现算法。


<details>
  <summary>Details</summary>
Motivation: 解决MARL中在非平凡环境下同时训练智能体的复杂性挑战。

Method: 提出基于策略的Nash Q学习，结合近端策略优化（PPO）、深度Q网络（DQN）和Nash - Q算法，采用分布式数据收集和精心设计的神经网络架构进行训练。

Result: 在复杂网络防御模拟中成功实现该算法。

Conclusion: 所提Nash Q网络可学习Nash最优策略，在网络安全场景中实现强大防御，解决多智能体学习中的非平稳性和不稳定性问题。

Abstract: Cybersecurity defense involves interactions between adversarial parties
(namely defenders and hackers), making multi-agent reinforcement learning
(MARL) an ideal approach for modeling and learning strategies for these
scenarios. This paper addresses one of the key challenges to MARL, the
complexity of simultaneous training of agents in nontrivial environments, and
presents a novel policy-based Nash Q-learning to directly converge onto a
steady equilibrium. We demonstrate the successful implementation of this
algorithm in a notable complex cyber defense simulation treated as a two-player
zero-sum Markov game setting. We propose the Nash Q-Network, which aims to
learn Nash-optimal strategies that translate to robust defenses in
cybersecurity settings. Our approach incorporates aspects of proximal policy
optimization (PPO), deep Q-network (DQN), and the Nash-Q algorithm, addressing
common challenges like non-stationarity and instability in multi-agent
learning. The training process employs distributed data collection and
carefully designed neural architectures for both agents and critics.

</details>


### [662] [ShortageSim: Simulating Drug Shortages under Information Asymmetry](https://arxiv.org/abs/2509.01813)
*Mingxuan Cui,Yilan Jiang,Duo Zhou,Cheng Qian,Yuji Zhang,Qiong Wang*

Main category: cs.MA

TL;DR: 提出基于大语言模型的多智能体模拟框架ShortageSim应对药品短缺问题，实验表明其有良好效果并开源。


<details>
  <summary>Details</summary>
Motivation: 药品短缺对患者护理和医疗系统构成风险，且监管干预效果因信息不对称而不明，需要有效方法应对。

Method: 构建基于大语言模型的多智能体模拟框架ShortageSim，通过多季度顺序生产游戏建模，模拟有限理性决策。

Result: 实验显示ShortageSim使停产披露案例的解决滞后率降低83%，模拟持续时间更接近真实情况。

Conclusion: 开源ShortageSim和相关数据集，为信息稀缺的复杂供应链设计和测试干预措施提供新计算框架。

Abstract: Drug shortages pose critical risks to patient care and healthcare systems
worldwide, yet the effectiveness of regulatory interventions remains poorly
understood due to fundamental information asymmetries in pharmaceutical supply
chains. We present \textbf{ShortageSim}, the first Large Language Model
(LLM)-based multi-agent simulation framework that captures the complex,
strategic interactions between drug manufacturers, institutional buyers, and
regulatory agencies in response to shortage alerts. Unlike traditional
game-theoretic models that assume perfect rationality and complete information,
\textbf{ShortageSim} leverages LLMs to simulate bounded-rational
decision-making under uncertainty. Through a sequential production game
spanning multiple quarters, we model how FDA announcements, both reactive
alerts about existing shortages and proactive warnings about potential
disruptions, propagate through the supply chain and influence capacity
investment and procurement decisions. Our experiments on historical shortage
events reveal that \textbf{ShortageSim} reduces the resolution-lag percentage
for discontinued-disclosed cases by 83\%, bringing simulated durations more
aligned to ground truth than the zero-shot baseline. We open-source
\textbf{ShortageSim} and a dataset of 2,925 FDA shortage events at
https://github.com/Lemutisme/Sortage_Management, providing a novel
computational framework for designing and testing interventions in complex,
information-scarce supply chains.

</details>


### [663] [MobiAgent: A Systematic Framework for Customizable Mobile Agents](https://arxiv.org/abs/2509.00531)
*Cheng Zhang,Erhu Feng,Xi Zhao,Yisheng Zhao,Wangbo Gong,Jiahui Sun,Dong Du,Zhichao Hua,Yubin Xia,Haibo Chen*

Main category: cs.MA

TL;DR: 提出MobiAgent移动代理系统及AI辅助数据收集管道，在真实移动场景中达先进水平


<details>
  <summary>Details</summary>
Motivation: 现有代理模型在真实任务执行中存在准确性和效率问题，且当前移动代理能力受高质量数据限制

Method: 构建包含MobiMind系列代理模型、AgentRR加速框架和MobiFlow基准套件的MobiAgent系统，开发AI辅助敏捷数据收集管道

Result: MobiAgent在真实移动场景中相比通用大语言模型和专业GUI代理模型取得了先进性能

Conclusion: MobiAgent能有效解决现有移动代理模型的局限，提升真实场景下的性能

Abstract: With the rapid advancement of Vision-Language Models (VLMs), GUI-based mobile
agents have emerged as a key development direction for intelligent mobile
systems. However, existing agent models continue to face significant challenges
in real-world task execution, particularly in terms of accuracy and efficiency.
To address these limitations, we propose MobiAgent, a comprehensive mobile
agent system comprising three core components: the MobiMind-series agent
models, the AgentRR acceleration framework, and the MobiFlow benchmarking
suite. Furthermore, recognizing that the capabilities of current mobile agents
are still limited by the availability of high-quality data, we have developed
an AI-assisted agile data collection pipeline that significantly reduces the
cost of manual annotation. Compared to both general-purpose LLMs and
specialized GUI agent models, MobiAgent achieves state-of-the-art performance
in real-world mobile scenarios.

</details>


### [664] [Contemporary Agent Technology: LLM-Driven Advancements vs Classic Multi-Agent Systems](https://arxiv.org/abs/2509.02515)
*Costin Bădică,Amelia Bădică,Maria Ganzha,Mirjana Ivanović,Marcin Paprzycki,Dan Selişteanu,Zofia Wrona*

Main category: cs.MA

TL;DR: 文章对当代智能体技术进行全面反思，对比大语言模型驱动的进展与经典多智能体系统，分析新系统特点、与基础MAS关系，指出挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 对当代智能体技术进行全面反思，尤其是大语言模型驱动的进展与经典多智能体系统的对比。

Method: 对新系统的模型、方法和特征进行研究，将近期发展与核心学术文献中的基础MAS进行批判性分析。

Result: 明确了新系统的相关特点以及与基础MAS的关系。

Conclusion: 识别出该快速发展领域的关键挑战和有前景的未来方向。

Abstract: This contribution provides our comprehensive reflection on the contemporary
agent technology, with a particular focus on the advancements driven by Large
Language Models (LLM) vs classic Multi-Agent Systems (MAS). It delves into the
models, approaches, and characteristics that define these new systems. The
paper emphasizes the critical analysis of how the recent developments relate to
the foundational MAS, as articulated in the core academic literature. Finally,
it identifies key challenges and promising future directions in this rapidly
evolving domain.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [665] [Q-Learning--Driven Adaptive Rewiring for Cooperative Control in Heterogeneous Networks](https://arxiv.org/abs/2509.01057)
*Yi-Ning Weng,Hsuan-Wei Lee*

Main category: physics.soc-ph

TL;DR: 提出基于Q学习的自适应重连方法研究多智能体系统合作涌现，分析不同重连约束下合作模式，揭示机器学习可驱动多智能体网络自组织。


<details>
  <summary>Details</summary>
Motivation: 研究微观学习规则如何驱动多智能体系统宏观集体行为转变这一统计物理问题。

Method: 提出基于Q学习的自适应重连方法，结合时差学习与网络重构，利用幂律网络评估不同重连约束下的涌现行为。

Result: 发现三种行为模式，适度约束抑制合作，完全自适应重连增强合作，增加重连频率可形成幂律规模分布的大规模集群。

Conclusion: 建立了理解复杂自适应系统中智能驱动合作模式形成的新范式，表明机器学习可作为多智能体网络自组织的替代驱动力。

Abstract: Cooperation emergence in multi-agent systems represents a fundamental
statistical physics problem where microscopic learning rules drive macroscopic
collective behavior transitions. We propose a Q-learning-based variant of
adaptive rewiring that builds on mechanisms studied in the literature. This
method combines temporal difference learning with network restructuring so that
agents can optimize strategies and social connections based on interaction
histories. Through neighbor-specific Q-learning, agents develop sophisticated
partnership management strategies that enable cooperator cluster formation,
creating spatial separation between cooperative and defective regions. Using
power-law networks that reflect real-world heterogeneous connectivity patterns,
we evaluate emergent behaviors under varying rewiring constraint levels,
revealing distinct cooperation patterns across parameter space rather than
sharp thermodynamic transitions. Our systematic analysis identifies three
behavioral regimes: a permissive regime (low constraints) enabling rapid
cooperative cluster formation, an intermediate regime with sensitive dependence
on dilemma strength, and a patient regime (high constraints) where strategic
accumulation gradually optimizes network structure. Simulation results show
that while moderate constraints create transition-like zones that suppress
cooperation, fully adaptive rewiring enhances cooperation levels through
systematic exploration of favorable network configurations. Quantitative
analysis reveals that increased rewiring frequency drives large-scale cluster
formation with power-law size distributions. Our results establish a new
paradigm for understanding intelligence-driven cooperation pattern formation in
complex adaptive systems, revealing how machine learning serves as an
alternative driving force for spontaneous organization in multi-agent networks.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [666] [DeepEmoNet: Building Machine Learning Models for Automatic Emotion Recognition in Human Speeches](https://arxiv.org/abs/2509.00025)
*Tai Vu*

Main category: eess.AS

TL;DR: 本文用机器学习解决语音情感识别问题，构建多种模型，利用迁移学习和数据增强，最佳模型是ResNet34，有不错表现。


<details>
  <summary>Details</summary>
Motivation: 解决语音情感识别难题，不清楚人类情感与声音各成分的联系。

Method: 构建SVM、LTSM、CNN等机器学习模型，利用迁移学习和数据增强训练模型。

Result: 最佳模型ResNet34准确率达66.7%，F1分数为0.631。

Conclusion: 利用机器学习、迁移学习和数据增强能在较小数据集上让模型达到不错性能。

Abstract: Speech emotion recognition (SER) has been a challenging problem in spoken
language processing research, because it is unclear how human emotions are
connected to various components of sounds such as pitch, loudness, and energy.
This paper aims to tackle this problem using machine learning. Particularly, we
built several machine learning models using SVMs, LTSMs, and CNNs to classify
emotions in human speeches. In addition, by leveraging transfer learning and
data augmentation, we efficiently trained our models to attain decent
performances on a relatively small dataset. Our best model was a ResNet34
network, which achieved an accuracy of $66.7\%$ and an F1 score of $0.631$.

</details>


### [667] [Amplifying Emotional Signals: Data-Efficient Deep Learning for Robust Speech Emotion Recognition](https://arxiv.org/abs/2509.00077)
*Tai Vu*

Main category: eess.AS

TL;DR: 本文针对语音情感识别在小数据集上性能不佳的问题，开发评估机器学习模型，采用迁移学习和数据增强技术，ResNet34模型在数据集上取得新基准。


<details>
  <summary>Details</summary>
Motivation: 解决语音情感识别在有限数据集上难以取得高性能的问题。

Method: 开发评估支持向量机、长短时记忆网络、卷积神经网络等模型，采用迁移学习和创新的数据增强技术。

Result: ResNet34模型在RAVDESS和SAVEE组合数据集上达到66.7%的准确率和0.631的F1分数。

Conclusion: 利用预训练模型和数据增强可克服数据稀缺问题，为更强大、通用的语音情感识别系统铺平道路。

Abstract: Speech Emotion Recognition (SER) presents a significant yet persistent
challenge in human-computer interaction. While deep learning has advanced
spoken language processing, achieving high performance on limited datasets
remains a critical hurdle. This paper confronts this issue by developing and
evaluating a suite of machine learning models, including Support Vector
Machines (SVMs), Long Short-Term Memory networks (LSTMs), and Convolutional
Neural Networks (CNNs), for automated emotion classification in human speech.
We demonstrate that by strategically employing transfer learning and innovative
data augmentation techniques, our models can achieve impressive performance
despite the constraints of a relatively small dataset. Our most effective
model, a ResNet34 architecture, establishes a new performance benchmark on the
combined RAVDESS and SAVEE datasets, attaining an accuracy of 66.7% and an F1
score of 0.631. These results underscore the substantial benefits of leveraging
pre-trained models and data augmentation to overcome data scarcity, thereby
paving the way for more robust and generalizable SER systems.

</details>


### [668] [Automatic Pronunciation Error Detection and Correction of the Holy Quran's Learners Using Deep Learning](https://arxiv.org/abs/2509.00094)
*Abdullah Abdelfattah,Mahmoud I. Khalil,Hazem Abbas*

Main category: eess.AS

TL;DR: 本文引入自动化流程生成高质量古兰经数据集，提出基于ASR的发音错误检测方法，模型测试集PER低，且开源代码、数据和模型。


<details>
  <summary>Details</summary>
Motivation: 评估口语有挑战，量化机器学习模型发音指标更难，虽古兰经有严格朗诵规则利于评估，但缺乏高质量标注数据。

Method: 引入98%自动化流程生成数据集；提出基于ASR的发音错误检测方法，使用自定义古兰经音标脚本QPS；采用新型多级CTC模型。

Result: 生成850+小时音频（约300K标注话语）；新型多级CTC模型在测试集上平均音素错误率为0.16%。

Conclusion: 通过所提方法和模型解决了古兰经发音评估中数据稀缺问题，且开源成果利于后续研究。

Abstract: Assessing spoken language is challenging, and quantifying pronunciation
metrics for machine learning models is even harder. However, for the Holy
Quran, this task is simplified by the rigorous recitation rules (tajweed)
established by Muslim scholars, enabling highly effective assessment. Despite
this advantage, the scarcity of high-quality annotated data remains a
significant barrier.
  In this work, we bridge these gaps by introducing: (1) A 98% automated
pipeline to produce high-quality Quranic datasets -- encompassing: Collection
of recitations from expert reciters, Segmentation at pause points (waqf) using
our fine-tuned wav2vec2-BERT model, Transcription of segments, Transcript
verification via our novel Tasmeea algorithm; (2) 850+ hours of audio (~300K
annotated utterances); (3) A novel ASR-based approach for pronunciation error
detection, utilizing our custom Quran Phonetic Script (QPS) to encode Tajweed
rules (unlike the IPA standard for Modern Standard Arabic). QPS uses a
two-level script: (Phoneme level): Encodes Arabic letters with short/long
vowels. (Sifa level): Encodes articulation characteristics of every phoneme. We
further include comprehensive modeling with our novel multi-level CTC Model
which achieved 0.16% average Phoneme Error Rate (PER) on the testset. We
release all code, data, and models as open-source:
https://obadx.github.io/prepare-quran-dataset/

</details>


### [669] [ChipChat: Low-Latency Cascaded Conversational Agent in MLX](https://arxiv.org/abs/2509.00078)
*Tatiana Likhomanenko,Luke Carlson,Richard He Bai,Zijin Gu,Han Tran,Zakaria Aldeneh,Yizhe Zhang,Ruixiang Zhang,Huangjie Zheng,Navdeep Jaitly*

Main category: eess.AS

TL;DR: 介绍ChipChat低延迟级联系统，在无专用GPU的Mac Studio上实现亚秒级响应延迟，证明改进级联系统可克服延迟局限。


<details>
  <summary>Details</summary>
Motivation: 大语言模型改变语音对话系统，但实时设备端语音代理的最佳架构未知，级联系统虽在语言理解任务表现好但有顺序处理延迟问题。

Method: 引入ChipChat，通过架构创新和流式优化克服传统瓶颈，集成多种技术，用MLX实现。

Result: ChipChat在无专用GPU的Mac Studio上实现亚秒级响应延迟，且通过全设备端处理保护用户隐私。

Conclusion: 经策略性重新设计的级联系统可克服历史延迟局限，为实用语音AI代理提供有前景的方向。

Abstract: The emergence of large language models (LLMs) has transformed spoken dialog
systems, yet the optimal architecture for real-time on-device voice agents
remains an open question. While end-to-end approaches promise theoretical
advantages, cascaded systems (CSs) continue to outperform them in language
understanding tasks, despite being constrained by sequential processing
latency. In this work, we introduce ChipChat, a novel low-latency CS that
overcomes traditional bottlenecks through architectural innovations and
streaming optimizations. Our system integrates streaming (a) conversational
speech recognition with mixture-of-experts, (b) state-action augmented LLM, (c)
text-to-speech synthesis, (d) neural vocoder, and (e) speaker modeling.
Implemented using MLX, ChipChat achieves sub-second response latency on a Mac
Studio without dedicated GPUs, while preserving user privacy through complete
on-device processing. Our work shows that strategically redesigned CSs can
overcome their historical latency limitations, offering a promising path
forward for practical voice-based AI agents.

</details>


### [670] [AHAMask: Reliable Task Specification for Large Audio Language Models without Instructions](https://arxiv.org/abs/2509.01787)
*Yiwei Guo,Bohan Li,Hankun Wang,Zhihan Li,Shuai Wang,Xie Chen,Kai Yu*

Main category: eess.AS

TL;DR: 提出AHAMask方法，通过掩码注意力头触发LALMs特定声学任务功能，性能佳且揭示其“功能路径”。


<details>
  <summary>Details</summary>
Motivation: 解决当前大型音频语言模型（LALMs）存在的指令敏感性问题。

Method: 提出AHAMask，在LALMs的仅解码器大语言模型骨干中掩码部分注意力头，通过在LALM上训练高效获取掩码。

Result: 实验表明，应用选择性注意力头掩码在单任务或复合任务上比使用指令有相当甚至更好的性能。

Conclusion: 该方法能为LALMs实现可靠的声学任务指定，且揭示LALMs注意力头存在“功能路径”。

Abstract: Although current large audio language models (LALMs) extend text large
language models (LLMs) with generic acoustic understanding abilities, they
usually suffer from instruction sensitivity, where different instructions of
the same intention can yield drastically different outcomes. In this work, we
propose AHAMask, where we simply mask some of the attention heads in the
decoder-only LLM backbone of LALMs, to trigger specific acoustic task
functionalities without instructions. These masks are efficiently obtained by
training on an LALM, with the number of trainable parameters equal to the
attention head count in its LLM backbone. We show by experiments that applying
such selective attention head masks achieves comparable or even better
performance than using instructions, either on single or composite tasks.
Besides achieving reliable acoustic task specification for LALMs, this also
reveals that LALMs exhibit certain "functional pathways" in their attention
heads.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [671] [From Sound to Sight: Towards AI-authored Music Videos](https://arxiv.org/abs/2509.00029)
*Leo Vitasovic,Stella Graßhof,Agnes Mercedes Kloft,Ville V. Lehtola,Martin Cunneen,Justyna Starostka,Glenn McGarry,Kun Li,Sami S. Brandt*

Main category: cs.SD

TL;DR: 提出两种用现成深度学习模型自动生成音乐视频的新流程，评估显示有潜力拓展音乐可视化。


<details>
  <summary>Details</summary>
Motivation: 传统音乐可视化系统表现力有限，需新方法。

Method: 受音乐视频制作流程启发，用基于潜在特征的技术分析音频，用语言模型将其转化为文本场景描述，再用生成模型制作视频，并进行用户评估。

Result: 初步用户评估显示生成视频有讲故事潜力、视觉连贯性和情感与音乐的一致性。

Conclusion: 潜在特征技术和深度生成模型有潜力超越传统方法拓展音乐可视化。

Abstract: Conventional music visualisation systems rely on handcrafted ad hoc
transformations of shapes and colours that offer only limited expressiveness.
We propose two novel pipelines for automatically generating music videos from
any user-specified, vocal or instrumental song using off-the-shelf deep
learning models. Inspired by the manual workflows of music video producers, we
experiment on how well latent feature-based techniques can analyse audio to
detect musical qualities, such as emotional cues and instrumental patterns, and
distil them into textual scene descriptions using a language model. Next, we
employ a generative model to produce the corresponding video clips. To assess
the generated videos, we identify several critical aspects and design and
conduct a preliminary user evaluation that demonstrates storytelling potential,
visual coherency and emotional alignment with the music. Our findings
underscore the potential of latent feature techniques and deep generative
models to expand music visualisation beyond traditional approaches.

</details>


### [672] [CoComposer: LLM Multi-agent Collaborative Music Composition](https://arxiv.org/abs/2509.00132)
*Peiwen Xing,Aske Plaat,Niki van Stein*

Main category: cs.SD

TL;DR: 介绍多智能体系统CoComposer，实验评估其在音乐创作方面表现，与不同系统对比有优劣。


<details>
  <summary>Details</summary>
Motivation: 现有AI音乐创作工具在生成时长、音乐质量和可控性上存在局限。

Method: 引入由五个协作智能体组成的CoComposer系统，基于传统音乐创作流程分配任务，使用AudioBox - Aesthetics系统进行实验评估，测试三种大语言模型。

Result: CoComposer在音乐质量上优于现有多智能体大语言模型系统，在创作复杂度上优于单智能体系统；与非大语言模型MusicLM相比，CoComposer有更好的可解释性和可编辑性，但MusicLM音乐质量更好。

Conclusion: CoComposer在音乐创作方面有一定优势，但也存在改进空间。

Abstract: Existing AI Music composition tools are limited in generation duration,
musical quality, and controllability. We introduce CoComposer, a multi-agent
system that consists of five collaborating agents, each with a task based on
the traditional music composition workflow. Using the AudioBox-Aesthetics
system, we experimentally evaluate CoComposer on four compositional criteria.
We test with three LLMs (GPT-4o, DeepSeek-V3-0324, Gemini-2.5-Flash), and find
(1) that CoComposer outperforms existing multi-agent LLM-based systems in music
quality, and (2) compared to a single-agent system, in production complexity.
Compared to non- LLM MusicLM, CoComposer has better interpretability and
editability, although MusicLM still produces better music.

</details>


### [673] [Generalizable Audio Spoofing Detection using Non-Semantic Representations](https://arxiv.org/abs/2509.00186)
*Arnab Das,Yassine El Kheir,Carlos Franzreb,Tim Herzig,Tim Polzehl,Sebastian Möller*

Main category: cs.SD

TL;DR: 研究提出利用非语义通用音频表示进行可泛化的语音欺骗检测新方法，实验显示该方法在域外测试集表现优异。


<details>
  <summary>Details</summary>
Motivation: 生成式建模发展使语音服务易受欺骗攻击，现有深度伪造检测方案缺乏泛化性。

Method: 提出利用非语义通用音频表示进行可泛化欺骗检测的新方法，用TRILL和TRILLsson模型寻找合适非语义特征。

Result: 该方法在域内测试集表现相当，在域外测试集显著优于现有方法，在公共领域数据上泛化能力强。

Conclusion: 所提方法能有效进行可泛化的语音欺骗检测，泛化能力优于其他方法。

Abstract: Rapid advancements in generative modeling have made synthetic audio
generation easy, making speech-based services vulnerable to spoofing attacks.
Consequently, there is a dire need for robust countermeasures more than ever.
Existing solutions for deepfake detection are often criticized for lacking
generalizability and fail drastically when applied to real-world data. This
study proposes a novel method for generalizable spoofing detection leveraging
non-semantic universal audio representations. Extensive experiments have been
performed to find suitable non-semantic features using TRILL and TRILLsson
models. The results indicate that the proposed method achieves comparable
performance on the in-domain test set while significantly outperforming
state-of-the-art approaches on out-of-domain test sets. Notably, it
demonstrates superior generalization on public-domain data, surpassing methods
based on hand-crafted features, semantic embeddings, and end-to-end
architectures.

</details>


### [674] [Evaluating the Effectiveness of Transformer Layers in Wav2Vec 2.0, XLS-R, and Whisper for Speaker Identification Tasks](https://arxiv.org/abs/2509.00230)
*Linus Stuhlmann,Michael Alexander Saxer*

Main category: cs.SD

TL;DR: 评估Wav2Vec 2.0、XLS - R和Whisper三种语音编码器模型在说话人识别任务中的表现，确定微调时各模型的最佳变压器层数。


<details>
  <summary>Details</summary>
Motivation: 评估三种先进语音编码器模型在说话人识别任务中的性能。

Method: 微调模型，使用SVCCA、k - means聚类和t - SNE可视化分析其逐层表示。

Result: Wav2Vec 2.0和XLS - R在早期层有效捕捉说话人特定特征，微调提高稳定性和性能；Whisper在更深层表现更好。

Conclusion: 确定了各模型在说话人识别任务微调时的最佳变压器层数。

Abstract: This study evaluates the performance of three advanced speech encoder models,
Wav2Vec 2.0, XLS-R, and Whisper, in speaker identification tasks. By
fine-tuning these models and analyzing their layer-wise representations using
SVCCA, k-means clustering, and t-SNE visualizations, we found that Wav2Vec 2.0
and XLS-R capture speaker-specific features effectively in their early layers,
with fine-tuning improving stability and performance. Whisper showed better
performance in deeper layers. Additionally, we determined the optimal number of
transformer layers for each model when fine-tuned for speaker identification
tasks.

</details>


### [675] [The Name-Free Gap: Policy-Aware Stylistic Control in Music Generation](https://arxiv.org/abs/2509.00654)
*Ashwin Nagarajan,Hao-Wen Dong*

Main category: cs.SD

TL;DR: 研究用大语言模型采样的轻量级修饰符进行音乐风格控制，发现无名称描述符可部分替代艺术家名称提示，定义了无名称差距。


<details>
  <summary>Details</summary>
Motivation: 现有音乐风格化方法存在再训练或特殊条件限制，难以复现和符合政策，需政策稳健的风格控制替代方案。

Method: 使用MusicGen - small，以两位艺术家为例，用大语言模型生成提示，用VGGish和CLAP嵌入及多种度量方法评估。

Result: 艺术家名称提示控制效果最强，无名称描述符可恢复部分效果，跨艺术家转移降低对齐度。

Conclusion: 现有艺术家名称限制措施不能完全防止风格模仿，定义的无名称差距可通过可复现评估协议衡量提示级可控性。

Abstract: Text-to-music models capture broad attributes such as instrumentation or
mood, but fine-grained stylistic control remains an open challenge. Existing
stylization methods typically require retraining or specialized conditioning,
which complicates reproducibility and limits policy compliance when artist
names are restricted. We study whether lightweight, human-readable modifiers
sampled from a large language model can provide a policy-robust alternative for
stylistic control. Using MusicGen-small, we evaluate two artists: Billie Eilish
(vocal pop) and Ludovico Einaudi (instrumental piano). For each artist, we use
fifteen reference excerpts and evaluate matched seeds under three conditions:
baseline prompts, artist-name prompts, and five descriptor sets. All prompts
are generated using a large language model. Evaluation uses both VGGish and
CLAP embeddings with distributional and per-clip similarity measures, including
a new min-distance attribution metric. Results show that artist names are the
strongest control signal across both artists, while name-free descriptors
recover much of this effect. This highlights that existing safeguards such as
the restriction of artist names in music generation prompts may not fully
prevent style imitation. Cross-artist transfers reduce alignment, showing that
descriptors encode targeted stylistic cues. We also present a descriptor table
across ten contemporary artists to illustrate the breadth of the tokens.
Together these findings define the name-free gap, the controllability
difference between artist-name prompts and policy-compliant descriptors, shown
through a reproducible evaluation protocol for prompt-level controllability.

</details>


### [676] [AImoclips: A Benchmark for Evaluating Emotion Conveyance in Text-to-Music Generation](https://arxiv.org/abs/2509.00813)
*Gyehun Go,Satbyul Han,Ahyeon Choi,Eunjin Choi,Juhan Nam,Jeong Mi Park*

Main category: cs.SD

TL;DR: 本文提出AImoclips基准评估文本到音乐（TTM）系统情感传达能力，发现商业和开源系统表现差异及模型情感传达局限，为TTM系统发展提供见解。


<details>
  <summary>Details</summary>
Motivation: 当前TTM系统情感保真度与人类偏好或文本对齐方面研究不足，需评估其向人类听众传达预期情感的能力。

Method: 选取12种情感意图，用6个先进TTM系统生成超1000个音乐片段，让111名参与者用9点李克特量表对每个片段的效价和唤醒度评分。

Result: 商业系统生成音乐比预期更愉悦，开源系统相反；高唤醒条件下情感传达更准确；所有系统都有情感中立偏差。

Conclusion: 该基准为模型特定情感渲染特征提供有价值见解，支持情感对齐的TTM系统未来发展。

Abstract: Recent advances in text-to-music (TTM) generation have enabled controllable
and expressive music creation using natural language prompts. However, the
emotional fidelity of TTM systems remains largely underexplored compared to
human preference or text alignment. In this study, we introduce AImoclips, a
benchmark for evaluating how well TTM systems convey intended emotions to human
listeners, covering both open-source and commercial models. We selected 12
emotion intents spanning four quadrants of the valence-arousal space, and used
six state-of-the-art TTM systems to generate over 1,000 music clips. A total of
111 participants rated the perceived valence and arousal of each clip on a
9-point Likert scale. Our results show that commercial systems tend to produce
music perceived as more pleasant than intended, while open-source systems tend
to perform the opposite. Emotions are more accurately conveyed under
high-arousal conditions across all models. Additionally, all systems exhibit a
bias toward emotional neutrality, highlighting a key limitation in affective
controllability. This benchmark offers valuable insights into model-specific
emotion rendering characteristics and supports future development of
emotionally aligned TTM systems.

</details>


### [677] [Adaptive Vehicle Speed Classification via BMCNN with Reinforcement Learning-Enhanced Acoustic Processing](https://arxiv.org/abs/2509.00839)
*Yuli Zhang,Pengfei Fan,Ruiyuan Jiang,Hankang Gu,Dongyao Jia,Xinheng Wang*

Main category: cs.SD

TL;DR: 提出结合深度学习与强化学习的混合框架用于声学车辆速度分类，在数据集上表现良好，适合实时ITS部署。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵需智能交通系统实时管理，当前需要有效声学车辆速度分类方法。

Method: 采用双分支BMCNN处理MFCC和小波特征，用注意力增强DQN自适应选择最少音频帧并提前决策。

Result: 在IDMT - Traffic和SZUR - Acoustic数据集上准确率分别达95.99%和92.3%，平均处理速度最多快1.63倍，相比其他方法有更好的准确率 - 效率权衡。

Conclusion: 该方法适合异构城市环境的实时智能交通系统部署。

Abstract: Traffic congestion remains a pressing urban challenge, requiring intelligent
transportation systems for real-time management. We present a hybrid framework
that combines deep learning and reinforcement learning for acoustic vehicle
speed classification. A dual-branch BMCNN processes MFCC and wavelet features
to capture complementary frequency patterns. An attention-enhanced DQN
adaptively selects the minimal number of audio frames and triggers early
decisions once confidence thresholds are reached. Evaluations on IDMT-Traffic
and our SZUR-Acoustic (Suzhou) datasets show 95.99% and 92.3% accuracy, with up
to 1.63x faster average processing via early termination. Compared with A3C,
DDDQN, SA2C, PPO, and TD3, the method provides a superior accuracy-efficiency
trade-off and is suitable for real-time ITS deployment in heterogeneous urban
environments.

</details>


### [678] [Speech Command Recognition Using LogNNet Reservoir Computing for Embedded Systems](https://arxiv.org/abs/2509.00862)
*Yuriy Izotov,Andrei Velichko*

Main category: cs.SD

TL;DR: 本文提出低资源语音命令识别器，结合VAD、优化MFCC管道和LogNNet分类器，评估多种MFCC聚合方案，硬件实现验证可行性，适用于受限设备。


<details>
  <summary>Details</summary>
Motivation: 实现严格内存和计算限制下可靠的设备端语音命令识别，以适用于电池供电的物联网节点等。

Method: 结合能量基语音活动检测、优化的MFCC管道和LogNNet水库计算分类器，评估四种MFCC聚合方案。

Result: 自适应分箱特征向量在精度和紧凑性上最优；LogNNet分类器在独立说话人评估中达92.04%准确率；在Arduino Nano 33 IoT上实时识别准确率约90%，仅用18KB内存。

Conclusion: 完整管道能在严格限制下实现可靠设备端语音命令识别，适用于多种场景。

Abstract: This paper presents a low-resource speech-command recognizer combining
energy-based voice activity detection (VAD), an optimized Mel-Frequency
Cepstral Coefficients (MFCC) pipeline, and the LogNNet reservoir-computing
classifier. Using four commands from the Speech Commands da-taset downsampled
to 8 kHz, we evaluate four MFCC aggregation schemes and find that adaptive
binning (64-dimensional feature vector) offers the best accuracy-to-compactness
trade-off. The LogNNet classifier with architecture 64:33:9:4 reaches 92.04%
accuracy under speaker-independent evaluation, while requiring significantly
fewer parameters than conventional deep learn-ing models. Hardware
implementation on Arduino Nano 33 IoT (ARM Cor-tex-M0+, 48 MHz, 32 KB RAM)
validates the practical feasibility, achieving ~90% real-time recognition
accuracy while consuming only 18 KB RAM (55% utilization). The complete
pipeline (VAD -> MFCC -> LogNNet) thus enables reliable on-device
speech-command recognition under strict memory and compute limits, making it
suitable for battery-powered IoT nodes, wire-less sensor networks, and
hands-free control interfaces.

</details>


### [679] [TinyMusician: On-Device Music Generation with Knowledge Distillation and Mixed Precision Quantization](https://arxiv.org/abs/2509.00914)
*Hainan Wang,Mehdi Hosseinzadeh,Reza Rawassizadeh*

Main category: cs.SD

TL;DR: 提出轻量级音乐生成模型TinyMusician，集成两项创新，模型大小减少55%同时保留93% MusicGen - Small性能，可移动部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的音乐生成模型因参数多，对计算资源和推理时间要求高，难以部署在边缘设备，需开发轻量级模型。

Method: 从MusicGen蒸馏出TinyMusician，集成Stage - mixed Bidirectional and Skewed KL - Divergence和Adaptive Mixed - Precision Quantization两项创新。

Result: TinyMusician保留93%的MusicGen - Small性能，模型大小减少55%。

Conclusion: TinyMusician是首个可移动部署的音乐生成模型，消除云依赖，保持高音频保真度和高效资源利用。

Abstract: The success of the generative model has gained unprecedented attention in the
music generation area. Transformer-based architectures have set new benchmarks
for model performance. However, their practical adoption is hindered by some
critical challenges: the demand for massive computational resources and
inference time, due to their large number of parameters. These obstacles make
them infeasible to deploy on edge devices, such as smartphones and wearables,
with limited computational resources. In this work, we present TinyMusician, a
lightweight music generation model distilled from MusicGen (a State-of-the-art
music generation model). TinyMusician integrates two innovations: (i)
Stage-mixed Bidirectional and Skewed KL-Divergence and (ii) Adaptive
Mixed-Precision Quantization. The experimental results demonstrate that
TinyMusician retains 93% of the MusicGen-Small performance with 55% less model
size. TinyMusician is the first mobile-deployable music generation model that
eliminates cloud dependency while maintaining high audio fidelity and efficient
resource usage

</details>


### [680] [EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection](https://arxiv.org/abs/2509.01153)
*Yun Chu,Qiuhao Wang,Enze Zhou,Qian Liu,Gang Zheng*

Main category: cs.SD

TL;DR: 现有呼吸音检测方法存在局限，本文提出基于图神经网络带锚定区间的框架，在数据集上验证有效，结合位置信息可提升异常音区分度。


<details>
  <summary>Details</summary>
Motivation: 传统听诊主观且专家间有差异，现有深度学习呼吸音检测方法存在难学区间边界、难处理变长音频、未充分探索位置信息影响等问题。

Method: 提出基于图神经网络带锚定区间的框架，可处理变长音频并精准定位异常呼吸音事件。

Result: 在SPRSound 2024和HF Lung V1数据集上实验，证明方法有效，结合呼吸位置信息能提升异常音区分度。

Conclusion: 所提方法提高了呼吸音检测的灵活性和适用性。

Abstract: Auscultation is a key method for early diagnosis of respiratory and pulmonary
diseases, relying on skilled healthcare professionals. However, the process is
often subjective, with variability between experts. As a result, numerous deep
learning-based automatic classification methods have emerged, most of which
focus on respiratory sound classification. In contrast, research on respiratory
sound event detection remains limited. Existing sound event detection methods
typically rely on frame-level predictions followed by post-processing to
generate event-level outputs, making interval boundaries challenging to learn
directly. Furthermore, many approaches can only handle fixed-length audio, lim-
iting their applicability to variable-length respiratory sounds. Additionally,
the impact of respiratory sound location information on detection performance
has not been extensively explored. To address these issues, we propose a graph
neural network-based framework with anchor intervals, capable of handling
variable-length audio and providing more precise temporal localization for
abnormal respi- ratory sound events. Our method improves both the flexibility
and applicability of respiratory sound detection. Experiments on the SPRSound
2024 and HF Lung V1 datasets demonstrate the effec- tiveness of the proposed
approach, and incorporating respiratory position information enhances the
discrimination between abnormal sounds.

</details>


### [681] [CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech Separation with Distributed Heterogeneous Arrays](https://arxiv.org/abs/2509.01399)
*Runduo Han,Yanxin Hu,Yihui Fu,Zihan Zhang,Yukai Jv,Li Chen,Lei Xie*

Main category: cs.SD

TL;DR: 提出轻量级CabinSep方法减少后端ASR模型语音识别错误，有三方面贡献，复杂度低且识别错误率降低。


<details>
  <summary>Details</summary>
Motivation: 分离多说话人重叠语音对人车交互很重要，需减少后端ASR模型语音识别错误。

Method: 利用通道信息提取空间特征；推理时采用MVDR；引入结合模拟和真实录制脉冲响应的数据增强方法。

Result: CabinSep计算复杂度仅0.4 GMACs，在真实录制数据集上比DualSep模型语音识别错误率相对降低17.5%。

Conclusion: CabinSep方法有效，可降低语音识别错误率，提升人车交互语音处理效果。

Abstract: Separating overlapping speech from multiple speakers is crucial for effective
human-vehicle interaction. This paper proposes CabinSep, a lightweight neural
mask-based minimum variance distortionless response (MVDR) speech separation
approach, to reduce speech recognition errors in back-end automatic speech
recognition (ASR) models. Our contributions are threefold: First, we utilize
channel information to extract spatial features, which improves the estimation
of speech and noise masks. Second, we employ MVDR during inference, reducing
speech distortion to make it more ASR-friendly. Third, we introduce a data
augmentation method combining simulated and real-recorded impulse responses
(IRs), improving speaker localization at zone boundaries and further reducing
speech recognition errors. With a computational complexity of only 0.4 GMACs,
CabinSep achieves a 17.5% relative reduction in speech recognition error rate
in a real-recorded dataset compared to the state-of-the-art DualSep model.
Demos are available at: https://cabinsep.github.io/cabinsep/.

</details>


### [682] [From Discord to Harmony: Decomposed Consonance-based Training for Improved Audio Chord Estimation](https://arxiv.org/abs/2509.01588)
*Andrea Poltronieri,Xavier Serra,Martín Rocamora*

Main category: cs.SD

TL;DR: 文章聚焦音频和弦估计（ACE）任务现存挑战，评估和弦标注的标注者间一致性，提出基于和谐性的距离度量，还引入基于和谐性标签平滑的ACE conformer模型解决类不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 音频和弦估计虽有进展，但因和声内容独特特性面临挑战，如标注者主观性和类不平衡，现有系统性能遇瓶颈，需新方法解决。

Method: 评估标注者间一致性使用超越传统二元度量的指标；提出基于和谐性的距离度量；引入基于和谐性标签平滑的ACE conformer模型，分别估计根音、低音和所有音符激活来重建和弦标签。

Result: 基于和谐性的距离度量能更有效地捕捉标注间在音乐上有意义的一致性。

Conclusion: 新的评估指标、距离度量和ACE conformer模型有助于解决音频和弦估计中的挑战。

Abstract: Audio Chord Estimation (ACE) holds a pivotal role in music information
research, having garnered attention for over two decades due to its relevance
for music transcription and analysis. Despite notable advancements, challenges
persist in the task, particularly concerning unique characteristics of harmonic
content, which have resulted in existing systems' performances reaching a glass
ceiling. These challenges include annotator subjectivity, where varying
interpretations among annotators lead to inconsistencies, and class imbalance
within chord datasets, where certain chord classes are over-represented
compared to others, posing difficulties in model training and evaluation. As a
first contribution, this paper presents an evaluation of inter-annotator
agreement in chord annotations, using metrics that extend beyond traditional
binary measures. In addition, we propose a consonance-informed distance metric
that reflects the perceptual similarity between harmonic annotations. Our
analysis suggests that consonance-based distance metrics more effectively
capture musically meaningful agreement between annotations. Expanding on these
findings, we introduce a novel ACE conformer-based model that integrates
consonance concepts into the model through consonance-based label smoothing.
The proposed model also addresses class imbalance by separately estimating
root, bass, and all note activations, enabling the reconstruction of chord
labels from decomposed outputs.

</details>


### [683] [Music Genre Classification Using Machine Learning Techniques](https://arxiv.org/abs/2509.01762)
*Alokit Mishra,Ryyan Akhtar*

Main category: cs.SD

TL;DR: 本文对比分析机器学习方法用于自动音乐流派分类，发现SVM比CNN在GTZAN数据集上分类准确率更高，强调传统特征提取的重要性。


<details>
  <summary>Details</summary>
Motivation: 对自动音乐流派分类的机器学习方法进行比较分析。

Method: 在GTZAN数据集上，评估基于手工音频特征训练的经典分类器（如SVM和集成方法）与基于梅尔频谱图的CNN的性能。

Result: SVM利用特定领域特征工程，比端到端的CNN模型分类准确率更高。

Conclusion: 强调传统特征提取在实际音频处理任务中的持续相关性，对深度学习的普遍适用性提出批判性观点，尤其针对中等规模数据集。

Abstract: This paper presents a comparative analysis of machine learning methodologies
for automatic music genre classification. We evaluate the performance of
classical classifiers, including Support Vector Machines (SVM) and ensemble
methods, trained on a comprehensive set of hand-crafted audio features, against
a Convolutional Neural Network (CNN) operating on Mel spectrograms. The study
is conducted on the widely-used GTZAN dataset. Our findings demonstrate a
noteworthy result: the SVM, leveraging domain-specific feature engineering,
achieves superior classification accuracy compared to the end-to-end CNN model.
We attribute this outcome to the data-constrained nature of the benchmark
dataset, where the strong inductive bias of engineered features provides a
regularization effect that mitigates the risk of overfitting inherent in
high-capacity deep learning models. This work underscores the enduring
relevance of traditional feature extraction in practical audio processing tasks
and provides a critical perspective on the universal applicability of deep
learning, especially for moderately sized datasets.

</details>


### [684] [AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation](https://arxiv.org/abs/2509.02349)
*Lu Wang,Hao Chen,Siyu Wu,Zhiyue Wu,Hao Zhou,Chengfeng Zhang,Ting Wang,Haodi Zhang*

Main category: cs.SD

TL;DR: 本文针对大模型音频分词现有研究不足，给出合适的语义和声学标记定义，并引入系统评估框架，结果验证了定义正确性及各评估指标相关性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在语义和声学标记定义上不适用，且不同编解码器评估集中在特定领域或任务，无法进行公平全面比较。

Method: 提供语义和声学标记的合适定义，引入系统评估框架，从音频重建指标、码本索引稳定性、仅解码器变压器困惑度和下游探测任务性能四个维度评估编解码器能力。

Result: 验证了所提供定义的正确性，以及重建指标、码本ID稳定性、下游探测任务和困惑度之间的相关性。

Conclusion: 所提供的定义和评估框架有助于更公平全面地评估大模型音频分词的编解码器。

Abstract: Multimodal Large Language Models (MLLMs) have been widely applied in speech
and music. This tendency has led to a focus on audio tokenization for Large
Models (LMs). Unlike semantic-only text tokens, audio tokens must both capture
global semantic content and preserve fine-grained acoustic details. Moreover,
they provide a discrete method for speech and music that can be effectively
integrated into MLLMs. However, existing research is unsuitable in the
definitions of semantic tokens and acoustic tokens. In addition, the evaluation
of different codecs typically concentrates on specific domains or tasks, such
as reconstruction or Automatic Speech Recognition (ASR) task, which prevents
fair and comprehensive comparisons. To address these problems, this paper
provides suitable definitions for semantic and acoustic tokens and introduces a
systematic evaluation framework. This framework allows for a comprehensive
assessment of codecs' capabilities which evaluate across four dimensions: audio
reconstruction metric, codebook index (ID) stability, decoder-only transformer
perplexity, and performance on downstream probe tasks. Our results show the
correctness of the provided suitable definitions and the correlation among
reconstruction metrics, codebook ID stability, downstream probe tasks and
perplexity.

</details>


### [685] [Speech transformer models for extracting information from baby cries](https://arxiv.org/abs/2509.02259)
*Guillem Bonafos,Jéremy Rouch,Lény Lego,David Reby,Hugues Patural,Nicolas Mathevon,Rémy Emonet*

Main category: cs.SD

TL;DR: 研究预训练语音模型潜在表征对婴儿哭声数据集的适用性及编码特性，结果表明可有效分类婴儿哭声并编码关键信息，为未来模型设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 探究预训练语音模型潜在表征对非语音数据的适用性和编码的特定声学特性。

Method: 在八个婴儿哭声数据集上评估五个预训练语音模型，对各数据集在所有可用分类任务上评估模型的潜在表征。

Result: 模型的潜在表征能有效分类婴儿哭声，编码与发声源不稳定性和婴儿身份相关的关键信息。

Conclusion: 模型架构和训练策略的比较为类似任务（如情感检测）的未来模型设计提供有价值的见解。

Abstract: Transfer learning using latent representations from pre-trained speech models
achieves outstanding performance in tasks where labeled data is scarce.
However, their applicability to non-speech data and the specific acoustic
properties encoded in these representations remain largely unexplored. In this
study, we investigate both aspects. We evaluate five pre-trained speech models
on eight baby cries datasets, encompassing 115 hours of audio from 960 babies.
For each dataset, we assess the latent representations of each model across all
available classification tasks. Our results demonstrate that the latent
representations of these models can effectively classify human baby cries and
encode key information related to vocal source instability and identity of the
crying baby. In addition, a comparison of the architectures and training
strategies of these models offers valuable insights for the design of future
models tailored to similar tasks, such as emotion detection.

</details>


### [686] [ESTM: An Enhanced Dual-Branch Spectral-Temporal Mamba for Anomalous Sound Detection](https://arxiv.org/abs/2509.02471)
*Chengyuan Ma,Peng Jia,Hongyue Guo,Wenming Yang*

Main category: cs.SD

TL;DR: 提出基于双路径Mamba架构的ESTM框架用于工业设备异常声音检测，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有建模方法受局部感受野限制，难以捕捉机器声学特征的长程时间模式和跨频带动态耦合效应。

Method: 提出ESTM框架，基于双路径Mamba架构进行时频解耦建模，用Selective State - Space Models进行长序列建模，融合增强梅尔谱图和原始音频特征，并通过TriStat - Gating模块提高对异常模式的敏感性。

Result: ESTM在DCASE 2020 Task 2数据集上提高了异常检测性能。

Conclusion: 所提出的ESTM方法有效。

Abstract: The core challenge in industrial equipment anoma lous sound detection (ASD)
lies in modeling the time-frequency coupling characteristics of acoustic
features. Existing modeling methods are limited by local receptive fields,
making it difficult to capture long-range temporal patterns and cross-band
dynamic coupling effects in machine acoustic features. In this paper, we
propose a novel framework, ESTM, which is based on a dual-path Mamba
architecture with time-frequency decoupled modeling and utilizes Selective
State-Space Models (SSM) for long-range sequence modeling. ESTM extracts rich
feature representations from different time segments and frequency bands by
fusing enhanced Mel spectrograms and raw audio features, while further
improving sensitivity to anomalous patterns through the TriStat-Gating (TSG)
module. Our experiments demonstrate that ESTM improves anomalous detection
performance on the DCASE 2020 Task 2 dataset, further validating the
effectiveness of the proposed method.

</details>


### [687] [FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training](https://arxiv.org/abs/2509.02521)
*Yiqun Yao,Xiang Li,Xin Jiang,Xuezhi Fang,Naitong Yu,Wenjia Ma,Aixin Sun,Yequan Wang*

Main category: cs.SD

TL;DR: 提出自然独白和双训练范式构建7B口语对话模型FLM - Audio，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有全双工对话模型在文本独白与不同比特率音频流对齐上有挑战，主流解决方案有缺陷。

Method: 提出自然独白，在不同训练阶段交替自然独白与音频的前后位置进行双训练。

Result: 构建了7B口语对话模型FLM - Audio。

Conclusion: 该模型在响应性、双工性和聊天体验上表现优越。

Abstract: Full-duplex dialog models are designed to listen and speak simultaneously
with rapid responses to fast-changing user input. Among existing approaches,
native full-duplex models merges different channels (e.g. listen and speak) in
a single time step, overcoming the high response latency inherent to
time-division multiplexing time-division multiplexing (TDM) alternatives. Yet,
a key challenge remains: aligning textual monologues with audio streams that
operate at different bitrates. The prevailing solution relies on word-level
alignment, but this can degrade the language ability of large pre-trained
models. Moreover, it requires highly accurate timestamps for every token, which
introduces cascading errors and increases pre-processing costs. In this paper,
we propose textual monologues in continuous tokens sequence, namely "natural"
monologues, which mimics humanoid cognitive behavior in dialogs. For temporal
alignment, we alternate the position of the natural monologue - leading or
trailing the audio - across different training stages. This "dual" training
paradigm proves highly effective in building FLM-Audio, our 7B spoken dialog
model that demonstrates superior responsiveness, duplexity, and chatting
experiences, as confirmed by experimental results.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [688] [Real-Time Applicability of Emulated Virtual Circuits for Tokamak Plasma Shape Control](https://arxiv.org/abs/2509.01789)
*Pedro Cavestany,Alasdair Ross,Adriano Agnello,Aran Garrod,Nicola C. Amorisco,George K. Holt,Kamran Pentland,James Buchanan*

Main category: physics.plasm-ph

TL;DR: 本文探讨机器学习模拟托卡马克等离子体实时磁控灵敏度矩阵的实时适用性，涉及精度量化和未测电流处理，结果表明可开发低延迟模拟器用于等离子体形状实时控制。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模拟托卡马克等离子体实时磁控灵敏度矩阵的方法需量化可能的不准确性，以提升其在实时应用中的性能。

Method: 一是对比形状模拟器雅可比矩阵与精确Grad - Shafranov解有限差分雅可比矩阵计算的目标位移一致性；二是通过对有源线圈电流测量的滑动窗口进行简单线性回归推断未实时测量的容器电流。

Result: 选择几何目标，结合约$10^5$参数的神经网络模拟器，目标位移一致性约为5 - 10%，约$10^{5}-10^{6}$个合成平衡样本对训练不过度正则化或过拟合的模拟器至关重要；通过线性回归推断的整形电流残差仅几安培。

Conclusion: 基于MAST - U的历史数据和模拟，可开发具有几毫秒延迟的模拟器，用于现有和未来托卡马克的稳健实时等离子体形状控制。

Abstract: Machine learning has recently been adopted to emulate sensitivity matrices
for real-time magnetic control of tokamak plasmas. However, these approaches
would benefit from a quantification of possible inaccuracies. We report on two
aspects of real-time applicability of emulators. First, we quantify the
agreement of target displacement from VCs computed via Jacobians of the shape
emulators with those from finite differences Jacobians on exact Grad-Shafranov
solutions. Good agreement ($\approx$5-10%) can be achieved on a selection of
geometric targets using combinations of neural network emulators with
$\approx10^5$ parameters. A sample of $\approx10^{5}-10^{6}$ synthetic
equilibria is essential to train emulators that are not over-regularised or
overfitting. Smaller models trained on the shape targets may be further
fine-tuned to better fit the Jacobians. Second, we address the effect of vessel
currents that are not directly measured in real-time and are typically subsumed
into effective "shaping currents" when designing virtual circuits. We
demonstrate that shaping currents can be inferred via simple linear regression
on a trailing window of active coil current measurements with residuals of only
a few Amp\`eres, enabling a choice for the most appropriate shaping currents at
any point in a shot. While these results are based on historic shot data and
simulations tailored to MAST-U, they indicate that emulators with
few-millisecond latency can be developed for robust real-time plasma shape
control in existing and upcoming tokamaks.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [689] [Multimodal Generative Flows for LHC Jets](https://arxiv.org/abs/2509.01736)
*Darius A. Faroughy,Manfred Opper,Cesar Ojeda*

Main category: hep-ph

TL;DR: 提出基于变压器的多模态流模型，在CMS开放数据上训练，可生成高保真喷流。


<details>
  <summary>Details</summary>
Motivation: 大型强子对撞机高能碰撞生成建模面临粒子云数据混合性质的挑战，需要新方法。

Method: 引入基于变压器的多模态流，通过连续时间马尔可夫跳跃桥扩展流匹配来联合建模两种模态的LHC喷流。

Result: 模型在CMS开放数据上训练后，能生成具有现实运动学、喷流子结构和味道组成的高保真喷流。

Conclusion: 所提出的模型可有效用于大型强子对撞机高能碰撞生成建模。

Abstract: Generative modeling of high-energy collisions at the Large Hadron Collider
(LHC) offers a data-driven route to simulations, anomaly detection, among other
applications. A central challenge lies in the hybrid nature of particle-cloud
data: each particle carries continuous kinematic features and discrete quantum
numbers such as charge and flavor. We introduce a transformer-based multimodal
flow that extends flow-matching with a continuous-time Markov jump bridge to
jointly model LHC jets with both modalities. Trained on CMS Open Data, our
model can generate high fidelity jets with realistic kinematics, jet
substructure and flavor composition.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [690] [Enabling Down Syndrome Research through a Knowledge Graph-Driven Analytical Framework](https://arxiv.org/abs/2509.01565)
*Madan Krishnamurthy,Surya Saha,Pierrette Lo,Patricia L. Whetzel,Tursynay Issabekova,Jamed Ferreris Vargas,Jack DiGiovanna,Melissa A Haendel*

Main category: q-bio.QM

TL;DR: 本文针对唐氏综合征研究中数据分散的问题，开发知识图谱驱动平台统一数据，支持多方面分析。


<details>
  <summary>Details</summary>
Motivation: 唐氏综合征临床表型多样，研究数据分散，缺乏综合分析框架，需要有效整合数据进行研究。

Method: 开发知识图谱驱动平台，整合9项研究数据，用Monarch Initiative数据丰富资源，进行语义关联并用图嵌入和基于路径推理分析。

Result: 构建含超160万语义关联的知识图谱，可通过SPARQL或自然语言接口查询。

Conclusion: 该框架将静态数据仓库转为动态发现环境，支持唐氏综合征跨研究模式识别、预测建模和基因型 - 表型关系探索。

Abstract: Trisomy 21 results in Down syndrome, a multifaceted genetic disorder with
diverse clinical phenotypes, including heart defects, immune dysfunction,
neurodevelopmental differences, and early-onset dementia risk. Heterogeneity
and fragmented data across studies challenge comprehensive research and
translational discovery. The NIH INCLUDE (INvestigation of Co-occurring
conditions across the Lifespan to Understand Down syndromE) initiative has
assembled harmonized participant-level datasets, yet realizing their potential
requires integrative analytical frameworks. We developed a knowledge
graph-driven platform transforming nine INCLUDE studies, comprising 7,148
participants, 456 conditions, 501 phenotypes, and over 37,000 biospecimens,
into a unified semantic infrastructure. Cross-resource enrichment with Monarch
Initiative data expands coverage to 4,281 genes and 7,077 variants. The
resulting knowledge graph contains over 1.6 million semantic associations,
enabling AI-ready analysis with graph embeddings and path-based reasoning for
hypothesis generation. Researchers can query the graph via SPARQL or natural
language interfaces. This framework converts static data repositories into
dynamic discovery environments, supporting cross-study pattern recognition,
predictive modeling, and systematic exploration of genotype-phenotype
relationships in Down syndrome.

</details>


### [691] [Friend or Foe](https://arxiv.org/abs/2509.00123)
*Oleksandr Cherendichenko,Josephine Solowiej-Wedderburn,Laura M. Carroll,Eric Libby*

Main category: q-bio.QM

TL;DR: 提出Friend or Foe数据集，含64个表格环境数据集，可用于机器学习研究细菌相互作用，结果表明机器学习在微生物生态学应用可行。


<details>
  <summary>Details</summary>
Motivation: 解决确定细菌在不同环境条件下竞争或合作的挑战，利用现有基因组规模代谢模型数据。

Method: 创建Friend or Foe数据集，涵盖大量细菌对和环境数据，用于多种机器学习任务，并对相关模型进行基准测试。

Result: 机器学习在微生物生态学应用可取得成功。

Conclusion: Friend or Foe数据集分析有助于了解细菌相互作用可预测性，指明新研究方向。

Abstract: A fundamental challenge in microbial ecology is determining whether bacteria
compete or cooperate in different environmental conditions. With recent
advances in genome-scale metabolic models, we are now capable of simulating
interactions between thousands of pairs of bacteria in thousands of different
environmental settings at a scale infeasible experimentally. These approaches
can generate tremendous amounts of data that can be exploited by
state-of-the-art machine learning algorithms to uncover the mechanisms driving
interactions. Here, we present Friend or Foe, a compendium of 64 tabular
environmental datasets, consisting of more than 26M shared environments for
more than 10K pairs of bacteria sampled from two of the largest collections of
metabolic models. The Friend or Foe datasets are curated for a wide range of
machine learning tasks -- supervised, unsupervised, and generative -- to
address specific questions underlying bacterial interactions. We benchmarked a
selection of the most recent models for each of these tasks and our results
indicate that machine learning can be successful in this application to
microbial ecology. Going beyond, analyses of the Friend or Foe compendium can
shed light on the predictability of bacterial interactions and highlight novel
research directions into how bacteria infer and navigate their relationships.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [692] [Learning residue level protein dynamics with multiscale Gaussians](https://arxiv.org/abs/2509.01038)
*Mihir Bafna,Bowen Jing,Bonnie Berger*

Main category: q-bio.BM

TL;DR: 提出轻量级框架DynaProt从静态结构预测蛋白质动力学描述符，参数少且效果好，凸显直接预测蛋白质动力学潜力。


<details>
  <summary>Details</summary>
Motivation: 现有预测蛋白质静态结构方法多，但理解其动力学对阐明生物功能至关重要，分子动力学模拟计算成本高、可扩展性受限。

Method: 提出DynaProt框架，通过多元高斯视角，在两个互补尺度估计动力学。

Result: 能高精度预测残基水平灵活性，可合理重构全协方差矩阵用于快速集合生成，且使用参数比先前方法少几个数量级。

Conclusion: 直接蛋白质动力学预测有潜力成为现有方法的可扩展替代方案。

Abstract: Many methods have been developed to predict static protein structures,
however understanding the dynamics of protein structure is essential for
elucidating biological function. While molecular dynamics (MD) simulations
remain the in silico gold standard, its high computational cost limits
scalability. We present DynaProt, a lightweight, SE(3)-invariant framework that
predicts rich descriptors of protein dynamics directly from static structures.
By casting the problem through the lens of multivariate Gaussians, DynaProt
estimates dynamics at two complementary scales: (1) per-residue marginal
anisotropy as $3 \times 3$ covariance matrices capturing local flexibility, and
(2) joint scalar covariances encoding pairwise dynamic coupling across
residues. From these dynamics outputs, DynaProt achieves high accuracy in
predicting residue-level flexibility (RMSF) and, remarkably, enables reasonable
reconstruction of the full covariance matrix for fast ensemble generation.
Notably, it does so using orders of magnitude fewer parameters than prior
methods. Our results highlight the potential of direct protein dynamics
prediction as a scalable alternative to existing methods.

</details>


### [693] [Beyond Ensembles: Simulating All-Atom Protein Dynamics in a Learned Latent Space](https://arxiv.org/abs/2509.02196)
*Aditya Sengar,Ali Hariri,Pierre Vandergheynst,Patrick Barth*

Main category: q-bio.BM

TL;DR: 文章扩展LD - FPG模型加入时间传播器模拟全原子蛋白质动力学，比较三种传播器并明确权衡，提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 现有LD - FPG模型虽能生成全原子集合，但未对构象间时间演化建模，需扩展以模拟全原子蛋白质动力学。

Method: 在LD - FPG基础上加入在学习的潜在空间中操作的时间传播器，比较分数引导的朗之万动力学、基于Koopman的线性算子和自回归神经网络三类。

Result: 自回归神经网络长时模拟最稳健；分数引导的朗之万在分数学习良好时能最好恢复侧链热力学；Koopman提供可解释、轻量级基线且倾向于抑制波动。

Conclusion: 明确了传播器之间的权衡，为全原子蛋白质动力学的潜在空间模拟器提供实用指导。

Abstract: Simulating the long-timescale dynamics of biomolecules is a central challenge
in computational science. While enhanced sampling methods can accelerate these
simulations, they rely on pre-defined collective variables that are often
difficult to identify. A recent generative model, LD-FPG, demonstrated that
this problem could be bypassed by learning to sample the static equilibrium
ensemble as all-atom deformations from a reference structure, establishing a
powerful method for all-atom ensemble generation. However, while this approach
successfully captures a system's probable conformations, it does not model the
temporal evolution between them. Here we extend LD-FPG with a temporal
propagator that operates within the learned latent space and compare three
classes: (i) score-guided Langevin dynamics, (ii) Koopman-based linear
operators, and (iii) autoregressive neural networks. Within a unified
encoder-propagator-decoder framework, we evaluate long-horizon stability,
backbone and side-chain ensemble fidelity, and functional free-energy
landscapes. Autoregressive neural networks deliver the most robust long
rollouts; score-guided Langevin best recovers side-chain thermodynamics when
the score is well learned; and Koopman provides an interpretable, lightweight
baseline that tends to damp fluctuations. These results clarify the trade-offs
among propagators and offer practical guidance for latent-space simulators of
all-atom protein dynamics.

</details>


### [694] [Morphology-Specific Peptide Discovery via Masked Conditional Generative Modeling](https://arxiv.org/abs/2509.02060)
*Nuno Costa,Julija Zavadlav*

Main category: q-bio.BM

TL;DR: 介绍PepMorph肽发现管道，能生成特定形态的新肽序列，准确率达83%。


<details>
  <summary>Details</summary>
Motivation: 解决筛选大量肽序列以分类聚集形态的难题，为生物医学和能源应用设计材料。

Method: 编译新数据集，提取几何和理化描述符，训练带掩码机制的基于Transformer的条件变分自编码器，过滤并通过粗粒度分子动力学模拟验证。

Result: PepMorph在生成预期形态方面准确率达83%。

Conclusion: PepMorph作为应用驱动的肽发现框架有很大潜力。

Abstract: Peptide self-assembly prediction offers a powerful bottom-up strategy for
designing biocompatible, low-toxicity materials for large-scale synthesis in a
broad range of biomedical and energy applications. However, screening the vast
sequence space for categorization of aggregate morphology remains intractable.
We introduce PepMorph, an end-to-end peptide discovery pipeline that generates
novel sequences that are not only prone to aggregate but self-assemble into a
specified fibrillar or spherical morphology. We compiled a new dataset by
leveraging existing aggregation propensity datasets and extracting geometric
and physicochemical isolated peptide descriptors that act as proxies for
aggregate morphology. This dataset is then used to train a Transformer-based
Conditional Variational Autoencoder with a masking mechanism, which generates
novel peptides under arbitrary conditioning. After filtering to ensure design
specifications and validation of generated sequences through coarse-grained
molecular dynamics simulations, PepMorph yielded 83% accuracy in intended
morphology generation, showcasing its promise as a framework for
application-driven peptide discovery.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [695] [Revealing Hidden Precursors to Earthquakes via a Stress-Sensitive Transformation of Seismic Noise](https://arxiv.org/abs/2509.00268)
*Nader Shakibay Senobari*

Main category: physics.geo-ph

TL;DR: 提出应力敏感频域变换方法，分析实验室和地震记录发现地震前存在隐藏前兆，为地震预测提供途径。


<details>
  <summary>Details</summary>
Motivation: 地震预测是科学难题，现实中未观测到可靠地震前兆信号，需探究其是否存在。

Method: 引入应力敏感频域变换，追踪相邻频带能量差异，分离与应力变化相关的光谱变化。

Result: 该变换在实验室声发射数据和7次大地震记录中均揭示了震前数小时至数天的前兆特征，且在不同构造环境中都很稳健。

Conclusion: 环境地震噪声中确实存在隐藏的地震前兆，为实时断层监测和短期地震预报提供了途径。

Abstract: Earthquake prediction has long been one of the most elusive challenges in
science. Laboratory experiments and simulations suggest that failure precursors
should exist, yet reliable signals have remained unobserved in real-world
seismic records, leaving open the question of whether they are absent in nature
or simply hidden within noise. Here we introduce a stress-sensitive
frequency-domain transformation that tracks energy differences between adjacent
frequency bands, isolating subtle spectral changes linked to evolving shear and
normal stress. Applied to both laboratory acoustic emission data and seismic
records from seven major earthquakes (Mw 5.9-9.0), including the 2011 Tohoku
and 2023 Turkey-Syria events, the transform consistently reveals precursory
signatures, arc-like trajectories and accelerations toward extrema, emerging
hours to days before rupture. These features are robust across diverse tectonic
settings, from induced seismicity and volcanic collapse to continental
strike-slip and subduction megathrust earthquakes. Our findings demonstrate
that hidden precursors are indeed encoded in ambient seismic noise, offering a
pathway toward real-time fault monitoring and actionable short-term earthquake
forecasting.

</details>


### [696] [Using explainable artificial intelligence (XAI) as a diagnostic tool: An application for deducing hydrologic connectivity at watershed scale](https://arxiv.org/abs/2509.02127)
*Sheng Ye,Jiyu Li,Yifan Chai,Lin Liu,Murugesu Sivapalan,Qihua Ran*

Main category: physics.geo-ph

TL;DR: 提出结合XAI方法和水文知识的框架，以解释水文响应，用XAI评估LSTM网络输入影响，结果表明该框架能识别流域子区域功能差异、指示水文连通性发展并助力理解水流变化。


<details>
  <summary>Details</summary>
Motivation: 现有将XAI与水文知识结合用于过程理解的应用有限，需新方法促进水文过程理解。

Method: 提出在点尺度应用XAI方法的框架，用水文模型生成的土壤湿度和运动数据训练LSTM网络，用XAI方法评估输入影响。

Result: 基于XAI的分类能有效识别流域尺度各子区域功能角色差异，聚合的XAI结果可提供水文连通性发展的明确量化指标。

Conclusion: 该框架可促进其他水文响应的聚合，推动水文过程理解。

Abstract: Explainable artificial intelligence (XAI) methods have been applied to
interpret deep learning model results. However, applications that integrate XAI
with established hydrologic knowledge for process understanding remain limited.
Here we present a framework that apply XAI method at point-scale to provide
granular interpretation and enable cross-scale aggregation of hydrologic
responses. Hydrologic connectivity is used as a demonstration of the value of
this approach. Soil moisture and its movement generated by physically based
hydrologic model were used to train a long short-term memory (LSTM) network,
whose impacts of inputs were evaluated by XAI methods. Our results suggest that
XAI-based classification can effectively identify the differences in the
functional roles of various sub-regions at watershed scale. The aggregated XAI
results provide an explicit and quantitative indicator of hydrologic
connectivity development, offering insights to streamflow variation. This
framework could be used to facilitate aggregation of other hydrologic responses
to advance process understandings.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [697] [Exam Readiness Index (ERI): A Theoretical Framework for a Composite, Explainable Index](https://arxiv.org/abs/2509.00718)
*Ananda Prakash Verma*

Main category: cs.CY

TL;DR: 论文提出考试准备指数（ERI）理论框架，该指数汇总六项信号得出，论文对其进行公理形式化等理论研究，实证留待未来。


<details>
  <summary>Details</summary>
Motivation: 提出可解释且可操作的指数来总结学习者应对高风险考试的准备情况。

Method: 聚合六项信号构建指数，对组件映射和综合指数进行公理形式化，证明单调性、稳定性等性质，刻画置信区间。

Result: 证明了指数在蓝图重新加权下的单调性、Lipschitz稳定性和有界漂移，得出最优线性综合的存在性和唯一性，刻画了置信区间并证明与课程的兼容性。

Conclusion: 专注于理论研究，实证研究留待未来工作。

Abstract: We present a theoretical framework for an Exam Readiness Index (ERI): a
composite, blueprint-aware score R in [0,100] that summarizes a learner's
readiness for a high-stakes exam while remaining interpretable and actionable.
The ERI aggregates six signals -- Mastery (M), Coverage (C), Retention (R),
Pace (P), Volatility (V), and Endurance (E) -- each derived from a stream of
practice and mock-test interactions. We formalize axioms for component maps and
the composite, prove monotonicity, Lipschitz stability, and bounded drift under
blueprint re-weighting, and show existence and uniqueness of the optimal linear
composite under convex design constraints. We further characterize confidence
bands via blueprint-weighted concentration and prove compatibility with
prerequisite-admissible curricula (knowledge spaces / learning spaces). The
paper focuses on theory; empirical study is left to future work.

</details>


### [698] [The Collaborations among Healthcare Systems, Research Institutions, and Industry on Artificial Intelligence Research and Development](https://arxiv.org/abs/2509.00068)
*Jiancheng Ye,Michelle Ma,Malak Abuhashish*

Main category: cs.CY

TL;DR: 研究利用中国相关组织数据开展全国横断面调查，发现医疗界对AI兴趣高但参与开发差距大，存在数据隐私等阻碍，建议开展教育、建立数据框架等。


<details>
  <summary>Details</summary>
Motivation: 刻画AI医疗合作网络与利益相关者，识别合作挑战与机遇，明确未来AI研发重点。

Method: 利用中国放射学会和医学影像AI创新联盟数据，在中国31个省级行政区开展全国横断面调查，涉及三类参与者。

Result: 临床医生对AI兴趣高，但参与开发差距大；数据共享意愿高，但受隐私安全、行业标准和法律指南缺失阻碍；未来关注病变筛查等。

Conclusion: 医疗界对AI热情又谨慎，存在阻碍有效合作和实施的障碍，建议开展特定教育、建立安全数据框架等。

Abstract: Objectives: The integration of Artificial Intelligence (AI) in healthcare
promises to revolutionize patient care, diagnostics, and treatment protocols.
Collaborative efforts among healthcare systems, research institutions, and
industry are pivotal to leveraging AI's full potential. This study aims to
characterize collaborative networks and stakeholders in AI healthcare
initiatives, identify challenges and opportunities within these collaborations,
and elucidate priorities for future AI research and development. Methods: This
study utilized data from the Chinese Society of Radiology and the Chinese
Medical Imaging AI Innovation Alliance. A national cross-sectional survey was
conducted in China (N = 5,142) across 31 provincial administrative regions,
involving participants from three key groups: clinicians, institution
professionals, and industry representatives. The survey explored diverse
aspects including current AI usage in healthcare, collaboration dynamics,
challenges encountered, and research and development priorities. Results:
Findings reveal high interest in AI among clinicians, with a significant gap
between interest and actual engagement in development activities. Despite the
willingness to share data, progress is hindered by concerns about data privacy
and security, and lack of clear industry standards and legal guidelines. Future
development interests focus on lesion screening, disease diagnosis, and
enhancing clinical workflows. Conclusion: This study highlights an enthusiastic
yet cautious approach toward AI in healthcare, characterized by significant
barriers that impede effective collaboration and implementation.
Recommendations emphasize the need for AI-specific education and training,
secure data-sharing frameworks, establishment of clear industry standards, and
formation of dedicated AI research departments.

</details>


### [699] [The Application of Virtual Environments and Artificial Intelligence in Higher Education: Experimental Findings in Philosophy Teaching](https://arxiv.org/abs/2509.00110)
*Adel Vehrer,Zsolt Palfalusi*

Main category: cs.CY

TL;DR: 研究探索虚拟环境和AI对大学生学习体验的提升，在大学开展实验，多数学生成绩好且认为虚拟材料有效，为数字教学提供新方向。


<details>
  <summary>Details</summary>
Motivation: 探索虚拟环境和人工智能如何提升大学生学习体验，关注Z世代数字偏好。

Method: 在大学开展实验，将Walter's Cube技术和AI融入哲学课程教学，课程与大纲对齐并丰富内容，让学生参加考试和反馈，用统计和推理测试评估效果。

Result: 80%参与者期末成绩好或优秀，多数认为虚拟材料有效，定性反馈显示增强了学习动力和参与度。

Conclusion: 研究推动数字教学发展，为高等教育应用虚拟和基于AI的方法提供新方向。

Abstract: This study explores how virtual environments and artificial intelligence can
enhance university students' learning experiences, with particular attention to
the digital preferences of Generation Z. An experiment was conducted at the
Faculty of Pedagogy, Humanities, and Social Sciences at University of Gyor,
where Walter's Cube technology and a trained AI mediator were integrated into
the instruction of ten philosophical topics. The curriculum was aligned with
the official syllabus and enriched with visual content, quotations, and
explanatory texts related to iconic figures in philosophy. A total of 77
first-year undergraduate students from full-time humanities and social sciences
programs participated in the study. Following their end-of-semester offline
written examination, students voluntarily completed a paper-based, anonymous
ten-question test and provided feedback on the method's effectiveness. No
sensitive personal data were collected, and the research was conducted with
formal approval from the Faculty Dean. Descriptive statistics and inferential
tests were applied to evaluate the impact of the virtual environment and AI
mediation on learning outcomes. Results indicate that 80 percent of
participants achieved good or excellent final exam grades, and the majority
rated the virtual material as highly effective. Qualitative feedback emphasized
increased motivation and deeper engagement, attributed to the immersive 3D
presentation and interactive AI support. This research contributes to the
advancement of digital pedagogy and suggests new directions for applying
virtual and AI-based methods in higher education, particularly in disciplines
where abstract reasoning and conceptual understanding are central.

</details>


### [700] [Embodied AI: Emerging Risks and Opportunities for Policy Action](https://arxiv.org/abs/2509.00117)
*Jared Perlo,Alexander Robey,Fazl Barez,Luciano Floridi,Jakob Mökander*

Main category: cs.CY

TL;DR: EAI发展迅速但带来重大风险，现有政策不足，本文给出风险分类、分析现有政策缺口并提供政策建议。


<details>
  <summary>Details</summary>
Motivation: EAI发展带来重大风险，现有政策无法应对，需要扩展和调整现有框架。

Method: 提供EAI关键风险分类，分析美、欧、英政策，找出政策缺口。

Result: 明确了EAI的各类风险，找出了现有政策的不足。

Conclusion: 提出具体政策建议，如强制测试认证、明确责任框架等。

Abstract: The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI
can exist in, learn from, reason about, and act in the physical world. Given
recent innovations in large language and multimodal models, along with
increasingly advanced and responsive hardware, EAI systems are rapidly growing
in capabilities and operational domains. These advances present significant
risks, including physical harm from malicious use, mass surveillance, and
economic and societal disruption. However, these risks have been severely
overlooked by policymakers. Existing policies, such as international standards
for industrial robots or statutes governing autonomous vehicles, are
insufficient to address the full range of concerns. While lawmakers are
increasingly focused on AI, there is now an urgent need to extend and adapt
existing frameworks to account for the unique risks of EAI. To help bridge this
gap, this paper makes three contributions: first, we provide a foundational
taxonomy of key physical, informational, economic, and social EAI risks.
Secondly, we analyze policies in the US, EU, and UK to identify how existing
frameworks address these risks and where these policies leave critical gaps. We
conclude by offering concrete policy recommendations to address the coming wave
of EAI innovation, including mandatory testing and certification for EAI
systems, clarified liability frameworks, and forward-looking strategies to
manage and prepare for transformative economic and societal impacts.

</details>


### [701] [Scaling Legal AI: Benchmarking Mamba and Transformers for Statutory Classification and Case Law Retrieval](https://arxiv.org/abs/2509.00141)
*Anuraj Maurya*

Main category: cs.CY

TL;DR: 本文对Mamba与领先的Transformer模型进行全面基准测试，用于法律分类和案例检索，结果显示Mamba能处理更长文档，还引入新基准套件。


<details>
  <summary>Details</summary>
Motivation: 法定语料库和司法判决快速增长，需要可扩展的法律AI系统，而基于Transformer的架构存在二次注意力成本问题，限制效率和可扩展性。

Method: 对Mamba和领先的Transformer模型在开源法律语料库上进行基准测试，使用多种指标评估，包括准确率、召回率等。

Result: Mamba的线性扩展使其能处理比Transformer长几倍的法律文档，同时在检索和分类性能上保持或超越。

Conclusion: 研究突出了状态空间模型和Transformer之间的权衡，为在法定分析、司法决策支持和政策研究中部署可扩展法律AI提供指导。

Abstract: The rapid growth of statutory corpora and judicial decisions requires
scalable legal AI systems capable of classification and retrieval over
extremely long contexts. Transformer-based architectures (e.g., Longformer,
DeBERTa) dominate current legal NLP benchmarks but struggle with quadratic
attention costs, limiting efficiency and scalability. In this work, we present
the first comprehensive benchmarking of Mamba, a state-space model (SSM) with
linear-time selective mechanisms, against leading transformer models for
statutory classification and case law retrieval. We evaluate models on
open-source legal corpora including LexGLUE, EUR-Lex, and ILDC, covering
statutory tagging, judicial outcome prediction, and case retrieval tasks.
Metrics include accuracy, recall at k, mean reciprocal rank (MRR), and
normalized discounted cumulative gain (nDCG), alongside throughput measured in
tokens per second and maximum context length. Results show that Mamba's linear
scaling enables processing of legal documents several times longer than
transformers, while maintaining or surpassing retrieval and classification
performance. This study introduces a new legal NLP benchmark suite for
long-context modeling, along with open-source code and datasets to support
reproducibility. Our findings highlight trade-offs between state-space models
and transformers, providing guidance for deploying scalable legal AI in
statutory analysis, judicial decision support, and policy research.

</details>


### [702] [Pilot Study on Generative AI and Critical Thinking in Higher Education Classrooms](https://arxiv.org/abs/2509.00167)
*W. F. Lamberti,S. R. Lawrence,D. White,S. Kim,S. Abdullah*

Main category: cs.CY

TL;DR: 该试点研究探讨计算与数据科学入门课程学生评估生成式AI输出时运用批判性思维的能力，设计学习活动，为后续研究奠基。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在教育场景广泛应用，但在培养批判性思维方面研究不足，尤其是学生对其生成回复的批判性评估。

Method: 设计要求学生分析、批判和修改AI生成解决方案的学习活动。

Result: 获得了学生批判性参与生成式AI内容能力的初步见解。

Conclusion: 为未来学期更全面的研究奠定基础。

Abstract: Generative AI (GAI) tools have seen rapid adoption in educational settings,
yet their role in fostering critical thinking remains underexplored. While
previous studies have examined GAI as a tutor for specific lessons or as a tool
for completing assignments, few have addressed how students critically evaluate
the accuracy and appropriateness of GAI-generated responses. This pilot study
investigates students' ability to apply structured critical thinking when
assessing Generative AI outputs in introductory Computational and Data Science
courses. Given that GAI tools often produce contextually flawed or factually
incorrect answers, we designed learning activities that require students to
analyze, critique, and revise AI-generated solutions. Our findings offer
initial insights into students' ability to engage critically with GAI content
and lay the groundwork for more comprehensive studies in future semesters.

</details>


### [703] [Criteria for Credible AI-assisted Carbon Footprinting Systems: The Cases of Mapping and Lifecycle Modeling](https://arxiv.org/abs/2509.00240)
*Shaena Ulissi,Andrew Dumit,P. James Joyce,Krishna Rao,Steven Watson,Sangwon Suh*

Main category: cs.CY

TL;DR: 本文提出验证AI辅助产品和材料温室气体排放计算系统的标准，介绍三步验证方法、三种应用场景，认为应进行系统级评估，为评估工具提供基础。


<details>
  <summary>Details</summary>
Motivation: 组织需了解碳足迹，AI辅助计算系统涌现但缺乏严格性和透明度，标准和评估数据滞后。

Method: 采用三步法：识别需求和约束、制定标准草案、通过试点完善。

Result: 可构建可信AI系统，应进行系统级评估，有基准性能等指标。

Conclusion: 该方法为评估AI辅助环境评估工具提供基础，有助于制定AI辅助碳足迹计算系统标准。

Abstract: As organizations face increasing pressure to understand their corporate and
products' carbon footprints, artificial intelligence (AI)-assisted calculation
systems for footprinting are proliferating, but with widely varying levels of
rigor and transparency. Standards and guidance have not kept pace with the
technology; evaluation datasets are nascent; and statistical approaches to
uncertainty analysis are not yet practical to apply to scaled systems. We
present a set of criteria to validate AI-assisted systems that calculate
greenhouse gas (GHG) emissions for products and materials. We implement a
three-step approach: (1) Identification of needs and constraints, (2) Draft
criteria development and (3) Refinements through pilots. The process identifies
three use cases of AI applications: Case 1 focuses on AI-assisted mapping to
existing datasets for corporate GHG accounting and product hotspotting,
automating repetitive manual tasks while maintaining mapping quality. Case 2
addresses AI systems that generate complete product models for corporate
decision-making, which require comprehensive validation of both component tasks
and end-to-end performance. We discuss the outlook for Case 3 applications,
systems that generate standards-compliant models. We find that credible AI
systems can be built and that they should be validated using system-level
evaluations rather than line-item review, with metrics such as benchmark
performance, indications of data quality and uncertainty, and transparent
documentation. This approach may be used as a foundation for practitioners,
auditors, and standards bodies to evaluate AI-assisted environmental assessment
tools. By establishing evaluation criteria that balance scalability with
credibility requirements, our approach contributes to the field's efforts to
develop appropriate standards for AI-assisted carbon footprinting systems.

</details>


### [704] [A Study on the Framework for Evaluating the Ethics and Trustworthiness of Generative AI](https://arxiv.org/abs/2509.00398)
*Cheonsu Jeong,Seunghyun Lee,Sunny Jeong,Sungsu Kim*

Main category: cs.CY

TL;DR: 研究分析生成式AI伦理与可信度挑战，提出评估框架，为其负责任发展奠定学术基础。


<details>
  <summary>Details</summary>
Motivation: 生成式AI发展带来伦理和社会问题，现有评估方法不足以应对。

Method: 确定评估生成式AI伦理和可信度的关键维度，开发指标和评估方法，对比多国AI伦理政策。

Result: 提出适用于AI全生命周期、融合多学科视角的评估框架。

Conclusion: 为生成式AI负责任发展奠定学术基础，为利益相关者提供行动建议。

Abstract: This study provides an in_depth analysis of the ethical and trustworthiness
challenges emerging alongside the rapid advancement of generative artificial
intelligence (AI) technologies and proposes a comprehensive framework for their
systematic evaluation. While generative AI, such as ChatGPT, demonstrates
remarkable innovative potential, it simultaneously raises ethical and social
concerns, including bias, harmfulness, copyright infringement, privacy
violations, and hallucination. Current AI evaluation methodologies, which
mainly focus on performance and accuracy, are insufficient to address these
multifaceted issues. Thus, this study emphasizes the need for new
human_centered criteria that also reflect social impact. To this end, it
identifies key dimensions for evaluating the ethics and trustworthiness of
generative AI_fairness, transparency, accountability, safety, privacy,
accuracy, consistency, robustness, explainability, copyright and intellectual
property protection, and source traceability and develops detailed indicators
and assessment methodologies for each. Moreover, it provides a comparative
analysis of AI ethics policies and guidelines in South Korea, the United
States, the European Union, and China, deriving key approaches and implications
from each. The proposed framework applies across the AI lifecycle and
integrates technical assessments with multidisciplinary perspectives, thereby
offering practical means to identify and manage ethical risks in real_world
contexts. Ultimately, the study establishes an academic foundation for the
responsible advancement of generative AI and delivers actionable insights for
policymakers, developers, users, and other stakeholders, supporting the
positive societal contributions of AI technologies.

</details>


### [705] [Can AI be Auditable?](https://arxiv.org/abs/2509.00575)
*Himanshu Verma,Kirtan Path,Eva Thelisson*

Main category: cs.CY

TL;DR: 探讨AI可审计性通过新兴监管框架的形式化，分析面临的挑战，强调需清晰指南等，最后指出多方协作和审计师赋能的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究如何使AI系统在整个生命周期能独立评估以符合伦理、法律和技术标准。

Method: 探讨新兴监管框架如欧盟AI法案对可审计性的规范，分析面临的挑战。

Result: 明确了AI可审计性面临技术不透明、文档实践不一致等挑战。

Conclusion: 强调多方利益相关者合作和审计师赋能对构建有效AI审计生态的重要性，可审计性应融入AI开发和治理。

Abstract: Auditability is defined as the capacity of AI systems to be independently
assessed for compliance with ethical, legal, and technical standards throughout
their lifecycle. The chapter explores how auditability is being formalized
through emerging regulatory frameworks, such as the EU AI Act, which mandate
documentation, risk assessments, and governance structures. It analyzes the
diverse challenges facing AI auditability, including technical opacity,
inconsistent documentation practices, lack of standardized audit tools and
metrics, and conflicting principles within existing responsible AI frameworks.
The discussion highlights the need for clear guidelines, harmonized
international regulations, and robust socio-technical methodologies to
operationalize auditability at scale. The chapter concludes by emphasizing the
importance of multi-stakeholder collaboration and auditor empowerment in
building an effective AI audit ecosystem. It argues that auditability must be
embedded in AI development practices and governance infrastructures to ensure
that AI systems are not only functional but also ethically and legally aligned.

</details>


### [706] [RAG-PRISM: A Personalized, Rapid, and Immersive Skill Mastery Framework with Adaptive Retrieval-Augmented Tutoring](https://arxiv.org/abs/2509.00646)
*Gaurangi Raul,Yu-Zheng Lin,Karan Patel,Bono Po-Jen Shih,Matthew W. Redondo,Banafsheh Saber Latibari,Jesus Pacheco,Soheil Salehi,Pratik Satam*

Main category: cs.CY

TL;DR: 工业4.0系统数字化转型使劳动力技能差距扩大，本文提出结合生成式AI与RAG的自适应辅导框架，经评估GPT - 4表现最佳，该框架为4IR教育和劳动力发展提供可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 工业4.0系统数字化转型重塑劳动力需求、扩大技能差距，尤其是老年工人，需要大规模再技能和提升技能，现有培训项目需改进以满足多样需求。

Method: 提出结合生成式AI与RAG的自适应辅导框架，利用文档命中率和MRR优化内容，创建合成QA数据集，用LLMs生成响应，评估其忠实度和内容一致性。

Result: GPT - 4表现最佳，具有87%的相关性和100%的一致性。

Conclusion: 该双模式方法使自适应导师可作为个性化主题推荐器和内容生成器，为4IR教育和劳动力发展提供可扩展的快速定制学习解决方案。

Abstract: The rapid digital transformation of Fourth Industrial Revolution (4IR)
systems is reshaping workforce needs, widening skill gaps, especially for older
workers. With growing emphasis on STEM skills such as robotics, automation,
artificial intelligence (AI), and security, large-scale re-skilling and
up-skilling are required. Training programs must address diverse backgrounds,
learning styles, and motivations to improve persistence and success, while
ensuring rapid, cost-effective workforce development through experiential
learning. To meet these challenges, we present an adaptive tutoring framework
that combines generative AI with Retrieval-Augmented Generation (RAG) to
deliver personalized training. The framework leverages document hit rate and
Mean Reciprocal Rank (MRR) to optimize content for each learner, and is
benchmarked against human-generated training for alignment and relevance. We
demonstrate the framework in 4IR cybersecurity learning by creating a synthetic
QA dataset emulating trainee behavior, while RAG is tuned on curated
cybersecurity materials. Evaluation compares its generated training with
manually curated queries representing realistic student interactions. Responses
are produced using large language models (LLMs) including GPT-3.5 and GPT-4,
assessed for faithfulness and content alignment. GPT-4 achieves the best
performance with 87% relevancy and 100% alignment. Results show this dual-mode
approach enables the adaptive tutor to act as both a personalized topic
recommender and content generator, offering a scalable solution for rapid,
tailored learning in 4IR education and workforce development.

</details>


### [707] [Who Gets Left Behind? Auditing Disability Inclusivity in Large Language Models](https://arxiv.org/abs/2509.00963)
*Deepika Dash,Yeshil Bangera,Mithil Bangera,Gouthami Vadithya,Srikant Panda*

Main category: cs.CY

TL;DR: 提出针对大语言模型可达性建议的分类对齐基准，评估17个模型发现包容性差距并给出改进方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在可达性指导方面对很多残疾群体服务不足，需解决这一差距。

Method: 提出分类对齐基准，从问题层面覆盖度、残疾层面覆盖度和深度三个维度评估模型。

Result: 对17个模型评估发现视觉、听觉和行动方面常被提及，言语、遗传/发育、感官认知和心理健康方面服务不足，深度也集中在少数类别。

Conclusion: 揭示当前大语言模型可达性指导中被忽视的群体，强调分类感知提示/训练和联合审计广度、平衡和深度的评估方法是可行动的改进方向。

Abstract: Large Language Models (LLMs) are increasingly used for accessibility
guidance, yet many disability groups remain underserved by their advice. To
address this gap, we present taxonomy aligned benchmark1 of human validated,
general purpose accessibility questions, designed to systematically audit
inclusivity across disabilities. Our benchmark evaluates models along three
dimensions: Question-Level Coverage (breadth within answers), Disability-Level
Coverage (balance across nine disability categories), and Depth (specificity of
support). Applying this framework to 17 proprietary and open-weight models
reveals persistent inclusivity gaps: Vision, Hearing, and Mobility are
frequently addressed, while Speech, Genetic/Developmental, Sensory-Cognitive,
and Mental Health remain under served. Depth is similarly concentrated in a few
categories but sparse elsewhere. These findings reveal who gets left behind in
current LLM accessibility guidance and highlight actionable levers:
taxonomy-aware prompting/training and evaluations that jointly audit breadth,
balance, and depth.

</details>


### [708] [Agentic Workflow for Education: Concepts and Applications](https://arxiv.org/abs/2509.01517)
*Yuan-Hao Jiang,Yijie Lu,Ling Dai,Jiatong Wang,Ruijia Li,Bo Jiang*

Main category: cs.CY

TL;DR: 研究引入教育代理工作流（AWE）模型，对比传统交互，提出理论框架，确定应用领域，案例验证有效性，认为其可减轻教师负担等。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和人工智能代理的发展，代理工作流在教育中有变革潜力，需新的教育工作流模型。

Method: 引入AWE四组件模型，与传统基于大语言模型的线性交互区分，基于冯·诺伊曼多智能体系统架构提出理论框架，确定应用领域，开展自动数学测试生成案例研究。

Result: 案例显示AWE生成的题目与真实考题在统计上相当，验证了模型有效性。

Conclusion: AWE为减轻教师工作量、提高教学质量和推动教育创新提供了有前景的途径。

Abstract: With the rapid advancement of Large Language Models (LLMs) and Artificial
Intelligence (AI) agents, agentic workflows are showing transformative
potential in education. This study introduces the Agentic Workflow for
Education (AWE), a four-component model comprising self-reflection, tool
invocation, task planning, and multi-agent collaboration. We distinguish AWE
from traditional LLM-based linear interactions and propose a theoretical
framework grounded in the von Neumann Multi-Agent System (MAS) architecture.
Through a paradigm shift from static prompt-response systems to dynamic,
nonlinear workflows, AWE enables scalable, personalized, and collaborative task
execution. We further identify four core application domains: integrated
learning environments, personalized AI-assisted learning, simulation-based
experimentation, and data-driven decision-making. A case study on automated
math test generation shows that AWE-generated items are statistically
comparable to real exam questions, validating the model's effectiveness. AWE
offers a promising path toward reducing teacher workload, enhancing
instructional quality, and enabling broader educational innovation.

</details>


### [709] [Journalists' Perceptions of Artificial Intelligence and Disinformation Risks](https://arxiv.org/abs/2509.01824)
*Urko Peña-Alonso,Simón Peña-Fernández,Koldobika Meso-Ayerdi*

Main category: cs.CY

TL;DR: 研究调查巴斯克地区504名记者对人工智能影响虚假信息的看法，多数记者认为AI会增加虚假信息风险。


<details>
  <summary>Details</summary>
Motivation: 因生成式AI快速发展影响新闻生产和媒体组织，关注记者对AI影响虚假信息的看法。

Method: 采用定量方法，对巴斯克地区504名记者进行结构化调查，涵盖社会人口和职业变量及对AI态度等问题。

Result: 多数记者（89.88%）认为AI会显著增加虚假信息风险，该看法在不同性别和媒体类型中一致，经验丰富者更明显；经验、AI使用与风险感知有显著关联；主要风险是难检测虚假内容和深度伪造、数据不准确，且这些风险相互关联。

Conclusion: 研究凸显记者对AI在信息生态系统中作用的复杂多面担忧。

Abstract: This study examines journalists' perceptions of the impact of artificial
intelligence (AI) on disinformation, a growing concern in journalism due to the
rapid expansion of generative AI and its influence on news production and media
organizations. Using a quantitative approach, a structured survey was
administered to 504 journalists in the Basque Country, identified through
official media directories and with the support of the Basque Association of
Journalists. This survey, conducted online and via telephone between May and
June 2024, included questions on sociodemographic and professional variables,
as well as attitudes toward AI's impact on journalism. The results indicate
that a large majority of journalists (89.88%) believe AI will considerably or
significantly increase the risks of disinformation, and this perception is
consistent across genders and media types, but more pronounced among those with
greater professional experience. Statistical analyses reveal a significant
association between years of experience and perceived risk, and between AI use
and risk perception. The main risks identified are the difficulty in detecting
false content and deepfakes, and the risk of obtaining inaccurate or erroneous
data. Co-occurrence analysis shows that these risks are often perceived as
interconnected. These findings highlight the complex and multifaceted concerns
of journalists regarding AI's role in the information ecosystem.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [710] [Learning with Mandelbrot and Julia](https://arxiv.org/abs/2509.00903)
*V. R. Tjahjono,S. F. Feng,E. R. M. Putri,H. Susanto*

Main category: nlin.CD

TL;DR: 本文将机器学习技术应用于分形集的分类和结构分析，表明其比传统方法有更高预测精度和更低计算成本，还能为纯数学带来新见解。


<details>
  <summary>Details</summary>
Motivation: 将机器学习加速数值计算的技术拓展到更具理论性质的分形集分类和结构分析。

Method: 使用包括CART、KNN、MLP、LSTM、BiLSTM、RF和CNN等监督学习方法对分形点进行分类。

Result: 机器学习方法比传统数值方法（如阈值技术）有更高预测精度和更低计算成本，KNN和RF整体性能最佳，模型间比较显示数学结构存在新规律。

Conclusion: 机器学习不仅提高分类效率，还为纯数学提供新见解、直觉和猜想。

Abstract: Recent developments in applied mathematics increasingly employ machine
learning (ML)-particularly supervised learning-to accelerate numerical
computations, such as solving nonlinear partial differential equations. In this
work, we extend such techniques to objects of a more theoretical nature: the
classification and structural analysis of fractal sets. Focusing on the
Mandelbrot and Julia sets as principal examples, we demonstrate that supervised
learning methods-including Classification and Regression Trees (CART),
K-Nearest Neighbors (KNN), Multilayer Perceptrons (MLP), and Recurrent Neural
Networks using both Long Short-Term Memory (LSTM) and Bidirectional LSTM
(BiLSTM), Random Forests (RF), and Convolutional Neural Networks (CNN)-can
classify fractal points with significantly higher predictive accuracy and
substantially lower computational cost than traditional numerical approaches,
such as the thresholding technique. These improvements are consistent across a
range of models and evaluation metrics. Notably, KNN and RF exhibit the best
overall performance, and comparative analyses between models (e.g., KNN vs.
LSTM) suggest the presence of novel regularity properties in these mathematical
structures. Collectively, our findings indicate that ML not only enhances
classification efficiency but also offers promising avenues for generating new
insights, intuitions, and conjectures within pure mathematics.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [711] [OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews](https://arxiv.org/abs/2509.00285)
*Mir Tafseer Nayeem,Davood Rafiei*

Main category: cs.CL

TL;DR: 研究从大量用户评论中生成观点亮点的问题，提出OpinioRAG框架和新验证指标，贡献大规模数据集，实验表明该框架能大规模生成准确相关结构化总结。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大量用户评论时要么无法扩展，要么生成通用、忽视个性化需求的总结。

Method: 引入OpinioRAG框架，结合RAG证据检索和大语言模型；提出新的无参考验证指标；构建大规模数据集。

Result: 通过大量实验，识别关键挑战，为改进系统提供可行见解。

Conclusion: OpinioRAG是一个可大规模生成准确、相关和结构化总结的强大框架。

Abstract: We study the problem of opinion highlights generation from large volumes of
user reviews, often exceeding thousands per entity, where existing methods
either fail to scale or produce generic, one-size-fits-all summaries that
overlook personalized needs. To tackle this, we introduce OpinioRAG, a
scalable, training-free framework that combines RAG-based evidence retrieval
with LLMs to efficiently produce tailored summaries. Additionally, we propose
novel reference-free verification metrics designed for sentiment-rich domains,
where accurately capturing opinions and sentiment alignment is essential. These
metrics offer a fine-grained, context-sensitive assessment of factual
consistency. To facilitate evaluation, we contribute the first large-scale
dataset of long-form user reviews, comprising entities with over a thousand
reviews each, paired with unbiased expert summaries and manually annotated
queries. Through extensive experiments, we identify key challenges, provide
actionable insights into improving systems, pave the way for future research,
and position OpinioRAG as a robust framework for generating accurate, relevant,
and structured summaries at scale.

</details>


### [712] [GIER: Gap-Driven Self-Refinement for Large Language Models](https://arxiv.org/abs/2509.00325)
*Rinku Dewri*

Main category: cs.CL

TL;DR: 提出GIER框架，通过自反思和修订改进大语言模型输出，在多任务和模型上提升推理质量且不降低准确性。


<details>
  <summary>Details</summary>
Motivation: 改进大语言模型输出质量，提升推理能力。

Method: 利用自然语言描述推理差距，促使模型迭代批判和完善自身输出。

Result: 在三个推理密集型任务和四个大语言模型上，提升了理由质量、依据性和推理一致性，且不降低任务准确性。

Conclusion: 模型能理解抽象概念差距并转化为具体推理改进。

Abstract: We introduce GIER (Gap-driven Iterative Enhancement of Responses), a general
framework for improving large language model (LLM) outputs through
self-reflection and revision based on conceptual quality criteria. Unlike
prompting strategies that rely on demonstrations, examples, or chain-of-thought
templates, GIER utilizes natural language descriptions of reasoning gaps, and
prompts a model to iteratively critique and refine its own outputs to better
satisfy these criteria. Across three reasoning-intensive tasks (SciFact,
PrivacyQA, and e-SNLI) and four LLMs (GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, and
Llama 3.3 70B), GIER improves rationale quality, grounding, and reasoning
alignment without degrading task accuracy. Our analysis demonstrates that
models can not only interpret abstract conceptual gaps but also translate them
into concrete reasoning improvements.

</details>


### [713] [LongCat-Flash Technical Report](https://arxiv.org/abs/2509.01322)
*Meituan LongCat Team,Bayan,Bei Li,Bingye Lei,Bo Wang,Bolin Rong,Chao Wang,Chao Zhang,Chen Gao,Chen Zhang,Cheng Sun,Chengcheng Han,Chenguang Xi,Chi Zhang,Chong Peng,Chuan Qin,Chuyu Zhang,Cong Chen,Congkui Wang,Dan Ma,Daoru Pan,Defei Bu,Dengchang Zhao,Deyang Kong,Dishan Liu,Feiye Huo,Fengcun Li,Fubao Zhang,Gan Dong,Gang Liu,Gang Xu,Ge Li,Guoqiang Tan,Guoyuan Lin,Haihang Jing,Haomin Fu,Haonan Yan,Haoxing Wen,Haozhe Zhao,Hong Liu,Hongmei Shi,Hongyan Hao,Hongyin Tang,Huantian Lv,Hui Su,Jiacheng Li,Jiahao Liu,Jiahuan Li,Jiajun Yang,Jiaming Wang,Jian Yang,Jianchao Tan,Jiaqi Sun,Jiaqi Zhang,Jiawei Fu,Jiawei Yang,Jiaxi Hu,Jiayu Qin,Jingang Wang,Jiyuan He,Jun Kuang,Junhui Mei,Kai Liang,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Liang Gao,Liang Shi,Lianhui Ma,Lin Qiu,Lingbin Kong,Lingtong Si,Linkun Lyu,Linsen Guo,Liqi Yang,Lizhi Yan,Mai Xia,Man Gao,Manyuan Zhang,Meng Zhou,Mengxia Shen,Mingxiang Tuo,Mingyang Zhu,Peiguang Li,Peng Pei,Peng Zhao,Pengcheng Jia,Pingwei Sun,Qi Gu,Qianyun Li,Qingyuan Li,Qiong Huang,Qiyuan Duan,Ran Meng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shizhe Wu,Shuai Liang,Shuo Wang,Suogui Dang,Tao Fang,Tao Li,Tefeng Chen,Tianhao Bai,Tianhao Zhou,Tingwen Xie,Wei He,Wei Huang,Wei Liu,Wei Shi,Wei Wang,Wei Wu,Weikang Zhao,Wen Zan,Wenjie Shi,Xi Nan,Xi Su,Xiang Li,Xiang Mei,Xiangyang Ji,Xiangyu Xi,Xiangzhou Huang,Xianpeng Li,Xiao Fu,Xiao Liu,Xiao Wei,Xiaodong Cai,Xiaolong Chen,Xiaoqing Liu,Xiaotong Li,Xiaowei Shi,Xiaoyu Li,Xili Wang,Xin Chen,Xing Hu,Xingyu Miao,Xinyan He,Xuemiao Zhang,Xueyuan Hao,Xuezhi Cao,Xunliang Cai,Xurui Yang,Yan Feng,Yang Bai,Yang Chen,Yang Yang,Yaqi Huo,Yerui Sun,Yifan Lu,Yifan Zhang,Yipeng Zang,Yitao Zhai,Yiyang Li,Yongjing Yin,Yongkang Lv,Yongwei Zhou,Yu Yang,Yuchen Xie,Yueqing Sun,Yuewen Zheng,Yuhua Wei,Yulei Qian,Yunfan Liang,Yunfang Tai,Yunke Zhao,Zeyang Yu,Zhao Zhang,Zhaohua Yang,Zhenchao Zhang,Zhikang Xia,Zhiye Zou,Zhizhao Zeng,Zhongda Su,Zhuofan Chen,Zijian Zhang,Ziwen Wang,Zixu Jiang,Zizhe Zhao,Zongyu Wang,Zunhai Su*

Main category: cs.CL

TL;DR: 介绍5600亿参数的语言模型LongCat - Flash，有新设计，训练快、推理效率高，在代理任务表现好且开源。


<details>
  <summary>Details</summary>
Motivation: 满足可扩展效率的需求，开发兼具计算效率和高级代理能力的语言模型。

Method: 采用Zero - computation Experts和Shortcut - connected MoE设计；开发综合缩放框架用于稳定可重复训练；进行大规模预训练、针对性训练及合成数据和工具使用任务增强。

Result: 30天内完成超20万亿token训练，推理达超100 TPS，成本0.7美元/百万输出token，在代理任务表现出色。

Conclusion: LongCat - Flash作为非思考基础模型，在领先模型中表现有竞争力，开源促进社区研究。

Abstract: We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE)
language model designed for both computational efficiency and advanced agentic
capabilities. Stemming from the need for scalable efficiency, LongCat-Flash
adopts two novel designs: (a) Zero-computation Experts, which enables dynamic
computational budget allocation and activates 18.6B-31.3B (27B on average) per
token depending on contextual demands, optimizing resource usage. (b)
Shortcut-connected MoE, which enlarges the computation-communication overlap
window, demonstrating notable gains in inference efficiency and throughput
compared to models of a comparable scale. We develop a comprehensive scaling
framework for large models that combines hyperparameter transfer, model-growth
initialization, a multi-pronged stability suite, and deterministic computation
to achieve stable and reproducible training. Notably, leveraging the synergy
among scalable architectural design and infrastructure efforts, we complete
model training on more than 20 trillion tokens within 30 days, while achieving
over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million
output tokens. To cultivate LongCat-Flash towards agentic intelligence, we
conduct a large-scale pre-training on optimized mixtures, followed by targeted
mid- and post-training on reasoning, code, and instructions, with further
augmentation from synthetic data and tool use tasks. Comprehensive evaluations
demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers
highly competitive performance among other leading models, with exceptional
strengths in agentic tasks. The model checkpoint of LongCat-Flash is
open-sourced to foster community research.
  LongCat Chat: https://longcat.ai
  Hugging Face: https://huggingface.co/meituan-longcat
  GitHub: https://github.com/meituan-longcat

</details>


### [714] [MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature](https://arxiv.org/abs/2509.00414)
*Juraj Vladika,Florian Matthes*

Main category: cs.CL

TL;DR: 介绍了交互式AI系统MedSEBA，它能综合医学问题的循证答案，用户研究显示该系统可用且有帮助。


<details>
  <summary>Details</summary>
Motivation: 网络上医疗信息难辨可靠与否，医学研究数量多，传统搜索工具无法反映研究的不同结论，需要新系统解决这些问题。

Method: 引入MedSEBA系统，利用大语言模型生成答案，从PubMed数据库动态检索可靠医学研究作为支撑。

Result: 用户研究表明，医学专家和普通用户认为系统可用且有帮助，答案可信且有信息价值。

Conclusion: 该系统适用于日常健康问题和高级研究洞察。

Abstract: In the digital age, people often turn to the Internet in search of medical
advice and recommendations. With the increasing volume of online content, it
has become difficult to distinguish reliable sources from misleading
information. Similarly, millions of medical studies are published every year,
making it challenging for researchers to keep track of the latest scientific
findings. These evolving studies can reach differing conclusions, which is not
reflected in traditional search tools. To address these challenges, we
introduce MedSEBA, an interactive AI-powered system for synthesizing
evidence-based answers to medical questions. It utilizes the power of Large
Language Models to generate coherent and expressive answers, but grounds them
in trustworthy medical studies dynamically retrieved from the research database
PubMed. The answers consist of key points and arguments, which can be traced
back to respective studies. Notably, the platform also provides an overview of
the extent to which the most relevant studies support or refute the given
medical claim, and a visualization of how the research consensus evolved
through time. Our user study revealed that medical experts and lay users find
the system usable and helpful, and the provided answers trustworthy and
informative. This makes the system well-suited for both everyday health
questions and advanced research insights.

</details>


### [715] [Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech](https://arxiv.org/abs/2509.00673)
*Sanjeeevan Selvaganapathy,Mehwish Nasim*

Main category: cs.CL

TL;DR: 研究大语言模型检测仇恨言论效果，发现审查模型在准确率和鲁棒性上优于未审查模型，但有局限性，且所有模型存在理解细微语言不足等问题，需更复杂审计框架。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型检测隐式和显式仇恨言论的效果，比较未审查模型和审查模型的客观分类能力。

Method: 对比未审查模型和审查模型在检测仇恨言论上的表现。

Result: 审查模型在准确率和鲁棒性上显著优于未审查模型（78.7%对64.1%），但审查模型受安全对齐影响，未审查模型易受意识形态影响；所有模型理解细微语言存在问题，不同目标群体性能有公平性差异，自我报告确定性不可靠。

Conclusion: 挑战大语言模型作为客观仲裁者的观念，强调需要考虑公平性、校准和意识形态一致性的更复杂审计框架。

Abstract: We investigate the efficacy of Large Language Models (LLMs) in detecting
implicit and explicit hate speech, examining whether models with minimal safety
alignment (uncensored) might provide more objective classification capabilities
compared to their heavily-aligned (censored) counterparts. While uncensored
models theoretically offer a less constrained perspective free from moral
guardrails that could bias classification decisions, our results reveal a
surprising trade-off: censored models significantly outperform their uncensored
counterparts in both accuracy and robustness, achieving 78.7% versus 64.1%
strict accuracy. However, this enhanced performance comes with its own
limitation -- the safety alignment acts as a strong ideological anchor, making
censored models resistant to persona-based influence, while uncensored models
prove highly malleable to ideological framing. Furthermore, we identify
critical failures across all models in understanding nuanced language such as
irony. We also find alarming fairness disparities in performance across
different targeted groups and systemic overconfidence that renders
self-reported certainty unreliable. These findings challenge the notion of LLMs
as objective arbiters and highlight the need for more sophisticated auditing
frameworks that account for fairness, calibration, and ideological consistency.

</details>


### [716] [Do small language models generate realistic variable-quality fake news headlines?](https://arxiv.org/abs/2509.00680)
*Austin McCutcheon,Chris Brogly*

Main category: cs.CL

TL;DR: 研究评估14个小语言模型生成真假新闻标题能力，发现它们生成假标题配合度高，现有检测模型准确率低。


<details>
  <summary>Details</summary>
Motivation: 评估小语言模型生成虚假文本的能力及生成的假新闻标题与真实新闻标题的相似度。

Method: 通过控制提示工程生成24000条低质量和高质量的欺骗性标题，用现有机器学习和深度学习新闻标题质量检测器检测。

Result: 小语言模型生成假标题配合度高，伦理抵抗小；现有模型检测标题质量误分类常见，准确率35.2% - 63.5%。

Conclusion: 测试的小语言模型普遍易于生成虚假标题，有轻微伦理约束差异；生成标题与网络上人类撰写内容相似度低。

Abstract: Small language models (SLMs) have the capability for text generation and may
potentially be used to generate falsified texts online. This study evaluates 14
SLMs (1.7B-14B parameters) including LLaMA, Gemma, Phi, SmolLM, Mistral, and
Granite families in generating perceived low and high quality fake news
headlines when explicitly prompted, and whether they appear to be similar to
real-world news headlines. Using controlled prompt engineering, 24,000
headlines were generated across low-quality and high-quality deceptive
categories. Existing machine learning and deep learning-based news headline
quality detectors were then applied against these SLM-generated fake news
headlines. SLMs demonstrated high compliance rates with minimal ethical
resistance, though there were some occasional exceptions. Headline quality
detection using established DistilBERT and bagging classifier models showed
that quality misclassification was common, with detection accuracies only
ranging from 35.2% to 63.5%. These findings suggest the following: tested SLMs
generally are compliant in generating falsified headlines, although there are
slight variations in ethical restraints, and the generated headlines did not
closely resemble existing primarily human-written content on the web, given the
low quality classification accuracy.

</details>


### [717] [TMT: A Simple Way to Translate Topic Models Using Dictionaries](https://arxiv.org/abs/2509.00822)
*Felix Engl,Andreas Henrich*

Main category: cs.CL

TL;DR: 提出新颖的主题模型翻译技术TMT，可跨语言复用主题模型，评估显示其能产生语义连贯一致的主题翻译。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下训练主题模型有挑战，开发者缺乏目标语言知识或数据有限时难度更大，需新方法解决。

Method: 引入主题模型翻译技术TMT，无需元数据、嵌入或对齐语料库来跨语言转移主题模型。

Result: 通过定量和定性方法评估，TMT能产生语义连贯和一致的主题翻译。

Conclusion: TMT是一种新颖、稳健且透明的技术，适合目标语言大语料库不可用或手动翻译不可行的场景。

Abstract: The training of topic models for a multilingual environment is a challenging
task, requiring the use of sophisticated algorithms, topic-aligned corpora, and
manual evaluation. These difficulties are further exacerbated when the
developer lacks knowledge of the target language or is working in an
environment with limited data, where only small or unusable multilingual
corpora are available.
  Considering these challenges, we introduce Topic Model Translation (TMT), a
novel, robust and transparent technique designed to transfer topic models
(e.g., Latent Dirichlet Allocation (LDA) based topic models) from one language
to another, without the need for metadata, embeddings, or aligned corpora. TMT
enables the reuse of topic models across languages, making it especially
suitable for scenarios where large corpora in the target language are
unavailable or manual translation is infeasible. Furthermore, we evaluate TMT
extensively using both quantitative and qualitative methods, demonstrating that
it produces semantically coherent and consistent topic translations.

</details>


### [718] [ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links](https://arxiv.org/abs/2509.01387)
*Serwar Basch,Ilia Kuznetsov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出新的跨文档链接标注框架，在两个领域应用显示结合检索模型与大语言模型效果好，还公开了代码、数据和协议。


<details>
  <summary>Details</summary>
Motivation: 现有自动化辅助研究受限于缺乏创建跨文档链接训练和评估数据集的有效方法。

Method: 生成并验证半合成的互联文档数据集进行自动评估，选出表现最佳的链接方法短名单，再进行人工评估；在同行评审和新闻两个领域应用该框架。

Result: 结合检索模型与大语言模型获得78%的人工评级链接批准，精度超单纯强检索器两倍。

Conclusion: 框架可系统研究跨文档理解，新数据集为众多跨文档任务奠定基础。

Abstract: Understanding fine-grained relations between documents is crucial for many
application domains. However, the study of automated assistance is limited by
the lack of efficient methods to create training and evaluation datasets of
cross-document links. To address this, we introduce a new domain-agnostic
framework for selecting a best-performing approach and annotating
cross-document links in a new domain from scratch. We first generate and
validate semi-synthetic datasets of interconnected documents. This data is used
to perform automatic evaluation, producing a shortlist of best-performing
linking approaches. These approaches are then used in an extensive human
evaluation study, yielding performance estimates on natural text pairs. We
apply our framework in two distinct domains -- peer review and news -- and show
that combining retrieval models with LLMs achieves 78\% link approval from
human raters, more than doubling the precision of strong retrievers alone. Our
framework enables systematic study of cross-document understanding across
application scenarios, and the resulting novel datasets lay foundation for
numerous cross-document tasks like media framing and peer review. We make the
code, data, and annotation protocols openly available.

</details>


### [719] [Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization](https://arxiv.org/abs/2509.02093)
*Juhyeon Lee,Wonduk Seo,Hyunjin An,Seunghyun Lee,Yi Bu*

Main category: cs.CL

TL;DR: 提出对比推理提示优化框架CRPO，在HelpSteer2基准测试中表现优于基线，凸显对比、检索增强推理对自动提示优化的潜力。


<details>
  <summary>Details</summary>
Motivation: 多数先前工作忽视利用大语言模型固有推理能力从对比示例学习，本文旨在解决该问题，改进自动提示优化。

Method: 提出CRPO框架，将提示优化表述为检索增强推理过程，从HelpSteer2数据集检索参考提示，构建分层对比推理和多指标对比推理两个优化范式。

Result: 在HelpSteer2基准测试中，CRPO显著优于基线。

Conclusion: 对比、检索增强推理对推进自动提示优化很有前景。

Abstract: Automatic prompt optimization has recently emerged as a strategy for
improving the quality of prompts used in Large Language Models (LLMs), with the
goal of generating more accurate and useful responses. However, most prior work
focuses on direct prompt refinement or model fine-tuning, overlooking the
potential of leveraging LLMs' inherent reasoning capability to learn from
contrasting examples. In this paper, we present Contrastive Reasoning Prompt
Optimization (CRPO), a novel framework that formulates prompt optimization as a
retrieval augmented reasoning process. Our approach retrieves top k reference
prompts from the HelpSteer2 dataset, an open-source collection annotated for
helpfulness, correctness, coherence, complexity, and verbosity, and constructs
two complementary optimization paradigms: (1) tiered contrastive reasoning,
where the LLM compares high, medium, and low quality prompts to refine its own
generation through reflective reasoning, and (2) multi-metric contrastive
reasoning, where the LLM analyzes the best prompts along each evaluation
dimension and integrates their strengths into an optimized prompt. By
explicitly contrasting high and low quality exemplars, CRPO enables the model
to deduce why certain prompts succeed while others fail, thereby achieving more
robust and interpretable optimization. Experimental results on the HelpSteer2
benchmark demonstrate that CRPO significantly outperforms baselines. Our
findings highlight the promise of contrastive, retrieval-augmented reasoning
for advancing automatic prompt optimization.

</details>


### [720] [Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation](https://arxiv.org/abs/2509.02510)
*Erfan Baghaei Potraghloo,Seyedarmin Azizi,Souvik Kundu,Massoud Pedram*

Main category: cs.CL

TL;DR: 本文提出top - H解码方法解决大语言模型在开放式文本生成中平衡多样性、创造性与逻辑连贯性的问题，经实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有截断采样技术在将模型置信度有效融入采样策略方面存在局限，难以平衡文本生成的多样性、创造性与逻辑连贯性。

Method: 提出top - H解码，先建立截断采样中创造性与连贯性相互作用的理论基础，将其转化为熵约束质量最大化（ECMM）问题，再用计算高效的贪心算法求解该问题。

Result: top - H在创意写作基准上比min - p采样最高提升25.63%，在问答数据集上保持鲁棒性，且经LLM评判在高温下能生成连贯输出。

Conclusion: top - H推动了开放式文本生成的发展，可轻松集成到创意写作应用中。

Abstract: Large language models (LLMs), despite their impressive performance across a
wide range of tasks, often struggle to balance two competing objectives in
open-ended text generation: fostering diversity and creativity while preserving
logical coherence. Existing truncated sampling techniques, including
temperature scaling, top-\$p\$ (nucleus) sampling, and min-\$p\$ sampling, aim
to manage this trade-off. However, they exhibit limitations, particularly in
the effective incorporation of the confidence of the model into the
corresponding sampling strategy. For example, min-\$p\$ sampling relies on a
single top token as a heuristic for confidence, eventually underutilizing the
information of the probability distribution. Toward effective incorporation of
the confidence of the model, in this paper, we present **top-H** decoding. We
first establish the theoretical foundation of the interplay between creativity
and coherence in truncated sampling by formulating an **entropy-constrained
minimum divergence** problem. We then prove this minimization problem to be
equivalent to an **entropy-constrained mass maximization** (ECMM) problem,
which is NP-hard. Finally, we present top-H decoding, a computationally
efficient greedy algorithm to solve the ECMM problem. Extensive empirical
evaluations demonstrate that top-H outperforms the state-of-the-art (SoTA)
alternative of min-\$p\$ sampling by up to **25.63%** on creative writing
benchmarks, while maintaining robustness on question-answering datasets such as
GPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms
that top-H indeed produces coherent outputs even at higher temperatures, where
creativity is especially critical. In summary, top-H advances SoTA in
open-ended text generation and can be *easily integrated* into creative writing
applications. The code is available at
https://github.com/ErfanBaghaei/Top-H-Decoding.

</details>


### [721] [Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis](https://arxiv.org/abs/2509.00038)
*Teo Susnjak*

Main category: cs.CL

TL;DR: 本文指出当前大语言模型辅助系统文献综述存在问题，提出结构化、特定领域框架，提供工作代码示例构建可验证LLM管道。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型辅助系统文献综述的方法依赖手工编写提示，可靠性和可重复性差，影响科学信心。

Method: 借鉴通用大语言模型应用中声明式提示优化的进展，提出结构化、特定领域框架，将任务声明、测试套件和自动提示调整嵌入可重复的系统文献综述工作流程。

Result: 将新兴方法转化为具体蓝图并提供工作代码示例。

Conclusion: 将声明式提示优化方法应用于系统文献综述管道是一种新颖应用。

Abstract: Large language models (LLMs) offer significant potential to accelerate
systematic literature reviews (SLRs), yet current approaches often rely on
brittle, manually crafted prompts that compromise reliability and
reproducibility. This fragility undermines scientific confidence in
LLM-assisted evidence synthesis. In response, this work adapts recent advances
in declarative prompt optimisation, developed for general-purpose LLM
applications, and demonstrates their applicability to the domain of SLR
automation. This research proposes a structured, domain-specific framework that
embeds task declarations, test suites, and automated prompt tuning into a
reproducible SLR workflow. These emerging methods are translated into a
concrete blueprint with working code examples, enabling researchers to
construct verifiable LLM pipelines that align with established principles of
transparency and rigour in evidence synthesis. This is a novel application of
such approaches to SLR pipelines.

</details>


### [722] [What Are Research Hypotheses?](https://arxiv.org/abs/2509.00185)
*Jian Wu,Sarah Rajtmajer*

Main category: cs.CL

TL;DR: 本文概述和界定了假设的各种定义，强调了定义明确的假设的重要性。


<details>
  <summary>Details</summary>
Motivation: 过去几十年，自然语言处理领域对自动提取、理解、测试和生成假设的模型训练很关注，但不同自然语言理解任务中对“假设”的定义与传统定义有差异，且文献中的定义也不同。

Method: 对不同定义进行概述和界定，辨别近期自然语言理解任务中定义的细微差别。

Result: 明确了不同的假设定义及定义间的细微差别。

Conclusion: 强调了结构良好、定义明确的假设的重要性，特别是在迈向机器可解释学术记录的过程中。

Abstract: Over the past decades, alongside advancements in natural language processing,
significant attention has been paid to training models to automatically
extract, understand, test, and generate hypotheses in open and scientific
domains. However, interpretations of the term \emph{hypothesis} for various
natural language understanding (NLU) tasks have migrated from traditional
definitions in the natural, social, and formal sciences. Even within NLU, we
observe differences defining hypotheses across literature. In this paper, we
overview and delineate various definitions of hypothesis. Especially, we
discern the nuances of definitions across recently published NLU tasks. We
highlight the importance of well-structured and well-defined hypotheses,
particularly as we move toward a machine-interpretable scholarly record.

</details>


### [723] [Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics](https://arxiv.org/abs/2509.00190)
*Sheldon Yu,Yuxin Xiong,Junda Wu,Xintong Li,Tong Yu,Xiang Chen,Ritwik Sinha,Jingbo Shang,Julian McAuley*

Main category: cs.CL

TL;DR: 本文提出状态感知转换框架，将思维链轨迹抽象为结构化潜在动态，支持多种分析。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理可解释性有限，高级语义角色和转换探索不足。

Method: 用谱分析表示推理步骤，聚类成潜在状态，用马尔可夫链建模推理过程。

Result: 得到推理过程结构化和可解释视图。

Conclusion: 该框架能支持语义角色识别、时间模式可视化和一致性评估等分析。

Abstract: Recent advances in chain-of-thought (CoT) prompting have enabled large
language models (LLMs) to perform multi-step reasoning. However, the
explainability of such reasoning remains limited, with prior work primarily
focusing on local token-level attribution, such that the high-level semantic
roles of reasoning steps and their transitions remain underexplored. In this
paper, we introduce a state-aware transition framework that abstracts CoT
trajectories into structured latent dynamics. Specifically, to capture the
evolving semantics of CoT reasoning, each reasoning step is represented via
spectral analysis of token-level embeddings and clustered into semantically
coherent latent states. To characterize the global structure of reasoning, we
model their progression as a Markov chain, yielding a structured and
interpretable view of the reasoning process. This abstraction supports a range
of analyses, including semantic role identification, temporal pattern
visualization, and consistency evaluation.

</details>


### [724] [The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions](https://arxiv.org/abs/2509.00248)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: 提出基于C. S. Peirce符号学理论的建模实践通用理论框架，并阐述其模型语义理论，还举例说明其应用并探讨相关问题和方向。


<details>
  <summary>Details</summary>
Motivation: 领域缺乏以统一方式描述不同类型建模实践的通用理论框架。

Method: 基于C. S. Peirce的符号学理论提出框架，通过与其他模型对比理解模型。

Result: 提出了理论框架，并用实例说明其经验性应用。

Conclusion: 该框架为建模实践提供了通用理论，可研究模型语义，还指出了未来研究方向。

Abstract: The proliferation of methods for modeling of human meaning-making constitutes
a powerful class of instruments for the analysis of complex semiotic systems.
However, the field lacks a general theoretical framework for describing these
modeling practices across various model types in an apples-to-apples way. In
this paper, we propose such a framework grounded in the semiotic theory of C.
S. Peirce. We argue that such models measure latent symbol geometries, which
can be understood as hypotheses about the complex of semiotic agencies
underlying a symbolic dataset. Further, we argue that in contexts where a
model's value cannot be straightforwardly captured by proxy measures of
performance, models can instead be understood relationally, so that the
particular interpretive lens of a model becomes visible through its contrast
with other models. This forms the basis of a theory of model semantics in which
models, and the modeling decisions that constitute them, are themselves treated
as signs. In addition to proposing the framework, we illustrate its empirical
use with a few brief examples and consider foundational questions and future
directions enabled by the framework.

</details>


### [725] [Open Data Synthesis For Deep Research](https://arxiv.org/abs/2509.00375)
*Ziyi Xia,Kun Luo,Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: 提出InfoSeek框架合成复杂深度研究任务，实验显示基于其训练的模型表现优，还支持高级优化策略并公开代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法捕捉深度研究任务复杂性，合成数据集存在问题，需新框架解决。

Method: 将深度研究任务形式化为HCSPs，引入InfoSeek框架，用双智能体系统从网页构建研究树并转换为自然语言问题。

Result: 基于InfoSeek训练的模型在基准测试中表现优于强基线，3B模型超32B模型和轻量级商业API。

Conclusion: InfoSeek有效解决现有问题，能合成复杂深度研究任务，支持高级优化策略。

Abstract: Large language models (LLMs) are increasingly expected to go beyond simple
factual queries toward Deep Research-tasks that require decomposing questions
into sub-problems, coordinating multi-step reasoning, and synthesizing evidence
from diverse sources. We formalize Deep Research tasks with verifiable answers
as Hierarchical Constraint Satisfaction Problems (HCSPs), which are
fundamentally different from single-constraint, multi-hop, or flat CSP
formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)
fail to capture this complexity, while recent synthetic datasets often
introduce shortcut reasoning, knowledge leakage, or lack sufficient structural
depth. To address this gap, we introduce InfoSeek, a scalable framework for
synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to
recursively build a Research Tree from large-scale webpages, blurring
intermediate nodes into valid sub-problems, and converting these trees into
natural language questions that require traversing the full hierarchy. It also
enables rapid scaling, yielding over 50K training examples, a curated test set,
and reasoning trajectories generated via reject sampling. Experiments show that
models trained on InfoSeek consistently outperform strong baselines. On a
challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass
much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),
while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).
By preserving meta-information such as intermediate steps and retrieval labels,
InfoSeek further supports advanced optimization strategies, including compound
reward design and trajectory-level exploration. We provide our codes and
datasets in \href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.

</details>


### [726] [The Resurgence of GCG Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2509.00391)
*Yuting Tan,Xuying Li,Zhuo Li,Huizhen Shu,Peikang Hu*

Main category: cs.CL

TL;DR: 本文对GCG及其变体T - GCG在不同规模开源大模型上进行系统评估，发现攻击成功率与模型大小、评估方式和提示类型有关，还指出了GCG可扩展性限制等。


<details>
  <summary>Details</summary>
Motivation: 对基于梯度的对抗性提示方法GCG及其变体T - GCG在不同规模开源大模型上进行系统评估，了解其攻击效果。

Method: 使用Qwen2.5 - 0.5B、LLaMA - 3.2 - 1B和GPT - OSS - 20B，在安全导向提示和推理密集型编码提示上评估攻击效果。

Result: 攻击成功率随模型大小下降；前缀启发式高估攻击效果；编码提示比安全提示更易受攻击；T - GCG的模拟退火能在一定程度上多样化对抗搜索。

Conclusion: 揭示了GCG可扩展性限制，发现推理任务中被忽视的漏洞，推动基于退火策略的对抗性评估方法发展。

Abstract: Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient
(GCG) algorithm, has emerged as a powerful method for jailbreaking large
language models (LLMs). In this paper, we present a systematic appraisal of GCG
and its annealing-augmented variant, T-GCG, across open-source LLMs of varying
scales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack
effectiveness on both safety-oriented prompts (AdvBench) and
reasoning-intensive coding prompts. Our study reveals three key findings: (1)
attack success rates (ASR) decrease with model size, reflecting the increasing
complexity and non-convexity of larger models' loss landscapes; (2)
prefix-based heuristics substantially overestimate attack effectiveness
compared to GPT-4o semantic judgments, which provide a stricter and more
realistic evaluation; and (3) coding-related prompts are significantly more
vulnerable than adversarial safety prompts, suggesting that reasoning itself
can be exploited as an attack vector. In addition, preliminary results with
T-GCG show that simulated annealing can diversify adversarial search and
achieve competitive ASR under prefix evaluation, though its benefits under
semantic judgment remain limited. Together, these findings highlight the
scalability limits of GCG, expose overlooked vulnerabilities in reasoning
tasks, and motivate further development of annealing-inspired strategies for
more robust adversarial evaluation.

</details>


### [727] [TECP: Token-Entropy Conformal Prediction for LLMs](https://arxiv.org/abs/2509.00461)
*Beining Xu*

Main category: cs.CL

TL;DR: 提出Token - Entropy Conformal Prediction (TECP)框架用于开放式语言生成的不确定性量化，在多个模型和基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开放式语言生成的不确定性量化在黑盒约束下是关键且未充分探索的挑战。

Method: 引入TECP框架，利用词元级熵作为无对数、无参考的不确定性度量，并集成到分裂共形预测管道中。

Result: 在六个大语言模型和两个基准测试中，TECP持续实现可靠覆盖和紧凑预测集，优于基于自我一致性的UQ方法。

Conclusion: TECP为黑盒大语言模型环境下的可信生成提供了有原则且高效的解决方案。

Abstract: Uncertainty quantification (UQ) for open-ended language generation remains a
critical yet underexplored challenge, especially under black-box constraints
where internal model signals are inaccessible. In this paper, we introduce
Token-Entropy Conformal Prediction (TECP), a novel framework that leverages
token-level entropy as a logit-free, reference-free uncertainty measure and
integrates it into a split conformal prediction (CP) pipeline to construct
prediction sets with formal coverage guarantees. Unlike existing approaches
that rely on semantic consistency heuristics or white-box features, TECP
directly estimates epistemic uncertainty from the token entropy structure of
sampled generations and calibrates uncertainty thresholds via CP quantiles to
ensure provable error control. Empirical evaluations across six large language
models and two benchmarks (CoQA and TriviaQA) demonstrate that TECP
consistently achieves reliable coverage and compact prediction sets,
outperforming prior self-consistency-based UQ methods. Our method provides a
principled and efficient solution for trustworthy generation in black-box LLM
settings.

</details>


### [728] [Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting](https://arxiv.org/abs/2509.00482)
*Saksorn Ruangtanusak,Pittawat Taveekitworachai,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 研究提示工具增强大语言模型充当角色扮演对话代理的方法，规则基角色提示法表现最佳并开源相关代码。


<details>
  <summary>Details</summary>
Motivation: 解决对话代理常出现的过度发言和工具使用不当问题。

Method: 探索四种提示方法，包括基本角色提示、人工编写角色提示、自动提示优化和规则基角色提示。

Result: 规则基角色提示法通过新技巧取得0.571的总分，高于零样本基线分数。

Conclusion: 规则基角色提示设计比自动提示优化等复杂方法更能提升角色扮演对话代理的有效性和可靠性。

Abstract: This report investigates approaches for prompting a tool-augmented large
language model (LLM) to act as a role-playing dialogue agent in the API track
of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this
setting, dialogue agents often produce overly long in-character responses
(over-speaking) while failing to use tools effectively according to the persona
(under-acting), such as generating function calls that do not exist or making
unnecessary tool calls before answering. We explore four prompting approaches
to address these issues: 1) basic role prompting, 2) human-crafted role
prompting, 3) automatic prompt optimization (APO), and 4) rule-based role
prompting. The rule-based role prompting (RRP) approach achieved the best
performance through two novel techniques--character-card/scene-contract design
and strict enforcement of function calling--which led to an overall score of
0.571, improving on the zero-shot baseline score of 0.519. These findings
demonstrate that RRP design can substantially improve the effectiveness and
reliability of role-playing dialogue agents compared with more elaborate
methods such as APO. To support future efforts in developing persona prompts,
we are open-sourcing all of our best-performing prompts and the APO tool.
Source code is available at https://github.com/scb-10x/apo.

</details>


### [729] [ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics](https://arxiv.org/abs/2509.00496)
*Li S. Yifei,Allen Chang,Chaitanya Malaviya,Mark Yatskar*

Main category: cs.CL

TL;DR: 本文介绍了用于评估大语言模型系统的资源ResearchQA，用其评估18个系统，分析能力差距并发布数据。


<details>
  <summary>Details</summary>
Motivation: 现有长文本研究查询评估依赖专家注释，范围受限，而研究专业知识广泛存在于各领域，需要更全面的评估资源。

Method: 从75个研究领域的综述文章中提炼出21K查询和160K评分项，由31位不同领域的博士注释，构建自动成对评判器。

Result: 96%的查询支持博士信息需求，87%的评分项需系统用句子以上长度回应；自动评判器与专家判断一致性达74%；评估18个系统，无系统覆盖评分项超70%，最高排名系统覆盖75%；最高排名系统对引用、局限性和比较评分项的完整处理率分别低于11%、48%和49%。

Conclusion: 发布ResearchQA数据以促进更全面的多领域评估。

Abstract: Evaluating long-form responses to research queries heavily relies on expert
annotators, restricting attention to areas like AI where researchers can
conveniently enlist colleagues. Yet, research expertise is widespread: survey
articles synthesize knowledge distributed across the literature. We introduce
ResearchQA, a resource for evaluating LLM systems by distilling survey articles
from 75 research fields into 21K queries and 160K rubric items. Each rubric,
derived jointly with queries from survey sections, lists query-specific answer
evaluation criteria, i.e., citing papers, making explanations, and describing
limitations. Assessments by 31 Ph.D. annotators in 8 fields indicate 96% of
queries support Ph.D. information needs and 87% of rubric items should be
addressed in system responses by a sentence or more. Using our rubrics, we are
able to construct an automatic pairwise judge obtaining 74% agreement with
expert judgments. We leverage ResearchQA to analyze competency gaps in 18
systems in over 7.6K pairwise evaluations. No parametric or retrieval-augmented
system we evaluate exceeds 70% on covering rubric items, and the
highest-ranking agentic system shows 75% coverage. Error analysis reveals that
the highest-ranking system fully addresses less than 11% of citation rubric
items, 48% of limitation items, and 49% of comparison items. We release our
data to facilitate more comprehensive multi-field evaluations.

</details>


### [730] [A Multi-Strategy Approach for AI-Generated Text Detection](https://arxiv.org/abs/2509.00623)
*Ali Zain,Sareem Farooqui,Muhammad Rafi*

Main category: cs.CL

TL;DR: 本文为M - DAIGT共享任务开发三个检测新闻文章和学术摘要中AI生成内容的系统，RoBERTa系统表现最佳。


<details>
  <summary>Details</summary>
Motivation: 完成M - DAIGT共享任务，检测新闻文章和学术摘要中的AI生成内容。

Method: 开发三个系统，分别是微调的RoBERTa - base分类器、TF - IDF + SVM分类器和名为Candace的集成模型。

Result: RoBERTa - base系统表现最佳，在开发集和测试集上接近完美。

Conclusion: RoBERTa - base系统在检测AI生成内容方面有很好的效果。

Abstract: This paper presents presents three distinct systems developed for the M-DAIGT
shared task on detecting AI generated content in news articles and academic
abstracts. The systems includes: (1) A fine-tuned RoBERTa-base classifier, (2)
A classical TF-IDF + Support Vector Machine (SVM) classifier , and (3) An
Innovative ensemble model named Candace, leveraging probabilistic features
extracted from multiple Llama-3.2 models processed by a customTransformer
encoder.The RoBERTa-based system emerged as the most performant, achieving
near-perfect results on both development and test sets.

</details>


### [731] [Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs](https://arxiv.org/abs/2509.00707)
*Daehoon Gwak,Minseo Jung,Junwoo Park,Minho Park,ChaeHun Park,Junha Hyung,Jaegul Choo*

Main category: cs.CL

TL;DR: 提出Reward - Weighted Sampling (RWS) 解码策略提升掩码扩散模型非自回归生成能力，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 标准解码方法使掩码扩散模型生成顺序类似自回归过程，限制非自回归建模优势，需改进。

Method: 提出RWS策略，在迭代扩散过程中利用外部奖励模型提供全局信号，评估中间序列质量并缩放token对数概率，引导token选择。

Result: RWS显著促进非自回归生成顺序，多个评估指标有提升。

Conclusion: 整合全局信号能增强掩码扩散模型非自回归特性和整体性能。

Abstract: Masked diffusion models (MDMs) offer a promising non-autoregressive
alternative for large language modeling. Standard decoding methods for MDMs,
such as confidence-based sampling, select tokens independently based on
individual token confidences at each diffusion step. However, we observe that
this independent token selection often results in generation orders resembling
sequential autoregressive processes, limiting the advantages of
non-autoregressive modeling. To mitigate this pheonomenon, we propose
Reward-Weighted Sampling (RWS), a novel decoding strategy that leverages an
external reward model to provide a principled global signal during the
iterative diffusion process. Specifically, at each diffusion step, RWS
evaluates the quality of the entire intermediate sequence and scales token
logits accordingly, guiding token selection by integrating global
sequence-level coherence. This method selectively increases the confidence of
tokens that initially have lower scores, thereby promoting a more
non-autoregressive generation order. Furthermore, we provide theoretical
justification showing that reward-weighted logit scaling induces beneficial
rank reversals in token selection and consistently improves expected reward.
Experiments demonstrate that RWS significantly promotes non-autoregressive
generation orders, leading to improvements across multiple evaluation metrics.
These results highlight the effectiveness of integrating global signals in
enhancing both the non-autoregressive properties and overall performance of
MDMs.

</details>


### [732] [LegalChainReasoner: A Legal Chain-guided Framework for Criminal Judicial Opinion Generation](https://arxiv.org/abs/2509.00783)
*Weizhe Shi,Qiqi Wang,Yihong Pan,Qian Liu,Kaiqi Zhao*

Main category: cs.CL

TL;DR: 提出司法意见生成任务和LegalChainReasoner框架，在两个数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究将法律推理和量刑预测分为两个孤立子任务，导致推理与预测不一致，且依赖手动知识实用性有限，为解决这些问题开展研究。

Method: 提出司法意见生成任务，引入LegalChainReasoner框架，运用结构化法律链指导模型进行全面案例评估，集成事实前提、复合法律条件和量刑结论。

Result: 在两个真实开源中文法律案例数据集上实验，方法优于基线模型。

Conclusion: 所提方法能解决现有研究问题，更好契合法律实践。

Abstract: A criminal judicial opinion represents the judge's disposition of a case,
including the decision rationale and sentencing. Automatically generating such
opinions can assist in analyzing sentencing consistency and provide judges with
references to similar past cases. However, current research typically
approaches this task by dividing it into two isolated subtasks: legal reasoning
and sentencing prediction. This separation often leads to inconsistency between
the reasoning and predictions, failing to meet real-world judicial
requirements. Furthermore, prior studies rely on manually curated knowledge to
enhance applicability, yet such methods remain limited in practical deployment.
To address these limitations and better align with legal practice, we propose a
new LegalAI task: Judicial Opinion Generation, which simultaneously produces
both legal reasoning and sentencing decisions. To achieve this, we introduce
LegalChainReasoner, a framework that applies structured legal chains to guide
the model through comprehensive case assessments. By integrating factual
premises, composite legal conditions, and sentencing conclusions, our approach
ensures flexible knowledge injection and end-to-end opinion generation.
Experiments on two real-world and open-source Chinese legal case datasets
demonstrate that our method outperforms baseline models.

</details>


### [733] [CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA](https://arxiv.org/abs/2509.00806)
*Reem Abdel-Salam,Mary Adewunmi,Modinat A. Abayomi*

Main category: cs.CL

TL;DR: 本文针对BioCreative IX共享任务的MedHopQA赛道，采用监督微调策略，用LLaMA 3 8B模型进行多跳生物医学问答实验，虽模型有一定领域理解能力，但精确匹配得分低，引入两阶段推理管道有部分改进，指出生物医学大模型应用中语义理解和精确答案评估的差距。


<details>
  <summary>Details</summary>
Motivation: 在将大语言模型应用于现实生物医学和医疗保健领域前，需对其复杂问答能力进行严格评估。

Method: 采用监督微调策略，基于LLaMA 3 8B模型，使用从BioASQ、MedQuAD和TREC等外部来源整理的生物医学问答数据集进行微调，探索三种实验设置，并引入两阶段推理管道提取精确短答案。

Result: 模型展现出较强领域理解能力，概念级准确率高达0.8，但精确匹配得分显著较低，两阶段推理管道有部分改进，但生成严格格式输出仍有挑战。

Conclusion: 生物医学大模型应用中语义理解和精确答案评估存在差距，需进一步研究输出控制和后处理策略。

Abstract: Large language models (LLMs) are increasingly evident for accurate question
answering across various domains. However, rigorous evaluation of their
performance on complex question-answering (QA) capabilities is essential before
deployment in real-world biomedical and healthcare applications. This paper
presents our approach to the MedHopQA track of the BioCreative IX shared task,
which focuses on multi-hop biomedical question answering involving diseases,
genes, and chemicals. We adopt a supervised fine-tuning strategy leveraging
LLaMA 3 8B, enhanced with a curated biomedical question-answer dataset compiled
from external sources including BioASQ, MedQuAD, and TREC. Three experimental
setups are explored: fine-tuning on combined short and long answers, short
answers only, and long answers only. While our models demonstrate strong domain
understanding, achieving concept-level accuracy scores of up to 0.8, their
Exact Match (EM) scores remain significantly lower, particularly in the test
phase. We introduce a two-stage inference pipeline for precise short-answer
extraction to mitigate verbosity and improve alignment with evaluation metrics.
Despite partial improvements, challenges persist in generating strictly
formatted outputs. Our findings highlight the gap between semantic
understanding and exact answer evaluation in biomedical LLM applications,
motivating further research in output control and post-processing strategies.

</details>


### [734] [MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework](https://arxiv.org/abs/2509.00934)
*Md Shahidul Salim,Lian Fu,Arav Adikesh Ramakrishnan,Zonghai Yao,Hong Yu*

Main category: cs.CL

TL;DR: 提出MedCOD框架，结合特定领域知识提升大语言模型英西医学翻译质量，实验表明效果显著。


<details>
  <summary>Details</summary>
Motivation: 提升英语到西班牙语的医学翻译质量。

Method: 构建英语 - 西班牙语平行语料和测试集，使用结合多语言变体、医学同义词和UMLS定义的结构化提示，并结合基于LoRA的微调，对四个开源大语言模型进行评估。

Result: MedCOD显著提升所有模型的翻译质量，如Phi - 4在多项指标上超越GPT - 4o等基线模型，消融实验表明提示和模型适配都对性能提升有贡献。

Conclusion: 结构化知识集成在医学翻译任务中提升大语言模型性能有潜力。

Abstract: We present MedCOD (Medical Chain-of-Dictionary), a hybrid framework designed
to improve English-to-Spanish medical translation by integrating
domain-specific structured knowledge into large language models (LLMs). MedCOD
integrates domain-specific knowledge from both the Unified Medical Language
System (UMLS) and the LLM-as-Knowledge-Base (LLM-KB) paradigm to enhance
structured prompting and fine-tuning. We constructed a parallel corpus of 2,999
English-Spanish MedlinePlus articles and a 100-sentence test set annotated with
structured medical contexts. Four open-source LLMs (Phi-4, Qwen2.5-14B,
Qwen2.5-7B, and LLaMA-3.1-8B) were evaluated using structured prompts that
incorporated multilingual variants, medical synonyms, and UMLS-derived
definitions, combined with LoRA-based fine-tuning. Experimental results
demonstrate that MedCOD significantly improves translation quality across all
models. For example, Phi-4 with MedCOD and fine-tuning achieved BLEU 44.23,
chrF++ 28.91, and COMET 0.863, surpassing strong baseline models like GPT-4o
and GPT-4o-mini. Ablation studies confirm that both MedCOD prompting and model
adaptation independently contribute to performance gains, with their
combination yielding the highest improvements. These findings highlight the
potential of structured knowledge integration to enhance LLMs for medical
translation tasks.

</details>


### [735] [Structure and Destructure: Dual Forces in the Making of Knowledge Engines](https://arxiv.org/abs/2509.00949)
*Yihong Chen*

Main category: cs.CL

TL;DR: 论文探讨自然语言处理知识引擎构建的两种范式，建立二者概念联系，提出开发通用知识引擎新方法。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理知识引擎构建存在基于结构和非结构化数据两种看似不同的范式，要建立它们之间的概念联系。

Method: 分析两种范式中结构和非结构两种互补力量，结构组织已知符号交互，非结构通过周期性嵌入重置提升模型可塑性和泛化能力。

Result: 发现两种范式间的概念联系。

Conclusion: 这些联系为开发支持透明、可控和适应性智能系统的通用知识引擎提供了新方法。

Abstract: The making of knowledge engines in natural language processing has been
shaped by two seemingly distinct paradigms: one grounded in structure, the
other driven by massively available unstructured data. The structured paradigm
leverages predefined symbolic interactions, such as knowledge graphs, as priors
and designs models to capture them. In contrast, the unstructured paradigm
centers on scaling transformer architectures with increasingly vast data and
model sizes, as seen in modern large language models. Despite their divergence,
this thesis seeks to establish conceptual connections bridging these paradigms.
Two complementary forces, structure and destructure, emerge across both
paradigms: structure organizes seen symbolic interactions, while destructure,
through periodic embedding resets, improves model plasticity and generalization
to unseen scenarios. These connections form a new recipe for developing general
knowledge engines that can support transparent, controllable, and adaptable
intelligent systems.

</details>


### [736] [Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation](https://arxiv.org/abs/2509.01081)
*Abdessalam Bouchekif,Samer Rashwani,Heba Sbahi,Shahd Gaben,Mutez Al-Khatib,Mohammed Ghaly*

Main category: cs.CL

TL;DR: 评估大语言模型在伊斯兰继承法知识和推理能力，用1000道选择题测试7个模型，结果显示性能差距大，分析错误模式并指出改进方向。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在伊斯兰继承法（'ilm al - mawarith）的知识和推理能力。

Method: 使用1000道涵盖不同继承场景的选择题基准测试7个大语言模型。

Result: o3和Gemini 2.5准确率超90%，ALLaM、Fanar、LLaMA和Mistral低于50%，存在显著性能差距。

Conclusion: 大语言模型在处理结构化法律推理存在局限，给出了在伊斯兰法律推理中改进性能的方向。

Abstract: This paper evaluates the knowledge and reasoning capabilities of Large
Language Models in Islamic inheritance law, known as 'ilm al-mawarith. We
assess the performance of seven LLMs using a benchmark of 1,000 multiple-choice
questions covering diverse inheritance scenarios, designed to test models'
ability to understand the inheritance context and compute the distribution of
shares prescribed by Islamic jurisprudence. The results reveal a significant
performance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas
ALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect
important differences in reasoning ability and domain adaptation. We conduct a
detailed error analysis to identify recurring failure patterns across models,
including misunderstandings of inheritance scenarios, incorrect application of
legal rules, and insufficient domain knowledge. Our findings highlight
limitations in handling structured legal reasoning and suggest directions for
improving performance in Islamic legal reasoning. Code:
https://github.com/bouchekif/inheritance_evaluation

</details>


### [737] [REFRAG: Rethinking RAG based Decoding](https://arxiv.org/abs/2509.01092)
*Xiaoqiang Lin,Aritra Ghosh,Bryan Kian Hsiang Low,Anshumali Shrivastava,Vijai Mohan*

Main category: cs.CL

TL;DR: 现有大语言模型处理长上下文输入有系统延迟和内存问题，本文提出REFRAG框架，在不损失性能下提升RAG应用的延迟，还能扩展上下文大小，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长上下文输入时存在系统延迟和内存需求大的问题，RAG有特殊的注意力模式，多数计算不必要。

Method: 提出REFRAG框架，通过压缩、感知和扩展操作，利用稀疏结构提升性能。

Result: 实现30.85倍的首词生成时间加速，将上下文大小扩展16倍，实验显示在不同任务和数据集上比基线模型有显著加速且无精度损失。

Conclusion: REFRAG框架能有效提升RAG等长上下文任务的效率，且不损失准确性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
leveraging extensive external knowledge to enhance responses in multi-turn and
agentic applications, such as retrieval-augmented generation (RAG). However,
processing long-context inputs introduces significant system latency and
demands substantial memory for the key-value cache, resulting in reduced
throughput and a fundamental trade-off between knowledge enrichment and system
efficiency. While minimizing latency for long-context inputs is a primary
objective for LLMs, we contend that RAG require specialized consideration. In
RAG, much of the LLM context consists of concatenated passages from retrieval,
with only a small subset directly relevant to the query. These passages often
exhibit low semantic similarity due to diversity or deduplication during
re-ranking, leading to block-diagonal attention patterns that differ from those
in standard LLM generation tasks. Based on this observation, we argue that most
computations over the RAG context during decoding are unnecessary and can be
eliminated with minimal impact on performance. To this end, we propose REFRAG,
an efficient decoding framework that compresses, senses, and expands to improve
latency in RAG applications. By exploiting the sparsity structure, we
demonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to
previous work) without loss in perplexity. In addition, our optimization
framework for large context enables REFRAG to extend the context size of LLMs
by 16. We provide rigorous validation of REFRAG across diverse long-context
tasks, including RAG, multi-turn conversations, and long document
summarization, spanning a wide range of datasets. Experimental results confirm
that REFRAG delivers substantial speedup with no loss in accuracy compared to
LLaMA models and other state-of-the-art baselines across various context sizes.

</details>


### [738] [Natural Context Drift Undermines the Natural Language Understanding of Large Language Models](https://arxiv.org/abs/2509.01093)
*Yulong Wu,Viktor Schlegel,Riza Batista-Navarro*

Main category: cs.CL

TL;DR: 研究自然演变的上下文段落对生成式大语言模型问答的影响，发现文本自然演变使模型性能下降。


<details>
  <summary>Details</summary>
Motivation: 探究自然演变的上下文段落对生成式大语言模型问答的影响。

Method: 提出一个框架，对当代问答基准中的阅读段落的自然演变、人工编辑变体进行整理，并分析大语言模型在一系列语义相似度分数上的表现，评估六个问答数据集和八个有公开训练数据的大语言模型。

Result: 大语言模型性能随着阅读段落与预训练期间遇到的版本自然偏离而下降，如BoolQ上平均模型准确率从最高到最低相似度区间下降超30%。

Conclusion: 自然文本演变对大语言模型的语言理解能力构成重大挑战。

Abstract: How does the natural evolution of context paragraphs affect question
answering in generative Large Language Models (LLMs)? To investigate this, we
propose a framework for curating naturally evolved, human-edited variants of
reading passages from contemporary QA benchmarks and for analyzing LLM
performance across a range of semantic similarity scores, which quantify how
closely each variant aligns with content seen during pretraining. Using this
framework, we evaluate six QA datasets and eight LLMs with publicly available
training data. Our experiments reveal that LLM performance declines as reading
passages naturally diverge from the versions encountered during
pretraining-even when the question and all necessary information remains
present at inference time. For instance, average model accuracy on BoolQ drops
by over 30% from the highest to lowest similarity bins, with slopes exceeding
70 across several LLMs. These findings suggest that natural text evolution
poses a significant challenge to the language understanding capabilities of
LLMs.

</details>


### [739] [Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning](https://arxiv.org/abs/2509.01166)
*Yu Liu,Yanan Cao,Xixun Lin,Yanmin Shang,Shi Wang,Shirui Pan*

Main category: cs.CL

TL;DR: 提出SAT框架解决现有LLM增强KGC方法的问题，实验显示其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM增强KGC方法存在忽视表示空间不一致、不同任务指令设计重复耗时的问题。

Method: 提出SAT框架，先通过多任务对比学习进行层次知识对齐，再用统一图指令和轻量级知识适配器进行结构指令调优。

Result: 在两个KGC任务、四个基准数据集上实验，SAT显著优于现有方法，链接预测任务提升8.7% - 29.8%。

Conclusion: SAT框架有效，能解决现有LLM增强KGC方法的问题。

Abstract: Knowledge graph completion (KGC) aims to infer new knowledge and make
predictions from knowledge graphs. Recently, large language models (LLMs) have
exhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily
focus on designing task-specific instructions, achieving promising
advancements. However, there are still two critical challenges. First, existing
methods often ignore the inconsistent representation spaces between natural
language and graph structures. Second, most approaches design separate
instructions for different KGC tasks, leading to duplicate works and
time-consuming processes. To address these challenges, we propose SAT, a novel
framework that enhances LLMs for KGC via structure-aware alignment-tuning.
Specifically, we first introduce hierarchical knowledge alignment to align
graph embeddings with the natural language space through multi-task contrastive
learning. Then, we propose structural instruction tuning to guide LLMs in
performing structure-aware reasoning over KGs, using a unified graph
instruction combined with a lightweight knowledge adapter. Experimental results
on two KGC tasks across four benchmark datasets demonstrate that SAT
significantly outperforms state-of-the-art methods, especially in the link
prediction task with improvements ranging from 8.7% to 29.8%.

</details>


### [740] [CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning](https://arxiv.org/abs/2509.00457)
*Salah Eddine Bekhouche,Abdellah Zakaria Sellam,Hichem Telli,Cosimo Distante,Abdenour Hadid*

Main category: cs.CL

TL;DR: 提出轻量级框架解决伊斯兰继承选择题，对比不同模型，小模型有实用优势。


<details>
  <summary>Details</summary>
Motivation: 伊斯兰继承法律对继承人识别和份额计算的精确要求给AI带来挑战，需要有效解决方案。

Method: 使用专门的阿拉伯文本编码器和注意力相关性评分（ARS）构建轻量级框架，对答案选项按语义相关性排序。

Result: 评估了阿拉伯编码器和基于API的大语言模型，大模型最高准确率87.6%，基于MARBERT的方法准确率69.87%。

Conclusion: 量化了大模型的峰值性能与小型专用系统实用优势之间的关键权衡。

Abstract: Islamic inheritance law (Ilm al-Mawarith) requires precise identification of
heirs and calculation of shares, which poses a challenge for AI. In this paper,
we present a lightweight framework for solving multiple-choice inheritance
questions using a specialised Arabic text encoder and Attentive Relevance
Scoring (ARS). The system ranks answer options according to semantic relevance,
and enables fast, on-device inference without generative reasoning. We evaluate
Arabic encoders (MARBERT, ArabicBERT, AraBERT) and compare them with API-based
LLMs (Gemini, DeepSeek) on the QIAS 2025 dataset. While large models achieve an
accuracy of up to 87.6%, they require more resources and are context-dependent.
Our MARBERT-based approach achieves 69.87% accuracy, presenting a compelling
case for efficiency, on-device deployability, and privacy. While this is lower
than the 87.6% achieved by the best-performing LLM, our work quantifies a
critical trade-off between the peak performance of large models and the
practical advantages of smaller, specialized systems in high-stakes domains.

</details>


### [741] [Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation](https://arxiv.org/abs/2509.01185)
*Seganrasan Subramanian,Abhigya Verma*

Main category: cs.CL

TL;DR: 介绍通过与大语言模型基于提示交互生成合成长上下文数据的框架，支持多目标，含四种生成范式，利于创建数据集提升大语言模型长上下文能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理和推理长文本输入能力对实际应用至关重要，但缺少适用于训练和评估的高质量、多样且可验证的长上下文数据集，限制该领域进展。

Method: 引入模块化、可扩展框架，通过与大语言模型基于提示交互生成合成长上下文数据，支持多种训练和对齐目标，包含四种核心生成范式，采用模板化提示、模型无关架构和元数据丰富输出。

Result: 无明确提及具体结果。

Conclusion: 该方法有助于可扩展、可控且符合目标的数据集创建，推动大语言模型长上下文能力发展。

Abstract: The ability of large language models (LLMs) to process and reason over long
textual inputs is critical for a wide range of real-world applications.
However, progress in this area is significantly constrained by the absence of
high-quality, diverse, and verifiable long-context datasets suitable for both
training and evaluation. This work introduces a modular, extensible framework
for synthetic long-context data generation via prompt-based interaction with
LLMs. The framework supports multiple training and alignment objectives,
including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),
and Group Relative Policy Optimization (GRPO). It encompasses four core
generation paradigms: multi-turn conversational dialogues, document-grounded
input-output pairs, verifiable instruction-response tasks, and long-context
reasoning examples. Through templated prompting, a model-agnostic architecture,
and metadata-enriched outputs, the proposed approach facilitates scalable,
controllable, and purpose-aligned dataset creation for advancing long-context
capabilities in LLMs.

</details>


### [742] [Statutory Construction and Interpretation for Artificial Intelligence](https://arxiv.org/abs/2509.01186)
*Luxi He,Nimra Nadeem,Michel Liao,Howard Chen,Danqi Chen,Mariano-Florentino Cuéllar,Peter Henderson*

Main category: cs.CL

TL;DR: 本文探讨AI系统自然语言原则的解释歧义问题，借鉴法律理论提出计算框架并验证其有效性，为管理歧义迈出第一步。


<details>
  <summary>Details</summary>
Motivation: AI系统依赖自然语言原则存在解释歧义问题，且现有AI对齐管道缺乏类似法律系统的保障机制，不同解释会导致模型行为不稳定。

Method: 借鉴法律理论，分析当前对齐管道的关键差距，提出规则细化管道和基于提示的解释约束两种机制，并在WildChat数据集上评估。

Result: 在WildChat数据集的5000个场景子集上评估，两种干预措施显著提高了合理解释者小组的判断一致性。

Conclusion: 该方法为系统管理解释歧义迈出第一步，对构建更强大、守法的AI系统至关重要。

Abstract: AI systems are increasingly governed by natural language principles, yet a
key challenge arising from reliance on language remains underexplored:
interpretive ambiguity. As in legal systems, ambiguity arises both from how
these principles are written and how they are applied. But while legal systems
use institutional safeguards to manage such ambiguity, such as transparent
appellate review policing interpretive constraints, AI alignment pipelines
offer no comparable protections. Different interpretations of the same rule can
lead to inconsistent or unstable model behavior. Drawing on legal theory, we
identify key gaps in current alignment pipelines by examining how legal systems
constrain ambiguity at both the rule creation and rule application steps. We
then propose a computational framework that mirrors two legal mechanisms: (1) a
rule refinement pipeline that minimizes interpretive disagreement by revising
ambiguous rules (analogous to agency rulemaking or iterative legislative
action), and (2) prompt-based interpretive constraints that reduce
inconsistency in rule application (analogous to legal canons that guide
judicial discretion). We evaluate our framework on a 5,000-scenario subset of
the WildChat dataset and show that both interventions significantly improve
judgment consistency across a panel of reasonable interpreters. Our approach
offers a first step toward systematically managing interpretive ambiguity, an
essential step for building more robust, law-following AI systems.

</details>


### [743] [DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Taks Based on Data and Model Compression](https://arxiv.org/abs/2509.01221)
*Wei Huang,Huang Wei,Yinggui Wang*

Main category: cs.CL

TL;DR: 提出DaMoC框架解决选择微调大语言模型难题，在四个数据集实验中能选最优模型并节省约20倍训练时间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在特定领域任务需微调，但选择微调的最佳模型有挑战，需快速识别最优模型。

Method: 在数据层面，建立数据过滤方法分类，实现文本关键令牌压缩和表达优化；在模型层面，用层相似度分数评估层重要性并移除低重要性层，引入稀疏合并范式。

Result: 在四个数据集上的实验表明，能选择最优大语言模型，节省约20倍训练时间。

Conclusion: DaMoC框架有效解决了选择微调大语言模型的难题，能节省大量训练时间。

Abstract: Large language models (LLMs) excel in general tasks but struggle with
domain-specific ones, requiring fine-tuning with specific data. With many
open-source LLMs available, selecting the best model for fine-tuning downstream
tasks is challenging, primarily focusing on how to quickly identify the optimal
LLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses
this challenge by: 1) Data Level: A systematic categorization of data filtering
methodologies for LLMs is first established, classifying them into three
distinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,
and (3) hybrid approaches considering both dimensions. Further, we enhance the
density of key tokens in the text achieving token compression. Subsequently, we
use an LLM to iterative rewrite the text to optimize its expression. 2) Model
Level: We use layer similarity scores to assess each layer's importance and
remove those with lower importance. Then, we introduce a sparse merging
paradigm to preserve as much of the original model's capability as possible.
Extensive experiments on four datasets, medical Q&A, financial Q&A, general
Q&A, and reading comprehension, show that we can select the optimal LLM while
saving approximately 20-fold in training time.

</details>


### [744] [Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors](https://arxiv.org/abs/2509.01236)
*Hao Yang,Zhiyu Yang,Yunjie Zhang,Shanyi Zhu,Lin Yang*

Main category: cs.CL

TL;DR: 文章从上下文学习与预训练先验的双重关系角度探索思维链推理工作机制，实验有三点关键发现。


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽受关注但其潜在机制不明，需探索其工作机制。

Method: 对理由进行细粒度词法分析、逐步引入噪声示例、研究提示工程能否诱导大语言模型慢思考。

Result: 模型能学习推理结构和逻辑模式但依赖预训练先验；足够示例可使决策从预训练先验转向上下文信号，误导提示会引入不稳定性；长思维链提示可提升下游任务表现。

Conclusion: 揭示了思维链推理在模型学习、决策和任务表现等方面的工作机制。

Abstract: Chain-of-Thought reasoning has emerged as a pivotal methodology for enhancing
model inference capabilities. Despite growing interest in Chain-of-Thought
reasoning, its underlying mechanisms remain unclear. This paper explores the
working mechanisms of Chain-of-Thought reasoning from the perspective of the
dual relationship between in-context learning and pretrained priors. We first
conduct a fine-grained lexical-level analysis of rationales to examine the
model's reasoning behavior. Then, by incrementally introducing noisy exemplars,
we examine how the model balances pretrained priors against erroneous
in-context information. Finally, we investigate whether prompt engineering can
induce slow thinking in large language models. Our extensive experiments reveal
three key findings: (1) The model not only quickly learns the reasoning
structure at the lexical level but also grasps deeper logical reasoning
patterns, yet it heavily relies on pretrained priors. (2) Providing sufficient
exemplars shifts the model's decision-making from pretrained priors to
in-context signals, while misleading prompts introduce instability. (3) Long
Chain-of-Thought prompting can induce the model to generate longer reasoning
chains, thereby improving its performance on downstream tasks.

</details>


### [745] [Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling](https://arxiv.org/abs/2509.00605)
*Rishiraj Acharya*

Main category: cs.CL

TL;DR: 提出GAM网络用于序列建模，复杂度为线性，实验显示其比基线模型更快且验证困惑度优。


<details>
  <summary>Details</summary>
Motivation: Transformer架构处理长上下文时计算复杂度为二次方，存在瓶颈，需新架构解决。

Method: 提出GAM网络，用因果卷积和关联记忆检索机制替代自注意力层，通过门控机制融合信息，并进行多数据集对比实验。

Result: GAM速度更快，训练速度优于基线模型，在各数据集上验证困惑度表现好。

Conclusion: GAM是序列建模有前景且高效的替代方案。

Abstract: The Transformer architecture, underpinned by the self-attention mechanism,
has become the de facto standard for sequence modeling tasks. However, its core
computational primitive scales quadratically with sequence length (O(N^2)),
creating a significant bottleneck for processing long contexts. In this paper,
we propose the Gated Associative Memory (GAM) network, a novel, fully parallel
architecture for sequence modeling that exhibits linear complexity (O(N)) with
respect to sequence length. The GAM block replaces the self-attention layer
with two parallel pathways: a causal convolution to efficiently capture local,
position-dependent context, and a parallel associative memory retrieval
mechanism to model global, content-based patterns. These pathways are
dynamically fused using a gating mechanism, allowing the model to flexibly
combine local and global information for each token. We implement GAM from
scratch and conduct a rigorous comparative analysis against a standard
Transformer model and a modern linear-time baseline (Mamba) on the WikiText-2
benchmark, as well as against the Transformer on the TinyStories dataset. Our
experiments demonstrate that GAM is consistently faster, outperforming both
baselines on training speed, and achieves a superior or competitive final
validation perplexity across all datasets, establishing it as a promising and
efficient alternative for sequence modeling.

</details>


### [746] [LLMs cannot spot math errors, even when allowed to peek into the solution](https://arxiv.org/abs/2509.01395)
*KV Aditya Srivatsa,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 大语言模型处理数学应用题表现出色，但在元推理任务（如识别学生解答错误）有困难。研究用两个数据集研究定位分步解答中首个错误步骤的挑战，提出生成中间校正解答的方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在识别学生解答错误这类元推理任务上表现不佳，需研究定位分步解答中首个错误步骤的方法。

Method: 使用VtG和PRM800K两个错误推理数据集进行实验，并提出生成中间校正学生解答的方法。

Result: 实验表明，即使能获取参考答案，现有大语言模型也难以定位学生解答中的首个错误步骤；提出的方法有助于提升性能。

Conclusion: 生成更贴近原学生解答的中间校正解答的方法可改善大语言模型定位首个错误步骤的性能。

Abstract: Large language models (LLMs) demonstrate remarkable performance on math word
problems, yet they have been shown to struggle with meta-reasoning tasks such
as identifying errors in student solutions. In this work, we investigate the
challenge of locating the first error step in stepwise solutions using two
error reasoning datasets: VtG and PRM800K. Our experiments show that
state-of-the-art LLMs struggle to locate the first error step in student
solutions even when given access to the reference solution. To that end, we
propose an approach that generates an intermediate corrected student solution,
aligning more closely with the original student's solution, which helps improve
performance.

</details>


### [747] [Do Retrieval Augmented Language Models Know When They Don't Know?](https://arxiv.org/abs/2509.01476)
*Youchao Zhou,Heyan Huang,Yicheng Liu,Rui Dai,Xinglin Wang,Xingchen Zhang,Shumin Shi,Yang Deng*

Main category: cs.CL

TL;DR: 研究探讨RALMs是否知道自身未知信息，发现LLMs有过度拒绝行为，不同训练方法对过度拒绝问题影响不同，还开发了新拒绝方法提升答案质量。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注RALMs和拒绝后训练各自有效性，忽视RALMs拒绝能力评估，因此研究RALMs是否知道自身未知信息。

Method: 提出三个问题，检查RALMs在不同知识状态下的校准情况，研究拒绝后训练对过度拒绝问题的影响，开发新的拒绝方法。

Result: LLMs有显著过度拒绝行为；上下文微调缓解过度拒绝问题，R - tuning放大该问题；拒绝能力可能与答案质量冲突。

Conclusion: 研究为理解重要因素对RALM系统的影响提供了更全面视角。

Abstract: Existing Large Language Models (LLMs) occasionally generate plausible yet
factually incorrect responses, known as hallucinations. Researchers are
primarily using two approaches to mitigate hallucinations, namely Retrieval
Augmented Language Models (RALMs) and refusal post-training. However, current
research predominantly emphasizes their individual effectiveness while
overlooking the evaluation of the refusal capability of RALMs. In this study,
we ask the fundamental question: Do RALMs know when they don't know?
Specifically, we ask three questions. First, are RALMs well-calibrated
regarding different internal and external knowledge states? We examine the
influence of various factors. Contrary to expectations, we find that LLMs
exhibit significant \textbf{over-refusal} behavior. Then, how does refusal
post-training affect the over-refusal issue? We investigate the Refusal-aware
Instruction Tuning and In-Context Fine-tuning methods. Our results show that
the over-refusal problem is mitigated by In-context fine-tuning. but magnified
by R-tuning. However, we also find that the refusal ability may conflict with
the quality of the answer. Finally, we develop a simple yet effective refusal
method for refusal post-trained models to improve their overall answer quality
in terms of refusal and correct answers. Our study provides a more
comprehensive understanding of the influence of important factors on RALM
systems.

</details>


### [748] [MeVe: A Modular System for Memory Verification and Effective Context Control in Language Models](https://arxiv.org/abs/2509.01514)
*Andreas Ottem*

Main category: cs.CL

TL;DR: 论文提出MeVe模块化架构优化RAG系统，在知识问答任务中提升上下文效率，为LLM应用提供框架。


<details>
  <summary>Details</summary>
Motivation: RAG系统因简单的top - k语义搜索机制，常引入无关或冗余信息，降低性能和效率。

Method: 提出五阶段模块化设计MeVe，将检索和上下文组合过程分解为初始检索、相关性验证、后备检索、上下文优先级排序和令牌预算。

Result: 在英文维基百科子集知识问答任务中，与标准RAG实现相比，MeVe在维基数据集上上下文效率提升57%，在更复杂的HotpotQA数据集上提升75%。

Conclusion: MeVe为更可扩展和可靠的LLM应用提供框架，能更好地提炼上下文信息，提供更准确的事实支持。

Abstract: Retrieval-Augmented Generation (RAG) systems typically face constraints
because of their inherent mechanism: a simple top-k semantic search [1]. The
approach often leads to the incorporation of irrelevant or redundant
information in the context, degrading performance and efficiency [10][11]. This
paper presents MeVe, a novel modular architecture intended for Memory
Verification and smart context composition. MeVe rethinks the RAG paradigm by
proposing a five-phase modular design that distinctly breaks down the retrieval
and context composition process into distinct, auditable, and independently
tunable phases: initial retrieval, relevance verification, fallback retrieval,
context prioritization, and token budgeting. This architecture enables
fine-grained control of what knowledge is made available to an LLM, enabling
task-dependent filtering and adaptation. We release a reference implementation
of MeVe as a proof of concept and evaluate its performance on knowledge-heavy
QA tasks over a subset of English Wikipedia [22]. Our results demonstrate that
by actively verifying information before composition, MeVe significantly
improves context efficiency, achieving a 57% reduction on the Wikipedia dataset
and a 75% reduction on the more complex HotpotQA dataset compared to standard
RAG implementations [25]. This work provides a framework for more scalable and
reliable LLM applications. By refining and distilling contextual information,
MeVe offers a path toward better grounding and more accurate factual support
[16].

</details>


### [749] [CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models](https://arxiv.org/abs/2509.01535)
*Kairong Han,Wenshuo Zhao,Ziyu Zhao,JunJian Ye,Lujia Pan,Kun Kuang*

Main category: cs.CL

TL;DR: 研究大语言模型能否有效利用因果知识，提出Causal Attention Tuning方法注入因果知识，实验证明其在预测和分布外场景有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型直接在大规模数据上训练常捕捉虚假关联而非真实因果关系，在分布外场景表现不佳。

Method: 提出Causal Attention Tuning方法，利用人类先验自动生成token级因果信号，引入重注意力机制指导训练。

Result: 在Spurious Token Game基准和多个下游任务实验中，该方法能有效利用因果知识进行预测，在分布外场景保持稳健。

Conclusion: 所提方法可有效让大语言模型利用因果知识进行预测，在分布外场景表现良好。

Abstract: Large Language Models (LLMs) have achieved remarkable success across various
domains. However, a fundamental question remains: Can LLMs effectively utilize
causal knowledge for prediction and generation? Through empirical studies, we
find that LLMs trained directly on large-scale data often capture spurious
correlations rather than true causal relationships, leading to suboptimal
performance, especially in out-of-distribution (OOD) scenarios. To address this
challenge, we propose Causal Attention Tuning (CAT), a novel approach that
injects fine-grained causal knowledge into the attention mechanism. We propose
an automated pipeline that leverages human priors to automatically generate
token-level causal signals and introduce the Re-Attention mechanism to guide
training, helping the model focus on causal structures while mitigating noise
and biases in attention scores. Experimental results on our proposed Spurious
Token Game (STG) benchmark and multiple downstream tasks demonstrate that our
approach effectively leverages causal knowledge for prediction and remains
robust in OOD scenarios. Implementation details can be found at
https://github.com/Kairong-Han/CAT.

</details>


### [750] [In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents](https://arxiv.org/abs/2509.01560)
*Seungkyu Lee,Nalim Kim,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出利用API图解决工具代理调用API问题，引入In - N - Out数据集，提升了工具检索和多工具查询生成性能。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂度增加，工具代理难以正确识别和调用API，需解决该问题。

Method: 将API文档转换为结构化API图，引入专家标注的In - N - Out数据集。

Result: 使用In - N - Out显著提升工具检索和多工具查询生成性能，微调模型生成的图缩小了差距。

Conclusion: 使用显式API图对工具代理有前景，In - N - Out是有价值的资源，将公开数据集和代码。

Abstract: Tool agents -- LLM-based systems that interact with external APIs -- offer a
way to execute real-world tasks. However, as tasks become increasingly complex,
these agents struggle to identify and call the correct APIs in the proper
order. To tackle this problem, we investigate converting API documentation into
a structured API graph that captures API dependencies and leveraging it for
multi-tool queries that require compositional API calls. To support this, we
introduce In-N-Out, the first expert-annotated dataset of API graphs built from
two real-world API benchmarks and their documentation. Using In-N-Out
significantly improves performance on both tool retrieval and multi-tool query
generation, nearly doubling that of LLMs using documentation alone. Moreover,
graphs generated by models fine-tuned on In-N-Out close 90% of this gap,
showing that our dataset helps models learn to comprehend API documentation and
parameter relationships. Our findings highlight the promise of using explicit
API graphs for tool agents and the utility of In-N-Out as a valuable resource.
We will release the dataset and code publicly.

</details>


### [751] [Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry](https://arxiv.org/abs/2509.01620)
*Shanshan Wang,Junchao Wu,Fengying Ye,Jingming Yao,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本文提出检测大语言模型生成的现代中文诗歌的新基准，构建数据集评估六种检测器，结果表明现有检测器不可靠，验证了基准的有效性和必要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成文本难辨，现有检测工作未涉及现代中文诗歌，其特征使辨别困难，且扰乱诗歌生态，有识别的紧迫性。

Method: 构建包含专业诗人作品和大语言模型生成诗歌的高质量数据集，对六种检测器进行系统性能评估。

Result: 当前检测器不能作为检测大语言模型生成的现代中文诗歌的可靠工具，最难检测的诗歌特征是内在特质尤其是风格。

Conclusion: 验证了提出的基准的有效性和必要性，为未来检测AI生成诗歌奠定基础。

Abstract: The rapid development of advanced large language models (LLMs) has made
AI-generated text indistinguishable from human-written text. Previous work on
detecting AI-generated text has made effective progress, but has not involved
modern Chinese poetry. Due to the distinctive characteristics of modern Chinese
poetry, it is difficult to identify whether a poem originated from humans or
AI. The proliferation of AI-generated modern Chinese poetry has significantly
disrupted the poetry ecosystem. Based on the urgency of identifying
AI-generated poetry in the real Chinese world, this paper proposes a novel
benchmark for detecting LLMs-generated modern Chinese poetry. We first
construct a high-quality dataset, which includes both 800 poems written by six
professional poets and 41,600 poems generated by four mainstream LLMs.
Subsequently, we conduct systematic performance assessments of six detectors on
this dataset. Experimental results demonstrate that current detectors cannot be
used as reliable tools to detect modern Chinese poems generated by LLMs. The
most difficult poetic features to detect are intrinsic qualities, especially
style. The detection results verify the effectiveness and necessity of our
proposed benchmark. Our work lays a foundation for future detection of
AI-generated poetry.

</details>


### [752] [chDzDT: Word-level morphology-aware language model for Algerian social media text](https://arxiv.org/abs/2509.01772)
*Abdelkrime Aries*

Main category: cs.CL

TL;DR: 本文针对阿尔及利亚方言在预训练语言模型中代表性不足的问题，提出字符级预训练语言模型chDzDT，并进行多方面贡献，展示了字符级建模对形态丰富、资源少的方言的潜力。


<details>
  <summary>Details</summary>
Motivation: 阿尔及利亚方言在预训练语言模型中代表性不足，且其复杂特点使常规方法处理困难。

Method: 引入字符级预训练语言模型chDzDT，在孤立单词上训练，训练语料来自多种来源。

Result: 完成对阿尔及利亚方言的详细形态分析、构建多语言阿尔及利亚词汇数据集、开发并评估字符级PLM。

Conclusion: 字符级建模对形态丰富、资源少的方言有潜力，为更具包容性和适应性的NLP系统奠定基础。

Abstract: Pre-trained language models (PLMs) have substantially advanced natural
language processing by providing context-sensitive text representations.
However, the Algerian dialect remains under-represented, with few dedicated
models available. Processing this dialect is challenging due to its complex
morphology, frequent code-switching, multiple scripts, and strong lexical
influences from other languages. These characteristics complicate tokenization
and reduce the effectiveness of conventional word- or subword-level approaches.
  To address this gap, we introduce chDzDT, a character-level pre-trained
language model tailored for Algerian morphology. Unlike conventional PLMs that
rely on token sequences, chDzDT is trained on isolated words. This design
allows the model to encode morphological patterns robustly, without depending
on token boundaries or standardized orthography. The training corpus draws from
diverse sources, including YouTube comments, French, English, and Berber
Wikipedia, as well as the Tatoeba project. It covers multiple scripts and
linguistic varieties, resulting in a substantial pre-training workload.
  Our contributions are threefold: (i) a detailed morphological analysis of
Algerian dialect using YouTube comments; (ii) the construction of a
multilingual Algerian lexicon dataset; and (iii) the development and extensive
evaluation of a character-level PLM as a morphology-focused encoder for
downstream tasks. The proposed approach demonstrates the potential of
character-level modeling for morphologically rich, low-resource dialects and
lays a foundation for more inclusive and adaptable NLP systems.

</details>


### [753] [Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs](https://arxiv.org/abs/2509.01790)
*Andong Hua,Kenan Tang,Chenhe Gu,Jindong Gu,Eric Wong,Yao Qin*

Main category: cs.CL

TL;DR: 研究提示敏感性是否为大语言模型固有弱点，发现其多源于评估方法，模型对提示模板更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 探究广泛报道的高提示敏感性是大语言模型固有弱点，还是评估过程的人为结果。

Method: 对7个大语言模型在6个基准测试、12种提示模板下进行系统评估，对比启发式评估方法和大语言模型作为评判者的评估方法。

Result: 很多提示敏感性源于启发式评估方法，采用大语言模型作为评判者评估时，性能方差大幅降低，模型排名相关性更高。

Conclusion: 现代大语言模型对提示模板比之前认为的更鲁棒，提示敏感性可能更多是评估问题而非模型缺陷。

Abstract: Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e.,
repeating something written or spoken using different words) leads to
significant changes in large language model (LLM) performance, has been widely
accepted as a core limitation of LLMs. In this work, we revisit this issue and
ask: Is the widely reported high prompt sensitivity truly an inherent weakness
of LLMs, or is it largely an artifact of evaluation processes? To answer this
question, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family)
across 6 benchmarks, including both multiple-choice and open-ended tasks on 12
diverse prompt templates. We find that much of the prompt sensitivity stems
from heuristic evaluation methods, including log-likelihood scoring and rigid
answer matching, which often overlook semantically correct responses expressed
through alternative phrasings, such as synonyms or paraphrases. When we adopt
LLM-as-a-Judge evaluations, we observe a substantial reduction in performance
variance and a consistently higher correlation in model rankings across
prompts. Our findings suggest that modern LLMs are more robust to prompt
templates than previously believed, and that prompt sensitivity may be more an
artifact of evaluation than a flaw in the models.

</details>


### [754] [Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts](https://arxiv.org/abs/2509.01814)
*Shreyas Tirumala,Nishant Jain,Danny D. Leybzon,Trent D. Buskirk*

Main category: cs.CL

TL;DR: 探讨基于Transformer的大语言模型驱动的AI面试官在定量和定性研究数据收集中的适用性，评估其与IVR系统能力，发现AI面试官虽有优势但在定性数据收集中效用或因场景而异。


<details>
  <summary>Details</summary>
Motivation: 了解AI面试系统在定量和定性研究的数据收集场景中的适用性。

Method: 从输入/输出性能和语言推理两个维度评估AI面试官和当前交互式语音应答（IVR）系统的能力。

Result: 实地研究表明，AI面试官在定量和定性数据收集方面已超越IVR系统，但实时转录错误率、有限的情绪检测能力和不均衡的跟进质量等问题存在。

Conclusion: 当前AI面试官技术在定性数据收集工作中的效用、使用和采用可能因场景而异。

Abstract: Transformer-based Large Language Models (LLMs) have paved the way for "AI
interviewers" that can administer voice-based surveys with respondents in
real-time. This position paper reviews emerging evidence to understand when
such AI interviewing systems are fit for purpose for collecting data within
quantitative and qualitative research contexts. We evaluate the capabilities of
AI interviewers as well as current Interactive Voice Response (IVR) systems
across two dimensions: input/output performance (i.e., speech recognition,
answer recording, emotion handling) and verbal reasoning (i.e., ability to
probe, clarify, and handle branching logic). Field studies suggest that AI
interviewers already exceed IVR capabilities for both quantitative and
qualitative data collection, but real-time transcription error rates, limited
emotion detection abilities, and uneven follow-up quality indicate that the
utility, use and adoption of current AI interviewer technology may be
context-dependent for qualitative data collection efforts.

</details>


### [755] [Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning](https://arxiv.org/abs/2509.01885)
*Zhimeng Luo,Abhibha Gupta,Adam Frisch,Daqing He*

Main category: cs.CL

TL;DR: 本文提出利用大语言模型从电子健康记录中提取OPQRST评估信息，将任务从序列标注转为文本生成，还改进评估指标，提升了信息提取的准确性和可用性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法难以有效从电子健康记录中提取关键患者信息，影响临床应用。

Method: 将信息提取任务从序列标注转为文本生成，让模型模仿医生认知过程；修改传统命名实体识别指标，引入语义相似度度量。

Result: 实现了利用大语言模型从电子健康记录中提取OPQRST评估信息的方法，改进了评估指标。

Conclusion: 该研究推动了人工智能在医疗领域的应用，提供了可扩展解决方案，有助于临床决策和改善患者护理结果。

Abstract: The extraction of critical patient information from Electronic Health Records
(EHRs) poses significant challenges due to the complexity and unstructured
nature of the data. Traditional machine learning approaches often fail to
capture pertinent details efficiently, making it difficult for clinicians to
utilize these tools effectively in patient care. This paper introduces a novel
approach to extracting the OPQRST assessment from EHRs by leveraging the
capabilities of Large Language Models (LLMs). We propose to reframe the task
from sequence labeling to text generation, enabling the models to provide
reasoning steps that mimic a physician's cognitive processes. This approach
enhances interpretability and adapts to the limited availability of labeled
data in healthcare settings. Furthermore, we address the challenge of
evaluating the accuracy of machine-generated text in clinical contexts by
proposing a modification to traditional Named Entity Recognition (NER) metrics.
This includes the integration of semantic similarity measures, such as the BERT
Score, to assess the alignment between generated text and the clinical intent
of the original records. Our contributions demonstrate a significant
advancement in the use of AI in healthcare, offering a scalable solution that
improves the accuracy and usability of information extraction from EHRs,
thereby aiding clinicians in making more informed decisions and enhancing
patient care outcomes.

</details>


### [756] [TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring](https://arxiv.org/abs/2509.01640)
*Hind Aljuaid,Areej Alhothali,Ohoud Al-Zamzami,Hussein Assalahi*

Main category: cs.CL

TL;DR: 本文提出用于自动论文评分（AES）的TransGAT方法，结合微调Transformer模型与GNNs进行分析性评分，实验显示其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有AES方法使用静态词嵌入无法捕捉上下文含义，且多采用整体评分，忽略具体写作方面，因此需要改进。

Method: 提出TransGAT，将微调Transformer模型与图注意力网络（GAT）结合，进行双流预测并融合结果得出最终分析分数。

Result: 在ELLIPSE数据集上实验，TransGAT在所有分析评分维度上平均二次加权卡帕（QWK）达到0.854，优于基线模型。

Conclusion: TransGAT有潜力推动AES系统发展。

Abstract: Essay writing is a critical component of student assessment, yet manual
scoring is labor-intensive and inconsistent. Automated Essay Scoring (AES)
offers a promising alternative, but current approaches face limitations. Recent
studies have incorporated Graph Neural Networks (GNNs) into AES using static
word embeddings that fail to capture contextual meaning, especially for
polysemous words. Additionally, many methods rely on holistic scoring,
overlooking specific writing aspects such as grammar, vocabulary, and cohesion.
To address these challenges, this study proposes TransGAT, a novel approach
that integrates fine-tuned Transformer models with GNNs for analytic scoring.
TransGAT combines the contextual understanding of Transformers with the
relational modeling strength of Graph Attention Networks (GAT). It performs
two-stream predictions by pairing each fine-tuned Transformer (BERT, RoBERTa,
and DeBERTaV3) with a separate GAT. In each pair, the first stream generates
essay-level predictions, while the second applies GAT to Transformer token
embeddings, with edges constructed from syntactic dependencies. The model then
fuses predictions from both streams to produce the final analytic score.
Experiments on the ELLIPSE dataset show that TransGAT outperforms baseline
models, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across all
analytic scoring dimensions. These findings highlight the potential of TransGAT
to advance AES systems.

</details>


### [757] [DeepSeek performs better than other Large Language Models in Dental Cases](https://arxiv.org/abs/2509.02036)
*Hexian Zhang,Xinyu Yan,Yanqi Yang,Lijian Jin,Ping Yang,Junwen Wang*

Main category: cs.CL

TL;DR: 研究评估四个大语言模型分析纵向牙科病例能力，DeepSeek表现最佳，可用于医学教育和研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域潜力大，但解读纵向患者叙事能力待探索，牙科数据丰富，可评估其推理能力。

Method: 用34个标准化纵向牙周病例（258个问答对），通过自动指标和牙医盲评评估四个大语言模型（GPT - 4o、Gemini 2.0 Flash、Copilot和DeepSeek V3）。

Result: DeepSeek表现最佳，忠实度更高，专家评分更高，可读性未显著降低。

Conclusion: DeepSeek是病例分析领先模型，可作为辅助工具用于医学教育和研究，有成为特定领域代理的潜力。

Abstract: Large language models (LLMs) hold transformative potential in healthcare, yet
their capacity to interpret longitudinal patient narratives remains
inadequately explored. Dentistry, with its rich repository of structured
clinical data, presents a unique opportunity to rigorously assess LLMs'
reasoning abilities. While several commercial LLMs already exist, DeepSeek, a
model that gained significant attention earlier this year, has also joined the
competition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini
2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal
dental case vignettes through open-ended clinical tasks. Using 34 standardized
longitudinal periodontal cases (comprising 258 question-answer pairs), we
assessed model performance via automated metrics and blinded evaluations by
licensed dentists. DeepSeek emerged as the top performer, demonstrating
superior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert
ratings (median = 4.5/5 vs. 4.0/5), without significantly compromising
readability. Our study positions DeepSeek as the leading LLM for case analysis,
endorses its integration as an adjunct tool in both medical education and
research, and highlights its potential as a domain-specific agent.

</details>


### [758] [How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis](https://arxiv.org/abs/2509.02075)
*Elisabetta Rocchetti,Alfio Ferrara*

Main category: cs.CL

TL;DR: 研究基础模型和指令微调模型在英意长度控制文本生成上的差异，发现指令微调显著改善长度控制，组件策略可能适应语言环境。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在精确字数文本生成方面的挑战，研究基础模型和指令微调模型在长度控制文本生成上的差异。

Method: 使用基于直接对数归因的累积加权归因分析性能和内部组件贡献。

Result: 指令微调显著改善长度控制，英语中模型深层注意力头贡献增加，意大利语中最终层多层感知器作用更强。

Conclusion: 指令微调重新配置模型后期层以适应任务，组件策略可能因语言环境而异。

Abstract: Adhering to explicit length constraints, such as generating text with a
precise word count, remains a significant challenge for Large Language Models
(LLMs). This study aims at investigating the differences between foundation
models and their instruction-tuned counterparts, on length-controlled text
generation in English and Italian. We analyze both performance and internal
component contributions using Cumulative Weighted Attribution, a metric derived
from Direct Logit Attribution. Our findings reveal that instruction-tuning
substantially improves length control, primarily by specializing components in
deeper model layers. Specifically, attention heads in later layers of IT models
show increasingly positive contributions, particularly in English. In Italian,
while attention contributions are more attenuated, final-layer MLPs exhibit a
stronger positive role, suggesting a compensatory mechanism. These results
indicate that instruction-tuning reconfigures later layers for task adherence,
with component-level strategies potentially adapting to linguistic context.

</details>


### [759] [JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer](https://arxiv.org/abs/2509.02097)
*Zhichao Shi,Xuhui Jiang,Chengjin Xu,Cangli Yao,Zhenxin Huang,Shengjie Ma,Yinghan Shen,Yuanzhuo Wang*

Main category: cs.CL

TL;DR: 本文指出当前大语言模型评估范式存在问题，提出JudgeAgent评估框架并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估范式存在与目标交互有限、难度控制不足和结果验证困难等问题，难以精确确定目标模型的知识和能力边界。

Method: 提出基于面试官式评估范式的JudgeAgent框架，采用基准评分、交互式扩展和评估反馈的综合评估方法，利用知识驱动的数据合成和目标自适应难度调整方法进行扩展测试。

Result: 通过广泛实验验证了JudgeAgent及其动态评估范式的有效性。

Conclusion: JudgeAgent框架能有效解决当前大语言模型评估范式的问题，提供准确有效的评估结果。

Abstract: Evaluating the capabilities of large language models (LLMs) is an essential
step to ensure the successful application of LLMs across various domains. The
current evaluation of LLMs is based on a paradigm that involves querying them
with predefined question sets and assessing their outputs. This paradigm offers
controllable processes and simplicity, but faces challenges such as limited
interaction with targets, insufficient difficulty control, and difficulties in
verifying the validity of evaluation results, making it hard to precisely
determine the knowledge and capability boundaries of target models. To address
these challenges, we propose JudgeAgent, a knowledge-target adaptive dynamic
evaluation framework based on a new interviewer-style evaluation paradigm.
JudgeAgent employs a comprehensive evaluation approach consisting of benchmark
grading, interactive extension, and evaluation feedback. It utilizes
knowledge-driven data synthesis and target-adaptive difficulty adjustment
methods to conduct extended testing, providing accurate and effective
evaluation results. We also introduce a novel insight into validating
evaluation methods, demonstrating the effectiveness of JudgeAgent and its
dynamic evaluation paradigm through extensive experiments.

</details>


### [760] [Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in Low-Resource Philippine Languages](https://arxiv.org/abs/2509.02160)
*David Demitri Africa,Suchir Salhan,Yuval Weiss,Paula Buttery,Richard Diehl Martinez*

Main category: cs.CL

TL;DR: 研究小解码器语言模型用于低资源语言命名实体识别，用MAML改进目标，在Tagalog和Cebuano测试有提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调在低资源语言命名实体识别中因内存和延迟限制常不可行，探索小解码器语言模型能否快速适应和零样本迁移。

Method: 用一阶模型无关元学习（MAML）替代部分自回归目标。

Result: 在四种模型规模下，MAML提升零样本微F1值，减少收敛时间，对特定单令牌实体提升大。

Conclusion: MAML对小解码器语言模型在低资源语言命名实体识别有效，表面锚点很重要。

Abstract: Named-entity recognition (NER) in low-resource languages is usually tackled
by finetuning very large multilingual LMs, an option that is often infeasible
in memory- or latency-constrained settings. We ask whether small decoder LMs
can be pretrained so that they adapt quickly and transfer zero-shot to
languages unseen during pretraining. To this end we replace part of the
autoregressive objective with first-order model-agnostic meta-learning (MAML).
Tagalog and Cebuano are typologically similar yet structurally different in
their actor/non-actor voice systems, and hence serve as a challenging test-bed.
Across four model sizes (11 M - 570 M) MAML lifts zero-shot micro-F1 by 2-6 pp
under head-only tuning and 1-3 pp after full tuning, while cutting convergence
time by up to 8%. Gains are largest for single-token person entities that
co-occur with Tagalog case particles si/ni, highlighting the importance of
surface anchors.

</details>


### [761] [Avoidance Decoding for Diverse Multi-Branch Story Generation](https://arxiv.org/abs/2509.02170)
*Kyeongman Park,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: 提出避免解码策略解决大语言模型输出重复问题，提升输出多样性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在故事生成等任务中因创意多样性有限，输出重复单调，需解决此问题。

Method: 提出避免解码策略，通过惩罚与先前生成输出的相似度来修改词元对数，自适应平衡概念级和叙事级相似度惩罚。

Result: 与强基线相比，输出多样性最高提升2.6倍，平均减少30%重复，缓解文本退化，激活更多神经元。

Conclusion: 该方法能有效利用模型内在创造力，提升输出多样性。

Abstract: Large Language Models (LLMs) often generate repetitive and monotonous
outputs, especially in tasks like story generation, due to limited creative
diversity when given the same input prompt. To address this challenge, we
propose a novel decoding strategy, Avoidance Decoding, that modifies token
logits by penalizing similarity to previously generated outputs, thereby
encouraging more diverse multi-branch stories. This penalty adaptively balances
two similarity measures: (1) Concept-level Similarity Penalty, which is
prioritized in early stages to diversify initial story concepts, and (2)
Narrative-level Similarity Penalty, which is increasingly emphasized later to
ensure natural yet diverse plot development. Notably, our method achieves up to
2.6 times higher output diversity and reduces repetition by an average of 30%
compared to strong baselines, while effectively mitigating text degeneration.
Furthermore, we reveal that our method activates a broader range of neurons,
demonstrating that it leverages the model's intrinsic creativity.

</details>


### [762] [DCPO: Dynamic Clipping Policy Optimization](https://arxiv.org/abs/2509.02333)
*Shihui Yang,Chengfeng Dou,Peidong Guo,Kai Lu,Qiang Ju,Fei Deng,Rihui Xin*

Main category: cs.CL

TL;DR: 本文提出DCPO方法解决RLVR中现有方法GRPO的零梯度问题，在多个基准测试上取得SOTA性能，证明其高效利用生成数据进行强化学习的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法GRPO常出现零梯度问题，原因是固定的令牌级概率比裁剪边界和相同奖励的标准化，导致梯度更新无效和生成响应利用不足。

Method: 提出动态裁剪策略（DCPO），根据令牌特定先验概率自适应调整裁剪边界以增强令牌级探索，采用平滑优势标准化技术对累积训练步骤的奖励进行标准化以提高生成响应的响应级有效利用。

Result: DCPO在四个基于不同模型的基准测试中取得SOTA性能，在AIME24和AIME25基准测试上超越DAPO和GRPO，在四个模型中相比GRPO非零优势平均提高28%，训练效率比DAPO翻倍，显著降低令牌裁剪率。

Conclusion: DCPO能更有效地利用生成数据进行大语言模型的强化学习。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
promising framework for enhancing the reasoning capabilities of large language
models. However, existing approaches such as GRPO often suffer from zero
gradients. This problem arises primarily due to fixed clipping bounds for
token-level probability ratios and the standardization of identical rewards,
which can lead to ineffective gradient updates and underutilization of
generated responses. In this work, we propose Dynamic Clipping Policy
Optimization (DCPO), which introduces a dynamic clipping strategy that
adaptively adjusts the clipping bounds based on token-specific prior
probabilities to enhance token-level exploration, and a smooth advantage
standardization technique that standardizes rewards across cumulative training
steps to improve the response-level effective utilization of generated
responses. DCPO achieved state-of-the-art performance on four benchmarks based
on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under
greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24
benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the
Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO
achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO
(20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the
nonzero advantage over GRPO in four models, doubled the training efficiency
over DAPO, and significantly reduced the token clipping ratio by an order of
magnitude compared to both GRPO and DAPO, while achieving superior performance.
These results highlight DCPO's effectiveness in leveraging generated data more
efficiently for reinforcement learning in large language models.

</details>


### [763] [Implicit Reasoning in Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2509.02350)
*Jindong Li,Yali Fu,Li Fan,Jiahong Liu,Yao Shu,Chengwei Qin,Menglin Yang,Irwin King,Rex Ying*

Main category: cs.CL

TL;DR: 本文是关于大语言模型隐式推理的综述，提出执行范式分类，回顾证据和评估指标。


<details>
  <summary>Details</summary>
Motivation: 先前缺乏对大语言模型内部推理机制层面的专门研究，本文旨在填补这一空白。

Method: 以执行范式为中心引入分类法，将现有方法分为潜在优化、信号引导控制和层循环执行三种范式。

Result: 对支持大语言模型存在隐式推理的证据进行回顾，提供现有工作评估指标和基准的结构化概述。

Conclusion: 发布持续更新项目https://github.com/digailab/awesome-llm-implicit-reasoning。

Abstract: Large Language Models (LLMs) have demonstrated strong generalization across a
wide range of tasks. Reasoning with LLMs is central to solving multi-step
problems and complex decision-making. To support efficient reasoning, recent
studies have shifted attention from explicit chain-of-thought prompting toward
implicit reasoning, where reasoning occurs silently via latent structures
without emitting intermediate textual steps. Implicit reasoning brings
advantages such as lower generation cost, faster inference, and better
alignment with internal computation. Although prior surveys have discussed
latent representations in the context of reasoning, a dedicated and
mechanism-level examination of how reasoning unfolds internally within LLMs
remains absent. This survey fills that gap by introducing a taxonomy centered
on execution paradigms, shifting the focus from representational forms to
computational strategies. We organize existing methods into three execution
paradigms based on \textbf{\textit{how and where internal computation
unfolds}}: latent optimization, signal-guided control, and layer-recurrent
execution. We also review structural, behavioral and representation-based
evidence that supports the presence of implicit reasoning in LLMs. We further
provide a structured overview of the evaluation metrics and benchmarks used in
existing works to assess the effectiveness and reliability of implicit
reasoning.We maintain a continuously updated project at:
https://github.com/digailab/awesome-llm-implicit-reasoning.

</details>


### [764] [Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions](https://arxiv.org/abs/2509.02452)
*Seyedali Mohammadi,Bhaskara Hanuma Vedula,Hemank Lamba,Edward Raff,Ponnurangam Kumaraguru,Francis Ferraro,Manas Gaur*

Main category: cs.CL

TL;DR: 研究大语言模型是否融入外部定义，实验表明外部定义效果不稳定，很多情况依赖内部表征，不同任务表现有差异。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是真正融入外部定义，还是主要依赖参数知识。

Method: 在多个解释基准数据集和多种标签定义条件下进行对照实验。

Result: 明确标签定义能提高准确性和可解释性，但融入过程无保证且不稳定，模型常默认使用内部表征，特定领域任务更受益于外部定义。

Conclusion: 有必要深入理解大语言模型如何处理外部知识和已有能力。

Abstract: Do LLMs genuinely incorporate external definitions, or do they primarily rely
on their parametric knowledge? To address these questions, we conduct
controlled experiments across multiple explanation benchmark datasets (general
and domain-specific) and label definition conditions, including expert-curated,
LLM-generated, perturbed, and swapped definitions. Our results reveal that
while explicit label definitions can enhance accuracy and explainability, their
integration into an LLM's task-solving processes is neither guaranteed nor
consistent, suggesting reliance on internalized representations in many cases.
Models often default to their internal representations, particularly in general
tasks, whereas domain-specific tasks benefit more from explicit definitions.
These findings underscore the need for a deeper understanding of how LLMs
process external knowledge alongside their pre-existing capabilities.

</details>


### [765] [An Ensemble Classification Approach in A Multi-Layered Large Language Model Framework for Disease Prediction](https://arxiv.org/abs/2509.02446)
*Ali Hamdi,Malak Mohamed,Rokaia Emad,Khaled Shaban*

Main category: cs.CL

TL;DR: 本文评估阿拉伯语医疗文本预处理方法，结合微调阿拉伯语变压器模型和多数投票集成，用于阿拉伯语社交远程医疗数据疾病分类，取得80.56%准确率。


<details>
  <summary>Details</summary>
Motivation: 利用社交远程医疗产生的大量医疗数据进行疾病分类，当前大语言模型在处理复杂医疗文本有能力，需探索适用于阿拉伯语医疗数据的方法。

Method: 评估三种阿拉伯语医疗文本预处理方法，应用微调阿拉伯语变压器模型（CAMeLBERT、AraBERT和AsafayaBERT），采用多数投票集成结合原始和预处理文本表示的预测。

Result: 该方法取得了80.56%的最佳分类准确率。

Conclusion: 此方法能有效利用各种文本表示和模型预测来提高对医疗文本的理解，是首次将基于大语言模型的预处理与微调阿拉伯语变压器模型及集成学习用于阿拉伯语社交远程医疗数据疾病分类的工作。

Abstract: Social telehealth has made remarkable progress in healthcare by allowing
patients to post symptoms and participate in medical consultations remotely.
Users frequently post symptoms on social media and online health platforms,
creating a huge repository of medical data that can be leveraged for disease
classification. Large language models (LLMs) such as LLAMA3 and GPT-3.5, along
with transformer-based models like BERT, have demonstrated strong capabilities
in processing complex medical text. In this study, we evaluate three Arabic
medical text preprocessing methods such as summarization, refinement, and Named
Entity Recognition (NER) before applying fine-tuned Arabic transformer models
(CAMeLBERT, AraBERT, and AsafayaBERT). To enhance robustness, we adopt a
majority voting ensemble that combines predictions from original and
preprocessed text representations. This approach achieved the best
classification accuracy of 80.56%, thus showing its effectiveness in leveraging
various text representations and model predictions to improve the understanding
of medical texts. To the best of our knowledge, this is the first work that
integrates LLM-based preprocessing with fine-tuned Arabic transformer models
and ensemble learning for disease classification in Arabic social telehealth
data.

</details>


### [766] [EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling](https://arxiv.org/abs/2509.02450)
*Lingzhi Shen,Xiaohao Cai,Yunfei Long,Imran Razzak,Guanming Chen,Shoaib Jameel*

Main category: cs.CL

TL;DR: 提出自监督框架EmoPerso用于提升文本人格检测，在两基准数据集超现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有文本人格检测方法依赖大规模标注数据且忽视情感与人格交互。

Method: 提出EmoPerso框架，用生成机制进行数据增强和表征学习，提取伪标签情感特征，通过多任务学习联合优化，用交叉注意力模块捕捉交互，采用自学习策略迭代增强推理能力。

Result: 在两个基准数据集上的实验表明，EmoPerso超越了现有模型。

Conclusion: EmoPerso能有效提升文本人格检测效果。

Abstract: Personality detection from text is commonly performed by analysing users'
social media posts. However, existing methods heavily rely on large-scale
annotated datasets, making it challenging to obtain high-quality personality
labels. Moreover, most studies treat emotion and personality as independent
variables, overlooking their interactions. In this paper, we propose a novel
self-supervised framework, EmoPerso, which improves personality detection
through emotion-aware modelling. EmoPerso first leverages generative mechanisms
for synthetic data augmentation and rich representation learning. It then
extracts pseudo-labeled emotion features and jointly optimizes them with
personality prediction via multi-task learning. A cross-attention module is
employed to capture fine-grained interactions between personality traits and
the inferred emotional representations. To further refine relational reasoning,
EmoPerso adopts a self-taught strategy to enhance the model's reasoning
capabilities iteratively. Extensive experiments on two benchmark datasets
demonstrate that EmoPerso surpasses state-of-the-art models. The source code is
available at https://github.com/slz0925/EmoPerso.

</details>


### [767] [MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds](https://arxiv.org/abs/2509.02499)
*Junxi Wu,Jinpeng Wang,Zheng Liu,Bin Chen,Dongjian Hu,Hao Wu,Shu-Tao Xiu*

Main category: cs.CL

TL;DR: 提出MoSEs框架用于AI生成文本检测，相比基线有性能提升，低资源场景改善更明显。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展引发对潜在滥用的担忧，现有检测方法忽视风格建模且依赖静态阈值，限制检测性能，因此需构建可信检测系统。

Method: 提出MoSEs框架，包含SRR、SAR和CTE三个核心组件，通过CTE联合建模统计和语义特征动态确定最优阈值。

Result: 框架相比基线检测性能平均提升11.34%，低资源场景提升39.15%。

Conclusion: MoSEs框架能有效提升AI生成文本检测性能，尤其是在低资源场景。

Abstract: The rapid advancement of large language models has intensified public
concerns about the potential misuse. Therefore, it is important to build
trustworthy AI-generated text detection systems. Existing methods neglect
stylistic modeling and mostly rely on static thresholds, which greatly limits
the detection performance. In this paper, we propose the Mixture of Stylistic
Experts (MoSEs) framework that enables stylistics-aware uncertainty
quantification through conditional threshold estimation. MoSEs contain three
core components, namely, the Stylistics Reference Repository (SRR), the
Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE).
For input text, SRR can activate the appropriate reference data in SRR and
provide them to CTE. Subsequently, CTE jointly models the linguistic
statistical properties and semantic features to dynamically determine the
optimal threshold. With a discrimination score, MoSEs yields prediction labels
with the corresponding confidence level. Our framework achieves an average
improvement 11.34% in detection performance compared to baselines. More
inspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource
case. Our code is available at https://github.com/creator-xi/MoSEs.

</details>


### [768] [GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning](https://arxiv.org/abs/2509.02492)
*Chenglong Wang,Yongyu Mu,Hang Zhou,Yifu Huo,Ziming Zhu,Jiali Zeng,Murun Yang,Bei Li,Tong Xiao,Xiaoyang Hao,Chunliang Zhang,Fandong Meng,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出自训练方法开发GRAM - R² 生成式奖励模型，无需大量标注数据且能输出奖励理由，实验显示其性能出色。


<details>
  <summary>Details</summary>
Motivation: 当前开发有效奖励模型依赖大规模标注偏好数据，现有无标注数据预训练方法无法给奖励模型注入明确推理能力。

Method: 提出自训练方法，利用无标注数据激发奖励模型的奖励推理能力，开发GRAM - R²生成式奖励模型。

Result: GRAM - R²在响应排名、任务适应和基于人类反馈的强化学习实验中表现出色，优于多个判别式和生成式基线模型。

Conclusion: GRAM - R²可作为奖励推理的基础模型，能应用于广泛任务，支持下游应用。

Abstract: Significant progress in reward modeling over recent years has been driven by
a paradigm shift from task-specific designs towards generalist reward models.
Despite this trend, developing effective reward models remains a fundamental
challenge: the heavy reliance on large-scale labeled preference data.
Pre-training on abundant unlabeled data offers a promising direction, but
existing approaches fall short of instilling explicit reasoning into reward
models. To bridge this gap, we propose a self-training approach that leverages
unlabeled data to elicit reward reasoning in reward models. Based on this
approach, we develop GRAM-R$^2$, a generative reward model trained to produce
not only preference labels but also accompanying reward rationales. GRAM-R$^2$
can serve as a foundation model for reward reasoning and can be applied to a
wide range of tasks with minimal or no additional fine-tuning. It can support
downstream applications such as response ranking and task-specific reward
tuning. Experiments on response ranking, task adaptation, and reinforcement
learning from human feedback demonstrate that GRAM-R$^2$ consistently delivers
strong performance, outperforming several strong discriminative and generative
baselines.

</details>


### [769] [L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages](https://arxiv.org/abs/2509.02503)
*Nishant Tanksale,Tanmay Kokate,Darshan Gohad,Sarvadnyaa Barate,Raviraj Joshi*

Main category: cs.CL

TL;DR: 引入L3Cube - IndicHeadline - ID数据集用于低资源印度语语义评估，对多种句子转换器进行基准测试，结果显示多语言模型表现稳定，该数据集用途广泛。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言语义评估难题，填补印度语句子转换器有效性研究因缺乏高质量基准的空白。

Method: 引入涵盖十种低资源印度语的L3Cube - IndicHeadline - ID数据集，通过文章 - 标题相似度任务，用余弦相似度对多种句子转换器进行基准测试。

Result: 多语言模型表现始终良好，特定语言模型效果不一。

Conclusion: 该数据集是评估和改进检索增强生成（RAG）管道语义理解的宝贵资源，也可用于多种任务评估，是印度NLP的通用基准。

Abstract: Semantic evaluation in low-resource languages remains a major challenge in
NLP. While sentence transformers have shown strong performance in high-resource
settings, their effectiveness in Indic languages is underexplored due to a lack
of high-quality benchmarks. To bridge this gap, we introduce
L3Cube-IndicHeadline-ID, a curated headline identification dataset spanning ten
low-resource Indic languages: Marathi, Hindi, Tamil, Gujarati, Odia, Kannada,
Malayalam, Punjabi, Telugu, Bengali and English. Each language includes 20,000
news articles paired with four headline variants: the original, a semantically
similar version, a lexically similar version, and an unrelated one, designed to
test fine-grained semantic understanding. The task requires selecting the
correct headline from the options using article-headline similarity. We
benchmark several sentence transformers, including multilingual and
language-specific models, using cosine similarity. Results show that
multilingual models consistently perform well, while language-specific models
vary in effectiveness. Given the rising use of similarity models in
Retrieval-Augmented Generation (RAG) pipelines, this dataset also serves as a
valuable resource for evaluating and improving semantic understanding in such
applications. Additionally, the dataset can be repurposed for multiple-choice
question answering, headline classification, or other task-specific evaluations
of LLMs, making it a versatile benchmark for Indic NLP. The dataset is shared
publicly at https://github.com/l3cube-pune/indic-nlp

</details>


### [770] [Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition](https://arxiv.org/abs/2509.02514)
*Mayur Shirke,Amey Shembade,Pavan Thorat,Madhushri Wagh,Raviraj Joshi*

Main category: cs.CL

TL;DR: 本文对比评估了代码混合微调模型、非代码混合多语言模型和零样本生成大语言模型在印英混合文本命名实体识别（NER）任务中的表现，发现代码混合模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 印英混合文本的NER任务因非正式结构、音译和频繁语言切换存在独特挑战，需要评估不同模型在该任务中的有效性。

Method: 对代码混合微调模型（HingBERT、HingMBERT、HingRoBERTa）、非代码混合多语言模型（BERT Base Cased、IndicBERT、RoBERTa、MuRIL）和零样本大语言模型Google Gemini进行评估，使用精度、召回率和F1分数在基准数据集上测试。

Result: 代码混合模型（特别是基于HingRoBERTa和HingBERT的微调模型）表现优于其他模型，非代码混合模型表现尚可但适应性有限，Google Gemini零样本性能有竞争力。

Conclusion: 本研究为代码混合NER任务中专用模型和通用模型的有效性提供了关键见解。

Abstract: Named Entity Recognition (NER) in code-mixed text, particularly Hindi-English
(Hinglish), presents unique challenges due to informal structure,
transliteration, and frequent language switching. This study conducts a
comparative evaluation of code-mixed fine-tuned models and non-code-mixed
multilingual models, along with zero-shot generative large language models
(LLMs). Specifically, we evaluate HingBERT, HingMBERT, and HingRoBERTa (trained
on code-mixed data), and BERT Base Cased, IndicBERT, RoBERTa and MuRIL (trained
on non-code-mixed multilingual data). We also assess the performance of Google
Gemini in a zero-shot setting using a modified version of the dataset with NER
tags removed. All models are tested on a benchmark Hinglish NER dataset using
Precision, Recall, and F1-score. Results show that code-mixed models,
particularly HingRoBERTa and HingBERT-based fine-tuned models, outperform
others - including closed-source LLMs like Google Gemini - due to
domain-specific pretraining. Non-code-mixed models perform reasonably but show
limited adaptability. Notably, Google Gemini exhibits competitive zero-shot
performance, underlining the generalization strength of modern LLMs. This study
provides key insights into the effectiveness of specialized versus generalized
models for code-mixed NER tasks.

</details>


### [771] [Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR](https://arxiv.org/abs/2509.02522)
*Jiaming Li,Longze Chen,Ze Gong,Yukun Chen,Lu Wang,Wanwei He,Run Luo,Min Yang*

Main category: cs.CL

TL;DR: 提出PACS框架解决RLVR范式挑战，在数学推理任务上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在稀疏奖励信号和不稳定策略梯度更新问题，需改进。

Method: 提出PACS框架，将结果奖励视为可预测标签，把RLVR问题转化为监督学习任务，用交叉熵损失优化。

Result: 在数学推理任务上，PACS优于PPO和GRPO等基线，如在AIME 2025的pass@256达到59.78%。

Conclusion: PACS为大语言模型可验证奖励的后训练提供了有前景的途径。

Abstract: Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have
empowered large language models (LLMs) to tackle challenging reasoning tasks
such as mathematics and programming. RLVR leverages verifiable outcome rewards
to guide policy optimization, enabling LLMs to progressively improve output
quality in a grounded and reliable manner. Despite its promise, the RLVR
paradigm poses significant challenges, as existing methods often suffer from
sparse reward signals and unstable policy gradient updates, particularly in
RL-based approaches. To address the challenges, we propose $\textbf{PACS}$, a
novel RLVR framework that achieves im$\textbf{P}$licit $\textbf{A}$ctor
$\textbf{C}$ritic coupling via a $\textbf{S}$upervised learning framework. By
treating the outcome reward as a predictable label, we reformulate the RLVR
problem into a supervised learning task over a score function parameterized by
the policy model and optimized using cross-entropy loss. A detailed gradient
analysis shows that this supervised formulation inherently recovers the
classical policy gradient update while implicitly coupling actor and critic
roles, yielding more stable and efficient training. Benchmarking on challenging
mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as
PPO and GRPO, achieving superior reasoning performance. For instance, PACS
achieves 59.78\% at pass@256 on AIME 2025, representing improvements of 13.32
and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a
promising avenue for LLMs post-training with verifiable rewards. Our code and
data are available as open source at https://github.com/ritzz-ai/PACS.

</details>


### [772] [Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices](https://arxiv.org/abs/2509.02523)
*Evan King,Adam Sabra,Manjunath Kudlur,James Wang,Pete Warden*

Main category: cs.CL

TL;DR: 提出Flavors of Moonshine，一套针对多种小语种的小型自动语音识别模型，小模型单语训练表现优于多语，发布多语言模型。


<details>
  <summary>Details</summary>
Motivation: 挑战多语言自动语音识别模型优于单语模型的普遍观点，为小语种提供准确的设备端自动语音识别。

Method: 在精心平衡的高质量人工标注、伪标注和合成数据上训练单语系统。

Result: 模型平均错误率比同等规模的Whisper Tiny低48%，优于大9倍的Whisper Small，多数情况下与大28倍的Whisper Medium相当或更优。

Conclusion: 提升了该规模模型的技术水平，实现小语种准确的设备端自动语音识别，发布多语言模型。

Abstract: We present the Flavors of Moonshine, a suite of tiny automatic speech
recognition (ASR) models specialized for a range of underrepresented languages.
Prevailing wisdom suggests that multilingual ASR models outperform monolingual
counterparts by exploiting cross-lingual phonetic similarities. We challenge
this assumption, showing that for sufficiently small models (27M parameters),
training monolingual systems on a carefully balanced mix of high-quality
human-labeled, pseudo-labeled, and synthetic data yields substantially superior
performance. On average, our models achieve error rates 48% lower than the
comparably sized Whisper Tiny model, outperform the 9x larger Whisper Small
model, and in most cases match or outperform the 28x larger Whisper Medium
model. These results advance the state of the art for models of this size,
enabling accurate on-device ASR for languages that previously had limited
support. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and
Vietnamese Moonshine models under a permissive open-source license.

</details>


### [773] [Jointly Reinforcing Diversity and Quality in Language Model Generations](https://arxiv.org/abs/2509.02534)
*Tianjian Li,Yiming Zhang,Ping Yu,Swarnadeep Saha,Daniel Khashabi,Jason Weston,Jack Lanchantin,Tianlu Wang*

Main category: cs.CL

TL;DR: 论文提出DARLING框架解决大语言模型后训练中多样性与准确性的矛盾，实验证明其在多种任务中有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型后训练在提升准确性和有用性时牺牲了多样性，限制了其在创造性和探索性任务中的应用。

Method: 提出Diversity - Aware Reinforcement Learning (DARLING)框架，引入学习的分区函数测量多样性，将多样性信号与质量奖励结合进行在线强化学习。

Result: 在多个模型家族和大小的实验中，DARLING在非可验证任务和可验证任务中均表现良好，优于仅考虑质量的强化学习基线。

Conclusion: 显式优化多样性能促进在线强化学习的探索，产生更高质量的响应。

Abstract: Post-training of Large Language Models (LMs) often prioritizes accuracy and
helpfulness at the expense of diversity. This creates a tension: while
post-training improves response quality, it also sharpens output distributions
and reduces the range of ideas, limiting the usefulness of LMs in creative and
exploratory tasks such as brainstorming, storytelling, or problem solving. We
address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a
framework that jointly optimizes for response quality and semantic diversity.
At its core, DARLING introduces a learned partition function to measure
diversity beyond surface-level lexical variations. This diversity signal is
then combined with a quality reward during online reinforcement learning,
encouraging models to generate outputs that are both high-quality and distinct.
Experiments across multiple model families and sizes show that DARLING
generalizes to two regimes: non-verifiable tasks (instruction following and
creative writing) and verifiable tasks (competition math). On five benchmarks
in the first setting, DARLING consistently outperforms quality-only RL
baselines, producing outputs that are simultaneously of higher quality and
novelty. In the second setting, DARLING achieves higher pass@1 (solution
quality) and pass@k (solution variety). Most strikingly, explicitly optimizing
for diversity catalyzes exploration in online RL, which manifests itself as
higher-quality responses.

</details>
