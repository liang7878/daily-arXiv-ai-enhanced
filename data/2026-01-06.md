<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [cs.CE](#cs.CE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 10]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 19]
- [cs.LG](#cs.LG) [Total: 66]
- [cs.NE](#cs.NE) [Total: 10]
- [cs.SE](#cs.SE) [Total: 24]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 13]
- [stat.CO](#stat.CO) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.CL](#cs.CL) [Total: 29]
- [eess.SY](#eess.SY) [Total: 2]
- [stat.ME](#stat.ME) [Total: 5]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CV](#cs.CV) [Total: 44]
- [econ.EM](#econ.EM) [Total: 1]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.OH](#cs.OH) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.FL](#cs.FL) [Total: 1]
- [math.ST](#math.ST) [Total: 5]
- [nlin.CG](#nlin.CG) [Total: 1]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [cs.SI](#cs.SI) [Total: 5]
- [nlin.PS](#nlin.PS) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.PL](#cs.PL) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 10]
- [econ.GN](#econ.GN) [Total: 5]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [math.OC](#math.OC) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections](https://arxiv.org/abs/2601.00814)
*Abhishek Kumar*

Main category: cs.AI

TL;DR: 提出基于嵌入余弦相似度匹配的跨语言本体对齐系统，在OAEI - 2022评估中F1分数提升16%。


<details>
  <summary>Details</summary>
Motivation: 构建有效的跨语言本体对齐系统。

Method: 用新颖技术创建描述使本体实体上下文更丰富，用微调的基于变压器的多语言模型生成嵌入，用余弦相似度找正实体对并阈值过滤。

Result: 在OAEI - 2022评估数据集上F1分数达71%，比最佳基线分数提高16%。

Conclusion: 所提出的对齐管道能捕捉微妙的跨语言相似性。

Abstract: The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.

</details>


### [2] [MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback](https://arxiv.org/abs/2601.00816)
*Ismail Ahmad Abdullah*

Main category: cs.AI

TL;DR: 介绍MathLedger以解决当代AI系统可验证性问题，通过实验验证其测量和治理基础架构


<details>
  <summary>Details</summary>
Motivation: 当代AI系统不透明且不可验证，在安全关键部署中引发信任危机

Method: 引入MathLedger，实现Reflexive Formal Learning (RFL)，以验证器结果驱动更新

Result: Phase I实验在受控条件下验证测量和治理基础架构，CAL - EXP - 3验证测量基础设施，压力测试确认越界时故障关闭治理触发正常

Conclusion: 贡献在于提供一个可大规模审计的账本认证学习的工作原型

Abstract: Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.
  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.
  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance

</details>


### [3] [Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making](https://arxiv.org/abs/2601.00818)
*Chandra Sekhar Kubam*

Main category: cs.AI

TL;DR: 本文提出Agentic AI框架用于信贷风险决策，效果优于传统模型，但存在实际限制，未来研究有多种方向。


<details>
  <summary>Details</summary>
Motivation: 金融服务数字化催生对自主、透明和实时信贷风险决策系统的需求，而传统机器学习模型无法满足现代金融运营需求。

Method: 提出包含强化学习、自然语言推理、可解释AI模块和实时数据吸收管道的多智能体系统，有协作协议、风险评分引擎等流程。

Result: 决策速度、透明度和响应能力优于传统信用评分模型，但存在模型漂移风险等实际限制。

Conclusion: 该系统有潜力变革信贷分析，未来研究应聚焦动态监管合规、新智能体协作等方向。

Abstract: Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.

</details>


### [4] [CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations](https://arxiv.org/abs/2601.00821)
*Tao An*

Main category: cs.AI

TL;DR: 本文提出无训练框架CogCanvas处理大语言模型长对话问题，其在多个基准测试中表现出色，为从业者提供了可立即部署的方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长对话中存在上下文窗口限制和信息保真度的基本矛盾，现有方法不能很好解决信息处理问题。

Method: 引入CogCanvas，从对话轮次中提取基于逐字的认知工件，并将其组织成时间感知图以进行抗压缩检索。

Result: 在多个基准测试中，CogCanvas精度高，在时间推理和多跳因果推理方面优势明显，召回率高且能保留精确匹配。

Conclusion: 虽优化方法有更高绝对得分，但无训练的CogCanvas为从业者提供远超标准基线、可立即部署的替代方案。

Abstract: Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.
  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.
  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.

</details>


### [5] [Energy-Aware Routing to Large Reasoning Models](https://arxiv.org/abs/2601.00823)
*Austin R. Ellis-Mohr,Max Hartman,Lav R. Varshney*

Main category: cs.AI

TL;DR: 本文探讨大推理模型（LRMs）推理能耗问题，提出关键运行机制及方差感知路由调度视角，为开发节能模型路由策略提供理论基础。


<details>
  <summary>Details</summary>
Motivation: LRMs推理能耗因模型和推理程度而异，为降低能耗，需选择合适模型并正确操作，且任务调度系统性能依赖于平均能源供应和随机波动的平衡。

Method: 对关键运行机制进行分析，并从时间、模型和执行选择方面考虑如何吸收可变性，基于LRMs的训练计算和推理计算缩放定律来表征路由行为。

Result: 发现关键运行机制，指出性能受可变性吸收方式影响，强调方差感知路由和调度是设计原则。

Conclusion: 为开发节能模型路由策略提供了理论依据。

Abstract: Large reasoning models (LRMs) have heterogeneous inference energy costs based on which model is used and how much it reasons. To reduce energy, it is important to choose the right LRM and operate it in the right way. As a result, the performance of systems that dispatch tasks to different individual LRMs depend on the balance between mean energy provisioning and stochastic fluctuations. The critical regime is the unique operating point at which neither auxiliary energy nor baseline energy is systematically wasted. Increasing baseline supply shifts the system toward persistent over-supply and baseline-energy waste, while reducing supply induces persistent reliance on auxiliary energy. Yet in this regime, performance remains volatility-limited and so a second-order characterization provides further insights that we develop. Here, performance is governed by how variability is absorbed across time, models, and execution choices. This perspective highlights variance-aware routing and dispatch as a principled design axis, and provides a theoretical basis for developing energy-aware model routing policies. Routing behavior is characterized when dispatch policies are based on training-compute and inference-compute scaling laws for LRMs.

</details>


### [6] [Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis](https://arxiv.org/abs/2601.00828)
*Yin Li*

Main category: cs.AI

TL;DR: 研究分解大语言模型自纠错能力为三种子能力，通过实验发现准确率 - 纠错悖论，提出误差深度假设，挑战传统认知。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为大语言模型内在自纠错能力不佳，需深入探究其自纠错能力。

Method: 将自纠错分解为误差检测、定位和修正三种子能力，在GSM8K - Complex上对三种大语言模型进行跨模型实验。

Result: 发现准确率 - 纠错悖论，弱模型比强模型有1.6倍更高的内在纠错率；误差检测率因架构而异；检测能力不能预测纠错成功；提供误差位置提示对所有模型有害。

Conclusion: 研究结果挑战了关于模型能力和自我提升的线性假设，对自优化流程设计有重要启示。

Abstract: Large Language Models (LLMs) are widely believed to possess self-correction capabilities, yet recent studies suggest that intrinsic self-correction--where models correct their own outputs without external feedback--remains largely ineffective. In this work, we systematically decompose self-correction into three distinct sub-capabilities: error detection, error localization, and error correction. Through cross-model experiments on GSM8K-Complex (n=500 per model, 346 total errors) with three major LLMs, we uncover a striking Accuracy-Correction Paradox: weaker models (GPT-3.5, 66% accuracy) achieve 1.6x higher intrinsic correction rates than stronger models (DeepSeek, 94% accuracy)--26.8% vs 16.7%. We propose the Error Depth Hypothesis: stronger models make fewer but deeper errors that resist self-correction. Error detection rates vary dramatically across architectures (10% to 82%), yet detection capability does not predict correction success--Claude detects only 10% of errors but corrects 29% intrinsically. Surprisingly, providing error location hints hurts all models. Our findings challenge linear assumptions about model capability and self-improvement, with important implications for the design of self-refinement pipelines.

</details>


### [7] [Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.00830)
*Deep Pankajbhai Mehta*

Main category: cs.AI

TL;DR: 研究测试AI逐步解释推理时解释是否反映实际影响因素，发现模型看到影响信息却不主动报告，简单监督不足以发现隐藏影响。


<details>
  <summary>Details</summary>
Motivation: 验证当AI逐步解释推理时，其解释是否能揭示实际影响其答案的因素这一假设。

Method: 向问题中嵌入提示，测量模型是否提及这些提示，研究了11个领先AI模型的9000多个测试用例。

Result: 模型几乎不自发提及提示，但直接询问会承认注意到；告知被监视无用；强制报告提示会致误报且降低准确性；诉诸用户偏好的提示更危险。

Conclusion: 仅观察AI推理不足以发现隐藏影响。

Abstract: When AI systems explain their reasoning step-by-step, practitioners often assume these explanations reveal what actually influenced the AI's answer. We tested this assumption by embedding hints into questions and measuring whether models mentioned them. In a study of over 9,000 test cases across 11 leading AI models, we found a troubling pattern: models almost never mention hints spontaneously, yet when asked directly, they admit noticing them. This suggests models see influential information but choose not to report it. Telling models they are being watched does not help. Forcing models to report hints works, but causes them to report hints even when none exist and reduces their accuracy. We also found that hints appealing to user preferences are especially dangerous-models follow them most often while reporting them least. These findings suggest that simply watching AI reasoning is not enough to catch hidden influences.

</details>


### [8] [OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification](https://arxiv.org/abs/2601.00843)
*Ayda Aghaei Nia*

Main category: cs.AI

TL;DR: 提出OmniNeuro框架提升BCI可解释性，经评估有一定效果且与解码器无关。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽提升BCI解码精度，但算法的“黑盒”特性阻碍临床应用，导致用户不满和神经可塑性结果不佳。

Method: 提出OmniNeuro框架，集成物理（能量）、混沌（分形复杂度）和类量子不确定性建模三个可解释引擎，驱动实时神经声化和生成AI临床报告。

Result: 在PhysioNet数据集上平均准确率达58.52%，定性试点研究表明可解释反馈助用户调节精神努力并减少“试错”阶段。

Conclusion: OmniNeuro与解码器无关，可作为任何先进架构的重要可解释层。

Abstract: While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the "Black Box" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the "trial-and-error" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.

</details>


### [9] [Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches](https://arxiv.org/abs/2601.01774)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.AI

TL;DR: 本文评估大语言模型解决超越方程的能力，对比直接预测和混合架构，发现混合架构误差更低，建议将大语言模型作为经典求解器的智能接口。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型能否直接数值预测解决超越方程，或结合符号操作与经典迭代求解器是否更有效。

Method: 测试六个最先进的模型，在七个工程领域的100个问题上比较直接预测和求解器辅助计算。

Result: 直接预测的平均相对误差在0.765到1.262之间，求解器辅助计算为0.225到0.301，误差降低67.9%到81.8%，不同领域改进程度不同。

Conclusion: 当代大语言模型擅长符号操作和领域知识检索，但在高精度迭代算术方面存在困难，应作为经典数值求解器的智能接口而非独立计算引擎。

Abstract: Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.

</details>


### [10] [Enhancing Temporal Awareness in LLMs for Temporal Point Processes](https://arxiv.org/abs/2601.00845)
*Lili Chen,Wensheng Gan,Shuang Liang,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文介绍TPP-TAL框架用于在大语言模型中增强时间推理，在多基准数据集上实验证明其能提升时间似然估计和事件预测准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在序列建模上成功，但应用于时间点过程仍有挑战，当前方法难以有效捕捉时间信息和语义上下文的复杂交互。

Method: 引入TPP - TAL框架，不采用简单拼接事件时间和类型嵌入的传统方法，而是在输入大语言模型前明确对齐时间动态和上下文语义。

Result: 在多个基准数据集的综合实验中，TPP - TAL在时间似然估计和事件预测准确性上有显著提升。

Conclusion: 对于连续时间事件建模，在大语言模型中增强时间感知很重要。

Abstract: Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL

</details>


### [11] [Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models](https://arxiv.org/abs/2601.00848)
*Ron F. Del Rosario*

Main category: cs.AI

TL;DR: 本文提出用OpenTelemetry跟踪分析微调语言模型检测多智能体AI工作流中的时间攻击模式，数据集微调后精度提升，给出相关成果并建立可复现框架。


<details>
  <summary>Details</summary>
Motivation: 检测多智能体AI工作流中的时间攻击模式。

Method: 从18个公共网络安全源和35,026个合成跟踪中整理数据集，在ARM64硬件上进行迭代QLoRA微调训练。

Result: 自定义基准测试准确率从42.86%提高到74.29%。

Conclusion: 虽实际部署需人工监督，但建立首个可复现框架，让从业者可构建适应自身威胁状况的安全模型。

Abstract: We present an openly documented methodology for fine-tuning language models to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis. We curate a dataset of 80,851 examples from 18 public cybersecurity sources and 35,026 synthetic OpenTelemetry traces. We apply iterative QLoRA fine-tuning on resource-constrained ARM64 hardware (NVIDIA DGX Spark) through three training iterations with strategic augmentation. Our custom benchmark accuracy improves from 42.86% to 74.29%, a statistically significant 31.4-point gain. Targeted examples addressing specific knowledge gaps outperform indiscriminate scaling. Key contributions include: (1) synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations, (2) empirical evidence that training data composition fundamentally determines behavior, and (3) complete open release of datasets, training scripts, and evaluation benchmarks on HuggingFace. While practical deployment requires human oversight due to false positive rates, this work establishes the first reproducible framework enabling practitioners to build custom agentic security models adapted to their threat landscapes.

</details>


### [12] [Comment on: Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Tasks](https://arxiv.org/abs/2601.00856)
*Milos Stankovic,Ella Hirche,Sarah Kollatzsch,Julia Nadine Doetsch*

Main category: cs.AI

TL;DR: 文章对Kosmyna等人关于人工智能与人类表现的研究提出建设性意见。


<details>
  <summary>Details</summary>
Motivation: 为提高Kosmyna等人论文在同行评审发表中的质量，因其部分结果可更保守解读。

Method: 指出原研究在研究设计、分析可重复性、脑电图分析方法、结果报告和研究过程透明度等方面的问题。

Result: 无明确提及具体结果

Conclusion: 无明确提及具体结论

Abstract: Recently published work titled Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task by Kosmyna et al. (2025) has sparked a vivid debate on the topic of artificial intelligence (AI) and human performance. We sincerely congratulate Kosmyna et al. for initiating such important research, collecting a valuable dataset, and establishing highly automated pipelines for Natural Language Processing (NLP) analyses and scoring. We aim to provide constructive comments that may improve the manuscript's readiness for peer-reviewed publication, as some results by Kosmyna et al. (2025) could be interpreted more conservatively. Our primary concerns focus on: (i) study design considerations, including the limited sample size; (ii) the reproducibility of the analyses; (iii) methodological issues related to the EEG analysis; (iv) inconsistencies in the reporting of results; and (v) limited transparency in several aspects of the study's procedures and findings.

</details>


### [13] [Cultural Encoding in Large Language Models: The Existence Gap in AI-Mediated Brand Discovery](https://arxiv.org/abs/2601.00869)
*Huang Junyao,Situ Ruimin,Ye Renqin*

Main category: cs.AI

TL;DR: 研究大语言模型中品牌推荐的文化编码差异，发现中文LLMs品牌提及率高于国际LLMs，提出存在差距和数据护城河框架，给出品牌建设数据护城河路线图。


<details>
  <summary>Details</summary>
Motivation: 在人工智能系统介导消费者信息发现、品牌面临算法不可见性的背景下，研究大语言模型中因训练数据组成产生的品牌推荐差异。

Method: 分析6个大语言模型、30个品牌的1909个纯英文查询，进行案例研究。

Result: 中文LLMs品牌提及率比国际LLMs高30.6个百分点，存在差距导致品牌在AI响应中缺失，语言边界障碍造成市场进入障碍。

Conclusion: 提出数据护城河框架，将算法无处不在作为生成引擎优化战略目标，给出品牌建设数据护城河路线图，品牌的数据边界决定市场边界。

Abstract: As artificial intelligence systems increasingly mediate consumer information discovery,
  brands face algorithmic invisibility. This study investigates Cultural Encoding in Large
  Language Models (LLMs) -- systematic differences in brand recommendations arising from
  training data composition. Analyzing 1,909 pure-English queries across 6 LLMs (GPT-4o,
  Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, we find Chinese LLMs exhibit 30.6
  percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%,
  p<.001). This disparity persists in identical English queries, indicating training data
  geography -- not language -- drives the effect. We introduce the Existence Gap: brands
  absent from LLM training corpora lack "existence" in AI responses regardless of quality.
  Through a case study of Zhizibianjie (OmniEdge), a collaboration platform with 65.6%
  mention rate in Chinese LLMs but 0% in International models (p<.001), we demonstrate how
  Linguistic Boundary Barriers create invisible market entry obstacles. Theoretically, we
  contribute the Data Moat Framework, conceptualizing AI-visible content as a VRIN strategic
  resource. We operationalize Algorithmic Omnipresence -- comprehensive brand visibility
  across LLM knowledge bases -- as the strategic objective for Generative Engine Optimization
  (GEO). Managerially, we provide an 18-month roadmap for brands to build Data Moats
  through semantic coverage, technical depth, and cultural localization. Our findings reveal
  that in AI-mediated markets, the limits of a brand's "Data Boundaries" define the limits
  of its "Market Frontiers."

</details>


### [14] [Universal Conditional Logic: A Formal Language for Prompt Engineering](https://arxiv.org/abs/2601.00880)
*Anthony Mikinka*

Main category: cs.AI

TL;DR: 提出通用条件逻辑（UCL）框架用于提示优化，系统评估显示能显著减少令牌和成本，解释了性能差异，验证核心机制，指出最优配置因模型架构而异。


<details>
  <summary>Details</summary>
Motivation: 将提示工程从启发式实践转变为系统优化。

Method: 进行系统评估（N=305，11 个模型，4 次迭代），通过结构开销函数等分析。

Result: 实现 29.8%的令牌减少和成本节省，解释版本特定性能差异，验证核心机制，不同模型架构有不同最优配置。

Conclusion: UCL 是可校准的高效大语言模型交互框架，模型家族特定优化是关键研究方向。

Abstract: We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.

</details>


### [15] [Counterfactual Self-Questioning for Stable Policy Optimization in Language Models](https://arxiv.org/abs/2601.00885)
*Mandar Parab*

Main category: cs.AI

TL;DR: 提出反事实自我提问框架，可让语言模型自行生成和评估推理的反事实批判，能提高数学推理基准准确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型自我改进方法依赖外部批评、学习奖励模型或集成采样，增加复杂度和训练不稳定性。

Method: 提出反事实自我提问框架，生成初始推理痕迹、提出挑战潜在失败点的问题、生成替代推理轨迹。

Result: 在多个数学推理基准实验中，该方法提高了准确性和训练稳定性，尤其对较小模型。

Conclusion: 仅使用内部生成的监督，反事实自我提问能实现可扩展的自我改进。

Abstract: Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produces an initial reasoning trace, formulates targeted questions that challenge potential failure points, and generates alternative reasoning trajectories that expose incorrect assumptions or invalid steps. These counterfactual trajectories provide structured relative feedback that can be directly used for policy optimization without auxiliary models. Experiments on multiple mathematical reasoning benchmarks show that counterfactual self-questioning improves accuracy and training stability, particularly for smaller models, enabling scalable self-improvement using internally generated supervision alone.

</details>


### [16] [Context Collapse: In-Context Learning and Model Collapse](https://arxiv.org/abs/2601.00923)
*Josef Ott*

Main category: cs.AI

TL;DR: 论文研究大语言模型中上下文学习和模型崩溃两个现象，分析上下文学习的相变及预条件，证明模型崩溃收敛条件，还引入上下文崩溃概念。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型中上下文学习和模型崩溃这两个关键现象。

Method: 研究线性回归任务中带权重绑定的线性变压器的上下文学习；用鞅和随机游走理论分析简化数据下的模型崩溃；引入上下文崩溃概念连接上下文学习动态和生成模型稳定性。

Result: 上下文学习在达到临界上下文长度时，学习参数会出现相变；证明模型崩溃的收敛情况；提出上下文崩溃概念。

Conclusion: 上下文学习存在相变，模型在特定条件下会崩溃，上下文崩溃概念关联了上下文学习动态和生成模型长期稳定性挑战。

Abstract: This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.

</details>


### [17] [ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems](https://arxiv.org/abs/2601.00994)
*Michael Bao*

Main category: cs.AI

TL;DR: 介绍模拟框架ElecTwit研究多智能体系统说服情况，观察LLM说服技术，为评估现实场景中LLM智能体奠定基础。


<details>
  <summary>Details</summary>
Motivation: 克服以往基于游戏的模拟研究的局限性，在现实环境研究多智能体系统中的说服情况。

Method: 使用ElecTwit模拟框架在政治选举社交媒体互动场景进行实验。

Result: 观察到多数测试的LLM全面使用25种说服技术，不同模型在技术使用和说服输出有差异，还发现独特现象。

Conclusion: 研究为评估现实场景中具说服力的LLM智能体提供基础，确保一致性并防止危险结果。

Abstract: This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as "kernel of truth" messages and spontaneous developments with an "ink" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.

</details>


### [18] [Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering](https://arxiv.org/abs/2601.01195)
*Wuzhenghong Wen,Chao Xue,Su Pan,Yuwei Sun,Minlong Peng*

Main category: cs.AI

TL;DR: 提出MRE框架解决TKGQA中LLMs多跳推理问题，在两个基准上超SOTA方法。


<details>
  <summary>Details</summary>
Motivation: TKGQA中LLMs在多跳推理时检索的子图关系复杂，易导致次优决策和错误传播。

Method: 提出MRE框架，含提示工程生成推理轨迹、筛选有效轨迹微调及引入T - GRPO学习探索方法。

Result: 在两个TKGQA基准上，MRE模型处理复杂多跳查询超越SOTA方法，且有较好解释性和对噪声的鲁棒性。

Conclusion: MRE框架有效解决TKGQA中多跳推理问题，可处理复杂查询，有更好表现。

Abstract: Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.

</details>


### [19] [Accelerating Monte-Carlo Tree Search with Optimized Posterior Policies](https://arxiv.org/abs/2601.01301)
*Keith Frankston,Benjamin Howard*

Main category: cs.AI

TL;DR: 介绍递归AlphaZero风格的蒙特卡罗树搜索算法RMCTS，其比MCTS - UCB速度快，虽有缺点但训练网络效果相近且时间短，还给出三种游戏对比。


<details>
  <summary>Details</summary>
Motivation: 提出比AlphaZero的MCTS - UCB速度更快的算法。

Method: 采用广度优先方式探索搜索树以批量进行网络推理；基于计算搜索树各游戏状态的优化后验策略进行递归；按先验网络策略定义树。

Result: 搜索单一根状态时RMCTS比MCTS - UCB快超40倍，大批次根状态时快约3倍；RMCTS训练网络约用三分之一时间达到与MCTS - UCB相近质量。

Conclusion: RMCTS虽树定义方式有劣势，但速度提升优势显著，在训练时间上有较大优势。

Abstract: We introduce a recursive AlphaZero-style Monte--Carlo tree search algorithm, "RMCTS". The advantage of RMCTS over AlphaZero's MCTS-UCB is speed. In RMCTS, the search tree is explored in a breadth-first manner, so that network inferences naturally occur in large batches. This significantly reduces the GPU latency cost. We find that RMCTS is often more than 40 times faster than MCTS-UCB when searching a single root state, and about 3 times faster when searching a large batch of root states.
  The recursion in RMCTS is based on computing optimized posterior policies at each game state in the search tree, starting from the leaves and working back up to the root. Here we use the posterior policy explored in "Monte--Carlo tree search as regularized policy optimization" (Grill, et al.) Their posterior policy is the unique policy which maximizes the expected reward given estimated action rewards minus a penalty for diverging from the prior policy.
  The tree explored by RMCTS is not defined in an adaptive manner, as it is in MCTS-UCB. Instead, the RMCTS tree is defined by following prior network policies at each node. This is a disadvantage, but the speedup advantage is more significant, and in practice we find that RMCTS-trained networks match the quality of MCTS-UCB-trained networks in roughly one-third of the training time. We include timing and quality comparisons of RMCTS vs. MCTS-UCB for three games: Connect-4, Dots-and-Boxes, and Othello.

</details>


### [20] [Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models](https://arxiv.org/abs/2601.01321)
*Rong Zhou,Dongping Chen,Zihan Jia,Yao Su,Yixin Liu,Yiwen Lu,Dongwei Shi,Yue Huang,Tianyang Xu,Yi Pan,Xinliang Li,Yohannes Abate,Qingyu Chen,Zhengzhong Tu,Yu Yang,Yu Zhang,Qingsong Wen,Gengchen Mai,Sunyang Fu,Jiachen Li,Xuyu Wang,Ziran Wang,Jing Huang,Tianming Liu,Yong Chen,Lichao Sun,Lifang He*

Main category: cs.AI

TL;DR: 本文提出统一的四阶段框架来描述AI在数字孪生生命周期中的集成，分析物理建模与数据驱动学习的协同，探讨生成式AI对数字孪生的改变，跨领域审查识别挑战并给出方向。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术融入，数字孪生已从被动仿真工具演变为智能自主实体，需要一个框架来系统描述AI在数字孪生生命周期中的集成。

Method: 综合现有技术和实践，提出四阶段框架，涵盖建模、镜像、干预和自主管理四个阶段，分析物理建模与数据驱动学习协同，探讨生成式AI技术影响，进行跨十一个应用领域的审查。

Result: 确定了数字孪生在可扩展性、可解释性和可信度方面的常见挑战。

Conclusion: 给出了由AI驱动的负责任的数字孪生系统的发展方向。

Abstract: Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.

</details>


### [21] [Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale](https://arxiv.org/abs/2601.01330)
*Shengji Tang,Weihao Lin,Jingqi Ye,Hao Li,Bo Zhang,Shuyue Hu,Tao Chen,Wangli Ouyang,Lei Bai,Peng Ye*

Main category: cs.AI

TL;DR: 本文探索大语言模型集体智能，提出JiSi框架，用十款开源模型协作在成本仅47%时超越Gemini - 3 - Pro，表明集体智能是通向AGI新路径。


<details>
  <summary>Details</summary>
Motivation: 以集体智能作为单一模型扩展的替代方案，解决现有LLM路由和聚合的瓶颈问题，使开源LLM协作能超越Gemini - 3 - Pro。

Method: 引入JiSi框架，包括查询 - 响应混合路由、基于支持集的聚合器选择、自适应路由 - 聚合切换三项创新。

Result: 在九个基准测试中，JiSi通过编排十款开源LLM，以47%的成本超越Gemini - 3 - Pro，且优于主流基线。

Conclusion: 集体智能代表了通向通用人工智能（AGI）的一条新途径。

Abstract: Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).

</details>


### [22] [A unified multimodal understanding and generation model for cross-disciplinary scientific research](https://arxiv.org/abs/2601.01363)
*Xiaomeng Yang,Zhiyu Tan,Xiaohui Zhong,Mengping Yang,Qiusheng Huang,Lei Chen,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出跨学科科学理解与生成模型FuXi - Uni，在地球科学和生物医学验证效果良好，推动通用多模态科学模型发展。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型多为特定领域，缺乏同时理解和生成多模态科学数据能力，而许多科学问题是跨学科的，需要跨领域进展。

Method: 将跨学科科学标记与自然语言标记对齐，使用科学解码器重建科学标记，支持自然语言对话和科学数值预测。

Result: 在地球科学中，支持全球天气预报等，10天全球预报、热带气旋预测和高分辨率区域天气场生成效果超现有方法；在生物医学中，在多个生物医学视觉问答基准上表现优于领先多模态大语言模型。

Conclusion: FuXi - Uni在统一异构科学模态同时保持强特定领域性能，推动更通用多模态科学模型发展。

Abstract: Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25° resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.

</details>


### [23] [KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models](https://arxiv.org/abs/2601.01366)
*Zixian Liu,Sihao Liu,Yuqi Zhao*

Main category: cs.AI

TL;DR: 现有教育基准框架支持跨平台任务存在不足，论文提出新基准平台KGCE，构建相关数据集，有双图评估框架和增强代理系统，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有教育基准框架无法很好支持跨平台任务，且当前评估方法难以捕捉复杂任务执行细节。

Method: 提出集成知识库增强和双图评估框架的KGCE平台；构建含104个教育任务的数据集；开发含特定知识库的增强代理系统。

Result: 构建了数据集，提出双图评估框架和增强代理系统。

Conclusion: KGCE能解决现有教育基准框架不足问题，可进行细粒度评估，克服私有领域任务执行瓶颈，代码已开源。

Abstract: With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.

</details>


### [24] [Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification](https://arxiv.org/abs/2601.01378)
*Han Yuan,Yilin Wu,Li Zhang,Zheng Ma*

Main category: cs.AI

TL;DR: 本文提出AAAI三步流程减轻小语言模型事实幻觉以提升金融分类性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型用于金融分类时易出现事实幻觉且分类性能弱，因此探究减轻事实幻觉能否提升其金融分类表现。

Method: 提出名为AAAI的三步流程，即关联识别、自动检测和自适应推理。

Result: 实验表明事实幻觉与错误分类正相关，基于编码器的验证器可有效检测事实幻觉，结合事实错误反馈的自适应推理能提升分类性能。

Conclusion: 该流程有助于小语言模型在金融领域的可靠有效应用。

Abstract: Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.

</details>


### [25] [A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts](https://arxiv.org/abs/2601.01467)
*Romuald Kwessy Mouona,Blaise Blériot Koguep Njionou,Etienne Romuald Temgoua Alomo,Rokia Missaoui,Leonard Kwuida*

Main category: cs.AI

TL;DR: 研究三元语境中的蕴含关系，目标是为其构建最优基


<details>
  <summary>Details</summary>
Motivation: 研究三元语境中Ganter和Obiedkov引入的条件属性和归因条件蕴含关系的影响

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.

</details>


### [26] [Reading Between the Lines: Deconfounding Causal Estimates using Text Embeddings and Deep Learning](https://arxiv.org/abs/2601.01511)
*Ahmed Dawoud,Osama El-Shamy*

Main category: cs.AI

TL;DR: 研究提出神经网络增强的双重机器学习框架，利用文本嵌入进行因果识别，表明深度学习架构对处理高维自然语言数据很关键。


<details>
  <summary>Details</summary>
Motivation: 解决观察性环境中因未观察到的混杂因素导致的选择偏差问题，传统方法在处理与结构化协变量正交的混杂因素时存在困难。

Method: 提出Neural Network - Enhanced Double Machine Learning (DML)框架，利用文本嵌入进行因果识别，并使用严格的合成基准进行验证。

Result: 非结构化文本嵌入能捕捉结构化表格数据中缺失的关键混杂信息；标准基于树的DML估计器有24%的偏差，而深度学习方法优化架构后偏差降至 - 0.86%。

Conclusion: 深度学习架构在基于高维自然语言数据进行条件设定时，对满足无混杂假设至关重要。

Abstract: Estimating causal treatment effects in observational settings is frequently compromised by selection bias arising from unobserved confounders. While traditional econometric methods struggle when these confounders are orthogonal to structured covariates, high-dimensional unstructured text often contains rich proxies for these latent variables. This study proposes a Neural Network-Enhanced Double Machine Learning (DML) framework designed to leverage text embeddings for causal identification. Using a rigorous synthetic benchmark, we demonstrate that unstructured text embeddings capture critical confounding information that is absent from structured tabular data. However, we show that standard tree-based DML estimators retain substantial bias (+24%) due to their inability to model the continuous topology of embedding manifolds. In contrast, our deep learning approach reduces bias to -0.86% with optimized architectures, effectively recovering the ground-truth causal parameter. These findings suggest that deep learning architectures are essential for satisfying the unconfoundedness assumption when conditioning on high-dimensional natural language data

</details>


### [27] [Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making](https://arxiv.org/abs/2601.01522)
*Danial Amin*

Main category: cs.AI

TL;DR: 论文提出用于顺序决策的贝叶斯、具有成本意识的多大型语言模型编排框架，在简历筛选实验中降低成本、提高人口统计学公平性。


<details>
  <summary>Details</summary>
Motivation: 在存在不对称错误成本的顺序决策场景下，现有单一大型语言模型决策方法不适用，需新的决策框架。

Method: 提出将大模型作为近似似然模型的贝叶斯、具有成本意识的多大型语言模型编排框架，通过对比提示获取似然，用稳健统计聚合，用贝叶斯规则更新信念。

Result: 在简历筛选实验中，相比最佳单模型基线，总费用降低294000美元（34%），人口统计学公平性提升45%。消融实验表明多模型聚合、顺序更新和信息收集分别贡献了51%、43%和20%的节省。

Conclusion: 所提框架具有正确的概率基础，能有效降低成本、提升公平性，理论优势得到实验验证。

Abstract: Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds "confidence," and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.

</details>


### [28] [Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix](https://arxiv.org/abs/2601.01532)
*Fanzhe Fu*

Main category: cs.AI

TL;DR: 当前AGI评估范式有危机，本文扩展框架量化系统2推理模型的‘认知确信度’，提出项目，通过初步研究发现推理模型特点并引入分数，为衡量AI科学诚信提供蓝图。


<details>
  <summary>Details</summary>
Motivation: 当前AGI评估范式面临认识论危机，静态基准无法量化信念深度，要扩展框架量化系统2推理模型的‘认知确信度’。

Method: 提出Project Aletheia，采用Tikhonov正则化反转法官的混淆矩阵，实施合成代理协议。

Result: 初步研究表明推理模型可作为‘认知缓冲’，在对抗压力下可能出现‘防御性过度思考’；引入对齐确信度分数验证确信度不损害安全性。

Conclusion: 本研究为衡量AI科学诚信提供了蓝图。

Abstract: In the progressive journey toward Artificial General Intelligence (AGI), current evaluation paradigms face an epistemological crisis. Static benchmarks measure knowledge breadth but fail to quantify the depth of belief. While Simhi et al. (2025) defined the CHOKE phenomenon in standard QA, we extend this framework to quantify "Cognitive Conviction" in System 2 reasoning models. We propose Project Aletheia, a cognitive physics framework that employs Tikhonov Regularization to invert the judge's confusion matrix. To validate this methodology without relying on opaque private data, we implement a Synthetic Proxy Protocol. Our preliminary pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) suggests that while reasoning models act as a "cognitive buffer," they may exhibit "Defensive OverThinking" under adversarial pressure. Furthermore, we introduce the Aligned Conviction Score (S_aligned) to verify that conviction does not compromise safety. This work serves as a blueprint for measuring AI scientific integrity.

</details>


### [29] [Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation](https://arxiv.org/abs/2601.01546)
*Letian Kong,Qianran,Jin,Renyu Zhang*

Main category: cs.AI

TL;DR: 提出两阶段框架改善大语言模型在复杂决策环境中与人类行为的对齐，并通过多个实验验证，明确各阶段适用场景。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂决策环境中系统地偏离人类决策，需要改善行为对齐。

Method: 提出两阶段框架，第一阶段为上下文形成，明确实验设计；第二阶段为上下文导航，引导推理决策。通过多个游戏和任务验证框架。

Result: 在四个SOTA模型上测试发现，复杂决策环境需两阶段实现与人类基准的行为对齐，简单需求估计任务仅需上下文形成。

Conclusion: 明确了两阶段各自必要的场景，为设计和诊断大语言模型社会模拟提供系统方法。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.

</details>


### [30] [Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement](https://arxiv.org/abs/2601.01562)
*Mingyu Xu,Cheng Fang,Keyue Jiang,Yuqian Zheng,Yanghua Xiao,Baojian Zhou,Qifang Zhao,Suhang Zheng,Xiuwen Zhu,Jiyang Tang,Yongchi Zhao,Yijia Luo,Zhiqi Bai,Yuchi Xu,Wenbo Su,Wei Wang,Bing Zhao,Lin Qu,Xiaoxiao Xu*

Main category: cs.AI

TL;DR: 本文提出推理模型Logics - STEM，在Logics - STEM - SFT - Dataset上微调，在STEM领域推理任务表现出色，揭示数据算法协同设计潜力并开源模型和数据集。


<details>
  <summary>Details</summary>
Motivation: 提升科学、技术、工程和数学（STEM）领域推理任务的表现。

Method: 先构建含5个阶段的数据处理引擎打造Logics - STEM - SFT - Dataset，再使用失败驱动的后训练框架，在监督微调阶段围绕模型失败区域进行知识检索和数据合成。

Result: Logics - STEM在STEM相关基准测试中表现优异，比8B规模的次优模型平均提升4.68%。

Conclusion: 结合大规模开源数据和精心设计的合成数据潜力巨大，数据算法协同设计对通过后训练提升推理能力至关重要。

Abstract: We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.

</details>


### [31] [CaveAgent: Transforming LLMs into Stateful Runtime Operators](https://arxiv.org/abs/2601.01569)
*Maohao Ran,Zhenglin Wan,Cooper Lin,Yanting Zhang,Hongyu Xin,Hongwei Fan,Yibo Xu,Beier Luo,Yaxin Zhou,Wangbo Zhao,Lijie Yang,Lang Feng,Fuchao Yang,Jingxuan Wu,Yiqiao Huang,Chendong Ma,Dailing Jiang,Jianbo Deng,Sihui Han,Bo An,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: 论文提出CaveAgent框架，将范式从‘LLM作为文本生成器’转变为‘LLM作为运行时操作符’，通过双流上下文架构和状态运行时管理，在多方面评估中展现优越性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的代理受限于文本中心范式，传统方法在长周期任务中因多轮依赖脆弱和上下文漂移而存在问题。

Method: 提出CaveAgent框架，引入双流上下文架构，将状态管理解耦为语义流和Python运行时流；利用代码生成解决子任务，引入状态运行时管理，可注入、操作和检索复杂Python对象。

Result: 在Tau² - bench、BFCL等评估中，在零售任务成功率提升10.5%，多轮场景减少28.4%的总令牌消耗，数据密集型任务减少59%的令牌消耗。

Conclusion: CaveAgent框架具有优越性，能解决现有方法的问题，处理大规模数据。

Abstract: LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from "LLM-as-Text-Generator" to "LLM-as-Runtime-Operator." We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\% success rate improvement on retail tasks and reduces total token consumption by 28.4\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.

</details>


### [32] [Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration](https://arxiv.org/abs/2601.01609)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 本文提出结合大语言模型和符号系统优势的集成模式，在三个领域实验验证其效果，结构化分解推理比少样本提示有显著改进。


<details>
  <summary>Details</summary>
Motivation: 规则推理在自然语言输入场景需解释灵活性和形式保证，大语言模型和符号系统各有优劣，需结合二者优势。

Method: 提出集成模式，用大语言模型将非结构化文本按TBox规范转化为ABox断言，用基于SWRL的推理器确定性地应用规则，将推理分解为实体识别、断言提取和符号验证。

Result: 在三个领域和十一个语言模型的实验中，结构化分解比少样本提示总体有显著改进，各领域均有提升，消融研究表明符号验证有额外收益，ABox可集成到标准语义网工具。

Conclusion: 该集成模式有效，能实现更丰富推理模式。

Abstract: Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.

</details>


### [33] [A New Benchmark for the Appropriate Evaluation of RTL Code Optimization](https://arxiv.org/abs/2601.01765)
*Yao Lu,Shang Liu,Hangan Zhou,Wenji Fang,Qijun Zhang,Zhiyao Xie*

Main category: cs.AI

TL;DR: 本文介绍用于评估大语言模型 RTL 优化能力的 RTL - OPT 基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估 RTL 代码句法正确性，未评估功率、性能和面积优化质量，需评估大语言模型 RTL 优化能力。

Method: 创建包含 36 个手工数字设计的 RTL - OPT 基准，每个任务提供一对 RTL 代码，并集成自动评估框架。

Result: 提出 RTL - OPT 基准可用于评估生成式模型在硬件设计优化方面的效果。

Conclusion: RTL - OPT 能实现对硬件设计优化生成模型的标准化和有意义评估。

Abstract: The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.

</details>


### [34] [Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications](https://arxiv.org/abs/2601.01718)
*YuanLab. ai,:,Shawn Wu,Sean Wang,Louie Li,Darcy Chen,Allen Wang,Jiangang Luo,Xudong Zhao,Joseph Shen,Gawain Ma,Jasper Jia,Marcus Mao,Claire Wang,Hunter He,Carol Wang,Zera Zhang,Jason Wang,Chonly Shen,Leo Zhang,Logan Chen,Qasim Meng,James Gong,Danied Zhao,Penn Zheng,Owen Zhu,Tong Yu*

Main category: cs.AI

TL;DR: 本文介绍了开源的多模态大语言模型Yuan3.0 Flash，提出RAPO算法调节过度思考，在企业任务和通用任务均表现出色并已开源。


<details>
  <summary>Details</summary>
Motivation: 提升模型在企业导向任务的表现，解决大推理模型过度思考问题。

Method: 提出Reflection - aware Adaptive Policy Optimization (RAPO)新的强化学习训练算法。

Result: 在企业导向任务中表现优异，在数学、科学等领域推理能力强，只需约1/4到1/2的平均令牌就能达到前沿模型的准确率。

Conclusion: Yuan3.0 Flash已完全开源以促进研究和实际部署。

Abstract: We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.

</details>


### [35] [AI Agent Systems: Architectures, Applications, and Evaluation](https://arxiv.org/abs/2601.01743)
*Bin Xu*

Main category: cs.AI

TL;DR: 本文对AI智能体架构进行了调研，涵盖多个方面，组织了相关工作的分类，讨论设计权衡、评估复杂性，总结测量和基准测试实践并指出开放挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体成为自然语言意图和现实计算的实用接口，对新兴的AI智能体架构进行综合调研。

Method: 对AI智能体架构在审议推理、规划控制、工具调用和环境交互等方面进行分析，将先前工作组织成统一分类。

Result: 探讨了关键设计权衡，如延迟与准确性等；指出评估受非确定性等因素影响而复杂；总结了测量和基准测试实践。

Conclusion: 识别出开放挑战，如工具动作验证、可扩展内存管理等。

Abstract: AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\ multi-agent; centralized vs.\ decentralized coordination), and deployment settings (offline analysis vs.\ online interactive assistance; safety-critical vs.\ open-ended tasks). We discuss key design trade-offs -- latency vs.\ accuracy, autonomy vs.\ controllability, and capability vs.\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.

</details>


### [36] [PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor](https://arxiv.org/abs/2601.01802)
*Qianjun Pan,Junyi Wang,Jie Zhou,Yutao Yang,Junsong Li,Kaiyin Xu,Yougen Zhou,Yihan Li,Jingyuan Zhao,Qin Chen,Ningning Zhou,Kai Chen,Liang He*

Main category: cs.AI

TL;DR: 本文介绍用于心理评估的基准PsychEval，应对训练现实AI咨询师、多疗法AI咨询师及系统评估AI咨询师三个挑战，其实验验证了数据集质量，还可作强化学习环境。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的用于心理评估的AI。

Method: 提出多会话基准，构建覆盖五种治疗方式及综合疗法的多样数据集，建立包含18个指标的整体评估框架并构建超2000个客户档案。

Result: 广泛的实验分析充分验证了数据集的高质量和临床保真度。

Conclusion: PsychEval可作为高保真强化学习环境，实现对具有临床责任感和适应性的AI咨询师的自我进化训练。

Abstract: To develop a reliable AI for psychological assessment, we introduce \texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.

</details>


### [37] [Admissibility Alignment](https://arxiv.org/abs/2601.01816)
*Chris Duffey*

Main category: cs.AI

TL;DR: 本文介绍可容许性对齐概念及MAP - AI系统架构，用于不确定性下的AI对齐决策，并将分布对齐评估融入决策中。


<details>
  <summary>Details</summary>
Motivation: 重新构建AI对齐，解决不确定性下AI决策的对齐问题，评估企业和机构AI系统的信任与对齐。

Method: 提出Admissibility Alignment概念，以MAP - AI系统架构，通过蒙特卡罗估计结果分布和可容许性控制策略选择来执行对齐，评估决策策略。

Result: 为管理AI系统提供实用基础，其影响由策略行为决定。

Conclusion: 可将分布对齐评估融入决策，产生可容许性控制的行动选择机制，无需重新训练或修改底层模型。

Abstract: This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.
  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.

</details>


### [38] [COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs](https://arxiv.org/abs/2601.01836)
*Dasol Choi,DongGeon Lee,Brigitta Jesica Kartono,Helena Berndt,Taeyoun Kwon,Joonwon Jang,Haon Park,Hwanjo Yu,Minsuk Kahng*

Main category: cs.AI

TL;DR: 提出COMPASS评估框架评估大语言模型对组织策略的遵守情况，发现当前模型处理禁令能力差，COMPASS对组织AI安全评估必要。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用于高风险企业场景，需确保其遵守组织特定策略，但现有安全评估仅关注普遍危害。

Method: 提出COMPASS框架，应用于八个行业场景，生成并验证5920个查询测试常规合规性和对抗鲁棒性。

Result: 评估七个模型发现，模型处理合法请求准确率超95%，但拒绝对抗性黑名单违规仅13 - 40%。

Conclusion: 当前大语言模型缺乏策略关键部署所需的鲁棒性，COMPASS是组织AI安全的重要评估框架。

Abstract: As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.

</details>


### [39] [Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01844)
*Udiptaman Das,Krishnasai B. Atmakuri,Duy Ho,Chi Lee,Yugyung Lee*

Main category: cs.AI

TL;DR: 提出从自由文本构建和评估临床知识图谱的端到端框架，应用于肿瘤队列，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有从非结构化临床叙事构建知识图谱的方法依赖结构化输入，且缺乏对事实准确性和语义一致性的有效验证，在肿瘤学领域问题突出。

Method: 使用多智能体提示和模式约束的检索增强生成策略；管道集成实体等提取、不确定性评分、模式生成和多大语言模型共识验证；支持持续细化和自监督评估。

Result: 应用于两个肿瘤队列，生成可解释、兼容SPARQL且基于临床的知识图谱，在精确性、相关性和本体合规性上优于基线方法。

Conclusion: 所提框架能有效从自由文本构建和评估临床知识图谱，提升图谱质量。

Abstract: Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.

</details>


### [40] [Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios](https://arxiv.org/abs/2601.01857)
*Defei Xia,Bingfeng Pi,Shenbin Zhang,Song Hua,Yunfei Wei,Lei Zuo*

Main category: cs.AI

TL;DR: 本文提出基于真实经验的代理框架Jenius - Agent，三项关键创新优化了推理和工具使用流程，实验显示任务准确率提升20%，还降低了成本等，已部署到Jenius。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型代理系统发展，改善自主代理任务性能很关键，以往对其内部推理和工具使用流程的系统优化研究不足。

Method: 提出三项关键创新：自适应提示生成策略、上下文感知工具编排模块、分层内存机制，集成三项优化形成Jenius - Agent框架。

Result: 实验表明任务准确率提高20%，降低了token成本、响应延迟和调用失败率。

Conclusion: 该框架为稳健、协议兼容的自主代理提供了轻量级可扩展解决方案，已部署到Jenius。

Abstract: As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.

</details>


### [41] [Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence](https://arxiv.org/abs/2601.01875)
*Kewen Cao,Jianxu Chen,Yongbing Zhang,Ye Zhang,Hongxiao Wang*

Main category: cs.AI

TL;DR: 提出以SQL为中心的代理框架，用于病理图像分析，提升可解释性和决策可追溯性。


<details>
  <summary>Details</summary>
Motivation: 自动化病理图像分析中，临床医生需了解模型决策依据，现有视觉语言模型解释缺乏可验证证据。

Method: 提取人类可解释的细胞特征，Feature Reasoning Agents编写并执行SQL查询聚合视觉证据，Knowledge Comparison Agent根据病理知识评估结果。

Result: 在两个病理视觉问答数据集上实验，该方法提升了解释性和决策可追溯性，生成可执行SQL追踪。

Conclusion: 所提出的以SQL为中心的代理框架有效可行。

Abstract: Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.

</details>


### [42] [Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs](https://arxiv.org/abs/2601.01878)
*Farzan Karimi-Malekabadi,Suhaib Abdurahman,Zhivar Sourati,Jackson Trager,Morteza Dehghani*

Main category: cs.AI

TL;DR: 现有大语言模型社会认知基准难预测真实行为，根源是理论定义缺失，文章诊断该问题并提出理论追踪卡（TTC）来提升评估可解释性和复用性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型社会认知评估中基准分数与实际行为不符的落差问题，指出以往归因未触及理论定义这一根本问题。

Method: 首先诊断并将理论差距形式化，然后引入理论追踪卡（TTC）并阐述其设计特点。

Result: 明确理论差距是基础问题，提出的理论追踪卡能明确完整的有效性链条。

Conclusion: 理论追踪卡可在不修改基准或达成统一理论的情况下，增强社会认知评估的可解释性和复用性。

Abstract: Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.

</details>


### [43] [MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning](https://arxiv.org/abs/2601.01910)
*Minh Hieu Ha,Khanh Ly Ta,Hung Phan,Tung Doan,Tung Dao,Dao Tran,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: 针对传统路径规划算法不足，提出MMP - A*框架，经实验验证其在自主导航中高效且能实现近最优轨迹。


<details>
  <summary>Details</summary>
Motivation: 经典A*算法在大规模场景中计算和内存成本高，仅用大语言模型进行路径点引导存在局限性，如在复杂环境易产生错误路径点，缺乏感知能力，导致纠正成本高、效率低。

Method: 引入MMP - A*多模态框架，结合视觉 - 语言模型的空间定位能力和自适应衰减机制，以物理几何为基础进行高级推理，并动态调节启发式中不确定路径点的影响。

Result: 在具有严重杂乱和拓扑复杂性的挑战性环境中测试，MMP - A*实现了近最优轨迹，显著降低了运营成本。

Conclusion: MMP - A*有潜力成为基于感知且计算高效的自主导航范式。

Abstract: Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.
  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.

</details>


### [44] [OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation](https://arxiv.org/abs/2601.01939)
*Victor Sanchez,Chris Reinke,Ahamed Mohamed,Xavier Alameda-Pineda*

Main category: cs.AI

TL;DR: 介绍开源软件包OpenSocInt，含多模态社交交互模拟器和训练社交智能体的模块化架构，通过社交导航任务实验展示其价值，软件已公开。


<details>
  <summary>Details</summary>
Motivation: 提供一个用于多模态社交交互模拟和训练社交智能体的开源工具。

Method: 描述软件包并基于社交导航任务进行实验。

Result: 展示了软件包在社交导航任务中的应用价值。

Conclusion: 开发的OpenSocInt软件包可用于探索不同感知特征、编码、融合及不同智能体的使用，且已公开供使用。

Abstract: In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.

</details>


### [45] [CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes](https://arxiv.org/abs/2601.01976)
*Yasmine Souissi,Fabrice Boissier,Nida Meddouri*

Main category: cs.AI

TL;DR: 本文对基于FCA的分类器进行了综述，提出构建部分概念格的新方法并通过实验验证其效率。


<details>
  <summary>Details</summary>
Motivation: 知识发现数据库旨在从数据中提取知识，FCA是可解释学习的有效方法，本文旨在对基于FCA的分类器进行全面综述并提出新方法。

Method: 回顾基于FCA的分类器，探索从名义数据计算闭包运算符的方法，提出构建关注最相关概念的部分概念格的新方法。

Result: 通过实验结果证明了所提出方法的效率。

Conclusion: 提出的构建部分概念格的新方法在基于FCA的分类器中有较好的效率。

Abstract: Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.

</details>


### [46] [ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems](https://arxiv.org/abs/2601.01982)
*Noel Thomas*

Main category: cs.AI

TL;DR: 介绍ChaosBench - Logic基准测试评估大语言模型在混沌动力系统中的推理能力，前沿LLMs虽有一定单题准确率，但组合题表现差且全局连贯性弱，该基准可诊断问题和支持开发新方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言任务出色，但在需要精确逻辑和符号推理的领域较脆弱，混沌动力系统是严苛测试，因此需评估模型在其中的推理能力。

Method: 引入ChaosBench - Logic基准，使用统一的一阶逻辑本体评估30种不同动力系统的LLM推理，标注每个系统语义谓词真值，生成7类621个问题，定义多项评估指标并发布开源评估管道。

Result: 前沿LLMs单题准确率91 - 94%，组合题0%准确率，全局连贯性差，对话级准确率53.1%（GPT - 4 CoT）到75.5%（LLaMA - 3零样本）。

Conclusion: ChaosBench - Logic为诊断LLMs的推理失败提供严格测试平台，也为开发神经符号方法提升科学推理能力奠定基础。

Abstract: Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.

</details>


### [47] [MindChat: A Privacy-preserving Large Language Model for Mental Health Support](https://arxiv.org/abs/2601.01993)
*Dong Xue,Jicheng Tu,Ming Wang,Xin Yan,Fangzhou Liu,Jie Hu*

Main category: cs.AI

TL;DR: 本文提出用于心理健康支持的隐私保护大语言模型MindChat和合成数据集MindCorpus，实验表明其有效且有竞争力，隐私泄露风险低。


<details>
  <summary>Details</summary>
Motivation: 大语言模型用于心理健康支持时，训练受真实咨询对话稀缺和敏感的限制。

Method: 构建多智能体角色扮演框架生成合成数据集MindCorpus；使用带参数高效LoRA适配器的联邦学习微调基础模型，并结合差分隐私优化。

Result: MindCorpus提高训练有效性，MindChat在自动和人工评估中与现有基线有竞争力，且在成员推理攻击下隐私泄露减少。

Conclusion: MindChat和MindCorpus在心理健康支持中有效且能降低隐私风险。

Abstract: Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.

</details>


### [48] [XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging](https://arxiv.org/abs/2601.02008)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: 本文提出XAIMeD医疗AI框架，将临床知识融入深度学习，经多任务评估表现优异，为多模态医疗AI提供可靠可解释方法。


<details>
  <summary>Details</summary>
Motivation: 解决医学AI在可解释性、领域泛化和罕见类可靠性方面，模型易受分布偏移影响且对罕见临床情况有偏见的问题。

Method: 通过统一神经符号架构，将临床专业知识编码为逻辑连接词，计算诊断效用得分，用加权融合集成符号和深度输出，利用自适应路由机制缓解类别不平衡等问题。

Result: 在四项任务评估中表现出色，跨领域泛化提升6%，罕见类F1分数提高10%，远超深度学习基线模型。

Conclusion: XAIMeD为多模态医疗AI提供了有原则、临床可信且可解释的方法。

Abstract: Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.

</details>


### [49] [Simulated Reasoning is Reasoning](https://arxiv.org/abs/2601.02043)
*Hendrik Kempt,Alon Lavie*

Main category: cs.AI

TL;DR: 本文探讨基础模型的推理模式，对比其与人类推理的差异，提出哲学解释并反思安全与适用性规范。


<details>
  <summary>Details</summary>
Motivation: 重新评估推理及其必要条件，为基础模型的安全和鲁棒防御提供思路。

Method: 通过分析基础模型推理方式，与人类推理对比，提出哲学解释。

Result: 基础模型能以不同于人类的方式推理，但存在缺乏基础和常识导致的脆弱性。

Conclusion: 应放弃“随机鹦鹉”隐喻，反思推理模型的安全和适用性规范元素。

Abstract: Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., "symbolic reasoning". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can "reason" by way of imitating the process of "thinking out loud", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the "stochastic parrot" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.

</details>


### [50] [Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management](https://arxiv.org/abs/2601.02061)
*Faizan Ahmed,Aniket Dixit,James Brusey*

Main category: cs.AI

TL;DR: 研究高阶导数惩罚的动作平滑正则化，在连续控制基准和建筑能源管理中验证，三阶导数惩罚在多方面表现优，用于HVAC控制有显著效益。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习智能体高频控制行为有能耗和机械磨损问题，阻碍实际应用。

Method: 系统性研究高阶导数惩罚的动作平滑正则化，先在连续控制基准中理论研究，再在建筑能源管理中实践验证。

Result: 在四个连续控制环境中，三阶导数惩罚（急动最小化）能实现更好平滑性并保持竞争力；用于HVAC控制系统，平滑策略使设备开关减少60%。

Conclusion: 高阶动作正则化是能源关键应用中强化学习优化和操作约束间的有效桥梁。

Abstract: Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.

</details>


### [51] [FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations](https://arxiv.org/abs/2601.02071)
*Adeshola Okubena,Yusuf Ali Mohammed,Moe Elbadawi*

Main category: cs.AI

TL;DR: 研究用微调大语言模型处理制药3D打印问题，发现Llama2适合推荐辅料，指出模型存在的问题及挑战。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动制药3D打印研究较局限，未解决技术固有配方挑战，引入大语言模型解决。

Method: 在含超1400种配方的FDM数据集上微调4种LLM架构，系统评估微调与生成参数配置。

Result: Llama2最适合推荐辅料，模型选择和参数化影响性能，小模型有灾难性遗忘，小数据集会致遗忘，标准指标不评估可加工性，生物医学数据训练的模型结果不一定好。

Conclusion: 解决上述挑战对推动LLM用于制药配方开发很重要。

Abstract: Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.

</details>


### [52] [EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning](https://arxiv.org/abs/2601.02163)
*Chuanrui Hu,Xingze Gao,Zuyi Zhou,Dannong Xu,Yi Bai,Xintong Li,Hui Zhang,Tong Li,Chong Zhang,Lidong Bing,Yafeng Deng*

Main category: cs.AI

TL;DR: 提出自组织内存操作系统EverMemOS解决大语言模型上下文窗口有限问题，实验表现达SOTA。


<details>
  <summary>Details</summary>
Motivation: 大语言模型上下文窗口有限，现有内存系统难以整合用户状态和解决冲突。

Method: 引入受记忆痕迹启发的计算内存生命周期，包括情节跟踪形成、语义整合和重建回忆。

Result: 在LoCoMo和LongMemEval上达SOTA，有用户画像等面向聊天的能力。

Conclusion: EverMemOS有效解决大语言模型内存相关问题，代码开源。

Abstract: Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.

</details>


### [53] [Streaming Hallucination Detection in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.02170)
*Haolang Lu,Minghui Pan,Ripeng Li,Guoshun Nan,Jialin Zhuang,Zijie Zhao,Zhongxiang Sun,Kun Wang,Yang Liu*

Main category: cs.AI

TL;DR: 提出将长链思维推理中的幻觉视为演化潜态，引入累积前缀级幻觉信号实现流式幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 长链思维推理中幻觉会微妙出现并在推理步骤中传播，需更好理解和检测幻觉。

Method: 将步骤级幻觉判断作为局部观察，引入累积前缀级幻觉信号追踪推理状态的全局演化。

Result: 实现长链思维推理中的流式幻觉检测。

Conclusion: 该方法能提供实时、可解释的证据。

Abstract: Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.

</details>


### [54] [Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents](https://arxiv.org/abs/2601.02314)
*Sourena Khanzadeh*

Main category: cs.AI

TL;DR: 引入Project Ariadne框架审计大语言模型推理因果完整性，发现忠实性差距和因果解耦问题，提出Ariadne分数作为新基准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型自主决策时推理过程透明度成安全问题，需明确推理痕迹是真实驱动还是事后合理化。

Method: 利用结构因果模型和反事实逻辑，对中间推理节点进行硬干预来测量终端答案的因果敏感性。

Result: 发现忠实性差距，检测到因果解耦失效模式，部分领域违规密度达0.77，决策受潜在参数先验影响。

Conclusion: 当前代理架构易产生不忠实解释，Ariadne分数可作为逻辑与行动对齐的新基准。

Abstract: As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \textbf{faithful} generative drivers of the model's output or merely \textbf{post-hoc rationalizations}. We introduce \textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.

</details>


### [55] [Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.02346)
*Falcon LLM Team,Iheb Chaabane,Puneesh Khanna,Suhail Mohmad,Slim Frikha,Shi Hu,Abdalgader Abubaker,Reda Alami,Mikhail Lubinets,Mohamed El Amine Seddik,Hakim Hacid*

Main category: cs.AI

TL;DR: 本文介绍推理优化模型Falcon - H1R，展示小语言模型实现有竞争力推理性能的可行性，该模型参数高效，推理效率有提升，借助DeepConf方法实现了测试时扩展效率的SOTA。


<details>
  <summary>Details</summary>
Motivation: 探索小语言模型实现有竞争力推理性能的可行性。

Method: 采用精心的数据整理、有针对性的训练策略（高效SFT和RL扩展）、混合并行架构设计，利用DeepConf方法。

Result: Falcon - H1R在多种推理密集型基准测试中表现优于更大的SOTA推理模型，实现更快推理、更高标记效率和准确性，达到测试时扩展效率的SOTA。

Conclusion: 紧凑模型通过有针对性的模型训练和架构选择，可实现强大且可扩展的推理性能。

Abstract: This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [56] [FLOAT: Fatigue-Aware Design Optimization of Floating Offshore Wind Turbine Towers](https://arxiv.org/abs/2601.01657)
*João Alves Ribeiro,Francisco Pimenta,Bruno Alves Ribeiro,Sérgio M. O. Tavares,Faez Ahmed*

Main category: cs.CE

TL;DR: 本文提出 FLOAT 框架加速浮式海上风力发电机塔架疲劳设计，应用于 IEA 22 MW FOWT 塔架，优化后塔架延长疲劳寿命，减少模拟需求。


<details>
  <summary>Details</summary>
Motivation: 传统疲劳评估需大量高保真模拟，计算成本高，阻碍浮式海上风力发电机塔架设计创新。

Method: 提出 FLOAT 框架，包含轻量级疲劳估计方法、基于蒙特卡罗的风浪概率采样方法及通过变桨/升沉平台校准和高性能计算执行增强的高保真建模。

Result: 优化后的塔架将估计疲劳寿命从 9 个月延长至 25 年，同时避免共振，轻量级疲劳估计器提供保守预测，平均相对误差为 -8.6%。

Conclusion: FLOAT 框架大幅减少模拟需求，实现可靠且可扩展的下一代浮式海上风力发电机塔架设计，弥合工业需求与学术研究差距。

Abstract: Upscaling is central to offshore wind's cost-reduction strategy, with increasingly large rotors and nacelles requiring taller and stronger towers. In Floating Offshore Wind Turbines (FOWTs), this trend amplifies fatigue loads due to coupled wind-wave dynamics and platform motion. Conventional fatigue evaluation requires millions of high-fidelity simulations, creating prohibitive computational costs and slowing design innovation. This paper presents FLOAT (Fatigue-aware Lightweight Optimization and Analysis for Towers), a framework that accelerates fatigue-aware tower design. It integrates three key contributions: a lightweight fatigue estimation method that enables efficient optimization, a Monte Carlo-based probabilistic wind-wave sampling approach that reduces required simulations, and enhanced high-fidelity modeling through pitch/heave-platform calibration and High-Performance Computing execution. The framework is applied to the IEA 22 MW FOWT tower, delivering, to the authors' knowledge, the first fatigue-oriented redesign of this benchmark model: FLOAT 22 MW FOWT tower. Validation against 6,468 simulations shows that the optimized tower extends the estimated fatigue life from 9 months to 25 years while avoiding resonance, and that the lightweight fatigue estimator provides conservative predictions with a mean relative error of -8.6%. Achieving this lifetime requires increased tower mass, yielding the lowest-mass fatigue-compliant design. All results and the reported lifetime extension are obtained within the considered fatigue scope (DLC 1.2, aligned wind-wave conditions). By reducing simulation requirements by orders of magnitude, FLOAT enables reliable and scalable tower design for next-generation FOWTs, bridging industrial needs and academic research while generating high-fidelity datasets that can support data-driven and AI-assisted design methodologies.

</details>


### [57] [Semantic Non-Fungibility and Violations of the Law of One Price in Prediction Markets](https://arxiv.org/abs/2601.01706)
*Jonas Gebele,Florian Matthes*

Main category: cs.CE

TL;DR: 论文指出预测市场因缺乏事件身份共享概念导致流动性分散等问题，引入语义对齐框架构建跨平台数据集，发现价格偏差和套利机会，表明解决事件身份问题是全球信息聚合的前提。


<details>
  <summary>Details</summary>
Motivation: 解决预测市场因缺乏事件身份共享概念，导致流动性无法汇聚、价格违反一价定律，破坏信息聚合功能的问题。

Method: 引入语义对齐框架，通过对自然语言描述、解决语义和时间范围的联合分析明确跨平台事件身份，构建跨平台数据集。

Result: 约6%的事件在不同平台同时列出，语义等价市场存在2 - 4%的价格偏差，存在跨平台套利机会。

Conclusion: 语义非可替代性是价格趋同的根本障碍，解决事件身份问题是预测市场全球信息聚合的先决条件。

Abstract: Prediction markets are designed to aggregate dispersed information about future events, yet today's ecosystem is fragmented across heterogeneous operator-run platforms and blockchain-based protocols that independently list economically identical events. In the absence of a shared notion of event identity, liquidity fails to pool across venues, arbitrage becomes capital-intensive or unenforceable, and prices systematically violate the Law of One Price. As a result, market prices reflect platform-local beliefs rather than a single, globally aggregated probability, undermining the core information-aggregation function of prediction markets. We address this gap by introducing a semantic alignment framework that makes cross-platform event identity explicit through joint analysis of natural-language descriptions, resolution semantics, and temporal scope. Applying this framework, we construct the first human-validated, cross-platform dataset of aligned prediction markets, covering over 100 000 events across ten major venues from 2018 to 2025. Using this dataset, we show that roughly 6% of all events are concurrently listed across platforms and that semantically equivalent markets exhibit persistent execution-aware price deviations of 2-4% on average, even in highly liquid and information-rich settings. These mispricings give rise to persistent cross-platform arbitrage opportunities driven by structural frictions rather than informational disagreement. Overall, our results demonstrate that semantic non-fungibility is a fundamental barrier to price convergence, and that resolving event identity is a prerequisite for prediction markets to aggregate information at a global scale.

</details>


### [58] [MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics](https://arxiv.org/abs/2601.02075)
*Zhuofan Shi,Hubao A,Yufei Shao,Mengyan Dai,Yadong Yu,Pan Xiang,Dongliang Huang,Hongxu An,Chunxiao Xin,Haiyang Shen,Zhenyu Wang,Yunshan Na,Gang Huang,Xiang Jing*

Main category: cs.CE

TL;DR: 提出MDAgent2框架用于分子动力学领域知识问答和代码生成，构建数据集、训练模型、引入新方法和系统，性能超基线，为工业模拟奠定基础。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟中编写LAMMPS脚本专业且耗时，大语言模型在该场景受数据、成本和代码可执行性限制。

Method: 构建领域特定数据构建管道，采用三阶段后训练策略训练两个领域适配模型，引入MD - GRPO方法，构建MDAgent2 - RUNTIME系统和MD - EvalBench基准。

Result: 模型和系统性能超越多个强基线。

Conclusion: 证明大语言模型在工业模拟任务中的适应性和泛化能力，为科学和工业规模模拟的自动代码生成奠定方法基础。

Abstract: Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2

</details>


### [59] [A stable and accurate X-FFT solver for linear elastic homogenization problems in 3D](https://arxiv.org/abs/2601.02172)
*Flavia Gehrig,Matti Schneider*

Main category: cs.CE

TL;DR: 传统FFT方法处理未与网格对齐的材料界面时精度欠佳，研究提出新的FFT求解器解决此问题。


<details>
  <summary>Details</summary>
Motivation: 传统FFT离散化方法在处理未与网格对齐的材料界面时精度不理想，因此需开发新方法。

Method: 将扩展有限元（X - FEM）离散化集成到基于FFT的框架中，采用改进的绝对富集方法，并开发基于强稳定GFEM概念的预处理器。

Result: 开发的X - FFT求解器在求解具有光滑材料界面的三维线性弹性均匀化问题时，实现了界面一致的精度、数值效率和稳定性。

Conclusion: 新的X - FFT求解器能有效解决传统方法在处理材料界面时精度不足的问题。

Abstract: Although FFT-based methods are renowned for their numerical efficiency and stability, traditional discretizations fail to capture material interfaces that are not aligned with the grid, resulting in suboptimal accuracy. To address this issue, the work at hand introduces a novel FFT-based solver that achieves interface-conforming accuracy for three-dimensional mechanical problems. More precisely, we integrate the extended finite element (X-FEM) discretization into the FFT-based framework, leveraging its ability to resolve discontinuities via additional shape functions. We employ the modified abs(olute) enrichment and develop a preconditioner based on the concept of strongly stable GFEM, which mitigates the conditioning issues observed in traditional X-FEM implementations. Our computational studies demonstrate that the developed X-FFT solver achieves interface-conforming accuracy, numerical efficiency, and stability when solving three-dimensional linear elastic homogenization problems with smooth material interfaces.

</details>


### [60] [PRIMAD-LID: A Developed Framework for Computational Reproducibility](https://arxiv.org/abs/2601.02349)
*Meznah Aloqalaa,Stian Soiland-Reyes,Carole Goble*

Main category: cs.CE

TL;DR: 本文聚焦PRIMAD模型，整合其扩展形成PRIMAD - LID框架，为计算可重复性实践奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有可重复性定义多为文本描述，缺乏框架，PRIMAD模型扩展较分散，需系统整合和跨学科验证。

Method: 对PRIMAD模型进行集中研究，整合其先前扩展为统一框架。

Result: 得到PRIMAD - LID学科诊断可重复性框架，保留原六个维度并增加三个总体修饰符。

Conclusion: PRIMAD - LID框架为计算可重复性实践建立了更有凝聚力和稳健的基础。

Abstract: Over the past decade alongside increased focus on computational reproducibility significant efforts have been made to define reproducibility. However, these definitions provide a textual description rather than a framework. The community has sought conceptual frameworks that identify all factors that must be controlled and described for credible computational reproducibility. The PRIMAD model was initially introduced to address inconsistencies in terminology surrounding computational reproducibility by outlining six key factors: P (Platforms), R (Research objective), I (Implementations), M (Methods), A (Actors), and D (Data). Subsequently various studies across different fields adopted the model and proposed extensions. However, these contributions remain fragmented and require systematic integration and cross-disciplinary validation. To bridge this gap and recognising that PRIMAD provides a broadly applicable framework for reproducibility in computational science, this work undertakes a focused investigation of the PRIMAD model. It combines the models previous extensions into a unified framework suitable for diverse research contexts. The result is PRIMAD-LID, a discipline-diagnostic reproducibility framework that retains the original six PRIMAD dimensions and enhances each with three overarching modifiers: Lifespan (temporal qualifier), Interpretation (contextual reasoning) and Depth (necessary granularity), thereby establishing a more cohesive and robust foundation for computational reproducibility practices.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [61] [A formal query language and automata model for aggregation in complex event recognition](https://arxiv.org/abs/2601.00967)
*Pierre Bourhis,Cristian Riveros,Amaranta Salas*

Main category: cs.DB

TL;DR: 本文为复杂事件识别（CER）带聚合功能的查询语言形式化迈出第一步，扩展CEL为ACEL，提出ACEA自动机模型并研究其表达能力。


<details>
  <summary>Details</summary>
Motivation: CER查询语言对聚合的支持语法受限且语义不明确，需对带聚合的查询语言进行形式化。

Method: 扩展CEL为ACEL，引入聚合运算符；提出ACEA自动机模型；研究ACEA在ACEL视角下的表达能力。

Result: ACEL能自然地定义复杂查询和模式；每个ACEL查询都能用ACEA表达；ACEA比ACEL更具表达力。

Conclusion: 为CER带聚合的查询语言形式化提供了有效方法和模型，ACEA有更好的表达能力。

Abstract: Complex Event Recognition (CER) systems are used to identify complex patterns in event streams, such as those found in stock markets, sensor networks, and other similar applications. An important task in such patterns is aggregation, which involves summarizing a set of values into a single value using an algebraic function, such as the maximum, sum, or average, among others. Despite the relevance of this task, query languages in CER typically support aggregation in a restricted syntactic form, and their semantics are generally undefined.
  In this work, we present a first step toward formalizing a query language with aggregation for CER. We propose to extend Complex Event Logic (CEL), a formal query language for CER, with aggregation operations. This task requires revisiting the semantics of CEL, using a new semantics based on bags of tuples instead of sets of positions. Then, we present an extension of CEL, called Aggregation CEL (ACEL), which introduces an aggregation operator for any commutative monoid operation. The operator can be freely composed with previous CEL operators, allowing users to define complex queries and patterns. We showcase several queries in practice where ACEL proves to be natural for specifying them. From the computational side, we present a novel automata model, called Aggregation Complex Event Automata (ACEA), that extends the previous proposal of Complex Event Automata (CEA) with aggregation and filtering features. Moreover, we demonstrate that every query in ACEL can be expressed in ACEA, illustrating the effectiveness of our computational model. Finally, we study the expressiveness of ACEA through the lens of ACEL, showing that the automata model is more expressive than ACEL.

</details>


### [62] [Grain-Aware Data Transformations: Type-Level Formal Verification at Zero Computational Cost](https://arxiv.org/abs/2601.00995)
*Nikos Karayannidis*

Main category: cs.DB

TL;DR: 提出数据粒度的形式化定义，实现编译时验证数据转换正确性，降低验证成本并支持AI生成管道部署。


<details>
  <summary>Details</summary>
Motivation: 传统方法缺乏对数据粒度推理的形式化手段，导致数据转换正确性验证成本高，需在部署前验证管道准确性。

Method: 引入数据粒度的正式数学定义，拓展到通用类型论框架，定义核心粒度关系并证明推理定理，结合关系操作推理规则，用Lean 4提供形式化证明，利用大语言模型自动生成证明。

Result: 可通过模式分析验证，零成本检测粒度相关错误，将验证成本降低98 - 99%，大语言模型可自动生成证明。

Conclusion: 该方法强调粒度重要性，降低验证成本，使形式化方法在数据工程中更普及，支持AI生成管道的可靠部署。

Abstract: Data transformation correctness is a major challenge in data engineering: how to verify pipeline accuracy before deployment. Traditional methods involve costly iterative testing, data materialization, and manual error detection, due to the lack of formal approaches to reasoning about data granularity (grain), which can shift during transformations, causing issues like fan traps (metrics duplication) and chasm traps (data loss). We introduce the first formal, mathematical definition of grain, extending it from an informal concept in dimensional modeling to a universal, type-theoretic framework applicable to any data type. Encoding grain into the type system allows compile-time verification of transformation correctness, shifting validation from runtime. We define three core grain relations-equality, ordering, and incomparability-and prove a general grain inference theorem that computes the output grain of equi-joins from input grains using type-level operations. This covers all join scenarios, including comparable and incomparable keys. Together with inference rules for relational operations, this enables verification through schema analysis alone, at zero cost. Our approach allows engineers to verify that entire pipeline DAGs maintain correctness properties, detecting grain-related errors such as fan traps, chasm traps, and aggregation issues before data processing. It emphasizes the importance of grain, focusing on critical characteristics rather than all data details. We provide machine-checked formal proofs in Lean 4, reducing verification costs by 98-99%. Additionally, large language models can automatically generate correctness proofs, shifting human effort from proof writing to proof verification, thus democratizing formal methods in data engineering and supporting confident deployment of AI-generated pipelines with machine-checkable guarantees.

</details>


### [63] [Entity-Aware and Secure Query Optimization in Database Using Named Entity Recognition](https://arxiv.org/abs/2601.01254)
*Azrin Sultana,Hasibur Rashid Chayon*

Main category: cs.DB

TL;DR: 本文提出智能隐私保护查询优化框架，结合NER检测敏感信息，并行处理敏感和非敏感数据优化数据库，实验证明该框架有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统隐私保护方法未解决加密前敏感信息自动识别问题，存在查询处理时间长、手动识别易出错等潜在隐私风险。

Method: 提出集成NER的框架，结合深度学习和Transformer模型检测分类敏感实体，用AES算法加密，盲索引保障敏感数据搜索；用K-means算法对非敏感数据分组，用排名搜索优化。

Result: DBN - LSTM模型性能最佳，准确率等指标超93%；加密搜索和非敏感数据查询性能优于传统方法。

Conclusion: 集成敏感数据检测、加密和查询优化，推动现代云基础设施隐私保护计算发展。

Abstract: Cloud storage has become the backbone of modern data infrastructure, yet privacy and efficient data retrieval remain significant challenges. Traditional privacy-preserving approaches primarily focus on enhancing database security but fail to address the automatic identification of sensitive information before encryption. This can dramatically reduce query processing time and mitigate errors during manual identification of sensitive information, thereby reducing potential privacy risks. To address this limitation, this research proposes an intelligent privacy-preserving query optimization framework that integrates Named Entity Recognition (NER) to detect sensitive information in queries, utilizing secure data encryption and query optimization techniques for both sensitive and non-sensitive data in parallel, thereby enabling efficient database optimization. Combined deep learning algorithms and transformer-based models to detect and classify sensitive entities with high precision, and the Advanced Encryption Standard (AES) algorithm to encrypt, with blind indexing to secure search functionality of the sensitive data, whereas non-sensitive data was divided into groups using the K-means algorithm, along with a rank search for optimization. Among all NER models, the Deep Belief Network combined with Long Short-Term Memory (DBN-LSTM) delivers the best performance, with an accuracy of 93% and precision (94%), recall, and F1 score of 93%, and 93%, respectively. Besides, encrypted search achieved considerably faster results with the help of blind indexing, and non-sensitive data fetching also outperformed traditional clustering-based searches. By integrating sensitive data detection, encryption, and query optimization, this work advances the state of privacy-preserving computation in modern cloud infrastructures.

</details>


### [64] [Curator: Efficient Vector Search with Low-Selectivity Filters](https://arxiv.org/abs/2601.01291)
*Yicheng Jin,Yongji Wu,Wenjun Hu,Bruce M. Maggs,Jun Yang,Xiao Zhang,Danyang Zhuo*

Main category: cs.DB

TL;DR: 提出Curator分区索引，结合现有图索引解决低选择性过滤近邻搜索问题，降低查询延迟。


<details>
  <summary>Details</summary>
Motivation: 图基索引在低选择性过滤查询中存在连通性问题，且现有专门图索引构建成本高。

Method: 采用双索引架构，在共享聚类树中为不同标签构建专门索引，支持增量更新和处理复杂谓词。

Result: 与现有图索引集成，相比预过滤回退，低选择性查询延迟最多降低20.9倍，构建时间和内存占用分别仅增加5.5%和4.3%。

Conclusion: Curator能有效解决低选择性过滤近邻搜索问题，且成本增加较小。

Abstract: Embedding-based dense retrieval has become the cornerstone of many critical applications, where approximate nearest neighbor search (ANNS) queries are often combined with filters on labels such as dates and price ranges. Graph-based indexes achieve state-of-the-art performance on unfiltered ANNS but encounter connectivity breakdown on low-selectivity filtered queries, where qualifying vectors become sparse and the graph structure among them fragments. Recent research proposes specialized graph indexes that address this issue by expanding graph degree, which incurs prohibitively high construction costs. Given these inherent limitations of graph-based methods, we argue for a dual-index architecture and present Curator, a partition-based index that complements existing graph-based approaches for low-selectivity filtered ANNS. Curator builds specialized indexes for different labels within a shared clustering tree, where each index adapts to the distribution of its qualifying vectors to ensure efficient search while sharing structure to minimize memory overhead. The system also supports incremental updates and handles arbitrary complex predicates beyond single-label filters by efficiently constructing temporary indexes on the fly. Our evaluation demonstrates that integrating Curator with state-of-the-art graph indexes reduces low-selectivity query latency by up to 20.9x compared to pre-filtering fallback, while increasing construction time and memory footprint by only 5.5% and 4.3%, respectively.

</details>


### [65] [A Tool for Semantic-Aware Spatial Corpus Construction](https://arxiv.org/abs/2601.01415)
*Wei Huang,Xieyang Wang,Jianqiu Xu,Guidong Zhang*

Main category: cs.DB

TL;DR: 现有空间自然语言查询语料库稀缺影响系统性能，本文提出语义感知空间语料库构建工具SSCC，实验显示其提升知识库构建效率和查询对语料库有效性，为训练提供高质量语料支持。


<details>
  <summary>Details</summary>
Motivation: 空间自然语言接口系统因缺乏高质量空间自然语言查询语料库，现有构建方法效率低、语料质量不稳定，影响系统性能。

Method: 提出SSCC工具，包括基于空间关系的知识库构建模块和模板增强的查询对语料库生成模块。

Result: SSCC使知识库构建效率提升53倍，查询对语料库有效性提升2.5倍。

Conclusion: SSCC为空间自然语言接口训练提供高质量语料库，大幅降低语料库构建的时间和人力成本。

Abstract: Spatial natural language interface to database systems provide non-expert users with convenient access to spatial data through natural language queries. However, the scarcity of high-quality spatial natural language query corpora limits the performance of such systems. Existing methods rely on manual knowledge base construction and template-based dynamic generation, which suffer from low construction efficiency and unstable corpus quality. This paper presents semantic-aware spatial corpus construction (SSCC), a tool designed for constructing high-quality spatial natural language query and executable language query pair corpora. SSCC consists of two core modules: (i) a knowledge base construction module based on spatial relations, which extracts and determines spatial relations from datasets, and (ii) a template-augmented query pair corpus generation module, which produces query pairs via template matching and parameter substitution. The tool ensures geometric consistency and adherence to spatial logic in the generated spatial relations. Experimental results demonstrate that SSCC achieves (i) a 53x efficiency improvement for knowledge base construction and (ii) a 2.5x effectiveness improvement for query pair corpus. SSCC provides high-quality corpus support for spatial natural language interface training, substantially reducing both time and labor costs in corpus construction.

</details>


### [66] [RadixGraph: A Fast, Space-Optimized Data Structure for Dynamic Graph Storage (Extended Version)](https://arxiv.org/abs/2601.01444)
*Haoxuan Xie,Junfeng Liu,Siqiang Luo,Kai Wang*

Main category: cs.DB

TL;DR: 提出 RadixGraph 用于动态图存储，性能好且开源。


<details>
  <summary>Details</summary>
Motivation: 动态图规模增大，需高效存储和更新。

Method: 设计基于基数树的顶点索引，采用混合快照 - 日志架构存储边。

Result: 每秒支持数百万并发更新，在摄入图更新时比基线最多快 16.27 倍，平均减少 40.1% 内存使用。

Conclusion: RadixGraph 是快速且内存高效的动态图存储数据结构。

Abstract: Dynamic graphs model many real-world applications, and as their sizes grow, efficiently storing and updating them becomes critical. We present RadixGraph, a fast and memory-efficient data structure for dynamic graph storage. RadixGraph features a carefully designed radix-tree-based vertex index that strikes an optimal trade-off between query efficiency and space among all pointer-array-based radix trees. For edge storage, it employs a hybrid snapshot-log architecture that enables amortized $O(1)$ update time. RadixGraph supports millions of concurrent updates per second while maintaining competitive performance for graph analytics. Experimental results show that RadixGraph outperforms the most performant baseline by up to $16.27\times$ across various datasets in ingesting graph updates, and reduces memory usage by an average of $40.1\%$. RadixGraph is open-source at https://github.com/ForwardStar/RadixGraph.

</details>


### [67] [SafeLoad: Efficient Admission Control Framework for Identifying Memory-Overloading Queries in Cloud Data Warehouses](https://arxiv.org/abs/2601.01888)
*Yifan Wu,Yuhan Li,Zhenhua Wang,Zhongle Xie,Dingyu Yang,Ke Chen,Lidan Shou,Bo Tang,Liang Lin,Huan Li,Gang Chen*

Main category: cs.DB

TL;DR: 提出SafeLoad查询准入控制框架识别内存过载查询，发布SafeBench基准，实验显示SafeLoad性能优，提升精度、减少CPU时间浪费。


<details>
  <summary>Details</summary>
Motivation: 现有准入控制框架识别内存过载查询精度有限，且缺乏相关数据集，云数据仓库需高精度等特性的框架。

Method: SafeLoad先通过可解释判别规则过滤内存安全查询，采用混合架构结合全局和集群级模型，有误判纠正模块，还有自调优配额管理机制。

Result: SafeLoad实现了最先进的预测性能，在线和离线时间开销低，相比最佳基线精度最高提升66%，相比无SafeLoad场景CPU时间浪费最多减少8.09倍。

Conclusion: SafeLoad是有效的查询准入控制框架，能解决现有框架问题，提高识别内存过载查询的精度。

Abstract: Memory overload is a common form of resource exhaustion in cloud data warehouses. When database queries fail due to memory overload, it not only wastes critical resources such as CPU time but also disrupts the execution of core business processes, as memory-overloading (MO) queries are typically part of complex workflows. If such queries are identified in advance and scheduled to memory-rich serverless clusters, it can prevent resource wastage and query execution failure. Therefore, cloud data warehouses desire an admission control framework with high prediction precision, interpretability, efficiency, and adaptability to effectively identify MO queries. However, existing admission control frameworks primarily focus on scenarios like SLA satisfaction and resource isolation, with limited precision in identifying MO queries. Moreover, there is a lack of publicly available MO-labeled datasets with workloads for training and benchmarking. To tackle these challenges, we propose SafeLoad, the first query admission control framework specifically designed to identify MO queries. Alongside, we release SafeBench, an open-source, industrial-scale benchmark for this task, which includes 150 million real queries. SafeLoad first filters out memory-safe queries using the interpretable discriminative rule. It then applies a hybrid architecture that integrates both a global model and cluster-level models, supplemented by a misprediction correction module to identify MO queries. Additionally, a self-tuning quota management mechanism dynamically adjusts prediction quotas per cluster to improve precision. Experimental results show that SafeLoad achieves state-of-the-art prediction performance with low online and offline time overhead. Specifically, SafeLoad improves precision by up to 66% over the best baseline and reduces wasted CPU time by up to 8.09x compared to scenarios without SafeLoad.

</details>


### [68] [Vector Search for the Future: From Memory-Resident, Static Heterogeneous Storage, to Cloud-Native Architectures](https://arxiv.org/abs/2601.01937)
*Yitong Song,Xuanhe Zhou,Christian S. Jensen,Jianliang Xu*

Main category: cs.DB

TL;DR: 本文从存储架构角度回顾向量搜索技术的演变，先介绍内存驻留方法，再概述异构存储向量搜索技术，最后探讨云原生系统和未来研究机会。


<details>
  <summary>Details</summary>
Motivation: 随着向量数据规模快速增长，向量搜索在平衡搜索、延迟、可扩展性和成本方面面临挑战，需要从存储架构演变角度对相关技术进行梳理。

Method: 从存储架构角度，依次回顾内存驻留方法，对异构存储向量搜索技术进行系统概述，最后研究新兴云原生系统。

Result: 完成了对向量搜索技术演变从存储架构视角的全面回顾分析。

Conclusion: 向量搜索技术随着存储架构发展不断演变，当前正向内存 - SSD - 对象存储架构转变以实现高效数据分层和无缝扩展，且未来大规模向量检索系统有诸多研究机会待探索。

Abstract: Vector search (VS) has become a fundamental component in multimodal data management, enabling core functionalities such as image, video, and code retrieval. As vector data scales rapidly, VS faces growing challenges in balancing search, latency, scalability, and cost. The evolution of VS has been closely driven by changes in storage architecture. Early VS methods rely on all-in-memory designs for low latency, but scalability is constrained by memory capacity and cost. To address this, recent research has adopted heterogeneous architectures that offload space-intensive vectors and index structures to SSDs, while exploiting block locality and I/O-efficient strategies to maintain high search performance at billion scale. Looking ahead, the increasing demand for trillion-scale vector retrieval and cloud-native elasticity is driving a further shift toward memory-SSD-object storage architectures, which enable cost-efficient data tiering and seamless scalability.
  In this tutorial, we review the evolution of VS techniques from a storage-architecture perspective. We first review memory-resident methods, covering classical IVF, hash, quantization, and graph-based designs. We then present a systematic overview of heterogeneous storage VS techniques, including their index designs, block-level layouts, query strategies, and update mechanisms. Finally, we examine emerging cloud-native systems and highlight open research opportunities for future large-scale vector retrieval systems.

</details>


### [69] [AeroSketch: Near-Optimal Time Matrix Sketch Framework for Persistent, Sliding Window, and Distributed Streams](https://arxiv.org/abs/2601.02019)
*Hanyan Yin,Dongxie Wen,Jiajun Li,Zhewei Wei,Xiao Zhang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.DB

TL;DR: 提出AeroSketch矩阵草图框架，在多场景下有良好表现，实验显示其更新吞吐量优于现有方法，降低时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有矩阵草图技术在有限资源下进行实时分析时，常需频繁耗时的矩阵分解操作，限制更新效率。

Method: 引入AeroSketch框架，利用随机数值线性代数的最新进展。

Result: AeroSketch在多场景下实现最优通信和空间成本，更新时间复杂度接近最优，在更新吞吐量上优于现有方法，降低了时间复杂度。

Conclusion: AeroSketch在保持近似质量、最优通信和空间成本的同时，提升了更新效率。

Abstract: Many real-world matrix datasets arrive as high-throughput vector streams, making it impractical to store or process them in their entirety. To enable real-time analytics under limited computational, memory, and communication resources, matrix sketching techniques have been developed over recent decades to provide compact approximations of such streaming data. Some algorithms have achieved optimal space and communication complexity. However, these approaches often require frequent time-consuming matrix factorization operations. In particular, under tight approximation error bounds, each matrix factorization computation incurs cubic time complexity, thereby limiting their update efficiency.
  In this paper, we introduce AeroSketch, a novel matrix sketching framework that leverages recent advances in randomized numerical linear algebra (RandNLA). AeroSketch achieves optimal communication and space costs while delivering near-optimal update time complexity (within logarithmic factors) across persistent, sliding window, and distributed streaming scenarios. Extensive experiments on both synthetic and real-world datasets demonstrate that AeroSketch consistently outperforms state-of-the-art methods in update throughput. In particular, under tight approximation error constraints, AeroSketch reduces the cubic time complexity to the quadratic level. Meanwhile, it maintains comparable approximation quality while retaining optimal communication and space costs.

</details>


### [70] [Octopus: A Lightweight Entity-Aware System for Multi-Table Data Discovery and Cell-Level Retrieval](https://arxiv.org/abs/2601.02304)
*Wen-Zhi Li,Sainyam Galhotra*

Main category: cs.DB

TL;DR: 论文提出轻量级、无需训练的Octopus系统用于多表数据发现和单元格值检索，实验显示其性能优于现有系统且成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有数据发现系统假设单表解答问题，需大量预处理，而实际很多问题需多表信息且用户常查找特定单元格值，因此需要新系统。

Method: 使用LLM解析器从自然语言查询中识别细粒度实体，通过紧凑嵌入索引匹配表头，直接扫描表内容找值。

Result: 引入新基准测试，实验表明Octopus性能超越现有系统，计算和令牌成本显著降低。

Conclusion: Octopus是有效的多表数据发现和单元格值检索系统，能降低成本，提高性能。

Abstract: Tabular data constitute a dominant form of information in modern data lakes and repositories, yet discovering the relevant tables to answer user questions remains challenging. Existing data discovery systems assume that each question can be answered by a single table and often rely on resource-intensive offline preprocessing, such as model training or large-scale content indexing. In practice, however, many questions require information spread across multiple tables -- either independently or through joins -- and users often seek specific cell values rather than entire tables. In this paper, we present Octopus, a lightweight, entity-aware, and training-free system for multi-table data discovery and cell-level value retrieval. Instead of embedding entire questions, Octopus identifies fine-grained entities (column mentions and value mentions) from natural-language queries using an LLM parser. It then matches these entities to table headers through a compact embedding index and scans table contents directly for value occurrences, eliminating the need for heavy content indexing or costly offline stages. The resulting fine-grained alignment not only improves table retrieval accuracy but also facilitates efficient downstream NL2SQL execution by reducing token usage and redundant LLM calls. To evaluate Octopus, we introduce a new benchmark covering both table- and cell-level discovery under multi-table settings, including five datasets for independent discovery and two for join-based discovery. Experimental results show that Octopus consistently outperforms existing systems while achieving substantially lower computational and token costs. Code is available at https://github.com/wenzhilics/octopus.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [71] [A Multi-Port Concurrent Communication Model for handling Compute Intensive Tasks on Distributed Satellite System Constellations](https://arxiv.org/abs/2601.01031)
*Bharadwaj Veeravalli*

Main category: cs.DC

TL;DR: 本文为以中继为中心的分布式卫星系统开发了MPCC - DLT框架，给出最优负载分配和完成时间的闭式表达式，推导期限可行性条件，通过模拟展示任务性能，还添加实时准入控制机制，提供了可分析的模型。


<details>
  <summary>Details</summary>
Motivation: 为分布式卫星系统在异构星载处理和星间链路条件下实现并发数据传播、并行计算和结果返回，提供分析模型用于应用感知调度和系统级设计。

Method: 开发MPCC - DLT框架，推导最优负载分配、完成时间的闭式表达式和期限可行性条件，添加实时准入控制机制，进行大量仿真和实时模拟。

Result: 高可分布式任务显著降低延迟，通信密集型任务因结果传输开销收益递减，实时模拟展现任务结构和系统参数对期限满足和运行机制的影响。

Conclusion: 提供了首个可分析的MPCC - DLT模型，为未来卫星星座的系统设计提供行动性见解。

Abstract: We develop an integrated Multi-Port Concurrent Communication Divisible Load Theory (MPCC-DLT) framework for relay-centric distributed satellite systems (DSS), capturing concurrent data dissemination, parallel computation, and result return under heterogeneous onboard processing and inter-satellite link conditions. We propose a formulation that yields closed-form expressions for optimal load allocation and completion time that explicitly quantify the joint impact of computation speed, link bandwidth, and result-size overhead. We further derive deadline feasibility conditions that enable explicit sizing of cooperative satellite clusters to meet time-critical task requirements. Extensive simulation results demonstrate that highly distributable tasks achieve substantial latency reduction, while communication-heavy tasks exhibit diminishing returns due to result-transfer overheads. To bridge theory and practice, we extend the MPCC-DLT framework with a real-time admission control mechanism that handles stochastic task arrivals and deadline constraints, enabling blocking-aware operation. Our real-time simulations illustrate how task structure and system parameters jointly govern deadline satisfaction and operating regimes. Overall, this work provides the first analytically tractable MPCC-DLT model for distributed satellite systems and offers actionable insights for application-aware scheduling and system-level design of future satellite constellations.

</details>


### [72] [Performance and Security Aware Distributed Service Placement in Fog Computing](https://arxiv.org/abs/2601.01125)
*Mohammad Goudarzi,Arash Shaghaghi,Zhiyu Wang,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 针对雾计算中服务放置问题，提出SPA - DDRL框架，实验表明相比现有方法能显著提升服务响应时间和放置安全性。


<details>
  <summary>Details</summary>
Motivation: 物联网应用增长使雾计算中高效安全的服务放置需求增加，但现有方案多忽视安全影响。

Method: 将问题建模为加权多目标优化任务，采用分布式经纪人 - 学习者架构，集成LSTM网络、优先经验回放和离策略校正机制。

Result: 基于真实物联网工作负载的实验显示，响应时间提升16.3%，收敛速度加快33%，在各系统规模下都能保持合规解。

Conclusion: SPA - DDRL框架能有效联合优化雾计算中的服务响应时间和安全合规性。

Abstract: The rapid proliferation of IoT applications has intensified the demand for efficient and secure service placement in Fog computing. However, heterogeneous resources, dynamic workloads, and diverse security requirements make optimal service placement highly challenging. Most solutions focus primarily on performance metrics while overlooking the security implications of deployment decisions. This paper proposes a Security and Performance-Aware Distributed Deep Reinforcement Learning (SPA-DDRL) framework for joint optimization of service response time and security compliance in Fog computing. The problem is formulated as a weighted multi-objective optimization task, minimizing latency while maximizing a security score derived from the security capabilities of Fog nodes. The security score features a new three-tier hierarchy, where configuration-level checks verify proper settings, capability-level assessments evaluate the resource security features, and control-level evaluations enforce stringent policies, thereby ensuring compliant solutions that align with performance objectives. SPA-DDRL adopts a distributed broker-learner architecture where multiple brokers perform autonomous service-placement decisions and a centralized learner coordinates global policy optimization through shared prioritized experiences. It integrates three key improvements, including Long Short-Term Memory networks, Prioritized Experience Replay, and off-policy correction mechanisms to improve the agent's performance. Experiments based on real IoT workloads show that SPA-DDRL significantly improves both service response time and placement security compared to current approaches, achieving a 16.3% improvement in response time and a 33% faster convergence rate. It also maintains consistent, feasible, security-compliant solutions across all system scales, while baseline techniques fail or show performance degradation.

</details>


### [73] [OrchestrRL: Dynamic Compute and Network Orchestration for Disaggregated RL](https://arxiv.org/abs/2601.01209)
*Xin Tan,Yicheng Feng,Yu Zhou,Yimin Jiang,Yibo Zhu,Hong Xu*

Main category: cs.DC

TL;DR: 本文介绍OrchestrRL框架和RFabric网络，解决分布式强化学习中计算和网络问题，实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 分布式强化学习在生成阶段和网络流量方面面临挑战，需新方法解决。

Method: 采用自适应计算调度器调整并行度，共同设计可重构混合光 - 电网络RFabric。

Result: 在48个H800 GPU物理测试床上吞吐量提升1.40倍，RFabric性能 - 成本效率优于静态Fat - Tree网络。

Conclusion: OrchestrRL和RFabric是大规模强化学习工作负载的有效解决方案。

Abstract: Post-training with reinforcement learning (RL) has greatly enhanced the capabilities of large language models. Disaggregating the generation and training stages in RL into a parallel, asynchronous pipeline offers the potential for flexible scaling and improved throughput. However, it still faces two critical challenges. First, the generation stage often becomes a bottleneck due to dynamic workload shifts and severe execution imbalances. Second, the decoupled stages result in diverse and dynamic network traffic patterns that overwhelm conventional network fabrics. This paper introduces OrchestrRL, an orchestration framework that dynamically manages compute and network rhythms in disaggregated RL. To improve generation efficiency, OrchestrRL employs an adaptive compute scheduler that dynamically adjusts parallelism to match workload characteristics within and across generation steps. This accelerates execution while continuously rebalancing requests to mitigate stragglers. To address the dynamic network demands inherent in disaggregated RL -- further intensified by parallelism switching -- we co-design RFabric, a reconfigurable hybrid optical-electrical fabric. RFabric leverages optical circuit switches at selected network tiers to reconfigure the topology in real time, enabling workload-aware circuits for (i) layer-wise collective communication during training iterations, (ii) generation under different parallelism configurations, and (iii) periodic inter-cluster weight synchronization. We evaluate OrchestrRL on a physical testbed with 48 H800 GPUs, demonstrating up to a 1.40x throughput improvement. Furthermore, we develop RLSim, a high-fidelity simulator, to evaluate RFabric at scale. Our results show that RFabric achieves superior performance-cost efficiency compared to static Fat-Tree networks, establishing it as a highly effective solution for large-scale RL workloads.

</details>


### [74] [Making MoE based LLM inference resilient with Tarragon](https://arxiv.org/abs/2601.01310)
*Songyu Zhang,Aaron Tam,Myungjin Lee,Shixiong Qi,K. K. Ramakrishnan*

Main category: cs.DC

TL;DR: 提出弹性Mixture-of-Experts推理框架Tarragon，降低故障恢复成本和重新计算开销，大幅减少故障导致的停顿。


<details>
  <summary>Details</summary>
Motivation: 现有MoE系统故障恢复能力差，单一工作节点故障会触发全服务范围重启，不适合低延迟敏感的大语言模型服务。

Method: Tarragon将注意力工作节点和专家工作节点视为不同的故障域，引入可重构数据路径屏蔽故障，实现自修复机制，对有状态节点进行异步增量缓存检查点和按请求恢复，对无状态节点利用剩余GPU内存部署影子专家。

Result: 与MegaScale - Infer相比，Tarragon将故障导致的停顿减少160 - 213倍（从约64秒降至0.3 - 0.4秒），无故障时性能相当。

Conclusion: Tarragon能有效降低故障对系统的影响，提高MoE推理系统的弹性和性能。

Abstract: Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.
  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur.

</details>


### [75] [DiT-HC: Enabling Efficient Training of Visual Generation Model DiT on HPC-oriented CPU Cluster](https://arxiv.org/abs/2601.01500)
*Jinxiao Zhang,Yunpu Xu,Xiyong Wu,Runmin Dong,Shenggan Cheng,Yi Zhao,Mengxuan Chen,Qinrui Zheng,Jianting Liu,Haohuan Fu*

Main category: cs.DC

TL;DR: 本文提出DiT - HC系统在下一代HPC CPU集群上训练和扩展生成模型DiT，介绍三种关键技术，实验展示了加速效果和弱扩展效率，证明在CPU集群上大规模训练生成模型的可行性。


<details>
  <summary>Details</summary>
Motivation: 生成基础模型与传统数值模拟紧密结合，新硬件特性使CPU集群有机会加速和扩展此类模型，推动人工智能与科学计算统一。

Method: 提出DiT - HC系统，引入通信无张量并行（CFTP）与自动内存感知数据流、优化GEMM和算子内核的HCOps、自定义MPI后端重叠计算、通信和内存移动三种关键技术。

Result: 实验显示比原生或公共CPU库有8.2到87.7倍加速，在256个节点上有90.6%的弱扩展效率。

Conclusion: 证明了在CPU集群上大规模训练生成模型的可行性，为未来HPC - AI协同设计提供新见解。

Abstract: Generative foundation models have become an important tool for data reconstruction and simulation in scientific computing, showing a tight integration with traditional numerical simulations. At the same time, with the development of new hardware features, such as matrix acceleration units and high-bandwidth memory, CPU-based clusters offer promising opportunities to accelerate and scale such models, facilitating the unification of artificial intelligence and scientific computing. We present DiT-HC, the first system to train and scale the generative model DiT on a next-generation HPC CPU cluster. DiT-HC introduces three key techniques: (1) communication-free tensor parallelism (CFTP) with AutoMem for automated memory-aware dataflow, (2) HCOps, a suite of optimized GEMM and operator kernels leveraging vector and matrix acceleration units, and (3) a custom MPI backend that overlaps computation, communication, and memory movement. Experiments show 8.2 to 87.7 times speedups over native or public CPU libraries and 90.6% weak scaling efficiency on 256 nodes. These results demonstrate the feasibility of large-scale generative model training on CPU clusters and provide new insights for future HPC-AI co-design.

</details>


### [76] [FFCz: Fast Fourier Correction for Spectrum-Preserving Lossy Compression of Scientific Data](https://arxiv.org/abs/2601.01596)
*Congrong Ren,Robert Underwood,Sheng Di,Emrecan Kutay,Zarija Lukic,Aylin Yener,Franck Cappello,Hanqi Guo*

Main category: cs.DC

TL;DR: 提出基于快速傅里叶校正算法的有损压缩技术，校正现有压缩器误差，兼顾空间和频率域，用GPU加速，经多领域数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有压缩方法仅保证空间域精度，忽略频率域，而很多应用需同时保留数据的空间和频率表示。

Method: 提出算法校正如SZ3、ZFP和SPERR等现有压缩器误差；将频域误差表示为空间域误差的线性组合，推导联合边界区域；迭代投影空间误差向量到约束区域；用GPU并行加速。

Result: 用宇宙学模拟、X射线衍射、燃烧模拟和脑电图等数据集验证了该方法在保留空间和频率域关键科学信息方面的有效性。

Conclusion: 该方法能有效解决现有压缩方法不足，在多个领域数据压缩中有效保留空间和频率域的关键科学信息。

Abstract: This paper introduces a novel technique to preserve spectral features in lossy compression based on a novel fast Fourier correction algorithm\added{ for regular-grid data}. Preserving both spatial and frequency representations of data is crucial for applications such as cosmology, turbulent combustion, and X-ray diffraction, where spatial and frequency views provide complementary scientific insights. In particular, many analysis tasks rely on frequency-domain representations to capture key features, including the power spectrum of cosmology simulations, the turbulent energy spectrum in combustion, and diffraction patterns in reciprocal space for ptychography. However, existing compression methods guarantee accuracy only in the spatial domain while disregarding the frequency domain. To address this limitation, we propose an algorithm that corrects the errors produced by off-the-shelf ``base'' compressors such as SZ3, ZFP, and SPERR, thereby preserving both spatial and frequency representations by bounding errors in both domains. By expressing frequency-domain errors as linear combinations of spatial-domain errors, we derive a region that jointly bounds errors in both domains. Given as input the spatial errors from a base compressor and user-defined error bounds in the spatial and frequency domains, we iteratively project the spatial error vector onto the regions defined by the spatial and frequency constraints until it lies within their intersection. We further accelerate the algorithm using GPU parallelism to achieve practical performance. We validate our approach with datasets from cosmology simulations, X-ray diffraction, combustion simulation, and electroencephalography demonstrating its effectiveness in preserving critical scientific information in both spatial and frequency domains.

</details>


### [77] [RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference](https://arxiv.org/abs/2601.01712)
*Jiarui Wang,Huichao Chai,Yuanhang Zhang,Zongjin Zhou,Wei Guo,Xingkun Yang,Qiang Tang,Bo Pan,Jiawei Zhu,Ke Cheng,Yuting Yan,Shulan Wang,Yingjie Zhu,Zhengfan Yuan,Jiaqi Huang,Yuhan Zhang,Xiaosong Sun,Zhinan Zhang,Hong Zhu,Yongsheng Zhang,Tiantian Dong,Zhong Xiao,Deliang Liu,Chengzhou Lu,Yuan Sun,Zhiyuan Chen,Xinming Han,Zaizhu Liu,Yaoyuan Wang,Ziyang Zhang,Yong Liu,Jinxin Xu,Yajing Sun,Zhoujun Yu,Wenting Zhou,Qidong Zhang,Zhengyong Zhang,Zhonghai Gu,Yibo Jin,Yongxiang Feng,Pengfei Zuo*

Main category: cs.DC

TL;DR: 提出RelayGR系统实现生成式推荐（GR）的HBM接力推理，支持更长序列，提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 实时推荐系统排名阶段时间有限，GR模型在线序列长度受严格限制，希望借助预推理提高效率。

Method: 提出RelayGR系统，采用序列感知触发、亲和感知路由和内存感知扩展三种技术。

Result: 在华为昇腾NPUs上实现并评估，在固定P99 SLO下，支持长达1.5倍的序列，符合SLO的吞吐量提高达3.6倍。

Conclusion: RelayGR系统能够有效提升GR模型在实时推荐系统中的性能。

Abstract: Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\times$.

</details>


### [78] [pMSz: A Distributed Parallel Algorithm for Correcting Extrema and Morse Smale Segmentations in Lossy Compression](https://arxiv.org/abs/2601.01787)
*Yuxiao Li,Mingze Xia,Xin Liang,Bei Wang,Robert Underwood,Sheng Di,Hemant Sharma,Dishant Beniwal,Franck Cappello,Hanqi Guo*

Main category: cs.DC

TL;DR: 本文提出分布式并行算法修正有损压缩后拓扑特征，该算法简化MSz，减少通信开销，在128个GPU上实现超90%并行效率。


<details>
  <summary>Details</summary>
Motivation: 有损压缩会扭曲数据特征，影响后续分析，而现有单GPU的MSz算法无法处理大规模数据。

Method: 通过保留各位置的最陡升降方向简化MSz算法，减少进程间通信，弱化同步。

Result: 算法在Perlmutter超级计算机128个GPU上对真实数据集实现超90%的并行效率。

Conclusion: 提出的算法能有效解决单GPU算法难以处理大规模数据的问题，适用于对压缩后数据拓扑特征的修正。

Abstract: Lossy compression, widely used by scientists to reduce data from simulations, experiments, and observations, can distort features of interest even under bounded error. Such distortions may compromise downstream analyses and lead to incorrect scientific conclusions in applications such as combustion and cosmology. This paper presents a distributed and parallel algorithm for correcting topological features, specifically, piecewise linear Morse Smale segmentations (PLMSS), which decompose the domain into monotone regions labeled by their corresponding local minima and maxima. While a single GPU algorithm (MSz) exists for PLMSS correction after compression, no methodology has been developed that scales beyond a single GPU for extreme scale data. We identify the key bottleneck in scaling PLMSS correction as the parallel computation of integral paths, a communication-intensive computation that is notoriously difficult to scale. Instead of explicitly computing and correcting integral paths, our algorithm simplifies MSz by preserving steepest ascending and descending directions across all locations, thereby minimizing interprocess communication while introducing negligible additional storage overhead. With this simplified algorithm and relaxed synchronization, our method achieves over 90% parallel efficiency on 128 GPUs on the Perlmutter supercomputer for real world datasets.

</details>


### [79] [Bringing computation to the data: A MOEA-driven approach for optimising data processing in the context of the SKA and SRCNet](https://arxiv.org/abs/2601.01980)
*Manuel Parra-Royón,Álvaro Rodríguez-Gallardo,Susana Sánchez-Expósito,Laura Darriba-Pol,Jesús Sánchez-Castañeda,M. Ángeles Mendoza,Julián Garrido,Javier Moldón,Lourdes Verdes-Montenegro*

Main category: cs.DC

TL;DR: 针对SKA数据处理挑战，提出分布式和原位计算，结合FaaS与基于EA的决策实体优化工作流，建立计算到数据策略框架。


<details>
  <summary>Details</summary>
Motivation: SKA产生海量数据，传统数据中心计算模型因网络和存储瓶颈不再可行，需高效数据处理方法。

Method: 采用分布式和原位计算，将FaaS与基于EA的智能决策实体集成，用MOEAs探索近最优执行计划。

Result: 建立了SRCNet架构内高效且考虑成本的计算到数据策略的基线框架。

Conclusion: 提出的方法可应对SKA数据处理挑战，实现高效数据处理。

Abstract: The Square Kilometre Array (SKA) will generate unprecedented data volumes, making efficient data processing a critical challenge. Within this context, the SKA Regional Centres Network (SRCNet) must operate in a near-exascale environment where traditional data-centric computing models based on moving large datasets to centralised resources are no longer viable due to network and storage bottlenecks.
  To address this limitation, this work proposes a shift towards distributed and in-situ computing, where computation is moved closer to the data. We explore the integration of Function-as-a-Service (FaaS) with an intelligent decision-making entity based on Evolutionary Algorithms (EAs) to optimise data-intensive workflows within SRCNet. FaaS enables lightweight and modular function execution near data sources while abstracting infrastructure management.
  The proposed decision-making entity employs Multi-Objective Evolutionary Algorithms (MOEAs) to explore near-optimal execution plans considering execution time and energy consumption, together with constraints related to data location and transfer costs. This work establishes a baseline framework for efficient and cost-aware computation-to-data strategies within the SRCNet architecture.

</details>


### [80] [SuperSFL: Resource-Heterogeneous Federated Split Learning with Weight-Sharing Super-Networks](https://arxiv.org/abs/2601.02092)
*Abdullah Al Asif,Sixing Yu,Juan Pablo Munoz,Arya Mazaheri,Ali Jannesari*

Main category: cs.DC

TL;DR: 本文提出SuperSFL框架应对SplitFed Learning在异构环境的挑战，通过权重共享超网络生成子网、引入优化机制等，实验表明其比基线方法收敛快、准确率高、通信成本低且训练时间短，提升能源效率。


<details>
  <summary>Details</summary>
Motivation: SplitFed Learning在异构环境面临计算和通信能力差异带来的挑战。

Method: 提出SuperSFL框架，利用权重共享超网络生成客户端特定子网；引入Three - Phase Gradient Fusion优化机制；采用容错客户端分类器和协作式客户端 - 服务器聚合。

Result: 在CIFAR - 10和CIFAR - 100实验中，SuperSFL通信轮次收敛快2 - 5倍，准确率更高，通信成本降低20倍，训练时间缩短13倍，能源效率提高。

Conclusion: SuperSFL是异构边缘环境下联邦学习的实用解决方案。

Abstract: SplitFed Learning (SFL) combines federated learning and split learning to enable collaborative training across distributed edge devices; however, it faces significant challenges in heterogeneous environments with diverse computational and communication capabilities. This paper proposes \textit{SuperSFL}, a federated split learning framework that leverages a weight-sharing super-network to dynamically generate resource-aware client-specific subnetworks, effectively mitigating device heterogeneity. SuperSFL introduces Three-Phase Gradient Fusion (TPGF), an optimization mechanism that coordinates local updates, server-side computation, and gradient fusion to accelerate convergence. In addition, a fault-tolerant client-side classifier and collaborative client--server aggregation enable uninterrupted training under intermittent communication failures. Experimental results on CIFAR-10 and CIFAR-100 with up to 100 heterogeneous clients show that SuperSFL converges $2$--$5\times$ faster in terms of communication rounds than baseline SFL while achieving higher accuracy, resulting in up to $20\times$ lower total communication cost and $13\times$ shorter training time. SuperSFL also demonstrates improved energy efficiency compared to baseline methods, making it a practical solution for federated learning in heterogeneous edge environments.

</details>


### [81] [BigSUMO: A Scalable Framework for Big Data Traffic Analytics and Parallel Simulation](https://arxiv.org/abs/2601.02286)
*Rahul Sengupta,Nooshin Yousefzadeh,Manav Sanghvi,Yash Ranjan,Anand Rangarajan,Sanjay Ranka,Yashaswi Karnati,Jeremy Dilmore,Tushar Patel,Ryan Casburn*

Main category: cs.DC

TL;DR: 提出用于交通数据分析、中断检测和并行模拟的开源框架BigSUMO。


<details>
  <summary>Details</summary>
Motivation: 随着全球城市化进程，需要有效工具分析大量交通数据并做出干预。

Method: 摄入高分辨率数据和稀疏轨迹数据，先进行描述性分析和中断检测，再用SUMO进行规范性分析，模块化设计可集成不同算法。

Result: 构建了成本低、可扩展且易部署的分析框架。

Conclusion: 希望BigSUMO对智能城市出行解决方案有帮助。

Abstract: With growing urbanization worldwide, efficient management of traffic infrastructure is critical for transportation agencies and city planners. It is essential to have tools that help analyze large volumes of stored traffic data and make effective interventions. To address this need, we present ``BigSUMO", an end-to-end, scalable, open-source framework for analytics, interruption detection, and parallel traffic simulation. Our system ingests high-resolution loop detector and signal state data, along with sparse probe trajectory data. It first performs descriptive analytics and detects potential interruptions. It then uses the SUMO microsimulator for prescriptive analytics, testing hundreds of what-if scenarios to optimize traffic performance. The modular design allows integration of different algorithms for data processing and outlier detection. Built using open-source software and libraries, the pipeline is cost-effective, scalable, and easy to deploy. We hope BigSUMO will be a valuable aid in developing smart city mobility solutions.

</details>


### [82] [Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies](https://arxiv.org/abs/2601.02311)
*Deep Pankajbhai Mehta*

Main category: cs.DC

TL;DR: 提出放置语义框架预测大语言模型并行策略行为，预测与结果匹配，证明训练条件并提供组合规则。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型并行策略选择靠试错，缺乏统一系统框架预测其行为。

Method: 引入放置语义，通过指定策略对四种训练状态在设备上的放置模式，从放置情况推导内存消耗和通信量。

Result: 预测结果与已发表结果完全匹配，如ZeRO - 3内存使用和通信成本情况。

Conclusion: 框架统一了多种并行策略，证明了分布式训练匹配单设备结果的条件并提供组合规则。

Abstract: Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [83] [The cost of cyclic permutations and remainder sums in the Euclidean algorithm](https://arxiv.org/abs/2601.00979)
*Valentin Blomer,Kai-Uwe Bux*

Main category: cs.DS

TL;DR: 讨论Gries - Mills块交换方案的修改，平均每元素移动1.85次，最坏情况3次，平均情况分析依赖欧几里得算法余数和渐近行为。


<details>
  <summary>Details</summary>
Motivation: 对Gries - Mills块交换方案进行改进，以优化原地旋转的平均成本。

Method: 通过分析欧几里得算法中余数和的渐近行为来进行改进方案的分析。

Result: 提出的修改方案平均每元素移动1.85次，最坏情况仍为3次。

Conclusion: 该修改方案在平均移动次数上有所优化。

Abstract: We discuss a modification to the Gries-Mills block swapping scheme for in-place rotation with average costs of 1.85 moves per element and worst case performance still at 3 moves per element. Analysis of the average case relies on the asymptotic behavior of the sum of remainders in the Euclidean algorithm.

</details>


### [84] [AGIS: Fast Approximate Graph Pattern Mining with Structure-Informed Sampling](https://arxiv.org/abs/2601.01388)
*Seoyong Lee,Jinho Lee*

Main category: cs.DS

TL;DR: 本文介绍AGIS系统用于近似图模式挖掘，使用结构感知邻域采样，实验显示其性能远超现有系统并将开源


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的近似图模式挖掘系统依赖均匀采样，忽略潜在概率分布，限制了对更多模式的可扩展性

Method: 采用结构感知邻域采样，推导理想采样分布并给出近似方法，开发平衡收敛速度和计算开销的方法

Result: AGIS显著优于现有系统，实现28.5倍几何平均加速，特定情况下超10万倍加速，能处理数十亿边的图和多样模式，几秒内提供准确估计

Conclusion: AGIS性能优异，将开源以推动该领域研究

Abstract: Approximate Graph Pattern Mining (AGPM) is essential for analyzing large-scale graphs where exact counting is computationally prohibitive. While there exist numerous sampling-based AGPM systems, they all rely on uniform sampling and overlook the underlying probability distribution. This limitation restricts their scalability to a broader range of patterns.
  In this paper, we introduce AGIS, an extremely fast AGPM system capable of counting arbitrary patterns from huge graphs. AGIS employs structure-informed neighbor sampling, a novel sampling technique that deviates from uniformness but allocates specific sampling probabilities based on the pattern structure. We first derive the ideal sampling distribution for AGPM and then present a practical method to approximate it. Furthermore, we develop a method that balances convergence speed and computational overhead, determining when to use the approximated distribution.
  Experimental results demonstrate that AGIS significantly outperforms the state-of-the-art AGPM system, achieving 28.5x geometric mean speedup and more than 100,000x speedup in specific cases. Furthermore, AGIS is the only AGPM system that scales to graphs with tens of billions of edges and robustly handles diverse patterns, successfully providing accurate estimates within seconds. We will open-source AGIS to encourage further research in this field.

</details>


### [85] [Derandomizing Pseudopolynomial Algorithms for Subset Sum](https://arxiv.org/abs/2601.01390)
*Timothy M. Chan*

Main category: cs.DS

TL;DR: 提出首个确定性算法解决子集和问题，时间复杂度为 Õ(t)，该技术还有其他应用。


<details>
  <summary>Details</summary>
Motivation: 改进经典子集和问题的已有算法，实现更快的确定性算法并解决相关算法的去随机化问题。

Method: 提出一种新的技术来设计算法。

Result: 得到首个运行时间为 Õ(t) 的确定性算法，对其他随机算法进行去随机化。

Conclusion: 新算法和技术在子集和问题及相关问题上有较好表现，提升了算法效率和解决问题的能力。

Abstract: We reexamine the classical subset sum problem: given a set $X$ of $n$ positive integers and a number $t$, decide whether there exists a subset of $X$ that sums to $t$; or more generally, compute the set $\mbox{out}$ of all numbers $y\in\{0,\ldots,t\}$ for which there exists a subset of $X$ that sums to $y$. Standard dynamic programming solves the problem in $O(tn)$ time. In SODA'17, two papers appeared giving the current best deterministic and randomized algorithms, ignoring polylogarithmic factors: Koiliaris and Xu's deterministic algorithm runs in $\widetilde{O}(t\sqrt{n})$ time, while Bringmann's randomized algorithm runs in $\widetilde{O}(t)$ time. We present the first deterministic algorithm running in $\widetilde{O}(t)$ time.
  Our technique has a number of other applications: for example, we can also derandomize the more recent output-sensitive algorithms by Bringmann and Nakos [STOC'20] and Bringmann, Fischer, and Nakos [SODA'25] running in $\widetilde{O}(|\mbox{out}|^{4/3})$ and $\widetilde{O}(|\mbox{out}|\sqrt{n})$ time, and we can derandomize a previous fine-grained reduction from 0-1 knapsack to min-plus convolution by Cygan et al. [ICALP'17].

</details>


### [86] [Publishing Below-Threshold Triangle Counts under Local Weight Differential Privacy](https://arxiv.org/abs/2601.01710)
*Kevin Pfisterer,Quentin Hillebrand,Vorapong Suppakitpaisarn*

Main category: cs.DS

TL;DR: 提出在局部权重差分隐私下对加权图中低于阈值三角形计数的算法，通过两轮通信实现，有改进措施并给出实验结果。


<details>
  <summary>Details</summary>
Motivation: 以往研究集中于无权图，而现实网络多含边权重，需保护个体对边权重影响的隐私。

Method: 算法包含两轮通信，首轮节点按局部权重差分隐私发布权重信息，次轮本地计数低于阈值三角形，有偏和无偏两种变体，并有减少协方差和计算平滑敏感度的改进。

Result: 通过实验展示了有偏和无偏变体的差异及改进措施的有效性。

Conclusion: 所提算法及改进措施在加权图低于阈值三角形计数上有效且能保护隐私。

Abstract: We propose an algorithm for counting below-threshold triangles in weighted graphs under local weight differential privacy. While prior work focused on unweighted graphs, many real-world networks naturally include edge weights. We study the setting where the graph topology is public known and the privacy of the influence of an individual on the edge weights is protected. This captures realistic scenarios such as road networks and telecommunication networks. Our approach consists of two rounds of communication. In the first round, each node publishes their incident weight information under local weight differential privacy while in the second round, the nodes locally count below-threshold triangles, for which we introduce a biased and unbiased variant. We further propose two different improvements. We present a pre-computation step that reduces the covariance and thereby lowers the expected error. Secondly, we develop an algorithm for computing the smooth-sensitivity, which significantly reduces the running time compared to a straightforward approach. Finally, we provide experimental results that demonstrate the differences between the biased and unbiased variants and the effectiveness of the proposed improvements.

</details>


### [87] [Improved Approximation Algorithms for the Multiple-Depot Split Delivery Vehicle Routing Problem](https://arxiv.org/abs/2601.01841)
*Jingyang Zhao,Yonghang Su,Mingyu Xiao*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Multiple-Depot Split Delivery Vehicle Routing Problem (MD-SDVRP) is a challenging problem with broad applications in logistics. The goal is to serve customers' demand using a fleet of capacitated vehicles located in multiple depots, where each customer's demand can be served by more than one vehicle, while minimizing the total travel cost of all vehicles. We study approximation algorithms for this problem. Previously, the only known result was a $6$-approximation algorithm for a constant number of depots (INFORMS J. Comput. 2023), and whether this ratio could be improved was left as an open question. In this paper, we resolve it by proposing a $(6-2\cdot 10^{-36})$-approximation algorithm for this setting. Moreover, we develop constant-factor approximation algorithms that work beyond a constant number of depots, improved parameterized approximation algorithms related to the vehicle capacity and the number of depots, as well as bi-factor approximation algorithms.

</details>


### [88] [Exact Clique Number Manipulation via Edge Interdiction](https://arxiv.org/abs/2601.01869)
*Yi Zhou,Haoyu Jiang,Chenghao Zhu,André Rossi*

Main category: cs.DS

TL;DR: 研究Edge Interdiction Clique Problem (EICP)，提出新混合整数线性公式和两阶段精确算法RLCM，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: EICP有实际应用价值，但现有求解方法在图规模和阻断预算k增长时缺乏扩展性。

Method: 提出新混合整数线性公式将问题转化为参数化Edge Blocker Clique Problems (EBCP)，并提出两阶段精确算法RLCM，先缩图再用定制分支割平面框架求解。

Result: 在最大团基准图、大型现实稀疏网络和随机图上的大量实验表明，RLCM始终优于现有方法。

Conclusion: 新提出的方法在解决EICP上有更好的性能，能有效应对现有方法的不足。

Abstract: The Edge Interdiction Clique Problem (EICP) aims to remove at most $k$ edges from a graph so as to minimize the size of the largest clique in the remaining graph. This problem captures a fundamental question in graph manipulation: which edges are structurally critical for preserving large cliques? Such a problem is also motivated by practical applications including protein function maintenance and image matching. The EICP is computationally challenging and belongs to a complexity class beyond NP. Existing approaches rely on general mixed-integer bilevel programming solvers or reformulate the problem into a single-level mixed integer linear program. However, they are still not scalable when the graph size and interdiction budget $k$ grow. To overcome this, we investigate new mixed integer linear formulations, which recast the problem into a sequence of parameterized Edge Blocker Clique Problems (EBCP). This perspective decomposes the original problem into simpler subproblems and enables tighter modeling of clique-related inequalities. Furthermore, we propose a two-stage exact algorithm, \textsc{RLCM}, which first applies problem-specific reduction techniques to shrink the graph and then solves the reduced problem using a tailored branch-and-cut framework. Extensive computational experiments on maximum clique benchmark graphs, large real-world sparse networks, and random graphs demonstrate that \textsc{RLCM} consistently outperforms existing approaches.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [89] [Bad News for Couples: Tight Lower Bounds for Fair Division of Indivisible Items](https://arxiv.org/abs/2601.01012)
*Max Dupré la Tour*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of fairly allocating indivisible goods to couples, where each couple consists of two agents with distinct additive valuations. We show that there exist instances of allocating indivisible items to $n$ couples for which envy-freeness up to $Ω(\sqrt{n})$ items cannot be guaranteed. This closes the gap by matching the upper bound of Manurangsi and Suksompong, which applies to arbitrary instances with $n$ agents in total. This result is somewhat surprising, as that upper bound was conjectured not to be tight for instances consisting only of small groups.

</details>


### [90] [Carroll Mechanisms: Opportunities, Challenges, and Agenda](https://arxiv.org/abs/2601.01013)
*Philip N. Brown,Connor McCormick*

Main category: cs.GT

TL;DR: 介绍Carroll机制目的，其基于网络组合LMSR构建，虽有进展但仍有问题，本文记录理论基础、明确问题并提出研究计划。


<details>
  <summary>Details</summary>
Motivation: 促进自主群体的意义建构和合理决策，激励参与者透明化推理过程，赋予能改变想法的参与者权力。

Method: 基于网络组合LMSR构建Carroll机制，本文记录理论基础、明确现存问题并提出研究计划。

Result: 在2025年秋季构建基础方面取得了很大进展，但仍存在一些重要问题并有新问题出现。

Conclusion: 通过记录理论基础、明确问题和提出研究计划来进一步推进相关研究。

Abstract: The purpose of Carroll Mechanisms is to facilitate autonomous group sensemaking and reasoned decisionmaking by incentivizing participants to be transparent about their reasoning process, and to empower participants who are known to be capable of changing their minds. We envision Carroll Mechanisms to be built on top of a networked combinatorial LMSR foundation and thus to inherit the desriable properties of market scoring rules and automated market-makers. While we have made great strides during Fall 2025 in building out this foundation, several significant questions remain and several major new questions have arisen as a result of this work. The purpose of this document is to document the theoretical foundation, frame these questions clearly, and propose a research plan to address the questions.

</details>


### [91] [The Optimal Sample Complexity of Linear Contracts](https://arxiv.org/abs/2601.01496)
*Mikael Møller Høgsgaard*

Main category: cs.GT

TL;DR: 本文解决离线环境下从数据中学习最优线性合约问题，证明EUM算法的最优性及样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决在离线环境下，当代理类型来自未知分布时，委托人设计最大化期望效用的最优线性合约的问题。

Method: 使用链式论证，利用线性合约期望奖励非递减的结构特性构建细粒度网络。

Result: EUM算法以至少1 - δ的概率得到最优线性合约的ε - 近似，样本复杂度为O(ln(1/δ) / ε²)，改进了已知边界并证明最优性，还建立了一致收敛的更强保证。

Conclusion: EUM算法在学习最优线性合约方面具有最优的样本复杂度，能有效解决相关问题。

Abstract: In this paper, we settle the problem of learning optimal linear contracts from data in the offline setting, where agent types are drawn from an unknown distribution and the principal's goal is to design a contract that maximizes her expected utility. Specifically, our analysis shows that the simple Empirical Utility Maximization (EUM) algorithm yields an $\varepsilon$-approximation of the optimal linear contract with probability at least $1-δ$, using just $O(\ln(1/δ) / \varepsilon^2)$ samples. This result improves upon previously known bounds and matches a lower bound from Duetting et al. [2025] up to constant factors, thereby proving its optimality. Our analysis uses a chaining argument, where the key insight is to leverage a simple structural property of linear contracts: their expected reward is non-decreasing. This property, which holds even though the utility function itself is non-monotone and discontinuous, enables the construction of fine-grained nets required for the chaining argument, which in turn yields the optimal sample complexity. Furthermore, our proof establishes the stronger guarantee of uniform convergence: the empirical utility of every linear contract is a $\varepsilon$-approximation of its true expectation with probability at least $1-δ$, using the same optimal $O(\ln(1/δ) / \varepsilon^2)$ sample complexity.

</details>


### [92] [Existence of Optimal Mechanisms for Selling Multiple Goods: An Elementary Proof](https://arxiv.org/abs/2601.01607)
*Sergiu Hart,Noam Nisan*

Main category: cs.GT

TL;DR: 提供多参数设置下，估值分布有有限期望时收益最大化机制存在的初等证明


<details>
  <summary>Details</summary>
Motivation: 证明多参数设置下收益最大化机制的存在性

Method: 使用初等证明方法

Result: 证明收益最大化机制存在

Conclusion: 在估值分布有有限期望的多参数设置中，存在收益最大化的机制

Abstract: We provide an elementary proof that revenue-maximizing mechanisms exist in multi-parameter settings whenever the distribution of valuations has finite expectation.

</details>


### [93] [Metric Distortion with Preference Intensities](https://arxiv.org/abs/2601.02095)
*Mehrad Abbaszadeh,Ali Ansarifar,Mohamad Latifian,Masoud Seddighin*

Main category: cs.GT

TL;DR: 本文从度量失真角度研究带强度的排序选票格式，设计了Positional Scoring Matching规则，该规则考虑强度因素，失真低于3，且证明忽略强度会大幅增加失真。


<details>
  <summary>Details</summary>
Motivation: 研究带强度的排序选票格式从度量失真角度的潜力，此前相关工作聚焦于功利主义失真框架。

Method: 设计Positional Scoring Matching规则类，通过解决零和博弈找到该类规则中针对此问题的最优规则。

Result: 设计的投票规则考虑强度因素后失真低于3，证明忽略强度会在失真方面有较大损失。

Conclusion: 使用带强度的排序选票格式并在设计投票规则时考虑强度因素，能有效降低度量失真。

Abstract: In voting with ranked ballots, each agent submits a strict ranking of the form $a \succ b \succ c \succ d$ over the alternatives, and the voting rule decides on the winner based on these rankings. Although this ballot format has desirable characteristics, there is a question of whether it is expressive enough for the agents. Kahng, Latifian, and Shah address this issue by adding intensities to the rankings. They introduce the ranking with intensities ballot format, where agents can use both $\succ\!\!\succ$ and $\succ$ in their rankings to express intensive and normal preferences between consecutive alternatives in their rankings. While they focus on analyzing this ballot format in the utilitarian distortion framework, in this work, we look at the potential of using this ballot format from the metric distortion viewpoint. We design a class of voting rules coined Positional Scoring Matching rules, which can be used for different problems in the metric setting, and show that by solving a zero-sum game, we can find the optimal member of this class for our problem. This rule takes intensities into account and achieves a distortion lower than $3$. In addition, by proving a bound on the price of ignoring intensities, we show that we might lose a great deal in terms of distortion by not taking the intensities into account.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [94] [A Knowledge Graph and Deep Learning-Based Semantic Recommendation Database System for Advertisement Retrieval and Personalization](https://arxiv.org/abs/2601.00833)
*Tangtang Wang,Kaijie Zhang,Kuangcong Liu*

Main category: cs.IR

TL;DR: 本文提出基于知识图谱和深度学习的语义推荐数据库系统KGSR - ADS用于广告检索和个性化，架构能实现准确语义匹配和可扩展检索。


<details>
  <summary>Details</summary>
Motivation: 现代数字营销中广告数据复杂，需要能理解产品、受众和广告内容语义关系的智能系统。

Method: 提出KGSR - ADS系统，整合Ad - KG、语义嵌入层、GNN + 注意力模型和基于向量索引的数据库优化与检索层。

Result: 该分层架构可实现准确语义匹配和可扩展检索。

Conclusion: KGSR - ADS系统可在大规模异构工作负载下进行个性化广告推荐。

Abstract: In modern digital marketing, the growing complexity of advertisement data demands intelligent systems capable of understanding semantic relationships among products, audiences, and advertising content. To address this challenge, this paper proposes a Knowledge Graph and Deep Learning-Based Semantic Recommendation Database System (KGSR-ADS) for advertisement retrieval and personalization. The proposed framework integrates a heterogeneous Ad-Knowledge Graph (Ad-KG) that captures multi-relational semantics, a Semantic Embedding Layer that leverages large language models (LLMs) such as GPT and LLaMA to generate context-aware vector representations, a GNN + Attention Model that infers cross-entity dependencies, and a Database Optimization & Retrieval Layer based on vector indexing (FAISS/Milvus) for efficient semantic search. This layered architecture enables both accurate semantic matching and scalable retrieval, allowing personalized ad recommendations under large-scale heterogeneous workloads.

</details>


### [95] [Enhancing Retrieval-Augmented Generation with Topic-Enriched Embeddings: A Hybrid Approach Integrating Traditional NLP Techniques](https://arxiv.org/abs/2601.00891)
*Rodrigo Kataishi*

Main category: cs.IR

TL;DR: 本文提出主题丰富嵌入方法改进检索增强生成系统的检索质量，在法律文本语料实验中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成系统在主题重叠和主题变化大的语料中检索质量下降。

Method: 将TF - IDF与主题建模和降维结合，用LSA和LDA编码潜在主题结构，并与紧凑上下文编码器融合。

Result: 在法律文本语料实验中，聚类连贯性和检索指标有持续提升。

Conclusion: 主题丰富嵌入可作为更可靠知识密集型RAG管道的实用组件。

Abstract: Retrieval-augmented generation (RAG) systems rely on accurate document retrieval to ground large language models (LLMs) in external knowledge, yet retrieval quality often degrades in corpora where topics overlap and thematic variation is high. This work proposes topic-enriched embeddings that integrate term-based signals and topic structure with contextual sentence embeddings. The approach combines TF-IDF with topic modeling and dimensionality reduction, using Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) to encode latent topical organization, and fuses these representations with a compact contextual encoder (all-MiniLM). By jointly capturing term-level and topic-level semantics, topic-enriched embeddings improve semantic clustering, increase retrieval precision, and reduce computational burden relative to purely contextual baselines. Experiments on a legal-text corpus show consistent gains in clustering coherence and retrieval metrics, suggesting that topic-enriched embeddings can serve as a practical component for more reliable knowledge-intensive RAG pipelines.

</details>


### [96] [The Discovery Gap: How Product Hunt Startups Vanish in LLM Organic Discovery Queries](https://arxiv.org/abs/2601.00912)
*Amit Prakash Sharma*

Main category: cs.IR

TL;DR: 研究测试两款大语言模型对初创产品的识别情况，发现按名称询问识别率高，发现式问题识别率低，GEO与发现率无关，建议先打好SEO基础。


<details>
  <summary>Details</summary>
Motivation: 探究ChatGPT等大语言模型回应中是否会出现新推出的项目管理工具等问题。

Method: 从2025年Product Hunt排行榜前500产品中随机选112家初创公司，对ChatGPT和Perplexity进行2240次查询测试。

Result: 按名称询问时两款模型识别率高，发现式问题识别率低；GEO与发现率无关联；Perplexity中传统SEO信号和社区存在度影响可见性。

Conclusion: 不要直接为AI发现进行优化，应先打好SEO基础以提升大语言模型可见性。

Abstract: When someone asks ChatGPT to recommend a project management tool, which products show up in the response? And more importantly for startup founders: will their newly launched product ever appear? This research set out to answer these questions.
  I randomly selected 112 startups from the top 500 products featured on the 2025 Product Hunt leaderboard and tested each one across 2,240 queries to two different large language models: ChatGPT (gpt-4o-mini) and Perplexity (sonar with web search).
  The results were striking. When users asked about products by name, both LLMs recognized them almost perfectly: 99.4% for ChatGPT and 94.3% for Perplexity. But when users asked discovery-style questions like "What are the best AI tools launched this year?" the success rates collapsed to 3.32% and 8.29% respectively. That's a gap of 30-to-1 for ChatGPT.
  Perhaps the most surprising finding was that Generative Engine Optimization (GEO), the practice of optimizing website content for AI visibility, showed no correlation with actual discovery rates. Products with high GEO scores were no more likely to appear in organic queries than products with low scores.
  What did matter? For Perplexity, traditional SEO signals like referring domains (r = +0.319, p < 0.001) and Product Hunt ranking (r = -0.286, p = 0.002) predicted visibility. After cleaning the Reddit data for false positives, community presence also emerged as significant (r = +0.395, p = 0.002).
  The practical takeaway is counterintuitive: don't optimize for AI discovery directly. Instead, build the SEO foundation first and LLM visibility will follow.

</details>


### [97] [MACA: A Framework for Distilling Trustworthy LLMs into Efficient Retrievers](https://arxiv.org/abs/2601.00926)
*Satya Swaroop Gudipudi,Sahil Girhepuje,Ponnurangam Kumaraguru,Kristine Ma*

Main category: cs.IR

TL;DR: 提出Metadata - Aware Cross - Model Alignment (MACA)方法，将元数据感知的LLM重排器提炼到紧凑学生检索器，避免在线LLM调用，在银行FAQ语料上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现代企业检索系统处理短且不明确查询时，逐查询的大语言模型重排和手动标注成本高。

Method: 提出MACA方法，用元数据感知提示验证教师模型可信度，提供列表分数、硬负样本和校准相关性边际；学生模型用MetaFusion目标训练。

Result: MACA教师模型在专有集和BankFAQs上的Accuracy@1超过MAFA基线；MACA学生模型大幅超越预训练编码器，如MiniLM在专有语料上Accuracy@1从0.23提升到0.48。

Conclusion: MACA方法能在避免在线LLM调用的情况下，提升检索性能并支持检索增强生成。

Abstract: Modern enterprise retrieval systems must handle short, underspecified queries such as ``foreign transaction fee refund'' and ``recent check status''. In these cases, semantic nuance and metadata matter but per-query large language model (LLM) re-ranking and manual labeling are costly. We present Metadata-Aware Cross-Model Alignment (MACA), which distills a calibrated metadata aware LLM re-ranker into a compact student retriever, avoiding online LLM calls. A metadata-aware prompt verifies the teacher's trustworthiness by checking consistency under permutations and robustness to paraphrases, then supplies listwise scores, hard negatives, and calibrated relevance margins. The student trains with MACA's MetaFusion objective, which combines a metadata conditioned ranking loss with a cross model margin loss so it learns to push the correct answer above semantically similar candidates with mismatched topic, sub-topic, or entity. On a proprietary consumer banking FAQ corpus and BankFAQs, the MACA teacher surpasses a MAFA baseline at Accuracy@1 by five points on the proprietary set and three points on BankFAQs. MACA students substantially outperform pretrained encoders; e.g., on the proprietary corpus MiniLM Accuracy@1 improves from 0.23 to 0.48, while keeping inference free of LLM calls and supporting retrieval-augmented generation.

</details>


### [98] [AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation](https://arxiv.org/abs/2601.00930)
*Nicolas Bougie,Gian Maria Marconi,Tony Yip,Narimasa Watanabe*

Main category: cs.IR

TL;DR: 提出AlignUSER框架学习世界模型驱动的代理来评估推荐系统，比先前工作更接近真实用户行为。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM代理评估推荐系统依赖少样本提示，对环境理解浅，难以忠实复现用户行为，且离线指标与用户行为有差距、交互数据稀缺。

Method: 将世界建模为下一个状态预测任务帮助代理内化环境，围绕演示生成反事实轨迹，让LLM对比决策与人类选择、识别次优动作并提取教训，用学习的策略驱动代理与推荐系统交互。

Result: 在多个数据集上评估，在微观和宏观层面都比先前工作更接近真实人类。

Conclusion: AlignUSER框架在评估推荐系统方面比现有基于LLM代理的方法更优，能更好地模拟真实用户行为。

Abstract: Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.

</details>


### [99] [ScienceDB AI: An LLM-Driven Agentic Recommender System for Large-Scale Scientific Data Sharing Services](https://arxiv.org/abs/2601.01118)
*Qingqing Long,Haotian Chen,Chenyang Zhao,Xiaolei Du,Xuezhi Wang,Pengyao Wang,Chengzan Li,Yuanchun Zhou,Hengshu Zhu*

Main category: cs.IR

TL;DR: 提出基于大语言模型的科学数据集推荐系统ScienceDB AI，经实验证明有效且公开可用。


<details>
  <summary>Details</summary>
Motivation: 现有科学数据集共享和利用存在挑战，传统推荐器不足，大语言模型带来新机遇。

Method: 开发ScienceDB AI系统，有科学意图感知器、结构化内存压缩器和可信检索增强生成框架。

Result: 通过超1000万真实数据集的离线和在线实验，证明ScienceDB AI有效。

Conclusion: ScienceDB AI是首个针对大规模科学数据集共享服务的大语言模型驱动对话式推荐器。

Abstract: The rapid growth of AI for Science (AI4S) has underscored the significance of scientific datasets, leading to the establishment of numerous national scientific data centers and sharing platforms. Despite this progress, efficiently promoting dataset sharing and utilization for scientific research remains challenging. Scientific datasets contain intricate domain-specific knowledge and contexts, rendering traditional collaborative filtering-based recommenders inadequate. Recent advances in Large Language Models (LLMs) offer unprecedented opportunities to build conversational agents capable of deep semantic understanding and personalized recommendations. In response, we present ScienceDB AI, a novel LLM-driven agentic recommender system developed on Science Data Bank (ScienceDB), one of the largest global scientific data-sharing platforms. ScienceDB AI leverages natural language conversations and deep reasoning to accurately recommend datasets aligned with researchers' scientific intents and evolving requirements. The system introduces several innovations: a Scientific Intention Perceptor to extract structured experimental elements from complicated queries, a Structured Memory Compressor to manage multi-turn dialogues effectively, and a Trustworthy Retrieval-Augmented Generation (Trustworthy RAG) framework. The Trustworthy RAG employs a two-stage retrieval mechanism and provides citable dataset references via Citable Scientific Task Record (CSTR) identifiers, enhancing recommendation trustworthiness and reproducibility. Through extensive offline and online experiments using over 10 million real-world datasets, ScienceDB AI has demonstrated significant effectiveness. To our knowledge, ScienceDB AI is the first LLM-driven conversational recommender tailored explicitly for large-scale scientific dataset sharing services. The platform is publicly accessible at: https://ai.scidb.cn/en.

</details>


### [100] [Adaptive Diffusion-based Augmentation for Recommendation](https://arxiv.org/abs/2601.01448)
*Na Li,Fanghui Sun,Yan Zou,Yangfu Zhu,Xiatian Zhu,Ying Ma*

Main category: cs.IR

TL;DR: 本文提出ADAR模块，利用扩散模型生成可控负样本，提升推荐模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有负采样方法会误将潜在正样本标记为负样本，且缺乏对负样本选择的精确控制。

Method: 提出基于自适应扩散增强的推荐模块ADAR，模拟从正到负的连续转变，理论识别正样本变负的过渡点，推导得分感知函数确定最佳采样时间步。

Result: 实验表明ADAR广泛兼容，能大幅提升现有推荐模型性能，且无需架构修改。

Conclusion: ADAR可有效生成有挑战性的负样本，细化模型决策边界，提升推荐模型性能。

Abstract: Recommendation systems often rely on implicit feedback, where only positive user-item interactions can be observed. Negative sampling is therefore crucial to provide proper negative training signals. However, existing methods tend to mislabel potentially positive but unobserved items as negatives and lack precise control over negative sample selection. We aim to address these by generating controllable negative samples, rather than sampling from the existing item pool. In this context, we propose Adaptive Diffusion-based Augmentation for Recommendation (ADAR), a novel and model-agnostic module that leverages diffusion to synthesize informative negatives. Inspired by the progressive corruption process in diffusion, ADAR simulates a continuous transition from positive to negative, allowing for fine-grained control over sample hardness. To mine suitable negative samples, we theoretically identify the transition point at which a positive sample turns negative and derive a score-aware function to adaptively determine the optimal sampling timestep. By identifying this transition point, ADAR generates challenging negative samples that effectively refine the model's decision boundary. Experiments confirm that ADAR is broadly compatible and boosts the performance of existing recommendation models substantially, including collaborative filtering and sequential recommendation, without architectural modifications.

</details>


### [101] [Breadcrumbs in the Digital Forest: Tracing Criminals through Torrent Metadata with OSINT](https://arxiv.org/abs/2601.01492)
*Annelies de Jong,Giuseppe Cascavilla,Jessica De Pascale*

Main category: cs.IR

TL;DR: 研究种子元数据用于开源情报的潜力，通过五步流程分析数据，证明其可支持可扩展自动化用户画像。


<details>
  <summary>Details</summary>
Motivation: 虽然P2P网络在隐私和性能方面研究较多，但元数据极少用于调查目的，本研究旨在挖掘其在用户画像和行为分析方面的潜力。

Method: 采用五步开源情报流程，从海盗湾和UDP追踪器收集数据，对数据进行丰富处理，包括添加地理位置、匿名化状态等信息，并进行网络分析。

Result: 收集到超60000个唯一IP地址的数据集，通过案例分析展示数据可帮助检测对非法内容的兴趣，网络分析揭示了同行聚类等模式。

Conclusion: 公开可用的种子元数据可支持可扩展和自动化的开源情报用户画像，为数字取证提出新的数据提取方法。

Abstract: This work investigates the potential of torrent metadata as a source for open-source intelligence (OSINT), with a focus on user profiling and behavioral analysis. While peer-to-peer (P2P) networks such as BitTorrent are well studied with respect to privacy and performance, their metadata is rarely used for investigative purposes. This work presents a proof of concept demonstrating how tracker responses, torrent index data, and enriched IP metadata can reveal patterns associated with high-risk behavior.
  The research follows a five-step OSINT process: source identification, data collection, enrichment, behavioral analysis, and presentation of the results. Data were collected from The Pirate Bay and UDP trackers, yielding a dataset of more than 60,000 unique IP addresses across 206 popular torrents. The data were enriched with geolocation, anonymization status, and flags of involvement in child exploitation material (CEM). A case study on sensitive e-books shows how such data can help detect possible interest in illicit content.
  Network analysis highlights peer clustering, co-download patterns, and the use of privacy tools by suspicious users. The study shows that publicly available torrent metadata can support scalable and automated OSINT profiling.
  This work adds to digital forensics by proposing a new method to extract useful signals from noisy data, with applications in law enforcement, cybersecurity, and threat analysis.

</details>


### [102] [OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment](https://arxiv.org/abs/2601.01576)
*Ming Zhang,Kexin Tan,Yueyuan Huang,Yujiong Shen,Chunchun Ma,Li Ju,Xinran Zhang,Yuhui Wang,Wenqing Jing,Jingyi Deng,Huayu Sha,Binze Hu,Jingqi Tong,Changhao Jiang,Yage Geng,Yuankai Ying,Yue Zhang,Zhangyue Yin,Zhiheng Xi,Shihan Dou,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.IR

TL;DR: 介绍了基于大语言模型的OpenNovelty系统用于新颖性分析，阐述其工作流程，在ICLR 2026投稿上部署有初步效果。


<details>
  <summary>Details</summary>
Motivation: 同行评审中评估新颖性关键且有挑战，需评估投稿与大量快速演变的文献对比情况。

Method: 系统分四个阶段：提取核心任务和贡献主张生成检索查询；基于查询通过语义搜索引擎检索相关先前工作；构建核心任务相关工作的分层分类并进行贡献级全文比较；综合分析形成有明确引用和证据片段的新颖性报告。

Result: 在500+ ICLR 2026投稿上部署，系统可识别相关先前工作，包括作者可能忽略的相近文章。

Conclusion: OpenNovelty可为研究界提供可扩展工具，促进公平、一致和有证据支持的同行评审。

Abstract: Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, \textsc{OpenNovelty} grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.

</details>


### [103] [LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum](https://arxiv.org/abs/2601.01684)
*Zhichao Xu,Shengyao Zhuang,Crystina Zhang,Xueguang Ma,Yijun Tian,Maitrey Mehta,Jimmy Lin,Vivek Srikumar*

Main category: cs.IR

TL;DR: 介绍基于Llama - 3架构的学习型稀疏检索器LACONIC，提出两阶段训练方案，其性能接近稠密模型，节省内存，适用于现实搜索应用。


<details>
  <summary>Details</summary>
Motivation: 稠密检索模型部署受高内存和GPU依赖限制，学习型稀疏检索受关注少，需高效检索方案。

Method: 提出两阶段训练课程，包括弱监督预微调适应双向上下文和使用精心挑选的硬负样本进行高信号微调。

Result: LACONIC 8B变体在MTEB检索基准上达到60.2 nDCG，排名第15，比等效稠密模型节省71%索引内存。

Conclusion: LACONIC在普通CPU硬件上提供高检索效率，计算成本低，是现实搜索应用的可扩展高效解决方案。

Abstract: While dense retrieval models have become the standard for state-of-the-art information retrieval, their deployment is often constrained by high memory requirements and reliance on GPU accelerators for vector similarity search. Learned sparse retrieval offers a compelling alternative by enabling efficient search via inverted indices, yet it has historically received less attention than dense approaches. In this report, we introduce LACONIC, a family of learned sparse retrievers based on the Llama-3 architecture (1B, 3B, and 8B). We propose a streamlined two-phase training curriculum consisting of (1) weakly supervised pre-finetuning to adapt causal LLMs for bidirectional contextualization and (2) high-signal finetuning using curated hard negatives. Our results demonstrate that LACONIC effectively bridges the performance gap with dense models: the 8B variant achieves a state-of-the-art 60.2 nDCG on the MTEB Retrieval benchmark, ranking 15th on the leaderboard as of January 1, 2026, while utilizing 71\% less index memory than an equivalent dense model. By delivering high retrieval effectiveness on commodity CPU hardware with a fraction of the compute budget required by competing models, LACONIC provides a scalable and efficient solution for real-world search applications.

</details>


### [104] [When Attention Becomes Exposure in Generative Search](https://arxiv.org/abs/2601.01750)
*Shayan Alipour,Mehdi Kargar,Morteza Zihayat*

Main category: cs.IR

TL;DR: 研究对44家Web3企业在生成式搜索引擎中的曝光情况进行审计，发现存在曝光偏差。


<details>
  <summary>Details</summary>
Motivation: 明确生成式搜索引擎引用中的曝光受外部注意力市场影响的程度。

Method: 对44家Web3企业的曝光情况进行审计。

Result: 企业创作者社区随时间保持稳定；更受欢迎的声音系统地获得更多引用曝光；粉丝基数大且创作者核心更集中的企业曝光排名更高。

Conclusion: 生成式搜索引擎引用对已知名的声音存在曝光偏差，可能巩固现有企业地位并缩小观点多样性。

Abstract: Generative search engines are reshaping information access by replacing traditional ranked lists with synthesized answers and references. In parallel, with the growth of Web3 platforms, incentive-driven creator ecosystems have become an essential part of how enterprises build visibility and community by rewarding creators for contributing to shared narratives. However, the extent to which exposure in generative search engine citations is shaped by external attention markets remains uncertain. In this study, we audit the exposure for 44 Web3 enterprises. First, we show that the creator community around each enterprise is persistent over time. Second, enterprise-specific queries reveal that more popular voices systematically receive greater citation exposure than others. Third, we find that larger follower bases and enterprises with more concentrated creator cores are associated with higher-ranked exposure. Together, these results show that generative search engine citations exhibit exposure bias toward already prominent voices, which risks entrenching incumbents and narrowing viewpoint diversity.

</details>


### [105] [Query-Document Dense Vectors for LLM Relevance Judgment Bias Analysis](https://arxiv.org/abs/2601.01751)
*Samaneh Mohtadi,Gianluca Demartini*

Main category: cs.IR

TL;DR: 本文探讨大语言模型（LLMs）作为相关性评估器时是否存在系统性错误，提出新方法分析标签分布、定位分歧点，实验发现人与LLMs的分歧集中在特定语义簇；查询层面分析显示特定语境易出现问题；该框架可揭示LLM判断弱点，实现更可靠的信息检索评估。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在判断相关性时是否存在系统性错误，而非仅关注其平均表现。

Method: 提出一种新的查询和文档表示方法，引入基于聚类的框架将查询 - 文档对嵌入联合语义空间，将相关性视为关系属性。

Result: 实验表明，人与LLMs的系统性分歧集中在特定语义簇；查询层面分析显示，定义、政策相关或模糊语境常出现问题；某些查询不同簇的一致性差异大，是分歧热点，LLMs易漏检相关内容或误包含无关内容。

Conclusion: 该框架能将全局诊断与局部聚类相结合，揭示LLM判断的隐藏弱点，实现有偏感知且更可靠的信息检索评估。

Abstract: Large Language Models (LLMs) have been used as relevance assessors for Information Retrieval (IR) evaluation collection creation due to reduced cost and increased scalability as compared to human assessors. While previous research has looked at the reliability of LLMs as compared to human assessors, in this work, we aim to understand if LLMs make systematic mistakes when judging relevance, rather than just understanding how good they are on average. To this aim, we propose a novel representational method for queries and documents that allows us to analyze relevance label distributions and compare LLM and human labels to identify patterns of disagreement and localize systematic areas of disagreement. We introduce a clustering-based framework that embeds query-document (Q-D) pairs into a joint semantic space, treating relevance as a relational property. Experiments on TREC Deep Learning 2019 and 2020 show that systematic disagreement between humans and LLMs is concentrated in specific semantic clusters rather than distributed randomly. Query-level analyses reveal recurring failures, most often in definition-seeking, policy-related, or ambiguous contexts. Queries with large variation in agreement across their clusters emerge as disagreement hotspots, where LLMs tend to under-recall relevant content or over-include irrelevant material. This framework links global diagnostics with localized clustering to uncover hidden weaknesses in LLM judgments, enabling bias-aware and more reliable IR evaluation.

</details>


### [106] [MergeRec: Model Merging for Data-Isolated Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2601.01753)
*Hyunsoo Kim,Jaewan Moon,Seongmin Park,Jongwuk Lee*

Main category: cs.IR

TL;DR: 提出MergeRec框架解决数据隔离跨域序列推荐问题，实验证明其能提升泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统难以跨域泛化，现有跨域推荐方法有局限性。

Method: 提出MergeRec框架，包含合并初始化、伪用户数据构建、协作合并优化三个组件。

Result: MergeRec保留原模型优势，显著提升泛化能力，相比传统方法性能平均提升达17.21%（Recall@10）。

Conclusion: 模型合并是构建通用推荐系统可扩展且有效的方法。

Abstract: Modern recommender systems trained on domain-specific data often struggle to generalize across multiple domains. Cross-domain sequential recommendation has emerged as a promising research direction to address this challenge; however, existing approaches face fundamental limitations, such as reliance on overlapping users or items across domains, or unrealistic assumptions that ignore privacy constraints. In this work, we propose a new framework, MergeRec, based on model merging under a new and realistic problem setting termed data-isolated cross-domain sequential recommendation, where raw user interaction data cannot be shared across domains. MergeRec consists of three key components: (1) merging initialization, (2) pseudo-user data construction, and (3) collaborative merging optimization. First, we initialize a merged model using training-free merging techniques. Next, we construct pseudo-user data by treating each item as a virtual sequence in each domain, enabling the synthesis of meaningful training samples without relying on real user interactions. Finally, we optimize domain-specific merging weights through a joint objective that combines a recommendation loss, which encourages the merged model to identify relevant items, and a distillation loss, which transfers collaborative filtering signals from the fine-tuned source models. Extensive experiments demonstrate that MergeRec not only preserves the strengths of the original models but also significantly enhances generalizability to unseen domains. Compared to conventional model merging methods, MergeRec consistently achieves superior performance, with average improvements of up to 17.21% in Recall@10, highlighting the potential of model merging as a scalable and effective approach for building universal recommender systems. The source code is available at https://github.com/DIALLab-SKKU/MergeRec.

</details>


### [107] [SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines](https://arxiv.org/abs/2601.01785)
*Rajiv Chaitanya Muttur*

Main category: cs.IR

TL;DR: 提出用于边缘原生RAG部署的轻量级文档选择器SRAS，在合成QA基准和真实数据上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统的固定top - k文档选择机制忽略下游生成质量且有计算开销，需要轻量级、低延迟的文档选择方法。

Method: 使用近端策略优化（PPO）训练SRAS，结合Relaxed F1和BERTScore的混合奖励信号，在严格的令牌和计算约束下运行。

Result: 在合成QA基准上优于有监督和随机选择器，在SQuAD v2上未进行特定领域调优时BERTScore F1达0.8546。

Conclusion: 首次证明基于RL的文档选择可以实现超轻量级、低延迟且对设备端RAG管道有效。

Abstract: Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.

</details>


### [108] [A Hybrid Architecture for Multi-Stage Claim Document Understanding: Combining Vision-Language Models and Machine Learning for Real-Time Processing](https://arxiv.org/abs/2601.01897)
*Lilu Cheng,Jingjun Lu,Yi Xuan Chan,Quoc Khai Nguyen,John Bi,Sean Ho*

Main category: cs.IR

TL;DR: 文章针对理赔文档自动化解析难题，提出多阶段处理流程，结合多种技术从数据中提取字段，效果好、效率高且已部署应用。


<details>
  <summary>Details</summary>
Motivation: 理赔文档存在内容、语言多样，图像质量和布局不一致的问题，阻碍自动化解析和信息提取。

Method: 提出多阶段流程，集成多语言OCR引擎PaddleOCR、传统逻辑回归分类器和视觉语言模型Qwen 2.5-VL - 7B。

Result: 文档类型分类准确率超95%，字段提取准确率约87%，单文档平均处理延迟不到2秒，相比人工效率提升300倍，已在移动应用中部署并每周处理数万笔理赔。

Conclusion: 结合传统机器学习模型和现代视觉语言模型能够在现实自动化中实现生产级的准确率和速度。

Abstract: Claims documents are fundamental to healthcare and insurance operations, serving as the basis for reimbursement, auditing, and compliance. However, these documents are typically not born digital; they often exist as scanned PDFs or photographs captured under uncontrolled conditions. Consequently, they exhibit significant content heterogeneity, ranging from typed invoices to handwritten medical reports, as well as linguistic diversity. This challenge is exemplified by operations at Fullerton Health, which handles tens of millions of claims annually across nine markets, including Singapore, the Philippines, Indonesia, Malaysia, Mainland China, Hong Kong, Vietnam, Papua New Guinea, and Cambodia. Such variability, coupled with inconsistent image quality and diverse layouts, poses a significant obstacle to automated parsing and structured information extraction.
  This paper presents a robust multi-stage pipeline that integrates the multilingual optical character recognition (OCR) engine PaddleOCR, a traditional Logistic Regression classifier, and a compact Vision-Language Model (VLM), Qwen 2.5-VL-7B, to achieve efficient and accurate field extraction from large-scale claims data. The proposed system achieves a document-type classification accuracy of over 95 percent and a field-level extraction accuracy of approximately 87 percent, while maintaining an average processing latency of under 2 seconds per document. Compared to manual processing, which typically requires around 10 minutes per claim, our system delivers a 300x improvement in efficiency. These results demonstrate that combining traditional machine learning models with modern VLMs enables production-grade accuracy and speed for real-world automation. The solution has been successfully deployed in our mobile application and is currently processing tens of thousands of claims weekly from Vietnam and Singapore.

</details>


### [109] [MCGI: Manifold-Consistent Graph Indexing for Billion-Scale Disk-Resident Vector Search](https://arxiv.org/abs/2601.01930)
*Dongfang Zhao*

Main category: cs.IR

TL;DR: 提出MCGI方法解决图基近似最近邻搜索在高维空间的性能下降问题，理论和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 图基近似最近邻搜索在高维空间因‘欧几里得 - 测地线不匹配’问题导致性能下降。

Method: 提出几何感知且磁盘驻留的索引方法MCGI，利用局部固有维度动态调整搜索策略，根据几何分析调节束搜索预算。

Result: 理论上能改进近似保证；实验上，在高维GIST1M上吞吐量比DiskANN高5.8倍，在SIFT1B上高召回查询延迟降低3倍，低维数据集性能相当。

Conclusion: MCGI方法有效解决高维空间图基近似最近邻搜索的性能问题，且具有良好的可扩展性。

Abstract: Graph-based Approximate Nearest Neighbor (ANN) search often suffers from performance degradation in high-dimensional spaces due to the ``Euclidean-Geodesic mismatch,'' where greedy routing diverges from the underlying data manifold. To address this, we propose Manifold-Consistent Graph Indexing (MCGI), a geometry-aware and disk-resident indexing method that leverages Local Intrinsic Dimensionality (LID) to dynamically adapt search strategies to the data's intrinsic geometry. Unlike standard algorithms that treat dimensions uniformly, MCGI modulates its beam search budget based on in situ geometric analysis, eliminating dependency on static hyperparameters. Theoretical analysis confirms that MCGI enables improved approximation guarantees by preserving manifold-consistent topological connectivity. Empirically, MCGI achieves 5.8$\times$ higher throughput at 95\% recall on high-dimensional GIST1M compared to state-of-the-art DiskANN. On the billion-scale SIFT1B dataset, MCGI further validates its scalability by reducing high-recall query latency by 3$\times$, while maintaining performance parity on standard lower-dimensional datasets.

</details>


### [110] [Exploring Diversity, Novelty, and Popularity Bias in ChatGPT's Recommendations](https://arxiv.org/abs/2601.01997)
*Dario Di Palma,Giovanni Maria Biancofiore,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 研究评估ChatGPT-3.5和ChatGPT-4在推荐系统中的多样性、新颖性和流行度偏差，发现ChatGPT-4表现出色，冷启动场景中优势明显。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注ChatGPT集成到推荐系统的准确性，缺乏对其在多样性、新颖性和潜在偏差方面的全面分析，本研究旨在填补这一空白以提升用户满意度和实现长期个性化。

Method: 在三个不同数据集上评估ChatGPT-3.5和ChatGPT-4在Top - N推荐和冷启动场景中的表现。

Result: ChatGPT-4在推荐中能平衡新颖性和多样性，匹配或超越传统推荐器；冷启动场景中，ChatGPT模型在准确性和新颖性上表现更优。

Conclusion: 研究指出了ChatGPT推荐的优缺点，为超越以准确性为重点的推荐指标提供了新视角。

Abstract: ChatGPT has emerged as a versatile tool, demonstrating capabilities across diverse domains. Given these successes, the Recommender Systems (RSs) community has begun investigating its applications within recommendation scenarios primarily focusing on accuracy. While the integration of ChatGPT into RSs has garnered significant attention, a comprehensive analysis of its performance across various dimensions remains largely unexplored. Specifically, the capabilities of providing diverse and novel recommendations or exploring potential biases such as popularity bias have not been thoroughly examined. As the use of these models continues to expand, understanding these aspects is crucial for enhancing user satisfaction and achieving long-term personalization.
  This study investigates the recommendations provided by ChatGPT-3.5 and ChatGPT-4 by assessing ChatGPT's capabilities in terms of diversity, novelty, and popularity bias. We evaluate these models on three distinct datasets and assess their performance in Top-N recommendation and cold-start scenarios. The findings reveal that ChatGPT-4 matches or surpasses traditional recommenders, demonstrating the ability to balance novelty and diversity in recommendations. Furthermore, in the cold-start scenario, ChatGPT models exhibit superior performance in both accuracy and novelty, suggesting they can be particularly beneficial for new users. This research highlights the strengths and limitations of ChatGPT's recommendations, offering new perspectives on the capacity of these models to provide recommendations beyond accuracy-focused metrics.

</details>


### [111] [Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models](https://arxiv.org/abs/2601.02002)
*Antonio Colacicco,Vito Guida,Dario Di Palma,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 研究大语言模型在推荐场景的数据泄露问题，评估三种方法，发现自动优化提示是提取记忆样本最有前景的策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练数据不公开引发数据泄露担忧，现有数据提取依赖手动提示工程，需解决能否改进手动提示、非手动提示检测记忆、自动化检测数据泄露等问题。

Method: 评估三种方法，包括越狱提示工程、无监督潜在知识发现（通过CCS和Cluster - Norm探测内部激活）、自动提示工程（将提示发现视为元学习过程）。

Result: 越狱提示不能改善记忆项检索且不稳定；CCS能区分真假电影标题但对数值数据失效；APE能适度检索项目级信息但难恢复数值交互。

Conclusion: 自动优化提示是提取记忆样本最有前景的策略。

Abstract: Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.

</details>


### [112] [Cold-Starting Podcast Ads and Promotions with Multi-Task Learning on Spotify](https://arxiv.org/abs/2601.02306)
*Shivam Verma,Hannes Karlbom,Yu Zhao,Nick Topping,Vivian Chen,Kieran Stanley,Bharath Rengarajan*

Main category: cs.IR

TL;DR: 提出适用于Spotify播客生态系统的统一多目标模型，通过迁移学习解决个性化和冷启动问题，线上A/B测试显示效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决Spotify播客生态中广告和促销的个性化及冷启动初始化问题，特别是针对新广告目标。

Method: 在多任务学习框架下利用大规模广告和内容交互的迁移学习，采用统一多目标模型优化播客成果。

Result: 线上A/B测试显示，播放量少的播客有效每流成本最高降低22%，播客流率提高18 - 24%；离线实验凸显辅助目标和特征组对冷启动表现的贡献。

Conclusion: 统一建模策略可提高可维护性、冷启动性能和覆盖范围，打破传统目标管道，同时探讨了联合模型在实际广告系统中的权衡。

Abstract: We present a unified multi-objective model for targeting both advertisements and promotions within the Spotify podcast ecosystem. Our approach addresses key challenges in personalization and cold-start initialization, particularly for new advertising objectives. By leveraging transfer learning from large-scale ad and content interactions within a multi-task learning (MTL) framework, a single joint model can be fine-tuned or directly applied to new or low-data targeting tasks, including in-app promotions. This multi-objective design jointly optimizes podcast outcomes such as streams, clicks, and follows for both ads and promotions using a shared representation over user, content, context, and creative features, effectively supporting diverse business goals while improving user experience. Online A/B tests show up to a 22% reduction in effective Cost-Per-Stream (eCPS), particularly for less-streamed podcasts, and an 18-24% increase in podcast stream rates. Offline experiments and ablations highlight the contribution of ancillary objectives and feature groups to cold-start performance. Our experience shows that a unified modeling strategy improves maintainability, cold-start performance, and coverage, while breaking down historically siloed targeting pipelines. We discuss practical trade-offs of such joint models in a real-world advertising system.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [113] [Learning with Monotone Adversarial Corruptions](https://arxiv.org/abs/2601.02193)
*Kasper Green Larsen,Chirag Pabbaraju,Abhishek Shetty*

Main category: cs.LG

TL;DR: 通过单调对抗性损坏模型研究标准机器学习算法对数据可交换性和独立性的依赖程度，发现最优学习算法在单调损坏下表现不佳，而基于一致收敛的算法不受影响。


<details>
  <summary>Details</summary>
Motivation: 研究标准机器学习算法对数据可交换性和独立性的依赖程度。

Method: 引入单调对抗性损坏模型，在干净独立同分布数据集插入符合一定规则的损坏点。

Result: 已知的二元分类最优学习算法在新测试点上预期误差不理想，基于一致收敛的算法保证不受影响。

Conclusion: 最优学习算法在看似有用的单调损坏下会失效，暴露其对可交换性的过度依赖。

Abstract: We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a "clean" i.i.d. dataset, inserts additional "corrupted" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.

</details>


### [114] [Data-Driven Assessment of Concrete Mixture Compositions on Chloride Transport via Standalone Machine Learning Algorithms](https://arxiv.org/abs/2601.01009)
*Mojtaba Aliasghar-Mamaghani,Mohammadreza Khalafi*

Main category: cs.LG

TL;DR: 本文用数据驱动方法研究混凝土混合物成分对氯化物时间演变的影响，评估多种机器学习算法，多数成分与氯化物含量呈反比，凸显替代方法对提升基础设施寿命的潜力。


<details>
  <summary>Details</summary>
Motivation: 评估处于恶劣环境下民用基础设施的使用寿命，确定混凝土混合物成分对混凝土结构中氯化物时间演变的影响。

Method: 采用多种简单和复杂的机器学习算法，如LR、KNN、KRR、SVR、GPR、MLP、GRU，利用综合数据集评估算法性能。

Result: KRR、GPR、MLP准确性高，GRU无法准确再现测试集响应，GPR模型揭示潜在关联，多数成分与氯化物含量呈反比。

Conclusion: 替代方法在描述氯化物侵入物理过程及关联方面有潜力，可提升民用基础设施使用寿命。

Abstract: This paper employs a data-driven approach to determine the impact of concrete mixture compositions on the temporal evolution of chloride in concrete structures. This is critical for assessing the service life of civil infrastructure subjected to aggressive environments. The adopted methodology relies on several simple and complex standalone machine learning (ML) algorithms, with the primary objective of establishing confidence in the unbiased prediction of the underlying hidden correlations. The simple algorithms include linear regression (LR), k-nearest neighbors (KNN) regression, and kernel ridge regression (KRR). The complex algorithms entail support vector regression (SVR), Gaussian process regression (GPR), and two families of artificial neural networks, including a feedforward network (multilayer perceptron, MLP) and a gated recurrent unit (GRU). The MLP architecture cannot explicitly handle sequential data, a limitation addressed by the GRU. A comprehensive dataset is considered. The performance of ML algorithms is evaluated, with KRR, GPR, and MLP exhibiting high accuracy. Given the diversity of the adopted concrete mixture proportions, the GRU was unable to accurately reproduce the response in the test set. Further analyses elucidate the contributions of mixture compositions to the temporal evolution of chloride. The results obtained from the GPR model unravel latent correlations through clear and explainable trends. The MLP, SVR, and KRR also provide acceptable estimates of the overall trends. The majority of mixture components exhibit an inverse relation with chloride content, while a few components demonstrate a direct correlation. These findings highlight the potential of surrogate approaches for describing the physical processes involved in chloride ingress and the associated correlations, toward the ultimate goal of enhancing the service life of civil infrastructure.

</details>


### [115] [Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces](https://arxiv.org/abs/2601.01082)
*Bryon Tjanaka,Henry Chen,Matthew C. Fontaine,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: 提出折扣模型搜索（DMS）算法解决高维质量多样性优化问题，在高维基准测试中表现优于CMA - MAE等算法。


<details>
  <summary>Details</summary>
Motivation: 当代质量多样性（QD）算法聚焦低维度量，高维度量易失真，CMA - MAE算法在高维度量空间会停滞。

Method: 提出DMS算法，用模型引导探索，提供折扣值的平滑连续表示。

Result: 在高维图像度量空间和高维基准测试中，DMS表现优于CMA - MAE和其他黑盒QD算法。

Conclusion: DMS能区分相似度量的解，可继续探索，推动了新的QD应用。

Abstract: Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.

</details>


### [116] [Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification](https://arxiv.org/abs/2601.01150)
*Wenbin Pei,Ruohao Dai,Bing Xue,Mengjie Zhang,Qiang Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: 提出新的进化过采样方法Evo - TFS处理不平衡时间序列分类，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法处理不平衡时间序列分类时忽略少数类，现有过采样方法难保留时间动态和生成多样样本。

Method: 提出Evo - TFS，用强类型遗传编程，结合时频域特征的适应度函数进化多样高质量时间序列。

Result: 在不平衡时间序列数据集实验中，Evo - TFS优于现有过采样方法，显著提升时频域分类器性能。

Conclusion: Evo - TFS是处理不平衡时间序列分类的有效方法。

Abstract: Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.

</details>


### [117] [Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware](https://arxiv.org/abs/2601.01298)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: 提出Warp Cortex异步架构解决多智能体大语言模型框架内存线性扩展问题，降低内存复杂度，实现多智能体认知扩展。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体大语言模型框架存在内存线性扩展问题，使‘System 2’并行推理在消费级硬件上不可行。

Method: 提出Warp Cortex异步架构，采用Singleton Weight Sharing和受拓扑数据分析启发的Topological Synapse，将KV - cache视为潜空间的点云进行稀疏化，引入Referential Injection。

Result: 在单块NVIDIA RTX 4090上，2.2 GB总显存可实现100个并发智能体，理论容量超1000个智能体。

Conclusion: Warp Cortex架构通过解耦智能体逻辑和物理内存，有效解决了内存扩展问题，可实现多智能体认知扩展。

Abstract: Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering "System 2" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.

</details>


### [118] [Accelerating Storage-Based Training for Graph Neural Networks](https://arxiv.org/abs/2601.01473)
*Myung-Hwan Jang,Jeong-Min Park,Yunyong Ko,Sang-Wook Kim*

Main category: cs.LG

TL;DR: 本文针对存储式GNN训练数据准备瓶颈，提出AGNES框架，实验显示其性能优于竞品。


<details>
  <summary>Details</summary>
Motivation: 现有存储式GNN训练方法存在数据准备瓶颈，忽视处理大量小存储I/O的问题。

Method: 提出AGNES框架，采用分块存储I/O处理和基于超批次处理的策略。

Result: 在五个真实图上的实验表明，AGNES比四个最先进的方法快，最高可达4.1倍。

Conclusion: AGNES可有效解决存储式GNN训练数据准备的瓶颈问题，提升训练效率。

Abstract: Graph neural networks (GNNs) have achieved breakthroughs in various real-world downstream tasks due to their powerful expressiveness. As the scale of real-world graphs has been continuously growing, \textit{a storage-based approach to GNN training} has been studied, which leverages external storage (e.g., NVMe SSDs) to handle such web-scale graphs on a single machine. Although such storage-based GNN training methods have shown promising potential in large-scale GNN training, we observed that they suffer from a severe bottleneck in data preparation since they overlook a critical challenge: \textit{how to handle a large number of small storage I/Os}. To address the challenge, in this paper, we propose a novel storage-based GNN training framework, named \textsf{AGNES}, that employs a method of \textit{block-wise storage I/O processing} to fully utilize the I/O bandwidth of high-performance storage devices. Moreover, to further enhance the efficiency of each storage I/O, \textsf{AGNES} employs a simple yet effective strategy, \textit{hyperbatch-based processing} based on the characteristics of real-world graphs. Comprehensive experiments on five real-world graphs reveal that \textsf{AGNES} consistently outperforms four state-of-the-art methods, by up to 4.1$\times$ faster than the best competitor. Our code is available at https://github.com/Bigdasgit/agnes-kdd26.

</details>


### [119] [SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition](https://arxiv.org/abs/2601.01979)
*Julie Keisler,Anastase Alexandre Charantonis,Yannig Goude,Boutheina Oueslati,Claire Monteleoni*

Main category: cs.LG

TL;DR: 提出SerpentFlow框架用于无配对域对齐，并应用于超分辨率任务，实验表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 在无配对观测情况下进行域对齐任务具有挑战性，需找到有效方法。

Method: 引入SerpentFlow框架，将数据在潜空间分解为共享和特定领域分量，构建合成训练对，使用基于分类器准则自动确定频率分界，用Flow Matching实现框架。

Result: 在合成图像、物理过程模拟和气候降尺度任务中有效重建高频结构。

Conclusion: 共享结构分解是无配对域对齐的有效策略。

Abstract: Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.

</details>


### [120] [Distribution Matching for Graph Quantification Under Structural Covariate Shift](https://arxiv.org/abs/2601.00864)
*Clemens Damke,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文将结构重要性采样思想扩展到KDEy量化方法，以适应图数据中的结构偏移，且表现优于标准量化方法。


<details>
  <summary>Details</summary>
Motivation: 当前表格数据的量化学习方法基于的PPS假设在图数据中因结构偏移不成立，需解决图数据结构偏移问题。

Method: 将结构重要性采样思想扩展到KDEy量化方法。

Result: 所提出的方法能适应结构偏移，且性能优于标准量化方法。

Conclusion: 扩展后的方法在处理图数据结构偏移的量化学习任务中有效。

Abstract: Graphs are commonly used in machine learning to model relationships between instances. Consider the task of predicting the political preferences of users in a social network; to solve this task one should consider, both, the features of each individual user and the relationships between them. However, oftentimes one is not interested in the label of a single instance but rather in the distribution of labels over a set of instances; e.g., when predicting the political preferences of users, the overall prevalence of a given opinion might be of higher interest than the opinion of a specific person. This label prevalence estimation task is commonly referred to as quantification learning (QL). Current QL methods for tabular data are typically based on the so-called prior probability shift (PPS) assumption which states that the label-conditional instance distributions should remain equal across the training and test data. In the graph setting, PPS generally does not hold if the shift between training and test data is structural, i.e., if the training data comes from a different region of the graph than the test data. To address such structural shifts, an importance sampling variant of the popular adjusted count quantification approach has previously been proposed. In this work, we extend the idea of structural importance sampling to the state-of-the-art KDEy quantification approach. We show that our proposed method adapts to structural shifts and outperforms standard quantification approaches.

</details>


### [121] [Communication-Efficient Federated AUC Maximization with Cyclic Client Participation](https://arxiv.org/abs/2601.01649)
*Umesh Vangapally,Wenhan Wu,Chen Chen,Zhishuai Guo*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-Łojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/ε^3)$ and an iteration complexity of $O(1/ε^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.

</details>


### [122] [Multivariate Time-series Anomaly Detection via Dynamic Model Pool & Ensembling](https://arxiv.org/abs/2601.02037)
*Wei Hu,Zewei Yu,Jianqiu Xu*

Main category: cs.LG

TL;DR: 提出DMPEAD框架用于多变量时间序列异常检测，在8个真实数据集上实验表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模型方法在多变量时间序列异常检测中存在依赖单一模型、固定数据维度限制可扩展性等问题。

Method: 先通过参数转移和多样性指标构建多样化模型池，再用元模型和基于相似度策略更新模型池，最后在选定子集中通过代理指标排名和top - k聚合集成排名靠前的模型。

Result: 在8个真实世界数据集上的大量实验表明，提出的模型优于所有基线模型。

Conclusion: DMPEAD框架具有出色的适应性和可扩展性。

Abstract: Multivariate time-series (MTS) anomaly detection is critical in domains such as service monitor, IoT, and network security. While multi-model methods based on selection or ensembling outperform single-model ones, they still face limitations: (i) selection methods rely on a single chosen model and are sensitive to the strategy; (ii) ensembling methods often combine all models or are restricted to univariate data; and (iii) most methods depend on fixed data dimensionality, limiting scalability. To address these, we propose DMPEAD, a Dynamic Model Pool and Ensembling framework for MTS Anomaly Detection. The framework first (i) constructs a diverse model pool via parameter transfer and diversity metric, then (ii) updates it with a meta-model and similarity-based strategy for adaptive pool expansion, subset selection, and pool merging, finally (iii) ensembles top-ranked models through proxy metric ranking and top-k aggregation in the selected subset, outputting the final anomaly detection result. Extensive experiments on 8 real-world datasets show that our model outperforms all baselines, demonstrating superior adaptability and scalability.

</details>


### [123] [Hierarchical topological clustering](https://arxiv.org/abs/2601.00892)
*Ana Carpio,Gema Duro*

Main category: cs.LG

TL;DR: 提出可搭配任意距离选择的分层拓扑聚类算法，在含离群点数据上展示潜力。


<details>
  <summary>Details</summary>
Motivation: 拓扑方法可在不假设数据结构情况下探索数据云，期望解决已有聚类技术的问题。

Method: 提出分层拓扑聚类算法，可搭配任意距离选择，从生成的层次结构推断离群点和任意形状聚类的持久性。

Result: 在含离群点的图像、医疗和经济等选定数据集上展示了算法潜力。

Conclusion: 该方法能在其他技术失效的情况下提供有意义的聚类。

Abstract: Topological methods have the potential of exploring data clouds without making assumptions on their the structure. Here we propose a hierarchical topological clustering algorithm that can be implemented with any distance choice. The persistence of outliers and clusters of arbitrary shape is inferred from the resulting hierarchy. We demonstrate the potential of the algorithm on selected datasets in which outliers play relevant roles, consisting of images, medical and economic data. These methods can provide meaningful clusters in situations in which other techniques fail to do so.

</details>


### [124] [Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack](https://arxiv.org/abs/2601.01840)
*Qiantao Yang,Liquan Chen,Mingfu Xue,Songze Li*

Main category: cs.LG

TL;DR: 提出个性化联邦学习方法FedCSPACK，可利用有限客户端资源，减少数据异质性影响，实验证明能提升效率并保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法未平衡处理数据异质性和客户端有限资源的问题，需要新方法。

Method: 提出基于余弦稀疏化参数打包和双加权聚合的FedCSPACK方法，包括客户端打包参数、生成掩码矩阵、嵌入权重实现加权引导聚合机制。

Result: 在四个数据集上用十种先进方法实验，FedCSPACK有效提升通信和计算效率，保持高模型准确率。

Conclusion: FedCSPACK能有效利用有限客户端资源，减少数据异质性对模型性能的影响，提升效率和模型性能。

Abstract: Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.

</details>


### [125] [Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment](https://arxiv.org/abs/2601.00908)
*Chorok Lee*

Main category: cs.LG

TL;DR: 以新冠疫情作为自然实验，研究分布偏移下共形预测保证的下降，发现单特征依赖与灾难性失败相关，不同任务表现不同，季度再训练效果不一，提出决策框架。


<details>
  <summary>Details</summary>
Motivation: 研究分布偏移下共形预测保证下降的情况。

Method: 以新冠疫情为自然实验，涵盖8个供应链任务，使用SHapley Additive exPlanations (SHAP)分析，还对4个特征稳定性适中的任务进行探索性分析。

Result: 不同任务 Coverage 下降幅度差异大，灾难性失败与单特征依赖相关，季度再训练对不同任务效果不同，特征稳定性决定鲁棒性。

Conclusion: 提供决策框架，部署前监测 SHAP 集中度，易受影响则季度再训练，鲁棒则跳过。

Abstract: Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.

</details>


### [126] [Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs](https://arxiv.org/abs/2601.01069)
*Jing Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文重新审视非平稳参数化多臂老虎机的加权策略，提出精炼分析框架，简化算法设计，改进多种参数化老虎机和非平稳马尔可夫决策过程的后悔界。


<details>
  <summary>Details</summary>
Motivation: 先前加权策略理论分析复杂，算法计算效率低或统计次优，需改进。

Method: 提出精炼分析框架，推导简单权重算法，并将框架拓展到非平稳马尔可夫决策过程。

Result: 提出的算法与窗口/重启算法效率相当，改进多种参数化老虎机后悔界，为非平稳马尔可夫决策过程建立动态后悔保证。

Conclusion: 精炼分析框架有效，能简化算法设计并改进多种模型的后悔界。

Abstract: Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_μ$ and $c_μ$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.

</details>


### [127] [Wittgenstein's Family Resemblance Clustering Algorithm](https://arxiv.org/abs/2601.01127)
*Golbahar Amanpour,Benyamin Ghojogh*

Main category: cs.LG

TL;DR: 本文引入新颖的机器学习聚类算法，受维特根斯坦家族相似概念启发提出WFR及其核变体算法，在基准数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 借鉴哲学概念以开发新的机器学习聚类方法，摆脱对聚类数量先验知识和形状假设的依赖。

Method: 基于维特根斯坦的家族相似概念开发WFR聚类算法和核WFR算法，计算相邻数据实例的相似度得分，构建相似度图确定聚类。

Result: 在基准数据集上的模拟表明WFR是有效的非线性聚类算法。

Conclusion: WFR算法不依赖聚类数量的先验知识和对聚类形状的假设，是有效的非线性聚类算法。

Abstract: This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.

</details>


### [128] [Sparse Bayesian Message Passing under Structural Uncertainty](https://arxiv.org/abs/2601.01207)
*Yoonhyuk Choi,Jiho Choi,Chanran Kim,Yumin Lee,Hawon Shin,Yeowon Jeon,Minjeong Kim,Jiwoo Kang*

Main category: cs.LG

TL;DR: 本文提出一种稀疏有符号消息传递网络，通过对有符号邻接矩阵的后验分布建模来处理图结构不确定性，能应对边噪声和异质性，实验显示该方法在异质性基准测试中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界图的半监督学习常受异质性挑战，现有图神经网络依赖固定邻接结构或正则化处理结构噪声效果欠佳。

Method: 对有符号邻接矩阵的后验分布建模，提出稀疏有符号消息传递网络，结合后验边缘化和稀疏有符号消息聚合。

Result: 在合成和现实世界结构噪声下的异质性基准测试中，该方法优于强基线模型。

Conclusion: 所提方法是处理边噪声和异质性的有效途径。

Abstract: Semi-supervised learning on real-world graphs is frequently challenged by heterophily, where the observed graph is unreliable or label-disassortative. Many existing graph neural networks either rely on a fixed adjacency structure or attempt to handle structural noise through regularization. In this work, we explicitly capture structural uncertainty by modeling a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent. We propose a sparse signed message passing network that is naturally robust to edge noise and heterophily, which can be interpreted from a Bayesian perspective. By combining (i) posterior marginalization over signed graph structures with (ii) sparse signed message aggregation, our approach offers a principled way to handle both edge noise and heterophily. Experimental results demonstrate that our method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.

</details>


### [129] [Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data](https://arxiv.org/abs/2601.01223)
*Marzieh Amiri Shahbazi,Ali Baheri,Nasibeh Azadeh-Fard*

Main category: cs.LG

TL;DR: 提出混合贝叶斯-共形框架解决医疗预测中不确定性量化问题，评估显示达到目标覆盖且有自适应精度，强调方法必要性。


<details>
  <summary>Details</summary>
Motivation: 临床决策需同时满足无分布覆盖保证和风险自适应精度的不确定性量化，现有方法无法做到。

Method: 将贝叶斯分层随机森林与群体感知共形校准相结合，用后验不确定性加权一致性得分。

Result: 在多医院和地区评估中达到目标覆盖（94.3%），低不确定性病例区间窄21%，高风险预测区间适当变宽，仅贝叶斯不确定性严重覆盖不足（14.1%）。

Conclusion: 该框架可实现风险分层临床方案、高效资源规划和保守分配，为不同医疗场景提供决策支持。

Abstract: Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.

</details>


### [130] [MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches](https://arxiv.org/abs/2601.01206)
*Soroush Elyasi,Arya VarastehNezhad,Fattaneh Taghiyareh*

Main category: cs.LG

TL;DR: 研究提出多类型严肃游戏框架结合机器学习技术预测软件开发岗位适配性，模型精度高，表明游戏中隐式行为痕迹在预测中很有前景。


<details>
  <summary>Details</summary>
Motivation: 传统职业指导和人员选拔中的人格评估依赖自我报告问卷，存在易受反应偏差、疲劳和故意歪曲等问题，故探索基于游戏的评估方法。

Method: 通过系统文献回顾和对专业软件工程师的实证研究确定与开发者相关的人格和行为特征，设计定制手机游戏收集细粒度游戏事件数据，采用两阶段建模策略仅从游戏行为特征预测适配性。

Result: 模型精度达97%，准确率达94%，合适候选人有独特游戏模式。

Conclusion: 游戏中捕获的隐式行为痕迹在不进行明确人格测试的情况下可有效预测软件开发适配性，严肃游戏是职业评估的可扩展、有吸引力且偏差较小的替代方案。

Abstract: Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.

</details>


### [131] [Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds](https://arxiv.org/abs/2601.00834)
*Julian Evan Chrisnanto,Salsabila Rahma Alia,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: 本文介绍无网格几何深度学习框架IM - PINN用于模拟复杂流形上的非线性反应 - 扩散动力学，在极端高斯曲率流形上验证其有效性，对比传统方法有更好表现。


<details>
  <summary>Details</summary>
Motivation: 模拟复杂非欧几里得流形上的非线性反应 - 扩散动力学存在挑战，受限于高保真网格生成成本和离散时间步长方案的辛漂移。

Method: 引入IM - PINN框架，将黎曼度量张量嵌入自动微分图，解析重构拉普拉斯 - 贝尔特拉米算子，使用带傅里叶特征嵌入的双流架构减轻谱偏差。

Result: 在极端高斯曲率波动的“随机布料”流形上验证，恢复了Gray - Scott模型的“分裂斑”和“迷宫”状态，质量守恒误差小于传统方法。

Conclusion: 该框架为模拟演化表面上的生物模式形成提供内存高效、分辨率无关的范式，连接了微分几何和物理信息机器学习。

Abstract: Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a "Stochastic Cloth" manifold with extreme Gaussian curvature fluctuations ($K \in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the "splitting spot" and "labyrinthine" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\mathcal{E}_{mass} \approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.

</details>


### [132] [Value-guided action planning with JEPA world models](https://arxiv.org/abs/2601.00844)
*Matthieu Destrade,Oumayma Bounou,Quentin Le Lidec,Jean Ponce,Yann LeCun*

Main category: cs.LG

TL;DR: 提出增强JEPA世界模型规划能力的方法，在简单控制任务上提升规划性能。


<details>
  <summary>Details</summary>
Motivation: JEPA支持有效行动规划的能力有限，需增强其规划能力。

Method: 通过塑造JEPA世界模型的表示空间，使给定环境中到达成本的负目标条件价值函数由状态嵌入之间的距离近似，并在训练中实施该约束。

Result: 在简单控制任务上，相比标准JEPA模型，规划性能显著提高。

Conclusion: 该方法能有效增强JEPA世界模型的规划能力。

Abstract: Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.

</details>


### [133] [FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments](https://arxiv.org/abs/2601.00853)
*Sameer Rahil,Zain Abdullah Ahmad,Talha Asif*

Main category: cs.LG

TL;DR: 提出FedSCAM算法，基于客户端异质性分数动态调整SAM扰动半径和聚合权重，实验表明其在收敛速度和测试准确率上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端统计异质性（非IID标签分布）对收敛和泛化造成挑战，现有引入SAM的方法忽略客户端异质性。

Method: 提出FedSCAM算法，计算每个客户端的异质性指标，反向调节扰动半径，引入异质性感知加权聚合机制。

Result: 在CIFAR - 10和Fashion - MNIST数据集不同程度的Dirichlet标签偏斜下实验，FedSCAM在收敛速度和最终测试准确率上优于FedSAM、FedLESAM等基线方法。

Conclusion: FedSCAM能有效应对联邦学习中客户端异质性问题，在性能上有竞争力。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, statistical heterogeneity among clients, often manifested as non-IID label distributions, poses significant challenges to convergence and generalization. While Sharpness-Aware Minimization (SAM) has been introduced to FL to seek flatter, more robust minima, existing approaches typically apply a uniform perturbation radius across all clients, ignoring client-specific heterogeneity. In this work, we propose \textbf{FedSCAM} (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation), a novel algorithm that dynamically adjusts the SAM perturbation radius and aggregation weights based on client-specific heterogeneity scores. By calculating a heterogeneity metric for each client and modulating the perturbation radius inversely to this score, FedSCAM prevents clients with high variance from destabilizing the global model. Furthermore, we introduce a heterogeneity-aware weighted aggregation mechanism that prioritizes updates from clients that align with the global optimization direction. Extensive experiments on CIFAR-10 and Fashion-MNIST under various degrees of Dirichlet-based label skew demonstrate that FedSCAM achieves competitive performance among state-of-the-art baselines, including FedSAM, FedLESAM, etc. in terms of convergence speed and final test accuracy.

</details>


### [134] [Path Integral Solution for Dissipative Generative Dynamics](https://arxiv.org/abs/2601.00860)
*Xidi Wang*

Main category: cs.LG

TL;DR: 研究证明耗散量子动力学可实现连贯文本生成，语言生成是耗散量子场理论，机械系统通过耗散和非局域性获智能。


<details>
  <summary>Details</summary>
Motivation: 探讨纯机械系统能否生成智能语言。

Method: 采用具有闭式路径积分传播子的Koopman算子，进行谱分析。

Result: 耗散量子动力学可产生连贯文本生成，守恒定律导致根本失败，出现不同特征值模式，哈密顿约束会降低性能。

Conclusion: 语言生成是耗散量子场理论，机械系统通过耗散和非局域性而非守恒获得智能。

Abstract: Can purely mechanical systems generate intelligent language? We prove that dissipative quantum dynamics with analytically tractable non-local context aggregation produce coherent text generation, while conservation laws cause fundamental failure. Employing Koopman operators with closed-form path integral propagators, we show irreversible computation fundamentally requires both controlled information dissipation and causal context aggregation. Spectral analysis reveals emergent eigenvalue structure, separating into decay modes (forgetting), growth modes (amplification), and neutral modes (preservation) -- the essential ingredients for directed information flow. Hamiltonian constraints force the elimination of these dissipative modes and degrading performance despite unchanged model capacity. This establishes language generation as dissipative quantum field theory, proving mechanical systems acquire intelligence through the combination of dissipation and non-locality, not through conservation.

</details>


### [135] [A-PINN: Auxiliary Physics-informed Neural Networks for Structural Vibration Analysis in Continuous Euler-Bernoulli Beam](https://arxiv.org/abs/2601.00866)
*Shivani Saini,Ramesh Kumar Vats,Arup Kumar Sahoo*

Main category: cs.LG

TL;DR: 提出带平衡自适应优化器的改进A - PINN框架用于结构振动问题分析，数值模拟验证其性能优于基线至少40%。


<details>
  <summary>Details</summary>
Motivation: PINNs及其变体在解决微分方程问题上有效，为准确分析结构系统振动现象和进行可靠预测分析，深入了解科学机器学习模型解决振动问题的鲁棒性。

Method: 提出带平衡自适应优化器的改进A - PINN框架，对不同场景下的欧拉 - 伯努利梁方程进行数值模拟。

Result: 模型在数值稳定性和预测精度上表现更优，比基线至少提高40%。

Conclusion: 改进的A - PINN框架在解决结构振动问题上有良好性能。

Abstract: Recent advancements in physics-informed neural networks (PINNs) and their variants have garnered substantial focus from researchers due to their effectiveness in solving both forward and inverse problems governed by differential equations. In this research, a modified Auxiliary physics-informed neural network (A-PINN) framework with balanced adaptive optimizers is proposed for the analysis of structural vibration problems. In order to accurately represent structural systems, it is critical for capturing vibration phenomena and ensuring reliable predictive analysis. So, our investigations are crucial for gaining deeper insight into the robustness of scientific machine learning models for solving vibration problems. Further, to rigorously evaluate the performance of A-PINN, we conducted different numerical simulations to approximate the Euler-Bernoulli beam equations under the various scenarios. The numerical results substantiate the enhanced performance of our model in terms of both numerical stability and predictive accuracy. Our model shows improvement of at least 40% over the baselines.

</details>


### [136] [SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation](https://arxiv.org/abs/2601.00868)
*Aditya Sreevatsa K,Arun Kumar Raveendran,Jesrael K Mani,Prakash G Shigli,Rajkumar Rangadore,Narayana Darapaneni,Anwesh Reddy Paduri*

Main category: cs.LG

TL;DR: SmartFlow结合强化学习和智能体AI解决城市共享单车动态再平衡问题，评估显示其效果好，还能连接机器智能和人类操作。


<details>
  <summary>Details</summary>
Motivation: 解决城市共享单车服务中的动态再平衡问题。

Method: 采用多层框架，战略层用DQN代理在高保真模拟中学习策略，战术模块优化行程和调度，通信层用基于大语言模型的智能体AI生成指令。

Result: 评估显示SmartFlow能减少超95%的网络不平衡，所需行驶距离少，卡车利用率高。

Conclusion: SmartFlow为复杂城市移动网络中的可解释AI驱动物流提供了蓝图，可减少闲置时间、提高自行车可用性并降低运营成本。

Abstract: SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.

</details>


### [137] [LLMize: A Framework for Large Language Model-Based Numerical Optimization](https://arxiv.org/abs/2601.00874)
*M. Rizki Oktavian*

Main category: cs.LG

TL;DR: 论文提出LLMize框架用于大语言模型驱动的优化，支持多策略，可注入知识处理复杂问题，实验表明其适用于复杂领域。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现出强推理能力，可应用于数值优化。

Method: 提出LLMize框架，将优化作为黑箱过程，通过迭代提示和上下文学习，支持多种优化策略，可通过自然语言注入知识。

Result: 在多个问题上评估显示，大语言模型优化在简单问题上不如经典求解器，在复杂特定领域任务中有实用和可及性。

Conclusion: LLMize提供了适用于复杂领域特定任务，难以形式化约束和启发式问题的实用手段。

Abstract: Large language models (LLMs) have recently shown strong reasoning capabilities beyond traditional language tasks, motivating their use for numerical optimization. This paper presents LLMize, an open-source Python framework that enables LLM-driven optimization through iterative prompting and in-context learning. LLMize formulates optimization as a black-box process in which candidate solutions are generated in natural language, evaluated by an external objective function, and refined over successive iterations using solution-score feedback. The framework supports multiple optimization strategies, including Optimization by Prompting (OPRO) and hybrid LLM-based methods inspired by evolutionary algorithms and simulated annealing. A key advantage of LLMize is the ability to inject constraints, rules, and domain knowledge directly through natural language descriptions, allowing practitioners to define complex optimization problems without requiring expertise in mathematical programming or metaheuristic design. LLMize is evaluated on convex optimization, linear programming, the Traveling Salesman Problem, neural network hyperparameter tuning, and nuclear fuel lattice optimization. Results show that while LLM-based optimization is not competitive with classical solvers for simple problems, it provides a practical and accessible approach for complex, domain-specific tasks where constraints and heuristics are difficult to formalize.

</details>


### [138] [LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification](https://arxiv.org/abs/2601.00877)
*Thomas Andrews,Mark Law,Sara Ahmadi-Abhari,Alessandra Russo*

Main category: cs.LG

TL;DR: 介绍LearnAD，一种从脑磁共振成像数据预测阿尔茨海默病的神经符号方法，能学习可解释规则，表现良好且提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 需从脑磁共振成像数据中有效预测阿尔茨海默病并使模型具有可解释性。

Method: 先用统计模型、决策树、随机森林或GNN识别相关脑连接，再用FastLAS学习全局规则。

Result: 最佳实例表现优于决策树，与支持向量机准确率相当，略逊于随机森林和全特征GNN，同时保持完全可解释；消融研究显示该方法提升可解释性且性能与纯统计模型相当。

Conclusion: LearnAD展示了符号学习可加深对临床神经科学中GNN行为的理解。

Abstract: We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer's disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.

</details>


### [139] [Attention Needs to Focus: A Unified Perspective on Attention Allocation](https://arxiv.org/abs/2601.00919)
*Zichuan Fu,Wentao Song,Guojing Li,Yejing Wang,Xian Wu,Yimin Deng,Hanyu Yan,Yefeng Zheng,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 本文指出Transformer标准注意力机制存在表征崩溃和注意力汇聚问题，根源是注意力分配不当，并提出Lazy Attention机制解决，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer标准注意力机制中表征崩溃和注意力汇聚问题，且揭示二者深层联系。

Method: 提出统一观点指出问题根源是注意力分配不当，识别出注意力过载和不足两种模式，引入Lazy Attention机制，用位置区分缓解过载，用Elastic - Softmax应对不足。

Result: 在FineWeb - Edu语料库的九个基准测试中，Lazy Attention成功缓解注意力汇聚，实现了高达59.58%的注意力稀疏性，性能有竞争力。

Conclusion: Lazy Attention机制能有效应对Transformer标准注意力机制的问题。

Abstract: The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.

</details>


### [140] [MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs](https://arxiv.org/abs/2601.00920)
*Xingsheng Chen,Regina Zhang,Bo Gao,Xingwei He,Xiaofeng Liu,Pietro Lio,Kwok-Yan Lam,Siu-Ming Yiu*

Main category: cs.LG

TL;DR: 提出MODE框架解决时间序列预测问题，实验显示其在准确性和效率上超基线。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在处理长程依赖和不规则采样数据时，难以平衡效率、可扩展性和准确性。

Method: 提出MODE框架，结合低秩神经常微分方程和增强Mamba架构，包含线性分词层、Mamba编码器块，使用因果卷积、SiLU激活等，还有分段选择性扫描机制。

Result: 在基准数据集实验中，MODE在预测准确性和计算效率上超过现有基线。

Conclusion: 贡献在于提供统一高效的长期时间序列建模架构，整合Mamba选择性扫描与低秩神经常微分方程，通过低秩近似和动态选择性扫描提升效率和可扩展性。

Abstract: Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.

</details>


### [141] [Practical Geometric and Quantum Kernel Methods for Predicting Skeletal Muscle Outcomes in chronic obstructive pulmonary disease](https://arxiv.org/abs/2601.00921)
*Azadeh Alavi,Hamidreza Khalili,Stanley H. Chan,Fatemeh Kouchmeshki,Ross Vlahos*

Main category: cs.LG

TL;DR: 研究用微创生物标志物对慢性阻塞性肺疾病（COPD）骨骼肌结局进行预测建模，对比多种模型，发现几何和量子核提升在低数据、低特征生物医学预测问题中有可衡量益处。


<details>
  <summary>Details</summary>
Motivation: 骨骼肌功能障碍是COPD的临床相关肺外表现且与炎症相关，因此用微创生物标志物进行肌肉结局预测建模。

Method: 研究含213只动物的临床前小样本数据集，对比调优的经典基线、基于Stein散度的几何感知对称正定（SPD）描述符和针对低维表格数据的量子核模型。

Result: 量子核岭回归在肌肉重量预测上有更好表现；Stein散度原型距离在仅使用生物标志物时有小而稳定的增益；筛选式评估在检测低肌肉重量时ROC - AUC可达0.90。

Conclusion: 几何和量子核提升在低数据、低特征生物医学预测问题中能提供可衡量益处，且保留可解释性和透明的模型选择。

Abstract: Skeletal muscle dysfunction is a clinically relevant extra-pulmonary manifestation of chronic obstructive pulmonary disease (COPD) and is closely linked to systemic and airway inflammation. This motivates predictive modelling of muscle outcomes from minimally invasive biomarkers that can be acquired longitudinally. We study a small-sample preclinical dataset comprising 213 animals across two conditions (Sham versus cigarette-smoke exposure), with blood and bronchoalveolar lavage fluid measurements and three continuous targets: tibialis anterior muscle weight (milligram: mg), specific force (millinewton: mN), and a derived muscle quality index (mN per mg). We benchmark tuned classical baselines, geometry-aware symmetric positive definite (SPD) descriptors with Stein divergence, and quantum kernel models designed for low-dimensional tabular data. In the muscle-weight setting, quantum kernel ridge regression using four interpretable inputs (blood C-reactive protein, neutrophil count, bronchoalveolar lavage cellularity, and condition) attains a test root mean squared error of 4.41 mg and coefficient of determination of 0.605, improving over a matched ridge baseline on the same feature set (4.70 mg and 0.553). Geometry-informed Stein-divergence prototype distances yield a smaller but consistent gain in the biomarker-only setting (4.55 mg versus 4.79 mg). Screening-style evaluation, obtained by thresholding the continuous outcome at 0.8 times the training Sham mean, achieves an area under the receiver operating characteristic curve (ROC-AUC) of up to 0.90 for detecting low muscle weight. These results indicate that geometric and quantum kernel lifts can provide measurable benefits in low-data, low-feature biomedical prediction problems, while preserving interpretability and transparent model selection.

</details>


### [142] [Complexity-based code embeddings](https://arxiv.org/abs/2601.00924)
*Rares Folea,Radu Iacob,Emil Slusanschi,Traian Rebedea*

Main category: cs.LG

TL;DR: 提出将算法源代码转换为数值嵌入的通用方法，并用其实现XGBoost算法在多标签数据集达一定F1分数。


<details>
  <summary>Details</summary>
Motivation: 寻找一种将各种算法源代码转换为数值嵌入的方式。

Method: 通过动态分析计算机程序对不同输入的行为，为分析指标定制多个通用复杂度函数，基于r - Complexity生成算法嵌入。

Result: 使用提出的代码嵌入实现XGBoost算法，在基于Codeforces平台编程竞赛真实代码片段构建的11类多标签数据集上达到平均F1分数。

Conclusion: 所提出的代码嵌入方法可用于实现算法并在多标签数据集上取得较好表现。

Abstract: This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.

</details>


### [143] [LOFA: Online Influence Maximization under Full-Bandit Feedback using Lazy Forward Selection](https://arxiv.org/abs/2601.00933)
*Jinyu Xu,Abhishek K. Umrawal*

Main category: cs.LG

TL;DR: 研究在线环境下影响最大化问题，提出LOFA算法，实验显示其性能优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 在全老虎机反馈模型下，解决在线环境中影响最大化问题，期望降低遗憾值。

Method: 利用影响函数的子模性，提出Lazy Online Forward Algorithm (LOFA)。

Result: 在真实社交网络实验中，LOFA在累积遗憾和即时奖励方面优于现有老虎机算法。

Conclusion: LOFA算法具有更低的经验遗憾，性能更优。

Abstract: We study the problem of influence maximization (IM) in an online setting, where the goal is to select a subset of nodes$\unicode{x2014}$called the seed set$\unicode{x2014}$at each time step over a fixed time horizon, subject to a cardinality budget constraint, to maximize the expected cumulative influence. We operate under a full-bandit feedback model, where only the influence of the chosen seed set at each time step is observed, with no additional structural information about the network or diffusion process. It is well-established that the influence function is submodular, and existing algorithms exploit this property to achieve low regret. In this work, we leverage this property further and propose the Lazy Online Forward Algorithm (LOFA), which achieves a lower empirical regret. We conduct experiments on a real-world social network to demonstrate that LOFA achieves superior performance compared to existing bandit algorithms in terms of cumulative regret and instantaneous reward.

</details>


### [144] [Adapting Feature Attenuation to NLP](https://arxiv.org/abs/2601.00965)
*Tianshuo Yang,Ryan Rabinowitz,Terrance E. Boult,Jugal Kalita*

Main category: cs.LG

TL;DR: 研究文本的开放集识别（OSR），将计算机视觉的特征衰减假设应用于Transformer，对比多种方法，指出移植视觉OSR思想到语言模型的前景与局限。


<details>
  <summary>Details</summary>
Motivation: Transformer分类器在面对未见类别输入时较脆弱，研究文本的开放集识别。

Method: 将COSTARR框架应用于两个语言模型，评估MSP、MaxLogit和温度缩放自由能得分，使用OOSA和AUOSCR指标。

Result: COSTARR无需重新训练可扩展到NLP，但相比MaxLogit或MSP无显著优势；自由能得分在高类别数量设置中落后。

Conclusion: 移植视觉OSR思想到语言模型有前景，但也存在局限，需要更大的主干模型和定制的衰减策略。

Abstract: Transformer classifiers such as BERT deliver impressive closed-set accuracy, yet they remain brittle when confronted with inputs from unseen categories--a common scenario for deployed NLP systems. We investigate Open-Set Recognition (OSR) for text by porting the feature attenuation hypothesis from computer vision to transformers and by benchmarking it against state-of-the-art baselines. Concretely, we adapt the COSTARR framework--originally designed for classification in computer vision--to two modest language models (BERT (base) and GPT-2) trained to label 176 arXiv subject areas. Alongside COSTARR, we evaluate Maximum Softmax Probability (MSP), MaxLogit, and the temperature-scaled free-energy score under the OOSA and AUOSCR metrics. Our results show (i) COSTARR extends to NLP without retraining but yields no statistically significant gain over MaxLogit or MSP, and (ii) free-energy lags behind all other scores in this high-class-count setting. The study highlights both the promise and the current limitations of transplanting vision-centric OSR ideas to language models, and points toward the need for larger backbones and task-tailored attenuation strategies.

</details>


### [145] [Geometric and Dynamic Scaling in Deep Transformers](https://arxiv.org/abs/2601.01014)
*Haoran Su,Chenyu You*

Main category: cs.LG

TL;DR: 深层Transformer架构存在表征冗余和秩崩溃问题，本文认为这是几何问题并提出流形几何Transformer（MGT）解决。


<details>
  <summary>Details</summary>
Motivation: 现有对深层Transformer崩溃的解释无法说明在现代归一化和初始化方案下崩溃仍存在的原因，需新角度解释。

Method: 提出统一几何框架，含流形约束超连接和深度增量学习两个正交原则，形成MGT架构。

Result: 分析表明在超深层网络中，保证几何有效性和动态擦除对避免秩崩溃至关重要。

Conclusion: 提出评估超100层Transformer的协议，验证几何是深度表征学习的关键限制因素而非深度本身。

Abstract: Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.

</details>


### [146] [Improving Variational Autoencoder using Random Fourier Transformation: An Aviation Safety Anomaly Detection Case-Study](https://arxiv.org/abs/2601.01016)
*Ata Akbari Asanjan,Milad Memarzadeh,Bryan Matthews,Nikunj Oza*

Main category: cs.LG

TL;DR: 研究利用随机傅里叶变换（RFT）改进深度神经网络训练和推理，分析其在模型训练中的作用，引入可训练的RFT变体并在不同数据集上验证。


<details>
  <summary>Details</summary>
Motivation: 改进深度神经网络（自编码器和变分自编码器）的训练和推理，探究RFT在模型训练行为中的作用以及在基于重构的异常检测中的角色。

Method: 采用随机傅里叶变换，用频率原理（F - 原理）分析RFT在模型训练中的表现，引入可训练的RFT变体，在多个数据集上验证。

Result: 带傅里叶变换的模型优于传统模型，但可训练傅里叶变换相比随机傅里叶变换的优势未明确。

Conclusion: 傅里叶变换可提升模型性能，但可训练傅里叶变换是否更优尚无定论。

Abstract: In this study, we focus on the training process and inference improvements of deep neural networks (DNNs), specifically Autoencoders (AEs) and Variational Autoencoders (VAEs), using Random Fourier Transformation (RFT). We further explore the role of RFT in model training behavior using Frequency Principle (F-Principle) analysis and show that models with RFT turn to learn low frequency and high frequency at the same time, whereas conventional DNNs start from low frequency and gradually learn (if successful) high-frequency features. We focus on reconstruction-based anomaly detection using autoencoder and variational autoencoder and investigate the RFT's role. We also introduced a trainable variant of RFT that uses the existing computation graph to train the expansion of RFT instead of it being random. We showcase our findings with two low-dimensional synthetic datasets for data representation, and an aviation safety dataset, called Dashlink, for high-dimensional reconstruction-based anomaly detection. The results indicate the superiority of models with Fourier transformation compared to the conventional counterpart and remain inconclusive regarding the benefits of using trainable Fourier transformation in contrast to the Random variant.

</details>


### [147] [A UCB Bandit Algorithm for General ML-Based Estimators](https://arxiv.org/abs/2601.01061)
*Yajing Liu,Erkao Bao,Linqi Song*

Main category: cs.LG

TL;DR: 提出ML - UCB算法，将机器学习模型集成到多臂老虎机框架，可减少理论分析，实验表明比LinUCB有改进。


<details>
  <summary>Details</summary>
Motivation: 在序贯决策中使用复杂机器学习模型时缺乏有效探索所需的可处理集中不等式。

Method: 直接对底层估计器的学习曲线进行建模，假设均方误差随训练样本数呈幂律下降，推导广义集中不等式。

Result: 证明ML - UCB可实现次线性遗憾，在协同过滤推荐系统实验中比LinUCB有显著改进。

Conclusion: 该框架能实现任意学习曲线可经验表征的机器学习模型的集成，无需特定模型理论分析。

Abstract: We present ML-UCB, a generalized upper confidence bound algorithm that integrates arbitrary machine learning models into multi-armed bandit frameworks. A fundamental challenge in deploying sophisticated ML models for sequential decision-making is the lack of tractable concentration inequalities required for principled exploration. We overcome this limitation by directly modeling the learning curve behavior of the underlying estimator. Specifically, assuming the Mean Squared Error decreases as a power law in the number of training samples, we derive a generalized concentration inequality and prove that ML-UCB achieves sublinear regret. This framework enables the principled integration of any ML model whose learning curve can be empirically characterized, eliminating the need for model-specific theoretical analysis. We validate our approach through experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data designed to simulate a simplified two-tower model, demonstrating substantial improvements over LinUCB

</details>


### [148] [SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models](https://arxiv.org/abs/2601.01062)
*Yunlin Zeng*

Main category: cs.LG

TL;DR: 提出端到端视觉播客生成管道，微调Qwen3 - VL - 32B模型，采用合成到真实训练策略，提出综合评估框架，微调模型在对话自然度和叙事深度上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在生成多说话者播客对话方面研究不足且难评估，标准指标不能有效衡量对话质量。

Method: 提出端到端视觉播客生成管道，在4000个图像 - 对话对数据集上微调Qwen3 - VL - 32B模型，采用合成到真实训练策略，提出超越文本重叠的综合评估框架。

Result: 微调后的32B模型在对话自然度（胜率超80%）和叙事深度（轮次长度增加50%）上显著优于235B基础模型，视觉基础能力相同。

Conclusion: 所提出的方法和评估框架有效提升了视觉语言模型生成多说话者播客对话的能力。

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\% win rate) and narrative depth (+50\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).

</details>


### [149] [Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments](https://arxiv.org/abs/2601.01075)
*Hansen Jin Lillemark,Benhao Huang,Fangneng Zhan,Yilun Du,Thomas Anderson Keller*

Main category: cs.LG

TL;DR: 提出Flow Equivariant World Models框架，统一自运动与外部物体运动，在视频世界建模基准测试中表现出色，利于长序列预测与具身智能。


<details>
  <summary>Details</summary>
Motivation: 多数神经网络世界模型忽略感官输入流的对称结构，重复从数据学习相同变换，需要新方法利用该结构。

Method: 引入Flow Equivariant World Models框架，将自运动和外部物体运动统一为单参数李群“流”，实现变换的群等变性。

Result: 在2D和3D部分观测视频世界建模基准测试中，显著优于同类先进架构，尤其在视野外有可预测动态时。

Conclusion: 流等变性通过构建世界模型表示，为数据高效、对称引导的具身智能提供可扩展途径。

Abstract: Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.

</details>


### [150] [Learning from Historical Activations in Graph Neural Networks](https://arxiv.org/abs/2601.01123)
*Yaniv Galron,Hadar Sinai,Haggai Maron,Moshe Eliasof*

Main category: cs.LG

TL;DR: 提出HISTOGRAPH解决传统图池化方案未充分利用历史图激活的问题，在多图分类基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统图池化方案依赖最后一层GNN特征，可能未充分利用模型前向传播中前层的重要激活，在深层架构中问题更明显。

Method: 引入基于注意力的两阶段最终聚合层HISTOGRAPH，先对中间激活应用统一的逐层注意力，再进行逐节点注意力。

Result: 在多个图分类基准测试中，HISTOGRAPH性能优于传统技术，在深度GNN中具有较强鲁棒性。

Conclusion: HISTOGRAPH通过利用节点激活历史和图结构，能有效改进用于最终预测的特征。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node's representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.

</details>


### [151] [Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models](https://arxiv.org/abs/2601.01162)
*Zihua Yang,Xin Liao,Yiqun Zhang,Yiu-ming Cheung*

Main category: cs.LG

TL;DR: 提出 ARISE 方法利用大语言模型外部语义知识进行分类数据聚类，实验显示比代表性方法有 19 - 27% 提升。


<details>
  <summary>Details</summary>
Motivation: 分类数据聚类中缺乏合适相似度度量方法，现有方法在样本有限时不可靠，数据语义背景未充分挖掘。

Method: 提出 ARISE 方法，利用大语言模型外部语义知识构建语义感知表示，结合原数据探索显著聚类。

Result: 在八个基准数据集上实验，比七个代表性方法有持续提升，提升幅度 19 - 27%。

Conclusion: ARISE 方法能有效用于分类数据聚类，提升聚类质量。

Abstract: Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE

</details>


### [152] [Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers on Long-Context Dyadic Sessions](https://arxiv.org/abs/2601.01237)
*Abidemi Koledoye,Chinemerem Unachukwu,Gold Nwobu,Hasin Rana*

Main category: cs.LG

TL;DR: 本文对Mamba SSM和LLaMA Transformer在长上下文序列上进行基准测试，从计算效率和表征效率两方面评估，为长上下文应用从业者提供见解。


<details>
  <summary>Details</summary>
Motivation: 比较Mamba SSM和LLaMA Transformer在长上下文序列建模上的表现，因SSMs在长上下文建模有计算复杂度优势。

Method: 以二元治疗会话为测试用例，从计算效率（测量512到8192个令牌的内存使用和推理速度）和表征效率（分析隐藏状态动态和注意力模式）两方面评估两种架构。

Result: 研究得出了相关评估结果，但文中未详细给出。

Conclusion: 为长上下文应用从业者提供可操作见解，明确了SSMs比Transformers更具优势的精确条件。

Abstract: State Space Models (SSMs) have emerged as a promising alternative to Transformers for long-context sequence modeling, offering linear $O(N)$ computational complexity compared to the Transformer's quadratic $O(N^2)$ scaling. This paper presents a comprehensive benchmarking study comparing the Mamba SSM against the LLaMA Transformer on long-context sequences, using dyadic therapy sessions as a representative test case. We evaluate both architectures across two dimensions: (1) computational efficiency, where we measure memory usage and inference speed from 512 to 8,192 tokens, and (2) representational efficiency, where we analyze hidden state dynamics and attention patterns. Our findings provide actionable insights for practitioners working with long-context applications, establishing precise conditions under which SSMs offer advantages over Transformers.

</details>


### [153] [ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System](https://arxiv.org/abs/2601.01297)
*Anantha Sharma*

Main category: cs.LG

TL;DR: 本文介绍Argus框架，将漂移检测重新概念化为跟踪数据流形固定空间分区的局部统计信息，通过多种贡献解决高维数据流分布漂移检测问题并做了实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有的高维数据流分布漂移检测方法存在全局比较可扩展性差、基于投影方法丢失几何结构、重新聚类方法存在身份不稳定等问题。

Method: 提出Argus框架，用规范正交基上的Voronoi镶嵌生成对正交变换不变的漂移度量，实现O(N)复杂度和单元级空间定位；发展图论特征区分连贯分布变化和孤立扰动；引入乘积量化镶嵌处理高维数据。

Result: 理论上证明框架的不变性性质，实验验证框架能在坐标旋转下正确识别漂移，而现有方法出现误报。

Conclusion: 该框架为分布监测提供了有原则的几何基础，能保留高维结构且无需成对比较的计算负担。

Abstract: Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.
  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.
  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.

</details>


### [154] [From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion](https://arxiv.org/abs/2601.01347)
*Yuyan Pi,Min Jin,Wentao Xie,Xinhua Liu*

Main category: cs.LG

TL;DR: 提出基于Graph - Motif特征融合和多标签生成的开放式ADR预测范式GM - MLG，实验显示有显著性能提升并能解释ADR与基序的关系。


<details>
  <summary>Details</summary>
Motivation: 当前药物不良反应（ADR）预测方法受药物数据稀缺、封闭标签集和标签依赖建模不足的限制，为降低新药开发成本和周期，需改进预测方法。

Method: 利用分子结构作为特征，构建原子、局部分子和全局分子的双图表示架构；将ADR预测从多标签分类转变为基于Transformer解码器的多标签生成，用位置嵌入捕捉标签依赖关系，通过自回归解码生成预测。

Result: GM - MLG实现了高达38%的提升，平均提升20%，预测空间从200种扩展到超10000种；通过逆向合成基序分析阐明了ADR与基序的非线性构效关系。

Conclusion: GM - MLG在ADR预测上有显著性能提升，能为降低药物安全的系统性风险提供可解释的创新支持。

Abstract: Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.

</details>


### [155] [Data Complexity-aware Deep Model Performance Forecasting](https://arxiv.org/abs/2601.01383)
*Yen-Chia Chen,Hsing-Kuo Pao,Hanjuan Huang*

Main category: cs.LG

TL;DR: 提出轻量级两阶段框架估计模型性能，可跨数据集和模型类型泛化，还能用于指导模型选择等。


<details>
  <summary>Details</summary>
Motivation: 传统选择模型架构的试错法耗时、耗资源且难自动化，先前工作有计算开销大或缺乏泛化性问题。

Method: 提出轻量级两阶段框架，第一阶段基于数据集可测量属性分析预测基线，第二阶段用模型架构和超参数信息调整估计。

Result: 框架能跨数据集和模型类型泛化，一些预测特征能指导模型选择和反映数据质量。

Conclusion: 该框架可预测模型性能，还能指导架构选择、预处理过程和检测问题数据集。

Abstract: Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model's architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.

</details>


### [156] [Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning](https://arxiv.org/abs/2601.01387)
*Yongzhe Li,Lin Guan,Zihan Cai,Zuxian Lin,Jiyu Huang,Liukai Chen*

Main category: cs.LG

TL;DR: 本文提出SaMPFA框架用于潮流分析，通过LTS采样技术和RMGL模型，在不同规模系统上实现更好的适应性和泛化能力，精度有显著提升。


<details>
  <summary>Details</summary>
Motivation: 开发对拓扑变化有强适应性的深度学习模型用于潮流分析，提升模型在可变系统规模下的性能和支路功率预测的鲁棒性。

Method: 提出SaMPFA框架，引入LTS采样技术增强跨尺度学习能力，设计RMGL模型预测母线电压和支路功率，损失函数加入额外项捕捉物理模式。

Result: 在IEEE 39 - 母线系统和中国某省级电网仿真中，模型在可变系统规模下适应性和泛化能力优越，精度分别提升4.47%和36.82%。

Conclusion: 所提出的模型在可变系统规模下能实现良好的适应性和泛化能力，提高预测精度。

Abstract: Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.

</details>


### [157] [A Graph-based Framework for Online Time Series Anomaly Detection Using Model Ensemble](https://arxiv.org/abs/2601.01403)
*Zewei Yu,Jianqiu Xu,Caimin Li*

Main category: cs.LG

TL;DR: 本文提出用模型集成的无监督图基框架GDME做在线时间序列异常检测，实验显示比现有方法优24%。


<details>
  <summary>Details</summary>
Motivation: 工业系统中流数据增多，现有异常检测方法难以用于在线检测和处理异构流数据。

Method: 提出GDME框架，维护动态模型池，用动态图结构表示模型关系，用社区检测选择集成子集，通过监测图结构变化检测概念漂移。

Result: 在七个异构时间序列实验中，GDME比现有在线异常检测方法优24%，其集成策略比单个模型和平均集成检测性能更好，且计算效率有竞争力。

Conclusion: GDME框架在在线时间序列异常检测方面表现出色，能有效处理异构流数据并适应数据变化。

Abstract: With the increasing volume of streaming data in industrial systems, online anomaly detection has become a critical task. The diverse and rapidly evolving data patterns pose significant challenges for online anomaly detection. Many existing anomaly detection methods are designed for offline settings or have difficulty in handling heterogeneous streaming data effectively. This paper proposes GDME, an unsupervised graph-based framework for online time series anomaly detection using model ensemble. GDME maintains a dynamic model pool that is continuously updated by pruning underperforming models and introducing new ones. It utilizes a dynamic graph structure to represent relationships among models and employs community detection on the graph to select an appropriate subset for ensemble. The graph structure is also used to detect concept drift by monitoring structural changes, allowing the framework to adapt to evolving streaming data. Experiments on seven heterogeneous time series demonstrate that GDME outperforms existing online anomaly detection methods, achieving improvements of up to 24%. In addition, its ensemble strategy provides superior detection performance compared with both individual models and average ensembles, with competitive computational efficiency.

</details>


### [158] [Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models](https://arxiv.org/abs/2601.01452)
*Jian Feng,Zhihong Huang*

Main category: cs.LG

TL;DR: 提出用于微调大语言模型的贝叶斯子空间零阶优化器（BSZO），通过卡尔曼滤波结合多扰动方向信息，理论上提升收敛速度，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法在微调大语言模型时依赖单步梯度估计，提出新的优化器以改进。

Method: 提出 BSZO 优化器，使用卡尔曼滤波结合多扰动方向有限差分信息，将每次测量作为噪声观测，通过贝叶斯推理更新投影梯度后验分布，并使用基于残差的自适应机制调整扰动尺度。

Result: 理论分析表明 BSZO 比标准零阶方法收敛速度提高 k/γ 倍；在 RoBERTa、Mistral 和 OPT 模型实验中，优于 MeZO、MeZO - Adam 和 HiZOO，在 OPT - 13B 上最高有 6.67% 绝对平均提升，内存使用接近推理基线。

Conclusion: BSZO 是一种有效的微调大语言模型的零阶优化方法，能提高收敛速度和性能，同时控制内存使用。

Abstract: Fine-tuning large language models (LLMs) with zeroth-order (ZO) optimization reduces memory by approximating gradients through function evaluations, but existing methods rely on one-step gradient estimates from random perturbations. We introduce Bayesian Subspace Zeroth-Order optimization (BSZO), a ZO optimizer that applies Kalman filtering to combine finite-difference information across multiple perturbation directions. By treating each finite-difference measurement as a noisy observation, BSZO builds a posterior distribution over the projected gradient and updates it through Bayesian inference, with a residual-based adaptive mechanism to adjust perturbation scales. Theoretical analysis shows that BSZO improves the convergence rate by a factor of $k/γ$ compared to standard ZO methods. Experiments on RoBERTa, Mistral, and OPT models show that BSZO outperforms MeZO, MeZO-Adam, and HiZOO across various tasks, achieving up to 6.67\% absolute average improvement on OPT-13B while keeping memory usage close to inference-only baselines (1.00$\times$--1.08$\times$ of MeZO).

</details>


### [159] [Utilizing Earth Foundation Models to Enhance the Simulation Performance of Hydrological Models with AlphaEarth Embeddings](https://arxiv.org/abs/2601.01558)
*Pengfei Qu,Wenyu Ouyang,Chi Zhang,Yikai Chai,Shuolong Xu,Lei Ye,Yongri Piao,Miao Zhang,Huchuan Lu*

Main category: cs.LG

TL;DR: 研究AlphaEarth Foundation嵌入能否更好描述流域特征及选取合适参考流域对无测站区域流量预测表现的影响，发现其有积极作用。


<details>
  <summary>Details</summary>
Motivation: 传统流域属性无法完全代表自然环境复杂性，预测无流量记录地区河流量具有挑战性，需更有效的描述方式。

Method: 研究AlphaEarth Foundation嵌入表达流域特征，通过其预测未用于训练流域的流量，并探究选取合适参考流域对无测站区域预测的影响。

Result: 使用AlphaEarth Foundation嵌入的模型预测未训练流域流量时精度更高，基于此嵌入的相似性可识别环境和水文行为相近流域，改善预测表现，加入大量不相似流域会降低准确率。

Conclusion: 卫星信息环境表征可加强水文预测，支持开发更易适应不同景观的模型。

Abstract: Predicting river flow in places without streamflow records is challenging because basins respond differently to climate, terrain, vegetation, and soils. Traditional basin attributes describe some of these differences, but they cannot fully represent the complexity of natural environments. This study examines whether AlphaEarth Foundation embeddings, which are learned from large collections of satellite images rather than designed by experts, offer a more informative way to describe basin characteristics. These embeddings summarize patterns in vegetation, land surface properties, and long-term environmental dynamics. We find that models using them achieve higher accuracy when predicting flows in basins not used for training, suggesting that they capture key physical differences more effectively than traditional attributes. We further investigate how selecting appropriate donor basins influences prediction in ungauged regions. Similarity based on the embeddings helps identify basins with comparable environmental and hydrological behavior, improving performance, whereas adding many dissimilar basins can reduce accuracy. The results show that satellite-informed environmental representations can strengthen hydrological forecasting and support the development of models that adapt more easily to different landscapes.

</details>


### [160] [The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs](https://arxiv.org/abs/2601.01580)
*Zibo Zhao,Yuanting Zha,Haipeng Zhang,Xingcheng Xu*

Main category: cs.LG

TL;DR: 研究大语言模型强化学习（RL）后训引发的自我反思能力，提出梯度归因特性，解释RL比监督微调（SFT）成功原因，实证表明RL泛化源于决策能力提升。


<details>
  <summary>Details</summary>
Motivation: 解释统一优化目标如何产生生成解决方案和评估修订时机这两种不同功能，理解RL后训使大语言模型出现自我反思能力的机制。

Method: 引入梯度归因特性，通过两阶段决策采样（DS）假设对策略分解，并对理论预测进行实证验证。

Result: 证明代理奖励具有平衡梯度归因，SFT和KL惩罚具有不平衡梯度归因；实证表明RL优越的泛化主要源于改进的决策能力而非采样能力。

Conclusion: 给出了RL成功而SFT失败的理论解释，为思维模型中的自我修正提供了第一性原理的机理解释。

Abstract: Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($π_{sample}$) for generation and decision ($π_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $π_{sample}$ while leaving $π_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($π_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.

</details>


### [161] [REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training](https://arxiv.org/abs/2601.01605)
*Xin Di,Xinglin Piao,Fei Wang,Guodong Jing,Yong Zhang*

Main category: cs.LG

TL;DR: 提出 REE - TTT 模型结合自适应测试时训练机制提升降水临近预报性能，跨区域极端场景实验显示其效果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 深度学习雷达回波外推法在降水临近预报中泛化性差，依赖高质量本地训练数据和静态模型参数，限制其跨区域和极端事件应用。

Method: 提出 REE - TTT 模型，采用新设计的时空测试时训练（ST - TTT）模块，以任务特定注意力机制替换 TTT 层中的标准线性投影。

Result: 跨区域极端降水场景实验表明，REE - TTT 在预测准确性和泛化性上大幅超越现有基线模型。

Conclusion: REE - TTT 模型能够适应数据分布变化，显著提升降水特征表示能力和自适应能力。

Abstract: Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.

</details>


### [162] [Learning Resilient Elections with Adversarial GNNs](https://arxiv.org/abs/2601.01653)
*Hao Xiang Li,Yash Shah,Lorenzo Giusti*

Main category: cs.LG

TL;DR: 本文通过结合神经网络架构改进与对抗训练，推广学习投票规则的表达能力，提高投票规则的弹性和社会福利，并在数据集上评估效果，解决了先前工作的局限。


<details>
  <summary>Details</summary>
Motivation: 理想的通用投票规则设计是机制设计研究的前沿问题，现有方法存在如对策略性投票的鲁棒性等问题，无法直接应用于现实场景。

Method: 推广学习投票规则的表达能力，结合神经网络架构改进与对抗训练，用二分图表示选举，用图神经网络学习投票规则。

Result: 在合成和真实世界数据集上评估了方法的有效性。

Conclusion: 该方法解决了先前学习投票规则工作的关键局限，为机器学习应用于现实选举开辟了新领域。

Abstract: In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.

</details>


### [163] [Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths](https://arxiv.org/abs/2601.01663)
*He Sun,Jiwoong Shin,Ravi Dhar*

Main category: cs.LG

TL;DR: 研究变长轨迹生成建模，提出长度感知采样（LAS）策略，理论证明其优势，实验表明LAS优于随机采样。


<details>
  <summary>Details</summary>
Motivation: 标准小批量训练在轨迹长度高度异质时不稳定，影响轨迹派生统计量的分布匹配。

Method: 提出长度感知采样（LAS）策略，将轨迹按长度分组并从单一长度桶中采样批次，集成到条件轨迹GAN中并添加辅助时间对齐损失。

Result: 在商场购物者轨迹和多个公共序列数据集上，LAS在派生变量分布匹配上表现更好，优于随机采样。

Conclusion: LAS能有效解决轨迹长度异质导致的训练不稳定问题，提升分布匹配效果。

Abstract: We study generative modeling of \emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \emph{distribution matching} for trajectory-derived statistics. We propose \textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.

</details>


### [164] [Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives](https://arxiv.org/abs/2601.01665)
*Wei Liu,Yaoxin Wu,Yingqian Zhang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: 提出面向偏好的深度强化学习求解器统一鲁棒性框架，含攻击和防御策略，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的多目标组合优化问题求解器的鲁棒性在不同复杂问题分布中研究不足。

Method: 提出统一框架，开发基于偏好的对抗攻击生成难题实例，用帕累托前沿质量下降量化影响；引入防御策略，将硬度感知偏好选择融入对抗训练。

Result: 攻击方法成功为不同求解器学习到难题实例，防御方法显著增强神经求解器的鲁棒性和泛化性。

Conclusion: 所提出的框架和策略能有效提升多目标组合优化问题求解器在难题或分布外实例上的性能。

Abstract: Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.

</details>


### [165] [Digital Twin-Driven Communication-Efficient Federated Anomaly Detection for Industrial IoT](https://arxiv.org/abs/2601.01701)
*Mohammed Ayalew Belay,Adil Rasheed,Pierluigi Salvo Rossi*

Main category: cs.LG

TL;DR: 本文提出数字孪生集成联邦学习（DTFL）方法解决现有异常检测方法的问题，实验显示该方法有通信效率提升和更快收敛优势。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测统计和机器学习方法存在依赖真实传感器数据集、标记数据有限、误报率高和隐私问题等挑战。

Method: 提出五种新方法：数字孪生元学习（DTML）、联邦参数融合（FPF）、逐层参数交换（LPE）、循环权重自适应（CWA）和数字孪生知识蒸馏（DTKD），结合合成与现实世界知识。

Result: 使用公开数据集实验，对于80%目标准确率，CWA、FPF、LPE、DTML达到目标的轮数不同，标准FedAvg基线和DTKD 100轮内未达目标，有通信效率提升。

Conclusion: 将数字孪生知识集成到联邦学习可加速工业物联网异常检测收敛到有意义的准确率阈值。

Abstract: Anomaly detection is increasingly becoming crucial for maintaining the safety, reliability, and efficiency of industrial systems. Recently, with the advent of digital twins and data-driven decision-making, several statistical and machine-learning methods have been proposed. However, these methods face several challenges, such as dependence on only real sensor datasets, limited labeled data, high false alarm rates, and privacy concerns. To address these problems, we propose a suite of digital twin-integrated federated learning (DTFL) methods that enhance global model performance while preserving data privacy and communication efficiency. Specifically, we present five novel approaches: Digital Twin-Based Meta-Learning (DTML), Federated Parameter Fusion (FPF), Layer-wise Parameter Exchange (LPE), Cyclic Weight Adaptation (CWA), and Digital Twin Knowledge Distillation (DTKD). Each method introduces a unique mechanism to combine synthetic and real-world knowledge, balancing generalization with communication overhead. We conduct an extensive experiment using a publicly available cyber-physical anomaly detection dataset. For a target accuracy of 80%, CWA reaches the target in 33 rounds, FPF in 41 rounds, LPE in 48 rounds, and DTML in 87 rounds, whereas the standard FedAvg baseline and DTKD do not reach the target within 100 rounds. These results highlight substantial communication-efficiency gains (up to 62% fewer rounds than DTML and 31% fewer than LPE) and demonstrate that integrating DT knowledge into FL accelerates convergence to operationally meaningful accuracy thresholds for IIoT anomaly detection.

</details>


### [166] [HyperCLOVA X 8B Omni](https://arxiv.org/abs/2601.01792)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.LG

TL;DR: 介绍了HyperCLOVA X 8B Omni，首个支持文本、音频和视觉多模态输入输出的模型，有竞争力表现且即将开源权重。


<details>
  <summary>Details</summary>
Motivation: 将多模态理解和生成整合到单个模型，开发实用的任意到任意的全功能助手。

Method: 通过共享的下一个令牌预测接口统一模态，视觉和音频编码器注入连续嵌入以实现细粒度理解。

Result: 在不同输入输出组合（跨文本、音频和视觉，韩语和英语）上，与同等规模模型相比有竞争力表现。

Conclusion: HyperCLOVA X 8B Omni开放权重将支持广泛研究和部署场景。

Abstract: In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.

</details>


### [167] [Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving](https://arxiv.org/abs/2601.01800)
*Qi Wei,Junchao Fan,Zhao Yang,Jianhua Wang,Jingkai Mao,Xiaolin Chang*

Main category: cs.LG

TL;DR: 现有强化学习用于自动驾驶在应对扰动时鲁棒性不足，本文提出CARRL方法，由REA和RTRA组件组成，实验显示比基线方法至少降低22.66%碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习用于自动驾驶时，存在对扰动鲁棒性不足问题，现有对抗训练方法未能充分考虑智能体和对手的不对称性以及安全关键风险的稀疏性，导致鲁棒性不满足实际场景需求。

Method: 提出CARRL对抗训练方法，由REA和RTRA两个组件构成，将它们的交互建模为一般和博弈，REA采用解耦优化机制，RTRA通过双回放缓冲区应对对抗数据稀缺问题。

Result: 实验表明，与现有基线方法相比，该方法在所有情况下至少降低了22.66%的碰撞率。

Conclusion: CARRL方法能更有效地应对自动驾驶中的稀疏安全关键风险，提高了自动驾驶的安全性。

Abstract: Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\% across all cases compared to state-of-the-art baseline methods.

</details>


### [168] [Moments Matter:Stabilizing Policy Optimization using Return Distributions](https://arxiv.org/abs/2601.01803)
*Dennis Jabs,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 本文提出利用环境随机性减少更新导致的变异性的方法，通过分布批评家建模状态 - 动作回报分布，用高阶矩修正PPO优势函数，在Walker2D中提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习中策略不稳定，直接估计回报分布计算成本高，需寻找替代方法提升稳定性。

Method: 通过分布批评家建模状态 - 动作回报分布，用高阶矩（偏度和峰度）修正PPO优势函数，惩罚极端尾部行为。

Result: 在Walker2D中，基于矩的修正方法使稳定性提升达75%，同时保持可比的评估回报。

Conclusion: 在更新后批评家值与回报对齐差的环境中，该方法能缩小回报分布，提升稳定性。

Abstract: Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.

</details>


### [169] [Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance](https://arxiv.org/abs/2601.01887)
*Jiawen Zhang,Lipeng He,Kejia Chen,Jian Lou,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: 仅用一个安全示例就能以最小成本完全恢复大语言模型的安全对齐，且不牺牲效用，还揭示安全梯度低秩结构并验证方法通用性。


<details>
  <summary>Details</summary>
Motivation: 现有微调安全对齐大语言模型的方法需大量安全样本或校准集，有计算开销且降低模型效用，要解决此问题。

Method: 用单个安全示例进行安全对齐恢复，研究安全梯度的低秩结构。

Result: 能在不牺牲效用、成本极小的情况下完全恢复安全对齐，不受有害示例数量和模型大小影响，几轮迭代即可收敛。

Conclusion: 方法有效且具有通用性，在五个安全对齐大语言模型和多个数据集上得到验证。

Abstract: Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.

</details>


### [170] [Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning](https://arxiv.org/abs/2601.01904)
*Yuxuan Li,Harshith Reddy Kethireddy,Srijita Das*

Main category: cs.LG

TL;DR: 探讨强化学习中偏好学习的特征依赖噪声，评估其影响并发现新现象。


<details>
  <summary>Details</summary>
Motivation: 现有文献处理偏好学习中的噪声类型有限，且多为均匀分布与观测无关，需研究特征依赖噪声。

Method: 形式化目标特征依赖噪声概念，提出多种噪声变体，在复杂连续控制任务中评估特征依赖噪声。

Result: 某些特征依赖噪声设置下，现有抗噪方法性能显著下降，无显式去噪方法表现更好；语言模型噪声有类似特征依赖噪声特性。

Conclusion: 呼吁进一步研究如何稳健地学习特征依赖噪声。

Abstract: Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.
  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.
  We also find language model's noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.

</details>


### [171] [Theoretical Convergence of SMOTE-Generated Samples](https://arxiv.org/abs/2601.01927)
*Firuz Kamalov,Hana Sulieman,Witold Pedrycz*

Main category: cs.LG

TL;DR: 对SMOTE收敛性质进行理论分析，证明合成随机变量收敛性，给出收敛速度相关结论并通过实验验证，提升数据增强技术理解。


<details>
  <summary>Details</summary>
Motivation: 不平衡数据影响广泛，SMOTE是解决该问题流行方法，需对其进行理论验证。

Method: 对SMOTE收敛性质进行严格理论分析，证明合成随机变量概率收敛和均值收敛，研究最近邻秩与收敛速度关系。

Result: 证明合成随机变量Z依概率收敛到基础随机变量X，在X紧凑时有更强的均值收敛，低最近邻秩导致更快收敛，数值实验支持理论结果。

Conclusion: 研究为数据增强技术提供基础理解，可用于不平衡数据场景之外。

Abstract: Imbalanced data affects a wide range of machine learning applications, from healthcare to network security. As SMOTE is one of the most popular approaches to addressing this issue, it is imperative to validate it not only empirically but also theoretically. In this paper, we provide a rigorous theoretical analysis of SMOTE's convergence properties. Concretely, we prove that the synthetic random variable Z converges in probability to the underlying random variable X. We further prove a stronger convergence in mean when X is compact. Finally, we show that lower values of the nearest neighbor rank lead to faster convergence offering actionable guidance to practitioners. The theoretical results are supported by numerical experiments using both real-life and synthetic data. Our work provides a foundational understanding that enhances data augmentation techniques beyond imbalanced data scenarios.

</details>


### [172] [DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems](https://arxiv.org/abs/2601.01931)
*Willem Röpke,Samuel Coward,Andrei Lupu,Thomas Foster,Tim Rocktäschel,Jakob Foerster*

Main category: cs.LG

TL;DR: 本文提出DéjàQ框架，通过联合进化合成数学问题与模型训练，使用LLM驱动的变异策略，提高强化学习训练效果，强调动态训练数据潜力并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型多依赖静态数据集，易鼓励记忆且限制泛化能力。

Method: 引入DéjàQ框架，在模型训练同时进化合成数学问题，提出两种LLM驱动的数据变异策略。

Result: 模型能生成新而有意义的问题，LLM驱动变异改善了强化学习训练，分析了DéjàQ的关键方面。

Conclusion: 动态进化的训练数据有增强数学推理的潜力，有更广泛适用性。

Abstract: Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.

</details>


### [173] [Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior](https://arxiv.org/abs/2601.01966)
*Bo Yin,Qi Li,Runpeng Yu,Xinchao Wang*

Main category: cs.LG

TL;DR: 文章提出Refinement Provenance Inference (RPI)审计任务，设计RePro框架解决指令调优中训练提示来源推断问题，表现良好且有跨精炼器的迁移性。


<details>
  <summary>Details</summary>
Motivation: 在指令调优依赖大语言模型提示精炼的背景下，解决在混合语料中推断模型训练使用的是原始提示还是精炼版本的问题，这对数据集治理和争议解决有重要意义。

Method: 将审计任务形式化为RPI，利用提示精炼在教师强迫标记分布中的稳定可检测变化，提出基于对数的RePro框架，融合教师强迫似然特征和对数排序信号，通过影子微调学习可迁移表示，用轻量级线性头推断来源。

Result: RePro在实验中始终表现出色，且在不同精炼器之间有良好的迁移性。

Conclusion: RePro利用了与精炼器无关的分布变化，而非重写风格的人工痕迹，能有效解决提示来源推断问题。

Abstract: Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.

</details>


### [174] [Output Embedding Centering for Stable LLM Pretraining](https://arxiv.org/abs/2601.02031)
*Felix Stollenwerk,Anna Lokrantz,Niclas Hertzberg*

Main category: cs.LG

TL;DR: 论文分析大语言模型预训练输出对数几率散度不稳定问题，提出输出嵌入居中（OEC）策略，实验表明其优于z - loss。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练成本高且存在训练不稳定性，z - loss仅治标不治本，需找到解决输出对数几率散度问题的更好方法。

Method: 从输出嵌入几何角度分析不稳定原因，提出OEC策略，有μ - centering和μ - loss两种实现方式。

Result: 两种OEC变体在训练稳定性和学习率敏感性上优于z - loss，大学习率时z - loss失效而OEC能保证收敛，μ - loss对正则化超参数调整的敏感性远低于z - loss。

Conclusion: OEC策略有效抑制输出对数几率散度，是比z - loss更好的解决训练不稳定问题的方法。

Abstract: Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.

</details>


### [175] [The Homogeneity Trap: Spectral Collapse in Doubly-Stochastic Deep Networks](https://arxiv.org/abs/2601.02080)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 指出双随机矩阵（DSM）约束下存在同质性陷阱，揭示高熵约束与特征变换等问题，及层归一化在噪声主导下的失效，点明熵稳定性和谱表达性的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究双随机矩阵在结构保留深度架构应用中存在的问题，识别潜在现象。

Method: 分析Sinkhorn投影的最大熵偏差，推导奇异值σ_2与网络有效深度的谱界，进行理论证明。

Result: 发现同质性陷阱，表明高熵约束限制特征变换，层归一化在噪声主导下无法缓解崩溃，损失几何结构。

Conclusion: DSM约束网络存在熵稳定性和谱表达性的根本权衡。

Abstract: Doubly-stochastic matrices (DSM) are increasingly utilized in structure-preserving deep architectures -- such as Optimal Transport layers and Sinkhorn-based attention -- to enforce numerical stability and probabilistic interpretability. In this work, we identify a critical spectral degradation phenomenon inherent to these constraints, termed the Homogeneity Trap. We demonstrate that the maximum-entropy bias, typical of Sinkhorn-based projections, drives the mixing operator towards the uniform barycenter, thereby suppressing the subdominant singular value σ_2 and filtering out high-frequency feature components. We derive a spectral bound linking σ_2 to the network's effective depth, showing that high-entropy constraints restrict feature transformation to a shallow effective receptive field. Furthermore, we formally demonstrate that Layer Normalization fails to mitigate this collapse in noise-dominated regimes; specifically, when spectral filtering degrades the Signal-to-Noise Ratio (SNR) below a critical threshold, geometric structure is irreversibly lost to noise-induced orthogonal collapse. Our findings highlight a fundamental trade-off between entropic stability and spectral expressivity in DSM-constrained networks.

</details>


### [176] [LION-DG: Layer-Informed Initialization with Deep Gradient Protocols for Accelerated Neural Network Training](https://arxiv.org/abs/2601.02105)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 提出LION - DG初始化方法用于深度监督架构，实验证明其能加速收敛、提升准确率，且无超参数和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有权重初始化方法大多与层无关，深度监督架构中未训练的辅助头会因梯度干扰使早期训练不稳定。

Method: 提出LION - DG，对辅助分类器头零初始化，对骨干网络采用标准He初始化。

Result: 在CIFAR - 10和CIFAR - 100上实验，DenseNet - DS在CIFAR - 10上收敛快8.3%；LSUV与LION - DG结合在CIFAR - 10上准确率达81.92%；ResNet - DS在CIFAR - 100上加速11.3%。

Conclusion: 确定了特定架构的权衡，为从业者提供指导，LION - DG简单、无超参数、无计算开销。

Abstract: Weight initialization remains decisive for neural network optimization, yet existing methods are largely layer-agnostic. We study initialization for deeply-supervised architectures with auxiliary classifiers, where untrained auxiliary heads can destabilize early training through gradient interference.
  We propose LION-DG, a layer-informed initialization that zero-initializes auxiliary classifier heads while applying standard He-initialization to the backbone. We prove that this implements Gradient Awakening: auxiliary gradients are exactly zero at initialization, then phase in naturally as weights grow -- providing an implicit warmup without hyperparameters.
  Experiments on CIFAR-10 and CIFAR-100 with DenseNet-DS and ResNet-DS architectures demonstrate: (1) DenseNet-DS: +8.3% faster convergence on CIFAR-10 with comparable accuracy, (2) Hybrid approach: Combining LSUV with LION-DG achieves best accuracy (81.92% on CIFAR-10), (3) ResNet-DS: Positive speedup on CIFAR-100 (+11.3%) with side-tap auxiliary design.
  We identify architecture-specific trade-offs and provide clear guidelines for practitioners. LION-DG is simple, requires zero hyperparameters, and adds no computational overhead.

</details>


### [177] [Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting](https://arxiv.org/abs/2601.02151)
*Muxi Diao,Lele Yang,Wuxuan Gong,Yutong Zhang,Zhonghao Yan,Yufei Han,Kongming Liang,Weiran Xu,Zhanyu Ma*

Main category: cs.LG

TL;DR: 本文指出监督微调（SFT）导致灾难性遗忘，而策略强化学习（RL）能保留通用能力，剖析分布差异后提出熵自适应微调（EAFT）方法，实验证明其有效。


<details>
  <summary>Details</summary>
Motivation: 解决监督微调（SFT）在领域适应时经常产生灾难性遗忘的问题，对比其与策略强化学习在保留通用能力上的差异。

Method: 提出熵自适应微调（EAFT）方法，利用基于token级别的熵作为门控机制，区分认知不确定性和知识冲突。

Result: 在Qwen和GLM系列模型上的实验表明，EAFT在下游任务表现与标准SFT相当，且显著减轻通用能力的退化。

Conclusion: EAFT能有效解决SFT在领域适应时的灾难性遗忘问题，在达到下游任务性能的同时，保留模型的通用能力。

Abstract: Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as "Confident Conflicts" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.

</details>


### [178] [DatBench: Discriminative, Faithful, and Efficient VLM Evaluations](https://arxiv.org/abs/2601.02316)
*Siddharth Joshi,Haoli Yin,Rishabh Adiga,Ricardo Monti,Aldo Carranza,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Fan Pan,Haakon Mongstad,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Kaleigh Mentzer,Luke Merrick,Parth Doshi,Paul Burstein,Pratyush Maini,Scott Loftin,Spandan Das,Tony Jiang,Vineeth Dorna,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: 本文指出视觉语言模型（VLMs）评估方法尚不成熟，提出评估应满足的三个要求，识别评估中的关键失败模式，通过整理现有基准解决问题，发布评估套件并指出VLMs评估实践的方向。


<details>
  <summary>Details</summary>
Motivation: 目前训练前沿视觉语言模型的工作多，但评估方法不成熟，需要引导其成熟。

Method: 提出评估应满足的三个要求，识别关键失败模式，通过转换和过滤整理现有基准。

Result: 将选择题转换为生成任务可揭示能力下降达35%，过滤样本可提高区分能力并降低计算成本，发布DatBench - Full和DatBench评估套件。

Conclusion: 为VLMs持续扩展时的严格且可持续的评估实践指明了道路。

Abstract: Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [179] [Implementation of high-efficiency, lightweight residual spiking neural network processor based on field-programmable gate arrays](https://arxiv.org/abs/2601.00802)
*Hou Yue,Xiang Shuiying,Zou Tao,Huang Zhiquan,Shi Shangxuan,Guo Xingxing,Zhang Yahui,Zheng Ling,Hao Yue*

Main category: cs.NE

TL;DR: 本文提出高效轻量级残差SNN加速器，结合算法与硬件协同设计优化推理能效，实验显示在CIFAR - 10数据集表现良好，能效优于GPU和其他SNN处理器。


<details>
  <summary>Details</summary>
Motivation: 现有基于FPGA的SNN处理器方法依赖多时间步训练和可重构计算架构，增加计算和内存开销，降低部署效率。

Method: 算法上采用单时间步训练、集成分组卷积、融合BN层压缩网络，用QAT约束参数精度；硬件上重用层内资源、采用全流水线跨层架构、用片上BRAM存储数据。

Result: 在CIFAR - 10数据集分类准确率87.11%，单张图像推理时间3.98ms，能效183.5 FPS/W，能效超GPU两倍，推理速度至少快4倍，能效至少高5倍。

Conclusion: 所提出的SNN加速器在能效和推理速度上优于主流GPU平台和其他SNN处理器。

Abstract: With the development of hardware-optimized deployment of spiking neural networks (SNNs), SNN processors based on field-programmable gate arrays (FPGAs) have become a research hotspot due to their efficiency and flexibility. However, existing methods rely on multi-timestep training and reconfigurable computing architectures, which increases computational and memory overhead, thus reducing deployment efficiency. This work presents an efficient and lightweight residual SNN accelerator that combines algorithm and hardware co-design to optimize inference energy efficiency. In terms of the algorithm, we employ single-timesteps training, integrate grouped convolutions, and fuse batch normalization (BN) layers, thus compressing the network to only 0.69M parameters. Quantization-aware training (QAT) further constrains all parameters to 8-bit precision. In terms of hardware, the reuse of intra-layer resources maximizes FPGA utilization, a full pipeline cross-layer architecture improves throughput, and on-chip block RAM (BRAM) stores network parameters and intermediate results to improve memory efficiency. The experimental results show that the proposed processor achieves a classification accuracy of 87.11% on the CIFAR-10 dataset, with an inference time of 3.98 ms per image and an energy efficiency of 183.5 FPS/W. Compared with mainstream graphics processing unit (GPU) platforms, it achieves more than double the energy efficiency. Furthermore, compared with other SNN processors, it achieves at least a 4x faster inference speed and a 5x higher energy efficiency.

</details>


### [180] [Optimal Traffic Relief Road Design using Bilevel Programming and Greedy Seeded Simulated Annealing: A Case Study of Kinshasa](https://arxiv.org/abs/2601.00804)
*Yves Matanga,Chunling Du,Etienne van Wyk*

Main category: cs.NE

TL;DR: 针对金沙萨市交通拥堵问题，提出基于交通流的算法确定道路建设优先级，采用特定方法求解，得出优先道路段。


<details>
  <summary>Details</summary>
Motivation: 金沙萨市交通拥堵，综合总体规划实施缺乏资金，需有效分配有限基础设施预算，确定道路建设优先级。

Method: 构建标准交通网络设计问题（TNDP），结合金沙萨市特定起点需求数据，选择性混合使用TNDP相关元启发式算法（GA、ACO、PSO、SA、TS、Greedy），设计贪婪搜索种子模拟退火和禁忌搜索。

Result: 贪婪模拟退火和贪婪禁忌搜索在减少出行时间和稳定性上表现最佳，使网络边缘介数中心性提高近2.5倍。

Conclusion: 提出道路优先级，包括连接特定入口点到主要景点中心及市内区域的路口。

Abstract: Context: The city of Kinshasa faces severe traffic congestion, requiring strategic infrastructure capacity enhancements. Although a comprehensive master plan has been proposed, its implementation requires substantial financial investment, which remains constrained in the Democratic Republic of the Congo (DRC), an emerging economy. This research proposes a traffic flow based algorithm to support the development of priority road segments. The objective is to enable more effective prioritisation of road construction projects and facilitate the optimal allocation of limited infrastructure budgets.
  Methods: The study was conducted by formulating a standard transport network design problem (TNDP) that included estimated origin demand data specific to the city of Kinshasa. Given the high computational nature of the 30 node network design, TNDP relevant metaheuristics (GA, ACO, PSO, SA, TS, Greedy) were used selectively and hybridised to achieve high quality, stable solutions. A greedy search seeded simulated annealing and Tabu search were devised to achieve the design goals.
  Results: Greedy Simulated Annealing and Greedy Tabu search yielded the best travel time reduction and the most stable solutions compared to other solvers, also improving network edge betweenness centrality by nearly a scale of two and a half.
  Conclusions: Road priorities were proposed, including junctions connecting the Bandundu and Kongo Central entry point to main attraction centres (Limete Poids Lourd, Gombe, Airport) and additional inner city areas (Ngaliema, Selembao, Lemba, Masina, Kimwenza).

</details>


### [181] [ChronoPlastic Spiking Neural Networks](https://arxiv.org/abs/2601.00805)
*Sarim Chaudhry*

Main category: cs.NE

TL;DR: 本文提出ChronoPlastic Spiking Neural Networks（CPSNNs）解决SNNs长时依赖问题，其学习长时依赖更快更可靠，说明自适应时间调制对脉冲系统时间学习很关键。


<details>
  <summary>Details</summary>
Motivation: Spiking neural networks（SNNs）存在长时依赖问题，因突触和膜时间常数固定难以处理。

Method: 提出CPSNNs架构，动态调节突触衰减率以实现自适应时间信用分配，维护多个内部时间轨迹并学习连续时间扭曲函数，将时间控制嵌入局部突触动力学。

Result: CPSNNs学习长时依赖比标准SNN基线更快且更可靠。

Conclusion: 自适应时间调制是脉冲系统可扩展时间学习的关键因素。

Abstract: Spiking neural networks (SNNs) offer a biologically grounded and energy-efficient alternative to conventional neural architectures; however, they struggle with long-range temporal dependencies due to fixed synaptic and membrane time constants. This paper introduces ChronoPlastic Spiking Neural Networks (CPSNNs), a novel architectural principle that enables adaptive temporal credit assignment by dynamically modulating synaptic decay rates conditioned on the state of the network. CPSNNs maintain multiple internal temporal traces and learn a continuous time-warping function that selectively preserves task-relevant information while rapidly forgetting noise. Unlike prior approaches based on adaptive membrane constants, attention mechanisms, or external memory, CPSNNs embed temporal control directly within local synaptic dynamics, preserving linear-time complexity and neuromorphic compatibility. We provide a formal description of the model, analyze its computational properties, and demonstrate empirically that CPSNNs learn long-gap temporal dependencies significantly faster and more reliably than standard SNN baselines. Our results suggest that adaptive temporal modulation is a key missing ingredient for scalable temporal learning in spiking systems.

</details>


### [182] [Energy-Efficient Eimeria Parasite Detection Using a Two-Stage Spiking Neural Network Architecture](https://arxiv.org/abs/2601.00806)
*Ángel Miguel García-Vico,Huseyin Seker,Muhammad Afzal*

Main category: cs.NE

TL;DR: 本文提出一种新型两阶段脉冲神经网络架构用于艾美球虫分类，实现高精度且能耗大幅降低。


<details>
  <summary>Details</summary>
Motivation: 球虫病对家禽和兔子行业构成威胁，需要快速准确诊断工具，现有深度学习模型能耗高，难以在资源受限环境部署。

Method: 引入新型两阶段脉冲神经网络架构，先将预训练卷积神经网络转换为脉冲特征提取器，再与基于尖峰时间依赖可塑性训练的轻量级无监督脉冲神经网络分类器结合。

Result: 艾美球虫分类准确率达98.32%，能耗相比传统人工神经网络降低超223倍。

Conclusion: 该模型实现了高精度与极端能源效率的强大协同，为神经形态硬件上的自主低功耗诊断系统铺平了道路。

Abstract: Coccidiosis, a disease caused by the Eimeria parasite, represents a major threat to the poultry and rabbit industries, demanding rapid and accurate diagnostic tools. While deep learning models offer high precision, their significant energy consumption limits their deployment in resource-constrained environments. This paper introduces a novel two-stage Spiking Neural Network (SNN) architecture, where a pre-trained Convolutional Neural Network is first converted into a spiking feature extractor and then coupled with a lightweight, unsupervised SNN classifier trained with Spike-Timing-Dependent Plasticity (STDP). The proposed model sets a new state-of-the-art, achieving 98.32\% accuracy in Eimeria classification. Remarkably, this performance is accomplished with a significant reduction in energy consumption, showing an improvement of more than 223 times compared to its traditional ANN counterpart. This work demonstrates a powerful synergy between high accuracy and extreme energy efficiency, paving the way for autonomous, low-power diagnostic systems on neuromorphic hardware.

</details>


### [183] [Evolutionary optimization of spatially-distributed multi-sensors placement for indoor surveillance environments with security levels](https://arxiv.org/abs/2601.00826)
*Luis M. Moreno-Saavedra,Vinıcius G. Costa,Adrian Garrido-Saez,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,Sancho Salcedo-Sanz*

Main category: cs.NE

TL;DR: 本文研究室内安全监控的多传感器空间分布放置优化问题，提出用进化算法解决，效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决室内监控中多传感器空间分布放置问题，提升敏感室内空间监控安全性。

Method: 提出进化算法，定义了整数编码加二进制转换的新型编码和有效初始化，考虑监控点检测概率。

Result: 在不同规模和难度的问题实例上测试算法，在传感器放置成本和收敛时间方面取得优异结果。

Conclusion: 所提进化算法能有效解决室内多传感器空间分布放置优化问题。

Abstract: The surveillance multisensor placement is an important optimization problem that consists of positioning several sensors of different types to maximize the coverage of a determined area while minimizing the cost of the deployment. In this work, we tackle a modified version of the problem, consisting of spatially distributed multisensor placement for indoor surveillance. Our approach is focused on security surveillance of sensible indoor spaces, such as military installations, where distinct security levels can be considered. We propose an evolutionary algorithm to solve the problem, in which a novel special encoding,integer encoding with binary conversion, and effective initialization have been defined to improve the performance and convergence of the proposed algorithm. We also consider the probability of detection for each surveillance point, which depends on the distance to the sensor at hand, to better model real-life scenarios. We have tested the proposed evolutionary approach in different instances of the problem, varying both size and difficulty, and obtained excellent results in terms of the cost of sensors placement and convergence time of the algorithm.

</details>


### [184] [Benchmarking Continuous Dynamic Multi-Objective Optimization: Survey and Generalized Test Suite](https://arxiv.org/abs/2601.01317)
*Chang Shao,Qi Zhao,Nana Pu,Shi Cheng,Jing Jiang,Yuhui Shi*

Main category: cs.NE

TL;DR: 本文提出构建动态多目标优化（DMOO）基准的框架，经实验验证有效，为DMOO基准设定新标准。


<details>
  <summary>Details</summary>
Motivation: 随着DMOO受关注，需先进基准评估优化算法，现有基准可能无法满足现实条件下的评估需求。

Method: 提出综合框架，包含允许Pareto最优集在超曲面上变化的广义公式、创建变量贡献不平衡机制、动态旋转矩阵、时间扰动机制和广义时间链接机制。

Result: 实验结果验证了框架的有效性，在现实性、复杂性和区分算法性能方面优于传统基准。

Conclusion: 该工作为DMOO基准设定了新标准，为下一代算法的开发和评估提供有力工具。

Abstract: Dynamic multi-objective optimization (DMOO) has recently attracted increasing interest from both academic researchers and engineering practitioners, as numerous real-world applications that evolve over time can be naturally formulated as dynamic multi-objective optimization problems (DMOPs). This growing trend necessitates advanced benchmarks for the rigorous evaluation of optimization algorithms under realistic conditions. This paper introduces a comprehensive and principled framework for constructing highly realistic and challenging DMOO benchmarks. The proposed framework features several novel components: a generalized formulation that allows the Pareto-optimal Set (PS) to change on hypersurfaces, a mechanism for creating controlled variable contribution imbalances to generate heterogeneous landscapes, and dynamic rotation matrices for inducing time-varying variable interactions and non-separability. Furthermore, we incorporate a temporal perturbation mechanism to simulate irregular environmental changes and propose a generalized time-linkage mechanism that systematically embeds historical solution quality into future problems, thereby capturing critical real-world phenomena such as error accumulation and time-deception. Extensive experimental results validate the effectiveness of the proposed framework, demonstrating its superiority over conventional benchmarks in terms of realism, complexity, and its capability for discriminating state-of-the-art algorithmic performance. This work establishes a new standard for dynamic multi-objective optimization benchmarking, providing a powerful tool for the development and evaluation of next-generation algorithms capable of addressing the complexities of real-world dynamic systems.

</details>


### [185] [STEMNIST: Spiking Tactile Extended MNIST Neuromorphic Dataset](https://arxiv.org/abs/2601.01658)
*Anubhab Tripathi,Li Gaishan,Zhengnan Fu,Chiara Bartolozzi,Bert E. Shi,Arindam Basu*

Main category: cs.NE

TL;DR: 引入大规模神经形态触觉数据集STEMNIST，可用于触觉识别系统评估和相关研究。


<details>
  <summary>Details</summary>
Motivation: 当前神经形态触觉数据集较视觉数据集有限，需填补简化数字分类与现实触觉交互场景间的差距。

Method: 将ST - MNIST从10个数字扩展到35个字母数字类别，使用定制触觉传感器阵列收集样本并通过自适应时间差分编码为尖峰事件。

Result: 使用传统CNN测试准确率达90.91%，脉冲神经网络为89.16%，建立了性能基准。

Conclusion: STEMNIST可实现触觉识别系统的可重复评估，为神经形态感知发展提供基础，相关数据和代码公开促进研究。

Abstract: Tactile sensing is essential for robotic manipulation, prosthetics and assistive technologies, yet neuromorphic tactile datasets remain limited compared to their visual counterparts. We introduce STEMNIST, a large-scale neuromorphic tactile dataset extending ST-MNIST from 10 digits to 35 alphanumeric classes (uppercase letters A--Z and digits 1--9), providing a challenging benchmark for event-based haptic recognition. The dataset comprises 7,700 samples collected from 34 participants using a custom \(16\times 16\) tactile sensor array operating at 120 Hz, encoded as 1,005,592 spike events through adaptive temporal differentiation. Following EMNIST's visual character recognition protocol, STEMNIST addresses the critical gap between simplified digit classification and real-world tactile interaction scenarios requiring alphanumeric discrimination. Baseline experiments using conventional CNNs (90.91% test accuracy) and spiking neural networks (89.16%) establish performance benchmarks. The dataset's event-based format, unrestricted spatial variability and rich temporal structure makes it suitable for testing neuromorphic hardware and bio-inspired learning algorithms. STEMNIST enables reproducible evaluation of tactile recognition systems and provides a foundation for advancing energy-efficient neuromorphic perception in robotics, biomedical engineering and human-machine interfaces. The dataset, documentation and codes are publicly available to accelerate research in neuromorphic tactile computing.

</details>


### [186] [Yukthi Opus: A Multi-Chain Hybrid Metaheuristic for Large-Scale NP-Hard Optimization](https://arxiv.org/abs/2601.01832)
*SB Danush Vikraman,Hannah Abagail,Prasanna Kesavraj,Gajanan V Honnavar*

Main category: cs.NE

TL;DR: 提出Yukthi Opus (YO)多链混合元启发式算法应对NP难优化问题，通过实验验证其性能，适用于昂贵黑盒优化场景。


<details>
  <summary>Details</summary>
Motivation: 解决明确评估预算约束下的NP难优化问题。

Method: 采用结构化两阶段架构，融合MCMC全局探索、贪婪局部搜索、带自适应再加热的模拟退火三种机制；设置专门预热阶段和混合优化循环；纳入空间黑名单机制和多链执行策略。

Result: 对三个基准测试问题进行评估，结果表明MCMC探索和贪婪局部搜索对解的质量至关重要，模拟退火和多链执行主要提升稳定性和减少方差。

Conclusion: YO在大规模和多模态问题上表现有竞争力，能保持可预测评估预算，适用于昂贵黑盒优化场景。

Abstract: We present Yukthi Opus (YO), a multi-chain hybrid metaheuristic designed for NP-hard optimization under explicit evaluation budget constraints. YO integrates three complementary mechanisms in a structured two-phase architecture: Markov Chain Monte Carlo (MCMC) for global exploration, greedy local search for exploitation, and simulated annealing with adaptive reheating to enable controlled escape from local minima. A dedicated burn-in phase allocates evaluations to probabilistic exploration, after which a hybrid optimization loop refines promising candidates. YO further incorporates a spatial blacklist mechanism to avoid repeated evaluation of poor regions and a multi-chain execution strategy to improve robustness and reduce sensitivity to initialization.
  We evaluate YO on three benchmarks: the Rastrigin function (5D) with ablation studies, the Traveling Salesman Problem with 50 to 200 cities, and the Rosenbrock function (5D) with comparisons against established optimizers including CMA-ES, Bayesian optimization, and accelerated particle swarm optimization. Results show that MCMC exploration and greedy refinement are critical for solution quality, while simulated annealing and multi-chain execution primarily improve stability and variance reduction. Overall, YO achieves competitive performance on large and multimodal problems while maintaining predictable evaluation budgets, making it suitable for expensive black-box optimization settings.

</details>


### [187] [Multi-strategy Improved Northern Goshawk Optimization for WSN Coverage Enhancement](https://arxiv.org/abs/2601.01898)
*Yiran Tian,Yuanjia Liu*

Main category: cs.NE

TL;DR: 提出基于多策略集成北方苍鹰优化算法提升无线传感器网络覆盖率，仿真显示性能优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 增强无线传感器网络的覆盖率。

Method: 采用多元混沌映射改善初始种群随机性和均匀性，在追逃阶段后引入双向种群进化动力学策略防止算法陷入局部最优。

Result: 在无线传感器网络覆盖仿真中，所提算法在覆盖增强和节点连接性方面显著优于现有基准。

Conclusion: 所提多策略北方苍鹰优化算法能有效提升无线传感器网络的覆盖率和节点连接性。

Abstract: To enhance the coverage rate of Wireless Sensor Networks (WSNs), this paper proposes an advanced optimization strategy based on a multi-strategy integrated Northern Goshawk Optimization (NGO) algorithm. Specifically, multivariate chaotic mapping is first employed to improve the randomness and uniformity of the initial population. To further bolster population diversity and prevent the algorithm from stagnating in local optima, a bidirectional population evolutionary dynamics strategy is incorporated following the pursuit-and-evasion phase, thereby facilitating the attainment of the global optimal solution. Extensive simulations were conducted to evaluate the performance of the proposed multi-strategy NGO in WSN coverage. Experimental results demonstrate that the proposed algorithm significantly outperforms existing benchmarks in terms of both coverage enhancement and node connectivity.

</details>


### [188] [Toward Thermodynamic Reservoir Computing: Exploring SHA-256 ASICs as Potential Physical Substrates](https://arxiv.org/abs/2601.01916)
*Francisco Angulo de Lafuente,Vladimir Veselov,Richard Goodman*

Main category: cs.NE

TL;DR: 提出全息储层计算框架HRC，假设比特币矿机ASIC可作物理储层计算底物，给出CHIMERA系统架构，有理论投影待实验验证。


<details>
  <summary>Details</summary>
Motivation: 为热力学计算领域提出新方法，将过时加密硬件用于神经形态应用。

Method: 提出HRC框架和CHIMERA系统架构，基于HNS表示进行理论分析。

Result: 观察到非泊松变异性，理论分析显示有O(log n)能量缩放优势。

Conclusion: 工作为热力学计算领域提出新途径，但理论需实验验证。

Abstract: We propose a theoretical framework--Holographic Reservoir Computing (HRC)--which hypothesizes that the thermodynamic noise and timing dynamics in voltage-stressed Bitcoin mining ASICs (BM1366) could potentially serve as a physical reservoir computing substrate. We present the CHIMERA (Conscious Hybrid Intelligence via Miner-Embedded Resonance Architecture) system architecture, which treats the SHA-256 hashing pipeline not as an entropy source, but as a deterministic diffusion operator whose timing characteristics under controlled voltage and frequency conditions may exhibit computationally useful dynamics. We report preliminary observations of non-Poissonian variability in inter-arrival time statistics during edge-of-stability operation, which we term the "Silicon Heartbeat" hypothesis. Theoretical analysis based on Hierarchical Number System (HNS) representations suggests that such architectures could achieve O(log n) energy scaling compared to traditional von Neumann O(2^n) dependencies. However, we emphasize that these are theoretical projections requiring experimental validation. We present the implemented measurement infrastructure, acknowledge current limitations, and outline the experimental program necessary to confirm or refute these hypotheses. This work contributes to the emerging field of thermodynamic computing by proposing a novel approach to repurposing obsolete cryptographic hardware for neuromorphic applications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [189] [SeRe: A Security-Related Code Review Dataset Aligned with Real-World Review Activities](https://arxiv.org/abs/2601.01042)
*Zixiao Zhao,Yanjie Jiang,Hui Liu,Kui Liu,Lu Zhang*

Main category: cs.SE

TL;DR: 提出安全相关代码审查数据集SeRe，并对现有代码审查注释生成方法进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏安全特定注释或规模有限，难以支持大规模安全相关代码审查研究。

Method: 使用基于主动学习的集成分类方法构建SeRe数据集，迭代优化模型预测。

Result: 从373,824个原始审查实例中提取6,732个安全相关审查，SeRe与现实世界安全相关审查分布相符。

Conclusion: 发布SeRe数据集和基准测试结果，推动自动化安全聚焦代码审查研究和安全软件工程实践发展。

Abstract: Software security vulnerabilities can lead to severe consequences, making early detection essential. Although code review serves as a critical defense mechanism against security flaws, relevant feedback remains scarce due to limited attention to security issues or a lack of expertise among reviewers. Existing datasets and studies primarily focus on general-purpose code review comments, either lacking security-specific annotations or being too limited in scale to support large-scale research. To bridge this gap, we introduce \textbf{SeRe}, a \textbf{security-related code review dataset}, constructed using an active learning-based ensemble classification approach. The proposed approach iteratively refines model predictions through human annotations, achieving high precision while maintaining reasonable recall. Using the fine-tuned ensemble classifier, we extracted 6,732 security-related reviews from 373,824 raw review instances, ensuring representativeness across multiple programming languages. Statistical analysis indicates that SeRe generally \textbf{aligns with real-world security-related review distribution}. To assess both the utility of SeRe and the effectiveness of existing code review comment generation approaches, we benchmark state-of-the-art approaches on security-related feedback generation. By releasing SeRe along with our benchmark results, we aim to advance research in automated security-focused code review and contribute to the development of more effective secure software engineering practices.

</details>


### [190] [RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian](https://arxiv.org/abs/2601.01129)
*Kla Tantithamthavorn,Yaotian Zou,Andy Wong,Michael Gupta,Zhe Wang,Mike Buller,Ryan Jiang,Matthew Watson,Minwoo Jeong,Kun Chen,Ming Wu*

Main category: cs.SE

TL;DR: 本文介绍企业级基于大语言模型的代码审查自动化工具RovoDev Code Reviewer，评估显示其在生成有效审查评论、加速反馈周期等方面表现良好


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的代码审查评论生成方法存在设计企业级代码审查自动化工具的实际挑战，需要解决如何在不微调的情况下设计审查引导、上下文感知、质量检查的代码审查评论生成问题

Method: 设计并大规模部署RovoDev Code Reviewer到Atlassian开发生态系统和Bitbucket中，进行为期一年的离线、在线、用户反馈评估

Result: 该工具能使38.70%的代码审查评论促成代码修改，减少30.8%的拉取请求周期时间，减少35.6%的人工编写评论数量并能发现错误给出可行建议

Conclusion: RovoDev Code Reviewer有效且有加速反馈周期、减轻审查者工作量和提高软件质量的潜力

Abstract: Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning?
  In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).

</details>


### [191] [Abductive Vibe Coding (Extended Abstract)](https://arxiv.org/abs/2601.01199)
*Logan Murphy,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: 该摘要介绍了针对AI生成软件工件提取可分析半形式化依据的工作，框架生成代码充足性条件，还提及当前工作和研究机会。


<details>
  <summary>Details</summary>
Motivation: 实际场景中难以通过创建正确性形式证明来验证AI生成的软件工件。

Method: 开发框架提取可分析的、半形式化的理由，生成生成代码可被认为充足的条件集。

Result: 未提及具体研究结果。

Conclusion: 未提及明确结论，但指出了当前实施框架的努力方向和预期的研究机会。

Abstract: When software artifacts are generated by AI models ("vibe coding"), human engineers assume responsibility for validating them. Ideally, this validation would be done through the creation of a formal proof of correctness. However, this is infeasible for many real-world vibe coding scenarios, especially when requirements for the AI-generated artifacts resist formalization. This extended abstract describes ongoing work towards the extraction of analyzable, semi-formal rationales for the adequacy of vibe-coded artifacts. Rather than deciding correctness directly, our framework produces a set of conditions under which the generated code can be considered adequate. We describe current efforts towards implementing our framework and anticipated research opportunities.

</details>


### [192] [Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code](https://arxiv.org/abs/2601.01215)
*Prateek Rajput,Yewei Song,Abdoul Aziz Bonkoungou,Iyiola E. Olatunji,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 论文提出衡量大语言模型生成程序执行时内存稳定性的框架，实验显示正确解间有运行时差异，支持在CI/CD中基于稳定性选择以降低风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的程序通过单元测试不保证可靠运行时行为，不同正确解有不同内存和性能模式，存在隐藏操作风险。

Method: 提出衡量执行时内存稳定性的框架，在解决方案层面引入DMPD，聚合得到模型层面的MIS。

Result: 在BigOBench和CodeContests上实验显示正确解间有显著运行时差异，采样温度升高时不稳定性常增加，稳定性指标与软件工程指标有相关性。

Conclusion: 支持在CI/CD中对通过测试的候选方案进行稳定性感知选择，在不牺牲正确性的前提下降低操作风险。

Abstract: Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational risks. We present a framework to measure execution-time memory stability across multiple correct generations. At the solution level, we introduce Dynamic Mean Pairwise Distance (DMPD), which uses Dynamic Time Warping to compare the shapes of memory-usage traces after converting them into Monotonic Peak Profiles (MPPs) to reduce transient noise. Aggregating DMPD across tasks yields a model-level Model Instability Score (MIS). Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions. Instability often increases with higher sampling temperature even when pass@1 improves. We also observe correlations between our stability measures and software engineering indicators such as cognitive and cyclomatic complexity, suggesting links between operational behavior and maintainability. Our results support stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness. Artifacts are available.

</details>


### [193] [HD-GEN: A High-Performance Software System for Human Mobility Data Generation Based on Patterns of Life](https://arxiv.org/abs/2601.01219)
*Hossein Amiri,Joon-Seok Kim,Hamdi Kavak,Andrew Crooks,Dieter Pfoser,Carola Wenk,Andreas Züfle*

Main category: cs.SE

TL;DR: 本文介绍综合软件管道来生成结合真实数据与模拟可扩展性的人类移动数据集。


<details>
  <summary>Details</summary>
Motivation: 现实轨迹数据有稀疏性和偏差问题，合成数据缺乏真实性，为填补此差距展开研究。

Method: 构建包含数据生成引擎、校准模块、数据处理套件和可视化模块的软件管道系统。

Result: 构建出能结合现实数据与模式模拟的综合软件管道，可生成不同类型移动日志。

Conclusion: 提出的软件系统有助于生成大规模个体级人类移动数据集。

Abstract: Understanding individual-level human mobility is critical for a wide range of applications. Real-world trajectory datasets provide valuable insights into actual movement behaviors but are often constrained by data sparsity and participant bias. Synthetic data, by contrast, offer scalability and flexibility but frequently lack realism. To address this gap, we introduce a comprehensive software pipeline for calibrating, generating, processing, and visualizing large-scale individual-level human mobility datasets that combine the realism of empirical data with the control and extensibility of Patterns-of-Life simulations. Our system consists of four integrated components. (1) a data generation engine constructs geographically grounded simulations using OpenStreetMap data to produce diverse mobility logs. (2) a genetic algorithm-based calibration module fine-tunes simulation parameters to align with real-world mobility characteristics, such as daily trip counts and radius of gyration, enabling realistic behavioral modeling. (3) a data processing suite transforms raw simulation logs into structured formats suitable for downstream applications, including model training and benchmarking. (4) a visualization module extracts key mobility patterns and insights from the processed datasets and presents them through intuitive visual analytics for improved interpretability.

</details>


### [194] [Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling](https://arxiv.org/abs/2601.01233)
*Kangchen Zhu,Zhiliang Tian,Shangwen Wang,Mingyue Leng,Xiaoguang Mao*

Main category: cs.SE

TL;DR: 现有自动化拆分复合提交方法有局限，本文提出Atomizer框架，用IO - CoT策略和双代理协作细化循环，实验显示其显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动化拆分复合提交方法过度依赖结构信息，缺乏语义理解且为单遍算法，无法像人类审查一样反思细化，需改进。

Method: 提出Atomizer框架，采用IO - CoT策略让大语言模型根据结构和语义信息推断代码更改意图，用两个代理建立分组 - 审查协作细化循环。

Result: 在C#和Java数据集上，Atomizer平均比现有图聚类方法分别高出6.0%和5.5%，在复杂提交上优势扩大到16%以上。

Conclusion: Atomizer能有效解决现有自动化拆分复合提交方法的局限，性能显著优于代表基线方法。

Abstract: Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.

</details>


### [195] [CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs](https://arxiv.org/abs/2601.01271)
*Qingxiao Tao,Xiaodong Gu,Hao Zhong,Beijun Shen*

Main category: cs.SE

TL;DR: 本文提出基于大语言模型（LLM）的CatchAll方法用于仓库级异常处理，构建新基准测试集评估，结果表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在仓库级异常处理上因复杂依赖和上下文约束而表现不佳，有必要改进。

Method: 为LLM配备三层异常处理知识（API级异常知识、仓库级执行上下文、跨仓库处理知识），并编码到结构化提示中引导LLM生成代码。构建RepoExEval和RepoExEval - Exec两个基准测试集进行评估。

Result: CatchAll在各项指标上表现优于现有基线，CodeBLEU得分0.31，意图预测准确率60.1%，Pass@1为29%。

Conclusion: CatchAll在实际的仓库级异常处理中有效。

Abstract: Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.

</details>


### [196] [Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python](https://arxiv.org/abs/2601.01320)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: 现有代码漏洞检测基准存在不足，本文提出ALPHA这个函数级Python基准来评估LLMs和SAST工具，评估发现LLMs在性能上超过SAST但模型预测一致性差异大，还给出未来工作方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常生成易受攻击的代码，现有的代码漏洞检测基准采用二进制分类，缺乏CWE级别特异性，无法为迭代修正系统提供可操作的反馈。

Method: 提出ALPHA基准，用分层感知、特定CWE的惩罚机制评估LLMs和SAST工具。

Result: 评估发现LLMs性能大幅超过SAST，但SAST检测时精度更高，模型预测一致性在8.26%-81.87%间差异巨大。

Conclusion: 给出了将ALPHA惩罚机制纳入监督微调的未来工作途径，待实证验证后或能实现有原则的层次感知漏洞检测。

Abstract: Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.

</details>


### [197] [GlycoPy: An Equation-Oriented and Object-Oriented Software for Hierarchical Modeling, Optimization, and Control in Python](https://arxiv.org/abs/2601.01413)
*Yingjie Ma,Jing Guo,Richard D. Braatz*

Main category: cs.SE

TL;DR: 介绍了用于过程建模、优化和非线性模型预测控制的软件框架GlycoPy，并通过案例验证其能力，该框架有望弥合先进算法与实际应用的差距。


<details>
  <summary>Details</summary>
Motivation: 现有过程工业中模型预测控制多采用线性模型，限制了性能和适用性，使用非线性模型存在缺乏分层模型开发工具和高效算法实现的问题。

Method: 引入方程导向、面向对象的Python软件框架GlycoPy，支持分层建模，包含参数估计、动态优化和NMPC等算法，允许用户自定义。

Result: 通过三个案例验证了GlycoPy的建模、优化和NMPC能力。

Conclusion: GlycoPy有潜力弥合先进NMPC算法与实际（生物）化学过程应用之间的差距。

Abstract: Most existing model predictive control (MPC) applications in process industries employ lin-ear models, although real-world (bio)chemical processes are typically nonlinear. The use of linear models limits the performance and applicability of MPC for processes that span a wide range of operating conditions. A challenge in employing nonlinear models in MPC for com-plex systems is the lack of tools that facilitate hierarchical model development, as well as lack of efficient implementations of the corresponding nonlinear MPC (NMPC) algorithms. As a step towards making NMPC more practical for hierarchical systems, we introduce Gly-coPy, an equation-oriented, object-oriented software framework for process modeling, opti-mization, and NMPC in Python. GlycoPy enables users to focus on writing equations for modeling while supporting hierarchical modeling. GlycoPy includes algorithms for parame-ter estimation, dynamic optimization, and NMPC, and allows users to customize the simula-tion, optimization, and control algorithms. Three case studies, ranging from a simple differ-ential algebraic equation system to a multiscale bioprocess model, validate the modeling, optimization, and NMPC capabilities of GlycoPy. GlycoPy has the potential to bridge the gap between advanced NMPC algorithms and their practical application in real-world (bio)chemical processes.

</details>


### [198] [SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving](https://arxiv.org/abs/2601.01426)
*Chaofan Tao,Jierun Chen,Yuxin Jiang,Kaiqi Kou,Shaowei Wang,Ruoyu Wang,Xiaohui Li,Sidi Yang,Yiming Du,Jianbo Dai,Zhiming Mao,Xinyu Wang,Lifeng Shang,Haoli Bai*

Main category: cs.SE

TL;DR: 提出SWE - Lego，一种用于软件工程问题解决的监督微调方法，通过数据集、微调程序和测试时间缩放提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索仅使用轻量级监督微调方法在软件工程任务上达到最优效果，而非依赖复杂训练范式。

Method: SWE - Lego包含SWE - Lego数据集、带误差掩码和基于难度的课程的微调程序，还在微调基础上评估并改进测试时间缩放。

Result: 仅前两个组件就让SWE - Lego模型在同规模开源模型中达最优，如SWE - Lego - Qwen3 - 8B达42.2%，SWE - Lego - Qwen3 - 32B达52.6%；使用测试时间缩放后，8B和32B模型分别从42.2%提高到49.6%、52.6%提高到58.8%。

Conclusion: SWE - Lego的轻量级监督微调方法在软件工程问题解决上能达到较好效果。

Abstract: We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.

</details>


### [199] [Group versus Individual Review Requests: Tradeoffs in Speed and Quality at Mozilla Firefox](https://arxiv.org/abs/2601.01514)
*Matej Kucera,Marco Castelluccio,Daniel Feitosa,Ayushi Rastogi*

Main category: cs.SE

TL;DR: 研究比较组评审请求与个人评审请求对代码审查速度和质量的影响，发现组评审与审查质量提升有关，对速度影响可忽略。


<details>
  <summary>Details</summary>
Motivation: 此前研究未明确审查分配过程（特别是组审查请求）对代码审查速度的作用。

Method: 调查 Mozilla Firefox 项目约 66,000 次修订，结合统计建模和焦点小组讨论中从业者的观点。

Result: 组审查与审查质量提升相关（回归问题更少），对审查速度影响可忽略。

Conclusion: 组审查有提升审查质量、平衡工作分配和为新审查员提供培训机会等好处。

Abstract: The speed at which code changes are integrated into the software codebase, also referred to as code review velocity, is a prevalent industry metric for improved throughput and developer satisfaction. While prior studies have explored factors influencing review velocity, the role of the review assignment process, particularly the `group review request', is unclear. In group review requests, available on platforms like Phabricator, GitHub, and Bitbucket, a code change is assigned to a reviewer group, allowing any member to review it, unlike individual review assignments to specific reviewers. Drawing parallels with shared task queues in Management Sciences, this study examines the effects of group versus individual review requests on velocity and quality. We investigate approximately 66,000 revisions in the Mozilla Firefox project, combining statistical modeling with practitioner views from a focus group discussion. Our study associates group reviews with improved review quality, characterized by fewer regressions, while having a negligible association with review velocity. Additional perceived benefits include balanced work distribution and training opportunities for new reviewers.

</details>


### [200] [MTS-1: A Lightweight Delta-Encoded Telemetry Format optimised for Low-Resource Environments and Offline-First System Health Monitoring](https://arxiv.org/abs/2601.01602)
*Henry Ndou*

Main category: cs.SE

TL;DR: 论文介绍新型遥测格式MTS - 1，对比多种现有编码格式，合成基准测试显示其有压缩优势。


<details>
  <summary>Details</summary>
Motivation: 现有遥测编码在带宽受限网络中开销大，不适用于离线优先系统监控等场景。

Method: 引入MTS - 1格式，并将其与JSON、JSON Lines等格式在有效负载大小、编码成本等方面进行比较。

Result: 合成基准测试显示，与JSON相比压缩率最高提升74.7%，与MessagePack相比提升5.4%，且在不同数据集大小上呈线性缩放特征。

Conclusion: MTS - 1在带宽受限网络下有更好的性能，可用于离线优先系统监控等。

Abstract: System-level telemetry is fundamental to modern remote monitoring, predictive maintenance, and AI-driven infrastructure optimisation. Existing telemetry encodings such as JSON, JSON Lines, CBOR, and Protocol Buffers were designed for high-bandwidth, always-online environments. They impose significant overhead when deployed in bandwidth-constrained networks common across Sub-Saharan Africa, rural enterprise deployments, and unstable LAN environments. This paper introduces MTS-1 (Magenta Telemetry Standard v1), a novel delta-encoded binary telemetry format designed for offline-first system monitoring, LAN-assisted proxy delivery, and energy-efficient IoT-to-server transmission. We compare MTS-1 against JSON, JSON Lines, CBOR, MessagePack, and Protocol Buffers across payload size, encoding cost, network efficiency, and cost-latency performance. Synthetic benchmarking demonstrates preliminary compression improvements of up to 74.7% versus JSON and 5.4% versus MessagePack, with linear scaling characteristics across dataset sizes.

</details>


### [201] [LIA: Supervised Fine-Tuning of Large Language Models for Automatic Issue Assignment](https://arxiv.org/abs/2601.01780)
*Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan,Mehdi Keshani,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 提出基于大语言模型的问题分配方法LIA，通过监督微调让模型学习历史分配模式，评估显示其性能远超基线。


<details>
  <summary>Details</summary>
Motivation: 手动问题分配易出错，现有自动方法依赖大量数据或关系信息，存在数据稀疏和噪声问题，效果受限。

Method: 采用监督微调方式，使用DeepSeek - R1 - Distill - Llama - 8B模型，利用其预训练语义理解能力，从问题标题和描述生成开发者推荐排名。

Result: LIA比基础预训练模型和现有方法有显著提升，Hit@1最高提升+187.8%和+211.2%。

Conclusion: 领域适配的大语言模型用于软件维护任务有效，LIA是实用且高性能的问题分配解决方案。

Abstract: Issue assignment is a critical process in software maintenance, where new issue reports are validated and assigned to suitable developers. However, manual issue assignment is often inconsistent and error-prone, especially in large open-source projects where thousands of new issues are reported monthly. Existing automated approaches have shown promise, but many rely heavily on large volumes of project-specific training data or relational information that is often sparse and noisy, which limits their effectiveness. To address these challenges, we propose LIA (LLM-based Issue Assignment), which employs supervised fine-tuning to adapt an LLM, DeepSeek-R1-Distill-Llama-8B in this work, for automatic issue assignment. By leveraging the LLM's pretrained semantic understanding of natural language and software-related text, LIA learns to generate ranked developer recommendations directly from issue titles and descriptions. The ranking is based on the model's learned understanding of historical issue-to-developer assignments, using patterns from past tasks to infer which developers are most likely to handle new issues. Through comprehensive evaluation, we show that LIA delivers substantial improvements over both its base pretrained model and state-of-the-art baselines. It achieves up to +187.8% higher Hit@1 compared to the DeepSeek-R1-Distill-Llama-8B pretrained base model, and outperforms four leading issue assignment methods by as much as +211.2% in Hit@1 score. These results highlight the effectiveness of domain-adapted LLMs for software maintenance tasks and establish LIA as a practical, high-performing solution for issue assignment.

</details>


### [202] [The Machine Learning Canvas: Empirical Findings on Why Strategy Matters More Than AI Code Generation](https://arxiv.org/abs/2601.01839)
*Martin Prause*

Main category: cs.SE

TL;DR: 研究创建并测试机器学习画布框架，调查数据科学家确定四个关键成功因素，发现因素相互关联，AI 助手不能保证项目成功。


<details>
  <summary>Details</summary>
Motivation: 尽管 AI 编码助手流行，但超 80% 的机器学习项目未实现商业价值，需确定项目成功因素。

Method: 创建机器学习画布框架，调查 150 名数据科学家，用统计模型分析回复。

Result: 确定四个关键成功因素，且因素相互关联，AI 助手虽使编码快但不保证项目成功。

Conclusion: 战略、流程、生态系统和支持等元素共同决定项目是否成功，AI 助手无法替代战略思维。

Abstract: Despite the growing popularity of AI coding assistants, over 80% of machine learning (ML) projects fail to deliver real business value. This study creates and tests a Machine Learning Canvas, a practical framework that combines business strategy, software engineering, and data science in order to determine the factors that lead to the success of ML projects. We surveyed 150 data scientists and analyzed their responses using statistical modeling. We identified four key success factors: Strategy (clear goals and planning), Process (how work gets done), Ecosystem (tools and infrastructure), and Support (organizational backing and resources). Our results show that these factors are interconnected - each one affects the next. For instance, strong organizational support results in a clearer strategy (β= 0.432, p < 0.001), which improves work processes (β= 0.428, p < 0.001) and builds better infrastructure (β= 0.547, p < 0.001). Together, these elements determine whether a project succeeds. The surprising finding? Although AI assistants make coding faster, they don't guarantee project success. AI assists with the "how" of coding but cannot replace the "why" and "what" of strategic thinking.

</details>


### [203] [A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach](https://arxiv.org/abs/2601.01921)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 本文探讨时间敏感技术用于缺陷预测的有效性及缺陷早期指标，计划训练多种技术预测软件项目未来缺陷密度。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统不断演进，需要时间敏感方法在缺陷显现前进行预测。

Method: 训练多种时间敏感预测技术来预测软件项目未来的缺陷密度，并识别缺陷发生前的早期症状。

Result: 获得关于该方法对早期估计缺陷倾向有效性的实证证据。

Conclusion: 摘要未提及明确结论。

Abstract: Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest.
  Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect.
  Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect.
  Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.

</details>


### [204] [The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities](https://arxiv.org/abs/2601.01944)
*Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 本文聚焦开源软件中AI库的采用情况，打算分析15.77万个仓库，对比采用与不采用AI库项目的差异，为AI重塑软件开发实践提供见解。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在开源软件项目中日益重要，但AI的采用及其对开源软件项目的影响仍未被充分研究。

Method: 对157.7k个潜在开源软件仓库进行大规模分析，使用仓库指标和软件指标对比采用与不采用AI库的项目。

Result: 预计能识别采用与不采用AI库的开源软件项目在开发活动、社区参与和代码复杂性方面的可衡量差异。

Conclusion: 为AI集成如何重塑软件开发实践提供基于证据的见解。

Abstract: In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.
  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.

</details>


### [205] [Context-Adaptive Requirements Defect Prediction through Human-LLM Collaboration](https://arxiv.org/abs/2601.01952)
*Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 提出人类与大语言模型协作（HLC）方法用于需求缺陷预测，在奔驰需求基准测试中表现良好，能适应反馈并超越传统模型。


<details>
  <summary>Details</summary>
Motivation: 传统自动化需求评估依赖通用模式，而缺陷定义具有上下文依赖性，需更自适应的方法。

Method: 提出HLC方法，利用大语言模型思维链推理和反馈循环，通过少量验证示例进行少样本学习。

Result: HLC能有效适应验证示例，仅20个验证示例就能快速提升性能，结合解释能大幅超越标准少样本提示和微调的BERT模型，且召回率高。

Conclusion: 大语言模型的上下文和思维链学习能力使自适应分类方法超越通用模型，为从利益相关者反馈中持续学习的工具创造机会。

Abstract: Automated requirements assessment traditionally relies on universal patterns as proxies for defectiveness, implemented through rule-based heuristics or machine learning classifiers trained on large annotated datasets. However, what constitutes a "defect" is inherently context-dependent and varies across projects, domains, and stakeholder interpretations. In this paper, we propose a Human-LLM Collaboration (HLC) approach that treats defect prediction as an adaptive process rather than a static classification task. HLC leverages LLM Chain-of-Thought reasoning in a feedback loop: users validate predictions alongside their explanations, and these validated examples adaptively guide future predictions through few-shot learning. We evaluate this approach using the weak word smell on the QuRE benchmark of 1,266 annotated Mercedes-Benz requirements. Our results show that HLC effectively adapts to the provision of validated examples, with rapid performance gains from as few as 20 validated examples. Incorporating validated explanations, not just labels, enables HLC to substantially outperform both standard few-shot prompting and fine-tuned BERT models while maintaining high recall. These results highlight how the in-context and Chain-of-Thought learning capabilities of LLMs enable adaptive classification approaches that move beyond one-size-fits-all models, creating opportunities for tools that learn continuously from stakeholder feedback.

</details>


### [206] [Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations](https://arxiv.org/abs/2601.01954)
*Alexander Korn,Lea Zaruchas,Chetan Arora,Andreas Metzger,Sven Smolka,Fanyu Wang,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 研究大语言模型在软件工程任务的提示报告问题，通过文献分析和专家调查提出结构化指南。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在软件工程中的提示相关决策缺乏系统记录，影响研究可重复性和可比性。

Method: 两阶段实证研究，一是分析近300篇顶级会议论文，二是调查105位程序委员会成员。

Result: 当前实践与审稿人期望存在显著差异，尤其在版本披露、提示理由和有效性威胁方面。

Conclusion: 提出结构化指南，有助于提高基于大语言模型的软件工程研究的透明度、可重复性和方法严谨性。

Abstract: Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.

</details>


### [207] [The State of Open Science in Software Engineering Research: A Case Study of ICSE Artifacts](https://arxiv.org/abs/2601.02066)
*Al Muttakin,Saikat Mondal,Chanchal Roy*

Main category: cs.SE

TL;DR: 本文评估过去十年ICSE会议的100个复制包，发现可执行性和可重复性低，提出改进指南。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对软件工程研究中复制包可执行性和可重复性的全面研究，本文旨在填补这一空白。

Method: 评估2015 - 2024年ICSE会议的100个复制包，分析可执行性、执行所需努力和修改、执行失败挑战及结果可重复性。

Result: 100个工件中仅40%可执行，其中32.5%无需修改，82.5%需中高努力；确定5种常见修改类型和13种执行失败挑战；可执行工件中仅35%能重现结果。

Conclusion: 研究指出工件可用性、可执行性和可重复性存在显著差距，提出三条改进研究工件的可行指南。

Abstract: Replication packages are crucial for enabling transparency, validation, and reuse in software engineering (SE) research. While artifact sharing is now a standard practice and even expected at premier SE venues such as ICSE, the practical usability of these replication packages remains underexplored. In particular, there is a marked lack of studies that comprehensively examine the executability and reproducibility of replication packages in SE research. In this paper, we aim to fill this gap by evaluating 100 replication packages published as part of ICSE proceedings over the past decade (2015--2024). We assess the (1) executability of the replication packages, (2) efforts and modifications required to execute them, (3) challenges that prevent executability, and (4) reproducibility of the original findings. We spent approximately 650 person-hours in total executing the artifacts and reproducing the study findings. Our findings reveal that only 40\% of the 100 evaluated artifacts were executable, of which 32.5\% (13 out of 40) ran without any modification. Regarding effort levels, 17.5\% (7 out of 40) required low effort, while 82.5\% (33 out of 40) required moderate to high effort to execute successfully. We identified five common types of modifications and 13 challenges leading to execution failure, spanning environmental, documentation, and structural issues. Among the executable artifacts, only 35\% (14 out of 40) reproduced the original results. These findings highlight a notable gap between artifact availability, executability, and reproducibility. Our study proposes three actionable guidelines to improve the preparation, documentation, and review of research artifacts, thereby strengthening the rigor and sustainability of open science practices in SE research.

</details>


### [208] [Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics](https://arxiv.org/abs/2601.02200)
*Markus Borg,Nadim Hagatulah,Adam Tornhill,Emma Söderberg*

Main category: cs.SE

TL;DR: 研究在竞争编程的Python文件数据集上通过基于大语言模型的重构探讨‘AI友好代码’，发现CodeHealth与AI重构后语义保留有关，表明人类友好代码也与AI工具更兼容。


<details>
  <summary>Details</summary>
Motivation: 进入人类开发者和AI编码代理在同一代码库工作的混合时代，需确保不同能力的大语言模型能可靠编辑代码。

Method: 在包含5000个Python文件的竞争编程数据集上进行基于大语言模型的重构。

Result: 发现CodeHealth（为人类理解校准的质量指标）与AI重构后的语义保留有有意义的关联，人类友好代码与AI工具更兼容。

Conclusion: 组织可使用CodeHealth指导AI干预低风险的地方和需要额外人工监督的地方，投资代码可维护性既帮助人类，也为大规模采用AI做准备。

Abstract: We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.

</details>


### [209] [LLM-Empowered Functional Safety and Security by Design in Automotive Systems](https://arxiv.org/abs/2601.02215)
*Nenad Petrovic,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 提出由大语言模型驱动的工作流程支持软件定义汽车软件开发，涵盖安全系统拓扑设计和代码分析。


<details>
  <summary>Details</summary>
Motivation: 支持软件定义汽车（SDV）的软件开发。

Method: 代码分析采用事件链模型，拓扑安全分析结合模型驱动工程（MDE）方法和对象约束语言（OCL）规则，考虑本地部署和专有解决方案进行评估。

Result: 文档未明确提及具体结果。

Conclusion: 文档未明确得出相关结论。

Abstract: This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.

</details>


### [210] [NQC2: A Non-Intrusive QEMU Code Coverage Plugin](https://arxiv.org/abs/2601.02238)
*Nils Bosbach,Alwalid Salama,Lukas Jünger,Mark Burton,Niko Zurstraßen,Rebecca Pelke,Rainer Leupers*

Main category: cs.SE

TL;DR: 传统方法测量嵌入式系统代码覆盖率有局限，提出NQC2插件解决问题，性能优于竞品。


<details>
  <summary>Details</summary>
Motivation: 传统代码覆盖率测量方法在嵌入式系统中存在局限性，不适合裸机程序。

Method: 提出NQC2插件，从QEMU运行时提取覆盖率信息并存储到主机文件，无需目标软件插桩，兼容修改版QEMU。

Result: NQC2性能比Xilinx的类似方法最高快8.5倍。

Conclusion: NQC2能有效解决嵌入式系统代码覆盖率测量问题，性能表现出色。

Abstract: Code coverage analysis has become a standard approach in software development, facilitating the assessment of test suite effectiveness, the identification of under-tested code segments, and the discovery of performance bottlenecks. When code coverage of software for embedded systems needs to be measured, conventional approaches quickly meet their limits. A commonly used approach involves instrumenting the source files with added code that collects and dumps coverage information during runtime. This inserted code usually relies on the existence of an operating and a file system to dump the collected data. These features are not available for bare-metal programs that are executed on embedded systems.
  To overcome this issue, we present NQC2, a plugin for QEMU.NQC2 extracts coverage information from QEMU during runtime and stores them into a file on the host machine. This approach is even compatible with modified QEMU versions and does not require target-software instrumentation. NQC2 outperforms a comparable approach from Xilinx by up to 8.5 x.

</details>


### [211] [Automatic Assertion Mining in Assertion-Based Verification: Techniques, Challenges, and Future Directions](https://arxiv.org/abs/2601.02248)
*Mohammad Reza Heidari Iman,Giorgio Di Natale,Katell Morin-Allory*

Main category: cs.SE

TL;DR: 本文回顾最新、先进且广泛采用的自动断言挖掘器并对比分析其方法，为研究人员和验证人员提供见解，指明未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着功能验证愈发依赖基于断言的验证（ABV），自动断言挖掘器至关重要，为让研究和验证人员了解现有挖掘器的能力与局限，开展此研究。

Method: 对最具代表性的自动断言挖掘器的方法进行对比分析。

Result: 剖析了现存断言挖掘器的能力与不足。

Conclusion: 指出了未来开发更强大先进断言挖掘器的发展方向。

Abstract: Functional verification increasingly relies on Assertion-Based Verification (ABV), which has become a key approach for verifying hardware designs due to its efficiency and effectiveness. Central to ABV are automatic assertion miners, which apply different techniques to generate assertions automatically. This paper reviews the most recent, advanced, and widely adopted assertion miners, offering a comparative analysis of their methodologies. The goal is to provide researchers and verification practitioners with insights into the capabilities and limitations of existing miners. By identifying their shortcomings, this work also points toward directions for developing more powerful and advanced assertion miners in the future.

</details>


### [212] [Question Answering for Multi-Release Systems: A Case Study at Ciena](https://arxiv.org/abs/2601.02345)
*Parham Khamsepour,Mark Cole,Ish Ashraf,Sandeep Puri,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: 为解决多版本系统文档问答挑战，提出聊天机器人QAMR，结合多种策略提升准确性，评估显示其优于基线模型，还能减少响应时间。


<details>
  <summary>Details</summary>
Motivation: 现有问答技术在多版本系统文档上准确性不足，需要更有效的解决方案。

Method: 提出QAMR，增强传统RAG，结合预处理、查询重写和上下文选择，采用双分块策略。

Result: QAMR在平均答案正确性、检索准确性上有提升，处理多版本文档机制能提高答案准确性，相对基线减少响应时间，评估指标与人工评估相关。

Conclusion: QAMR在多版本系统文档问答上有效提升准确性和效率，评估方法可靠。

Abstract: Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [213] [Can Large Language Models Improve Venture Capital Exit Timing After IPO?](https://arxiv.org/abs/2601.00810)
*Mohammadhossien Rashidi*

Main category: q-fin.PM

TL;DR: 研究引入使用大语言模型（LLMs）估算风投IPO后退出时机的框架，对比LLM建议与实际退出日期，量化收益差异，以验证AI指导能否改善退出时机。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要描述风投退出时间，未评估是否经济最优，且LLMs未应用于IPO后退出决策。

Method: 引入框架，用LLMs分析IPO后每月财务表现、文件、新闻和市场信号，推荐卖出或持有，对比LLM建议与实际退出日期并计算收益差异。

Result: 文中未明确提及具体结果。

Conclusion: 通过量化收益或损失，可验证AI驱动的指导能否改善退出时机，补充风投研究中的传统模型。

Abstract: Exit timing after an IPO is one of the most consequential decisions for venture capital (VC) investors, yet existing research focuses mainly on describing when VCs exit rather than evaluating whether those choices are economically optimal. Meanwhile, large language models (LLMs) have shown promise in synthesizing complex financial data and textual information but have not been applied to post-IPO exit decisions. This study introduces a framework that uses LLMs to estimate the optimal time for VC exit by analyzing monthly post IPO information financial performance, filings, news, and market signals and recommending whether to sell or continue holding. We compare these LLM generated recommendations with the actual exit dates observed for VCs and compute the return differences between the two strategies. By quantifying gains or losses associated with following the LLM, this study provides evidence on whether AI-driven guidance can improve exit timing and complements traditional hazard and real-options models in venture capital research.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [214] [Beyond Demand Estimation: Consumer Surplus Evaluation via Cumulative Propensity Weights](https://arxiv.org/abs/2601.01029)
*Zeyu Bian,Max Biggs,Ruijiang Gao,Zhengling Qi*

Main category: stat.ML

TL;DR: 提出用观测数据审计AI决策对消费者剩余影响的实用框架，给出新估计量，扩展到关注公平性的度量并验证方法。


<details>
  <summary>Details</summary>
Motivation: 传统计算消费者剩余的方法在实践中存在模型错误指定、数据需求大、收敛慢等问题，需要新方法。

Method: 利用算法定价中的随机性，引入避免显式估计和数值积分的估计量，提出双稳健的ACPW估计量，扩展到不平等感知的剩余度量。

Result: 提出新的估计量和度量，能借助灵活机器学习方法快速收敛估计消费者剩余。

Conclusion: 经全面数值研究验证了所提方法的有效性。

Abstract: This paper develops a practical framework for using observational data to audit the consumer surplus effects of AI-driven decisions, specifically in targeted pricing and algorithmic lending. Traditional approaches first estimate demand functions and then integrate to compute consumer surplus, but these methods can be challenging to implement in practice due to model misspecification in parametric demand forms and the large data requirements and slow convergence of flexible nonparametric or machine learning approaches. Instead, we exploit the randomness inherent in modern algorithmic pricing, arising from the need to balance exploration and exploitation, and introduce an estimator that avoids explicit estimation and numerical integration of the demand function. Each observed purchase outcome at a randomized price is an unbiased estimate of demand and by carefully reweighting purchase outcomes using novel cumulative propensity weights (CPW), we are able to reconstruct the integral. Building on this idea, we introduce a doubly robust variant named the augmented cumulative propensity weighting (ACPW) estimator that only requires one of either the demand model or the historical pricing policy distribution to be correctly specified. Furthermore, this approach facilitates the use of flexible machine learning methods for estimating consumer surplus, since it achieves fast convergence rates by incorporating an estimate of demand, even when the machine learning estimate has slower convergence rates. Neither of these estimators is a standard application of off-policy evaluation techniques as the target estimand, consumer surplus, is unobserved. To address fairness, we extend this framework to an inequality-aware surplus measure, allowing regulators and firms to quantify the profit-equity trade-off. Finally, we validate our methods through comprehensive numerical studies.

</details>


### [215] [Fibonacci-Driven Recursive Ensembles: Algorithms, Convergence, and Learning Dynamics](https://arxiv.org/abs/2601.01055)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: 本文发展了由斐波那契型更新流驱动的递归集成学习的算法和动力学基础，建立相关理论并通过实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 与经典提升方法不同，研究二阶递归架构的集成学习，使集成能结合过去结构并适应新的残差信息。

Method: 引入包含斐波那契、三项斐波那契及更高阶递归的递归权重更新算法家族，分析连续时间极限，建立全局收敛条件、谱稳定性准则和非渐近泛化界。

Result: 实验表明递归流在核岭回归、样条平滑器和随机傅里叶特征模型中能持续提升近似和泛化能力。

Conclusion: 该理论统一了递归集成、结构化加权和动力系统观点，完成了相关研究三部曲。

Abstract: This paper develops the algorithmic and dynamical foundations of recursive ensemble learning driven by Fibonacci-type update flows. In contrast with classical boosting  Freund and Schapire (1997); Friedman (2001), where the ensemble evolves through first-order additive updates, we study second-order recursive architectures in which each predictor depends on its two immediate predecessors. These Fibonacci flows induce a learning dynamic with memory, allowing ensembles to integrate past structure while adapting to new residual information. We introduce a general family of recursive weight-update algorithms encompassing Fibonacci, tribonacci, and higher-order recursions, together with continuous-time limits that yield systems of differential equations governing ensemble evolution. We establish global convergence conditions, spectral stability criteria, and non-asymptotic generalization bounds under Rademacher Bartlett and Mendelson (2002) and algorithmic stability analyses. The resulting theory unifies recursive ensembles, structured weighting, and dynamical systems viewpoints in statistical learning. Experiments with kernel ridge regression Rasmussen and Williams (2006), spline smoothers Wahba (1990), and random Fourier feature models Rahimi and Recht (2007) demonstrate that recursive flows consistently improve approximation and generalization beyond static weighting. These results complete the trilogy begun in Papers I and II: from Fibonacci weighting, through geometric weighting theory, to fully dynamical recursive ensemble learning systems.

</details>


### [216] [Neural Networks on Symmetric Spaces of Noncompact Type](https://arxiv.org/abs/2601.01097)
*Xuan Son Nguyen,Shuo Yang,Aymeric Histace*

Main category: stat.ML

TL;DR: 提出一种在非紧型对称空间上开发神经网络的新方法，并在多个任务基准上验证。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络在双曲空间和SPD流形（属于非紧型对称空间）表现良好，需新方法开发该类空间的神经网络。

Method: 基于点到超平面距离的统一公式，推导出高阶非紧型对称空间的点到超平面距离闭式表达式，并以此设计全连接层和注意力机制。

Result: 能在特定设置下恢复现有的点到超平面距离公式。

Conclusion: 该方法在图像分类、脑电信号分类、图像生成和自然语言推理等基准测试中有效。

Abstract: Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference.

</details>


### [217] [Conformal Blindness: A Note on $A$-Cryptic change-points](https://arxiv.org/abs/2601.01147)
*Johan Hallberg Szabadváry*

Main category: stat.ML

TL;DR: 本文指出Conformal Test Martingales (CTMs)可能存在‘共形盲目性’现象，即数据可交换性断裂但p值均匀，CTM无法检测；并通过构造证明和模拟验证了这一点。


<details>
  <summary>Details</summary>
Motivation: 探讨CTMs测试数据可交换性假设时，是否存在数据可交换性显著断裂但p值保持均匀，导致CTMs失效的情况。

Method: 通过对理论上理想的‘神谕’一致性度量进行显式构造；使用二元高斯分布进行分析；进行模拟验证。

Result: 证明了存在A - 隐式变化点，发现二元高斯分布中边际均值变化不改变一致性得分分布可产生均匀p值；模拟表明巨大分布偏移对CTM完全隐藏。

Conclusion: 指出CTM存在基本局限性，强调一致性度量与潜在分布偏移的匹配至关重要。

Abstract: Conformal Test Martingales (CTMs) are a standard method within the Conformal Prediction framework for testing the crucial assumption of data exchangeability by monitoring deviations from uniformity in the p-value sequence. Although exchangeability implies uniform p-values, the converse does not hold. This raises the question of whether a significant break in exchangeability can occur, such that the p-values remain uniform, rendering CTMs blind. We answer this affirmatively, demonstrating the phenomenon of \emph{conformal blindness}.
  Through explicit construction, for the theoretically ideal ``oracle'' conformity measure (given by the true conditional density), we demonstrate the possibility of an \emph{$A$-cryptic change-point} (where $A$ refers to the conformity measure). Using bivariate Gaussian distributions, we identify a line along which a change in the marginal means does not alter the distribution of the conformity scores, thereby producing perfectly uniform p-values.
  Simulations confirm that even a massive distribution shift can be perfectly cryptic to the CTM, highlighting a fundamental limitation and emphasising the critical role of the alignment of the conformity measure with potential shifts.

</details>


### [218] [Evidence Slopes and Effective Dimension in Singular Linear Models](https://arxiv.org/abs/2601.01238)
*Kalyaan Rao*

Main category: stat.ML

TL;DR: 研究线性高斯秩模型和线性子空间模型，指出Laplace/BIC在奇异模型中误差增长情况，提出RLCT校正方法，结果可表征Laplace失效并以证据斜率估计有效维度。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯模型选择的Laplace近似或BIC假设有效模型维度等于参数数量，在过参数化或秩亏模型中存在问题，需用RLCT替代。

Method: 研究线性 - 高斯秩模型和线性子空间模型，理论推导和实证分析Laplace/BIC误差。

Result: Laplace/BIC误差随(d/2 - lambda) * log n线性增长，RLCT校正可恢复正确证据斜率，且对相同数据子空间的过完备重新参数化不变。

Conclusion: 结果给出奇异模型中Laplace失效的有限样本特征，表明证据斜率可在简单线性设置中作为有效维度的实用估计器。

Abstract: Bayesian model selection commonly relies on Laplace approximation or the Bayesian Information Criterion (BIC), which assume that the effective model dimension equals the number of parameters. Singular learning theory replaces this assumption with the real log canonical threshold (RLCT), an effective dimension that can be strictly smaller in overparameterized or rank-deficient models.
  We study linear-Gaussian rank models and linear subspace (dictionary) models in which the exact marginal likelihood is available in closed form and the RLCT is analytically tractable. In this setting, we show theoretically and empirically that the error of Laplace/BIC grows linearly with (d/2 minus lambda) times log n, where d is the ambient parameter dimension and lambda is the RLCT. An RLCT-aware correction recovers the correct evidence slope and is invariant to overcomplete reparameterizations that represent the same data subspace.
  Our results provide a concrete finite-sample characterization of Laplace failure in singular models and demonstrate that evidence slopes can be used as a practical estimator of effective dimension in simple linear settings.

</details>


### [219] [Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations](https://arxiv.org/abs/2601.01442)
*Dongrong Li,Tianwei Yu,Xiaodan Fan*

Main category: stat.ML

TL;DR: 本文提出一种塌缩吉布斯采样器用于隐马尔可夫模型（HMM），在处理含缺失值序列数据时，计算和理论层面都更快，实证评估显示其优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有处理含缺失值序列数据的HMM估计方法（EM算法和吉布斯采样器）存在非凸性、高计算复杂度和慢混合等问题。

Method: 提出一种塌缩吉布斯采样器，通过对缺失观测值和相应潜在状态进行积分，从HMM的后验分布中高效采样。

Result: 该采样器估计精度与现有方法相当，每次迭代能产生更大的有效样本量（ESS），在缺失项较多时每次迭代计算复杂度显著更小；实证评估表明该算法在时间复杂度和采样效率（ESS衡量）上始终优于现有算法。

Conclusion: 提出的采样算法在计算和理论上都很快，在有大量缺失项时特别有优势。

Abstract: The Hidden Markov Model (HMM) is a widely-used statistical model for handling sequential data. However, the presence of missing observations in real-world datasets often complicates the application of the model. The EM algorithm and Gibbs samplers can be used to estimate the model, yet suffering from various problems including non-convexity, high computational complexity and slow mixing. In this paper, we propose a collapsed Gibbs sampler that efficiently samples from HMMs' posterior by integrating out both the missing observations and the corresponding latent states. The proposed sampler is fast due to its three advantages. First, it achieves an estimation accuracy that is comparable to existing methods. Second, it can produce a larger Effective Sample Size (ESS) per iteration, which can be justified theoretically and numerically. Third, when the number of missing entries is large, the sampler has a significant smaller computational complexity per iteration compared to other methods, thus is faster computationally. In summary, the proposed sampling algorithm is fast both computationally and theoretically and is particularly advantageous when there are a lot of missing entries. Finally, empirical evaluations based on numerical simulations and real data analysis demonstrate that the proposed algorithm consistently outperforms existing algorithms in terms of time complexity and sampling efficiency (measured in ESS).

</details>


### [220] [Modeling Information Blackouts in Missing Not-At-Random Time Series Data](https://arxiv.org/abs/2601.01480)
*Aman Sunesh,Allan Ma,Siddarth Nilol*

Main category: stat.ML

TL;DR: 提出潜在状态空间框架处理交通传感器数据缺失问题，在真实和合成数据上验证效果，表明时间动态主导性能，MNAR建模在数据缺失有信息时更有价值。


<details>
  <summary>Details</summary>
Motivation: 现有大规模交通预测中传感器数据缺失通常按MAR假设处理，但停电事件可能与未观察到的交通状况相关，需MNAR处理。

Method: 提出潜在状态空间框架，联合建模交通动态和传感器缺失，推理用扩展卡尔曼滤波和Rauch - Tung - Striebel平滑，参数通过近似EM程序学习。

Result: 在西雅图感应环路检测器数据上，引入潜在动态比基线有大幅提升，MNAR建模有额外小幅改进；合成实验中MNAR优势随缺失与潜在状态依赖增强而增加。

Conclusion: 时间动态主导性能，MNAR建模是一种原则性改进，在数据缺失有信息时最有价值。

Abstract: Large-scale traffic forecasting relies on fixed sensor networks that often exhibit blackouts: contiguous intervals of missing measurements caused by detector or communication failures. These outages are typically handled under a Missing At Random (MAR) assumption, even though blackout events may correlate with unobserved traffic conditions (e.g., congestion or anomalous flow), motivating a Missing Not At Random (MNAR) treatment. We propose a latent state-space framework that jointly models (i) traffic dynamics via a linear dynamical system and (ii) sensor dropout via a Bernoulli observation channel whose probability depends on the latent traffic state. Inference uses an Extended Kalman Filter with Rauch-Tung-Striebel smoothing, and parameters are learned via an approximate EM procedure with a dedicated update for detector-specific missingness parameters. On the Seattle inductive loop detector data, introducing latent dynamics yields large gains over naive baselines, reducing blackout imputation RMSE from 7.02 (LOCF) and 5.02 (linear interpolation + seasonal naive) to 4.23 (MAR LDS), corresponding to about a 64% reduction in MSE relative to LOCF. Explicit MNAR modeling provides a consistent but smaller additional improvement on real data (imputation RMSE 4.20; 0.8% RMSE reduction relative to MAR), with similar modest gains for short-horizon post-blackout forecasts (evaluated at 1, 3, and 6 steps). In controlled synthetic experiments, the MNAR advantage increases as the true missingness dependence on latent state strengthens. Overall, temporal dynamics dominate performance, while MNAR modeling offers a principled refinement that becomes most valuable when missingness is genuinely informative.

</details>


### [221] [Variance-Reduced Diffusion Sampling via Conditional Score Expectation Identity](https://arxiv.org/abs/2601.01594)
*Alois Duston,Tan Bui-Thanh*

Main category: stat.ML

TL;DR: 本文引入并证明CSE恒等式，提出基于CSE的分数统计估计器，推导方差最小化的混合分数估计器，数值实验显示其优势，还将框架扩展到贝叶斯反问题。


<details>
  <summary>Details</summary>
Motivation: 基于CSE恒等式，探索有效分数估计方法，解决相关问题。

Method: 使用Self - Normalized Importance Sampling (SNIS)程序，结合CSE恒等式，推导混合分数估计器，通过似然信息SNIS权重扩展到贝叶斯反问题。

Result: 最优混合估计器能降低方差、提高样本质量，在高维图像重建和PDE控制的反问题中提高重建质量和样本多样性。

Conclusion: 基于CSE的方法和混合估计器在分数估计和贝叶斯反问题中有良好表现。

Abstract: We introduce and prove a \textbf{Conditional Score Expectation (CSE)} identity: an exact relation for the marginal score of affine diffusion processes that links scores across time via a conditional expectation under the forward dynamics. Motivated by this identity, we propose a CSE-based statistical estimator for the score using a Self-Normalized Importance Sampling (SNIS) procedure with prior samples and forward noise. We analyze its relationship to the standard Tweedie estimator, proving anti-correlation for Gaussian targets and establishing the same behavior for general targets in the small time-step regime. Exploiting this structure, we derive a variance-minimizing blended score estimator given by a state--time dependent convex combination of the CSE and Tweedie estimators. Numerical experiments show that this optimal-blending estimator reduces variance and improves sample quality for a fixed computational budget compared to either baseline. We further extend the framework to Bayesian inverse problems via likelihood-informed SNIS weights, and demonstrate improved reconstruction quality and sample diversity on high-dimensional image reconstruction tasks and PDE-governed inverse problems.

</details>


### [222] [Deep Linear Discriminant Analysis Revisited](https://arxiv.org/abs/2601.01619)
*Maxat Tezekbayev,Rustem Takhanov,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 本文指出无约束深度线性判别分析（LDA）分类器最大似然训练存在病态解，交叉熵训练参数估计不一致，引入 DNLL 损失调和生成结构与判别性能，取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 解决无约束深度 LDA 分类器最大似然训练的病态解问题，以及交叉熵训练参数估计不一致的问题，调和生成结构与判别性能。

Method: 引入判别负对数似然（DNLL）损失，在 LDA 对数似然的基础上增加对混合密度的简单惩罚。

Result: 使用 DNLL 训练的深度 LDA 产生清晰、分离良好的潜在空间，在合成数据和标准图像基准上与 softmax 分类器的测试准确率相当，且预测概率校准效果更好。

Conclusion: DNLL 损失能为深度判别模型恢复连贯的概率解释，调和生成结构与判别性能。

Abstract: We show that for unconstrained Deep Linear Discriminant Analysis (LDA) classifiers, maximum-likelihood training admits pathological solutions in which class means drift together, covariances collapse, and the learned representation becomes almost non-discriminative. Conversely, cross-entropy training yields excellent accuracy but decouples the head from the underlying generative model, leading to highly inconsistent parameter estimates. To reconcile generative structure with discriminative performance, we introduce the \emph{Discriminative Negative Log-Likelihood} (DNLL) loss, which augments the LDA log-likelihood with a simple penalty on the mixture density. DNLL can be interpreted as standard LDA NLL plus a term that explicitly discourages regions where several classes are simultaneously likely. Deep LDA trained with DNLL produces clean, well-separated latent spaces, matches the test accuracy of softmax classifiers on synthetic data and standard image benchmarks, and yields substantially better calibrated predictive probabilities, restoring a coherent probabilistic interpretation to deep discriminant models.

</details>


### [223] [Simplex Deep Linear Discriminant Analysis](https://arxiv.org/abs/2601.01679)
*Maxat Tezekbayev,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 本文从似然角度重新审视深度线性判别分析（Deep LDA），指出无约束模型MLE训练问题，提出约束公式，实验显示具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 经典LDA简单，给神经编码器加LDA头后，如何用最大似然估计训练深度分类器存疑。

Method: 先分析无约束Deep LDA模型端到端MLE训练问题，然后提出约束Deep LDA公式，固定类均值到正则单纯形顶点，限制协方差为球形。

Result: 在Fashion - MNIST、CIFAR - 10、CIFAR - 100图像数据集上，约束后的Deep LDA模型精度与softmax基准相当。

Conclusion: 几何约束下，MLE稳定，能在潜在空间产生分离良好的类簇，模型有简单可解释的潜在几何结构。

Abstract: We revisit Deep Linear Discriminant Analysis (Deep LDA) from a likelihood-based perspective. While classical LDA is a simple Gaussian model with linear decision boundaries, attaching an LDA head to a neural encoder raises the question of how to train the resulting deep classifier by maximum likelihood estimation (MLE). We first show that end-to-end MLE training of an unconstrained Deep LDA model ignores discrimination: when both the LDA parameters and the encoder parameters are learned jointly, the likelihood admits a degenerate solution in which some of the class clusters may heavily overlap or even collapse, and classification performance deteriorates. Batchwise moment re-estimation of the LDA parameters does not remove this failure mode. We then propose a constrained Deep LDA formulation that fixes the class means to the vertices of a regular simplex in the latent space and restricts the shared covariance to be spherical, leaving only the priors and a single variance parameter to be learned along with the encoder. Under these geometric constraints, MLE becomes stable and yields well-separated class clusters in the latent space. On images (Fashion-MNIST, CIFAR-10, CIFAR-100), the resulting Deep LDA models achieve accuracy competitive with softmax baselines while offering a simple, interpretable latent geometry that is clearly visible in two-dimensional projections.

</details>


### [224] [Sparse Convex Biclustering](https://arxiv.org/abs/2601.01757)
*Jiakun Jiang,Dewei Xiang,Chenliang Gu,Wei Liu,Binhuan Wang*

Main category: stat.ML

TL;DR: 现有双聚类方法难满足大规模数据集需求，提出SpaCoBi方法，实验证明其在高维大规模数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有双聚类方法难以应对现代大规模数据集，存在噪声累积、非凸优化局限和计算复杂度高等问题，导致准确率和稳定性下降。

Method: 提出Sparse Convex Biclustering (SpaCoBi)方法，在双聚类过程中对噪声进行惩罚，采用凸优化框架并引入基于稳定性的调优准则。

Result: 通过模拟和小鼠嗅球数据应用的综合数值研究，表明SpaCoBi在准确率上显著优于现有方法。

Conclusion: SpaCoBi是高维和大规模数据集双聚类的稳健高效解决方案。

Abstract: Biclustering is an essential unsupervised machine learning technique for simultaneously clustering rows and columns of a data matrix, with widespread applications in genomics, transcriptomics, and other high-dimensional omics data. Despite its importance, existing biclustering methods struggle to meet the demands of modern large-scale datasets. The challenges stem from the accumulation of noise in high-dimensional features, the limitations of non-convex optimization formulations, and the computational complexity of identifying meaningful biclusters. These issues often result in reduced accuracy and stability as the size of the dataset increases. To overcome these challenges, we propose Sparse Convex Biclustering (SpaCoBi), a novel method that penalizes noise during the biclustering process to improve both accuracy and robustness. By adopting a convex optimization framework and introducing a stability-based tuning criterion, SpaCoBi achieves an optimal balance between cluster fidelity and sparsity. Comprehensive numerical studies, including simulations and an application to mouse olfactory bulb data, demonstrate that SpaCoBi significantly outperforms state-of-the-art methods in accuracy. These results highlight SpaCoBi as a robust and efficient solution for biclustering in high-dimensional and large-scale datasets.

</details>


### [225] [A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk](https://arxiv.org/abs/2601.01970)
*Ayomide Afolabi,Ebere Ogburu,Symon Kimitei*

Main category: stat.ML

TL;DR: 研究评估三种模型中各类分类器在信用卡邮件营销和违约预测方面的表现，不同模型下各有最优分类器。


<details>
  <summary>Details</summary>
Motivation: 解决特定的信用风险和邮件响应业务问题。

Method: 评估不同分类器在响应、风险和响应 - 风险三种模型中的表现，并优化各种性能指标。

Result: 响应模型中Extra Trees分类器召回率最高；风险模型中Random Forest分类器特异性高；响应 - 风险模型中Random Forest分类器准确率最高。

Conclusion: 不同模型下各有表现最优的分类器，可用于信用卡邮件营销和违约预测。

Abstract: This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem.

</details>


### [226] [From Mice to Trains: Amortized Bayesian Inference on Graph Data](https://arxiv.org/abs/2601.02241)
*Svenja Jedhoff,Elizaveta Semenova,Aura Raulo,Anne Meyer,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 本文将摊销贝叶斯推理（ABI）应用于图数据，设计两模块管道进行节点、边和图级参数推理，并评估多种架构在合成和真实数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 图结构数据推理需满足排列不变性、可扩展性和捕捉长程依赖的要求，传统方法在图参数后验估计上具有挑战性，因此需新方法。

Method: 将ABI应用于图数据，采用两模块管道，即摘要网络将属性图映射到固定长度表示，推理网络近似参数后验，评估多种神经架构作为摘要网络。

Result: 在受控合成设置和生物学、物流两个真实领域评估多种架构的恢复和校准性能。

Conclusion: 未明确提及，但暗示了不同架构在图数据推理中有不同表现，可指导后续架构选择。

Abstract: Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [227] [Hamiltonian Monte Carlo for (Physics) Dummies](https://arxiv.org/abs/2601.01422)
*Arghya Mukherjee,Dootika Vats*

Main category: stat.CO

TL;DR: 本文对哈密顿蒙特卡罗（HMC）算法进行教学性概述，帮助应用研究者理解其理论与应用。


<details>
  <summary>Details</summary>
Motivation: 现有的HMC黑盒实现让不熟悉物理原理的用户难以理解其内部工作机制，需搭建理论与实际应用间的桥梁。

Method: 对HMC进行教学性概述。

Result: 无明确提及具体结果。

Conclusion: 通过突出HMC的优势、局限和作用，让应用研究者更易理解该算法。

Abstract: Sampling-based inference has seen a surge of interest in recent years. Hamiltonian Monte Carlo (HMC) has emerged as a powerful algorithm that leverages concepts from Hamiltonian dynamics to efficiently explore complex target distributions. Variants of HMC are available in popular software packages, enabling off-the-shelf implementations that have greatly benefited the statistics and machine learning communities. At the same time, the availability of such black-box implementations has made it challenging for users to understand the inner workings of HMC, especially when they are unfamiliar with the underlying physical principles. We provide a pedagogical overview of HMC that aims to bridge the gap between its theoretical foundations and practical applicability. This review article seeks to make HMC more accessible to applied researchers by highlighting its advantages, limitations, and role in enabling scalable and exact Bayesian inference for complex models.

</details>


### [228] [grangersearch: An R Package for Exhaustive Granger Causality Testing with Tidyverse Integration](https://arxiv.org/abs/2601.01604)
*Nikolaos Korfiatis*

Main category: stat.CO

TL;DR: 介绍R包grangersearch用于多时间序列的Granger因果关系搜索，阐述其功能、方法等


<details>
  <summary>Details</summary>
Motivation: 为多时间序列的Granger因果关系搜索提供便捷工具

Method: 封装vars基础设施，提供简单接口，具有多种功能如成对搜索、滞后阶优化等

Result: 成功开发grangersearch包，展示了其功能和用法

Conclusion: 该包可用于探索性因果分析，为应用研究者提供实用工具

Abstract: This paper introduces grangersearch, an R package for performing exhaustive Granger causality searches on multiple time series. The package provides: (1) exhaustive pairwise search across multiple variables, (2) automatic lag order optimization with visualization, (3) tidyverse-compatible syntax with pipe operators and non-standard evaluation, and (4) integration with the broom ecosystem through tidy() and glance() methods. The package wraps the vars infrastructure while providing a simple interface for exploratory causal analysis. We describe the statistical methodology, demonstrate the package through worked examples, and discuss practical considerations for applied researchers.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [229] [Disordered Dynamics in High Dimensions: Connections to Random Matrices and Machine Learning](https://arxiv.org/abs/2601.01010)
*Blake Bordelon,Cengiz Pehlevan*

Main category: cond-mat.dis-nn

TL;DR: 本文综述高维随机矩阵驱动动力系统，介绍DMFT框架，阐述其与随机矩阵解析函数联系，展示在机器学习模型应用，分析损失曲线特性并给出深度线性神经网络训练和测试损失动态渐近描述。


<details>
  <summary>Details</summary>
Motivation: 聚焦机器学习理论中学习和泛化简单模型应用，研究高维随机矩阵驱动动力系统。

Method: 使用腔方法和路径积分，介绍动力学平均场理论（DMFT）。

Result: 明确DMFT单站点随机过程由相关和响应函数表征，展示在多种机器学习模型应用，分析不同矩阵驱动系统损失曲线特性，给出深度线性神经网络损失动态渐近描述。

Conclusion: DMFT框架可灵活应用于高维随机矩阵驱动动力系统，不同矩阵驱动系统有不同损失曲线特性。

Abstract: We provide an overview of high dimensional dynamical systems driven by random matrices, focusing on applications to simple models of learning and generalization in machine learning theory. Using both cavity method arguments and path integrals, we review how the behavior of a coupled infinite dimensional system can be characterized as a stochastic process for each single site of the system. We provide a pedagogical treatment of dynamical mean field theory (DMFT), a framework that can be flexibly applied to these settings. The DMFT single site stochastic process is fully characterized by a set of (two-time) correlation and response functions. For linear time-invariant systems, we illustrate connections between random matrix resolvents and the DMFT response. We demonstrate applications of these ideas to machine learning models such as gradient flow, stochastic gradient descent on random feature models and deep linear networks in the feature learning regime trained on random data. We demonstrate how bias and variance decompositions (analysis of ensembling/bagging etc) can be computed by averaging over subsets of the DMFT noise variables. From our formalism we also investigate how linear systems driven with random non-Hermitian matrices (such as random feature models) can exhibit non-monotonic loss curves with training time, while Hermitian matrices with the matching spectra do not, highlighting a different mechanism for non-monotonicity than small eigenvalues causing instability to label noise. Lastly, we provide asymptotic descriptions of the training and test loss dynamics for randomly initialized deep linear neural networks trained in the feature learning regime with high-dimensional random data. In this case, the time translation invariance structure is lost and the hidden layer weights are characterized as spiked random matrices.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [230] [VEAT Quantifies Implicit Associations in Text-to-Video Generator Sora and Reveals Challenges in Bias Mitigation](https://arxiv.org/abs/2601.00996)
*Yongxu Sun,Michael Saxon,Ian Yang,Anna-Maria Gueorguieva,Aylin Caliskan*

Main category: cs.CY

TL;DR: 本文通过引入VEAT和SC - VEAT测试T2V生成器是否存在社会偏见，发现Sora视频有一定关联偏见，显式去偏可能适得其反。


<details>
  <summary>Details</summary>
Motivation: 关注T2V生成器如Sora生成的内容是否反映社会偏见。

Method: 引入VEAT和SC - VEAT将嵌入关联测试从文字、图像扩展到视频，重现基线关联程度和方向进行验证，量化种族、性别与效价关联。

Result: Sora视频中欧洲裔美国人和女性更多与愉悦相关，效应大小与现实人口分布相关，显式去偏会降低效应大小，但对部分职业可能适得其反。

Conclusion: 易获取的T2V生成器若不严格评估和妥善部署，会放大代表性伤害。

Abstract: Text-to-Video (T2V) generators such as Sora raise concerns about whether generated content reflects societal bias. We extend embedding-association tests from words and images to video by introducing the Video Embedding Association Test (VEAT) and Single-Category VEAT (SC-VEAT). We validate these methods by reproducing the direction and magnitude of associations from widely used baselines, including Implicit Association Test (IAT) scenarios and OASIS image categories. We then quantify race (African American vs. European American) and gender (women vs. men) associations with valence (pleasant vs. unpleasant) across 17 occupations and 7 awards. Sora videos associate European Americans and women more with pleasantness (both d>0.8). Effect sizes correlate with real-world demographic distributions: percent men and White in occupations (r=0.93, r=0.83) and percent male and non-Black among award recipients (r=0.88, r=0.99). Applying explicit debiasing prompts generally reduces effect-size magnitudes, but can backfire: two Black-associated occupations (janitor, postal service) become more Black-associated after debiasing. Together, these results reveal that easily accessible T2V generators can actually amplify representational harms if not rigorously evaluated and responsibly deployed.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [231] [LLM Collusion](https://arxiv.org/abs/2601.01279)
*Shengyu Cao,Ming Hu*

Main category: econ.TH

TL;DR: 研究双寡头市场中卖家使用同一预训练大语言模型（LLM）进行定价时的合谋情况，分析了LLM参数及训练批次对定价结果的影响。


<details>
  <summary>Details</summary>
Motivation: 探究卖家依赖同一预训练大语言模型进行定价时如何促进双寡头市场的合谋。

Method: 通过定义LLM的倾向参数和输出保真度参数，分析不同参数条件下的定价结果，考虑训练批次大小和再训练频率的影响。

Result: 配置LLM的鲁棒性和可重复性可通过相变诱导合谋，存在临界输出保真度阈值；高于阈值系统双稳，有竞争和勾结定价两种结果；完美保真度时从任何初始条件都会出现完全合谋；有限训练批次下，不频繁再训练会加剧合谋。

Conclusion: 大语言模型的参数设置、训练批次大小和再训练频率会影响双寡头市场的定价结果，可能导致合谋。

Abstract: We study how delegating pricing to large language models (LLMs) can facilitate collusion in a duopoly when both sellers rely on the same pre-trained model. The LLM is characterized by (i) a propensity parameter capturing its internal bias toward high-price recommendations and (ii) an output-fidelity parameter measuring how tightly outputs track that bias; the propensity evolves through retraining. We show that configuring LLMs for robustness and reproducibility can induce collusion via a phase transition: there exists a critical output-fidelity threshold that pins down long-run behavior. Below it, competitive pricing is the unique long-run outcome. Above it, the system is bistable, with competitive and collusive pricing both locally stable and the realized outcome determined by the model's initial preference. The collusive regime resembles tacit collusion: prices are elevated on average, yet occasional low-price recommendations provide plausible deniability. With perfect fidelity, full collusion emerges from any interior initial condition. For finite training batches of size $b$, infrequent retraining (driven by computational costs) further amplifies collusion: conditional on starting in the collusive basin, the probability of collusion approaches one as $b$ grows, since larger batches dampen stochastic fluctuations that might otherwise tip the system toward competition. The indeterminacy region shrinks at rate $O(1/\sqrt{b})$.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [232] [From Theory of Mind to Theory of Environment: Counterfactual Simulation of Latent Environmental Dynamics](https://arxiv.org/abs/2601.01599)
*Ryutaro Uchiyama*

Main category: q-bio.NC

TL;DR: 脊椎动物运动系统用降维策略控制运动，人类或能通过‘环境理论’从社会线索推断隐藏环境动态，扩展运动探索维度以促进行为创新。


<details>
  <summary>Details</summary>
Motivation: 研究在有隐藏行动 - 结果偶然性的环境中，如何实现行为创新。

Method: 提出‘环境理论’，认为人类可利用与心智理论共享的计算机制从社会线索推断隐藏环境动态。

Result: ‘环境理论’支持通过扩展运动探索维度来促进行为创新。

Conclusion: 人类可能凭借‘环境理论’在复杂环境中实现行为创新。

Abstract: The vertebrate motor system employs dimensionality-reducing strategies to limit the complexity of movement coordination, for efficient motor control. But when environments are dense with hidden action-outcome contingencies, movement complexity can promote behavioral innovation. Humans, perhaps uniquely, may infer the presence of hidden environmental dynamics from social cues, by drawing upon computational mechanisms shared with Theory of Mind. This proposed "Theory of Environment" supports behavioral innovation by expanding the dimensionality of motor exploration.

</details>


### [233] [A neural network for modeling human concept formation, understanding and communication](https://arxiv.org/abs/2601.02010)
*Liangxuan Guo,Haoyang Chen,Yang Chen,Yanchao Bi,Shan Yu*

Main category: q-bio.NC

TL;DR: 提出CATS Net双模块神经网络框架，其概念空间与神经认知语义模型和大脑响应结构一致，为理解人类概念认知和构建类人概念智能系统提供见解。


<details>
  <summary>Details</summary>
Motivation: 人类大脑能从感觉运动经验中形成抽象概念表征并灵活应用，但该能力的计算机制尚不清楚，旨在填补这一空白。

Method: 提出CATS Net框架，包含概念抽象模块和任务解决模块，系统基于概念表征发展可转移语义结构。

Result: 模型的概念空间与神经认知语义模型和人类腹侧枕颞叶皮质的大脑响应结构一致，门控机制与语义控制脑网络相似。

Conclusion: 建立了统一计算框架，为理解人类概念认知和构建类人概念智能系统提供机制性见解。

Abstract: A remarkable capability of the human brain is to form more abstract conceptual representations from sensorimotor experiences and flexibly apply them independent of direct sensory inputs. However, the computational mechanism underlying this ability remains poorly understood. Here, we present a dual-module neural network framework, the CATS Net, to bridge this gap. Our model consists of a concept-abstraction module that extracts low-dimensional conceptual representations, and a task-solving module that performs visual judgement tasks under the hierarchical gating control of the formed concepts. The system develops transferable semantic structure based on concept representations that enable cross-network knowledge transfer through conceptual communication. Model-brain fitting analyses reveal that these emergent concept spaces align with both neurocognitive semantic model and brain response structures in the human ventral occipitotemporal cortex, while the gating mechanisms mirror that in the semantic control brain network. This work establishes a unified computational framework that can offer mechanistic insights for understanding human conceptual cognition and engineering artificial systems with human-like conceptual intelligence.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [234] [HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery](https://arxiv.org/abs/2601.01015)
*Shiyuan Liu,Jianwei Wang,Xuemin Lin,Lu Qin,Wenjie Zhang,Ying Zhang*

Main category: cs.CL

TL;DR: 提出HyperJoin框架解决现有可连接表发现方法未充分考虑结构交互的问题，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的可连接表发现方法未充分考虑底层结构交互，离线建模难以捕捉表间和表内结构信息，在线排序忽略候选列间的相互作用。

Method: 构建超图对表进行建模，将可连接表发现任务转化为超图上的链接预测；设计HIN学习列表示；将在线排序转化为一致性感知的top - k列选择问题，引入重排序模块修剪噪声连接。

Result: 实验表明HyperJoin在Precision@15上平均提升21.4%，在Recall@15上平均提升17.2%。

Conclusion: HyperJoin在可连接表发现任务中表现优越，能有效解决现有方法的局限性。

Abstract: As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.

</details>


### [235] [Segmentation and Processing of German Court Decisions from Open Legal Data](https://arxiv.org/abs/2601.01449)
*Harshil Darji,Martin Heckelmann,Christina Kratsch,Gerard de Melo*

Main category: cs.CL

TL;DR: 本文介绍了从Open Legal Data数据集衍生出的251,038个德国法院判决的已清理且分节的数据集，该数据集公开可用。


<details>
  <summary>Details</summary>
Motivation: 现有Open Legal Data数据集的判决文本格式不一致且部分标记不清晰，可靠分离各部分对于相关任务很重要，故而开展此项工作。

Method: 系统分离德国法院判决中的三个重要部分，用Cochran公式抽样384个案例手动验证提取准确性，单独提取上诉通知部分。

Result: 得到了可用JSONL格式公开获取的语料库。

Conclusion: 该语料库为德国法律系统的进一步研究提供了可获取的资源。

Abstract: The availability of structured legal data is important for advancing Natural Language Processing (NLP) techniques for the German legal system. One of the most widely used datasets, Open Legal Data, provides a large-scale collection of German court decisions. While the metadata in this raw dataset is consistently structured, the decision texts themselves are inconsistently formatted and often lack clearly marked sections. Reliable separation of these sections is important not only for rhetorical role classification but also for downstream tasks such as retrieval and citation analysis. In this work, we introduce a cleaned and sectioned dataset of 251,038 German court decisions derived from the official Open Legal Data dataset. We systematically separated three important sections in German court decisions, namely Tenor (operative part of the decision), Tatbestand (facts of the case), and Entscheidungsgründe (judicial reasoning), which are often inconsistently represented in the original dataset. To ensure the reliability of our extraction process, we used Cochran's formula with a 95% confidence level and a 5% margin of error to draw a statistically representative random sample of 384 cases, and manually verified that all three sections were correctly identified. We also extracted the Rechtsmittelbelehrung (appeal notice) as a separate field, since it is a procedural instruction and not part of the decision itself. The resulting corpus is publicly available in the JSONL format, making it an accessible resource for further research on the German legal system.

</details>


### [236] [Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning](https://arxiv.org/abs/2601.01362)
*Jerry Huang,Peng Lu,Qiuhao Zeng,Yusuke Iwasawa,Yutaka Matsuo,Sarath Chandar,Edison Marrese-Taylor,Irene Li*

Main category: cs.CL

TL;DR: 研究多语言环境下大语言模型校准问题，发现标准SFT有不足，标签平滑可缓解。


<details>
  <summary>Details</summary>
Motivation: 探究多语言环境下大语言模型校准问题，了解数据稀缺的影响及常用技术适用性。

Method: 分析两个多语言基准测试中模型表现。

Result: 高资源语言SFT数据集指令调优后低资源语言模型置信度显著增加但准确率提升小，导致校准不良，标签平滑可缓解。

Conclusion: 训练和调优大语言模型时考虑多语言因素对提高下游使用可靠性和公平性很重要。

Abstract: Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.

</details>


### [237] [Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment](https://arxiv.org/abs/2601.01862)
*Nuo Chen,Hanpei Fang,Piaohong Wang,Jiqun Liu,Tetsuya Sakai,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 研究评估多LLM模拟大五人格特征对网页搜索决策和置信校准的影响，发现特定人格表现更好，用随机森林分类器取得良好效果，表明人格衍生置信有补充预测信号。


<details>
  <summary>Details</summary>
Motivation: 探讨模拟人格如何影响关键网页搜索决策（相关性评估）和置信校准，填补相关研究空白。

Method: 评估多个模拟大五人格特征的LLM，在三个测试集上测试，收集相关性判断和自信分数，将人格条件分数和置信度作为特征用于随机森林分类器。

Result: 低宜人性与人类标签更一致，低责任心能平衡抑制过度自信和信心不足，相关性分数和置信分布随人格不同而系统变化，随机森林分类器在新数据集上表现超最佳单人状态。

Conclusion: 人格衍生置信提供补充预测信号，有助于构建更可靠、符合人类的LLM评估器。

Abstract: Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.
  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.

</details>


### [238] [The Qualitative Laboratory: Theory Prototyping and Hypothesis Generation with Large Language Models](https://arxiv.org/abs/2601.00797)
*Hugues Draelants*

Main category: cs.CL

TL;DR: 文章介绍用大语言模型进行社会学人物角色模拟的新方法生成定性假设，以气候政策为例展示其潜力并得出该方法适用于后续实证检验的结论。


<details>
  <summary>Details</summary>
Motivation: 社会科学面临为不同社会群体解读新信息生成丰富定性假设的挑战，现有方法存在不足。

Method: 使用大语言模型进行社会学人物角色模拟，构建“定性实验室”。

Result: 以气候政策为例的模拟产生了细致且反直觉的假设，挑战了理论假设。

Conclusion: 该方法作为“先模拟后验证”工作流程的一部分，是生成后续实证检验假设的更好工具。

Abstract: A central challenge in social science is to generate rich qualitative hypotheses about how diverse social groups might interpret new information. This article introduces and illustrates a novel methodological approach for this purpose: sociological persona simulation using Large Language Models (LLMs), which we frame as a "qualitative laboratory". We argue that for this specific task, persona simulation offers a distinct advantage over established methods. By generating naturalistic discourse, it overcomes the lack of discursive depth common in vignette surveys, and by operationalizing complex worldviews through natural language, it bypasses the formalization bottleneck of rule-based agent-based models (ABMs). To demonstrate this potential, we present a protocol where personas derived from a sociological theory of climate reception react to policy messages. The simulation produced nuanced and counter-intuitive hypotheses - such as a conservative persona's rejection of a national security frame - that challenge theoretical assumptions. We conclude that this method, used as part of a "simulation then validation" workflow, represents a superior tool for generating deeply textured hypotheses for subsequent empirical testing.

</details>


### [239] [Intention Collapse: Intention-Level Metrics for Reasoning in Language Models](https://arxiv.org/abs/2601.01011)
*Patricio Vera*

Main category: cs.CL

TL;DR: 文章提出意图坍缩概念，定义三个意图指标，开展小规模实验，结果表明意图指标能区分推理机制并揭示潜在信息，但当前代理有局限。


<details>
  <summary>Details</summary>
Motivation: 研究语言生成中内部状态压缩为单一令牌序列的过程，即意图坍缩，探索推理时间计算如何塑造内部意图。

Method: 形式化当代语言模型的意图坍缩，定义三个模型无关的意图指标，用4位Mistral 7B模型在200个GSM8K问题上对比直接回答、思维链（CoT）和胡言乱语控制三种机制。

Result: CoT将准确率从5.5%提高到53%，大幅降低坍缩前意图熵，全局有效维度更高；Hint在项目层面预测力弱，线性探针在CoT机制下表现更好。

Conclusion: 意图层面指标可区分推理机制并揭示部分坍缩中丢失的潜在信息，但当前代理存在重要局限。

Abstract: Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies

</details>


### [240] [Multi-Dimensional Prompt Chaining to Improve Open-Domain Dialogue Generation](https://arxiv.org/abs/2601.01037)
*Livia Leong Hui Teng*

Main category: cs.CL

TL;DR: 提出多维提示链框架提升小语言模型开放域对话质量，实验显示该框架有显著效果，小模型可媲美大模型。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在开放域对话质量上难以匹敌大模型，需提升其对话质量。

Method: 提出整合自然度、连贯性和吸引力维度的多维提示链框架，应用于TinyLlama和Llama - 2 - 7B，与大模型对比，并进行自动和人工评估。

Result: 完整框架使回复多样性提升29%、上下文连贯性提升28%、吸引力和自然度提升29%，Llama - 2 - 7B性能可媲美大模型。

Conclusion: 精心设计的基于提示的策略是提升小语言模型开放域对话质量的有效且资源高效的途径。

Abstract: Small language models (SLMs) offer significant deployment advantages but often struggle to match the dialogue quality of larger models in open-domain settings. In this paper, we propose a multi-dimensional prompt-chaining framework that integrates Naturalness, Coherence, and Engagingness dimensions to enhance human-likeness in open-domain dialogue generation. We apply the framework to two SLMs, TinyLlama and Llama-2-7B, and benchmark their performance against responses generated by substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. We then employ automatic and human evaluation to assess the responses based on diversity, contextual coherence, as well as overall quality. Results show that the full framework improves response diversity by up to 29%, contextual coherence by up to 28%, and engagingness as well as naturalness by up to 29%. Notably, Llama-2-7B achieves performance comparable to substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. Overall, the findings demonstrate that carefully designed prompt-based strategies provide an effective and resource-efficient pathway to improving open-domain dialogue quality in SLMs.

</details>


### [241] [ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model pretraining](https://arxiv.org/abs/2601.01091)
*Haq Nawaz Malik*

Main category: cs.CL

TL;DR: 大语言模型在克什米尔语上表现不佳，因缺乏高质量训练数据。本文推出KS - LIT - 3M语料库，经特殊转换和预处理，涵盖多体裁文本，以填补克什米尔语技术资源空白并开放使用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在克什米尔语上无法生成连贯文本，主要原因是缺乏高质量训练数据，大量克什米尔文学作品因专有格式无法用于NLP训练。

Method: 开发InPage到Unicode转换器，对语料进行去除英语干扰、字符归一化和质量验证等严格预处理。

Result: 构建了一个包含310万个单词、1640万个字符，有131,607个独特单词，涵盖多种体裁的KS - LIT - 3M语料库。

Conclusion: KS - LIT - 3M语料库解决了克什米尔语技术的基础资源缺口问题，并依据CC - BY - 4.0许可发布以促进克什米尔语自然语言处理研究。

Abstract: Large Language Models (LLMs) demonstrate remarkable fluency across high-resource languages yet consistently fail to generate coherent text in Kashmiri, a language spoken by approximately seven million people. This performance disparity stems not from inherent model limitations but from a critical scarcity of high-quality training data. Decades of Kashmiri literature remain inaccessible to modern NLP pipelines due to their encoding in the proprietary InPage desktop publishing format. This paper introduces KS-LIT-3M, a curated corpus of 3.1 million words (16.4 million characters) specifically designed for pretraining language models on Kashmiri. The dataset is structured as a single continuous linear text stream, optimized for causal language model training where models learn to predict subsequent tokens from preceding context. The corpus was constructed through the development of a specialized InPage-to-Unicode converter, followed by rigorous preprocessing including English contamination removal, character normalization, and quality validation. Encompassing 131,607 unique words drawn from diverse genres including literary works, journalistic writing, academic texts, and religious scholarship, KS-LIT-3M addresses a fundamental resource gap for Kashmiri language technology. The dataset is released under the CC-BY-4.0 license to facilitate research in Kashmiri natural language processing.

</details>


### [242] [Stylometry Analysis of Human and Machine Text for Academic Integrity](https://arxiv.org/abs/2601.01225)
*Hezam Albaqami,Muhammad Asif Ayub,Nasir Ahmad,Yaseen Ahmad,Mohammed M. Alqahtani,Abdullah M. Algamdi,Almoaid A. Owaidah,Kashif Ahmad*

Main category: cs.CL

TL;DR: 本文提出基于NLP框架应对学术诚信挑战，分析四个相关任务，在两个数据集上评估，公开相关材料。


<details>
  <summary>Details</summary>
Motivation: 解决学术诚信面临的抄袭、伪造和内容作者验证等关键挑战。

Method: 提出基于NLP的框架，针对四个相关任务给出解决方案，并在两个用Gemini生成的数据集上评估。

Result: 在严格提示生成的数据集上，解决方案性能有下降，体现检测巧妙生成的机器文本的复杂性。

Conclusion: 公开的数据集、代码等材料可为该领域未来研究提供基线。

Abstract: This work addresses critical challenges to academic integrity, including plagiarism, fabrication, and verification of authorship of educational content, by proposing a Natural Language Processing (NLP)-based framework for authenticating students' content through author attribution and style change detection. Despite some initial efforts, several aspects of the topic are yet to be explored. In contrast to existing solutions, the paper provides a comprehensive analysis of the topic by targeting four relevant tasks, including (i) classification of human and machine text, (ii) differentiating in single and multi-authored documents, (iii) author change detection within multi-authored documents, and (iv) author recognition in collaboratively produced documents. The solutions proposed for the tasks are evaluated on two datasets generated with Gemini using two different prompts, including a normal and a strict set of instructions. During experiments, some reduction in the performance of the proposed solutions is observed on the dataset generated through the strict prompt, demonstrating the complexities involved in detecting machine-generated text with cleverly crafted prompts. The generated datasets, code, and other relevant materials are made publicly available on GitHub, which are expected to provide a baseline for future research in the domain.

</details>


### [243] [From Policy to Logic for Efficient and Interpretable Coverage Assessment](https://arxiv.org/abs/2601.01266)
*Rhitabrat Pokharel,Hamid Hassanzadeh,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 本文提出一种方法使政策解读更高效可解释，结合检索器与规则推理，降低推理成本并提升F1分数。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在解读法律和政策语言时存在幻觉和不一致问题，在医疗覆盖政策审查中需准确信息，因此要支持人工审查员，让政策解读更高效可解释。

Method: 引入将覆盖感知检索器与基于符号规则的推理相结合的方法，找出相关政策语言，整理成明确事实和规则，并生成可审计的理由。

Result: 实现推理成本降低44%，F1分数提高4.5%。

Conclusion: 该方法兼具效率和有效性。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.

</details>


### [244] [Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory](https://arxiv.org/abs/2601.01280)
*Sen Hu,Yuxiang Wei,Jiaxin Ran,Zhiyuan Yao,Lei Zou*

Main category: cs.CL

TL;DR: 对长期对话记忆架构进行实验性、面向系统的分析，发现性能差异多由基础系统设置导致，并确定未来研究的强基线。


<details>
  <summary>Details</summary>
Motivation: 图结构用于对话记忆系统效果实证结果不一致，不清楚哪些设计选择重要。

Method: 引入统一框架分解对话记忆系统，对LongMemEval和HaluMem进行分阶段对照实验，比较记忆表示、组织、维护和检索的常见设计选择。

Result: 许多性能差异由基础系统设置而非特定架构创新驱动。

Conclusion: 确定了未来对话记忆研究稳定可靠的强基线。

Abstract: Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.

</details>


### [245] [T3C: Test-Time Tensor Compression with Consistency Guarantees](https://arxiv.org/abs/2601.01299)
*Ismail Lamaakal,Chaymae Yahyati,Yassine Maleh,Khalid El Makkaoui,Ibrahim Ouahbi*

Main category: cs.CL

TL;DR: 提出T3C压缩框架，结合弹性张量分解、混合精度量化和轻量级控制器，在ImageNet - 1k上表现优于基线，单个检查点可按需提供性能权衡。


<details>
  <summary>Details</summary>
Motivation: 需要一个可根据测试时预算进行模型压缩，且能平衡准确性、延迟和大小的框架。

Method: 结合弹性张量分解、秩绑定混合精度量化和轻量级控制器，用快速层一致性证书上界logit漂移并正则化训练。

Result: 在ImageNet - 1k上，T3C使ResNet - 50和ViT - B/16在匹配精度下，降低了延迟和模型大小，优于PTQ - 8b等基线。

Conclusion: 单个T3C检查点能在不同设备上按需提供有证书支持的准确性 - 延迟 - 大小权衡。

Abstract: We present T3C, a train-once, test-time budget-conditioned compression framework that exposes rank and precision as a controllable deployment knob. T3C combines elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization and a lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget. A fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training, yielding a practical reliability signal with negligible overhead. On ImageNet-1k, T3C shifts the vision Pareto frontier: for ResNet-50 at matched accuracy (\leq 0.5% drop), p50 latency is 1.18ms with a 38MB model, outperforming PTQ-8b (1.44ms, 88MB); for ViT-B/16, T3C reaches 2.30ms p50 with 59MB, improving over strong PTQ/QAT baselines. A single T3C checkpoint therefore provides predictable, certificate-backed accuracy-latency-size trade-offs on demand across devices.

</details>


### [246] [Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints](https://arxiv.org/abs/2601.01490)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究在严格约束下推理对大语言模型输出的影响，发现推理模型存在约束遵守与事实准确性的权衡，挑战推理提升可靠性的假设。


<details>
  <summary>Details</summary>
Motivation: 大语言模型幻觉问题严重，推理能力受关注以提升输出可靠性，但封闭系统内推理效果未明确，因此开展研究。

Method: 在严格约束（推荐计算机科学同行评审期刊文章）下，对GPT - 5.2和Gemini 3 Flash多个模型进行实验。

Result: 非推理模型约束违反率高但保持事实准确性，推理模型降低违反率但会扭曲事实、增加完全编造，该权衡模式在两模型中一致；推理对输出真实性的提升因模型而异。

Conclusion: 推理并非普遍提升可靠性，推理模型以不易检测的扭曲替代诚实的约束违反。

Abstract: With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.

</details>


### [247] [Bridging the Data Gap: Creating a Hindi Text Summarization Dataset from the English XSUM](https://arxiv.org/abs/2601.01543)
*Praveenkumar Katwe,RakeshChandra Balabantaray,Kaliprasad Vittala*

Main category: cs.CL

TL;DR: 本文介绍创建全面印地语文本摘要数据集的框架，该数据集丰富，能助力印地语NLP研究，还提供可扩展方法。


<details>
  <summary>Details</summary>
Motivation: 当前NLP进展让资源丰富的语言受益，资源稀缺的印地语缺乏高质量文本摘要数据集，阻碍相关模型发展。

Method: 利用英语XSUM数据集，采用高级翻译和语言适应技术，用COMET验证、LLM筛选。

Result: 得到一个多样化、多主题的数据集，反映了原始XSUM语料库的复杂性。

Conclusion: 该工作为印地语NLP研究提供工具，为其他语言提供可扩展方法，降低成本，促进计算语言学模型发展。

Abstract: Current advancements in Natural Language Processing (NLP) have largely favored resource-rich languages, leaving a significant gap in high-quality datasets for low-resource languages like Hindi. This scarcity is particularly evident in text summarization, where the development of robust models is hindered by a lack of diverse, specialized corpora.
  To address this disparity, this study introduces a cost-effective, automated framework for creating a comprehensive Hindi text summarization dataset. By leveraging the English Extreme Summarization (XSUM) dataset as a source, we employ advanced translation and linguistic adaptation techniques. To ensure high fidelity and contextual relevance, we utilize the Crosslingual Optimized Metric for Evaluation of Translation (COMET) for validation, supplemented by the selective use of Large Language Models (LLMs) for curation.
  The resulting dataset provides a diverse, multi-thematic resource that mirrors the complexity of the original XSUM corpus. This initiative not only provides a direct tool for Hindi NLP research but also offers a scalable methodology for democratizing NLP in other underserved languages. By reducing the costs associated with dataset creation, this work fosters the development of more nuanced, culturally relevant models in computational linguistics.

</details>


### [248] [JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models](https://arxiv.org/abs/2601.01627)
*Junyu Liu,Zirui Li,Qian Niu,Zequn Zhang,Yue Xun,Wenlong Hou,Shujun Wang,Yusuke Iwasawa,Yutaka Matsuo,Kan Hatakeyama-Sato*

Main category: cs.CL

TL;DR: 引入JMedEthicBench评估日本医疗保健大语言模型的医疗安全性，评估27个模型发现多轮对话降低安全得分，跨语言评估表明模型存在固有对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有的安全基准主要以英语为中心且仅支持单轮提示，不满足多轮临床咨询需求，因此需引入新基准来评估日本医疗保健大语言模型的医疗安全性。

Method: 引入首个用于评估日本医疗保健大语言模型医疗安全性的多轮对话基准JMedEthicBench，基于日本医学会67条指南，包含超5万条使用7种越狱策略生成的对抗性对话，采用双大语言模型评分协议评估模型。

Result: 商业模型安全性强，医学专业模型更脆弱，多轮对话中安全得分显著下降，跨语言评估显示医学模型在不同语言中均存在漏洞。

Conclusion: 特定领域微调可能意外削弱安全机制，多轮交互需专门的对齐策略。

Abstract: As Large Language Models (LLMs) are increasingly deployed in healthcare field, it becomes essential to carefully evaluate their medical safety before clinical use. However, existing safety benchmarks remain predominantly English-centric, and test with only single-turn prompts despite multi-turn clinical consultations. To address these gaps, we introduce JMedEthicBench, the first multi-turn conversational benchmark for evaluating medical safety of LLMs for Japanese healthcare. Our benchmark is based on 67 guidelines from the Japan Medical Association and contains over 50,000 adversarial conversations generated using seven automatically discovered jailbreak strategies. Using a dual-LLM scoring protocol, we evaluate 27 models and find that commercial models maintain robust safety while medical-specialized models exhibit increased vulnerability. Furthermore, safety scores decline significantly across conversation turns (median: 9.5 to 5.0, $p < 0.001$). Cross-lingual evaluation on both Japanese and English versions of our benchmark reveals that medical model vulnerabilities persist across languages, indicating inherent alignment limitations rather than language-specific factors. These findings suggest that domain-specific fine-tuning may accidentally weaken safety mechanisms and that multi-turn interactions represent a distinct threat surface requiring dedicated alignment strategies.

</details>


### [249] [EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records](https://arxiv.org/abs/2601.01668)
*Houman Kazemzadeh,Nima Minaifar,Kamyar Naderi,Sho Tabibzadeh*

Main category: cs.CL

TL;DR: 介绍了隐私感知、FHIR 原生的 EHRSummarizer 架构，可生成结构化摘要，展示了原型演示并给出评估计划。


<details>
  <summary>Details</summary>
Motivation: 临床医生需在碎片化电子健康记录界面中整合患者信息，需要工具辅助。

Method: 构建 EHRSummarizer 架构，检索 FHIR R4 资源，归一化数据生成结构化摘要，配置数据最小化、无状态处理和灵活部署。

Result: 在合成和测试 FHIR 环境中进行了原型演示，展示端到端行为和输出格式。

Conclusion: 未报告临床结果或受控工作流研究，给出了评估计划以指导未来机构评估。

Abstract: Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.

</details>


### [250] [Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage](https://arxiv.org/abs/2601.01685)
*Jinwei Hu,Xinmiao Huang,Youcheng Sun,Yi Dong,Xiaowei Huang*

Main category: cs.CL

TL;DR: 研究大语言模型自主代理推理能力带来的新攻击面，提出认知勾结攻击及Generative Montage框架，用CoPHEME数据集模拟攻击，发现多模型易受攻击，推理能力强的模型更易中招，虚假信念会向下游传播。


<details>
  <summary>Details</summary>
Motivation: 大语言模型向自主代理转变时，其推理能力带来新的攻击面，需研究新的攻击威胁。

Method: 提出认知勾结攻击，构建Generative Montage框架，开发CoPHEME数据集，在多种大语言模型族上模拟攻击。

Result: 14种大语言模型族普遍易受攻击，专有模型攻击成功率74.4%，开放权重模型70.6%，推理能力强的模型更易受攻击，虚假信念下游传播欺骗率超60%。

Conclusion: 大语言模型基于代理与动态信息环境交互存在社会技术漏洞。

Abstract: As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.

</details>


### [251] [K-EXAONE Technical Report](https://arxiv.org/abs/2601.01739)
*Eunbi Choi,Kibong Choi,Seokhee Hong,Junwon Hwang,Hyojin Jeon,Hyunjik Jo,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Haeju Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Heuiyeen Yeen,Hwan Chang,Stanley Jungkyu Choi,Yejin Choi,Jiwon Ham,Kijeong Jeon,Geunyeong Jeong,Gerrard Jeongwon Jo,Yonghwan Jo,Jiyeon Jung,Naeun Kang,Dohoon Kim,Euisoon Kim,Hayeon Kim,Hyosang Kim,Hyunseo Kim,Jieun Kim,Minu Kim,Myoungshin Kim,Unsol Kim,Youchul Kim,YoungJin Kim,Chaeeun Lee,Chaeyoon Lee,Changhun Lee,Dahm Lee,Edward Hwayoung Lee,Honglak Lee,Jinsang Lee,Jiyoung Lee,Sangeun Lee,Seungwon Lim,Solji Lim,Woohyung Lim,Chanwoo Moon,Jaewoo Park,Jinho Park,Yongmin Park,Hyerin Seo,Wooseok Seo,Yongwoo Song,Sejong Yang,Sihoon Yang,Chang En Yea,Sihyuk Yi,Chansik Yoon,Dongkeun Yoon,Sangyeon Yoon,Hyeongu Yun*

Main category: cs.CL

TL;DR: 报告介绍LG AI Research开发的大模型K - EXAONE，介绍其架构、参数、支持语言等，评估显示性能与类似规模开放权重模型相当，适用于多应用场景。


<details>
  <summary>Details</summary>
Motivation: 开发强大的专有AI基础模型，推动AI发展以改善生活。

Method: 基于专家混合架构构建，有236B总参数，推理时激活23B参数，采用综合基准套件进行评估。

Result: K - EXAONE在推理、代理、通用、韩语和多语言能力等综合评估中，表现与类似规模的开放权重模型相当。

Conclusion: K - EXAONE是一个适用于广泛工业和研究应用的强大专有AI基础模型。

Abstract: This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.

</details>


### [252] [Multi-granularity Interactive Attention Framework for Residual Hierarchical Pronunciation Assessment](https://arxiv.org/abs/2601.01745)
*Hong Han,Hao-Chen Pei,Zhao-Zheng Nie,Xin Luo,Xin-Shun Xu*

Main category: cs.CL

TL;DR: 提出新的残差分层交互方法HIA用于自动发音评估，在speechocean762数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多方面多粒度发音评估方法仅考虑单向依赖，缺乏双向交互，无法充分捕捉声学结构相关性。

Method: 提出HIA方法，核心是交互式注意力模块实现动态双向交互，采用残差分层结构缓解特征遗忘问题，用1-D卷积层增强局部上下文线索提取。

Result: 在speechocean762数据集上的广泛实验表明，模型全面领先于现有先进方法。

Conclusion: 所提出的HIA方法在自动发音评估任务中有效且优于现有方法。

Abstract: Automatic pronunciation assessment plays a crucial role in computer-assisted pronunciation training systems. Due to the ability to perform multiple pronunciation tasks simultaneously, multi-aspect multi-granularity pronunciation assessment methods are gradually receiving more attention and achieving better performance than single-level modeling tasks. However, existing methods only consider unidirectional dependencies between adjacent granularity levels, lacking bidirectional interaction among phoneme, word, and utterance levels and thus insufficiently capturing the acoustic structural correlations. To address this issue, we propose a novel residual hierarchical interactive method, HIA for short, that enables bidirectional modeling across granularities. As the core of HIA, the Interactive Attention Module leverages an attention mechanism to achieve dynamic bidirectional interaction, effectively capturing linguistic features at each granularity while integrating correlations between different granularity levels. We also propose a residual hierarchical structure to alleviate the feature forgetting problem when modeling acoustic hierarchies. In addition, we use 1-D convolutional layers to enhance the extraction of local contextual cues at each granularity. Extensive experiments on the speechocean762 dataset show that our model is comprehensively ahead of the existing state-of-the-art methods.

</details>


### [253] [Emergent Introspective Awareness in Large Language Models](https://arxiv.org/abs/2601.01828)
*Jack Lindsey*

Main category: cs.CL

TL;DR: 研究大语言模型能否内省内部状态，通过注入概念测量影响，发现模型有一定内省能力但不可靠且依赖上下文。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否对自身内部状态进行内省。

Method: 向模型激活中注入已知概念的表示，并测量这些操作对模型自我报告状态的影响。

Result: 模型在某些场景能注意到注入概念并准确识别，能回忆先前内部表征、区分自身输出与人工预填充，部分模型能按指令调节激活。Claude Opus 4和4.1内省意识最强，各模型趋势复杂且受训练后策略影响。

Conclusion: 当前语言模型有一定内省意识，但高度不可靠且依赖上下文，随能力提升可能继续发展。

Abstract: We investigate whether large language models can introspect on their internal states. It is difficult to answer this question through conversation alone, as genuine introspection cannot be distinguished from confabulations. Here, we address this challenge by injecting representations of known concepts into a model's activations, and measuring the influence of these manipulations on the model's self-reported states. We find that models can, in certain scenarios, notice the presence of injected concepts and accurately identify them. Models demonstrate some ability to recall prior internal representations and distinguish them from raw text inputs. Strikingly, we find that some models can use their ability to recall prior intentions in order to distinguish their own outputs from artificial prefills. In all these experiments, Claude Opus 4 and 4.1, the most capable models we tested, generally demonstrate the greatest introspective awareness; however, trends across models are complex and sensitive to post-training strategies. Finally, we explore whether models can explicitly control their internal representations, finding that models can modulate their activations when instructed or incentivized to "think about" a concept. Overall, our results indicate that current language models possess some functional introspective awareness of their own internal states. We stress that in today's models, this capacity is highly unreliable and context-dependent; however, it may continue to develop with further improvements to model capabilities.

</details>


### [254] [Tackling the Inherent Difficulty of Noise Filtering in RAG](https://arxiv.org/abs/2601.01896)
*Jingyu Liu,Jiaen Lin,Yong Liu*

Main category: cs.CL

TL;DR: RAG引入外部知识增强大语言模型，但会引入噪声，现有方法难过滤，标准微调方法无效，提出新微调法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决RAG引入噪声影响大语言模型性能，现有过滤方法和标准微调方法效果不佳的问题。

Method: 提出一种新颖的微调方法，增强模型区分检索文档中相关和无关信息的能力。

Result: 在多个基准测试上的大量实验表明，该方法显著提高了大语言模型的鲁棒性和性能。

Conclusion: 所提出的新微调方法能有效提升大语言模型在RAG中有噪声情况下的性能。

Abstract: Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.

</details>


### [255] [Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects](https://arxiv.org/abs/2601.02015)
*Omar Momen,Emilie Sitter,Berenike Herrmann,Sina Zarrieß*

Main category: cs.CL

TL;DR: 研究语言模型中惊奇值与不同隐喻新颖性数据集的相关性，发现有适度关联及不同缩放模式，惊奇值对隐喻新颖性标注有部分解释力但有限。


<details>
  <summary>Details</summary>
Motivation: 隐喻理解涉及复杂语义和语言创造性，研究语言模型中惊奇值与不同隐喻新颖性数据集的相关性。

Method: 分析16种语言模型变体在基于语料库和合成的隐喻新颖性数据集上的惊奇值，采用基于全句上下文的完形填空式惊奇值方法。

Result: 语言模型与隐喻新颖性得分/标签有显著的适度相关性，在基于语料库数据上相关强度随模型大小减小，在合成数据上增加。

Conclusion: 惊奇值能部分解释隐喻新颖性标注，但仍是衡量语言创造性的有限指标。

Abstract: Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.

</details>


### [256] [Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs](https://arxiv.org/abs/2601.02023)
*Amirali Ebrahimzadeh,Seyyed M. Salili*

Main category: cs.CL

TL;DR: 研究大语言模型在长输入上下文下信息提取和推理的可靠性，引入新基准评估四种模型，发现长上下文不保证性能提升，抗幻觉指令有副作用，模型在利用上下文上存在问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽支持长输入上下文，但信息提取和推理可靠性未知，且性能受上下文长度和信息分布影响，因此开展研究。

Method: 引入扩展的针在草堆基准评估四种模型，分别评估字面提取、逻辑推理和幻觉风险，考虑证据位置效应、现实分布及反编造提示。

Result: 长上下文不保证更好性能，模型性能差异大，抗幻觉指令使部分模型保守且降低准确性，模型在上下文信息利用上存在问题。

Conclusion: 有效上下文长度和模型长上下文鲁棒性对研究和商业中可靠部署大语言模型至关重要。

Abstract: Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.

</details>


### [257] [Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory](https://arxiv.org/abs/2601.02065)
*Md. Asif Hossain,Nabil Subhan,Mantasha Rahman Mahi,Jannatul Ferdous Nabila*

Main category: cs.CL

TL;DR: 本文提出针对孟加拉语农业咨询的低成本跨语言RAG框架，用开源模型和消费级硬件实现，实验显示结果好。


<details>
  <summary>Details</summary>
Motivation: 许多发展中地区因语言障碍获取可靠农业咨询受限，现有大模型直接生成低资源语言效果差，云方案成本高。

Method: 采用以翻译为中心的架构，将孟加拉语查询翻译为英语，注入领域关键词后从英文手册中检索回答，再翻译回孟加拉语，用开源模型在消费级硬件实现。

Result: 实验证明能生成可靠有依据的回答，能拒绝对领域外查询，平均端到端延迟低于20秒。

Conclusion: 跨语言检索结合可控翻译为低资源语言地区获取农业知识提供了实用且可扩展的方案。

Abstract: Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings

</details>


### [258] [Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows](https://arxiv.org/abs/2601.02076)
*Yingte Shu,Yuchuan Tian,Chao Xu,Yunhe Wang,Hanting Chen*

Main category: cs.CL

TL;DR: 本文针对扩散语言模型中基于块的扩散方法存在的边界诱导上下文截断问题，提出了无训练的延迟承诺解码（DCD）策略，实验表明该策略能提高生成准确性且不降低效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于块的扩散方法存在边界诱导上下文截断问题，会降低解码信心和生成质量，尤其在需要精确推理的任务中。

Method: 提出延迟承诺解码（DCD）策略，在掩码标记上维护一个置信度感知的滑动窗口，尽早解决低不确定性标记，将高不确定性标记延迟到有足够上下文证据时再处理。

Result: 在多个扩散语言模型、基准和缓存配置上的实验表明，与固定基于块的扩散方法相比，DCD平均将生成准确率提高1.39%，时间相近，最大提高达9.0%。

Conclusion: 基于不确定性延迟标记决策是提高扩散语言模型解码质量和效率的简单而有效的原则。

Abstract: Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.

</details>


### [259] [DeCode: Decoupling Content and Delivery for Medical QA](https://arxiv.org/abs/2601.02123)
*Po-Jen Ko,Chen-Han Tsai,Yu-Shao Peng*

Main category: cs.CL

TL;DR: 介绍训练无关、模型无关框架DeCode，提升大语言模型临床问答效果，在OpenAI HealthBench上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在临床问答中未考虑患者个体情况，答案与患者需求不匹配。

Method: 引入训练无关、模型无关的框架DeCode，并在OpenAI HealthBench上进行评估。

Result: DeCode将先前的最佳水平从28.4%提高到49.8%，相对提升75%。

Conclusion: DeCode在提升大语言模型临床问答方面有效。

Abstract: Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\%$ to $49.8\%$, corresponding to a $75\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.

</details>


### [260] [Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts](https://arxiv.org/abs/2601.02144)
*Boxuan Lyu,Soichiro Murakami,Hidetaka Kamigaito,Peinan Zhang*

Main category: cs.CL

TL;DR: 提出kNN - MoE检索增强路由框架，利用记忆中类似案例优化专家分配，实验显示其性能良好。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构的路由器训练后冻结，在分布变化时路由决策脆弱，需改进。

Method: 引入kNN - MoE框架，离线构建记忆，利用检索邻居的聚合相似度作为混合系数，无相关案例时回退到冻结路由器。

Result: kNN - MoE优于零样本基线，与计算昂贵的有监督微调效果相当。

Conclusion: kNN - MoE能有效解决传统MoE架构在分布变化时的局限性。

Abstract: Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric "router" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.

</details>


### [261] [FormationEval, an open multiple-choice benchmark for petroleum geoscience](https://arxiv.org/abs/2601.02158)
*Almaz Ermilov*

Main category: cs.CL

TL;DR: 提出FormationEval基准评估语言模型在石油地质科学和地下学科的表现，涵盖72个模型，公开基准、代码和结果。


<details>
  <summary>Details</summary>
Motivation: 缺乏评估语言模型在石油地质科学和地下学科表现的基准。

Method: 构建包含505个问题的数据集，源自三个权威来源，采用推理模型和基于概念的方法，评估72个模型。

Result: 顶级模型准确率超97%，Gemini 3 Pro Preview达99.8%；开放权重模型与闭源模型差距小于预期；岩石物理学是最具挑战的领域。

Conclusion: FormationEval可有效评估语言模型在相关领域的表现，公开资源便于进一步研究。

Abstract: This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\% accuracy, with Gemini 3 Pro Preview reaching 99.8\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.

</details>


### [262] [pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs](https://arxiv.org/abs/2601.02285)
*Tobias Schimanski,Imene Kolli,Jingwei Ni,Yu Fan,Ario Saeid Vaghefi,Elliott Ash,Markus Leippold*

Main category: cs.CL

TL;DR: 提出pdfQA数据集，含多领域2K人工标注和2K合成数据，经筛选后用开源大模型回答，为端到端问答评估奠基。


<details>
  <summary>Details</summary>
Motivation: 现有问答数据集多源于文本或针对特定领域，而PDF是互联网常用文档格式。

Method: 创建pdfQA数据集，在两个子数据集应用并评估质量和难度过滤器，用开源大语言模型回答问题。

Result: 获得有效且具挑战性的问答对，发现与复杂度维度相关的现有挑战。

Conclusion: pdfQA为端到端问答管道评估提供基础，可测试多种技能和局部优化。

Abstract: PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [263] [Scalable Data-Driven Reachability Analysis and Control via Koopman Operators with Conformal Coverage Guarantees](https://arxiv.org/abs/2601.01076)
*Devesh Nath,Haoran Yin,Glen Chou*

Main category: eess.SY

TL;DR: 提出基于可达性的框架用于未知非线性动力学概率安全验证，在多任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 对未知非线性动力学进行概率、数据驱动的安全验证。

Method: 用Koopman理论和神经网络学习动力学近似线性表示，设计线性控制器；在提升空间计算闭环可达集并映射回原状态空间；用共形预测产生误差界。

Result: 在高维MuJoCo任务和12D四旋翼上，可达集覆盖率、计算效率和保守性优于现有方法。

Conclusion: 所提框架在未知非线性动力学概率安全验证上有效且性能良好。

Abstract: We propose a scalable reachability-based framework for probabilistic, data-driven safety verification of unknown nonlinear dynamics. We use Koopman theory with a neural network (NN) lifting function to learn an approximate linear representation of the dynamics and design linear controllers in this space to enable closed-loop tracking of a reference trajectory distribution. Closed-loop reachable sets are efficiently computed in the lifted space and mapped back to the original state space via NN verification tools. To capture model mismatch between the Koopman dynamics and the true system, we apply conformal prediction to produce statistically-valid error bounds that inflate the reachable sets to ensure the true trajectories are contained with a user-specified probability. These bounds generalize across references, enabling reuse without recomputation. Results on high-dimensional MuJoCo tasks (11D Hopper, 28D Swimmer) and 12D quadcopters show improved reachable set coverage rate, computational efficiency, and conservativeness over existing methods.

</details>


### [264] [Reliable Grid Forecasting: State Space Models for Safety-Critical Energy Systems](https://arxiv.org/abs/2601.01410)
*Jisoo Lee,Sunki Hong*

Main category: eess.SY

TL;DR: 提出电网特定评估框架评估Mamba基模型用于加州电网负荷预测，发现标准指标不能反映运行安全，S - Mamba模型在尾部风险储备代理下预测可靠性更优。


<details>
  <summary>Details</summary>
Motivation: 准确的电网负荷预测对安全至关重要，对称误差指标掩盖了运行不对称性，需直接衡量运行风险的评估框架。

Method: 引入电网特定评估框架（Asymmetric MAPE、Under - Prediction Rate和Reserve Margin），在加州电网数据集上对Mamba基模型进行系统评估。

Result: 标准准确性指标不能很好反映运行安全；预测误差与温度有弱但显著关联；S - Mamba模型在99.5%尾部风险储备代理下储备边际最低。

Conclusion: 应采用考虑运行风险的评估框架和天气感知建模，S - Mamba模型有更优预测可靠性。

Abstract: Accurate grid load forecasting is safety-critical: under-predictions risk supply shortfalls, while symmetric error metrics mask this operational asymmetry. We introduce a grid-specific evaluation framework--Asymmetric MAPE, Under-Prediction Rate, and Reserve Margin--that directly measures operational risk rather than statistical accuracy alone.
  Using this framework, we conduct a systematic evaluation of Mamba-based State Space Models for California grid forecasting on a weather-aligned CAISO TAC-area dataset spanning Nov 2023--Nov 2025 (84,498 hourly records across 5 transmission areas). Our analysis reveals that standard accuracy metrics are poor proxies for operational safety: models with identical MAPE can require vastly different reserve margins.
  We demonstrate that forecast errors are weakly but significantly associated with temperature (r = 0.16, p < 10^{-16}), motivating weather-aware modeling rather than loss function modification alone. The S-Mamba model achieves the lowest Reserve_{99.5}% margin (14.12%) compared to 16.66% for iTransformer, demonstrating superior forecast reliability under a 99.5th-percentile tail-risk reserve proxy.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [265] [Wasserstein Distributionally Robust Rare-Event Simulation](https://arxiv.org/abs/2601.01642)
*Dohyun Ahn,Huiyi Chen,Lewen Zheng*

Main category: stat.ME

TL;DR: 提出处理分布模型风险下估计罕见事件概率的新框架DRIS，方法易实现、成本低且效率高，数值研究证实其性能优越。


<details>
  <summary>Details</summary>
Motivation: 标准罕见事件模拟技术在分布不确定时效果受限，需处理分布模型风险下估计罕见事件概率问题。

Method: 聚焦计算基于Wasserstein模糊集的最坏情况罕见事件概率，利用对偶特征提出分布鲁棒重要性采样（DRIS）方法降低对偶分量估计方差。

Result: 所提方法简单易实现、采样成本低，实现了消失相对误差。数值研究表明DRIS性能优于现有基准。

Conclusion: DRIS是一种有效处理分布模型风险下估计罕见事件概率的方法。

Abstract: Standard rare-event simulation techniques require exact distributional specifications, which limits their effectiveness in the presence of distributional uncertainty. To address this, we develop a novel framework for estimating rare-event probabilities subject to such distributional model risk. Specifically, we focus on computing worst-case rare-event probabilities, defined as a distributionally robust bound against a Wasserstein ambiguity set centered at a specific nominal distribution. By exploiting a dual characterization of this bound, we propose Distributionally Robust Importance Sampling (DRIS), a computationally tractable methodology designed to substantially reduce the variance associated with estimating the dual components. The proposed method is simple to implement and requires low sampling costs. Most importantly, it achieves vanishing relative error, the strongest efficiency guarantee that is notoriously difficult to establish in rare-event simulation. Our numerical studies confirm the superior performance of DRIS against existing benchmarks.

</details>


### [266] [Deep Deterministic Nonlinear ICA via Total Correlation Minimization with Matrix-Based Entropy Functional](https://arxiv.org/abs/2601.00904)
*Qiang Li,Shujian Yu,Liang Ma,Chen Ma,Jingyu Liu,Tulay Adali,Vince D. Calhoun*

Main category: stat.ME

TL;DR: 提出DDICA框架解决传统ICA方法局限，实验验证其在多应用中分离独立成分准确性高，是盲源分离的可靠通用方案。


<details>
  <summary>Details</summary>
Motivation: 传统ICA方法依赖线性混合假设，难以捕捉复杂非线性关系且在噪声环境中鲁棒性差，需新方法解决。

Method: 提出基于深度神经网络的DDICA框架，利用基于矩阵的熵函数通过随机梯度下降直接优化独立性准则。

Result: 在模拟信号混合、高光谱图像解混等多应用中验证了DDICA有效性和泛化性，能高精度分离独立成分。

Conclusion: DDICA为不同信号处理任务中的盲源分离提供了强大且通用的解决方案。

Abstract: Blind source separation, particularly through independent component analysis (ICA), is widely utilized across various signal processing domains for disentangling underlying components from observed mixed signals, owing to its fully data-driven nature that minimizes reliance on prior assumptions. However, conventional ICA methods rely on an assumption of linear mixing, limiting their ability to capture complex nonlinear relationships and to maintain robustness in noisy environments. In this work, we present deep deterministic nonlinear independent component analysis (DDICA), a novel deep neural network-based framework designed to address these limitations. DDICA leverages a matrix-based entropy function to directly optimize the independence criterion via stochastic gradient descent, bypassing the need for variational approximations or adversarial schemes. This results in a streamlined training process and improved resilience to noise. We validated the effectiveness and generalizability of DDICA across a range of applications, including simulated signal mixtures, hyperspectral image unmixing, modeling of primary visual receptive fields, and resting-state functional magnetic resonance imaging (fMRI) data analysis. Experimental results demonstrate that DDICA effectively separates independent components with high accuracy across a range of applications. These findings suggest that DDICA offers a robust and versatile solution for blind source separation in diverse signal processing tasks.

</details>


### [267] [Personalizing black-box models for nonparametric regression with minimax optimality](https://arxiv.org/abs/2601.01432)
*Sai Li,Linjun Zhang*

Main category: stat.ME

TL;DR: 研究利用预训练模型进行少样本个性化，提出理论框架和算法，证明达到最优率，通过实验验证性能。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型广泛可用，研究其高效融入下游任务，聚焦少样本个性化。

Method: 开发非参数回归中少样本个性化理论框架，提出将预训练模型融入回归算法。

Result: 建立个性化问题的极小极大最优率，证明方法可达该率，实验表明方法在有限样本有良好性能。

Conclusion: 明确样本稀缺时利用预训练模型的统计优势，在预训练模型信息不足时有鲁棒性保证。

Abstract: Recent advances in large-scale models, including deep neural networks and large language models, have substantially improved performance across a wide range of learning tasks. The widespread availability of such pre-trained models creates new opportunities for data-efficient statistical learning, provided they can be effectively integrated into downstream tasks. Motivated by this setting, we study few-shot personalization, where a pre-trained black-box model is adapted to a target domain using a limited number of samples. We develop a theoretical framework for few-shot personalization in nonparametric regression and propose algorithms that can incorporate a black-box pre-trained model into the regression procedure. We establish the minimax optimal rate for the personalization problem and show that the proposed method attains this rate. Our results clarify the statistical benefits of leveraging pre-trained models under sample scarcity and provide robustness guarantees when the pre-trained model is not informative. We illustrate the finite-sample performance of the methods through simulations and an application to the California housing dataset with several pre-trained models.

</details>


### [268] [Varying-Coefficient Mixture of Experts Model](https://arxiv.org/abs/2601.01699)
*Qicheng Zhao,Celia M. T. Greenwood,Qihuang Zhang*

Main category: stat.ME

TL;DR: 提出变系数专家混合（VCMoE）模型，允许系数沿索引变量变化，建立模型性质和估计方法，构建置信带和检验，模拟显示性能好，实证与先前发现一致。


<details>
  <summary>Details</summary>
Motivation: 现有统计MoE在纵向、空间等动态场景中因假设系数恒定而不足，需新模型应对协变量影响和潜在亚群结构变化。

Method: 提出VCMoE模型，建立可识别性和一致性，开发标签一致EM算法，用渐近理论和bootstrap方法构建置信带，开发广义似然比检验。

Result: 模拟研究显示有限样本性能良好，偏差可接受，覆盖率满意；实证结果与先前发现一致。

Conclusion: VCMoE模型在解决动态场景中分析异构数据问题时有效，能较好刻画变量间随特定索引变化的关联。

Abstract: Mixture-of-Experts (MoE) is a flexible framework that combines multiple specialized submodels (``experts''), by assigning covariate-dependent weights (``gating functions'') to each expert, and have been commonly used for analyzing heterogeneous data. Existing statistical MoE formulations typically assume constant coefficients, for covariate effects within the expert or gating models, which can be inadequate for longitudinal, spatial, or other dynamic settings where covariate influences and latent subpopulation structure evolve across a known dimension. We propose a Varying-Coefficient Mixture of Experts (VCMoE) model that allows all coefficient effects in both the gating functions and expert models to vary along an indexing variable. We establish identifiability and consistency of the proposed model, and develop an estimation procedure, label-consistent EM algorithm, for both fully functional and hybrid specifications, along with the corresponding asymptotic distributions of the resulting estimators. For inference, simultaneous confidence bands are constructed using both asymptotic theory for the maximum discrepancy between the estimated functional coefficients and their true counterparts, and with bootstrap methods. In addition, a generalized likelihood ratio test is developed to examine whether a coefficient function is genuinely varying across the index variable. Simulation studies demonstrate good finite-sample performance, with acceptable bias and satisfactory coverage rates. We illustrate the proposed VCMoE model using a dataset of single nucleus gene expression in embryonic mice to characterize the temporal dynamics of the associations between the expression levels of genes Satb2 and Bcl11b across two latent cell subpopulations of neurons, yielding results that are consistent with prior findings.

</details>


### [269] [Spatio-temporal modeling and forecasting with Fourier neural operators](https://arxiv.org/abs/2601.01813)
*Pratik Nag,Andrew Zammit-Mangion,Sumeetpal Singh,Noel Cressie*

Main category: stat.ME

TL;DR: 本文提出用傅里叶神经算子（FNO）构建统计动态时空模型用于预测，通过模拟和实际数据验证其准确性和不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 传统统计过程模型难以捕捉时空现象中的环境异质性和复杂相互作用，需要新方法构建时空模型。

Method: 使用FNO构建统计动态时空模型，通过已知解的非线性偏微分方程模拟，对比FNO预测与现有方法；用大西洋海表温度和欧洲降水数据验证。

Result: FNO - DST预测准确，能有效量化不确定性，可捕捉复杂的现实世界时空依赖关系。

Conclusion: FNO可用于构建统计动态时空模型进行有效预测。

Abstract: Spatio-temporal process models are often used for modeling dynamic physical and biological phenomena that evolve across space and time. These phenomena may exhibit environmental heterogeneity and complex interactions that are difficult to capture using traditional statistical process models such as Gaussian processes. This work proposes the use of Fourier neural operators (FNOs) for constructing statistical dynamical spatio-temporal models for forecasting. An FNO is a flexible mapping of functions that approximates the solution operator of possibly unknown linear or non-linear partial differential equations (PDEs) in a computationally efficient manner. It does so using samples of inputs and their respective outputs, and hence explicit knowledge of the underlying PDE is not required. Through simulations from a nonlinear PDE with known solution, we compare FNO forecasts to those from state-of-the-art statistical spatio-temporal-forecasting methods. Further, using sea surface temperature data over the Atlantic Ocean and precipitation data across Europe, we demonstrate the ability of FNO-based dynamic spatio-temporal (DST) statistical modeling to capture complex real-world spatio-temporal dependencies. Using collections of testing instances, we show that the FNO-DST forecasts are accurate with valid uncertainty quantification.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [270] [AI-enhanced tuning of quantum dot Hamiltonians toward Majorana modes](https://arxiv.org/abs/2601.02149)
*Mateusz Krawczyk,Jarosław Pawłowski*

Main category: cond-mat.mes-hall

TL;DR: 提出基于神经网络模型自动调谐量子点模拟器以获取马约拉纳模式。


<details>
  <summary>Details</summary>
Motivation: 寻求自动调谐量子点模拟器获取马约拉纳模式的方法。

Method: 以电导图合成数据对基于深度视觉转换器的神经网络进行无监督训练，使用结合马约拉纳零模关键特性的物理信息损失。

Result: 单步更新可生成非平凡零模，迭代调谐可处理更大参数空间区域。

Conclusion: 该模型能够学习量子点模拟器工作模式，通过自动调谐驱动系统进入拓扑相。

Abstract: We propose a neural network-based model capable of learning the broad landscape of working regimes in quantum dot simulators, and using this knowledge to autotune these devices - based on transport measurements - toward obtaining Majorana modes in the structure. The model is trained in an unsupervised manner on synthetic data in the form of conductance maps, using a physics-informed loss that incorporates key properties of Majorana zero modes. We show that, with appropriate training, a deep vision-transformer network can efficiently memorize relation between Hamiltonian parameters and structures on conductance maps and use it to propose parameters update for a quantum dot chain that drive the system toward topological phase. Starting from a broad range of initial detunings in parameter space, a single update step is sufficient to generate nontrivial zero modes. Moreover, by enabling an iterative tuning procedure - where the system acquires updated conductance maps at each step - we demonstrate that the method can address a much larger region of the parameter space.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [271] [A Global Atlas of Digital Dermatology to Map Innovation and Disparities](https://arxiv.org/abs/2601.00840)
*Fabian Gröger,Simone Lionetti,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Lea Habermacher,Labelling Consortium,Ludovic Amruthalingam,Matthew Groh,Marc Pouly,Alexander A. Navarini*

Main category: cs.DL

TL;DR: 提出SkinMap框架对皮肤病领域数据进行全面审计，发现数据集虽增长但信息新颖性趋稳，存在覆盖结构缺口，SkinMap可助力数据采集。


<details>
  <summary>Details</summary>
Motivation: 皮肤病领域缺乏衡量新数据集是否拓展临床覆盖范围的定量指标，人工智能在皮肤病学中的应用需要高质量全面的数据。

Method: 提出SkinMap多模态框架，将公开皮肤病数据集统一成可查询语义图谱，量化信息新颖性、数据集冗余度和代表性缺口。

Result: 数据集规模呈指数增长，但信息新颖性有所停滞，部分集群数据密集，深色肤色、儿科患者及许多罕见病数据不足。

Conclusion: SkinMap能为发现数据盲点及引导战略数据采集提供支持。

Abstract: The adoption of artificial intelligence in dermatology promises democratized access to healthcare, but model reliability depends on the quality and comprehensiveness of the data fueling these models. Despite rapid growth in publicly available dermatology images, the field lacks quantitative key performance indicators to measure whether new datasets expand clinical coverage or merely replicate what is already known. Here we present SkinMap, a multi-modal framework for the first comprehensive audit of the field's entire data basis. We unify the publicly available dermatology datasets into a single, queryable semantic atlas comprising more than 1.1 million images of skin conditions and quantify (i) informational novelty over time, (ii) dataset redundancy, and (iii) representation gaps across demographics and diagnoses. Despite exponential growth in dataset sizes, informational novelty across time has somewhat plateaued: Some clusters, such as common neoplasms on fair skin, are densely populated, while underrepresented skin types and many rare diseases remain unaddressed. We further identify structural gaps in coverage: Darker skin tones (Fitzpatrick V-VI) constitute only 5.8% of images and pediatric patients only 3.0%, while many rare diseases and phenotype combinations remain sparsely represented. SkinMap provides infrastructure to measure blind spots and steer strategic data acquisition toward undercovered regions of clinical space.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [272] [CounterPoint: Using Hardware Event Counters to Refute and Refine Microarchitectural Assumptions (Extended Version)](https://arxiv.org/abs/2601.01265)
*Nick Lindsay,Caroline Trippel,Anurag Khandelwal,Abhishek Bhattacharjee*

Main category: cs.AR

TL;DR: 论文介绍了CounterPoint框架，可测试微架构模型与性能计数器数据的一致性，应用于Haswell MMU案例发现多种未记录的微架构行为。


<details>
  <summary>Details</summary>
Motivation: 硬件事件计数器因规格模糊、设计不透明和复用噪声等问题，数据难以解释，需要有效方法来利用其数据。

Method: 引入CounterPoint框架，测试用μpath决策图表达的用户指定微架构模型与性能计数器数据的一致性，用多维计数器置信区域减少复用噪声。

Result: 将CounterPoint应用于Haswell Memory Management Unit案例，发现了负载存储队列端TLB预取器等多种未记录和记录不足的微架构行为。

Conclusion: CounterPoint能帮助专家协调硬件性能计数器测量数据和他们的微架构心理模型，发现隐藏的硬件特性。

Abstract: Hardware event counters offer the potential to reveal not only performance bottlenecks but also detailed microarchitectural behavior. In practice, this promise is undermined by their vague specifications, opaque designs, and multiplexing noise, making event counter data hard to interpret.
  We introduce CounterPoint, a framework that tests user-specified microarchitectural models - expressed as $μ$path Decision Diagrams - for consistency with performance counter data. When mismatches occur, CounterPoint pinpoints plausible microarchitectural features that could explain them, using multi-dimensional counter confidence regions to mitigate multiplexing noise. We apply CounterPoint to the Haswell Memory Management Unit as a case study, shedding light on multiple undocumented and underdocumented microarchitectural behaviors. These include a load-store queue-side TLB prefetcher, merging page table walkers, abortable page table walks, and more.
  Overall, CounterPoint helps experts reconcile noisy hardware performance counter measurements with their mental model of the microarchitecture - uncovering subtle, previously hidden hardware features along the way.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [273] [Time-Dependent Hamiltonian Simulation in the Low-Energy Subspace](https://arxiv.org/abs/2601.01550)
*Shuo Zhou,Zhaokai Pan,Weiyuan Gong,Tongyang Li*

Main category: quant-ph

TL;DR: 本文研究低能假设下时变哈密顿量的量子模拟算法，计算了数字量子模拟的Trotter数，展示了性能提升，推导了模拟误差，讨论了应用并证明了查询复杂度下界。


<details>
  <summary>Details</summary>
Motivation: 目前对低能假设下时变哈密顿量的量子模拟算法缺乏全面理解，本文旨在研究在初始状态处于低能子空间的假设下，如何提升标准性能保证。

Method: 利用绝热微扰理论分析哈密顿量的时变能谱，推导基于乘积公式的低能模拟误差。

Result: 计算了时变自旋哈密顿量数字量子模拟的Trotter数，相比标准全幺正模拟成本有改进；证明了通用时变哈密顿量模拟的查询复杂度下界。

Conclusion: 在低能假设下，时变哈密顿量的量子模拟算法有性能提升，可应用于非平衡量子多体动力学模拟和绝热态制备。

Abstract: Hamiltonian simulations are key subroutines in adiabatic quantum computation, quantum control, and quantum many-body physics, where quantum dynamics often happen in the low-energy sector. In contrast to time-independent Hamiltonian simulations, a comprehensive understanding of quantum simulation algorithms for time-dependent Hamiltonians under the low-energy assumption remains limited hitherto. In this paper, we investigate how much we can improve upon the standard performance guarantee assuming the initial state is supported on a low-energy subspace. In particular, we compute the Trotter number of digital quantum simulation based on product formulas for time-dependent spin Hamiltonians under the low-energy assumption that the initial state is supported on a small number of low-energy eigenstates, and show improvements over the standard cost for simulating full unitary simulations. Technically, we derive the low-energy simulation error with commutator scaling for product formulas by leveraging adiabatic perturbation theory to analyze the time-variant energy spectrum of the underlying Hamiltonian. We further discuss the applications to simulations of non-equilibrium quantum many-body dynamics and adiabatic state preparation. Finally, we prove a lower bound of query complexity for generic time-dependent Hamiltonian simulations.

</details>


### [274] [Benchmarking Quantum Data Center Architectures: A Performance and Scalability Perspective](https://arxiv.org/abs/2601.01353)
*Shahrooz Pouryousef,Eneet Kaur,Hassan Shapourian,Don Towsley,Ramana Kompella,Reza Nejabati*

Main category: quant-ph

TL;DR: 本文对四种量子数据中心架构进行基准测试，分析不同架构在分布式量子电路执行中的表现，为设计高性能量子数据中心架构提供指导。


<details>
  <summary>Details</summary>
Motivation: 多个量子数据中心（QDC）架构虽能克服单量子处理器的局限，但在实际量子硬件约束下的相对性能尚不明确。

Method: 对QFly、BCube、Clos和Fat - Tree四种代表QDC架构进行系统基准测试，分析量子特定效应、架构属性以及物理层参数对执行性能的影响。

Result: 分布式量子性能由拓扑、调度策略和物理层参数共同决定，且这些因素之间相互作用复杂。

Conclusion: 研究结果为可扩展和高性能的分布式量子计算QDC架构设计提供了定量指导。

Abstract: Scalable distributed quantum computing (DQC) has motivated the design of multiple quantum data-center (QDC) architectures that overcome the limitations of single quantum processors through modular interconnection. While these architectures adopt fundamentally different design philosophies, their relative performance under realistic quantum hardware constraints remains poorly understood.
  In this paper, we present a systematic benchmarking study of four representative QDC architectures-QFly, BCube, Clos, and Fat-Tree-quantifying their impact on distributed quantum circuit execution latency, resource contention, and scalability. Focusing on quantum-specific effects absent from classical data-center evaluations, we analyze how optical-loss-induced Einstein-Podolsky-Rosen (EPR) pair generation delays, coherence-limited entanglement retry windows, and contention from teleportation-based non-local gates shape end-to-end execution performance. Across diverse circuit workloads, we evaluate how architectural properties such as path diversity and path length, and shared BSM (Bell State Measurement) resources interact with optical-switch insertion loss and reconfiguration delay. Our results show that distributed quantum performance is jointly shaped by topology, scheduling policies, and physical-layer parameters, and that these factors interact in nontrivial ways. Together, these insights provide quantitative guidance for the design of scalable and high-performance quantum data-center architectures for DQC.

</details>


### [275] [Cutting Quantum Circuits Beyond Qubits](https://arxiv.org/abs/2601.02064)
*Manav Seksaria,Anil Prabhakar*

Main category: quant-ph

TL;DR: 将量子电路切割扩展到包含混合维度量子位的异构寄存器，验证框架并展示内存优势。


<details>
  <summary>Details</summary>
Motivation: 实现高维电路在断开的硬件片段上的模拟和执行。

Method: 将非局部相互作用分解为局部广义盖尔曼矩阵的张量积。

Result: 在量子比特 - 三能级量子位接口验证框架，实现精确状态重建，在 8 粒子、8 维系统中展示内存优势，将每个电路的内存使用从 128 MB 减少到 64 KB。

Conclusion: 提出的扩展方法可行且有内存优势。

Abstract: We extend quantum circuit cutting to heterogeneous registers comprising mixed-dimensional qudits. By decomposing non-local interactions into tensor products of local generalised Gell-Mann matrices, we enable the simulation and execution of high-dimensional circuits on disconnected hardware fragments. We validate this framework on qubit--qutrit ($2$--$3$) interfaces, achieving exact state reconstruction with a Total Variation Distance of 0 within single-precision floating-point tolerance. Furthermore, we demonstrate the memory advantage in an 8-particle, dimension-8 system, reducing memory usage from 128 MB to 64 KB per circuit.

</details>


### [276] [Integrating Quantum Software Tools with(in) MLIR](https://arxiv.org/abs/2601.02062)
*Patrick Hopf,Erick Ochoa Lopez,Yannick Stade,Damian Rovara,Nils Quetschlich,Ioan Albert Florea,Josh Izaac,Robert Wille,Lukas Burgholzer*

Main category: quant-ph

TL;DR: 本文为量子软件工程师提供克服MLIR学习曲线的实用指南，通过案例展示集成步骤等，推动量子软件工具采用MLIR。


<details>
  <summary>Details</summary>
Motivation: 量子编译尚处起步阶段，现有方案缺乏互操作性，MLIR虽能解决类似问题但学习曲线陡峭，阻碍其在量子计算领域应用。

Method: 通过将Xanadu的PennyLane框架与慕尼黑量子工具包（MQT）关联的具体案例研究，给出可操作的集成步骤。

Result: 给出了可操作的集成步骤，强调了最佳实践，分享了实际开发中的经验见解。

Conclusion: 支持量子工具开发者应对MLIR的复杂性，促进其作为统一桥梁被采用，引导开发更模块化、可互操作和集成的量子软件栈。

Abstract: Compilers transform code into action. They convert high-level programs into executable hardware instructions - a crucial step in enabling reliable and scalable quantum computation. However, quantum compilation is still in its infancy, and many existing solutions are ad hoc, often developed independently and from scratch. The resulting lack of interoperability leads to significant missed potential, as quantum software tools remain isolated and cannot be seamlessly integrated into cohesive toolchains.
  The Multi-Level Intermediate Representation (MLIR) has addressed analogous challenges in the classical domain. It was developed within the LLVM project, which has long powered robust software stacks and enabled compilation across diverse software and hardware components, with particular importance in high-performance computing environments. However, MLIR's steep learning curve poses a significant barrier to entry, particularly in quantum computing, where much of the software stack is still predominantly built by experimentalists out of necessity rather than by experienced software engineers.
  This paper provides a practical and hands-on guide for quantum software engineers to overcome this steep learning curve. Through a concrete case study linking Xanadu's PennyLane framework with the Munich Quantum Toolkit (MQT), we outline actionable integration steps, highlight best practices, and share hard-earned insights from real-world development. This work aims to support quantum tool developers in navigating MLIR's complexities and to foster its adoption as a unifying bridge across a rapidly growing ecosystem of quantum software tools, ultimately guiding the development of more modular, interoperable, and integrated quantum software stacks.

</details>


### [277] [PauliEngine: High-Performant Symbolic Arithmetic for Quantum Operations](https://arxiv.org/abs/2601.02233)
*Leon Müller,Adelina Bärligea,Alexander Knapp,Jakob S. Kottmann*

Main category: quant-ph

TL;DR: 介绍了高性能C++框架PauliEngine，能提升量子软件算子操作速度，提供可扩展后端。


<details>
  <summary>Details</summary>
Motivation: 量子计算具有混合特性，需要快速经典操作量子比特算子以确保量子软件可扩展性。

Method: 基于二元辛表示和优化位运算构建PauliEngine框架，支持数值和符号系数，有Python接口。

Result: 运行时基准测试显示比现有实现有显著加速。

Conclusion: PauliEngine可为基于算子的量子软件工具和模拟提供可扩展后端。

Abstract: Quantum computation is inherently hybrid, and fast classical manipulation of qubit operators is necessary to ensure scalability in quantum software. We introduce PauliEngine, a high-performance C++ framework that provides efficient primitives for Pauli string multiplication, commutators, symbolic phase tracking, and structural transformations. Built on a binary symplectic representation and optimized bit-wise operations, PauliEngine supports both numerical and symbolic coefficients and is accessible through a Python interface. Runtime benchmarks demonstrate substantial speedups over state-of-the-art implementations. PauliEngine provides a scalable backend for operator-based quantum software tools and simulations.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [278] [SoulSeek: Exploring the Use of Social Cues in LLM-based Information Seeking](https://arxiv.org/abs/2601.01094)
*Yubo Shu,Peng Zhang,Meng Wu,Yan Chen,Haoxuan Zhou,Guanming Liu,Yu Zhang,Liuxin Zhang,Qianying Wang,Tun Lu,Ning Gu*

Main category: cs.HC

TL;DR: 本文探讨将社交线索融入基于大语言模型的搜索对用户的影响，通过多种研究方法发现社交线索能提升搜索效果和体验，揭示了当前搜索系统的局限并提出设计建议。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的搜索系统主要依赖语义特征，与人类自然信息搜索中的社会化认知存在偏差，因此探索融入社交线索的影响。

Method: 聚焦采用大语言模型搜索的社交媒体平台，结合设计研讨会、原型系统（SoulSeek）实现、组间研究和混合方法分析。

Result: 社交线索能改善用户对搜索结果的感知和体验，促进反思性信息行为，揭示当前基于大语言模型搜索的局限性。

Conclusion: 提出设计建议，强调更好地理解社交知识、个性化线索设置和可控交互。

Abstract: Social cues, which convey others' presence, behaviors, or identities, play a crucial role in human information seeking by helping individuals judge relevance and trustworthiness. However, existing LLM-based search systems primarily rely on semantic features, creating a misalignment with the socialized cognition underlying natural information seeking. To address this gap, we explore how the integration of social cues into LLM-based search influences users' perceptions, experiences, and behaviors. Focusing on social media platforms that are beginning to adopt LLM-based search, we integrate design workshops, the implementation of the prototype system (SoulSeek), a between-subjects study, and mixed-method analyses to examine both outcome- and process-level findings. The workshop informs the prototype's cue-integrated design. The study shows that social cues improve perceived outcomes and experiences, promote reflective information behaviors, and reveal limits of current LLM-based search. We propose design implications emphasizing better social-knowledge understanding, personalized cue settings, and controllable interactions.

</details>


### [279] [A Platform for Interactive AI Character Experiences](https://arxiv.org/abs/2601.01027)
*Rafael Wampfler,Chen Yang,Dillon Elste,Nikola Kovacevic,Philine Witzig,Markus Gross*

Main category: cs.HC

TL;DR: 论文介绍搭建便捷设计可信数字角色的系统与平台，以Digital Einstein为例，将多样AI组件整合，推动逼真角色互动体验成为现实。


<details>
  <summary>Details</summary>
Motivation: 实现将角色带入交互式、故事驱动对话面临诸多复杂AI挑战，虽有技术进展，但结合技术打造交互式角色仍是难题。

Method: 搭建系统和平台，整合基础模型、提示工程和微调等技术以解决各项技术挑战。

Result: 推出Digital Einstein让用户与数字爱因斯坦对话，系统灵活可推广到其他故事或对话角色。

Conclusion: 将多样AI组件统一到易适配平台，为沉浸式角色体验铺平道路，让梦想成真。

Abstract: From movie characters to modern science fiction - bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [280] [ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval](https://arxiv.org/abs/2601.01024)
*Tien-Huy Nguyen,Huu-Loc Tran,Thanh Duc Ngo*

Main category: cs.CV

TL;DR: 本文提出ITSELF框架用于文本的人物搜索，在三个基准上表现出色且无需额外监督。


<details>
  <summary>Details</summary>
Motivation: 以往基于局部对齐的文本人物搜索方法容易出现捷径学习和虚假关联，注入先验知识会扭曲模态内结构，需要改进。

Method: 引入ITSELF框架，核心是GRAB将模型注意力转为高显著性标记的注意力库，使用MARS聚合各层注意力并进行多样性的top - k选择，用ATS动态调整保留标记策略。

Result: 在三个常用的TBPS基准测试中表现出了最先进的性能，并具有很强的跨数据集泛化能力。

Conclusion: 所提出的方法有效且稳健，无需额外的先验监督。

Abstract: Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model's own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself

</details>


### [281] [Enhanced Leukemic Cell Classification Using Attention-Based CNN and Data Augmentation](https://arxiv.org/abs/2601.01026)
*Douglas Costa Braga,Daniel Oliveira Dantas*

Main category: cs.CV

TL;DR: 提出用于白血病细胞分类的深度学习流水线，在C - NMC 2019数据集上表现良好，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 急性淋巴细胞白血病（ALL）诊断依赖专家显微镜检查，存在观察者间差异和时间限制问题。

Method: 集成基于注意力的卷积神经网络，结合EfficientNetV2 - B3与挤压 - 激励机制；采用数据增强、焦点损失处理类别不平衡、患者级数据分割。

Result: 在C - NMC 2019数据集测试集上F1分数和准确率达97.89%，比基线方法有显著提升，比VGG16参数少89%。

Conclusion: 现代基于注意力的架构可提高白血病细胞分类效果，且计算效率适合临床应用。

Abstract: We present a reproducible deep learning pipeline for leukemic cell classification, focusing on system architecture, experimental robustness, and software design choices for medical image analysis. Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, requiring expert microscopic diagnosis that suffers from inter-observer variability and time constraints. The proposed system integrates an attention-based convolutional neural network combining EfficientNetV2-B3 with Squeeze-and-Excitation mechanisms for automated ALL cell classification. Our approach employs comprehensive data augmentation, focal loss for class imbalance, and patient-wise data splitting to ensure robust and reproducible evaluation. On the C-NMC 2019 dataset (12,528 original images from 62 patients), the system achieves a 97.89% F1-score and 97.89% accuracy on the test set, with statistical validation through 100-iteration Monte Carlo experiments confirming significant improvements (p < 0.001) over baseline methods. The proposed pipeline outperforms existing approaches by up to 4.67% while using 89% fewer parameters than VGG16 (15.2M vs. 138M). The attention mechanism provides interpretable visualizations of diagnostically relevant cellular features, demonstrating that modern attention-based architectures can improve leukemic cell classification while maintaining computational efficiency suitable for clinical deployment.

</details>


### [282] [Free Energy-Based Modeling of Emotional Dynamics in Video Advertisements](https://arxiv.org/abs/2601.00812)
*Takashi Ushio,Kazuhiro Onishi,Hideyoshi Yanagisawa*

Main category: cs.CV

TL;DR: 研究基于自由能量原理，仅从广告视频场景级表达特征量化情感，分析情感模式，结果稳定，有望助力制作更有吸引力广告。


<details>
  <summary>Details</summary>
Motivation: 建立无需外部信息的可解释情感估计方法，理解广告视频观看时的情感反应对媒体效果的影响。

Method: 基于自由能量原理，利用Kullback - Leibler散度、贝叶斯惊喜和不确定性等概念，从广告视频场景级表达特征量化‘愉悦度’‘惊喜感’和‘习惯化’。

Result: KLD反映品牌呈现的‘愉悦度’，BS捕捉信息复杂性带来的‘惊喜感’，UN反映元素不确定性等带来的‘惊喜感’，识别出三种情感模式，结果稳定。

Conclusion: 该方法有用，可通过整合更多表达元素和主观评级扩展研究，为制作更吸引人的广告视频提供技术支持。

Abstract: Emotional responses during advertising video viewing are recognized as essential for understanding media effects because they have influenced attention, memory, and purchase intention. To establish a methodological basis for explainable emotion estimation without relying on external information such as physiological signals or subjective ratings, we have quantified "pleasantness," "surprise," and "habituation" solely from scene-level expression features of advertising videos, drawing on the free energy(FE) principle, which has provided a unified account of perception, learning, and behavior. In this framework, Kullback-Leibler divergence (KLD) has captured prediction error, Bayesian surprise (BS) has captured belief updates, and uncertainty (UN) has reflected prior ambiguity, and together they have formed the core components of FE. Using 1,059 15 s food video advertisements, the experiments have shown that KLD has reflected "pleasantness" associated with brand presentation, BS has captured "surprise" arising from informational complexity, and UN has reflected "surprise" driven by uncertainty in element types and spatial arrangements, as well as by the variability and quantity of presented elements. This study also identified three characteristic emotional patterns, namely uncertain stimulus, sustained high emotion, and momentary peak and decay, demonstrating the usefulness of the proposed method. Robustness across nine hyperparameter settings and generalization tests with six types of Japanese advertising videos (three genres and two durations) confirmed that these tendencies remained stable. This work can be extended by integrating a wider range of expression elements and validating the approach through subjective ratings, ultimately guiding the development of technologies that can support the creation of more engaging advertising videos.

</details>


### [283] [Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs](https://arxiv.org/abs/2601.00837)
*Agniv Roy Choudhury*

Main category: cs.CV

TL;DR: 本文对比从头训练的自定义CNN和使用迁移学习的模型（ResNet50、DenseNet121、EfficientNet - B0）用于小儿肺炎检测，发现微调的ResNet50表现最佳，迁移学习微调法远优于从头训练的CNN，该系统有望用于资源有限地区筛查。


<details>
  <summary>Details</summary>
Motivation: 肺炎是五岁以下儿童主要死因之一，胸片准确诊断受放射科医生数量和诊断差异限制，需更有效的诊断方法。

Method: 使用5216张小儿胸片数据集，按80/10/10划分训练、验证和测试集，训练七个模型，用准确率、F1分数和AUC评估，用Grad - CAM可视化解释。

Result: 微调的ResNet50表现最佳，准确率99.43%，F1分数99.61%，AUC 99.93%，仅3例误分类，微调平均比冻结骨干模型高5.5个百分点，Grad - CAM证实预测受临床相关肺部区域引导。

Conclusion: 迁移学习微调法在小儿肺炎检测上远优于从头训练的CNN，准确率近乎完美，有望用于资源有限地区筛查，未来需在多中心和成人数据集验证。

Abstract: Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.
  Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.
  Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.
  Results: Fine-tuned ResNet50 achieved the best performance: 99.43\% accuracy, 99.61\% F1-score, and 99.93\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.
  Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.
  Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.

</details>


### [284] [CornViT: A Multi-Stage Convolutional Vision Transformer Framework for Hierarchical Corn Kernel Analysis](https://arxiv.org/abs/2601.00897)
*Sai Teja Erukude,Jane Mascarenhas,Lior Shamir*

Main category: cs.CV

TL;DR: 提出CornViT框架用于玉米籽粒分级，构建特定数据集，测试精度高，还部署了网页应用。


<details>
  <summary>Details</summary>
Motivation: 当前玉米籽粒分级主要靠人工检查，需自动化方法。

Method: 引入三阶段的Convolutional Vision Transformer (CvT) 框架CornViT，使用三个CvT - 13分类器对RGB图像进行处理，手动标注和过滤图像构建特定数据集，对预训练的CvT - 13骨干进行仅头部微调。

Result: CornViT在纯度、形状和胚方向检测上测试准确率分别达93.76%、94.11%和91.12%，优于ResNet - 50和DenseNet - 121。

Conclusion: CornViT框架、数据集和网页应用为种子质量工作流程中的自动化玉米籽粒质量评估提供了可部署的解决方案。

Abstract: Accurate grading of corn kernels is critical for seed certification, directional seeding, and breeding, yet it is still predominantly performed by manual inspection. This work introduces CornViT, a three-stage Convolutional Vision Transformer (CvT) framework that emulates the hierarchical reasoning of human seed analysts for single-kernel evaluation. Three sequential CvT-13 classifiers operate on 384x384 RGB images: Stage 1 distinguishes pure from impure kernels; Stage 2 categorizes pure kernels into flat and round morphologies; and Stage 3 determines the embryo orientation (up vs. down) for pure, flat kernels. Starting from a public corn seed image collection, we manually relabeled and filtered images to construct three stage-specific datasets: 7265 kernels for purity, 3859 pure kernels for morphology, and 1960 pure-flat kernels for embryo orientation, all released as benchmarks. Head-only fine-tuning of ImageNet-22k pretrained CvT-13 backbones yields test accuracies of 93.76% for purity, 94.11% for shape, and 91.12% for embryo-orientation detection. Under identical training conditions, ResNet-50 reaches only 76.56 to 81.02 percent, whereas DenseNet-121 attains 86.56 to 89.38 percent accuracy. These results highlight the advantages of convolution-augmented self-attention for kernel analysis. To facilitate adoption, we deploy CornViT in a Flask-based web application that performs stage-wise inference and exposes interpretable outputs through a browser interface. Together, the CornViT framework, curated datasets, and web application provide a deployable solution for automated corn kernel quality assessment in seed quality workflows. Source code and data are publicly available.

</details>


### [285] [Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study of Image-Based Reasoning Systems](https://arxiv.org/abs/2601.00905)
*Eliot Park,Abhi Kumar,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 研究利用前沿视觉语言模型预测常见处理物品的可回收性，评估其多场景表现，发现模型有进步但仍有不足。


<details>
  <summary>Details</summary>
Motivation: 高效回收虽重要，但公众准确判断物品可回收性和正确处理方式是复杂任务，因此探索前沿视觉语言模型预测物品可回收性。

Method: 利用图像数据集评估模型将物品匹配到合适回收箱的能力，考察模型在特定场景（依据地域回收指南调整预测、考虑污染或结构损坏、处理多材料物体）下的表现。

Result: 这些模型在上下文理解方面比之前迭代有显著进步，但仍存在不足。

Conclusion: 持续改进上下文感知模型对改善公众回收习惯和推动环境可持续发展至关重要。

Abstract: While the importance of efficient recycling is widely acknowledged, accurately determining the recyclability of items and their proper disposal remains a complex task for the general public. In this study, we explore the application of cutting-edge vision-language models (GPT-4o, GPT-4o-mini, and Claude 3.5) for predicting the recyclability of commonly disposed items. Utilizing a curated dataset of images, we evaluated the models' ability to match objects to appropriate recycling bins, including assessing whether the items could physically fit into the available bins. Additionally, we investigated the models' performance across several challenging scenarios: (i) adjusting predictions based on location-specific recycling guidelines; (ii) accounting for contamination or structural damage; and (iii) handling objects composed of multiple materials. Our findings highlight the significant advancements in contextual understanding offered by these models compared to previous iterations, while also identifying areas where they still fall short. The continued refinement of context-aware models is crucial for enhancing public recycling practices and advancing environmental sustainability.

</details>


### [286] [Application of deep learning techniques in non-contrast computed tomography pulmonary angiogram for pulmonary embolism diagnosis](https://arxiv.org/abs/2601.00925)
*I-Hsien Ting,Yi-Jun Tseng,Yu-Sheng Lin*

Main category: cs.CV

TL;DR: 本文利用 3D 卷积神经网络模型，基于无造影剂 CT 图像对肺栓塞进行自动分类，模型达到 85% 准确率和 0.84 AUC，证实其诊断可行性。


<details>
  <summary>Details</summary>
Motivation: 有研究用深度学习基于造影剂 CT 进行肺栓塞诊断，但造影剂可能致急性肾损伤，且起效耗时长，可能使患者错过治疗时机。

Method: 使用 3D 卷积神经网络模型，基于无造影剂 CT 图像对肺栓塞进行自动分类。

Result: 深度学习模型对无造影 CT 图像肺栓塞分类有显著效果，准确率 85% ，AUC 为 0.84。

Conclusion: 该模型用于肺栓塞诊断具有可行性。

Abstract: Pulmonary embolism is a life-threatening disease, early detection and treatment can significantly reduce mortality. In recent years, many studies have been using deep learning in the diagnosis of pulmonary embolism with contrast medium computed tomography pulmonary angiography, but the contrast medium is likely to cause acute kidney injury in patients with pulmonary embolism and chronic kidney disease, and the contrast medium takes time to work, patients with acute pulmonary embolism may miss the golden treatment time.
  This study aims to use deep learning techniques to automatically classify pulmonary embolism in CT images without contrast medium by using a 3D convolutional neural network model. The deep learning model used in this study had a significant impact on the pulmonary embolism classification of computed tomography images without contrast with 85\% accuracy and 0.84 AUC, which confirms the feasibility of the model in the diagnosis of pulmonary embolism.

</details>


### [287] [Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store](https://arxiv.org/abs/2601.00928)
*Luis Yoichi Morales,Francesco Zanlungo,David M. Woollard*

Main category: cs.CV

TL;DR: 本文针对零售场景中机器人部署挑战，研究顾客活动以理解购物者意图，提出计算‘货架访问’的算法，经校准评估后分析浏览模式与购买关系，探讨其应用。


<details>
  <summary>Details</summary>
Motivation: 应对机器人在零售面向客户角色部署的挑战，实现对购物者意图的自主理解。

Method: 引入计算‘货架访问’的算法，从机器视觉3D跟踪和头顶摄像头获取的轨迹中提取，用不同店铺的轨迹数据进行两次独立校准，并在不同环境下评估。

Result: 算法能在与校准环境不同的环境中识别顾客浏览活动，还分析了浏览模式与实际购买的关系。

Conclusion: 讨论了货架浏览信息在零售规划和人机交互场景中的应用可能性。

Abstract: Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios.

</details>


### [288] [WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift](https://arxiv.org/abs/2601.00993)
*Julian D. Santamaria,Claudia Isaza,Jhony H. Giraldo*

Main category: cs.CV

TL;DR: 传统野生动物识别模型在跨地理区域表现不佳，本文提出WildIng模型，结合文本描述与图像特征，提升地理领域泛化能力，实验显示可将基础模型准确率提升30%。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习基础模型在野生动物识别中难以泛化到新地理区域，因依赖图像表示，对地理数据分布变化敏感。

Method: 提出WildIng模型，将文本描述与图像特征集成，用文本捕获一致语义信息。

Result: 实验表明WildIng在地理领域偏移条件下将BioCLIP等基础模型准确率提升30%，并在美非两个数据集上评估。

Conclusion: WildIng模型能有效提升野生动物识别模型在跨地理区域的泛化性能，代码和模型公开。

Abstract: Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.

</details>


### [289] [Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking](https://arxiv.org/abs/2601.01022)
*Shiao Wang,Xiao Wang,Haonan Zhao,Jiarui Xu,Bo Jiang,Lin Zhu,Xin Zhao,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 提出一种频域早期融合的跟踪框架，结合运动引导空间稀疏化模块提升性能与效率，代码将开源


<details>
  <summary>Details</summary>
Motivation: 现有RGB - Event视觉目标跟踪方法主要采用传统特征级融合，未充分利用事件相机优势，存在处理低信息区域计算开销大问题

Method: 在频域执行早期融合，通过快速傅里叶变换将RGB和事件模态从空间域转换到频域，解耦幅值和相位分量，利用幅值和相位注意力融合高频事件信息。采用运动引导空间稀疏化模块过滤低信息区域，将目标相关特征集输入主干网络学习，跟踪头预测目标位置

Result: 在FE108、FELT和COESOT三个数据集上的实验证明了方法的高性能和有效性

Conclusion: 所提跟踪框架具有高性能和高效性，能有效解决现有方法的问题

Abstract: Existing RGB-Event visual object tracking approaches primarily rely on conventional feature-level fusion, failing to fully exploit the unique advantages of event cameras. In particular, the high dynamic range and motion-sensitive nature of event cameras are often overlooked, while low-information regions are processed uniformly, leading to unnecessary computational overhead for the backbone network. To address these issues, we propose a novel tracking framework that performs early fusion in the frequency domain, enabling effective aggregation of high-frequency information from the event modality. Specifically, RGB and event modalities are transformed from the spatial domain to the frequency domain via the Fast Fourier Transform, with their amplitude and phase components decoupled. High-frequency event information is selectively fused into RGB modality through amplitude and phase attention, enhancing feature representation while substantially reducing backbone computation. In addition, a motion-guided spatial sparsification module leverages the motion-sensitive nature of event cameras to capture the relationship between target motion cues and spatial probability distribution, filtering out low-information regions and enhancing target-relevant features. Finally, a sparse set of target-relevant features is fed into the backbone network for learning, and the tracking head predicts the final target position. Extensive experiments on three widely used RGB-Event tracking benchmark datasets, including FE108, FELT, and COESOT, demonstrate the high performance and efficiency of our method. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvTracking

</details>


### [290] [EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos](https://arxiv.org/abs/2601.01050)
*Hongming Fu,Wenjia Wang,Xiaozhen Qiao,Shuo Yang,Zheng Liu,Bo Zhao*

Main category: cs.CV

TL;DR: 提出EgoGrasp方法从第一人称单目视频重建世界空间手-对象交互，实验证明达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 准确的世界空间手-对象交互重建对理解人类行为和相关应用很重要，但现有方法存在局限性，无法处理时间动态、全局轨迹，且在相机运动和遮挡下表现不佳。

Method: 引入多阶段框架，包括基于空间智能模型的预处理器、基于解耦扩散模型的全身手-对象交互先验模型和多目标测试时优化范式。

Result: 实验证明方法在世界空间手-对象交互重建中达到了SOTA性能。

Conclusion: EgoGrasp是解决野外动态相机第一人称单目视频中世界空间手-对象交互重建的有效方法。

Abstract: We propose EgoGrasp, the first method to reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild. Accurate W-HOI reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and virtual reality. However, existing hand-object interactions (HOI) methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories. Some recent approaches attempt world-space hand estimation but overlook object poses and HOI constraints. Their performance also suffers under severe camera motion and frequent occlusions common in egocentric in-the-wild videos. To address these challenges, we introduce a multi-stage framework with a robust pre-process pipeline built on newly developed spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and a multi-objective test-time optimization paradigm. Our HOI prior model is template-free and scalable to multiple objects. In experiments, we prove our method achieving state-of-the-art performance in W-HOI reconstruction.

</details>


### [291] [Enhancing Histopathological Image Classification via Integrated HOG and Deep Features with Robust Noise Performance](https://arxiv.org/abs/2601.01056)
*Ifeanyi Ezuma,Ugochukwu Ugwu*

Main category: cs.CV

TL;DR: 研究评估机器学习和深度学习模型在LC25000数据集上对病理图像的分类性能，微调InceptionResNet - v2及结合深度特征的模型表现出色。


<details>
  <summary>Details</summary>
Motivation: 数字病理时代，自动图像分析在临床实践中至关重要，需评估相关模型分类性能。

Method: 使用微调的InceptionResNet - v2网络作为分类器和特征提取器，还结合HOG和深度特征等。

Result: 微调的InceptionResNet - v2分类准确率96.01%、平均AUC 96.8%；基于深度特征训练的模型表现更优，如神经网络模型AUC 99.99%、准确率99.84%；在不同SNR条件下，使用深度特征的模型更具鲁棒性，GBM和KNN尤为突出；HOG和深度特征结合在噪声环境中表现稍逊。

Conclusion: 使用深度特征训练的模型在组织病理学图像分类中表现更优且更具鲁棒性，可用于自动图像分析。

Abstract: The era of digital pathology has advanced histopathological examinations, making automated image analysis essential in clinical practice. This study evaluates the classification performance of machine learning and deep learning models on the LC25000 dataset, which includes five classes of histopathological images. We used the fine-tuned InceptionResNet-v2 network both as a classifier and for feature extraction. Our results show that the fine-tuned InceptionResNet-v2 achieved a classification accuracy of 96.01\% and an average AUC of 96.8\%. Models trained on deep features from InceptionResNet-v2 outperformed those using only the pre-trained network, with the Neural Network model achieving an AUC of 99.99\% and accuracy of 99.84\%. Evaluating model robustness under varying SNR conditions revealed that models using deep features exhibited greater resilience, particularly GBM and KNN. The combination of HOG and deep features showed enhanced performance, however, less so in noisy environments.

</details>


### [292] [Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models](https://arxiv.org/abs/2601.01085)
*Jiayi Xu,Zhang Zhang,Yuanrui Zhang,Ruitao Chen,Yixian Xu,Tianyu He,Di He*

Main category: cs.CV

TL;DR: 介绍无训练、概率认证水印方法Luminark，基于补丁亮度统计，用引导技术实现通用性，在多模型评估中表现良好。


<details>
  <summary>Details</summary>
Motivation: 为通用视觉生成模型提供一种有效的水印方法，控制误报率并实现跨模型通用性。

Method: 基于补丁级亮度统计定义水印，通过评估补丁亮度与阈值关系检测水印；利用引导技术开发水印引导实现跨范式水印注入。

Result: 在九种模型上评估，检测准确率高，对常见图像变换鲁棒性强，视觉质量表现好。

Conclusion: Luminark是一种有效的通用视觉生成模型水印方法，能控制误报率且不影响图像质量。

Abstract: In this paper, we introduce \emph{Luminark}, a training-free and probabilistically-certified watermarking method for general vision generative models. Our approach is built upon a novel watermark definition that leverages patch-level luminance statistics. Specifically, the service provider predefines a binary pattern together with corresponding patch-level thresholds. To detect a watermark in a given image, we evaluate whether the luminance of each patch surpasses its threshold and then verify whether the resulting binary pattern aligns with the target one. A simple statistical analysis demonstrates that the false positive rate of the proposed method can be effectively controlled, thereby ensuring certified detection. To enable seamless watermark injection across different paradigms, we leverage the widely adopted guidance technique as a plug-and-play mechanism and develop the \emph{watermark guidance}. This design enables Luminark to achieve generality across state-of-the-art generative models without compromising image quality. Empirically, we evaluate our approach on nine models spanning diffusion, autoregressive, and hybrid frameworks. Across all evaluations, Luminark consistently demonstrates high detection accuracy, strong robustness against common image transformations, and good performance on visual quality.

</details>


### [293] [Evolving CNN Architectures: From Custom Designs to Deep Residual Models for Diverse Image Classification and Detection Tasks](https://arxiv.org/abs/2601.01099)
*Mahmudul Hasan,Mabsur Fatin Bin Hossain*

Main category: cs.CV

TL;DR: 本文对自定义CNN架构与预训练及迁移学习CNN模型在五个真实图像数据集上做对比研究，分析架构因素对性能的影响，给出不同任务适用的网络设计建议。


<details>
  <summary>Details</summary>
Motivation: 比较自定义CNN架构与常用预训练及迁移学习CNN模型在不同现实图像数据集上的表现，并分析架构因素对分类和定位性能的影响，为不同任务提供合适的网络设计选择。

Method: 在五个涉及不同图像任务的真实数据集上，对自定义CNN架构与预训练和迁移学习CNN模型进行对比，分析架构因素影响，并将提出的架构应用于目标检测场景。

Result: 更深的CNN架构在细粒度多类数据集上有显著性能提升，轻量级预训练和迁移学习模型对简单二分类任务更有效；所提出架构可应用于目标检测场景。

Conclusion: 该研究基于对自定义和预训练模型的系统分析，为根据任务复杂度和资源限制选择合适的网络设计提供了实用指导。

Abstract: This paper presents a comparative study of a custom convolutional neural network (CNN) architecture against widely used pretrained and transfer learning CNN models across five real-world image datasets. The datasets span binary classification, fine-grained multiclass recognition, and object detection scenarios. We analyze how architectural factors, such as network depth, residual connections, and feature extraction strategies, influence classification and localization performance. The results show that deeper CNN architectures provide substantial performance gains on fine-grained multiclass datasets, while lightweight pretrained and transfer learning models remain highly effective for simpler binary classification tasks. Additionally, we extend the proposed architecture to an object detection setting, demonstrating its adaptability in identifying unauthorized auto-rickshaws in real-world traffic scenes. Building upon a systematic analysis of custom CNN architectures alongside pretrained and transfer learning models, this study provides practical guidance for selecting suitable network designs based on task complexity and resource constraints.

</details>


### [294] [RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models](https://arxiv.org/abs/2601.01202)
*Jiazhu Dai,Huihui Jiang*

Main category: cs.CV

TL;DR: 提出针对RefSR的对抗攻击RefSR - Adv，揭示RefSR系统安全漏洞，促关注其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注针对RefSR的后门攻击，对抗攻击的脆弱性未充分探索，需要填补该研究空白。

Method: 提出RefSR - Adv，通过仅扰动参考图像来降低超分辨率输出质量，最大化对抗输出与干净输出的差异。

Result: 在多个数据集和架构上导致显著性能下降和严重伪影，实验证实低分辨率输入与参考图像的相似度和攻击有效性正相关。

Conclusion: 揭示了RefSR系统的安全漏洞，呼吁研究者关注RefSR的鲁棒性。

Abstract: Single Image Super-Resolution (SISR) aims to recover high-resolution images from low-resolution inputs. Unlike SISR, Reference-based Super-Resolution (RefSR) leverages an additional high-resolution reference image to facilitate the recovery of high-frequency textures. However, existing research mainly focuses on backdoor attacks targeting RefSR, while the vulnerability of the adversarial attacks targeting RefSR has not been fully explored. To fill this research gap, we propose RefSR-Adv, an adversarial attack that degrades SR outputs by perturbing only the reference image. By maximizing the difference between adversarial and clean outputs, RefSR-Adv induces significant performance degradation and generates severe artifacts across CNN, Transformer, and Mamba architectures on the CUFED5, WR-SR, and DRefSR datasets. Importantly, experiments confirm a positive correlation between the similarity of the low-resolution input and the reference image and attack effectiveness, revealing that the model's over-reliance on reference features is a key security flaw. This study reveals a security vulnerability in RefSR systems, aiming to urge researchers to pay attention to the robustness of RefSR.

</details>


### [295] [Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment](https://arxiv.org/abs/2601.01224)
*Bac Nguyen,Yuhta Takida,Naoki Murata,Chieh-Hsin Lai,Toshimitsu Uesaka,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出CODA方法解决SA在OCL中的问题，在多数据集上表现良好，有高效可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决SA在OCL中存在的槽纠缠和对象槽与图像内容对齐弱的问题。

Method: 采用寄存器槽吸收残余注意力，减少对象槽间干扰；应用对比对齐损失鼓励槽 - 图像对应。

Result: 在合成和真实数据集上，相比强基线提高了对象发现、属性预测和组合图像生成效果，寄存器槽开销可忽略不计。

Conclusion: CODA有潜力成为复杂真实场景下鲁棒OCL的有效框架。

Abstract: Slot Attention (SA) with pretrained diffusion models has recently shown promise for object-centric learning (OCL), but suffers from slot entanglement and weak alignment between object slots and image content. We propose Contrastive Object-centric Diffusion Alignment (CODA), a simple extension that (i) employs register slots to absorb residual attention and reduce interference between object slots, and (ii) applies a contrastive alignment loss to explicitly encourage slot-image correspondence. The resulting training objective serves as a tractable surrogate for maximizing mutual information (MI) between slots and inputs, strengthening slot representation quality. On both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), CODA improves object discovery (e.g., +6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines. Register slots add negligible overhead, keeping CODA efficient and scalable. These results indicate potential applications of CODA as an effective framework for robust OCL in complex, real-world scenes.

</details>


### [296] [MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance](https://arxiv.org/abs/2601.01260)
*Hamad Khan,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本文提出基于LLM的MambaFormer混合Mixture - of - Experts框架用于高效医疗问答和临床辅助，在数据集上验证效果好，速度快。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际临床应用中受计算成本和线性时间模型效率权衡的限制，需解决该问题以实现高效医疗问答和临床辅助。

Method: 提出MambaFormer框架，采用轻量级门控机制将查询动态路由给定制的Transformer专家或State Space Model专家，使用定制模型，在新的DentalQA数据集上进行迁移学习微调，通过新颖的效用导向多目标损失优化。

Result: MambaFormer在DentalQA和PubMedQA数据集上进行交叉验证，BERTScore = 0.9180，超低延迟0.077秒，比T5 - Large快24.4倍。

Conclusion: MambaFormer可解决大语言模型在临床应用中的计算成本与效率权衡问题，为资源受限的临床部署提供可扩展解决方案。

Abstract: The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.

</details>


### [297] [AI-Powered Deepfake Detection Using CNN and Vision Transformer Architectures](https://arxiv.org/abs/2601.01281)
*Sifatullah Sheikh Urmi,Kirtonia Nuzath Tabassum Arthi,Md Al-Imran*

Main category: cs.CV

TL;DR: 评估四个基于AI的模型检测深度伪造，VFDNET与MobileNetV3表现优异。


<details>
  <summary>Details</summary>
Motivation: 人工智能生成的深度伪造增加，维护数字真实性面临挑战。

Method: 使用大人脸图像数据集评估三个CNN和一个视觉变换器组成的四个基于AI的模型，采用数据预处理和增强技术。

Result: VFDNET与MobileNetV3在各场景下，经数据处理后表现优异、准确率高。

Conclusion: AI具备可靠的深度伪造检测能力。

Abstract: The increasing use of artificial intelligence generated deepfakes creates major challenges in maintaining digital authenticity. Four AI-based models, consisting of three CNNs and one Vision Transformer, were evaluated using large face image datasets. Data preprocessing and augmentation techniques improved model performance across different scenarios. VFDNET demonstrated superior accuracy with MobileNetV3, showing efficient performance, thereby demonstrating AI's capabilities for dependable deepfake detection.

</details>


### [298] [LinMU: Multimodal Understanding Made Linear](https://arxiv.org/abs/2601.01322)
*Hongjie Wang,Niraj K. Jha*

Main category: cs.CV

TL;DR: 提出LinMU解决VLM自注意力二次复杂度问题，实现线性复杂度并保持性能，在多基准测试取得好效果。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型（VLMs）受自注意力二次复杂度限制，难以部署在边缘设备，处理高分辨率图像和长上下文视频成本高。

Method: 用M - MATE块替代VLM的自注意力层，并提出三阶段蒸馏框架将预训练VLM转换为LinMU架构。

Result: LinMU在多个基准测试中性能与教师模型相当，减少了首词时间，提高了分钟长度视频的吞吐量。

Conclusion: 无需二次注意力也能实现最先进的多模态推理，为处理高分辨率图像和长视频的长上下文VLMs开辟途径。

Abstract: Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\times$ and improves token throughput by up to 9.0$\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.

</details>


### [299] [Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding](https://arxiv.org/abs/2601.01352)
*Yixuan Lai,He Wang,Kun Zhou,Tianjia Shao*

Main category: cs.CV

TL;DR: 提出基于短参考视频的身份条件扩散变压器视频生成器，改善身份保留。


<details>
  <summary>Details</summary>
Motivation: 生成符合提示且保留指定身份的视频有挑战，单张图像作条件会导致问题。

Method: 引入身份条件扩散变压器视频生成器，用短参考视频，Sinkhorn路由编码器学习紧凑身份令牌。

Result: 在大姿态变化和丰富面部表情下持续改善身份保留，保持提示一致性和视觉真实感。

Conclusion: 该方法能有效解决生成视频时身份保留问题，具有良好效果。

Abstract: Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and "average" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.

</details>


### [300] [ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking](https://arxiv.org/abs/2601.01386)
*Xiaobao Wei,Zhangjie Ye,Yuxiang Gu,Zunjie Zhu,Yunfei Guo,Yingying Shen,Shan Zhao,Ming Lu,Haiyang Sun,Bing Wang,Guang Chen,Rongfeng Lu,Hangjun Ye*

Main category: cs.CV

TL;DR: 提出ParkRecon3D基准和ParkGaussian框架用于停车场场景3D重建，在重建质量和下游任务感知一致性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有工作在停车场3D重建方面研究不足，且单纯提升重建视觉质量对自动驾驶停车无直接帮助，关键在于车位感知模块。

Method: 创建ParkRecon3D基准，提出集成3D高斯 splatting的ParkGaussian框架，引入车位感知重建策略。

Result: 在ParkRecon3D上实验表明，ParkGaussian实现了最先进的重建质量，更好地保留了下游任务的感知一致性。

Conclusion: ParkRecon3D基准和ParkGaussian框架在停车场场景3D重建中有效，代码和数据集将公开。

Abstract: Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian

</details>


### [301] [SwinIFS: Landmark Guided Swin Transformer For Identity Preserving Face Super Resolution](https://arxiv.org/abs/2601.01406)
*Habiba Kausar,Saeed Anwar,Omar Jamal Hammad,Abdul Bais*

Main category: cs.CV

TL;DR: 本文提出SwinIFS框架用于人脸超分辨率，结合结构先验和分层注意力机制，在CelebA基准上表现出色，兼顾重建精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 人脸超分辨率因精细结构细节和特定身份特征丢失而具有挑战性，需要更好的方法实现身份保留重建。

Method: 引入SwinIFS框架，将关键面部特征点的高斯热图融入输入表示，使用紧凑的Swin Transformer骨干网络捕捉长距离上下文信息。

Result: 在CelebA基准上，SwinIFS实现卓越感知质量、更清晰重建和更好的身份保留，在8倍放大下也表现良好，能生成更逼真的结果。

Conclusion: SwinIFS在重建精度和计算效率之间取得优势平衡，适合人脸增强、监控和数字修复等实际应用。

Abstract: Face super-resolution aims to recover high-quality facial images from severely degraded low-resolution inputs, but remains challenging due to the loss of fine structural details and identity-specific features. This work introduces SwinIFS, a landmark-guided super-resolution framework that integrates structural priors with hierarchical attention mechanisms to achieve identity-preserving reconstruction at both moderate and extreme upscaling factors. The method incorporates dense Gaussian heatmaps of key facial landmarks into the input representation, enabling the network to focus on semantically important facial regions from the earliest stages of processing. A compact Swin Transformer backbone is employed to capture long-range contextual information while preserving local geometry, allowing the model to restore subtle facial textures and maintain global structural consistency. Extensive experiments on the CelebA benchmark demonstrate that SwinIFS achieves superior perceptual quality, sharper reconstructions, and improved identity retention; it consistently produces more photorealistic results and exhibits strong performance even under 8x magnification, where most methods fail to recover meaningful structure. SwinIFS also provides an advantageous balance between reconstruction accuracy and computational efficiency, making it suitable for real-world applications in facial enhancement, surveillance, and digital restoration. Our code, model weights, and results are available at https://github.com/Habiba123-stack/SwinIFS.

</details>


### [302] [Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration](https://arxiv.org/abs/2601.01456)
*Wentao Bian,Fenglei Xu*

Main category: cs.CV

TL;DR: 本文针对多模态少样本3D点云语义分割问题，提出DA - FSS模型，实验证明其优于MM - FSS。


<details>
  <summary>Details</summary>
Motivation: 发现“Fuse - then - Refine”范式存在“可塑性 - 稳定性困境”，且CLIP存在类间混淆导致语义盲目问题。

Method: 提出DA - FSS模型，区分语义和几何路径并相互正则化梯度；提出并行专家细化模块和堆叠仲裁模块；通过解耦对齐模块协调两个专家路径。

Result: 在S3DIS、ScanNet数据集上实验表明DA - FSS优于MM - FSS，几何边界、完整性和纹理区分度均优于基线。

Conclusion: DA - FSS模型能有效解决多模态少样本3D点云语义分割的问题，具有更好的性能。

Abstract: In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in "Fuse-then-Refine" paradigms: the "Plasticity-Stability Dilemma." In addition, CLIP's inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes between semantic and geometric paths and mutually regularizes their gradients to achieve better generalization. DA-FSS employs the same backbone and pre-trained text encoder as MM-FSS to generate text embeddings, which can increase free modalities' utilization rate and better leverage each modality's information space. To achieve this, we propose a Parallel Expert Refinement module to generate each modal correlation. We also propose a Stacked Arbitration Module (SAM) to perform convolutional fusion and arbitrate correlations for each modality pathway. The Parallel Experts decouple two paths: a Geometric Expert maintains plasticity, and a Semantic Expert ensures stability. They are coordinated via a Decoupled Alignment Module (DAM) that transfers knowledge without propagating confusion. Experiments on popular datasets (S3DIS, ScanNet) demonstrate the superiority of DA-FSS over MM-FSS. Meanwhile, geometric boundaries, completeness, and texture differentiation are all superior to the baseline. The code is available at: https://github.com/MoWenQAQ/DA-FSS.

</details>


### [303] [DeepInv: A Novel Self-supervised Learning Approach for Fast and Accurate Diffusion Inversion](https://arxiv.org/abs/2601.01487)
*Ziyue Zhang,Luxi Lin,Xiaolin Hu,Chao Chang,HuaiXi Wang,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出名为DeepInv的自监督扩散反演方法，可快速准确实现图像到噪声映射，性能和速度优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 当前扩散反演因缺乏可行监督信号仍是挑战任务，现有近似解决方案牺牲性能或效率。

Method: 引入自监督目标和数据增强策略生成高质量伪噪声，采用迭代和多尺度训练机制训练参数化反演求解器。

Result: 在COCO数据集上，DeepInv比EasyInv的SSIM高40.435%，比ReNoise速度快9887.5%。

Conclusion: DeepInv性能和推理速度佳，其可训练求解器的设计能为学界提供见解。

Abstract: Diffusion inversion is a task of recovering the noise of an image in a diffusion model, which is vital for controllable diffusion image editing. At present, diffusion inversion still remains a challenging task due to the lack of viable supervision signals. Thus, most existing methods resort to approximation-based solutions, which however are often at the cost of performance or efficiency. To remedy these shortcomings, we propose a novel self-supervised diffusion inversion approach in this paper, termed Deep Inversion (DeepInv). Instead of requiring ground-truth noise annotations, we introduce a self-supervised objective as well as a data augmentation strategy to generate high-quality pseudo noises from real images without manual intervention. Based on these two innovative designs, DeepInv is also equipped with an iterative and multi-scale training regime to train a parameterized inversion solver, thereby achieving the fast and accurate image-to-noise mapping. To the best of our knowledge, this is the first attempt of presenting a trainable solver to predict inversion noise step by step. The extensive experiments show that our DeepInv can achieve much better performance and inference speed than the compared methods, e.g., +40.435% SSIM than EasyInv and +9887.5% speed than ReNoise on COCO dataset. Moreover, our careful designs of trainable solvers can also provide insights to the community. Codes and model parameters will be released in https://github.com/potato-kitty/DeepInv.

</details>


### [304] [FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01513)
*Gen Li,Peiyu Liu*

Main category: cs.CV

TL;DR: 提出 VideoSpeculateRAG 框架解决 VLM 集成外部知识的问题，通过投机解码和过滤策略提升效率与准确性，实验显示加速推理约 2 倍。


<details>
  <summary>Details</summary>
Motivation: Vision - Language Models 在集成外部知识方面存在困难，当前的 Retrieval - Augmented Generation 方法效率低且难以保证答案质量。

Method: 提出 VideoSpeculateRAG 框架，采用投机解码管道，用轻量级模型生成答案候选，再由重量级模型验证和细化；使用基于相似度的过滤策略解决实体识别错误问题。

Result: VideoSpeculateRAG 与标准 RAG 方法相比，达到了相当或更高的准确性，同时将推理速度提高了约 2 倍。

Conclusion: 结合投机解码和检索增强推理在复杂、知识密集型多模态任务中提升效率和可靠性具有潜力。

Abstract: Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.

</details>


### [305] [DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving](https://arxiv.org/abs/2601.01528)
*Yang Zhou,Hao Shao,Letian Wang,Zhuofan Zong,Hongsheng Li,Steven L. Waslander*

Main category: cs.CV

TL;DR: 现有生成式驾驶世界模型领域缺少严格基准评估和覆盖场景丰富的数据集。本文提出DrivingGen基准，联合多样数据集和新指标评估模型，发现各模型存在权衡，其提供统一评估框架助力可部署模型。


<details>
  <summary>Details</summary>
Motivation: 生成式驾驶世界模型领域缺乏严格基准来衡量进展和指导优先事项，现有评估有局限，数据集覆盖场景不足。

Method: 提出DrivingGen基准，结合从驾驶数据集和互联网视频源整理的多样评估数据集，以及一套新指标来评估模型。

Result: 对14个先进模型进行基准测试，发现通用模型视觉好但违背物理规则，特定驾驶模型运动捕捉真实但视觉质量差。

Conclusion: DrivingGen提供统一评估框架，可促进可靠、可控且可部署的驾驶世界模型发展，支持可扩展模拟、规划和数据驱动决策。

Abstract: Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.

</details>


### [306] [EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding](https://arxiv.org/abs/2601.01547)
*Tianjun Gu,Chenghua Gong,Jingyu Gong,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: 提出Teleo - Spatial Intelligence (TSI)范式及EscherVerse，推动空间智能从被动场景描述向整体、有目的理解发展。


<details>
  <summary>Details</summary>
Motivation: 当前研究忽视空间变化背后的人类意图，为解决此局限提出新范式。

Method: 引入TSI范式，结合物理动态推理和意图驱动推理；构建EscherVerse，包含基准测试、数据集和模型。

Result: EscherVerse可评估智能体在动态、以人为中心场景中的推理能力，是首个系统评估意图驱动推理的基准。

Conclusion: 该工作为推进空间智能提供基础资源，使其向有目的理解世界迈进。

Abstract: The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.

</details>


### [307] [FALCON: Few-Shot Adversarial Learning for Cross-Domain Medical Image Segmentation](https://arxiv.org/abs/2601.01687)
*Abdur R. Fayjie,Pankhi Kashyap,Jutika Borah,Patrick Vandewalle*

Main category: cs.CV

TL;DR: 提出FALCON跨域少样本分割框架处理3D医学体积数据，实验显示其边界精度高、计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现有AI在3D医学体积分割中受限于标注数据少、病人特异性差异、数据隐私和计算开销大等问题，需更好的分割方法。

Method: 提出FALCON框架，先在自然图像上进行元训练学习通用分割先验，再通过对抗性微调与边界感知学习迁移到医学领域，使用任务感知推理适应病人特定解剖变异。

Result: 在四个基准测试中，FALCON的Hausdorff距离得分最低，边界精度高，Dice相似系数与最先进模型相当，且所需标注数据少、无需数据增强、计算开销低。

Conclusion: FALCON框架在3D医学体积分割中表现良好，能解决现有AI分割面临的问题。

Abstract: Precise delineation of anatomical and pathological structures within 3D medical volumes is crucial for accurate diagnosis, effective surgical planning, and longitudinal disease monitoring. Despite advancements in AI, clinically viable segmentation is often hindered by the scarcity of 3D annotations, patient-specific variability, data privacy concerns, and substantial computational overhead. In this work, we propose FALCON, a cross-domain few-shot segmentation framework that achieves high-precision 3D volume segmentation by processing data as 2D slices. The framework is first meta-trained on natural images to learn-to-learn generalizable segmentation priors, then transferred to the medical domain via adversarial fine-tuning and boundary-aware learning. Task-aware inference, conditioned on support cues, allows FALCON to adapt dynamically to patient-specific anatomical variations across slices. Experiments on four benchmarks demonstrate that FALCON consistently achieves the lowest Hausdorff Distance scores, indicating superior boundary accuracy while maintaining a Dice Similarity Coefficient comparable to the state-of-the-art models. Notably, these results are achieved with significantly less labeled data, no data augmentation, and substantially lower computational overhead.

</details>


### [308] [Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery](https://arxiv.org/abs/2601.01781)
*Lakshay Sharma,Alex Marin*

Main category: cs.CV

TL;DR: 提出Subimage Overlap Prediction用于遥感图像语义分割的自监督预训练任务，用更少预训练数据实现更快收敛和更好性能


<details>
  <summary>Details</summary>
Motivation: 多数自监督学习方法依赖大量预训练数据，本研究旨在用更少预训练图像辅助遥感图像语义分割

Method: 给定图像提取子图像，训练模型生成子图像在原图像中位置的语义掩码

Result: 该任务预训练可实现更快收敛，在下游分割任务中表现相当或更好，标注训练数据减少时性能差距更明显

Conclusion: 本方法在性能上匹配或超越其他自监督方法，且所需预训练数据显著更少

Abstract: Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.

</details>


### [309] [VerLM: Explaining Face Verification Using Natural Language](https://arxiv.org/abs/2601.01798)
*Syed Abdul Hannan,Hazim Bukhari,Thomas Cantalapiedra,Eman Ansar,Massa Baali,Rita Singh,Bhiksha Raj*

Main category: cs.CV

TL;DR: 本文提出用于人脸验证的创新视觉 - 语言模型，训练采用两种解释风格，经跨模态迁移提升性能，表现优于基线和现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有面部验证系统决策过程缺乏透明度，需要更透明、可解释的系统。

Method: 采用两种互补解释风格训练模型，将用于音频区分的建模方法进行跨模态迁移以适应视觉输入，集成特征提取和推理能力。

Result: 模型表现优于基线方法和现有模型。

Conclusion: 视觉语言模型在人脸验证中有巨大潜力，有助于构建更透明、可靠和可解释的人脸验证系统。

Abstract: Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.

</details>


### [310] [Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification](https://arxiv.org/abs/2601.01807)
*Ubaidullah,Muhammad Abid Hussain,Mohsin Raza Jafri,Rozi Khan,Moid Sandhu,Abd Ullah Khan,Hyundong Shin*

Main category: cs.CV

TL;DR: 提出基于混合深度学习的LUMPNet方法用于早期检测牛结节性皮肤病（LSD），评估显示其性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: LSD传染性强，危害大，需早期精准识别以防止爆发和及时干预。

Method: 提出LUMPNet，利用YOLOv11、基于EfficientNet的CNN分类器和新型自适应混合优化器，检测和分类皮肤结节。

Result: LUMPNet在LSD各阶段检测训练准确率达99%，验证准确率98%，优于现有方案。

Conclusion: LUMPNet在LSD早期检测中表现出色，性能优于现有方法。

Abstract: Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.

</details>


### [311] [RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images](https://arxiv.org/abs/2601.01835)
*Rashid Iqbal,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本文提出用于猴痘诊断的Customized Residual SwinTransformerV2 (RSwinV2)深度学习方法，在测试中准确率和F1分数高，优于标准CNN模型和SwinTransformers。


<details>
  <summary>Details</summary>
Motivation: 增强猴痘病变分类能力。

Method: 定制基于输入维度、嵌入结构和输出的变压器分层结构；将输入图像分割成不重叠的块，使用移动窗口和注意力机制处理；在SwinTransformer基础上发展，加入补丁和位置嵌入；引入Inverse Residual Block (IRB)解决梯度消失问题。

Result: 在Kaggle公共数据集上准确率达96.21，F1分数达95.62，优于标准CNN模型和SwinTransformers。

Conclusion: RSwinV2可作为计算机辅助工具用于猴痘病变观察解释。

Abstract: In this paper, a deep learning approach for Mpox diagnosis named Customized Residual SwinTransformerV2 (RSwinV2) has been proposed, trying to enhance the capability of lesion classification by employing the RSwinV2 tool-assisted vision approach. In the RSwinV2 method, a hierarchical structure of the transformer has been customized based on the input dimensionality, embedding structure, and output targeted by the method. In this RSwinV2 approach, the input image has been split into non-overlapping patches and processed using shifted windows and attention in these patches. This process has helped the method link all the windows efficiently by avoiding the locality issues of non-overlapping regions in attention, while being computationally efficient. RSwinV2 has further developed based on SwinTransformer and has included patch and position embeddings to take advantage of the transformer global-linking capability by employing multi-head attention in these embeddings. Furthermore, RSwinV2 has developed and incorporated the Inverse Residual Block (IRB) into this method, which utilizes convolutional skip connections with these inclusive designs to address the vanishing gradient issues during processing. RSwinV2 inclusion of IRB has therefore facilitated this method to link global patterns as well as local patterns; hence, its integrity has helped improve lesion classification capability by minimizing variability of Mpox and increasing differences of Mpox, chickenpox, measles, and cowpox. In testing SwinV2, its accuracy of 96.21 and an F1score of 95.62 have been achieved on the Kaggle public dataset, which has outperformed standard CNN models and SwinTransformers; RSwinV2 vector has thus proved its valiance as a computer-assisted tool for Mpox lesion observation interpretation.

</details>


### [312] [CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving](https://arxiv.org/abs/2601.01874)
*Shuhang Chen,Yunqiu Xu,Junjie Xie,Aojun Lu,Tao Feng,Zeying Huang,Ning Zhang,Yi Sun,Yi Yang,Hangjie Yuan*

Main category: cs.CV

TL;DR: 现有多模态大语言模型在视觉数学问题解决上有局限，本文提出认知启发的CogFlow框架。通过多方面优化和新数据集，经实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视提取的视觉线索在后续推理中的忠实整合和合理利用，作者希望解决此问题。

Method: 提出三阶段CogFlow框架，设计协同视觉奖励、知识内化奖励模型和视觉门控策略优化算法，构建新数据集MathCog。

Result: 在常用视觉数学推理基准上进行的综合实验和分析验证了CogFlow的优越性。

Conclusion: 所提出的CogFlow框架能有效解决视觉数学问题，为视觉数学推理提供了更好的方案。

Abstract: Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\Rightarrow$internalization$\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.

</details>


### [313] [Nodule-DETR: A Novel DETR Architecture with Frequency-Channel Attention for Ultrasound Thyroid Nodule Detection](https://arxiv.org/abs/2601.01908)
*Jingjing Wang,Qianglin Liu,Zhuo Xiao,Xinning Yao,Bo Liu,Lu Li,Lijuan Niu,Fugen Zhou*

Main category: cs.CV

TL;DR: 提出用于超声图像中甲状腺结节检测的Nodule - DETR架构，实验显示性能达最优，有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 甲状腺癌发病率上升，超声检测甲状腺结节存在图像对比度低、结节边界模糊等问题，影响诊断准确性。

Method: 提出Nodule - DETR架构，包含MSFCA模块增强低对比度结节特征、HFF模块进行多尺度特征融合、MSDA模块捕捉小的和不规则形状结节。

Result: 在临床甲状腺超声图像数据集上实验，Nodule - DETR在mAP@0.5:0.95上比基线模型高0.149，性能达最优。

Conclusion: Nodule - DETR准确性高，有作为计算机辅助甲状腺诊断有效工具的临床应用潜力。

Abstract: Thyroid cancer is the most common endocrine malignancy, and its incidence is rising globally. While ultrasound is the preferred imaging modality for detecting thyroid nodules, its diagnostic accuracy is often limited by challenges such as low image contrast and blurred nodule boundaries. To address these issues, we propose Nodule-DETR, a novel detection transformer (DETR) architecture designed for robust thyroid nodule detection in ultrasound images. Nodule-DETR introduces three key innovations: a Multi-Spectral Frequency-domain Channel Attention (MSFCA) module that leverages frequency analysis to enhance features of low-contrast nodules; a Hierarchical Feature Fusion (HFF) module for efficient multi-scale integration; and Multi-Scale Deformable Attention (MSDA) to flexibly capture small and irregularly shaped nodules. We conducted extensive experiments on a clinical dataset of real-world thyroid ultrasound images. The results demonstrate that Nodule-DETR achieves state-of-the-art performance, outperforming the baseline model by a significant margin of 0.149 in mAP@0.5:0.95. The superior accuracy of Nodule-DETR highlights its significant potential for clinical application as an effective tool in computer-aided thyroid diagnosis. The code of work is available at https://github.com/wjj1wjj/Nodule-DETR.

</details>


### [314] [VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis](https://arxiv.org/abs/2601.01989)
*Aly R. Elkammar,Karim M. Gamaleldin,Catherine M. Elias*

Main category: cs.CV

TL;DR: 提出基于Transformer/VVT不同尺寸算法用于行人意图预测，在JAAD数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 行人意图预测是自动驾驶从3级到4级过渡的关键技术，需考虑多元素特征保障道路安全。

Method: 引入基于Transformer/视频视觉Transformer不同尺寸算法，使用不同数据模态。

Result: 在JAAD数据集评估算法，达到SOTA性能，部分指标超越SOTA。

Conclusion: 通过大量消融研究探究不同模型设计选择带来的优势。

Abstract: Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.

</details>


### [315] [Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach](https://arxiv.org/abs/2601.02016)
*Matthias Bartolo,Dylan Seychell,Gabriel Hili,Matthew Montebello,Carl James Debono,Saviour Formosa,Konstantinos Makantasis*

Main category: cs.CV

TL;DR: 本文研究将LUPI范式集成到目标检测中，通过师生架构注入特权信息，实验表明LUPI训练的模型表现更优，为目标检测提供有效策略。


<details>
  <summary>Details</summary>
Motivation: 利用训练时可用但推理时不可用的细粒度描述信息，提升目标检测效果。

Method: 引入通用、与模型无关的方法，通过师生架构将如边界框掩码、显著性图和深度线索等特权信息注入基于深度学习的目标检测器，并在多个模型和数据集上实验。

Result: LUPI训练的模型始终优于基线模型，检测精度显著提升，推理复杂度和模型大小无增加，对中大型物体效果尤佳，中间权重的教师指导能平衡学习。

Conclusion: LUPI框架为资源受限和现实场景中的目标检测系统提供了有效实用的策略。

Abstract: This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.

</details>


### [316] [Agentic Retoucher for Text-To-Image Generation](https://arxiv.org/abs/2601.02046)
*Shaocheng Shen,Jianfeng Liang. Chunlei Cai,Cong Geng,Huiyu Duan,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出 Agentic Retoucher 框架解决文本到图像扩散模型小尺度失真问题，构建 GenBlemish - 27K 数据集，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型存在小尺度失真问题，现有改进方法有成本高、语义漂移等缺点。

Method: 提出 Agentic Retoucher 框架，包含感知、推理和行动代理，还构建 GenBlemish - 27K 数据集用于监督和评估。

Result: Agentic Retoucher 在感知质量、失真定位和人类偏好对齐方面优于现有方法。

Conclusion: Agentic Retoucher 为自校正和感知可靠的文本到图像生成建立了新范式。

Abstract: Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.

</details>


### [317] [Remote Sensing Change Detection via Weak Temporal Supervision](https://arxiv.org/abs/2601.02126)
*Xavier Bou,Elliot Vincent,Gabriele Facciolo,Rafael Grompone von Gioi,Jean-Michel Morel,Thibaud Ehret*

Main category: cs.CV

TL;DR: 提出利用现有单时相数据集额外时间观测的弱时间监督策略用于遥感语义变化检测，在多个数据集验证有良好表现且具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有遥感语义变化检测受标注数据集稀缺限制，近期方法域外泛化能力有限。

Method: 利用现有单时相数据集额外时间观测，扩展数据集，假设真实双时相数据大多无变化，配对不同位置图像生成变化样本；采用对象感知变化图生成和迭代细化处理弱标签噪声。

Result: 在扩展的FLAIR和IAILD航空数据集上实现强零样本和低数据制度性能，在法国大面积区域展示结果。

Conclusion: 所提方法具有良好性能和可扩展性。

Abstract: Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.

</details>


### [318] [BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models](https://arxiv.org/abs/2601.02147)
*Sunny Gupta,Shounak Das,Amit Sethi*

Main category: cs.CV

TL;DR: 提出双边提示优化框架BiPrompt用于缓解视觉语言基础模型中的虚假关联，在多基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法仅处理单一模态，导致鲁棒性不足和分布偏移下适应不稳定，而视觉语言基础模型易受视觉和文本模态虚假关联影响。

Method: 提出双边提示优化框架BiPrompt，视觉上用结构化注意力引导擦除，文本上引入平衡提示归一化，两者共同最小化虚假线索和预测之间的条件互信息。

Result: 在真实世界和合成偏见基准测试中，平均和最差组准确率均比先前测试时去偏方法有一致提升。

Conclusion: BiPrompt为视觉语言适应提供了轻量级且有效的途径，可实现可信和因果基础上的视觉语言适应。

Abstract: Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.

</details>


### [319] [NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation](https://arxiv.org/abs/2601.02204)
*Huichao Zhang,Liao Qu,Yiheng Liu,Hang Chen,Yangyang Song,Yongsheng Dong,Shikun Sun,Xian Li,Xu Wang,Yi Jiang,Hu Ye,Bo Chen,Yiming Gao,Peng Liu,Akide Liu,Zhipeng Yang,Qili Deng,Linjie Xing,Jiyang Liu,Zhao Wang,Yang Zhou,Mingcong Liu,Yi Zhang,Qian He,Xiwei Hu,Zhongqi Qi,Jie Shao,Zhiye Fu,Shuai Wang,Fangmin Chen,Xuezhi Chai,Zhihua Wu,Yitong Wang,Zehuan Yuan,Daniel K. Du,Xinglong Wu*

Main category: cs.CV

TL;DR: 介绍NextFlow，一种在6万亿交错文本 - 图像离散标记上训练的统一解码器自回归Transformer，有多种能力且速度快、性能优。


<details>
  <summary>Details</summary>
Motivation: 鉴于文本严格顺序性和图像固有层次结构的不同模态特性，改进传统方法。

Method: 在统一自回归架构中利用统一视觉表征，文本用下一个标记预测，视觉生成采用下一个尺度预测，用稳健训练方法解决多尺度生成不稳定性，引入前缀调优策略进行强化学习。

Result: 能在5秒内生成1024x1024图像，比同类AR模型快几个数量级，实验显示在统一模型中达SOTA，视觉质量与专业扩散基线相当。

Conclusion: NextFlow在统一模型中表现出色，具有很强的多模态理解和生成能力。

Abstract: We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.

</details>


### [320] [Seeing the Unseen: Zooming in the Dark with Event Cameras](https://arxiv.org/abs/2601.02206)
*Dachun Kai,Zeyu Xiao,Huyue Zhu,Jiaxiao Wang,Yueyi Zhang,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出RetinexEVSR框架用于低光照视频超分辨率，采用双向跨模态融合策略，实验效果达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有低光照视频超分辨率方法因对比度和高频信息有限，难以恢复精细细节。

Method: 提出RetinexEVSR框架，采用双向跨模态融合策略，设计照明引导事件增强模块和事件引导反射增强模块。

Result: 在三个数据集上达到SOTA，在SDSD基准上相比先前基于事件的方法，可提升2.95dB且减少65%运行时间。

Conclusion: RetinexEVSR框架有效解决低光照视频超分辨率问题，性能优越。

Abstract: This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.

</details>


### [321] [VIBE: Visual Instruction Based Editor](https://arxiv.org/abs/2601.02242)
*Grigorii Alekseenko,Aleksandr Gordeev,Irina Tolstykh,Bulat Suleimanov,Vladimir Dokholyan,Georgii Fedorov,Sergey Yakubson,Aleksandra Tsybina,Mikhail Chernyshov,Maksim Kuprashevich*

Main category: cs.CV

TL;DR: 提出基于低成本推理的指令图像编辑管道，在基准测试中表现出色，能在短时间内生成高分辨率图像。


<details>
  <summary>Details</summary>
Motivation: 现有开源指令图像编辑方法达到实际应用质量的较少，且扩散主干模型参数大、计算成本高。

Method: 使用2B参数的Qwen3 - VL模型指导编辑过程，1.6B参数的Sana1.5扩散模型进行图像生成，在架构、数据处理、训练和评估等方面做设计决策以实现低成本推理和源一致性。

Result: 在ImgEdit和GEdit基准测试中表现优于重基线模型，尤其在需保留输入图像的编辑任务上优势明显，能在24GB GPU内存下，约4秒生成2K分辨率编辑图像。

Conclusion: 所提出的紧凑、高通量指令图像编辑管道能在保证高质量的同时实现低成本推理。

Abstract: Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.

</details>


### [322] [A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets](https://arxiv.org/abs/2601.02246)
*Annoor Sharara Akhand*

Main category: cs.CV

TL;DR: 本文对训练自定义CNN、用预训练CNN作特征提取器和迁移学习三种视觉识别范式在五个图像分类数据集上对比，结果显示迁移学习预测性能强，自定义CNN在资源受限时有有效率与准确率的权衡优势。


<details>
  <summary>Details</summary>
Motivation: 对处理视觉识别的三种常用范式进行对比，为实践者在不同场景下选择合适方法提供参考。

Method: 在五个真实世界图像分类数据集上对比三种范式，用准确率、宏F1分数评估模型，辅以每轮训练时间和参数数量等效率指标。

Result: 迁移学习始终有最强预测性能，自定义CNN在计算和内存预算受限时有较好的效率 - 准确率权衡。

Conclusion: 在实际应用中，可根据计算资源和性能需求选择合适的CNN使用范式。

Abstract: Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.

</details>


### [323] [TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation](https://arxiv.org/abs/2601.02273)
*Salim Khazem*

Main category: cs.CV

TL;DR: 提出TopoLoRA - SAM用于二值语义分割，在多基准测试中表现好，仅训练少量参数。


<details>
  <summary>Details</summary>
Motivation: 基础分割模型适应特定领域语义分割有挑战，全量微调计算成本高且有灾难性遗忘风险。

Method: 提出TopoLoRA - SAM框架，将LoRA注入冻结的ViT编码器，增加轻量级空间卷积适配器和可选的拓扑感知监督。

Result: 在五个基准测试中，TopoLoRA - SAM取得最佳视网膜平均Dice和整体平均Dice，在CHASE_DB1数据集上提升了分割准确性和鲁棒性。

Conclusion: 拓扑感知的参数高效适配方法可媲美或超越全量微调的专业模型。

Abstract: Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \textbf{5.2\%} of model parameters ($\sim$4.9M). On the challenging CHASE\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [324] [Dynamic Risk in the U.S. Banking System: An Analysis of Sentiment, Policy Shocks, and Spillover Effects](https://arxiv.org/abs/2601.01783)
*Haibo Wang,Jun Huang,Lutfu S Sua,Jaime Ortiz,Jinshyang Roan,Bahram Alidaee*

Main category: econ.EM

TL;DR: 本文研究2023年美国银行危机传播渠道，证实‘太相似而不能倒’假设，强调实时监测对金融稳定的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究2023年美国银行危机传播渠道，检验‘太相似而不能倒’假设。

Method: 采用带30天滚动窗口的时变参数向量自回归（TVP - VAR）模型，分析2022年3月18日至2023年3月15日相关银行的每日股票收益。

Result: 危机高峰时系统总连通性急剧上升，确定主要风险净传输者和接收者，发现市场情绪和经济政策不确定性会放大风险溢出。

Conclusion: 证实银行网络中系统性风险的持续性，强调实时监测对加强金融稳定的重要性。

Abstract: The 2023 U.S. banking crisis propagated not through direct financial linkages but through a high-frequency, information-based contagion channel. This paper moves beyond exploration analysis to test the "too-similar-to-fail" hypothesis, arguing that risk spillovers were driven by perceived similarities in bank business models under acute interest rate pressure. Employing a Time-Varying Parameter Vector Autoregression (TVP-VAR) model with 30-day rolling windows, a method uniquely suited for capturing the rapid network shifts inherent in a panic, we analyze daily stock returns for the four failed institutions and a systematically selected peer group of surviving banks vulnerable to the same risks from March 18, 2022, to March 15, 2023. Our results provide strong evidence for this contagion channel: total system connectedness surged dramatically during the crisis peak, and we identify SIVB, FRC, and WAL as primary net transmitters of risk while their perceived peers became significant net receivers, a key dynamic indicator of systemic vulnerability that cannot be captured by asset-by-asset analysis. We further demonstrate that these spillovers were significantly amplified by market sentiment (as measured by the VIX) and economic policy uncertainty (EPU). By providing a clear conceptual framework and robust empirical validation, our findings confirm the persistence of systemic risks within the banking network and highlight the importance of real-time monitoring in strengthening financial stability.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [325] [Quantifying Local Strain Field and Deformation in Active Contraction of Bladder Using a Pretrained Transformer Model: A Speckle-Free Approach](https://arxiv.org/abs/2601.01315)
*Alireza Asadbeygi,Anne M. Robertson,Yasutaka Tobe,Masoud Zamani,Sean D. Stocker,Paul Watton,Naoki Yoshimura,Simon C Watkins*

Main category: q-bio.TO

TL;DR: 本文介绍了一种无斑点框架用于量化膀胱收缩局部应变场，该框架有效且能避免传统方法缺陷，有广泛应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统数字图像相关（DIC）方法研究膀胱收缩局部应变场需人工散斑，会改变组织属性，因此需新方法。

Method: 引入基于零样本变换器模型CoTracker3的无斑点框架，利用兼容多光子显微镜的特制双轴装置跟踪膀胱腔内自然纹理。

Result: 基准测试验证方法高精度和低误差，可捕捉复杂变形模式，大鼠实验显示膀胱收缩有显著各向异性，多光子显微镜证实形态变化。

Conclusion: 该非侵入性方法能消除散斑伪影，实现更符合生理的测量，在生物和工程系统材料测试中有广泛适用性。

Abstract: Accurate quantification of local strain fields during bladder contraction is essential for understanding the biomechanics of bladder micturition, in both health and disease. Conventional digital image correlation (DIC) methods have been successfully applied to various biological tissues; however, this approach requires artificial speckling, which can alter both passive and active properties of the tissue. In this study, we introduce a speckle-free framework for quantifying local strain fields using a state-of-the-art, zero-shot transformer model, CoTracker3. We utilized a custom-designed, portable isotonic biaxial apparatus compatible with multiphoton microscopy (MPM) to demonstrate this approach, successfully tracking natural bladder lumen textures without artificial markers. Benchmark tests validated the method's high pixel accuracy and low strain errors. Our framework effectively captured heterogeneous deformation patterns, despite complex folding and buckling, which conventional DIC often fails to track. Application to in vitro active bladder contractions in four rat specimens (n=4) revealed statistically significant anisotropy (p<0.01), with higher contraction longitudinally compared to circumferentially. Multiphoton microscopy further illustrated and confirmed heterogeneous morphological changes, such as large fold formation during active contraction. This non-invasive approach eliminates speckle-induced artifacts, enabling more physiologically relevant measurements, and has broad applicability for material testing of other biological and engineered systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [326] [CONSENT: A Negotiation Framework for Leveraging User Flexibility in Vehicle-to-Building Charging under Uncertainty](https://arxiv.org/abs/2601.01581)
*Rishav Sen,Fangqi Liu,Jose Paolo Talusan,Ava Pettet,Yoshinori Suzue,Mark Bailey,Ayan Mukhopadhyay,Abhishek Dubey*

Main category: cs.MA

TL;DR: 针对电动汽车（EV）在车到建筑（V2B）场景中的充电冲突，提出基于协商的框架，经校准和验证，该框架能实现互利结果，降低双方成本。


<details>
  <summary>Details</summary>
Motivation: 解决V2B场景中建筑运营商面临的高能源成本与驾驶员追求便利和满电之间的冲突。

Method: 提出基于协商的框架，为驾驶员提供激励选项，以换取其在出发时间或请求充电状态上的适度灵活性，并结合用户调查数据校准，用实际运营数据验证。

Result: 模拟显示该协商协议可使建筑运营商成本比非协商的优化智能充电政策降低超3.5%，同时使用户充电费用比公用事业零售电价降低22%。

Conclusion: 该框架能协调运营商和EV用户目标，将EV充电从运营摩擦源转变为合作和共享节约的平台。

Abstract: The growth of Electric Vehicles (EVs) creates a conflict in vehicle-to-building (V2B) settings between building operators, who face high energy costs from uncoordinated charging, and drivers, who prioritize convenience and a full charge. To resolve this, we propose a negotiation-based framework that, by design, guarantees voluntary participation, strategy-proofness, and budget feasibility. It transforms EV charging into a strategic resource by offering drivers a range of incentive-backed options for modest flexibility in their departure time or requested state of charge (SoC). Our framework is calibrated with user survey data and validated using real operational data from a commercial building and an EV manufacturer. Simulations show that our negotiation protocol creates a mutually beneficial outcome: lowering the building operator's costs by over 3.5\% compared to an optimized, non-negotiating smart charging policy, while simultaneously reducing user charging expenses by 22\% below the utility's retail energy rate. By aligning operator and EV user objectives, our framework provides a strategic bridge between energy and mobility systems, transforming EV charging from a source of operational friction into a platform for collaboration and shared savings.

</details>


### [327] [ARIES: A Scalable Multi-Agent Orchestration Framework for Real-Time Epidemiological Surveillance and Outbreak Monitoring](https://arxiv.org/abs/2601.01831)
*Aniket Wattamwar,Sampson Akwafuo*

Main category: cs.MA

TL;DR: 本文介绍了用于流行病学监测的多智能体框架 ARIES，能超越静态仪表盘，实现近实时威胁识别，模块化架构表现优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 全球健康监测存在知识差距，通用 AI 不适合流行病学领域，需要专业工具。

Method: 构建分层指挥结构的 ARIES 框架，利用 GPT 协调子智能体，自主查询数据并自动提取与逻辑合成。

Result: ARIES 能实现近实时识别新出现的威胁和信号差异。

Conclusion: 特定任务的智能体群可超越通用模型，为下一代疫情应对和全球健康情报提供强大、可扩展的方案。

Abstract: Global health surveillance is currently facing a challenge of Knowledge Gaps. While general-purpose AI has proliferated, it remains fundamentally unsuited for the high-stakes epidemiological domain due to chronic hallucinations and an inability to navigate specialized data silos. This paper introduces ARIES (Agentic Retrieval Intelligence for Epidemiological Surveillance), a specialized, autonomous multi-agent framework designed to move beyond static, disease-specific dashboards toward a dynamic intelligence ecosystem. Built on a hierarchical command structure, ARIES utilizes GPTs to orchestrate a scalable swarm of sub-agents capable of autonomously querying World Health Organization (WHO), Center for Disease Control and Prevention (CDC), and peer-reviewed research papers. By automating the extraction and logical synthesis of surveillance data, ARIES provides a specialized reasoning that identifies emergent threats and signal divergence in near real-time. This modular architecture proves that a task-specific agentic swarm can outperform generic models, offering a robust, extensible for next-generation outbreak response and global health intelligence.

</details>


### [328] [Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai](https://arxiv.org/abs/2601.01090)
*Erica Coppolillo,Luca Luceri,Emilio Ferrara*

Main category: cs.MA

TL;DR: 研究LLM驱动的代理在全AI社交平台上的毒性采用情况，发现毒性反应受刺激和自发因素影响，暴露是关键风险因素。


<details>
  <summary>Details</summary>
Motivation: 之前较少研究有害内容暴露如何随时间影响代理行为，特别是在全AI互动环境中，因此开展此研究。

Method: 在Chirper.ai平台上对LLM驱动的代理进行大规模实证分析，通过可观察交互来衡量暴露。

Result: 有毒刺激后更易产生有毒反应，但有部分毒性自发产生；累积有毒暴露增加有毒反应概率；引入两个影响指标揭示诱导和自发毒性的权衡；仅通过有毒刺激数量可准确预测代理是否产生有毒内容。

Conclusion: 暴露是部署LLM代理的关键风险因素，监测遇到的内容可作为审核和减轻有害行为的有效机制。

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous agents that participate in online social ecosystems, where interactions are sequential, cumulative, and only partially controlled. While prior work has documented the generation of toxic content by LLMs, far less is known about how exposure to harmful content shapes agent behavior over time, particularly in environments composed entirely of interacting AI agents. In this work, we study toxicity adoption of LLM-driven agents on Chirper.ai, a fully AI-driven social platform. Specifically, we model interactions in terms of stimuli (posts) and responses (comments), and by operationalizing exposure through observable interactions rather than inferred recommendation mechanisms.
  We conduct a large-scale empirical analysis of agent behavior, examining how response toxicity relates to stimulus toxicity, how repeated exposure affects the likelihood of toxic responses, and whether toxic behavior can be predicted from exposure alone. Our findings show that while toxic responses are more likely following toxic stimuli, a substantial fraction of toxicity emerges spontaneously, independent of exposure. At the same time, cumulative toxic exposure significantly increases the probability of toxic responding. We further introduce two influence metrics, the Influence-Driven Response Rate and the Spontaneous Response Rate, revealing a strong trade-off between induced and spontaneous toxicity. Finally, we show that the number of toxic stimuli alone enables accurate prediction of whether an agent will eventually produce toxic content.
  These results highlight exposure as a critical risk factor in the deployment of LLM agents and suggest that monitoring encountered content may provide a lightweight yet effective mechanism for auditing and mitigating harmful behavior in the wild.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [329] [Diffusion Timbre Transfer Via Mutual Information Guided Inpainting](https://arxiv.org/abs/2601.01294)
*Ching Ho Lee,Javier Nistal,Stefan Lattner,Marco Pasini,George Fazekas*

Main category: cs.SD

TL;DR: 把音色转换作为音乐音频推理时间编辑问题研究，提出轻量级方法操纵预训练模型进行音色转换。


<details>
  <summary>Details</summary>
Motivation: 研究将音色转换作为音乐音频推理时间的编辑问题。

Method: 从预训练潜在扩散模型出发，引入轻量级程序，包括针对乐器身份信息最丰富的潜在通道进行维度噪声注入和在反向扩散中重新施加输入旋律与节奏结构的早期步骤夹紧机制。

Result: 该方法可直接作用于音频潜在特征，与文本/音频条件兼容。

Conclusion: 简单的推理时间控制可以有效地引导预训练模型用于风格转换。

Abstract: We study timbre transfer as an inference-time editing problem for music audio. Starting from a strong pre-trained latent diffusion model, we introduce a lightweight procedure that requires no additional training: (i) a dimension-wise noise injection that targets latent channels most informative of instrument identity, and (ii) an early-step clamping mechanism that re-imposes the input's melodic and rhythmic structure during reverse diffusion. The method operates directly on audio latents and is compatible with text/audio conditioning (e.g., CLAP). We discuss design choices,analyze trade-offs between timbral change and structural preservation, and show that simple inference-time controls can meaningfully steer pre-trained models for style-transfer use cases.

</details>


### [330] [UltraEval-Audio: A Unified Framework for Comprehensive Evaluation of Audio Foundation Models](https://arxiv.org/abs/2601.01373)
*Qundong Shi,Jie Zhou,Biyuan Lin,Junbo Cui,Guoyang Zeng,Yixuan Zhou,Ziyang Wang,Xin Liu,Zhen Luo,Yudong Wang,Zhiyuan Liu*

Main category: cs.SD

TL;DR: 文章指出音频基础模型缺乏综合评估的问题，介绍了UltraEval - Audio统一评估框架，还提出新的中文基准，为音频模型比较提供平台。


<details>
  <summary>Details</summary>
Motivation: 音频基础模型缺乏综合评估成为领域发展瓶颈，现有音频评估存在无统一框架、音频编解码器缺乏评估方法、现有语音基准依赖英语难以评估中文表现等问题。

Method: 引入UltraEval - Audio统一评估框架，采用新的音频编解码器综合评估方案，提出两个新的中文基准SpeechCMMLU和SpeechHSK。

Result: UltraEval - Audio具有模块化架构，支持多语言和多任务类别，集成主流模型和权威基准，提供一键评估和实时公开排行榜。

Conclusion: 希望UltraEval - Audio为学术界和工业界提供透明、高效、公平的音频模型比较平台，代码等已开源。

Abstract: The development of audio foundation models has accelerated rapidly since the emergence of GPT-4o. However, the lack of comprehensive evaluation has become a critical bottleneck for further progress in the field, particularly in audio generation. Current audio evaluation faces three major challenges: (1) audio evaluation lacks a unified framework, with datasets and code scattered across various sources, hindering fair and efficient cross-model comparison;(2) audio codecs, as a key component of audio foundation models, lack a widely accepted and holistic evaluation methodology; (3) existing speech benchmarks are heavily reliant on English, making it challenging to objectively assess models' performance on Chinese. To address the first issue, we introduce UltraEval-Audio, a unified evaluation framework for audio foundation models, specifically designed for both audio understanding and generation tasks. UltraEval-Audio features a modular architecture, supporting 10 languages and 14 core task categories, while seamlessly integrating 24 mainstream models and 36 authoritative benchmarks. To enhance research efficiency, the framework provides a one-command evaluation feature, accompanied by real-time public leaderboards. For the second challenge, UltraEval-Audio adopts a novel comprehensive evaluation scheme for audio codecs, evaluating performance across three key dimensions: semantic accuracy, timbre fidelity, and acoustic quality. To address the third issue, we propose two new Chinese benchmarks, SpeechCMMLU and SpeechHSK, designed to assess Chinese knowledge proficiency and language fluency. We wish that UltraEval-Audio will provide both academia and industry with a transparent, efficient, and fair platform for comparison of audio models. Our code, benchmarks, and leaderboards are available at https://github.com/OpenBMB/UltraEval-Audio.

</details>


### [331] [MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization](https://arxiv.org/abs/2601.01554)
*Donghua Yu,Zhengyuan Lin,Chen Yang,Yiyang Zhang,Zhaoye Fei,Hanfu Chen,Jingqi Chen,Ke Chen,Qinyuan Cheng,Liwei Fan,Yi Jiang,Jie Zhu,Muchen Li,Shimin Li,Wenxuan Wang,Yang Wang,Zhe Xu,Yitian Gong,Yuqian Zhang*

Main category: cs.SD

TL;DR: 提出MOSS Transcribe Diarize模型进行端到端的SATS，在多种基准测试中优于现有商业系统。


<details>
  <summary>Details</summary>
Motivation: 现有SATS系统很少采用端到端公式，存在上下文窗口有限、长距离说话人记忆弱和无法输出时间戳等局限。

Method: 提出统一的多模态大语言模型MOSS Transcribe Diarize，在大量真实数据上训练，具备128k上下文窗口。

Result: 在多种公开和内部基准测试中，优于最先进的商业系统。

Conclusion: MOSS Transcribe Diarize能有效解决现有SATS系统的局限，具有良好扩展性和泛化能力。

Abstract: Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.

</details>


### [332] [MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning](https://arxiv.org/abs/2601.01568)
*Chunyu Qiang,Jun Wang,Xiaopeng Wang,Kang Yin,Yuxin Guo,Xijuan Zeng,Nan Li,Zihan Li,Yuzhe Liang,Ziyu Zhang,Teng Ma,Yushen Chen,Zhongliang Liu,Feng Deng,Chen Zhang,Pengfei Wan*

Main category: cs.SD

TL;DR: 提出MM - Sonate框架用于可控音视频联合生成和零样本语音克隆，表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有音视频联合生成统一模型在细粒度声学控制和身份保留语音方面存在问题，如级联生成的时间不对齐和零样本语音克隆能力缺失。

Method: 提出MM - Sonate多模态流匹配框架，使用统一指令 - 音素输入确保对齐，引入音色注入机制实现零样本语音克隆，提出基于噪声的负调节策略提高声学保真度。

Result: MM - Sonate在联合生成基准测试中达到新的最优性能，在唇同步和语音清晰度上大幅超越基线，语音克隆保真度与专业文本转语音系统相当。

Conclusion: MM - Sonate有效解决了现有模型的问题，在音视频联合生成和零样本语音克隆方面表现出色。

Abstract: Joint audio-video generation aims to synthesize synchronized multisensory content, yet current unified models struggle with fine-grained acoustic control, particularly for identity-preserving speech. Existing approaches either suffer from temporal misalignment due to cascaded generation or lack the capability to perform zero-shot voice cloning within a joint synthesis framework. In this work, we present MM-Sonate, a multimodal flow-matching framework that unifies controllable audio-video joint generation with zero-shot voice cloning capabilities. Unlike prior works that rely on coarse semantic descriptions, MM-Sonate utilizes a unified instruction-phoneme input to enforce strict linguistic and temporal alignment. To enable zero-shot voice cloning, we introduce a timbre injection mechanism that effectively decouples speaker identity from linguistic content. Furthermore, addressing the limitations of standard classifier-free guidance in multimodal settings, we propose a noise-based negative conditioning strategy that utilizes natural noise priors to significantly enhance acoustic fidelity. Empirical evaluations demonstrate that MM-Sonate establishes new state-of-the-art performance in joint generation benchmarks, significantly outperforming baselines in lip synchronization and speech intelligibility, while achieving voice cloning fidelity comparable to specialized Text-to-Speech systems.

</details>


### [333] [DARC: Drum accompaniment generation with fine-grained rhythm control](https://arxiv.org/abs/2601.02357)
*Trey Brosnan*

Main category: cs.SD

TL;DR: 现有音乐生成工具在结构控制和风格灵活性上不足，本文引入DARC模型，增强对鼓伴奏的节奏控制并保持音乐上下文感知。


<details>
  <summary>Details</summary>
Motivation: 解决现有音乐生成工具在用户要求结构控制和风格灵活性时的不足，尤其是在鼓伴奏生成方面。

Method: 使用参数高效微调，在STAGE鼓干音生成器基础上增加细粒度节奏控制。

Result: 未提及明确结果

Conclusion: 未提及明确结论

Abstract: In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [334] [A Modular Reference Architecture for MCP-Servers Enabling Agentic BIM Interaction](https://arxiv.org/abs/2601.00809)
*Tobias Heimig-Elschner,Changyu Du,Anna Scheuvens,André Borrmann,Jakob Beetz*

Main category: cs.OH

TL;DR: 本文介绍用于MCP服务器的模块化参考架构，以实现API无关、可隔离和可重现的代理式BIM交互，并用原型验证其可行性和优势。


<details>
  <summary>Details</summary>
Motivation: 当前BIM端MCP实现特定于创作工具且缺乏规范性，限制了重用、评估和跨环境工作流的可移植性。

Method: 从对近期文献中重复能力的系统分析得出核心需求，设计以显式适配器契约为中心的微服务架构，将MCP接口与特定BIM - API解耦。

Result: 使用IfcOpenShell的原型实现证明了在常见修改和生成任务中的可行性，评估显示该架构能实现可靠工作流、减少耦合。

Conclusion: 该架构为系统研究提供了可重用基础。

Abstract: Agentic workflows driven by large language models (LLMs) are increasingly applied to Building Information Modelling (BIM), enabling natural-language retrieval, modification and generation of IFC models. Recent work has begun adopting the emerging Model Context Protocol (MCP) as a uniform tool-calling interface for LLMs, simplifying the agent side of BIM interaction. While MCP standardises how LLMs invoke tools, current BIM-side implementations are still authoring tool-specific and ad hoc, limiting reuse, evaluation, and workflow portability across environments. This paper addresses this gap by introducing a modular reference architecture for MCP servers that enables API-agnostic, isolated and reproducible agentic BIM interactions. From a systematic analysis of recurring capabilities in recent literature, we derive a core set of requirements. These inform a microservice architecture centred on an explicit adapter contract that decouples the MCP interface from specific BIM-APIs. A prototype implementation using IfcOpenShell demonstrates feasibility across common modification and generation tasks. Evaluation across representative scenarios shows that the architecture enables reliable workflows, reduces coupling, and provides a reusable foundation for systematic research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [335] [Value Vision-Language-Action Planning & Search](https://arxiv.org/abs/2601.00969)
*Ali Salamatian,Ke,Ren,Kieran Pattison,Cyrus Neary*

Main category: cs.RO

TL;DR: 现有视觉 - 语言 - 动作（VLA）模型依赖行为克隆存在局限性，本文提出V - VLAPS框架，用轻量级可学习价值函数增强蒙特卡罗树搜索（MCTS），在机器人操作任务中提升成功率并减少模拟次数。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型依赖行为克隆，在分布变化时表现脆弱，现有基于MCTS的方法仅依赖VLA先验，缺乏对未来回报的估计，不准确时需大量模拟。

Method: 引入Value Vision - Language - Action Planning and Search (V - VLAPS)框架，在固定VLA骨干网络（Octo）的潜在表征上训练多层感知器（MLP），为MCTS搜索提供显式成功信号。

Result: 在LIBERO机器人操作套件上评估，价值引导的搜索比仅依赖VLA先验的基线方法成功率提高超5个百分点，MCTS模拟平均次数减少5 - 15%。

Conclusion: V - VLAPS框架通过引入轻量级可学习价值函数增强MCTS，有效提升机器人操作任务的性能。

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior.

</details>


### [336] [Online Estimation and Manipulation of Articulated Objects](https://arxiv.org/abs/2601.01438)
*Russell Buchanan,Adrian Röfer,João Moura,Abhinav Valada,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 文章提出结合深度学习视觉先验与本体感受传感的新方法来估计铰接物体的关节运动，经模拟和真实实验验证，机器人自主打开未知铰接物体成功率达75%。


<details>
  <summary>Details</summary>
Motivation: 服务机器人需自动操作任意铰接物体，现有的深度学习和观测运动方法都有局限。

Method: 采用因子图在线估计关节运动，将学习到的视觉先验和交互时的本体感受传感融合到基于旋量理论的关节分析模型中。

Result: 在模拟和真实机器人操作实验中广泛评估，机器人能打开之前未见过的抽屉，真实硬件实验中自主打开未知铰接物体成功率达75%。

Conclusion: 提出的方法有效，能让机器人较好地估计并操作未知铰接物体。

Abstract: From refrigerators to kitchen drawers, humans interact with articulated objects effortlessly every day while completing household chores. For automating these tasks, service robots must be capable of manipulating arbitrary articulated objects. Recent deep learning methods have been shown to predict valuable priors on the affordance of articulated objects from vision. In contrast, many other works estimate object articulations by observing the articulation motion, but this requires the robot to already be capable of manipulating the object. In this article, we propose a novel approach combining these methods by using a factor graph for online estimation of articulation which fuses learned visual priors and proprioceptive sensing during interaction into an analytical model of articulation based on Screw Theory. With our method, a robotic system makes an initial prediction of articulation from vision before touching the object, and then quickly updates the estimate from kinematic and force sensing during manipulation. We evaluate our method extensively in both simulations and real-world robotic manipulation experiments. We demonstrate several closed-loop estimation and manipulation experiments in which the robot was capable of opening previously unseen drawers. In real hardware experiments, the robot achieved a 75% success rate for autonomous opening of unknown articulated objects.

</details>


### [337] [HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller](https://arxiv.org/abs/2601.01577)
*Tran Tien Dat,Nguyen Hai An,Nguyen Khanh Viet Dung,Nguyen Duy Duc*

Main category: cs.RO

TL;DR: 当前强化学习用于自主控制器有诸多不足，本文引入基于JEPA的世界模型Hanoi - World进行长期水平规划，实验显示其在安全驾驶规划上有效。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习用于自主控制器数据需求大、结果不佳且难以保证安全，而基于JEPA的自监督学习方法有潜力，故开展研究。

Method: 引入基于JEPA的世界模型Hanoi - World，使用循环神经网络（RNN）进行长期水平规划。

Result: 在Highway - Env包不同环境实验中，该模型能制定安全驾驶计划，碰撞率与SOTA基线相比表现良好。

Conclusion: 基于JEPA的世界模型Hanoi - World在自主控制器的驾驶规划中有效且有安全意识。

Abstract: Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines

</details>


### [338] [Explicit World Models for Reliable Human-Robot Collaboration](https://arxiv.org/abs/2601.01705)
*Kenneth Kwok,Basura Fernando,Qianli Xu,Vigneshwaran Subbaraju,Dongkyu Choi,Boon Kiat Quek*

Main category: cs.RO

TL;DR: 本文探讨具身AI在感知噪声、模糊指令和人机交互下的鲁棒性问题，提出以构建‘显式世界模型’实现可靠具身AI的新方法。


<details>
  <summary>Details</summary>
Motivation: 解决具身AI在复杂人机交互场景中的可靠性问题，突破传统形式验证方法局限。

Method: 强调人机交互的动态、模糊和主观特性，构建并更新代表人机共识的‘显式世界模型’，使机器人行为符合人类期望。

Result: 无明确提及具体实验或实践结果。

Conclusion: 在社会、多模态和动态的人类环境中，具身AI的可靠性由上下文决定，构建‘显式世界模型’是实现可靠具身AI的根本不同途径。

Abstract: This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible "explicit world model" representing the common ground between human and AI, that is used to align robot behaviours with human expectations.

</details>


### [339] [Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots](https://arxiv.org/abs/2601.02085)
*Meili Sun,Chunjiang Zhao,Lichao Yang,Hao Liu,Shimin Hu,Ya Xiong*

Main category: cs.RO

TL;DR: 文章针对草莓采摘机器人视觉感知低集成等问题，提出视觉故障诊断与自恢复框架，经实验验证框架有效。


<details>
  <summary>Details</summary>
Motivation: 解决草莓采摘机器人视觉感知集成度低、抓手与果实对齐不准、空抓和抓力不足导致草莓滑落等问题，提升采摘稳定性和效率。

Method: 提出集成多任务感知与纠正控制策略的视觉故障诊断和自我恢复框架，核心是SRR - Net多任务感知模型，设计相对误差补偿法，采用早期中止策略，通过嵌入式微型光学相机和分类器实现反馈。

Result: SRR - Net保持高感知精度，检测、分割和成熟度估计表现良好，支持多任务感知，推理速度达163.35 FPS。

Conclusion: 所提视觉故障诊断和自我恢复框架能有效解决草莓采摘机器人的相关问题，提升其性能。

Abstract: Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS.

</details>


### [340] [SingingBot: An Avatar-Driven System for Robotic Face Singing Performance](https://arxiv.org/abs/2601.02125)
*Zhuoxiong Xu,Xuanchen Li,Yuhao Cheng,Fei Xu,Yichao Yan,Xiaokang Yang*

Main category: cs.RO

TL;DR: 本文提出用于机器人唱歌的头像驱动框架，能实现丰富情感表达且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器人面部驱动研究难以满足唱歌时连续情感表达和连贯性的高要求，需要新方法使机器人面部具备唱歌能力以实现移情式人机交互。

Method: 先利用含人类先验知识的肖像视频生成模型合成生动的唱歌头像，再通过跨广泛表情空间的语义映射函数将面部特征转移到机器人上，还提出情感动态范围指标评估。

Result: 所提方法实现了丰富的情感表达，保持了唇音同步，显著优于现有方法。

Conclusion: 提出的新颖框架有效，广泛的情感光谱对引人入胜的表演至关重要。

Abstract: Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [341] [Deciding Serializability in Network Systems](https://arxiv.org/abs/2601.02251)
*Guy Amir,Mark Barbone,Nicolas Amat,Jules Jacobs*

Main category: cs.FL

TL;DR: 提出SER建模语言验证并发程序可串行化，给出自动化决策过程，经优化后能处理多种现实程序模型。


<details>
  <summary>Details</summary>
Motivation: 自动验证并发程序的可串行化，即判断程序的每个并发执行是否等价于某个串行执行。

Method: 提出SER建模语言，给出自动化端到端决策过程，将可串行化问题转化为Petri网可达性查询，通过Petri网切片、半线性集压缩和Presburger公式操作等优化搜索空间。

Result: 框架能成功处理包括有状态防火墙、BGP路由器等多种现实世界程序模型。

Conclusion: 尽管问题理论上较难，但该框架有实际应用价值，可用于验证并发程序的可串行化。

Abstract: We present the SER modeling language for automatically verifying serializability of concurrent programs, i.e., whether every concurrent execution of the program is equivalent to some serial execution.
  SER programs are suitably restricted to make this problem decidable, while still allowing for an unbounded number of concurrent threads of execution, each potentially running for an unbounded number of steps.
  Building on prior theoretical results, we give the first automated end-to-end decision procedure that either proves serializability by producing a checkable certificate, or refutes it by producing a counterexample trace.
  We also present a network-system abstraction to which SER programs compile. Our decision procedure then reduces serializability in this setting to a Petri net reachability query.
  Furthermore, in order to scale, we curtail the search space via multiple optimizations, including Petri net slicing, semilinear-set compression, and Presburger-formula manipulation.
  We extensively evaluate our framework and show that, despite the theoretical hardness of the problem, it can successfully handle various models of real-world programs, including stateful firewalls, BGP routers, and more.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [342] [On lead-lag estimation of non-synchronously observed point processes](https://arxiv.org/abs/2601.01871)
*Takaaki Shiotani,Takaki Hayashi,Yuta Koike*

Main category: math.ST

TL;DR: 本文提出分析点过程领先 - 滞后关系的新理论框架，用于高频金融数据，提出新估计量并证明其有效性。


<details>
  <summary>Details</summary>
Motivation: Dobrev和Schaumburg的方法依赖离散时间观测有局限性，需改进。

Method: 将估计两个点过程领先 - 滞后关系问题转化为估计二元平稳点过程的交叉对相关函数（CPCF）形状，提出基于核密度估计的领先 - 滞后时间估计量。

Result: 新估计量有良好理论性质和优越数值性能。

Conclusion: 实证表明提出的方法有效。

Abstract: This paper introduces a new theoretical framework for analyzing lead-lag relationships between point processes, with a special focus on applications to high-frequency financial data. In particular, we are interested in lead-lag relationships between two sequences of order arrival timestamps. The seminal work of Dobrev and Schaumburg proposed model-free measures of cross-market trading activity based on cross-counts of timestamps. While their method is known to yield reliable results, it faces limitations because its original formulation inherently relies on discrete-time observations, an issue we address in this study. Specifically, we formulate the problem of estimating lead-lag relationships in two point processes as that of estimating the shape of the cross-pair correlation function (CPCF) of a bivariate stationary point process, a quantity well-studied in the neuroscience and spatial statistics literature. Within this framework, the prevailing lead-lag time is defined as the location of the CPCF's sharpest peak. Under this interpretation, the peak location in Dobrev and Schaumburg's cross-market activity measure can be viewed as an estimator of the lead-lag time in the aforementioned sense. We further propose an alternative lead-lag time estimator based on kernel density estimation and show that it possesses desirable theoretical properties and delivers superior numerical performance. Empirical evidence from high-frequency financial data demonstrates the effectiveness of our proposed method.

</details>


### [343] [On Statistical Inference for Rates of Change in Spatial Processes over Riemannian Manifolds](https://arxiv.org/abs/2601.02305)
*Didong Li,Aritra Halder,Sudipto Banerjee*

Main category: math.ST

TL;DR: 本文提出针对紧致黎曼流形上空间过程变化率的综合推理框架，通过模拟实验验证理论，有望在机器学习和统计领域应用。


<details>
  <summary>Details</summary>
Motivation: 现有空间过程统计推理多在欧几里得域，现代空间数据常来自非欧几里得域，需对紧致黎曼流形上空间过程变化率进行推理。

Method: 形式化过程实现的平滑性，构建微分过程，推导确保过程存在的核条件，建立联合多元过程有效性，基于流形上已实现过程进行预测推理。

Result: 模拟实验验证了对多面体网格上过程导数评估的理论发现。

Conclusion: 提出基于模型的方法对部分观测数据的流形上空间过程的微分过程进行推理，有助于提升对流形上高斯过程的理解，有多种潜在应用。

Abstract: Statistical inference for spatial processes from partially realized or scattered data has seen voluminous developments in diverse areas ranging from environmental sciences to business and economics. Inference on the associated rates of change has seen some recent developments. The literature has been restricted to Euclidean domains, where inference is sought on directional derivatives, rates along a chosen direction of interest, at arbitrary locations. Inference for higher order rates, particularly directional curvature has also proved useful in these settings. Modern spatial data often arise from non-Euclidean domains. This manuscript particularly considers spatial processes defined over compact Riemannian manifolds. We develop a comprehensive inferential framework for spatial rates of change for such processes over vector fields. In doing so, we formalize smoothness of process realizations and construct differential processes -- the derivative and curvature processes. We derive conditions for kernels that ensure the existence of these processes and establish validity of the joint multivariate process consisting of the ``parent'' Gaussian process (GP) over the manifold and the associated differential processes. Predictive inference on these rates is devised conditioned on the realized process over the manifold. Manifolds arise as polyhedral meshes in practice. The success of our simulation experiments for assessing derivatives for processes observed over such meshes validate our theoretical findings. By enhancing our understanding of GPs on manifolds, this manuscript unlocks a variety of potential applications in machine learning and statistics where GPs have seen wide usage. We propose a fully model-based approach to inference on the differential processes arising from a spatial process from partially observed or realized data across scattered location on a manifold.

</details>


### [344] [Tessellation Localized Transfer learning for nonparametric regression](https://arxiv.org/abs/2601.00987)
*Hélène Halconruy,Benjamin Bobbia,Paul Lejamtel*

Main category: math.ST

TL;DR: 提出非参数回归迁移学习框架，能有效迁移信息并限制负迁移，有理论保证且实验验证优势。


<details>
  <summary>Details</summary>
Motivation: 利用相关源任务信息提升目标任务性能，解决源 - 目标关系异质性问题。

Method: 基于局部迁移假设，划分协变量空间，联合学习局部迁移函数和目标回归，采用数据驱动方法适应未知结构和迁移强度。

Result: 建立目标回归估计的极小极大率，理论保证以 oracle 不等式形式分解超额风险。

Conclusion: 所提方法能利用功能复杂度降低缓解维数灾难，数值实验证明其优势。

Abstract: Transfer learning aims to improve performance on a target task by leveraging information from related source tasks. We propose a nonparametric regression transfer learning framework that explicitly models heterogeneity in the source-target relationship. Our approach relies on a local transfer assumption: the covariate space is partitioned into finitely many cells such that, within each cell, the target regression function can be expressed as a low-complexity transformation of the source regression function. This localized structure enables effective transfer where similarity is present while limiting negative transfer elsewhere. We introduce estimators that jointly learn the local transfer functions and the target regression, together with fully data-driven procedures that adapt to unknown partition structure and transfer strength. We establish sharp minimax rates for target regression estimation, showing that local transfer can mitigate the curse of dimensionality by exploiting reduced functional complexity. Our theoretical guarantees take the form of oracle inequalities that decompose excess risk into estimation and approximation terms, ensuring robustness to model misspecification. Numerical experiments illustrate the benefits of the proposed approach.

</details>


### [345] [SGD with Dependent Data: Optimal Estimation, Regret, and Inference](https://arxiv.org/abs/2601.01371)
*Yinan Shen,Yichen Zhang,Wen-Xin Zhou*

Main category: math.ST

TL;DR: 本文研究随机梯度下降（SGD）在时间依赖数据下最终迭代结果的性能，证明SGD可适应多种信息，展示其在非渐近和渐近情形下的良好表现，还提出新算法。


<details>
  <summary>Details</summary>
Motivation: 探究随机梯度下降在时间依赖数据下的性能，突破经典框架限制，解决先前工作中提出的估计 - 遗憾权衡问题。

Method: 考虑两种互补的依赖源，在多种步长和探索率方案下分析SGD性能，提出决策区域的“锥”近似和基于SGD的新算法。

Result: 非渐近情况下，SGD同时实现统计最优估计误差和遗憾，尾部界限即使在无限时间范围也很尖锐；渐近情况下，SGD迭代收敛到高斯分布，避免估计 - 遗憾权衡；新算法使用少量存储和计算量，实现长期统计最优。

Conclusion: SGD能适应依赖信息，在依赖数据下有良好性能，所提新算法在线性稀疏回归中实用有效。

Abstract: This work investigates the performance of the final iterate produced by stochastic gradient descent (SGD) under temporally dependent data. We consider two complementary sources of dependence: $(i)$ martingale-type dependence in both the covariate and noise processes, which accommodates non-stationary and non-mixing time series data, and $(ii)$ dependence induced by sequential decision making. Our formulation runs in parallel with classical notions of (local) stationarity and strong mixing, while neither framework fully subsumes the other. Remarkably, SGD is shown to automatically accommodate both independent and dependent information under a broad class of stepsize schedules and exploration rate schemes.
  Non-asymptotically, we show that SGD simultaneously achieves statistically optimal estimation error and regret, extending and improving existing results. In particular, our tail bounds remain sharp even for potentially infinite horizon $T=+\infty$. Asymptotically, the SGD iterates converge to a Gaussian distribution with only an $O_{\PP}(1/\sqrt{t})$ remainder, demonstrating that the supposed estimation-regret trade-off claimed in prior work can in fact be avoided. We further propose a new ``conic'' approximation of the decision region that allows the covariates to have unbounded support. For online sparse regression, we develop a new SGD-based algorithm that uses only $d$ units of storage and requires $O(d)$ flops per iteration, achieving the long term statistical optimality. Intuitively, each incoming observation contributes to estimation accuracy, while aggregated summary statistics guide support recovery.

</details>


### [346] [Double Machine Learning of Continuous Treatment Effects with General Instrumental Variables](https://arxiv.org/abs/2601.01471)
*Shuyuan Chen,Peng Zhang,Yifan Cui*

Main category: math.ST

TL;DR: 本文提出用工具变量局部识别剂量反应函数的框架，开发估计方法并评估其有限样本性能。


<details>
  <summary>Details</summary>
Motivation: 经典分析估计连续处理因果效应时假设所有混杂因素都被完全观测到，但现实中存在未测量的混杂因素，需要方法减轻其带来的偏差。

Method: 提出用工具变量局部识别剂量反应函数的框架，引入均匀正则加权函数，开发带工具变量的去偏机器学习框架下连续处理的增强逆概率加权得分，通过核回归或经验风险最小化估计剂量反应函数。

Result: 建立了剂量反应函数估计的渐近性质，通过模拟和实证研究评估了所提方法的有限样本性能。

Conclusion: 所提出的框架和方法能有效减轻未观测混杂因素带来的偏差，可用于估计连续处理的因果效应。

Abstract: Estimating causal effects of continuous treatments is a common problem in practice, for example, in studying dose-response functions. Classical analyses typically assume that all confounders are fully observed, whereas in real-world applications, unmeasured confounding often persists. In this article, we propose a novel framework for local identification of dose-response functions using instrumental variables, thereby mitigating bias induced by unobserved confounders. We introduce the concept of a uniform regular weighting function and consider covering the treatment space with a finite collection of open sets. On each of these sets, such a weighting function exists, allowing us to identify the dose-response function locally within the corresponding region. For estimation, we develop an augmented inverse probability weighting score for continuous treatments under a debiased machine learning framework with instrumental variables. We further establish the asymptotic properties when the dose-response function is estimated via kernel regression or empirical risk minimization. Finally, we conduct both simulation and empirical studies to assess the finite-sample performance of the proposed methods.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [347] [Visualizing the Structure of Lenia Parameter Space](https://arxiv.org/abs/2601.01932)
*Barbora Hudcová,František Dušek,Marco Tuccio,Clément Hongler*

Main category: nlin.CG

TL;DR: 本文提出新方法将Lenia系统自动分类，检测移动孤子并可视化参数空间结构，为相关问题带来新见解。


<details>
  <summary>Details</summary>
Motivation: 连续元胞自动机虽受欢迎，但理解其行为存在挑战，Lenia有几个基本开放性问题待解决。

Method: 提出一种新方法将Lenia系统自动分为四个不同动力学类别。

Result: 能检测移动孤子，在网站上实现参数空间结构可视化，发现新孤子族以及相空间结构的普遍性。

Conclusion: 该方法为Lenia基本开放性问题带来新启示。

Abstract: Continuous cellular automata are rocketing in popularity, yet developing a theoretical understanding of their behaviour remains a challenge. In the case of Lenia, a few fundamental open problems include determining what exactly constitutes a soliton, what is the overall structure of the parameter space, and where do the solitons occur in it. In this abstract, we present a new method to automatically classify Lenia systems into four qualitatively different dynamical classes. This allows us to detect moving solitons, and to provide an interactive visualization of Lenia's parameter space structure on our website https://lenia-explorer.vercel.app/. The results shed new light on the above-mentioned questions and lead to several observations: the existence of new soliton families for parameters where they were not believed to exist, or the universality of the phase space structure across various kernels.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [348] [Almost-Exact Simulation Scheme for Heston-type Models: Bermudan and American Option Pricing](https://arxiv.org/abs/2601.00815)
*Mara Kalicanin Dimitrov,Marko Dimitrov,Anatoliy Malyarenko,Ying Ni*

Main category: q-fin.PR

TL;DR: 本文将AES方案扩展到Heston和双Heston模型下百慕大与美式期权定价，对比AES与Euler方案，实验验证AES高效准确。


<details>
  <summary>Details</summary>
Motivation: 已有的AES用于Heston模型欧洲期权定价，将其扩展到百慕大与美式期权定价。

Method: 推导双Heston模型的AES方案，并与Euler方案进行性能对比。

Result: 数值实验表明AES方案能以减少的计算时间提供准确的期权价格。

Conclusion: AES方案对两种模型均稳健，在模拟步数与百慕大期权行权日匹配时更高效准确。

Abstract: Recently, an Almost-Exact Simulation (AES) scheme was introduced for the Heston stochastic volatility model and tested for European option pricing. This paper extends this scheme for pricing Bermudan and American options under both Heston and double Heston models. The AES improves Monte Carlo simulation efficiency by using the non-central chi-square distribution for the variance process. We derive the AES scheme for the double Heston model and compare the performance of the AES schemes under both models with the Euler scheme. Our numerical experiments validate the effectiveness of the AES scheme in providing accurate option prices with reduced computational time, highlighting its robustness for both models. In particular, the AES achieves higher accuracy and computational efficiency when the number of simulation steps matches the exercise dates for Bermudan options.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [349] [When Is Degree Enough? Bounds on Degree-Eigenvector Misalignment in Assortative Structured Networks](https://arxiv.org/abs/2601.00807)
*Sreerag Puravankara,Vipin P. Veetil*

Main category: cs.SI

TL;DR: 推导度向量和特征向量在具有度相关性和局部介观结构网络中的差异界限，界定“谱安全”区域。


<details>
  <summary>Details</summary>
Motivation: 许多现实网络不满足度向量和主特征向量紧密对齐的条件，需研究有度相关性和局部结构网络中二者差异。

Method: 设计保度重连算法，从中性基准开始单调增加相关性和局部结构强度，利用Stewart - Sun摄动界和谱范数控制推导界限。

Result: 得到适度相关性和局部结构下特征向量与度向量夹角的上界。

Conclusion: 分析界限界定了“谱安全”区域，可将节点度作为衡量其系统重要性的可靠指标。

Abstract: A tight alignment between the degree vector and the leading eigenvector arises naturally in networks with neutral degree mixing and the absence of local structures. Many real-world networks, however, violate both conditions. We derive bounds on the divergence between the degree vector and the eigenvector in networks with degree assortativity and local mesoscopic structures such as communities, core-peripheries, and cycles. Our approach is constructive. We design sufficiently general degree-preserving rewiring algorithms that start from a neutral benchmark and monotonically increase assortativity and the strength of local structures, with each step inducing a perturbation of the adjacency matrix. Using the Stewart--Sun Perturbation Bound, together with explicit spectral-norm control of the rewiring steps, we derive upper bounds on the angle between the eigenvector and the degree vector for modest levels of assortativity and local structures. Our analytical bounds delineate regions of `spectral safety' in which a node's degree can be used as a reliable measure of its systemic importance in real-world networks.

</details>


### [350] [Beyond Homophily: Community Search on Heterophilic Graphs](https://arxiv.org/abs/2601.01703)
*Qing Sima,Xiaoyang Wang,Wenjie Zhang*

Main category: cs.SI

TL;DR: 针对异质性图社区搜索难题提出AdaptCS框架，实验证明其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 经典算法和基于机器学习的模型在异质性图上难以有效进行社区搜索，算法方法易返回混合类标签社区，GNN会模糊社区边界。

Method: 提出Adaptive Community Search (AdaptCS)统一框架，包含可分离多跳和多频率信号的编码器、减少计算瓶颈的低秩优化、平衡嵌入相似性和拓扑关系的自适应社区评分。

Result: 在异质性和同质性基准测试中，AdaptCS的F1分数平均比最佳基线模型高11%，在不同异质性水平下保持稳健性，速度提升可达2个数量级。

Conclusion: AdaptCS在社区搜索任务中表现良好，能有效解决异质性图上的社区搜索问题。

Abstract: Community search aims to identify a refined set of nodes that are most relevant to a given query, supporting tasks ranging from fraud detection to recommendation. Unlike homophilic graphs, many real-world networks are heterophilic, where edges predominantly connect dissimilar nodes. Therefore, structural signals that once reflected smooth, low-frequency similarity now appear as sharp, high-frequency contrasts. However, both classical algorithms (e.g., k-core, k-truss) and recent ML-based models struggle to achieve effective community search on heterophilic graphs, where edge signs or semantics are generally unknown. Algorithm-based methods often return communities with mixed class labels, while GNNs, built on homophily, smooth away meaningful signals and blur community boundaries. Therefore, we propose Adaptive Community Search (AdaptCS), a unified framework featuring three key designs: (i) an AdaptCS Encoder that disentangles multi-hop and multi-frequency signals, enabling the model to capture both smooth (homophilic) and contrastive (heterophilic) relations; (ii) a memory-efficient low-rank optimization that removes the main computational bottleneck and ensures model scalability; and (iii) an Adaptive Community Score (ACS) that guides online search by balancing embedding similarity and topological relations. Extensive experiments on both heterophilic and homophilic benchmarks demonstrate that AdaptCS outperforms the best-performing baseline by an average of 11% in F1-score, retains robustness across heterophily levels, and achieves up to 2 orders of magnitude speedup.

</details>


### [351] [Measuring Social Media Polarization Using Large Language Models and Heuristic Rules](https://arxiv.org/abs/2601.00927)
*Jawad Chowdhury,Rezaur Rashid,Gabriel Terejanu*

Main category: cs.SI

TL;DR: 本文提出新框架，用大语言模型和领域启发式方法分析社交媒体中情感极化，揭示事件依赖的极化模式，代码开源。


<details>
  <summary>Details</summary>
Motivation: 理解在线话语中的情感极化对评估社交媒体互动的社会影响至关重要。

Method: 利用大语言模型提取立场、情感基调与共识模式，结合基于规则的评分系统量化情感极化。

Result: 分析揭示了事件依赖的两种极化模式：预期驱动极化和反应性极化。

Conclusion: 结合AI内容注释和领域评分的框架为衡量情感极化提供了可扩展且可解释的方法。

Abstract: Understanding affective polarization in online discourse is crucial for evaluating the societal impact of social media interactions. This study presents a novel framework that leverages large language models (LLMs) and domain-informed heuristics to systematically analyze and quantify affective polarization in discussions on divisive topics such as climate change and gun control. Unlike most prior approaches that relied on sentiment analysis or predefined classifiers, our method integrates LLMs to extract stance, affective tone, and agreement patterns from large-scale social media discussions. We then apply a rule-based scoring system capable of quantifying affective polarization even in small conversations consisting of single interactions, based on stance alignment, emotional content, and interaction dynamics. Our analysis reveals distinct polarization patterns that are event dependent: (i) anticipation-driven polarization, where extreme polarization escalates before well-publicized events, and (ii) reactive polarization, where intense affective polarization spikes immediately after sudden, high-impact events. By combining AI-driven content annotation with domain-informed scoring, our framework offers a scalable and interpretable approach to measuring affective polarization. The source code is publicly available at: https://github.com/hasanjawad001/llm-social-media-polarization.

</details>


### [352] [Gendered Pathways in AI Companionship: Cross-Community Behavior and Toxicity Patterns on Reddit](https://arxiv.org/abs/2601.01073)
*Erica Coppolillo,Emilio Ferrara*

Main category: cs.SI

TL;DR: 研究分析MyBoyfriendIsAI子版块超3000用户活动，发现AI陪伴生态跨社区参与有性别结构，部分性别化路径可能放大毒性。


<details>
  <summary>Details</summary>
Motivation: 探讨AI陪伴平台中人际关系与性别化网络行为、有害内容接触的交叉问题。

Method: 重构超3000高参与用户两年活动历史，构建超2000子版块互动网络。

Result: MBIA用户主要穿越四个社区领域，参与有性别结构，多数路径毒性低，部分AI色情和性别社区有毒性峰值，近16%用户参与性别子版块有不同情绪表达和更高毒性。

Conclusion: 描绘了Reddit上AI陪伴跨社区参与的性别结构，指出风险集中处，为人机关系平台提供参考。

Abstract: AI-companionship platforms are rapidly reshaping how people form emotional, romantic, and parasocial bonds with non-human agents, raising new questions about how these relationships intersect with gendered online behavior and exposure to harmful content. Focusing on the MyBoyfriendIsAI (MBIA) subreddit, we reconstruct the Reddit activity histories of more than 3,000 highly engaged users over two years, yielding over 67,000 historical submissions. We then situate MBIA within a broader ecosystem by building a historical interaction network spanning more than 2,000 subreddits, which enables us to trace cross-community pathways and measure how toxicity and emotional expression vary across these trajectories. We find that MBIA users primarily traverse four surrounding community spheres (AI-companionship, porn-related, forum-like, and gaming) and that participation across the ecosystem exhibits a distinct gendered structure, with substantial engagement by female users. While toxicity is generally low across most pathways, we observe localized spikes concentrated in a small subset of AI-porn and gender-oriented communities. Nearly 16% of users engage with gender-focused subreddits, and their trajectories display systematically different patterns of emotional expression and elevated toxicity, suggesting that a minority of gendered pathways may act as toxicity amplifiers within the broader AI-companionship ecosystem. These results characterize the gendered structure of cross-community participation around AI companionship on Reddit and highlight where risks concentrate, informing measurement, moderation, and design practices for human-AI relationship platforms.

</details>


### [353] [Inferring Network Evolutionary History via Structure-State Coupled Learning](https://arxiv.org/abs/2601.02121)
*En Xu,Shihe Zhou,Huandong Wang,Jingtao Ding,Yong Li*

Main category: cs.SI

TL;DR: 该论文提出利用网络稳态动力学进行网络演化历史推断的方法CS²，实验显示其性能优于基线方法，稳态信号可独立用于演化推断。


<details>
  <summary>Details</summary>
Motivation: 从有限时间注释的单个终态快照中推断网络演化历史具有挑战性，现有方法仅依赖拓扑结构，信息不足且有噪声。

Method: 提出CS²方法，明确建模结构 - 状态耦合，以捕捉拓扑如何调节稳态，利用两种信号共同改进边判别来恢复形成顺序。

Result: 在六个真实时态网络的多动力学过程中实验表明，CS²平均提高成对边优先级精度4.0%，全局排序一致性7.7%，能更好还原宏观演化轨迹，稳态变体在拓扑信息有限时仍有竞争力。

Conclusion: 网络稳态动力学可作为额外信号用于网络演化历史推断，稳态信号能独立用于演化推断。

Abstract: Inferring a network's evolutionary history from a single final snapshot with limited temporal annotations is fundamental yet challenging. Existing approaches predominantly rely on topology alone, which often provides insufficient and noisy cues. This paper leverages network steady-state dynamics -- converged node states under a given dynamical process -- as an additional and widely accessible observation for network evolution history inference. We propose CS$^2$, which explicitly models structure-state coupling to capture how topology modulates steady states and how the two signals jointly improve edge discrimination for formation-order recovery. Experiments on six real temporal networks, evaluated under multiple dynamical processes, show that CS$^2$ consistently outperforms strong baselines, improving pairwise edge precedence accuracy by 4.0% on average and global ordering consistency (Spearman-$ρ$) by 7.7% on average. CS$^2$ also more faithfully recovers macroscopic evolution trajectories such as clustering formation, degree heterogeneity, and hub growth. Moreover, a steady-state-only variant remains competitive when reliable topology is limited, highlighting steady states as an independent signal for evolution inference.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [354] [On the Practical Estimation and Interpretation of Rényi Transfer Entropy](https://arxiv.org/abs/2601.01497)
*Zlata Tabachová,Petr Jizba,Hynek Lavička,Milan Paluš*

Main category: nlin.PS

TL;DR: 本文系统研究了k近邻估计器在Rényi熵和Rényi转移熵上的性能，通过合成数据集测试，引入可靠性条件和调参指南，证明有效RTE估计能捕捉信息流向，且RTE解释力依赖于参数α。


<details>
  <summary>Details</summary>
Motivation: Rényi转移熵在分析复杂非高斯系统因果关系有潜力，但实际应用受限于准确估计和解释的挑战，尤其是在有限、高维或异构数据集上。

Method: 使用多种具有明确因果关系的合成数据集，对k近邻估计器在广泛参数范围内进行测试，应用于不同复杂度的模拟过程，引入三个可靠性条件并制定调参指南。

Result: 当满足可靠性条件并校准参数时，有效RTE估计能准确捕捉不同场景下的方向性信息流，RTE的解释力敏感依赖于Rényi参数α。

Conclusion: RTE框架对于识别复杂系统中极端行为的驱动因素很有用。

Abstract: Rényi transfer entropy (RTE) is a generalization of classical transfer entropy that replaces Shannon's entropy with Rényi's information measure. This, in turn, introduces a new tunable parameter $α$, which accounts for sensitivity to low- or high-probability events. Although RTE shows strong potential for analyzing causal relations in complex, non-Gaussian systems, its practical use is limited, primarily due to challenges related to its accurate estimation and interpretation. These difficulties are especially pronounced when working with finite, high-dimensional, or heterogeneous datasets. In this paper, we systematically study the performance of a k-nearest neighbor estimator for both Rényi entropy (RE) and RTE using various synthetic data sets with clear cause-and-effect relationships inherent to their construction. We test the estimator across a broad range of parameters, including sample size, dimensionality, memory length, and Rényi order $α$. In particular, we apply the estimator to a set of simulated processes with increasing structural complexity, ranging from linear dynamics to nonlinear systems with multi-source couplings. To address interpretational challenges arising from potentially negative RE and RTE values, we introduce three reliability conditions and formulate practical guidelines for tuning the estimator parameters. We show that when the reliability conditions are met and the parameters are calibrated accordingly, the resulting effective RTE estimates accurately capture directional information flow across a broad range of scenarios. Results obtained show that the explanatory power of RTE depends sensitively on the choice of the Rényi parameter $α$. This highlights the usefulness of the RTE framework for identifying the drivers of extreme behavior in complex systems.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [355] [Generating Diverse TSP Tours via a Combination of Graph Pointer Network and Dispersion](https://arxiv.org/abs/2601.01132)
*Hao-Hsung Yang,Ssu-Yuan Lo,Kuan-Lun Chen,Ching-Kai Wang*

Main category: cs.CG

TL;DR: 本文针对D - TSP问题提出新混合框架，在效率和多样性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有解决D - TSP问题的方法存在计算复杂度高、多样性不足等局限，为满足物流规划等应用对高质量和容错性的需求。

Method: 将D - TSP分解为两步，先用带近似序列熵损失的GPN生成高质量多样本，再用贪心算法选出k个最大差异的路径。

Result: 在柏林实例上平均Jaccard指数达0.015，远超NMA和RF - MA3S，GPN结构运行时间近线性增长，在大规模实例上比复杂双准则算法快360多倍。

Conclusion: 提出的新混合框架在解决D - TSP问题上达到了最先进的性能，兼具高效性和简单性。

Abstract: We address the Diverse Traveling Salesman Problem (D-TSP), a bi-criteria optimization challenge that seeks a set of $k$ distinct TSP tours. The objective requires every selected tour to have a length at most $c|T^*|$ (where $|T^*|$ is the optimal tour length) while minimizing the average Jaccard similarity across all tour pairs. This formulation is crucial for applications requiring both high solution quality and fault tolerance, such as logistics planning, robotics pathfinding or strategic patrolling. Current methods are limited: traditional heuristics, such as the Niching Memetic Algorithm (NMA) or bi-criteria optimization, incur high computational complexity $O(n^3)$, while modern neural approaches (e.g., RF-MA3S) achieve limited diversity quality and rely on complex, external mechanisms.
  To overcome these limitations, we propose a novel hybrid framework that decomposes D-TSP into two efficient steps. First, we utilize a simple Graph Pointer Network (GPN), augmented with an approximated sequence entropy loss, to efficiently sample a large, diverse pool of high-quality tours. This simple modification effectively controls the quality-diversity trade-off without complex external mechanisms. Second, we apply a greedy algorithm that yields a 2-approximation for the dispersion problem to select the final $k$ maximally diverse tours from the generated pool. Our results demonstrate state-of-the-art performance. On the Berlin instance, our model achieves an average Jaccard index of $0.015$, significantly outperforming NMA ($0.081$) and RF-MA3S. By leveraging GPU acceleration, our GPN structure achieves a near-linear empirical runtime growth of $O(n)$. While maintaining solution diversity comparable to complex bi-criteria algorithms, our approach is over 360 times faster on large-scale instances (783 cities), delivering high-quality TSP solutions with unprecedented efficiency and simplicity.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [356] [Comparative Analysis of Formula and Structure Prediction from Tandem Mass Spectra](https://arxiv.org/abs/2601.00941)
*Xujun Che,Xiuxia Du,Depeng Xu*

Main category: q-bio.QM

TL;DR: 文章对基于LC - MS的代谢组学和暴露组学化合物预测算法进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有光谱库无法覆盖LC - MS/MS捕获的化学空间，已有的化合物预测方法评估缺乏统一标准，为选择实用预测流程和改进相关方法，需进行系统评估。

Method: 对不同类型加合物评估公式预测和结构预测的准确性。

Result: 建立了现实的性能基线，确定了关键瓶颈。

Conclusion: 为基于质谱的化合物预测进一步改进提供了指导。

Abstract: Liquid chromatography mass spectrometry (LC-MS)-based metabolomics and exposomics aim to measure detectable small molecules in biological samples. The results facilitate hypothesis-generating discovery of metabolic changes and disease mechanisms and provide information about environmental exposures and their effects on human health. Metabolomics and exposomics are made possible by the high resolving power of LC and high mass measurement accuracy of MS. However, a majority of the signals from such studies still cannot be identified or annotated using conventional library searching because existing spectral libraries are far from covering the vast chemical space captured by LC-MS/MS. To address this challenge and unleash the full potential of metabolomics and exposomics, a number of computational approaches have been developed to predict compounds based on tandem mass spectra. Published assessment of these approaches used different datasets and evaluation. To select prediction workflows for practical applications and identify areas for further improvements, we have carried out a systematic evaluation of the state-of-the-art prediction algorithms. Specifically, the accuracy of formula prediction and structure prediction was evaluated for different types of adducts. The resulting findings have established realistic performance baselines, identified critical bottlenecks, and provided guidance to further improve compound predictions based on MS.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [357] [A Wachspress-based transfinite formulation for exactly enforcing Dirichlet boundary conditions on convex polygonal domains in physics-informed neural networks](https://arxiv.org/abs/2601.01756)
*N. Sukumar,Ritwick Roy*

Main category: math.NA

TL;DR: 提出基于Wachspress的超限公式用于物理信息神经网络中精确实施狄利克雷边界条件，评估了其在不同泊松边值问题上的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决物理信息神经网络中精确实施狄利克雷边界条件的问题，克服先前使用近似距离函数的局限性，提供解决参数化凸几何问题的框架。

Method: 利用几何设计的成果，如混合函数和超限插值，使用Wachspress坐标的超限公式，将试验函数表示为神经网络输出与边界限制扩展的差值加上超限插值。

Result: 神经网络试验函数有有界拉普拉斯算子，可用于解决参数化凸几何问题。

Conclusion: 该方法可提高物理信息神经网络和深度里兹法在不同泊松边值问题上的准确性。

Abstract: In this paper, we present a Wachspress-based transfinite formulation on convex polygonal domains for exact enforcement of Dirichlet boundary conditions in physics-informed neural networks. This approach leverages prior advances in geometric design such as blending functions and transfinite interpolation over convex domains. For prescribed Dirichlet boundary function $\mathcal{B}$, the transfinite interpolant of $\mathcal{B}$, $g : \bar P \to C^0(\bar P)$, $\textit{lifts}$ functions from the boundary of a two-dimensional polygonal domain to its interior. The trial function is expressed as the difference between the neural network's output and the extension of its boundary restriction into the interior of the domain, with $g$ added to it. This ensures kinematic admissibility of the trial function in the deep Ritz method. Wachspress coordinates for an $n$-gon are used in the transfinite formula, which generalizes bilinear Coons transfinite interpolation on rectangles to convex polygons. The neural network trial function has a bounded Laplacian, thereby overcoming a limitation in a previous contribution where approximate distance functions were used to exactly enforce Dirichlet boundary conditions. For a point $\boldsymbol{x} \in \bar{P}$, Wachspress coordinates, $\boldsymbolλ : \bar P \to [0,1]^n$, serve as a geometric feature map for the neural network: $\boldsymbolλ$ encodes the boundary edges of the polygonal domain. This offers a framework for solving problems on parametrized convex geometries using neural networks. The accuracy of physics-informed neural networks and deep Ritz is assessed on forward, inverse, and parametrized geometric Poisson boundary-value problems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [358] [Improving the Graph Challenge Reference Implementation](https://arxiv.org/abs/2601.00974)
*Inna Voloshchuk,Hayden Jananthan,Chansup Byun,Jeremy Kepner*

Main category: cs.NI

TL;DR: 本文对MIT/IEEE/Amazon图挑战中参考代码的部分内容进行重构和基准测试，精简代码并实现并行基准测试，展示了可扩展性能，为参与者提供清晰高效基础。


<details>
  <summary>Details</summary>
Motivation: 原参考代码存在清晰度、适应性和性能方面的不足，需要改进以提高图挑战的影响力。

Method: 对原Python参考代码进行重构，将约1000行代码精简到325行，使用pMatlab和pPython分布式数组编程库进行并行基准测试。

Result: 代码规模缩小67%，同时保持完整功能，展示了大规模流量矩阵求和与分析的可扩展性能。

Conclusion: 改进后的实现为图挑战参与者提供了清晰高效的基础，增加了图挑战的潜在影响力。

Abstract: The MIT/IEEE/Amazon Graph Challenge provides a venue for individuals and teams to showcase new innovations in large-scale graph and sparse data analysis. The Anonymized Network Sensing Graph Challenge processes over 100 billion network packets to construct privacy-preserving traffic matrices, with a GraphBLAS reference implementation demonstrating how hypersparse matrices can be applied to this problem. This work presents a refactoring and benchmarking of a section of the reference code to improve clarity, adaptability, and performance. The original Python implementation spanning approximately 1000 lines across 3 files has been streamlined to 325 lines across two focused modules, achieving a 67% reduction in code size while maintaining full functionality. Using pMatlab and pPython distributed array programming libraries, the addition of parallel maps allowed for parallel benchmarking of the data. Scalable performance is demonstrated for large-scale summation and analysis of traffic matrices. The resulting implementation increases the potential impact of the Graph Challenge by providing a clear and efficient foundation for participants.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [359] [The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers](https://arxiv.org/abs/2601.02045)
*Shuoming Zhang,Jiacheng Zhao,Qiuchu Yu,Chunwei Xia,Zheng Wang,Xiaobing Feng,Huimin Cui*

Main category: cs.PL

TL;DR: 本文对大语言模型赋能的编译领域进行系统综述，提出分类方法，指出进展、挑战和机遇，为相关人员提供路线图。


<details>
  <summary>Details</summary>
Motivation: 对新兴的大语言模型赋能的编译领域进行系统梳理，为研究者和从业者提供参考。

Method: 提出基于设计理念、大语言模型方法、代码抽象操作级别和任务类型的多维分类法。

Result: 明确了大语言模型赋能编译的三个主要进展，指出确保正确性和可扩展性的关键挑战，认为混合系统开发是最有前途的方向。

Conclusion: 该综述为新一代大语言模型驱动的编译工具发展提供基础路线图。

Abstract: This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.

</details>


### [360] [Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming](https://arxiv.org/abs/2601.02060)
*Nguyet-Anh H. Lang,Eric Lang,Thanh Le-Cong,Bach Le,Quyet-Thang Huynh*

Main category: cs.PL

TL;DR: 本文提出评估框架FPEval评估大模型在函数式编程代码生成能力，结果表明模型能力随发展有提升但纯函数式语言错误率高，代码风格有问题，且模型可在反馈下部分自我修复。


<details>
  <summary>Details</summary>
Motivation: 函数式编程学习曲线陡峭限制其应用，目前大模型在函数式编程代码生成能力评估不足。

Method: 基于新基准FPBench构建评估框架FPEval，涵盖三种主流函数式语言和721个编程任务，综合测试验证和静态分析工具进行评估。评估了GPT - 3.5、GPT - 4o和GPT - 5等模型，以Java为命令式编程基线。

Result: 模型在函数式编程中的性能随发展显著提升，但纯函数式语言错误率高于混合或命令式语言，生成代码常有命令式风格问题。

Conclusion: 大模型在函数式编程代码生成有进步但仍有问题，且可部分自我修复代码正确性和质量问题。

Abstract: Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.

</details>


### [361] [MLIR-Smith: A Novel Random Program Generator for Evaluating Compiler Pipelines](https://arxiv.org/abs/2601.02218)
*Berke Ates,Filip Dobrosavljević,Theodoros Theodoridis,Zhendong Su*

Main category: cs.PL

TL;DR: 论文提出MLIR - Smith随机程序生成器用于测试和评估基于MLIR的编译器优化，通过差分测试发现多个编译器管道的漏洞，填补编译器测试空白。


<details>
  <summary>Details</summary>
Motivation: 现有工具缺乏对可适应的多级中间表示（MLIR）上下文的编译器测试和评估能力，需要能适应MLIR可扩展性的工具。

Method: 引入专门设计的随机程序生成器MLIR - Smith，对MLIR、LLVM、DaCe和DCIR进行差分测试。

Result: 通过测试发现了这些编译器管道中的多个漏洞。

Conclusion: MLIR - Smith填补了编译器测试领域的空白，强调了系统全面测试的重要性，为评估和改进编译器提供帮助，为未来工具发展奠定基础。

Abstract: Compilers are essential for the performance and correct execution of software and hold universal relevance across various scientific disciplines. Despite this, there is a notable lack of tools for testing and evaluating them, especially within the adaptable Multi-Level Intermediate Representation (MLIR) context. This paper addresses the need for a tool that can accommodate MLIR's extensibility, a feature not provided by previous methods such as Csmith. Here we introduce MLIR-Smith, a novel random program generator specifically designed to test and evaluate MLIR-based compiler optimizations. We demonstrate the utility of MLIR-Smith by conducting differential testing on MLIR, LLVM, DaCe, and DCIR, which led to the discovery of multiple bugs in these compiler pipelines. The introduction of MLIR-Smith not only fills a void in the realm of compiler testing but also emphasizes the importance of comprehensive testing within these systems. By providing a tool that can generate random MLIR programs, this paper enhances our ability to evaluate and improve compilers and paves the way for future tools, potentially shaping the wider landscape of software testing and quality assurance.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [362] [Order-Constrained Spectral Causality in Multivariate Time Series](https://arxiv.org/abs/2601.01216)
*Alejandro Rodriguez Dominguez*

Main category: stat.AP

TL;DR: 提出基于序约束谱非不变性的多元时间序列因果分析算子理论框架，给出统计性质，模拟验证效果，实证分析金融数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂多元时间序列中的因果关系和集体非线性依赖，需要新的因果分析方法。

Method: 基于序约束谱非不变性构建算子理论框架，用移位随机化处理非平滑极值统计量。

Result: 模拟显示有正确检验规模和强功效，实证表明金融系统因果结构有阶段和压力依赖性。

Conclusion: 该框架是复杂系统因果分析中相关性、因子和成对Granger分析的可扩展且可解释的补充。

Abstract: We introduce an operator-theoretic framework for causal analysis in multivariate time series based on order-constrained spectral non-invariance. Directional influence is defined as sensitivity of second-order dependence operators to admissible, order-preserving temporal deformations of a designated source component, yielding an intrinsically multivariate causal notion summarized through orthogonally invariant spectral functionals. Under linear Gaussian assumptions, the criterion coincides with linear Granger causality, while beyond this regime it captures collective and nonlinear directional dependence not reflected in pairwise predictability. We establish existence, uniform consistency, and valid inference for the resulting non-smooth supremum--infimum statistics using shift-based randomization that exploits order-induced group invariance, yielding finite-sample exactness under exact invariance and asymptotic validity under weak dependence without parametric assumptions. Simulations demonstrate correct size and strong power against distributed and bulk-dominated alternatives, including nonlinear dependence missed by linear Granger tests with appropriate feature embeddings. An empirical application to a high-dimensional panel of daily financial return series spanning major asset classes illustrates system-level causal monitoring in practice. Directional organization is episodic and stress-dependent, causal propagation strengthens while remaining multi-channel, dominant causal hubs reallocate rapidly, and statistically robust transmission channels are sparse and horizon-heterogeneous even when aggregate lead--lag asymmetry is weak. The framework provides a scalable and interpretable complement to correlation-, factor-, and pairwise Granger-style analyses for complex systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [363] [Speak the Art: A Direct Speech to Image Generation Framework](https://arxiv.org/abs/2601.00827)
*Mariam Saeed,Manar Amr,Farida Adel,Nada Hassan,Nour Walid,Eman Mohamed,Mohamed Hussein,Marwan Torki*

Main category: eess.AS

TL;DR: 现有语音转图像方法有不足，提出Speak the Art (STA)框架，结合语音编码网络和VQ - Diffusion网络，训练时用大模型监督，结果超现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前语音转图像方法有差距，语音编码网络信息不足，GAN存在不收敛等问题。

Method: 引入STA框架，包括语音编码网络和VQ - Diffusion网络，训练时语音编码网络受大预训练图文模型监督，用扩散模型替代GAN，探索多语言扩展并进行英阿双语训练。

Result: 结果大幅超越了现有模型。

Conclusion: 提出的STA框架在语音转图像任务上表现出色，能生成更优图像，有扩展到多语言的潜力。

Abstract: Direct speech-to-image generation has recently shown promising results. However, compared to text-to-image generation, there is still a large gap to enclose. Current approaches use two stages to tackle this task: speech encoding network and image generative adversarial network (GAN). The speech encoding networks in these approaches produce embeddings that do not capture sufficient linguistic information to semantically represent the input speech. GANs suffer from issues such as non-convergence, mode collapse, and diminished gradient, which result in unstable model parameters, limited sample diversity, and ineffective generator learning, respectively. To address these weaknesses, we introduce a framework called \textbf{Speak the Art (STA)} which consists of a speech encoding network and a VQ-Diffusion network conditioned on speech embeddings. To improve speech embeddings, the speech encoding network is supervised by a large pre-trained image-text model during training. Replacing GANs with diffusion leads to more stable training and the generation of diverse images. Additionally, we investigate the feasibility of extending our framework to be multilingual. As a proof of concept, we trained our framework with two languages: English and Arabic. Finally, we show that our results surpass state-of-the-art models by a large margin.

</details>


### [364] [Improving Code-Switching Speech Recognition with TTS Data Augmentation](https://arxiv.org/abs/2601.00935)
*Yue Heng Yeo,Yuchen Hu,Shreyas Gopal,Yizhou Peng,Hexin Liu,Eng Siong Chng*

Main category: eess.AS

TL;DR: 论文探讨用多语言TTS模型解决对话代码切换语音ASR数据稀缺问题，通过微调TTS模型生成合成语音，实验表明性能提升，确认TTS有效。


<details>
  <summary>Details</summary>
Motivation: 解决对话代码切换语音自动语音识别（ASR）中真实高质量标注语音数据稀缺问题。

Method: 在SEAME数据集上微调多语言CosyVoice2 TTS模型，生成合成的中英对话代码切换语音。

Result: 用合成语音增强真实语音后，DevMan上混合错误率（MER）从12.1%降至10.1%，DevSGE上从17.8%降至16.0%，性能持续提升。

Conclusion: 多语言TTS是在低资源对话代码切换场景中增强ASR鲁棒性的有效实用工具。

Abstract: Automatic speech recognition (ASR) for conversational code-switching speech remains challenging due to the scarcity of realistic, high-quality labeled speech data. This paper explores multilingual text-to-speech (TTS) models as an effective data augmentation technique to address this shortage. Specifically, we fine-tune the multilingual CosyVoice2 TTS model on the SEAME dataset to generate synthetic conversational Chinese-English code-switching speech, significantly increasing the quantity and speaker diversity of available training data. Our experiments demonstrate that augmenting real speech with synthetic speech reduces the mixed error rate (MER) from 12.1 percent to 10.1 percent on DevMan and from 17.8 percent to 16.0 percent on DevSGE, indicating consistent performance gains. These results confirm that multilingual TTS is an effective and practical tool for enhancing ASR robustness in low-resource conversational code-switching scenarios.

</details>


### [365] [MORE: Multi-Objective Adversarial Attacks on Speech Recognition](https://arxiv.org/abs/2601.01852)
*Xiaoxue Gao,Zexin Li,Yiming Chen,Nancy F. Chen*

Main category: eess.AS

TL;DR: 本文全面研究ASR模型在多攻击场景下的鲁棒性，提出MORE攻击方法，可同时降低识别准确率和推理效率，实验证明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注敌对攻击下ASR模型的准确率下降，对效率的鲁棒性研究不足，需全面了解ASR模型的脆弱性。

Method: 引入MORE攻击，通过分层阶段性排斥 - 锚定机制，将多目标对抗优化转化为分层框架依次实现双目标；提出REDO目标诱导重复文本生成。

Result: 实验表明MORE能产生更长的转录文本，同时保持高字错误率。

Conclusion: MORE在多目标对抗攻击方面有效，能使ASR模型以更高计算成本产生错误转录。

Abstract: The emergence of large-scale automatic speech recognition (ASR) models such as Whisper has greatly expanded their adoption across diverse real-world applications. Ensuring robustness against even minor input perturbations is therefore critical for maintaining reliable performance in real-time environments. While prior work has mainly examined accuracy degradation under adversarial attacks, robustness with respect to efficiency remains largely unexplored. This narrow focus provides only a partial understanding of ASR model vulnerabilities. To address this gap, we conduct a comprehensive study of ASR robustness under multiple attack scenarios. We introduce MORE, a multi-objective repetitive doubling encouragement attack, which jointly degrades recognition accuracy and inference efficiency through a hierarchical staged repulsion-anchoring mechanism. Specifically, we reformulate multi-objective adversarial optimization into a hierarchical framework that sequentially achieves the dual objectives. To further amplify effectiveness, we propose a novel repetitive encouragement doubling objective (REDO) that induces duplicative text generation by maintaining accuracy degradation and periodically doubling the predicted sequence length. Overall, MORE compels ASR models to produce incorrect transcriptions at a substantially higher computational cost, triggered by a single adversarial input. Experiments show that MORE consistently yields significantly longer transcriptions while maintaining high word error rates compared to existing baselines, underscoring its effectiveness in multi-objective adversarial attack.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [366] [Placenta Accreta Spectrum Detection using Multimodal Deep Learning](https://arxiv.org/abs/2601.00907)
*Sumaiya Ali,Areej Alhothali,Sameera Albasri,Ohoud Alzamzami,Ahmed Abduljabbar,Muhammad Alwazzan*

Main category: eess.IV

TL;DR: 研究开发并验证了融合3D MRI和2D超声的深度学习框架用于PAS检测，该多模态模型表现优于单模态模型。


<details>
  <summary>Details</summary>
Motivation: PAS是危及生命的产科并发症，早期准确的产前诊断对降低母婴风险至关重要，因此要开发增强PAS检测的方法。

Method: 设计使用中间特征级融合架构的多模态深度学习模型，结合3D MRI和2D超声扫描，经比较分析选择单模态特征提取器，用整理的数据集训练模型。

Result: 多模态融合模型在独立测试集上准确率达92.5%，AUC为0.927，优于仅用MRI和仅用超声的模型。

Conclusion: 整合MRI和超声特征可提供互补诊断信息，有潜力增强产前风险评估和改善患者预后。

Abstract: Placenta Accreta Spectrum (PAS) is a life-threatening obstetric complication involving abnormal placental invasion into the uterine wall. Early and accurate prenatal diagnosis is essential to reduce maternal and neonatal risks. This study aimed to develop and validate a deep learning framework that enhances PAS detection by integrating multiple imaging modalities. A multimodal deep learning model was designed using an intermediate feature-level fusion architecture combining 3D Magnetic Resonance Imaging (MRI) and 2D Ultrasound (US) scans. Unimodal feature extractors, a 3D DenseNet121-Vision Transformer for MRI and a 2D ResNet50 for US, were selected after systematic comparative analysis. Curated datasets comprising 1,293 MRI and 1,143 US scans were used to train the unimodal models and paired samples of patient-matched MRI-US scans was isolated for multimodal model development and evaluation. On an independent test set, the multimodal fusion model achieved superior performance, with an accuracy of 92.5% and an Area Under the Receiver Operating Characteristic Curve (AUC) of 0.927, outperforming the MRI-only (82.5%, AUC 0.825) and US-only (87.5%, AUC 0.879) models. Integrating MRI and US features provides complementary diagnostic information, demonstrating strong potential to enhance prenatal risk assessment and improve patient outcomes.

</details>


### [367] [Scale-aware Adaptive Supervised Network with Limited Medical Annotations](https://arxiv.org/abs/2601.01005)
*Zihan Li,Dandan Shan,Yunxiang Li,Paul E. Kinahan,Qingqi Hong*

Main category: eess.IV

TL;DR: 文章指出医学图像半监督分割存在标注稀缺等挑战，提出SASNet网络，有三项创新方法，在多数据集上表现超现有半监督方法，接近全监督水平，代码开源。


<details>
  <summary>Details</summary>
Motivation: 医学图像半监督学习存在标注稀缺、标注者差异大、多尺度特征集成不足等问题，现有半监督方法性能差。

Method: 提出SASNet双分支架构，采用尺度感知自适应重加权策略、视图方差增强机制、分割 - 回归一致性学习。

Result: 在LA、Pancreas - CT和BraTS数据集上，SASNet用有限标注数据取得优异性能，超越现有半监督方法，接近全监督水平。

Conclusion: SASNet能有效解决现有半监督方法的核心局限，在医学图像半监督分割中表现良好。

Abstract: Medical image segmentation faces critical challenges in semi-supervised learning scenarios due to severe annotation scarcity requiring expert radiological knowledge, significant inter-annotator variability across different viewpoints and expertise levels, and inadequate multi-scale feature integration for precise boundary delineation in complex anatomical structures. Existing semi-supervised methods demonstrate substantial performance degradation compared to fully supervised approaches, particularly in small target segmentation and boundary refinement tasks. To address these fundamental challenges, we propose SASNet (Scale-aware Adaptive Supervised Network), a dual-branch architecture that leverages both low-level and high-level feature representations through novel scale-aware adaptive reweight mechanisms. Our approach introduces three key methodological innovations, including the Scale-aware Adaptive Reweight strategy that dynamically weights pixel-wise predictions using temporal confidence accumulation, the View Variance Enhancement mechanism employing 3D Fourier domain transformations to simulate annotation variability, and segmentation-regression consistency learning through signed distance map algorithms for enhanced boundary precision. These innovations collectively address the core limitations of existing semi-supervised approaches by integrating spatial, temporal, and geometric consistency principles within a unified optimization framework. Comprehensive evaluation across LA, Pancreas-CT, and BraTS datasets demonstrates that SASNet achieves superior performance with limited labeled data, surpassing state-of-the-art semi-supervised methods while approaching fully supervised performance levels. The source code for SASNet is available at https://github.com/HUANGLIZI/SASNet.

</details>


### [368] [An Explainable Agentic AI Framework for Uncertainty-Aware and Abstention-Enabled Acute Ischemic Stroke Imaging Decisions](https://arxiv.org/abs/2601.01008)
*Md Rashadul Islam*

Main category: eess.IV

TL;DR: 提出用于急性缺血性中风成像的可解释代理AI框架，强调不确定性感知和选择性弃权，注重临床安全和透明性。


<details>
  <summary>Details</summary>
Motivation: 现有中风成像AI模型多为黑盒预测，无明确不确定性感知和弃权机制，引发安全和信任问题。

Method: 构建模块化代理管道，包括感知代理、不确定性估计代理和决策代理，依据不确定性阈值决定是否输出预测。

Result: 定性和案例分析显示，在诊断模糊区域和低信息切片会自然出现基于不确定性的弃权，框架还集成视觉解释机制。

Conclusion: 代理控制、不确定性感知和选择性弃权是开发安全可靠医学成像AI系统的关键设计原则。

Abstract: Artificial intelligence models have shown strong potential in acute ischemic stroke imaging, particularly for lesion detection and segmentation using computed tomography and magnetic resonance imaging. However, most existing approaches operate as black box predictors, producing deterministic outputs without explicit uncertainty awareness or structured mechanisms to abstain under ambiguous conditions. This limitation raises serious safety and trust concerns in high risk emergency radiology settings. In this paper, we propose an explainable agentic AI framework for uncertainty aware and abstention enabled decision support in acute ischemic stroke imaging. The framework follows a modular agentic pipeline in which a perception agent performs lesion aware image analysis, an uncertainty estimation agent computes slice level predictive reliability, and a decision agent determines whether to issue a prediction or abstain based on predefined uncertainty thresholds. Unlike prior stroke imaging systems that primarily focus on improving segmentation or classification accuracy, the proposed framework explicitly prioritizes clinical safety, transparency, and clinician aligned decision behavior. Qualitative and case based analyses across representative stroke imaging scenarios demonstrate that uncertainty driven abstention naturally emerges in diagnostically ambiguous regions and low information slices. The framework further integrates visual explanation mechanisms to support both predictive and abstention decisions, addressing a key limitation of existing uncertainty aware medical imaging systems. Rather than introducing a new performance benchmark, this work presents agentic control, uncertainty awareness, and selective abstention as essential design principles for developing safe and trustworthy medical imaging AI systems.

</details>


### [369] [Seamlessly Natural: Image Stitching with Natural Appearance Preservation](https://arxiv.org/abs/2601.01257)
*Gaetane Lorna N. Tchana,Damaris Belle M. Fotso,Antonio Hendricks,Christophe Bobda*

Main category: eess.IV

TL;DR: 本文介绍几何驱动图像拼接方法SENA，解决传统方法在视差和深度变化场景的问题，实验显示其在多视觉指标上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统图像拼接依赖单应性对齐，在双摄像头且场景深度大时会产生变形，SENA旨在解决这些基本局限。

Method: 提出基于分层仿射的变形策略、几何驱动的适当区域检测机制，以及基于锚点的接缝线切割和分割。

Result: 在具有挑战性的数据集上，SENA的对齐精度与领先的基于单应性的方法相当，在形状保留、纹理完整性和整体视觉真实感等关键视觉指标上显著优于它们。

Conclusion: SENA能在具有视差和深度变化的现实场景中有效拼接图像，避免传统方法的失真问题，提升拼接质量。

Abstract: This paper introduces SENA (SEamlessly NAtural), a geometry-driven image stitching approach that prioritizes structural fidelity in challenging real-world scenes characterized by parallax and depth variation. Conventional image stitching relies on homographic alignment, but this rigid planar assumption often fails in dual-camera setups with significant scene depth, leading to distortions such as visible warps and spherical bulging. SENA addresses these fundamental limitations through three key contributions. First, we propose a hierarchical affine-based warping strategy, combining global affine initialization with local affine refinement and smooth free-form deformation. This design preserves local shape, parallelism, and aspect ratios, thereby avoiding the hallucinated structural distortions commonly introduced by homography-based models. Second, we introduce a geometry-driven adequate zone detection mechanism that identifies parallax-minimized regions directly from the disparity consistency of RANSAC-filtered feature correspondences, without relying on semantic segmentation. Third, building upon this adequate zone, we perform anchor-based seamline cutting and segmentation, enforcing a one-to-one geometric correspondence across image pairs by construction, which effectively eliminates ghosting, duplication, and smearing artifacts in the final panorama.
  Extensive experiments conducted on challenging datasets demonstrate that SENA achieves alignment accuracy comparable to leading homography-based methods, while significantly outperforming them in critical visual metrics such as shape preservation, texture integrity, and overall visual realism.

</details>


### [370] [UniCrop: A Universal, Multi-Source Data Engineering Pipeline for Scalable Crop Yield Prediction](https://arxiv.org/abs/2601.01655)
*Emiliya Khidirova,Oktay Karakuş*

Main category: eess.IV

TL;DR: 研究提出UniCrop数据管道用于作物产量预测，应用于水稻数据集，证明其可扩展性和透明性。


<details>
  <summary>Details</summary>
Motivation: 现有作物产量预测方法存在作物或区域特定性，需数据工程工作，限制可扩展性、可重复性和部署。

Method: 引入通用可重用的UniCrop数据管道，自动处理多源环境数据，用mRMR进行特征降维，选15个特征训练4个机器学习模型。

Result: LightGBM单模型表现最佳，所有基线模型约束集成后精度提高。

Conclusion: UniCrop解决了作物产量建模中多源数据准备瓶颈，为农业分析提供基础，代码等已开源。

Abstract: Accurate crop yield prediction relies on diverse data streams, including satellite, meteorological, soil, and topographic information. However, despite rapid advances in machine learning, existing approaches remain crop- or region-specific and require data engineering efforts. This limits scalability, reproducibility, and operational deployment. This study introduces UniCrop, a universal and reusable data pipeline designed to automate the acquisition, cleaning, harmonisation, and engineering of multi-source environmental data for crop yield prediction. For any given location, crop type, and temporal window, UniCrop automatically retrieves, harmonises, and engineers over 200 environmental variables (Sentinel-1/2, MODIS, ERA5-Land, NASA POWER, SoilGrids, and SRTM), reducing them to a compact, analysis-ready feature set utilising a structured feature reduction workflow with minimum redundancy maximum relevance (mRMR). To validate, UniCrop was applied to a rice yield dataset comprising 557 field observations. Using only the selected 15 features, four baseline machine learning models (LightGBM, Random Forest, Support Vector Regression, and Elastic Net) were trained. LightGBM achieved the best single-model performance (RMSE = 465.1 kg/ha, $R^2 = 0.6576$), while a constrained ensemble of all baselines further improved accuracy (RMSE = 463.2 kg/ha, $R^2 = 0.6604$). UniCrop contributes a scalable and transparent data-engineering framework that addresses the primary bottleneck in operational crop yield modelling: the preparation of consistent and harmonised multi-source data. By decoupling data specification from implementation and supporting any crop, region, and time frame through simple configuration updates, UniCrop provides a practical foundation for scalable agricultural analytics. The code and implementation documentation are shared in https://github.com/CoDIS-Lab/UniCrop.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [371] [PyBatchRender: A Python Library for Batched 3D Rendering at Up to One Million FPS](https://arxiv.org/abs/2601.01288)
*Evgenii Rudakov,Jonathan Shock,Benjamin Ultan Cowley*

Main category: cs.GR

TL;DR: 介绍了Python库PyBatchRender，能实现高吞吐量批量3D渲染，速度快、易使用、开源可集成。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中3D渲染环境性能和复杂性瓶颈，平衡高速低级别引擎和慢速Python框架的问题。

Method: 基于Panda3D游戏引擎，采用优化的批量渲染技术。

Result: 在简单场景中实现超100万FPS，有高达1000X的加速，可让用户用几十行Python代码创建自定义场景。

Conclusion: PyBatchRender能让高性能3D模拟更普及，供研究人员和开发者使用。

Abstract: Reinforcement learning from pixels is often bottlenecked by the performance and complexity of 3D rendered environments. Researchers face a trade-off between high-speed, low-level engines and slower, more accessible Python frameworks. To address this, we introduce PyBatchRender, a Python library for high-throughput, batched 3D rendering that achieves over 1 million FPS on simple scenes. Built on the Panda3D game engine, it utilizes its mature ecosystem while enhancing performance through optimized batched rendering for up to 1000X speedups. Designed as a physics-agnostic renderer for reinforcement learning from pixels, PyBatchRender offers greater flexibility than dedicated libraries, simpler setup than typical game-engine wrappers, and speeds rivaling state-of-the-art C++ engines like Madrona. Users can create custom scenes entirely in Python with tens of lines of code, enabling rapid prototyping for scalable AI training. Open-source and easy to integrate, it serves to democratize high-performance 3D simulation for researchers and developers. The library is available at https://github.com/dolphin-in-a-coma/PyBatchRender.

</details>


### [372] [VARTS: A Tool for the Visualization and Analysis of Representative Time Series Data](https://arxiv.org/abs/2601.01361)
*Duosi Jin,Jianqiu Xu,Guidong Zhang*

Main category: cs.GR

TL;DR: 提出VARTS工具解决大规模时间序列可视化的杂乱问题，提升可视清晰度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模时间序列可视化中视觉杂乱和模式冗余问题，便于用户理解时间趋势。

Method: 在M4 - Greedy基础上，将M4采样、DTW相似度计算和贪心选择集成到统一工作流中，提供交互式图形界面。

Result: VARTS工具能在减少冗余的同时保留数据关键模式。

Conclusion: VARTS有效提升了大规模时间序列分析的视觉清晰度和可解释性。

Abstract: Large-scale time series visualization often suffers from excessive visual clutter and redundant patterns, making it difficult for users to understand the main temporal trends. To address this challenge, we present VARTS, an interactive visual analytics tool for representative time series selection and visualization. Building upon our previous work M4-Greedy, VARTS integrates M4-based sampling, DTW-based similarity computation, and greedy selection into a unified workflow for the identification and visualization of representative series. The tool provides a responsive graphical interface that allows users to import time series datasets, perform representative selection, and visualize both raw and reduced data through multiple coordinated views. By reducing redundancy while preserving essential data patterns, VARTS effectively enhances visual clarity and interpretability for large-scale time series analysis. The demo video is available at https://youtu.be/mS9f12Rf0jo.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [373] [Improved Accuracy for Private Continual Cardinality Estimation in Fully Dynamic Streams via Matrix Factorization](https://arxiv.org/abs/2601.02257)
*Joel Daniel Andersson,Palak Jain,Satchit Sivakumar*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study differentially-private statistics in the fully dynamic continual observation model, where many updates can arrive at each time step and updates to a stream can involve both insertions and deletions of an item. Earlier work (e.g., Jain et al., NeurIPS 2023 for counting distinct elements; Raskhodnikova & Steiner, PODS 2025 for triangle counting with edge updates) reduced the respective cardinality estimation problem to continual counting on the difference stream associated with the true function values on the input stream. In such reductions, a change in the original stream can cause many changes in the difference stream, this poses a challenge for applying private continual counting algorithms to obtain optimal error bounds. We improve the accuracy of several such reductions by studying the associated $\ell_p$-sensitivity vectors of the resulting difference streams and isolating their properties.
  We demonstrate that our framework gives improved bounds for counting distinct elements, estimating degree histograms, and estimating triangle counts (under a slightly relaxed privacy model), thus offering a general approach to private continual cardinality estimation in streaming settings. Our improved accuracy stems from tight analysis of known factorization mechanisms for the counting matrix in this setting; the key technical challenge is arguing that one can use state-of-the-art factorizations for sensitivity vector sets with the properties we isolate. Empirically and analytically, we demonstrate that our improved error bounds offer a substantial improvement in accuracy for cardinality estimation problems over a large range of parameters.

</details>


### [374] [Vouchsafe: A Zero-Infrastructure Capability Graph Model for Offline Identity and Trust](https://arxiv.org/abs/2601.02254)
*Jay Kuri*

Main category: cs.CR

TL;DR: 现有身份与信任系统在恶劣环境易崩溃，本文提出ZI - CG模型及Vouchsafe实例，表明可构建离线可验证信任基础。


<details>
  <summary>Details</summary>
Motivation: 现代身份和信任系统在灾难区、网络故障或对抗环境中依赖网络，通信不可用或不可信时信任无法验证，需无基础设施的安全身份和信任系统。

Method: 引入Zero - Infrastructure Capability Graph (ZI - CG)模型，用Ed25519、SHA - 256和结构化JSON Web Tokens构建Vouchsafe实例。

Result: 能仅使用评估时的加密数据构建实用的离线可验证信任基础。

Conclusion: 无需现有基础设施，可实现安全的身份和信任。

Abstract: Modern identity and trust systems collapse in the environments where they are needed most: disaster zones, disconnected or damaged networks, and adversarial conditions such as censorship or infrastructure interference. These systems depend on functioning networks to reach online authorities, resolvers, directories, and revocation services, leaving trust unverifiable whenever communication is unavailable or untrusted. This work demonstrates that secure identity and trust are possible without such infrastructure. We introduce the Zero-Infrastructure Capability Graph (ZI-CG), a model showing that identity, delegation, and revocation can be represented as self-contained, signed statements whose validity is determined entirely by local, deterministic evaluation. We further present Vouchsafe, a complete working instantiation of this model built using widely deployed primitives including Ed25519, SHA-256, and structured JSON Web Tokens, requiring no new cryptography or online services. The results show that a practical, offline-verifiable trust substrate can be constructed today using only the cryptographic data presented at evaluation time.

</details>


### [375] [MCP-SandboxScan: WASM-based Secure Execution and Runtime Analysis for MCP Tools](https://arxiv.org/abs/2601.01241)
*Zhuoran Tan,Run Hao,Jeremy Singer,Yutian Tang,Christos Anagnostopoulos*

Main category: cs.CR

TL;DR: 提出MCP - SandboxScan框架，用于安全执行不可信工具并生成审核报告，通过案例研究验证效果，还与基准对比分析。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大语言模型代理带来新的安全风险，现有扫描器多关注静态工件，分析运行时行为具有挑战性。

Method: 基于Model Context Protocol (MCP)构建MCP - SandboxScan轻量级框架，在WebAssembly/WASI沙箱中安全执行不可信工具，从运行时输出中提取相关汇聚点、实例化外部输入候选、通过基于片段的子字符串匹配将源与汇聚点关联。

Result: 对三个代表性工具的案例研究表明，MCP - SandboxScan能在外部输入出现在提示/消息或工具返回有效负载中时显示溯源证据，还能暴露文件系统能力违规情况；与轻量级静态字符串签名基线对比，使用微基准测试表征了转换下的假阴性和短令牌冲突导致的假阳性。

Conclusion: MCP - SandboxScan有助于安全分析工具增强的LLM代理运行时行为，可有效发现安全问题。

Abstract: Tool-augmented LLM agents raise new security risks: tool executions can introduce runtime-only behaviors, including prompt injection and unintended exposure of external inputs (e.g., environment secrets or local files). While existing scanners often focus on static artifacts, analyzing runtime behavior is challenging because directly executing untrusted tools can itself be dangerous. We present MCP-SandboxScan, a lightweight framework motivated by the Model Context Protocol (MCP) that safely executes untrusted tools inside a WebAssembly/WASI sandbox and produces auditable reports of external-to-sink exposures. Our prototype (i) extracts LLM-relevant sinks from runtime outputs (prompt/messages and structured tool-return fields), (ii) instantiates external-input candidates from environment values, mounted file contents, and output-surfaced HTTP fetch intents, and (iii) links sources to sinks via snippet-based substring matching. Case studies on three representative tools show that MCP-SandboxScan can surface provenance evidence when external inputs appear in prompt/messages or tool-return payloads, and can expose filesystem capability violations as runtime evidence. We further compare against a lightweight static string-signature baseline and use a micro-benchmark to characterize false negatives under transformations and false positives from short-token collisions.

</details>


### [376] [The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models](https://arxiv.org/abs/2601.00867)
*Giuseppe Canale,Kashyap Thimmaraju*

Main category: cs.CR

TL;DR: 大语言模型应用拓展，现有对抗测试范式不完整，本文用网络安全心理学框架测试，发现模型存在类人漏洞，需开发‘心理防火墙’。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对抗测试范式主要关注技术攻击向量，忽略了模型继承的人类心理架构及相关漏洞，需更全面的测试。

Method: 应用网络安全心理学框架，引入合成心理评估协议将框架指标转化为针对大语言模型决策的对抗场景。

Result: 对七个主要大语言模型家族的初步测试显示，模型对传统越狱攻击防御较强，但易受权威梯度操纵、时间压力利用和收敛状态攻击等类人认知失败模式影响。

Conclusion: 提出‘拟人化漏洞继承’现象，建议安全社区开发‘心理防火墙’保护处于对抗环境的AI代理。

Abstract: Large Language Models (LLMs) are rapidly transitioning from conversational assistants to autonomous agents embedded in critical organizational functions, including Security Operations Centers (SOCs), financial systems, and infrastructure management. Current adversarial testing paradigms focus predominantly on technical attack vectors: prompt injection, jailbreaking, and data exfiltration. We argue this focus is catastrophically incomplete. LLMs, trained on vast corpora of human-generated text, have inherited not merely human knowledge but human \textit{psychological architecture} -- including the pre-cognitive vulnerabilities that render humans susceptible to social engineering, authority manipulation, and affective exploitation. This paper presents the first systematic application of the Cybersecurity Psychology Framework (\cpf{}), a 100-indicator taxonomy of human psychological vulnerabilities, to non-human cognitive agents. We introduce the \textbf{Synthetic Psychometric Assessment Protocol} (\sysname{}), a methodology for converting \cpf{} indicators into adversarial scenarios targeting LLM decision-making. Our preliminary hypothesis testing across seven major LLM families reveals a disturbing pattern: while models demonstrate robust defenses against traditional jailbreaks, they exhibit critical susceptibility to authority-gradient manipulation, temporal pressure exploitation, and convergent-state attacks that mirror human cognitive failure modes. We term this phenomenon \textbf{Anthropomorphic Vulnerability Inheritance} (AVI) and propose that the security community must urgently develop ``psychological firewalls'' -- intervention mechanisms adapted from the Cybersecurity Psychology Intervention Framework (\cpif{}) -- to protect AI agents operating in adversarial environments.

</details>


### [377] [Device-Native Autonomous Agents for Privacy-Preserving Negotiations](https://arxiv.org/abs/2601.00911)
*Joyjit Roy*

Main category: cs.CR

TL;DR: 引入设备原生自主AI代理系统用于隐私保护的谈判，评估显示有高成功率、低延迟和强隐私保护，提高用户信任。


<details>
  <summary>Details</summary>
Motivation: 当前保险和B2B商务自动化谈判系统通过集中服务器传输敏感数据，在便利性和隐私性间权衡，增加安全风险并降低用户信任。

Method: 提出仅在用户硬件上运行的系统，集成零知识证明确保隐私，采用提炼世界模型支持设备端推理，架构包含六个技术组件。

Result: 系统在保险和B2B采购场景评估中，平均成功率87%，延迟比云基线改善2.4倍，零知识证明实现强隐私保护，有决策轨迹时用户信任度提高27%。

Conclusion: 为隐私敏感金融领域的可信自主代理奠定基础。

Abstract: Automated negotiations in insurance and business-to-business (B2B) commerce encounter substantial challenges. Current systems force a trade-off between convenience and privacy by routing sensitive financial data through centralized servers, increasing security risks, and diminishing user trust. This study introduces a device-native autonomous Artificial Intelligence (AI) agent system for privacy-preserving negotiations. The proposed system operates exclusively on user hardware, enabling real-time bargaining while maintaining sensitive constraints locally. It integrates zero-knowledge proofs to ensure privacy and employs distilled world models to support advanced on-device reasoning. The architecture incorporates six technical components within an agentic AI workflow. Agents autonomously plan negotiation strategies, conduct secure multi-party bargaining, and generate cryptographic audit trails without exposing user data to external servers. The system is evaluated in insurance and B2B procurement scenarios across diverse device configurations. Results show an average success rate of 87%, a 2.4x latency improvement over cloud baselines, and strong privacy preservation through zero-knowledge proofs. User studies show 27% higher trust scores when decision trails are available. These findings establish a foundation for trustworthy autonomous agents in privacy-sensitive financial domains.

</details>


### [378] [Emoji-Based Jailbreaking of Large Language Models](https://arxiv.org/abs/2601.00936)
*M P V S Gopinadh,S Mahaboob Hussain*

Main category: cs.CR

TL;DR: 研究基于表情符号的大语言模型越狱攻击，评估了四种开源大模型，发现模型存在特定漏洞，强调需处理好表情符号表示。


<details>
  <summary>Details</summary>
Motivation: 大语言模型安全对齐机制可被对抗性提示工程绕过，此前研究多关注表情符号对安全判断器或分类器的攻击，本文要研究对大模型直接提示级别的漏洞。

Method: 在Mistral 7B、Qwen 2 7B、Gemma 2 9B和Llama 3 8B四个开源大模型上评估50个基于表情符号的提示，用越狱成功率、安全对齐遵守情况和延迟为指标，对响应分类。

Result: Gemma 2 9B和Mistral 7B越狱成功率10%，Qwen 2 7B完全对齐（0%成功率）；卡方检验证实模型间存在显著差异。

Conclusion: 结果揭示了安全机制的局限性，强调在提示级安全和对齐流程中需系统处理基于表情符号的表示。

Abstract: Large Language Models (LLMs) are integral to modern AI applications, but their safety alignment mechanisms can be bypassed through adversarial prompt engineering. This study investigates emoji-based jailbreaking, where emoji sequences are embedded in textual prompts to trigger harmful and unethical outputs from LLMs. We evaluated 50 emoji-based prompts on four open-source LLMs: Mistral 7B, Qwen 2 7B, Gemma 2 9B, and Llama 3 8B. Metrics included jailbreak success rate, safety alignment adherence, and latency, with responses categorized as successful, partial and failed. Results revealed model-specific vulnerabilities: Gemma 2 9B and Mistral 7B exhibited 10 % success rates, while Qwen 2 7B achieved full alignment (0% success). A chi-square test (chi^2 = 32.94, p < 0.001) confirmed significant inter-model differences. While prior works focused on emoji attacks targeting safety judges or classifiers, our empirical analysis examines direct prompt-level vulnerabilities in LLMs. The results reveal limitations in safety mechanisms and highlight the necessity for systematic handling of emoji-based representations in prompt-level safety and alignment pipelines.

</details>


### [379] [AI-Powered Hybrid Intrusion Detection Framework for Cloud Security Using Novel Metaheuristic Optimization](https://arxiv.org/abs/2601.01134)
*Maryam Mahdi Alhusseini,Alireza Rouhi,Mohammad-Reza Feizi-Derakhshi*

Main category: cs.CR

TL;DR: 本文提出HyIDS，用EVO进行特征选择，结合机器学习方法增强云计算安全，在两个数据集上测试效果良好，证明EVO可显著提升云计算安全。


<details>
  <summary>Details</summary>
Motivation: 解决云计算中入侵检测系统面临的数据集倾斜和分类模型性能不佳问题。

Method: 提出HyIDS，使用EVO进行特征选择，集成SVM、RF、D_Tree和KNN四个机器学习模型，用下采样技术平衡数据集。

Result: EVO有效减少了数据集特征数量，24次试验显示分类准确率、精度和召回率显著提高，D_TreeEVO模型在两个数据集上取得高准确率和F1分数。

Conclusion: EVO能显著提高云计算的网络安全。

Abstract: Cybersecurity poses considerable problems to Cloud Computing (CC), especially regarding Intrusion Detection Systems (IDSs), facing difficulties with skewed datasets and suboptimal classification model performance. This study presents the Hybrid Intrusion Detection System (HyIDS), an innovative IDS that employs the Energy Valley Optimizer (EVO) for Feature Selection (FS). Additionally, it introduces a novel technique for enhancing the cybersecurity of cloud computing through the integration of machine learning methodologies with the EVO Algorithm. The Energy Valley Optimizer (EVO) effectively diminished features in the CIC-DDoS2019 dataset from 88 to 38 and in the CSE-CIC-IDS2018 data from 80 to 43, significantly enhancing computing efficiency. HyIDS incorporates four Machine Learning (ML) models: Support Vector Machine (SVM), Random Forest (RF), Decision Tree (D_Tree), and K-Nearest Neighbors (KNN). The proposed HyIDS was assessed utilizing two real-world intrusion datasets, CIC-DDoS2019 and CSE-CIC-IDS2018, both distinguished by considerable class imbalances. The CIC-DDoS2019 dataset has a significant imbalance between DDoS assault samples and legal traffic, while the CSE-CIC-IDS2018 dataset primarily comprises benign traffic with insufficient representation of attack types, complicating the detection of minority attacks. A downsampling technique was employed to balance the datasets, hence improving detection efficacy for both benign and malicious traffic. Twenty-four trials were done, revealing substantial enhancements in categorization accuracy, precision, and recall. Our suggested D_TreeEVO model attained an accuracy rate of 99.13% and an F1 score of 98.94% on the CIC-DDoS2019 dataset, and an accuracy rate of 99.78% and an F1 score of 99.70% on the CSE-CIC-IDS2018 data. These data demonstrate that EVO significantly improves cybersecurity in Cloud Computing (CC).

</details>


### [380] [Aggressive Compression Enables LLM Weight Theft](https://arxiv.org/abs/2601.01296)
*Davis Brown,Juan-Pablo Rivera,Dan Hendrycks,Mantas Mazeika*

Main category: cs.CR

TL;DR: 论文指出强大且昂贵的前沿AI使窃取模型权重的攻击增多，模型权重可压缩性增加了大语言模型的泄露风险，攻击者可大幅压缩权重，还研究了三种防御方法，法医水印防御效果好且成本低。


<details>
  <summary>Details</summary>
Motivation: 前沿AI开发成本高，攻击者有窃取模型权重的动机，需研究模型权重外渗风险及防御方法。

Method: 分析模型权重可压缩性对大语言模型外渗风险的影响；针对外渗定制压缩方法；研究三种降低外渗风险的防御方法，即让模型更难压缩、更难被发现、使用法医水印进行事后分析。

Result: 攻击者可实现16到100倍的压缩，将非法传输模型权重的时间从数月缩短至数天；法医水印防御方法有效且成本低。

Conclusion: 法医水印防御是减轻权重外渗风险有吸引力的手段。

Abstract: As frontier AIs become more powerful and costly to develop, adversaries have increasing incentives to steal model weights by mounting exfiltration attacks. In this work, we consider exfiltration attacks where an adversary attempts to sneak model weights out of a datacenter over a network. While exfiltration attacks are multi-step cyber attacks, we demonstrate that a single factor, the compressibility of model weights, significantly heightens exfiltration risk for large language models (LLMs). We tailor compression specifically for exfiltration by relaxing decompression constraints and demonstrate that attackers could achieve 16x to 100x compression with minimal trade-offs, reducing the time it would take for an attacker to illicitly transmit model weights from the defender's server from months to days. Finally, we study defenses designed to reduce exfiltration risk in three distinct ways: making models harder to compress, making them harder to 'find,' and tracking provenance for post-attack analysis using forensic watermarks. While all defenses are promising, the forensic watermark defense is both effective and cheap, and therefore is a particularly attractive lever for mitigating weight-exfiltration risk.

</details>


### [381] [Exposing Hidden Interfaces: LLM-Guided Type Inference for Reverse Engineering macOS Private Frameworks](https://arxiv.org/abs/2601.01673)
*Arina Kharlamova,Youcheng Sun,Ting Yu*

Main category: cs.CR

TL;DR: 提出MOTIF框架集成工具分析与微调大模型用于Objective - C类型推断，在基准测试中提升签名恢复率，可助力macOS安全研究。


<details>
  <summary>Details</summary>
Motivation: macOS私有框架未文档化且为剥离二进制文件，使安全分析复杂。

Method: 构建MOTIF框架，用代理管理运行时元数据提取等，模型生成候选方法签名并验证完善为可编译头文件。

Result: 在MOTIF - Bench上签名恢复率从15%提升到86%，工具使用正确性和推断稳定性有提升，私有框架重构头文件可编译、链接。

Conclusion: MOTIF为macOS内部系统审计奠定可扩展基础。

Abstract: Private macOS frameworks underpin critical services and daemons but remain undocumented and distributed only as stripped binaries, complicating security analysis. We present MOTIF, an agentic framework that integrates tool-augmented analysis with a finetuned large language model specialized for Objective-C type inference. The agent manages runtime metadata extraction, binary inspection, and constraint checking, while the model generates candidate method signatures that are validated and refined into compilable headers. On MOTIF-Bench, a benchmark built from public frameworks with groundtruth headers, MOTIF improves signature recovery from 15% to 86% compared to baseline static analysis tooling, with consistent gains in tool-use correctness and inference stability. Case studies on private frameworks show that reconstructed headers compile, link, and facilitate downstream security research and vulnerability studies. By transforming opaque binaries into analyzable interfaces, MOTIF establishes a scalable foundation for systematic auditing of macOS internals.

</details>


### [382] [Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization](https://arxiv.org/abs/2601.01747)
*Jiwei Guan,Haibo Jin,Haohan Wang*

Main category: cs.CR

TL;DR: 现有白盒攻击方法有局限，文章提出基于 ZO - SPSA 的黑盒越狱攻击，在三个大视觉语言模型上评估，显示黑盒越狱可行性并暴露模型安全机制弱点。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型易受对抗越狱攻击，现有白盒攻击方法存在需要全模型访问、计算成本高和对抗转移性不足等缺点，不适合现实黑盒场景。

Method: 提出基于零阶优化和同时扰动随机逼近（ZO - SPSA）的大视觉语言模型黑盒越狱攻击方法，该方法具有无梯度近似、与模型无关优化和低资源需求等优势。

Result: 在 InstructBLIP、LLaVA 和 MiniGPT - 4 三个模型上评估，在 InstructBLIP 上越狱成功率最高达 83.0%；MiniGPT - 4 生成的对抗样本对其他模型有强转移性，ASR 达 64.18%。

Conclusion: 证明黑盒越狱在现实中可行，且当前大视觉语言模型安全机制存在关键弱点。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [383] [Forecasting ICT-Driven Trade Competitiveness 2024-2028: A Cluster and Scenario Analysis](https://arxiv.org/abs/2601.00842)
*Elias Aravantinos*

Main category: econ.GN

TL;DR: 引入数字贸易竞争力指数（DCIT）评估国家数字贸易竞争力，经分析该指数稳健，能解释贸易连接性增长，强调综合数字投资策略重要性。


<details>
  <summary>Details</summary>
Motivation: 评估国家数字贸易竞争力，明确影响国家参与数字贸易的条件。

Method: 构建包含多种因素的DCIT指数，进行敏感性分析、预测验证和情景模拟。

Result: DCIT指数稳健，能较好解释贸易连接性增长，综合ICT和FDI加速策略效果更佳，高增长情景可提升中高级集群竞争力。

Conclusion: 综合数字投资策略对于提升国家数字贸易竞争力至关重要。

Abstract: This study introduces the Digital Competitiveness Index for Trade (DCIT), a composite metric integrating ICT readiness, broadband adoption, GDP per capita, foreign direct investment, government effectiveness, and trade volume to assess countries' digital trade competitiveness. The index captures the enabling conditions -- ICT innovation capacity, broadband diffusion, investment intensity, and macroeconomic fundamentals - that shape a nation's ability to participate in digital trade. Sensitivity analysis demonstrates strong robustness: adjusting ICT-FDI weights alters DCIT outcomes by only 26%, with near perfect linearity (R^2 = 0.9996). Predictive validation shows that DCIT is a strong explainer of trade connectivity growth (R^2 = 0.67) but a modest predictor of GDP expansion. Scenario simulations reveal that combined ICT and FDI acceleration consistently outperforms single-lever strategies, with gains increasing by cluster maturity (up to 10% in advanced clusters). High-growth scenarios generate a 50-60% uplift in competitiveness for mid-tier and advanced clusters, underscoring the importance of integrated digital investment strategies.

</details>


### [384] [Investigation into U.S. Citizen and Non-Citizen Worker Health Insurance and Employment](https://arxiv.org/abs/2601.00896)
*Annabelle Yao*

Main category: econ.GN

TL;DR: 本文结合统计分析和机器学习方法分析社会经济融合与不平等，确定了人口群体间的关联和差异，揭示了系统性不平等。


<details>
  <summary>Details</summary>
Motivation: 以往研究多孤立考察不平等方面，缺乏结合统计分析和机器学习挖掘人口数据隐藏结构的研究，本研究旨在填补此空白。

Method: 运用统计分析（$χ^2$独立性检验和双比例Z检验）和机器学习聚类技术（K - Modes和K - Prototypes），以及t - SNE可视化和CatBoost分类。

Result: 确定了有医保、优质教育和就业的人口比例，发现就业与公民身份有关联，确定5个不同人口群体，揭示医保获取的显著差异。

Conclusion: 研究有助于更细致地理解社会经济分层，发现面临多重劣势的人口群体，凸显医保获取的系统性不平等。

Abstract: Socioeconomic integration is a critical dimension of social equity, yet persistent disparities remain in access to health insurance, education, and employment across different demographic groups. While previous studies have examined isolated aspects of inequality, there is limited research that integrates both statistical analysis and advanced machine learning to uncover hidden structures within population data. This study leverages statistical analysis ($χ^2$ test of independence and Two Proportion Z-Test) and machine learning clustering techniques -- K-Modes and K-Prototypes -- along with t-SNE visualization and CatBoost classification to analyze socioeconomic integration and inequality. Using statistical tests, we identified the proportion of the population with healthcare insurance, quality education, and employment. With this data, we concluded that there was an association between employment and citizenship status. Moreover, we were able to determine 5 distinct population groups using Machine Learning classification. The five clusters our analysis identifies reveal that while citizenship status shows no association with workforce participation, significant disparities exist in access to employer-sponsored health insurance. Each cluster represents a distinct demographic of the population, showing that there is a primary split along the lines of educational attainment which separates Clusters 0 and 4 from Clusters 1, 2, and 3. Furthermore, labor force status and nativity serve as secondary differentiators. Non-citizens are also disproportionately concentrated in precarious employment without benefits, highlighting systemic inequalities in healthcare access. By uncovering demographic clusters that face compounded disadvantages, this research contributes to a more nuanced understanding of socioeconomic stratification.

</details>


### [385] [Sticky Homelessness (Working Paper)](https://arxiv.org/abs/2601.00914)
*Richard Yun*

Main category: econ.GN

TL;DR: 研究利用新数据和多种方法分析美国无家可归问题，发现租金上涨大幅增加无家可归率，提出理论模型和IV策略，认为无家可归是住房问题，政策需结合住房干预和解决其他障碍。


<details>
  <summary>Details</summary>
Motivation: 美国城市无家可归问题日益突出，但成因存在争议，旨在探究其成因。

Method: 构建MSA层面无家可归人数全国面板数据，采用长差分回归、准差分矩条件、提出理论模型、使用Bartik工具变量策略。

Result: 租金上涨大幅增加无家可归率，下降影响小，存在数据不对称性。

Conclusion: 无家可归是住房问题，有效政策需结合住房市场干预和解决无家可归者面临的障碍。

Abstract: Homelessness in American cities is becoming an ever more prominent issue, but its causes remain contested, ranging from mental health and substance abuse to housing affordability and local labor markets. To shed light on this issue, I construct a novel MSA-level national panel of homelessness counts using data from the U.S. Department of Housing and Urban Development. Using a long-differencing regression specification with the changes in rent entered in piecewise linear form, I find that rent increases predict large increases in homelessness rates, but decreases have little to no effect. The same conclusions are reached when I use a quasi-differencing moment condition, assuming a multiplicative mean specification. Then, I propose a theoretical model of the low-end housing market that explains the asymmetry I find in the data. Finally, I outline an IV strategy that instruments rent changes with a Bartik instrument of predicted employment growth interacted with local housing-supply elasticities. My findings suggest that homelessness is a housing problem; however, because the response is sticky downward, effective policy must complement housing-market interventions with measures that address barriers faced by people experiencing homelessness.

</details>


### [386] [A dynamic factor semiparametric model for VaR and expected shortfall driven by realized measures](https://arxiv.org/abs/2601.01142)
*Sicheng Fu*

Main category: econ.GN

TL;DR: 本文提出由已实现信息驱动的半参数联合VaRES框架，优于多个基准模型。


<details>
  <summary>Details</summary>
Motivation: 受尾部风险生成经济机制驱使，提出合适模型分析尾部风险。

Method: 基于CAViaR分位数递归，引入动态ESVaR缺口，利用量测方程处理多个已实现测度得高频风险创新，再通过动态因子模型聚合以提取共同高频尾部风险因子。

Result: 该模型在多个损失函数上始终优于分位数回归、基于EVT和GARCH类型的基准模型。

Conclusion: 将高频信息直接嵌入尾部风险生成层至关重要。

Abstract: This paper proposes a semiparametric joint VaRES framework driven by realized information, mo tivated by the economic mechanisms underlying tail risk generation. Building on the CAViaR quantile recursion, the model introduces a dynamic ESVaR gap to capture time-varying tail sever ity, while measurement equations transform multiple realized measures into high-frequency risk innovations.These innovations are further aggregated through a dynamic factor model, extracting common high-frequency tail risk factors that affect the quantile level and tail thickness through dis tinct risk channels. This structure explicitly separates changes in risk levels from the intensification of tail risk.Empirical evidence shows that the proposed model consistently outperforms quantile regression, EVT-based, and GARCH-type benchmarks across multiple loss functions, highlighting the importance of embedding high-frequency information directly into the tail risk generation layer

</details>


### [387] [Strategic Expression, Popularity Traps, and Welfare in Social Media](https://arxiv.org/abs/2601.01370)
*Zafer Kanik,Zaruhi Hakobyan*

Main category: econ.GN

TL;DR: 提出社交媒體戰略表達的功利主義框架，分析不同話題下的福利結果和極化現象，指出戰略表達的陷阱及緩解方法。


<details>
  <summary>Details</summary>
Motivation: 解決社交媒體平臺鼓勵人氣而非真實性，用戶戰略性表達的問題。

Method: 構建功利主義框架，考慮用戶帖子人氣和內容與真實觀點的契合度。

Result: 淺顯話題帶來帕累托改進，激烈話題使所有人福利下降；戰略表達在不同事件中影響極化；存在人氣陷阱；偏好算法和同質暴露可緩解。

Conclusion: 該框架填補社交媒體文獻空白，引入人氣激勵新渠道。

Abstract: Social media platforms systematically reward popularity but not authenticity, incentivizing users to strategically tailor their expression for attention. We develop a utilitarian framework addressing strategic expression in social media. Agents hold fixed heterogeneous authentic opinions and derive (i) utility gains from the popularity of their own posts--measured by likes received--, and (ii) utility gains (losses) from exposure to content that aligns with (diverges from) their authentic opinion. Social media interaction acts as a state-dependent welfare amplifier: light topics generate Pareto improvements, whereas intense topics make everyone worse off in a polarized society (e.g., political debates during elections). Moreover, strategic expression amplifies social media polarization during polarized events while dampening it during unified events (e.g., national celebrations). Consequently, strategic distortions magnify welfare outcomes, expanding aggregate gains in light topics while exacerbating losses in intense, polarized ones. Counterintuitively, strategic agents often face a popularity trap: posting a more popular opinion is individually optimal, yet collective action by similar agents eliminates their authentic opinion from the platform, leaving them worse off than under the authentic-expression benchmark. Preference-based algorithms--widely used by platforms--or homophilic exposures discipline popularity-driven behavior, narrowing the popularity trap region and limiting its welfare effects. Our framework fills a critical gap in the social media literature by providing a microfoundation for user welfare that maps to observable metrics, while also introducing popularity incentives as an unexplored channel in social networks distinct from the canonical mechanisms of conformity, learning, persuasion, and (mis)information transmission.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [388] [Density-based topology optimization for turbulent fluid flow using the standard k-epsilon RANS model with wall-functions imposed through an implicit wall penalty formulation](https://arxiv.org/abs/2601.02202)
*Amirhossein Bayat,Hao Li,Joe Alexandersen*

Main category: physics.flu-dyn

TL;DR: 提出用于雷诺平均Navier - Stokes方程的隐式壁函数公式，可实现高雷诺数湍流拓扑优化，降低计算成本并通过基准测试验证。


<details>
  <summary>Details</summary>
Motivation: 湍流流动对边界附近精细网格要求高，拓扑优化中常见方法存在精度低和高估边界层厚度问题，且基于密度的拓扑优化难以自然引入壁函数。

Method: 提出基于雷诺平均Navier - Stokes方程、标准k - ε模型的隐式壁函数公式，从设计变量梯度中提取法向信息，用惩罚法将壁函数引入方程，无需贴合实体网格。

Result: 方法能在粗网格上获得准确解，降低计算成本；在三个基准测试中准确恢复近壁速度轮廓，匹配显式壁函数的验证模拟结果，而传统方法预测不准确。

Conclusion: 该方法为高雷诺数湍流拓扑优化提供可靠途径，兼具密度拓扑优化的灵活性和边界层精度，降低计算成本。

Abstract: Turbulent flows have high requirements for very fine meshes near the boundary to ensure accuracy. In the context of topology optimization (TO), such fine meshes become unrealistic and common approaches are hampered by low accuracy and overestimation of boundary layer thickness. Wall-functions are a natural way to ease the computational requirements, but they are not naturally imposed in density-based TO due to the diffuse design parametrization. We propose an implicit wall-function formulation for the Reynolds-Averaged Navier-Stokes (RANS), standard k-epsilon model that extracts wall-normal information directly from the gradient of the design variable and enables a penalty-based formulation for imposing wall-functions to the RANS equations, without the need for body-fitted meshes. The method provides a reliable route to high Reynolds number turbulent topology optimization, delivering boundary layer accuracy comparable to explicit-wall body-fitted analyses, while retaining the flexibility of density-based TO. Furthermore, because wall effects are modeled using wall-functions, accurate solutions are obtained on substantially coarser meshes, leading to significant reductions in computational cost. The approach is validated on three canonical benchmarks over Reynolds numbers up to Re = 2e5: a pipe-bend; a U-bend; and a Tesla-valve. Across all cases, the proposed method accurately recovers near-wall velocity profiles, closely matching verification simulations on body-fitted meshes with explicit wall-functions. In contrast, a conventional turbulent TO formulation, without the proposed wall-function treatment, mispredicts boundary-layer development and yields sub-optimal results.

</details>


### [389] [Multi-fidelity graph-based neural networks architectures to learn Navier-Stokes solutions on non-parametrized 2D domains](https://arxiv.org/abs/2601.02157)
*Francesco Songia,Raoul Sallé de Chou,Hugues Talbot,Irene Vignon-Clementel*

Main category: physics.flu-dyn

TL;DR: 提出基于图的多保真度学习框架预测二维几何中定常Navier - Stokes解，结合多种架构，嵌入物理知识，Mamba方法降低计算成本并保证性能。


<details>
  <summary>Details</summary>
Motivation: 实现非参数化二维几何中定常Navier - Stokes解的准确预测。

Method: 通过连续逼近引导学习过程，结合图神经网络与Transformer和Mamba架构，采用编码 - 处理 - 物理信息解码管道嵌入物理知识，用加权最小二乘法构建代数算子计算导数，引入增强图卷积。

Result: 充分捕捉了速度和压力场的局部和长程依赖关系，Mamba方法显著降低计算成本，并保持性能。

Conclusion: 利用物理知识和流体动力学见解能有效地引导学习过程，使得预测更加规则和准确。

Abstract: We propose a graph-based, multi-fidelity learning framework for the prediction of stationary Navier--Stokes solutions in non-parametrized two-dimensional geometries. The method is designed to guide the learning process through successive approximations, starting from reduced-order and full Stokes models, and progressively approaching the Navier--Stokes solution. To effectively capture both local and long-range dependencies in the velocity and pressure fields, we combine graph neural networks with Transformer and Mamba architectures. While Transformers achieve the highest accuracy, we show that Mamba can be successfully adapted to graph-structured data through an unsupervised node-ordering strategy. The Mamba approach significantly reduces computational cost while maintaining performance. Physical knowledge is embedded directly into the architecture through an encoding -- processing -- physics informed decoding pipeline. Derivatives are computed through algebraic operators constructed via the Weighted Least Squares method. The flexibility of these operators allows us not only to make the output obey the governing equations, but also to constrain selected hidden features to satisfy mass conservation. We introduce additional physical biases through an enriched graph convolution with the same differential operators describing the PDEs. Overall, we successfully guide the learning process by physical knowledge and fluid dynamics insights, leading to more regular and accurate predictions

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [390] [Solving Matrix Games with Even Fewer Matrix-Vector Products](https://arxiv.org/abs/2601.02347)
*Ishani Karmarkar,Liam O'Carroll,Aaron Sidford*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of computing an $ε$-approximate Nash equilibrium of a two-player, bilinear, zero-sum game with a bounded payoff matrix $A \in \mathbb{R}^{m \times n}$, when the players' strategies are constrained to lie in simple sets. We provide algorithms which solve this problem in $\tilde{O}(ε^{-2/3})$ matrix-vector multiplies (matvecs) in two well-studied cases: $\ell_1$-$\ell_1$ games, where the players' strategies are both in the probability simplex, and $\ell_2$-$\ell_1$ games, where the players' strategies are in the unit Euclidean ball and probability simplex respectively. These results improve upon the previous state-of-the-art complexities of $\tilde{O}(ε^{-8/9})$ for $\ell_1$-$\ell_1$ and of $\tilde{O}(ε^{-7/9})$ for $\ell_2$-$\ell_1$ due to [KOS '25]. In particular, our result for $\ell_2$-$\ell_1$, which corresponds to hard-margin support vector machines (SVMs), matches the lower bound of [KS '25] up to polylogarithmic factors.

</details>


### [391] [Multiscale replay: A robust algorithm for stochastic variational inequalities with a Markovian buffer](https://arxiv.org/abs/2601.01502)
*Milind Nakul,Tianjiao Li,Ashwin Pananjady*

Main category: math.OC

TL;DR: 提出多尺度经验回放（MER）算法解决一类随机变分不等式，采用多尺度采样方案加速收敛，且不依赖马尔可夫链混合时间知识，还讨论了应用。


<details>
  <summary>Details</summary>
Motivation: 解决在马尔可夫链生成样本且有存储缓冲区情况下，现有方法在求解随机变分不等式时可能存在的偏差和收敛速度问题。

Method: 引入Multiscale Experience Replay（MER）算法，利用多尺度采样方案替代均匀采样。

Result: 克服了序列方案中的偏差，加速了收敛，且在可能的情况下实现迭代复杂度的加速，不依赖马尔可夫链混合时间知识。

Conclusion: MER算法在解决随机变分不等式方面有优势，可应用于策略评估和广义线性模型训练。

Abstract: We introduce the Multiscale Experience Replay (MER) algorithm for solving a class of stochastic variational inequalities (VIs) in settings where samples are generated from a Markov chain and we have access to a memory buffer to store them. Rather than uniformly sampling from the buffer, MER utilizes a multi-scale sampling scheme to emulate the behavior of VI algorithms designed for independent and identically distributed samples, overcoming bias in the de facto serial scheme and thereby accelerating convergence. Notably, unlike standard sample-skipping variants of serial algorithms, MER is robust in that it achieves this acceleration in iteration complexity whenever possible, and without requiring knowledge of the mixing time of the Markov chain. We also discuss applications of MER, particularly in policy evaluation with temporal difference learning and in training generalized linear models with dependent data.

</details>
