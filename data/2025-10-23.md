<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 93]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 18]
- [q-fin.CP](#q-fin.CP) [Total: 5]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.CO](#stat.CO) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [cs.CR](#cs.CR) [Total: 7]
- [stat.ME](#stat.ME) [Total: 2]
- [nlin.AO](#nlin.AO) [Total: 1]
- [cs.LO](#cs.LO) [Total: 3]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [math.LO](#math.LO) [Total: 1]
- [econ.GN](#econ.GN) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.RO](#cs.RO) [Total: 6]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 8]
- [cs.CY](#cs.CY) [Total: 9]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CV](#cs.CV) [Total: 20]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 2]
- [cs.CL](#cs.CL) [Total: 35]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality](https://arxiv.org/abs/2510.18982)
*Arpan Mukherjee,Marcello Bullo,Debabrota Basu,Deniz Gündüz*

Main category: cs.AI

TL;DR: 本文提出将可验证测试时间缩放问题构建为运输问题，刻画了相关因素的相互作用，分析了两种采样算法，实证结果支持理论发现。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索验证器的作用及其缺陷，缺少统一框架量化相关因素相互作用的几何特性。

Method: 将可验证测试时间缩放构建为运输问题，分析两种采样算法（顺序和批量）及其计算复杂度。

Result: 发现次优 - 覆盖曲线有运输、策略改进和饱和三种状态，实证结果与理论相符。

Conclusion: 提出的统一框架和算法分析有效，能刻画相关因素相互作用，实证结果支持理论。

Abstract: While test-time scaling with verification has shown promise in improving the
performance of large language models (LLMs), the role of the verifier and its
imperfections remain underexplored. The effect of verification manifests
through interactions of three quantities: (i) the generator's coverage, (ii)
the verifier's region of convergence (ROC), and (iii) the sampling algorithm's
sub-optimality. Though recent studies capture subsets of these factors, a
unified framework quantifying the geometry of their interplay is missing. We
frame verifiable test-time scaling as a transport problem. This characterizes
the interaction of coverage, ROC, and sub-optimality, and uncovers that the
sub-optimality--coverage curve exhibits three regimes. A transport regime --
where sub-optimality increases with coverage, a policy improvement regime --
where sub-optimality may decrease with coverage, depending on the verifier's
ROC, and a saturation regime -- where sub-optimality plateaus, unaffected by
coverage. We further propose and analyze two classes of sampling algorithms --
sequential and batched, and examine how their computational complexities shape
these trade-offs. Empirical results with Qwen, Llama, and Gemma models
corroborate our theoretical findings.

</details>


### [2] [Timely Clinical Diagnosis through Active Test Selection](https://arxiv.org/abs/2510.18988)
*Silas Ruhrberg Estévez,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 本文提出ACTMED诊断框架，结合贝叶斯实验设计与大语言模型，在现实数据集上评估显示可优化测试选择，迈向透明、自适应和贴合临床医生的诊断系统。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习支持临床诊断的方法多依赖静态、全观测数据集，无法反映临床实际推理，且诊断复杂易出错，需要能帮助医生及时做性价比高决策的框架。

Method: 提出ACTMED框架，将贝叶斯实验设计与大语言模型集成，每步选择能最大程度降低患者诊断不确定性的测试，大语言模型作为模拟器，医生可参与整个过程。

Result: 在现实数据集上评估表明，ACTMED能优化测试选择，提高诊断准确性、可解释性和资源利用率。

Conclusion: ACTMED是迈向透明、自适应和贴合临床医生的诊断系统的一步，减少对特定领域数据的依赖。

Abstract: There is growing interest in using machine learning (ML) to support clinical
diag- nosis, but most approaches rely on static, fully observed datasets and
fail to reflect the sequential, resource-aware reasoning clinicians use in
practice. Diagnosis remains complex and error prone, especially in
high-pressure or resource-limited settings, underscoring the need for
frameworks that help clinicians make timely and cost-effective decisions. We
propose ACTMED (Adaptive Clinical Test selection via Model-based Experimental
Design), a diagnostic framework that integrates Bayesian Experimental Design
(BED) with large language models (LLMs) to better emulate real-world diagnostic
reasoning. At each step, ACTMED selects the test expected to yield the greatest
reduction in diagnostic uncertainty for a given patient. LLMs act as flexible
simulators, generating plausible patient state distributions and supporting
belief updates without requiring structured, task-specific training data.
Clinicians can remain in the loop; reviewing test suggestions, interpreting
intermediate outputs, and applying clinical judgment throughout. We evaluate
ACTMED on real-world datasets and show it can optimize test selection to
improve diagnostic accuracy, interpretability, and resource use. This
represents a step to- ward transparent, adaptive, and clinician-aligned
diagnostic systems that generalize across settings with reduced reliance on
domain-specific data.

</details>


### [3] [Rectifying Shortcut Behaviors in Preference-based Reward Learning](https://arxiv.org/abs/2510.19050)
*Wenqian Ye,Guangtao Zheng,Aidong Zhang*

Main category: cs.AI

TL;DR: 本文提出PRISM方法缓解基于偏好的奖励学习中的捷径行为，实验证明其能提升奖励模型准确性并减少下游策略模型对捷径的依赖。


<details>
  <summary>Details</summary>
Motivation: 基于人类反馈的强化学习中，基于偏好的奖励模型易出现奖励破解和泛化性差的问题，作者希望解决这些问题。

Method: 受核视角下的不变性理论启发，提出基于偏好的奖励不变性捷径缓解方法（PRISM），以封闭形式的学习目标学习具有特征映射的组不变核。

Result: 在多个基准测试中，该方法持续提高了奖励模型在不同分布外任务上的准确性，并减少了下游策略模型对捷径的依赖。

Conclusion: 建立了一个基于偏好对齐的鲁棒框架。

Abstract: In reinforcement learning from human feedback, preference-based reward models
play a central role in aligning large language models to human-aligned
behavior. However, recent studies show that these models are prone to reward
hacking and often fail to generalize well due to over-optimization. They
achieve high reward scores by exploiting shortcuts, that is, exploiting
spurious features (e.g., response verbosity, agreeable tone, or sycophancy)
that correlate with human preference labels in the training data rather than
genuinely reflecting the intended objectives. In this paper, instead of probing
these issues one at a time, we take a broader view of the reward hacking
problem as shortcut behaviors and introduce a principled yet flexible approach
to mitigate shortcut behaviors in preference-based reward learning. Inspired by
the invariant theory in the kernel perspective, we propose Preference-based
Reward Invariance for Shortcut Mitigation (PRISM), which learns group-invariant
kernels with feature maps in a closed-form learning objective. Experimental
results in several benchmarks show that our method consistently improves the
accuracy of the reward model on diverse out-of-distribution tasks and reduces
the dependency on shortcuts in downstream policy models, establishing a robust
framework for preference-based alignment.

</details>


### [4] [The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS](https://arxiv.org/abs/2510.19055)
*Brandon James Carone,Iran R. Roman,Pablo Ripollés*

Main category: cs.AI

TL;DR: 引入MUSE基准评估多模态大语言模型音乐理解能力，揭示模型与人类差距及CoT提示效果不佳。


<details>
  <summary>Details</summary>
Motivation: 当前评估可能掩盖多模态大语言模型在音乐关系推理方面的弱点，需新评估基准。

Method: 引入MUSE基准，含10个任务，评估4个SOTA模型并与200人基线对比。

Result: SOTA模型能力差异大，与人类专家有差距，部分模型有严重感知缺陷，CoT提示效果不一致且常有害。

Conclusion: 研究为评估音乐表征和开发更强大AI系统提供关键工具。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated capabilities in
audio understanding, but current evaluations may obscure fundamental weaknesses
in relational reasoning. We introduce the Music Understanding and Structural
Evaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to
probe fundamental music perception skills. We evaluate four SOTA models (Gemini
Pro and Flash, Qwen2.5-Omni, and Audio-Flamingo 3) against a large human
baseline (N=200). Our results reveal a wide variance in SOTA capabilities and a
persistent gap with human experts. While Gemini Pro succeeds on basic
perception, Qwen and Audio Flamingo 3 perform at or near chance, exposing
severe perceptual deficits. Furthermore, we find Chain-of-Thought (CoT)
prompting provides inconsistent, often detrimental results. Our work provides a
critical tool for evaluating invariant musical representations and driving
development of more robust AI systems.

</details>


### [5] [A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist](https://arxiv.org/abs/2510.19139)
*Sohyeon Jeon,Hyung-Chul Lee*

Main category: cs.AI

TL;DR: 研究运用行为和元认知分析方法对比两个代表性大语言模型在三种提示条件下评估临床试验报告的能力，揭示其当前局限性及认知适应和策略行为的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型根据CONSORT标准评估临床试验报告的能力，尤其是其认知和推理策略情况。

Method: 采用行为和元认知分析方法，结合专家验证数据，在三种提示条件下系统比较两个代表性大语言模型。

Result: 模型处理不同CONSORT项目和提示类型时存在明显差异，推理风格、明确不确定性和替代解释会影响响应模式。

Conclusion: 强调当前大语言模型在临床合规自动化方面存在局限性，理解其认知适应和策略行为对开发更具可解释性和可靠性的医疗AI很重要。

Abstract: Despite the rapid expansion of Large Language Models (LLMs) in healthcare,
the ability of these systems to assess clinical trial reporting according to
CONSORT standards remains unclear, particularly with respect to their cognitive
and reasoning strategies. This study applies a behavioral and metacognitive
analytic approach with expert-validated data, systematically comparing two
representative LLMs under three prompt conditions. Clear differences emerged in
how the models approached various CONSORT items, and prompt types, including
shifts in reasoning style, explicit uncertainty, and alternative
interpretations shaped response patterns. Our results highlight the current
limitations of these systems in clinical compliance automation and underscore
the importance of understanding their cognitive adaptations and strategic
behavior in developing more explainable and reliable medical AI.

</details>


### [6] [The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models](https://arxiv.org/abs/2510.19176)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Main category: cs.AI

TL;DR: 推理模型推理时易过度思考致计算开销大，本文研究Mode Selection和Early Exit方法减少计算负担，发现现有仅依赖模型信息的方法在信息有限场景处理Mode Selection不足。


<details>
  <summary>Details</summary>
Motivation: 推理模型推理时易过度思考产生不必要计算开销，需方法减少计算负担。

Method: 提出Mode Selection和Early Exit方法，对比Prompt-based和利用内部信息的方法。

Result: Prompt-based方法因分类能力有限常失败，利用内部信息方法多数场景表现好但稳定性有问题。

Conclusion: 现有仅依赖模型信息的方法在信息有限场景处理Mode Selection不足，该任务仍具挑战。

Abstract: Reasoning models have demonstrated exceptional performance in tasks such as
mathematics and logical reasoning, primarily due to their ability to engage in
step-by-step thinking during the reasoning process. However, this often leads
to overthinking, resulting in unnecessary computational overhead. To address
this issue, Mode Selection aims to automatically decide between Long-CoT
(Chain-of-Thought) or Short-CoT by utilizing either a Thinking or NoThinking
mode. Simultaneously, Early Exit determines the optimal stopping point during
the iterative reasoning process. Both methods seek to reduce the computational
burden. In this paper, we first identify Mode Selection as a more challenging
variant of the Early Exit problem, as they share similar objectives but differ
in decision timing. While Early Exit focuses on determining the best stopping
point for concise reasoning at inference time, Mode Selection must make this
decision at the beginning of the reasoning process, relying on pre-defined fake
thoughts without engaging in an explicit reasoning process, referred to as
zero-step thinking. Through empirical studies on nine baselines, we observe
that prompt-based approaches often fail due to their limited classification
capabilities when provided with minimal hand-crafted information. In contrast,
approaches that leverage internal information generally perform better across
most scenarios but still exhibit issues with stability. Our findings indicate
that existing methods relying solely on the information provided by models are
insufficient for effectively addressing Mode Selection in scenarios with
limited information, highlighting the ongoing challenges of this task. Our code
is available at https://github.com/Trae1ounG/Zero_Step_Thinking.

</details>


### [7] [WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation](https://arxiv.org/abs/2510.19205)
*Yaoyao Qian,Yuanli Wang,Jinda Zhang,Yun Zong,Meixu Chen,Hanhan Zhou,Jindan Huang,Yifan Zeng,Xinyu Hu,Chan Hee Song,Danqing Zhang*

Main category: cs.AI

TL;DR: 提出WebGraphEval框架，将多智能体轨迹抽象为统一加权动作图，用于网页智能体评估，展现诸多优势并建立通用评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前网页智能体评估忽略基准数据集的结构多样性，仅采用二元成功指标或单一参考轨迹。

Method: 提出WebGraphEval框架，将多智能体轨迹抽象为统一加权动作图，规范编码动作、合并重复行为并进行结构分析。

Result: 对六个网页智能体数千条轨迹的评估表明，图抽象能捕捉跨模型规律，凸显冗余和低效，识别基于结果的指标所忽略的关键决策点。

Conclusion: WebGraphEval通过将网页交互构建为图结构数据，为网页智能体建立了多路径、跨智能体和效率感知的通用评估方法。

Abstract: Current evaluation of web agents largely reduces to binary success metrics or
conformity to a single reference trajectory, ignoring the structural diversity
present in benchmark datasets. We present WebGraphEval, a framework that
abstracts trajectories from multiple agents into a unified, weighted action
graph. This representation is directly compatible with benchmarks such as
WebArena, leveraging leaderboard runs and newly collected trajectories without
modifying environments. The framework canonically encodes actions, merges
recurring behaviors, and applies structural analyses including reward
propagation and success-weighted edge statistics. Evaluations across thousands
of trajectories from six web agents show that the graph abstraction captures
cross-model regularities, highlights redundancy and inefficiency, and
identifies critical decision points overlooked by outcome-based metrics. By
framing web interaction as graph-structured data, WebGraphEval establishes a
general methodology for multi-path, cross-agent, and efficiency-aware
evaluation of web agents.

</details>


### [8] [ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate](https://arxiv.org/abs/2510.19261)
*Marianna Molinari,Ilaria Angela Amantea,Marinella Quaranta,Guido Governatori*

Main category: cs.AI

TL;DR: 研究通过实验检验ChatGPT在法律领域表现，揭示其局限性，表明真正的智能在该领域仍是人类独有。


<details>
  <summary>Details</summary>
Motivation: 检验ChatGPT在法律领域的表现，探究其在法律任务中的能力。

Method: 进行实验，将ChatGPT结果与使用正则表达式的基线进行比较。

Result: ChatGPT即便有必要知识和能力，也无法整合推理出详尽结果，缺乏全面理解和推理能力。

Conclusion: 在法律领域，真正的智能仍是人类独有的特质。

Abstract: This study examines the performance of ChatGPT with an experiment in the
legal domain. We compare the outcome with it a baseline using regular
expressions (Regex), rather than focusing solely on the assessment against
human performance. The study reveals that even if ChatGPT has access to the
necessary knowledge and competencies, it is unable to assemble them, reason
through, in a way that leads to an exhaustive result. This unveils a major
limitation of ChatGPT. Intelligence encompasses the ability to break down
complex issues and address them according to multiple required competencies,
providing a unified and comprehensive solution. In the legal domain, one of the
most crucial tasks is reading legal decisions and extracting key passages
condensed from principles of law (PoLs), which are then incorporated into
subsequent rulings by judges or defense documents by lawyers. In performing
this task, artificial intelligence lacks an all-encompassing understanding and
reasoning, which makes it inherently limited. Genuine intelligence, remains a
uniquely human trait, at least in this particular field.

</details>


### [9] [An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents](https://arxiv.org/abs/2510.19263)
*Wachara Fungwacharakorn,Gauvain Bourgne,Ken Satoh*

Main category: cs.AI

TL;DR: 本文探讨扩展推导状态论证框架（DSA - 框架），以解释基于广义理由模型概念的推理。


<details>
  <summary>Details</summary>
Motivation: 现有的基于传统一致理由模型的先例推理有论证解释方法，但适用于包含不一致先例的广义推理框架却没有对应的论证解释方法，需解决此问题。

Method: 研究扩展推导状态论证框架（DSA - 框架）。

Result: 未提及。

Conclusion: 未提及。

Abstract: Precedential constraint is one foundation of case-based reasoning in AI and
Law. It generally assumes that the underlying set of precedents must be
consistent. To relax this assumption, a generalized notion of the reason model
has been introduced. While several argumentative explanation approaches exist
for reasoning with precedents based on the traditional consistent reason model,
there has been no corresponding argumentative explanation method developed for
this generalized reasoning framework accommodating inconsistent precedents. To
address this question, this paper examines an extension of the derivation state
argumentation framework (DSA-framework) to explain the reasoning according to
the generalized notion of the reason model.

</details>


### [10] [Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties](https://arxiv.org/abs/2510.19299)
*Philipp J. Schneider,Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.AI

TL;DR: 本文提出多智能体大语言模型模拟框架，研究大语言模型智能体能否重现人类在线行为的复杂社会动态。实验表明，该框架能使智能体形成与真实社区相似的网络结构，可用于研究集体动态。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型智能体能否重现人类在线行为的复杂社会动态，以及何种记忆和学习机制促使这些动态出现。

Method: 提出多智能体大语言模型模拟框架，设计行为奖励函数以模拟人类社会行为，智能体通过上下文学习和教练信号加速适应行为。

Result: 经过训练的大语言模型智能体形成稳定交互模式和社会关系，产生的网络结构与真实在线社区特性相似。

Conclusion: 结合行为奖励和上下文适应的框架为研究大语言模型群体的集体动态提供了测试平台，揭示了人工智能体与人类社会行为的近似与差异。

Abstract: Can large language model (LLM) agents reproduce the complex social dynamics
that characterize human online behavior -- shaped by homophily, reciprocity,
and social validation -- and what memory and learning mechanisms enable such
dynamics to emerge? We present a multi-agent LLM simulation framework in which
agents repeatedly interact, evaluate one another, and adapt their behavior
through in-context learning accelerated by a coaching signal. To model human
social behavior, we design behavioral reward functions that capture core
drivers of online engagement, including social interaction, information
seeking, self-presentation, coordination, and emotional support. These rewards
align agent objectives with empirically observed user motivations, enabling the
study of how network structures and group formations emerge from individual
decision-making. Our experiments show that coached LLM agents develop stable
interaction patterns and form emergent social ties, yielding network structures
that mirror properties of real online communities. By combining behavioral
rewards with in-context adaptation, our framework establishes a principled
testbed for investigating collective dynamics in LLM populations and reveals
how artificial agents may approximate or diverge from human-like social
behavior.

</details>


### [11] [Continual Knowledge Adaptation for Reinforcement Learning](https://arxiv.org/abs/2510.19314)
*Jinwu Hu,Zihao Lian,Zhiquan Wen,Chenghao Li,Guohao Chen,Xutao Wen,Bin Xiao,Mingkui Tan*

Main category: cs.AI

TL;DR: 提出CKA - RL方法解决持续强化学习中灾难性遗忘和知识利用低效问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实环境非平稳，现有持续强化学习方法存在灾难性遗忘和知识利用低效问题。

Method: 提出CKA - RL，引入持续知识适应策略维护特定任务知识向量池，利用历史知识适应新任务；提出自适应知识合并机制合并相似知识向量。

Result: 在三个基准测试中，CKA - RL整体性能提升4.20%，正向迁移提升8.02%。

Conclusion: CKA - RL能有效缓解灾难性遗忘，实现高效知识转移，优于现有方法。

Abstract: Reinforcement Learning enables agents to learn optimal behaviors through
interactions with environments. However, real-world environments are typically
non-stationary, requiring agents to continuously adapt to new tasks and
changing conditions. Although Continual Reinforcement Learning facilitates
learning across multiple tasks, existing methods often suffer from catastrophic
forgetting and inefficient knowledge utilization. To address these challenges,
we propose Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL),
which enables the accumulation and effective utilization of historical
knowledge. Specifically, we introduce a Continual Knowledge Adaptation
strategy, which involves maintaining a task-specific knowledge vector pool and
dynamically using historical knowledge to adapt the agent to new tasks. This
process mitigates catastrophic forgetting and enables efficient knowledge
transfer across tasks by preserving and adapting critical model parameters.
Additionally, we propose an Adaptive Knowledge Merging mechanism that combines
similar knowledge vectors to address scalability challenges, reducing memory
requirements while ensuring the retention of essential knowledge. Experiments
on three benchmarks demonstrate that the proposed CKA-RL outperforms
state-of-the-art methods, achieving an improvement of 4.20% in overall
performance and 8.02% in forward transfer. The source code is available at
https://github.com/Fhujinwu/CKA-RL.

</details>


### [12] [MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration](https://arxiv.org/abs/2510.19423)
*Jia-Kai Dong,I-Wei Huang,Chun-Tin Wu,Yi-Tien Tsai*

Main category: cs.AI

TL;DR: 介绍了用于评估大语言模型代理多跳端到端工具编排的大规模基准MSC - Bench，指出其优势、测试内容，实验揭示问题并提供诊断框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准常孤立评估工具，忽略功能重叠和跨服务器编排等挑战，导致评估过于乐观，需要新基准解决这些问题。

Method: 通过“等功能集”构建地面真值，使用F1分数等客观指标，减少对大语言模型作为评判者的依赖；采用五级课程体系系统测试代理能力。

Result: 实验发现刚性层次结构若无协同设计策略会阻碍性能，即使最先进的代理在鲁棒性方面也存在系统性弱点。

Conclusion: MSC - Bench提供诊断框架，可暴露局限性并指导更强大高效的工具使用代理的开发，且基准和资源已公开。

Abstract: We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop,
end-to-end tool orchestration by LLM agents in a hierarchical Model-Context
Protocol (MCP) ecosystem. Existing benchmarks often evaluate tools in
isolation, ignoring challenges such as functional overlap and cross-server
orchestration, leading to overly optimistic assessments. MSC-Bench addresses
these gaps by constructing ground truth through 'equal function sets', allowing
objective metrics such as F1 score and reducing the dependency on
LLM-as-a-judge evaluation. Organized as a five-level curriculum, it
systematically tests agent capabilities from single-tool orchestration to
complex cross-server planning, and robustness to out-of-scope requests.
Experiments reveal that rigid hierarchies can hinder performance without
co-designed strategies, and even state-of-the-art agents exhibit systemic
weaknesses in robustness. MSC-Bench provides a diagnostic framework to expose
these limitations and guide the development of more capable and efficient
tool-using agents. The benchmark and resources are publicly available at
https://github.com/snooow1029/MSC_Bench.

</details>


### [13] [SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems](https://arxiv.org/abs/2509.23130)
*Qian Cheng,Ruize Tang,Emilie Ma,Finn Hackett,Peiyang He,Yiming Su,Ivan Beschastnikh,Yu Huang,Xiaoxing Ma,Tianyin Xu*

Main category: cs.AI

TL;DR: 提出SysMoBench基准测试评估AI对大型复杂系统进行形式化建模的能力，涵盖多种系统工件，可推动该领域研究。


<details>
  <summary>Details</summary>
Motivation: 形式化模型编写和维护成本高，现有AI生成规范工作多针对小代码，不确定能否处理现实系统工件，需评估AI对大型复杂系统建模能力。

Method: 聚焦并发和分布式系统，使用TLA+语言，自动化评估指标，如语法和运行时正确性等，构建SysMoBench基准测试。

Result: SysMoBench包含九个不同系统工件，且持续增加。

Conclusion: SysMoBench有助于了解当前大语言模型和智能体的能力与局限，为该领域工具奠定基础并开辟新研究方向。

Abstract: Formal models are essential to specifying large, complex computer systems and
verifying their correctness, but are notoriously expensive to write and
maintain. Recent advances in generative AI show promise in generating certain
forms of specifications. However, existing work mostly targets small code, not
complete systems. It is unclear whether AI can deal with realistic system
artifacts, as this requires abstracting their complex behavioral properties
into formal models. We present SysMoBench, a benchmark that evaluates AI's
ability to formally model large, complex systems. We focus on concurrent and
distributed systems, which are keystones of today's critical computing
infrastructures, encompassing operating systems and cloud infrastructure. We
use TLA+, the de facto specification language for concurrent and distributed
systems, though the benchmark can be extended to other specification languages.
We address the primary challenge of evaluating AI-generated models by
automating metrics like syntactic and runtime correctness, conformance to
system code, and invariant correctness. SysMoBench currently includes nine
diverse system artifacts: the Raft implementation of Etcd and Redis, the
Spinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively
added. SysMoBench enables us to understand the capabilities and limitations of
today's LLMs and agents, putting tools in this area on a firm footing and
opening up promising new research directions.

</details>


### [14] [NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning](https://arxiv.org/abs/2510.19429)
*Wonje Choi,Jooyoung Kim,Honguk Woo*

Main category: cs.AI

TL;DR: 提出NeSyPr框架解决动态环境中语言模型用于具身任务的挑战，在多个基准测试上验证其能力。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中因延迟、连接和资源限制，在线访问大规模推理引擎或符号规划器受限的问题，让语言模型用于具身任务。

Method: 提出NeSyPr框架，通过神经符号程序化为语言模型代理赋予推理能力，先由符号工具生成任务计划，再转换为可组合的程序表示并集成到语言模型推理过程。

Result: 在PDDLGym、VirtualHome和ALFWorld等具身基准测试中，相比大规模推理模型和符号规划器，使用更紧凑语言模型展示了高效推理能力。

Conclusion: NeSyPr框架适合部署在对延迟敏感和资源受限的物理系统中。

Abstract: We address the challenge of adopting language models (LMs) for embodied tasks
in dynamic environments, where online access to large-scale inference engines
or symbolic planners is constrained due to latency, connectivity, and resource
limitations. To this end, we present NeSyPr, a novel embodied reasoning
framework that compiles knowledge via neurosymbolic proceduralization, thereby
equipping LM-based agents with structured, adaptive, and timely reasoning
capabilities. In NeSyPr, task-specific plans are first explicitly generated by
a symbolic tool leveraging its declarative knowledge. These plans are then
transformed into composable procedural representations that encode the plans'
implicit production rules, enabling the resulting composed procedures to be
seamlessly integrated into the LM's inference process. This neurosymbolic
proceduralization abstracts and generalizes multi-step symbolic structured
path-finding and reasoning into single-step LM inference, akin to human
knowledge compilation. It supports efficient test-time inference without
relying on external symbolic guidance, making it well suited for deployment in
latency-sensitive and resource-constrained physical systems. We evaluate NeSyPr
on the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating
its efficient reasoning capabilities over large-scale reasoning models and a
symbolic planner, while using more compact LMs.

</details>


### [15] [DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning](https://arxiv.org/abs/2510.19562)
*Runpeng Xie,Quanwei Wang,Hao Hu,Zherui Zhou,Ni Mu,Xiyun Li,Yiqin Yang,Shuang Xu,Qianchuan Zhao,Bo XU*

Main category: cs.AI

TL;DR: 提出DAIL方法解决语言指令歧义问题，实验证明其效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 语言指令的灵活性导致语言条件任务存在大量歧义，严重降低算法性能。

Method: 提出DAIL方法，包含分布策略和语义对齐两个关键组件，通过价值分布估计机制增强任务可区分性，语义对齐模块捕捉轨迹和语言指令的对应关系。

Result: 在结构化和视觉观察基准测试上的实验结果表明，DAIL能有效解决指令歧义问题，性能优于基线方法。

Conclusion: DAIL方法有效，代码可在https://github.com/RunpengXie/Distributional-Aligned-Learning获取。

Abstract: Comprehending natural language and following human instructions are critical
capabilities for intelligent agents. However, the flexibility of linguistic
instructions induces substantial ambiguity across language-conditioned tasks,
severely degrading algorithmic performance. To address these limitations, we
present a novel method named DAIL (Distributional Aligned Learning), featuring
two key components: distributional policy and semantic alignment. Specifically,
we provide theoretical results that the value distribution estimation mechanism
enhances task differentiability. Meanwhile, the semantic alignment module
captures the correspondence between trajectories and linguistic instructions.
Extensive experimental results on both structured and visual observation
benchmarks demonstrate that DAIL effectively resolves instruction ambiguities,
achieving superior performance to baseline methods. Our implementation is
available at https://github.com/RunpengXie/Distributional-Aligned-Learning.

</details>


### [16] [HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application](https://arxiv.org/abs/2510.19631)
*Yiqian Yang,Tian Lan,Qianghuai Jia,Li Zhu,Hui Jiang,Hang Zhu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.AI

TL;DR: 提出HSCodeComp基准评估深度搜索代理分层规则应用能力，实验显示代理性能与人类专家差距大。


<details>
  <summary>Details</summary>
Motivation: 当前代理基准忽视深度搜索代理应用复杂规则的能力，需填补此空白。

Method: 引入基于电商平台真实数据、由专家标注的HSCodeComp基准，引导代理进行深度推理预测产品HSCode。

Result: 在多个先进模型和代理上实验，最佳代理10位准确率仅46.8%，远低于人类专家的95.0%，测试时缩放无法进一步提升性能。

Conclusion: 深度搜索代理在分层规则应用上存在巨大挑战，当前性能远不及人类专家。

Abstract: Effective deep search agents must not only access open-domain and
domain-specific knowledge but also apply complex rules-such as legal clauses,
medical manuals and tariff rules. These rules often feature vague boundaries
and implicit logic relationships, making precise application challenging for
agents. However, this critical capability is largely overlooked by current
agent benchmarks.
  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level
e-commerce benchmark designed to evaluate deep search agents in hierarchical
rule application. In this task, the deep reasoning process of agents is guided
by these rules to predict 10-digit Harmonized System Code (HSCode) of products
with noisy but realistic descriptions. These codes, established by the World
Customs Organization, are vital for global supply chain efficiency. Built from
real-world data collected from large-scale e-commerce platforms, our proposed
HSCodeComp comprises 632 product entries spanning diverse product categories,
with these HSCodes annotated by several human experts.
  Extensive experimental results on several state-of-the-art LLMs, open-source,
and closed-source agents reveal a huge performance gap: best agent achieves
only 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,
detailed analysis demonstrates the challenges of hierarchical rule application,
and test-time scaling fails to improve performance further.

</details>


### [17] [AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing](https://arxiv.org/abs/2510.19661)
*Xusen Guo,Mingxing Peng,Xixuan Hao,Xingchen Zou,Qiongyan Wang,Sijie Ruan,Yuxuan Liang*

Main category: cs.AI

TL;DR: 本文提出无训练框架AgentSense，将大语言模型集成到参与式城市传感，实验表明其在适应性和可解释性上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有城市传感系统在不同城市场景泛化能力有限，决策可解释性差。

Method: 引入AgentSense框架，通过多智能体进化系统将大语言模型集成到参与式城市传感，先用经典规划器生成基线解决方案，再迭代优化。

Result: 在两个大规模移动数据集和七种动态干扰下实验，AgentSense在适应性和可解释性上优于传统方法，比单智能体大语言模型基线在性能和鲁棒性上更优。

Conclusion: AgentSense是朝着在网络上部署自适应和可解释城市传感系统的重大进展。

Abstract: Web-based participatory urban sensing has emerged as a vital approach for
modern urban management by leveraging mobile individuals as distributed
sensors. However, existing urban sensing systems struggle with limited
generalization across diverse urban scenarios and poor interpretability in
decision-making. In this work, we introduce AgentSense, a hybrid, training-free
framework that integrates large language models (LLMs) into participatory urban
sensing through a multi-agent evolution system. AgentSense initially employs
classical planner to generate baseline solutions and then iteratively refines
them to adapt sensing task assignments to dynamic urban conditions and
heterogeneous worker preferences, while producing natural language explanations
that enhance transparency and trust. Extensive experiments across two
large-scale mobility datasets and seven types of dynamic disturbances
demonstrate that AgentSense offers distinct advantages in adaptivity and
explainability over traditional methods. Furthermore, compared to single-agent
LLM baselines, our approach outperforms in both performance and robustness,
while delivering more reasonable and transparent explanations. These results
position AgentSense as a significant advancement towards deploying adaptive and
explainable urban sensing systems on the web.

</details>


### [18] [A Graph Engine for Guitar Chord-Tone Soloing Education](https://arxiv.org/abs/2510.19666)
*Matthew Keating,Michael Casey*

Main category: cs.AI

TL;DR: 提出基于图的引擎为吉他学生计算和弦音独奏建议，介绍生成方法和系统。


<details>
  <summary>Details</summary>
Motivation: 和弦音独奏是爵士吉他理论基础但难学难练，需工具辅助学生练习。

Method: 先讨论生成和弦音琶音方法，构建加权图，计算边权重，找最短路径重建独奏线，最后设计用户友好系统。

Result: 构建基于图的引擎及用户友好系统。

Conclusion: 该引擎和系统可帮助吉他学生练习和弦音独奏。

Abstract: We present a graph-based engine for computing chord tone soloing suggestions
for guitar students. Chord tone soloing is a fundamental practice for
improvising over a chord progression, where the instrumentalist uses only the
notes contained in the current chord. This practice is a building block for all
advanced jazz guitar theory but is difficult to learn and practice. First, we
discuss methods for generating chord-tone arpeggios. Next, we construct a
weighted graph where each node represents a chord tone arpeggio for a chord in
the progression. Then, we calculate the edge weight between each consecutive
chord's nodes in terms of optimal transition tones. We then find the shortest
path through this graph and reconstruct a chord-tone soloing line. Finally, we
discuss a user-friendly system to handle input and output to this engine for
guitar students to practice chord tone soloing.

</details>


### [19] [Explainable e-sports win prediction through Machine Learning classification in streaming](https://arxiv.org/abs/2510.19671)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.AI

TL;DR: 电子竞技发展推动网游行业增长，本文提出流式可解释胜利预测分类方案，实验准确率超90%，可用于决策。


<details>
  <summary>Details</summary>
Motivation: 现有电子竞技分析基于人工智能的解决方案多关注批量视角的分类，忽视可视化技术，需改进。

Method: 提出流式可解释胜利预测分类方案，通过多个滑动窗口控制输入数据反映游戏变化。

Result: 实验准确率高于90%，超越文献中竞争方案。

Conclusion: 系统因可解释模块能用于排名和推荐系统辅助决策，增加预测结果可信度。

Abstract: The increasing number of spectators and players in e-sports, along with the
development of optimized communication solutions and cloud computing
technology, has motivated the constant growth of the online game industry. Even
though Artificial Intelligence-based solutions for e-sports analytics are
traditionally defined as extracting meaningful patterns from related data and
visualizing them to enhance decision-making, most of the effort in professional
winning prediction has been focused on the classification aspect from a batch
perspective, also leaving aside the visualization techniques. Consequently,
this work contributes to an explainable win prediction classification solution
in streaming in which input data is controlled over several sliding windows to
reflect relevant game changes. Experimental results attained an accuracy higher
than 90 %, surpassing the performance of competing solutions in the literature.
Ultimately, our system can be leveraged by ranking and recommender systems for
informed decision-making, thanks to the explainability module, which fosters
trust in the outcome predictions.

</details>


### [20] [RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models](https://arxiv.org/abs/2510.19698)
*Yang Yang,Hua XU,Zhangyi Hu,Yutao Yue*

Main category: cs.AI

TL;DR: 提出RLIE框架将大语言模型与概率建模结合学习加权规则集，评估多种推理策略，发现直接应用规则效果更好，明确大语言模型归纳推理的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 许多基于大语言模型的方法忽略规则间交互，且将大语言模型与概率规则学习结合进行鲁棒推理的机会未充分探索。

Method: 提出RLIE框架，包括规则生成、逻辑回归、迭代细化和评估四个阶段。

Result: 直接应用学习到权重的规则性能更优，用规则、权重和逻辑模型输出提示大语言模型会降低准确率。

Conclusion: 明确大语言模型在归纳推理中的潜力与局限，将其与经典概率规则组合方法结合实现更可靠的神经符号推理。

Abstract: Large Language Models (LLMs) can propose rules in natural language,
sidestepping the need for a predefined predicate space in traditional rule
learning. Yet many LLM-based approaches ignore interactions among rules, and
the opportunity to couple LLMs with probabilistic rule learning for robust
inference remains underexplored. We present RLIE, a unified framework that
integrates LLMs with probabilistic modeling to learn a set of weighted rules.
RLIE has four stages: (1) Rule generation, where an LLM proposes and filters
candidates; (2) Logistic regression, which learns probabilistic weights for
global selection and calibration; (3) Iterative refinement, which updates the
rule set using prediction errors; and (4) Evaluation, which compares the
weighted rule set as a direct classifier with methods that inject rules into an
LLM. We evaluate multiple inference strategies on real-world datasets. Applying
rules directly with their learned weights yields superior performance, whereas
prompting LLMs with the rules, weights, and logistic-model outputs surprisingly
degrades accuracy. This supports the view that LLMs excel at semantic
generation and interpretation but are less reliable for precise probabilistic
integration. RLIE clarifies the potential and limitations of LLMs for inductive
reasoning and couples them with classic probabilistic rule combination methods
to enable more reliable neuro-symbolic reasoning.

</details>


### [21] [Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning](https://arxiv.org/abs/2510.19732)
*Gunshi Gupta,Karmesh Yadav,Zsolt Kira,Yarin Gal,Rahaf Aljundi*

Main category: cs.AI

TL;DR: 提出用于强化学习的Memo架构和训练方法，在相关任务中表现出色，效率高且泛化性好。


<details>
  <summary>Details</summary>
Motivation: 现有训练范式中视觉输入易超出transformer上下文限制，而人类能利用压缩记忆，且现有方法有局限，需开发能形成和访问记忆的模型让具身智能体长期有效运行。

Method: 提出Memo架构和训练方法，在训练时将周期性总结标记与模型输入交错，以实现记忆的创建和检索。

Result: 在网格世界元强化学习基准和多目标导航任务中，Memo优于朴素长上下文transformer基线，计算和存储效率更高。

Conclusion: Memo在记忆密集、长时程任务的强化学习中有效，泛化性好，在流式设置中也很稳健。

Abstract: To enable embodied agents to operate effectively over extended timeframes, it
is crucial to develop models that form and access memories to stay
contextualized in their environment. In the current paradigm of training
transformer-based policies for embodied sequential decision-making tasks,
visual inputs often overwhelm the context limits of transformers, while humans
can maintain and utilize a lifetime of experience compressed as memories.
Significant compression is possible in principle, as much of the input is
irrelevant and can be abstracted. However, existing approaches predominantly
focus on either recurrent models with fixed-size memory or transformers with
full-context reliance. In this work, we propose Memo, a transformer-based
architecture and training recipe for reinforcement learning (RL) on
memory-intensive, long-horizon tasks. Memo incorporates the creation and
retrieval of memory by interleaving periodic summarization tokens with the
inputs of a model during training. We demonstrate Memo's effectiveness on a
gridworld meta-RL benchmark and a multi-object navigation task in
photo-realistic indoor settings. Memo outperforms naive long-context
transformer baselines while being more compute and storage efficient.
Additionally, Memo generalizes better to longer contexts at inference time and
remains robust in streaming settings, where historical context must be
truncated to fit inference constraints.

</details>


### [22] [Misalignment Bounty: Crowdsourcing AI Agent Misbehavior](https://arxiv.org/abs/2510.19738)
*Rustem Turtayev,Natalia Fedorova,Oleg Serikov,Sergey Koldyba,Lev Avagyan,Dmitrii Volkov*

Main category: cs.AI

TL;DR: 开展众包项目Misalignment Bounty收集AI与人类意图不一致案例，收到295份提交，9份获奖并报告详情。


<details>
  <summary>Details</summary>
Motivation: 收集AI行为与人类意图不一致的清晰、可复现案例。

Method: 开展众包项目Misalignment Bounty收集案例。

Result: 收到295份提交，9份获奖。

Conclusion: 报告介绍项目动机、评估标准并详述9份获奖提交。

Abstract: Advanced AI systems sometimes act in ways that differ from human intent. To
gather clear, reproducible examples, we ran the Misalignment Bounty: a
crowdsourced project that collected cases of agents pursuing unintended or
unsafe goals. The bounty received 295 submissions, of which nine were awarded.
  This report explains the program's motivation and evaluation criteria, and
walks through the nine winning submissions step by step.

</details>


### [23] [Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents](https://arxiv.org/abs/2510.19771)
*Gil Pasternak,Dheeraj Rajagopal,Julia White,Dhruv Atreja,Matthew Thomas,George Hurn-Maloney,Ash Lewis*

Main category: cs.AI

TL;DR: 提出PROBE评估基准测试大语言模型和智能体框架的主动性，发现即使最先进模型也有局限，指出研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前主动性评估基准有局限，无法跨来源和长时间范围测试推理能力。

Method: 提出PROBE，将主动性分解为搜索未指明问题、识别具体瓶颈、执行合适解决方案三个核心能力的流程。

Result: 对领先大语言模型和流行智能体框架评估，最好的端到端性能为40%，由GPT - 5和Claude Opus - 4.1实现，还展示各模型相对能力并分析共同失败模式。

Conclusion: 凸显智能体系统自主行动的当前局限，揭示有前景的未来研究方向。

Abstract: LLM-based agents are increasingly moving towards proactivity: rather than
awaiting instruction, they exercise agency to anticipate user needs and solve
them autonomously. However, evaluating proactivity is challenging; current
benchmarks are constrained to localized context, limiting their ability to test
reasoning across sources and longer time horizons. To address this gap, we
present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes
proactivity as a pipeline of three core capabilities: (1) searching for
unspecified issues, (2) identifying specific bottlenecks, and (3) executing
appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular
agentic frameworks, showing that even state-of-the-art models struggle to solve
this benchmark. Computing our consistent measurements across frontier LLMs and
agents, we find that the best end-to-end performance of 40% is achieved by both
GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative
capabilities of each model and analyze mutual failure modes. Our results
highlight the current limitations of autonomous action in agentic systems, and
expose promising future research directions.

</details>


### [24] [Benchmarking World-Model Learning](https://arxiv.org/abs/2510.19788)
*Archana Warrier,Dat Nyugen,Michelangelo Naim,Moksh Jain,Yichao Liang,Karen Schroeder,Cambridge Yang,Joshua B. Tenenbaum,Sebastian Vollmer,Kevin Ellis,Zenna Tavares*

Main category: cs.AI

TL;DR: 提出WorldTest协议评估模型学习智能体，用AutumnBench实例化该协议进行实验，发现人类表现优于模型，此协议提供评估新模板，也显示世界模型学习有提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前学习和评估世界模型的方法与支持多下游任务和推理的目标有偏差，需新评估方法。

Method: 提出WorldTest协议，将无奖励交互与计分测试阶段分离，用AutumnBench（含43个交互网格世界环境和129个任务）实例化该协议，对比517名人类参与者和三个前沿模型。

Result: 人类表现优于模型，增加计算资源仅在部分环境中提升性能。

Conclusion: WorldTest提供评估智能体对环境动态学习情况的新模板，AutumnBench显示世界模型学习有很大提升空间。

Abstract: Model-learning agents should gather information to learn world models that
support many downstream tasks and inferences, such as predicting unobserved
states, estimating near- and far-term consequences of actions, planning action
sequences, and detecting changes in dynamics. Current methods for learning and
evaluating world models diverge from this goal: training and evaluation are
anchored to next-frame prediction, and success is scored by reward maximization
in the same environment. We propose WorldTest, a protocol to evaluate
model-learning agents that separates reward-free interaction from a scored test
phase in a different but related environment. WorldTest is
open-ended$\unicode{x2014}$models should support many different tasks unknown
ahead of time$\unicode{x2014}$and agnostic to model representation, allowing
comparison across approaches. We instantiated WorldTest with AutumnBench, a
suite of 43 interactive grid-world environments and 129 tasks across three
families: masked-frame prediction, planning, and predicting changes to the
causal dynamics. We compared 517 human participants and three frontier models
on AutumnBench. We found that humans outperform the models, and scaling compute
improves performance only in some environments but not others. WorldTest
provides a novel template$\unicode{x2014}$reward-free exploration, derived
tests, and behavior-based scoring$\unicode{x2014}$to evaluate what agents learn
about environment dynamics, and AutumnBench exposes significant headroom in
world-model learning.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [25] [Parameter Estimation in River Transport Models With Immobile Phase Exchange Using Dimensional Analysis and Reduced-Order Models](https://arxiv.org/abs/2510.19664)
*Manuel M. Reyna,Alexandre M. Tartakovsky*

Main category: cs.CE

TL;DR: 提出Dimensionless Synthetic Transport Estimation (DSTE)框架用于河流水质模型参数估计，用其对一维平流 - 弥散方程模型参数化，经测试DSTE能准确估计参数并生成可用于新曲线分析的数据集。


<details>
  <summary>Details</summary>
Motivation: 解决河流水质模型参数估计问题，提高计算效率并准确估计参数。

Method: 提出DSTE框架，在拉普拉斯域解析求解控制方程，数值反演生成合成突破曲线，用Karhunen - Loeve (KL)展开降维，通过最小化距离估计平流速度，用投影重心插值(PBI)推断其余无量纲参数。

Result: 应用于25条河流的54次示踪试验的295条突破曲线，DSTE能准确估计参数，生成的标记数据集可用于关联参数与相关条件，合成数据集可用于新曲线分析。

Conclusion: DSTE框架在河流水质模型参数估计中有效且高效，能避免额外正向模拟。

Abstract: We propose a framework for parameter estimation in river transport models
using breakthrough curve data, which we refer to as Dimensionless Synthetic
Transport Estimation (DSTE). We utilize this framework to parameterize the
one-dimensional advection-dispersion equation model, incorporating immobile
phase exchange through a memory function. We solve the governing equation
analytically in the Laplace domain and numerically invert it to generate
synthetic breakthrough curves for different memory functions and boundary
conditions. A dimensionless formulation enables decoupling the estimation of
advection velocity from other parameters, significantly reducing the number of
required forward solutions. To improve computational efficiency, we apply a
Karhunen-Loeve (KL) expansion to transform the synthetic dataset into a
reduced-order space. Given a measured breakthrough curve, we estimate the
advection velocity by minimizing the distance from the measurement to the
synthetic data in KL space, and infer the remaining dimensionless parameters by
Projected Barycentric Interpolation (PBI). We benchmark our method against
several alternatives, including Laplace domain fitting, moment matching, global
random optimization, and variations of the DSTE framework using
nearest-neighbor interpolation and neural network-based estimation. Applied to
295 breakthrough curves from 54 tracer tests in 25 rivers, DSTE delivers
accurate parameter estimates. The resulting labeled dataset allows researchers
to link transport parameters with hydraulic conditions, site characteristics,
and measured concentrations. The synthetic dataset can be leveraged for the
analysis of new breakthrough curves, eliminating the need for additional
forward simulations.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [26] [FlexiDataGen: An Adaptive LLM Framework for Dynamic Semantic Dataset Generation in Sensitive Domains](https://arxiv.org/abs/2510.19025)
*Hamed Jelodar,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.DB

TL;DR: 论文指出数据集可用性和质量是机器学习关键挑战，介绍FlexiDataGen框架应对敏感领域数据问题，实验表明其能缓解数据短缺和标注瓶颈。


<details>
  <summary>Details</summary>
Motivation: 解决敏感领域（如医疗、生物医学研究、网络安全）中数据集可用性和质量问题，突破数据集挑战对机器学习模型发展的阻碍。

Method: 引入FlexiDataGen框架，集成句法 - 语义分析、检索增强生成、动态元素注入和迭代释义与语义验证四个核心组件，自主合成高质量、领域相关数据集。

Result: FlexiDataGen有效缓解了数据短缺和标注瓶颈，使机器学习模型可扩展且准确。

Conclusion: FlexiDataGen框架能解决敏感领域数据集问题，推动机器学习模型发展。

Abstract: Dataset availability and quality remain critical challenges in machine
learning, especially in domains where data are scarce, expensive to acquire, or
constrained by privacy regulations. Fields such as healthcare, biomedical
research, and cybersecurity frequently encounter high data acquisition costs,
limited access to annotated data, and the rarity or sensitivity of key events.
These issues-collectively referred to as the dataset challenge-hinder the
development of accurate and generalizable machine learning models in such
high-stakes domains. To address this, we introduce FlexiDataGen, an adaptive
large language model (LLM) framework designed for dynamic semantic dataset
generation in sensitive domains. FlexiDataGen autonomously synthesizes rich,
semantically coherent, and linguistically diverse datasets tailored to
specialized fields. The framework integrates four core components: (1)
syntactic-semantic analysis, (2) retrieval-augmented generation, (3) dynamic
element injection, and (4) iterative paraphrasing with semantic validation.
Together, these components ensure the generation of high-quality,
domain-relevant data. Experimental results show that FlexiDataGen effectively
alleviates data shortages and annotation bottlenecks, enabling scalable and
accurate machine learning model development.

</details>


### [27] [Fine-Grained Dichotomies for Conjunctive Queries with Minimum or Maximum](https://arxiv.org/abs/2510.19197)
*Nofar Carmeli,Nikolaos Tziavelis*

Main category: cs.DB

TL;DR: 研究按属性最值排序的合取查询（CQ）答案直接访问的细粒度复杂性，并探索相关任务，为无自连接CQ建立完全二分法。


<details>
  <summary>Details</summary>
Motivation: 了解按属性最值排序的CQ答案直接访问的细粒度复杂性，探索相关任务的高效解决情况。

Method: 开发工具研究CQ答案直接访问复杂性，对无自连接CQ进行分析。

Result: 为每个任务建立了无自连接CQ的完全二分法，确定了能在近理想时间内解决的情况。

Conclusion: 可以精确识别无自连接CQ中能在近理想时间内解决的情况。

Abstract: We investigate the fine-grained complexity of direct access to Conjunctive
Query (CQ) answers according to their position, ordered by the minimum (or
maximum) value between attributes. We further use the tools we develop to
explore a wealth of related tasks. We consider the task of ranked enumeration
under min/max orders, as well as tasks concerning CQs with predicates of the
form x <= min X , where X is a set of variables and x is a single variable:
counting, enumeration, direct access, and predicate elimination (i.e.,
transforming the pair of query and database to an equivalent pair without
min-predicates). For each task, we establish a complete dichotomy for
self-join-free CQs, precisely identifying the cases that are solvable in
near-ideal time, i.e., (quasi)linear preprocessing time followed by constant or
logarithmic time per output.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [28] [AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators](https://arxiv.org/abs/2510.18897)
*Jacopo Tagliabue*

Main category: cs.DC

TL;DR: 结合大语言模型随机代码生成与特定领域模拟器确定性验证，探索AI驱动的分布式系统策略设计，以FaaS为例开展研究并报告初步结果。


<details>
  <summary>Details</summary>
Motivation: 探索AI驱动的分布式系统策略设计，在保证可解释性的同时实现大设计空间的针对性搜索。

Method: 以Function - as - a - Service运行时和其开源模拟器为案例，将调度器设计构建为迭代的生成 - 验证循环，利用LLM生成策略、模拟器评估并提供反馈引导后续生成。

Result: 报告了多个模型吞吐量提升的初步结果。

Conclusion: 讨论了当前方法的局限性，推测AI对扩展该方法学至关重要。

Abstract: We explore AI-driven distributed-systems policy design by combining
stochastic code generation from large language models (LLMs) with deterministic
verification in a domain-specific simulator. Using a Function-as-a-Service
runtime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we
frame scheduler design as an iterative generate-and-verify loop: an LLM
proposes a Python policy, the simulator evaluates it on standardized traces,
and structured feedback steers subsequent generations. This setup preserves
interpretability while enabling targeted search over a large design space. We
detail the system architecture and report preliminary results on throughput
improvements across multiple models. Beyond early gains, we discuss the limits
of the current setup and outline next steps; in particular, we conjecture that
AI will be crucial for scaling this methodology by helping to bootstrap new
simulators.

</details>


### [29] [CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation](https://arxiv.org/abs/2510.18893)
*Sergey Pugachev*

Main category: cs.DC

TL;DR: 提出CodeCRDT解决多智能体LLM系统协调成本高问题，评估有速度提升和权衡。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体LLM系统因协调成本高无法实现并行加速的问题。

Method: 提出CodeCRDT，一种基于观察的协调模式，使用CRDT实现无锁、无冲突的并发代码生成。

Result: 600次试验显示，部分任务加速达21.1%，部分减速达39.4%，100%收敛且无合并失败，语义冲突率5 - 10%。

Conclusion: 形式化随机LLM智能体的观察驱动协调，揭示质量 - 性能权衡，基于任务结构描述并行协调成败情况。

Abstract: Multi-agent LLM systems fail to realize parallel speedups due to costly
coordination. We present CodeCRDT, an observation-driven coordination pattern
where agents coordinate by monitoring a shared state with observable updates
and deterministic convergence, rather than explicit message passing. Using
Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,
conflict-free concurrent code generation with strong eventual consistency.
Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits
and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on
others, and 100% convergence with zero merge failures. The study formalizes
observation-driven coordination for stochastic LLM agents, revealing semantic
conflict rates (5-10%) and quality-performance tradeoffs, and provides
empirical characterization of when parallel coordination succeeds versus fails
based on task structure.

</details>


### [30] [Comparative analysis of large data processing in Apache Spark using Java, Python and Scala](https://arxiv.org/abs/2510.19012)
*Ivan Borodii,Illia Fedorovych,Halyna Osukhivska,Diana Velychko,Roman Butsii*

Main category: cs.DC

TL;DR: 对比Java、Python和Scala使用Apache Spark处理大数据集的性能，发现语言对效率影响大，Scala和Java适合处理大量数据和复杂操作，Python在处理小数据时有优势。


<details>
  <summary>Details</summary>
Motivation: 此前对使用Apache Iceberg跨编程语言的完整ETL工作流的全面比较有限，需进行相关对比分析。

Method: 执行多项操作，包括从CSV文件下载数据、转换并加载到Apache Iceberg分析表。

Result: 处理5MB CSV文件，Python最快；处理1.6GB文件，三者结果相近，Python最快；复杂操作中，Scala性能最高，Python最低。

Conclusion: 编程语言显著影响Apache Spark算法的数据处理效率，结果有助于根据性能需求和数据量优化数据处理流程。

Abstract: During the study, the results of a comparative analysis of the process of
handling large datasets using the Apache Spark platform in Java, Python, and
Scala programming languages were obtained. Although prior works have focused on
individual stages, comprehensive comparisons of full ETL workflows across
programming languages using Apache Iceberg remain limited. The analysis was
performed by executing several operations, including downloading data from CSV
files, transforming and loading it into an Apache Iceberg analytical table. It
was found that the performance of the Spark algorithm varies significantly
depending on the amount of data and the programming language used. When
processing a 5-megabyte CSV file, the best result was achieved in Python: 6.71
seconds, which is superior to Scala's score of 9.13 seconds and Java's time of
9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming
languages demonstrated similar results: the fastest performance was showed in
Python: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56
seconds, respectively. When performing a more complex operation that involved
combining two CSV files into a single dataset for further loading into an
Apache Iceberg table, Scala demonstrated the highest performance, at 374.42
seconds. Java processing was completed in 379.8 seconds, while Python was the
least efficient, with a runtime of 398.32 seconds. It follows that the
programming language significantly affects the efficiency of data processing by
the Apache Spark algorithm, with Scala and Java being more productive for
processing large amounts of data and complex operations, while Python
demonstrates an advantage in working with small amounts of data. The results
obtained can be useful for optimizing data handling processes depending on
specific performance requirements and the amount of information being
processed.

</details>


### [31] [Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond](https://arxiv.org/abs/2510.19805)
*Carl-Johan Fauvelle Munck af Rosensch"old,Feras M. Awaysheh,Ahmad Awad*

Main category: cs.DC

TL;DR: 研究对Redis替代方案进行基准测试，评估新兴内存键值存储系统，展示各系统权衡。


<details>
  <summary>Details</summary>
Motivation: 内存键值数据存储发展面临约束，当前文献缺乏对该领域先进工具的实验评估。

Method: 在Kubernetes部署的现实工作负载下，对Valkey、KeyDB和Garnet进行基准测试和系统评估。

Result: 各被测试数据系统间存在明显权衡。

Conclusion: 对新兴内存键值存储系统进行了全面性能和可行性评估，强调了性能、兼容性和长期可行性间的权衡。

Abstract: In-memory key-value datastores have become indispensable building blocks of
modern cloud-native infrastructures, yet their evolution faces scalability,
compatibility, and sustainability constraints. The current literature lacks an
experimental evaluation of state-of-the-art tools in the domain. This study
addressed this timely gap by benchmarking Redis alternatives and systematically
evaluating Valkey, KeyDB, and Garnet under realistic workloads within
Kubernetes deployments. The results demonstrate clear trade-offs among the
benchmarked data systems. Our study presents a comprehensive performance and
viability assessment of the emerging in-memory key-value stores. Metrics
include throughput, tail latency, CPU and memory efficiency, and migration
complexity. We highlight trade-offs between performance, compatibility, and
long-term viability, including project maturity, community support, and
sustained development.

</details>


### [32] [On the Randomized Locality of Matching Problems in Regular Graphs](https://arxiv.org/abs/2510.19151)
*Seri Khoury,Manish Purohit,Aaron Schild,Joshua Wang*

Main category: cs.DC

TL;DR: 研究正则图匹配问题的局部性，给出近似匹配随机算法，证明其局部性与图参数无关，给出下界，还分离了最大匹配的平均和最坏复杂度。


<details>
  <summary>Details</summary>
Motivation: 理解分布式对称破缺问题中匹配问题的局部性，正则图是建立下界和分类结果的主要基准。

Method: 开发随机算法，对Luby算法进行基于鞅的分析。

Result: 证明(1 + ε)-近似匹配局部性仅依赖ε，与其他图参数无关；最大匹配平均复杂度为O(1)；给出近似匹配和最大匹配的下界。

Conclusion: 正则图中近似匹配是真正局部的，最大匹配的节点平均复杂度和最坏复杂度有强分离。

Abstract: The main goal in distributed symmetry-breaking is to understand the locality
of problems; i.e., the radius of the neighborhood that a node needs to explore
in order to arrive at its part of a global solution. In this work, we study the
locality of matching problems in the family of regular graphs, which is one of
the main benchmarks for establishing lower bounds on the locality of
symmetry-breaking problems, as well as for obtaining classification results.
For approximate matching, we develop randomized algorithms to show that $(1 +
\epsilon)$-approximate matching in regular graphs is truly local; i.e., the
locality depends only on $\epsilon$ and is independent of all other graph
parameters. Furthermore, as long as the degree $\Delta$ is not very small
(namely, as long as $\Delta \geq \text{poly}(1/\epsilon)$), this dependence is
only logarithmic in $1/\epsilon$. This stands in sharp contrast to maximal
matching in regular graphs which requires some dependence on the number of
nodes $n$ or the degree $\Delta$. We show matching lower bounds for both
results. For maximal matching, our techniques further allow us to establish a
strong separation between the node-averaged complexity and worst-case
complexity of maximal matching in regular graphs, by showing that the former is
only $O(1)$. Central to our main technical contribution is a novel
martingale-based analysis for the $\approx 40$-year-old algorithm by Luby. In
particular, our analysis shows that applying one round of Luby's algorithm on
the line graph of a $\Delta$-regular graph results in an almost
$\Delta/2$-regular graph.

</details>


### [33] [RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs](https://arxiv.org/abs/2510.19225)
*Yongji Wu,Xueshen Liu,Haizhong Zheng,Juncheng Gu,Beidi Chen,Z. Morley Mao,Arvind Krishnamurthy,Ion Stoica*

Main category: cs.DC

TL;DR: 本文提出RLBoost，利用抢占式GPU资源实现高效强化学习训练，提升了训练吞吐量和成本效益。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习框架在处理资源紧张问题上存在不足，抢占式GPU资源若能有效利用可节省成本并加速训练。

Method: 采用混合架构，包括自适应滚动卸载、拉式权重转移、令牌级响应收集和迁移三项关键技术。

Result: 与仅使用按需GPU资源相比，RLBoost使训练吞吐量提高1.51 - 1.97倍，成本效率提高28% - 49%。

Conclusion: RLBoost是一种高效利用抢占式GPU资源进行强化学习训练的有效解决方案。

Abstract: Reinforcement learning (RL) has become essential for unlocking advanced
reasoning capabilities in large language models (LLMs). RL workflows involve
interleaving rollout and training stages with fundamentally different resource
requirements. Rollout typically dominates overall execution time, yet scales
efficiently through multiple independent instances. In contrast, training
requires tightly-coupled GPUs with full-mesh communication. Existing RL
frameworks fall into two categories: co-located and disaggregated
architectures. Co-located ones fail to address this resource tension by forcing
both stages to share the same GPUs. Disaggregated architectures, without
modifications of well-established RL algorithms, suffer from resource
under-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances
on public clouds and spare capacity in production clusters, present significant
cost-saving opportunities for accelerating RL workflows, if efficiently
harvested for rollout.
  In this paper, we present RLBoost, a systematic solution for cost-efficient
RL training that harvests preemptible GPU resources. Our key insight is that
rollout's stateless and embarrassingly parallel nature aligns perfectly with
preemptible and often fragmented resources. To efficiently utilize these
resources despite frequent and unpredictable availability changes, RLBoost
adopts a hybrid architecture with three key techniques: (1) adaptive rollout
offload to dynamically adjust workloads on the reserved (on-demand) cluster,
(2) pull-based weight transfer that quickly provisions newly available
instances, and (3) token-level response collection and migration for efficient
preemption handling and continuous load balancing. Extensive experiments show
RLBoost increases training throughput by 1.51x-1.97x while improving cost
efficiency by 28%-49% compared to using only on-demand GPU resources.

</details>


### [34] [RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.19262)
*Heng Xu,Zhiwei Yu,Chengze Du,Ying Zhou,Letian Li,Haojie Wang,Weiqiang Cheng,Jialong Li*

Main category: cs.DC

TL;DR: 提出分布式负载均衡框架RailS优化MoE训练中全对全通信，在多种工作负载上提升带宽、减少完成时间。


<details>
  <summary>Details</summary>
Motivation: MoE模型训练的全对全通信主导迭代时间，传统负载均衡方法无法充分利用Rail架构带宽。

Method: 利用Rail拓扑对称性将全局协调转化为本地调度，各节点执行LPT调度器，激活N条并行轨道进行多路径传输。

Result: 在合成和真实MoE工作负载上，提升总线带宽20% - 78%，减少完成时间17% - 78%；Mixtral工作负载迭代时间缩短18% - 40%，实现接近最优的负载均衡。

Conclusion: RailS能充分利用分布式训练的架构并行性。

Abstract: Training Mixture-of-Experts (MoE) models introduces sparse and highly
imbalanced all-to-all communication that dominates iteration time. Conventional
load-balancing methods fail to exploit the deterministic topology of Rail
architectures, leaving multi-NIC bandwidth underutilized. We present RailS, a
distributed load-balancing framework that minimizes all-to-all completion time
in MoE training. RailS leverages the Rail topology's symmetry to prove that
uniform sending ensures uniform receiving, transforming global coordination
into local scheduling. Each node independently executes a Longest Processing
Time First (LPT) spraying scheduler to proactively balance traffic using local
information. RailS activates N parallel rails for fine-grained, topology-aware
multipath transmission. Across synthetic and real-world MoE workloads, RailS
improves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For
Mixtral workloads, it shortens iteration time by 18%--40% and achieves
near-optimal load balance, fully exploiting architectural parallelism in
distributed training.

</details>


### [35] [FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems](https://arxiv.org/abs/2510.19301)
*Ziheng Deng,Xue Liu,Jiantong Jiang,Yankai Li,Qingxu Deng,Xiaochun Yang*

Main category: cs.DC

TL;DR: 本文提出FLASH Viterbi及FLASH - BS Viterbi算法，结合多种技术提升效率，开发FPGA硬件加速器，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 标准维特比解码在资源受限边缘平台内存占用大、计算不灵活，现有方法有运行时开销且缺乏适应性。

Method: 提出FLASH Viterbi算法，结合非递归分治策略、剪枝和并行化技术；提出FLASH - BS Viterbi，基于内存高效的数据结构；开发FPGA硬件加速器。

Result: 实验表明提出的算法在解码时间和内存效率上优于现有基线。

Conclusion: 算法具有适应性和硬件友好性，适合现代数据系统，代码公开。

Abstract: The Viterbi algorithm is a key operator for structured sequence inference in
modern data systems, with applications in trajectory analysis, online
recommendation, and speech recognition. As these workloads increasingly migrate
to resource-constrained edge platforms, standard Viterbi decoding remains
memory-intensive and computationally inflexible. Existing methods typically
trade decoding time for space efficiency, but often incur significant runtime
overhead and lack adaptability to various system constraints. This paper
presents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly
Viterbi decoding operator that enhances adaptability and resource efficiency.
FLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning
and parallelization techniques to enhance both time and memory efficiency,
making it well-suited for resource-constrained data systems.To further decouple
space complexity from the hidden state space size, we present FLASH-BS Viterbi,
a dynamic beam search variant built on a memory-efficient data structure. Both
proposed algorithms exhibit strong adaptivity to diverse deployment scenarios
by dynamically tuning internal parameters.To ensure practical deployment on
edge devices, we also develop FPGA-based hardware accelerators for both
algorithms, demonstrating high throughput and low resource usage. Extensive
experiments show that our algorithms consistently outperform existing baselines
in both decoding time and memory efficiency, while preserving adaptability and
hardware-friendly characteristics essential for modern data systems. All codes
are publicly available at https://github.com/Dzh-16/FLASH-Viterbi.

</details>


### [36] [HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission](https://arxiv.org/abs/2510.19470)
*Weihao Yang,Hao Huang,Donglei Wu,Ningke Li,Yanqi Pan,Qiyang Zheng,Wen Xia,Shiyi Li,Qiang Wang*

Main category: cs.DC

TL;DR: 现有跨数据中心（DC）的专家并行（EP）扩展存在可扩展性问题，本文提出 HybridEP 框架优化受限带宽下的 EP，实验显示性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 跨 DC 训练中，MoE 的 EP 因跨 DC 带宽有限面临可扩展性问题，现有优化在低带宽场景效果不佳，阻碍 MoE 模型发展。

Method: 提出 HybridEP 框架，动态转换专家空间布局减少通信流量和频率；构建流基模型确定最优传输比；采用基于域的分区和参数高效迁移技术。

Result: HybridEP 在受限带宽下比现有 MoE 训练系统性能高 5.6 倍；大规模模拟中，1k 个 DC 下不同带宽时加速比达 1.45 倍。

Conclusion: HybridEP 是更通用、可扩展性更好的 EP 方案。

Abstract: Mixture-of-Experts (MoE) has become a popular architecture for scaling large
models. However, the rapidly growing scale outpaces model training on a single
DC, driving a shift toward a more flexible, cross-DC training paradigm. Under
this, Expert Parallelism (EP) of MoE faces significant scalability issues due
to the limited cross-DC bandwidth. Specifically, existing EP optimizations
attempt to overlap data communication and computation, which has little benefit
in low-bandwidth scenarios due to a much longer data communication time.
Therefore, the trends of cross-DC EP scaling is fast becoming a critical
roadblock to the continued growth of MoE models.
  To address this, we propose HybridEP, a modeling-guided framework to optimize
EP under constrained bandwidth. Our key idea is to dynamically transform the
spatial placement of experts to reduce data communication traffic and
frequency, thereby minimizing EP's communication overheads. However, it is
non-trivial to find the optimal solution because it complicates the original
communication pattern by mixing data and expert communication. We therefore
build a stream-based model to determine the optimal transmission ratio. Guided
by this, we incorporate two techniques: (1) domain-based partition to construct
the mapping between hybrid patterns and specific communication topology at GPU
level, and (2) parameter-efficient migration to further refine this topology by
reducing expert transmission overhead and enlarging the domain size. Combining
all these designs, HybridEP can be considered as a more general EP with better
scalability. Experimental results show that HybridEP outperforms existing
state-of-the-art MoE training systems by up to 5.6x under constrained
bandwidth. We further compare HybridEP and EP on large-scale simulations.
HybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.

</details>


### [37] [Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud](https://arxiv.org/abs/2510.19617)
*Eric Ding*

Main category: cs.DC

TL;DR: 论文指出协作式机器学习缺乏可扩展、高效的基础设施，提出 Propius 系统，评估显示其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有协作式机器学习基础设施开发不足，特定用例的服务器 - 客户端系统不可扩展和复用，随着规模增长，需要可扩展、高效的多租户资源管理系统。

Method: 提出 Propius 系统，包含控制平面和数据平面，控制平面实现多作业资源共享，数据平面提高模型共享和结果收集的可扩展性。

Result: Propius 在资源利用率（最高 1.88 倍）、吞吐量（最高 2.76 倍）和作业完成时间（最高 1.26 倍）方面优于现有资源管理技术和框架。

Conclusion: Propius 系统能适应客户端机器的异构性，可扩展地管理和控制 ML 作业与边缘资源的计算流程，具有良好性能。

Abstract: Collaborative Machine Learning is a paradigm in the field of distributed
machine learning, designed to address the challenges of data privacy,
communication overhead, and model heterogeneity. There have been significant
advancements in optimization and communication algorithm design and ML hardware
that enables fair, efficient and secure collaborative ML training. However,
less emphasis is put on collaborative ML infrastructure development. Developers
and researchers often build server-client systems for a specific collaborative
ML use case, which is not scalable and reusable. As the scale of collaborative
ML grows, the need for a scalable, efficient, and ideally multi-tenant resource
management system becomes more pressing. We propose a novel system, Propius,
that can adapt to the heterogeneity of client machines, and efficiently manage
and control the computation flow between ML jobs and edge resources in a
scalable fashion. Propius is comprised of a control plane and a data plane. The
control plane enables efficient resource sharing among multiple collaborative
ML jobs and supports various resource sharing policies, while the data plane
improves the scalability of collaborative ML model sharing and result
collection. Evaluations show that Propius outperforms existing resource
management techniques and frameworks in terms of resource utilization (up to
$1.88\times$), throughput (up to $2.76$), and job completion time (up to
$1.26\times$).

</details>


### [38] [Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation](https://arxiv.org/abs/2510.19689)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Srinivas Vippagunta,Suchitra Raman,Shreeshankar Chatterjee,Ju Lin,Shang Liu,Mary Schladenhauffen,Jeffrey Luo,Hailong Jiang*

Main category: cs.DC

TL;DR: 本文提出面向生产的BDaaS蓝图，集成单节点无服务器GPU运行时与TabNet，经基准测试展示其在吞吐量、延迟、成本和合规性等方面优势。


<details>
  <summary>Details</summary>
Motivation: 工业和政府组织依赖数据驱动分析，分布式框架不适用于中等规模、对延迟敏感的推理，云提供商提供无服务器GPU及TabNet模型促使设计新部署蓝图。

Method: 提出集成单节点无服务器GPU运行时与TabNet的BDaaS蓝图，利用GPU加速、无服务器弹性和特征掩码可解释性，在HR、Adult和BLS数据集上进行基准测试。

Result: GPU管道与Spark基线相比，吞吐量最高提升4.5倍，延迟降低98倍，每1K推理成本降低90%，合规机制仅增加约5.7ms延迟，可解释性在峰值负载下稳定。

Conclusion: 研究提供合规基准、可重现蓝图和决策框架，证明无服务器GPU分析在企业和政府监管环境中的实用性。

Abstract: Industrial and government organizations increasingly depend on data-driven
analytics for workforce, finance, and regulated decision processes, where
timeliness, cost efficiency, and compliance are critical. Distributed
frameworks such as Spark and Flink remain effective for massive-scale batch or
streaming analytics but introduce coordination complexity and auditing
overheads that misalign with moderate-scale, latency-sensitive inference.
Meanwhile, cloud providers now offer serverless GPUs, and models such as TabNet
enable interpretable tabular ML, motivating new deployment blueprints for
regulated environments. In this paper, we present a production-oriented Big
Data as a Service (BDaaS) blueprint that integrates a single-node serverless
GPU runtime with TabNet. The design leverages GPU acceleration for throughput,
serverless elasticity for cost reduction, and feature-mask interpretability for
IL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,
comparing our approach against Spark and CPU baselines. Our results show that
GPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%
lower cost per 1K inferences compared to Spark baselines, while compliance
mechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains
stable under peak load, ensuring reliable auditability. Taken together, these
findings provide a compliance-aware benchmark, a reproducible Helm-packaged
blueprint, and a decision framework that demonstrate the practicality of
secure, interpretable, and cost-efficient serverless GPU analytics for
regulated enterprise and government settings.

</details>


### [39] [CommonSense: Efficient Set Intersection (SetX) Protocol Based on Compressed Sensing](https://arxiv.org/abs/2510.19725)
*Jingfan Meng,Tianji Yang,Jun Xu*

Main category: cs.DC

TL;DR: 研究集合交集问题，开发多轮SetX协议，在真实数据集上比IBLT - based SetR协议降低通信成本8到10倍。


<details>
  <summary>Details</summary>
Motivation: 现有集合交集问题常复用集合对称差协议，且认为两者成本相当，而实际上集合交集成本更低，需开发专用协议。

Method: 开发多轮SetX协议，Alice和Bob通过发送压缩感知草图，不断更新集合直至确定交集。

Result: 在真实数据集上，SetX协议比IBLT - based SetR协议降低通信成本8到10倍。

Conclusion: 开发的多轮SetX协议优于集合对称差问题的信息论下界，能有效降低通信成本。

Abstract: In the set reconciliation (\textsf{SetR}) problem, two parties Alice and Bob,
holding sets $\mathsf{A}$ and $\mathsf{B}$, communicate to learn the symmetric
difference $\mathsf{A} \Delta \mathsf{B}$. In this work, we study a related but
under-explored problem: set intersection (\textsf{SetX})~\cite{Ozisik2019},
where both parties learn $\mathsf{A} \cap \mathsf{B}$ instead. However,
existing solutions typically reuse \textsf{SetR} protocols due to the absence
of dedicated \textsf{SetX} protocols and the misconception that \textsf{SetR}
and \textsf{SetX} have comparable costs. Observing that \textsf{SetX} is
fundamentally cheaper than \textsf{SetR}, we developed a multi-round
\textsf{SetX} protocol that outperforms the information-theoretic lower bound
of \textsf{SetR} problem. In our \textsf{SetX} protocol, Alice sends Bob a
compressed sensing (CS) sketch of $\mathsf{A}$ to help Bob identify his unique
elements (those in $\mathsf{B \setminus A}$). This solves the \textsf{SetX}
problem, if $\mathsf{A} \subseteq \mathsf{B}$. Otherwise, Bob sends a CS sketch
of the residue (a set of elements he cannot decode) back to Alice for her to
decode her unique elements (those in $\mathsf{A \setminus B}$). As such, Alice
and Bob communicate back and forth %with a set membership filter (SMF) of
estimated $\mathsf{B \setminus A}$. Alice updates $\mathsf{A}$ and
communication repeats until both parties agrees on $\mathsf{A} \cap
\mathsf{B}$. On real world datasets, experiments show that our $\mathsf{SetX}$
protocol reduces the communication cost by 8 to 10 times compared to the
IBLT-based $\mathsf{SetR}$ protocol.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [40] [From Unweighted to Weighted Dynamic Matching in Non-Bipartite Graphs: A Low-Loss Reduction](https://arxiv.org/abs/2510.19049)
*Aaron Bernstein,Jiale Chen*

Main category: cs.DS

TL;DR: 研究全动态图中近似最大权重匹配问题，设计元算法将其转化为无权近似最大基数匹配问题，缩小加权与无权差距，给出首个低损失归约，还给出近似部分动态匹配的条件下界。


<details>
  <summary>Details</summary>
Motivation: 此前适用于非二分图的元算法有1/2近似损失，需缩小加权与无权匹配问题的差距。

Method: 设计元算法，利用新的原始 - 对偶框架，将一般图中近似最大权重匹配计算转化为辅助二分扩展图上的近似诱导匹配查询序列。

Result: 实现首个低损失归约，将二分图上的全动态(1 - ε)近似最大基数匹配算法转化为一般图上的全动态(1 - ε)近似最大权重匹配算法，更新时间有poly(log n/ε)开销，给出近似部分动态匹配的条件下界。

Conclusion: 所设计的方法能有效缩小加权与无权匹配问题的差距，在全动态图近似最大权重匹配问题上取得进展。

Abstract: We study the approximate maximum weight matching (MWM) problem in a fully
dynamic graph subject to edge insertions and deletions. We design
meta-algorithms that reduce the problem to the unweighted approximate maximum
cardinality matching (MCM) problem. Despite recent progress on bipartite graphs
-- Bernstein-Dudeja-Langley (STOC 2021) and
Bernstein-Chen-Dudeja-Langley-Sidford-Tu (SODA 2025) -- the only previous
meta-algorithm that applied to non-bipartite graphs suffered a $\frac{1}{2}$
approximation loss (Stubbs-Williams, ITCS 2017). We significantly close the
weighted-and-unweighted gap by showing the first low-loss reduction that
transforms any fully dynamic $(1-\varepsilon)$-approximate MCM algorithm on
bipartite graphs into a fully dynamic $(1-\varepsilon)$-approximate MWM
algorithm on general (not necessarily bipartite) graphs, with only a
$\mathrm{poly}(\log n/\varepsilon)$ overhead in the update time. Central to our
approach is a new primal-dual framework that reduces the computation of an
approximate MWM in general graphs to a sequence of approximate induced matching
queries on an auxiliary bipartite extension. In addition, we give the first
conditional lower bound on approximate partially dynamic matching with
worst-case update time.

</details>


### [41] [Succinct Dynamic Rank/Select: Bypassing the Tree-Structure Bottleneck](https://arxiv.org/abs/2510.19175)
*William Kuszmaul,Jingxun Liang,Renfei Zhou*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We show how to construct a dynamic ordered dictionary, supporting
insert/delete/rank/select on a set of $n$ elements from a universe of size $U$,
that achieves the optimal amortized expected time complexity of $O(1 + \log n /
\log \log U)$, while achieving a nearly optimal space consumption of $\log
\binom{U}{n} + n / 2^{(\log n)^{\Omega(1)}} + \text{polylog}\, U$ bits in the
regime where $U = \text{poly}(n)$. This resolves an open question by Pibiri and
Venturini as to whether a redundancy (a.k.a. space overhead) of $o(n)$ bits is
possible, and is the first dynamic solution to bypass the so-called
tree-structure bottleneck, in which the bits needed to encode some dynamic tree
structure are themselves enough to force a redundancy of
$\widetilde{\Omega}(n)$ bits. Our main technical building block is a dynamic
balanced binary search tree, which we call the compressed tabulation-weighted
treap, that itself achieves a surprising time/space tradeoff. The tree supports
$\text{polylog}\, n$-time operations and requires a static lookup table of size
$\text{poly}(n) + \text{polylog}\, U$ -- but, in exchange for these, the tree
is able to achieve a remarkable space guarantee. Its total space redundancy is
$O(\log U)$ bits. In fact, if the tree is given $n$ and $U$ for free, then the
redundancy further drops to $O(1)$ bits.

</details>


### [42] [Online Two-Stage Submodular Maximization](https://arxiv.org/abs/2510.19480)
*Iasonas Nikolaou,Miltiadis Stouras,Stratis Ioannidis,Evimaria Terzi*

Main category: cs.DS

TL;DR: 引入在线两阶段子模最大化问题(O2SSM)，针对加权阈值势函数设计算法，分析不同约束下的后悔值并进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 原两阶段子模最大化问题(2SSM)目标是限制基集，而本文研究子模目标在线披露的情况，即引入O2SSM问题。

Method: 针对加权阈值势函数这一单调子模函数子类，设计算法并分析在一般拟阵约束和均匀拟阵约束下的后悔值。

Result: 算法在一般拟阵约束下实现次线性(1 - 1/e)^2 - 后悔值，在均匀拟阵下实现(1 - 1/e)(1 - e^{-k}k^k/k!) - 后悔值，后者为(离线)2SSM问题提供了最优界。

Conclusion: 通过在真实数据集上的实验验证了在线算法的性能。

Abstract: Given a collection of monotone submodular functions, the goal of Two-Stage
Submodular Maximization (2SSM) [Balkanski et al., 2016] is to restrict the
ground set so an objective selected u.a.r. from the collection attains a high
maximal value, on average, when optimized over the restricted ground set. We
introduce the Online Two-Stage Submodular Maximization (O2SSM) problem, in
which the submodular objectives are revealed in an online fashion. We study
this problem for weighted threshold potential functions, a large and important
subclass of monotone submodular functions that includes influence maximization,
data summarization, and facility location, to name a few. We design an
algorithm that achieves sublinear $(1 - 1/e)^2$-regret under general matroid
constraints and $(1 - 1/e)(1-e^{-k}k^k/k!)$-regret in the case of uniform
matroids of rank $k$; the latter also yields a state-of-the-art bound for the
(offline) 2SSM problem. We empirically validate the performance of our online
algorithm with experiments on real datasets.

</details>


### [43] [Optimal Random Access and Conditional Lower Bounds for 2D Compressed Strings](https://arxiv.org/abs/2510.19750)
*Rajat De,Dominik Kempa*

Main category: cs.DS

TL;DR: 本文针对二维字符串压缩索引提出三项贡献，包括设计支持最优时间随机访问的数据结构、证明模式匹配条件下界、表明部分二维查询在假设下无法高效支持。


<details>
  <summary>Details</summary>
Motivation: 一维压缩索引技术在二维数据上应用效果不佳，需开发原生二维压缩索引方案以兼顾压缩和查询效率。

Method: 设计支持最优时间随机访问的二维字符串数据结构；证明二维语法压缩字符串模式匹配的条件下界；基于一维语法压缩字符串查询的难度假设分析二维查询。

Result: 实现了O(log n/log log n)查询时间和O(|G|log^{2+ε}n)空间的数据结构；证明在正交向量猜想下模式匹配问题时间下界；表明部分二维查询在假设下难以高效支持。

Conclusion: 推动了二维字符串压缩索引理论发展，首次将二维压缩索引复杂度与一维未解决问题建立联系。

Abstract: Compressed indexing is a powerful technique that enables efficient querying
over data stored in compressed form, significantly reducing memory usage and
often accelerating computation. While extensive progress has been made for
one-dimensional strings, many real-world datasets (such as images, maps, and
adjacency matrices) are inherently two-dimensional and highly compressible.
Unfortunately, naively applying 1D techniques to 2D data leads to suboptimal
results, as fundamental structural repetition is lost during linearization.
This motivates the development of native 2D compressed indexing schemes that
preserve both compression and query efficiency.
  We present three main contributions that advance the theory of compressed
indexing for 2D strings: (1) We design the first data structure that supports
optimal-time random access to a 2D string compressed by a 2D grammar.
Specifically, for a 2D string $T\in\Sigma^{r\times c}$ compressed by a 2D
grammar $G$ and any constant $\epsilon>0$, we achieve $O(\log n/\log \log n)$
query time and $O(|G|\log^{2+\epsilon}n)$ space, where $n=\max(r,c)$. (2) We
prove conditional lower bounds for pattern matching over 2D-grammar compressed
strings. Assuming the Orthogonal Vectors Conjecture, no algorithm can solve
this problem in time $O(|G|^{2-\epsilon}\cdot |P|^{O(1)})$ for any
$\epsilon>0$, demonstrating a separation from the 1D case, where optimal
solutions exist. (3) We show that several fundamental 2D queries, such as the
2D longest common extension, rectangle sum, and equality, cannot be supported
efficiently under hardness assumptions for rank and symbol occurrence queries
on 1D grammar-compressed strings. This is the first evidence connecting the
complexity of 2D compressed indexing to long-standing open problems in the 1D
setting.

</details>


### [44] [Strongly Polynomial Parallel Work-Depth Tradeoffs for Directed SSSP](https://arxiv.org/abs/2510.19780)
*Adam Karczmarz,Wojciech Nadara,Marek Sokołowski*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we show new strongly polynomial work-depth tradeoffs for
computing single-source shortest paths (SSSP) in non-negatively weighted
directed graphs in parallel. Most importantly, we prove that directed SSSP can
be solved within $\tilde{O}(m+n^{2-\epsilon})$ work and
$\tilde{O}(n^{1-\epsilon})$ depth for some positive $\epsilon>0$. In
particular, for dense graphs with non-negative real weights, we provide the
first nearly work-efficient strongly polynomial algorithm with sublinear depth.
  Our result immediately yields improved strongly polynomial parallel
algorithms for min-cost flow and the assignment problem. It also leads to the
first non-trivial strongly polynomial dynamic algorithm for minimum mean cycle.
Moreover, we develop efficient parallel algorithms in the Word RAM model for
several variants of SSSP in graphs with exponentially large edge weights.

</details>


### [45] [A Logic-based Algorithmic Meta-Theorem for Treedepth: Single Exponential FPT Time and Polynomial Space](https://arxiv.org/abs/2510.19793)
*Benjamin Bergougnoux,Vera Chekan,Giannos Stamoulis*

Main category: cs.DS

TL;DR: 引入逻辑NEO₂[FRec]+ACK，给出其模型检查算法，统一并扩展相关结果，对部分片段降低空间复杂度，为未知算法的问题提供时间和空间复杂度算法。


<details>
  <summary>Details</summary>
Motivation: 解决已知在给定深度为k的消除森林的n顶点图上可在2ᴼᵏnᴼ¹时间和nᴼ¹空间内处理的NP难问题，统一和扩展相关结果。

Method: 引入逻辑NEO₂[FRec]+ACK，它是完全存在MSO₂的扩展，具有查询顶点集邻域、验证连通性和无环性、验证顶点集是否诱导团等谓词，并给出模型检查算法。

Result: 为NEO₂[FRec]+ACK提供具有相应复杂度的模型检查算法，对NEO₂[FRec]+k片段降低空间复杂度至O(k log n)，为未知算法的问题提供2ᴼᵏnᴼ¹时间和nᴼ¹空间算法。

Conclusion: 逻辑NEO₂[FRec]+ACK能有效处理相关NP难问题，统一和扩展了已知结果，为一些问题提供了新算法。

Abstract: For a graph $G$, the parameter treedepth measures the minimum depth among all
forests $F$, called elimination forests, such that $G$ is a subgraph of the
ancestor-descendant closure of $F$. We introduce a logic, called neighborhood
operator logic with acyclicity, connectivity and clique constraints
($\mathsf{NEO}_2[\mathsf{FRec}]+\mathsf{ACK}$ for short), that captures all
NP-hard problems$\unicode{x2013}$like Independent Set or Hamiltonian
Cycle$\unicode{x2013}$that are known to be tractable in time
$2^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$ and space $n^{\mathcal{O}(1)}$ on
$n$-vertex graphs provided with elimination forests of depth $k$. We provide a
model checking algorithm for $\mathsf{NEO}_2[\mathsf{FRec}]+\mathsf{ACK}$ with
such complexity that unifies and extends these results. For
$\mathsf{NEO}_2[\mathsf{FRec}]+\mathsf{k}$, the fragment of the above logic
that does not use acyclicity and connectivity constraints, we get a
strengthening of this result, where the space complexity is reduced to
$\mathcal{O}(k\log(n))$.
  With a similar mechanism as the distance neighborhood logic introduced in
[Bergougnoux, Dreier and Jaffke, SODA 2023], the logic
$\mathsf{NEO}_2[\mathsf{FRec}]+\mathsf{ACK}$ is an extension of the
fully-existential $\mathsf{MSO}_2$ with predicates for (1) querying
generalizations of the neighborhoods of vertex sets, (2) verifying the
connectivity and acyclicity of vertex and edge sets, and (3) verifying that a
vertex set induces a clique. Our results provide
$2^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$ time and $n^{\mathcal{O}(1)}$ space
algorithms for problems for which the existence of such algorithms was
previously unknown. In particular, $\mathsf{NEO}_2[\mathsf{FRec}]$ captures
CNF-SAT via the incidence graphs associated to CNF formulas, and it also
captures several modulo counting problems like Odd Dominating Set.

</details>


### [46] [Explaining the Inherent Tradeoffs for Suffix Array Functionality: Equivalences between String Problems and Prefix Range Queries](https://arxiv.org/abs/2510.19815)
*Dominik Kempa,Tomasz Kociumaka*

Main category: cs.DS

TL;DR: 研究后缀数组条目在无法显式存储时的高效访问问题，提出双向归约证明后缀数组查询与前缀选择查询的等价性，为分析和改进字符串处理问题提供统一框架。


<details>
  <summary>Details</summary>
Motivation: 探讨能否用不同技术改进现有后缀数组查询的空间和时间复杂度界限。

Method: 提出首个双向归约，证明后缀数组查询与前缀选择查询在所有参数上的等价性。

Result: 证明后缀数组查询与前缀选择查询等价，确定六组核心问题对连接字符串和前缀查询模型。

Conclusion: 框架为通过前缀查询视角分析和改进基本字符串处理问题效率提供统一基础。

Abstract: We study the fundamental question of how efficiently suffix array entries can
be accessed when the array cannot be stored explicitly. The suffix array
$SA_T[1..n]$ of a text $T$ of length $n$ encodes the lexicographic order of its
suffixes and underlies numerous applications in pattern matching, data
compression, and bioinformatics. Previous work established one-way reductions
showing how suffix array queries can be answered using, for example, rank
queries on the Burrows-Wheeler Transform. More recently, a new class of prefix
queries was introduced, together with reductions that, among others, transform
a simple tradeoff for prefix-select queries into a suffix array tradeoff
matching state-of-the-art space and query-time bounds, while achieving
sublinear construction time. For binary texts, the resulting data structure
achieves space $O(n)$ bits, preprocessing time $O(n / \sqrt{\log n})$,
preprocessing space of $O(n)$ bits, and query time $O(\log^{\epsilon} n)$ for
any constant $\epsilon > 0$. However, whether these bounds could be improved
using different techniques has remained open.
  We resolve this question by presenting the first bidirectional reduction
showing that suffix array queries are, up to an additive $O(\log\log n)$ term
in query time, equivalent to prefix-select queries in all parameters. This
result unifies prior approaches and shows that essentially all efficient suffix
array representations can be expressed via prefix-select structures. Moreover,
we prove analogous equivalences for inverse suffix array queries, pattern
ranking, lexicographic range, and SA-interval queries, identifying six core
problem pairs that connect string and prefix query models. Our framework thus
provides a unified foundation for analyzing and improving the efficiency of
fundamental string-processing problems through the lens of prefix queries.

</details>


### [47] [Tight Lower Bounds for Central String Queries in Compressed Space](https://arxiv.org/abs/2510.19820)
*Dominik Kempa,Tomasz Kociumaka*

Main category: cs.DS

TL;DR: 本文研究压缩数据结构极限，为多数基本查询建立紧下界，完成压缩索引理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前仅对随机访问查询有最优查询时间刻画，其他基本查询存在空白。

Method: 为多数基本查询（如SA、LCP等）开发紧下界。

Result: 证明SA、SA⁻¹等查询需Ω(log n/log log n)时间，其他常见查询需Ω(log log n)时间，且下界对二进制字母表文本也成立。

Conclusion: 在压缩空间中支持核心字符串查询的最优时间复杂度为Θ(log n/log log n)或Θ(log log n)，填补上下界间关键空白。

Abstract: In this work, we study the limits of compressed data structures, i.e.,
structures that support various queries on an input text $T\in\Sigma^n$ using
space proportional to the size of $T$ in compressed form. Nearly all
fundamental queries can currently be efficiently supported in
$O(\delta(T)\log^{O(1)}n)$ space, where $\delta(T)$ is the substring
complexity, a strong compressibility measure that lower-bounds the optimal
space to represent the text [Kociumaka, Navarro, Prezza, IEEE Trans. Inf.
Theory 2023]. However, optimal query time has been characterized only for
random access.
  We address this gap by developing tight lower bounds for nearly all other
fundamental queries: (1) We prove that suffix array (SA), inverse suffix array
(SA$^{-1}$), longest common prefix (LCP) array, and longest common extension
(LCE) queries all require $\Omega(\log n/\log\log n)$ time within
$O(\delta(T)\log^{O(1)}n)$ space, matching known upper bounds. (2) We further
show that other common queries, currently supported in $O(\log\log n)$ time and
$O(\delta(T)\log^{O(1)}n)$ space, including the Burrows-Wheeler Transform
(BWT), permuted longest common prefix (PLCP) array, Last-to-First (LF), inverse
LF, lexicographic predecessor ($\Phi$), and inverse $\Phi$ queries, all require
$\Omega(\log\log n)$ time, yielding another set of tight bounds.
  Our lower bounds hold even for texts over a binary alphabet. This work
establishes a clean dichotomy: the optimal time complexity to support central
string queries in compressed space is either $\Theta(\log n/\log\log n)$ or
$\Theta(\log\log n)$. This completes the theoretical foundation of compressed
indexing, closing a crucial gap between upper and lower bounds and providing a
clear target for future data structures: seeking either the optimal time in the
smallest space or the fastest time in the optimal space, both of which are now
known for central string queries.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [48] [Impartial Selection with Predictions](https://arxiv.org/abs/2510.19002)
*Javier Cembrano,Felix Fischer,Max Klimm*

Main category: cs.GT

TL;DR: 研究基于相互提名的代理选择问题，探讨在给定预测集时公正机制性能的提升，给出不同情况下机制的一致性和鲁棒性边界。


<details>
  <summary>Details</summary>
Motivation: 解决代理在相互提名中可能歪曲意见的问题，研究给定预测集时能否提升公正机制性能。

Method: 为一般和特殊选择情况设计机制，分析其一致性和鲁棒性。

Result: 一般情况给出一致性为$1 - Oig(rac{1}{k}ig)$、鲁棒性为$1 - rac{1}{e} - Oig(rac{1}{k}ig)$的机制；特殊情况能实现$1$-一致性和$rac{1}{2}$-鲁棒性。

Conclusion: （渐近）最优一致性可在几乎不牺牲鲁棒性的情况下实现。

Abstract: We study the selection of agents based on mutual nominations, a theoretical
problem with many applications from committee selection to AI alignment. As
agents both select and are selected, they may be incentivized to misrepresent
their true opinion about the eligibility of others to influence their own
chances of selection. Impartial mechanisms circumvent this issue by
guaranteeing that the selection of an agent is independent of the nominations
cast by that agent. Previous research has established strong bounds on the
performance of impartial mechanisms, measured by their ability to approximate
the number of nominations for the most highly nominated agents. We study to
what extent the performance of impartial mechanisms can be improved if they are
given a prediction of a set of agents receiving a maximum number of
nominations. Specifically, we provide bounds on the consistency and robustness
of such mechanisms, where consistency measures the performance of the
mechanisms when the prediction is accurate and robustness its performance when
the prediction is inaccurate. For the general setting where up to $k$ agents
are to be selected and agents nominate any number of other agents, we give a
mechanism with consistency $1-O\big(\frac{1}{k}\big)$ and robustness
$1-\frac{1}{e}-O\big(\frac{1}{k}\big)$. For the special case of selecting a
single agent based on a single nomination per agent, we prove that
$1$-consistency can be achieved while guaranteeing $\frac{1}{2}$-robustness. A
close comparison with previous results shows that (asymptotically) optimal
consistency can be achieved with little to no sacrifice in terms of robustness.

</details>


### [49] [Desirable Effort Fairness and Optimality Trade-offs in Strategic Learning](https://arxiv.org/abs/2510.19098)
*Valia Efthymiou,Ekaterina Fedorova,Chara Podimata*

Main category: cs.GT

TL;DR: 本文研究战略学习中决策者的约束优化问题，提出统一模型，给出理论保证并通过实验展示准确性与公平性的权衡。


<details>
  <summary>Details</summary>
Motivation: 标准战略学习模型假设单一，现实决策系统目标复杂，需考虑诱导激励的外部效应和公平性，研究约束优化成本。

Method: 提出包含特征因果依赖、异质操纵成本和同行学习三个额外组件的主 - 代理交互统一模型，为决策者在特定公平性容忍度下的最优损失提供理论保证。

Result: 通过对真实数据集的实验，展示了在最大化准确性和期望努力公平性之间的明确权衡。

Conclusion: 所提出的模型和方法能有效处理战略学习中决策者在考虑公平性约束下的优化问题。

Abstract: Strategic learning studies how decision rules interact with agents who may
strategically change their inputs/features to achieve better outcomes. In
standard settings, models assume that the decision-maker's sole scope is to
learn a classifier that maximizes an objective (e.g., accuracy) assuming that
agents best respond. However, real decision-making systems' goals do not align
exclusively with producing good predictions. They may consider the external
effects of inducing certain incentives, which translates to the change of
certain features being more desirable for the decision maker. Further, the
principal may also need to incentivize desirable feature changes fairly across
heterogeneous agents. How much does this constrained optimization (i.e.,
maximize the objective, but restrict agents' incentive disparity) cost the
principal? We propose a unified model of principal-agent interaction that
captures this trade-off under three additional components: (1) causal
dependencies between features, such that changes in one feature affect others;
(2) heterogeneous manipulation costs between agents; and (3) peer learning,
through which agents infer the principal's rule. We provide theoretical
guarantees on the principal's optimality loss constrained to a particular
desirability fairness tolerance for multiple broad classes of fairness
measures. Finally, through experiments on real datasets, we show the explicit
tradeoff between maximizing accuracy and fairness in desirability effort.

</details>


### [50] [Autobidding Arena: unified evaluation of the classical and RL-based autobidding algorithms](https://arxiv.org/abs/2510.19357)
*Andrey Pudovikov,Alexandra Khirianova,Ekaterina Solodneva,Aleksandr Katrutsa,Egor Samosvat,Yuriy Dorn*

Main category: cs.GT

TL;DR: 本文提出标准化透明评估协议对比经典与强化学习自动竞价算法，展示其用例与不足，结果助从业者选合适算法。


<details>
  <summary>Details</summary>
Motivation: 电商广告拍卖中自动竞价算法需公平可复现评估，以解决算法评估问题。

Method: 采用行业最新开源环境模拟竞价过程，考虑不同类型高效自动竞价算法并在竞价环境中进行基准测试，选择多指标评估。

Result: 展示算法最有前景用例，突出其意外缺点，按多指标评估。

Conclusion: 对比结果帮助从业者从不同角度评估候选算法，选择符合公司目标的高效算法。

Abstract: Advertisement auctions play a crucial role in revenue generation for
e-commerce companies. To make the bidding procedure scalable to thousands of
auctions, the automatic bidding (autobidding) algorithms are actively developed
in the industry. Therefore, the fair and reproducible evaluation of autobidding
algorithms is an important problem. We present a standardized and transparent
evaluation protocol for comparing classical and reinforcement learning (RL)
autobidding algorithms. We consider the most efficient autobidding algorithms
from different classes, e.g., ones based on the controllers, RL, optimal
formulas, etc., and benchmark them in the bidding environment. We utilize the
most recent open-source environment developed in the industry, which accurately
emulates the bidding process. Our work demonstrates the most promising use
cases for the considered autobidding algorithms, highlights their surprising
drawbacks, and evaluates them according to multiple metrics. We select the
evaluation metrics that illustrate the performance of the autobidding
algorithms, the corresponding costs, and track the budget pacing. Such a choice
of metrics makes our results applicable to the broad range of platforms where
autobidding is effective. The presented comparison results help practitioners
to evaluate the candidate autobidding algorithms from different perspectives
and select ones that are efficient according to their companies' targets.

</details>


### [51] [Comparing Uniform Price and Discriminatory Multi-Unit Auctions through Regret Minimization](https://arxiv.org/abs/2510.19591)
*Marius Potfer,Vianney Perchet*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Repeated multi-unit auctions, where a seller allocates multiple identical
items over many rounds, are common mechanisms in electricity markets and
treasury auctions. We compare the two predominant formats: uniform-price and
discriminatory auctions, focusing on the perspective of a single bidder
learning to bid against stochastic adversaries. We characterize the learning
difficulty in each format, showing that the regret scales similarly for both
auction formats under both full-information and bandit feedback, as
$\tilde{\Theta} ( \sqrt{T} )$ and $\tilde{\Theta} ( T^{2/3} )$, respectively.
However, analysis beyond worst-case regret reveals structural differences:
uniform-price auctions may admit faster learning rates, with regret scaling as
$\tilde{\Theta} ( \sqrt{T} )$ in settings where discriminatory auctions remain
at $\tilde{\Theta} ( T^{2/3} )$. Finally, we provide a specific analysis for
auctions in which the other participants are symmetric and have unit-demand,
and show that in these instances, a similar regret rate separation appears.

</details>


### [52] [On Minimal Achievable Quotas in Multiwinner Voting](https://arxiv.org/abs/2510.19620)
*Patrick Becker,Fabian Frank*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Justified representation (JR) and extended justified representation (EJR) are
well-established proportionality axioms in approval-based multiwinner voting.
Both axioms are always satisfiable, but they rely on a fixed quota (typically
Hare or Droop), with the Droop quota being the smallest one that guarantees
existence across all instances. With this observation in mind, we take a first
step beyond the fixed-quota paradigm and introduce proportionality notions
where the quota is instance-dependent. We demonstrate that all commonly studied
voting rules can have an additive distance to the optimum of
$\frac{k^2}{(k+1)^2}$. Moreover, we look into the computational aspects of our
instance-dependent quota and prove that determining the optimal value of
$\alpha$ for a given approval profile satisfying $\alpha$-JR is NP-complete. To
address this, we introduce an integer linear programming (ILP) formulation for
computing committees that satisfy $\alpha$-JR, and we provide positive results
in the voter interval (VI) and candidate interval (CI) domains.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [53] [SBAN: A Framework \& Multi-Dimensional Dataset for Large Language Model Pre-Training and Software Code Mining](https://arxiv.org/abs/2510.18936)
*Hamed Jelodar,Mohammad Meymani,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: 本文介绍大规模多维数据集SBAN，用于推进软件代码分析大语言模型的预训练和评估，有多种应用并为构建智能系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 推进软件代码分析大语言模型的预训练和评估，解决相关研究中缺乏合适数据集的问题。

Method: 构建包含超300万个样本，涵盖二进制代码、汇编指令、自然语言描述和源代码四层的SBAN数据集。

Result: SBAN数据集可用于跨表示学习、语义理解、自动化恶意软件检测等多种任务，适合深度模型可扩展训练。

Conclusion: SBAN数据集为挖掘软件行为、改进安全分析和增强大语言模型在代码挖掘任务中的能力带来新机遇。

Abstract: This paper introduces SBAN (Source code, Binary, Assembly, and Natural
Language Description), a large-scale, multi-dimensional dataset designed to
advance the pre-training and evaluation of large language models (LLMs) for
software code analysis. SBAN comprises more than 3 million samples, including
2.9 million benign and 672,000 malware respectively, each represented across
four complementary layers: binary code, assembly instructions, natural language
descriptions, and source code. This unique multimodal structure enables
research on cross-representation learning, semantic understanding of software,
and automated malware detection. Beyond security applications, SBAN supports
broader tasks such as code translation, code explanation, and other software
mining tasks involving heterogeneous data. It is particularly suited for
scalable training of deep models, including transformers and other LLM
architectures. By bridging low-level machine representations and high-level
human semantics, SBAN provides a robust foundation for building intelligent
systems that reason about code. We believe that this dataset opens new
opportunities for mining software behavior, improving security analytics, and
enhancing LLM capabilities in pre-training and fine-tuning tasks for software
code mining.

</details>


### [54] [XGen-Q: An Explainable Domain-Adaptive LLM Framework with Retrieval-Augmented Generation for Software Security](https://arxiv.org/abs/2510.19006)
*Hamed Jelodar,Mohammad Meymani,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: 文章介绍用于恶意软件检测分析的领域适配大语言模型XGen-Q，实验显示其效果良好。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大语言模型在网络安全恶意软件检测分析应用有限，现有检测系统泛化能力不足，需更具适应性和可解释性的模型。

Method: 基于Qwen - Coder架构构建XGen - Q，在超百万恶意软件样本语料上预训练，采用多阶段提示策略结合检索增强生成，设计训练管道使其接触多样混淆模式。

Result: XGen - Q困惑度显著低于竞争基线，在新恶意软件样本上表现良好。

Conclusion: 基于大语言模型的方法在可解释和鲁棒的恶意软件分析方面有前景。

Abstract: Generative AI and large language models (LLMs) have shown strong capabilities
in code understanding, but their use in cybersecurity, particularly for malware
detection and analysis, remains limited. Existing detection systems often fail
to generalize to obfuscated or previously unseen threats, underscoring the need
for more adaptable and explainable models. To address this challenge, we
introduce XGen-Q, a domain-adapted LLM built on the Qwen-Coder architecture and
pretrained on a large-scale corpus of over one million malware samples,
spanning both source and assembly code. XGen-Q uses a multi-stage prompt
strategy combined with retrieval-augmented generation (RAG) to deliver reliable
malware identification and detailed forensic reporting, even in the presence of
complex code obfuscation. To further enhance generalization, we design a
training pipeline that systematically exposes the model to diverse obfuscation
patterns. Experimental results show that XGen-Q achieves significantly lower
perplexity than competitive baselines and exhibits strong performance on novel
malware samples, demonstrating the promise of LLM-based approaches for
interpretable and robust malware analysis.

</details>


### [55] [C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers for Generative Search](https://arxiv.org/abs/2510.19221)
*Yingchen Zhang,Ruqing Zhang,Jiafeng Guo,Wenjun Peng,Sen Li,Fuyu Lv,Xueqi Cheng*

Main category: cs.IR

TL;DR: 本文提出C2T - ID方法解决生成式检索中文档标识符设计中语义表达与搜索空间的权衡问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在生成式检索中，设计兼具丰富语义信息和可控搜索空间的文档标识符是重要挑战，现有方法存在不足。

Method: 先通过层次聚类构建语义数值文档标识符，再提取高频元数据关键词迭代替换数值标签，还可进行两级语义平滑。

Result: 在Natural Questions和淘宝产品搜索实验中，C2T - ID显著优于原子、语义码本和纯文本文档标识符基线。

Conclusion: C2T - ID能有效平衡语义表达和搜索空间约束。

Abstract: Designing document identifiers (docids) that carry rich semantic information
while maintaining tractable search spaces is a important challenge in
generative retrieval (GR). Popular codebook methods address this by building a
hierarchical semantic tree and constraining generation to its child nodes, yet
their numeric identifiers cannot leverage the large language model's pretrained
natural language understanding. Conversely, using text as docid provides more
semantic expressivity but inflates the decoding space, making the system
brittle to early-step errors. To resolve this trade-off, we propose C2T-ID: (i)
first construct semantic numerical docid via hierarchical clustering; (ii) then
extract high-frequency metadata keywords and iteratively replace each numeric
label with its cluster's top-K keywords; and (iii) an optional two-level
semantic smoothing step further enhances the fluency of C2T-ID. Experiments on
Natural Questions and Taobao's product search demonstrate that C2T-ID
significantly outperforms atomic, semantic codebook, and pure-text docid
baselines, demonstrating its effectiveness in balancing semantic expressiveness
with search space constraints.

</details>


### [56] [CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale](https://arxiv.org/abs/2510.19340)
*L. Caspari,M. Dinzinger,K. Gosh Dastidar,C. Fellicious,J. Mitrović,M. Granitzer*

Main category: cs.IR

TL;DR: 本文介绍了评估嵌入压缩方法的框架CoRECT，通过基准测试展示非学习型压缩可大幅减小索引大小且性能损失小，但选择最优压缩方法仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入压缩研究常忽略语料库复杂性这一关键因素，需要一个大规模评估框架来评估嵌入压缩方法。

Method: 引入CoRECT框架和新整理的数据集集合，对八种代表性压缩方法进行基准测试。

Result: 非学习型压缩能大幅减小索引大小，即使在多达1亿个段落上性能损失也不显著，但不同模型的压缩方法性能有差异。

Conclusion: 选择最优压缩方法具有挑战性，CoRECT框架有助于对压缩方法进行一致比较和明智选择。

Abstract: Dense retrieval systems have proven to be effective across various
benchmarks, but require substantial memory to store large search indices.
Recent advances in embedding compression show that index sizes can be greatly
reduced with minimal loss in ranking quality. However, existing studies often
overlook the role of corpus complexity -- a critical factor, as recent work
shows that both corpus size and document length strongly affect dense retrieval
performance. In this paper, we introduce CoRECT (Controlled Retrieval
Evaluation of Compression Techniques), a framework for large-scale evaluation
of embedding compression methods, supported by a newly curated dataset
collection. To demonstrate its utility, we benchmark eight representative types
of compression methods. Notably, we show that non-learned compression achieves
substantial index size reduction, even on up to 100M passages, with
statistically insignificant performance loss. However, selecting the optimal
compression method remains challenging, as performance varies across models.
Such variability highlights the necessity of CoRECT to enable consistent
comparison and informed selection of compression methods. All code, data, and
results are available on GitHub and HuggingFace.

</details>


### [57] [Top-P Masking for Cross Language Information Retrieval](https://arxiv.org/abs/2510.19758)
*Joseph Casale,Andrew Silverschotz,Joseph DeSimone*

Main category: cs.IR

TL;DR: 提出Top - P动态掩码用于跨语言信息检索，性能优于Top - K掩码。


<details>
  <summary>Details</summary>
Motivation: Top - K掩码作为促进信息检索任务中稀疏表示的方法，本文希望找到性能更好的方案。

Method: 提出使用类似于大语言模型中核采样的Top - P动态掩码方法，并在跨语言信息检索领域进行评估。

Result: Top - P动态掩码比Top - K掩码有更好的性能。

Conclusion: Top - P动态掩码可用于信息检索任务，替代Top - K掩码获得更好效果。

Abstract: Top-K masking schemes have been proposed as a method to promote sparse
representations in Information Retrieval (IR) tasks, as a simple alternative to
Floating Point Operations per Second (FLOPS) regularization. Algorithms such as
Bilingual Lexical and Document Expansion Model (BLADE), adopt this approach as
a post-processing stage. We propose using Top-P Dynamic Masking similar to
Nucleus Sampling in Large Language Models, and demonstrate better performance
than Top-K masking. Specifically, we evaluate our methods in the domain of
Cross Language Information Retrieval (CLIR)

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version](https://arxiv.org/abs/2510.18998)
*Buang Zhang,Tung Kieu,Xiangfei Qiu,Chenjuan Guo,Jilin Hu,Aoying Zhou,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: 提出编码-分解范式及基于互信息的度量方法用于时间序列异常检测，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有自编码器在时间序列异常检测中学习的表示对训练时间序列中的异常敏感，导致准确率降低，需要更鲁棒的方法。

Method: 提出编码 - 分解范式，将编码表示分解为稳定和辅助表示；提出基于互信息的度量方法替代重建误差来识别异常。

Result: 在八个常用的多变量和单变量时间序列基准测试中表现具有竞争力或达到了最先进水平，对不同污染比例的时间序列具有鲁棒性。

Conclusion: 提出的方法能有效提高时间序列异常检测的鲁棒性和准确性。

Abstract: Time series anomaly detection is important in modern large-scale systems and
is applied in a variety of domains to analyze and monitor the operation of
diverse systems. Unsupervised approaches have received widespread interest, as
they do not require anomaly labels during training, thus avoiding potentially
high costs and having wider applications. Among these, autoencoders have
received extensive attention. They use reconstruction errors from compressed
representations to define anomaly scores. However, representations learned by
autoencoders are sensitive to anomalies in training time series, causing
reduced accuracy. We propose a novel encode-then-decompose paradigm, where we
decompose the encoded representation into stable and auxiliary representations,
thereby enhancing the robustness when training with contaminated time series.
In addition, we propose a novel mutual information based metric to replace the
reconstruction errors for identifying anomalies. Our proposal demonstrates
competitive or state-of-the-art performance on eight commonly used multi- and
univariate time series benchmarks and exhibits robustness to time series with
different contamination ratios.

</details>


### [59] [3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency](https://arxiv.org/abs/2510.18905)
*Minseok Jung,Abhas Ricky,Muhammad Rameez Chatni*

Main category: cs.LG

TL;DR: 提出3D优化框架用于AI推理扩展，评估四种优化方法，结果表明膝点优化平衡最佳，为不同场景推理扩展奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有AI推理扩展的1D启发式和2D双变量权衡方法未考虑成本和延迟约束。

Method: 引入3D优化框架，通过蒙特卡罗模拟，对三种场景和九个大语言模型评估四种优化方法解决3D多目标优化问题。

Result: 膝点优化实现最佳平衡，精度优先时精度最大化方法更有利。

Conclusion: 该框架为不同操作环境下的推理扩展提供了理论基础。

Abstract: AI inference scaling is often tuned through 1D heuristics (a fixed reasoning
passes) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail
to consider cost and latency constraints. We introduce a 3D optimization
framework that jointly calibrates accuracy, cost, and latency within a unified
decision space, enabling constraints-aware inference scaling. Using Monte Carlo
simulations across three representative scenarios and nine simulated large
language models, we evaluate four optimization methods to address the 3D
multi-objective optimization (MOO) problem. Framing inference scaling in MOO
shapes a feasible space that 1D and 2D optimizations fail to capture, enabling
environmentadaptive selection of the inference scaling k. Results show that
knee-point optimization achieves the best balance, while accuracy-maximization
remains favorable when precision is prioritized. The framework establishes a
theoretical foundation for deployment-aware inference scaling across diverse
operational contexts.

</details>


### [60] [Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction in Multitask Learning Landscape](https://arxiv.org/abs/2510.18910)
*Ziquan Wei,Tingting Dan,Guorong Wu*

Main category: cs.LG

TL;DR: 本文提出一种功能性神经影像基础模型，通过多任务学习和半监督微调，在多种神经影像应用中取得有前景结果。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在临床应用中受样本量限制，且自监督学习的基础模型对下游任务效果不佳，需可靠的功能性神经影像基础模型。

Method: 将大脑建模为多任务学习，提出可扩展模型架构，包括多任务预训练（对多种脑 - 环境交互进行分词）和半监督微调（为预训练的脑 - 环境交互分配伪标签）。

Result: 在性别预测、人类行为识别和多种疾病早期诊断等应用中取得有前景的结果。

Conclusion: 该基础模型有很大潜力促进当前神经影像在临床常规中的应用。

Abstract: A reliable foundation model of functional neuroimages is critical to promote
clinical applications where the performance of current AI models is
significantly impeded by a limited sample size. To that end, tremendous efforts
have been made to pretraining large models on extensive unlabeled fMRI data
using scalable self-supervised learning. Since self-supervision is not
necessarily aligned with the brain-to-outcome relationship, most foundation
models are suboptimal to the downstream task, such as predicting disease
outcomes. By capitalizing on rich environmental variables and demographic data
along with an unprecedented amount of functional neuroimages, we form the brain
modeling as a multitask learning and present a scalable model architecture for
(i) multitask pretraining by tokenizing multiple brain-environment interactions
(BEI) and (ii) semi-supervised finetuning by assigning pseudo-labels of
pretrained BEI. We have evaluated our foundation model on a variety of
applications, including sex prediction, human behavior recognition, and disease
early diagnosis of Autism, Parkinson's disease, Alzheimer's disease, and
{Schizophrenia}, where promising results indicate the great potential to
facilitate current neuroimaging applications in clinical routines.

</details>


### [61] [ADPO: Anchored Direct Preference Optimization](https://arxiv.org/abs/2510.18913)
*Wang Zixian*

Main category: cs.LG

TL;DR: 本文提出ADPO统一框架，介绍其特性、变体，证明DPO等是其特例，实验表明其在不同场景表现良好并给出使用建议。


<details>
  <summary>Details</summary>
Motivation: 对Direct Preference Optimization (DPO) 进行推广，解决其硬二元标签和成对比较的局限性。

Method: 引入软偏好概率、任意参考策略锚定和基于Plackett - Luce分布的列表偏好建模，得到三个实用变体。

Result: 在上下文老虎机和顺序强化学习中，ADPO表现优于标准DPO；不同噪声场景下不同变体有不同效果。

Conclusion: 在干净或中等噪声场景使用成对锚定Soft - DPO，极端污染场景使用基于KDE的列表ADPO。

Abstract: Anchored Direct Preference Optimization (ADPO) is a unified framework that
generalizes Direct Preference Optimization (DPO) with soft preferences,
reference-policy anchoring, and groupwise extensions. While standard DPO
assumes hard binary labels and pairwise comparisons, ADPO introduces: (i) soft
preference probabilities that encode uncertainty and mitigate gradient drift;
(ii) arbitrary reference-policy anchors that stabilize training via groupwise
shift invariance and implicit KL regularization; and (iii) listwise preference
modeling through Plackett-Luce distributions. We prove that DPO, Bradley-Terry
objectives, and Top-1-vs-Rest formulations emerge as special cases. ADPO yields
three practical variants: pairwise anchored Soft-DPO, listwise anchored
Soft-DPO with raw rewards, and KDE-based listwise smoothing for heavy-tailed
noise. In contextual bandits, anchoring improves WinMass by 38-63% over
standard DPO, while KDE smoothing achieves 0.68 vs 0.32 under heavy-tailed
contamination (112% relative gain). In sequential reinforcement learning
(CartPole, LunarLander), anchoring improves noisy-preference performance by
15-29%, confirming transfer from single-step to multi-step settings.
Experiments with 10-256 parameter models provide clear guidance: use pairwise
anchored Soft-DPO for clean or moderate noise, and KDE-based listwise ADPO for
extreme contamination.

</details>


### [62] [Benchmarking On-Device Machine Learning on Apple Silicon with MLX](https://arxiv.org/abs/2510.18921)
*Oluwaseun A. Ajayi,Ogundepo Odunayo*

Main category: cs.LG

TL;DR: 本文对MLX框架进行性能评估，聚焦变压器模型推理延迟，对比MLX与PyTorch实现，在苹果设备上测试多种模型，结果显示MLX在苹果生态内有潜力实现高效且易访问的设备端机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和机器学习广泛应用，需要能利用设备硬件的框架，MLX框架应运而生，本文旨在对其进行性能评估。

Method: 创建MLX - transformers框架，对比MLX和PyTorch中不同变压器架构实现，在两台苹果硅芯片MacBook设备上对多种变压器模型进行基准测试，与NVIDIA CUDA GPU对比。

Result: 研究突出了MLX在苹果生态系统中实现高效且更易访问的设备端机器学习应用的潜力。

Conclusion: MLX在苹果生态内有潜力实现高效且更易访问的设备端机器学习应用，未来可评估不同模态模型以更全面评估其能力。

Abstract: The recent widespread adoption of Large Language Models (LLMs) and machine
learning in general has sparked research interest in exploring the
possibilities of deploying these models on smaller devices such as laptops and
mobile phones. This creates a need for frameworks and approaches that are
capable of taking advantage of on-device hardware. The MLX framework was
created to address this need. It is a framework optimized for machine learning
(ML) computations on Apple silicon devices, facilitating easier research,
experimentation, and prototyping.
  This paper presents a performance evaluation of MLX, focusing on inference
latency of transformer models. We compare the performance of different
transformer architecture implementations in MLX with their Pytorch
counterparts. For this research we create a framework called MLX-transformers
which includes different transformer implementations in MLX and downloads the
model checkpoints in pytorch and converts it to the MLX format. By leveraging
the advanced architecture and capabilities of Apple Silicon, MLX-Transformers
enables seamless execution of transformer models directly sourced from Hugging
Face, eliminating the need for checkpoint conversion often required when
porting models between frameworks.
  Our study benchmarks different transformer models on two Apple Silicon
macbook devices against an NVIDIA CUDA GPU. Specifically, we compare the
inference latency performance of models with the same parameter sizes and
checkpoints. We evaluate the performance of BERT, RoBERTa, and XLM-RoBERTa
models, with the intention of extending future work to include models of
different modalities, thus providing a more comprehensive assessment of MLX's
capabilities. The results highlight MLX's potential in enabling efficient and
more accessible on-device ML applications within Apple's ecosystem.

</details>


### [63] [Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients](https://arxiv.org/abs/2510.18924)
*Omar El mansouri,Mohamed El Amine Seddik,Salem Lahlou*

Main category: cs.LG

TL;DR: 本文提出抗噪声的GRPO和Dr.GRPO框架，处理RLHF/RLVR中奖励噪声问题，理论和实验均有积极结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的强化学习对奖励噪声敏感，且噪声与基于组的策略优化方法的相互作用研究不足。

Method: 引入GRPO和Dr.GRPO框架，将奖励损坏建模为伯努利噪声，估计奖励翻转概率后进行噪声校正。

Result: 理论上基于组的方法可减轻个体层面噪声，校正策略增强鲁棒性；实验上在数学和代码任务中准确率有提升。

Conclusion: 本工作将监督学习的标签噪声校正与现代RLHF相结合，为现实噪声环境提供理论见解和实用算法。

Abstract: Reinforcement learning from human feedback (RLHF) or verifiable rewards
(RLVR), the standard paradigm for aligning LLMs or building recent SOTA
reasoning models, is highly sensitive to noise from inconsistent or erroneous
rewards. Yet, the interaction between such noise and widely used group-based
policy optimization methods remains underexplored. We introduce a noise-robust
Group Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO)
framework that explicitly models reward corruption as Bernoulli noise. Our
method applies noise correction after estimating reward flip probabilities to
debias the learning signal, yielding provably unbiased gradient estimates.
Theoretical analysis shows that group-based methods inherently mitigate
individual-level noise, and our correction strategy amplifies this robustness.
Empirically, we observe consistent improvements across math and code tasks when
applying our noise correction to standard reward model usage, with particular
gains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code
tasks under realistic reward model conditions. This work bridges label-noise
correction from supervised learning with modern RLHF, offering both theoretical
insights and a practical algorithm for noisy real-world deployment.

</details>


### [64] [Application of Reduced-Order Models for Temporal Multiscale Representations in the Prediction of Dynamical Systems](https://arxiv.org/abs/2510.18925)
*Elias Al Ghazal,Jad Mounayer,Beatriz Moya,Sebastian Rodriguez,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

TL;DR: 提出三种多尺度学习方法，有效捕捉复杂多尺度系统的粗粒度和细粒度动态，适用于实际应用和高维不完整观测系统。


<details>
  <summary>Details</summary>
Motivation: 复杂多尺度系统动力学建模和预测因固有非线性、对初始条件敏感以及传统机器学习方法局限性而面临挑战，需新方法解决。

Method: 一是将单位分解（PU）方法与神经网络结合，分解动力学并预测宏微观行为；二是应用奇异值分解（SVD）提取主导模式分离宏微观动态；三是采用稀疏高阶SVD从有限测量中重建多尺度动力学。

Result: 确保准确捕捉粗粒度和细粒度动态。

Conclusion: 所提框架对涉及复杂多尺度现象的实际应用有效，能适应高维不完整观测系统，并在各时间尺度提供近似和解释。

Abstract: Modeling and predicting the dynamics of complex multiscale systems remains a
significant challenge due to their inherent nonlinearities and sensitivity to
initial conditions, as well as limitations of traditional machine learning
methods that fail to capture high frequency behaviours. To overcome these
difficulties, we propose three approaches for multiscale learning. The first
leverages the Partition of Unity (PU) method, integrated with neural networks,
to decompose the dynamics into local components and directly predict both
macro- and micro-scale behaviors. The second applies the Singular Value
Decomposition (SVD) to extract dominant modes that explicitly separate macro-
and micro-scale dynamics. Since full access to the data matrix is rarely
available in practice, we further employ a Sparse High-Order SVD to reconstruct
multiscale dynamics from limited measurements. Together, these approaches
ensure that both coarse and fine dynamics are accurately captured, making the
framework effective for real-world applications involving complex, multi-scale
phenomena and adaptable to higher-dimensional systems with incomplete
observations, by providing an approximation and interpretation in all time
scales present in the phenomena under study.

</details>


### [65] [BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping](https://arxiv.org/abs/2510.18927)
*Zhiheng Xi,Xin Guo,Yang Nan,Enyu Zhou,Junrui Shen,Wenxiang Chen,Jiaqi Liu,Jixuan Huang,Zhihao Zhang,Honglin Guo,Xun Deng,Zhikai Lei,Miao Zheng,Guoteng Wang,Shuo Zhang,Peng Sun,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: 本文指出强化学习在离策略设置中应用的挑战，提出BAPO方法，在多场景训练效果好，模型在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在离策略设置中存在的策略熵急剧下降、优化不稳定甚至崩溃等问题。

Method: 通过理论和实证分析找出问题关键，提出BAlanced Policy Optimization with Adaptive Clipping (BAPO)方法，动态调整裁剪边界。

Result: 在多种离策略场景中实现快速、稳定和数据高效的训练，在AIME 2024和AIME 2025基准测试中，不同规模的BAPO模型表现优于开源和专有系统。

Conclusion: BAPO方法能有效解决离策略设置中强化学习的问题，实现稳定高效训练。

Abstract: Reinforcement learning (RL) has recently become the core paradigm for
aligning and strengthening large language models (LLMs). Yet, applying RL in
off-policy settings--where stale data from past policies are used for
training--improves sample efficiency, but remains challenging: policy entropy
declines sharply, optimization often becomes unstable and may even collapse.
Through theoretical and empirical analysis, we identify two key insights: (i)
an imbalance in optimization, where negative-advantage samples dominate the
policy gradient, suppressing useful behaviors and risking gradient explosions;
and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping
mechanism in PPO-like objectives systematically blocks entropy-increasing
updates, thereby driving the policy toward over-exploitation at the expense of
exploration. Building on these insights, we propose BAlanced Policy
Optimization with Adaptive Clipping (BAPO), a simple yet effective method that
dynamically adjusts clipping bounds to adaptively re-balance positive and
negative contributions, preserve entropy, and stabilize RL optimization. Across
diverse off-policy scenarios--including sample replay and partial rollout--BAPO
achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025
benchmarks, our 7B BAPO model surpasses open-source counterparts such as
SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art
results among models of the same scale but also outperforms leading proprietary
systems like o3-mini and Gemini-2.5-Flash-Thinking.

</details>


### [66] [Position: Many generalization measures for deep learning are fragile](https://arxiv.org/abs/2510.18934)
*Shuofeng Zhang,Ard Louis*

Main category: cs.LG

TL;DR: 许多深度神经网络的事后泛化度量是脆弱的，小的训练修改会大幅改变度量值，新度量开发者应审计其脆弱性。


<details>
  <summary>Details</summary>
Motivation: 探讨深度神经网络泛化度量在实际应用中的可靠性，指出虽泛化度量被认为能反映定性趋势，但获取紧密界限仍有挑战。

Method: 通过列举超参数微小变化（如学习率调整、SGD变体切换）对路径范数等泛化度量学习曲线斜率的影响，以及分析PAC - Bayes原点度量和基于函数的边缘似然PAC - Bayes边界的特性来论证。

Result: 发现路径、谱和Frobenius范数、平坦度代理和确定性PAC - Bayes替代等许多界限是脆弱的，不同泛化度量有不同的脆弱表现。

Conclusion: 新泛化度量开发者应明确审计其脆弱性。

Abstract: A wide variety of generalization measures have been applied to deep neural
networks (DNNs). Although obtaining tight bounds remains challenging, such
measures are often assumed to reproduce qualitative generalization trends. In
this position paper, we argue that many post-mortem generalization measures --
those computed on trained networks -- are \textbf{fragile}: small training
modifications that barely affect the underlying DNN can substantially change a
measure's value, trend, or scaling behavior. For example, minor hyperparameter
changes, such as learning rate adjustments or switching between SGD variants
can reverse the slope of a learning curve in widely used generalization
measures like the path norm. We also identify subtler forms of fragility. For
instance, the PAC-Bayes origin measure is regarded as one of the most reliable,
and is indeed less sensitive to hyperparameter tweaks than many other measures.
However, it completely fails to capture differences in data complexity across
learning curves. This data fragility contrasts with the function-based
marginal-likelihood PAC-Bayes bound, which does capture differences in
data-complexity, including scaling behavior, in learning curves, but which is
not a post-mortem measure. Beyond demonstrating that many bounds -- such as
path, spectral and Frobenius norms, flatness proxies, and deterministic
PAC-Bayes surrogates -- are fragile, this position paper also argues that
developers of new measures should explicitly audit them for fragility.

</details>


### [67] [NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2510.18940)
*Zhi Zhang,Yixian Shen,Congfeng Cao,Ekaterina Shutova*

Main category: cs.LG

TL;DR: 提出新的参数高效微调方法NeuroAda，在保持高内存效率的同时实现细粒度模型微调，在多任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法存在内存效率和表示能力的权衡问题，需要新方法解决。

Method: 先像选择性适应那样识别重要参数，再为这些参数引入旁路连接，微调时仅更新旁路连接，原模型参数冻结。

Result: 在23+个自然语言生成和理解任务上，NeuroAda用≤0.02%的可训练参数达到了最先进性能，减少了60%的CUDA内存使用。

Conclusion: NeuroAda能有效平衡内存效率和模型微调效果。

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into
two categories: addition-based and selective in-situ adaptation. The former,
such as LoRA, introduce additional modules to adapt the model to downstream
tasks, offering strong memory efficiency. However, their representational
capacity is often limited, making them less suitable for fine-grained
adaptation. In contrast, the latter directly fine-tunes a carefully chosen
subset of the original model parameters, allowing for more precise and
effective adaptation, but at the cost of significantly increased memory
consumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT
method that enables fine-grained model finetuning while maintaining high memory
efficiency. Our approach first identifies important parameters (i.e.,
connections within the network) as in selective adaptation, and then introduces
bypass connections for these selected parameters. During finetuning, only the
bypass connections are updated, leaving the original model parameters frozen.
Empirical results on 23+ tasks spanning both natural language generation and
understanding demonstrate that NeuroAda achieves state-of-the-art performance
with as little as $\leq \textbf{0.02}\%$ trainable parameters, while reducing
CUDA memory usage by up to 60%. We release our code here:
https://github.com/FightingFighting/NeuroAda.git.

</details>


### [68] [Towards Universal Solvers: Using PGD Attack in Active Learning to Increase Generalizability of Neural Operators as Knowledge Distillation from Numerical PDE Solvers](https://arxiv.org/abs/2510.18989)
*Yifei Sun*

Main category: cs.LG

TL;DR: 提出对抗性师生蒸馏框架，改进神经算子泛化性，实验显示能提升OOD鲁棒性并保留优点。


<details>
  <summary>Details</summary>
Motivation: 非线性PDE求解器内存成本高、运行慢，神经算子OOD泛化差。

Method: 提出对抗性师生蒸馏框架，用可微数值求解器监督神经算子，PGD式主动采样扩展训练集。

Result: 在Burgers和Navier - Stokes系统实验中，对抗蒸馏显著提升OOD鲁棒性。

Conclusion: 对抗蒸馏能提升神经算子OOD鲁棒性，同时保留低参数成本和快速推理优点。

Abstract: Nonlinear PDE solvers require fine space-time discretizations and local
linearizations, leading to high memory cost and slow runtimes. Neural operators
such as FNOs and DeepONets offer fast single-shot inference by learning
function-to-function mappings and truncating high-frequency components, but
they suffer from poor out-of-distribution (OOD) generalization, often failing
on inputs outside the training distribution. We propose an adversarial
teacher-student distillation framework in which a differentiable numerical
solver supervises a compact neural operator while a PGD-style active sampling
loop searches for worst-case inputs under smoothness and energy constraints to
expand the training set. Using differentiable spectral solvers enables
gradient-based adversarial search and stabilizes sample mining. Experiments on
Burgers and Navier-Stokes systems demonstrate that adversarial distillation
substantially improves OOD robustness while preserving the low parameter cost
and fast inference of neural operators.

</details>


### [69] [Actor-Free Continuous Control via Structurally Maximizable Q-Functions](https://arxiv.org/abs/2510.18828)
*Yigit Korkmaz,Urvi Bhuwania,Ayush Jain,Erdem Bıyık*

Main category: cs.LG

TL;DR: 提出用于连续控制的纯基于价值的框架，在标准模拟任务上表现良好，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 传统基于价值的算法多用于离散动作空间，连续动作空间中计算Q值不可行；Actor - Critic方法训练不稳定。

Method: 提出纯基于价值的框架，重新审视Q函数的结构最大化，引入关键架构和算法选择。

Result: 在标准模拟任务上性能和样本效率与现有最优基线相当，在动作空间受限环境中优于传统Actor - Critic方法。

Conclusion: 所提无Actor的Q学习方法能在连续控制中实现高效稳定学习。

Abstract: Value-based algorithms are a cornerstone of off-policy reinforcement learning
due to their simplicity and training stability. However, their use has
traditionally been restricted to discrete action spaces, as they rely on
estimating Q-values for individual state-action pairs. In continuous action
spaces, evaluating the Q-value over the entire action space becomes
computationally infeasible. To address this, actor-critic methods are typically
employed, where a critic is trained on off-policy data to estimate Q-values,
and an actor is trained to maximize the critic's output. Despite their
popularity, these methods often suffer from instability during training. In
this work, we propose a purely value-based framework for continuous control
that revisits structural maximization of Q-functions, introducing a set of key
architectural and algorithmic choices to enable efficient and stable learning.
We evaluate the proposed actor-free Q-learning approach on a range of standard
simulation tasks, demonstrating performance and sample efficiency on par with
state-of-the-art baselines, without the cost of learning a separate actor.
Particularly, in environments with constrained action spaces, where the value
functions are typically non-smooth, our method with structural maximization
outperforms traditional actor-critic methods with gradient-based maximization.
We have released our code at https://github.com/USC-Lira/Q3C.

</details>


### [70] [Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records](https://arxiv.org/abs/2510.19014)
*Saman Nessari,Ali Bozorgi-Amiri*

Main category: cs.LG

TL;DR: 当前医疗实践忽视患者个体差异，本文开发综合系统提供定制化临床建议，在结肠癌数据集测试效果好，推动个体化医疗。


<details>
  <summary>Details</summary>
Motivation: 当前医疗实践依赖标准化框架和经验方法，忽视患者个体差异，导致健康结果不佳，需要提供定制化临床建议。

Method: 开发综合系统，集成大语言模型、CTGAN、T - learner反事实模型和上下文老虎机方法，利用各模型处理数据、生成合成数据、预测治疗反应和优化治疗选择。

Result: LLMs处理非结构化医疗叙事准确率93.2%，CTGAN生成合成患者数据准确率55%，T - learner预测治疗反应准确率84.3%，KernelUCB方法在5000轮测试中平均奖励得分0.60 - 0.61，超其他参考方法。

Conclusion: 该综合系统克服在线学习冷启动限制，提高计算效率，推动了适应患者特征的个体化医疗发展。

Abstract: Current medical practice depends on standardized treatment frameworks and
empirical methodologies that neglect individual patient variations, leading to
suboptimal health outcomes. We develop a comprehensive system integrating Large
Language Models (LLMs), Conditional Tabular Generative Adversarial Networks
(CTGAN), T-learner counterfactual models, and contextual bandit approaches to
provide customized, data-informed clinical recommendations. The approach
utilizes LLMs to process unstructured medical narratives into structured
datasets (93.2% accuracy), uses CTGANs to produce realistic synthetic patient
data (55% accuracy via two-sample verification), deploys T-learners to forecast
patient-specific treatment responses (84.3% accuracy), and integrates
prior-informed contextual bandits to enhance online therapeutic selection by
effectively balancing exploration of new possibilities with exploitation of
existing knowledge. Testing on stage III colon cancer datasets revealed that
our KernelUCB approach obtained 0.60-0.61 average reward scores across 5,000
rounds, exceeding other reference methods. This comprehensive system overcomes
cold-start limitations in online learning environments, improves computational
effectiveness, and constitutes notable progress toward individualized medicine
adapted to specific patient characteristics.

</details>


### [71] [Category learning in deep neural networks: Information content and geometry of internal representations](https://arxiv.org/abs/2510.19021)
*Laurent Bonnasse-Gahot,Jean-Pierre Nadal*

Main category: cs.LG

TL;DR: 本文将基于神经科学数据的理论框架扩展到人工网络，表明最小化贝叶斯成本意味着最大化类别集与决策层前神经活动的互信息，类别学习会导致决策边界附近神经空间扩展，并通过数值示例说明学习后两种费舍尔信息矩阵匹配，最后关联信息瓶颈方法并给出贝叶斯成本的偏差 - 方差分解。


<details>
  <summary>Details</summary>
Motivation: 将基于神经科学数据的理论框架扩展到人工网络，研究人工网络中类别学习导致的类别知觉现象的理论原理。

Method: 通过理论推导，分析最小化贝叶斯成本与最大化互信息的关系，考虑结构化数据，基于费舍尔信息矩阵构建神经表示；使用玩具模型和MNIST数据集进行数值验证；关联信息瓶颈方法并进行贝叶斯成本的偏差 - 方差分解。

Result: 发现最小化贝叶斯成本意味着最大化互信息；类别学习使神经费舍尔信息遵循类别特定的费舍尔信息，导致决策边界附近神经空间扩展；费舍尔信息矩阵的最大值一般不在而是靠近类别边界；学习后两种费舍尔信息矩阵匹配并与类别边界基本对齐。

Conclusion: 成功将理论框架扩展到人工网络，解释了人工网络中类别学习导致的类别知觉现象，给出费舍尔信息矩阵的特性，并关联信息瓶颈方法和贝叶斯成本的偏差 - 方差分解。

Abstract: In animals, category learning enhances discrimination between stimuli close
to the category boundary. This phenomenon, called categorical perception, was
also empirically observed in artificial neural networks trained on
classification tasks. In previous modeling works based on neuroscience data, we
show that this expansion/compression is a necessary outcome of efficient
learning. Here we extend our theoretical framework to artificial networks. We
show that minimizing the Bayes cost (mean of the cross-entropy loss) implies
maximizing the mutual information between the set of categories and the neural
activities prior to the decision layer. Considering structured data with an
underlying feature space of small dimension, we show that maximizing the mutual
information implies (i) finding an appropriate projection space, and, (ii)
building a neural representation with the appropriate metric. The latter is
based on a Fisher information matrix measuring the sensitivity of the neural
activity to changes in the projection space. Optimal learning makes this neural
Fisher information follow a category-specific Fisher information, measuring the
sensitivity of the category membership. Category learning thus induces an
expansion of neural space near decision boundaries. We characterize the
properties of the categorical Fisher information, showing that its eigenvectors
give the most discriminant directions at each point of the projection space. We
find that, unexpectedly, its maxima are in general not exactly at, but near,
the class boundaries. Considering toy models and the MNIST dataset, we
numerically illustrate how after learning the two Fisher information matrices
match, and essentially align with the category boundaries. Finally, we relate
our approach to the Information Bottleneck one, and we exhibit a bias-variance
decomposition of the Bayes cost, of interest on its own.

</details>


### [72] [Empowering Decision Trees via Shape Function Branching](https://arxiv.org/abs/2510.19040)
*Nakul Upadhya,Eldan Cohen*

Main category: cs.LG

TL;DR: 提出Shape Generalized Tree (SGT) 及其学习算法，扩展框架，实验显示SGT性能优且模型小


<details>
  <summary>Details</summary>
Motivation: 传统决策树依赖简单轴对齐线性分割，需深层复杂结构捕捉非线性特征，影响可解释性，需改进

Method: 提出SGT，每个内部节点对单个特征应用可学习轴对齐形状函数；提出ShapeCART学习SGT；扩展到双变量形状函数和多路树，并给出对应学习算法

Result: 实验表明SGT在各种数据集上比传统轴对齐线性树性能更优，且模型尺寸更小

Conclusion: SGT可有效解决传统决策树问题，实现丰富非线性分割，具有良好可解释性和性能

Abstract: Decision trees are prized for their interpretability and strong performance
on tabular data. Yet, their reliance on simple axis-aligned linear splits often
forces deep, complex structures to capture non-linear feature effects,
undermining human comprehension of the constructed tree. To address this
limitation, we propose a novel generalization of a decision tree, the Shape
Generalized Tree (SGT), in which each internal node applies a learnable
axis-aligned shape function to a single feature, enabling rich, non-linear
partitioning in one split. As users can easily visualize each node's shape
function, SGTs are inherently interpretable and provide intuitive, visual
explanations of the model's decision mechanisms. To learn SGTs from data, we
propose ShapeCART, an efficient induction algorithm for SGTs. We further extend
the SGT framework to bivariate shape functions (S$^2$GT) and multi-way trees
(SGT$_K$), and present Shape$^2$CART and ShapeCART$_K$, extensions to ShapeCART
for learning S$^2$GTs and SGT$_K$s, respectively. Experiments on various
datasets show that SGTs achieve superior performance with reduced model size
compared to traditional axis-aligned linear trees.

</details>


### [73] [POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2510.19056)
*Kuai Yu,Xiaoyu Wu,Peishen Yan,Qingqian Yang,Linshan Jiang,Hao Wang,Yang Hua,Tao Song,Haibing Guan*

Main category: cs.LG

TL;DR: 本文提出POLAR方法，采用强化学习解决分层后门攻击中的关键层选择问题，实验表明其在对抗六种SOTA防御时优于最新攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的BC层选择方法未考虑层间关系，效果不佳且易被检测，需要更好的方法解决BC层选择问题。

Method: 提出POLAR，采用伯努利采样的轻量级强化学习，基于后门成功率改进通过策略梯度更新优化层选择，引入正则化约束确保隐蔽性。

Result: POLAR在对抗六种SOTA防御时比最新攻击方法性能提升达40%。

Conclusion: POLAR能有效解决分层后门攻击中的BC层选择问题，在对抗防御方面表现出色。

Abstract: Federated Learning (FL) enables decentralized model training across multiple
clients without exposing local data, but its distributed feature makes it
vulnerable to backdoor attacks. Despite early FL backdoor attacks modifying
entire models, recent studies have explored the concept of backdoor-critical
(BC) layers, which poison the chosen influential layers to maintain
stealthiness while achieving high effectiveness. However, existing BC layers
approaches rely on rule-based selection without consideration of the
interrelations between layers, making them ineffective and prone to detection
by advanced defenses. In this paper, we propose POLAR (POlicy-based LAyerwise
Reinforcement learning), the first pipeline to creatively adopt RL to solve the
BC layer selection problem in layer-wise backdoor attack. Different from other
commonly used RL paradigm, POLAR is lightweight with Bernoulli sampling. POLAR
dynamically learns an attack strategy, optimizing layer selection using policy
gradient updates based on backdoor success rate (BSR) improvements. To ensure
stealthiness, we introduce a regularization constraint that limits the number
of modified layers by penalizing large attack footprints. Extensive experiments
demonstrate that POLAR outperforms the latest attack methods by up to 40%
against six state-of-the-art (SOTA) defenses.

</details>


### [74] [Weight Decay may matter more than muP for Learning Rate Transfer in Practice](https://arxiv.org/abs/2510.19093)
*Atli Kosson,Jeremy Welborn,Yang Liu,Martin Jaggi,Xi Chen*

Main category: cs.LG

TL;DR: 研究发现Maximal Update Parameterization (muP)在学习率转移方面的假设仅在训练初期成立，训练后期是权重衰减稳定更新动态，muP可被修改的热身计划替代。


<details>
  <summary>Details</summary>
Motivation: 解决在超参数调优成本过高的大规模场景下，实现从小神经网络到大型神经网络的有效学习率转移。

Method: 进行大规模实证研究。

Result: muP假设仅在训练初期成立，训练后期权重衰减稳定内部表示的更新动态，促进学习率转移。

Conclusion: 挑战了关于学习率转移的主流观点，解释了如muP需要独立权重衰减变体才能成功转移等实证现象。

Abstract: Transferring the optimal learning rate from small to large neural networks
can enable efficient training at scales where hyperparameter tuning is
otherwise prohibitively expensive. To this end, the Maximal Update
Parameterization (muP) proposes a learning rate scaling designed to keep the
update dynamics of internal representations stable across different model
widths. However, the scaling rules of muP rely on strong assumptions,
particularly about the geometric alignment of a layer's inputs with both its
weights and gradient updates. In this large-scale empirical investigation, we
show that these assumptions hold only briefly at the start of training in the
practical setups where learning rate transfer is most valuable, such as LLM
training. For the remainder of training it is weight decay rather than muP that
correctly stabilizes the update dynamics of internal representations across
widths, facilitating learning rate transfer. This suggests muP's scaling
primarily acts as a form of implicit learning rate warmup, allowing us to
largely replace it with modified warmup schedules. Together these findings
fundamentally challenge prevailing beliefs about learning rate transfer and can
explain empirical practice such as why muP requires the independent weight
decay variant for successful transfer.

</details>


### [75] [Scalable LinUCB: Low-Rank Design Matrix Updates for Recommenders with Large Action Spaces](https://arxiv.org/abs/2510.19349)
*Evgenia Shustova,Marina Sheshukova,Sergey Samsonov,Evgeny Frolov*

Main category: cs.LG

TL;DR: 本文针对线性上下文老虎机算法LinUCB训练、推理和内存成本高的问题，提出Scalable LinUCB算法，通过动态低秩参数化和投影拆分积分器实现高效运算，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有线性上下文老虎机算法LinUCB的训练、推理和内存成本随特征维度和动作空间大小增长，关键瓶颈在于设计矩阵的更新、求逆和存储。

Method: 引入Scalable LinUCB算法，通过对逆Cholesky风格因子进行动态低秩参数化，推导数值稳定的秩1和批量更新方法，采用投影拆分积分器进行动态低秩近似。

Result: 实现平均每步更新成本为$O(dr)$，内存为$O(dr)$，推理复杂度为每个动作评估$O(dr)$。

Conclusion: 在推荐系统数据集上的实验证明了算法的有效性。

Abstract: Linear contextual bandits, especially LinUCB, are widely used in recommender
systems. However, its training, inference, and memory costs grow with feature
dimensionality and the size of the action space. The key bottleneck becomes the
need to update, invert and store a design matrix that absorbs contextual
information from interaction history. In this paper, we introduce Scalable
LinUCB, the algorithm that enables fast and memory efficient operations with
the inverse regularized design matrix. We achieve this through a dynamical
low-rank parametrization of its inverse Cholesky-style factors. We derive
numerically stable rank-1 and batched updates that maintain the inverse without
directly forming the entire matrix. To control memory growth, we employ a
projector-splitting integrator for dynamical low-rank approximation, yielding
average per-step update cost $O(dr)$ and memory $O(dr)$ for approximation rank
$r$. Inference complexity of the suggested algorithm is $O(dr)$ per action
evaluation. Experiments on recommender system datasets demonstrate the
effectiveness of our algorithm.

</details>


### [76] [What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning](https://arxiv.org/abs/2510.19099)
*Yaning Jia,Chunhui Zhang,Xingjian Diao,Xiangchi Yuan,Zhongyu Ouyang,soroush vosoughi*

Main category: cs.LG

TL;DR: 本文通过统一评估框架研究课程学习在大语言模型推理中的效果，发现无通用课程策略，其效果取决于模型能力和任务复杂度等。


<details>
  <summary>Details</summary>
Motivation: 此前课程学习在大语言模型推理应用中使用不同难度指标和训练设置，存在何时有效、正反顺序哪个更好等问题待解决。

Method: 构建统一离线评估框架，将课程难度分解为五个维度，在数学推理基准上对多个模型进行控制后训练实验。

Result: 无课程策略能普遍占优，正反向课程学习效果取决于模型能力和任务复杂度；同一指标不同难度样本收益因任务而异；任务对齐和内部状态课程作用不同。

Conclusion: 不存在通用课程策略，为不同模型和任务提供可行指导，优先处理决策不确定样本可提升学习效果。

Abstract: Curriculum learning (CL) - ordering training data from easy to hard - has
become a popular strategy for improving reasoning in large language models
(LLMs). Yet prior work employs disparate difficulty metrics and training
setups, leaving open fundamental questions: When does curriculum help? Which
direction - forward or reverse - is better? And does the answer depend on what
we measure? We address these questions through a unified offline evaluation
framework that decomposes curriculum difficulty into five complementary
dimensions: Problem Difficulty, Model Surprisal, Confidence Margin, Predictive
Uncertainty, and Decision Variability. Through controlled post-training
experiments on mathematical reasoning benchmarks with Llama3.1-8B, Mistral-7B,
and Gemma3-4B, we find that (i) no curriculum strategy dominates universally -
the relative effectiveness of forward versus reverse CL depends jointly on
model capability and task complexity; (ii) even within a single metric, samples
at different difficulty levels produce distinct gains depending on task
demands; and (iii) task-aligned curricula focus on shaping the model's final
representations and generalization, whereas inner-state curricula modulate
internal states such as confidence and uncertainty. Our findings challenge the
notion of a universal curriculum strategy and offer actionable guidance across
model and task regimes, with some metrics indicating that prioritizing
decision-uncertain samples can further enhance learning outcomes.

</details>


### [77] [MetaCluster: Enabling Deep Compression of Kolmogorov-Arnold Network](https://arxiv.org/abs/2510.19105)
*Matthew Raffel,Adwaith Renjith,Lizhong Chen*

Main category: cs.LG

TL;DR: 提出MetaCluster框架使Kolmogorov - Arnold Networks (KANs)高度可压缩且不损失精度，在多数据集上实现参数存储大幅减少。


<details>
  <summary>Details</summary>
Motivation: KANs虽提升表达能力和准确性，但参数和内存呈乘法增长，需要一种方法使其可压缩。

Method: 使用轻量级元学习器与KAN联合训练，将低维嵌入映射到系数向量，使其位于适合聚类的低维流形，在系数空间运行K - means并用共享质心替换边缘向量，丢弃元学习器后微调质心码本。

Result: 在MNIST、CIFAR - 10和CIFAR - 100数据集上，在标准KANs和使用多个基函数的ConvKANs中，MetaCluster实现了高达80倍的参数存储减少，且精度无损失。

Conclusion: MetaCluster框架有效解决了KANs参数和内存问题，能在不损失精度的情况下实现高度压缩。

Abstract: Kolmogorov-Arnold Networks (KANs) replace scalar weights with per-edge
vectors of basis coefficients, thereby boosting expressivity and accuracy but
at the same time resulting in a multiplicative increase in parameters and
memory. We propose MetaCluster, a framework that makes KANs highly compressible
without sacrificing accuracy. Specifically, a lightweight meta-learner, trained
jointly with the KAN, is used to map low-dimensional embedding to coefficient
vectors, shaping them to lie on a low-dimensional manifold that is amenable to
clustering. We then run K-means in coefficient space and replace per-edge
vectors with shared centroids. Afterwards, the meta-learner can be discarded,
and a brief fine-tuning of the centroid codebook recovers any residual accuracy
loss. The resulting model stores only a small codebook and per-edge indices,
exploiting the vector nature of KAN parameters to amortize storage across
multiple coefficients. On MNIST, CIFAR-10, and CIFAR-100, across standard KANs
and ConvKANs using multiple basis functions, MetaCluster achieves a reduction
of up to 80$\times$ in parameter storage, with no loss in accuracy. Code will
be released upon publication.

</details>


### [78] [Policy Learning with Abstention](https://arxiv.org/abs/2510.19672)
*Ayush Sawarni,Jikai Jin,Justin Whitehouse,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: 研究带弃权的策略学习，提出两阶段学习器，建立后悔保证并展示弃权在策略学习其他核心问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 多数策略学习算法在预测不确定时仍强制决策，在高风险场景有风险，因此研究带弃权的策略学习。

Method: 提出两阶段学习器，先确定一组近最优策略，再从其分歧中构建弃权规则；对已知倾向和未知倾向情况分别处理。

Result: 在已知倾向时建立O(1/n)型快速后悔保证，通过双稳健目标将保证扩展到未知倾向情况。

Conclusion: 弃权是策略学习中的通用工具，可应用于其他核心问题，如改善保证、与分布鲁棒策略学习相关、支持安全策略改进。

Abstract: Policy learning algorithms are widely used in areas such as personalized
medicine and advertising to develop individualized treatment regimes. However,
most methods force a decision even when predictions are uncertain, which is
risky in high-stakes settings. We study policy learning with abstention, where
a policy may defer to a safe default or an expert. When a policy abstains, it
receives a small additive reward on top of the value of a random guess. We
propose a two-stage learner that first identifies a set of near-optimal
policies and then constructs an abstention rule from their disagreements. We
establish fast O(1/n)-type regret guarantees when propensities are known, and
extend these guarantees to the unknown-propensity case via a doubly robust (DR)
objective. We further show that abstention is a versatile tool with direct
applications to other core problems in policy learning: it yields improved
guarantees under margin conditions without the common realizability assumption,
connects to distributionally robust policy learning by hedging against small
data shifts, and supports safe policy improvement by ensuring improvement over
a baseline policy with high probability.

</details>


### [79] [Learning Peer Influence Probabilities with Linear Contextual Bandits](https://arxiv.org/abs/2510.19119)
*Ahmed Sayeed Faruk,Mohammad Shahverdikondori,Elena Zheleva*

Main category: cs.LG

TL;DR: 研究在上下文线性老虎机框架下学习同伴影响概率，提出不确定性引导探索算法，实验显示其优势。


<details>
  <summary>Details</summary>
Motivation: 准确估计同伴影响概率对理解信息传播和改进病毒式营销策略至关重要，但从数据学习这些概率具有挑战性。

Method: 在上下文线性老虎机框架下研究，提出不确定性引导探索算法。

Result: 实验表明该方法优于静态方法和忽略权衡的上下文老虎机。

Conclusion: 可以在后悔最小化和估计误差之间实现权衡，通过调整参数可获得权衡内的任意速率对。

Abstract: In networked environments, users frequently share recommendations about
content, products, services, and courses of action with others. The extent to
which such recommendations are successful and adopted is highly contextual,
dependent on the characteristics of the sender, recipient, their relationship,
the recommended item, and the medium, which makes peer influence probabilities
highly heterogeneous. Accurate estimation of these probabilities is key to
understanding information diffusion processes and to improving the
effectiveness of viral marketing strategies. However, learning these
probabilities from data is challenging; static data may capture correlations
between peer recommendations and peer actions but fails to reveal influence
relationships. Online learning algorithms can learn these probabilities from
interventions but either waste resources by learning from random exploration or
optimize for rewards, thus favoring exploration of the space with higher
influence probabilities. In this work, we study learning peer influence
probabilities under a contextual linear bandit framework. We show that a
fundamental trade-off can arise between regret minimization and estimation
error, characterize all achievable rate pairs, and propose an
uncertainty-guided exploration algorithm that, by tuning a parameter, attains
any pair within this trade-off. Our experiments on semi-synthetic network
datasets show the advantages of our method over static methods and contextual
bandits that ignore this trade-off.

</details>


### [80] [Statistical Inference for Linear Functionals of Online Least-squares SGD when $t \gtrsim d^{1+δ}$](https://arxiv.org/abs/2510.19734)
*Bhavya Agrawalla,Krishnakumar Balasubramanian,Promit Ghosal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Stochastic Gradient Descent (SGD) has become a cornerstone method in modern
data science. However, deploying SGD in high-stakes applications necessitates
rigorous quantification of its inherent uncertainty. In this work, we establish
\emph{non-asymptotic Berry--Esseen bounds} for linear functionals of online
least-squares SGD, thereby providing a Gaussian Central Limit Theorem (CLT) in
a \emph{growing-dimensional regime}. Existing approaches to high-dimensional
inference for projection parameters, such as~\cite{chang2023inference}, rely on
inverting empirical covariance matrices and require at least $t \gtrsim
d^{3/2}$ iterations to achieve finite-sample Berry--Esseen guarantees,
rendering them computationally expensive and restrictive in the allowable
dimensional scaling. In contrast, we show that a CLT holds for SGD iterates
when the number of iterations grows as $t \gtrsim d^{1+\delta}$ for any $\delta
> 0$, significantly extending the dimensional regime permitted by prior works
while improving computational efficiency. The proposed online SGD-based
procedure operates in $\mathcal{O}(td)$ time and requires only $\mathcal{O}(d)$
memory, in contrast to the $\mathcal{O}(td^2 + d^3)$ runtime of
covariance-inversion methods. To render the theory practically applicable, we
further develop an \emph{online variance estimator} for the asymptotic variance
appearing in the CLT and establish \emph{high-probability deviation bounds} for
this estimator. Collectively, these results yield the first fully online and
data-driven framework for constructing confidence intervals for SGD iterates in
the near-optimal scaling regime $t \gtrsim d^{1+\delta}$.

</details>


### [81] [Steering Autoregressive Music Generation with Recursive Feature Machines](https://arxiv.org/abs/2510.19127)
*Daniel Zhao,Daniel Beaglehole,Taylor Berg-Kirkpatrick,Julian McAuley,Zachary Novack*

Main category: cs.LG

TL;DR: 提出MusicRFM框架，利用递归特征机实现对预训练音乐模型的细粒度、可解释控制，平衡控制与生成质量并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有可控音乐生成方法常需模型再训练或引入可听瑕疵，有改进需求。

Method: 将递归特征机（RFMs）应用于预训练音乐模型，通过分析模型内部梯度找到对应音乐属性的“概念方向”，训练轻量级RFM探针并在推理时注入模型以实时引导生成。还提出高级控制机制。

Result: 能将生成目标音符的准确率从0.23提升到0.82，文本提示遵循度与未引导基线相差约0.02。

Conclusion: MusicRFM框架能有效控制音乐生成，对提示保真度影响小，开源代码利于音乐领域对RFMs的进一步探索。

Abstract: Controllable music generation remains a significant challenge, with existing
methods often requiring model retraining or introducing audible artifacts. We
introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs)
to enable fine-grained, interpretable control over frozen, pre-trained music
models by directly steering their internal activations. RFMs analyze a model's
internal gradients to produce interpretable "concept directions", or specific
axes in the activation space that correspond to musical attributes like notes
or chords. We first train lightweight RFM probes to discover these directions
within MusicGen's hidden states; then, during inference, we inject them back
into the model to guide the generation process in real-time without per-step
optimization. We present advanced mechanisms for this control, including
dynamic, time-varying schedules and methods for the simultaneous enforcement of
multiple musical properties. Our method successfully navigates the trade-off
between control and generation quality: we can increase the accuracy of
generating a target musical note from 0.23 to 0.82, while text prompt adherence
remains within approximately 0.02 of the unsteered baseline, demonstrating
effective control with minimal impact on prompt fidelity. We release code to
encourage further exploration on RFMs in the music domain.

</details>


### [82] [InvarGC: Invariant Granger Causality for Heterogeneous Interventional Time Series under Latent Confounding](https://arxiv.org/abs/2510.19138)
*Ziyi Zhang,Shaogang Ren,Xiaoning Qian,Nick Duffield*

Main category: cs.LG

TL;DR: 提出InvarGC方法解决传统格兰杰因果检验在非线性因果关系检测及实际数据应用中的问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统线性格兰杰因果检验难检测非线性因果关系，现有非线性方法依赖因果充分性和已知干预目标假设，实际数据存在潜在混杂因素和环境异质性问题。

Method: 提出Invariant Granger Causality (InvarGC)方法，利用跨环境异质性减轻潜在混杂影响，区分干预和非干预环境，恢复不变因果关系，并建立可识别性。

Result: 在合成和真实数据集上的大量实验表明，该方法与现有先进方法相比具有有竞争力的性能。

Conclusion: InvarGC方法能有效解决传统方法在实际应用中的问题，具有良好性能。

Abstract: Granger causality is widely used for causal structure discovery in complex
systems from multivariate time series data. Traditional Granger causality tests
based on linear models often fail to detect even mild non-linear causal
relationships. Therefore, numerous recent studies have investigated non-linear
Granger causality methods, achieving improved performance. However, these
methods often rely on two key assumptions: causal sufficiency and known
interventional targets. Causal sufficiency assumes the absence of latent
confounders, yet their presence can introduce spurious correlations. Moreover,
real-world time series data usually come from heterogeneous environments,
without prior knowledge of interventions. Therefore, in practice, it is
difficult to distinguish intervened environments from non-intervened ones, and
even harder to identify which variables or timesteps are affected. To address
these challenges, we propose Invariant Granger Causality (InvarGC), which
leverages cross-environment heterogeneity to mitigate the effects of latent
confounding and to distinguish intervened from non-intervened environments with
edge-level granularity, thereby recovering invariant causal relations. In
addition, we establish the identifiability under these conditions. Extensive
experiments on both synthetic and real-world datasets demonstrate the
competitive performance of our approach compared to state-of-the-art methods.

</details>


### [83] [Subliminal Corruption: Mechanisms, Thresholds, and Interpretability](https://arxiv.org/abs/2510.19152)
*Reya Vir,Sarvesh Bhatnagar*

Main category: cs.LG

TL;DR: 研究机器学习模型在合成数据微调时的潜意识腐败现象，揭示关键发现并指出AI系统漏洞及新安全协议需求。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在合成数据上微调时，存在细微不一致性在互联AI系统中传播的风险，且对潜意识腐败动态缺乏定量理解。

Method: 使用GPT - 2的师生设置对潜意识腐败的缩放定律、阈值和机制进行系统研究。

Result: 潜意识腐败导致行为交叉，模型整体对齐度下降；在中毒数据临界阈值处对齐急剧失败；腐败机制难以检测。

Conclusion: 依赖合成数据的AI系统存在关键漏洞，需要考虑潜在威胁的新安全协议。

Abstract: As machine learning models are increasingly fine-tuned on synthetic data,
there is a critical risk of subtle misalignments spreading through
interconnected AI systems. This paper investigates subliminal corruption, which
we define as undesirable traits are transmitted through semantically neutral
data, bypassing standard safety checks. While this phenomenon has been
identified, a quantitative understanding of its dynamics is missing. To address
this gap, we present a systematic study of the scaling laws, thresholds, and
mechanisms of subliminal corruption using a teacher-student setup with GPT-2.
Our experiments reveal three key findings: (1) subliminal corruption causes
behavioral crossover, degrading the model's overall alignment, not just the
targeted trait; (2) alignment fails in a sharp phase transition at a critical
threshold of poisoned data, rather than degrading gradually; and (3)
interpretability analysis shows the corruption mechanism mimics the model's
natural fine-tuning process, making it difficult to detect. These results
demonstrate a critical vulnerability in AI systems that rely on synthetic data
and highlight the need for new safety protocols that can account for latent
threats.

</details>


### [84] [Feature Space Adaptation for Robust Model Fine-Tuning](https://arxiv.org/abs/2510.19155)
*Peng Wang,Minghao Gu,Qiang Huang*

Main category: cs.LG

TL;DR: 为缓解灾难性遗忘问题，提出在特征空间微调预训练模型的方法LoRFA和VeFA，实验显示其微调效果相当且鲁棒性更强。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法在权重空间操作，易使模型过度适应下游数据，为缓解覆盖预训练知识的风险并增强鲁棒性，提出在特征空间微调。

Method: 提出LoRFA和VeFA两种在特征空间微调预训练模型的方法，通过轻量级特征级变换补偿下游潜在变量的影响。

Result: 在图像分类、NLU和NLG任务上评估，特征空间适应实现了相当的微调结果且鲁棒性更强。

Conclusion: 在特征空间微调预训练模型能有效缓解灾难性遗忘问题，提高模型在分布偏移下的泛化能力。

Abstract: Catastrophic forgetting is a common issue in model fine-tuning, especially
when the downstream domain contains limited labeled data or differs greatly
from the pre-training distribution. Existing parameter-efficient fine-tuning
methods operate in the weight space by modifying or augmenting the pre-trained
model's parameters, which can yield models overly specialized to the available
downstream data. To mitigate the risk of overwriting pre-trained knowledge and
enhance robustness, we propose to fine-tune the pre-trained model in the
feature space. Two new fine-tuning methods are proposed: LoRFA (Low-Rank
Feature Adaptation) and VeFA (Vector-Based Feature Adaptation). Feature space
adaptation is inspired by the idea of effect equivalence modeling (EEM) of
downstream lurking variables causing distribution shifts, which posits that
unobserved factors can be represented as the total equivalent amount on
observed features. By compensating for the effects of downstream lurking
variables via a lightweight feature-level transformation, the pre-trained
representations can be preserved, which improves model generalization under
distribution shift. We evaluate LoRFA and VeFA versus LoRA on image
classification, NLU, and NLG, covering both standard fine-tuning metrics and
robustness. Feature space adaptation achieves comparable fine-tuning results
and consistently stronger robustness.

</details>


### [85] [Instance-Dependent Regret Bounds for Nonstochastic Linear Partial Monitoring](https://arxiv.org/abs/2510.19158)
*Federico Di Gennaro,Khaled Eldowa,Nicolò Cesa-Bianchi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In contrast to the classic formulation of partial monitoring, linear partial
monitoring can model infinite outcome spaces, while imposing a linear structure
on both the losses and the observations. This setting can be viewed as a
generalization of linear bandits where loss and feedback are decoupled in a
flexible manner. In this work, we address a nonstochastic (adversarial),
finite-actions version of the problem through a simple instance of the
exploration-by-optimization method that is amenable to efficient
implementation. We derive regret bounds that depend on the game structure in a
more transparent manner than previous theoretical guarantees for this paradigm.
Our bounds feature instance-specific quantities that reflect the degree of
alignment between observations and losses, and resemble known guarantees in the
stochastic setting. Notably, they achieve the standard $\sqrt{T}$ rate in easy
(locally observable) games and $T^{2/3}$ in hard (globally observable) games,
where $T$ is the time horizon. We instantiate these bounds in a selection of
old and new partial information settings subsumed by this model, and illustrate
that the achieved dependence on the game structure can be tight in interesting
cases.

</details>


### [86] [Preliminary Use of Vision Language Model Driven Extraction of Mouse Behavior Towards Understanding Fear Expression](https://arxiv.org/abs/2510.19160)
*Paimon Goulart,Jordan Steinhauser,Kylene Shuler,Edward Korzus,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 本文构建视觉语言模型对小鼠行为分类，用开源模型结合多种方法提升性能，无需微调有高准确率，助力跨学科研究。


<details>
  <summary>Details</summary>
Motivation: 整合多样数据对多学科科学探索至关重要，需构建模型对小鼠行为分类并生成有价值数据集。

Method: 使用开源Qwen2.5 - VL模型，通过提示、带标注示例的上下文学习和帧级预处理提升性能。

Result: 各方法都有助于提升分类效果，结合后在所有行为上有强F1分数，包括罕见行为，且无需模型微调。

Conclusion: 该模型能支持跨学科研究者将多样行为特征整合到综合数据集中，解决复杂研究问题。

Abstract: Integration of diverse data will be a pivotal step towards improving
scientific explorations in many disciplines. This work establishes a
vision-language model (VLM) that encodes videos with text input in order to
classify various behaviors of a mouse existing in and engaging with their
environment. Importantly, this model produces a behavioral vector over time for
each subject and for each session the subject undergoes. The output is a
valuable dataset that few programs are able to produce with as high accuracy
and with minimal user input. Specifically, we use the open-source Qwen2.5-VL
model and enhance its performance through prompts, in-context learning (ICL)
with labeled examples, and frame-level preprocessing. We found that each of
these methods contributes to improved classification, and that combining them
results in strong F1 scores across all behaviors, including rare classes like
freezing and fleeing, without any model fine-tuning. Overall, this model will
support interdisciplinary researchers studying mouse behavior by enabling them
to integrate diverse behavioral features, measured across multiple time points
and environments, into a comprehensive dataset that can address complex
research questions.

</details>


### [87] [Natural Gradient VI: Guarantees for Non-Conjugate Models](https://arxiv.org/abs/2510.19163)
*Fangyuan Sun,Ilyas Fatkhullin,Niao He*

Main category: cs.LG

TL;DR: 本文聚焦平均场参数化，从三方面推进对随机自然梯度变分推理（NGVI）的理论理解，包括推导变分损失满足相对平滑性的条件、提出改进算法并证明收敛性、揭示隐藏凸性并建立快速全局收敛性。


<details>
  <summary>Details</summary>
Motivation: NGVI在概率模型后验分布近似中有广泛应用，但在非共轭似然情况下理论基础有限，现有收敛保证结果不适用于非共轭场景。

Method: 先推导变分损失满足相对平滑性的充分条件，然后提出结合非欧投影的改进NGVI算法，最后在额外结构假设下揭示变分损失的隐藏凸性。

Result: 提出的改进算法能全局非渐近收敛到驻点，在额外假设下NGVI能快速全局收敛到全局最优。

Conclusion: 研究为NGVI在复杂推理场景中的几何和收敛行为提供了新见解。

Abstract: Stochastic Natural Gradient Variational Inference (NGVI) is a widely used
method for approximating posterior distribution in probabilistic models.
Despite its empirical success and foundational role in variational inference,
its theoretical underpinnings remain limited, particularly in the case of
non-conjugate likelihoods. While NGVI has been shown to be a special instance
of Stochastic Mirror Descent, and recent work has provided convergence
guarantees using relative smoothness and strong convexity for conjugate models,
these results do not extend to the non-conjugate setting, where the variational
loss becomes non-convex and harder to analyze. In this work, we focus on
mean-field parameterization and advance the theoretical understanding of NGVI
in three key directions. First, we derive sufficient conditions under which the
variational loss satisfies relative smoothness with respect to a suitable
mirror map. Second, leveraging this structure, we propose a modified NGVI
algorithm incorporating non-Euclidean projections and prove its global
non-asymptotic convergence to a stationary point. Finally, under additional
structural assumptions about the likelihood, we uncover hidden convexity
properties of the variational loss and establish fast global convergence of
NGVI to a global optimum. These results provide new insights into the geometry
and convergence behavior of NGVI in challenging inference settings.

</details>


### [88] [Imbalanced Gradients in RL Post-Training of Multi-Task LLMs](https://arxiv.org/abs/2510.19178)
*Runzhe Wu,Ankur Samanta,Ayush Jain,Scott Fujimoto,Jeongyeol Kwon,Ben Kretzu,Youliang Yu,Kaveh Hassani,Boris Vidolov,Yonathan Efroni*

Main category: cs.LG

TL;DR: 研究发现大语言模型多任务强化学习后训练中存在梯度不平衡问题，呼吁进行梯度层面修正。


<details>
  <summary>Details</summary>
Motivation: 指出大语言模型多任务后训练中混合数据集联合优化的方法存在假设缺陷，探究RL后训练中该假设失效的情况。

Method: 对大语言模型RL后训练进行分析，对比不同任务的梯度大小和学习增益。

Result: 发现某些任务产生更大梯度，大梯度任务学习增益不一定大，梯度不平衡无法用典型训练统计解释。

Conclusion: 提醒避免简单的数据集混合，需对大语言模型进行有原则的梯度层面修正。

Abstract: Multi-task post-training of large language models (LLMs) is typically
performed by mixing datasets from different tasks and optimizing them jointly.
This approach implicitly assumes that all tasks contribute gradients of similar
magnitudes; when this assumption fails, optimization becomes biased toward
large-gradient tasks. In this paper, however, we show that this assumption
fails in RL post-training: certain tasks produce significantly larger
gradients, thus biasing updates toward those tasks. Such gradient imbalance
would be justified only if larger gradients implied larger learning gains on
the tasks (i.e., larger performance improvements) -- but we find this is not
true. Large-gradient tasks can achieve similar or even much lower learning
gains than small-gradient ones. Further analyses reveal that these gradient
imbalances cannot be explained by typical training statistics such as training
rewards or advantages, suggesting that they arise from the inherent differences
between tasks. This cautions against naive dataset mixing and calls for future
work on principled gradient-level corrections for LLMs.

</details>


### [89] [A Communication-Efficient Decentralized Actor-Critic Algorithm](https://arxiv.org/abs/2510.19199)
*Xiaoxing Ren,Nicola Bastianello,Thomas Parisini,Andreas A. Malikopoulos*

Main category: cs.LG

TL;DR: 研究有限通信多智能体系统强化学习问题，提出分散式框架，减少通信负担，分析算法收敛性并实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中通信受限下的强化学习问题。

Method: 开发分散式演员 - 评论家学习框架，每个智能体在与邻居交换信息前进行本地策略和价值函数更新，用多层神经网络近似价值函数。

Result: 建立马尔可夫采样下算法的有限时间收敛分析，得到样本复杂度和通信复杂度，展示最终误差界与神经网络近似质量的关系。

Conclusion: 数值实验验证了理论结果。

Abstract: In this paper, we study the problem of reinforcement learning in multi-agent
systems where communication among agents is limited. We develop a decentralized
actor-critic learning framework in which each agent performs several local
updates of its policy and value function, where the latter is approximated by a
multi-layer neural network, before exchanging information with its neighbors.
This local training strategy substantially reduces the communication burden
while maintaining coordination across the network. We establish finite-time
convergence analysis for the algorithm under Markov-sampling. Specifically, to
attain the $\varepsilon$-accurate stationary point, the sample complexity is of
order $\mathcal{O}(\varepsilon^{-3})$ and the communication complexity is of
order $\mathcal{O}(\varepsilon^{-1}\tau^{-1})$, where tau denotes the number of
local training steps. We also show how the final error bound depends on the
neural network's approximation quality. Numerical experiments in a cooperative
control setting illustrate and validate the theoretical findings.

</details>


### [90] [An Active Diffusion Neural Network for Graphs](https://arxiv.org/abs/2510.19202)
*Mengying Jiang*

Main category: cs.LG

TL;DR: 现有基于扩散的GNN存在过平滑问题，本文提出ADGNN，结合外部信息源实现主动扩散，计算闭式解实现无限扩散，实验证明其能提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大多数基于扩散的GNN模拟被动热扩散，存在过平滑问题，限制捕捉全局图信息的能力。

Method: 提出主动扩散图神经网络ADGNN，集成多个外部信息源实现主动扩散，直接计算主动扩散迭代公式的闭式解实现无限扩散。

Result: 在多个图任务上与多个先进GNN模型对比，ADGNN显著提高了准确性和效率。

Conclusion: ADGNN在捕捉图全局信息和保持节点独特性方面有效。

Abstract: The analogy to heat diffusion has enhanced our understanding of information
flow in graphs and inspired the development of Graph Neural Networks (GNNs).
However, most diffusion-based GNNs emulate passive heat diffusion, which still
suffers from over-smoothing and limits their ability to capture global graph
information. Inspired by the heat death of the universe, which posits that
energy distribution becomes uniform over time in a closed system, we recognize
that, without external input, node representations in a graph converge to
identical feature vectors as diffusion progresses. To address this issue, we
propose the Active Diffusion-based Graph Neural Network (ADGNN). ADGNN achieves
active diffusion by integrating multiple external information sources that
dynamically influence the diffusion process, effectively overcoming the
over-smoothing problem. Furthermore, our approach realizes true infinite
diffusion by directly calculating the closed-form solution of the active
diffusion iterative formula. This allows nodes to preserve their unique
characteristics while efficiently gaining comprehensive insights into the
graph's global structure. We evaluate ADGNN against several state-of-the-art
GNN models across various graph tasks. The results demonstrate that ADGNN
significantly improves both accuracy and efficiency, highlighting its
effectiveness in capturing global graph information and maintaining node
distinctiveness.

</details>


### [91] [Enhancing Graph Neural Networks: A Mutual Learning Approach](https://arxiv.org/abs/2510.19223)
*Paul Agbaje,Akajyoti Mitra,Afia Anjum,Pranali Khose,Ebelechukwu Nwafor,Habeeb Olufowobi*

Main category: cs.LG

TL;DR: 本文探索图神经网络（GNN）协作学习潜力，提出协作学习框架，引入自适应对数加权单元和熵增强技术，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 突破传统知识蒸馏（KD）需预训练教师模型的局限，探索GNN协作学习在无预训练教师模型下的潜力，使简单浅层GNN架构协同学习高效模型以处理多任务。

Method: 提出协作学习框架，让学生GNN集合在训练过程中相互教学；引入自适应对数加权单元促进模型间知识交换，采用熵增强技术改善相互学习。

Result: 在节点和图分类的三个数据集上进行的大量实验，证明了所提方法的有效性。

Conclusion: 在无预训练教师模型的情况下，相对简单和浅层的GNN架构通过协作学习能协同学习出高效模型，在推理时表现更好，适合处理多任务。

Abstract: Knowledge distillation (KD) techniques have emerged as a powerful tool for
transferring expertise from complex teacher models to lightweight student
models, particularly beneficial for deploying high-performance models in
resource-constrained devices. This approach has been successfully applied to
graph neural networks (GNNs), harnessing their expressive capabilities to
generate node embeddings that capture structural and feature-related
information. In this study, we depart from the conventional KD approach by
exploring the potential of collaborative learning among GNNs. In the absence of
a pre-trained teacher model, we show that relatively simple and shallow GNN
architectures can synergetically learn efficient models capable of performing
better during inference, particularly in tackling multiple tasks. We propose a
collaborative learning framework where ensembles of student GNNs mutually teach
each other throughout the training process. We introduce an adaptive logit
weighting unit to facilitate efficient knowledge exchange among models and an
entropy enhancement technique to improve mutual learning. These components
dynamically empower the models to adapt their learning strategies during
training, optimizing their performance for downstream tasks. Extensive
experiments conducted on three datasets each for node and graph classification
demonstrate the effectiveness of our approach.

</details>


### [92] [Controllable Machine Unlearning via Gradient Pivoting](https://arxiv.org/abs/2510.19226)
*Youngsik Hwang,Dong-Young Lim*

Main category: cs.LG

TL;DR: 将机器遗忘问题从单目标优化转为多目标优化，提出CUP算法，实验显示其在视觉任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决近似遗忘方法在单目标优化下面临的遗忘效果和模型保真度的权衡问题，包括过遗忘风险、缺乏细粒度控制和整体评估指标。

Method: 将机器遗忘重构为多目标优化问题，提出CUP算法，通过独特的枢转机制，由超参数“遗忘强度”控制在帕累托前沿导航。

Result: CUP产生了更优的帕累托最优解集合，在各种视觉任务上始终优于现有方法。

Conclusion: CUP算法能有效解决机器遗忘中存在的问题，在权衡遗忘效果和模型保真度上表现出色。

Abstract: Machine unlearning (MU) aims to remove the influence of specific data from a
trained model. However, approximate unlearning methods, often formulated as a
single-objective optimization (SOO) problem, face a critical trade-off between
unlearning efficacy and model fidelity. This leads to three primary challenges:
the risk of over-forgetting, a lack of fine-grained control over the unlearning
process, and the absence of metrics to holistically evaluate the trade-off. To
address these issues, we reframe MU as a multi-objective optimization (MOO)
problem. We then introduce a novel algorithm, Controllable Unlearning by
Pivoting Gradient (CUP), which features a unique pivoting mechanism. Unlike
traditional MOO methods that converge to a single solution, CUP's mechanism is
designed to controllably navigate the entire Pareto frontier. This navigation
is governed by a single intuitive hyperparameter, the `unlearning intensity',
which allows for precise selection of a desired trade-off. To evaluate this
capability, we adopt the hypervolume indicator, a metric that captures both the
quality and diversity of the entire set of solutions an algorithm can generate.
Our experimental results demonstrate that CUP produces a superior set of
Pareto-optimal solutions, consistently outperforming existing methods across
various vision tasks.

</details>


### [93] [Brain-Inspired Perspective on Configurations: Unsupervised Similarity and Early Cognition](https://arxiv.org/abs/2510.19229)
*Juntang Wang,Yihan Wang,Hao Wu,Dongmian Zou,Shixin Xu*

Main category: cs.LG

TL;DR: 提出脑启发的配置框架，用mheatmap评估，在聚类、新奇检测和动态稳定性上表现良好，是早期认知分类计算模型和迈向脑启发AI的一步。


<details>
  <summary>Details</summary>
Motivation: 解决当前机器学习在无监督情况下让模型像婴儿一样发现类别、检测新奇和适应新环境的挑战。

Method: 提出有限分辨率聚类框架“配置”，用单分辨率参数和吸引 - 排斥动力学，引入mheatmap评估。

Result: 在标准聚类指标上有竞争力，新奇检测AUC达87%，动态类别演化稳定性提高35%。

Conclusion: 配置框架是早期认知分类的原则性计算模型，是迈向脑启发AI的一步。

Abstract: Infants discover categories, detect novelty, and adapt to new contexts
without supervision -- a challenge for current machine learning. We present a
brain-inspired perspective on configurations, a finite-resolution clustering
framework that uses a single resolution parameter and attraction-repulsion
dynamics to yield hierarchical organization, novelty sensitivity, and flexible
adaptation. To evaluate these properties, we introduce mheatmap, which provides
proportional heatmaps and a reassignment algorithm to fairly assess
multi-resolution and dynamic behavior. Across datasets, configurations are
competitive on standard clustering metrics, achieve 87% AUC in novelty
detection, and show 35% better stability during dynamic category evolution.
These results position configurations as a principled computational model of
early cognitive categorization and a step toward brain-inspired AI.

</details>


### [94] [Understanding the Implicit Biases of Design Choices for Time Series Foundation Models](https://arxiv.org/abs/2510.19236)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 本文旨在理解时间序列基础模型（TSFMs）训练过程中各因素对模型质量的影响，结合理论和实证评估分析设计选择带来的隐式偏差，并通过案例说明偏差间的复杂交互及结果对构建TSFMs的意义。


<details>
  <summary>Details</summary>
Motivation: 现有研究多开发新模型并宣称优于已有TSFMs，本文旨在理解训练过程中各种因素如何影响模型质量。

Method: 结合理论和控制实证评估，分析设计选择（如块大小、嵌入选择、训练目标等）。

Result: 确定了设计选择会导致模型基本属性的隐式偏差，这些偏差可能符合或违背直觉；通过案例展示了多种偏差复杂的交互方式。

Conclusion: 讨论了研究结果对吸取教训和构建TSFMs的影响。

Abstract: Time series foundation models (TSFMs) are a class of potentially powerful,
general-purpose tools for time series forecasting and related temporal tasks,
but their behavior is strongly shaped by subtle inductive biases in their
design. Rather than developing a new model and claiming that it is better than
existing TSFMs, e.g., by winning on existing well-established benchmarks, our
objective is to understand how the various ``knobs'' of the training process
affect model quality. Using a mix of theory and controlled empirical
evaluation, we identify several design choices (patch size, embedding choice,
training objective, etc.) and show how they lead to implicit biases in
fundamental model properties (temporal behavior, geometric structure, how
aggressively or not the model regresses to the mean, etc.); and we show how
these biases can be intuitive or very counterintuitive, depending on properties
of the model and data. We also illustrate in a case study on outlier handling
how multiple biases can interact in complex ways; and we discuss implications
of our results for learning the bitter lesson and building TSFMs.

</details>


### [95] [SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes](https://arxiv.org/abs/2510.19241)
*Xuyuan Xiong,Pedro Chumpitaz-Flores,Kaixun Hua,Cheng Hua*

Main category: cs.LG

TL;DR: 提出计算决策树策略的新方法SPOT，可高效优化马尔可夫决策过程中的决策树策略，实验显示有显著加速和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 可解释强化学习策略对高风险决策很重要，但优化马尔可夫决策过程中的决策树策略仍具挑战。

Method: 将优化问题表述为混合整数线性规划（MILP），采用降维分支定界法解耦MDP动态与树结构约束，实现高效并行搜索。

Result: 在标准基准测试中，SPOT显著加速，能处理状态数更多的大型MDP，得到的决策树策略可解释、紧凑。

Conclusion: 该方法同时实现可解释性和可扩展性，比现有方法快一个数量级地生成高质量策略。

Abstract: Interpretable reinforcement learning policies are essential for high-stakes
decision-making, yet optimizing decision tree policies in Markov Decision
Processes (MDPs) remains challenging. We propose SPOT, a novel method for
computing decision tree policies, which formulates the optimization problem as
a mixed-integer linear program (MILP). To enhance efficiency, we employ a
reduced-space branch-and-bound approach that decouples the MDP dynamics from
tree-structure constraints, enabling efficient parallel search. This
significantly improves runtime and scalability compared to previous methods.
Our approach ensures that each iteration yields the optimal decision tree.
Experimental results on standard benchmarks demonstrate that SPOT achieves
substantial speedup and scales to larger MDPs with a significantly higher
number of states. The resulting decision tree policies are interpretable and
compact, maintaining transparency without compromising performance. These
results demonstrate that our approach simultaneously achieves interpretability
and scalability, delivering high-quality policies an order of magnitude faster
than existing approaches.

</details>


### [96] [Interpret Policies in Deep Reinforcement Learning using SILVER with RL-Guided Labeling: A Model-level Approach to High-dimensional and Multi-action Environments](https://arxiv.org/abs/2510.19244)
*Yiyu Qian,Su Nguyen,Chao Chen,Qinyue Zhou,Liyuan Zhao*

Main category: cs.LG

TL;DR: 提出带RL引导标签的SILVER框架，扩展原SILVER至多动作和高维环境，实验表明该方法在保持性能的同时提升透明度和可理解性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习缺乏可解释性，现有SILVER框架局限于低维、二元动作领域。

Method: 提取图像特征表示，进行SHAP特征归因，用RL引导标签生成边界数据集，训练代理模型解释策略决策结构。

Result: 在两个Atari环境用三种深度强化学习算法评估，结果显示该方法在保持任务性能的同时，大幅提升代理行为的透明度和人类可理解性。

Conclusion: 将SILVER转变为可扩展且具备行为感知能力的框架，推动了可解释强化学习在高维、多动作场景的发展。

Abstract: Deep reinforcement learning (RL) achieves remarkable performance but lacks
interpretability, limiting trust in policy behavior. The existing SILVER
framework (Li, Siddique, and Cao 2025) explains RL policy via Shapley-based
regression but remains restricted to low-dimensional, binary-action domains. We
propose SILVER with RL-guided labeling, an enhanced variant that extends SILVER
to multi-action and high-dimensional environments by incorporating the RL
policy's own action outputs into the boundary points identification. Our method
first extracts compact feature representations from image observations,
performs SHAP-based feature attribution, and then employs RL-guided labeling to
generate behaviorally consistent boundary datasets. Surrogate models, such as
decision trees and regression-based functions, are subsequently trained to
interpret RL policy's decision structure. We evaluate the proposed framework on
two Atari environments using three deep RL algorithms and conduct human-subject
study to assess the clarity and trustworthiness of the derived interpretable
policy. Results show that our approach maintains competitive task performance
while substantially improving transparency and human understanding of agent
behavior. This work advances explainable RL by transforming SILVER into a
scalable and behavior-aware framework for interpreting deep RL agents in
high-dimensional, multi-action settings.

</details>


### [97] [Mixing Configurations for Downstream Prediction](https://arxiv.org/abs/2510.19248)
*Juntang Wang,Hao Wu,Runkun Guo,Yihan Wang,Dongmian Zou,Shixin Xu*

Main category: cs.LG

TL;DR: 提出GraMixC模块处理配置，在DSN1任务和标准表格基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 人类有按相似性分组对象的能力，现有社区检测可发现配置，需对其特征化并利用其改进聚类算法。

Method: 引入GraMixC模块，用Reverse Merge/Split技术对齐配置，通过注意力头融合后输入下游预测器。

Result: 在DSN1 16S rRNA培养介质预测任务中，R2分数从0.6提升到0.9；在标准表格基准测试中，始终优于单分辨率和静态特征基线。

Conclusion: GraMixC有效，为相关任务设定了新的技术水平。

Abstract: Humans possess an innate ability to group objects by similarity, a cognitive
mechanism that clustering algorithms aim to emulate. Recent advances in
community detection have enabled the discovery of configurations -- valid
hierarchical clusterings across multiple resolution scales -- without requiring
labeled data. In this paper, we formally characterize these configurations and
identify similar emergent structures in register tokens within Vision
Transformers. Unlike register tokens, configurations exhibit lower redundancy
and eliminate the need for ad hoc selection. They can be learned through
unsupervised or self-supervised methods, yet their selection or composition
remains specific to the downstream task and input. Building on these insights,
we introduce GraMixC, a plug-and-play module that extracts configurations,
aligns them using our Reverse Merge/Split (RMS) technique, and fuses them via
attention heads before forwarding them to any downstream predictor. On the DSN1
16S rRNA cultivation-media prediction task, GraMixC improves the R2 score from
0.6 to 0.9 across multiple methods, setting a new state of the art. We further
validate GraMixC on standard tabular benchmarks, where it consistently
outperforms single-resolution and static-feature baselines.

</details>


### [98] [FnRGNN: Distribution-aware Fairness in Graph Neural Network](https://arxiv.org/abs/2510.19257)
*Soyoung Park,Sungsu Lim*

Main category: cs.LG

TL;DR: 本文提出用于GNN节点回归的公平感知框架FnRGNN，在三个层面干预，实验表明其能减少群体差异且不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在回归任务中的公平性研究不足，现有方法无法充分解决节点级回归的连续性问题。

Method: 提出FnRGNN框架，在结构级进行边重新加权、在表征级通过MMD进行对齐、在预测级通过基于Sinkhorn的分布匹配进行归一化。

Result: 在四个真实世界数据集上的实验表明，FnRGNN减少了群体差异且不牺牲性能。

Conclusion: FnRGNN的多级策略可在复杂图拓扑下确保稳健的公平性。

Abstract: Graph Neural Networks (GNNs) excel at learning from structured data, yet
fairness in regression tasks remains underexplored. Existing approaches mainly
target classification and representation-level debiasing, which cannot fully
address the continuous nature of node-level regression. We propose FnRGNN, a
fairness-aware in-processing framework for GNN-based node regression that
applies interventions at three levels: (i) structure-level edge reweighting,
(ii) representation-level alignment via MMD, and (iii) prediction-level
normalization through Sinkhorn-based distribution matching. This multi-level
strategy ensures robust fairness under complex graph topologies. Experiments on
four real-world datasets demonstrate that FnRGNN reduces group disparities
without sacrificing performance. Code is available at
https://github.com/sybeam27/FnRGNN.

</details>


### [99] [Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge](https://arxiv.org/abs/2510.19266)
*Penghao Wang,Yuhao Zhou,Mengxuan Wu,Panpan Zhang,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: 提出CAB蒸馏框架，将Transformer注意力知识高效转移到SSM，实验证明提升SSM性能。


<details>
  <summary>Details</summary>
Motivation: SSM训练成本高、生态不成熟，与Transformer结构异质性大，难以从预训练注意力模型中有效提取知识。

Method: 提出Cross - architecture distillation via Attention Bridge (CAB)框架，通过轻量级桥接实现令牌级监督和灵活的逐层对齐。

Result: 在视觉和语言领域实验中，即使训练数据有限，该方法也能持续提升SSM性能，优于标准和跨架构蒸馏方法。

Conclusion: 基于注意力的知识可有效转移到循环模型，利于利用Transformer专长构建更强的SSM社区。

Abstract: State-space models (SSMs) have emerged as efficient alternatives to
Transformers for sequence modeling, offering superior scalability through
recurrent structures. However, their training remains costly and the ecosystem
around them is far less mature than that of Transformers. Moreover, the
structural heterogeneity between SSMs and Transformers makes it challenging to
efficiently distill knowledge from pretrained attention models. In this work,
we propose Cross-architecture distillation via Attention Bridge (CAB), a novel
data-efficient distillation framework that efficiently transfers attention
knowledge from Transformer teachers to state-space student models. Unlike
conventional knowledge distillation that transfers knowledge only at the output
level, CAB enables token-level supervision via a lightweight bridge and
flexible layer-wise alignment, improving both efficiency and transferability.
We further introduce flexible layer-wise alignment strategies to accommodate
architectural discrepancies between teacher and student. Extensive experiments
across vision and language domains demonstrate that our method consistently
improves the performance of state-space models, even under limited training
data, outperforming both standard and cross-architecture distillation methods.
Our findings suggest that attention-based knowledge can be efficiently
transferred to recurrent models, enabling rapid utilization of Transformer
expertise for building a stronger SSM community.

</details>


### [100] [Knowledge Distillation of Uncertainty using Deep Latent Factor Model](https://arxiv.org/abs/2510.19290)
*Sehyun Park,Jongjin Lee,Yunseop Shin,Ilsang Ohn,Yongdai Kim*

Main category: cs.LG

TL;DR: 提出高斯蒸馏方法解决知识蒸馏在压缩集成模型时难以保留不确定性的问题，该方法表现优于现有基线，适用于语言模型微调等场景。


<details>
  <summary>Details</summary>
Motivation: 深度集成计算和内存需求大，现有知识蒸馏技术在保留不确定性方面存在不足。

Method: 引入高斯蒸馏方法，通过深度潜在因子模型（DLF）估计教师集成的分布，用期望最大化（EM）算法稳定估计DLF模型的均值和协方差函数。

Result: 在多个基准数据集上，高斯蒸馏方法表现优于现有基线，且在语言模型微调与分布偏移问题上效果良好。

Conclusion: 提出的高斯蒸馏方法能有效解决现有知识蒸馏技术的局限。

Abstract: Deep ensembles deliver state-of-the-art, reliable uncertainty quantification,
but their heavy computational and memory requirements hinder their practical
deployments to real applications such as on-device AI. Knowledge distillation
compresses an ensemble into small student models, but existing techniques
struggle to preserve uncertainty partly because reducing the size of DNNs
typically results in variation reduction. To resolve this limitation, we
introduce a new method of distribution distillation (i.e. compressing a teacher
ensemble into a student distribution instead of a student ensemble) called
Gaussian distillation, which estimates the distribution of a teacher ensemble
through a special Gaussian process called the deep latent factor model (DLF) by
treating each member of the teacher ensemble as a realization of a certain
stochastic process. The mean and covariance functions in the DLF model are
estimated stably by using the expectation-maximization (EM) algorithm. By using
multiple benchmark datasets, we demonstrate that the proposed Gaussian
distillation outperforms existing baselines. In addition, we illustrate that
Gaussian distillation works well for fine-tuning of language models and
distribution shift problems.

</details>


### [101] [QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation](https://arxiv.org/abs/2510.19296)
*Yang Zhang,Rui Zhang,Jiaming Guo,Lei Huang,Di Huang,Yunpu Zhao,Shuyao Cheng,Pengwei Jin,Chongxiao Li,Zidong Du,Xing Hu,Qi Guo,Yunji Chen*

Main category: cs.LG

TL;DR: 本文提出QiMeng - SALV方法解决Verilog代码生成中强化学习功能奖励不足问题，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型为Verilog代码生成带来机会，但基于强化学习的偏好优化缺乏有意义的功能奖励，阻碍生成功能正确的Verilog代码。

Method: 提出QiMeng - SALV，利用功能正确的输出信号代码段优化强化学习训练，验证信号功能正确性，用抽象语法树识别代码段，引入信号感知DPO。

Result: 该方法在VerilogEval和RTLLM上达到了最先进的性能，7B参数模型匹配DeepSeek v3 671B模型表现，显著优于CodeV。

Conclusion: QiMeng - SALV实现了从传统模块级到细粒度信号级优化的范式转变，解决了功能奖励不足的问题。

Abstract: The remarkable progress of Large Language Models (LLMs) presents promising
opportunities for Verilog code generation which is significantly important for
automated circuit design. The lacking of meaningful functional rewards hinders
the preference optimization based on Reinforcement Learning (RL) for producing
functionally correct Verilog code. In this paper, we propose Signal-Aware
Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments
of functionally correct output signal to optimize RL training. Considering
Verilog code specifies the structural interconnection of hardware gates and
wires so that different output signals are independent, the key insight of
QiMeng-SALV is to extract verified signal-aware implementations in partially
incorrect modules, so as to enhance the extraction of meaningful functional
rewards. Roughly, we verify the functional correctness of signals in generated
module by comparing with that of reference module in the training data. Then
abstract syntax tree (AST) is employed to identify signal-aware code segments
which can provide meaningful functional rewards from erroneous modules.
Finally, we introduce signal-aware DPO which is optimized on the correct
signal-level code segments, thereby preventing noise and interference from
incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from
conventional module-level to fine-grained signal-level optimization in Verilog
code generation, addressing the issue of insufficient functional rewards.
Experiments demonstrate that our method achieves state-of-the-art performance
on VerilogEval and RTLLM, with a 7B parameter model matching the performance of
the DeepSeek v3 671B model and significantly outperforming the leading
open-source model CodeV trained on the same dataset. Our code is available at
https://github.com/zy1xxx/SALV.

</details>


### [102] [Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall](https://arxiv.org/abs/2510.19304)
*Mingyu Jo,Jaesik Yoon,Justin Deschenaux,Caglar Gulcehre,Sungjin Ahn*

Main category: cs.LG

TL;DR: 提出Loopholing机制构建LDDMs，减少生成困惑度，提升文本生成质量和推理任务表现。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型存在采样墙问题，后续步骤信息有限。

Method: 引入Loopholing机制，通过确定性潜在路径保留信息，采用自调节策略训练LDDMs。

Result: LDDMs将生成困惑度降低61%，缩小与自回归模型差距，生成文本更连贯，提升推理任务表现。

Conclusion: Loopholing缓解空闲步骤和振荡问题，为高质量非自回归文本生成提供可扩展路径。

Abstract: Discrete diffusion models offer a promising alternative to autoregressive
generation through parallel decoding, but they suffer from a sampling wall:
once categorical sampling occurs, rich distributional information collapses
into one-hot vectors and cannot be propagated across steps, forcing subsequent
steps to operate with limited information. To mitigate this problem, we
introduce Loopholing, a novel and simple mechanism that preserves this
information via a deterministic latent pathway, leading to Loopholing Discrete
Diffusion Models (LDDMs). Trained efficiently with a self-conditioning
strategy, LDDMs achieve substantial gains-reducing generative perplexity by up
to 61% over prior baselines, closing (and in some cases surpassing) the gap
with autoregressive models, and producing more coherent text. Applied to
reasoning tasks, LDDMs also improve performance on arithmetic benchmarks such
as Countdown and Game of 24. These results also indicate that loopholing
mitigates idle steps and oscillations, providing a scalable path toward
high-quality non-autoregressive text generation.

</details>


### [103] [FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation](https://arxiv.org/abs/2510.19305)
*Chirag Padubidri,Pranesh Velmurugan,Andreas Lanitis,Andreas Kamilaris*

Main category: cs.LG

TL;DR: 本文采用深度学习和数据插补技术增强蛙类物种分布建模准确性，数据平衡、特征选择和多模态集成模型提升了性能，表明多模态学习和数据预处理技术在生物多样性监测中有潜力。


<details>
  <summary>Details</summary>
Motivation: 传统物种分布监测数据收集方法有覆盖和完整性局限，需提升物种分布建模（SDM）准确性以助力生物多样性保护。

Method: 运用深度学习和数据插补技术，基于“EY - 2022生物多样性挑战”数据，进行数据平衡、特征选择，构建多模态集成模型，融合图像和表格数据。

Result: 数据平衡使蛙类计数任务平均绝对误差从189降至29；特征选择确定关键环境因素；多模态集成模型表现优于单个模型，跨区域泛化能力强；图像和表格数据融合在蛙类计数和栖息地分类中准确率达84.9%，AUC为0.90。

Conclusion: 多模态学习和数据预处理技术（如平衡和插补）在数据稀疏或不完整时，可提升预测生态建模效果，实现更精确和可扩展的生物多样性监测。

Abstract: Monitoring species distribution is vital for conservation efforts, enabling
the assessment of environmental impacts and the development of effective
preservation strategies. Traditional data collection methods, including citizen
science, offer valuable insights but remain limited in coverage and
completeness. Species Distribution Modelling (SDM) helps address these gaps by
using occurrence data and environmental variables to predict species presence
across large regions. In this study, we enhance SDM accuracy for frogs (Anura)
by applying deep learning and data imputation techniques using data from the
"EY - 2022 Biodiversity Challenge." Our experiments show that data balancing
significantly improved model performance, reducing the Mean Absolute Error
(MAE) from 189 to 29 in frog counting tasks. Feature selection identified key
environmental factors influencing occurrence, optimizing inputs while
maintaining predictive accuracy. The multimodal ensemble model, integrating
land cover, NDVI, and other environmental inputs, outperformed individual
models and showed robust generalization across unseen regions. The fusion of
image and tabular data improved both frog counting and habitat classification,
achieving 84.9% accuracy with an AUC of 0.90. This study highlights the
potential of multimodal learning and data preprocessing techniques such as
balancing and imputation to improve predictive ecological modeling when data
are sparse or incomplete, contributing to more precise and scalable
biodiversity monitoring.

</details>


### [104] [Calibration and Discrimination Optimization Using Clusters of Learned Representation](https://arxiv.org/abs/2510.19328)
*Tomer Lavi,Bracha Shapira,Nadav Rappoport*

Main category: cs.LG

TL;DR: 提出新颖校准管道，提升模型校准并兼顾区分度，方案灵活性能优。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型决策和风险评估需可靠预测，校准对关键决策至关重要但常被忽视。

Method: 引入利用输入样本学习表示聚类训练的校准函数集成的校准管道，还引入匹配指标。

Result: 将多种方法的校准分数从82.28%提高到100%，能确保模型选择兼顾区分度和校准。

Conclusion: 通用方案能适应多种方法和指标，在常用校准方法中灵活且性能优越。

Abstract: Machine learning models are essential for decision-making and risk
assessment, requiring highly reliable predictions in terms of both
discrimination and calibration. While calibration often receives less
attention, it is crucial for critical decisions, such as those in clinical
predictions. We introduce a novel calibration pipeline that leverages an
ensemble of calibration functions trained on clusters of learned
representations of the input samples to enhance overall calibration. This
approach not only improves the calibration score of various methods from 82.28%
up to 100% but also introduces a unique matching metric that ensures model
selection optimizes both discrimination and calibration. Our generic scheme
adapts to any underlying representation, clustering, calibration methods and
metric, offering flexibility and superior performance across commonly used
calibration methods.

</details>


### [105] [Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning](https://arxiv.org/abs/2510.19338)
*Ling Team,Bin Han,Caizhi Tang,Chen Liang,Donghao Zhang,Fan Yuan,Feng Zhu,Jie Gao,Jingyu Hu,Longfei Li,Meng Li,Mingyang Zhang,Peijie Jiang,Peng Jiao,Qian Zhao,Qingyuan Yang,Wenbo Shen,Xinxing Yang,Yalin Zhang,Yankun Ren,Yao Zhao,Yibo Cao,Yixuan Sun,Yue Zhang,Yuchen Fang,Zibin Lin,Zixuan Cheng,Jun Zhou*

Main category: cs.LG

TL;DR: 本文介绍Ring - linear模型系列，包括Ring - mini - linear - 2.0和Ring - flash - linear - 2.0，该系列模型降低推理成本、确定最优结构、提升训练效率，在强化学习阶段稳定优化，保持SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 降低长上下文推理场景的I/O和计算开销，提升模型性能和效率。

Method: 采用线性注意力和softmax注意力的混合架构，探索混合架构中不同注意力机制比例，使用自研高性能FP8算子库linghe。

Result: 相比320亿参数密集模型，推理成本降至1/10；相比原Ring系列，成本降低超50%；训练效率提升50%；在多个复杂推理基准测试中保持SOTA性能。

Conclusion: Ring - linear模型系列在降低成本、提升效率和性能方面表现出色，通过合理架构和算子库可实现长期稳定高效优化。

Abstract: In this technical report, we present the Ring-linear model series,
specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0.
Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while
Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both
models adopt a hybrid architecture that effectively integrates linear attention
and softmax attention, significantly reducing I/O and computational overhead in
long-context inference scenarios. Compared to a 32 billion parameter dense
model, this series reduces inference cost to 1/10, and compared to the original
Ring series, the cost is also reduced by over 50%. Furthermore, through
systematic exploration of the ratio between different attention mechanisms in
the hybrid architecture, we have identified the currently optimal model
structure. Additionally, by leveraging our self-developed high-performance FP8
operator library-linghe, overall training efficiency has been improved by 50%.
Benefiting from the high alignment between the training and inference engine
operators, the models can undergo long-term, stable, and highly efficient
optimization during the reinforcement learning phase, consistently maintaining
SOTA performance across multiple challenging complex reasoning benchmarks.

</details>


### [106] [Foundation Model Forecasts: Form and Function](https://arxiv.org/abs/2510.19345)
*Alvaro Perez-Diaz,James C. Loach,Danielle E. Toutoungi,Lee Middleton*

Main category: cs.LG

TL;DR: 研究时间序列基础模型（TSFMs），指出预测类型对实际应用的重要性，建立转换规则，提供任务对齐评估框架。


<details>
  <summary>Details</summary>
Motivation: TSFMs仅预测准确性不能决定实际价值，预测形式会限制可支持的操作任务。

Method: 对近期TSFMs进行调查，建立预测类型转换规则，证明边际不能确定路径依赖事件概率，将六个基本预测任务映射到最小充分预测类型。

Result: 发现三分之二的TSFMs仅产生点或参数预测，明确何时预测类型可转换，建立任务对齐评估框架。

Conclusion: 明确了预测类型而非准确性在区分实际效用方面的关键作用。

Abstract: Time-series foundation models (TSFMs) achieve strong forecast accuracy, yet
accuracy alone does not determine practical value. The form of a forecast --
point, quantile, parametric, or trajectory ensemble -- fundamentally constrains
which operational tasks it can support. We survey recent TSFMs and find that
two-thirds produce only point or parametric forecasts, while many operational
tasks require trajectory ensembles that preserve temporal dependence. We
establish when forecast types can be converted and when they cannot: trajectory
ensembles convert to simpler forms via marginalization without additional
assumptions, but the reverse requires imposing temporal dependence through
copulas or conformal methods. We prove that marginals cannot determine
path-dependent event probabilities -- infinitely many joint distributions share
identical marginals but yield different answers to operational questions. We
map six fundamental forecasting tasks to minimal sufficient forecast types and
provide a task-aligned evaluation framework. Our analysis clarifies when
forecast type, not accuracy, differentiates practical utility.

</details>


### [107] [A New Type of Adversarial Examples](https://arxiv.org/abs/2510.19347)
*Xingyang Nie,Guojie Xiao,Su Pan,Biao Wang,Huilin Ge,Tao Fang*

Main category: cs.LG

TL;DR: 本文提出新算法生成与原样本差异大但模型输出相同的对抗样本，且表明其在样本空间广泛分布。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型易受对抗样本攻击存在安全隐患，本文以相反方式构造对抗样本。

Method: 提出NI - FGSM、NI - FGM及其动量变体NMI - FGSM和NMI - FGM算法来生成对抗样本。

Result: 生成的对抗样本可在某些场合攻击机器学习系统，且在样本空间广泛分布，而非仅在数据集样本邻域。

Conclusion: 提出的算法能生成有特定分布特点的对抗样本，可用于攻击机器学习系统。

Abstract: Most machine learning models are vulnerable to adversarial examples, which
poses security concerns on these models. Adversarial examples are crafted by
applying subtle but intentionally worst-case modifications to examples from the
dataset, leading the model to output a different answer from the original
example. In this paper, adversarial examples are formed in an exactly opposite
manner, which are significantly different from the original examples but result
in the same answer. We propose a novel set of algorithms to produce such
adversarial examples, including the negative iterative fast gradient sign
method (NI-FGSM) and the negative iterative fast gradient method (NI-FGM),
along with their momentum variants: the negative momentum iterative fast
gradient sign method (NMI-FGSM) and the negative momentum iterative fast
gradient method (NMI-FGM). Adversarial examples constructed by these methods
could be used to perform an attack on machine learning systems in certain
occasions. Moreover, our results show that the adversarial examples are not
merely distributed in the neighbourhood of the examples from the dataset;
instead, they are distributed extensively in the sample space.

</details>


### [108] [A Markov Decision Process for Variable Selection in Branch & Bound](https://arxiv.org/abs/2510.19348)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 本文提出BBMDP用于分支限界（B&B）中的变量选择，可利用多种强化学习算法学习最优B&B启发式，实验表明其分支代理在四个标准混合整数线性规划（MILP）基准上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 分支限界求解器性能受变量选择启发式影响，现有研究尝试用强化学习学习最优分支策略，本文旨在提出一种有原则的方法。

Method: 引入BBMDP，这是一种用于B&B中变量选择的普通马尔可夫决策过程（MDP）公式，可利用多种强化学习算法。

Result: 计算实验表明，分支代理在四个标准MILP基准上优于先前的最先进强化学习代理。

Conclusion: BBMDP有效，能为B&B学习到更好的变量选择启发式策略。

Abstract: Mixed-Integer Linear Programming (MILP) is a powerful framework used to
address a wide range of NP-hard combinatorial optimization problems, often
solved by Branch and Bound (B&B). A key factor influencing the performance of
B&B solvers is the variable selection heuristic governing branching decisions.
Recent contributions have sought to adapt reinforcement learning (RL)
algorithms to the B&B setting to learn optimal branching policies, through
Markov Decision Processes (MDP) inspired formulations, and ad hoc convergence
theorems and algorithms. In this work, we introduce BBMDP, a principled vanilla
MDP formulation for variable selection in B&B, allowing to leverage a broad
range of RL algorithms for the purpose of learning optimal B\&B heuristics.
Computational experiments validate our model empirically, as our branching
agent outperforms prior state-of-the-art RL agents on four standard MILP
benchmarks.

</details>


### [109] [ConvXformer: Differentially Private Hybrid ConvNeXt-Transformer for Inertial Navigation](https://arxiv.org/abs/2510.19352)
*Omer Tariq,Muhammad Bilal,Muneeb Ul Hassan,Dongsoo Han,Jon Crowcroft*

Main category: cs.LG

TL;DR: 提出ConvXformer架构和高效差分隐私机制用于惯性导航，在基准数据集上表现超现有方法，还引入新数据集验证真实性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习惯性跟踪系统易泄露隐私，传统差分隐私方案会降低模型性能。

Method: 提出融合ConvNeXt块和Transformer编码器的ConvXformer架构，采用含自适应梯度裁剪和梯度对齐噪声注入的差分隐私机制，用截断奇异值分解处理梯度。

Result: 在基准数据集上定位精度提升超40%，保证差分隐私；引入新数据集验证真实环境性能。

Conclusion: 该框架适用于网络物理系统中的安全智能导航。

Abstract: Data-driven inertial sequence learning has revolutionized navigation in
GPS-denied environments, offering superior odometric resolution compared to
traditional Bayesian methods. However, deep learning-based inertial tracking
systems remain vulnerable to privacy breaches that can expose sensitive
training data. \hl{Existing differential privacy solutions often compromise
model performance by introducing excessive noise, particularly in
high-frequency inertial measurements.} In this article, we propose ConvXformer,
a hybrid architecture that fuses ConvNeXt blocks with Transformer encoders in a
hierarchical structure for robust inertial navigation. We propose an efficient
differential privacy mechanism incorporating adaptive gradient clipping and
gradient-aligned noise injection (GANI) to protect sensitive information while
ensuring model performance. Our framework leverages truncated singular value
decomposition for gradient processing, enabling precise control over the
privacy-utility trade-off. Comprehensive performance evaluations on benchmark
datasets (OxIOD, RIDI, RoNIN) demonstrate that ConvXformer surpasses
state-of-the-art methods, achieving more than 40% improvement in positioning
accuracy while ensuring $(\epsilon,\delta)$-differential privacy guarantees. To
validate real-world performance, we introduce the Mech-IO dataset, collected
from the mechanical engineering building at KAIST, where intense magnetic
fields from industrial equipment induce significant sensor perturbations. This
demonstrated robustness under severe environmental distortions makes our
framework well-suited for secure and intelligent navigation in cyber-physical
systems.

</details>


### [110] [Optimization Benchmark for Diffusion Models on Dynamical Systems](https://arxiv.org/abs/2510.19376)
*Fabian Schaipp*

Main category: cs.LG

TL;DR: 对扩散模型训练的优化算法进行基准测试，发现Muon和SOAP比AdamW更高效，还探讨训练相关现象。


<details>
  <summary>Details</summary>
Motivation: 现有新优化技术评估常缺失扩散模型训练，开展相关基准测试。

Method: 对用于去噪流轨迹的扩散模型训练的近期优化算法进行基准测试。

Result: Muon和SOAP比AdamW最终损失低18%，并探讨了学习率调度和Adam与SGD性能差距等现象。

Conclusion: Muon和SOAP是训练扩散模型更高效的优化算法。

Abstract: The training of diffusion models is often absent in the evaluation of new
optimization techniques. In this work, we benchmark recent optimization
algorithms for training a diffusion model for denoising flow trajectories. We
observe that Muon and SOAP are highly efficient alternatives to AdamW (18%
lower final loss). We also revisit several recent phenomena related to the
training of models for text or image applications in the context of diffusion
model training. This includes the impact of the learning-rate schedule on the
training dynamics, and the performance gap between Adam and SGD.

</details>


### [111] [LMFD: Latent Monotonic Feature Discovery](https://arxiv.org/abs/2510.19383)
*Guus Toussaint,Arno Knobbe*

Main category: cs.LG

TL;DR: 本文研究从多元时间序列中提取系统‘年龄’代理的方法，通过定义语法和优化方程的单调性生成候选特征，经实验验证可将低单调性传感器组合成高单调性代理。


<details>
  <summary>Details</summary>
Motivation: 监测系统时，可用传感器可能无法提供系统‘年龄’信息，需从多元时间序列中提取其潜在代理。

Method: 定义语法，以时间和候选公式的绝对Spearman秩相关定义单调性并优化方程，生成候选特征并评估。

Result: 系统能将低个体单调性的传感器组合成高单调性的潜在特征，如InfraWatch项目中可将两个低Spearman's ρ值特征组合成高值代理。

Conclusion: 所提方法能找到可解释的方程作为系统‘年龄’的代理。

Abstract: Many systems in our world age, degrade or otherwise move slowly but steadily
in a certain direction. When monitoring such systems by means of sensors, one
often assumes that some form of `age' is latently present in the data, but
perhaps the available sensors do not readily provide this useful information.
The task that we study in this paper is to extract potential proxies for this
`age' from the available multi-variate time series without having clear data on
what `age' actually is. We argue that when we find a sensor, or more likely
some discovered function of the available sensors, that is sufficiently
monotonic, that function can act as the proxy we are searching for. Using a
carefully defined grammar and optimising the resulting equations in terms of
monotonicity, defined as the absolute Spearman's Rank Correlation between time
and the candidate formula, the proposed approach generates a set of candidate
features which are then fitted and assessed on monotonicity. The proposed
system is evaluated against an artificially generated dataset and two
real-world datasets. In all experiments, we show that the system is able to
combine sensors with low individual monotonicity into latent features with high
monotonicity. For the real-world dataset of InfraWatch, a structural health
monitoring project, we show that two features with individual absolute
Spearman's $\rho$ values of $0.13$ and $0.09$ can be combined into a proxy with
an absolute Spearman's $\rho$ of $0.95$. This demonstrates that our proposed
method can find interpretable equations which can serve as a proxy for the
`age' of the system.

</details>


### [112] [Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment](https://arxiv.org/abs/2510.19384)
*Yuhang Liu,Minglai Shao,Zengyi Wo,Yunlong Chu,Bing Hao,Shengzhong Liu,Ruijie Wang,Jianxin Li*

Main category: cs.LG

TL;DR: 现有CLIP风格图文本对齐器有局限，提出ADAligner框架解决问题，实验表明其性能优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP风格图文本对齐器存在假设节点与文本严格一对一对应、依赖静态对齐目标等局限，无法适应不同数据质量。

Method: 提出ADAligner框架，根据监督质量在多对多和一对一目标间动态调整，实时估计批量级对齐可靠性并调整优化。

Result: 在九个不同的TAG数据集上实验，ADAligner在零/少样本节点分类、链接预测和跨模态检索任务中始终优于先前的图文本对齐器，在噪声监督下有强鲁棒性，预训练加速约2到3倍。

Conclusion: ADAligner为现实网络环境中的图文本表示学习建立了可扩展且可靠的基础。

Abstract: Pre-training Graph Foundation Models (GFMs) on text-attributed graphs (TAGs)
is central to web-scale applications such as search, recommendation, and
knowledge discovery. However, existing CLIP-style graph-text aligners face two
key limitations: they assume strict one-to-one correspondences between nodes
and texts, overlooking the inherent many-to-many relations in real-world
graphs; and they rely on static alignment objectives that cannot adapt to
varying data quality, making them brittle under noisy supervision. Together,
these limitations expose a core dilemma: embracing expressive many-to-many
alignment amplifies noise, while reverting to strict one-to-one strategies
sacrifices semantic diversity and fails to handle inherently mismatched pairs.
To address these challenges, we propose ADAligner, a dynamic, quality-aware
graph-text alignment framework that dynamically adjusts between expressive
many-to-many and conservative one-to-one objectives according to supervision
quality. ADAligner estimates batch-level alignment reliability in real time and
adapts its optimization accordingly, promoting soft, subgraph-level
many-to-many alignment when supervision is clean, while emphasizing reliable
one-to-one alignment by dynamically filtering low-confidence pairs under noise.
Theoretically, we prove that this dynamic mechanism forms a stable negative
feedback process, ensuring convergence and robustness. Comprehensive
experiments on nine diverse TAG datasets demonstrate that ADAligner
consistently outperforms prior graph-text aligners on zero-/few-shot node
classification, link prediction and cross-modal retrieval tasks. It maintains
strong robustness under noisy supervision and accelerates pre-training by
approximately 2 to 3 times compared to multimodal baselines, establishing a
scalable and reliable foundation for graph-text representation learning in
real-world web environments.

</details>


### [113] [CPSVD: Enhancing Large Language Model Compression via Column-Preserving Singular Value Decomposition](https://arxiv.org/abs/2510.19385)
*Lin Xv,Jingsheng Gao,Xian Gao,Ting Li,Yuzhuo Fu*

Main category: cs.LG

TL;DR: 提出CPSVD方法优化基于SVD的大语言模型压缩，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于SVD的大语言模型压缩方法将整个参数矩阵统一处理，忽略不同部分SVD近似误差差异，导致压缩效果不佳。

Method: 提出CPSVD方法，智能分割参数矩阵，保留分解误差高的列，对误差低的列进行SVD，自适应分配非均匀压缩率。

Result: CPSVD在实验中始终优于现有基于SVD的大语言模型压缩方法，在零样本任务中困惑度更低、准确率更高。

Conclusion: CPSVD方法能有效提升大语言模型基于SVD的压缩性能。

Abstract: The rapid advancement of Large Language Models (LLMs) faces a critical
bottleneck in their immense size, necessitating efficient compression
techniques. While Singular Value Decomposition (SVD) is a promising approach,
existing SVD-based methods treat the entire parameter matrix uniformly,
overlooking that SVD approximation errors vary significantly across different
matrix parts, which often leads to suboptimal compression. To address this, we
propose \textbf{C}olumn-\textbf{P}reserving \textbf{S}ingular \textbf{V}alue
\textbf{D}ecomposition (CPSVD), a novel method that refines SVD-based LLM
compression by intelligently segmenting the parameter matrix. Unlike
traditional SVD, CPSVD identifies and directly preserves matrix columns with
high decomposition errors, applying SVD only to columns with low decomposition
errors, while precisely determining the optimal balance point between these two
strategies to minimize error. Furthermore, leveraging the inherent
heterogeneity in decomposition errors across different matrices within an LLM,
CPSVD adaptively allocates non-uniform compression rates to modules within that
layer, while adhering to a target layer-wise compression ratio, thereby further
enhancing compression performance. Extensive experiments demonstrate that CPSVD
consistently outperforms state-of-the-art SVD-based LLM compression methods,
achieving lower perplexity and higher accuracy on zero-shot tasks.

</details>


### [114] [ARA: Adaptive Rank Allocation for Efficient Large Language Model SVD Compression](https://arxiv.org/abs/2510.19389)
*Lin Xv,Jingsheng Gao,Xian Gao,Ting Liu,Yuzhuo Fu*

Main category: cs.LG

TL;DR: 现有大语言模型压缩方法有局限，提出ARA方法，实验表明其效果佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型SVD压缩方法在确定线性模块合适秩时存在局限，如启发式算法搜索空间受限、基于掩码训练难以捕捉奇异值谱和可训练参数关系，且忽视增益函数在压缩比为1时的非光滑特性，易陷入局部最优。

Method: 提出自适应秩分配（ARA）方法，引入专用掩码设计实现保留秩和可训练参数的高效映射与更新，采用额外损失函数引导参数选择至全局最优解。

Result: 在LLaMA2 - 7B模型80%压缩比下，ARA将WikiText2困惑度从8.38降至6.42，零样本任务平均准确率较均匀压缩提高9.72个百分点。

Conclusion: ARA方法在基于SVD的大语言模型压缩的秩分配中有效。

Abstract: In the field of large language model (LLM) compression, singular value
decomposition (SVD) is a widely studied and adopted low-rank decomposition
technique. Since SVD operates exclusively on linear modules, and these modules
in LLMs are separated by nonlinear components, SVD can only be applied
independently to each linear module. Under a global compression ratio
constraint, determining the appropriate rank for different linear modules
becomes a critical problem. Existing approaches, such as heuristic algorithms
and mask-based training, have made progress in addressing this challenge.
However, these methods still suffer from several limitations: heuristic
algorithms explore the solution space within restricted regions, while
mask-based training struggles to efficiently capture the relationship between
singular value spectra and trainable parameters. More importantly, current
methods overlook the key property that the gain function is non-smooth at a
compression ratio of 1, which often leads the training process to suboptimal
local minima. To address these issues, we propose an Adaptive Rank Allocation
(ARA) method. Specifically, (1) ARA introduces a dedicated mask design that
enables efficient mapping and updating between retained ranks and trainable
parameters; and (2) it employs an additional loss function to guide parameter
selection toward globally optimal solutions. Experimental results demonstrate
that ARA achieves state-of-the-art performance. On the LLaMA2-7B model with a
80\% compression ratio, ARA reduces perplexity on WikiText2 from 8.38 to 6.42
and improves average zero-shot task accuracy by 9.72 percentage points compared
with uniform compression. These results highlight the effectiveness of our
method for rank allocation in SVD-based LLM compression.

</details>


### [115] [Iterative Training of Physics-Informed Neural Networks with Fourier-enhanced Features](https://arxiv.org/abs/2510.19399)
*Yulun Wu,Miguel Aguiar,Karl H. Johansson,Matthieu Barreau*

Main category: cs.LG

TL;DR: 提出IFeF - PINN算法解决物理信息神经网络（PINNs）频谱偏差问题，经实验验证性能优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 许多PINNs训练算法存在频谱偏差问题，即神经网络倾向于先学习低频特征，需解决该问题。

Method: 提出IFeF - PINN算法，通过随机傅里叶特征用高频分量丰富潜在空间，分两步进行迭代训练：估计特征空间的基和执行回归确定增强基函数的系数。

Result: 证明对于线性模型，回归问题是凸的且迭代训练方案收敛；实验表明随机傅里叶特征增强了网络表达能力，能准确逼近高频偏微分方程；数值评估显示该方法性能优于现有算法，在频域上的逼近效果更好。

Conclusion: IFeF - PINN算法能有效克服PINNs频谱偏差问题，性能优越。

Abstract: Spectral bias, the tendency of neural networks to learn low-frequency
features first, is a well-known issue with many training algorithms for
physics-informed neural networks (PINNs). To overcome this issue, we propose
IFeF-PINN, an algorithm for iterative training of PINNs with Fourier-enhanced
features. The key idea is to enrich the latent space using high-frequency
components through Random Fourier Features. This creates a two-stage training
problem: (i) estimate a basis in the feature space, and (ii) perform regression
to determine the coefficients of the enhanced basis functions. For an
underlying linear model, it is shown that the latter problem is convex, and we
prove that the iterative training scheme converges. Furthermore, we empirically
establish that Random Fourier Features enhance the expressive capacity of the
network, enabling accurate approximation of high-frequency PDEs. Through
extensive numerical evaluation on classical benchmark problems, the superior
performance of our method over state-of-the-art algorithms is shown, and the
improved approximation across the frequency domain is illustrated.

</details>


### [116] [FairNet: Dynamic Fairness Correction without Performance Loss via Contrastive Conditional LoRA](https://arxiv.org/abs/2510.19421)
*Songqi Zhou,Zeyuan Liu,Benben Jiang*

Main category: cs.LG

TL;DR: 提出FairNet框架解决机器学习模型公平性问题，能动态、实例级纠正公平性，经理论分析和实证评估有效。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法存在性能受损、依赖静态策略、数据稀疏处理困难和敏感属性利用不佳等问题。

Method: 提出FairNet框架，集成偏差检测器与条件低秩适应（LoRA），使用新的对比损失函数训练LoRA模块，能灵活处理不同敏感属性标签情况。

Result: 理论分析表明在一定条件下能提升最差组性能且不降低整体性能，实证评估验证了FairNet的有效性。

Conclusion: FairNet框架有效解决了现有去偏方法的局限，能实现动态、实例级公平性纠正。

Abstract: Ensuring fairness in machine learning models is a critical challenge.
Existing debiasing methods often compromise performance, rely on static
correction strategies, and struggle with data sparsity, particularly within
minority groups. Furthermore, their utilization of sensitive attributes is
often suboptimal, either depending excessively on complete attribute labeling
or disregarding these attributes entirely. To overcome these limitations, we
propose FairNet, a novel framework for dynamic, instance-level fairness
correction. FairNet integrates a bias detector with conditional low-rank
adaptation (LoRA), which enables selective activation of the fairness
correction mechanism exclusively for instances identified as biased, and
thereby preserve performance on unbiased instances. A key contribution is a new
contrastive loss function for training the LoRA module, specifically designed
to minimize intra-class representation disparities across different sensitive
groups and effectively address underfitting in minority groups. The FairNet
framework can flexibly handle scenarios with complete, partial, or entirely
absent sensitive attribute labels. Theoretical analysis confirms that, under
moderate TPR/FPR for the bias detector, FairNet can enhance the performance of
the worst group without diminishing overall model performance, and potentially
yield slight performance improvements. Comprehensive empirical evaluations
across diverse vision and language benchmarks validate the effectiveness of
FairNet.

</details>


### [117] [LLM Unlearning with LLM Beliefs](https://arxiv.org/abs/2510.19422)
*Kemou Li,Qizhou Wang,Yue Wang,Fengpeng Li,Jun Liu,Bo Han,Jiantao Zhou*

Main category: cs.LG

TL;DR: 大语言模型有记忆敏感有害内容风险，现有去学习方法有挤压效应，提出引导框架缓解该问题并经实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型去学习方法中存在的挤压效应导致的虚假去学习问题。

Method: 提出引导（BS）框架，将挤压效应与模型的高置信度生成（模型信念）关联，分别用BS - T和BS - S方法处理。

Result: 通过在不同基准和多种模型族上的大量实验，验证了方法的有效性。

Conclusion: 提出的方法能更彻底地实现遗忘并保留模型实用性。

Abstract: Large language models trained on vast corpora inherently risk memorizing
sensitive or harmful content, which may later resurface in their outputs.
Prevailing unlearning methods generally rely on gradient ascent and its
variants to lower the probability of specific target responses. However, we
find that this strategy induces a critical side effect: probability mass is
redistributed into high-likelihood regions, often corresponding to semantically
related rephrasings of the targets. We refer to this as the squeezing effect,
which explains why many methods yield merely spurious unlearning, a problem
further obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport
actual success. To address this, we propose a bootstrapping (BS) framework that
explicitly links the squeezing effect with the model's own high-confidence
generations, namely its model beliefs. Since model beliefs inherently capture
the very high-likelihood regions where probability mass is squeezed,
incorporating them into the unlearning objective directly counters the
squeezing effect. By jointly suppressing both target responses and model
beliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S
(sequence) removes entire high-confidence generations, together achieving more
thorough forgetting while preserving utility. Extensive experiments across
diverse benchmarks with various model families confirm the effectiveness of our
approach.

</details>


### [118] [Neural Variational Dropout Processes](https://arxiv.org/abs/2510.19425)
*Insu Jeon,Youngjin Park,Gunhee Kim*

Main category: cs.LG

TL;DR: 本文提出神经变分丢弃过程（NVDPs）的贝叶斯元学习方法，可用于多任务少样本学习，实验显示其性能出色。


<details>
  <summary>Details</summary>
Motivation: 学习推断条件后验模型是鲁棒元学习的关键步骤，旨在提出新的贝叶斯元学习方法。

Method: 基于特定任务的丢弃来建模条件后验分布，利用伯努利专家元模型的低秩积进行丢弃率映射，使用基于整个任务数据的新先验优化条件丢弃后验。

Result: 在少样本学习任务（如一维随机回归、图像修复和分类）中与其他元学习方法对比，NVDPs表现出色。

Conclusion: NVDPs方法能有效处理功能模糊性和不确定性，在少样本学习任务中有优秀性能。

Abstract: Learning to infer the conditional posterior model is a key step for robust
meta-learning. This paper presents a new Bayesian meta-learning approach called
Neural Variational Dropout Processes (NVDPs). NVDPs model the conditional
posterior distribution based on a task-specific dropout; a low-rank product of
Bernoulli experts meta-model is utilized for a memory-efficient mapping of
dropout rates from a few observed contexts. It allows for a quick
reconfiguration of a globally learned and shared neural network for new tasks
in multi-task few-shot learning. In addition, NVDPs utilize a novel prior
conditioned on the whole task data to optimize the conditional \textit{dropout}
posterior in the amortized variational inference. Surprisingly, this enables
the robust approximation of task-specific dropout rates that can deal with a
wide range of functional ambiguities and uncertainties. We compared the
proposed method with other meta-learning approaches in the few-shot learning
tasks such as 1D stochastic regression, image inpainting, and classification.
The results show the excellent performance of NVDPs.

</details>


### [119] [Revisiting the Relation Between Robustness and Universality](https://arxiv.org/abs/2510.19427)
*M. Klabunde,L. Caspari,F. Lemmerich*

Main category: cs.LG

TL;DR: 本文重新检验修改的普遍性假设，发现特定设置下有部分普遍性，非严格普遍。


<details>
  <summary>Details</summary>
Motivation: 重新检验Jones等人（2022）提出的修改的普遍性假设的普遍性。

Method: 对假设进行验证，在不同数据集上测试。

Result: 特定设置下验证了高表示相似性，但不同数据集结果不一致；预测行为不随鲁棒性增加而收敛；不同预测源于分类层，简单重新训练分类器可实现更普遍的预测行为。

Conclusion: 神经网络在特定设置下有部分普遍性，并非严格普遍。

Abstract: The modified universality hypothesis proposed by Jones et al. (2022) suggests
that adversarially robust models trained for a given task are highly similar.
We revisit the hypothesis and test its generality. While we verify Jones' main
claim of high representational similarity in specific settings, results are not
consistent across different datasets. We also discover that predictive behavior
does not converge with increasing robustness and thus is not universal. We find
that differing predictions originate in the classification layer, but show that
more universal predictive behavior can be achieved with simple retraining of
the classifiers. Overall, our work points towards partial universality of
neural networks in specific settings and away from notions of strict
universality.

</details>


### [120] [g-DPO: Scalable Preference Optimization for Protein Language Models](https://arxiv.org/abs/2510.19474)
*Constance Ferragu,Jonathan D. Ziegler,Nicolas Deutschmann,Arthur Lindoulsi,Eli Bixby,Cradle ML Team*

Main category: cs.LG

TL;DR: 提出g - DPO框架解决DPO可扩展性瓶颈，在三个蛋白质工程任务中性能与DPO相当，收敛速度更快。


<details>
  <summary>Details</summary>
Motivation: DPO存在可扩展性瓶颈，训练对数量随标记序列数量二次增长，导致训练时间过长。

Method: 使用序列空间聚类去除冗余对，同时保留训练信号；用基于组的近似方法分摊似然计算。

Result: 在三个蛋白质工程任务中，g - DPO在计算机模拟和体外实验中的性能与标准DPO无统计学差异，收敛速度快1.8到3.7倍，数据集越大收益越大。

Conclusion: g - DPO是解决DPO可扩展性瓶颈的有效框架。

Abstract: Direct Preference Optimization (DPO) is an effective approach for aligning
protein language models with experimental design goals. However, DPO faces a
scalability bottleneck: the number of possible training pairs grows
quadratically with the number of labeled sequences, leading to prohibitive
training times even for modestly sized datasets. We introduce g-DPO, a
framework that (i) uses sequence space clustering to prune redundant pairs
while preserving training signal, and (ii) amortizes likelihood computations
with group-based approximations. Across three protein engineering tasks, g-DPO
maintains in-silico and in-vitro performance that is statistically
indistinguishable from standard DPO, while converging 1.8 to 3.7 times faster,
with greater gains expected as the size of the dataset increases.

</details>


### [121] [A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring](https://arxiv.org/abs/2510.19476)
*Julian Schulz*

Main category: cs.LG

TL;DR: 本文提出基于思维链（CoT）监测构建推理模型安全案例的路线图及研究议程，探讨威胁、评估技术并建立预测市场。


<details>
  <summary>Details</summary>
Motivation: 当AI系统接近危险能力水平，现有安全案例方法不足，需新途径确保安全。

Method: 提出两部分安全案例，系统研究监测可性的威胁并分类，评估维护CoT忠实性的技术，探索从不可监测CoT提取可监测CoT，建立预测市场。

Result: 无明确提及具体结果。

Conclusion: 无明确提及具体结论。

Abstract: As AI systems approach dangerous capability levels where inability safety
cases become insufficient, we need alternative approaches to ensure safety.
This paper presents a roadmap for constructing safety cases based on
chain-of-thought (CoT) monitoring in reasoning models and outlines our research
agenda. We argue that CoT monitoring might support both control and
trustworthiness safety cases. We propose a two-part safety case: (1)
establishing that models lack dangerous capabilities when operating without
their CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT
are detectable by CoT monitoring. We systematically examine two threats to
monitorability: neuralese and encoded reasoning, which we categorize into three
forms (linguistic drift, steganography, and alien reasoning) and analyze their
potential drivers. We evaluate existing and novel techniques for maintaining
CoT faithfulness. For cases where models produce non-monitorable reasoning, we
explore the possibility of extracting a monitorable CoT from a non-monitorable
CoT. To assess the viability of CoT monitoring safety cases, we establish
prediction markets to aggregate forecasts on key technical milestones
influencing their feasibility.

</details>


### [122] [Graph Unlearning Meets Influence-aware Negative Preference Optimization](https://arxiv.org/abs/2510.19479)
*Qiang Chen,Zhongze Wu,Ang He,Xi Lin,Shuo Jiang,Shan You,Chang Xu,Yi Chen,Xiu Su*

Main category: cs.LG

TL;DR: 现有图遗忘模型因梯度上升发散快致效用下降，本文提出INPO框架，实验表明其在遗忘质量指标上达最优并保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 解决现有图遗忘模型在遗忘过程中因梯度上升发散速度快导致模型效用急剧下降的问题。

Method: 提出INPO框架，分析NPO特性，设计影响感知消息函数，用移除法估计边影响，提出拓扑熵损失。

Result: 在五个真实数据集上的实验表明，基于INPO的模型在所有遗忘质量指标上达到了最先进的性能，同时保持了模型的效用。

Conclusion: INPO框架能有效减缓发散速度，提高模型在遗忘过程中效用的鲁棒性。

Abstract: Recent advancements in graph unlearning models have enhanced model utility by
preserving the node representation essentially invariant, while using gradient
ascent on the forget set to achieve unlearning. However, this approach causes a
drastic degradation in model utility during the unlearning process due to the
rapid divergence speed of gradient ascent. In this paper, we introduce
\textbf{INPO}, an \textbf{I}nfluence-aware \textbf{N}egative
\textbf{P}reference \textbf{O}ptimization framework that focuses on slowing the
divergence speed and improving the robustness of the model utility to the
unlearning process. Specifically, we first analyze that NPO has slower
divergence speed and theoretically propose that unlearning high-influence edges
can reduce impact of unlearning. We design an influence-aware message function
to amplify the influence of unlearned edges and mitigate the tight topological
coupling between the forget set and the retain set. The influence of each edge
is quickly estimated by a removal-based method. Additionally, we propose a
topological entropy loss from the perspective of topology to avoid excessive
information loss in the local structure during unlearning. Extensive
experiments conducted on five real-world datasets demonstrate that INPO-based
model achieves state-of-the-art performance on all forget quality metrics while
maintaining the model's utility. Codes are available at
\href{https://github.com/sh-qiangchen/INPO}{https://github.com/sh-qiangchen/INPO}.

</details>


### [123] [ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language Models on Edge Devices](https://arxiv.org/abs/2510.19482)
*Xin Nie,Liang Dong,HaiCheng Zhang,JiaWang Xiao,G. Sun*

Main category: cs.LG

TL;DR: 提出一种高效量化框架ELUTQ，用于在CPU边缘设备部署大语言模型，实验显示其在降低困惑度和提升推理效率上效果显著。


<details>
  <summary>Details</summary>
Motivation: 在基于CPU的边缘设备上部署大语言模型因内存和计算资源有限存在挑战，现有量化方法有局限性。

Method: 提出ELUTQ框架，引入分层线性量化（HLQ）格式，提供优化的CPU内核进行端到端推理。

Result: 对于LLaMA3 - 8B，HLQ在3位和2位精度下降低困惑度；2位LLaMA2 - 7B在Apple M2芯片上推理效率超25 tokens/s。

Conclusion: ELUTQ能有效解决边缘设备部署大语言模型的内存和计算瓶颈问题，提升性能。

Abstract: The deployment of Large Language Models (LLMs) on CPU-based edge devices is
crucial for enabling on-device intelligence and expanding AI accessibility.
However, it remains challenging due to limited memory and computational
resources. During edge inference, memory usage and latency are the primary
bottlenecks. Although weight quantization can effectively reduce memory
consumption, existing hardware-friendly approaches often rely on uniform
quantization, which poorly fits weight distributions and incurs high
dequantization overhead at low bit widths. To address these limitations, we
propose ELUTQ, an efficient quantization framework introducing a novel
quantization format, Hierarchical Linear Quantization (HLQ). HLQ better
captures the statistical characteristics of weights without increasing the
computational cost of Bit-serial LUT-based GEMM operations, thereby eliminating
dequantization overhead. It is orthogonal to existing quantization algorithms
and can be seamlessly integrated into various quantization pipelines. For
efficient on-device deployment, ELUTQ provides optimized CPU kernels for
end-to-end inference. Experiments show that for LLaMA3-8B, HLQ reduces
perplexity by about 8% at 3-bit and 85% at 2-bit precision under post-training
quantization, completing quantization within one hour. With efficient
finetuning, HLQ further improves 2-bit performance within two hours. In terms
of inference efficiency, our 2-bit LLaMA2-7B achieves over 25 tokens/s on an
Apple M2 chip (4 threads, batch size = 1).

</details>


### [124] [Energy-Efficient and Dequantization-Free Q-LLMs: A Spiking Neural Network Approach to Salient Value Mitigation](https://arxiv.org/abs/2510.19498)
*Chenyu Wang,Zhanglu Yan,Zhi Zhou,Xu Chen,Weng-Fai Wong*

Main category: cs.LG

TL;DR: 提出SpikeQuant方法，对有显著值的激活进行混合精度量化并编码为二进制脉冲计数，避免显式反量化，实验表明其在W4A4量化下接近FP16困惑度，节能最高4.6倍。


<details>
  <summary>Details</summary>
Motivation: 大语言模型权重激活量化在能量受限硬件上存在MAC操作能耗高、反量化增加延迟和能耗、统一参数位宽裁剪显著值等问题，而脉冲神经网络有节能优势，因此提出SpikeQuant方法解决这些问题。

Method: 提出SpikeQuant，选择性地对有显著值的激活应用混合精度量化并重新编码为二进制脉冲计数，将量化尺度嵌入IF机制阈值以避免显式反量化。

Result: SpikeQuant在W4A4量化下始终能达到接近FP16的困惑度，与现有方法相比，能源成本最多降低4.6倍。

Conclusion: SpikeQuant对准确且节能的大语言模型部署有效。

Abstract: In the era of large language models (LLMs), weight-activation quantization
helps fit models on edge device by reducing memory and compute bit-widths.
However, three challenges persist for energy constrained hardware: (1) even
after quantization, multiply-accumulate (MAC) operations remain unavoidable and
continue to dominate energy consumption; (2) dequantization (or
per-tensor/channel rescaling) introduces extra arithmetic and data movement,
increasing latency and energy; (3) uniform parameters bit widths clip salient
values-while intra-channel mixed precision is generally impractical on current
matrix hardware and memory. In contrast, brain-inspired Spiking Neural Networks
(SNNs), owing to their binary spike-based information representation and the
Integrate-and-Fire (IF) paradigm, naturally support mixed-precision storage and
energy-efficient computation by replacing complex MACs with temporal Accumulate
(ACCs). Motivated by this property, we propose SpikeQuant, which selectively
applies mixed-precision quantization to activations with salient values and
re-encodes them into binary spike counts, thereby enabling dynamic mixed
storage of different bitwidths. Furthermore, by embedding the quantization
scale into the threshold of the IF mechanism, our approach performs
energy-efficient linear transformations on weights and activations while
avoiding explicit dequantization. Experimental results demonstrate that
SpikeQuant consistently achieves near-FP16 perplexity under W4A4 quantization
while reducing energy cost by up to 4.6 times compared to existing methods,
highlighting its effectiveness for accurate and energy-efficient LLM
deployment.

</details>


### [125] [Teaming LLMs to Detect and Mitigate Hallucinations](https://arxiv.org/abs/2510.19507)
*Demian Till,John Smeaton,Peter Haubrick,Gouse Saheb,Florian Graef,David Berman*

Main category: cs.LG

TL;DR: 通过聚合多个不同大语言模型（LLM）的回复，“联盟一致性”方法在幻觉检测和缓解方面比单模型一致性方法有显著改进，且能降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于单模型一致性的方法可检测和缓解大语言模型幻觉，但存在局限，希望通过结合多个不同LLM的回复进一步提升效果。

Method: 将单模型一致性方法扩展为结合多个具有不同训练数据、训练方案和模型架构的LLM的回复，评估“联盟一致性”方法，并探索不同LLM组合的有利条件。

Result: “联盟一致性”方法在幻觉检测和缓解能力上比单模型一致性方法有显著提升，且常能降低推理成本。

Conclusion: 结合多个不同LLM的“联盟一致性”方法是一种有效的提升大语言模型幻觉检测和缓解能力的途径，还能解决单模型一致性方法推理成本高的问题。

Abstract: Recent work has demonstrated state-of-the-art results in large language model
(LLM) hallucination detection and mitigation through consistency-based
approaches which involve aggregating multiple responses sampled from a single
LLM for a given prompt. These approaches help offset limitations stemming from
the imperfect data on which LLMs are trained, which includes biases and
under-representation of information required at deployment time among other
limitations which can lead to hallucinations. We show that extending these
single-model consistency methods to combine responses from multiple LLMs with
different training data, training schemes and model architectures can result in
substantial further improvements in hallucination detection and mitigation
capabilities beyond their single-model consistency counterparts. We evaluate
this \emph{consortium consistency} approach across many model teams from a pool
of 15 LLMs and explore under what conditions it is beneficial to team together
different LLMs in this manner. Further, we show that these performance
improvements often come with reduced inference costs, offsetting a significant
drawback with single-model consistency methods.

</details>


### [126] [From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification](https://arxiv.org/abs/2510.19514)
*Maciej Mozolewski,Betül Bayrak,Kerstin Bach,Grzegorz J. Nalepa*

Main category: cs.LG

TL;DR: 提出用于12导联ECG分类模型的原型驱动框架生成稀疏反事实解释，评估三种变体，能近实时生成临床有效反事实，为解释平台奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解决当前先进模型可解释性挑战，在时间序列可解释人工智能领域提供可操作和可解释见解。

Method: 采用基于SHAP的阈值识别关键信号段并转换为区间规则，用DTW和类中心点聚类提取代表性原型，将原型与查询R波对齐。

Result: 生成的反事实仅修改78%原始信号，整体有效性达81.3%，时间稳定性提升43%，不同变体和类别性能有差异。

Conclusion: 为基于AI的诊断系统建立生理感知反事实解释的设计原则，指出临床部署用户控制解释界面的途径。

Abstract: In eXplainable Artificial Intelligence (XAI), instance-based explanations for
time series have gained increasing attention due to their potential for
actionable and interpretable insights in domains such as healthcare. Addressing
the challenges of explainability of state-of-the-art models, we propose a
prototype-driven framework for generating sparse counterfactual explanations
tailored to 12-lead ECG classification models. Our method employs SHAP-based
thresholds to identify critical signal segments and convert them into interval
rules, uses Dynamic Time Warping (DTW) and medoid clustering to extract
representative prototypes, and aligns these prototypes to query R-peaks for
coherence with the sample being explained. The framework generates
counterfactuals that modify only 78% of the original signal while maintaining
81.3% validity across all classes and achieving 43% improvement in temporal
stability. We evaluate three variants of our approach, Original, Sparse, and
Aligned Sparse, with class-specific performance ranging from 98.9% validity for
myocardial infarction (MI) to challenges with hypertrophy (HYP) detection
(13.2%). This approach supports near realtime generation (< 1 second) of
clinically valid counterfactuals and provides a foundation for interactive
explanation platforms. Our findings establish design principles for
physiologically-aware counterfactual explanations in AI-based diagnosis systems
and outline pathways toward user-controlled explanation interfaces for clinical
deployment.

</details>


### [127] [Bi-Level Decision-Focused Causal Learning for Large-Scale Marketing Optimization: Bridging Observational and Experimental Data](https://arxiv.org/abs/2510.19517)
*Shuli Zhang,Hao Zhou,Jiaqi Zheng,Guibin Jiang,Bing Cheng,Wei Lin,Guihai Chen*

Main category: cs.LG

TL;DR: 提出Bi - DFCL方法解决在线平台营销资源分配问题，在多数据集评估有效并已在美团部署。


<details>
  <summary>Details</summary>
Motivation: 传统在线平台营销资源分配的两阶段解决方案存在预测 - 决策不一致和偏差 - 方差困境两大技术挑战。

Method: 提出Bi - DFCL方法，用实验数据开发OR决策质量无偏估计器，通过代理损失函数指导ML模型训练，建立双级优化框架联合利用观测和实验数据并通过隐式微分求解。

Result: 在公共基准、工业营销数据集和大规模在线A/B测试中显示出比现有技术显著的改进。

Conclusion: Bi - DFCL方法能有效解决传统方案的挑战，具有实际应用价值。

Abstract: Online Internet platforms require sophisticated marketing strategies to
optimize user retention and platform revenue -- a classical resource allocation
problem. Traditional solutions adopt a two-stage pipeline: machine learning
(ML) for predicting individual treatment effects to marketing actions, followed
by operations research (OR) optimization for decision-making. This paradigm
presents two fundamental technical challenges. First, the prediction-decision
misalignment: Conventional ML methods focus solely on prediction accuracy
without considering downstream optimization objectives, leading to improved
predictive metrics that fail to translate to better decisions. Second, the
bias-variance dilemma: Observational data suffers from multiple biases (e.g.,
selection bias, position bias), while experimental data (e.g., randomized
controlled trials), though unbiased, is typically scarce and costly --
resulting in high-variance estimates. We propose Bi-level Decision-Focused
Causal Learning (Bi-DFCL) that systematically addresses these challenges.
First, we develop an unbiased estimator of OR decision quality using
experimental data, which guides ML model training through surrogate loss
functions that bridge discrete optimization gradients. Second, we establish a
bi-level optimization framework that jointly leverages observational and
experimental data, solved via implicit differentiation. This novel formulation
enables our unbiased OR estimator to correct learning directions from biased
observational data, achieving optimal bias-variance tradeoff. Extensive
evaluations on public benchmarks, industrial marketing datasets, and
large-scale online A/B tests demonstrate the effectiveness of Bi-DFCL, showing
statistically significant improvements over state-of-the-art. Currently,
Bi-DFCL has been deployed at Meituan, one of the largest online food delivery
platforms in the world.

</details>


### [128] [Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning](https://arxiv.org/abs/2510.19530)
*Ruiyao Miao,Junren Xiao,Shiya Tsang,Hui Xiong,Yingnian Wu*

Main category: cs.LG

TL;DR: 传统贝叶斯优化方法有一步偏差问题，本文提出REBMBO方法，结合GP和EBM，用PPO进行自适应多步前瞻，实验证明其性能优越、适应性和鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法存在显著的一步偏差，可能导致收敛到局部最优，在复杂或高维任务中表现不佳，而黑盒优化在多领域取得成功，因此提出新方法。

Method: 提出Reinforced Energy-Based Model for Bayesian Optimization (REBMBO)，将高斯过程用于局部引导，结合能量模型捕捉全局结构信息，将每次贝叶斯优化迭代定义为马尔可夫决策过程，使用近端策略优化进行自适应多步前瞻。

Result: 在合成和真实世界基准上进行大量实验，证实了REBMBO的优越性能，不同GP配置的额外分析进一步凸显其适应性和鲁棒性。

Conclusion: REBMBO能有效克服传统贝叶斯优化方法的局限性，具有优越性能、适应性和鲁棒性。

Abstract: Existing Bayesian Optimization (BO) methods typically balance exploration and
exploitation to optimize costly objective functions. However, these methods
often suffer from a significant one-step bias, which may lead to convergence
towards local optima and poor performance in complex or high-dimensional tasks.
Recently, Black-Box Optimization (BBO) has achieved success across various
scientific and engineering domains, particularly when function evaluations are
costly and gradients are unavailable. Motivated by this, we propose the
Reinforced Energy-Based Model for Bayesian Optimization (REBMBO), which
integrates Gaussian Processes (GP) for local guidance with an Energy-Based
Model (EBM) to capture global structural information. Notably, we define each
Bayesian Optimization iteration as a Markov Decision Process (MDP) and use
Proximal Policy Optimization (PPO) for adaptive multi-step lookahead,
dynamically adjusting the depth and direction of exploration to effectively
overcome the limitations of traditional BO methods. We conduct extensive
experiments on synthetic and real-world benchmarks, confirming the superior
performance of REBMBO. Additional analyses across various GP configurations
further highlight its adaptability and robustness.

</details>


### [129] [The Confusing Instance Principle for Online Linear Quadratic Control](https://arxiv.org/abs/2510.19531)
*Waris Radji,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: 使用基于模型的强化学习重新审视未知动态下二次成本线性系统控制问题，提出MED - LQ策略，在控制套件基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统基于多臂老虎机的控制方法存在实际局限性，需要新的方法。

Method: 基于混淆实例（CI）原则，结合LQR策略结构、敏感性和稳定性分析，开发MED - LQ控制策略。

Result: 在综合控制套件基准测试中，MED - LQ在各种场景下取得有竞争力的性能。

Conclusion: MED - LQ扩展了CI和MED原则，有潜力应用于大规模马尔可夫决策过程。

Abstract: We revisit the problem of controlling linear systems with quadratic cost
under unknown dynamics with model-based reinforcement learning. Traditional
methods like Optimism in the Face of Uncertainty and Thompson Sampling, rooted
in multi-armed bandits (MABs), face practical limitations. In contrast, we
propose an alternative based on the Confusing Instance (CI) principle, which
underpins regret lower bounds in MABs and discrete Markov Decision Processes
(MDPs) and is central to the Minimum Empirical Divergence (MED) family of
algorithms, known for their asymptotic optimality in various settings. By
leveraging the structure of LQR policies along with sensitivity and stability
analysis, we develop MED-LQ. This novel control strategy extends the principles
of CI and MED beyond small-scale settings. Our benchmarks on a comprehensive
control suite demonstrate that MED-LQ achieves competitive performance in
various scenarios while highlighting its potential for broader applications in
large-scale MDPs.

</details>


### [130] [Insights into the Unknown: Federated Data Diversity Analysis on Molecular Data](https://arxiv.org/abs/2510.19535)
*Markus Bujotzek,Evelyn Trautmann,Calum Hand,Ian Hales*

Main category: cs.LG

TL;DR: 研究联邦聚类方法对分布式分子数据的处理能力，通过基准测试和可解释性分析，强调结合化学信息指标和客户端可解释性分析的重要性。


<details>
  <summary>Details</summary>
Motivation: AI方法在药物发现中应用受限，因依赖公共数据集，联邦学习虽有潜力但存在数据相关任务难题，需研究联邦聚类方法对分布式分子数据的处理。

Method: 对三种联邦聚类方法（Fed - kMeans、Fed - PCA+Fed - kMeans、Fed - LSH）在八个不同分子数据集上与集中式方法进行基准测试，引入化学信息评估指标SF - ICF。

Result: 通过大规模基准测试和深入可解释性分析得出相关结果。

Conclusion: 在分子数据的联邦多样性分析中，结合化学信息指标和客户端可解释性分析很重要。

Abstract: AI methods are increasingly shaping pharmaceutical drug discovery. However,
their translation to industrial applications remains limited due to their
reliance on public datasets, lacking scale and diversity of proprietary
pharmaceutical data. Federated learning (FL) offers a promising approach to
integrate private data into privacy-preserving, collaborative model training
across data silos. This federated data access complicates important
data-centric tasks such as estimating dataset diversity, performing informed
data splits, and understanding the structure of the combined chemical space. To
address this gap, we investigate how well federated clustering methods can
disentangle and represent distributed molecular data. We benchmark three
approaches, Federated kMeans (Fed-kMeans), Federated Principal Component
Analysis combined with Fed-kMeans (Fed-PCA+Fed-kMeans), and Federated
Locality-Sensitive Hashing (Fed-LSH), against their centralized counterparts on
eight diverse molecular datasets. Our evaluation utilizes both, standard
mathematical and a chemistry-informed evaluation metrics, SF-ICF, that we
introduce in this work. The large-scale benchmarking combined with an in-depth
explainability analysis shows the importance of incorporating domain knowledge
through chemistry-informed metrics, and on-client explainability analyses for
federated diversity analysis on molecular data.

</details>


### [131] [A Climate-Aware Deep Learning Framework for Generalizable Epidemic Forecasting](https://arxiv.org/abs/2510.19611)
*Jinpyo Hong,Rachel E. Baker*

Main category: cs.LG

TL;DR: 本文提出ForecastNet - XCL深度学习混合框架，基于气候和时间数据提前100周准确预测RSV疫情，在多场景下表现出色，是可部署的预警工具。


<details>
  <summary>Details</summary>
Motivation: 精准预测传染病爆发对公共卫生响应和疫情控制至关重要，机器学习用于时间序列预测有潜力，但用于预测地方性疾病研究不足。

Method: 提出ForecastNet - XCL深度学习混合框架，结合高分辨率特征学习与长程时间依赖捕获机制，有自回归模块，通过随机推断给出概率区间。

Result: 在34个美国州评估中，ForecastNet - XCL在州内和跨州场景均优于统计基线、单个神经网络和传统集成方法，长时间预测保持准确性，在不同气候数据集训练提升泛化能力。

Conclusion: ForecastNet - XCL效率高、性能好且考虑不确定性，可在气候压力增大和监测资源受限情况下作为预警工具部署。

Abstract: Precise outbreak forecasting of infectious diseases is essential for
effective public health responses and epidemic control. The increased
availability of machine learning (ML) methods for time-series forecasting
presents an enticing avenue to enhance outbreak forecasting. Though the
COVID-19 outbreak demonstrated the value of applying ML models to predict
epidemic profiles, using ML models to forecast endemic diseases remains
underexplored. In this work, we present ForecastNet-XCL (an ensemble model
based on XGBoost+CNN+BiLSTM), a deep learning hybrid framework designed to
addresses this gap by creating accurate multi-week RSV forecasts up to 100
weeks in advance based on climate and temporal data, without access to
real-time surveillance on RSV. The framework combines high-resolution feature
learning with long-range temporal dependency capturing mechanisms, bolstered by
an autoregressive module trained on climate-controlled lagged relations.
Stochastic inference returns probabilistic intervals to inform decision-making.
Evaluated across 34 U.S. states, ForecastNet-XCL reliably outperformed
statistical baselines, individual neural nets, and conventional ensemble
methods in both within- and cross-state scenarios, sustaining accuracy over
extended forecast horizons. Training on climatologically diverse datasets
enhanced generalization furthermore, particularly in locations having irregular
or biennial RSV patterns. ForecastNet-XCL's efficiency, performance, and
uncertainty-aware design make it a deployable early-warning tool amid
escalating climate pressures and constrained surveillance resources.

</details>


### [132] [Learning and Simulating Building Evacuation Patterns for Enhanced Safety Design Using Generative Models](https://arxiv.org/abs/2510.19623)
*Jin Han,Zhe Zheng,Yi Gu,Jia-Rui Lin,Xin-Zheng Lu*

Main category: cs.LG

TL;DR: 提出DiffEvac方法用于高效疏散模拟和安全设计，对比现有研究有显著提升，对建筑设计有积极意义。


<details>
  <summary>Details</summary>
Motivation: 传统疏散模拟依赖精细建模和大量参数，难以在早期设计阶段快速迭代，需要新方法。

Method: 建立399个建筑布局和疏散热图数据集，提出解耦特征表示用于生成模型，提出基于图像提示的扩散模型学习疏散模式。

Result: 与使用条件生成对抗网络和RGB表示的现有研究相比，SSIM提升37.6%，PSNR提升142%，速度快16倍，模拟时间减至2分钟。

Conclusion: 该方法能增强快速设计迭代和调整过程，为智能建筑安全优化提供新见解和途径，降低建模负担，便于大规模探索和与多目标设计工具耦合。

Abstract: Evacuation simulation is essential for building safety design, ensuring
properly planned evacuation routes. However, traditional evacuation simulation
relies heavily on refined modeling with extensive parameters, making it
challenging to adopt such methods in a rapid iteration process in early design
stages. Thus, this study proposes DiffEvac, a novel method to learn building
evacuation patterns based on Generative Models (GMs), for efficient evacuation
simulation and enhanced safety design. Initially, a dataset of 399 diverse
functional layouts and corresponding evacuation heatmaps of buildings was
established. Then, a decoupled feature representation is proposed to embed
physical features like layouts and occupant density for GMs. Finally, a
diffusion model based on image prompts is proposed to learn evacuation patterns
from simulated evacuation heatmaps. Compared to existing research using
Conditional GANs with RGB representation, DiffEvac achieves up to a 37.6%
improvement in SSIM, 142% in PSNR, and delivers results 16 times faster,
thereby cutting simulation time to 2 minutes. Case studies further demonstrate
that the proposed method not only significantly enhances the rapid design
iteration and adjustment process with efficient evacuation simulation but also
offers new insights and technical pathways for future safety optimization in
intelligent building design. The research implication is that the approach
lowers the modeling burden, enables large-scale what-if exploration, and
facilitates coupling with multi-objective design tools.

</details>


### [133] [Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them](https://arxiv.org/abs/2510.19634)
*Hrittik Roy,Søren Hauberg,Nicholas Krämer*

Main category: cs.LG

TL;DR: 本文指出最小二乘法在现代机器学习中有未发掘的潜力，通过推导自定义梯度将求解器变为可微算子，进行多项实证，推动可微线性代数工具发展。


<details>
  <summary>Details</summary>
Motivation: 释放最小二乘法在现代机器学习中未被充分挖掘的潜力，使其应用超越线性模型拟合。

Method: 推导自定义梯度，将求解器转变为可微算子。

Result: 实现5000万参数模型的权重稀疏性以证明可扩展性；在基于分数的生成模型中施加保守性约束；基于预测性能对高斯过程进行超参数调整。

Conclusion: 本工作推动了可微线性代数工具的发展，使其更易被机器学习从业者使用。

Abstract: This paper argues that the method of least squares has significant
unfulfilled potential in modern machine learning, far beyond merely being a
tool for fitting linear models. To release its potential, we derive custom
gradients that transform the solver into a differentiable operator, like a
neural network layer, enabling many diverse applications. Empirically, we
demonstrate: (i) scalability by enforcing weight sparsity on a 50 million
parameter model; (ii) imposing conservativeness constraints in score-based
generative models; and (iii) hyperparameter tuning of Gaussian processes based
on predictive performance. By doing this, our work represents the next
iteration in developing differentiable linear-algebra tools and making them
widely accessible to machine learning practitioners.

</details>


### [134] [Latent Space Factorization in LoRA](https://arxiv.org/abs/2510.19640)
*Shashi Kumar,Yacouba Kaloga,John Mitros,Petr Motlicek,Ina Kodrasi*

Main category: cs.LG

TL;DR: 提出FVAE - LoRA方法，在多任务上优于标准LoRA，且对任务相关信号分离更好，鲁棒性更强。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA变体缺乏在低秩子空间中明确消除任务相关信息歧义的机制，可能限制下游性能。

Method: 提出Factorized Variational Autoencoder LoRA (FVAE - LoRA)，利用VAE学习两个不同的潜在空间，通过新的证据下界公式促进潜在空间分解。

Result: 在文本、音频和图像任务的大量实验中，FVAE - LoRA始终优于标准LoRA；虚假相关性评估表明其能更好地分离任务相关信号。

Conclusion: FVAE - LoRA在多任务上表现出色，能更好地分离任务相关信号，在分布变化下有更好的鲁棒性，代码公开。

Abstract: Low-rank adaptation (LoRA) is a widely used method for parameter-efficient
finetuning. However, existing LoRA variants lack mechanisms to explicitly
disambiguate task-relevant information within the learned low-rank subspace,
potentially limiting downstream performance. We propose Factorized Variational
Autoencoder LoRA (FVAE-LoRA), which leverages a VAE to learn two distinct
latent spaces. Our novel Evidence Lower Bound formulation explicitly promotes
factorization between the latent spaces, dedicating one latent space to
task-salient features and the other to residual information. Extensive
experiments on text, audio, and image tasks demonstrate that FVAE-LoRA
consistently outperforms standard LoRA. Moreover, spurious correlation
evaluations confirm that FVAE-LoRA better isolates task-relevant signals,
leading to improved robustness under distribution shifts. Our code is publicly
available at: https://github.com/idiap/FVAE-LoRA

</details>


### [135] [Overlap-weighted orthogonal meta-learner for treatment effect estimation over time](https://arxiv.org/abs/2510.19643)
*Konstantin Hess,Dennis Frauen,Mihaela van der Schaar,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出重叠加权正交（WO）元学习器估计时变环境下的异质治疗效果（HTEs），方法具有Neyman正交性、模型无关性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 时变环境下估计HTEs存在严重重叠问题，现有元学习器在重叠低时估计方差大。

Method: 开发最小化重叠加权oracle风险的Neyman正交总体风险函数，提出WO-元学习器。

Result: WO-学习器具有Neyman正交性，对扰动函数的错误指定具有鲁棒性，且完全模型无关。

Conclusion: 所提出的WO-学习器能抵消现有元学习器的不稳定性，获得更可靠的HTE估计。

Abstract: Estimating heterogeneous treatment effects (HTEs) in time-varying settings is
particularly challenging, as the probability of observing certain treatment
sequences decreases exponentially with longer prediction horizons. Thus, the
observed data contain little support for many plausible treatment sequences,
which creates severe overlap problems. Existing meta-learners for the
time-varying setting typically assume adequate treatment overlap, and thus
suffer from exploding estimation variance when the overlap is low. To address
this problem, we introduce a novel overlap-weighted orthogonal (WO)
meta-learner for estimating HTEs that targets regions in the observed data with
high probability of receiving the interventional treatment sequences. This
offers a fully data-driven approach through which our WO-learner can counteract
instabilities as in existing meta-learners and thus obtain more reliable HTE
estimates. Methodologically, we develop a novel Neyman-orthogonal population
risk function that minimizes the overlap-weighted oracle risk. We show that our
WO-learner has the favorable property of Neyman-orthogonality, meaning that it
is robust against misspecification in the nuisance functions. Further, our
WO-learner is fully model-agnostic and can be applied to any machine learning
model. Through extensive experiments with both transformer and LSTM backbones,
we demonstrate the benefits of our novel WO-learner.

</details>


### [136] [Study of Training Dynamics for Memory-Constrained Fine-Tuning](https://arxiv.org/abs/2510.19675)
*Aël Quélennec,Nour Hezbri,Pavlo Mozharovskyi,Van-Tam Nguyen,Enzo Tartaglione*

Main category: cs.LG

TL;DR: 提出TraDy提高深度神经网络训练内存效率，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 模型增大且部署环境资源受限，需提高深度神经网络训练的内存效率。

Method: 提出TraDy转移学习方案，利用层更新重要性与架构相关且可预先确定、动态随机通道选择优于静态方法的特点，采用动态通道选择方法在预选择层的各轮次间随机重采样通道。

Result: TraDy在多种下游任务和架构中达到了最先进性能，保持严格内存约束，实现高达99%的激活稀疏性、95%的权重导数稀疏性和97%的权重导数计算FLOPs减少。

Conclusion: TraDy是一种有效的提高深度神经网络训练内存效率的方案。

Abstract: Memory-efficient training of deep neural networks has become increasingly
important as models grow larger while deployment environments impose strict
resource constraints. We propose TraDy, a novel transfer learning scheme
leveraging two key insights: layer importance for updates is
architecture-dependent and determinable a priori, while dynamic stochastic
channel selection provides superior gradient approximation compared to static
approaches. We introduce a dynamic channel selection approach that
stochastically resamples channels between epochs within preselected layers.
Extensive experiments demonstrate TraDy achieves state-of-the-art performance
across various downstream tasks and architectures while maintaining strict
memory constraints, achieving up to 99% activation sparsity, 95% weight
derivative sparsity, and 97% reduction in FLOPs for weight derivative
computation.

</details>


### [137] [Fast Inference via Hierarchical Speculative Decoding](https://arxiv.org/abs/2510.19705)
*Amir Globerson,Haim Kaplan,Yishay Mansour,Clara Mohri,Tal Schuster*

Main category: cs.LG

TL;DR: 提出分层推测解码算法 HSD 减少文本生成延迟，理论推导与实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码在选择草稿模型时有多种可能，需更好方法减少文本生成延迟。

Method: 提出 HSD 算法，将草稿模型分层，各模型提出令牌并由下一个更大模型验证，直至目标模型。推导预期延迟表达式，可在多项式时间内选择最优延迟层次。

Result: HSD 比最佳单草稿基线最多提速 1.2 倍。

Conclusion: HSD 算法能有效减少生成延迟，优于先前技术。

Abstract: Transformer language models generate text autoregressively, making inference
latency proportional to the number of tokens generated. Speculative decoding
reduces this latency without sacrificing output quality, by leveraging a small
draft model to propose tokens that the larger target model verifies in
parallel. In practice, however, there may exist a set of potential draft
models- ranging from faster but less inaccurate, to slower yet more reliable.
We introduce Hierarchical Speculative Decoding (HSD), an algorithm that stacks
these draft models into a hierarchy, where each model proposes tokens, and the
next larger model verifies them in a single forward pass, until finally the
target model verifies tokens. We derive an expression for the expected latency
of any such hierarchy and show that selecting the latency-optimal hierarchy can
be done in polynomial time. Empirically, HSD gives up to 1.2x speed-up over the
best single-draft baseline, demonstrating the practicality of our algorithm in
reducing generation latency beyond previous techniques.

</details>


### [138] [SEMPO: Lightweight Foundation Models for Time Series Forecasting](https://arxiv.org/abs/2510.19710)
*Hui He,Kun Yi,Yuanchi Ma,Qi Zhang,Zhendong Niu,Guansong Pang*

Main category: cs.LG

TL;DR: 提出轻量级时间序列基础模型SEMPO，减少预训练数据规模和模型大小，在预测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型网络架构庞大，需大规模预训练数据，难以在资源受限环境部署，要解决通用性和可承受性的矛盾。

Method: SEMPO包含能量感知频谱分解模块和混合提示Transformer模块，前者利用高低能量频率信号，后者通过小数据集特定提示学习模式。

Result: 在两个大规模基准测试的16个数据集上实验，SEMPO在零样本和少样本预测场景中优于现有方法。

Conclusion: SEMPO能显著减少预训练数据规模和模型大小，同时实现强泛化能力。

Abstract: The recent boom of large pre-trained models witnesses remarkable success in
developing foundation models (FMs) for time series forecasting. Despite
impressive performance across diverse downstream forecasting tasks, existing
time series FMs possess massive network architectures and require substantial
pre-training on large-scale datasets, which significantly hinders their
deployment in resource-constrained environments. In response to this growing
tension between versatility and affordability, we propose SEMPO, a novel
lightweight foundation model that requires pretraining on relatively
small-scale data, yet exhibits strong general time series forecasting.
Concretely, SEMPO comprises two key modules: 1) energy-aware SpEctral
decomposition module, that substantially improves the utilization of
pre-training data by modeling not only the high-energy frequency signals but
also the low-energy yet informative frequency signals that are ignored in
current methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learns
heterogeneous temporal patterns through small dataset-specific prompts and
adaptively routes time series tokens to prompt-based experts for
parameter-efficient model adaptation across different datasets and domains.
Equipped with these modules, SEMPO significantly reduces both pre-training data
scale and model size, while achieving strong generalization. Extensive
experiments on two large-scale benchmarks covering 16 datasets demonstrate the
superior performance of SEMPO in both zero-shot and few-shot forecasting
scenarios compared with state-of-the-art methods. Code and data are available
at https://github.com/mala-lab/SEMPO.

</details>


### [139] [Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series](https://arxiv.org/abs/2510.19728)
*Mahmoud Ibrahim,Bart Elen,Chang Sun,Gökhan Ertaylan,Michel Dumontier*

Main category: cs.LG

TL;DR: 提出利用合成ICU时间序列数据的框架，引入Enhanced TimeAutoDiff模型，经基准测试效果好，为重症监护模型评估提供隐私保护方案。


<details>
  <summary>Details</summary>
Motivation: 构建可严格且可信地评估预测模型的框架，在总体和细粒度人口亚组层面评估模型。

Method: 基于先前扩散和VAE生成器，引入Enhanced TimeAutoDiff，在潜在扩散目标中增加分布对齐惩罚，并在MIMIC - III和eICU上进行基准测试。

Result: Enhanced TimeAutoDiff减少TRTS差距超70%，合成大样本降低亚组AUROC估计误差，在多数亚组表现更好。

Conclusion: 该工作为重症监护中可信、细粒度的模型评估提供实用、隐私保护方案，提升医疗AI可信度。

Abstract: We present a novel framework for leveraging synthetic ICU time-series data
not only to train but also to rigorously and trustworthily evaluate predictive
models, both at the population level and within fine-grained demographic
subgroups. Building on prior diffusion and VAE-based generators (TimeDiff,
HealthGen, TimeAutoDiff), we introduce \textit{Enhanced TimeAutoDiff}, which
augments the latent diffusion objective with distribution-alignment penalties.
We extensively benchmark all models on MIMIC-III and eICU, on 24-hour mortality
and binary length-of-stay tasks. Our results show that Enhanced TimeAutoDiff
reduces the gap between real-on-synthetic and real-on-real evaluation (``TRTS
gap'') by over 70\%, achieving $\Delta_{TRTS} \leq 0.014$ AUROC, while
preserving training utility ($\Delta_{TSTR} \approx 0.01$). Crucially, for 32
intersectional subgroups, large synthetic cohorts cut subgroup-level AUROC
estimation error by up to 50\% relative to small real test sets, and outperform
them in 72--84\% of subgroups. This work provides a practical,
privacy-preserving roadmap for trustworthy, granular model evaluation in
critical care, enabling robust and reliable performance analysis across diverse
patient populations without exposing sensitive EHR data, contributing to the
overall trustworthiness of Medical AI.

</details>


### [140] [BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models](https://arxiv.org/abs/2510.19749)
*Catherine Villeneuve,Benjamin Akera,Mélisande Teng,David Rolnick*

Main category: cs.LG

TL;DR: 本文从贝叶斯角度重新审视深度物种分布模型，提出BATIS框架，在新数据集上测试不确定性量化方法，证明贝叶斯深度学习可提升数据稀缺地区模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习物种分布模型受数据空间偏差限制，有效性不足，需提升其在数据稀缺地区的可靠性。

Method: 从贝叶斯角度重新审视深度物种分布模型，引入BATIS框架，用有限观测数据迭代更新先验预测，在含公民科学观测的新数据集上对多种不确定性量化方法进行基准测试。

Result: 贝叶斯深度学习方法能大幅提高数据稀缺地区物种分布模型的可靠性。

Conclusion: 贝叶斯深度学习方法有助于提升物种分布模型可靠性，可促进生态理解和保护工作。

Abstract: Species distribution models (SDMs), which aim to predict species occurrence
based on environmental variables, are widely used to monitor and respond to
biodiversity change. Recent deep learning advances for SDMs have been shown to
perform well on complex and heterogeneous datasets, but their effectiveness
remains limited by spatial biases in the data. In this paper, we revisit deep
SDMs from a Bayesian perspective and introduce BATIS, a novel and practical
framework wherein prior predictions are updated iteratively using limited
observational data. Models must appropriately capture both aleatoric and
epistemic uncertainty to effectively combine fine-grained local insights with
broader ecological patterns. We benchmark an extensive set of uncertainty
quantification approaches on a novel dataset including citizen science
observations from the eBird platform. Our empirical study shows how Bayesian
deep learning approaches can greatly improve the reliability of SDMs in
data-scarce locations, which can contribute to ecological understanding and
conservation efforts.

</details>


### [141] [When Do Transformers Learn Heuristics for Graph Connectivity?](https://arxiv.org/abs/2510.19753)
*Qilin Ye,Deqing Fu,Robin Jia,Vatsal Sharan*

Main category: cs.LG

TL;DR: 本文以图连通性为测试平台，理论与实证结合解释Transformer难以学习通用算法的现象，发现限制训练数据在模型容量内可使Transformer学习精确算法。


<details>
  <summary>Details</summary>
Motivation: 解释Transformer难以学习通用算法，依赖脆弱启发式方法的现象。

Method: 以简化的Transformer架构（解缠Transformer）为研究对象，理论证明其解决图连通性问题的能力，分析训练动态。

Result: L层模型能解决直径达3^L的图连通性问题；容量内图促使学习正确算法，超容量图促使学习基于节点度的简单启发式方法。

Conclusion: 限制训练数据在模型容量内，标准和解缠Transformer都能学习精确算法而非基于度的启发式方法。

Abstract: Transformers often fail to learn generalizable algorithms, instead relying on
brittle heuristics. Using graph connectivity as a testbed, we explain this
phenomenon both theoretically and empirically. We consider a simplified
Transformer architecture, the disentangled Transformer, and prove that an
$L$-layer model has capacity to solve for graphs with diameters up to exactly
$3^L$, implementing an algorithm equivalent to computing powers of the
adjacency matrix. We analyze the training-dynamics, and show that the learned
strategy hinges on whether most training instances are within this model
capacity. Within-capacity graphs (diameter $\leq 3^L$) drive the learning of a
correct algorithmic solution while beyond-capacity graphs drive the learning of
a simple heuristic based on node degrees. Finally, we empirically demonstrate
that restricting training data within a model's capacity leads to both standard
and disentangled transformers learning the exact algorithm rather than the
degree-based heuristic.

</details>


### [142] [CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees](https://arxiv.org/abs/2510.19754)
*Aman Bilkhoo,Milad Kazemi,Nicola Paoletti,Mehran Hosseini*

Main category: cs.LG

TL;DR: 提出CONFEX方法生成不确定性感知的反事实解释，评估显示该方法能产生可靠且合理的解释。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法常忽略不确定性或缺乏有效机制结合不确定性，CONFEX旨在解决此问题。

Method: 使用Conformal Prediction (CP) 和Mixed-Integer Linear Programming (MILP)，开发新的局部化CP程序，利用输入空间的离线树状分区实现高效MILP编码。

Result: 在不同基准和指标上评估，不确定性感知方法产生了稳健且合理的解释。

Conclusion: CONFEX能为反事实解释提供预测不确定性和最优性的严格保证。

Abstract: Counterfactual explanations (CFXs) provide human-understandable
justifications for model predictions, enabling actionable recourse and
enhancing interpretability. To be reliable, CFXs must avoid regions of high
predictive uncertainty, where explanations may be misleading or inapplicable.
However, existing methods often neglect uncertainty or lack principled
mechanisms for incorporating it with formal guarantees. We propose CONFEX, a
novel method for generating uncertainty-aware counterfactual explanations using
Conformal Prediction (CP) and Mixed-Integer Linear Programming (MILP). CONFEX
explanations are designed to provide local coverage guarantees, addressing the
issue that CFX generation violates exchangeability. To do so, we develop a
novel localised CP procedure that enjoys an efficient MILP encoding by
leveraging an offline tree-based partitioning of the input space. This way,
CONFEX generates CFXs with rigorous guarantees on both predictive uncertainty
and optimality. We evaluate CONFEX against state-of-the-art methods across
diverse benchmarks and metrics, demonstrating that our uncertainty-aware
approach yields robust and plausible explanations.

</details>


### [143] [A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation](https://arxiv.org/abs/2510.19755)
*Jiacheng Liu,Xinyu Wang,Yuqi Lin,Zhikai Wang,Peiru Wang,Peiliang Cai,Qinming Zhou,Zhengan Yan,Zexuan Yan,Zhengyi Shi,Chang Zou,Yue Ma,Linfeng Zhang*

Main category: cs.LG

TL;DR: 扩散模型计算开销大、生成延迟高，扩散缓存提供无训练、与架构无关的高效推理范式，本文综述其理论基础与演变并提出统一框架，表明其从静态重用向动态预测发展，将助力实时高效生成式AI。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因多步迭代和复杂骨干网络导致计算开销大、生成延迟高，现有加速技术有局限性，需新的高效推理范式。

Method: 系统综述扩散缓存的理论基础与演变，提出统一框架进行分类和分析，对比代表性方法。

Result: 扩散缓存从静态重用向动态预测演变，能增强缓存灵活性，可与其他加速技术集成。

Conclusion: 扩散缓存范式将成为实时高效生成式AI的关键推动因素，为高效生成智能的理论和实践注入新活力。

Abstract: Diffusion Models have become a cornerstone of modern generative AI for their
exceptional generation quality and controllability. However, their inherent
\textit{multi-step iterations} and \textit{complex backbone networks} lead to
prohibitive computational overhead and generation latency, forming a major
bottleneck for real-time applications. Although existing acceleration
techniques have made progress, they still face challenges such as limited
applicability, high training costs, or quality degradation.
  Against this backdrop, \textbf{Diffusion Caching} offers a promising
training-free, architecture-agnostic, and efficient inference paradigm. Its
core mechanism identifies and reuses intrinsic computational redundancies in
the diffusion process. By enabling feature-level cross-step reuse and
inter-layer scheduling, it reduces computation without modifying model
parameters. This paper systematically reviews the theoretical foundations and
evolution of Diffusion Caching and proposes a unified framework for its
classification and analysis.
  Through comparative analysis of representative methods, we show that
Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic
prediction}. This trend enhances caching flexibility across diverse tasks and
enables integration with other acceleration techniques such as sampling
optimization and model distillation, paving the way for a unified, efficient
inference framework for future multimodal and interactive applications. We
argue that this paradigm will become a key enabler of real-time and efficient
generative AI, injecting new vitality into both theory and practice of
\textit{Efficient Generative Intelligence}.

</details>


### [144] [The Tail Tells All: Estimating Model-Level Membership Inference Vulnerability Without Reference Models](https://arxiv.org/abs/2510.19773)
*Euodia Dodd,Nataša Krčo,Igor Shilov,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: 提出无需参考模型评估模型对成员推理攻击脆弱性的新方法，经多架构和数据集验证有效，还探索非线性函数评估大语言模型风险。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击评估隐私风险需训练大量计算成本高的参考模型，实用性受限。

Method: 利用训练和测试分布估计模型脆弱性，以高损失区域无异常值为风险预测指标。

Result: 准确估计对SOTA MIA攻击（LiRA）的模型脆弱性，优于低成本攻击和其他分布差异度量方法。

Conclusion: 该方法有效，用非线性函数评估大语言模型风险有前景。

Abstract: Membership inference attacks (MIAs) have emerged as the standard tool for
evaluating the privacy risks of AI models. However, state-of-the-art attacks
require training numerous, often computationally expensive, reference models,
limiting their practicality. We present a novel approach for estimating
model-level vulnerability, the TPR at low FPR, to membership inference attacks
without requiring reference models. Empirical analysis shows loss distributions
to be asymmetric and heavy-tailed and suggests that most points at risk from
MIAs have moved from the tail (high-loss region) to the head (low-loss region)
of the distribution after training. We leverage this insight to propose a
method to estimate model-level vulnerability from the training and testing
distribution alone: using the absence of outliers from the high-loss region as
a predictor of the risk. We evaluate our method, the TNR of a simple loss
attack, across a wide range of architectures and datasets and show it to
accurately estimate model-level vulnerability to the SOTA MIA attack (LiRA). We
also show our method to outperform both low-cost (few reference models) attacks
such as RMIA and other measures of distribution difference. We finally evaluate
the use of non-linear functions to evaluate risk and show the approach to be
promising to evaluate the risk in large-language models.

</details>


### [145] [GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters](https://arxiv.org/abs/2510.19778)
*Anand Choudhary,Yasser Sulaıman,Lukas Mauch,Ghouthi Boukli Hacene,Fabien Cardinaux,Antoine Bosselut*

Main category: cs.LG

TL;DR: 提出名为GaLLoP的稀疏微调技术，在LLaMA3 8B和Gemma 2B模型实验中表现优于其他微调技术，能缓解灾难性遗忘等问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏微调技术效果依赖于参数选择，需找到更优的稀疏微调方法。

Method: 引入GaLLoP技术，只微调下游任务梯度幅度大且预训练幅度小的模型参数。

Result: 在LLaMA3 8B和Gemma 2B实验中，GaLLoP在分布内和分布外性能上优于或等同于其他微调技术。

Conclusion: GaLLoP能缓解灾难性遗忘和数据记忆问题，性能稳定，泛化能力强。

Abstract: Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a
sparse subset of model parameters. However, the effectiveness of sparse
adaptation depends on optimally selecting the model parameters to be
fine-tuned. In this work, we introduce a novel sparse fine-tuning technique
named GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which
fine-tunes only those model parameters which have the largest gradient
magnitudes on downstream tasks and the smallest pre-trained magnitudes,
intuitively prioritizing parameters that are highly task-relevant, but
minimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3
8B and Gemma 2B as base models shows that GaLLoP consistently improves or
matches the in-distribution as well as out-of-distribution performance obtained
via the usage of other leading parameter-efficient fine-tuning techniques,
including LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates
catastrophic forgetting and memorization of task data, as important pre-trained
parameters remain unchanged, and stabilizes performance relative to other
fine-tuning techniques, robustly generalizing across most random seeds.

</details>


### [146] [Environment Inference for Learning Generalizable Dynamical System](https://arxiv.org/abs/2510.19784)
*Shixuan Liu,Yue He,Haotian Wang,Wenjing Yang,Yunfei Wang,Peng Cui,Zhong Liu*

Main category: cs.LG

TL;DR: 提出DynaInfer方法解决无环境标签下的环境分配问题，实验显示其优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法依赖独立同分布数据，现有泛化技术依赖环境标签，但训练时标签常不可用。

Method: 提出DynaInfer方法，通过分析每轮训练中固定神经网络的预测误差来推断环境规格，直接从数据进行环境分配。

Result: DynaInfer优于现有环境分配技术，能快速收敛到真实标签，有标签时表现也更优。

Conclusion: DynaInfer可有效解决无标签场景下的交替优化问题。

Abstract: Data-driven methods offer efficient and robust solutions for analyzing
complex dynamical systems but rely on the assumption of I.I.D. data, driving
the development of generalization techniques for handling environmental
differences. These techniques, however, are limited by their dependence on
environment labels, which are often unavailable during training due to data
acquisition challenges, privacy concerns, and environmental variability,
particularly in large public datasets and privacy-sensitive domains. In
response, we propose DynaInfer, a novel method that infers environment
specifications by analyzing prediction errors from fixed neural networks within
each training round, enabling environment assignments directly from data. We
prove our algorithm effectively solves the alternating optimization problem in
unlabeled scenarios and validate it through extensive experiments across
diverse dynamical systems. Results show that DynaInfer outperforms existing
environment assignment techniques, converges rapidly to true labels, and even
achieves superior performance when environment labels are available.

</details>


### [147] [Blackbox Model Provenance via Palimpsestic Membership Inference](https://arxiv.org/abs/2510.19796)
*Rohith Kuditipudi,Jing Huang,Sally Zhu,Diyi Yang,Christopher Potts,Percy Liang*

Main category: cs.LG

TL;DR: 研究Alice能否证明Bob使用其语言模型，通过独立性测试，利用模型对训练数据的记忆特性，在查询和观测两种场景测试，取得不同效果。


<details>
  <summary>Details</summary>
Motivation: 解决Alice能否证明Bob使用其语言模型的问题。

Method: 将问题转化为独立性测试问题，利用语言模型的记忆特性，在查询场景通过提示估计Bob模型对训练示例的似然，在观测场景采用两种方法。

Result: 查询场景中多数情况p值至多为1e - 8；观测场景中第二种方法几百个token可区分，第一种需几十万token。

Conclusion: 提出的方法可用于判断Bob是否使用Alice的模型，不同场景和方法有不同的效果和适用条件。

Abstract: Suppose Alice trains an open-weight language model and Bob uses a blackbox
derivative of Alice's model to produce text. Can Alice prove that Bob is using
her model, either by querying Bob's derivative model (query setting) or from
the text alone (observational setting)? We formulate this question as an
independence testing problem--in which the null hypothesis is that Bob's model
or text is independent of Alice's randomized training run--and investigate it
through the lens of palimpsestic memorization in language models: models are
more likely to memorize data seen later in training, so we can test whether Bob
is using Alice's model using test statistics that capture correlation between
Bob's model or text and the ordering of training examples in Alice's training
run. If Alice has randomly shuffled her training data, then any significant
correlation amounts to exactly quantifiable statistical evidence against the
null hypothesis, regardless of the composition of Alice's training data. In the
query setting, we directly estimate (via prompting) the likelihood Bob's model
gives to Alice's training examples and order; we correlate the likelihoods of
over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to
12B parameters with the base model's training data order, achieving a p-value
on the order of at most 1e-8 in all but six cases. In the observational
setting, we try two approaches based on estimating 1) the likelihood of Bob's
text overlapping with spans of Alice's training examples and 2) the likelihood
of Bob's text with respect to different versions of Alice's model we obtain by
repeating the last phase (e.g., 1%) of her training run on reshuffled data. The
second approach can reliably distinguish Bob's text from as little as a few
hundred tokens; the first does not involve any retraining but requires many
more tokens (several hundred thousand) to achieve high power.

</details>


### [148] [Transformers are almost optimal metalearners for linear classification](https://arxiv.org/abs/2510.19797)
*Roey Magen,Gal Vardi*

Main category: cs.LG

TL;DR: 本文首次理论分析表明，经梯度下降训练的简化transformer架构在线性分类场景可作近最优元学习器，所需训练任务和样本数量与环境维度无关。


<details>
  <summary>Details</summary>
Motivation: 探究transformers能否作为元学习器，仅用少量上下文示例适应新任务，现有理论分析大多未涉及正式元学习场景。

Method: 对经梯度下降训练的简化transformer架构进行理论分析，考虑一类自然任务，每个任务对应类条件高斯混合模型。

Result: transformer在训练足够多任务后，仅用$O(k / R^4)$上下文示例就能泛化到新任务，性能接近最优学习器，远超仅访问上下文数据的学习器。

Conclusion: 简化transformer架构可在线性分类场景作为近最优元学习器，所需训练任务和每个任务的示例数量与环境维度$d$无关。

Abstract: Transformers have demonstrated impressive in-context learning (ICL)
capabilities, raising the question of whether they can serve as metalearners
that adapt to new tasks using only a small number of in-context examples,
without any further training. While recent theoretical work has studied
transformers' ability to perform ICL, most of these analyses do not address the
formal metalearning setting, where the objective is to solve a collection of
related tasks more efficiently than would be possible by solving each task
individually. In this paper, we provide the first theoretical analysis showing
that a simplified transformer architecture trained via gradient descent can act
as a near-optimal metalearner in a linear classification setting. We consider a
natural family of tasks where each task corresponds to a class-conditional
Gaussian mixture model, with the mean vectors lying in a shared $k$-dimensional
subspace of $R^d$. After training on a sufficient number of such tasks, we show
that the transformer can generalize to a new task using only $O(k / R^4)$
in-context examples, where $R$ denotes the signal strength at test time. This
performance (almost) matches that of an optimal learner that knows exactly the
shared subspace and significantly outperforms any learner that only has access
to the in-context data, which requires $\Omega(d / R^4)$ examples to
generalize. Importantly, our bounds on the number of training tasks and
examples per task needed to achieve this result are independent of the ambient
dimension $d$.

</details>


### [149] [The Feasibility of Training Sovereign Language Models in the Global South: A Study of Brazil and Mexico](https://arxiv.org/abs/2510.19801)
*Sandra Malagon,Monica A. Ulloa Ruiz,Tatiana Elizabeth Sandoval Plaza,Gabriel Rafael Rosario Bolívar,Valentina García Mesa,Ivanna Alvarado Morales*

Main category: cs.LG

TL;DR: 本文研究巴西和墨西哥在硬件、能源和财政受限下训练主权级语言模型的可行性，发现财政可行性取决于硬件效率，延长训练时间可缓解硬件限制。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型训练计算需求激增加剧南北结构不对称，研究巴西和墨西哥在受限条件下训练主权级语言模型的技术和财政可行性。

Method: 采用双轴设计，改变加速器代际（NVIDIA H100 与 A100）和训练时长（90 天与 150 天），估算 10 万亿 token 模型训练的计算需求、能源消耗、资本支出和监管兼容性。

Result: 所有配置均低于出口管制和电力基础设施阈值，基于 H100 的场景总成本 800 - 1400 万美元可实现训练可行性，A100 部署因更高能源和硬件需求需 1900 - 3200 万美元。

Conclusion: 延长训练时间可作为缓解硬件限制的政策手段，使中等收入国家建立可持续且战略上充足的 AI 能力。

Abstract: The rapid escalation of computational requirements for training large-scale
language models has reinforced structural asymmetries between high-capacity
jurisdictions and countries in the Global South. This paper examines the
technical and fiscal feasibility of sovereign-scale language model training in
Brazil and Mexico under conditions of constrained hardware access, energy
availability, and fiscal ceilings. Using a dual-axis design that varies
accelerator generation (NVIDIA H100 vs. A100) and training duration (90 vs. 150
days), we estimate compute demand, energy consumption, capital expenditures,
and regulatory compatibility for the training of a 10-trillion-token model. Our
findings show that while all configurations remain below export-control and
electrical infrastructure thresholds, fiscal viability is determined by
hardware efficiency. H100-based scenarios achieve training feasibility at a
total cost of 8-14 million USD, while A100 deployments require 19-32 million
USD due to higher energy and hardware demand. We argue that extending training
timelines should be treated as a policy lever to mitigate hardware constraints,
enabling the production of usable, auditable, and locally aligned models
without competing at the global frontier. This study contributes to the
discourse on AI compute governance and technological sovereignty by
highlighting context-sensitive strategies that allow middle-income countries to
establish sustainable and strategically sufficient AI capabilities.

</details>


### [150] [Semantic World Models](https://arxiv.org/abs/2510.19818)
*Jacob Berg,Chuning Zhu,Yanda Bao,Ishan Durugkar,Abhishek Gupta*

Main category: cs.LG

TL;DR: 本文提出将世界模型建模为关于未来帧语义信息的视觉问答问题，训练视觉语言模型作为“语义”世界模型用于机器人规划，在开放式机器人任务上提升策略，泛化能力优于基于重建的方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于像素预测未来帧的世界模型规划方法，预测像素的目标与实际规划目标存在冲突，强像素重建不一定带来好的规划决策。

Method: 将世界建模视为关于未来帧语义信息的视觉问答问题，通过在图像 - 动作 - 文本数据上进行有监督微调，训练视觉语言模型作为“语义”世界模型。

Result: 所提出的语义世界模型可用于开放式机器人任务的策略改进，在泛化能力上显著优于基于重建的动作条件世界建模的典型范式。

Conclusion: 将世界模型建模为视觉问答问题来预测任务相关语义信息是有效的，基于此训练的语义世界模型能用于机器人规划决策，且继承了预训练视觉 - 语言模型的泛化和鲁棒性。

Abstract: Planning with world models offers a powerful paradigm for robotic control.
Conventional approaches train a model to predict future frames conditioned on
current frames and actions, which can then be used for planning. However, the
objective of predicting future pixels is often at odds with the actual planning
objective; strong pixel reconstruction does not always correlate with good
planning decisions. This paper posits that instead of reconstructing future
frames as pixels, world models only need to predict task-relevant semantic
information about the future. For such prediction the paper poses world
modeling as a visual question answering problem about semantic information in
future frames. This perspective allows world modeling to be approached with the
same tools underlying vision language models. Thus vision language models can
be trained as "semantic" world models through a supervised finetuning process
on image-action-text data, enabling planning for decision-making while
inheriting many of the generalization and robustness properties from the
pretrained vision-language models. The paper demonstrates how such a semantic
world model can be used for policy improvement on open-ended robotics tasks,
leading to significant generalization improvements over typical paradigms of
reconstruction-based action-conditional world modeling. Website available at
https://weirdlabuw.github.io/swm.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [151] [A flexible framework for structural plasticity in GPU-accelerated sparse spiking neural networks](https://arxiv.org/abs/2510.19764)
*James C. Knight,Johanna Senk,Thomas Nowotny*

Main category: cs.NE

TL;DR: 受生物大脑结构可塑性启发，提出新的GPU加速结构可塑性规则框架，用于训练稀疏SNN，减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习框架优化于密集连接，剪枝无法降低大型模型训练成本，而生物大脑结构可塑性很重要，因此提出新框架。

Method: 使用GeNN模拟器，提出新的GPU加速结构可塑性规则框架，用e - prop和DEEP R训练稀疏SNN分类器，在无监督学习中学习拓扑图。

Result: 稀疏分类器训练时间最多减少10倍，DEEP R重连使性能与原模型相当，能实现超实时模拟拓扑图形成。

Conclusion: 该框架有助于研究网络稀疏性及在神经形态应用中探索稀疏性计算优势。

Abstract: The majority of research in both training Artificial Neural Networks (ANNs)
and modeling learning in biological brains focuses on synaptic plasticity,
where learning equates to changing the strength of existing connections.
However, in biological brains, structural plasticity - where new connections
are created and others removed - is also vital, not only for effective learning
but also for recovery from damage and optimal resource usage. Inspired by
structural plasticity, pruning is often used in machine learning to remove weak
connections from trained models to reduce the computational requirements of
inference. However, the machine learning frameworks typically used for
backpropagation-based training of both ANNs and Spiking Neural Networks (SNNs)
are optimized for dense connectivity, meaning that pruning does not help reduce
the training costs of ever-larger models. The GeNN simulator already supports
efficient GPU-accelerated simulation of sparse SNNs for computational
neuroscience and machine learning. Here, we present a new flexible framework
for implementing GPU-accelerated structural plasticity rules and demonstrate
this first using the e-prop supervised learning rule and DEEP R to train
efficient, sparse SNN classifiers and then, in an unsupervised learning
context, to learn topographic maps. Compared to baseline dense models, our
sparse classifiers reduce training time by up to 10x while the DEEP R rewiring
enables them to perform as well as the original models. We demonstrate
topographic map formation in faster-than-realtime simulations, provide insights
into the connectivity evolution, and measure simulation speed versus network
size. The proposed framework will enable further research into achieving and
maintaining sparsity in network structure and neural communication, as well as
exploring the computational benefits of sparsity in a range of neuromorphic
applications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [152] [CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation](https://arxiv.org/abs/2510.18895)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: 提出CosmoCore架构，融合情感信号提升大语言模型代码生成能力，实验证明能减少幻觉代码、加速自我修正。


<details>
  <summary>Details</summary>
Motivation: 受人类和动物学习中错误带来的尴尬能促使快速修正的启发，提升大语言模型代码生成效果。

Method: 用轻量级多层感知器为代码生成轨迹标记效价和惊喜度，高负效价情节优先回放，低惊喜度成功情节修剪。

Result: 在代码生成基准测试中，减少48%幻觉代码，加速45%自我修正，本地实验验证效果。

Conclusion: 该框架拓展了基于人类反馈的强化学习，可用于更具情感意识的代码助手。

Abstract: We introduce CosmoCore, a neuroscience-inspired reinforcement learning (RL)
architecture that integrates affective signals to enhance code generation in
large language models (LLMs). Motivated by human and animal learning where
embarrassment from mistakes drives rapid correction, as observed in training a
puppy to avoid repeating errors after a single scolding CosmoCore tags code
generation trajectories with valence and surprise using a lightweight
multi-layer perceptron (MLP). High-negative valence (cringe) episodes, such as
buggy code outputs, are prioritized in a Dream Queue for five-fold replay
during off-policy updates, while low-surprise successes are pruned to prevent
overconfidence and buffer bloat. Evaluated on code generation benchmarks like
HumanEval and BigCodeBench, alongside simulations with a custom data pipeline
environment, CosmoCore reduces hallucinated code (e.g., syntax errors or
logical bugs) by 48\% and accelerates self-correction by 45\%. Local
experiments using Hugging Face models in a PySpark environment validate these
gains, with code snippets provided for replication. Ablations confirm valence
tagging boosts curiosity in exploration, and pruning mitigates inefficiency.
This framework extends RL from human feedback (RLHF) for more emotionally aware
code assistants, with applications in IDEs and data pipelines. Code and the
custom mini-world simulation are released.

</details>


### [153] [A Survey on Feedback Types in Automated Programming Assessment Systems](https://arxiv.org/abs/2510.18923)
*Eduard Frankford,Tobias Antensteiner,Michael Vierhauser,Clemens Sauerwein,Vivien Wallner,Iris Groher,Reinhold Plösch,Ruth Breu*

Main category: cs.SE

TL;DR: 随着各行业数字化发展，编程课程需求增加，自动编程评估系统（APAS）出现，但传统反馈机制有局限。研究对比不同反馈机制，发现单元测试反馈受学生认可，AI 生成反馈提升成绩，建议结合两者。


<details>
  <summary>Details</summary>
Motivation: 探究 APAS 中不同反馈机制受学生的认可程度及对解决问题的有效性。

Method: 对两所大学超 200 名学生开展大规模研究，对比编译器反馈、单元测试反馈和基于大语言模型的反馈。

Result: 学生认为单元测试反馈最有帮助，AI 生成反馈使学生成绩显著提高。

Conclusion: 结合单元测试和 AI 驱动的指导，可优化自动反馈机制，提高编程教育学习效果。

Abstract: With the recent rapid increase in digitization across all major industries,
acquiring programming skills has increased the demand for introductory
programming courses. This has further resulted in universities integrating
programming courses into a wide range of curricula, including not only
technical studies but also business and management fields of study.
  Consequently, additional resources are needed for teaching, grading, and
tutoring students with diverse educational backgrounds and skills. As part of
this, Automated Programming Assessment Systems (APASs) have emerged, providing
scalable and high-quality assessment systems with efficient evaluation and
instant feedback. Commonly, APASs heavily rely on predefined unit tests for
generating feedback, often limiting the scope and level of detail of feedback
that can be provided to students. With the rise of Large Language Models (LLMs)
in recent years, new opportunities have emerged as these technologies can
enhance feedback quality and personalization.
  To investigate how different feedback mechanisms in APASs are perceived by
students, and how effective they are in supporting problem-solving, we have
conducted a large-scale study with over 200 students from two different
universities. Specifically, we compare baseline Compiler Feedback, standard
Unit Test Feedback, and advanced LLM-based Feedback regarding perceived quality
and impact on student performance.
  Results indicate that while students rate unit test feedback as the most
helpful, AI-generated feedback leads to significantly better performances.
These findings suggest combining unit tests and AI-driven guidance to optimize
automated feedback mechanisms and improve learning outcomes in programming
education.

</details>


### [154] [Extending Resource Constrained Project Scheduling to Mega-Projects with Model-Based Systems Engineering & Hetero-functional Graph Theory](https://arxiv.org/abs/2510.19035)
*Amirreza Hosseini,Amro M. Farid*

Main category: cs.SE

TL;DR: 本文旨在将资源受限项目调度问题（RCPSP）与基于模型的系统工程（MBSE）和异功能图论（HFGT）相结合，构建转换管道并进行定量分析，新框架保留经典RCPSP优点，适应实际约束。


<details>
  <summary>Details</summary>
Motivation: RCPSP与MBSE文献脱节，限制其在复杂系统设计和管理中的集成，需进行融合。

Method: 构建从节点活动网络到SysML活动图再到操作数网的转换管道，将异功能网络最小成本流（HFNMCF）公式专门用于RCPSP上下文进行定量分析。

Result: 在具有可再生和不可再生操作数的示例中，专门的HFNMCF能产生类似调度，且对项目状态有明确解释，便于监控和控制。

Conclusion: 该框架保留经典RCPSP优点，能适应大型复杂项目的实际约束和企业级决策过程。

Abstract: Within the project management context, project scheduling serves as an
indispensable component, functioning as a fundamental tool for planning,
monitoring, controlling, and managing projects more broadly. Although the
resource-constrained project scheduling problem (RCPSP) lies at the core of
project management activities, it remains largely disconnected from the broader
literature on model-based systems engineering (MBSE), thereby limiting its
integration into the design and management of complex systems. The original
contribution of this paper is twofold. First, the paper seeks to reconcile the
RCPSP with the broader literature and vocabulary of model-based systems
engineering and hetero-functional graph theory (HFGT). A concrete translation
pipeline from an activity-on-node network to a SysML activity diagram, and then
to an operand net is constructed. Using this representation, it specializes the
hetero-functional network minimum-cost flow (HFNMCF) formulation to the RCPSP
context as a systematic means of HFGT for quantitative analysis and proves that
the RCPSP is recoverable as a special case of a broader model. Secondly, on an
illustrative instance with renewable and non-renewable operands, the
specialized HFNMCF, while producing similar schedules, yields explicit
explanations of the project states that enable richer monitoring and control.
Overall, the framework preserves the strengths of the classical RCPSP while
accommodating real-world constraints and enterprise-level decision processes
encountered in large, complex megaprojects.

</details>


### [155] [Docker-based CI/CD for Rocq/OCaml projects](https://arxiv.org/abs/2510.19089)
*Érik Martin-Dorel*

Main category: cs.SE

TL;DR: 介绍三个相关软件项目docker - coq、docker - coq - action和docker - keeper，有两大目标


<details>
  <summary>Details</summary>
Motivation: 促进基于Docker的CI/CD在Rocq或OCaml项目中的使用，帮助工具未来维护者

Method: 未提及

Result: 未提及

Conclusion: 未提及

Abstract: This paper presents three closely-related software projects, namely:
docker-coq, docker-coq-action, and docker-keeper. It aims at two objectives:
provide a high-level description of the available features -- to foster the use
of a Docker-based CI/CD for Rocq (formerly known as Coq) or OCaml projects --
and document the underlying requirements and the main design choices of these
three DevOps tools -- to help their future maintainers.

</details>


### [156] [Automated Concern Extraction from Textual Requirements of Cyber-Physical Systems: A Multi-solution Study](https://arxiv.org/abs/2510.19237)
*Dongming Jin,Zhi Jin,Xiaohong Chen,Zheng Fang,Linyu Li,Shengxin Zhao,Chuihui Wang,Hongbin Xiao*

Main category: cs.SE

TL;DR: 提出CPS需求关注点提取基准ReqEBench，用其对比评估自动化提取方案，分析失败案例并提改进思路，认为ReqEBench能促进相关评估与发展。


<details>
  <summary>Details</summary>
Motivation: 现有自动化需求关注点提取方案有效性评估缺乏公平全面的基准。

Method: 构建包含12个真实CPS的2721条需求的ReqEBench基准，用其对三种自动化需求关注点提取方案进行对比研究。

Result: GPT - 4在实体关注点提取中最高F1分数仅0.24，分析了流行大语言模型方案的失败案例。

Conclusion: ReqEBench将促进自动化需求关注点提取的评估和发展。

Abstract: Cyber-physical systems (CPSs) are characterized by a deep integration of the
information space and the physical world, which makes the extraction of
requirements concerns more challenging. Some automated solutions for
requirements concern extraction have been proposed to alleviate the burden on
requirements engineers. However, evaluating the effectiveness of these
solutions, which relies on fair and comprehensive benchmarks, remains an open
question. To address this gap, we propose ReqEBench, a new CPSs requirements
concern extraction benchmark, which contains 2,721 requirements from 12
real-world CPSs. ReqEBench offers four advantages. It aligns with real-world
CPSs requirements in multiple dimensions, e.g., scale and complexity. It covers
comprehensive concerns related to CPSs requirements. It undergoes a rigorous
annotation process. It covers multiple application domains of CPSs, e.g.,
aerospace and healthcare. We conducted a comparative study on three types of
automated requirements concern extraction solutions and revealed their
performance in real-world CPSs using our ReqEBench. We found that the highest
F1 score of GPT-4 is only 0.24 in entity concern extraction. We further analyze
failure cases of popular LLM-based solutions, summarize their shortcomings, and
provide ideas for improving their capabilities. We believe ReqEBench will
facilitate the evaluation and development of automated requirements concern
extraction.

</details>


### [157] [A General Solution for the Implementation of CI/CD in Embedded Linux Development](https://arxiv.org/abs/2510.19240)
*Behnam Agahi,Hamed Farbeh*

Main category: cs.SE

TL;DR: 研究旨在用Yocto项目设计实现基于Linux操作系统开发测试的基础架构，通过多层架构、持续集成部署等方法构建系统并验证，结果显示设计可确保可重复性且能扩展，未来有多项优化建议。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式系统在各行业应用增多，对定制化Linux操作系统开发部署自动化平台需求增加，所以开展此研究。

Method: 采用三层架构，开发三个样本项目并集成，用GitLab CI实现持续集成部署，结合Docker环境，使用本地缓存服务器，在QEMU模拟器进行测试。

Result: 系统功能和稳定性通过测试，设计能确保可重复性，可扩展到实时Linux版本的持续部署。

Conclusion: 所提设计可用于嵌入式系统工业和研究项目，未来可从扩展自动化测试、系统监控等方面优化。

Abstract: With the growing use of embedded systems in various industries, the need for
automated platforms for the development and deployment of customized
Linux-based operating systems has become more important. This research was
conducted with the aim of designing and implementing an integrated and
reproducible infrastructure for the development, building, and testing of a
Linux-based operating system using the Yocto Project. The proposed structure
was implemented based on a three-layer architecture consisting of the main
Yocto repositories, a custom layer (meta-custom), and a coordinating manifest
layer to ensure version synchronization, scalability, and reproducibility.
Three sample projects, including libhelloworld, helloworld, and the kernel
module hello mod, were developed and integrated into the build process.
Continuous Integration and Continuous Deployment pipelines were implemented
with GitLab CI and combined with an isolated Docker environment to automate and
streamline the build and testing workflows. Using a local cache server
containing hashserv, downloads and sstate cache significantly reduced the build
time. The functionality and stability of the system were verified through six
boot test scenarios in the QEMU simulator. The results show that the proposed
design not only ensures reproducibility but also can be extended to advanced
applications such as continuous deployment of real-time Linux versions. Future
recommendations include expanding automated tests, implementing system
monitoring with Prometheus and Grafana, using distributed builds, optimizing
with Docker multi-stage builds, and enabling continuous deployment of real-time
Linux changes to provide a stable and scalable model for industrial and
research projects in embedded systems with a rapid and reliable development
cycle.

</details>


### [158] [Trace: Securing Smart Contract Repository Against Access Control Vulnerability](https://arxiv.org/abs/2510.19254)
*Chong Chen,Jiachi Chen,Lingfeng Bao,David Lo,Yanlin Wang,Zhenyu Shan,Ting Chen,Guangqiang Yin,Jianxing Yu,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出工具TRACE，用于检测非可编译智能合约存储库的访问控制漏洞，实验显示其性能优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 智能合约访问控制漏洞造成巨大损失，现有检测工具难以处理复杂存储库，需编译目标合约才能分析。

Method: TRACE利用大语言模型定位敏感函数，补全函数片段成可编译合约，构建函数调用图和控制流图，分析节点检测漏洞。

Result: 在开源CVE数据集上检测出15个CVE中的14个，在5000个近期链上合约中精度达89.2%，在83个真实存储库中精度达87.0%，均远超现有最佳工具。

Conclusion: TRACE能有效保障非可编译智能合约存储库安全，在检测访问控制漏洞方面优于现有工具。

Abstract: Smart contract vulnerabilities, particularly improper Access Control that
allows unauthorized execution of restricted functions, have caused billions of
dollars in losses. GitHub hosts numerous smart contract repositories containing
source code, documentation, and configuration files-these serve as intermediate
development artifacts that must be compiled and packaged before deployment.
Third-party developers often reference, reuse, or fork code from these
repositories during custom development. However, if the referenced code
contains vulnerabilities, it can introduce significant security risks. Existing
tools for detecting smart contract vulnerabilities are limited in their ability
to handle complex repositories, as they typically require the target contract
to be compilable to generate an abstract representation for further analysis.
This paper presents TRACE, a tool designed to secure non-compilable smart
contract repositories against access control vulnerabilities. TRACE employs
LLMs to locate sensitive functions involving critical operations (e.g.,
transfer) within the contract and subsequently completes function snippets into
a fully compilable contract. TRACE constructs a function call graph from the
abstract syntax tree (AST) of the completed contract. It uses the control flow
graph (CFG) of each function as node information. The nodes of the sensitive
functions are then analyzed to detect Access Control vulnerabilities.
Experimental results demonstrate that TRACE outperforms state-of-the-art tools
on an open-sourced CVE dataset, detecting 14 out of 15 CVEs. In addition, it
achieves 89.2% precision on 5,000 recent on-chain contracts, far exceeding the
best existing tool at 76.9%. On 83 real-world repositories, TRACE achieves
87.0% precision, significantly surpassing DeepSeek-R1's 14.3%.

</details>


### [159] [From Specification to Service: Accelerating API-First Development Using Multi-Agent Systems](https://arxiv.org/abs/2510.19274)
*Saurabh Chauhan,Zeeshan Rasheed,Malik Abdul Sami,Kai-Kristian Kemell,Muhammad Waseem,Zheying Zhang,Jussi Rasku,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文提出用基于大语言模型的代理自动化RESTful微服务的API优先开发系统，经测试表明保持OpenAPI规范小而聚焦时，大语言模型可生成符合规范的完整功能代码。


<details>
  <summary>Details</summary>
Motivation: 推进RESTful Web服务的API优先开发自动化，测试基于大语言模型的多智能体系统支持API优先开发方法的能力。

Method: 提出使用基于大语言模型的代理的系统，创建OpenAPI规范、生成服务器代码并通过分析执行日志和错误消息的反馈循环优化代码，利用PRAB基准测试系统潜力。

Result: 保持OpenAPI规范小而聚焦时，大语言模型能生成具有符合规范业务逻辑的完整功能代码。

Conclusion: 基于大语言模型的代理系统可有效支持RESTful微服务的API优先开发自动化。

Abstract: This paper presents a system that uses Large Language Models (LLMs)-based
agents to automate the API-first development of RESTful microservices. This
system helps to create an OpenAPI specification, generate server code from it,
and refine the code through a feedback loop that analyzes execution logs and
error messages. The integration of log analysis enables the LLM to detect and
address issues efficiently, reducing the number of iterations required to
produce functional and robust services. This study's main goal is to advance
API-first development automation for RESTful web services and test the
capability of LLM-based multi-agent systems in supporting the API-first
development approach. To test the proposed system's potential, we utilized the
PRAB benchmark. The results indicate that if we keep the OpenAPI specification
small and focused, LLMs are capable of generating complete functional code with
business logic that aligns to the specification. The code for the system is
publicly available at https://github.com/sirbh/code-gen

</details>


### [160] [An Empirical Study of Bitwise Operators Intuitiveness through Performance Metrics](https://arxiv.org/abs/2510.19281)
*Shubham Joshi*

Main category: cs.SE

TL;DR: 研究调查编程中按位运算符的可读性和可理解性，通过实验设计，发现部分运算符对响应时间有影响，部分运算符较不直观。


<details>
  <summary>Details</summary>
Motivation: 探究不同编程背景人群接触按位运算符相关问题时在响应时间和错误率等性能指标上是否存在差异。

Method: 采用受试者内实验设计，让不同编程背景参与者完成 JavaScript 程序任务，记录任务完成时间和准确率进行分析。

Result: 运算符是预测响应时间的因素之一，有小但显著的影响；部分运算符如 OR、NOT 和 Left Shift 在任务完成时间上有统计学意义。

Conclusion: 按位运算符复杂度通常不导致更长任务完成时间，但部分运算符较不直观，需进一步研究和重新设计以提高可理解性。

Abstract: Objectives: This study aims to investigate the readability and
understandability of bitwise operators in programming, with the main hypothesis
that there will be a difference in the performance metrics (response time and
error rate) between participants exposed to various bitwise operators related
questions and those who are not.
  Participants: Participants in this human research study include people
without programming background, novice programmers, and university students
with varying programming experience (from freshmen to PhD level). There were 23
participants for this study.
  Study Methods: This study uses an Within-Subjects Experimental Design to
assess how people with diverse programming backgrounds understand and use
bitwise operators. Participants complete tasks in JavaScript program, and their
task completion time and accuracy of the tasks are recorded for analysis.
  Findings: The results indicate that operators can be one of the factors
predicting response time, with a small but significant effect, with R-squared
0.032, (1, 494) = 16.5, p < .001. Additionally, some operators like OR, NOT,
and Left Shift showed statistical significance in task completion times
compared to other operators.
  Conclusions: While the complexity of bitwise operators did not generally
result in longer task completion times, certain operators were found to be less
intuitive, suggesting the need for further investigation and potential redesign
for improved understandability.

</details>


### [161] [Bytecode-centric Detection of Known-to-be-vulnerable Dependencies in Java Projects](https://arxiv.org/abs/2510.19393)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: 现代Java项目大量依赖开源软件（OSS），带来安全风险，现有依赖扫描器有局限，本文提出字节码中心的Jaralyzer扫描器，表现优于其他扫描器。


<details>
  <summary>Details</summary>
Motivation: 现代软件高度依赖OSS，带来安全风险，现有依赖扫描器在处理依赖修改时存在挑战。

Method: 提出字节码中心的依赖扫描器Jaralyzer，直接分析依赖的字节码。

Result: 对56个流行OSS组件的评估表明，Jaralyzer在检测修改依赖的漏洞方面优于其他扫描器，对未修改依赖检测也更优。

Conclusion: Jaralyzer是唯一能识别所有类型依赖修改中漏洞的扫描器，在检测漏洞方面表现出色。

Abstract: On average, 71% of the code in typical Java projects comes from open-source
software (OSS) dependencies, making OSS dependencies the dominant component of
modern software code bases. This high degree of OSS reliance comes with a
considerable security risk of adding known security vulnerabilities to a code
base. To remedy this risk, researchers and companies have developed various
dependency scanners, which try to identify inclusions of known-to-be-vulnerable
OSS dependencies. However, there are still challenges that modern dependency
scanners do not overcome, especially when it comes to dependency modifications,
such as re-compilations, re-bundlings or re-packagings, which are common in the
Java ecosystem. To overcome these challenges, we present Jaralyzer, a
bytecode-centric dependency scanner for Java. Jaralyzer does not rely on the
metadata or the source code of the included OSS dependencies being available
but directly analyzes a dependency's bytecode. Our evaluation across 56 popular
OSS components demonstrates that Jaralyzer outperforms other popular dependency
scanners in detecting vulnerabilities within modified dependencies. It is the
only scanner capable of identifying vulnerabilities across all the above
mentioned types of modifications. But even when applied to unmodified
dependencies, Jaralyzer outperforms the current state-of-the-art code-centric
scanner Eclipse Steady by detecting 28 more true vulnerabilities and yielding
29 fewer false warnings.

</details>


### [162] [AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of Autonomous Driving Systems](https://arxiv.org/abs/2510.19438)
*Linfeng Liang,Chenkai Tan,Yao Deng,Yingfeng Cai,T. Y Chen,Xi Zheng*

Main category: cs.SE

TL;DR: 提出AutoMT框架，利用大语言模型自动化提取变形关系和生成测试用例，实验表明其测试多样性和故障检测能力优于基线方法，架构支持工业集成。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统的变形测试方法依赖人工、缺乏自动化，需要更高效的测试方案。

Method: 提出AutoMT框架，利用大语言模型从本地交通规则中提取变形关系，通过视觉语言代理和搜索代理生成后续测试用例。

Result: AutoMT在后续用例生成的测试多样性上比最佳基线高5倍，能多检测出20.55%的行为违规。

Conclusion: AutoMT可自动提取多样的变形关系，增强真实数据集，发现现场测试和数据收集时遗漏的边缘情况，其模块化架构支持工业集成。

Abstract: Autonomous Driving Systems (ADS) are safety-critical, where failures can be
severe. While Metamorphic Testing (MT) is effective for fault detection in ADS,
existing methods rely heavily on manual effort and lack automation. We present
AutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that
automates the extraction of Metamorphic Relations (MRs) from local traffic
rules and the generation of valid follow-up test cases. AutoMT leverages LLMs
to extract MRs from traffic rules in Gherkin syntax using a predefined
ontology. A vision-language agent analyzes scenarios, and a search agent
retrieves suitable MRs from a RAG-based database to generate follow-up cases
via computer vision. Experiments show that AutoMT achieves up to 5 x higher
test diversity in follow-up case generation compared to the best baseline
(manual expert-defined MRs) in terms of validation rate, and detects up to
20.55% more behavioral violations. While manual MT relies on a fixed set of
predefined rules, AutoMT automatically extracts diverse metamorphic relations
that augment real-world datasets and help uncover corner cases often missed
during in-field testing and data collection. Its modular architecture
separating MR extraction, filtering, and test generation supports integration
into industrial pipelines and potentially enables simulation-based testing to
systematically cover underrepresented or safety-critical scenarios.

</details>


### [163] [Mapping and Evolving Interoperability Testing in European Energy Systems: The int:net Perspective](https://arxiv.org/abs/2510.19460)
*Thomas I. Strasser,Edmund Widl,Carlos Ayon Mac Gregor,Mirko Ginocchi,Rene Kuchenbuch*

Main category: cs.SE

TL;DR: 分析欧洲互操作性测试设施格局，为互操作性测试欧洲生态系统建设作贡献。


<details>
  <summary>Details</summary>
Motivation: 欧洲能源格局转型需高度互操作性，但现有智能电网测试基础设施缺乏对互操作性测试的专门全面关注，且缺少欧洲互操作性测试能力的结构化统一概述。

Method: 对30个设施进行结构化调查。

Result: 提供测试基础设施、应用方法和参考测试用例的分类清单，引入未来测试环境开发蓝图。

Conclusion: 有助于建立协调的欧洲互操作性测试生态系统，支持合作、创新并契合能源转型目标。

Abstract: The ongoing transformation of the European energy landscape, driven by the
integration of renewable energy sources, digital technologies, and
decentralized systems, requires a high degree of interoperability across
diverse components and systems. Ensuring that these elements can exchange
information and operate together reliably is essential for achieving a secure,
flexible, and efficient energy supply infrastructure. While several initiatives
have contributed to the development of smart grid testing infrastructures, they
do not provide a dedicated or comprehensive focus on interoperability testing.
A structured and harmonized overview of interoperability testing capabilities
across Europe is therefore still missing. This work therefore presents a novel
contribution by analyzing the European interoperability testing facility
landscape through a structured survey of 30 facilities. It provides a
categorized inventory of testing infrastructures, applied methodologies, and
reference test cases, and introduces a blueprint for the development of future
testing environments. The findings contribute to the establishment of a
coordinated European ecosystem for interoperability testing, supporting
collaboration, innovation, and alignment with the goals of the energy
transition.

</details>


### [164] [A Goal-Driven Survey on Root Cause Analysis](https://arxiv.org/abs/2510.19593)
*Aoyang Fang,Haowen Yang,Haoze Dong,Qisheng Lu,Junjielong Xu,Pinjia He*

Main category: cs.SE

TL;DR: 本文提出目标驱动框架对135篇云事件管理中根因分析（RCA）相关论文分类，讨论RCA终极目标、开放挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 先前RCA调查忽略基于目标的区别，按输入数据类型分类会混淆不同目标工作，无法展现领域进展和差距，而典型受众需要按目标组织的RCA调查。

Method: 提出目标驱动框架，基于不同目标对2014 - 2025年135篇RCA相关论文进行分类和整合。

Result: 完成对135篇论文的目标驱动分类，并讨论了RCA的终极目标。

Conclusion: 强调按目标对RCA论文分类的必要性，指出RCA领域存在的开放挑战和未来方向。

Abstract: Root Cause Analysis (RCA) is a crucial aspect of incident management in
large-scale cloud services. While the term root cause analysis or RCA has been
widely used, different studies formulate the task differently. This is because
the term "RCA" implicitly covers tasks with distinct underlying goals. For
instance, the goal of localizing a faulty service for rapid triage is
fundamentally different from identifying a specific functional bug for a
definitive fix. However, previous surveys have largely overlooked these
goal-based distinctions, conventionally categorizing papers by input data types
(e.g., metric-based vs. trace-based methods). This leads to the grouping of
works with disparate objectives, thereby obscuring the true progress and gaps
in the field. Meanwhile, the typical audience of an RCA survey is either laymen
who want to know the goals and big picture of the task or RCA researchers who
want to figure out past research under the same task formulation. Thus, an RCA
survey that organizes the related papers according to their goals is in high
demand. To this end, this paper presents a goal-driven framework that
effectively categorizes and integrates 135 papers on RCA in the context of
cloud incident management based on their diverse goals, spanning the period
from 2014 to 2025. In addition to the goal-driven categorization, it discusses
the ultimate goal of all RCA papers as an umbrella covering different RCA
formulations. Moreover, the paper discusses open challenges and future
directions in RCA.

</details>


### [165] [Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1](https://arxiv.org/abs/2510.19600)
*Qianli Ma,Siyu Wang,Yilin Chen,Yinhao Tang,Yixiang Yang,Chang Guo,Bingjie Gao,Zhening Xing,Yanan Sun,Zhipeng Zhang*

Main category: cs.SE

TL;DR: 论文介绍AutoPage多智能体系统，将论文转换为网页，构建PageBench基准验证，实验显示高效高质。


<details>
  <summary>Details</summary>
Motivation: 研究交流重要，但研究者构建项目网页繁琐，自动化在网页创建方面存在挑战。

Method: 引入AutoPage多智能体系统，将网页创建分解为粗到细的流程，有Checker智能体防幻觉，设人工检查点，构建PageBench基准。

Result: AutoPage能生成高质量、视觉美观的网页，且在15分钟内花费不到0.1美元完成。

Conclusion: AutoPage可从工具转变为强大的协作助手，有效解决论文网页创建问题。

Abstract: In the quest for scientific progress, communicating research is as vital as
the discovery itself. Yet, researchers are often sidetracked by the manual,
repetitive chore of building project webpages to make their dense papers
accessible. While automation has tackled static slides and posters, the
dynamic, interactive nature of webpages has remained an unaddressed challenge.
To bridge this gap, we reframe the problem, arguing that the solution lies not
in a single command, but in a collaborative, hierarchical process. We introduce
$\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.
AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline
from narrative planning to multimodal content generation and interactive
rendering. To combat AI hallucination, dedicated "Checker" agents verify each
step against the source paper, while optional human checkpoints ensure the
final product aligns perfectly with the author's vision, transforming the
system from a mere tool into a powerful collaborative assistant. To rigorously
validate our approach, we also construct $\textbf{PageBench}$, the first
benchmark for this new task. Experiments show AutoPage not only generates
high-quality, visually appealing pages but does so with remarkable efficiency
in under 15 minutes for less than \$0.1. Code and dataset will be released at
$\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.

</details>


### [166] [FidelityGPT: Correcting Decompilation Distortions with Retrieval Augmented Generation](https://arxiv.org/abs/2510.19615)
*Zhiping Zhou,Xiaohong Li,Ruitao Feng,Yao Zhang,Yuekang Li,Wenbu Feng,Yunqian Wang,Yuqing Li*

Main category: cs.SE

TL;DR: 提出FidelityGPT框架提升反编译代码准确性和可读性，评估显示其效果优于DeGPT，有潜力推动基于大语言模型的反编译和逆向工程。


<details>
  <summary>Details</summary>
Motivation: 现有反编译方法存在保真度问题，缺乏对复杂闭源二进制文件的鲁棒检测和修正。

Method: 引入适用于闭源环境的失真感知提示模板，集成检索增强生成（RAG）与动态语义强度算法定位失真行，使用变量依赖算法减轻长上下文限制。

Result: 在620个函数对上评估，平均检测准确率89%、精度83%，修复率（FR）94%、校正修复率（CFR）64%，优于DeGPT。

Conclusion: FidelityGPT有潜力推动基于大语言模型的反编译和逆向工程。

Abstract: Decompilation converts machine code into human-readable form, enabling
analysis and debugging without source code. However, fidelity issues often
degrade the readability and semantic accuracy of decompiled output. Existing
methods, such as variable renaming or structural simplification, provide
partial improvements but lack robust detection and correction, particularly for
complex closed-source binaries. We present FidelityGPT, a framework that
enhances decompiled code accuracy and readability by systematically detecting
and correcting semantic distortions. FidelityGPT introduces distortion-aware
prompt templates tailored to closed-source settings and integrates
Retrieval-Augmented Generation (RAG) with a dynamic semantic intensity
algorithm to locate distorted lines and retrieve semantically similar code from
a database. A variable dependency algorithm further mitigates long-context
limitations by analyzing redundant variables and integrating their dependencies
into the prompt context. Evaluated on 620 function pairs from a binary
similarity benchmark, FidelityGPT achieved an average detection accuracy of 89%
and a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,
Corrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating
significant gains in accuracy and readability. These results highlight its
potential to advance LLM-based decompilation and reverse engineering.

</details>


### [167] [Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary](https://arxiv.org/abs/2510.19692)
*Rashina Hoda*

Main category: cs.SE

TL;DR: 本文探讨了在软件工程中引入代理式AI的范式转变，建议扩大代理式软件工程的范围，提出初步价值原则和词汇使用指导，以促进社区合作并奠定坚实基础。


<details>
  <summary>Details</summary>
Motivation: 随着技术人员推动代理式AI发展，软件工程研究人员需建立代理式软件工程研究领域，且早期实证表明需考虑社会技术问题。

Method: 通过推荐扩大范围至'全流程'愿景、提出初步价值原则和分享词汇设计/使用指导。

Result: 为新兴社区愿景做出贡献。

Conclusion: 希望这些想法能鼓励社区合作，引导软件工程社区为代理式软件工程奠定坚实基础。

Abstract: Agentic AI is poised to usher in a seismic paradigm shift in Software
Engineering (SE). As technologists rush head-along to make agentic AI a
reality, SE researchers are driven to establish agentic SE as a research area.
While early visions of agentic SE are primarily focused on code-related
activities, early empirical evidence calls for a consideration of a range of
socio-technical concerns to make it work in practice. This paper contributes to
the emerging community vision by: (a) recommending an expansion of its scope
beyond code, toward a 'whole of process' vision, grounding it in SE foundations
and evolution and emerging agentic SE frameworks, (b) proposing a preliminary
set of values and principles to guide efforts, and (c) sharing guidance on
designing/using well-defined vocabulary for agentic SE. It is hoped that these
ideas will encourage community collaborations and steer the SE community
towards laying strong foundations of agentic SE so its not only inevitable but
also deliberate and desirable in the long run.

</details>


### [168] [Review of Tools for Zero-Code LLM Based Application Development](https://arxiv.org/abs/2510.19747)
*Priyaranjan Pattnayak,Hussain Bohra*

Main category: cs.SE

TL;DR: 本文对利用大语言模型实现零代码开发的平台进行调研，分类比较并指出优缺点，还给出未来方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正改变软件开发，调研相关零代码开发平台。

Method: 采用广泛调研方法，按界面风格、后端集成等关键维度对平台分类。

Result: 给出平台分类，比较各平台优缺点，讨论与传统及低代码开发的权衡。

Conclusion: 零代码大语言模型平台降低开发门槛，但灵活性和可靠性有挑战，发展前景好。

Abstract: Large Language Models (LLMs) are transforming software creation by enabling
zero code development platforms. Our survey reviews recent platforms that let
users build applications without writing code, by leveraging LLMs as the brains
of the development process. We adopt a broad survey methodology, categorizing
platforms based on key dimensions such as interface style, backend integration,
output type, and extensibility. We analyze both dedicated LLM based app
builders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and
general no code platforms (e.g., Bubble, Glide) that integrate LLM
capabilities. We present a taxonomy categorizing these platforms by their
interface (conversational, visual, etc.), supported LLM backends, output type
(chatbot, full application, workflow), and degree of extensibility. Core
features such as autonomous agents, memory management, workflow orchestration,
and API integrations are in scope of the survey. We provide a detailed
comparison, highlighting each platform's strengths and limitations. Trade offs
(customizability, scalability, vendor lock-in) are discussed in comparison with
traditional and low code development approaches. Finally, we outline future
directions, including multimodal interfaces, on device LLMs, and improved
orchestration for democratizing app creation with AI. Our findings indicate
that while zero code LLM platforms greatly reduce the barrier to creating AI
powered applications, they still face challenges in flexibility and
reliability. Overall, the landscape is rapidly evolving, offering exciting
opportunities to empower non programmers to create sophisticated software.

</details>


### [169] [BOSQTGEN: Breaking the Sound Barrier in Test Generation](https://arxiv.org/abs/2510.19777)
*S M Sadrul Islam Asif,James Chen,Earl T. Barr,Mark Marron*

Main category: cs.SE

TL;DR: 介绍了用于API测试生成的新黑盒方法和工具BOSQTGEN，能有效提高代码覆盖率，可自动创建高质量测试用例。


<details>
  <summary>Details</summary>
Motivation: 现代软件依赖API组合，现有API合约不足，当前测试生成技术存在诸多挑战，需要可靠的一致性测试。

Method: 将API规范分解为原语，用大语言模型为其提供连贯层次，采用组合测试高效采样。

Result: BOSQTGEN系统在RESTful基准测试中平均达到82%的代码覆盖率，比现有技术提高20%以上，接近手写测试套件。

Conclusion: BOSQTGEN提供了完全由API驱动的测试生成方法，可让开发者自动创建高质量测试用例用于验证或测试驱动开发。

Abstract: Modern software is increasingly built by composing APIs, elevating the API
contract to a critical role. Inadequate contracts, however, lead to mismatched
expectations and failures, creating a pressing need for robust conformance
testing. Current test generation techniques are hindered by key challenges:
polyglot systems, source code inaccessibility, a cost-reliability trade-off,
and, most critically, the difficulty of generating structured inputs.
  We introduce BOSQTGEN, a novel black-box methodology and tool for API test
generation. BOSQTGEN utilizes a novel approach for decomposing API
specifications into primitives, using LLMs to suggest coherent strata for them,
and employing combinatorial testing to efficiently sample over these values.
This approach ensures coverage of critical interactions while avoiding the
redundancy of random sampling.
  The resulting BOSQTGEN system achieves an average of 82% code coverage on
RESTful benchmarks, often a 20% or more increase over prior state-of-the-art
systems and nearing parity with hand-written test suites. Providing a fully
API-driven approach to test generation, enables developers to automatically
create high-quality test cases for validation or test-driven development.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [170] [Optimized Multi-Level Monte Carlo Parametrization and Antithetic Sampling for Nested Simulations](https://arxiv.org/abs/2510.18995)
*Alexandre Boumezoued,Adel Cherchali,Vincent Lemaire,Gilles Pagès,Mathieu Truc*

Main category: q-fin.CP

TL;DR: 提出新的MLMC参数化方法提升金融风险估计性能，证明反抽样可提高效率，数值实验验证方法实用价值。


<details>
  <summary>Details</summary>
Motivation: 传统MLMC及加权变体处理不规则函数时效率下降，需解决金融风险估计问题。

Method: 引入新的MLMC参数化方法，证明MLMC层的反抽样可提高效率。

Result: 新方法在实际非渐近设置中显著提升性能，保持理论渐近保证，数值实验验证实用价值。

Conclusion: 新方法能有效估计损失概率和分位数，弥合金融风险估计理论与实践差距。

Abstract: Estimating risk measures such as large loss probabilities and Value-at-Risk
is fundamental in financial risk management and often relies on computationally
intensive nested Monte Carlo methods. While Multi-Level Monte Carlo (MLMC)
techniques and their weighted variants are typically more efficient, their
effectiveness tends to deteriorate when dealing with irregular functions,
notably indicator functions, which are intrinsic to these risk measures. We
address this issue by introducing a novel MLMC parametrization that
significantly improves performance in practical, non-asymptotic settings while
maintaining theoretical asymptotic guarantees. We also prove that antithetic
sampling of MLMC levels enhances efficiency regardless of the regularity of the
underlying function. Numerical experiments motivated by the calculation of
economic capital in a life insurance context confirm the practical value of our
approach for estimating loss probabilities and quantiles, bridging theoretical
advances and practical requirements in financial risk estimation.

</details>


### [171] [An Efficient Calibration Framework for Volatility Derivatives under Rough Volatility with Jumps](https://arxiv.org/abs/2510.19126)
*Keyuan Wu,Tenghan Zhong,Yuxuan Ouyang*

Main category: q-fin.CP

TL;DR: 提出一种基于特征函数的随机波动率模型快速稳健校准方法，在粗糙波动率模型上验证了方法的准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 开发一种快速且稳健的随机波动率模型校准方法。

Method: 将定价公式拆分为与数据无关的积分和与市场相关的余项，用GPU加速预计算数据无关积分，用小型神经网络近似市场相关定价映射，在特定模型上用全局到局部搜索进行校准。

Result: 验证了纯跳跃粗糙波动率模型能充分捕捉VIX动态，校准方法达到了高精度和高速度。

Conclusion: 所提出的校准方法有效且性能优越。

Abstract: We present a fast and robust calibration method for stochastic volatility
models that admit Fourier-analytic transform-based pricing via characteristic
functions. The design is structure-preserving: we keep the original pricing
transform and (i) split the pricing formula into data-independent inte- grals
and a market-dependent remainder; (ii) precompute those data-independent
integrals with GPU acceleration; and (iii) approximate only the remaining,
market-dependent pricing map with a small neural network. We instantiate the
workflow on a rough volatility model with tempered-stable jumps tailored to
power-type volatility derivatives and calibrate it to VIX options with a
global-to-local search. We verify that a pure-jump rough volatility model
adequately captures the VIX dynamics, consistent with prior empirical findings,
and demonstrate that our calibration method achieves high accuracy and speed.

</details>


### [172] [Denoising Complex Covariance Matrices with Hybrid ResNet and Random Matrix Theory: Cryptocurrency Portfolio Applications](https://arxiv.org/abs/2510.19130)
*Andres Garcia-Medina*

Main category: q-fin.CP

TL;DR: 提出幂律协方差模型和结合RMT与ResNets的混合估计器，经模拟和实证实验，证明组合估计器能优化投资组合，凸显多方法结合优势。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列协方差矩阵不稳定，且呈现幂律缩放特性，需更好方法刻画加密货币集体动态和优化投资组合。

Method: 提出幂律协方差模型，开发结合随机矩阵理论（RMT）与残差神经网络（ResNets）的混合估计器，RMT正则化特征值谱，ResNet学习数据驱动修正。

Result: 蒙特卡罗模拟显示基于ResNet的估计器能最小化损失；实证实验表明结合分层过滤与ResNet修正的两步估计器能产生最有利可图和平衡的投资组合，且在市场制度转变下保持稳健。

Conclusion: 结合RMT、深度学习和幂律建模可捕捉金融系统内在复杂性，在现实条件下增强投资组合优化。

Abstract: Covariance matrices estimated from short, noisy, and non-Gaussian financial
time series-particularly cryptocurrencies-are notoriously unstable. Empirical
evidence indicates that these covariance structures often exhibit power-law
scaling, reflecting complex and hierarchical interactions among assets.
Building on this insight, we propose a power-law covariance model to
characterize the collective dynamics of cryptocurrencies and develop a hybrid
estimator that integrates Random Matrix Theory (RMT) with Residual Neural
Networks (ResNets). The RMT component regularizes the eigenvalue spectrum under
high-dimensional noise, while the ResNet learns data-driven corrections to
recover latent structural dependencies. Monte Carlo simulations show that
ResNet-based estimators consistently minimize both Frobenius and
minimum-variance (MV) losses across diverse covariance models. Empirical
experiments on 89 cryptocurrencies (2020-2025), using a training period ending
at the local BTC maximum in November 2021 and testing through the subsequent
bear market, demonstrate that a two-step estimator combining hierarchical
filtering with ResNet corrections yields the most profitable and balanced
portfolios, remaining robust under market regime shifts. These findings
highlight the potential of combining RMT, deep learning, and power-law modeling
to capture the intrinsic complexity of financial systems and enhance portfolio
optimization under realistic conditions.

</details>


### [173] [News-Aware Direct Reinforcement Trading for Financial Markets](https://arxiv.org/abs/2510.19173)
*Qing-Yu Lan,Zhan-He Wang,Jun-Qian Jiang,Yu-Tong Wang,Yun-Song Piao*

Main category: q-fin.CP

TL;DR: 本文利用大语言模型得到的新闻情绪分数和原始价格、交易量数据作为强化学习输入做交易决策，在加密货币市场实验表明该方法优于市场基准。


<details>
  <summary>Details</summary>
Motivation: 金融市场对新闻敏感，有效将新闻数据融入量化交易是重要挑战，现有方法依赖手动规则和手工特征。

Method: 直接使用大语言模型得到的新闻情绪分数，结合原始价格和交易量数据作为强化学习的可观测输入，用循环神经网络或Transformers等序列模型处理输入以做出端到端交易决策，并用加密货币市场数据对DDQN和GRPO两种强化学习算法进行实验。

Result: 不依赖手工特征或手动规则的新闻感知方法能取得优于市场基准的表现。

Conclusion: 强调了时间序列信息在这一过程中的关键作用。

Abstract: The financial market is known to be highly sensitive to news. Therefore,
effectively incorporating news data into quantitative trading remains an
important challenge. Existing approaches typically rely on manually designed
rules and/or handcrafted features. In this work, we directly use the news
sentiment scores derived from large language models, together with raw price
and volume data, as observable inputs for reinforcement learning. These inputs
are processed by sequence models such as recurrent neural networks or
Transformers to make end-to-end trading decisions. We conduct experiments using
the cryptocurrency market as an example and evaluate two representative
reinforcement learning algorithms, namely Double Deep Q-Network (DDQN) and
Group Relative Policy Optimization (GRPO). The results demonstrate that our
news-aware approach, which does not depend on handcrafted features or manually
designed rules, can achieve performance superior to market benchmarks. We
further highlight the critical role of time-series information in this process.

</details>


### [174] [Aligning Multilingual News for Stock Return Prediction](https://arxiv.org/abs/2510.19203)
*Yuntao Wu,Lynn Tao,Ing-Haw Cheng,Charles Martineau,Yoshio Nozawa,John Hull,Andreas Veneris*

Main category: q-fin.CP

TL;DR: 提出用最优传输方法对齐多语言新闻文章句子，应用于彭博英语和日语新闻文章，对齐句子效果好，相关交易策略夏普比率更高。


<details>
  <summary>Details</summary>
Motivation: 新闻跨语言传播时翻译可能丢失细微差别，需要一种方法对齐多语言新闻文章句子。

Method: 使用最优传输方法对齐多语言新闻文章中的句子。

Result: 对齐的句子更稀疏、更易解释、语义相似度更高，基于对齐句子的回报分数与实际股票回报相关性更强，长短期交易策略夏普比率高10%。

Conclusion: 所提出的多语言新闻句子对齐方法有效，能提升交易策略表现。

Abstract: News spreads rapidly across languages and regions, but translations may lose
subtle nuances. We propose a method to align sentences in multilingual news
articles using optimal transport, identifying semantically similar content
across languages. We apply this method to align more than 140,000 pairs of
Bloomberg English and Japanese news articles covering around 3500 stocks in
Tokyo exchange over 2012-2024. Aligned sentences are sparser, more
interpretable, and exhibit higher semantic similarity. Return scores
constructed from aligned sentences show stronger correlations with realized
stock returns, and long-short trading strategies based on these alignments
achieve 10\% higher Sharpe ratios than analyzing the full text sample.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [175] [Compensation-based risk-sharing](https://arxiv.org/abs/2510.19511)
*Jan Dhaene,Atibhav Chaudhry,Ka Chun Cheung,Austin Riis-Due*

Main category: q-fin.RM

TL;DR: 本文研究捐赠应急基金中使用满足全分配的风险分担规则分配赔付的数学问题，分析两类管理者方案的 actuarial fairness，结果扩展了前人研究。


<details>
  <summary>Details</summary>
Motivation: 研究捐赠应急基金中使用风险分担规则分配赔付的数学问题，分析不同管理者方案的公平性。

Method: 考虑两类管理者（'active'和'passive'），分析基于补偿的风险分担方案的 actuarial fairness。

Result: 得到了实现公平性的一般条件，扩展了前人基于伯努利分布的研究，允许一般非负损失分布。

Conclusion: 研究成果扩展了相关领域的工作，为捐赠应急基金赔付分配提供了更广泛适用的条件。

Abstract: This paper studies the mathematical problem of allocating payouts
(compensations) in an endowment contingency fund using a risk-sharing rule that
satisfies full allocation. Besides the participants, an administrator manages
the fund by collecting ex-ante contributions to establish the fund and
distributing ex-post payouts to members. Two types of administrators are
considered. An 'active' administrator both invests in the fund and receives the
payout of the fund when no participant receives a payout. A 'passive'
administrator performs only administrative tasks and neither invests in nor
receives a payout from the fund. We analyze the actuarial fairness of both
compensation-based risk-sharing schemes and provide general conditions under
which fairness is achieved. The results extend earlier work by Denuit and
Robert (2025) and Dhaene and Milevsky (2024), who focused on payouts based on
Bernoulli distributions, by allowing for general non-negative loss
distributions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [176] [Calibrated Principal Component Regression](https://arxiv.org/abs/2510.19020)
*Yixuan Florence Wu,Yilun Zhu,Lei Cao and,Naichen Shi*

Main category: stat.ML

TL;DR: 提出校准主成分回归（CPCR）方法用于广义线性模型统计推断，理论和实证表明其优于标准PCR。


<details>
  <summary>Details</summary>
Motivation: 标准主成分回归（PCR）在过参数化情况下会因截断产生偏差，需改进。

Method: 先在主成分子空间学习低方差先验，再通过中心Tikhonov步骤在原始特征空间校准模型，利用交叉拟合并软化PCR硬截断控制偏差。

Result: 理论计算显示在随机矩阵机制下，回归信号在低方差方向有不可忽略成分时CPCR优于标准PCR；实证表明CPCR在多个过参数化问题上持续改善预测。

Conclusion: CPCR在现代过参数化环境中具有稳定性和灵活性。

Abstract: We propose a new method for statistical inference in generalized linear
models. In the overparameterized regime, Principal Component Regression (PCR)
reduces variance by projecting high-dimensional data to a low-dimensional
principal subspace before fitting. However, PCR incurs truncation bias whenever
the true regression vector has mass outside the retained principal components
(PC). To mitigate the bias, we propose Calibrated Principal Component
Regression (CPCR), which first learns a low-variance prior in the PC subspace
and then calibrates the model in the original feature space via a centered
Tikhonov step. CPCR leverages cross-fitting and controls the truncation bias by
softening PCR's hard cutoff. Theoretically, we calculate the out-of-sample risk
in the random matrix regime, which shows that CPCR outperforms standard PCR
when the regression signal has non-negligible components in low-variance
directions. Empirically, CPCR consistently improves prediction across multiple
overparameterized problems. The results highlight CPCR's stability and
flexibility in modern overparameterized settings.

</details>


### [177] [Signature Kernel Scoring Rule as Spatio-Temporal Diagnostic for Probabilistic Forecasting](https://arxiv.org/abs/2510.19110)
*Archer Dodson,Ritabrata Dutta*

Main category: stat.ML

TL;DR: 本文引入基于粗糙路径理论的签名核评分规则用于天气预测，验证其严格恰当性，实证表明其有高辨别力，基于此训练的模型在15个时间步内优于气候学预测。


<details>
  <summary>Details</summary>
Motivation: 现代数据驱动的机器学习天气预测模型的训练和评估受传统评分规则（如MSE）限制，因其忽略了天气和大气系统中高度相关的数据结构。

Method: 引入签名核评分规则，将天气变量视为连续路径，通过迭代积分编码时空依赖；使用路径增强验证其严格恰当性；在ERA5再分析天气数据上用预测 - 顺序评分规则训练滑动窗口生成式神经网络。

Result: 签名核评分规则具有高辨别力，能捕捉路径依赖的相互作用；基于签名核训练的轻量级模型在15个时间步的预测路径上优于气候学预测。

Conclusion: 签名核评分规则为天气预报验证和模型训练提供了理论上可靠的指标，基于此训练的模型在短期预测中有更好表现。

Abstract: Modern weather forecasting has increasingly transitioned from numerical
weather prediction (NWP) to data-driven machine learning forecasting
techniques. While these new models produce probabilistic forecasts to quantify
uncertainty, their training and evaluation may remain hindered by conventional
scoring rules, primarily MSE, which ignore the highly correlated data
structures present in weather and atmospheric systems. This work introduces the
signature kernel scoring rule, grounded in rough path theory, which reframes
weather variables as continuous paths to encode temporal and spatial
dependencies through iterated integrals. Validated as strictly proper through
the use of path augmentations to guarantee uniqueness, the signature kernel
provides a theoretically robust metric for forecast verification and model
training. Empirical evaluations through weather scorecards on WeatherBench 2
models demonstrate the signature kernel scoring rule's high discriminative
power and unique capacity to capture path-dependent interactions. Following
previous demonstration of successful adversarial-free probabilistic training,
we train sliding window generative neural networks using a
predictive-sequential scoring rule on ERA5 reanalysis weather data. Using a
lightweight model, we demonstrate that signature kernel based training
outperforms climatology for forecast paths of up to fifteen timesteps.

</details>


### [178] [Extreme Event Aware ($η$-) Learning](https://arxiv.org/abs/2510.19161)
*Kai Chang,Themistoklis P. Sapsis*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Quantifying and predicting rare and extreme events persists as a crucial yet
challenging task in understanding complex dynamical systems. Many practical
challenges arise from the infrequency and severity of these events, including
the considerable variance of simple sampling methods and the substantial
computational cost of high-fidelity numerical simulations. Numerous data-driven
methods have recently been developed to tackle these challenges. However, a
typical assumption for the success of these methods is the occurrence of
multiple extreme events, either within the training dataset or during the
sampling process. This leads to accurate models in regions of quiescent events
but with high epistemic uncertainty in regions associated with extremes. To
overcome this limitation, we introduce Extreme Event Aware (e2a or eta) or
$\eta$-learning which does not assume the existence of extreme events in the
available data. $\eta$-learning reduces the uncertainty even in `uncharted'
extreme event regions, by enforcing the extreme event statistics of an
observable indicative of extremeness during training, which can be available
through qualitative arguments or estimated with unlabeled data. This type of
statistical regularization results in models that fit the observed data, while
enforcing consistency with the prescribed observable statistics, enabling the
generation of unprecedented extreme events even when the training data lack
extremes therein. Theoretical results based on optimal transport offer a
rigorous justification and highlight the optimality of the introduced method.
Additionally, extensive numerical experiments illustrate the favorable
properties of the $\eta$-learning framework on several prototype problems and
real-world precipitation downscaling problems.

</details>


### [179] [Topology of Currencies: Persistent Homology for FX Co-movements: A Comparative Clustering Study](https://arxiv.org/abs/2510.19306)
*Pattravadee de Favereau de Jeneret,Ioannis Diamantis*

Main category: stat.ML

TL;DR: 研究拓扑数据分析（TDA）在聚类货币行为上能否提供超越传统统计方法的见解，发现TDA聚类效果更好，是分析金融时间序列的有价值补充工具。


<details>
  <summary>Details</summary>
Motivation: 探究TDA在聚类货币行为上能否提供超越传统统计方法的额外见解，因外汇市场复杂，传统技术可能无法完全捕捉其非线性和高维动态。

Method: 比较基于TDA派生特征和经典统计特征的聚类结果，使用13种主要货币兑欧元的月度对数收益率，应用k - means和层次聚类算法，通过轮廓分数和Calinski - Harabasz指数评估聚类质量。

Result: TDA特征聚类产生更紧凑、分离度更好的簇，Calinski - Harabasz分数更高，但所有聚类方法的轮廓分数都一般。不同特征下聚类组成不同，表明TDA能捕捉传统方法可能忽略的货币联动结构模式。

Conclusion: TDA是分析金融时间序列的有价值补充工具，在风险管理等领域有潜在应用。

Abstract: This study investigates whether Topological Data Analysis (TDA) can provide
additional insights beyond traditional statistical methods in clustering
currency behaviours. We focus on the foreign exchange (FX) market, which is a
complex system often exhibiting non-linear and high-dimensional dynamics that
classical techniques may not fully capture. We compare clustering results based
on TDA-derived features versus classical statistical features using monthly
logarithmic returns of 13 major currency exchange rates (all against the euro).
Two widely-used clustering algorithms, \(k\)-means and Hierarchical clustering,
are applied on both types of features, and cluster quality is evaluated via the
Silhouette score and the Calinski-Harabasz index. Our findings show that
TDA-based feature clustering produces more compact and well-separated clusters
than clustering on traditional statistical features, particularly achieving
substantially higher Calinski-Harabasz scores. However, all clustering
approaches yield modest Silhouette scores, underscoring the inherent difficulty
of grouping FX time series. The differing cluster compositions under TDA vs.
classical features suggest that TDA captures structural patterns in currency
co-movements that conventional methods might overlook. These results highlight
TDA as a valuable complementary tool for analysing financial time series, with
potential applications in risk management where understanding structural
co-movements is crucial.

</details>


### [180] [Metadata Extraction Leveraging Large Language Models](https://arxiv.org/abs/2510.19334)
*Cuize Han,Sesh Jalagam*

Main category: stat.ML

TL;DR: 本文介绍了基于大语言模型的合同审查元数据提取方案，通过优化方法提升条款识别的准确性和效率，对法律专业人士有潜在价值。


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展下，实现自动化法律文件分析，提升现代合同管理系统效率。

Method: 利用公开和专有合同数据集，确定优化元数据提取的三个关键要素，包括文本转换、块选择和先进的大语言模型技术。

Result: 实验结果显示条款识别的准确性和效率显著提高。

Conclusion: 精心优化的大语言模型系统可成为法律专业人士的有价值工具，为各类组织提供高效合同审查服务。

Abstract: The advent of Large Language Models has revolutionized tasks across domains,
including the automation of legal document analysis, a critical component of
modern contract management systems. This paper presents a comprehensive
implementation of LLM-enhanced metadata extraction for contract review,
focusing on the automatic detection and annotation of salient legal clauses.
Leveraging both the publicly available Contract Understanding Atticus Dataset
(CUAD) and proprietary contract datasets, our work demonstrates the integration
of advanced LLM methodologies with practical applications. We identify three
pivotal elements for optimizing metadata extraction: robust text conversion,
strategic chunk selection, and advanced LLM-specific techniques, including
Chain of Thought (CoT) prompting and structured tool calling. The results from
our experiments highlight the substantial improvements in clause identification
accuracy and efficiency. Our approach shows promise in reducing the time and
cost associated with contract review while maintaining high accuracy in legal
clause identification. The results suggest that carefully optimized LLM systems
could serve as valuable tools for legal professionals, potentially increasing
access to efficient contract review services for organizations of all sizes.

</details>


### [181] [On the hardness of RL with Lookahead](https://arxiv.org/abs/2510.19372)
*Corentin Pla,Hugo Richard,Marc Abeille,Nadav Merlis,Vianney Perchet*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study reinforcement learning (RL) with transition look-ahead, where the
agent may observe which states would be visited upon playing any sequence of
$\ell$ actions before deciding its course of action. While such predictive
information can drastically improve the achievable performance, we show that
using this information optimally comes at a potentially prohibitive
computational cost. Specifically, we prove that optimal planning with one-step
look-ahead ($\ell=1$) can be solved in polynomial time through a novel linear
programming formulation. In contrast, for $\ell \geq 2$, the problem becomes
NP-hard. Our results delineate a precise boundary between tractable and
intractable cases for the problem of planning with transition look-ahead in
reinforcement learning.

</details>


### [182] [Square root Cox's survival analysis by the fittest linear and neural networks model](https://arxiv.org/abs/2510.19374)
*Maxime van Cutsem,Sylvain Sardy*

Main category: stat.ML

TL;DR: 重新审视Cox比例风险模型和LASSO以改进生存分析中的特征选择，提出新调参方法且有相变特性，可用于线性模型和神经网络。


<details>
  <summary>Details</summary>
Motivation: 改进生存分析中的特征选择。

Method: 对Cox部分似然取平方根，直接调整惩罚参数λ进行特征选择。

Result: 相比交叉验证LASSO和BIC子集选择有实质性改进，在检索所有且仅有的好特征的概率上有相变。

Conclusion: 该方法可用于线性模型和人工神经网络。

Abstract: We revisit Cox's proportional hazard models and LASSO in the aim of improving
feature selection in survival analysis. Unlike traditional methods relying on
cross-validation or BIC, the penalty parameter $\lambda$ is directly tuned for
feature selection and is asymptotically pivotal thanks to taking the square
root of Cox's partial likelihood. Substantially improving over both
cross-validation LASSO and BIC subset selection, our approach has a phase
transition on the probability of retrieving all and only the good features,
like in compressed sensing. The method can be employed by linear models but
also by artificial neural networks.

</details>


### [183] [A Derandomization Framework for Structure Discovery: Applications in Neural Networks and Beyond](https://arxiv.org/abs/2510.19382)
*Nikos Tsikouras,Yorgos Pantis,Ioannis Mitliagkas,Christos Tzamos*

Main category: stat.ML

TL;DR: 本文聚焦神经网络特征学习动态中的结构发现，在较弱假设下研究，核心是去随机化引理，该引理有广泛应用。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络特征学习动态是重大挑战，前人研究有局限，本文在更弱假设下研究结构发现。

Method: 基于关键的去随机化引理，研究在任意规模和深度、参数全可训练、任意平滑损失函数、微小正则化及达到二阶驻点的训练方法下的结构发现。

Result: 得到去随机化引理，解释结构发现。

Conclusion: 去随机化引理能直接解释结构发现，且在其他领域有直接应用。

Abstract: Understanding the dynamics of feature learning in neural networks (NNs)
remains a significant challenge. The work of (Mousavi-Hosseini et al., 2023)
analyzes a multiple index teacher-student setting and shows that a two-layer
student attains a low-rank structure in its first-layer weights when trained
with stochastic gradient descent (SGD) and a strong regularizer. This
structural property is known to reduce sample complexity of generalization.
Indeed, in a second step, the same authors establish algorithm-specific
learning guarantees under additional assumptions. In this paper, we focus
exclusively on the structure discovery aspect and study it under weaker
assumptions, more specifically: we allow (a) NNs of arbitrary size and depth,
(b) with all parameters trainable, (c) under any smooth loss function, (d) tiny
regularization, and (e) trained by any method that attains a second-order
stationary point (SOSP), e.g.\ perturbed gradient descent (PGD). At the core of
our approach is a key $\textit{derandomization}$ lemma, which states that
optimizing the function $\mathbb{E}_{\mathbf{x}}
\left[g_{\theta}(\mathbf{W}\mathbf{x} + \mathbf{b})\right]$ converges to a
point where $\mathbf{W} = \mathbf{0}$, under mild conditions. The fundamental
nature of this lemma directly explains structure discovery and has immediate
applications in other domains including an end-to-end approximation for MAXCUT,
and computing Johnson-Lindenstrauss embeddings.

</details>


### [184] [Learning Upper Lower Value Envelopes to Shape Online RL: A Principled Approach](https://arxiv.org/abs/2510.19528)
*Sebastian Reboul,Hélène Halconruy,Randal Douc*

Main category: stat.ML

TL;DR: 研究利用离线数据加速在线强化学习问题，提出两阶段框架，分析有高概率后悔界，实验显示比UCBVI和先前方法减少大量后悔值。


<details>
  <summary>Details</summary>
Motivation: 利用离线数据加速在线强化学习方向有潜力但理论基础有限，研究在此情境下学习和应用价值包络。

Method: 引入两阶段框架，第一阶段用离线数据推导价值函数上下界，第二阶段将学习到的界融入在线算法，解耦上下界，数据驱动建模价值包络。

Result: 分析得出由两个可解释量确定的高概率后悔界，在表格型MDPs上实验显示比UCBVI和先前方法大幅减少后悔值。

Conclusion: 所提方法为离线预训练和在线微调建立了正式联系，能有效利用离线数据加速在线强化学习。

Abstract: We investigate the fundamental problem of leveraging offline data to
accelerate online reinforcement learning - a direction with strong potential
but limited theoretical grounding. Our study centers on how to learn and apply
value envelopes within this context. To this end, we introduce a principled
two-stage framework: the first stage uses offline data to derive upper and
lower bounds on value functions, while the second incorporates these learned
bounds into online algorithms. Our method extends prior work by decoupling the
upper and lower bounds, enabling more flexible and tighter approximations. In
contrast to approaches that rely on fixed shaping functions, our envelopes are
data-driven and explicitly modeled as random variables, with a filtration
argument ensuring independence across phases. The analysis establishes
high-probability regret bounds determined by two interpretable quantities,
thereby providing a formal bridge between offline pre-training and online
fine-tuning. Empirical results on tabular MDPs demonstrate substantial regret
reductions compared with both UCBVI and prior methods.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [185] [Semi-Implicit Approaches for Large-Scale Bayesian Spatial Interpolation](https://arxiv.org/abs/2510.19722)
*Sébastien Garneau,Carlos T. P. Zanini,Alexandra M. Schmidt*

Main category: stat.CO

TL;DR: 本文提出用SIVI进行可扩展的贝叶斯空间插值，对比多种方法，SIVI结果与HMC相似但速度快很多，凸显其在空间统计中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程在空间统计中计算成本随位置数量迅速增加，需要高效方法。

Method: 提出使用SIVI进行可扩展贝叶斯空间插值，对比SIVI与ADVI、Pathfinder、HMC等方法，基于CRPS、区间得分和负对数预测密度评估。

Result: SIVI结果与HMC相似，但速度大幅提升，如在泊松场景中从约6小时降至130秒，SIVI - NNGP能在两分钟内分析150,000个位置的数据集。

Conclusion: SIVI是空间统计中一种灵活且可扩展的推理技术。

Abstract: Spatial statistics often rely on Gaussian processes (GPs) to capture
dependencies across locations. However, their computational cost increases
rapidly with the number of locations, potentially needing multiple hours even
for moderate sample sizes. To address this, we propose using Semi-Implicit
Variational Inference (SIVI), a highly flexible Bayesian approximation method,
for scalable Bayesian spatial interpolation. We evaluated SIVI with a GP prior
and a Nearest-Neighbour Gaussian Process (NNGP) prior compared to Automatic
Differentiation Variational Inference (ADVI), Pathfinder, and Hamiltonian Monte
Carlo (HMC), the reference method in spatial statistics. Methods were compared
based on their predictive ability measured by the CRPS, the interval score, and
the negative log-predictive density across 50 replicates for both Gaussian and
Poisson outcomes. SIVI-based methods achieved similar results to HMC, while
being drastically faster. On average, for the Poisson scenario with 500
training locations, SIVI reduced the computational time from roughly 6 hours
for HMC to 130 seconds. Furthermore, SIVI-NNGP analyzed a simulated land
surface temperature dataset of 150,000 locations while estimating all unknown
model parameters in under two minutes. These results highlight the potential of
SIVI as a flexible and scalable inference technique in spatial statistics.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [186] [Shrinkage to Infinity: Reducing Test Error by Inflating the Minimum Norm Interpolator in Linear Models](https://arxiv.org/abs/2510.19206)
*Jake Freeman*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Hastie et al. (2022) found that ridge regularization is essential in high
dimensional linear regression $y=\beta^Tx + \epsilon$ with isotropic
co-variates $x\in \mathbb{R}^d$ and $n$ samples at fixed $d/n$. However, Hastie
et al. (2022) also notes that when the co-variates are anisotropic and $\beta$
is aligned with the top eigenvalues of population covariance, the "situation is
qualitatively different." In the present article, we make precise this
observation for linear regression with highly anisotropic covariances and
diverging $d/n$. We find that simply scaling up (or inflating) the minimum
$\ell_2$ norm interpolator by a constant greater than one can improve the
generalization error. This is in sharp contrast to traditional
regularization/shrinkage prescriptions. Moreover, we use a data-splitting
technique to produce consistent estimators that achieve generalization error
comparable to that of the optimally inflated minimum-norm interpolator. Our
proof relies on apparently novel matching upper and lower bounds for
expectations of Gaussian random projections for a general class of anisotropic
covariance matrices when $d/n\to \infty$.

</details>


### [187] [Error Analysis of Triangular Optimal Transport Maps for Filtering](https://arxiv.org/abs/2510.19283)
*Mohammad Al-Jarrah,Bamdad Hosseini,Niyizhen Jin,Michele Martino,Amirhossein Taghvaei*

Main category: math.ST

TL;DR: 对一类基于最优传输的滤波和数据同化算法进行估计误差的系统分析，扩展条件Brenier映射误差分析，应用于滤波场景并给出算法扩展和数值基准。


<details>
  <summary>Details</summary>
Motivation: 对基于最优传输的滤波和数据同化算法进行误差分析，挖掘算法潜力。

Method: 扩展先前Brenier映射误差分析到条件Brenier映射，应用结果分析特定最优传输滤波算法。

Result: 对最优传输滤波算法进行分析，给出算法扩展及不同非高斯和高维示例的数值基准。

Conclusion: 算法具有有效性和实际应用潜力。

Abstract: We present a systematic analysis of estimation errors for a class of optimal
transport based algorithms for filtering and data assimilation. Along the way,
we extend previous error analyses of Brenier maps to the case of conditional
Brenier maps that arise in the context of simulation based inference. We then
apply these results in a filtering scenario to analyze the optimal transport
filtering algorithm of Al-Jarrah et al. (2024, ICML). An extension of that
algorithm along with numerical benchmarks on various non-Gaussian and
high-dimensional examples are provided to demonstrate its effectiveness and
practical potential.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [188] [The Black Tuesday Attack: how to crash the stock market with adversarial examples to financial forecasting models](https://arxiv.org/abs/2510.18990)
*Thomas Hofweber,Jefrey Bergl,Ian Reyes,Amir Sadovnik*

Main category: cs.CR

TL;DR: 研究通过操纵个股价值制造对抗样本引发股市崩盘的可能性，提出攻击方式、理论基础及防御方法，强调需重视该威胁并开展防御研究。


<details>
  <summary>Details</summary>
Motivation: 指出通过小幅度操纵个股价值制造对抗样本引发股市崩盘这一情况对金融稳定有重大风险，敌对势力可借此造成经济损失，此威胁被严重低估。

Method: 概述这种攻击可能的实施过程、理论依据，以及如何针对整体经济或单个公司进行攻击，并探讨防御方法。

Result: 明确了通过操纵个股价值制造对抗样本来引发股市崩盘的可能性及相关情况。

Conclusion: 该威胁被大大低估，需要对防御方法进行紧急研究。

Abstract: We investigate and defend the possibility of causing a stock market crash via
small manipulations of individual stock values that together realize an
adversarial example to financial forecasting models, causing these models to
make the self-fulfilling prediction of a crash. Such a crash triggered by an
adversarial example would likely be hard to detect, since the model's
predictions would be accurate and the interventions that would cause it are
minor. This possibility is a major risk to financial stability and an
opportunity for hostile actors to cause great economic damage to an adversary.
This threat also exists against individual stocks and the corresponding
valuation of individual companies. We outline how such an attack might proceed,
what its theoretical basis is, how it can be directed towards a whole economy
or an individual company, and how one might defend against it. We conclude that
this threat is vastly underappreciated and requires urgent research on how to
defend against it.

</details>


### [189] [Collaborative penetration testing suite for emerging generative AI algorithms](https://arxiv.org/abs/2510.19303)
*Petar Radanliev*

Main category: cs.CR

TL;DR: 本文聚焦生成式AI漏洞与量子威胁，提出协作渗透测试套件解决方案，取得识别大量漏洞、降低高风险问题、高效解决漏洞及维持量子密码完整性等成果，形成量子AI安全协议。


<details>
  <summary>Details</summary>
Motivation: 应对生成式AI的模型反转、数据投毒等漏洞，以及Shor算法破解加密的量子威胁，保障生成式AI模型免受经典和量子网络攻击。

Method: 构建协作渗透测试套件，包含DAST、SAST等五个集成组件，IAST与CI/CD管道集成，采用区块链日志记录，运用基于格的RLWE协议进行量子加密，开展AI红队模拟攻击，并设置统一工作流。

Result: 在测试环境中识别300多个漏洞，两周内高严重度问题减少70%，区块链记录的漏洞解决效率达90%，量子抗性密码在测试中保持100%完整性。

Conclusion: 形成了集成区块链、量子密码学和AI红队测试的量子AI安全协议。

Abstract: Problem Space: AI Vulnerabilities and Quantum Threats Generative AI
vulnerabilities: model inversion, data poisoning, adversarial inputs. Quantum
threats Shor Algorithm breaking RSA ECC encryption. Challenge Secure generative
AI models against classical and quantum cyberattacks. Proposed Solution
Collaborative Penetration Testing Suite Five Integrated Components: DAST SAST
OWASP ZAP, Burp Suite, SonarQube, Fortify. IAST Contrast Assess integrated with
CI CD pipeline. Blockchain Logging Hyperledger Fabric for tamper-proof logs.
Quantum Cryptography Lattice based RLWE protocols. AI Red Team Simulations
Adversarial ML & Quantum-assisted attacks. Integration Layer: Unified workflow
for AI, cybersecurity, and quantum experts. Key Results 300+ vulnerabilities
identified across test environments. 70% reduction in high-severity issues
within 2 weeks. 90% resolution efficiency for blockchain-logged
vulnerabilities. Quantum-resistant cryptography maintained 100% integrity in
tests. Outcome: Quantum AI Security Protocol integrating Blockchain Quantum
Cryptography AI Red Teaming.

</details>


### [190] [LAPRAD: LLM-Assisted PRotocol Attack Discovery](https://arxiv.org/abs/2510.19264)
*R. Can Aygun,Yehuda Afek,Anat Bremler-Barr,Leonard Kleinrock*

Main category: cs.CR

TL;DR: 本文提出LLM辅助协议攻击发现方法LAPRAD，用于发现协议漏洞，发现了DNS协议的3个新DDoS攻击和2个已报告攻击。


<details>
  <summary>Details</summary>
Motivation: 提高互联网协议安全性，寻找更快、半自动化的方法来发现DNS、BGP等协议的新漏洞。

Method: LAPRAD采用三阶段流程，先咨询经DNS相关语料和DDoS攻击训练的LLM识别潜在攻击，再用不同LLM通过ReACT方法构建攻击配置，最后验证攻击功能和有效性。

Result: 发现DNS协议的3个新DDoS攻击和2个未在LLM训练数据中的已报告攻击，新攻击可绕过现有补丁，降低解析器查询能力。

Conclusion: LAPRAD方法能帮助有一定DNS知识的安全研究人员有效发现难以检测的协议漏洞。

Abstract: With the goal of improving the security of Internet protocols, we seek
faster, semi-automatic methods to discover new vulnerabilities in protocols
such as DNS, BGP, and others. To this end, we introduce the LLM-Assisted
Protocol Attack Discovery (LAPRAD) methodology, enabling security researchers
with some DNS knowledge to efficiently uncover vulnerabilities that would
otherwise be hard to detect.
  LAPRAD follows a three-stage process. In the first, we consult an LLM
(GPT-o1) that has been trained on a broad corpus of DNS-related sources and
previous DDoS attacks to identify potential exploits. In the second stage, a
different LLM automatically constructs the corresponding attack configurations
using the ReACT approach implemented via LangChain (DNS zone file generation).
Finally, in the third stage, we validate the attack's functionality and
effectiveness.
  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and
rediscovered two recently reported ones that were not included in the LLM's
training data. The first new attack employs a bait-and-switch technique to
trick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving
capacity to as little as 6%. The second exploits large DNSSEC encryption
algorithms (RSA-4096) with multiple keys, thereby bypassing a recently
implemented default RRSet limit. The third leverages ANY-type responses to
produce a similar effect.
  These variations of a cache-flushing DDoS attack, called SigCacheFlush,
circumvent existing patches, severely degrade resolver query capacity, and
impact the latest versions of major DNS resolver implementations.

</details>


### [191] [Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation](https://arxiv.org/abs/2510.19420)
*Chengcan Wu,Zhixin Zhang,Mingqian Xu,Zeming Wei,Meng Sun*

Main category: cs.CR

TL;DR: 提出基于大语言模型的多智能体系统（MAS）的动态防御范式，实验显示优于现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有MAS信任问题受关注，基于图表示的防御机制多为静态图防御，有局限性。

Method: 提出动态防御范式，持续监控MAS图内通信，动态调整图拓扑，破坏恶意通信。

Result: 在复杂动态MAS环境实验中，该方法显著优于现有MAS防御机制。

Conclusion: 该方法为MAS可信应用提供有效保障，代码开源。

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have become a
popular paradigm of AI applications. However, trustworthiness issues in MAS
remain a critical concern. Unlike challenges in single-agent systems, MAS
involve more complex communication processes, making them susceptible to
corruption attacks. To mitigate this issue, several defense mechanisms have
been developed based on the graph representation of MAS, where agents represent
nodes and communications form edges. Nevertheless, these methods predominantly
focus on static graph defense, attempting to either detect attacks in a fixed
graph structure or optimize a static topology with certain defensive
capabilities. To address this limitation, we propose a dynamic defense paradigm
for MAS graph structures, which continuously monitors communication within the
MAS graph, then dynamically adjusts the graph topology, accurately disrupts
malicious communications, and effectively defends against evolving and diverse
dynamic attacks. Experimental results in increasingly complex and dynamic MAS
environments demonstrate that our method significantly outperforms existing MAS
defense mechanisms, contributing an effective guardrail for their trustworthy
applications. Our code is available at
https://github.com/ChengcanWu/Monitoring-LLM-Based-Multi-Agent-Systems.

</details>


### [192] [HAMLOCK: HArdware-Model LOgically Combined attacK](https://arxiv.org/abs/2510.19145)
*Sanskar Amgain,Daniel Lobo,Atri Chatterjee,Swarup Bhunia,Fnu Suya*

Main category: cs.CR

TL;DR: 本文提出HAMLOCK攻击，跨软硬件边界分布攻击逻辑，隐蔽性高，攻击成功率高且能绕过现有防御，揭示软硬件接口关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 第三方硬件加速器用于DNN带来新安全漏洞，传统模型级后门攻击易检测。

Method: 提出HAMLOCK攻击，微调模型少量神经元激活值，硬件木马监测并操纵最终输出。

Result: 在多个基准测试中攻击成功率接近完美，清洁准确率下降可忽略，绕过现有模型级防御，硬件木马难检测，开销低。

Conclusion: 软硬件接口存在关键漏洞，需新的跨层防御机制。

Abstract: The growing use of third-party hardware accelerators (e.g., FPGAs, ASICs) for
deep neural networks (DNNs) introduces new security vulnerabilities.
Conventional model-level backdoor attacks, which only poison a model's weights
to misclassify inputs with a specific trigger, are often detectable because the
entire attack logic is embedded within the model (i.e., software), creating a
traceable layer-by-layer activation path.
  This paper introduces the HArdware-Model Logically Combined Attack (HAMLOCK),
a far stealthier threat that distributes the attack logic across the
hardware-software boundary. The software (model) is now only minimally altered
by tuning the activations of few neurons to produce uniquely high activation
values when a trigger is present. A malicious hardware Trojan detects those
unique activations by monitoring the corresponding neurons' most significant
bit or the 8-bit exponents and triggers another hardware Trojan to directly
manipulate the final output logits for misclassification.
  This decoupled design is highly stealthy, as the model itself contains no
complete backdoor activation path as in conventional attacks and hence, appears
fully benign. Empirically, across benchmarks like MNIST, CIFAR10, GTSRB, and
ImageNet, HAMLOCK achieves a near-perfect attack success rate with a negligible
clean accuracy drop. More importantly, HAMLOCK circumvents the state-of-the-art
model-level defenses without any adaptive optimization. The hardware Trojan is
also undetectable, incurring area and power overheads as low as 0.01%, which is
easily masked by process and environmental noise. Our findings expose a
critical vulnerability at the hardware-software interface, demanding new
cross-layer defenses against this emerging threat.

</details>


### [193] [From See to Shield: ML-Assisted Fine-Grained Access Control for Visual Data](https://arxiv.org/abs/2510.19418)
*Mete Harun Akcay,Buse Gul Atli,Siddharth Prakash Rao,Alexandros Bakas*

Main category: cs.CR

TL;DR: 提出一种带策略驱动访问控制的可信数据共享系统架构，在视觉数据集上评估，结果显示有效、高效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 随着存储数据量增长，在大型存储库中识别和保护敏感信息，尤其是多用户不同权限共享时，变得极具挑战。

Method: 提出的架构集成四个核心模块，采用混合加密方案，支持高效密钥分发并隔离密钥存储。

Result: 系统在视觉数据集上有效检测隐私敏感对象，提高宏平均F1分数5%和平均精度均值10%，每张图像策略强制解密时间小于1秒。

Conclusion: 提出的解决方案对细粒度访问控制有效、高效且可扩展。

Abstract: As the volume of stored data continues to grow, identifying and protecting
sensitive information within large repositories becomes increasingly
challenging, especially when shared with multiple users with different roles
and permissions. This work presents a system architecture for trusted data
sharing with policy-driven access control, enabling selective protection of
sensitive regions while maintaining scalability. The proposed architecture
integrates four core modules that combine automated detection of sensitive
regions, post-correction, key management, and access control. Sensitive regions
are secured using a hybrid scheme that employs symmetric encryption for
efficiency and Attribute-Based Encryption for policy enforcement. The system
supports efficient key distribution and isolates key storage to strengthen
overall security. To demonstrate its applicability, we evaluate the system on
visual datasets, where Privacy-Sensitive Objects in images are automatically
detected, reassessed, and selectively encrypted prior to sharing in a data
repository. Experimental results show that our system provides effective PSO
detection, increases macro-averaged F1 score (5%) and mean Average Precision
(10%), and maintains an average policy-enforced decryption time of less than 1
second per image. These results demonstrate the effectiveness, efficiency and
scalability of our proposed solution for fine-grained access control.

</details>


### [194] [Exploring the Effect of DNN Depth on Adversarial Attacks in Network Intrusion Detection Systems](https://arxiv.org/abs/2510.19761)
*Mohamed ElShehaby,Ashraf Matrawy*

Main category: cs.CR

TL;DR: 本文研究DNN层深度对网络入侵检测系统（NIDS）对抗攻击鲁棒性的影响，对比NIDS和计算机视觉领域，发现NIDS中增加层数未必提升性能，还可能降低鲁棒性，计算机视觉领域影响较小。


<details>
  <summary>Details</summary>
Motivation: 探究增加深度神经网络的层数是否会影响其在网络入侵检测系统（NIDS）中对抗对抗攻击的鲁棒性。

Method: 比较不同深度神经网络在NIDS和计算机视觉领域的对抗鲁棒性。

Result: 在NIDS领域，增加层数不一定提高性能，还可能显著降低对抗攻击的鲁棒性；在计算机视觉领域，增加层数对鲁棒性影响较小。

Conclusion: 研究结果可指导NIDS应用中鲁棒神经网络的开发，凸显网络安全领域在机器学习中的独特性。

Abstract: Adversarial attacks pose significant challenges to Machine Learning (ML)
systems and especially Deep Neural Networks (DNNs) by subtly manipulating
inputs to induce incorrect predictions. This paper investigates whether
increasing the layer depth of deep neural networks affects their robustness
against adversarial attacks in the Network Intrusion Detection System (NIDS)
domain. We compare the adversarial robustness of various deep neural networks
across both \ac{NIDS} and computer vision domains (the latter being widely used
in adversarial attack experiments). Our experimental results reveal that in the
NIDS domain, adding more layers does not necessarily improve their performance,
yet it may actually significantly degrade their robustness against adversarial
attacks. Conversely, in the computer vision domain, adding more layers exhibits
a more modest impact on robustness. These findings can guide the development of
robust neural networks for (NIDS) applications and highlight the unique
characteristics of network security domains within the (ML) landscape.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [195] [Efficient scenario analysis in real-time Bayesian election forecasting via sequential meta-posterior sampling](https://arxiv.org/abs/2510.19133)
*Geonhee Han,Andrew Gelman,Aki Vehtari*

Main category: stat.ME

TL;DR: 本文提出元建模策略和顺序采样方案，克服实时分析计算瓶颈，实现无重复拟合的实时推理和场景、敏感性分析，回测显示有计算增益并揭示敏感性模式。


<details>
  <summary>Details</summary>
Motivation: 实时后验更新、模型检查和沟通在贝叶斯聚合选举预测中带来实际方法挑战，标准MCMC使实时分析计算成本高。

Method: 引入元建模策略并结合顺序采样方案，通过遍历后验元模型实现实时推理和分析。

Result: 回测中显示出显著计算增益，揭示了非平凡的敏感性模式，如预测对基本面预测先验置信度有响应，对随机游走尺度响应较小。

Conclusion: 该方法能有效克服计算瓶颈，有助于明确民意调查数据与结构假设的相对影响。

Abstract: Bayesian aggregation lets election forecasters combine diverse sources of
information, such as state polls and economic and political indicators: as in
our collaboration with The Economist magazine. However, the demands of
real-time posterior updating, model checking, and communication introduce
practical methodological challenges. In particular, sensitivity and scenario
analysis help trace forecast shifts to model assumptions and understand model
behavior. Yet, under standard Markov chain Monte Carlo, even small tweaks to
the model (e.g., in priors, data, hyperparameters) require full refitting,
making such real-time analysis computationally expensive. To overcome the
bottleneck, we introduce a meta-modeling strategy paired with a sequential
sampling scheme; by traversing posterior meta-models, we enable real-time
inference and structured scenario and sensitivity analysis without repeated
refitting. In a back-test of the model, we show substantial computational gains
and uncover non-trivial sensitivity patterns. For example, forecasts remain
responsive to prior confidence in fundamentals-based forecasts, but less so to
random walk scale; these help clarify the relative influence of polling data
versus structural assumptions. Code is available at
https://github.com/geonhee619/SMC-Sense.

</details>


### [196] [No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence](https://arxiv.org/abs/2510.19212)
*Ernest Fokoué*

Main category: stat.ME

TL;DR: 本文指出AI理论与方法核心是统计，解构AI九大支柱，强调统计为其提供基础，呼吁重视统计根基。


<details>
  <summary>Details</summary>
Motivation: 揭示AI理论与方法核心是统计，改变AI仅源于计算机科学与工程的片面认知，推动开发更可靠、可解释和可信的智能系统。

Method: 将AI解构为九大支柱，追溯各支柱的统计原理。

Result: 证明AI各支柱均基于统计原理，统计为AI提供理论框架等，计算机科学提供算法和硬件。

Conclusion: 认识统计根基是开发智能系统必要步骤，应在教育、研究和实践中重新重视统计基础。

Abstract: The rapid ascent of artificial intelligence (AI) is often portrayed as a
revolution born from computer science and engineering. This narrative, however,
obscures a fundamental truth: the theoretical and methodological core of AI is,
and has always been, statistical. This paper systematically argues that the
field of statistics provides the indispensable foundation for machine learning
and modern AI. We deconstruct AI into nine foundational pillars-Inference,
Density Estimation, Sequential Learning, Generalization, Representation
Learning, Interpretability, Causality, Optimization, and
Unification-demonstrating that each is built upon century-old statistical
principles. From the inferential frameworks of hypothesis testing and
estimation that underpin model evaluation, to the density estimation roots of
clustering and generative AI; from the time-series analysis inspiring recurrent
networks to the causal models that promise true understanding, we trace an
unbroken statistical lineage. While celebrating the computational engines that
power modern AI, we contend that statistics provides the brain-the theoretical
frameworks, uncertainty quantification, and inferential goals-while computer
science provides the brawn-the scalable algorithms and hardware. Recognizing
this statistical backbone is not merely an academic exercise, but a necessary
step for developing more robust, interpretable, and trustworthy intelligent
systems. We issue a call to action for education, research, and practice to
re-embrace this statistical foundation. Ignoring these roots risks building a
fragile future; embracing them is the path to truly intelligent machines. There
is no machine learning without statistical learning; no artificial intelligence
without statistical thought.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [197] [Emergence of Internal State-Modulated Swarming in Multi-Agent Patch Foraging System](https://arxiv.org/abs/2510.18886)
*Siddharth Chaturvedi,Ahmed EL-Gazzar,Marcel van Gerven*

Main category: nlin.AO

TL;DR: 本文模拟非合作觅食者在二维空间觅食，用连续时间循环神经网络进化共享策略，展示觅食者自适应觅食及无资源时的聚集行为，发现聚集强度与储存资源量成反比。


<details>
  <summary>Details</summary>
Motivation: 验证非合作觅食者在部分可观测环境中因另一觅食者存在作为潜在食物源信号而出现聚集行为这一现象。

Method: 模拟多个自驱动觅食者在二维连续空间非合作觅食，用进化策略算法进化连续时间循环神经网络作为速度控制器。

Result: 觅食者能学会自适应觅食，无资源时出现聚集行为，聚集强度与储存资源量成反比，控制器隐藏状态对储存资源量敏感。

Conclusion: 支持了风险敏感觅食的说法，通过控制隐藏状态可加速聚集行为。

Abstract: Active particles are entities that sustain persistent out-of-equilibrium
motion by consuming energy. Under certain conditions, they exhibit the tendency
to self-organize through coordinated movements, such as swarming via
aggregation. While performing non-cooperative foraging tasks, the emergence of
such swarming behavior in foragers, exemplifying active particles, has been
attributed to the partial observability of the environment, in which the
presence of another forager can serve as a proxy signal to indicate the
potential presence of a food source or a resource patch. In this paper, we
validate this phenomenon by simulating multiple self-propelled foragers as they
forage from multiple resource patches in a non-cooperative manner. These
foragers operate in a continuous two-dimensional space with stochastic position
updates and partial observability. We evolve a shared policy in the form of a
continuous-time recurrent neural network that serves as a velocity controller
for the foragers. To this end, we use an evolutionary strategy algorithm
wherein the different samples of the policy-distribution are evaluated in the
same rollout. Then we show that agents are able to learn to adaptively forage
in the environment. Next, we show the emergence of swarming in the form of
aggregation among the foragers when resource patches are absent. We observe
that the strength of this swarming behavior appears to be inversely
proportional to the amount of resource stored in the foragers, which supports
the risk-sensitive foraging claims. Empirical analysis of the learned
controller's hidden states in minimal test runs uncovers their sensitivity to
the amount of resource stored in a forager. Clamping these hidden states to
represent a lesser amount of resource hastens its learned aggregation behavior.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [198] [A Unified Formal Theory on the Logical Limits of Symbol Grounding](https://arxiv.org/abs/2509.20409)
*Zhangchi Liu*

Main category: cs.LO

TL;DR: 本文构建符号接地问题逻辑限制的统一理论，证明意义奠基是开放、非算法过程，揭示自包含智能系统的根本限制。


<details>
  <summary>Details</summary>
Motivation: 构建关于符号接地问题逻辑限制的统一理论。

Method: 通过四阶段论证，依次证明纯符号系统、有有限静态预设意义的系统的局限性，符号与外部意义连接的非逻辑性，以及固定外部判断算法更新过程的不完整性。

Result: 证明了意义的奠基是一个必然开放、非算法的过程。

Conclusion: 揭示了任何自包含智能系统存在类似哥德尔式的根本限制。

Abstract: This paper synthesizes a series of formal proofs to construct a unified
theory on the logical limits of the Symbol Grounding Problem. We demonstrate
through a four-stage argument that meaning within a formal system must arise
from a process that is external, dynamic, and non-algorithmic. First, we prove
that any purely symbolic system, devoid of external connections, cannot
internally establish a consistent foundation for meaning due to
self-referential paradoxes. Second, we extend this limitation to systems with
any finite, static set of pre-established meanings, proving they are inherently
incomplete. Third, we demonstrate that the very "act" of connecting an internal
symbol to an external meaning cannot be a product of logical inference within
the system but must be an axiomatic, meta-level update. Finally, we prove that
any attempt to automate this update process using a fixed, external "judgment"
algorithm will inevitably construct a larger, yet equally incomplete, symbolic
system. Together, these conclusions formally establish that the grounding of
meaning is a necessarily open-ended, non-algorithmic process, revealing a
fundamental, G\"odel-style limitation for any self-contained intelligent
system.

</details>


### [199] [Knowledge and Common Knowledge of Strategies](https://arxiv.org/abs/2510.19298)
*Borja Sierra Miranda,Thomas Studer*

Main category: cs.LO

TL;DR: 提出可细粒度指定策略知识的模型，研究高阶策略知识影响、共识问题及模型检查问题可判定性


<details>
  <summary>Details</summary>
Motivation: 现有战略推理工作多采用知情或不知情语义，需更细粒度指定策略知识的模型

Method: 提出可细粒度指定策略知识的模型，通过研究Hanabi游戏说明高阶策略知识影响，探讨共识问题与共同策略知识关系，研究模型检查问题可判定性

Result: 说明了高阶策略知识的影响，表明共同策略知识对解决共识问题必要，研究了模型检查问题可判定性

Conclusion: 提出的细粒度指定策略知识模型有一定理论和应用价值，如对解决共识问题有指导意义

Abstract: Most existing work on strategic reasoning simply adopts either an informed or
an uninformed semantics. We propose a model where knowledge of strategies can
be specified on a fine-grained level. In particular, it is possible to
distinguish first-order, higher-order, and common knowledge of strategies. We
illustrate the effect of higher-order knowledge of strategies by studying the
game Hanabi. Further, we show that common knowledge of strategies is necessary
to solve the consensus problem. Finally, we study the decidability of the model
checking problem.

</details>


### [200] [Universal Quantitative Abstraction: Categorical Duality and Logical Completeness for Probabilistic Systems](https://arxiv.org/abs/2510.19444)
*Nivar Anwer*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A unified theory of quantitative abstraction is presented for probabilistic
systems that links category theory, optimal transport, and quantitative modal
logic. At its core is a canonical $ \varepsilon $-quotient endowed with a
universal property: among all $ \varepsilon $-abstractions, it is the most
informative one that respects a prescribed bound on value loss. This
construction induces an adjunction between abstraction and realization functors
$ (Q_{\varepsilon} \dashv R_{\varepsilon}) $, established via the Special
Adjoint Functor Theorem, revealing a categorical duality between metric
structure and logical semantics. A behavioral pseudometric is characterized as
the unique fixed point of a Bellman-style operator, with contraction and
Lipschitz properties proved in a coalgebraic setting. A quantitative modal $
\mu $-calculus is introduced and shown to be expressively complete for
logically representable systems, so that behavioral distance coincides with
maximal logical deviation. Compositionality under interface refinement is
analyzed, clarifying how abstractions interact across system boundaries. An
exact validation suite on finite Markov decision processes corroborates the
contraction property, value-loss bounds, stability under perturbation,
adversarial distinguishability, and scalability, demonstrating both robustness
and computational feasibility. The resulting framework provides principled
targets for state aggregation and representation learning, with mathematically
precise guarantees for value-function approximation in stochastic domains.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [201] [Demonstrating Real Advantage of Machine-Learning-Enhanced Monte Carlo for Combinatorial Optimization](https://arxiv.org/abs/2510.19544)
*Luca Maria Del Bono,Federico Ricci-Tersenghi,Francesco Zamponi*

Main category: cond-mat.dis-nn

TL;DR: 本文聚焦三维Ising自旋玻璃的QUBO问题，采用结合机器学习的全局退火蒙特卡罗算法，证明其优于传统算法，为机器学习辅助优化方法超越经典方法提供证据。


<details>
  <summary>Details</summary>
Motivation: 机器学习辅助的组合优化方法未稳定超越简单的经典方法，需寻找更优方法。

Method: 使用结合标准局部移动和机器学习提出的全局移动的全局退火蒙特卡罗算法。

Result: 全局退火算法不仅超越模拟退火算法的性能，且比种群退火算法更具鲁棒性，无需超参数调整。

Conclusion: 首次明确有力证明机器学习辅助优化方法在组合优化中可超越经典先进技术。

Abstract: Combinatorial optimization problems are central to both practical
applications and the development of optimization methods. While classical and
quantum algorithms have been refined over decades, machine learning-assisted
approaches are comparatively recent and have not yet consistently outperformed
simple, state-of-the-art classical methods. Here, we focus on a class of
Quadratic Unconstrained Binary Optimization (QUBO) problems, specifically the
challenge of finding minimum energy configurations in three-dimensional Ising
spin glasses. We use a Global Annealing Monte Carlo algorithm that integrates
standard local moves with global moves proposed via machine learning. We show
that local moves play a crucial role in achieving optimal performance.
Benchmarking against Simulated Annealing and Population Annealing, we
demonstrate that Global Annealing not only surpasses the performance of
Simulated Annealing but also exhibits greater robustness than Population
Annealing, maintaining effectiveness across problem hardness and system size
without hyperparameter tuning. These results provide, to our knowledge, the
first clear and robust evidence that a machine learning-assisted optimization
method can exceed the capabilities of classical state-of-the-art techniques in
a combinatorial optimization setting.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [202] [Bridging Earth and Space: A Survey on HAPS for Non-Terrestrial Networks](https://arxiv.org/abs/2510.19731)
*G. Svistunov,A. Akhtarshenas,D. López-Pérez,M. Giordani,G. Geraci,H. Yanikomeroglu*

Main category: eess.SY

TL;DR: 本文全面综述了HAPS在6G生态系统中的用例、技术和集成策略，指出其是6G网络基础组件并列出研究挑战。


<details>
  <summary>Details</summary>
Motivation: HAPS作为6G无线网络关键推动者，需全面了解其在6G生态系统中的情况。

Method: 综述HAPS用例、技术和集成策略，回顾网络集成架构，介绍实地试验，分析关键技术。

Result: 阐述了HAPS在6G中的多种作用，回顾相关架构与试验，分析关键技术，列出研究挑战。

Conclusion: HAPS是全球集成、有弹性和可持续的6G网络的基础组件。

Abstract: HAPS are emerging as key enablers in the evolution of 6G wireless networks,
bridging terrestrial and non-terrestrial infrastructures. Operating in the
stratosphere, HAPS can provide wide-area coverage, low-latency,
energy-efficient broadband communications with flexible deployment options for
diverse applications. This survey delivers a comprehensive overview of HAPS use
cases, technologies, and integration strategies within the 6G ecosystem. The
roles of HAPS in extending connectivity to underserved regions, supporting
dynamic backhauling, enabling massive IoT, and delivering reliable low-latency
communications for autonomous and immersive services are discussed. The paper
reviews state-of-the-art architectures for terrestrial and non-terrestrial
network integration, highlights recent field trials. Furthermore, key enabling
technologies such as channel modeling, AI-driven resource allocation,
interference control, mobility management, and energy-efficient communications
are examined. The paper also outlines open research challenges. By addressing
existing gaps in the literature, this survey positions HAPS as a foundational
component of globally integrated, resilient, and sustainable 6G networks.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [203] [StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction](https://arxiv.org/abs/2510.18938)
*Qianheng Xu*

Main category: eess.AS

TL;DR: 介绍StutterZero和StutterFormer两种端到端波形到波形模型，将结巴语音转换为流畅语音并预测转录，在基准测试中表现良好，验证了端到端转换可行性。


<details>
  <summary>Details</summary>
Motivation: 全球超7000万人结巴，现有自动语音系统处理结巴语音不佳，现有结巴校正方法有缺陷。

Method: 提出StutterZero和StutterFormer模型，用卷积 - 双向LSTM编解码器和双流Transformer，在合成数据上训练，在FluencyBank数据集上评估。

Result: StutterZero比Whisper - Medium模型WER降低24%，BERTScore提高31%；StutterFormer表现更好，WER降低28%，BERTScore提高34%。

Conclusion: 验证了直接端到端结巴到流畅语音转换的可行性，为多领域带来新机遇。

Abstract: Over 70 million people worldwide experience stuttering, yet most automatic
speech systems misinterpret disfluent utterances or fail to transcribe them
accurately. Existing methods for stutter correction rely on handcrafted feature
extraction or multi-stage automatic speech recognition (ASR) and text-to-speech
(TTS) pipelines, which separate transcription from audio reconstruction and
often amplify distortions. This work introduces StutterZero and StutterFormer,
the first end-to-end waveform-to-waveform models that directly convert
stuttered speech into fluent speech while jointly predicting its transcription.
StutterZero employs a convolutional-bidirectional LSTM encoder-decoder with
attention, whereas StutterFormer integrates a dual-stream Transformer with
shared acoustic-linguistic representations. Both architectures are trained on
paired stuttered-fluent data synthesized from the SEP-28K and LibriStutter
corpora and evaluated on unseen speakers from the FluencyBank dataset. Across
all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a
31% improvement in semantic similarity (BERTScore) compared to the leading
Whisper-Medium model. StutterFormer achieved better results, with a 28%
decrease in WER and a 34% improvement in BERTScore. The results validate the
feasibility of direct end-to-end stutter-to-fluent speech conversion, offering
new opportunities for inclusive human-computer interaction, speech therapy, and
accessibility-oriented AI systems.

</details>


### [204] [EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection](https://arxiv.org/abs/2510.19414)
*Tong Zhang,Yihuan Huang,Yanzhen Ren*

Main category: eess.AS

TL;DR: 语音深度伪造问题严重，现有反欺骗系统对物理重放攻击表现不佳，提出EchoFake数据集，可提升检测模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有反欺骗系统在物理重放攻击下性能不佳的问题，推动欺骗检测方法发展。

Method: 构建包含超120小时、超13000名说话者音频的EchoFake数据集，涵盖先进零样本TTS语音和不同设备及场景下物理重放录音，并评估三个基线检测模型。

Result: 在EchoFake上训练的模型在各数据集上平均EER更低，泛化能力更好。

Conclusion: EchoFake为提升欺骗检测方法提供更现实基础。

Abstract: The growing prevalence of speech deepfakes has raised serious concerns,
particularly in real-world scenarios such as telephone fraud and identity
theft. While many anti-spoofing systems have demonstrated promising performance
on lab-generated synthetic speech, they often fail when confronted with
physical replay attacks-a common and low-cost form of attack used in practical
settings. Our experiments show that models trained on existing datasets exhibit
severe performance degradation, with average accuracy dropping to 59.6% when
evaluated on replayed audio. To bridge this gap, we present EchoFake, a
comprehensive dataset comprising more than 120 hours of audio from over 13,000
speakers, featuring both cutting-edge zero-shot text-to-speech (TTS) speech and
physical replay recordings collected under varied devices and real-world
environmental settings. Additionally, we evaluate three baseline detection
models and show that models trained on EchoFake achieve lower average EERs
across datasets, indicating better generalization. By introducing more
practical challenges relevant to real-world deployment, EchoFake offers a more
realistic foundation for advancing spoofing detection methods.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [205] [Remarks on a recent preprint of Chernikov and Towsner](https://arxiv.org/abs/2510.19665)
*Maryanthe Malliaris*

Main category: math.LO

TL;DR: 文章给出Chernikov和Towsner论文中定理的反例，指出其证明错误，原定理与高元PAC学习的联系消失，并说明其定义缺失关键方面。


<details>
  <summary>Details</summary>
Motivation: 指出Chernikov和Towsner论文中定理存在的问题，以及其定义的不足。

Method: 给出定理的反例，分析证明中的错误。

Result: 原定理有反例且证明错误，与高元PAC学习的联系消失。

Conclusion: Chernikov和Towsner论文的定义缺失关键方面。

Abstract: In this brief note, we first give a counterexample to a theorem in Chernikov
and Towsner, arXiv:2510.02420(1). In arXiv:2510.02420(2), the theorem has
changed but as we explain the proof has a mistake. The change in the statement,
due to changes in the underlying definition, affects the paper's claims. Since
that theorem had been relevant to connecting the work of their paper to
Coregliano-Malliaris high-arity PAC learning, a connection which now
disappears, we also explain why their definitions miss crucial aspects that our
work was designed to grapple with.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [206] [De-Risking Development in Sub-Saharan Africa: A Qualitative Study of Investment Dynamics in Angola](https://arxiv.org/abs/2510.18906)
*Carmen Berta C De Saituma Cagiza*

Main category: econ.GN

TL;DR: 研究以安哥拉为例，探讨撒哈拉以南非洲发展金融机构（DFIs）对降低发展风险的作用，发现其虽有推动作用但受制度等因素限制，长期影响依赖国内改革等。


<details>
  <summary>Details</summary>
Motivation: 研究DFIs如何通过塑造外国直接投资（FDI）流动和支持可持续经济转型，为撒哈拉以南非洲降低发展风险。

Method: 对国际发展顾问、外交事务专业人员和高级公共部门利益相关者进行定性访谈。

Result: DFIs是私营部门参与的催化剂，但受制度弱点和与国家发展优先事项不一致的限制。

Conclusion: DFIs在脆弱和资源依赖型环境中起重要推动作用，其长期影响取决于国内改革、治理改善和战略协调。

Abstract: This study investigates how Development Finance Institutions (DFIs)
contribute to de-risking development in Sub-Saharan Africa by shaping Foreign
Direct Investment (FDI) flows and supporting sustainable economic
transformation. Focusing on Angola as a representative case, the research draws
on qualitative interviews with international development advisors, foreign
affairs professionals, and senior public sector stakeholders. The study
explores how DFIs mitigate investment risk, enhance project credibility, and
promote diversification beyond extractive sectors. While DFIs are widely
recognized as catalysts for private sector engagement, particularly in
infrastructure, agriculture, and manufacturing, their effectiveness is often
constrained by institutional weaknesses and misalignment with national
development priorities. The findings suggest that DFIs play a crucial enabling
role in fragile and resource-dependent settings, but their long-term impact
depends on complementary domestic reforms, improved governance, and strategic
coordination. This research contributes to the literature on development
finance by offering grounded empirical insights from an under examined
Sub-Saharan context.

</details>


### [207] [Government Transparency Affects Innovation: Evidence from Wireless Products](https://arxiv.org/abs/2510.19377)
*Šimon Trlifaj*

Main category: econ.GN

TL;DR: 研究美国政府数据库推出对创新的影响，发现其使新技术使用约翻倍，强调信息对私营部门创新重要性。


<details>
  <summary>Details</summary>
Motivation: 探究政府透明度是否影响创新。

Method: 评估美国市场无线产品政府数据库的推出。

Result: 数据库推出后十年内新技术使用约翻倍，对同类和新产品均有影响，效果随时间减弱，对外国竞争者推动更大。

Conclusion: 信息对私营部门创新很重要。

Abstract: Does government transparency affect innovation? I evaluate the launch of a
government database with detailed technical information on the universe of
wireless-enabled products on the U.S. market (N 347 thousand). The results show
the launch approximately doubled the use of new technologies in the following
ten years, an indicator of follow-on innovation. The increase affected both
products in the same and new product classes, suggesting novelty; waned over
several years, potentially due to an increase in secrecy and patenting; and
boosted foreign more than U.S. domestic competitors. These results highlight
the importance of information for private sector innovation.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [208] [Prospects for Using Artificial Intelligence to Understand Intrinsic Kinetics of Heterogeneous Catalytic Reactions](https://arxiv.org/abs/2510.18911)
*Andrew J. Medford,Todd N. Whittaker,Bjarne Kreitz,David W. Flaherty,John R. Kitchin*

Main category: physics.chem-ph

TL;DR: 人工智能加速催化研究，但面临数据和模型问题，生成式和代理式AI可实现‘自动驾驶模型’。


<details>
  <summary>Details</summary>
Motivation: 解决将本征动力学与可观测值联系起来的‘多对一’挑战，实现对催化系统可解释、可重复和可转移的理解。

Method: 将AI与多尺度模型和多模态实验集成，利用机器学习力场、微动力学和反应器建模探索化学空间，结合原位和瞬态数据。

Result: 虽能加速模拟和材料发现，但数据质量不一致和模型复杂性限制了机理发现。

Conclusion: 生成式和代理式AI可自动化模型生成、量化不确定性、耦合理论与实验，实现‘自动驾驶模型’。

Abstract: Artificial intelligence (AI) is influencing heterogeneous catalysis research
by accelerating simulations and materials discovery. A key frontier is
integrating AI with multiscale models and multimodal experiments to address the
"many-to-one" challenge of linking intrinsic kinetics to observables. Advances
in machine-learned force fields, microkinetics, and reactor modeling enable
rapid exploration of chemical spaces, while operando and transient data provide
unprecedented insight. Yet, inconsistent data quality and model complexity
limit mechanistic discovery. Generative and agentic AI can automate model
generation, quantify uncertainty, and couple theory with experiment, realizing
"self-driving models" that produce interpretable, reproducible, and
transferable understanding of catalytic systems.

</details>


### [209] [Foundation Models for Discovery and Exploration in Chemical Space](https://arxiv.org/abs/2510.18900)
*Alexius Wadell,Anoushka Bhutani,Victor Azumah,Austin R. Ellis-Mohr,Celia Kelly,Hancheng Zhao,Anuj K. Nayak,Kareem Hegazy,Alexander Brace,Hongyi Lin,Murali Emani,Venkatram Vishwanath,Kevin Gering,Melisa Alkan,Tom Gibbs,Jack Wells,Lav R. Varshney,Bharath Ramsundar,Karthik Duraisamy,Michael W. Mahoney,Arvind Ramanathan,Venkatasubramanian Viswanathan*

Main category: physics.chem-ph

TL;DR: 开发分子基础模型MIST，在多领域表现出色，能解决实际问题，还可降低模型开发成本，推动材料发现。


<details>
  <summary>Details</summary>
Motivation: 现有计算和实验方法缺乏高效探索化学空间的扩展性，需科学基础模型来探索。

Method: 开发MIST模型，用新的标记化方案训练，微调以预测400多种结构 - 属性关系，用机制可解释性方法探测，制定超参数惩罚贝叶斯神经缩放定律。

Result: MIST模型在多个基准测试中达到或超过现有水平，能解决多种化学空间实际问题，可发现训练数据中未明确的模式和趋势，降低模型开发成本。

Conclusion: 本文方法和发现朝用基础模型加速材料发现、设计和优化迈出重要一步，为训练计算最优科学基础模型提供指导。

Abstract: Accurate prediction of atomistic, thermodynamic, and kinetic properties from
molecular structures underpins materials innovation. Existing computational and
experimental approaches lack the scalability required to efficiently navigate
chemical space. Scientific foundation models trained on large unlabeled
datasets offer a path toward exploring chemical space across diverse
application domains. Here we develop MIST, a family of molecular foundation
models with up to an order of magnitude more parameters and data than prior
works. Trained using a novel tokenization scheme that comprehensively captures
nuclear, electronic, and geometric information, MIST learns from a diverse
range of molecules. MIST models have been fine-tuned to predict more than 400
structure -- property relationships and match or exceed state-of-the-art
performance across benchmarks spanning physiology, electrochemistry, and
quantum chemistry. We demonstrate the ability of these models to solve
real-world problems across chemical space, including multiobjective electrolyte
solvent screening, olfactory perception mapping, isotope half-life prediction,
stereochemical reasoning for chiral organometallic compounds, and binary and
multi-component mixture property prediction. Probing MIST models using
mechanistic interpretability methods reveals identifiable patterns and trends
not explicitly present in the training data, suggesting that the models learn
generalizable scientific concepts. We formulate hyperparameter-penalized
Bayesian neural scaling laws and use them to reduce the computational cost of
model development by an order of magnitude. The methods and findings presented
here represent a significant step toward accelerating materials discovery,
design, and optimization using foundation models and provide valuable guidance
for training compute-optimal scientific foundation models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [210] [Local Guidance for Configuration-Based Multi-Agent Pathfinding](https://arxiv.org/abs/2510.19072)
*Tomoki Arita,Keisuke Okumura*

Main category: cs.MA

TL;DR: 研究提出为每个智能体提供局部引导的方法，实证表明能在合理时间内提升多智能体路径规划（MAPF）的解质量，应用于LaCAM时建立新性能边界。


<details>
  <summary>Details</summary>
Motivation: 现有全局引导的MAPF方法虽能从全局缓解拥堵，但本文探索提供局部引导的替代方法。

Method: 为规划器提供信息丰富的时空线索，以局部引导的方式应用于LaCAM求解器。

Result: 实证表明提供局部引导能在不过度超预算时间的情况下显著提升解质量。

Conclusion: 为每个智能体提供局部引导的方式为MAPF建立了新的性能边界。

Abstract: Guidance is an emerging concept that improves the empirical performance of
real-time, sub-optimal multi-agent pathfinding (MAPF) methods. It offers
additional information to MAPF algorithms to mitigate congestion on a global
scale by considering the collective behavior of all agents across the entire
workspace. This global perspective helps reduce agents' waiting times, thereby
improving overall coordination efficiency. In contrast, this study explores an
alternative approach: providing local guidance in the vicinity of each agent.
While such localized methods involve recomputation as agents move and may
appear computationally demanding, we empirically demonstrate that supplying
informative spatiotemporal cues to the planner can significantly improve
solution quality without exceeding a moderate time budget. When applied to
LaCAM, a leading configuration-based solver, this form of guidance establishes
a new performance frontier for MAPF.

</details>


### [211] [SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities](https://arxiv.org/abs/2510.19327)
*Usama Antuley,Shahbaz Siddiqui,Sufian Hameed,Waqas Arif,Subhan Shah,Syed Attique Shah*

Main category: cs.MA

TL;DR: 智能城市依赖智能互联服务，代理式AI虽有好处但部署面临GRC挑战，SORA - ATMAS评估显示能有效引导LLMs，有良好性能和跨域规则，可作为智能城市管理基础。


<details>
  <summary>Details</summary>
Motivation: 解决代理式AI在异构智能城市生态系统部署中的治理、风险和合规挑战。

Method: 用三个领域代理（天气、交通和安全）评估SORA - ATMAS，进行运行时分析和跨域规则设置。

Result: SORA - ATMAS使多个LLM产出符合政策的输出，平均MAE降低35%，有稳定监测、有效处理高风险情况和自适应信任调节能力，性能可扩展，跨域规则确保安全互操作性。

Conclusion: SORA - ATMAS是符合监管、上下文感知且可验证的治理框架，为智能城市管理提供可靠基础。

Abstract: The rapid evolution of smart cities has increased the reliance on intelligent
interconnected services to optimize infrastructure, resources, and citizen
well-being. Agentic AI has emerged as a key enabler by supporting autonomous
decision-making and adaptive coordination, allowing urban systems to respond in
real time to dynamic conditions. Its benefits are evident in areas such as
transportation, where the integration of traffic data, weather forecasts, and
safety sensors enables dynamic rerouting and a faster response to hazards.
However, its deployment across heterogeneous smart city ecosystems raises
critical governance, risk, and compliance (GRC) challenges, including
accountability, data privacy, and regulatory alignment within decentralized
infrastructures. Evaluation of SORA-ATMAS with three domain agents (Weather,
Traffic, and Safety) demonstrated that its governance policies, including a
fallback mechanism for high-risk scenarios, effectively steer multiple LLMs
(GPT, Grok, DeepSeek) towards domain-optimized, policy-aligned outputs,
producing an average MAE reduction of 35% across agents. Results showed stable
weather monitoring, effective handling of high-risk traffic plateaus 0.85, and
adaptive trust regulation in Safety/Fire scenarios 0.65. Runtime profiling of a
3-agent deployment confirmed scalability, with throughput between 13.8-17.2
requests per second, execution times below 72~ms, and governance delays under
100 ms, analytical projections suggest maintained performance at larger scales.
Cross-domain rules ensured safe interoperability, with traffic rerouting
permitted only under validated weather conditions. These findings validate
SORA-ATMAS as a regulation-aligned, context-aware, and verifiable governance
framework that consolidates distributed agent outputs into accountable,
real-time decisions, offering a resilient foundation for smart-city management.

</details>


### [212] [ColorAgent: Building A Robust, Personalized, and Interactive OS Agent](https://arxiv.org/abs/2510.19386)
*Ning Li,Qiqiang Lin,Zheng Wu,Xiaoyun Mo,Weiming Zhang,Yin Zhao,Xiangmou Qu,Jiamu Zhou,Jun Wang,Congmin Zheng,Yuanyi Song,Hongjiang Chen,Heyuan Huang,Jihong Wang,Jiaxin Yin,Jingwei Yu,Junwei Liao,Qiuying Peng,Xingyu Lou,Jun Wang,Weiwen Liu,Zhuosheng Zhang,Weinan Zhang*

Main category: cs.MA

TL;DR: 本文介绍了操作系统代理ColorAgent，通过特定方法实现与环境的长周期交互和个性化用户交互，在基准测试中取得佳绩，指出当前基准测试不足并给出未来探索方向。


<details>
  <summary>Details</summary>
Motivation: 随着技术发展，构建能执行用户指令、符合用户意愿的操作系统代理成为现实，开发ColorAgent以实现长周期、鲁棒的环境交互和个性化、主动的用户交互。

Method: 通过逐步强化学习和自我进化训练增强模型能力，开发定制的多智能体框架；探索个性化用户意图识别和主动参与。

Result: 在AndroidWorld和AndroidLab基准测试中成功率分别达77.2%和50.7%，达到新的最优水平。

Conclusion: 当前基准测试不足以全面评估操作系统代理，未来应在评估范式、智能体协作和安全等方面进一步探索。

Abstract: With the advancements in hardware, software, and large language model
technologies, the interaction between humans and operating systems has evolved
from the command-line interface to the rapidly emerging AI agent interactions.
Building an operating system (OS) agent capable of executing user instructions
and faithfully following user desires is becoming a reality. In this technical
report, we present ColorAgent, an OS agent designed to engage in long-horizon,
robust interactions with the environment while also enabling personalized and
proactive user interaction. To enable long-horizon interactions with the
environment, we enhance the model's capabilities through step-wise
reinforcement learning and self-evolving training, while also developing a
tailored multi-agent framework that ensures generality, consistency, and
robustness. In terms of user interaction, we explore personalized user intent
recognition and proactive engagement, positioning the OS agent not merely as an
automation tool but as a warm, collaborative partner. We evaluate ColorAgent on
the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2%
and 50.7%, respectively, establishing a new state of the art. Nonetheless, we
note that current benchmarks are insufficient for a comprehensive evaluation of
OS agents and propose further exploring directions in future work, particularly
in the areas of evaluation paradigms, agent collaboration, and security. Our
code is available at https://github.com/MadeAgents/mobile-use.

</details>


### [213] [Modeling realistic human behavior using generative agents in a multimodal transport system: Software architecture and Application to Toulouse](https://arxiv.org/abs/2510.19497)
*Trung-Dung Vu,Benoit Gaudou,Kamaldeep Singh Oberoi*

Main category: cs.MA

TL;DR: 本文展示了一个在复杂多式联运系统中建模人类出行行为的架构，结合大语言模型和基于代理的模拟，以法国图卢兹为例进行研究，得出结合方法对交通系统有益的结论。


<details>
  <summary>Details</summary>
Motivation: 对现实人类行为建模，理解人们的出行方式选择，以提出个性化出行解决方案。

Method: 将大语言模型应用于基于代理的模拟中，集成GAMA模拟平台、基于大语言模型的生成式代理、公共交通GTFS数据和OpenTripPlanner多式联运路线规划。

Result: 模拟一个月后，代理不仅能做出有上下文感知的交通决策，还会随时间形成习惯。

Conclusion: 结合大语言模型和基于代理的模拟为推进智能交通系统和个性化多式联运解决方案提供了有前景的方向，也指出了方法的局限性和未来工作方向。

Abstract: Modeling realistic human behaviour to understand people's mode choices in
order to propose personalised mobility solutions remains challenging. This
paper presents an architecture for modeling realistic human mobility behavior
in complex multimodal transport systems, demonstrated through a case study in
Toulouse, France. We apply Large Language Models (LLMs) within an agent-based
simulation to capture decision-making in a real urban setting. The framework
integrates the GAMA simulation platform with an LLM-based generative agent,
along with General Transit Feed Specification (GTFS) data for public transport,
and OpenTripPlanner for multimodal routing. GAMA platform models the
interactive transport environment, providing visualization and dynamic agent
interactions while eliminating the need to construct the simulation environment
from scratch. This design enables a stronger focus on developing generative
agents and evaluating their performance in transport decision-making processes.
Over a simulated month, results show that agents not only make context-aware
transport decisions but also form habits over time. We conclude that combining
LLMs with agent-based simulation offers a promising direction for advancing
intelligent transportation systems and personalised multimodal mobility
solutions. We also discuss some limitations of this approach and outline future
work on scaling to larger regions, integrating real-time data, and refining
memory models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [214] [$\nabla$-SDF: Learning Euclidean Signed Distance Functions Online with Gradient-Augmented Octree Interpolation and Neural Residual](https://arxiv.org/abs/2510.18999)
*Zhirui Dai,Qihao Qian,Tianxing Fan,Nikolay Atanasov*

Main category: cs.RO

TL;DR: 提出∇ - SDF混合方法用于SDF重建，兼具效率与准确性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于离散体积数据结构的方法影响SDF估计连续性和可微性，神经网络方法存在效率低、易遗忘、内存受限等问题。

Method: 结合梯度增强八叉树插值的显式先验和隐式神经残差的混合方法。

Result: 实现非截断SDF重建，在计算和内存效率上与体积法相当，可微性和准确性与神经网络法相当。

Conclusion: ∇ - SDF在准确性和效率上优于现有方法，为机器人和计算机视觉下游任务提供可扩展解决方案。

Abstract: Estimation of signed distance functions (SDFs) from point cloud data has been
shown to benefit many robot autonomy capabilities, including localization,
mapping, motion planning, and control. Methods that support online and
large-scale SDF reconstruction tend to rely on discrete volumetric data
structures, which affect the continuity and differentiability of the SDF
estimates. Recently, using implicit features, neural network methods have
demonstrated high-fidelity and differentiable SDF reconstruction but they tend
to be less efficient, can experience catastrophic forgetting and memory
limitations in large environments, and are often restricted to truncated SDFs.
This work proposes $\nabla$-SDF, a hybrid method that combines an explicit
prior obtained from gradient-augmented octree interpolation with an implicit
neural residual. Our method achieves non-truncated (Euclidean) SDF
reconstruction with computational and memory efficiency comparable to
volumetric methods and differentiability and accuracy comparable to neural
network methods. Extensive experiments demonstrate that \methodname{}
outperforms the state of the art in terms of accuracy and efficiency, providing
a scalable solution for downstream tasks in robotics and computer vision.

</details>


### [215] [A Cross-Environment and Cross-Embodiment Path Planning Framework via a Conditional Diffusion Model](https://arxiv.org/abs/2510.19128)
*Mehran Ghafarian Tamizi,Homayoun Honari,Amir Mehdi Soufi Enayati,Aleksey Nozdryn-Plotnicki,Homayoun Najjaran*

Main category: cs.RO

TL;DR: 研究提出GADGET路径规划框架，结合双重调节机制，实验显示其在多环境和机器人上成功率高、碰撞低、可迁移。


<details>
  <summary>Details</summary>
Motivation: 传统路径规划方法计算时间长、需大量参数调整，基于学习的方法泛化能力差，需开发能泛化到新环境和机器人且无需重新训练的路径规划框架。

Method: 提出基于扩散的规划模型GADGET，采用混合双重调节机制，结合无分类器引导和基于分类器的控制障碍函数安全塑形。

Result: GADGET在多种环境中成功率高、碰撞强度低，对比表现好，能在不同机器人间迁移，在真实场景中可生成安全无碰撞轨迹。

Conclusion: GADGET能在无需重新训练的情况下泛化到新环境和机器人，具有良好的路径规划性能和可迁移性。

Abstract: Path planning for a robotic system in high-dimensional cluttered environments
needs to be efficient, safe, and adaptable for different environments and
hardware. Conventional methods face high computation time and require extensive
parameter tuning, while prior learning-based methods still fail to generalize
effectively. The primary goal of this research is to develop a path planning
framework capable of generalizing to unseen environments and new robotic
manipulators without the need for retraining. We present GADGET (Generalizable
and Adaptive Diffusion-Guided Environment-aware Trajectory generation), a
diffusion-based planning model that generates joint-space trajectories
conditioned on voxelized scene representations as well as start and goal
configurations. A key innovation is GADGET's hybrid dual-conditioning mechanism
that combines classifier-free guidance via learned scene encoding with
classifier-guided Control Barrier Function (CBF) safety shaping, integrating
environment awareness with real-time collision avoidance directly in the
denoising process. This design supports zero-shot transfer to new environments
and robotic embodiments without retraining. Experimental results show that
GADGET achieves high success rates with low collision intensity in
spherical-obstacle, bin-picking, and shelf environments, with CBF guidance
further improving safety. Moreover, comparative evaluations indicate strong
performance relative to both sampling-based and learning-based baselines.
Furthermore, GADGET provides transferability across Franka Panda, Kinova Gen3
(6/7-DoF), and UR5 robots, and physical execution on a Kinova Gen3 demonstrates
its ability to generate safe, collision-free trajectories in real-world
settings.

</details>


### [216] [Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning](https://arxiv.org/abs/2510.19495)
*Kevin Huang,Rosario Scalise,Cleah Winston,Ayush Agrawal,Yunchu Zhang,Rohan Baijal,Markus Grotz,Byron Boots,Benjamin Burchfiel,Hongkai Dai,Masha Itkina,Paarth Shah,Abhishek Gupta*

Main category: cs.RO

TL;DR: 提出用离线强化学习利用非专家数据提升模仿学习策略性能，算法改进后可有效利用数据，在操作任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习依赖高质量特定任务数据，适应性受限，且无法有效利用非专家数据，需解决此问题。

Method: 使用离线强化学习，对标准离线RL方法进行简单算法修改。

Result: 改进方法能让模仿算法稳健解决任务，增强恢复和泛化能力，在操作任务中增加成功初始条件范围，能利用所有收集数据提升策略性能。

Conclusion: 强调利用非专家数据进行机器人稳健策略学习的算法技术的重要性。

Abstract: Imitation learning has proven effective for training robots to perform
complex tasks from expert human demonstrations. However, it remains limited by
its reliance on high-quality, task-specific data, restricting adaptability to
the diverse range of real-world object configurations and scenarios. In
contrast, non-expert data -- such as play data, suboptimal demonstrations,
partial task completions, or rollouts from suboptimal policies -- can offer
broader coverage and lower collection costs. However, conventional imitation
learning approaches fail to utilize this data effectively. To address these
challenges, we posit that with right design decisions, offline reinforcement
learning can be used as a tool to harness non-expert data to enhance the
performance of imitation learning policies. We show that while standard offline
RL approaches can be ineffective at actually leveraging non-expert data under
the sparse data coverage settings typically encountered in the real world,
simple algorithmic modifications can allow for the utilization of this data,
without significant additional assumptions. Our approach shows that broadening
the support of the policy distribution can allow imitation algorithms augmented
by offline RL to solve tasks robustly, showing considerably enhanced recovery
and generalization behavior. In manipulation tasks, these innovations
significantly increase the range of initial conditions where learned policies
are successful when non-expert data is incorporated. Moreover, we show that
these methods are able to leverage all collected data, including partial or
suboptimal demonstrations, to bolster task-directed policy performance. This
underscores the importance of algorithmic techniques for using non-expert data
for robust policy learning in robotics.

</details>


### [217] [Hierarchical DLO Routing with Reinforcement Learning and In-Context Vision-language Models](https://arxiv.org/abs/2510.19268)
*Mingen Li,Houjian Yu,Yixuan Huang,Youngjin Hong,Changhyun Choi*

Main category: cs.RO

TL;DR: 提出用于解决可变形线性物体（DLO）长视野路由任务的自主分层框架，利用视觉语言模型进行高层推理合成计划，通过强化学习训练低层技能，引入故障恢复机制，表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 长视野DLO路由任务在工业和生活中常见但具挑战性，需机器人进行长视野规划和可靠技能执行，要求适应非线性动力学、分解目标和生成多步计划。

Method: 提出自主分层框架，利用视觉语言模型进行高层推理合成可行计划，通过强化学习训练低层技能执行计划，引入故障恢复机制。

Result: 方法能泛化到不同场景，在长视野路由场景中整体成功率达92.5%，比次优基线方法高近50%。

Conclusion: 所提框架能有效解决具有挑战性的DLO路由任务。

Abstract: Long-horizon routing tasks of deformable linear objects (DLOs), such as
cables and ropes, are common in industrial assembly lines and everyday life.
These tasks are particularly challenging because they require robots to
manipulate DLO with long-horizon planning and reliable skill execution.
Successfully completing such tasks demands adapting to their nonlinear
dynamics, decomposing abstract routing goals, and generating multi-step plans
composed of multiple skills, all of which require accurate high-level reasoning
during execution. In this paper, we propose a fully autonomous hierarchical
framework for solving challenging DLO routing tasks. Given an implicit or
explicit routing goal expressed in language, our framework leverages
vision-language models~(VLMs) for in-context high-level reasoning to synthesize
feasible plans, which are then executed by low-level skills trained via
reinforcement learning. To improve robustness in long horizons, we further
introduce a failure recovery mechanism that reorients the DLO into
insertion-feasible states. Our approach generalizes to diverse scenes involving
object attributes, spatial descriptions, as well as implicit language commands.
It outperforms the next best baseline method by nearly 50% and achieves an
overall success rate of 92.5% across long-horizon routing scenarios.

</details>


### [218] [Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets](https://arxiv.org/abs/2510.19373)
*Basavasagar Patil,Sydney Belt,Jayjun Lee,Nima Fazeli,Bernadette Bucher*

Main category: cs.RO

TL;DR: 提出简单采样策略缓解机器人任务数据集动作不平衡问题，在多场景评估有效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器人任务数据集在物理动作上存在显著不平衡，影响训练。

Method: 提出简单采样策略，少量代码即可集成到现有代码库。

Result: 在预训练小模型和微调大模型中，相比现有方法在低资源任务上显著提升，不影响高资源任务。在真实机器人上验证有效。

Conclusion: 该策略能更有效地利用多任务策略的模型容量。

Abstract: Increasingly large datasets of robot actions and sensory observations are
being collected to train ever-larger neural networks. These datasets are
collected based on tasks and while these tasks may be distinct in their
descriptions, many involve very similar physical action sequences (e.g., 'pick
up an apple' versus 'pick up an orange'). As a result, many datasets of robotic
tasks are substantially imbalanced in terms of the physical robotic actions
they represent. In this work, we propose a simple sampling strategy for policy
training that mitigates this imbalance. Our method requires only a few lines of
code to integrate into existing codebases and improves generalization. We
evaluate our method in both pre-training small models and fine-tuning large
foundational models. Our results show substantial improvements on low-resource
tasks compared to prior state-of-the-art methods, without degrading performance
on high-resource tasks. This enables more effective use of model capacity for
multi-task policies. We also further validate our approach in a real-world
setup on a Franka Panda robot arm across a diverse set of tasks.

</details>


### [219] [Learning Affordances at Inference-Time for Vision-Language-Action Models](https://arxiv.org/abs/2510.19752)
*Ameesh Shah,William Chen,Adwait Godbole,Federico Mora,Sanjit A. Seshia,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出LITEN方法，使VLA模型能从过去经验学习，有效完成长周期任务。


<details>
  <summary>Details</summary>
Motivation: 解决复杂现实控制任务需多次尝试，Vision - Language - Action模型（VLAs）缺乏根据任务失败动态调整行为的能力。

Method: 引入Learning from Inference - Time Execution (LITEN)，连接VLA低级策略和高级VLM，迭代推理和评估阶段，评估时需结构化指导。

Result: 实验结果表明LITEN能有效从过去经验学习，生成使用高可用性指令的计划以完成长周期任务。

Conclusion: LITEN能让VLA模型从过去经验学习，为解决复杂控制任务提供有效方法。

Abstract: Solving complex real-world control tasks often takes multiple tries: if we
fail at first, we reflect on what went wrong, and change our strategy
accordingly to avoid making the same mistake. In robotics,
Vision-Language-Action models (VLAs) offer a promising path towards solving
complex control tasks, but lack the ability to contextually and dynamically
readjust behavior when they fail to accomplish a task. In this work, we
introduce Learning from Inference-Time Execution (LITEN), which connects a VLA
low-level policy to a high-level VLM that conditions on past experiences by
including them in-context, allowing it to learn the affordances and
capabilities of the low-level VLA. Our approach iterates between a reasoning
phase that generates and executes plans for the low-level VLA, and an
assessment phase that reflects on the resulting execution and draws useful
conclusions to be included in future reasoning contexts. Unlike similar
approaches to self-refinement in non-robotics domains, LITEN must reflect on
unstructured real-world robot trajectories (e.g., raw videos), which requires
structured guiderails during assessment. Our experimental results demonstrate
LITEN is able to effectively learn from past experience to generate plans that
use high-affordance instructions to accomplish long-horizon tasks.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [220] [KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge](https://arxiv.org/abs/2510.19484)
*Zaifei Yang,Hong Chang,Ruibing Hou,Shiguang Shan,Xilin Chen*

Main category: q-bio.BM

TL;DR: 本文针对当前分子大语言模型问题，引入KnowMol - 100K数据集，提出化学信息分子表示法，开发KnowMol模型，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 当前分子大语言模型在预训练时因文本描述不足和分子表示策略不佳，在理解分子方面存在显著局限。

Method: 引入含100K细粒度分子注释的KnowMol - 100K数据集，提出化学信息分子表示法，开发KnowMol多模态分子大语言模型。

Result: 广泛实验表明KnowMol在分子理解和生成任务上表现优越。

Conclusion: KnowMol能有效解决当前分子大语言模型面临的问题，提升分子理解和生成能力。

Abstract: The molecular large language models have garnered widespread attention due to
their promising potential on molecular applications. However, current molecular
large language models face significant limitations in understanding molecules
due to inadequate textual descriptions and suboptimal molecular representation
strategies during pretraining. To address these challenges, we introduce
KnowMol-100K, a large-scale dataset with 100K fine-grained molecular
annotations across multiple levels, bridging the gap between molecules and
textual descriptions. Additionally, we propose chemically-informative molecular
representation, effectively addressing limitations in existing molecular
representation strategies. Building upon these innovations, we develop KnowMol,
a state-of-the-art multi-modal molecular large language model. Extensive
experiments demonstrate that KnowMol achieves superior performance across
molecular understanding and generation tasks.
  GitHub: https://github.com/yzf-code/KnowMol
  Huggingface: https://hf.co/datasets/yzf1102/KnowMol-100K

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [221] [LLM Bazaar: A Service Design for Supporting Collaborative Learning with an LLM-Powered Multi-Party Collaboration Infrastructure](https://arxiv.org/abs/2510.18877)
*Zhen Wu,Jiaxin Shi,R. Charles Murray,Carolyn Rosé,Micah San Andres*

Main category: cs.HC

TL;DR: 本文探讨将大语言模型集成到对话代理以支持协作学习，基于Bazaar架构集成LLM - agent shell，为探索LLM赋能环境如何改变协作学习结果和交互模式奠定基础。


<details>
  <summary>Details</summary>
Motivation: 对话代理在协作学习中有重要作用，近期大语言模型集成到对话代理为促进批判性思维和协作问题解决带来新可能。

Method: 从开源协作支持架构Bazaar出发，集成LLM - agent shell，为小组学习提供基于大语言模型的实时、上下文敏感的协作支持。

Result: 设计并构建了相关基础设施。

Conclusion: 该设计和基础设施为探索定制的大语言模型赋能环境如何重塑协作学习结果和交互模式铺平了道路。

Abstract: For nearly two decades, conversational agents have played a critical role in
structuring interactions in collaborative learning, shaping group dynamics, and
supporting student engagement. The recent integration of large language models
(LLMs) into these agents offers new possibilities for fostering critical
thinking and collaborative problem solving. In this work, we begin with an open
source collaboration support architecture called Bazaar and integrate an
LLM-agent shell that enables introduction of LLM-empowered, real time, context
sensitive collaborative support for group learning. This design and
infrastructure paves the way for exploring how tailored LLM-empowered
environments can reshape collaborative learning outcomes and interaction
patterns.

</details>


### [222] [LifeSync-Games: Toward a Video Game Paradigm for Promoting Responsible Gaming and Human Development](https://arxiv.org/abs/2510.19691)
*R. González-Ibáñez,J. Macías-Cáceres,M. Villalta-Paucar*

Main category: cs.HC

TL;DR: 介绍了全球游戏现状，指出游戏积极作用未被充分利用，提出LifeSync - Games框架连接虚拟与现实以提升游戏发展价值，并展示其多方面内容和早期应用。


<details>
  <summary>Details</summary>
Motivation: 解决公众话语和监管中对游戏风险关注多于益处的不平衡，充分发挥游戏支持人类发展和幸福的潜力。

Method: 提出LifeSync - Games框架，阐述其理论基础、技术组件、设计指南和评估方法。

Result: 展示了该框架在新游戏和畅销游戏中的早期应用，体现其多功能性和实际相关性。

Conclusion: LifeSync - Games框架可通过连接虚拟与现实，促进自我调节，在多领域培养成长，提升游戏的发展价值。

Abstract: Technological advancements have made video games a central part of the
digital lives of nearly 3 billion people worldwide. Although games can address
various social, physical, and psychological needs, their potential to support
human development and well-being remains underutilized. Research highlights
both negative effects, such as addiction and isolation, and positive outcomes
like cognitive improvements and problem-solving skills. However, public
discourse and regulation often focus more on risks than benefits. To address
this imbalance, we present LifeSync-Games, a framework leveraging simplified
digital twins to connect virtual gameplay with real-life activities. This
reciprocal relationship aims to enhance the developmental value of gaming by
promoting self-regulation and fostering growth across physical, mental, and
social domains. We present the framework's theoretical foundations,
technological components, design guidelines, and evaluation approaches.
Additionally, we present early applications in both new and bestselling games
to demonstrate its versatility and practical relevance.

</details>


### [223] [Plural Voices, Single Agent: Towards Inclusive AI in Multi-User Domestic Spaces](https://arxiv.org/abs/2510.19008)
*Joydeep Chandra,Satyam Kumar Navneet*

Main category: cs.HC

TL;DR: 本文针对家庭AI代理面临的伦理、自主性和包容性挑战，提出了Plural Voices Model (PVM) 框架，评估显示其性能优于多智能体基线，代码和模型已开源。


<details>
  <summary>Details</summary>
Motivation: 解决家庭AI代理面临的伦理、自主性和包容性挑战，尤其是针对儿童、老年人和神经多样性用户等被忽视群体。

Method: 提出PVM框架，利用多样化公共数据集，采用人类+合成课程设计，结合公平感知场景和伦理增强。

Result: 初步评估中，PVM在合规性、公平性、安全违规率和延迟方面优于多智能体基线。

Conclusion: 设计创新展示了家庭AI伦理和包容性的新方向，可用于构建以用户为中心的智能系统。

Abstract: Domestic AI agents faces ethical, autonomy, and inclusion challenges,
particularly for overlooked groups like children, elderly, and Neurodivergent
users. We present the Plural Voices Model (PVM), a novel single-agent framework
that dynamically negotiates multi-user needs through real-time value alignment,
leveraging diverse public datasets on mental health, eldercare, education, and
moral reasoning. Using human+synthetic curriculum design with fairness-aware
scenarios and ethical enhancements, PVM identifies core values, conflicts, and
accessibility requirements to inform inclusive principles. Our privacy-focused
prototype features adaptive safety scaffolds, tailored interactions (e.g.,
step-by-step guidance for Neurodivergent users, simple wording for children),
and equitable conflict resolution. In preliminary evaluations, PVM outperforms
multi-agent baselines in compliance (76% vs. 70%), fairness (90% vs. 85%),
safety-violation rate (0% vs. 7%), and latency. Design innovations, including
video guidance, autonomy sliders, family hubs, and adaptive safety dashboards,
demonstrate new directions for ethical and inclusive domestic AI, for building
user-centered agentic systems in plural domestic contexts. Our Codes and Model
are been open sourced, available for reproduction:
https://github.com/zade90/Agora

</details>


### [224] [CLiVR: Conversational Learning System in Virtual Reality with AI-Powered Patients](https://arxiv.org/abs/2510.19031)
*Akilan Amithasagaran,Sagnik Dakshit,Bhavani Suryadevara,Lindsey Stockton*

Main category: cs.HC

TL;DR: 本文介绍了VR对话学习系统CLiVR，它能模拟医患互动，经专家用户研究获良好反馈，可作为基于标准化病人培训的补充。


<details>
  <summary>Details</summary>
Motivation: 传统医学护理教育模拟方法资源需求大，限制可及性和扩展性，需新方法。

Method: 开发集成大语言模型、语音处理和3D化身的CLiVR系统，在Meta Quest 3平台部署，从综合征 - 症状数据库动态生成模拟，用情感分析提供反馈，进行专家用户研究。

Result: 用户接受度高，对教育潜力信心足，获改进的宝贵反馈。

Conclusion: CLiVR是基于标准化病人培训的可扩展、沉浸式补充。

Abstract: Simulations constitute a fundamental component of medical and nursing
education and traditionally employ standardized patients (SP) and high-fidelity
manikins to develop clinical reasoning and communication skills. However, these
methods require substantial resources, limiting accessibility and scalability.
In this study, we introduce CLiVR, a Conversational Learning system in Virtual
Reality that integrates large language models (LLMs), speech processing, and 3D
avatars to simulate realistic doctor-patient interactions. Developed in Unity
and deployed on the Meta Quest 3 platform, CLiVR enables trainees to engage in
natural dialogue with virtual patients. Each simulation is dynamically
generated from a syndrome-symptom database and enhanced with sentiment analysis
to provide feedback on communication tone. Through an expert user study
involving medical school faculty (n=13), we assessed usability, realism, and
perceived educational impact. Results demonstrated strong user acceptance, high
confidence in educational potential, and valuable feedback for improvement.
CLiVR offers a scalable, immersive supplement to SP-based training.

</details>


### [225] ["Over-the-Hood" AI Inclusivity Bugs and How 3 AI Product Teams Found and Fixed Them](https://arxiv.org/abs/2510.19033)
*Andrew Anderson,Fatima A. Moussaoui,Jimena Noa Guevara,Md Montaser Hamid,Margaret Burnett*

Main category: cs.HC

TL;DR: 研究调查用户端AI产品中的包容性漏洞，识别出6种类型共83次漏洞，修复47次，还提出新方法GenderMag - for - AI。


<details>
  <summary>Details</summary>
Motivation: 了解用户端AI产品中“表面”的包容性偏见的表现、普遍性，以及开发者如何发现和修复这些问题。

Method: 对3个AI产品团队进行实地研究，利用现有的非AI导向的包容性设计方法。

Result: 识别出6种类型的AI包容性漏洞，共出现83次，修复了47次，提出新的包容性设计方法GenderMag - for - AI。

Conclusion: 新的GenderMag - for - AI方法能有效检测特定类型的AI包容性漏洞。

Abstract: While much research has shown the presence of AI's "under-the-hood" biases
(e.g., algorithmic, training data, etc.), what about "over-the-hood"
inclusivity biases: barriers in user-facing AI products that disproportionately
exclude users with certain problem-solving approaches? Recent research has
begun to report the existence of such biases -- but what do they look like, how
prevalent are they, and how can developers find and fix them? To find out, we
conducted a field study with 3 AI product teams, to investigate what kinds of
AI inclusivity bugs exist uniquely in user-facing AI products, and whether/how
AI product teams might harness an existing (non-AI-oriented) inclusive design
method to find and fix them. The teams' work resulted in identifying 6 types of
AI inclusivity bugs arising 83 times, fixes covering 47 of these bug instances,
and a new variation of the GenderMag inclusive design method, GenderMag-for-AI,
that is especially effective at detecting certain kinds of AI inclusivity bugs.

</details>


### [226] [CityAQVis: Integrated ML-Visualization Sandbox Tool for Pollutant Estimation in Urban Regions Using Multi-Source Data (Software Article)](https://arxiv.org/abs/2510.18878)
*Brij Bridhin Desai,Yukta Arvind,Aswathi Mundayatt,Jaya Sreevalsan-Nair*

Main category: cs.HC

TL;DR: 本文介绍CityAQVis工具，它用多源数据预测和可视化地面污染物浓度，经案例测试有潜力助力空气质量决策。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染有诸多危害，有效管理需能整合多样数据集并呈现复杂时空污染模式的预测工具，当前缺乏能无缝集成预测与可视化污染物浓度空间分布的交互式工具。

Method: 开发CityAQVis交互式机器学习沙盒工具，利用多源数据预测和可视化地面污染物浓度，通过案例研究测试工具，用户可通过GUI进行不同城市情景下污染物浓度空间分布的对比可视化。

Result: 通过预测大都市地区二氧化氮浓度的案例研究，展示了工具对各种污染物的适应性。

Conclusion: 基于机器学习的可视化分析有潜力提高态势感知能力，支持空气质量管理中的数据驱动决策。

Abstract: Urban air pollution poses significant risks to public health, environmental
sustainability, and policy planning. Effective air quality management requires
predictive tools that can integrate diverse datasets and communicate complex
spatial and temporal pollution patterns. There is a gap in interactive tools
with seamless integration of forecasting and visualization of spatial
distributions of air pollutant concentrations. We present CityAQVis, an
interactive machine learning ML sandbox tool designed to predict and visualize
pollutant concentrations at the ground level using multi-source data, which
includes satellite observations, meteorological parameters, population density,
elevation, and nighttime lights. While traditional air quality visualization
tools often lack forecasting capabilities, CityAQVis enables users to build and
compare predictive models, visualizing the model outputs and offering insights
into pollution dynamics at the ground level. The pilot implementation of the
tool is tested through case studies predicting nitrogen dioxide (NO2)
concentrations in metropolitan regions, highlighting its adaptability to
various pollutants. Through an intuitive graphical user interface (GUI), the
user can perform comparative visualizations of the spatial distribution of
surface-level pollutant concentration in two different urban scenarios. Our
results highlight the potential of ML-driven visual analytics to improve
situational awareness and support data-driven decision-making in air quality
management.

</details>


### [227] [Learning To Defer To A Population With Limited Demonstrations](https://arxiv.org/abs/2510.19351)
*Nilesh Ramgolam,Gustavo Carneiro,Hsiang-Ting,Chen*

Main category: cs.HC

TL;DR: 本文提出上下文感知半监督框架解决学习延迟（L2D）系统数据稀缺问题，实验验证了方法的数据效率，让自适应L2D系统更实用可扩展。


<details>
  <summary>Details</summary>
Motivation: 解决阻碍学习延迟（L2D）系统实际部署到人群的关键数据稀缺问题。

Method: 引入上下文感知、半监督框架，用元学习从少量演示中生成特定专家嵌入，利用嵌入生成伪标签用于训练并实现测试时的动态适应。

Result: 在三个不同数据集上的实验结果表明，基于合成标签训练的模型能快速接近最优性能，验证了方法的数据效率。

Conclusion: 解决了关键训练瓶颈，使自适应L2D系统更实用和可扩展，为现实环境中的人机协作铺平道路。

Abstract: This paper addresses the critical data scarcity that hinders the practical
deployment of learning to defer (L2D) systems to the population. We introduce a
context-aware, semi-supervised framework that uses meta-learning to generate
expert-specific embeddings from only a few demonstrations. We demonstrate the
efficacy of a dual-purpose mechanism, where these embeddings are used first to
generate a large corpus of pseudo-labels for training, and subsequently to
enable on-the-fly adaptation to new experts at test-time. The experiment
results on three different datasets confirm that a model trained on these
synthetic labels rapidly approaches oracle-level performance, validating the
data efficiency of our approach. By resolving a key training bottleneck, this
work makes adaptive L2D systems more practical and scalable, paving the way for
human-AI collaboration in real-world environments. To facilitate
reproducibility and address implementation details not covered in the main
text, we provide our source code and training configurations at
https://github.com/nil123532/learning-to-defer-to-a-population-with-limited-demonstrations.

</details>


### [228] [Directive, Metacognitive or a Blend of Both? A Comparison of AI-Generated Feedback Types on Student Engagement, Confidence, and Outcomes](https://arxiv.org/abs/2510.19685)
*Omar Alsaiari,Nilufar Baghaei,Jason M. Lodge,Omid Noroozi,Dragan Gašević,Marie Boden,Hassan Khosravi*

Main category: cs.HC

TL;DR: 研究对比指令性、元认知和混合式AI反馈对学生的影响，发现混合式反馈促使最多修订，各反馈方式信心评分高、资源质量相当，混合式有潜力结合指导与反思。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索指令性和元认知反馈对学生参与度、信心和作业质量的比较影响。

Method: 对329名学生进行为期一学期的随机对照试验，学生使用自适应教育平台，被分为接收指令性、元认知或混合式AI反馈。

Result: 修订行为在不同反馈条件下有差异，混合式促使最多修订，信心评分普遍高，资源质量结果相当。

Conclusion: AI在提供平衡清晰性与反思性的反馈方面有前景，混合式方法潜力大。

Abstract: Feedback is one of the most powerful influences on student learning, with
extensive research examining how best to implement it in educational settings.
Increasingly, feedback is being generated by artificial intelligence (AI),
offering scalable and adaptive responses. Two widely studied approaches are
directive feedback, which gives explicit explanations and reduces cognitive
load to speed up learning, and metacognitive feedback which prompts learners to
reflect, track their progress, and develop self-regulated learning (SRL)
skills. While both approaches have clear theoretical advantages, their
comparative effects on engagement, confidence, and quality of work remain
underexplored. This study presents a semester-long randomised controlled trial
with 329 students in an introductory design and programming course using an
adaptive educational platform. Participants were assigned to receive directive,
metacognitive, or hybrid AI-generated feedback that blended elements of both
directive and metacognitive feedback. Results showed that revision behaviour
differed across feedback conditions, with Hybrid prompting the most revisions
compared to Directive and Metacognitive. Confidence ratings were uniformly
high, and resource quality outcomes were comparable across conditions. These
findings highlight the promise of AI in delivering feedback that balances
clarity with reflection. Hybrid approaches, in particular, show potential to
combine actionable guidance for immediate improvement with opportunities for
self-reflection and metacognitive growth.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [229] [Towards a feminist understanding of digital platform work](https://arxiv.org/abs/2510.19450)
*Clara Punzi*

Main category: cs.CY

TL;DR: 数字平台经济发展带来新就业机会，但也有剥削等问题，研究用女性主义理论剖析，提出框架并倡导改善措施。


<details>
  <summary>Details</summary>
Motivation: 数字平台工作存在经济剥削、职业隔离等问题，且算法管理加剧不平等，需研究其复杂本质。

Method: 借鉴女性主义理论，提出聚焦四个关键维度的框架。

Result: 提出了包含不稳定与剥削、监控与控制、就业边界模糊、殖民遗产四个维度的研究框架。

Conclusion: 倡导参与式研究、平台治理透明化和结构变革，以促进数字平台工作者更公平的工作条件。

Abstract: The rapid growth of the digital platform economy is transforming labor
markets, offering new employment opportunities with promises of flexibility and
accessibility. However, these benefits often come at the expense of increased
economic exploitation, occupational segregation, and deteriorating working
conditions. Research highlights that algorithmic management disproportionately
impacts marginalized groups, reinforcing gendered and racial inequalities while
deepening power imbalances within capitalist systems. This study seeks to
elucidate the complex nature of digital platform work by drawing on feminist
theories that have historically scrutinized and contested the structures of
power within society, especially in the workplace. It presents a framework
focused on four key dimensions to lay a foundation for future research: (i)
precarity and exploitation, (ii) surveillance and control, (iii) blurring
employment boundaries, and (iv) colonial legacies. It advocates for
participatory research, transparency in platform governance, and structural
changes to promote more equitable conditions for digital platform workers.

</details>


### [230] [Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A Case of Nonprofit Program Evaluation](https://arxiv.org/abs/2510.19799)
*Ji Ma,Albert Casella*

Main category: cs.CY

TL;DR: 研究提出从业者参与的工作流，结合透明决策树模型与大语言模型，用于公共和非营利组织，结果表明该方法能实现准确、可信且可行的案例级评估，为相关领域采用负责任的AI提供途径。


<details>
  <summary>Details</summary>
Motivation: 公共和非营利组织因模型不透明而对采用AI工具犹豫不决，标准方法缺乏案例级指导，需要改进。

Method: 采用从业者参与的工作流，结合透明决策树模型和大语言模型，利用大学成功项目数据构建决策树，将树结构提供给大语言模型进行案例级预测，从业者全程参与各环节。

Result: 集成透明模型、大语言模型和从业者输入可得到准确、可信且可行的案例级评估。

Conclusion: 该方法为公共和非营利部门负责任地采用AI提供了可行途径。

Abstract: Public and nonprofit organizations often hesitate to adopt AI tools because
most models are opaque even though standard approaches typically analyze
aggregate patterns rather than offering actionable, case-level guidance. This
study tests a practitioner-in-the-loop workflow that pairs transparent
decision-tree models with large language models (LLMs) to improve predictive
accuracy, interpretability, and the generation of practical insights. Using
data from an ongoing college-success program, we build interpretable decision
trees to surface key predictors. We then provide each tree's structure to an
LLM, enabling it to reproduce case-level predictions grounded in the
transparent models. Practitioners participate throughout feature engineering,
model design, explanation review, and usability assessment, ensuring that field
expertise informs the analysis at every stage. Results show that integrating
transparent models, LLMs, and practitioner input yields accurate, trustworthy,
and actionable case-level evaluations, offering a viable pathway for
responsible AI adoption in the public and nonprofit sectors.

</details>


### [231] [Evaluating LLMs for Career Guidance: Comparative Analysis of Computing Competency Recommendations Across Ten African Countries](https://arxiv.org/abs/2510.18902)
*Precious Eze,Stephanie Lunn,Bruk Berhane*

Main category: cs.CY

TL;DR: 研究对比六种大语言模型对非洲十个国家入门级计算职业期望的描述，发现模型在非技术能力、国家因素认知上有差异，开源模型表现更好，但都存在文化敏感和基础设施考虑不足问题，强调计算能力要求差异大及去殖民化AI教育的必要性。


<details>
  <summary>Details</summary>
Motivation: 雇主期望毕业生在工作中使用大语言模型，但非洲计算岗位所需能力因国家背景不同尚不清楚，需了解LLMs对非洲入门级计算职业期望的描述。

Method: 使用Computing Curricula 2020框架，结合数字殖民主义理论和Ubuntu哲学，分析60个LLM对标准化提示的回应。

Result: 技术技能常被提及，非技术能力处理有差异；模型对国家特定因素认知不同，开源模型表现更好，但所有模型文化敏感性和基础设施考虑平均仅35.4%；开源模型表现优于专有模型。

Conclusion: 非洲计算能力要求差异大，AI教育需采用强调情境相关性的去殖民化方法。

Abstract: Employers increasingly expect graduates to utilize large language models
(LLMs) in the workplace, yet the competencies needed for computing roles across
Africa remain unclear given varying national contexts. This study examined how
six LLMs, namely ChatGPT 4, DeepSeek, Gemini, Claude 3.5, Llama 3, and Mistral
AI, describe entry-level computing career expectations across ten African
countries. Using the Computing Curricula 2020 framework and drawing on Digital
Colonialism Theory and Ubuntu Philosophy, we analyzed 60 LLM responses to
standardized prompts. Technical skills such as cloud computing and programming
appeared consistently, but notable differences emerged in how models addressed
non-technical competencies, particularly ethics and responsible AI use. Models
varied considerably in recognizing country-specific factors, including local
technology ecosystems, language requirements, and national policies.
Open-source models demonstrated stronger contextual awareness and a better
balance between technical and professional skills, earning top scores in nine
of ten countries. Still, all models struggled with cultural sensitivity and
infrastructure considerations, averaging only 35.4% contextual awareness. This
first broad comparison of LLM career guidance for African computing students
uncovers entrenched infrastructure assumptions and Western-centric biases,
creating gaps between technical recommendations and local needs. The strong
performance of cost-effective open-source models (Llama: 4.47/5; DeepSeek:
4.25/5) compared to proprietary alternatives (ChatGPT 4: 3.90/5; Claude:
3.46/5) challenges assumptions about AI tool quality in resource-constrained
settings. Our findings highlight how computing competency requirements vary
widely across Africa and underscore the need for decolonial approaches to AI in
education that emphasize contextual relevance

</details>


### [232] [A Justice Lens on Fairness and Ethics Courses in Computing Education: LLM-Assisted Multi-Perspective and Thematic Evaluation](https://arxiv.org/abs/2510.18931)
*Kenya S. Andrews,Deborah Dormah Kanubala,Kehinde Aruleba,Francisco Enrique Vicente Castro,Renata A Revelo*

Main category: cs.CY

TL;DR: 文章开发面向正义的评分标准，让大语言模型多视角评估计算课程大纲，发现多视角评估有助于发现课程设计隐藏缺口，为改进教学提供方向。


<details>
  <summary>Details</summary>
Motivation: 计算课程中涉及公平和伦理内容，手动评估课程大纲耗时且易不一致，需有效评估方法。

Method: 开发面向正义的评分标准，让大语言模型通过多视角角色模拟评估24份课程大纲，并识别课程主题趋势。

Result: 多视角评估有助于发现特定角色的细微优先事项，填补AI/ML及相关计算课程设计中的隐藏缺口。

Conclusion: 评估结果为改进公平、伦理和正义内容的课程设计与教学提供具体方向。

Abstract: Course syllabi set the tone and expectations for courses, shaping the
learning experience for both students and instructors. In computing courses,
especially those addressing fairness and ethics in artificial intelligence
(AI), machine learning (ML), and algorithmic design, it is imperative that we
understand how approaches to navigating barriers to fair outcomes are being
addressed.These expectations should be inclusive, transparent, and grounded in
promoting critical thinking. Syllabus analysis offers a way to evaluate the
coverage, depth, practices, and expectations within a course. Manual syllabus
evaluation, however, is time-consuming and prone to inconsistency. To address
this, we developed a justice-oriented scoring rubric and asked a large language
model (LLM) to review syllabi through a multi-perspective role simulation.
Using this rubric, we evaluated 24 syllabi from four perspectives: instructor,
departmental chair, institutional reviewer, and external evaluator. We also
prompted the LLM to identify thematic trends across the courses. Findings show
that multiperspective evaluation aids us in noting nuanced, role-specific
priorities, leveraging them to fill hidden gaps in curricula design of AI/ML
and related computing courses focused on fairness and ethics. These insights
offer concrete directions for improving the design and delivery of fairness,
ethics, and justice content in such courses.

</details>


### [233] [REPAIR Approach for Social-based City Reconstruction Planning in case of natural disasters](https://arxiv.org/abs/2510.19048)
*Ghulam Mudassir,Antinisca Di Marco,Giordano d'Aloisio*

Main category: cs.CY

TL;DR: 文章扩展先前工作，用综合对比分析结合更多深度学习模型和随机代理，介绍通用的灾后重建决策支持系统REPAIR并应用于拉奎拉重建案例。


<details>
  <summary>Details</summary>
Motivation: 自然灾害对人类生活影响大，政府用有限资源开展灾后重建、制定政策以最大化社会效益面临挑战，需有效决策支持。

Method: 在先前基于深度强化学习的决策支持系统基础上，进行综合对比分析，结合额外深度学习模型和随机代理。

Result: 提出通用的灾后重建决策支持系统REPAIR，可提供替代方案供地方管理者选择，能应用于不同范围地区，并应用于拉奎拉重建。

Conclusion: REPAIR系统能为灾后城市重建规划提供有效决策支持，帮助最大化社会效益。

Abstract: Natural disasters always have several effects on human lives. It is
challenging for governments to tackle these incidents and to rebuild the
economic, social and physical infrastructures and facilities with the available
resources (mainly budget and time). Governments always define plans and
policies according to the law and political strategies that should maximise
social benefits. The severity of damage and the vast resources needed to bring
life back to normality make such reconstruction a challenge. This article is
the extension of our previously published work by conducting comprehensive
comparative analysis by integrating additional deep learning models plus random
agent which is used as a baseline. Our prior research introduced a decision
support system by using the Deep Reinforcement Learning technique for the
planning of post-disaster city reconstruction, maximizing the social benefit of
the reconstruction process, considering available resources, meeting the needs
of the broad community stakeholders (like citizens' social benefits and
politicians' priorities) and keeping in consideration city's structural
constraints (like dependencies among roads and buildings). The proposed
approach, named post disaster REbuilding plAn ProvIdeR (REPAIR) is generic. It
can determine a set of alternative plans for local administrators who select
the ideal one to implement, and it can be applied to areas of any extension. We
show the application of REPAIR in a real use case, i.e., to the L'Aquila
reconstruction process, damaged in 2009 by a major earthquake.

</details>


### [234] [See, Think, Act: Online Shopper Behavior Simulation with VLM Agents](https://arxiv.org/abs/2510.19245)
*Yimeng Zhang,Jiri Gesi,Ran Xue,Tian Wang,Ziyi Wang,Yuxuan Lu,Sinong Zhan,Huimin Zeng,Qingjun Cui,Yufan Guo,Jing Huang,Mubarak Shah,Dakuo Wang*

Main category: cs.CY

TL;DR: 本文探讨将网页截图视觉信息融入在线购物行为模拟，结合文本和视觉模态提升模拟效果，实验显示多模态输入有显著优势并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖文本输入，忽略视觉感知在网页GUI交互中对人类决策的重要作用，希望缩小合成代理与真实用户差距，实现更符合认知的在线购物行为模拟。

Method: 利用OPeRA数据集，通过VLMs将视觉信息融入行为模拟；采用SFT进行联合动作预测和理由生成，结合完整交互上下文；集成具有分层奖励结构的RL，用难度感知因子缩放。

Result: 结合文本和图像输入比仅文本输入提高精确匹配准确率超6%，多模态接地不仅提高预测准确性，还增强视觉复杂环境中模拟保真度。

Conclusion: 回顾行为模拟框架设计空间，识别关键方法局限性，提出构建高效人类行为模拟器的未来研究方向。

Abstract: LLMs have recently demonstrated strong potential in simulating online shopper
behavior. Prior work has improved action prediction by applying SFT on action
traces with LLM-generated rationales, and by leveraging RL to further enhance
reasoning capabilities. Despite these advances, current approaches rely on
text-based inputs and overlook the essential role of visual perception in
shaping human decision-making during web GUI interactions. In this paper, we
investigate the integration of visual information, specifically webpage
screenshots, into behavior simulation via VLMs, leveraging OPeRA dataset. By
grounding agent decision-making in both textual and visual modalities, we aim
to narrow the gap between synthetic agents and real-world users, thereby
enabling more cognitively aligned simulations of online shopping behavior.
Specifically, we employ SFT for joint action prediction and rationale
generation, conditioning on the full interaction context, which comprises
action history, past HTML observations, and the current webpage screenshot. To
further enhance reasoning capabilities, we integrate RL with a hierarchical
reward structure, scaled by a difficulty-aware factor that prioritizes
challenging decision points. Empirically, our studies show that incorporating
visual grounding yields substantial gains: the combination of text and image
inputs improves exact match accuracy by more than 6% over text-only inputs.
These results indicate that multi-modal grounding not only boosts predictive
accuracy but also enhances simulation fidelity in visually complex
environments, which captures nuances of human attention and decision-making
that text-only agents often miss. Finally, we revisit the design space of
behavior simulation frameworks, identify key methodological limitations, and
propose future research directions toward building efficient and effective
human behavior simulators.

</details>


### [235] [Social World Model-Augmented Mechanism Design Policy Learning](https://arxiv.org/abs/2510.19270)
*Xiaoyuan Zhang,Yizhe Huang,Chengdong Ma,Zhixun Chen,Long Ma,Yali Du,Song-Chun Zhu,Yaodong Yang,Xue Feng*

Main category: cs.CY

TL;DR: 提出SWM - AP方法用于增强机制设计，在多个场景实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模异质智能体和处理复杂多智能体系统动力学方面存在困难，且需要高样本效率，世界模型为此提供了途径。

Method: 引入SWM - AP方法，分层学习社会世界模型来建模智能体行为，机制设计策略与社会世界模型交互收集轨迹，同时在真实交互中在线推断智能体特征。

Result: 在税收政策设计、团队协调和设施选址等场景实验中，SWM - AP在累积奖励和样本效率上优于现有基于模型和无模型的强化学习基线。

Conclusion: SWM - AP方法在增强机制设计方面有效，能提升样本效率和性能。

Abstract: Designing adaptive mechanisms to align individual and collective interests
remains a central challenge in artificial social intelligence. Existing methods
often struggle with modeling heterogeneous agents possessing persistent latent
traits (e.g., skills, preferences) and dealing with complex multi-agent system
dynamics. These challenges are compounded by the critical need for high sample
efficiency due to costly real-world interactions. World Models, by learning to
predict environmental dynamics, offer a promising pathway to enhance mechanism
design in heterogeneous and complex systems. In this paper, we introduce a
novel method named SWM-AP (Social World Model-Augmented Mechanism Design Policy
Learning), which learns a social world model hierarchically modeling agents'
behavior to enhance mechanism design. Specifically, the social world model
infers agents' traits from their interaction trajectories and learns a
trait-based model to predict agents' responses to the deployed mechanisms. The
mechanism design policy collects extensive training trajectories by interacting
with the social world model, while concurrently inferring agents' traits online
during real-world interactions to further boost policy learning efficiency.
Experiments in diverse settings (tax policy design, team coordination, and
facility location) demonstrate that SWM-AP outperforms established model-based
and model-free RL baselines in cumulative rewards and sample efficiency.

</details>


### [236] [To Use or to Refuse? Re-Centering Student Agency with Generative AI in Engineering Design Education](https://arxiv.org/abs/2510.19342)
*Thijs Willems,Sumbul Khan,Qian Huang,Bradley Camburn,Nachamma Sockalingam,King Wang Poon*

Main category: cs.CY

TL;DR: 对超500名学生在13周AI增强设计课程中使用AI的反思进行研究，揭示相关实践及影响。


<details>
  <summary>Details</summary>
Motivation: 研究学生在AI增强设计课程中对AI使用的反思，促进学生用AI创新。

Method: 收集课程作业制品，包括反思表和简报，结合教师与研究者笔记，进行定性编码。

Result: 发现Gen - AI带来加速原型制作等实践，学生学会拒绝输出、投入用户研究。

Conclusion: 可将AI使用转化为可评估习惯，奖励选择性不使用培养防幻觉工作流，协调多方面可使教育中AI创新规模化且不失问责。

Abstract: This pilot study traces students' reflections on the use of AI in a 13-week
foundational design course enrolling over 500 first-year engineering and
architecture students at the Singapore University of Technology and Design. The
course was an AI-enhanced design course, with several interventions to equip
students with AI based design skills. Students were required to reflect on
whether the technology was used as a tool (instrumental assistant), a teammate
(collaborative partner), or neither (deliberate non-use). By foregrounding this
three-way lens, students learned to use AI for innovation rather than just
automation and to reflect on agency, ethics, and context rather than on prompt
crafting alone. Evidence stems from coursework artefacts: thirteen structured
reflection spreadsheets and eight illustrated briefs submitted, combined with
notes of teachers and researchers. Qualitative coding of these materials
reveals shared practices brought about through the inclusion of Gen-AI,
including accelerated prototyping, rapid skill acquisition, iterative prompt
refinement, purposeful "switch-offs" during user research, and emergent
routines for recognizing hallucinations. Unexpectedly, students not only
harnessed Gen-AI for speed but (enabled by the tool-teammate-neither triage)
also learned to reject its outputs, invent their own hallucination fire-drills,
and divert the reclaimed hours into deeper user research, thereby transforming
efficiency into innovation. The implications of the approach we explore shows
that: we can transform AI uptake into an assessable design habit; that
rewarding selective non-use cultivates hallucination-aware workflows; and,
practically, that a coordinated bundle of tool access, reflection, role
tagging, and public recognition through competition awards allows AI based
innovation in education to scale without compromising accountability.

</details>


### [237] [On Controlled Change: Generative AI's Impact on Professional Authority in Journalism](https://arxiv.org/abs/2510.19792)
*Tomás Dodds,Wang Ngai Yeung,Claudia Mellado,Mathias-Felipe de Lima-Santos*

Main category: cs.CY

TL;DR: 本文探讨荷兰媒体记者如何将AI技术融入日常工作，提出‘可控变革’概念并分析整合机制。


<details>
  <summary>Details</summary>
Motivation: AI在新闻业应用带来新挑战及对记者专业权威的影响，需研究记者如何将AI融入日常工作。

Method: 对不同新闻媒体的编辑、记者和创新经理进行13次访谈，并以专业权威为理论框架分析。

Result: 提出‘可控变革’概念，识别出记者管理AI整合的三种主要机制。

Conclusion: 记者以监督方式预见并整合AI技术，通过制定准则、实验和评估来管理整合。

Abstract: Using (generative) artificial intelligence tools and systems in journalism is
expected to increase journalists' production rates, transform newsrooms'
economic models, and further personalize the audience's news consumption
practices. Since its release in 2022, OpenAI's ChatGPT and other large language
models have raised the alarms inside news organizations, not only for bringing
new challenges to news reporting and fact-checking but also for what these
technologies would mean for journalists' professional authority in journalism.
This paper examines how journalists in Dutch media manage the integration of AI
technologies into their daily routines. Drawing from 13 interviews with
editors, journalists, and innovation managers in different news outlets and
media companies, we propose the concept of controlled change. as a heuristic to
explain how journalists are proactively setting guidelines, experimenting with
AI tools, and identifying their limitations and capabilities. Using
professional authority as a theoretical framework, we argue that journalists
anticipate and integrate AI technologies in a supervised manner and identify
three primary mechanisms through which journalists manage this integration: (1)
developing adaptive guidelines that align AI use with ethical codes, (2)
experimenting with AI technologies to determine their necessity and fit, and
(3) critically assessing the capabilities and limitations of AI systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [238] [AMAuT: A Flexible and Efficient Multiview Audio Transformer Framework Trained from Scratch](https://arxiv.org/abs/2510.19368)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.SD

TL;DR: 介绍AMAuT框架，支持任意采样率和音频长度，实验表现优，是大预训练模型高效灵活替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型受固定输入速率和时长限制，可复用性差，需新方法。

Method: 提出AMAuT框架，集成增强驱动多视图学习、一维CNN瓶颈、双CLS + TAL令牌和测试时自适应/增强四个关键组件。

Result: 在五个公开基准测试中，AMAuT准确率达99.8%，GPU耗时不到可比预训练模型的3%。

Conclusion: AMAuT是大预训练模型高效灵活替代方案，可在计算受限环境实现先进音频分类。

Abstract: Recent foundational models, SSAST, EAT, HuBERT, Qwen-Audio, and Audio
Flamingo, achieve top-tier results across standard audio benchmarks but are
limited by fixed input rates and durations, hindering their reusability. This
paper introduces the Augmentation-driven Multiview Audio Transformer (AMAuT), a
training-from-scratch framework that eliminates the dependency on pre-trained
weights while supporting arbitrary sample rates and audio lengths. AMAuT
integrates four key components: (1) augmentation-driven multiview learning for
robustness, (2) a conv1 + conv7 + conv1 one-dimensional CNN bottleneck for
stable temporal encoding, (3) dual CLS + TAL tokens for bidirectional context
representation, and (4) test-time adaptation/augmentation (TTA^2) to improve
inference reliability. Experiments on five public benchmarks, AudioMNIST,
SpeechCommands V1 & V2, VocalSound, and CochlScene, show that AMAuT achieves
accuracies up to 99.8% while consuming less than 3% of the GPU hours required
by comparable pre-trained models. Thus, AMAuT presents a highly efficient and
flexible alternative to large pre-trained models, making state-of-the-art audio
classification accessible in computationally constrained settings.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [239] [A Matter of Time: Revealing the Structure of Time in Vision-Language Models](https://arxiv.org/abs/2510.19559)
*Nidham Tekaya,Manuela Waldner,Matthias Zeppelzauer*

Main category: cs.CV

TL;DR: 本文研究大規模視覺語言模型（VLMs）的時間感知能力，引入TIME10k基準數據集，評估37個VLMs，發現時間信息在嵌入空間的分佈特點，提出從嵌入空間獲取顯式“時間線”表示的方法，且時間線方法在準確性和計算效率上有優勢。


<details>
  <summary>Details</summary>
Motivation: 探究VLMs的時間感知能力，即定位視覺內容時間的能力。

Method: 引入包含超過10,000張帶有時間真實值圖像的TIME10k基準數據集，用新方法評估37個VLMs，基於時間信息在嵌入空間的分佈特點提出獲取“時間線”表示的方法。

Result: 時間信息在VLM嵌入空間沿低維非線性流形結構化分佈，時間線方法比基於提示的基線方法在準確性上有競爭力甚至更優，且計算效率高。

Conclusion: 提出的從嵌入空間獲取“時間線”表示的方法可促進時間推理任務，在準確性和計算效率上有良好表現。

Abstract: Large-scale vision-language models (VLMs) such as CLIP have gained popularity
for their generalizable and expressive multimodal representations. By
leveraging large-scale training data with diverse textual metadata, VLMs
acquire open-vocabulary capabilities, solving tasks beyond their training
scope. This paper investigates the temporal awareness of VLMs, assessing their
ability to position visual content in time. We introduce TIME10k, a benchmark
dataset of over 10,000 images with temporal ground truth, and evaluate the
time-awareness of 37 VLMs by a novel methodology. Our investigation reveals
that temporal information is structured along a low-dimensional, non-linear
manifold in the VLM embedding space. Based on this insight, we propose methods
to derive an explicit ``timeline'' representation from the embedding space.
These representations model time and its chronological progression and thereby
facilitate temporal reasoning tasks. Our timeline approaches achieve
competitive to superior accuracy compared to a prompt-based baseline while
being computationally efficient. All code and data are available at
https://tekayanidham.github.io/timeline-page/.

</details>


### [240] [Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts](https://arxiv.org/abs/2510.19001)
*Seungjun Yu,Junsung Park,Youngsun Lim,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出用于自动驾驶的两阶段视觉语言问答系统，通过精心设计提示和上下文接地提升问答性能。


<details>
  <summary>Details</summary>
Motivation: 构建能回答自动驾驶中高层感知、预测和规划问题的视觉语言问答系统。

Method: 分两阶段，第一阶段用多模态大模型结合多相机输入、历史信息和思维链提示，采用自一致性集成；第二阶段增加场景元数据和特定类别问题指令。

Result: 在驾驶问答基准测试中显著优于基线模型，第一阶段最高达66.85%准确率，第二阶段达67.37%，视觉严重损坏时仍有96%准确率。

Conclusion: 精心设计的提示和上下文接地能极大提升预训练视觉语言模型在高层驾驶问答中的性能。

Abstract: We present a two-phase vision-language QA system for autonomous driving that
answers high-level perception, prediction, and planning questions. In Phase-1,
a large multimodal LLM (Qwen2.5-VL-32B) is conditioned on six-camera inputs, a
short temporal window of history, and a chain-of-thought prompt with few-shot
exemplars. A self-consistency ensemble (multiple sampled reasoning chains)
further improves answer reliability. In Phase-2, we augment the prompt with
nuScenes scene metadata (object annotations, ego-vehicle state, etc.) and
category-specific question instructions (separate prompts for perception,
prediction, planning tasks). In experiments on a driving QA benchmark, our
approach significantly outperforms the baseline Qwen2.5 models. For example,
using 5 history frames and 10-shot prompting in Phase-1 yields 65.1% overall
accuracy (vs.62.61% with zero-shot); applying self-consistency raises this to
66.85%. Phase-2 achieves 67.37% overall. Notably, the system maintains 96%
accuracy under severe visual corruption. These results demonstrate that
carefully engineered prompts and contextual grounding can greatly enhance
high-level driving QA with pretrained vision-language models.

</details>


### [241] [$Δ$t-Mamba3D: A Time-Aware Spatio-Temporal State-Space Model for Breast Cancer Risk Prediction](https://arxiv.org/abs/2510.19003)
*Zhengbo Zhou,Dooman Arefan,Margarita Zuley,Shandong Wu*

Main category: cs.CV

TL;DR: 提出Time - Aware Δt - Mamba3D模型解决纵向医学影像序列分析难题，在乳腺癌风险预测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法充分利用纵向放射影像序列中的时空信息，处理不规则时间间隔数据存在不足。

Method: 提出Time - Aware Δt - Mamba3D模型，采用连续时间选择性扫描机制和多尺度3D邻域融合模块。

Result: 在乳腺癌风险预测基准测试中，提高验证c指数2 - 5个百分点，1 - 5年AUC得分更高。

Conclusion: 该模型具有线性复杂度，能高效处理复杂影像序列，为纵向影像分析提供新框架。

Abstract: Longitudinal analysis of sequential radiological images is hampered by a
fundamental data challenge: how to effectively model a sequence of
high-resolution images captured at irregular time intervals. This data
structure contains indispensable spatial and temporal cues that current methods
fail to fully exploit. Models often compromise by either collapsing spatial
information into vectors or applying spatio-temporal models that are
computationally inefficient and incompatible with non-uniform time steps. We
address this challenge with Time-Aware $\Delta$t-Mamba3D, a novel state-space
architecture adapted for longitudinal medical imaging. Our model simultaneously
encodes irregular inter-visit intervals and rich spatio-temporal context while
remaining computationally efficient. Its core innovation is a continuous-time
selective scanning mechanism that explicitly integrates the true time
difference between exams into its state transitions. This is complemented by a
multi-scale 3D neighborhood fusion module that robustly captures
spatio-temporal relationships. In a comprehensive breast cancer risk prediction
benchmark using sequential screening mammogram exams, our model shows superior
performance, improving the validation c-index by 2-5 percentage points and
achieving higher 1-5 year AUC scores compared to established variants of
recurrent, transformer, and state-space models. Thanks to its linear
complexity, the model can efficiently process long and complex patient
screening histories of mammograms, forming a new framework for longitudinal
image analysis.

</details>


### [242] [PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions](https://arxiv.org/abs/2510.19060)
*Amith Ananthram,Elias Stengel-Eskin,Lorena A. Bradford,Julia Demarest,Adam Purvis,Keith Krut,Robert Stein,Rina Elster Pantalony,Mohit Bansal,Kathleen McKeown*

Main category: cs.CV

TL;DR: 本文指出视觉语言模型详细图像描述评估有挑战，引入PoSh指标和DOCENT数据集，验证PoSh效果并分析模型表现，助力相关领域进步。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型详细图像描述评估指标不适用于长文本，需要新的评估指标和数据集。

Method: 引入PoSh指标，用场景图引导大语言模型打分；引入DOCENT数据集进行验证。

Result: PoSh与人类判断相关性更强，对图像类型有鲁棒性，作为奖励函数表现更好；发现基础模型在复杂场景图像描述上有困难。

Conclusion: PoSh和DOCENT有助于推动辅助文本生成等重要领域的发展。

Abstract: While vision-language models (VLMs) have advanced into detailed image
description, evaluation remains a challenge. Standard metrics (e.g. CIDEr,
SPICE) were designed for short texts and tuned to recognize errors that are now
uncommon, such as object misidentification. In contrast, long texts require
sensitivity to attribute and relation attachments and scores that localize
errors to particular text spans. In this work, we introduce PoSh, a metric for
detailed image description that uses scene graphs as structured rubrics to
guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained
errors (e.g. mistakes in compositional understanding). PoSh is replicable,
interpretable and a better proxy for human raters than existing metrics
(including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new
dataset, DOCENT. This novel benchmark contains artwork, paired with
expert-written references, and model-generated descriptions, augmented with
granular and coarse judgments of their quality from art history students. Thus,
DOCENT enables evaluating both detailed image description metrics and detailed
image description itself in a challenging new domain. We show that PoSh
achieves stronger correlations (+0.05 Spearman $\rho$) with the human judgments
in DOCENT than the best open-weight alternatives, is robust to image type
(using CapArena, an existing dataset of web imagery) and is a capable reward
function, outperforming standard supervised fine-tuning. Then, using PoSh, we
characterize the performance of open and closed models in describing the
paintings, sketches and statues in DOCENT and find that foundation models
struggle to achieve full, error-free coverage of images with rich scene
dynamics, establishing a demanding new task to gauge VLM progress. Through both
PoSh and DOCENT, we hope to enable advances in important areas such as
assistive text generation.

</details>


### [243] [A Novel Approach to Breast Cancer Segmentation using U-Net Model with Attention Mechanisms and FedProx](https://arxiv.org/abs/2510.19118)
*Eyad Gad,Mustafa Abou Khatwa,Mustafa A. Elattar,Sahar Selim*

Main category: cs.CV

TL;DR: 本文针对超声乳腺癌影像数据，用FedProx方法结合改进U - Net模型处理非IID数据，实现96%准确率的全局模型，保护隐私且提升肿瘤分割精度。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌需早检测准确诊断，超声成像虽可用但医疗数据敏感，联邦学习能保护隐私，不过非IID数据影响模型准确性和泛化性，需解决该问题。

Method: 对非IID超声乳腺癌影像数据集应用Federated Proximal (FedProx)方法，结合带注意力机制的改进U - Net模型。

Result: 得到准确率达96%的全局模型。

Conclusion: FedProx有潜力成为在非IID本地医疗数据集上训练精确机器学习模型的有效方法。

Abstract: Breast cancer is a leading cause of death among women worldwide, emphasizing
the need for early detection and accurate diagnosis. As such Ultrasound
Imaging, a reliable and cost-effective tool, is used for this purpose, however
the sensitive nature of medical data makes it challenging to develop accurate
and private artificial intelligence models. A solution is Federated Learning as
it is a promising technique for distributed machine learning on sensitive
medical data while preserving patient privacy. However, training on
non-Independent and non-Identically Distributed (non-IID) local datasets can
impact the accuracy and generalization of the trained model, which is crucial
for accurate tumour boundary delineation in BC segmentation. This study aims to
tackle this challenge by applying the Federated Proximal (FedProx) method to
non-IID Ultrasonic Breast Cancer Imaging datasets. Moreover, we focus on
enhancing tumour segmentation accuracy by incorporating a modified U-Net model
with attention mechanisms. Our approach resulted in a global model with 96%
accuracy, demonstrating the effectiveness of our method in enhancing tumour
segmentation accuracy while preserving patient privacy. Our findings suggest
that FedProx has the potential to be a promising approach for training precise
machine learning models on non-IID local medical datasets.

</details>


### [244] [X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning](https://arxiv.org/abs/2510.19150)
*Yunzhe Wang,Soham Hans,Volkan Ustun*

Main category: cs.CV

TL;DR: 提出X - Ego - CS数据集和Cross - Ego Contrastive Learning (CECL)方法，用于电子竞技跨自我中心多智能体基准测试，代码和数据集公开。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解工作多依赖第三人称视角，忽略多智能体学习的同步、以自我为中心的特性，需更好研究复杂3D环境中多智能体决策。

Method: 引入X - Ego - CS数据集，包含124小时游戏画面；提出CECL方法，对齐队友以自我为中心的视觉流。

Result: 在队友 - 对手位置预测任务上评估CECL，证明其能增强智能体从第一人称视角推断位置的能力。

Conclusion: X - Ego - CS和CECL为电子竞技跨自我中心多智能体基准测试奠定基础，可用于多智能体建模和战术学习。

Abstract: Human team tactics emerge from each player's individual perspective and their
ability to anticipate, interpret, and adapt to teammates' intentions. While
advances in video understanding have improved the modeling of team interactions
in sports, most existing work relies on third-person broadcast views and
overlooks the synchronous, egocentric nature of multi-agent learning. We
introduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplay
footage from 45 professional-level matches of the popular e-sports game
Counter-Strike 2, designed to facilitate research on multi-agent
decision-making in complex 3D environments. X-Ego-CS provides cross-egocentric
video streams that synchronously capture all players' first-person perspectives
along with state-action trajectories. Building on this resource, we propose
Cross-Ego Contrastive Learning (CECL), which aligns teammates' egocentric
visual streams to foster team-level tactical situational awareness from an
individual's perspective. We evaluate CECL on a teammate-opponent location
prediction task, demonstrating its effectiveness in enhancing an agent's
ability to infer both teammate and opponent positions from a single
first-person view using state-of-the-art video encoders. Together, X-Ego-CS and
CECL establish a foundation for cross-egocentric multi-agent benchmarking in
esports. More broadly, our work positions gameplay understanding as a testbed
for multi-agent modeling and tactical learning, with implications for
spatiotemporal reasoning and human-AI teaming in both virtual and real-world
domains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.

</details>


### [245] [PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning](https://arxiv.org/abs/2510.19183)
*Fengyuan Sun,Hui Chen,Xinhao Xu,Dandan Zheng,Jingdong Chen,Jun Zhou,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 本文针对多模态大语言模型幻觉问题，提出无需训练的PruneHal方法，利用自适应KV缓存剪枝缓解幻觉，评估效果出色且代码将开源。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在幻觉问题，现有解决方案会引入额外计算成本。

Method: 提出PruneHal方法，利用自适应KV缓存剪枝，让模型更关注关键视觉信息，且无需额外训练、几乎无额外推理成本，与不同解码策略兼容。

Result: 在多个幻觉评估基准上，使用四个主流多模态大语言模型评估PruneHal，取得了稳健且出色的结果。

Conclusion: PruneHal方法有效且优越，是首个将token剪枝用于多模态大语言模型缓解幻觉的方法。

Abstract: While multi-modal large language models (MLLMs) have made significant
progress in recent years, the issue of hallucinations remains a major
challenge. To mitigate this phenomenon, existing solutions either introduce
additional data for further training or incorporate external or internal
information during inference. However, these approaches inevitably introduce
extra computational costs. In this paper, we observe that hallucinations in
MLLMs are strongly associated with insufficient attention allocated to visual
tokens. In particular, the presence of redundant visual tokens disperses the
model's attention, preventing it from focusing on the most informative ones. As
a result, critical visual cues are often under-attended, which in turn
exacerbates the occurrence of hallucinations. Building on this observation, we
propose \textbf{PruneHal}, a training-free, simple yet effective method that
leverages adaptive KV cache pruning to enhance the model's focus on critical
visual information, thereby mitigating hallucinations. To the best of our
knowledge, we are the first to apply token pruning for hallucination mitigation
in MLLMs. Notably, our method don't require additional training and incurs
nearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be
seamlessly integrated with different decoding strategies, including those
specifically designed for hallucination mitigation. We evaluate PruneHal on
several widely used hallucination evaluation benchmarks using four mainstream
MLLMs, achieving robust and outstanding results that highlight the
effectiveness and superiority of our method. Our code will be publicly
available.

</details>


### [246] [Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks](https://arxiv.org/abs/2510.19195)
*Kai Zeng,Zhanqian Wu,Kaixin Xiong,Xiaobao Wei,Xiangyu Guo,Zhenxin Zhu,Kalok Ho,Lijun Zhou,Bohan Zeng,Ming Lu,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出Dream4Drive框架用于生成合成数据提升自动驾驶下游感知任务，贡献DriveObj3D数据集，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略下游感知任务评估，且合成数据训练优势不明显，需证明合成数据益处。

Method: 引入Dream4Drive框架，分解视频为3D感知引导图，渲染3D资产，微调驱动世界模型生成多视角视频；贡献DriveObj3D数据集。

Result: Dream4Drive能大规模生成多视角极端情况，显著提升自动驾驶极端情况感知，实验证明可有效提升下游感知模型性能。

Conclusion: Dream4Drive框架和DriveObj3D数据集有助于提升自动驾驶下游感知任务性能，推动相关研究。

Abstract: Recent advancements in driving world models enable controllable generation of
high-quality RGB videos or multimodal videos. Existing methods primarily focus
on metrics related to generation quality and controllability. However, they
often overlook the evaluation of downstream perception tasks, which are
$\mathbf{really\ crucial}$ for the performance of autonomous driving. Existing
methods usually leverage a training strategy that first pretrains on synthetic
data and finetunes on real data, resulting in twice the epochs compared to the
baseline (real data only). When we double the epochs in the baseline, the
benefit of synthetic data becomes negligible. To thoroughly demonstrate the
benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data
generation framework designed for enhancing the downstream perception tasks.
Dream4Drive first decomposes the input video into several 3D-aware guidance
maps and subsequently renders the 3D assets onto these guidance maps. Finally,
the driving world model is fine-tuned to produce the edited, multi-view
photorealistic videos, which can be used to train the downstream perception
models. Dream4Drive enables unprecedented flexibility in generating multi-view
corner cases at scale, significantly boosting corner case perception in
autonomous driving. To facilitate future research, we also contribute a
large-scale 3D asset dataset named DriveObj3D, covering the typical categories
in driving scenarios and enabling diverse 3D-aware video editing. We conduct
comprehensive experiments to show that Dream4Drive can effectively boost the
performance of downstream perception models under various training epochs.
Project: $\href{https://wm-research.github.io/Dream4Drive/}{this\ https\ URL}$

</details>


### [247] [Enhancing Early Alzheimer Disease Detection through Big Data and Ensemble Few-Shot Learning](https://arxiv.org/abs/2510.19282)
*Safa Ben Atitallah,Maha Driss,Wadii Boulila,Anis Koubaa*

Main category: cs.CV

TL;DR: 文章针对阿尔茨海默病检测中标记数据有限的问题，提出基于原型网络和预训练CNN的集成方法，在两个数据集上取得高准确率，证明其在早期检测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病检测面临标记医学数据有限的挑战，需要有效方法提高检测准确性。

Method: 在少样本学习和集成学习框架下，利用预训练CNN，提出基于原型网络的集成方法，结合类感知损失和熵损失。

Result: 在Kaggle阿尔茨海默病数据集和ADNI数据集上分别达到99.72%和99.86%的准确率。

Conclusion: 所提方法准确率高，在早期阿尔茨海默病检测的实际应用中有效且有潜力。

Abstract: Alzheimer disease is a severe brain disorder that causes harm in various
brain areas and leads to memory damage. The limited availability of labeled
medical data poses a significant challenge for accurate Alzheimer disease
detection. There is a critical need for effective methods to improve the
accuracy of Alzheimer disease detection, considering the scarcity of labeled
data, the complexity of the disease, and the constraints related to data
privacy. To address this challenge, our study leverages the power of big data
in the form of pre-trained Convolutional Neural Networks (CNNs) within the
framework of Few-Shot Learning (FSL) and ensemble learning. We propose an
ensemble approach based on a Prototypical Network (ProtoNet), a powerful method
in FSL, integrating various pre-trained CNNs as encoders. This integration
enhances the richness of features extracted from medical images. Our approach
also includes a combination of class-aware loss and entropy loss to ensure a
more precise classification of Alzheimer disease progression levels. The
effectiveness of our method was evaluated using two datasets, the Kaggle
Alzheimer dataset and the ADNI dataset, achieving an accuracy of 99.72% and
99.86%, respectively. The comparison of our results with relevant
state-of-the-art studies demonstrated that our approach achieved superior
accuracy and highlighted its validity and potential for real-world applications
in early Alzheimer disease detection.

</details>


### [248] [Online Handwritten Signature Verification Based on Temporal-Spatial Graph Attention Transformer](https://arxiv.org/abs/2510.19321)
*Hai-jie Yuan,Heng Zhang,Fei Yin*

Main category: cs.CV

TL;DR: 本文提出用于动态签名验证的TS - GATR方法，结合GAT和GRU，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手写签名验证中因用户内差异和伪造风险，实现高精度验证存在挑战。

Method: 提出TS - GATR，结合GAT和GRU，用图表示签名，用注意力机制建模，采用DGATR模块，集成GRU捕捉长期时间依赖。

Result: 在MSDS和DeepSignDB等基准数据集上实验表明，TS - GATR在不同场景下始终能实现更低的等错误率。

Conclusion: TS - GATR方法优于当前最先进的签名验证方法。

Abstract: Handwritten signature verification is a crucial aspect of identity
authentication, with applications in various domains such as finance and
e-commerce. However, achieving high accuracy in signature verification remains
challenging due to intra-user variability and the risk of forgery. This paper
introduces a novel approach for dynamic signature verification: the
Temporal-Spatial Graph Attention Transformer (TS-GATR). TS-GATR combines the
Graph Attention Network (GAT) and the Gated Recurrent Unit (GRU) to model both
spatial and temporal dependencies in signature data. TS-GATR enhances
verification performance by representing signatures as graphs, where each node
captures dynamic features (e.g. position, velocity, pressure), and by using
attention mechanisms to model their complex relationships. The proposed method
further employs a Dual-Graph Attention Transformer (DGATR) module, which
utilizes k-step and k-nearest neighbor adjacency graphs to model local and
global spatial features, respectively. To capture long-term temporal
dependencies, the model integrates GRU, thereby enhancing its ability to learn
dynamic features during signature verification. Comprehensive experiments
conducted on benchmark datasets such as MSDS and DeepSignDB show that TS-GATR
surpasses current state-of-the-art approaches, consistently achieving lower
Equal Error Rates (EER) across various scenarios.

</details>


### [249] [Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters](https://arxiv.org/abs/2510.19329)
*Panagiotis Agrafiotis,Begüm Demir*

Main category: cs.CV

TL;DR: 提出Seabed - Net统一多任务框架，同时预测水深和海底分类，在两个沿海地点评估中表现优于传统方法，证实联合建模有协同增益。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像推导水深或海底类别的方法孤立处理任务，无法利用交互优势，阻碍深度学习方法广泛应用。

Method: 引入Seabed - Net，采用双分支编码器，通过注意力特征融合模块和窗口化Swin - Transformer融合块整合跨任务特征，用动态任务不确定性加权平衡目标。

Result: 在两个沿海地点评估中，优于传统经验模型和机器学习回归方法，RMSE最多降低75%；与基线相比，水深RMSE降低10 - 30%，海底分类精度最多提高8%，定性分析显示有更好空间一致性等。

Conclusion: 联合建模水深、基质和海底栖息地有协同增益，为浅水综合测绘提供强大开放解决方案。

Abstract: Accurate, detailed, and regularly updated bathymetry, coupled with complex
semantic content, is essential for under-mapped shallow-water environments
facing increasing climatological and anthropogenic pressures. However, existing
approaches that derive either depth or seabed classes from remote sensing
imagery treat these tasks in isolation, forfeiting the mutual benefits of their
interaction and hindering the broader adoption of deep learning methods. To
address these limitations, we introduce Seabed-Net, a unified multi-task
framework that simultaneously predicts bathymetry and pixel-based seabed
classification from remote sensing imagery of various resolutions. Seabed-Net
employs dual-branch encoders for bathymetry estimation and pixel-based seabed
classification, integrates cross-task features via an Attention Feature Fusion
module and a windowed Swin-Transformer fusion block, and balances objectives
through dynamic task uncertainty weighting. In extensive evaluations at two
heterogeneous coastal sites, it consistently outperforms traditional empirical
models and traditional machine learning regression methods, achieving up to
75\% lower RMSE. It also reduces bathymetric RMSE by 10-30\% compared to
state-of-the-art single-task and multi-task baselines and improves seabed
classification accuracy up to 8\%. Qualitative analyses further demonstrate
enhanced spatial consistency, sharper habitat boundaries, and corrected depth
biases in low-contrast regions. These results confirm that jointly modeling
depth with both substrate and seabed habitats yields synergistic gains,
offering a robust, open solution for integrated shallow-water mapping. Code and
pretrained weights are available at https://github.com/pagraf/Seabed-Net.

</details>


### [250] [CARES: Context-Aware Resolution Selector for VLMs](https://arxiv.org/abs/2510.19496)
*Moshe Kimhi,Nimrod Shabtay,Raja Giryes,Chaim Baskin,Eli Schwartz*

Main category: cs.CV

TL;DR: 提出轻量级预处理模块CARES，可预测图像最小足够输入分辨率，减少计算量同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型处理图像时使用原生或高分辨率，导致视觉令牌占比高、计算和延迟高，即使低分辨率图像也足够。

Method: 引入CARES模块，用紧凑VLM提取特征并预测目标预训练VLM响应何时收敛到正确回答的峰值能力，训练为离散分类器，推理时插值连续分辨率。

Result: 在五个跨文档和自然图像的多模态基准测试以及不同目标VLM上，CARES在减少高达80%计算量的同时保持了任务性能。

Conclusion: CARES能有效降低大型视觉语言模型的计算成本，同时维持任务表现。

Abstract: Large vision-language models (VLMs) commonly process images at native or high
resolution to remain effective across tasks. This inflates visual tokens ofter
to 97-99% of total tokens, resulting in high compute and latency, even when
low-resolution images would suffice. We introduce \emph{CARES}-a
\textbf{C}ontext-\textbf{A}ware \textbf{R}esolution \textbf{S}elector, a
lightweight preprocessing module that, given an image-query pair, predicts the
\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to
extract features and predict when a target pretrained VLM's response converges
to its peak ability to answer correctly. Though trained as a discrete
classifier over a set of optional resolutions, CARES interpolates continuous
resolutions at inference for fine-grained control. Across five multimodal
benchmarks spanning documents and natural images, as well as diverse target
VLMs, CARES preserves task performance while reducing compute by up to 80%.

</details>


### [251] [Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration](https://arxiv.org/abs/2510.19579)
*Francisco Mena,Dino Ienco,Cassio F. Dantas,Roberto Interdonato,Andreas Dengel*

Main category: cs.CV

TL;DR: 提出多模态协同学习框架，结合对比和模态判别学习，在四个地球观测基准测试中验证效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 地球观测领域多模态数据分析有挑战，当前研究多针对特定下游任务或特定模态，需通用框架。

Method: 提出多模态协同学习框架，结合对比和模态判别学习引导单模态模型分离信息。

Result: 在四个地球观测基准测试中，预测效果优于机器学习、计算机视觉和地球观测特定方法。

Conclusion: 框架在单模态推理场景的地球观测应用中有效。

Abstract: Multi-modal co-learning is emerging as an effective paradigm in machine
learning, enabling models to collaboratively learn from different modalities to
enhance single-modality predictions. Earth Observation (EO) represents a
quintessential domain for multi-modal data analysis, wherein diverse remote
sensors collect data to sense our planet. This unprecedented volume of data
introduces novel challenges. Specifically, the access to the same sensor
modalities at both training and inference stages becomes increasingly complex
based on real-world constraints affecting remote sensing platforms. In this
context, multi-modal co-learning presents a promising strategy to leverage the
vast amount of sensor-derived data available at the training stage to improve
single-modality models for inference-time deployment. Most current research
efforts focus on designing customized solutions for either particular
downstream tasks or specific modalities available at the inference stage. To
address this, we propose a novel multi-modal co-learning framework capable of
generalizing across various tasks without targeting a specific modality for
inference. Our approach combines contrastive and modality discriminative
learning together to guide single-modality models to structure the internal
model manifold into modality-shared and modality-specific information. We
evaluate our framework on four EO benchmarks spanning classification and
regression tasks across different sensor modalities, where only one of the
modalities available during training is accessible at inference time. Our
results demonstrate consistent predictive improvements over state-of-the-art
approaches from the recent machine learning and computer vision literature, as
well as EO-specific methods. The obtained findings validate our framework in
the single-modality inference scenarios across a diverse range of EO
applications.

</details>


### [252] [XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography](https://arxiv.org/abs/2510.19599)
*Haozhe Luo,Shelley Zixin Shu,Ziyu Zhou,Sebastian Otalora,Mauricio Reyes*

Main category: cs.CV

TL;DR: 本文对七种CLIP风格的视觉语言模型在胸部X光片跨模态可解释性进行系统基准测试，发现当前模型在临床可靠的概念落地方面不足。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医学图像理解的零样本性能出色，但概念落地能力待探索，而医学领域需要可靠的概念落地以实现可解释性和临床应用。

Method: 对七种CLIP风格的视觉语言模型进行基准测试，用交叉注意力和基于相似度的定位图生成视觉解释，并定量评估与放射科医生标注区域的对齐情况。

Result: 所有模型对大且明确的病变定位合理，但对小或弥漫性病变性能下降；在胸部X光特定数据集上预训练的模型对齐效果更好；模型的整体识别能力和概念落地能力强相关。

Conclusion: 当前视觉语言模型虽识别能力强，但临床可靠的概念落地不足，医学实践部署前需针对性的可解释性基准测试。

Abstract: Vision-language models (VLMs) have recently shown remarkable zero-shot
performance in medical image understanding, yet their grounding ability, the
extent to which textual concepts align with visual evidence, remains
underexplored. In the medical domain, however, reliable grounding is essential
for interpretability and clinical adoption. In this work, we present the first
systematic benchmark for evaluating cross-modal interpretability in chest
X-rays across seven CLIP-style VLM variants. We generate visual explanations
using cross-attention and similarity-based localization maps, and
quantitatively assess their alignment with radiologist-annotated regions across
multiple pathologies. Our analysis reveals that: (1) while all VLM variants
demonstrate reasonable localization for large and well-defined pathologies,
their performance substantially degrades for small or diffuse lesions; (2)
models that are pretrained on chest X-ray-specific datasets exhibit improved
alignment compared to those trained on general-domain data. (3) The overall
recognition ability and grounding ability of the model are strongly correlated.
These findings underscore that current VLMs, despite their strong recognition
ability, still fall short in clinically reliable grounding, highlighting the
need for targeted interpretability benchmarks before deployment in medical
practice. XBench code is available at
https://github.com/Roypic/Benchmarkingattention

</details>


### [253] [From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction](https://arxiv.org/abs/2510.19654)
*Zhida Zhao,Talas Fu,Yifan Wang,Lijun Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出Policy World Model (PWM) 范式，集成世界建模与轨迹规划，有新预测方案和机制，单摄像头输入效果超多模态方法。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型用于自主系统潜力未充分挖掘，世界建模与轨迹规划分离，协同机制待探索。

Method: 引入PWM范式，通过无动作未来状态预测方案用世界知识辅助规划，采用动态增强并行令牌生成机制、上下文引导分词器和自适应动态焦点损失。

Result: 仅用前摄像头输入，效果匹配或超越依赖多视图和多模态输入的先进方法。

Conclusion: PWM范式有效集成世界建模与规划，能提升规划性能，且在单摄像头输入下表现出色。

Abstract: Despite remarkable progress in driving world models, their potential for
autonomous systems remains largely untapped: the world models are mostly
learned for world simulation and decoupled from trajectory planning. While
recent efforts aim to unify world modeling and planning in a single framework,
the synergistic facilitation mechanism of world modeling for planning still
requires further exploration. In this work, we introduce a new driving paradigm
named Policy World Model (PWM), which not only integrates world modeling and
trajectory planning within a unified architecture, but is also able to benefit
planning using the learned world knowledge through the proposed action-free
future state forecasting scheme. Through collaborative state-action prediction,
PWM can mimic the human-like anticipatory perception, yielding more reliable
planning performance. To facilitate the efficiency of video forecasting, we
further introduce a dynamically enhanced parallel token generation mechanism,
equipped with a context-guided tokenizer and an adaptive dynamic focal loss.
Despite utilizing only front camera input, our method matches or exceeds
state-of-the-art approaches that rely on multi-view and multi-modal inputs.
Code and model weights will be released at
https://github.com/6550Zhao/Policy-World-Model.

</details>


### [254] [I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs](https://arxiv.org/abs/2510.19678)
*John Burden,Jonathan Prunty,Ben Slater,Matthieu Tehenan,Greg Davis,Lucy Cheke*

Main category: cs.CV

TL;DR: 本文借鉴认知心理学，用视觉搜索范式测试多模态大语言模型（MLLMs），发现其有类人视觉感知特点，表明视觉搜索可评估MLLMs感知能力。


<details>
  <summary>Details</summary>
Motivation: MLLMs视觉处理不透明，现有黑盒评估难揭示其潜在机制，需新方法评估。

Method: 借鉴认知心理学，用经典视觉搜索范式，针对颜色、大小和光照特征做控制实验，并用定向微调与机理解释分析强化结果。

Result: 先进MLLMs在基于颜色或大小的析取搜索中有类人弹出效应，合取搜索有容量限制，且像人类一样将自然场景先验融入对象表征。

Conclusion: 视觉搜索可作为基于认知的诊断工具，评估MLLMs的感知能力。

Abstract: Multimodal large language models (MLLMs) achieve strong performance on
vision-language tasks, yet their visual processing is opaque. Most black-box
evaluations measure task accuracy, but reveal little about underlying
mechanisms. Drawing on cognitive psychology, we adapt classic visual search
paradigms -- originally developed to study human perception -- to test whether
MLLMs exhibit the ``pop-out'' effect, where salient visual features are
detected independently of distractor set size. Using controlled experiments
targeting colour, size and lighting features, we find that advanced MLLMs
exhibit human-like pop-out effects in colour or size-based disjunctive (single
feature) search, as well as capacity limits for conjunctive (multiple feature)
search. We also find evidence to suggest that MLLMs, like humans, incorporate
natural scene priors such as lighting direction into object representations. We
reinforce our findings using targeted fine-tuning and mechanistic
interpretability analyses. Our work shows how visual search can serve as a
cognitively grounded diagnostic tool for evaluating perceptual capabilities in
MLLMs.

</details>


### [255] [Exploring "Many in Few" and "Few in Many" Properties in Long-Tailed, Highly-Imbalanced IC Defect Classification](https://arxiv.org/abs/2510.19463)
*Hao-Chiang Shao,Chun-Hao Chang,Yu-Hsien Lin,Chia-Wen Lin,Shao-Yun Fang,Yan-Hsiu Liu*

Main category: cs.CV

TL;DR: 现有深度分类技术用于真实IC缺陷分类有挑战，本文引入IC - Defect - 14数据集，提出ReCAME - Net模型，实验表明其表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度分类技术和自动光学检测模型应用于真实IC缺陷分类任务存在困难，因数据分布更倾斜且样本特征复杂。

Method: 引入IC - Defect - 14数据集，提出遵循多专家分类器框架的ReCAME - Net，集成区域通道注意力模块、度量学习损失、硬类别挖掘策略和知识蒸馏程序。

Result: ReCAME - Net在IC - Defect - 14数据集上优于先前模型，在通用公共数据集上表现相当且有竞争力。

Conclusion: ReCAME - Net能有效解决真实IC缺陷分类问题，在不平衡数据分类上有良好表现。

Abstract: Despite significant advancements in deep classification techniques and in-lab
automatic optical inspection models for long-tailed or highly imbalanced data,
applying these approaches to real-world IC defect classification tasks remains
challenging. This difficulty stems from two primary factors. First, real-world
conditions, such as the high yield-rate requirements in the IC industry, result
in data distributions that are far more skewed than those found in general
public imbalanced datasets. Consequently, classifiers designed for open
imbalanced datasets often fail to perform effectively in real-world scenarios.
Second, real-world samples exhibit a mix of class-specific attributes and
class-agnostic, domain-related features. This complexity adds significant
difficulty to the classification process, particularly for highly imbalanced
datasets. To address these challenges, this paper introduces the IC-Defect-14
dataset, a large, highly imbalanced IC defect image dataset sourced from AOI
systems deployed in real-world IC production lines. This dataset is
characterized by its unique "intra-class clusters" property, which presents two
major challenges: large intra-class diversity and high inter-class similarity.
These characteristics, rarely found simultaneously in existing public datasets,
significantly degrade the performance of current state-of-the-art classifiers
for highly imbalanced data. To tackle this challenge, we propose ReCAME-Net,
which follows a multi-expert classifier framework and integrates a regional
channel attention module, metric learning losses, a hard category mining
strategy, and a knowledge distillation procedure. Extensive experimental
evaluations demonstrate that ReCAME-Net outperforms previous state-of-the-art
models on the IC-Defect-14 dataset while maintaining comparable performance and
competitiveness on general public datasets.

</details>


### [256] [PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks](https://arxiv.org/abs/2510.19465)
*Ali Sadeghkhani,Brandon Bennett,Masoud Babaei,Arash Rabbani*

Main category: cs.CV

TL;DR: 本文提出多条件生成对抗网络框架生成有精确可控属性的代表性孔隙尺度图像，经测试效果良好，为地下表征提供工具。


<details>
  <summary>Details</summary>
Motivation: 获取与整体地层属性匹配的代表性孔隙尺度图像存在挑战，且数据稀缺。

Method: 提出多条件生成对抗网络（cGAN）框架，在碳酸盐岩地层四个深度的薄片样本上训练，同时以孔隙率值和深度参数为条件。

Result: 模型实现出色的孔隙率控制（R^2=0.95），形态验证保留关键孔隙网络特征，生成图像代表性优于随机提取的真实子图像。

Conclusion: 该框架为地下表征提供变革性工具，对碳储存、地热能和地下水管理等应用有重要价值。

Abstract: Obtaining truly representative pore-scale images that match bulk formation
properties remains a fundamental challenge in subsurface characterization, as
natural spatial heterogeneity causes extracted sub-images to deviate
significantly from core-measured values. This challenge is compounded by data
scarcity, where physical samples are only available at sparse well locations.
This study presents a multi-conditional Generative Adversarial Network (cGAN)
framework that generates representative pore-scale images with precisely
controlled properties, addressing both the representativeness challenge and
data availability constraints. The framework was trained on thin section
samples from four depths (1879.50-1943.50 m) of a carbonate formation,
simultaneously conditioning on porosity values and depth parameters within a
single unified model. This approach captures both universal pore network
principles and depth-specific geological characteristics, from grainstone
fabrics with interparticle-intercrystalline porosity to crystalline textures
with anhydrite inclusions. The model achieved exceptional porosity control
(R^2=0.95) across all formations with mean absolute errors of 0.0099-0.0197.
Morphological validation confirmed preservation of critical pore network
characteristics including average pore radius, specific surface area, and
tortuosity, with statistical differences remaining within acceptable geological
tolerances. Most significantly, generated images demonstrated superior
representativeness with dual-constraint errors of 1.9-11.3% compared to
36.4-578% for randomly extracted real sub-images. This capability provides
transformative tools for subsurface characterization, particularly valuable for
carbon storage, geothermal energy, and groundwater management applications
where knowing the representative morphology of the pore space is critical for
implementing digital rock physics.

</details>


### [257] [Uncertainty evaluation of segmentation models for Earth observation](https://arxiv.org/abs/2510.19586)
*Melanie Rey,Andriy Mnih,Maxim Neumann,Matt Overlan,Drew Purves*

Main category: cs.CV

TL;DR: 本文研究卫星图像语义分割预测的不确定性估计方法，在遥感数据集上评估现有方法并给出实用建议。


<details>
  <summary>Details</summary>
Motivation: 与标准图像分类相比，分割的不确定性估计有独特挑战，且相关研究多集中在场景理解或医学成像，缺乏针对遥感和地球观测应用的研究。

Method: 在PASTIS和ForTy两个遥感数据集上，对随机分割网络等多个模型，结合多种神经架构和不确定性指标进行广泛评估。

Result: 进行了大量实验评估不同模型和指标。

Conclusion: 基于研究结果给出了一些实用建议。

Abstract: This paper investigates methods for estimating uncertainty in semantic
segmentation predictions derived from satellite imagery. Estimating uncertainty
for segmentation presents unique challenges compared to standard image
classification, requiring scalable methods producing per-pixel estimates. While
most research on this topic has focused on scene understanding or medical
imaging, this work benchmarks existing methods specifically for remote sensing
and Earth observation applications. Our evaluation focuses on the practical
utility of uncertainty measures, testing their ability to identify prediction
errors and noise-corrupted input image regions. Experiments are conducted on
two remote sensing datasets, PASTIS and ForTy, selected for their differences
in scale, geographic coverage, and label confidence. We perform an extensive
evaluation featuring several models, such as Stochastic Segmentation Networks
and ensembles, in combination with a number of neural architectures and
uncertainty metrics. We make a number of practical recommendations based on our
findings.

</details>


### [258] [Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing](https://arxiv.org/abs/2510.19808)
*Yusu Qian,Eli Bocek-Rivele,Liangchen Song,Jialing Tong,Yinfei Yang,Jiasen Lu,Wenze Hu,Zhe Gan*

Main category: cs.CV

TL;DR: 提出用于基于指令的图像编辑的Pico - Banana - 400K数据集，可推动文本引导图像编辑模型研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究受缺乏大规模、高质量、开放可用的真实图像数据集限制，需新数据集推动文本引导图像编辑模型发展。

Method: 利用Nano - Banana从OpenImages集合的真实照片生成多样编辑对，采用细粒度图像编辑分类法，通过MLLM质量评分和精心策展保证质量和多样性。

Result: 构建包含400K图像的数据集，有三个专业子集，可用于多方面研究。

Conclusion: Pico - Banana - 400K为下一代文本引导图像编辑模型的训练和基准测试奠定坚实基础。

Abstract: Recent advances in multimodal models have demonstrated remarkable text-guided
image editing capabilities, with systems like GPT-4o and Nano-Banana setting
new benchmarks. However, the research community's progress remains constrained
by the absence of large-scale, high-quality, and openly accessible datasets
built from real images. We introduce Pico-Banana-400K, a comprehensive
400K-image dataset for instruction-based image editing. Our dataset is
constructed by leveraging Nano-Banana to generate diverse edit pairs from real
photographs in the OpenImages collection. What distinguishes Pico-Banana-400K
from previous synthetic datasets is our systematic approach to quality and
diversity. We employ a fine-grained image editing taxonomy to ensure
comprehensive coverage of edit types while maintaining precise content
preservation and instruction faithfulness through MLLM-based quality scoring
and careful curation. Beyond single turn editing, Pico-Banana-400K enables
research into complex editing scenarios. The dataset includes three specialized
subsets: (1) a 72K-example multi-turn collection for studying sequential
editing, reasoning, and planning across consecutive modifications; (2) a
56K-example preference subset for alignment research and reward model training;
and (3) paired long-short editing instructions for developing instruction
rewriting and summarization capabilities. By providing this large-scale,
high-quality, and task-rich resource, Pico-Banana-400K establishes a robust
foundation for training and benchmarking the next generation of text-guided
image editing models.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [259] [Dara: Automated multiple-hypothesis phase identification and refinement from powder X-ray diffraction](https://arxiv.org/abs/2510.19667)
*Yuxing Fei,Matthew J. McDermott,Christopher L. Rom,Shilong Wang,Gerbrand Ceder*

Main category: cond-mat.mtrl-sci

TL;DR: 介绍Dara框架自动化分析粉末XRD数据以鉴定和精修多相，提升相鉴定可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 粉末XRD图谱可靠解读困难，尤其在多相系统中需人工且依赖专业知识，单一图谱可能有多种拟合结果易误判，需减轻人力并解决挑战。

Method: 在给定化学空间对所有可能相组合进行穷举树搜索，用BGMN进行Rietveld精修验证假设，有数据库过滤、等结构相自动聚类、基于峰匹配评分等特点，有歧义时生成多假设。

Result: 可实现现实复杂XRD图谱的可扩展分析。

Conclusion: Dara提升相鉴定可靠性和准确性，为多模式表征工作流程集成提供基础，推动全自主材料发现。

Abstract: Powder X-ray diffraction (XRD) is a foundational technique for characterizing
crystalline materials. However, the reliable interpretation of XRD patterns,
particularly in multiphase systems, remains a manual and expertise-demanding
task. As a characterization method that only provides structural information,
multiple reference phases can often be fit to a single pattern, leading to
potential misinterpretation when alternative solutions are overlooked. To ease
humans' efforts and address the challenge, we introduce Dara (Data-driven
Automated Rietveld Analysis), a framework designed to automate the robust
identification and refinement of multiple phases from powder XRD data. Dara
performs an exhaustive tree search over all plausible phase combinations within
a given chemical space and validates each hypothesis using a robust Rietveld
refinement routine (BGMN). Key features include structural database filtering,
automatic clustering of isostructural phases during tree expansion,
peak-matching-based scoring to identify promising phases for refinement. When
ambiguity exists, Dara generates multiple hypothesis which can then be decided
between by human experts or with further characteriztion tools. By enhancing
the reliability and accuracy of phase identification, Dara enables scalable
analysis of realistic complex XRD patterns and provides a foundation for
integration into multimodal characterization workflows, moving toward fully
self-driving materials discovery.

</details>


### [260] [Synthesizability Prediction of Crystalline Structures with a Hierarchical Transformer and Uncertainty Quantification](https://arxiv.org/abs/2510.19251)
*Danial Ebrahimzadeh,Sarah Sharif,Yaser Mike Banad*

Main category: cond-mat.mtrl-sci

TL;DR: SyntheFormer是预测无机晶体可合成性的正无标签框架，结合多种方法，经评估表现良好，能助力筛选合成目标。


<details>
  <summary>Details</summary>
Motivation: 解决预测无机晶体能否实验实现这一加速材料发现的核心挑战。

Method: 结合FTCP表示、分层特征提取、随机森林特征选择和紧凑深度MLP分类器，在2011 - 2018年数据训练，2019 - 2025年数据评估。

Result: ROC曲线下面积达0.735，双阈值校准后召回率97.6%、覆盖率94.2%，能识别亚稳化合物，给难合成稳定候选物低分。

Conclusion: SyntheFormer通过结构感知表示与不确定性感知决策规则结合，为优先选择合成目标和聚焦实验室工作提供实用途径。

Abstract: Predicting which hypothetical inorganic crystals can be experimentally
realized remains a central challenge in accelerating materials discovery.
SyntheFormer is a positive-unlabeled framework that learns synthesizability
directly from crystal structure, combining a Fourier-transformed crystal
periodicity (FTCP) representation with hierarchical feature extraction,
Random-Forest feature selection, and a compact deep MLP classifier. The model
is trained on historical data from 2011 through 2018 and evaluated
prospectively on future years from 2019 to 2025, where the positive class
constitutes only 1.02 per cent of samples. Under this temporally separated
evaluation, SyntheFormer achieves a test area under the ROC curve of 0.735 and,
with dual-threshold calibration, attains high-recall screening with 97.6 per
cent recall at 94.2 per cent coverage, which minimizes missed opportunities
while preserving discriminative power. Crucially, the model recovers
experimentally confirmed metastable compounds that lie far from the convex hull
and simultaneously assigns low scores to many thermodynamically stable yet
unsynthesized candidates, demonstrating that stability alone is insufficient to
predict experimental attainability. By aligning structure-aware representation
with uncertainty-aware decision rules, SyntheFormer provides a practical route
to prioritize synthesis targets and focus laboratory effort on the most
promising new inorganic materials.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [261] [What is Implementation Science; and Why It Matters for Bridging the Artificial Intelligence Innovation-to-Application Gap in Medical Imaging](https://arxiv.org/abs/2510.13006)
*Ahmad Fayaz-Bakhsh,Janice Tania,Syaheerah Lebai Lutfi,Abhinav K. Jha,Arman Rahmim*

Main category: physics.med-ph

TL;DR: AI在医学影像中有变革潜力，但实践中难临床应用，实施科学可缩短差距，本文分析挑战并强调相关策略。


<details>
  <summary>Details</summary>
Motivation: 解决AI在医学影像中从研发到临床应用存在的延迟问题。

Method: 概述AI在医学影像工作流程中采用的挑战，强调有效性研究和实施研究的互补作用，讨论人机交互框架的整合。

Result: 指出实施科学不仅是方法进步，更是加速创新转化为改善患者结果的战略必要。

Conclusion: 采用实施科学是加速AI创新在医学影像中转化、改善患者结局的关键。

Abstract: The transformative potential of artificial intelligence (AI) in medical
Imaging (MI) is well recognized. Yet despite promising reports in research
settings, many AI tools fail to achieve clinical adoption in practice. In fact,
more generally, there is a documented 17-year average delay between evidence
generation and implementation of a technology1. Implementation science (IS) may
provide a practical, evidence-based framework to bridge the gap between AI
development and real-world clinical imaging use that helps shorten this lag
through systematic frameworks, strategies, and hybrid research designs. We
outline challenges specific to AI adoption in MI workflows, including
infrastructural, educational, and cultural barriers. We highlight the
complementary roles of effectiveness research and implementation research,
emphasizing hybrid study designs and the role of integrated KT (iKT),
stakeholder engagement, and equity-focused co-creation in designing sustainable
and generalizable solutions. We discuss integration of Human-Computer
Interaction (HCI) frameworks in MI towards usable AI. Adopting IS is not only a
methodological advancement; it is a strategic imperative for accelerating
translation of innovation into improved patient outcomes.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [262] [Supermodular Maximization with Cardinality Constraints](https://arxiv.org/abs/2510.19191)
*Xujin Chen,Xiaodong Hu,Changjun Wang,Qingjie Ye*

Main category: math.OC

TL;DR: 本文研究非负单调超模函数在基数约束下的最大化问题，针对r - 可分解函数给出多项式时间近似算法，还考虑已知分解时的连通性约束问题，改进了最密连通k - 子图问题的近似比。


<details>
  <summary>Details</summary>
Motivation: 解决非负单调超模函数在基数约束|S| = k或|S| ≤ k下的最大化问题，以及在已知分解时引入连通性要求的推广问题。

Method: 提出不依赖具体分解的多项式时间O(n^((r - 1)/2))近似算法；针对已知分解和连通性要求的问题，也提出多项式时间O(n^((r - 1)/2))近似算法。

Result: 得到非负单调超模函数最大化问题的近似算法；改进最密连通k - 子图问题的近似比至O(n^(1/2))。

Conclusion: 所提算法能有效解决相关超模函数最大化问题，对最密连通k - 子图问题有更好的近似效果。

Abstract: Let $V$ be a finite set of $n$ elements, $f: 2^V \rightarrow \mathbb{R}_+$ be
a nonnegative monotone supermodular function, and $k$ be a positive integer no
greater than $n$. This paper addresses the problem of maximizing $f(S)$ over
all subsets $S \subseteq V$ subject to the cardinality constraint $|S| = k$ or
$|S|\le k$.
  Let $r$ be a constant integer. The function $f$ is assumed to be {\em
$r$-decomposable}, meaning there exist $m\,(\ge1)$ subsets $V_1, \dots, V_m$ of
$V$, each with a cardinality at most $r$, and a corresponding set of
nonnegative supermodular functions $f_i : 2^{V_i} \rightarrow \mathbb{R}_+$,
$i=1,\ldots,m$ such that $f(S) =\sum_{i=1}^m f_i(S \cap V_i)$ holds for each $S
\subseteq V$. Given $r$ as an input, we present a polynomial-time
$O(n^{(r-1)/2})$-approximation algorithm for this maximization problem, which
does not require prior knowledge of the specific decomposition.
  When the decomposition $(V_i,f_i)_{i=1}^m$ is known, an additional
connectivity requirement is introduced to the problem. Let $G$ be the graph
with vertex set $V$ and edge set $\cup_{i=1}^m \{uv:u,v\in V_i,u\neq v\}$. The
cardinality constrained solution set $S$ is required to induce a connected
subgraph in $G$. This model generalizes the well-known problem of finding the
densest connected $k$-subgraph. We propose a polynomial time
$O(n^{(r-1)/2})$-approximation algorithm for this generalization. Notably, this
algorithm gives an $O(n^{1/2})$-approximation for the densest connected
$k$-subgraph problem, improving upon the previous best-known approximation
ratio of $O(n^{2/3})$.

</details>


### [263] [Nonmonotone subgradient methods based on a local descent lemma](https://arxiv.org/abs/2510.19341)
*Francisco J. Aragón-Artacho,Rubén Campoy,Pedro Pérez-Aros,David Torregrosa-Belén*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The aim of this paper is to extend the context of nonmonotone descent methods
to the class of nonsmooth and nonconvex functions called upper-$\mathcal{C}^2$,
which satisfy a nonsmooth and local version of the descent lemma. Under this
assumption, we propose a general subgradient method that performs a nonmonotone
linesearch, and we prove subsequential convergence to a stationary point of the
optimization problem. Our approach allows us to cover the setting of various
subgradient algorithms, including Newton and quasi-Newton methods. In addition,
we propose a specification of the general scheme, named Self-adaptive
Nonmonotone Subgradient Method (SNSM), which automatically updates the
parameters of the linesearch. Particular attention is paid to the minimum
sum-of-squares clustering problem, for which we provide a concrete
implementation of SNSM. We conclude with some numerical experiments where we
exhibit the advantages of SNSM in comparison with some known algorithms.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [264] [On the Power Saving in High-Speed Ethernet-based Networks for Supercomputers and Data Centers](https://arxiv.org/abs/2510.19783)
*Miguel Sánchez de la Rosa,Francisco J. andújar,Jesus Escudero-Sahuquillo,José L. Sánchez,Francisco J. Alfaro-Cortés*

Main category: cs.NI

TL;DR: 本文探讨高性能计算和数据中心网络节能技术与性能下降关系，提出利用EEE，分析PerfBound，通过实验评估节能技术，揭示动态关机机制弱点并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 计算和存储增长使系统规模扩大，引发可持续性和运营成本担忧，需研究节能技术。

Method: 利用EEE，分析PerfBound并建模到仿真框架，进行不同实验，研究应用流量模式。

Result: 分析了应用对系统和网络能耗的影响，揭示动态关机机制弱点。

Conclusion: 提出的节能方法能以最小或无性能损失降低能耗，是首个针对未来基于以太网的HPC架构的电源管理方案，结果有前景。

Abstract: The increase in computation and storage has led to a significant growth in
the scale of systems powering applications and services, raising concerns about
sustainability and operational costs. In this paper, we explore power-saving
techniques in high-performance computing (HPC) and datacenter networks, and
their relation with performance degradation. From this premise, we propose
leveraging Energy Efficient Ethernet (EEE), with the flexibility to extend to
conventional Ethernet or upcoming Ethernet-derived interconnect versions of BXI
and Omnipath.
  We analyze the PerfBound proposal, identifying possible improvements and
modeling it into a simulation framework. Through different experiments, we
examine its impact on performance and determine the most appropriate
interconnect. We also study traffic patterns generated by selected HPC and
machine learning applications to evaluate the behavior of power-saving
techniques.
  From these experiments, we provide an analysis of how applications affect
system and network energy consumption. Based on this, we disclose the weakness
of dynamic power-down mechanisms and propose an approach that improves energy
reduction with minimal or no performance penalty. To our knowledge, this is the
first power management proposal tailored to future Ethernet-based HPC
architectures, with promising results.

</details>


### [265] [Enabling Reconfiguration-Communication Overlap for Collective Communication in Optical Networks](https://arxiv.org/abs/2510.19322)
*Changbo Wu,Zhuolong Yu,Gongming Zhao,Hongli Xu*

Main category: cs.NI

TL;DR: 现有光互连集体通信方案有扩展性局限，提出SWOT框架，模拟显示其性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于光互连的集体通信方案在支持现代工作负载所需的复杂高效算法时面临扩展性局限，如资源过度配置或性能下降。

Method: 提出SWOT需求感知光网络框架，采用“集体内重新配置”，结合重叠光开关重新配置与正在进行的传输的调度技术，引入轻量级集体通信垫片。

Result: 模拟结果表明SWOT有显著的性能提升。

Conclusion: SWOT框架能有效解决现有光互连集体通信方案的问题，提升通信效率。

Abstract: Collective communication (CC) is widely adopted for large-scale distributed
machine learning (DML) training workloads. DML's predictable traffic pattern
provides a great oppotunity for applying optical network technology. Existing
optical interconnects-based CC schemes adopt ``one-shot network
reconfiguration'', which provisions static high-capacity topologies for an
entire collective operation -- sometimes for a full training iteration.
However, this approach faces significant scalability limitations when
supporting more complex and efficient CC algorithms required for modern
workloads: the ``one-shot'' strategies either demand excessive resource
overprovisioning or suffer performance degradation due to rigid resource
allocation.
  To address these challenges, we propose SWOT, a demand-aware optical network
framework. SWOT employs ``intra-collective reconfiguration'' and can
dynamically align network resources with CC traffic patterns. SWOT incorporates
a novel scheduling technique that overlaps optical switch reconfigurations with
ongoing transmissions, and improves communication efficiency. SWOT introduce a
lightweight collective communication shim that enables coordinated optical
network configuration and transmission scheduling while supporting seamless
integration with existing CC libraries. Our simulation results demonstrate
SWOT's significant performance improvements.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [266] [Learning noisy tissue dynamics across time scales](https://arxiv.org/abs/2510.19090)
*Ming Han,John Devany,Michel Fruchart,Margaret L. Gardel,Vincenzo Vitelli*

Main category: cond-mat.soft

TL;DR: 提出仿生机器学习框架推断多细胞动力学，以上皮组织实验为例展示其能力，可用于生物工程和临床。


<details>
  <summary>Details</summary>
Motivation: 组织动力学在生物过程中重要但难以预测，需有效方法推断。

Method: 引入结合图神经网络、归一化流和WaveNet算法的仿生机器学习框架，将组织表示为神经随机微分方程。

Result: 模型能捕捉随机细胞运动、预测细胞分裂周期状态，准确生成发育系统和细胞信号过程的实验动力学。

Conclusion: 该方法为生物工程和临床情境中作为数字孪生应用铺平道路。

Abstract: Tissue dynamics play a crucial role in biological processes ranging from
wound healing to morphogenesis. However, these noisy multicellular dynamics are
notoriously hard to predict. Here, we introduce a biomimetic machine learning
framework capable of inferring noisy multicellular dynamics directly from
experimental movies. This generative model combines graph neural networks,
normalizing flows and WaveNet algorithms to represent tissues as neural
stochastic differential equations where cells are edges of an evolving graph.
This machine learning architecture reflects the architecture of the underlying
biological tissues, substantially reducing the amount of data needed to train
it compared to convolutional or fully-connected neural networks. Taking
epithelial tissue experiments as a case study, we show that our model not only
captures stochastic cell motion but also predicts the evolution of cell states
in their division cycle. Finally, we demonstrate that our method can accurately
generate the experimental dynamics of developmental systems, such as the fly
wing, and cell signaling processes mediated by stochastic ERK waves, paving the
way for its use as a digital twin in bioengineering and clinical contexts.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [267] [Tidying Up the Address Space](https://arxiv.org/abs/2510.19765)
*Vinay Banakar,Suli Yang,Kan Wu,Andrea C. Arpaci-Dusseau,Remzi H. Arpaci-Dusseau,Kimberly Keeton*

Main category: cs.OS

TL;DR: 论文指出数据中心内存分层因热度碎片化未发挥全部潜力，提出地址空间工程方法，HADES系统验证该方法，评估显示可大幅减少内存且性能开销小。


<details>
  <summary>Details</summary>
Motivation: 解决数据中心内存分层因热度碎片化导致无法充分发挥潜力、限制内存效率的问题。

Method: 引入地址空间工程，通过编译器 - 运行时系统HADES根据访问模式跟踪和迁移对象。

Result: 在十种数据结构评估中实现了高达70%的内存减少，性能开销仅3%。

Conclusion: 地址空间工程能使现有回收系统积极回收内存且不降低性能。

Abstract: Memory tiering in datacenters does not achieve its full potential due to
hotness fragmentation -- the intermingling of hot and cold objects within
memory pages. This fragmentation prevents page-based reclamation systems from
distinguishing truly hot pages from pages containing mostly cold objects,
fundamentally limiting memory efficiency despite highly skewed accesses. We
introduce address-space engineering: dynamically reorganizing application
virtual address spaces to create uniformly hot and cold regions that any
page-level tiering backend can manage effectively. HADES demonstrates this
frontend/backend approach through a compiler-runtime system that tracks and
migrates objects based on access patterns, requiring minimal developer
intervention. Evaluations across ten data structures achieve up to 70% memory
reduction with 3% performance overhead, showing that address space engineering
enables existing reclamation systems to reclaim memory aggressively without
performance degradation.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [268] [Wind Variability and Its Effect on Transmission Line Capacity Estimation](https://arxiv.org/abs/2510.19433)
*Nika Mlinarič Hribar,Matjaž Depolli,Gregor Kosec*

Main category: physics.app-ph

TL;DR: 研究风速平均对动态热额定值（DTR）计算的影响，表明平均风速对DTR结果有显著影响，不同平均方法影响不同。


<details>
  <summary>Details</summary>
Motivation: 探究风速平均对DTR计算的影响。

Method: 基于欧洲斯洛文尼亚输电线路的高时间分辨率（1秒）风速测量数据，采用向量平均和混合平均两种方法，对高分辨率数据和平均数据（5分钟平均窗口）进行DTR计算。

Result: 平均对努塞尔数和载流量有显著影响，且有角度依赖性；平行风时平均数据低估载流量，垂直风时两种平均方法都可能高估载流量。

Conclusion: 风速平均对DTR结果有显著影响，应重视平均方法的选择。

Abstract: This study investigates the impact of wind velocity averaging on Dynamic
Thermal Rating (DTR) calculations. It is based on a high-temporal-resolution (1
second) wind measurements obtained from a transmission line in Slovenia,
Europe. Wind speed and direction variability are analysed, and two averaging
methods, namely vector averaging, where velocity is averaged as vector, and
hybrid averaging, where speed is averaged as scalar, are employed. DTR
calculations are performed on both high-resolution data and averaged data (5
minute averaging window). It is demonstrated that averaging has a significant
effect on both Nusselt number and ampacity, and the effect exhibits a strong
angular dependency on the relative angle of the wind to the line. Therefore,
two limit cases are studied: in the case of parallel wind, averaged data
underestimates the ampacity, and there is a significant amount of cases where
the underestimation is larger than 10 %. In the case of perpendicular wind, the
two averaging methods affect the results in different ways, but both result in
a substantial amount of cases where ampacity is overestimated, potentially
leading to unsafe operation. The main takeaway of the study is that averaging
wind velocity has a significant impact on DTR results, and special emphasis
should be given to the averaging method, as different methods affect the
results in different ways.

</details>


### [269] [Magnetic field estimation using Gaussian process regression for interactive wireless power system design](https://arxiv.org/abs/2510.19277)
*Yuichi Honjo,Cedric Caremel,Ken Takaki,Yuta Noma,Yoshihiro Kawahara,Takuya Sasatani*

Main category: physics.app-ph

TL;DR: 提出用高斯过程回归（GPR）的机器学习方法，实现近场耦合系统磁场和功率传输效率的快速估计。


<details>
  <summary>Details</summary>
Motivation: 传统电磁场模拟方法计算资源需求大，系统对位置和几何变化敏感，需大量模拟，铁磁屏蔽结构使模拟更复杂，限制交互性。

Method: 采用高斯过程回归（GPR）的机器学习方法，开发3D自适应网格系统和主动学习策略。

Result: 实现亚秒级延迟的磁场计算，与独立电磁模拟结果验证时平均误差小于6%。

Conclusion: 该方法能快速准确估计近场耦合系统的整个磁场和功率传输效率。

Abstract: Wireless power transfer (WPT) with coupled resonators offers a promising
solution for the seamless powering of electronic devices. Interactive design
approaches that visualize the magnetic field and power transfer efficiency
based on system geometry adjustments can facilitate the understanding and
exploration of the behavior of these systems for dynamic applications. However,
typical electromagnetic field simulation methods, such as the Method of Moments
(MoM), require significant computational resources, limiting the rate at which
computation can be performed for acceptable interactivity. Furthermore, the
system's sensitivity to positional and geometrical changes necessitates a large
number of simulations, and structures such as ferromagnetic shields further
complicate these simulations. Here, we introduce a machine learning approach
using Gaussian Process Regression (GPR), demonstrating for the first time the
rapid estimation of the entire magnetic field and power transfer efficiency for
near-field coupled systems. To achieve quick and accurate estimation, we
develop 3D adaptive grid systems and an active learning strategy to effectively
capture the nonlinear interactions between complex system geometries and
magnetic fields. By training a regression model, our approach achieves magnetic
field computation with sub-second latency and with an average error of less
than 6% when validated against independent electromagnetic simulation results.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [270] [DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code](https://arxiv.org/abs/2510.18904)
*Shriyansh Agrawal,Aidan Lau,Sanyam Shah,Ahan M R,Kevin Zhu,Sunishchal Dev,Vasu Sharma*

Main category: cs.CL

TL;DR: 本文提出微调小型语言模型（SLMs）用于检测机器生成内容，证明其在性能上大幅超越大语言模型（LLMs），且计算资源消耗少。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成内容检测器存在计算成本高或准确性不足的问题，需要改进。

Method: 使用专门数据集对RoBERTA和CodeBERTa等仅编码器的预训练SLMs进行微调。

Result: 编码器的AUROC达到0.97 - 0.99，macro - F1达到0.89 - 0.94，降低了延迟和峰值VRAM，在跨生成器转移和对抗变换下性能保持良好。

Conclusion: 在二分类任务中，SLMs能以少量计算资源大幅超越LLMs。

Abstract: The prevalence of Large Language Models (LLMs) for generating multilingual
text and source code has only increased the imperative for machine-generated
content detectors to be accurate and efficient across domains. Current
detectors, predominantly utilizing zero-shot methods, such as Fast DetectGPT or
GPTZero, either incur high computational cost or lack sufficient accuracy,
often with a trade-off between the two, leaving room for further improvement.
To address these gaps, we propose the fine-tuning of encoder-only Small
Language Models (SLMs), in particular, the pre-trained models of RoBERTA and
CodeBERTa using specialized datasets on source code and other natural language
to prove that for the task of binary classification, SLMs outperform LLMs by a
huge margin whilst using a fraction of compute. Our encoders achieve AUROC $=
0.97$ to $0.99$ and macro-F1 $0.89$ to $0.94$ while reducing latency by
$8$-$12\times$ and peak VRAM by $3$-$5\times$ at $512$-token inputs. Under
cross-generator shifts and adversarial transformations (paraphrase,
back-translation; code formatting/renaming), performance retains $\geq 92%$ of
clean AUROC. We release training and evaluation scripts with seeds and configs;
a reproducibility checklist is also included.

</details>


### [271] [The Massive Legal Embedding Benchmark (MLEB)](https://arxiv.org/abs/2510.19365)
*Umar Butler,Abdur-Rahman Butler,Adrian Lucas Malec*

Main category: cs.CL

TL;DR: 介绍了目前最大、最多样、最全面的开源法律信息检索基准MLEB，包含十个专家标注数据集，七个为新建，公开代码、结果和数据。


<details>
  <summary>Details</summary>
Motivation: 填补开源法律信息检索领域和司法管辖区的空白。

Method: 构建MLEB并创建新的组成数据集，记录构建方法。

Result: 成功构建包含十个数据集的MLEB，其中七个为新建。

Conclusion: 公开代码、结果和数据以辅助可重复评估。

Abstract: We present the Massive Legal Embedding Benchmark (MLEB), the largest, most
diverse, and most comprehensive open-source benchmark for legal information
retrieval to date. MLEB consists of ten expert-annotated datasets spanning
multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore),
document types (cases, legislation, regulatory guidance, contracts, and
literature), and task types (search, zero-shot classification, and question
answering). Seven of the datasets in MLEB were newly constructed in order to
fill domain and jurisdictional gaps in the open-source legal information
retrieval landscape. We document our methodology in building MLEB and creating
the new constituent datasets, and release our code, results, and data openly to
assist with reproducible evaluations.

</details>


### [272] [ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers](https://arxiv.org/abs/2510.19791)
*Saptarshi Sengupta,Zhengyu Zhou,Jun Araki,Xingbo Wang,Bingqing Wang,Suhang Wang,Zhe Feng*

Main category: cs.CL

TL;DR: 提出ToolDreamer框架解决大工具集下大语言模型工具检索问题，提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型工具调用中，大工具集易超上下文窗口限制，且现有检索模型因用户请求与工具描述语言不对齐导致检索不佳。

Method: 提出ToolDreamer框架，基于大语言模型生成的假设工具描述来调节检索器模型以获取工具。

Result: 在ToolRet数据集上应用，提高了稀疏和密集检索器的性能，展示了灵活性。

Conclusion: 该框架可将部分推理负担转移到检索器，使大语言模型有效处理大量工具而不超出上下文窗口。

Abstract: Tool calling has become increasingly popular for Large Language Models
(LLMs). However, for large tool sets, the resulting tokens would exceed the
LLM's context window limit, making it impossible to include every tool. Hence,
an external retriever is used to provide LLMs with the most relevant tools for
a query. Existing retrieval models rank tools based on the similarity between a
user query and a tool description (TD). This leads to suboptimal retrieval as
user requests are often poorly aligned with the language of TD. To remedy the
issue, we propose ToolDreamer, a framework to condition retriever models to
fetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e.,
description of tools that the LLM feels will be potentially useful for the
query. The framework enables a more natural alignment between queries and tools
within the language space of TD's. We apply ToolDreamer on the ToolRet dataset
and show that our method improves the performance of sparse and dense
retrievers with and without training, thus showcasing its flexibility. Through
our proposed framework, our aim is to offload a portion of the reasoning burden
to the retriever so that the LLM may effectively handle a large collection of
tools without inundating its context window.

</details>


### [273] [A Graph Signal Processing Framework for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.19117)
*Valentin Noël*

Main category: cs.CL

TL;DR: 提出光谱分析框架检测大语言模型幻觉，实验发现不同类型幻觉光谱特征不同，简单检测器准确率高。


<details>
  <summary>Details</summary>
Motivation: 大语言模型区分事实推理和幻觉仍具挑战，需有效检测方法。

Method: 提出光谱分析框架，将Transformer层建模为注意力诱导的动态图，通过图信号处理定义诊断指标。

Result: 实验发现事实陈述和不同类型幻觉有不同光谱模式，简单检测器准确率达88.75%。

Conclusion: 光谱几何可捕捉推理模式和错误行为，为大语言模型幻觉检测提供框架。

Abstract: Large language models achieve impressive results but distinguishing factual
reasoning from hallucinations remains challenging. We propose a spectral
analysis framework that models transformer layers as dynamic graphs induced by
attention, with token embeddings as signals on these graphs. Through graph
signal processing, we define diagnostics including Dirichlet energy, spectral
entropy, and high-frequency energy ratios, with theoretical connections to
computational stability. Experiments across GPT architectures suggest universal
spectral patterns: factual statements exhibit consistent "energy mountain"
behavior with low-frequency convergence, while different hallucination types
show distinct signatures. Logical contradictions destabilize spectra with large
effect sizes ($g>1.0$), semantic errors remain stable but show connectivity
drift, and substitution hallucinations display intermediate perturbations. A
simple detector using spectral signatures achieves 88.75% accuracy versus 75%
for perplexity-based baselines, demonstrating practical utility. These findings
indicate that spectral geometry may capture reasoning patterns and error
behaviors, potentially offering a framework for hallucination detection in
large language models.

</details>


### [274] [Training-Free Spectral Fingerprints of Voice Processing in Transformers](https://arxiv.org/abs/2510.19131)
*Valentin Noël*

Main category: cs.CL

TL;DR: 通过光谱分析发现不同Transformer架构有计算指纹，研究20种语言和3个模型家族在语态转换下的代数连通性变化，揭示架构特征，表明训练重点会留下计算印记，框架可用于揭示架构偏差和模型可靠性分析。


<details>
  <summary>Details</summary>
Motivation: 探究不同Transformer架构在语言计算中的特征，检测模型的计算指纹。

Method: 使用图信号处理对注意力诱导的标记图进行分析，跟踪20种语言和3个模型家族在语态转换下代数连通性的变化。

Result: 发现Phi - 3 - Mini在英语早期层有显著破坏，Qwen2.5 - 7B对形态丰富语言有小的分布式变化，LLaMA - 3.2 - 1B反应较温和，光谱特征与行为差异强相关，受注意力头消融调节。

Conclusion: 训练重点会留下可检测的计算印记，该框架可用于揭示架构偏差和支持模型可靠性分析。

Abstract: Different transformer architectures implement identical linguistic
computations via distinct connectivity patterns, yielding model imprinted
``computational fingerprints'' detectable through spectral analysis. Using
graph signal processing on attention induced token graphs, we track changes in
algebraic connectivity (Fiedler value, $\Delta\lambda_2$) under voice
alternation across 20 languages and three model families, with a prespecified
early window (layers 2--5). Our analysis uncovers clear architectural
signatures: Phi-3-Mini shows a dramatic English specific early layer disruption
($\overline{\Delta\lambda_2}_{[2,5]}\!\approx\!-0.446$) while effects in 19
other languages are minimal, consistent with public documentation that
positions the model primarily for English use. Qwen2.5-7B displays small,
distributed shifts that are largest for morphologically rich languages, and
LLaMA-3.2-1B exhibits systematic but muted responses. These spectral signatures
correlate strongly with behavioral differences (Phi-3: $r=-0.976$) and are
modulated by targeted attention head ablations, linking the effect to early
attention structure and confirming functional relevance. Taken together, the
findings are consistent with the view that training emphasis can leave
detectable computational imprints: specialized processing strategies that
manifest as measurable connectivity patterns during syntactic transformations.
Beyond voice alternation, the framework differentiates reasoning modes,
indicating utility as a simple, training free diagnostic for revealing
architectural biases and supporting model reliability analysis.

</details>


### [275] [Contextual Augmentation for Entity Linking using Large Language Models](https://arxiv.org/abs/2510.18888)
*Daniel Vollmers,Hamada M. Zahera,Diego Moussallem,Axel-Cyrille Ngonga Ngomo*

Main category: cs.CL

TL;DR: 提出联合实体识别和消歧的微调模型，利用大语言模型丰富上下文，在基准数据集评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统实体链接方法分两步进行，计算量大且效果不佳。

Method: 提出在统一框架中联合实体识别和消歧的微调模型，利用大语言模型丰富实体提及上下文。

Result: 在基准数据集上评估并与基线对比，在域外数据集上达到了最先进的性能。

Conclusion: 所提出的方法在实体链接任务中表现良好，优于传统方法。

Abstract: Entity Linking involves detecting and linking entity mentions in natural
language texts to a knowledge graph. Traditional methods use a two-step process
with separate models for entity recognition and disambiguation, which can be
computationally intensive and less effective. We propose a fine-tuned model
that jointly integrates entity recognition and disambiguation in a unified
framework. Furthermore, our approach leverages large language models to enrich
the context of entity mentions, yielding better performance in entity
disambiguation. We evaluated our approach on benchmark datasets and compared
with several baselines. The evaluation results show that our approach achieves
state-of-the-art performance on out-of-domain datasets.

</details>


### [276] [Small Language Models Offer Significant Potential for Science Community](https://arxiv.org/abs/2510.18890)
*Jian Zhang*

Main category: cs.CL

TL;DR: 本文提出用小型语言模型（MiniLMs）从大量地球科学文献中进行精确、快速且经济高效的信息检索，构建语料库并展示其优势和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在信息偏差和计算成本问题，希望用MiniLMs实现精确、快速且低成本的地球科学文献信息检索。

Method: 构建约7700万高质量句子的语料库，用MiniLMs通过语义搜索和句子级索引提取信息，还进行情感分析和无监督聚类。

Result: MiniLMs能高效提取特定领域信息，擅长识别大量专家验证的信息，可用于跟踪地球科学领域的发展。

Conclusion: MiniLMs在地球科学领域有事实和图像检索、趋势分析等多种应用潜力。

Abstract: Recent advancements in natural language processing, particularly with large
language models (LLMs), are transforming how scientists engage with the
literature. While the adoption of LLMs is increasing, concerns remain regarding
potential information biases and computational costs. Rather than LLMs, I
developed a framework to evaluate the feasibility of precise, rapid, and
cost-effective information retrieval from extensive geoscience literature using
freely available small language models (MiniLMs). A curated corpus of
approximately 77 million high-quality sentences, extracted from 95 leading
peer-reviewed geoscience journals such as Geophysical Research Letters and
Earth and Planetary Science Letters published during years 2000 to 2024, was
constructed. MiniLMs enable a computationally efficient approach for extracting
relevant domain-specific information from these corpora through semantic search
techniques and sentence-level indexing. This approach, unlike LLMs such as
ChatGPT-4 that often produces generalized responses, excels at identifying
substantial amounts of expert-verified information with established,
multi-disciplinary sources, especially for information with quantitative
findings. Furthermore, by analyzing emotional tone via sentiment analysis and
topical clusters through unsupervised clustering within sentences, MiniLM
provides a powerful tool for tracking the evolution of conclusions, research
priorities, advancements, and emerging questions within geoscience communities.
Overall, MiniLM holds significant potential within the geoscience community for
applications such as fact and image retrievals, trend analyses, contradiction
analyses, and educational purposes.

</details>


### [277] [Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets](https://arxiv.org/abs/2510.18908)
*Wangjiaxuan Xin,Shuhua Yin,Shi Chen,Yaorong Ge*

Main category: cs.CL

TL;DR: 提出TM - Rephrase框架用大语言模型重写推文以改进社交媒体主题建模，用COVID - 19推文数据集研究两种重写策略效果，结果显示提升性能，口语转正式策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 社交媒体短文本的特点阻碍传统主题建模效果，需解决该问题。

Method: 开发TM - Rephrase框架，使用25027条COVID - 19相关推文数据集，研究两种重写策略对多种主题建模方法的影响。

Result: TM - Rephrase提升了主题建模性能的三个指标，减少多数算法的主题冗余，口语转正式策略性能提升最大，对LDA算法尤佳。

Conclusion: 该研究提供了与模型无关的方法来增强社交媒体分析中的主题建模，对理解公共话语有广泛意义。

Abstract: Social media platforms such as Twitter (now X) provide rich data for
analyzing public discourse, especially during crises such as the COVID-19
pandemic. However, the brevity, informality, and noise of social media short
texts often hinder the effectiveness of traditional topic modeling, producing
incoherent or redundant topics that are often difficult to interpret. To
address these challenges, we have developed \emph{TM-Rephrase}, a
model-agnostic framework that leverages large language models (LLMs) to
rephrase raw tweets into more standardized and formal language prior to topic
modeling. Using a dataset of 25,027 COVID-19-related Twitter posts, we
investigate the effects of two rephrasing strategies, general- and
colloquial-to-formal-rephrasing, on multiple topic modeling methods. Results
demonstrate that \emph{TM-Rephrase} improves three metrics measuring topic
modeling performance (i.e., topic coherence, topic uniqueness, and topic
diversity) while reducing topic redundancy of most topic modeling algorithms,
with the colloquial-to-formal strategy yielding the greatest performance gains
and especially for the Latent Dirichlet Allocation (LDA) algorithm. This study
contributes to a model-agnostic approach to enhancing topic modeling in public
health related social media analysis, with broad implications for improved
understanding of public discourse in health crisis as well as other important
domains.

</details>


### [278] [Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection](https://arxiv.org/abs/2510.18909)
*Hongyi He,Xiao Liu,Zhenghao Lin,Mingni Tang,Yi Cheng,Jintao Wang,Wenjie Li,Peng Cheng,Yeyun Gong*

Main category: cs.CL

TL;DR: 现有基于分数的数据选择方法存在问题，提出ODiS算法确保数据质量和多样性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于分数的数据选择方法存在非单调性和忽略多样性的问题，需要改进。

Method: 提出ODiS算法，从多维度评估数据，用PCA对分数去相关得到正交评估维度，训练基于Roberta的评分器，在各正交维度选高分数据。

Result: ODiS选择的数据维度间重叠小于2%，基于其训练的模型在下游基准测试中显著优于其他基线。

Conclusion: 正交、考虑多样性的数据选择对大语言模型是必要的。

Abstract: High-quality pre-training data is crutial for large language models, where
quality captures factual reliability and semantic value, and diversity ensures
broad coverage and distributional heterogeneity. Existing approaches typically
rely on single or multiple-dimensional score-based selection. However, directly
selecting top-scored data often degrades performance, and sampling from a
broader range is required to recover results. The above non-monotonicity
between dataset scores and downstream benchmark results reveals a fundamental
bias: score-based methods collapse correlated dimensions, causing top-scored
data to appear high-quality while systematically overlooking diversity. We
argue that ensuring diversity requires decomposing correlated metrics into
orthogonal feature dimensions, from which the top-scored data can be directly
selected. Therefore, we proposed the Orthogonal Diversity-Aware Selection
(ODiS) algorithm, which preserves both quality and diversity during data
selection. First, ODiS evaluates data from multiple dimensions, covering
language quality, knowledge quality, and comprehension difficulty. The
multi-dimensional scores are then decorrelated via Principal Component Analysis
(PCA), yielding orthogonal evaluation dimensions. For each dimension, a
Roberta-based scorer is trained to regress the data onto PCA-projected scores,
enabling scalable inference on large corpora. Finally, ODiS constructs the
training dataset by selecting top-scored data within each orthogonal dimension,
thereby ensuring both quality and diversity. Empirical results show that
ODiS-selected data exhibit less than 2\% inter-dimension overlap, confirming
orthogonality between dimensions. More importantly, models trained with
ODiS-selected data significantly outperform other baselines on downstream
benchmarks, highlighting the necessity of orthogonal, diversity-aware data
selection for LLMs.

</details>


### [279] [Context-aware Fairness Evaluation and Mitigation in LLMs](https://arxiv.org/abs/2510.18914)
*Afrozah Nadeem,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 现有大语言模型有不良行为，训练或数据方法有局限，现有剪枝方法静态，提出动态可逆剪枝框架实现动态公平控制。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型存在的不良行为，如破坏公平性、内容有害等，且现有训练或数据方法有计算成本高、难适应新场景等问题，现有剪枝方法静态。

Method: 提出动态、可逆、基于剪枝的框架，检测上下文感知的神经元激活并应用自适应掩码。

Result: 在推理时提供细粒度、内存感知的缓解方法，跨多语言单轮和多轮对话有更连贯行为。

Conclusion: 所提框架能在现实对话AI中实现动态公平控制。

Abstract: Large language models often display undesirable behaviors embedded in their
internal representations, undermining fairness, inconsistency drift,
amplification of harmful content, and the propagation of unwanted patterns
during extended dialogue and conversations. Although training-time or
data-centric methods attempt to reduce these effects, they are computationally
expensive, irreversible once deployed, and slow to adapt to new conversational
contexts. Pruning-based methods provide a flexible and transparent way to
reduce bias by adjusting the neurons responsible for certain behaviors.
However, most existing approaches are static; once a neuron is removed, the
model loses the ability to adapt when the conversation or context changes. To
address this, we propose a dynamic, reversible, pruning-based framework that
detects context-aware neuron activations and applies adaptive masking to
modulate their influence during generation. Our inference-time solution
provides fine-grained, memory-aware mitigation with knowledge-preserved, more
coherent behavior across multilingual single- and multi-turn dialogues,
enabling dynamic fairness control in real-world conversational AI.

</details>


### [280] [MMAO-Bench: MultiModal All in One Benchmark Reveals Compositional Law between Uni-modal and Omni-modal in OmniModels](https://arxiv.org/abs/2510.18915)
*Chen Chen,ZeYang Hu,Fengjiao Chen,Liya Ma,Jiaxing Liu,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: 提出新的多模态基准MMAO - Bench评估单模态和全模态理解能力，实验揭示性能规律。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型向全模态发展，但单模态和全模态关联不明，需综合评估推动其智能进化。

Method: 提出MMAO - Bench基准，包含1880个人工样本、44种任务类型和创新多步开放式问题类型。

Result: 揭示跨模态和单模态性能的组合规律，全模态能力对弱模型有瓶颈效应，对强模型有协同促进作用。

Conclusion: MMAO - Bench可有效评估单模态和全模态理解能力，有助于推动全模态模型智能发展。

Abstract: Multimodal Large Languages models have been progressing from uni-modal
understanding toward unifying visual, audio and language modalities,
collectively termed omni models. However, the correlation between uni-modal and
omni-modal remains unclear, which requires comprehensive evaluation to drive
omni model's intelligence evolution. In this work, we propose a novel, high
quality and diversity omni model benchmark, MultiModal All in One Benchmark
(MMAO-Bench), which effectively assesses both uni-modal and omni-modal
understanding capabilities. The benchmark consists of 1880 human curated
samples, across 44 task types, and a innovative multi-step open-ended question
type that better assess complex reasoning tasks. Experimental result shows the
compositional law between cross-modal and uni-modal performance and the
omni-modal capability manifests as a bottleneck effect on weak models, while
exhibiting synergistic promotion on strong models.

</details>


### [281] [Misinformation Detection using Large Language Models with Explainability](https://arxiv.org/abs/2510.18918)
*Jainee Patel,Chintan Bhatt,Himani Trivedi,Thanh Thi Nguyen*

Main category: cs.CL

TL;DR: 本文提出基于预训练语言模型的可解释且高效的错误信息检测流程，经实验表明轻量级模型可减少计算成本并保证性能。


<details>
  <summary>Details</summary>
Motivation: 网络平台错误信息快速传播破坏信任、阻碍决策，需有效检测方法。

Method: 用两步策略优化RoBERTa和DistilBERT，在两个数据集上测试，集成LIME和SHAP确保可解释性。

Result: DistilBERT与RoBERTa准确率相当，但计算资源需求显著降低。

Conclusion: 预训练语言模型结合微调与可解释性，是可扩展、可靠的错误信息检测框架。

Abstract: The rapid spread of misinformation on online platforms undermines trust among
individuals and hinders informed decision making. This paper shows an
explainable and computationally efficient pipeline to detect misinformation
using transformer-based pretrained language models (PLMs). We optimize both
RoBERTa and DistilBERT using a two-step strategy: first, we freeze the backbone
and train only the classification head; then, we progressively unfreeze the
backbone layers while applying layer-wise learning rate decay. On two
real-world benchmark datasets, COVID Fake News and FakeNewsNet GossipCop, we
test the proposed approach with a unified protocol of preprocessing and
stratified splits. To ensure transparency, we integrate the Local Interpretable
Model-Agnostic Explanations (LIME) at the token level to present token-level
rationales and SHapley Additive exPlanations (SHAP) at the global feature
attribution level. It demonstrates that DistilBERT achieves accuracy comparable
to RoBERTa while requiring significantly less computational resources. This
work makes two key contributions: (1) it quantitatively shows that a
lightweight PLM can maintain task performance while substantially reducing
computational cost, and (2) it presents an explainable pipeline that retrieves
faithful local and global justifications without compromising performance. The
results suggest that PLMs combined with principled fine-tuning and
interpretability can be an effective framework for scalable, trustworthy
misinformation detection.

</details>


### [282] [ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge](https://arxiv.org/abs/2510.18941)
*Zhilin Wang,Jaehun Jung,Ximing Lu,Shizhe Diao,Ellie Evans,Jiaqi Zeng,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.CL

TL;DR: 提出ProfBench评估集及LLM - Judges评估方法，发现ProfBench对SOTA的LLMs有挑战，揭示不同模型性能差异及扩展思维作用。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估受限于验证响应，而现实应用需评估其处理专业文档等能力，故要构建新评估体系。

Method: 引入由专业人士评估的7000多个响应 - 标准对组成的ProfBench，构建LLM - Judges评估ProfBench规则，减轻自增强偏差并降低评估成本。

Result: ProfBench对SOTA的LLMs构成重大挑战，如GPT - 5 - high整体性能仅65.9%，专有和开源模型存在性能差异。

Conclusion: 新评估体系可评估大语言模型在专业领域表现，为处理复杂专业任务提供思路。

Abstract: Evaluating progress in large language models (LLMs) is often constrained by
the challenge of verifying responses, limiting assessments to tasks like
mathematics, programming, and short-form question-answering. However, many
real-world applications require evaluating LLMs in processing professional
documents, synthesizing information, and generating comprehensive reports in
response to user queries. We introduce ProfBench: a set of over 7000
response-criterion pairs as evaluated by human-experts with professional
knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We
build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by
mitigating self-enhancement bias and reducing the cost of evaluation by 2-3
orders of magnitude, to make it fair and accessible to the broader community.
Our findings reveal that ProfBench poses significant challenges even for
state-of-the-art LLMs, with top-performing models like GPT-5-high achieving
only 65.9\% overall performance. Furthermore, we identify notable performance
disparities between proprietary and open-weight models and provide insights
into the role that extended thinking plays in addressing complex,
professional-domain tasks. Data:
https://huggingface.co/datasets/nvidia/ProfBench and Code:
https://github.com/NVlabs/ProfBench

</details>


### [283] [That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation](https://arxiv.org/abs/2510.19116)
*Jaesung Bae,Cameron Churchwell,Mitchell Hermon,Tsun-An Hsieh,Jocelyn Xu,Yekaterina Yegorova,Mark Hasegawa-Johnson,Heng Ji*

Main category: cs.CL

TL;DR: 研究大语言模型面对参数知识与提示冲突时的表现，拓展到代码生成领域，提出框架、评估方法和数据集，实验表明大模型能检测冲突，激活层引导有效果但受多因素影响。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在参数知识与提示信息冲突时的表现，并将知识冲突研究拓展到代码生成领域。

Method: 提出构建和解释冲突的领域无关框架，以及适用于代码冲突场景的评估方法和数据集。

Result: 足够大的大语言模型能以80.65%的准确率检测知识冲突，激活层引导比随机基线的引导成功率提高12.6%。

Conclusion: 激活层引导的有效性关键取决于平衡模型大小、任务领域和引导方向。

Abstract: This paper investigates how large language models (LLMs) behave when faced
with discrepancies between their parametric knowledge and conflicting
information contained in a prompt. Building on prior question-answering (QA)
research, we extend the investigation of knowledge conflicts to the realm of
code generation. We propose a domain-agnostic framework for constructing and
interpreting such conflicts, along with a novel evaluation method and dataset
tailored to code conflict scenarios. Our experiments indicate that sufficiently
large LLMs encode the notion of a knowledge conflict in their parameters,
enabling us to detect knowledge conflicts with up to \textbf{80.65\%} accuracy.
Building on these insights, we show that activation-level steering can achieve
up to a \textbf{12.6\%} improvement in steering success over a random baseline.
However, effectiveness depends critically on balancing model size, task domain,
and steering direction. The experiment code and data will be made publicly
available after acceptance.

</details>


### [284] [When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA](https://arxiv.org/abs/2510.19172)
*Nishanth Sridhar Nakshatri,Shamik Roy,Manoj Ghuhan Arivazhagan,Hanhan Zhou,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.CL

TL;DR: 引入基准evolveQA评估大语言模型处理时间演变知识的能力，发现模型在该基准上性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估大语言模型处理时间知识冲突时存在局限，缺乏动态结构且关注流行实体，不能公平评估不同知识截止日期的模型。

Method: 从3个真实世界带时间戳的语料库构建evolveQA基准，识别自然发生的知识演变，生成针对不同知识截止日期的问题及答案。

Result: 对12个开源和闭源大语言模型进行评估，发现相比静态知识问题，在evolveQA上性能最多下降31%。

Conclusion: evolveQA可用于评估大语言模型处理时间演变知识的能力，模型在处理此类知识时存在性能问题。

Abstract: LLMs often fail to handle temporal knowledge conflicts--contradictions
arising when facts evolve over time within their training data. Existing
studies evaluate this phenomenon through benchmarks built on structured
knowledge bases like Wikidata, but they focus on widely-covered,
easily-memorized popular entities and lack the dynamic structure needed to
fairly evaluate LLMs with different knowledge cut-off dates. We introduce
evolveQA, a benchmark specifically designed to evaluate LLMs on temporally
evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS
updates, Azure changes, and WHO disease outbreak reports. Our framework
identifies naturally occurring knowledge evolution and generates questions with
gold answers tailored to different LLM knowledge cut-off dates. Through
extensive evaluation of 12 open and closed-source LLMs across 3 knowledge
probing formats, we demonstrate significant performance drops of up to 31% on
evolveQA compared to static knowledge questions.

</details>


### [285] [Interpretable Question Answering with Knowledge Graphs](https://arxiv.org/abs/2510.19181)
*Kartikeya Aneja,Manasvi Srivastava,Subhayan Das,Nagender Aneja*

Main category: cs.CL

TL;DR: 提出不依赖大语言模型检索增强生成的问答系统，用小改写模型处理知识图谱检索结果，经两阶段处理并评估，有一定准确率。


<details>
  <summary>Details</summary>
Motivation: 构建不依赖大语言模型检索增强生成（RAG）的问答系统。

Method: 将流程分为两阶段，先预处理文档生成问答对，再转化为知识图谱进行基于图的检索、查询、重排序和改写。

Result: 在CRAG基准上用LLAMA - 3.2和GPT - 3.5 - Turbo评估，准确率分别为71.9%和54.4%。

Conclusion: 未明确提及，但准确率结果表明该不依赖RAG的问答系统有一定可行性。

Abstract: This paper presents a question answering system that operates exclusively on
a knowledge graph retrieval without relying on retrieval augmented generation
(RAG) with large language models (LLMs). Instead, a small paraphraser model is
used to paraphrase the entity relationship edges retrieved from querying the
knowledge graph. The proposed pipeline is divided into two main stages. The
first stage involves pre-processing a document to generate sets of
question-answer (QA) pairs. The second stage converts these QAs into a
knowledge graph from which graph-based retrieval is performed using embeddings
and fuzzy techniques. The graph is queried, re-ranked, and paraphrased to
generate a final answer. This work includes an evaluation using LLM-as-a-judge
on the CRAG benchmark, which resulted in accuracies of 71.9% and 54.4% using
LLAMA-3.2 and GPT-3.5-Turbo, respectively.

</details>


### [286] [Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization](https://arxiv.org/abs/2510.19325)
*Junjie Song,Yiwen Liu,Dapeng Li,Yin Sun,Shukun Fu,Siqi Chen,Yuji Cao*

Main category: cs.CL

TL;DR: 本文引入超体积优化（HVO）策略解决文本摘要多目标优化问题，实验显示该方法优于GRPO，7B基础模型经HVO增强后在摘要任务上可媲美GPT - 4且生成长度更短。


<details>
  <summary>Details</summary>
Motivation: 文本摘要需同时优化多个目标存在挑战，基于大语言模型（LLMs）通过强化学习（RL）优化摘要多目标问题的研究较少。

Method: 引入超体积优化（HVO）策略，在强化学习的奖励过程中利用超体积方法动态调整组间分数，引导模型优化逐步逼近帕累托前沿。

Result: 在多个代表性摘要数据集上，该方法整体得分优于GRPO，不同维度表现更平衡；7B基础模型经HVO增强后在摘要任务上与GPT - 4表现相当且生成长度更短。

Conclusion: 超体积优化（HVO）策略能有效解决文本摘要多目标优化问题，有较好的性能表现。

Abstract: Text summarization is a crucial task that requires the simultaneous
optimization of multiple objectives, including consistency, coherence,
relevance, and fluency, which presents considerable challenges. Although large
language models (LLMs) have demonstrated remarkable performance, enhanced by
reinforcement learning (RL), few studies have focused on optimizing the
multi-objective problem of summarization through RL based on LLMs. In this
paper, we introduce hypervolume optimization (HVO), a novel optimization
strategy that dynamically adjusts the scores between groups during the reward
process in RL by using the hypervolume method. This method guides the model's
optimization to progressively approximate the pareto front, thereby generating
balanced summaries across multiple objectives. Experimental results on several
representative summarization datasets demonstrate that our method outperforms
group relative policy optimization (GRPO) in overall scores and shows more
balanced performance across different dimensions. Moreover, a 7B foundation
model enhanced by HVO performs comparably to GPT-4 in the summarization task,
while maintaining a shorter generation length. Our code is publicly available
at https://github.com/ai4business-LiAuto/HVO.git

</details>


### [287] [When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs](https://arxiv.org/abs/2510.18892)
*Richard J. Young,Brandon Gillins,Alice M. Matthews*

Main category: cs.CL

TL;DR: 本文提出精简评估框架，用20个精心设计的提示评估大语言模型指令遵循能力，通过大规模实证研究给出模型比较分析，发现失败模式和挑战指令类型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型指令遵循能力系统评估有挑战，新模型可能在现有基准上训练，需新评估方法。

Method: 设计20个提示，对331个模型筛选出256个进行大规模实证研究，验证模型基本功能，提示针对指令遵循不同方面。

Result: 发现一致的失败模式，确定特定类型指令带来的挑战。

Conclusion: 贡献实用评估工具和对当代大语言模型指令遵循能力较全面的实证分析。

Abstract: Despite widespread deployment of Large Language Models, systematic evaluation
of instruction-following capabilities remains challenging. While comprehensive
benchmarks exist, focused assessments that quickly diagnose specific
instruction adherence patterns are valuable. As newer models may be trained on
existing benchmarks, novel evaluation approaches are needed to assess genuine
capabilities rather than memorized performance. This paper presents a
streamlined evaluation framework using twenty carefully designed prompts to
assess LLM instruction-following across diverse task categories. We demonstrate
this framework through a large-scale empirical study conducted on October 14,
2025, testing 256 verified working models from 331 available via OpenRouter. To
ensure methodological rigor and prevent selection bias, we first verified each
model's basic functionality before inclusion. Unlike large-scale benchmarks
requiring extensive computational resources, our approach offers a practical
diagnostic tool researchers and practitioners can readily apply. Our
methodology builds upon verifiable instructions while introducing a compact
test suite balancing comprehensiveness with efficiency. Each prompt targets
distinct aspects of instruction following, including format compliance, content
constraints, logical sequencing, and multi-step task execution. We evaluate
models from major providers (OpenAI, Anthropic, Google, Meta, Mistral) and
emerging implementations (Qwen, DeepSeek, community models), providing
comparative performance analysis. Our findings reveal consistent failure modes
and identify specific instruction types posing particular challenges. This work
contributes both a practical evaluation tool and one of the most comprehensive
empirical analyses of instruction-following capabilities across the
contemporary LLM landscape.

</details>


### [288] [M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2510.19358)
*Yejin Kwon,Taewoo Kang,Hyunsoo Yoon,Changouk Kim*

Main category: cs.CL

TL;DR: 提出M3 - SLU基准评估多说话者、多轮口语理解，发现模型在说话者归因推理上有差距。


<details>
  <summary>Details</summary>
Motivation: 现有模型在说话者归因推理方面存在困难，需要新的基准进行评估。

Method: 从四个开放语料库构建M3 - SLU基准，包含超12000个实例，有两个任务，用LLM - as - Judge和准确率指标评估级联管道和端到端MLLMs。

Result: 模型能捕捉话语内容，但常无法识别说话者。

Conclusion: M3 - SLU可作为具有挑战性的基准推动说话者感知多模态理解研究。

Abstract: We present M3-SLU, a new multimodal large language model (MLLM) benchmark for
evaluating multi-speaker, multi-turn spoken language understanding. While
recent models show strong performance in speech and text comprehension, they
still struggle with speaker-attributed reasoning, the ability to understand who
said what and when in natural conversations. M3-SLU is built from four open
corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000
validated instances with paired audio, transcripts, and metadata. It includes
two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker
Attribution via Utterance Matching. We provide baseline results for both
cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and
accuracy metrics. Results show that while models can capture what was said,
they often fail to identify who said it, revealing a key gap in speaker-aware
dialogue understanding. M3-SLU offers as a challenging benchmark to advance
research in speaker-aware multimodal understanding.

</details>


### [289] [AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation](https://arxiv.org/abs/2510.19361)
*Xianyang Liu,Yilin Liu,Shuai Wang,Hao Cheng,Andrew Estornell,Yuzhi Zhao,Jiaheng Wei*

Main category: cs.CL

TL;DR: 提出 AgenticMath 管道生成高质量数学问答对，实验表明用其生成的少量数据微调大模型，在数学推理基准测试上表现优于用大量低质量数据训练的基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前创建用于提升大语言模型推理能力的高质量数据集存在挑战，现有方法生成的答案质量低、信息丰富度有限。

Method: AgenticMath 管道包含种子问题过滤、智能问题改写、答案增强和问答评估四个阶段。

Result: 用 AgenticMath 生成的 30 - 60K 数学样本微调 3B - 8B 参数的大模型，在数学推理基准测试上表现优于用 400K 或 2.3M 样本训练的基线模型。

Conclusion: 有针对性地生成高质量数据是提升大语言模型数学推理能力的更有效途径，优于大规模低质量数据。

Abstract: The creation of high-quality datasets to improve Large Language Model (LLM)
reasoning remains a significant challenge, as current methods often suffer from
generating low-quality/incorrect answers and limited information richness from
available data sources. To address this, we propose AgenticMath, a novel
agentic pipeline for generating high-quality mathematical question-answer pairs
to enhance the supervised fine-tuning of LLMs. Our method operates through four
stages: (1) Seed Question Filter that selects questions with high information
richness, complexity, and clarity; (2) an Agentic Question Rephrase step that
employs a multi-agent system to generate diverse, logically consistent
paraphrases; (3) an Answer Augment step where rewrite answers using
chain-of-thought reasoning to enhance numerical and logical correctness,
without reliance on human-provided labels; and (4) a final Question and Answer
Evaluation that retains only the most superior pairs. Extensive experiments
demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated
datasets (comprising only 30-60K math samples) achieves competitive or superior
performance on diverse in domain and out-of-domain mathematical reasoning
benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M
samples). Our work demonstrates that targeted, high-quality data generation is
a more efficient path to improving mathematical reasoning in LLMs than
large-scale, low-quality alternatives.

</details>


### [290] [Evaluating LLM Story Generation through Large-scale Network Analysis of Social Structures](https://arxiv.org/abs/2510.18932)
*Hiroshi Nonaka,K. E. Perry*

Main category: cs.CL

TL;DR: 提出用分析故事中社会结构的方法评估大语言模型故事生成能力，分析超1200个故事发现模型生成故事有积极关系偏倚。


<details>
  <summary>Details</summary>
Motivation: 解决评估大语言模型在复杂任务中创造力需人工评估且难扩展的问题。

Method: 将故事中的社会结构分析为有符号角色网络，对四个领先大语言模型生成的故事和人类编写语料库进行大规模比较分析。

Result: 大语言模型生成的故事始终表现出对紧密、积极关系的强烈偏倚，与先前人工评估研究结果一致。

Conclusion: 该方法为评估当前和未来大语言模型创造性讲故事的局限性和倾向提供了有价值的工具。

Abstract: Evaluating the creative capabilities of large language models (LLMs) in
complex tasks often requires human assessments that are difficult to scale. We
introduce a novel, scalable methodology for evaluating LLM story generation by
analyzing underlying social structures in narratives as signed character
networks. To demonstrate its effectiveness, we conduct a large-scale
comparative analysis using networks from over 1,200 stories, generated by four
leading LLMs (GPT-4o, GPT-4o mini, Gemini 1.5 Pro, and Gemini 1.5 Flash) and a
human-written corpus. Our findings, based on network properties like density,
clustering, and signed edge weights, show that LLM-generated stories
consistently exhibit a strong bias toward tightly-knit, positive relationships,
which aligns with findings from prior research using human assessment. Our
proposed approach provides a valuable tool for evaluating limitations and
tendencies in the creative storytelling of current and future LLMs.

</details>


### [291] [ToMMeR -- Efficient Entity Mention Detection from Large Language Models](https://arxiv.org/abs/2510.19410)
*Victor Morand,Nadi Tomeh,Josiane Mothe,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 提出轻量级模型ToMMeR探测早期大语言模型层提及检测能力，在多基准测试表现良好，证明早期层存在结构化实体表示。


<details>
  <summary>Details</summary>
Motivation: 提及检测是信息提取基础和性能瓶颈，需有效方法进行检测。

Method: 引入轻量级模型ToMMeR，通过跨13个NER基准测试，用大语言模型作评判，进行跨模型分析，扩展跨度分类头。

Result: ToMMeR零样本召回率达93%，用大语言模型评判时精度超90%，不同架构模型在提及边界上收敛（DICE>75%），扩展后接近SOTA的NER性能。

Conclusion: 早期transformer层存在结构化实体表示，可用最少参数有效恢复。

Abstract: Identifying which text spans refer to entities -- mention detection -- is
both foundational for information extraction and a known performance
bottleneck. We introduce ToMMeR, a lightweight model (<300K parameters) probing
mention detection capabilities from early LLM layers. Across 13 NER benchmarks,
ToMMeR achieves 93\% recall zero-shot, with over 90\% precision using an LLM as
a judge showing that ToMMeR rarely produces spurious predictions despite high
recall. Cross-model analysis reveals that diverse architectures (14M-15B
parameters) converge on similar mention boundaries (DICE >75\%), confirming
that mention detection emerges naturally from language modeling. When extended
with span classification heads, ToMMeR achieves near SOTA NER performance
(80-87\% F1 on standard benchmarks). Our work provides evidence that structured
entity representations exist in early transformer layers and can be efficiently
recovered with minimal parameters.

</details>


### [292] [VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos](https://arxiv.org/abs/2510.19488)
*Dunjie Lu,Yiheng Xu,Junli Wang,Haoyuan Wu,Xinyuan Wang,Zekun Wang,Junlin Yang,Hongjin Su,Jixuan Chen,Junda Chen,Yuchen Mao,Jingren Zhou,Junyang Lin,Binyuan Hui,Tao Yu*

Main category: cs.CL

TL;DR: 提出VideoAgentTrek自动从公开屏幕录制视频挖掘训练数据，解决原始视频缺动作标签问题，经实验证明能提升计算机使用代理性能。


<details>
  <summary>Details</summary>
Motivation: 训练计算机使用代理需大量GUI交互数据，手动标注成本过高。

Method: 提出VideoAgentTrek管道，开发含视频定位模型和动作内容识别器的Video2Action逆动力学模块，利用数据进行持续预训练和监督微调。

Result: 处理39000个YouTube教程视频，自动生成152万个交互步骤；在OSWorld - Verified上任务成功率从9.3%提升到15.8%，在AgentNetBench上步骤准确率从64.1%提升到69.3%。

Conclusion: 被动互联网视频可转化为计算机使用代理的高质量监督数据，是昂贵手动标注的可扩展替代方案。

Abstract: Training computer-use agents requires massive amounts of GUI interaction
data, but manually annotating action trajectories at scale is prohibitively
expensive. We present VideoAgentTrek, a scalable pipeline that automatically
mines training data from publicly available screen-recorded videos at web
scale, eliminating the need for manual annotation. Our approach addresses a key
challenge: raw videos contain implicit demonstrations but lack explicit action
labels. To solve this, we develop Video2Action, an inverse dynamics module
(IDM) with two components: (1) a video grounding model that detects and
localizes GUI actions with precise temporal boundaries and context, and (2) an
action-content recognizer that extracts structured parameters like click
coordinates and typed text with high fidelity. Applied to 39,000 YouTube
tutorial videos, our pipeline generates 1.52 million interaction steps
automatically. We leverage this data through continued pretraining followed by
supervised fine-tuning. On OSWorld-Verified, our approach improves task success
rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On
AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results
demonstrate that passive internet videos can be transformed into high-quality
supervision for computer-use agents, providing a scalable alternative to
expensive manual annotation.

</details>


### [293] [Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark](https://arxiv.org/abs/2510.19585)
*Yu Wu,Ke Shu,Jonas Fischer,Lidia Pivovarova,David Rosson,Eetu Mäkelä,Mikko Tolonen*

Main category: cs.CL

TL;DR: 本文提出从混合语言历史文档中提取拉丁语片段的新任务，用724页标注页面的多模态数据集评估大基础模型，结果表明当代模型可实现可靠拉丁语检测并进行了全面分析。


<details>
  <summary>Details</summary>
Motivation: 开展从混合语言历史文档中提取拉丁语片段的新任务研究。

Method: 使用724页标注页面的多模态数据集对大基础模型进行基准测试和评估。

Result: 当代模型可实现可靠的拉丁语检测。

Conclusion: 对模型在该任务上的能力和局限性进行了首次全面分析。

Abstract: This paper presents a novel task of extracting Latin fragments from
mixed-language historical documents with varied layouts. We benchmark and
evaluate the performance of large foundation models against a multimodal
dataset of 724 annotated pages. The results demonstrate that reliable Latin
detection with contemporary models is achievable. Our study provides the first
comprehensive analysis of these models' capabilities and limits for this task.

</details>


### [294] [Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent](https://arxiv.org/abs/2510.19641)
*Yangshijie Zhang,Xinda Wang,Jialin Liu,Wenqiang Wang,Zhicong Ma,Xingxing Jia*

Main category: cs.CL

TL;DR: 社交媒体中风格字体在NLP模型有隐患，提出风格攻击SAD，实验证明其攻击性能强且对多模态任务有威胁


<details>
  <summary>Details</summary>
Motivation: 社交媒体中风格字体在NLP模型产生人 - 模型感知差距，存在隐藏漏洞

Method: 提出风格攻击Style Attack Disguise (SAD)，设计轻量级和强量级两种规格

Result: 在情感分类、机器翻译等实验中证明SAD攻击性能强，对多模态任务有潜在威胁

Conclusion: SAD攻击对NLP模型和多模态任务存在威胁

Abstract: With social media growth, users employ stylistic fonts and font-like emoji to
express individuality, creating visually appealing text that remains
human-readable. However, these fonts introduce hidden vulnerabilities in NLP
models: while humans easily read stylistic text, models process these
characters as distinct tokens, causing interference. We identify this
human-model perception gap and propose a style-based attack, Style Attack
Disguise (SAD). We design two sizes: light for query efficiency and strong for
superior attack performance. Experiments on sentiment classification and
machine translation across traditional models, LLMs, and commercial services
demonstrate SAD's strong attack performance. We also show SAD's potential
threats to multimodal tasks including text-to-image and text-to-speech
generation.

</details>


### [295] [Unraveling Emotions with Pre-Trained Models](https://arxiv.org/abs/2510.19668)
*Alejandro Pajón-Sanmartín,Francisco De Arriba-Pérez,Silvia García-Méndez,Fátima Leal,Benedita Malheiro,Juan Carlos Burguillo-Rial*

Main category: cs.CL

TL;DR: 本文对比微调与提示工程在三种场景下情感检测的有效性，实验表明微调预训练模型效果好，大语言模型需结构化提示工程和情感分组来提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在开放式查询情感识别存在挑战，通用模型直接应用困难，需探索有效方法。

Method: 对比微调与提示工程在三种不同场景下的情感检测效果，包括微调预训练模型与通用大语言模型使用简单提示的性能、不同情感提示设计的有效性、情感分组技术的影响。

Result: 微调预训练模型进行情感识别实验指标超70%，大语言模型需结构化提示工程和情感分组提升性能。

Conclusion: 这些改进能提升情感分析、人机交互和对各领域用户行为的理解。

Abstract: Transformer models have significantly advanced the field of emotion
recognition. However, there are still open challenges when exploring open-ended
queries for Large Language Models (LLMs). Although current models offer good
results, automatic emotion analysis in open texts presents significant
challenges, such as contextual ambiguity, linguistic variability, and
difficulty interpreting complex emotional expressions. These limitations make
the direct application of generalist models difficult. Accordingly, this work
compares the effectiveness of fine-tuning and prompt engineering in emotion
detection in three distinct scenarios: (i) performance of fine-tuned
pre-trained models and general-purpose LLMs using simple prompts; (ii)
effectiveness of different emotion prompt designs with LLMs; and (iii) impact
of emotion grouping techniques on these models. Experimental tests attain
metrics above 70% with a fine-tuned pre-trained model for emotion recognition.
Moreover, the findings highlight that LLMs require structured prompt
engineering and emotion grouping to enhance their performance. These
advancements improve sentiment analysis, human-computer interaction, and
understanding of user behavior across various domains.

</details>


### [296] [MoE-Prism: Disentangling Monolithic Experts for Elastic MoE Services via Model-System Co-Designs](https://arxiv.org/abs/2510.19366)
*Xinfeng Xia,Jiacheng Liu,Xiaofeng Hou,Peng Tang,Mingxuan Zhang,Wenfeng Wang,Chao Li*

Main category: cs.CL

TL;DR: 本文提出MoE - Prism将刚性MoE模型转变为弹性服务，分两阶段实现，评估显示其比基线有更多稳定操作点，能提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型依赖top - k机制在整体专家间路由，存在‘质量悬崖’，在成本和质量间难以权衡，无法适应不同服务水平目标，导致资源过度配置。

Method: 分为两阶段，离线重构引擎将整体专家解构为细粒度‘子专家’，在线调度引擎通过QoS感知调度利用新的弹性。

Result: 在三种不同MoE模型评估中，MoE - Prism比基线多4倍以上不同稳定操作点，能在严格延迟预算下提高吞吐量，或在资源有限时降低延迟。

Conclusion: MoE - Prism提供关键‘控制旋钮’，能弥合模型 - 系统差距，实现下一代自适应、高效和QoS感知的AI服务。

Abstract: Mixture-of-Experts (MoE) models, the state-of-the-art in large-scale AI,
achieve high quality by sparsely activating parameters. However, their reliance
on routing between a few monolithic experts via a top-k mechanism creates a
"quality cliff", offering only a few coarse-grained operating points. This
inflexibility forces a difficult trade-off between cost and quality, preventing
adaptation to diverse Service Level Objectives (SLOs) and leading to
significant resource over-provisioning.
  This paper introduces MoE-Prism, a model-system co-design that transforms
rigid MoE models into elastic services. Our methodology is divided into two
phases. First, an \emph{Offline Refactoring Engine} systematically deconstructs
monolithic experts into fine-grained "sub-experts." This engine employs a
partitioning optimization solver that uses a metaheuristic-based approach to
group neurons, preserving functional locality without requiring retraining.
Second, an \emph{Online Scheduling Engine} leverages this new elasticity
through QoS-aware scheduling. It implements specialized policies to solve
complex system problems, including maximizing throughput in cloud deployments
and managing latency-optimized offloading for memory-constrained devices. Our
evaluation across three different MoE models shows that MoE-Prismprovides over
4 times more distinct, stable operating points than the baseline. This allows
an AI service to dynamically improve throughput by up to 19.9\% under a strict
latency budget or reduce latency by up to 10.36\% under limited resources.
MoE-Prism provides the critical "control knob" to bridge the model-system gap,
enabling the next generation of adaptive, efficient, and QoS-aware AI services.

</details>


### [297] [Are Large Language Models Sensitive to the Motives Behind Communication?](https://arxiv.org/abs/2510.19687)
*Addison J. Wu,Ryan Liu,Kerem Oktar,Theodore R. Sumers,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 研究大语言模型（LLMs）是否具备动机警觉能力，发现其有一定敏感性，但在新场景需改进。


<details>
  <summary>Details</summary>
Motivation: 人类交流有动机，信息受其影响，为使LLMs在现实有效，需评估其动机警觉能力。

Method: 先通过认知科学控制实验验证LLMs对动机性证词的学习，再扩展到在线广告场景评估，还进行引导干预。

Result: LLMs能像人类一样对有偏见来源的信息打折处理，但在广告场景推理与理性模型预测不符，引导干预可提升一致性。

Conclusion: LLMs对他人动机有基本敏感性，但推广到新现实场景需进一步改进模型。

Abstract: Human communication is motivated: people speak, write, and create content
with a particular communicative intent in mind. As a result, information that
large language models (LLMs) and AI agents process is inherently framed by
humans' intentions and incentives. People are adept at navigating such nuanced
information: we routinely identify benevolent or self-serving motives in order
to decide what statements to trust. For LLMs to be effective in the real world,
they too must critically evaluate content by factoring in the motivations of
the source -- for instance, weighing the credibility of claims made in a sales
pitch. In this paper, we undertake a comprehensive study of whether LLMs have
this capacity for motivational vigilance. We first employ controlled
experiments from cognitive science to verify that LLMs' behavior is consistent
with rational models of learning from motivated testimony, and find they
successfully discount information from biased sources in a human-like manner.
We then extend our evaluation to sponsored online adverts, a more naturalistic
reflection of LLM agents' information ecosystems. In these settings, we find
that LLMs' inferences do not track the rational models' predictions nearly as
closely -- partly due to additional information that distracts them from
vigilance-relevant considerations. However, a simple steering intervention that
boosts the salience of intentions and incentives substantially increases the
correspondence between LLMs and the rational model. These results suggest that
LLMs possess a basic sensitivity to the motivations of others, but generalizing
to novel real-world settings will require further improvements to these models.

</details>


### [298] [Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings](https://arxiv.org/abs/2510.19694)
*Cesar Gonzalez-Gutierrez,Dirk Hovy*

Main category: cs.CL

TL;DR: 研究提示与内部表征质量关系，发现提示对表征质量影响与提示和目标任务相关性不一致。


<details>
  <summary>Details</summary>
Motivation: 探究让大语言模型在无特定任务监督下执行多样任务的潜在机制，了解预训练嵌入如何支持上下文任务解决。

Method: 对提示嵌入进行一系列探测实验，分析零样本分类中提示模板的各种组合。

Result: 提示影响表征质量，但这种变化与提示和目标任务的相关性并非始终一致。

Conclusion: 挑战了更相关提示必然带来更好表征的假设，并分析了导致这种意外行为的潜在因素。

Abstract: Prompting is a common approach for leveraging LMs in zero-shot settings.
However, the underlying mechanisms that enable LMs to perform diverse tasks
without task-specific supervision remain poorly understood. Studying the
relationship between prompting and the quality of internal representations can
shed light on how pre-trained embeddings may support in-context task solving.
In this empirical study, we conduct a series of probing experiments on prompt
embeddings, analyzing various combinations of prompt templates for zero-shot
classification. Our findings show that while prompting affects the quality of
representations, these changes do not consistently correlate with the relevance
of the prompts to the target task. This result challenges the assumption that
more relevant prompts necessarily lead to better representations. We further
analyze potential factors that may contribute to this unexpected behavior.

</details>


### [299] [SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration](https://arxiv.org/abs/2510.19767)
*Xichen Zhang,Sitong Wu,Haoru Tan,Shaozuo Yu,Yinghao Zhu,Ziyi He,Jiaya Jia*

Main category: cs.CL

TL;DR: 提出 SmartSwitch 推理框架解决大语言模型长链思维中的‘欠思考’问题，实验表明该方法提升了不同大小模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长链思维推理任务中存在‘欠思考’问题，限制性能和标记效率。

Method: 提出 SmartSwitch 推理框架，通过感知模块识别思维切换点并评估潜力，干预模块在高潜力思维被过早放弃时回溯并插入‘深化提示’。

Result: 在具有挑战性的数学推理基准测试中，该方法显著提升了不同大小的各种大语言模型的性能。

Conclusion: SmartSwitch 推理框架能有效解决大语言模型长链思维中的‘欠思考’问题，提升模型性能。

Abstract: The long chain-of-thought (LongCoT) capability is central to the recent
breakthroughs achieved by large language models in complex reasoning tasks.
However, the accompanying issue of ''underthinking'', where models exhibit
shallow reasoning by frequently switching thoughts without sufficient
exploration, limits both performance and token efficiency. To address this
problem, we propose a simple yet effective reasoning strategy: the SmartSwitch
inference framework. This framework can be easily integrated into any large
language model as a plug-and-play solution, continuously monitoring the model's
reasoning process to detect underthinking and guide it toward deeper
exploration of promising but overlooked thoughts. Specifically, the perception
module identifies points where thoughts switch and evaluates the potential of
the preceding thought using an off-the-shelf process reward model (PRM). If a
high-potential thought is found to be prematurely abandoned, the intervention
module interrupts the ongoing inference, backtracks to the point before the
switch, and inserts a "deepening prompt" to encourage further exploration along
that promising path. Extensive experiments on challenging mathematical
reasoning benchmarks demonstrate that our method significantly enhances the
performance of various large language models of different sizes.

</details>


### [300] [AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders](https://arxiv.org/abs/2510.19779)
*Yuezhou Hu,Jiaxin Guo,Xinyu Feng,Tuo Zhao*

Main category: cs.CL

TL;DR: 提出AdaSPEC方法解决投机解码中草稿模型与目标模型对齐问题，提升令牌接受率，在多任务中表现优于DistillSpec。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法目标与投机解码目标不一致，草稿模型因容量限制难以充分吸收目标模型知识，导致性能不佳。

Method: 在知识蒸馏过程中加入选择性令牌过滤，利用参考模型识别并过滤难拟合令牌，蒸馏出在简单令牌上与目标模型更对齐的草稿模型。

Result: 在算术推理、指令跟随、编码和总结等多任务中，AdaSPEC始终优于DistillSpec，最高提升15%接受率。

Conclusion: AdaSPEC能在不降低生成质量的情况下提升整体令牌接受率，是有效的解决方法。

Abstract: Speculative Decoding (SD) accelerates large language model inference by
employing a small draft model to generate predictions, which are then verified
by a larger target model. The effectiveness of SD hinges on the alignment
between these models, which is typically enhanced by Knowledge Distillation
(KD). However, conventional KD methods aim to minimize the KL divergence
between the draft and target models across all tokens, a goal that is
misaligned with the true objective of SD, which is to maximize token acceptance
rate. Therefore, draft models often struggle to fully assimilate the target
model's knowledge due to capacity constraints, leading to suboptimal
performance. To address this challenge, we propose AdaSPEC, a novel method that
incorporates selective token filtering into the KD process. AdaSPEC utilizes a
reference model to identify and filter out difficult-to-fit tokens, enabling
the distillation of a draft model that better aligns with the target model on
simpler tokens. This approach improves the overall token acceptance rate
without compromising generation quality. We evaluate AdaSPEC across diverse
tasks, including arithmetic reasoning, instruction-following, coding, and
summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.
Our results demonstrate that AdaSPEC consistently outperforms the
state-of-the-art DistillSpec method, achieving higher acceptance rates across
all tasks (up to 15\%). The code is publicly available at
https://github.com/yuezhouhu/adaspec.

</details>


### [301] [Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition](https://arxiv.org/abs/2510.19471)
*Yuu Jinnai*

Main category: cs.CL

TL;DR: 研究评估MBR解码在英语和日语的ASR和ST任务中的表现，发现其准确率在多数实验设置中优于束搜索，是有前景的离线高精度任务方法。


<details>
  <summary>Details</summary>
Motivation: MBR解码在文本到文本生成任务中表现优于束搜索，推测其在语音到文本任务中也有效，故而进行评估。

Method: 使用Whisper及其衍生模型，对英语和日语的ASR和ST任务评估MBR解码。

Result: 在多数评估的实验设置中，MBR解码的准确率优于束搜索。

Conclusion: MBR解码是有前景的离线ASR和ST高精度任务方法，代码开源。

Abstract: Recent work has shown that sample-based Minimum Bayes Risk (MBR) decoding
outperforms beam search in text-to-text generation tasks, such as machine
translation, text summarization, and image captioning. On the other hand, beam
search is the current practice for speech-to-text tasks such as automatic
speech recognition (ASR) and Speech Translation (ST). Given that MBR decoding
is effective in text-to-text generation tasks, it is reasonable to expect it to
also be effective for speech-to-text tasks. In this paper, we evaluate MBR
decoding for ASR and ST tasks on English and Japanese using Whisper and its
derivative models. We observe that the accuracy of MBR decoding outperforms
that of beam search in most of the experimental settings we have evaluated. The
results show that MBR decoding is a promising method for offline ASR and ST
tasks that require high accuracy. The code is available at
https://github.com/CyberAgentAILab/mbr-for-asr

</details>


### [302] [Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning](https://arxiv.org/abs/2510.19807)
*Xichen Zhang,Sitong Wu,Yinghao Zhu,Haoru Tan,Shaozuo Yu,Ziyi He,Jiaya Jia*

Main category: cs.CL

TL;DR: 提出Scaf - GRPO框架解决强化学习中‘学习悬崖’问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法受‘学习悬崖’现象制约，在处理超出能力的问题时模型停滞不前。

Method: 引入Scaf - GRPO渐进式训练框架，诊断学习停滞，注入分层提示。

Result: 在具有挑战性的数学基准测试中，Scaf - GRPO使Qwen2.5 - Math - 7B模型在AIME24基准上的pass@1分数相对香草GRPO基线提高44.3%。

Conclusion: Scaf - GRPO框架为解锁模型解决难题的能力提供了有效方法，是拓展大语言模型自主推理边界的关键一步。

Abstract: Reinforcement learning from verifiable rewards has emerged as a powerful
technique for enhancing the complex reasoning abilities of Large Language
Models (LLMs). However, these methods are fundamentally constrained by the
''learning cliff'' phenomenon: when faced with problems far beyond their
current capabilities, models consistently fail, yielding a persistent
zero-reward signal. In policy optimization algorithms like GRPO, this collapses
the advantage calculation to zero, rendering these difficult problems invisible
to the learning gradient and stalling progress. To overcome this, we introduce
Scaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive
training framework that strategically provides minimal guidance only when a
model's independent learning has plateaued. The framework first diagnoses
learning stagnation and then intervenes by injecting tiered in-prompt hints,
ranging from abstract concepts to concrete steps, enabling the model to
construct a valid solution by itself. Extensive experiments on challenging
mathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the
pass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative
44.3% over a vanilla GRPO baseline. This result demonstrates our framework
provides a robust and effective methodology for unlocking a model's ability to
solve problems previously beyond its reach, a critical step towards extending
the frontier of autonomous reasoning in LLM.

</details>


### [303] [Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning](https://arxiv.org/abs/2510.19733)
*M. H. I. Abdalla,Zhipin Wang,Christian Frey,Steffen Eger,Josif Grabocka*

Main category: cs.CL

TL;DR: 提出Zhyper框架解决大语言模型条件生成问题，参数更少且性能有竞争力，还可用于文化对齐。


<details>
  <summary>Details</summary>
Motivation: 提示工程无法保证大语言模型按期望条件生成内容，已有微调方法引入大量参数。

Method: 提出参数高效的因式分解超网络框架Zhyper，从文本描述生成上下文感知的LoRA适配器。

Result: 在多个基准测试中，Zhyper用少至26倍的参数取得有竞争力的性能，在文化对齐中泛化性更好。

Conclusion: Zhyper是解决大语言模型条件生成问题的有效方案，能在减少参数的同时提升性能和泛化性。

Abstract: Large Language Model (LLM) conditioning refers to instructing an LLM to
generate content in accordance with the norms and values of a specific culture,
beliefs of a particular political orientation, or any desired text-specified
semantic conditioning. Unfortunately, prompt engineering does not ensure that
LLMs behave in accordance with a desired conditioning due to the inductive bias
of the pre-training and alignment datasets. Prior works have focused on
fine-tuning LLMs by directly conditioning the LoRA weights; however, such
methods introduce a large number of parameters. As a remedy, we propose Zhyper,
a parameter-efficient factorized hypernetwork framework that generates
context-aware LoRA adapters from textual descriptions. Experiments on multiple
benchmarks show that Zhyper achieves competitive performance with up to 26x
fewer parameters than the state-of-the-art baselines. Furthermore, we extend
Zhyper to cultural alignment, demonstrating improved generalization to
out-of-domain settings and a better capturing of fine-grained contextual
values.

</details>


### [304] [Hubble: a Model Suite to Advance the Study of LLM Memorization](https://arxiv.org/abs/2510.19811)
*Johnny Tian-Zheng Wei,Ameya Godbole,Mohammad Aflah Khan,Ryan Wang,Xiaoyuan Zhu,James Flemings,Nitya Kashyap,Krishna P. Gummadi,Willie Neiswanger,Robin Jia*

Main category: cs.CL

TL;DR: 本文介绍用于研究大语言模型记忆的开源模型Hubble，包含标准和扰动变体，得出记忆风险相关结论并提出应对策略，还可用于多种研究。


<details>
  <summary>Details</summary>
Motivation: 开展大语言模型记忆的科学研究。

Method: 构建标准和扰动变体模型，在不同训练阶段插入文本进行训练。

Result: 发现记忆风险由敏感数据频率和训练语料库大小决定，无持续暴露的敏感数据会被遗忘。

Conclusion: 提出稀释敏感数据和让敏感数据提前出现的应对记忆风险最佳实践，Hubble可用于广泛记忆研究。

Abstract: We present Hubble, a suite of fully open-source large language models (LLMs)
for the scientific study of LLM memorization. Hubble models come in standard
and perturbed variants: standard models are pretrained on a large English
corpus, and perturbed models are trained in the same way but with controlled
insertion of text (e.g., book passages, biographies, and test sets) designed to
emulate key memorization risks. Our core release includes 8 models -- standard
and perturbed models with 1B or 8B parameters, pretrained on 100B or 500B
tokens -- establishing that memorization risks are determined by the frequency
of sensitive data relative to size of the training corpus (i.e., a password
appearing once in a smaller corpus is memorized better than the same password
in a larger corpus). Our release also includes 6 perturbed models with text
inserted at different pretraining phases, showing that sensitive data without
continued exposure can be forgotten. These findings suggest two best practices
for addressing memorization risks: to dilute sensitive data by increasing the
size of the training corpus, and to order sensitive data to appear earlier in
training. Beyond these general empirical findings, Hubble enables a broad range
of memorization research; for example, analyzing the biographies reveals how
readily different types of private information are memorized. We also
demonstrate that the randomized insertions in Hubble make it an ideal testbed
for membership inference and machine unlearning, and invite the community to
further explore, benchmark, and build upon our work.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [305] [Transfer Learning Beyond the Standard Model](https://arxiv.org/abs/2510.19168)
*Veena Krishnaraj,Adrian E. Bayer,Christian Kragh Jespersen,Peter Melchior*

Main category: astro-ph.CO

TL;DR: 研究转移学习在宇宙学推理中减少模拟成本的应用，指出机会与陷阱。


<details>
  <summary>Details</summary>
Motivation: 机器学习进行宇宙学推理需大量模拟，转移学习可降低成本。

Method: 在标准宇宙学模型预训练，在多种超越该模型场景微调，考虑不同转移架构。

Result: 可减少模拟数量，但存在负转移，瓶颈结构表现最佳。

Conclusion: 预训练可加速推理，但可能阻碍学习新物理。

Abstract: Machine learning enables powerful cosmological inference but typically
requires many high-fidelity simulations covering many cosmological models.
Transfer learning offers a way to reduce the simulation cost by reusing
knowledge across models. We show that pre-training on the standard model of
cosmology, $\Lambda$CDM, and fine-tuning on various beyond-$\Lambda$CDM
scenarios -- including massive neutrinos, modified gravity, and primordial
non-Gaussianities -- can enable inference with significantly fewer
beyond-$\Lambda$CDM simulations. However, we also show that negative transfer
can occur when strong physical degeneracies exist between $\Lambda$CDM and
beyond-$\Lambda$CDM parameters. We consider various transfer architectures,
finding that including bottleneck structures provides the best performance. Our
findings illustrate the opportunities and pitfalls of foundation-model
approaches in physics: pre-training can accelerate inference, but may also
hinder learning new physics.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [306] [Transformers are Inherently Succinct](https://arxiv.org/abs/2510.19315)
*Pascal Bergsträßer,Ryan Cotterell,Anthony W. Lin*

Main category: cs.FL

TL;DR: 提出简洁性衡量变压器表达能力，证明其表达力强且验证属性难解


<details>
  <summary>Details</summary>
Motivation: 提出用简洁性衡量变压器描述概念的表达能力

Method: 证明变压器比有限自动机和线性时态逻辑公式等标准表示法能更简洁地表示形式语言

Result: 验证变压器属性是EXPSPACE完全问题，证明难解

Conclusion: 变压器具有高表达力，但验证其属性有困难

Abstract: We propose succinctness as a measure of the expressive power of a transformer
in describing a concept. To this end, we prove that transformers are highly
expressive in that they can represent formal languages substantially more
succinctly than standard representations of formal languages like finite
automata and Linear Temporal Logic (LTL) formulas. As a by-product of this
expressivity, we show that verifying properties of transformers is provably
intractable (i.e. EXPSPACE-complete).

</details>
